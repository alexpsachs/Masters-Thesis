{
    "mleinart": "Thanks, merging this in. I do have a rewrite from jblaine I still need to tweak and commit but this is fair and simple start\n. Thanks for catching this, merged\n. I'm a bit hesitant on this as Graphlot has basically been abandoned since it was merged in. Are you actively using the feature or just experimenting at this point?\n. Looks like there's enough interest to merge this. Sorry for letting it sit so long.\nI hope putting this in will prompt others to do some work on the Graphlot interface.\n. I've merged this into both master and 0.9.x branches. Thanks for the pull!\n. This has been merged though I've started a new distro/ tree for stuff like the postinstall script and init scripts and moved the postinstall there\n. Makes sense to have this, thanks a lot for the addition\n. I think supporting 2.4 for everything but Ceres might be a decent compromise, though making Ceres work with 2.4 might be ok. My hope was that we could start phasing out the 2.4 support since RHEL6 was finally released (RHEL5 being seemingly the only reason anyone needs 2.4 support) and Ceres is a natural place to start.\nIn the meantime we should make this version not fail to start if ceres doesnt exist, that way it doesn't block anyone from testing on 2.4.\n. Thanks!\n. Beautiful, thanks\n. Thanks, this looks good. I'll merge it just as soon as I have a chance to test it myself\n. I've merged this into master and cherry-picked it into 0.9.x as well. Thanks again!\n. Thanks a lot for finding this. This was an oversight that unfortunately made it into the last release.\n. This looks to be fixed with b31bfea273161cbdff305d287a6b1783fac3a03b\n. Thanks a lot for this, looks like RRD didnt get well tested in master before the storage refactor was merged.\n. Thanks, sorry this got missed\n. So the browser you're using is set to CEST, right? The problem is that it's using that rather than the timezone set in Django - see: https://github.com/graphite-project/graphite-web/blob/master/webapp/content/js/composer_widgets.js#L85-L86\nI think the solution is to include the &tz variable by default in the generated graph URL and use it in the composer's time header. Arguably, it makes sense to default to displaying in the user's timezone rather than the server's.\nOptimally we'd allow the user to change the timezone explicitly in the UI as well\n. Opened #244 for this enhancement\n. I believe the patch as is is causing the dates to display in the browser's timezone in which case it'd still be incorrect as in issue #19\nI think both issues can be knocked out together by adding the tz parameter by default and picking it up here\n. Opened #244 for this as an enhancemen\n. Thanks for this fix, it's very helpful\n. You can hit /metrics/index.json to get the entire tree but this will cause the webapp to walk the entire metric directory tree on the filesystem each time it's called which could take longer than you want (or even longer than your webserver's request timeout).\nYou may instead want to walk the tree as the graphite webapp does: use the /metrics/find endpoint. Start with /metrics/find?query=* and build up from there as needed (/metrics/find?query=carbon.* etc).\n. Looks good, thanks for the pull request\n. Great, I'll get this looked at as soon as I can.\nThanks for the work\n. Sorry that I've let this sit a month without comment @Dieterbe. I'll get this merged in today\n. I've been struggling to reproduce this in 0.9.x, 0.9.10, and 0.9.9. I know it's a pain but a sample render URL along with some whisper files would be helpful here\n. @obfuscurity Yeah, I think that's fine - I'm guessing it's something about the other metrics that's blocking me from reproducing it. The deploy metrics should be straightforward to reproduce. Honestly a format=json might've been good enough but it'll save me some work to see the whisper files.\nCan you give me the storage-schemas blurb for the deploys though? I take it those have a value of '1'?\n. Finally reproduced after creating a few thousand 'deploy' metrics for the wildcard and using transformNull() to set them to 0.\nAdded another messy if clause to skip drawAsInfinite metrics when computing the stack to resolve :/\n. Thanks! I've finally merged this into master\n. @diegovar indeed, I've reached the same conclusion. I think it exists in dashboard just from being copied from the same thing in browser.js:  https://github.com/graphite-project/graphite-web/blob/master/webapp/content/js/browser.js#L122\nThat was added here: 4f7c4eecc90f6d38c2b4d5c5c261e7bfc328921a but it's not clear why.\nAlso, @slackhappy's right that ff8e1a5e7db25193d3bbb1b891494ec2aefc5a27 seemed to make things worse rather than better as it breaks url decoding in some of the most common cases.\nI'd like to try reverting the decodeURIComponent, but first it'd be great if a few people tried it (add .patch at the end to download a diff): https://github.com/mleinart/graphite-web/commit/1985a8900d0b7cd41c6dcf0e15d4d4c65491fec1\nThat should apply to master, 0.9.x, and 0.9.10 without reject.\nWith this patch applied, % and # in metric names seem to be handled alright in both composer and dashboard including saving and loading user graphs, but I'm not confident I've tested it enough to merge it so some +1s would be helpful\n. Alright, I've merged b31bfea273161cbdff305d287a6b1783fac3a03b which I think resolves these encoding issues. Please reopen if there are any recurring symptoms\n. Related to issue #31 (as noted above)\n. This was fixed in de3f08c8157e75eadba80bc09eaa07201fa396ce\n. I just grabbed the same change from pull request #18 so it's fixed now.\nThanks for taking the time to submit a pull\n. Thanks for the correction, merged\n. It was attempted but we weren't able to get the build on readthedocs.org to work with the custom install location - the import of functions.py for the functions doc generation depends on the virtualenv used to build the docs being able to import it and the install fails (due to /opt/graphite not being writable).\nDefinitely still something we want to do, it's just going to have to wait until we either decide to stop installing in /opt/graphite by default (possible but not yet discussed) or come up with a patch for readthedocs to allow the appropriate options to be passed to pip/setup.py\n. Thanks for the documentation! I've merged this in with a few tweaks\n. Thanks, I've merged this in with some changes/fixes. fetchData() wont handle the case where series input into the movingAverage() and movingMedian() functions - evaluateTarget is a better choice. I also switched this to use the holt-winter functions' _fetchWithBootstrap which does the fetch without re-requesting the data already passed in as well as adjusting for the case where the bootstrap data has a different step than the original series\n. Thanks, this has been merged into master and 0.9.x\n. Thanks, I've merged this into master and 0.9.x branches\n. Thanks for all the eyes on this. I've closed issue #33 with a commit of what I think fixes the issues. Please reopen it if you still see any problems \n. Thanks, this is merged into master and 0.9.x \n. You should be able to do this as-is. If you set your remote graphite servers in the CLUSTER_SERVERS setting in local_settings.py, they will be queried for their metric names and data while browsing and rendering. If you have three servers, you can set this on each of them pointing to the remaining two and be able to see all data from any of the instances.\n. This is fixed with pull #397 \n. Looks good, thanks for the catch\n. Thanks, documentation corrections are great to see\n. These commits are in there, it looks like the history is just different - probably due to the branch being merge into 0.9.x first and then cherry picked (resulting in different SHAs for the commits). A merge doesn't do anything but add an extraneous merge commit to add the additional SHAs to the history:\n```\n$ git show vimeo/master\ncommit 98d4562446124e82a96257c5cfb5059c3040f64e\nMerge: fa7768b 0af48ff\nAuthor: Dieter Plaetinck dieter@vimeo.com\nDate:   Thu Oct 11 12:29:30 2012 -0400\nMerge branch 'master-with-missing-commits'\n\n$ git checkout -b test_vimeo_merge\nSwitched to a new branch 'test_vimeo_merge'\n$ git merge vimeo/master\nAlready up-to-date!\nMerge made by the 'recursive' strategy.\nmleinartas@calicommon:~/src/projects/graphite-web$ git diff --stat master\nmleinartas@calicommon:~/src/projects/graphite-web$\n```\nYou should be able to merge the official master back into yours to reconcile it. In the future you might want to commit work intended for pull requests into separate independent branches (branched from upstream may be optimal). This way, even if your commit history differs somewhat from master or you've commited forks of your own, pulls can still be made clean.\n. You should use the 0.9.x branch as a stable branch. A new release (0.9.11) should be coming in the next month\n. I've seen this requested a couple of times in the past so it's clear this will be useful for at least a few use cases.\nOne note on implementation:\nThe normalize() call will give the resultant series the same precision as the lowest-precision series passed to it. For instance, if 3 series match and two are minutely metrics and one is hourly, the resulting series will be hourly. In reality this isn't very likely to be an issue at all as the metrics a user wildcards will likely be in the same general category.\n+1\n. Thanks! Time range makes more sense here than points. This has been merged in\n. I've updated check-dependencies.py to be more accurate and only require one\n. This is addressed in enhancement #242 (was #144)\n. I suspect this may be possible to base on the built in svg rendering too (with supporting javascript)\n. safeSum(), safeDiff() I think are too low-level of a place to make this change. See for instance averageSeries(): https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/functions.py#L258\nIgnoring the None values makes sense in a lot of the places these functions are used. I think this may be an appropriate option for sumSeries, diffSeries, etc functions to have but I wouldn't want to change the default as it'd break quite a few people - a common use case for sumSeries is to join up sparse data. \n. Merged into master and 0.9.x, thanks!\n. Looks good, thanks for the pull\n. I can't reproduce this on 0.9.x anymore. I believe this was fixed by b31bfea273161cbdff305d287a6b1783fac3a03b\n. I believe this is fixed with pull #397. I'm not sure what the reported admin console issue was (perhaps fixed by 180fe4692edbd74698a5634bae290caf0f9aaf89 ?) but I can't reproduce it. \n. I've switched this to %g which should do the right thing, though it'll use scientific notation after 4 decimal places. To satisfy that, i've fixed the grammar to parse scientific notation as well here: ae910ae391995e34f3dee5da63d2793e3114b19e\n. Thanks, I'm going to leave this in 0.9.x branch but for master cleanups like this make sense to add\n. @drawks sounds good. I'll see what I can get closed out over the weekend\n. Thanks for your patience all, we should have an 0.9.11-pre1 cut this week. From there we can see where the bugs take us :)\n. Good news: I'm sitting down right now to go through issues and get a clear list together for bugs we should squash before 0.9.11. I'm also finally migrating old issues off of Launchpad and closing that down as we're still getting fresh bugs over there.\nWatch this and grab anything you have a mind to: https://github.com/graphite-project/graphite-web/issues?milestone=1&state=open\nWe did indeed cut 0.9.11-pre1 so non-bugfix pulls aren't going to make 0.9.11. We'll do another pass real soon though to merge in most of them.\nThanks all for your patience!\n. Boom. http://graphite.readthedocs.org/en/0.9.11/releases/0_9_11.html\nEveryone note the security mentions!!\n. Thanks!\n. Grabbed this out of pull #110 \n. FYI I've cherry-picked the commit in question into 0.9.x:c8b6e1ec8db45fc36c11a148d6065c121b858efd\n. Thanks, I've squash-merged this and added a small doc blurb and menu item for it.\n. It's not optimal, but you can use https://graphite.readthedocs.org/en/0.9.x/functions.html#graphite.render.functions.removeAboveValue to turn anything above 0 to Nones and then https://graphite.readthedocs.org/en/0.9.x/functions.html#graphite.render.functions.transformNull to change those back into 1s\n. This is pretty neat, if someone would build on this further I think it'd be a good addition. A few things that would make this more complete:\n- support mixed negative and positive numbers\n- use drawRectangle() and fill the bars rather than doing the trace manually\n- make barWidth or barSpacing a configurable value (perhaps as a max or min rather than a constant)\n- draw multiple series by laying bars beside each other\n  - This may entail making a special case for xStep calculation/series consolidation\n- support stacked bars\n  - This would need different rendering logic from the normal stacked mode\nI started on 1 and 2 here https://github.com/mleinart/graphite-web/commit/9ed56690438500c499577e61b285a171c3d897ca but probably wont get around to getting it fixed up myself\n. This appears to merely be a quirk of the stacking code and will exist until/unless someone refactors the stacking code in glyph.py (especially the per-series stacks) or could possibly be dealt with in #241\n. Thanks for this, it's a good idea. I've added a few comments on how we can improve it further\n. Thanks! This has been merged to 0.9.x and master\n. Thanks, I think this is fixed now here: https://github.com/graphite-project/graphite-web/commit/7f649535d02c21a527e8aa555876999d82d4d94e\nPlease reopen if you see any other issues relating to this\n. Thanks for the cleanup\n. Thanks for this, I'll pop it into both branches\n. Thanks for this - index.json is kindof a hack but there are some tools that depend on it so it's good to extend this support. At some future point we may want to actually use the Finder for this so that it can include remote data or other datasources, but for now let's get it in\n. This should be fixed in the above commit. The tz param previously was only picked up by the renderer itself and ignored by requests like format=csv that are handled entirely in the view itself\n. The json timestamps (and format=raw as well) are in unix epoch time which is UTC by definition. Anything needing timezone conversion when receiving epoch time will be doing it itself with that assumption (http://en.wikipedia.org/wiki/Epoch_time)\nThanks\n. Thanks @Dieterbe, this is straightforward and useful. Can you add this to composer_widgets.js as well? I supposes this would be in the \"Special\" category\n. Yeah I think i was wrong to suggest this go in composer_widgets as right now we dont have any functions in there that generated data without a timeseries.\nThis got merged so I think we're good now as is. Thanks!\n. Hm, changing this to secondly will mean that the values for an updating graph (until=now) will only ever be cached for 1 second - after that, the key the cached data is stored under will have changed (before it's actually expired). Have you tried setting DEFAULT_CACHE_DURATION=10 in local_settings.py? Though the cache key will still be the same that entire minute, the lower ttl on the entry will ensure a cache miss and re-read from disk every 10 seconds for a given graph.\n. I've created an enhancement for this: #243 \n. Sorry, I just reverted this as I havent had a chance to look closely at it and I think it needs some deeper review to see how this will work given the way the browse tree works in master. Additionally, I think it may make sense for this to be implemented in the grammar so that for instance the '...' isn't necessary. \nI'm doing my best to find a way to 're-open' this, though github sees it as 'merged'\n. Thanks for submitting this. I've merged along with Justin's suggested 2.4 compat\n. Does your load balancer have the equivalent of Apache mod_proxy's ProxyPreserveHost option?\nI wont rule out Graphite doing something wrong here, but it should be using what's passed in the HOST header for any redirects. If your load balancer is setting the target machine instead of passing through what's sent by the user's browser, it's going to use that for redirects and whether the request is secure or not.\n. Sorry to let this sit so long!\nThanks, this is fantastic work. I'm especially grateful for the documentation work, you've done a great job.\nI'm ready to merge it, but i wonder if we could improve the config a bit. It feels confusing that the three settings conflict with each other. I expect we'll be seeing people confused that when they enable all three, their groups and users arent enforced (since the first short-circuits the remaining).\nDo you have any thoughts on how better to apply the settings? I'll leave it to your judgement but I see a few options:\n1. Leave DASHBOARD_REQUIRE_AUTHENTICATION as the main on/off switch for the feature and have a separate option to configure what permissions are required (something like 'all', 'group', 'user')\n2. Leave DASHBOARD_REQUIRE_AUTHENTICATION as the main on/off switch and allow any combination of the other two options. It's conceivable that you check whether a user's in a group OR if they're explicitly allowed via their permissions.\n3. Evaluate them such that the effective precedence is respected. That is, all 3 enabled would be equivalent to only DASHBOARD_REQUIRE_PERMISSIONS being enabled. This is effectively the same as current but less prone to user error\nI think I lean towards 2 and there are probably other options to explore. What do you think?\n. I like it (DASHBOARD_REQUIRE_EDIT_GROUP). Thanks!\n. Damn, missed your quick update last week, sorry for another delay. Merged!\nThanks again\n. Doh. I just opened a new one for this after forgetting about this one. Closing in favor of #242 as there are a few more implementation notes\n. Thanks, this has been merged in\n. Thanks Dieter\n. Thanks Jeff\n. Thanks for the detailed reports\n. Thanks! I've put the ATTime fix into 0.9.x as well\n. Doh. Thanks\n. Definitely per-pattern defaults is slipping down the slope pretty hard..\nMy own thoughts have been that we could stand to allow most anything to be set in graphTemplates.conf for per-site defaults. yMin=0 would be a sane default for many shops to configure, as would specific render styles like lineMode=connected.\nDefinitely this could end up causing some headaches in supporting these users - asking \"so what's your graphTemplates.conf look like\" would likely end up being a standard question.\nPersonally I dont have a very strong opinion against it and it's been requested a lot lately, but I am always concerned about things affecting our ability to support users when they have problems.\nA thought: perhaps it solves some of the problem to instead allow this as a composer configuration? The built-in dashboard already supports this (in an ugly way). This way any arbitrary parameters that a user sets end up in the render request explicitly.\n. Thanks Dieter, this is in master and 0.9.x now\n. Thanks, looks good\n. Thanks for the fix, I'm pretty sure this is reasonable behavior to have. The node who has the metric data should be responsible for doing the carbon query. Without this fix, some installs may be querying their carbon-cache more than necessary.\nManually merged this into 0.9.x since it was on master pre-split of 0.9.x\n. #### Comment by chrismd on Jun 08, 2011:\nI completely agree, functions.py's days are numbered. Purely for organizational purposes it should be broken up into a package, but also the function table should pull function definitions from multiple sources: the builtin functions package and optionally a user-defined package.\n. #### Comment by mleinartas on May 29, 2012:\nAdditionally, the webapp should serve up an endpoint with information about the available functions, their method signatures, category, and docstrings. This would make it possible for the frontend to generate the functions menu dynamically and provide full help within the interface.\n. #### Comment by nick-stielau on Feb 09, 2012:\nFrom mleinart in #graphite:\nHolt-winters is fragile about missing data points.  You can try using keepLastValue or movingAverage to smooth out the data.\nI'm still seeing issues, would love this to be a little more robust!\n. #### Comment by mleinartas on May 29, 2012:\nI think where this tends to fail is when the bootstrap times cant be pulled or are empty. Use of keepLastValue() has failed in the past due to it not setting a pathExpression on the new series, causing the bootstrap to be pulled without keepLastValue applied. The pathExpression has been corrected on this as well as other functions which create new series in 0.9.10 which may help this somewhat.\n. #### Comment by nate-natemurray on Nov 21, 2012:\nThis still seems to be an issue even in 0.9.10. I'm not actually to get holtWinters* to work with any metrics. I always get \"IndexError: list index out of range.\" This includes using keepLastValue or movingAverage. e.g.:\ntarget=holtWintersForecast(keepLastValue(statsd.numStats))\nGiven that I have never successfully gotten holtWinters* to run, I wonder, am I using the function incorrectly?\n. This was begat by this question: https://answers.launchpad.net/graphite/+question/193066\n. #### Comment by thomasfalkenberg on Aug 01, 2012:\nThis workaround did not work for me, still the same behavior. \n. #### Comment by cody-stevens on Nov 01, 2012:\nJust to add some more \"hints\".  I was experiencing this same hanging behavior.  I started digging into the code and troubleshooting  using the workaround mentioned above.  That was producing svg output that the browser would render up until the error.  With it partially working I put the original code back in just out of curiousity and now it is working just fine.  It made me feel like I was taking crazy pills.  I saw in some other bug submissions that for some users if they let it time out subsequent requests would work.  Not sure if that is what I am seeing here or not but thought it may be useful information. \n. Marking as a duplicate of #216 \n. #### Comment by chrismd on Oct 26, 2009:\nOne step closer to implementing an auto-refresh that is based on the lowest-common-denominator for the current metrics' precision. I've implemented per-metric context data persistence that I will use in in the future to determine the refresh rate.\n. #### Comment by chrismd on Sep 09, 2010:\nI agree there needs to be a better way to manage complex graph elements. The best approach is probably to handle this in the UI code, to be able to store complex graph elements in some fashion that is separate from the graph.\n. #### Comment by chrismd on Dec 31, 2010:\nThere is actually a recently added function that can help with this type of problem, called exclude(). It is available under the 'Filter' functions menu (in trunk and the upcoming 0.9.7 release that is). You apply this function to a graph target that matches too many things and give it a regular expression which is used to hide any metrics in the dataset that match the regex. It isn't an ideal solution but it may be helpful in this situation.\n. #### Comment by mleinartas on Feb 20, 2012:\nConfirmed, this is annoying especially when using drawNullAsZero.\nI think it would be reasonable behavior to drop the last value retrieved if it's None and the current time is still within the window of that value's time slot. \n. #### Comment by nleskiw on Feb 23, 2012:\nConfirmed.\n. #### Comment by mleinartas on May 29, 2012:\nThis behavior also existed in the composer view and was fixed in 0.9.10. It should also be fixed in the dashboard target view\n. #### Comment by mleinartas on May 29, 2012:\nIndeed, this is inconsistent and should be corrected\n. #### Comment by elfving-anders on Jan 31, 2013:\nThis seems to be caused by hardcoded strftime for the xaxis in glyph.py.\nI did a quick and dirty fix in my personal deployment for now by replacing:\ndict(seconds=100,   minorGridUnit=HOUR, minorGridStep=2,  majorGridUnit=HOUR, majorGridStep=4,  labelUnit=HOUR, labelStep=4,  format=\"%a %l%p\", maxInterval=6*DAY),\ndict(seconds=255,   minorGridUnit=HOUR, minorGridStep=6,  majorGridUnit=HOUR, majorGridStep=12, labelUnit=HOUR, labelStep=12, format=\"%m/%d %l%p\"),\nwith\ndict(seconds=100,   minorGridUnit=HOUR, minorGridStep=2,  majorGridUnit=HOUR, majorGridStep=4,  labelUnit=HOUR, labelStep=4,  format=\"%a %H:%M\", maxInterval=6*DAY),\ndict(seconds=255,   minorGridUnit=HOUR, minorGridStep=6,  majorGridUnit=HOUR, majorGridStep=12, labelUnit=HOUR, labelStep=12, format=\"%m/%d %H:%M\"),\n. #### Comment by 3-eamon on Nov 08, 2012:\nSflow & HBASE all want to send in data with dots. \nBut that breaks graphite.... :(\n. #### Comment by nleskiw on Nov 09, 2012:\nEverything in Graphite (whisper, carbon, ceres, the webapp, all the Javascript in Composer and Dashboard) makes the assumption that the separator is a dot.  It would be a significant undertaking to try and find all the references and subtle assumptions about dots everywhere and change it to use an arbitrary delimiter.  It would be much easier to write a daemon that sits in between your collector and alters the string before it gets to Graphite or, if your tool supports it, an output plugin specifically for Graphite that will change dots to underscores and send Graphite friendly metrics to Carbon.\nThat being said, I'm not closing this out or marking it 'Won't Fix'.  If some enterprising developer does all the leg work and gets it working, we would (after considerable testing) most likely accept a patch for this. \n. #### Comment by shane-hathawaymix on Mar 09, 2011:\nFWIW, RestrictedPython might help here.  It would allow Graphite to use arbitrary Python expressions without adding any new security concerns.\nhttp://pypi.python.org/pypi/RestrictedPython\n. #### Comment by chrismd on Sep 25, 2011:\nDeferring this til after 0.9.9 when I'll be merging in the completely refactored storage API from the 1.1 branch.\n. #### Comment by nleskiw on Sep 27, 2011:\nThis is probably a duplicate of 8148000 and 850475. Read the explinations there and check if your data is changing with &rawData=true as one of the URL parameters. \n. #### Comment by nleskiw on Sep 27, 2011:\nThere's  an extra zero on the first bug number. Should be 814800.\n. #### Comment by ohlol on Sep 27, 2011:\nHmm, possibly. I'll check the raw data.\nThe thing is, I see other discrepancies that look a lot worse after reloading:\nhttp://dl.dropbox.com/u/1613178/render4.png\nhttp://dl.dropbox.com/u/1613178/render5.png\n. #### Comment by ohlol on Sep 27, 2011:\nHere's another grab of the same graph from just now:\nhttp://dl.dropbox.com/u/1613178/render6.png\nIt looks like at least some of the datapoints from the blue line in render4.png are being plotted for the green series in this render! Wth?!\n. #### Comment by nleskiw on Sep 28, 2011:\nPlease send the rawData, and read comment 5 in bug #850475 carefully.\nThat explains some of Graphite's behind the scenes logic on how to solve a complicated problem.\n. #### Comment by nleskiw on Sep 28, 2011:\nAlso, for http://dl.dropbox.com/u/1613178/render6.png areaMode=all WILL cover other metrics up.  I think you meant to use areaMode=stacked\n. #### Comment by ohlol on Sep 28, 2011:\nIt does use areaMode=stacked (just double checked).\nI'm at a conference all week, but I'll try to get the timeseries data.\n. #### Comment by ohlol on Sep 28, 2011:\nHere is a screenshot where you can see that I set areaMode=stacked but the render doesn't seem to obey that for the timeseries in blue.\nhttp://dl.dropbox.com/u/1613178/areamode.png\nI also verified that there is only one areaMode parameter in the URL.\n. #### Comment by chrismd on Oct 04, 2011:\nThis doesn't look like x-axis aggregation, it looks like your series are aggregates of many individual metrics, probably scattered throughout a cluster, and the problem is that some of the cluster calls are timing out, resulting in partial aggregations in your graphs. A few things to check:\nHow loaded is apache on each machine in the cluster?\nDo you see errors or timeouts in webapp logs?\nAlso please let me know the layout of your cluster from a carbon daemon standpoint. Are you using relay rules or consistent hashing? Duplicating data or just sharding it?\n. #### Comment by ohlol on Oct 04, 2011:\nHey Chris, thanks for the reply.\nIt is an aggregate of a bunch of metrics. I do experience this on graphs with only a few (under 10) metrics sumSeries()'d.\nI've only got one graphite/carbon host and it's not very loaded.\ntop - 23:28:02 up 24 days, 22:59,  1 user,  load average: 2.30, 2.88, 2.90\nTasks: 162 total,   3 running, 159 sleeping,   0 stopped,   0 zombie\nCpu0  : 29.9%us,  5.9%sy,  0.0%ni, 52.5%id,  4.5%wa,  0.0%hi,  7.3%si,  0.0%st\nCpu1  : 36.2%us,  7.6%sy,  0.0%ni, 54.1%id,  1.8%wa,  0.0%hi,  0.3%si,  0.0%st\nCpu2  : 30.1%us,  7.9%sy,  0.0%ni, 50.3%id, 10.8%wa,  0.3%hi,  0.6%si,  0.0%st\nCpu3  : 28.9%us,  6.5%sy,  0.0%ni, 60.9%id,  3.4%wa,  0.0%hi,  0.3%si,  0.0%st\nMem:   4057064k total,  3413264k used,   643800k free,    90284k buffers\nSwap:   999416k total,    21168k used,   978248k free,  1221708k cached\n$ iostat -dx sdb 1 5\nLinux 2.6.32-24-generic (host.domain.com)    10/04/2011      x86_64        (4 CPU)\nDevice:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util\nsdb               1.50   422.71   31.98 1143.03   968.23 12525.89    11.48     1.08    0.92   0.22  25.48\nDevice:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util\nsdb               0.00  2387.00    4.00   51.00    32.00 19504.00   355.20     0.04    0.73   0.18   1.00\nDevice:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util\nsdb               0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00   0.00   0.00\nDevice:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util\nsdb               0.00     8.00    0.00 4333.00     0.00 34728.00     8.01    49.49   11.13   0.08  35.00\nDevice:         rrqm/s   wrqm/s     r/s     w/s   rsec/s   wsec/s avgrq-sz avgqu-sz   await  svctm  %util\nsdb               0.00   722.00    0.00 4003.00     0.00 37800.00     9.44    85.36   21.64   0.15  59.00\nIt's an SSD, so has plenty of more room. The webapp exception.log hasn't been written in over two days. Apache error log only complains about a missing handler for the logger \"cache\".\nThis is the only host that writes data. In terms of carbon layout, this data is sent to statsd on the same host which writes to carbon. I do have a relay but it doesn't relay these metrics.\n. #### Comment by chrismd on Oct 04, 2011:\nWow this is extremely unusual behavior for a single machine to exhibit. The reason I thought you had a cluster is because the results look very much like certain individual metrics that contribute to the aggregate are only returned in some graphs but not all. That is easily explainable in a cluster where a remote fetch timing out causes this, but on a single machine I must save I've never seen anything like this before. I would suggest experimenting some more to try and find a pattern, specifically: remove the aggregation functions so you get all the individual metrics graphed (or better yet, use rawData=true so you can differentiate them) and repeat that request many times, see which metrics come and go from the results, if any.\n. #### Comment by chrismd on Oct 05, 2011:\nAnother thing to try, just in case its a bizarre form of x-axis aggregation I've never seen before is to repeat the same request one a second and see if there is any regularity to when it changes, if it were caused by x-axis aggregation I'd expect it to change every time a new datapoint is received.\nAlso, are you using memcached?\n. #### Comment by ohlol on Oct 05, 2011:\nHey Chris,\nI do not use memcached.\nI got about two minutes worth of images and raw data (JSON) for them (or at least close). You can view the first 20 here: http://dl.dropbox.com/u/1613178/render-weirdness/index.html\nI've uploaded a tarball of those 20 images and the raw data at http://dl.dropbox.com/u/1613178/render-weirdness.tgz .\nI'll try to analyze the raw data today but I don't see a pattern in the images. It looks like there are three different renders here.\n. #### Comment by ohlol on Oct 05, 2011:\nSorry, meant to add: specifically compare the data around 12am on image 1, image 9 and image 16. There is variance in other parts of the image but that spot is the most obvious.\n. #### Comment by ohlol on Oct 05, 2011:\nI just compared the datapoints in those 20 graphs. The only datapoint that doesn't match is the very last one. This seems to reaffirm my hunch that this is a rendering bug.\nThe script I used to compare them is at http://dl.dropbox.com/u/1613178/render-weirdness/compare.py\n. #### Comment by chrismd on Oct 08, 2011:\nOk I just took a fresh look at your original graphs and I can definitely see the problem, can't believe I missed this the first time.\nCompare render1.png with render3.png. The shapes match up perfectly, and the values do to with 1-2 exceptions, the only real difference between the two graphs is the Y-axis range. On render1 it ranges from 4k-14k and on render3 its 0-14k. Graphite picks the Y-axis range dynamically based on the dataset its rendering, and this means that during render1 it only had values in the 4k-14k range, while the dataset being rendered in render3 had a value < 4k. You were refreshing the same image repeatedly so you'd think its the same dataset being rendered but actually, depending on your retention configuration you could be flipping between results in different precisions, I bet your finest precision archive covers 1-day (the duration of the graphs, so this request would be 'on the cusp' of aggregation). This would explain the 1-2 exceptions like the dip at 20:00, there is a datapoint < 4k in the finest archive but the averaged, lower precision value is > 4k.\nSimilarly, compare render1.png to render2.png. It's the same story, close values but different Y-axis range. Also render2 visibly has fewer datapoints than render1. This  makes render2 look smoother and render1 more jittery.\nCan you post your storage-schemas.conf?\n. #### Comment by ohlol on Oct 10, 2011:\nSure, here it is:\n[collectd]\npattern = ^collectd..*\nretentions = 10s:12h,1min:1d,5min:2y\n[haproxy]\npattern = ^haproxy..*\nretentions = 10s:12h,1min:1d,5min:2y\n[puppet_reports]\npattern = ^puppet.reports..*\nretentions = 30min:7d\n[statsd]\npattern = ^stats..*\nretentions = 10s:12h,1min:1d,5min:2y\n[everything_else]\npattern = .*\nretentions = 10s:12h,1min:1d,5min:2y\nHere's another set of graphs varying through a few reloads:\nhttp://dl.dropbox.com/u/1613178/more-render-badness.png\n. #### Comment by chrismd on Oct 11, 2011:\nWhich scheme is used by the affected metrics? No metric paths in the graphs.\n. #### Comment by ohlol on Oct 11, 2011:\nIn the graphs I've posted so far, I think just statsd and collectd (the last one I posted today)\n. #### Comment by chrismd on Oct 18, 2011:\nHm... so there is more than one issue going on here. Your latest example, more-render-badness.png, shows a big metric simply vanishing. That can't really be explained by aggregation, do you have any errors in your webapp logs?\nAll the examples so far seem to be from=-24h requests, can you try from=-25h and from=-23h and see if any issues occur in those cases? I would expect the aggregation issues to go away in those cases.\n. #### Comment by ohlol on Oct 20, 2011:\nHey Chris,\nSo first off, the \"vanishing metric\" actually doesn't exist at all - that time series shouldn't look like that at all. In that case the third graph is correct, but the first two are not. Also note the change in color between graphs 1 and 2.\nI just fetched 100 graphs with the following timeframes:\nfrom=-23h\nfrom=-1d (and from=-24h)\nfrom=-25h\nThere is quite a visible difference in the graph rendering between retention period boundaries. -23h/-25h all rendered fine, but -24h/-1d had messed up data similar to the render-badness graph.\n. #### Comment by ohlol on Oct 30, 2011:\nI just began experiencing this (vanishing/moving metric data) with from=-25h on data that has a retention of:\n1min:1d, 5min:2y\n. #### Comment by chrismd on Nov 03, 2011:\nHey scott sorry for the delay I finally got a chance to look at this again. I believe you are correct that it is a rendering problem because the data in the .dat files does not show any abnormalities while the graphs obviously do. I think this is related to the areaMode=stacked code paths because the affected graphs show that either both metric2 and metric3, or just metric3, do not seem properly stacked on top of metric1. I've reviewed the relevant code and nothing obvious jumps out at me as being wrong, so we'll need to get some more data to narrow this down.\nCan you try to replicate this problem using areaMode=first as well as no areaMode at all? I am very confused by the huge difference between what changes in the more-render-badness.png vs in the render-weirdness.tgz examples, so more data would help at this point.\nHere are some specific questions that would help.\n- Do all graphs exhibit this problem or just some?\n- How many exhibit it?\n- Is there any commonality in those that do?\n- For the graphs that do have the problem, what % of the time would you say they render incorrectly?\n. #### Comment by chrismd on Nov 03, 2011:\nAlso to expand on the \"Is there any commonality in affected graphs\" question, do the affected graphs always involve rendering functions? Which ones?\n. #### Comment by trygve-vea-gmail on Sep 21, 2012:\nThis sounds similar to what I'm experiencing;\nhttp://i.imgur.com/hNpd5.png\nhttp://i.imgur.com/KJHld.png\nAlso note that the Y-scale changes significantly between reloads - and the first graph is completely wrong (the Idle value is suddenly 4 times of what it should be - which Munin renders correctly).\nThe render URL I used was;\n/render/?tz=Europe/Oslo&from=-1week&hideLegend=false&bgcolor=FFFFFF&fgcolor=000000&width=450&height=270&title=CPU%20usage%20(1%20week)&target=alias(color(stacked(scale(derivative(jigsaw.example_acme_org.munin.cpu.system)%2C0.01667))%2C%22%2300cc00%22)%2C%22system%22)&target=alias(color(stacked(scale(derivative(jigsaw.example_acme_org.munin.cpu.user)%2C0.01667))%2C%22%230066b3%22)%2C%22user%22)&target=alias(color(stacked(scale(derivative(jigsaw.example_acme_org.munin.cpu.nice)%2C0.01667))%2C%22%23ff8000%22)%2C%22nice%22)&target=alias(color(stacked(scale(derivative(jigsaw.example_acme_org.munin.cpu.idle)%2C0.01667))%2C%22%23ffcc00%22)%2C%22idle%22)&target=alias(color(stacked(scale(derivative(jigsaw.example_acme_org.munin.cpu.iowait)%2C0.01667))%2C%22%23330099%22)%2C%22iowait%22)&target=alias(color(stacked(scale(derivative(jigsaw.example_acme_org.munin.cpu.irq)%2C0.01667))%2C%22%23990099%22)%2C%22irq%22)&target=alias(color(stacked(scale(derivative(jigsaw.example_acme_org.munin.cpu.softirq)%2C0.01667))%2C%22%23ccff00%22)%2C%22softirq%22)&target=alias(color(stacked(scale(derivative(jigsaw.example_acme_org.munin.cpu.steal)%2C0.01667))%2C%22%23ff0000%22)%2C%22steal%22)\n. #### Comment by geoff-flarity on Jan 19, 2012:\nThis would be great, but there may be an easier way to make people happy.\nThe problem I have is that outliers don't get plotted. I'm assuming there's some quantization going on using averages? If we could specify max/min instead of average for graph quantization this would solve my issues. Though scatter graph would be awesome as well.\n. #### Comment by mjseger on Jan 19, 2012:\nWhen I first reported this problem to rrd, as this was the first place I saw this happening, it was recommended I do min/max.  While at first thought this sounds like a reasonable solution it's still not great.  Consider an interval with 1 normal point and 99 spikes.  You'd see 2 point, a min and a max.  Now consider 99 normal points and 1 spike.  You'd see the identical data.  Now it you scatter everything, idential highs or lows would overlay each other but experience has shown data points are not identical and so the scatter plot allows you to see just how many outliers there are.\nbtw - with gnuplot I still use connected plots and still see the outliers and normal data w/o the need to scatter them.\n-mark\n. #### Comment by nleskiw on Feb 21, 2012:\nI actually got a scatter graph working, but it turns out that the data aggregation happens in the carbon API.  So I was left with the same problem. This is a very non-trivial change, and I don't know when or even if it can be implemented.  All kinds of functions, especially those that operate on two series at the same time, will break because the lists are different lengths.  That's the real reason that the aggregation happens, so that all these neat math functions will work.\nExample: If one metric is collected once a minute, another once every 10 seconds, you can't sum() them, there's six data points during the same period. \nLots of functions assume that the data lists are the same length. \nI'm not sure how to handle this. My initial thought is that a new function must be added to carbon to give all the data over sans aggregation, and a new webapp function must be made that will prevent multiple series with different length lists from being used in functions together. \nWhat does everyone think?  This has been a pain point for many people and I'd like to do something to fix it.\n. #### Comment by fmikus on May 19, 2012:\nForward decaying priority sampling . To apply statistically representative algorithm with a recency bias.\nThis would be a method to represent the underlying trend that is visually represented by a scatted graph.\n. #### Comment by fmikus on May 19, 2012:\nscatted = scatter\n. #### Comment by mleinartas on May 22, 2012:\nI'll note for those watching this that a way to get some of the behavior wanted here is to play with the minXStep parameter which is now documented here: http://readthedocs.org/docs/graphite/en/0.9.x/render_api.html#minxstep\nIf you reduce this to zero, every point will be drawn - the disadvantage is that they will be drawn so close together that the graph lines will be smooshed (for lack of a better word) together. This can be somewhat compensated for by reducing the lineWidth.\nThis doesn't solve the issue Nick refers to though caused by the behavior of Whisper where in the case of multiple retentions it will return only the first retention that completely satisfies the requested time - that is, if you have 1secondly data for a day and minutely afterwards, if you draw a 1-week graph you'll only get the minutely data returned. \n. #### Comment by nleskiw on Jul 06, 2012:\nSO there's an update. \ncumulative() can show min and max now in the master branch.\nIt doesn't completely solve the issue, but if you apply cumulative(foo.bar,\"max\")  and you have your storage-aggregation.conf set to max, you'll keep your peaks. \nthe min would work in the opposite direction as well. \nnot a 100% fix, but better than nothing...\n. #### Comment by nleskiw on Jul 06, 2012:\nExample:\nmax min and avg for the same data, 14 day graph. \nhttp://i.imgur.com/W264e.png\n. #### Comment by fmikus on Oct 01, 2012:\nTo have a scattergraph type information I will reiterate the suggestion from above : \nForward decaying priority sampling. \nExample code here, git://gist.github.com/904980.git, can also be found in codahale/metrics.\nMore representative method of rollup than the typical RRD: (min, max, avg) and now Graphite (min,max,avg). \nStraight-Line-Interpolative aggregation is also a good way to be faithful to a time-series within +/- max_deviation of the actual value in rollup.\nThese are methods that can be done on time-series prior to sending to graphite or by graphite itself.\nThis of course does nothing for the alignment/averaging problem when displaying or doing math on different length series. But it does help in keeping the rolled-up data more accurate! Which eases the pain of mixing accurate and rolled up data.\nNice example Nicholas.\n. #### Comment by bennett on Jun 12, 2012:\nI agree that the result should be an empty or null dataset, rather than an exception.  Perhaps instead of advancing the fromTime and untilTime, whisper could correctly calculate any subset of the requested interval that overlaps the retention period, and return data from that subset.  If the requested interval completely misses the retention window, just return a base case of null or an empty data array.\n. #### Comment by neil-prosser on May 09, 2012:\n. \n. #### Comment by nleskiw on May 24, 2012:\nHave you tried using areaMode=all with min,avg,max metrics in order? Maybe also set line mode=staircase?\nIt's not exactly what you want but it might be better than nothing.\n-Nick\nGuido Serra zeph@fsfe.org wrote:\n\nPublic bug reported:\nthis is more a feature request instead of a bug\nbut I prefer to create it as a placeholder, maybe one day I'll be able\nto do it myself, or someone else will do\nI collect data about minimum response times of APIs, average, maximum\nand count of executed calls ... perfect if I would have had the\npossibility to plot a candle sticks chart...\nhttp://en.wikipedia.org/wiki/Candlestick_chart\nI'd use the response times min, max as HIGH, LOW value of the\ncandlestick ... avg as middle point in between the OPEN/CLOSE\nof course, I don't have an OPEN/CLOSE value, but I have the count of calls made at that sampling (I have aggregated samples)\nand that would be giving a height of the central element of the candlestick...  (proportional, in a way that will still leave the chart readable)\nin this way I could spot the trend of an API, and the amount of\nexecutions associated...\n** Affects: graphite\n    Importance: Undecided\n        Status: New\n** Tags: candlesticks charts\n\nYou received this bug notification because you are subscribed to\nGraphite.\nhttps://bugs.launchpad.net/bugs/1003847\nTitle:\n \"candlesticks\" like chart missing\nStatus in Graphite - Enterprise scalable realtime graphing:\n New\nBug description:\n this is more a feature request instead of a bug\nbut I prefer to create it as a placeholder, maybe one day I'll be able\n to do it myself, or someone else will do\nI collect data about minimum response times of APIs, average, maximum\n and count of executed calls ... perfect if I would have had the\n possibility to plot a candle sticks chart...\nhttp://en.wikipedia.org/wiki/Candlestick_chart\nI'd use the response times min, max as HIGH, LOW value of the\n candlestick ... avg as middle point in between the OPEN/CLOSE\nof course, I don't have an OPEN/CLOSE value, but I have the count of calls made at that sampling (I have aggregated samples)\n and that would be giving a height of the central element of the candlestick...  (proportional, in a way that will still leave the chart readable)\nin this way I could spot the trend of an API, and the amount of\n executions associated...\nTo manage notifications about this bug go to:\nhttps://bugs.launchpad.net/graphite/+bug/1003847/+subscriptions\n. #### Comment by zeph1ro on May 24, 2012:\n\nI do... what I need is the correlation with the \"amount\" of calls, not just the response times min/avg/max\nI need a \"3rd dimension\" ...\n. #### Comment by tewner on Jun 11, 2012:\nI have a similar request with \"divideSeries\" - I want to specify a wildcard list of hosts and divide two metrics from each host - I still would love to use the coolest feature of Graphite -  I want new plots for new hosts as the hosts are created.\ncactiStyle(\n   alias(\n    divideSeries(\n      perSecond(\n        servers.Server1.com_application_GatewayStats.TotalParsingTime\n      ),\n      perSecond(\n        servers.Server1.com_application_GatewayStats.CallsCount\n      )\n    )\n  ,\"Server1 - TotalParsingTime per Req\"\n  )\n)\n. #### Comment by tewner on Jun 11, 2012:\nSmall difference with my situation is that because I want to use wildcards, I need the 2 series' matched by host, and not by position in the series. Group-by index in the NODE name (likegroupByNode) SHOULD be ok, though.\n. #### Comment by ciranor on Jun 19, 2012:\nhttps://bugs.launchpad.net/graphite/+bug/1015039/+attachment/3195845/+files/graphite-web-sorted.patch\n. Indeed! I've moved it here: https://github.com/graphite-project/carbon/issues/86\n. #### Comment by a-sleep on Aug 27, 2012:\ndid a git pull to get changes up to and including 0a308363f3ed7c598c48e39c0667d8cd22c1ab9d. Ran python setup.py install and restarted Apache.\nIssue still exists.\nJust in case anyone actually looks at this.\n. #### Comment by mleinartas on Aug 27, 2012:\nSounds like a URL parsing problem. Can you paste a sample url (scrubbed if necessary) which reproduces it?\n. #### Comment by drawks on Aug 16, 2012:\nTo reproduce send a metric with an embedded nulll character in the metric name to the tcp linereceiver.\n. #### Comment by rmatteso on Aug 31, 2012:\n. #### Comment by gposton1040 on Oct 22, 2012:\nI second this.  I use the abberation in a tasseo graph, and in the event that one of the metrics I am forcasting has null values for some period of time, the whole dashboard fails to update.  I would like to see some error handling around this.\n. I'm thinking we just remove the check in favor of being more clear on not setting it. It's not a disaster if the local host gets added as a \"remote\" anyway since it sets localOnly=true (preventing a loop) and is merely going to re-do the browsing of the tree it already did. In the data fetching case, local is preferred anyway so if it's no local it'll merely be a few failed os.path.exists calls. \n. I'm thinking we just remove the check in favor of being more clear on not setting it. It's not a disaster if the local host gets added as a \"remote\" anyway since it sets localOnly=true (preventing a loop) and is merely going to re-do the browsing of the tree it already did. In the data fetching case, local is preferred anyway so if it's no local it'll merely be a few failed os.path.exists calls. \n. #### Comment by maximilian-weber on Sep 24, 2012:\n\n. #### Comment by zaa on Sep 28, 2012:\nLooks like the issue was fixed:\nhttps://github.com/graphite-project/graphite-web/commit/540fec4ccb7869a2eebec7dd234f49a055f38d5d\n. #### Comment by alex-hal9000 on Oct 01, 2012:\nWhich release version includes this change? We are using 0.9.10 and still see 500 error. \n. Fixed in 0af377c5203e8f0bb8f695ef6f17a24631ef46a4\n. #### Comment by diego-varese on Nov 02, 2012:\nI don't think this is the same as bug #993625. Yes, both mean that events don't work with a mysql backend, but for different reasons.\n. #### Comment by mleinartas on Nov 02, 2012:\nAck, upon closer inspection I think you're right. Sorry for that, I'll leave this one open \n. #### Comment by seif1001 on Nov 23, 2012:\nWhile I can add events with no issue through the django admin console (Issue 1) I do get the same error and callstack when trying to use flot (Issue 2) using a mySql backend.\nMySql Version: 5.1.40sp1-enterprise-gpl-advanced-log\n. #### Comment by dushyanth on Dec 10, 2012:\nOops - this affects graphite-web-0.9.10\n. #### Comment by otac0n on Dec 14, 2012:\n. #### Comment by campbell-stephenson on Dec 20, 2012:\n. #### Comment by jakabov-launchpad on Jan 15, 2013:\nThis issue crops up even without without a y-axis minimum:\nTo reproduce: specify these params to the render api: &target=removeBelowValue(sinFunction(\"1000*sin(x)\",1000),1)&logBase=10\nExpected output: a graph with maximum value rendered at 1000\nActual output: maximum value is rendered between 100 and 1000 on the y axis.\nIt appears that the labels on the y-axis are not aligned correctly with the grid and the rendered data.\n. #### Comment by mat-lau on Feb 01, 2013:\nThere are 3 others chars to exclude from validMetricChars: '/', '{', and '}'.\n. #### Comment by mat-lau on Feb 01, 2013:\nA better way to write validMetricChars:\nvalidMetricChars = u''.join(unichr(c) for c in xrange(65536) if c > 127 or chr(c) not in '(),/{}')\n. #### Comment by xuguizhou1121 on Feb 18, 2013:\nDo graphite metric support UTF-8 after your hack?\nI met the same problem even I have patch your hacking.\nURL is \nrender/?width=586&height=308&_salt=1361182176.112&target=box.test.test_ch.%E5%8F%98%E9%87%8F%E6%B5%8B%E8%AF%95\nERROR info\uff1a\n\nTraceback (most recent call last):\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/core/handlers/base.py\", line 111, in get_response\n    response = callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 58, in renderView\n    requestKey = hashRequest(request)\n  File \"/opt/graphite/webapp/graphite/render/hashing.py\", line 33, in hashRequest\n    def hashRequest(request):\n  File \"/opt/graphite/webapp/graphite/render/hashing.py\", line 52, in compactHash\n    return compactHash(myHash)\nUnicodeEncodeError: 'ascii' codec can't encode characters in position 35-38: ordinal not in range(128)\n\n. #### Comment by mat-lau on Feb 18, 2013:\nI tried metrics containing your 3 byte wide chars, and it worked (except for the graph legend) without changing anything...\nI realized the hack considers all values in 0..255 as valid chars (except '(),/{}').\n. #### Comment by xuguizhou1121 on Feb 19, 2013:\nyes, there is no error log in graphite/storage/webapp/log, but the graph legend is not working .\nThe error log exist in responese of graphite (just like the log I posted befroe)\nI catched the log by wireshark, the log data is in TCP follow stream.\n. +1\nCertainly better than what we have now\n. For those who dont know what 'solarized' refers to: http://ethanschoonover.com/solarized\n. Opinion's on @damm's comment above? Just changing graphTemplates.conf.example wont change the default for those who arent using this config file. OTOH changing only the config file is likely lower impact (i.e. no one's existing install gets their colors swapped).\nI dont have any strong opinions here, down for whatever consensus is\n. Thanks, this is obviously much more correct behavior\n. LGTM thanks\nI'm going to put this in 0.9.x too as I think this was technically a bug\n. Yes, you can configure the metrics which you expect to be sparse to have a lower (or zero) xFilesFactor in storage-aggregation.conf. This is a good writeup of how it works with mention of how to change existing files: http://obfuscurity.com/2012/04/Unhelpful-Graphite-Tip-9\n. Doh!\nThanks!\n. Going to merge this in as I'm pretty comfortable with it at this point\n. The diff is pretty enormous in github's interface, here's a diffstat. dashboard.html changes are merely for the path change of the icons:\nwebapp/content/js/ext/adapter/ext/ext-base-debug.js                       |    55 +-\n webapp/content/js/ext/adapter/ext/ext-base.js                             |     6 +-\n webapp/content/js/ext/adapter/jquery/ext-jquery-adapter-debug.js          |  1797 ---------------\n webapp/content/js/ext/adapter/jquery/ext-jquery-adapter.js                |     7 -\n webapp/content/js/ext/adapter/prototype/ext-prototype-adapter-debug.js    |  1826 ---------------\n webapp/content/js/ext/adapter/prototype/ext-prototype-adapter.js          |     7 -\n webapp/content/js/ext/adapter/yui/ext-yui-adapter-debug.js                |  1592 -------------\n webapp/content/js/ext/adapter/yui/ext-yui-adapter.js                      |     7 -\n webapp/content/js/ext/examples/shared/icons/arrow-down.gif                |   Bin 881 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/arrow-up.gif                  |   Bin 881 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/SILK.txt                  |     3 -\n webapp/content/js/ext/examples/shared/icons/fam/accept.png                |   Bin 781 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/add.gif                   |   Bin 994 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/add.png                   |   Bin 733 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/application_go.png        |   Bin 634 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/application_view_list.png |   Bin 473 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/book.png                  |   Bin 593 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/cog.png                   |   Bin 512 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/cog_edit.png              |   Bin 865 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/connect.gif               |   Bin 998 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/connect.png               |   Bin 748 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/control_rewind.png        |   Bin 614 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/cross.gif                 |   Bin 944 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/delete.gif                |   Bin 989 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/error.png                 |   Bin 666 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/feed_add.png              |   Bin 763 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/feed_delete.png           |   Bin 746 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/feed_error.png            |   Bin 770 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/folder_go.png             |   Bin 694 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/folder_wrench.png         |   Bin 740 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/grid.png                  |   Bin 513 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/image_add.png             |   Bin 653 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/information.png           |   Bin 778 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/plugin.gif                |   Bin 988 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/plugin_add.gif            |   Bin 1010 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/rss_go.png                |   Bin 635 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/table_refresh.png         |   Bin 795 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user.gif                  |   Bin 987 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user.png                  |   Bin 741 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user_add.gif              |   Bin 1001 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user_add.png              |   Bin 746 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user_comment.png          |   Bin 743 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user_delete.gif           |   Bin 1001 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user_delete.png           |   Bin 767 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user_edit.png             |   Bin 833 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user_female.gif           |   Bin 978 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user_female.png           |   Bin 663 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user_gray.png             |   Bin 706 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user_green.gif            |   Bin 985 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user_green.png            |   Bin 722 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user_orange.png           |   Bin 723 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user_red.png              |   Bin 717 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user_suit.gif             |   Bin 988 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/fam/user_suit.png             |   Bin 748 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/save.gif                      |   Bin 1014 -> 0 bytes\n webapp/content/js/ext/examples/shared/icons/silk.css                      |   185 --\n webapp/content/js/ext/ext-all-debug.js                                    |  1189 ++++++++--\n webapp/content/js/ext/ext-all.js                                          |     8 +-\n webapp/content/js/ext/license.txt                                         |    41 -\n webapp/content/js/ext/pkgs/cmp-foundation-debug.js                        | 14754 --------------------------------------------------------------------------------------------------------------------\n webapp/content/js/ext/pkgs/cmp-foundation.js                              |     7 -\n webapp/content/js/ext/pkgs/data-foundation-debug.js                       |  4804 --------------------------------------\n webapp/content/js/ext/pkgs/data-foundation.js                             |     7 -\n webapp/content/js/ext/pkgs/data-grouping-debug.js                         |   263 ---\n webapp/content/js/ext/pkgs/data-grouping.js                               |     7 -\n webapp/content/js/ext/pkgs/data-json-debug.js                             |   649 ------\n webapp/content/js/ext/pkgs/data-json.js                                   |     7 -\n webapp/content/js/ext/pkgs/data-list-views-debug.js                       |  1539 ------------\n webapp/content/js/ext/pkgs/data-list-views.js                             |     7 -\n webapp/content/js/ext/pkgs/data-xml-debug.js                              |   479 ----\n webapp/content/js/ext/pkgs/data-xml.js                                    |     7 -\n webapp/content/js/ext/pkgs/direct-debug.js                                |  1162 ----------\n webapp/content/js/ext/pkgs/direct.js                                      |     7 -\n webapp/content/js/ext/pkgs/ext-core-debug.js                              |  7918 --------------------------------------------------------------\n webapp/content/js/ext/pkgs/ext-core.js                                    |     7 -\n webapp/content/js/ext/pkgs/ext-dd-debug.js                                |  4802 --------------------------------------\n webapp/content/js/ext/pkgs/ext-dd.js                                      |     7 -\n webapp/content/js/ext/pkgs/ext-foundation-debug.js                        |  8022 ---------------------------------------------------------------\n webapp/content/js/ext/pkgs/ext-foundation.js                              |     7 -\n webapp/content/js/ext/pkgs/pkg-buttons-debug.js                           |  1146 ---------\n webapp/content/js/ext/pkgs/pkg-buttons.js                                 |     7 -\n webapp/content/js/ext/pkgs/pkg-charts-debug.js                            |  1927 ----------------\n webapp/content/js/ext/pkgs/pkg-charts.js                                  |    10 -\n webapp/content/js/ext/pkgs/pkg-forms-debug.js                             |  9220 ------------------------------------------------------------------------\n webapp/content/js/ext/pkgs/pkg-forms.js                                   |     7 -\n webapp/content/js/ext/pkgs/pkg-grid-editor-debug.js                       |   621 -----\n webapp/content/js/ext/pkgs/pkg-grid-editor.js                             |     7 -\n webapp/content/js/ext/pkgs/pkg-grid-foundation-debug.js                   |  7212 ---------------------------------------------------------\n webapp/content/js/ext/pkgs/pkg-grid-foundation.js                         |     7 -\n webapp/content/js/ext/pkgs/pkg-grid-grouping-debug.js                     |   609 -----\n webapp/content/js/ext/pkgs/pkg-grid-grouping.js                           |     7 -\n webapp/content/js/ext/pkgs/pkg-grid-property-debug.js                     |   468 ----\n webapp/content/js/ext/pkgs/pkg-grid-property.js                           |     7 -\n webapp/content/js/ext/pkgs/pkg-history-debug.js                           |   216 --\n webapp/content/js/ext/pkgs/pkg-history.js                                 |     7 -\n webapp/content/js/ext/pkgs/pkg-menu-debug.js                              |  1701 --------------\n webapp/content/js/ext/pkgs/pkg-menu.js                                    |     7 -\n webapp/content/js/ext/pkgs/pkg-tabs-debug.js                              |  1132 ---------\n webapp/content/js/ext/pkgs/pkg-tabs.js                                    |     7 -\n webapp/content/js/ext/pkgs/pkg-tips-debug.js                              |  1146 ---------\n webapp/content/js/ext/pkgs/pkg-tips.js                                    |     7 -\n webapp/content/js/ext/pkgs/pkg-toolbars-debug.js                          |  1216 ----------\n webapp/content/js/ext/pkgs/pkg-toolbars.js                                |     7 -\n webapp/content/js/ext/pkgs/pkg-tree-debug.js                              |  4836 --------------------------------------\n webapp/content/js/ext/pkgs/pkg-tree.js                                    |     7 -\n webapp/content/js/ext/pkgs/resizable-debug.js                             |   770 -------\n webapp/content/js/ext/pkgs/resizable.js                                   |     7 -\n webapp/content/js/ext/pkgs/state-debug.js                                 |   304 ---\n webapp/content/js/ext/pkgs/state.js                                       |     7 -\n webapp/content/js/ext/pkgs/window-debug.js                                |  2024 ----------------\n webapp/content/js/ext/pkgs/window.js                                      |     7 -\n webapp/content/js/ext/resources/charts.swf                                |   Bin 81653 -> 0 bytes\n webapp/content/js/ext/resources/css/README.txt                            |     6 -\n webapp/content/js/ext/resources/css/debug.css                             |    43 -\n webapp/content/js/ext/resources/css/ext-all-notheme.css                   |  5302 ------------------------------------------\n webapp/content/js/ext/resources/css/ext-all.css                           |    28 +-\n webapp/content/js/ext/resources/css/reset-min.css                         |     7 -\n webapp/content/js/ext/resources/css/structure/borders.css                 |    54 -\n webapp/content/js/ext/resources/css/structure/box.css                     |    80 -\n webapp/content/js/ext/resources/css/structure/button.css                  |   445 ----\n webapp/content/js/ext/resources/css/structure/combo.css                   |    45 -\n webapp/content/js/ext/resources/css/structure/core.css                    |   341 ---\n webapp/content/js/ext/resources/css/structure/date-picker.css             |   271 ---\n webapp/content/js/ext/resources/css/structure/dd.css                      |    61 -\n webapp/content/js/ext/resources/css/structure/debug.css                   |    26 -\n webapp/content/js/ext/resources/css/structure/dialog.css                  |    59 -\n webapp/content/js/ext/resources/css/structure/editor.css                  |    92 -\n webapp/content/js/ext/resources/css/structure/form.css                    |   573 -----\n webapp/content/js/ext/resources/css/structure/grid.css                    |   588 -----\n webapp/content/js/ext/resources/css/structure/layout.css                  |   296 ---\n webapp/content/js/ext/resources/css/structure/list-view.css               |    86 -\n webapp/content/js/ext/resources/css/structure/menu.css                    |   245 --\n webapp/content/js/ext/resources/css/structure/panel-reset.css             |   130 --\n webapp/content/js/ext/resources/css/structure/panel.css                   |   493 ----\n webapp/content/js/ext/resources/css/structure/pivotgrid.css               |    65 -\n webapp/content/js/ext/resources/css/structure/progress.css                |    46 -\n webapp/content/js/ext/resources/css/structure/qtips.css                   |   153 --\n webapp/content/js/ext/resources/css/structure/reset.css                   |    13 -\n webapp/content/js/ext/resources/css/structure/resizable.css               |   149 --\n webapp/content/js/ext/resources/css/structure/slider.css                  |   103 -\n webapp/content/js/ext/resources/css/structure/tabs.css                    |   392 ----\n webapp/content/js/ext/resources/css/structure/toolbar.css                 |   246 --\n webapp/content/js/ext/resources/css/structure/tree.css                    |   218 --\n webapp/content/js/ext/resources/css/structure/window.css                  |   222 --\n webapp/content/js/ext/resources/css/theme-access/borders.css              |    25 -\n webapp/content/js/ext/resources/css/theme-access/box.css                  |    74 -\n webapp/content/js/ext/resources/css/theme-access/button.css               |   136 --\n webapp/content/js/ext/resources/css/theme-access/combo.css                |    43 -\n webapp/content/js/ext/resources/css/theme-access/core.css                 |    81 -\n webapp/content/js/ext/resources/css/theme-access/date-picker.css          |   145 --\n webapp/content/js/ext/resources/css/theme-access/dd.css                   |    29 -\n webapp/content/js/ext/resources/css/theme-access/debug.css                |    24 -\n webapp/content/js/ext/resources/css/theme-access/dialog.css               |    34 -\n webapp/content/js/ext/resources/css/theme-access/editor.css               |    16 -\n webapp/content/js/ext/resources/css/theme-access/form.css                 |   176 --\n webapp/content/js/ext/resources/css/theme-access/grid.css                 |   288 ---\n webapp/content/js/ext/resources/css/theme-access/layout.css               |    56 -\n webapp/content/js/ext/resources/css/theme-access/list-view.css            |    43 -\n webapp/content/js/ext/resources/css/theme-access/menu.css                 |    79 -\n webapp/content/js/ext/resources/css/theme-access/panel.css                |    94 -\n webapp/content/js/ext/resources/css/theme-access/progress.css             |    35 -\n webapp/content/js/ext/resources/css/theme-access/qtips.css                |    44 -\n webapp/content/js/ext/resources/css/theme-access/resizable.css            |    44 -\n webapp/content/js/ext/resources/css/theme-access/slider.css               |    21 -\n webapp/content/js/ext/resources/css/theme-access/tabs.css                 |   119 -\n webapp/content/js/ext/resources/css/theme-access/toolbar.css              |   120 -\n webapp/content/js/ext/resources/css/theme-access/tree.css                 |   165 --\n webapp/content/js/ext/resources/css/theme-access/window.css               |    87 -\n webapp/content/js/ext/resources/css/theme-gray/borders.css                |    29 -\n webapp/content/js/ext/resources/css/theme-gray/box.css                    |    74 -\n webapp/content/js/ext/resources/css/theme-gray/button.css                 |    94 -\n webapp/content/js/ext/resources/css/theme-gray/combo.css                  |    43 -\n webapp/content/js/ext/resources/css/theme-gray/core.css                   |    83 -\n webapp/content/js/ext/resources/css/theme-gray/date-picker.css            |   143 --\n webapp/content/js/ext/resources/css/theme-gray/dd.css                     |    29 -\n webapp/content/js/ext/resources/css/theme-gray/debug.css                  |    24 -\n webapp/content/js/ext/resources/css/theme-gray/dialog.css                 |    34 -\n webapp/content/js/ext/resources/css/theme-gray/editor.css                 |    13 -\n webapp/content/js/ext/resources/css/theme-gray/form.css                   |   117 -\n webapp/content/js/ext/resources/css/theme-gray/grid.css                   |   276 ---\n webapp/content/js/ext/resources/css/theme-gray/layout.css                 |    53 -\n webapp/content/js/ext/resources/css/theme-gray/list-view.css              |    37 -\n webapp/content/js/ext/resources/css/theme-gray/menu.css                   |    82 -\n webapp/content/js/ext/resources/css/theme-gray/panel.css                  |    87 -\n webapp/content/js/ext/resources/css/theme-gray/pivotgrid.css              |    28 -\n webapp/content/js/ext/resources/css/theme-gray/progress.css               |    32 -\n webapp/content/js/ext/resources/css/theme-gray/qtips.css                  |    44 -\n webapp/content/js/ext/resources/css/theme-gray/resizable.css              |    43 -\n webapp/content/js/ext/resources/css/theme-gray/slider.css                 |    21 -\n webapp/content/js/ext/resources/css/theme-gray/tabs.css                   |   127 -\n webapp/content/js/ext/resources/css/theme-gray/toolbar.css                |    95 -\n webapp/content/js/ext/resources/css/theme-gray/tree.css                   |   157 --\n webapp/content/js/ext/resources/css/theme-gray/window.css                 |    86 -\n webapp/content/js/ext/resources/css/visual/borders.css                    |    25 -\n webapp/content/js/ext/resources/css/visual/box.css                        |    74 -\n webapp/content/js/ext/resources/css/visual/button.css                     |    94 -\n webapp/content/js/ext/resources/css/visual/combo.css                      |    43 -\n webapp/content/js/ext/resources/css/visual/core.css                       |    82 -\n webapp/content/js/ext/resources/css/visual/date-picker.css                |   143 --\n webapp/content/js/ext/resources/css/visual/dd.css                         |    29 -\n webapp/content/js/ext/resources/css/visual/debug.css                      |    24 -\n webapp/content/js/ext/resources/css/visual/dialog.css                     |    34 -\n webapp/content/js/ext/resources/css/visual/editor.css                     |    13 -\n webapp/content/js/ext/resources/css/visual/form.css                       |   123 -\n webapp/content/js/ext/resources/css/visual/grid.css                       |   277 ---\n webapp/content/js/ext/resources/css/visual/layout.css                     |    53 -\n webapp/content/js/ext/resources/css/visual/list-view.css                  |    37 -\n webapp/content/js/ext/resources/css/visual/menu.css                       |    87 -\n webapp/content/js/ext/resources/css/visual/panel.css                      |    87 -\n webapp/content/js/ext/resources/css/visual/pivotgrid.css                  |    28 -\n webapp/content/js/ext/resources/css/visual/progress.css                   |    32 -\n webapp/content/js/ext/resources/css/visual/qtips.css                      |    44 -\n webapp/content/js/ext/resources/css/visual/resizable.css                  |    43 -\n webapp/content/js/ext/resources/css/visual/slider.css                     |    21 -\n webapp/content/js/ext/resources/css/visual/tabs.css                       |   127 -\n webapp/content/js/ext/resources/css/visual/toolbar.css                    |    95 -\n webapp/content/js/ext/resources/css/visual/tree.css                       |   152 --\n webapp/content/js/ext/resources/css/visual/window.css                     |    86 -\n webapp/content/js/ext/resources/css/xtheme-access.css                     |  1820 ---------------\n webapp/content/js/ext/resources/css/xtheme-blue.css                       |  1674 --------------\n webapp/content/js/ext/resources/css/xtheme-gray.css                       |  1682 --------------\n webapp/content/js/ext/resources/css/yourtheme.css                         |  1652 -------------\n webapp/content/js/ext/resources/expressinstall.swf                        |   Bin 4823 -> 0 bytes\n webapp/content/js/ext/resources/icons/fam/SILK.txt                        |     3 +\n webapp/content/js/ext/resources/icons/fam/accept.png                      |   Bin 0 -> 781 bytes\n webapp/content/js/ext/resources/icons/fam/add.gif                         |   Bin 0 -> 994 bytes\n webapp/content/js/ext/resources/icons/fam/add.png                         |   Bin 0 -> 733 bytes\n webapp/content/js/ext/resources/icons/fam/application_go.png              |   Bin 0 -> 634 bytes\n webapp/content/js/ext/resources/icons/fam/application_view_list.png       |   Bin 0 -> 473 bytes\n webapp/content/js/ext/resources/icons/fam/book.png                        |   Bin 0 -> 593 bytes\n webapp/content/js/ext/resources/icons/fam/cog.png                         |   Bin 0 -> 512 bytes\n webapp/content/js/ext/resources/icons/fam/cog_edit.png                    |   Bin 0 -> 865 bytes\n webapp/content/js/ext/resources/icons/fam/connect.gif                     |   Bin 0 -> 998 bytes\n webapp/content/js/ext/resources/icons/fam/connect.png                     |   Bin 0 -> 748 bytes\n webapp/content/js/ext/resources/icons/fam/control_rewind.png              |   Bin 0 -> 614 bytes\n webapp/content/js/ext/resources/icons/fam/cross.gif                       |   Bin 0 -> 944 bytes\n webapp/content/js/ext/resources/icons/fam/delete.gif                      |   Bin 0 -> 989 bytes\n webapp/content/js/ext/resources/icons/fam/error.png                       |   Bin 0 -> 666 bytes\n webapp/content/js/ext/resources/icons/fam/feed_add.png                    |   Bin 0 -> 763 bytes\n webapp/content/js/ext/resources/icons/fam/feed_delete.png                 |   Bin 0 -> 746 bytes\n webapp/content/js/ext/resources/icons/fam/feed_error.png                  |   Bin 0 -> 770 bytes\n webapp/content/js/ext/resources/icons/fam/folder_go.png                   |   Bin 0 -> 694 bytes\n webapp/content/js/ext/resources/icons/fam/folder_wrench.png               |   Bin 0 -> 740 bytes\n webapp/content/js/ext/resources/icons/fam/grid.png                        |   Bin 0 -> 513 bytes\n webapp/content/js/ext/resources/icons/fam/image_add.png                   |   Bin 0 -> 653 bytes\n webapp/content/js/ext/resources/icons/fam/information.png                 |   Bin 0 -> 778 bytes\n webapp/content/js/ext/resources/icons/fam/plugin.gif                      |   Bin 0 -> 988 bytes\n webapp/content/js/ext/resources/icons/fam/plugin_add.gif                  |   Bin 0 -> 1010 bytes\n webapp/content/js/ext/resources/icons/fam/rss_go.png                      |   Bin 0 -> 635 bytes\n webapp/content/js/ext/resources/icons/fam/table_refresh.png               |   Bin 0 -> 795 bytes\n webapp/content/js/ext/resources/icons/fam/user.gif                        |   Bin 0 -> 987 bytes\n webapp/content/js/ext/resources/icons/fam/user.png                        |   Bin 0 -> 741 bytes\n webapp/content/js/ext/resources/icons/fam/user_add.gif                    |   Bin 0 -> 1001 bytes\n webapp/content/js/ext/resources/icons/fam/user_add.png                    |   Bin 0 -> 746 bytes\n webapp/content/js/ext/resources/icons/fam/user_comment.png                |   Bin 0 -> 743 bytes\n webapp/content/js/ext/resources/icons/fam/user_delete.gif                 |   Bin 0 -> 1001 bytes\n webapp/content/js/ext/resources/icons/fam/user_delete.png                 |   Bin 0 -> 767 bytes\n webapp/content/js/ext/resources/icons/fam/user_edit.png                   |   Bin 0 -> 833 bytes\n webapp/content/js/ext/resources/icons/fam/user_female.gif                 |   Bin 0 -> 978 bytes\n webapp/content/js/ext/resources/icons/fam/user_female.png                 |   Bin 0 -> 663 bytes\n webapp/content/js/ext/resources/icons/fam/user_gray.png                   |   Bin 0 -> 706 bytes\n webapp/content/js/ext/resources/icons/fam/user_green.gif                  |   Bin 0 -> 985 bytes\n webapp/content/js/ext/resources/icons/fam/user_green.png                  |   Bin 0 -> 722 bytes\n webapp/content/js/ext/resources/icons/fam/user_orange.png                 |   Bin 0 -> 723 bytes\n webapp/content/js/ext/resources/icons/fam/user_red.png                    |   Bin 0 -> 717 bytes\n webapp/content/js/ext/resources/icons/fam/user_suit.gif                   |   Bin 0 -> 988 bytes\n webapp/content/js/ext/resources/icons/fam/user_suit.png                   |   Bin 0 -> 748 bytes\n webapp/content/js/ext/ux/DataViewTransition.js                            |    11 +-\n webapp/graphite/templates/dashboard.html                                  |    10 +-\n 269 files changed, 1092 insertions(+), 108168 deletions(-)\n. Awesome, thanks\n. Merged post 0.9.11\n. Tested, clear fix\n. I talked this over with @obfuscurity a few days back and we agreed to add\nit in time, but I completely forgot :(\nI'll see about merging it tomorrow and we'll get it in 0.9.12 which is\nscheduled only two months out.\nSorry for missing this and thanks for the solid contribution and the\nreminder.\n On Aug 21, 2013 12:06 AM, \"gingerlime\" notifications@github.com wrote:\n\n@obfuscurity https://github.com/obfuscurity @SEJeffhttps://github.com/SEJeff- any particular reason why this got excluded and didn't get merged into\n0.9.11 ??\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/334#issuecomment-22995757\n.\n. Congratulations, you're now familiar with one of the more problemmatic aspects of functions.py :)\n\nThanks a lot for going through these, this is a good set of fixes to have. Merged into 0.9.x and master\n. :thumbsup: Looks good \n. Merging this in and cherry-picking to 0.9.x\n. :thumbsup: looks good and tests okay applied to master for me.\n. Nice, thanks a lot\n. Closes #354 \n. Wow, crazy timing - this came in just after #394 :)\nI merged both, though I've only the django.conf.urls.defaults change into master as we're still supporting Django 1.3 in 0.9.x (for now).\nThanks for the contribution!\n. Merged this in\n. Indeed, this is one of the regressions that lead to the release of 0.9.12.\nYou should use that version instead\n On Aug 27, 2013 5:22 AM, \"Robert Krombholz\" notifications@github.com\nwrote:\n\nThis actually introduced a dependency to Django >= 1.4 (and by that to\npython >= 2.5).\nThis is sad as the release notes for Graphite 0.9.11 still state that\nDjango 1.3 (and above) and python 2.4 should work fine.\nWithout knowing anythin about the code: is there a way to avoid the\ntimezone module (which does not exist in Django 1.3)?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/397#issuecomment-23325958\n.\n. I only merged fc3f018 into master, 0.9.11 should remain compatible with Django 1.3. I think it's reasonable to go to 1.4 in master but I'll certainly listen to arguments for keeping it at 1.3 there.\n\n@deniszh  - looks like this is a separate problem, introduced by the fix to #101. I just filed #400 to cover this problem\n. I cherry-picked this into master rather than 0.9.x because that's the only place 1.4+ is intended. Closing out as such but reopen if I've gotten it wrong\n. +1\n. :+1:\nOn Sep 27, 2013 4:57 AM, \"Marco Hoyer\" notifications@github.com wrote:\n\nFollowing the python doc at http://docs.python.org/2/library/string.html(7.1.3.2. Format examples), {}.{}\\n,format() is available since python 2.7.\nTo ensure compatibility with python2.6, i added tuple-indexes.\nYou can merge this Pull Request by running\ngit pull https://github.com/marco-hoyer/graphite-web patch-2\nOr view, comment on, or merge it at:\nhttps://github.com/graphite-project/graphite-web/pull/441\nCommit Summary\n- modified to ensure compatibility with python2.6\nFile Changes\n- M webapp/graphite/util.pyhttps://github.com/graphite-project/graphite-web/pull/441/files#diff-0(2)\nPatch Links:\n- https://github.com/graphite-project/graphite-web/pull/441.patch\n- https://github.com/graphite-project/graphite-web/pull/441.diff\n. http://graphite.readthedocs.org/\n\nThe variable there is to allow functions.py to be imported for the docs to generated off of it. You can experiment by running \"READTHEDOCS=true make html\" from the docs/ subdir\n. The READTHEDOCS environment variable is set to allow the docs to be built on readthedocs.org (mainly for functions.py). readthedocs sets the variable before running the makefile in docs/\nTo prevent it from being loaded in this case, the code block (along with getProfile and related) should probably just be moved to its own module - maybe account/util.py? or inside account/models.py? As a side effect, it may make the READTHEDOCS clause unnecessary \n. :+1:\nOn Oct 25, 2013 7:03 AM, \"Valentin Haenel\" notifications@github.com wrote:\n\nWe encountered several issues with graphite-web when running in a high\navailability setup and when switching off a node.\nThis helps with the following excetpions (and maybe others):\nFile \"/opt/graphite/webapp/graphite/readers.py\", line 30, in waitForResults\n    return self.wait_callback()\n  File \"/opt/graphite/webapp/graphite/remote_storage.py\", line 208, in extract_my_results\n    for series in wait_for_results():\n  File \"/opt/graphite/webapp/graphite/remote_storage.py\", line 203, in wait_for_results\n    raise Exception(\"Passive remote fetch failed to find cached results\")\nException: Passive remote fetch failed to find cached results\nFile \"/opt/graphite/webapp/graphite/readers.py\", line 30, in waitForResults\n    return self.wait_callback()\n  File \"/opt/graphite/webapp/graphite/remote_storage.py\", line 208, in extract_my_results\n    for series in wait_for_results():\n  File \"/opt/graphite/webapp/graphite/remote_storage.py\", line 182, in wait_for_results\n    response = connection.getresponse()\nNameError: free variable 'connection' referenced before assignment in enclosing scope\nFile \"/opt/graphite/webapp/graphite/readers.py\", line 30, in waitForResults\n    return self.wait_callback()\n  File \"/opt/graphite/webapp/graphite/remote_storage.py\", line 208, in extract_my_results\n    for series in wait_for_results():\n  File \"/opt/graphite/webapp/graphite/remote_storage.py\", line 182, in wait_for_results\n    response = connection.getresponse()\n  File \"/usr/lib64/python2.6/httplib.py\", line 990, in getresponse\n    response.begin()\n  File \"/usr/lib64/python2.6/httplib.py\", line 391, in begin\n    version, status, reason = self._read_status()\n  File \"/usr/lib64/python2.6/httplib.py\", line 349, in _read_status\n    line = self.fp.readline()\n  File \"/usr/lib64/python2.6/socket.py\", line 433, in readline\n    data = recv(1)\nerror: [Errno 104] Connection reset by peer\nAlso, this might be related to #421https://github.com/graphite-project/graphite-web/pull/421\nYou can merge this Pull Request by running\ngit pull https://github.com/ImmobilienScout24/graphite-web fix_remote_fetch_find_exceptions\nOr view, comment on, or merge it at:\nhttps://github.com/graphite-project/graphite-web/pull/476\nCommit Summary\n- fix uncaught exceptions during fetch and find\nFile Changes\n- M webapp/graphite/local_settings.py.examplehttps://github.com/graphite-project/graphite-web/pull/476/files#diff-0(3)\n- M webapp/graphite/render/datalib.pyhttps://github.com/graphite-project/graphite-web/pull/476/files#diff-1(69)\n- M webapp/graphite/settings.pyhttps://github.com/graphite-project/graphite-web/pull/476/files#diff-2(2)\nPatch Links:\n- https://github.com/graphite-project/graphite-web/pull/476.patch\n- https://github.com/graphite-project/graphite-web/pull/476.diff\n. :+1:\n On Jan 10, 2014 4:16 AM, \"Valentin Haenel\" notifications@github.com\nwrote:\nPing. Can this be merged?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/487#issuecomment-32016082\n.\n. :thumbs:\nLooks good to me, solves a long standing issue and annoying issue\nOn Dec 10, 2013 12:05 PM, \"jcsp\" notifications@github.com wrote:\nPreviously, datetime.now() was called more than once\nwhen rendering data. This led to non-deterministic\nselection of sample resolution. For example, if\nthe schema is 60s:1d,300s:7d, and a request for\n'-1d'->'now' came in, one would sometimes get\nthe 300s data and sometimes the 60s data, depending\non the length of time between the call to now() in\nrender.views and the call to now() in whisper.fetch.\nRelated: graphite-project/whisper#56https://github.com/graphite-project/whisper/pull/56\nYou can merge this Pull Request by running\ngit pull https://github.com/jcsp/graphite-web consistent-now\nOr view, comment on, or merge it at:\nhttps://github.com/graphite-project/graphite-web/pull/523\nCommit Summary\n- Fix 'now' handling in render queries\nFile Changes\n- M webapp/graphite/render/attime.pyhttps://github.com/graphite-project/graphite-web/pull/523/files#diff-0(11)\n- M webapp/graphite/render/datalib.pyhttps://github.com/graphite-project/graphite-web/pull/523/files#diff-1(2)\n- M webapp/graphite/render/views.pyhttps://github.com/graphite-project/graphite-web/pull/523/files#diff-2(16)\n- M webapp/graphite/storage.pyhttps://github.com/graphite-project/graphite-web/pull/523/files#diff-3(11)\nPatch Links:\n- https://github.com/graphite-project/graphite-web/pull/523.patch\n- https://github.com/graphite-project/graphite-web/pull/523.diff\n. :thumbsup:\nOn Dec 11, 2013 3:11 AM, \"Bruno Reni\u00e9\" notifications@github.com wrote:\ndefaultUser is no more. It was only needed for getProfile, which now\ncreates the profile on the fly.\nThe code also uses Foo.objects.get_or_create(), which is safer than a try\u2026\nexcept block around Foo.objects.get().\nThe default profile is now created with an unusable password instead of a\nrandom one, which avoids computing an expensive hash each time\ndefault_profile() is called.\nYou can merge this Pull Request by running\ngit pull https://github.com/brutasse/graphite-web fix/default-profile\nOr view, comment on, or merge it at:\nhttps://github.com/graphite-project/graphite-web/pull/527\nCommit Summary\n- Fix #403 #445 -- avoid creating the default profile at import time\nFile Changes\n- M webapp/graphite/browser/views.pyhttps://github.com/graphite-project/graphite-web/pull/527/files#diff-0(4)\n- M webapp/graphite/graphlot/views.pyhttps://github.com/graphite-project/graphite-web/pull/527/files#diff-1(2)\n- M webapp/graphite/metrics/views.pyhttps://github.com/graphite-project/graphite-web/pull/527/files#diff-2(2)\n- M webapp/graphite/util.pyhttps://github.com/graphite-project/graphite-web/pull/527/files#diff-3(45)\nPatch Links:\n- https://github.com/graphite-project/graphite-web/pull/527.patch\n- https://github.com/graphite-project/graphite-web/pull/527.diff\n. thanks!\n. Doesn't a pip install work (assuming proper headers and libs for Cairo and\nsuch)? I know it did work at one point\n On Jan 6, 2014 7:03 PM, \"Jason Dixon\" notifications@github.com wrote:\n@brutasse https://github.com/brutasse I agree, that would be nice.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/555#issuecomment-31704339\n.\n. Huh. I guess it doesnt work with graphite-web. carbon does declare its dependencies in setup.py so pip install does work in that case.\n\nCan you pin dependencies via install_requires like you can with requirements.txt? This'll be necessary for a few deps e.g. pyparsing whose latest version is python3 only.\nIn general I'm :+1: on anything to improve installation with the caveat that it needs to be supportable or adopted by someone - there are periodically drive-by additions, some of which have caused more trouble than they were worth.\n. Yeah, the stacktrace points to the issue fixed via #1122. Are you sure you have that patch applied?\nTo fix this you'll either need to apply the workaround to graphite-web: https://github.com/graphite-project/graphite-web/pull/1122.patch \nor this fix to carbon: https://github.com/graphite-project/carbon/pull/496.patch\n(or update either to the latest master)\n. I'm a bit confused.. Are you sure youre running 0.9.15 across the board? It sounds like you have a version running master branch trying to talk to 0.9.x branch. afaik the is_leaf=>isLeave et al changes have always only been between 0.9.x and master\n. I'm :+1: for not swallowing the exceptions but I really dont think we should be bringing in the intervals stuff into 0.9.x or shimming in compatibility to cluster with the master branch\n. Yeah, the unpickler code should be cleaned up - I'm sure that's just\ncopypasta\nI'd still like to see that exception swallowing go away, but I remain\nunconvinced that bringing in compatibility across major versions is a good\nidea. Supporting cross-version clusters complicates debugging user problems\nand could cause people silent breakage rather than a fail-fast break as\nthings in master change. It's also bad precedent to set - we need to be\nable to break compatibility to move forward. For instance, supporting\narchived timesteps in Ceres will require significant changes to\nintervals.py (support for comparing intervals of different timesteps). We\nalready have a sort of hell keeping the changes that come in to 0.9.x added\nto master, I'd rather us not start committing to going the other direction\ntoo\nOn Tue, Feb 2, 2016 at 11:07 AM, Dave Rawks notifications@github.com\nwrote:\n\nSo whats the consensus here? I'm fine running this patch out of tree to\nscratch my own itch, as it were. I still think this looks like the right\nway to handle this in tree. If bringing intervals from master is really\nthat distasteful it seems that the unpickler code in 0.9.x should also have\nreferences to it scrubbed.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1455#issuecomment-178692183\n.\n. Note giant style changes like this will break git blame and make merges and\ncherry picks (e.g. porting any new patches from 0.9.x to master) difficult\nor impossible.\n\nI'll encourage a re-reading of pep8's first section... but I'm going to let\ngo. You all do what you feel\nOn Jul 27, 2016 7:21 AM, \"Christopher Bowman\" notifications@github.com\nwrote:\n\nI'm of the opinion that we make the change now. We have the most test\ncoverage we ever have and the backlog is as small as I've ever seen it.\n\u2014\nYou are receiving this because you are on a team that was mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1621#issuecomment-235568613,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AATaL7_B-aIYA3smQIxC9rn_T06KLhkeks5qZ003gaJpZM4JVkNC\n.\n. If I understand SECRET_KEY right, you'd want to persist it so that login\ncookies don't expire every time you restart (or maybe it's worse and per\nworker?)\nThe real trick is that the secret should be the same across all frontend\nwebapps in a cluster setup\nOn Dec 26, 2013 11:42 PM, \"Jeff Schroeder\" notifications@github.com wrote:\nIn webapp/tests/settings.py:\n\n@@ -1,3 +1,9 @@\n+from django.conf import settings, global_settings\n+\n+# Silence the warning about an insecure SECRET_KEY\n+global_settings.SECRET_KEY = 'SUPER_SAFE_TESTING_KEY'\n\n@brutasse https://github.com/brutasse Instead, what do you think of an\napproach vaguely like this onehttps://github.com/ndarville/pony-forum/blob/master/ponyforum/settings.py#L126\n?\nSometing kind of like:\n\n\n\n''.join([random.SystemRandom().choice(string.printable) for i in range(100)]).replace(' ', '')\n'kAgDA\\t($ez5kiYQ<;m^@]p1\"4mCwEYC;!_U{[,L\\x0c1H.\\x0bzEPH7zyWR\"\\'4BYMtH05u(20G%gw#qd%s9;;mI.X?a3\\r:V!F*2v,xm'\n\n\n\nThat automatically sets the secret key each time the application is first\nstarted. Thoughts?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/544/files#r8564053\n.\n. \n",
    "xiaods": "+1\n. ",
    "magec": "I use it to identify a given metric out of a graph, sometimes the legend is not enough. My actual example are graphs of network switches with several ports, sometimes I have more that 50 metrics shown in the same graph, in order to know the one that has a traffic peak, for example, I used to keep eliminating them from the entries shown until I get the one I needed. Now I just  open it on graphplot and focus the metric with the mouse. I'll keep the button in my installation, either way.\n. Still thinking on it :expressionless: \n. Seems ok, thought I no longer maintain the cluster I had when I opened the ticket. Either way, the check is simply to test the feature with  /proc/sys/net/ipv4/ip_nonlocal_bind set to 1.\n. ",
    "syepes": "+1\n. +1 For bug hunting ;-)\n. +1 This would be a great addition\n. +1 for the backporting into 0.9.x\n. Does there exist any kind workaround?\n. ",
    "xkilian": "We have the exact same use case.\nGraphlot features that are really worthwhile are: finding values and time-series out of a graph that has a lot (as magec mentioned) and showing events.\nI would use the feature that magec proposed as a stopgap measure until the mentioned features are integrated in the dashboard/composer.\n. @Dieterbe Anything new on this topic? \nHave you had time to prepare your changes for the master branch. What you are doing very interesting.\n. All these new functions and variations on functions that have been added lately are really nice, but if the documentation is not updated they are kind of useless for everyone else.\nInclude the documentation changes in your pull requests. (Or if missing when merging the commits add the docs.)\n. You are right, they are pulled from the docstrings. Explanation + example is good. I guess, I will reformulate my comment to mean, make sure the docstring has enough meat to make use of the new functions. :-)\n. @leonidlm See issue #94. I am sure you could contribute to that effort which seems to be the most promising avenue.\n. I would say it is not being developed... My outsider view from the last few years is that no one has taken ownership of megacarbon and ceres. There is always hope, but right now it is a dim flame.\n. This is a great set of patches. I really look forward to seeing this make it in master.\n@SEJeff any news on merging this?\n. This issue is noticed when a Graphite webapp is querying a carbon-cache for data held in cache. Data received via external sources (ex. carbon-relay) is now displayed correctly in a frontend. (Grafana or Graphite) This user described the symptoms in the following thread (exact same issue to our installation): http://librelist.com/browser//graphite/2014/4/6/reason-for-latest-datapoints-are-missing-from-graphs/#ed4c8138db92cfd2feaf08918b270275\nThe patch fixes the described symptoms with the exception of metrics generated by carbon itself. Any points held in the carbon-cache will not be displayed in the web app until they are commited to disk. There aren't any tracebacks but the data is still not there.\nSo strictly speaking the traceback is gone for displaying data in carbon-cache but one of the symptoms is still there, failure to graph carbon's own metrics. I can open a new issue if required.\n. How do you feel about creating the Graphite-Api project, marking it as experimental. This way people can submit their code get it rapidly integrated so that the kinks can be worked out.  \nDemocratize Graphite-API, let people know that the team will be responsive with submissions, permit more people to commit to it. When Graphite-API has reached a point when the expected basic set of API features is there and the cruft has been removed. Advertise that the first experimental phase has ended. \nThe second phase could be choosing a couple design improvements, once again with an experimental mindset. When that is over, then it becomes quality focused for a release.\nIf you build it, they will come.\n. ",
    "Dieterbe": "+1. either include graphlot support or don't.  at this point we include it, and this button makes sense for it.  if you ever decide to remove graphlot, you can remove the button too (or better: feature flags?)\n. @thesamet there is no webapp/local_settings.py, only webapp/graphite/local_settings.py and like I said in my opening post, I configured TIME_ZONE in there.\n. I'm using the built-in webserver, precisely because I want to have a minimal setup that's easy to set up and where I can easily reproduce this issue....\n. @mleinart yes my browser was set to CEST at the time of reporting this bug. what you say seems to make perfect sense.\n. ping @mleinart ?\n. so what's the way to have a simple setup in which you can develop and test your changes right away?  In the official docs (http://graphite.readthedocs.org/en/latest/index.html) I only find instructions to do installs in separate locations which makes it hard to quickly make a change and see the result.\n. #22 is about breakage when your from time is older than your retention. This ticket is about from time > until time, which is an invalid request that needs to be handled.\n. thanks that works\n. I have tested this and everything still seems to work.  I'm actually building a new flot-based interface (TBA soon) that relies on some of these commits.  I will probably refine \"allow jquery.graphite to be used with multiple graphs on one page\" as I test and develop my new interface, so this should probably not get merged yet, though I welcome any feedback.\n. after further testing, I think this is merge ready\n. did more testing. all seems to work fine. merge-ready as far as i'm concerned...\nthe only note I have is that when entering tags for events, the number of events shows up but not the events themselves.  but I have the same issue with the original code so I don't think I broke that.\nplease merge or give feedback, thanks\n. @mleinart any updates? @luciotorre is the author of the original code, and he likes my changes (see above), so... :)\n. ping @mleinart ? this is a trivial improvement.\n. Hi, it should be trivial for me to make a new PR for the master branch. stay tuned.\n. not sure what you mean.  I would argue proper version control includes the ability to see all logical changes made, the reasoning for it (their commit messages), the order in which, etc. This helps explain things during future developments  and/or when bughunting.  That's why git merges by default include the individual commit objects. \n. > I think that if there's at least 1 value something should be returned. safeSum of [1, 2, 3,] and [None, 2, 3] would be [1, 4, 6]\n\nAnyone concur / disagree with that?\n\ndisagree. as soon as one of the values in the expression is unknown, the result is unknown.  that sounds like basic logic to me. there's no way to yield a result that is meaningfull.  if you want to avoid this situation add a keepLastValue() around those terms where you think it's okay to emit a best guess.\n. looks interesting, but I wonder if there's any overlap with consolidateBy()\n. I was having the same problem, I'm using https://github.com/vimeo/timeserieswidget/ to do client side rendering, and over a long period of time there's indeed too many datapoints and performance degrades and can indeed lead to browser crashes.\nI think it's totally reasonable to have a width argument for json output (and other data outputs), as it gives a hint as to how many pixels/datapoints the client side renderer wants to draw.\nI think all current consolidation should also work for raw outputs (see #153).  Your suggestion of sampling instead of consolidating seems reasonable (for both png and raw) as that will allow even better (backend) performance, at the expense of some accuracy.\n. I got a bit stuck using this approach. the code is quite complicated and hard to untangle.\nso I'm trying an other approach (with much more success): I have a generic jquery plugin that makes it trivial to render graphite charts https://github.com/Dieterbe/graphitejs (supports multiple backends: flot, rickshaw, png's).  i'm already using it in my dashboard project (https://github.com/Dieterbe/graph-explorer)\n. yeah i guess that's the right thing to do.\n@luciotorre (original author of jquery.graphite.js) liked the approach and agreed it's a good thing, but neither of us has time to continue with this..\nI've however kept working on https://github.com/Dieterbe/timeserieswidget that project is doing fine..\n. also without format=json (i.e. rendering a png), it's shown twice.\n. but all applied functions are part of the name, so I'm basically suggesting deduplicating based on the entire target strings (including all function definitions) which is safe.\nif we stick with the current behavior then I don't see any way to leverage the globbing without having duplicates, in case there is overlap (like the example above).  the only way to avoid duplicates would be listing all series individually (possibly fetching a list from graphite first, parsing it, etc)\n. another effect of this, is that for scale(...,5) the output becomes scale(...,5.0) , even though case isn't as bad, I would still argue the returned target string should display the factor how I choose to display it\n. Btw according to @sivy it seems like it does the same for movingAverage, but it's actually worse:\nwhen he specifies windowSize 30, graphite claims it was '30.0' instead of '30' (i don't really understand why because the code does the code does \"%d\" % 30 which should really yield \"30\", and he's using 0.9.9)\nif you actually specify 30.0 then graphite complains like so https://gist.github.com/522f5c20ea3103719207\n. sure, i can do this next week.\n. i haven't had a chance yet to reference it from composer_widgets.js, and i'm a bit busy lately.. @drawks could you maybe add it?\n. looking at this again, it seems the Special category doesn't fit well. this is not a function to be applied on a target. it effectively adds a new target with a given name.  I'm not sure how to do that in composer_widgets.js\n. so I guess I'll just submit a new PR that uses the formatPathExpressions function from above\n. I would like to challenge the notion that event management should be performed by graphite.\nevents have different requirements for intake, for storage and for representation.  I believe that graphite should stick to its original focus, of being a metrics storage/management/computation/retrieval system, which is hard enough as it is.\nhere's an example of an existing event/change management system:\nhttps://github.com/Dieterbe/anthracite\n... and a widget that's basically a drop-in replacement for graphite's png images (but clientside rendered) and supports events from anthracite: https://github.com/Dieterbe/timeserieswidget\n. - movingAverage specifically changes an argument '10' into '10.0', but if you try to give '10.0' as input you get\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/django/core/handlers/base.py\", line 111, in get_response\n    response = callback(request, *callback_args, **callback_kwargs)\n  File \"/usr/lib/python2.6/site-packages/graphite/render/views.py\", line 105, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/usr/lib/python2.6/site-packages/graphite/render/evaluator.py\", line 10, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/usr/lib/python2.6/site-packages/graphite/render/evaluator.py\", line 21, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression)\n  File \"/usr/lib/python2.6/site-packages/graphite/render/evaluator.py\", line 29, in evaluateTokens\n    return func(requestContext, *args)\n  File \"/usr/lib/python2.6/site-packages/graphite/render/functions.py\", line 371, in movingAverage\n    window = series[i - windowIndex : i + 1]\nTypeError: slice indices must be integers or None or have an __index__ method\n. note: as a workaround, you can wrap everything with alias(<target>,<key>) and rely on the fact that graphite will return <key> in the returned json. as shown here: https://github.com/Dieterbe/timeserieswidget/commit/3c439739788c4effda00e1315a711bba1e2a3b5a\n. with the latest graphite version (1.0.1) this trick seems to no longer work, getting the same error again. I tried + , %2B, quoted, unquoted, with curl and with browser, with space instead of plus, etc.\n(note i believe curl/browser should convert + to %2B automatically in the background)\nlike originally reported, with curl until=noon+tomorrow works fine but until=6pm+tomorrow (or any other variant using %2B etc) does not.\n. URLS can get too long when requesting many targets and functions applied.\nI've run in to this where apache/graphite returned \"414 Request-URI Too Large\" instead of serving the request.\nI've started to work around that by doing multiple requests and merging their results together on completion.\n(see https://github.com/vimeo/timeserieswidget/commit/0fd6b50858f21f0856d273830092f1247a80dc80), but supporting POST seems much nicer.\nas per \nhttp://stackoverflow.com/questions/417142/what-is-the-maximum-length-of-a-url-in-different-browsers going over 2000 characters may be problematic.\n. another use case: say you have metrics named foo.bar.$var with enough different values of $var.\nin my dashboard I want to support \"show the sum of all\"  (i.e. the graphite query can be pretty short: sumSeries(foo.bar.*) but I also want to support \"show the sum of all minus the ones for $var having a given value. we now need to resort to naming every series explicitly in the sumSeries. my workaround doesn't apply here because it's one huge target that can't be split up.\n. that said, I wonder why the specific check for json input data. why not just support normal POST requests that don't use json?\n. @manolama you seem to be talking about different things. are your comments meant for a different ticket? otherwise i don't see what they have to do with the graphite api accepting post variables.\nthat said, I just discovered that it looks like graphite currently just doesn't care whether the params come in over GET or POST. so it just works:\ncurl <graphite> -d from=-3mins -d target=foo\ncurl creates a POST request and graphite treats it no differently and returns the image B)\nbut the OP wanted json input for some reason..\n. :wave: \n. i like this\n. btw I can reproduce without the scale and summarize, i.e. with merely\nhttp://graphite/render/?target=averageSeries(stats.timers.*tran*.fetcher.dfdfs.duration.success.upper_90)\n. i PR'd a fix. (see above) hopefully they accept it soon cause this error is annoying.\n. This is awesome.\njust one thing:\nthe documentation explains that \"The setting supports multiple values, meaning you can read data from both a Whisper database and a Ceres database\"\ndoes this mean that for every metric requested, it will automatically figure out which storage backend it is in, so that when requesting a graph with N metrics, those can be fetched from different sources based on where they are? \n. i'm confused. it already contains import sys ?\n. i'm confused. it already contains import sys ?\n. yeay thanks Jason!\n. I'm not very familiar with this code and your description is a quite in-depth.\nwhat does this do, in high level terminology?\nthe ticket you're referencing is basically about GE needing a way to get all metric names from a graphite cluster, how does this help with that?\n. so using '**' means it will get the metrics from all cluster nodes? does this work with whisper? rules based and consistent hashing? how about ceres?\n. > not sure about modifying the finder in a way that is inconsistent between ceres and whisper and rrd if there's a way to avoid it.\n+1\n. looks like #329 provides most of what this PR aimed to do.\n. the other day i went over all commits from 0.9.10 to 0.9.12 but couldn't find what caused it. @abhinav-upadhyay is your graphite machine heavily loaded? (can you check iowait & cpu idle on your graphite machine).  i wonder if the issues appears more often is a machine is heavily loaded.\n. I run it in apache, I enabled your CACHES setting in /webapp/graphite/local_settings.py, now let's see whether I don't see the bug for a few days.\n. this is a dup of #272 \n. and frankly, can you explain why the input format has to be json?\ngraphite-web supports POST input just fine. and since we're only dealing with N parameters that all have a value, what would be the point of a structured format like json?\nI've been using POST to submit all my args (target strings, from, to, etc) for a while now (see https://github.com/vimeo/timeserieswidget/) and it works just fine.\n. thanks @brutasse so basically this is a dup of #511 \n. i'm looking into this more , using graphite 1.0.1 and I can report that while this bug is indeed fixed (e.g. date specification is interpreted in the timezone) when specifying timezone with a HH:MM_YYYMMDD pattern, it does not work when using other at-style patterns, such as noon. see examples below. (default TZ is set up to UTC)\npatterns it does work with.\ntarget=carbon.agents.b2dd5129e459-a.updateOperations&from=08:00_20170706&tz=America/New_York\n\ntarget=carbon.agents.b2dd5129e459-a.updateOperations&from=08:00_20170706\n\npatterns it does not work with\nas originally reported, here the TZ just controls the timestamps returned, it does not control how the data specification is interpreted (thus, which data is fetched)\ntarget=carbon.agents.b2dd5129e459-a.updateOperations&from=noon&tz=America/New_York\n\ntarget=carbon.agents.b2dd5129e459-a.updateOperations&from=noon\n\n. i was trying to rewrite this algorithm and tune it, but then i wondered:\nwould it make sense to start doing more of this math using standard packages like numpy? might be faster too if numpy uses C modules.\n. > I resisted this sort of opinionated formatting a while back but I'm softening to the idea if it helps make everyone's lives easier.\nFWIW I'm also in favor of the aility to emit data directly in the format that a renderer expects. would certainly help with flot as well.\n. I think this can be closed.  with an old graphite server, i was getting:\nFile &#34;/opt/graphite/webapp/graphite/render/datalib.py&#34;, line 96, in fetchData&#10;\n    startTime = int( epoch( requestContext['startTime'] ) )&#10;  \nFile &#34;/opt/graphite/webapp/graphite/util.py&#34;, line 60, in epoch&#10;\n    return calendar.timegm(dt.astimezone(pytz.utc).timetuple())&#10;\nValueError: astimezone() cannot be applied to a naive datetime&#10;&#10;</pre>&#10;</div>&#10;&#10;</center>&#10; \nbut using graphite 1.0.2, it works.. looks good to me. i need this. i applied the patch to functions.py and am already using it in prod :+1: \n. I used to unconditionally agree with that, see http://dieter.plaetinck.be/post/25-graphite-grafana-statsd-gotchas/#runtime.consolidation\ni've been working on an alternative backend for graphite which does this. but then a colleague of mine (@DanCech) pointed out this isn't always right, specifically not when combined with specific processing functions like summarize. you can read the entire discussion here https://github.com/raintank/metrictank/issues/463\n. thanks @bmhatfield let's proceed with that one.\n. re my blog post, that was basically the same storage model as what you're describing here (series would end on .bin_<bucket>) with data generated by statsd. I don't think i ever heard of anyone -besides me- using this statsd feature though, and after a while i also stopped cause there was no good way to render the data.\nif the goal is to get percentiles and counts out of the data  (as seems the case in this PR) then I do think this a bit of a weird approach.  It's really hard to choose sensible buckets in advance IMHO, esp as the data will change over time and the preferred/ideal choice of class intervals is often bound (no pun intented) to change over time. If the goal is to plot actual histograms or heatmaps (histogram over time) then this data model is more tolerable. But to get percentiles and counts, then statsd seems simpler and more accurate.\nI think the real problem though is that graphite has no storage model for actual histograms.\nIt would be great if we could implement something like https://github.com/HdrHistogram/HdrHistogram which has dynamic bucketing and less hassle. I believe they can even be aggregated (rolled up)\n. Fair enough @iksaif IMHO. Note that there's now histogram and heatmap plugins for grafana, would be cool if we can make it work with that.\n. note that consolidateBy only applies to runtime consolidation.\nIf you're loading data from historical archives that are aggregated, you depend on what the store gives you.\nWhisper in particular only supports one consolidation function at a time, on a per-series basis, which may not be what you're asking for with consolidateBy. In fact you may be getting data that is off when you combine two different aggregation functions (one set by whisper, one via consolidateBy at runtime)\nSee http://dieter.plaetinck.be/post/25-graphite-grafana-statsd-gotchas/#runtime.consolidation\n. FWIW i've seen a case where a metric that had an empty node (eg foo.bar..baz) resulted in \"IndexError: list index out of range\" when used with aliasByNode. does the extra abstraction/function calling come with a noticeable overhead?. I think I'm a relative outsider since I never had commit access, but my thoughts:\non \"many different implementations\"\nI suspect that a decent amount of people that have found out about graphite, get overwhelmed because the ecosystem is so complicated. so many projects with similar names (BTW I have contributed to this problem with graphite-ng). I suspect it scares them off. (It would be nice to get more insights into whether this is merely a perceived concern or real, but not sure how. inquiries suffer from survivor bias). This notion that \"the official graphite stuff is merely a reference implementation\":\n1) is rarely explained, if at all. adding much to the confusion (even the frontpage of https://graphiteapp.org/ claims booking.com uses \"graphite\" and then goes on to list \"graphite's components\" which simply lists the stock python stuff\n2) is imho needlessly complicated. That's why I would seek to simplify the ecosystem to the extent possible. in particular it seems go-carbon should be merged into the official project (other ideas: get rid of graphite-ng, merge graphite-api and graphite-web, rename carbonapi to something more appropriate).\nI do agree it makes sense to keep evolving the python+whisper stack - a stack optimized for small/medium scale - in a non-breaking way. And it also makes sense for a few projects to exist targeting large scale (e.g. metrictank, clickhouse, etc), though code reuse is always a worthwhile persuit\ngo-carbon part of official stack?\nre \"go-carbon is a separate project with own maintainers\". Who maintains it now, if lomik left it? Do the goals of the project align with the goals of the official graphite-project? In the pursuit of simplifying stuff, just bringing the project under the graphite umbrella seems to make more sense.\nIs it drop-in backwards compatible? (e.g. config syntax and feature completeness). If it contains \"more new stuff\" such as carbonserver that we're not sure we want to support, we can mark it as extra/experimental/unsupported, but i'm more concerned with stuff breaking if we were to switch to it\ngo vs python\nI liked your blog post, Denis, and you point out Go scales easier vertically than python (basically goroutines vs threads), but let's also be clear that Go seems to generally perform better (do more with fewer resources). This has been shown in charts in various projects (go-carbon, carbonapi, etc), and it seems to be a very useful property with installations pushing the limits of what a \"medium size installation\" is.\nWhile I wouldn't suggest spending significant efforts transforming large python code bases to go, I do think that for those pieces where the work has already been mostly done (and has been battle tested) it should be a fairly low-effort way to bring in good improvements. \nEverything else about go vs python gets personal/subjective quickly (do we want to have 2 languages in the code bases, static typing vs dynamic typing, deployment model etc). Personally I think go is a great fit and a reasonable long term language choice, but this is very debatable and I think this decision should be largely made by those willing to put in the work.\ncommittee / governance\nI liked the idea of a very informal governance committee. I see it as a good way to be able to resolve difficult project decisions, such as the ones being discussed now. (and let's be frank, some of this stuff has been discussed for years)\nIf candidates stand up / get nominated and are voted into an odd-numbered group, at least binary decisions can be made with a simple quorum vote. We can make rules such as \"a company can only be represented by one member\", etc. It would bring some clarity to the decision making process.\nThen again, if the current maintainers don't think there's a problem with the decision making process, then there simply is no need for a committee.\nstatsd\nstatsd suffers from an even worse form of the ecosystem chaos problem I mentioned above for graphite, in addition to be unmaintained. But almost always, statsd goes hand in hand with graphite.\nI explained my thoughts in more detail at https://gist.github.com/Dieterbe/c94d5ea9e747f89e34801894a39aa68f and concluded that I think the graphite project should either adopt a statsd implementation (and it should become \"the reference implementation\") or even merge it into a carbon relay.\nother new feature idea to make graphite more relevant again\nfirst class support for units (track units along with timeseries, change unit strings when processed with functions, provide units in render responses so dashboards like grafana can automatically put right unit on axis labels etc)\nother\n\n@deniszh what did you mean with \"IMO lack of flexible aggregators\" ?\nwhat are the downsides to carbonserver compared to our current cluster method? do we lose any functionality or desirable characteristics?\n. or multiple nodes\n. I'm a bit surprised / confused by how this works. I understand '.'.join(<somelist> and i've done list comprehensions before, but it's interesting how the list here is constructed with the <list>[n] syntax wherein [n] is part of a list comprehension.\n\n```\n\n\n\nname = \"abc.def.ghi.jkl\"\nnodes = [0,2]\nname.split(\".\")[n] for n in nodes\n  File \"\", line 1\n    name.split(\".\")[n] for n in nodes\n                         ^\nSyntaxError: invalid syntax\n\".\".join(name.split(\".\")[n] for n in nodes)\n'abc.ghi'\n```\n\n\n\nanyway, just some python magic / syntactic sugar i guess. it seems to work fine.\n. this could be a bit clearer.  AFAIK \"query-time aggregation\" is new terminology that we haven't used before.\nexactly which functionality does this apply to? I see 3 cases:\nA) runtime consolidation (over time. as driven by consolidateBy)\nB) math functions over time (like movingAverage)\nC) math functions that work across series but not over time (like sumSeries, avg); \nto which does this apply?. that's great but i do think we should clean up the terminology to be more clear.  my suggestion is to instead of introducing the new terminology \"query-time aggregation\" , say \"math functions and runtime consolidation\" or phrase it like it's phrased in the docs for the setXFilesFactor function.\n. ",
    "bmhatfield": "Never mind -- I think this idea is more broken than I had originally assumed; there's some bugs related to my changes. I'm going to have to abandon this idea for now and revert to 0.9.10.\nI really believe we should not drop Python 2.4 support yet though. Even if that means pain rewriting Ceres in places to support it.\n. That alternate course of action would be totally awesome too -- I could not tell at a fast glance what the architecture around ceres was, and therefore had a hard time figuring out where to safely cut it out. Making it an upgrade if-and-only-if you upgrade to py > 2.6 is entirely reasonable to me.\n. @supre - I have an alternate approach to this problem. On the Graphite server itself, I run some variation of this command: sudo find /opt/graphite/storage/whisper -type f -name *.wsp -mtime +7 -ls -delete && sudo find /opt/graphite/storage/whisper/ -type d -empty -delete, which will remove all metrics that have not been written to in the past week.\n. After further review, it appears that the line \"from functools import partial\" is not needed, at least, partial is not used anywhere in that file.\nPerhaps it can be outright removed?\n. Pull Request Submitted: https://github.com/graphite-project/graphite-web/pull/110\n. I also hit this - I am surprised that this was not caught prior to release?\n. This fixes a bug related to https://github.com/graphite-project/graphite-web/pull/523, which adds now=None almost everywhere it's needed.\n. I don't remember if this is still valid. I think this still fixes a bug though. Looking through #523, this appears to be still useful. Thoughts?\n. I would recommend attaching \"before and after\" screenshots so we can see how it affects the resulting graphs, but my instinct is that the problem attempting to be solved is real and would benefit from a fix. \nHowever, this change as it exists today could benefit from a little TLC prior to merging. My high level concerns are:\n- Difficult to interpret variable names like conso1 and conso2. I would prefer longer variable names to improve clarity, like source_consolidation_method or whatever. Really just looking for readability for folks who might be coming in to debug this in later months/years.\n- Use of empty string as a sentinel for 'we don't know'. Perhaps None would apply better? Or the code could be refactored a bit to make this make more sense and reduce/remove the need for that?\n- Lack of tests to demonstrate that this change provides the value expected.\nApologies for the fact that there's so much feedback after this change has sat so long. Thank you for your patience and continued energy (!!) to try and get this improvement in, @g76r!\n. Let 'er rip!\n. We're also looking to bring our install up to current as well, so I'll be upgrading over the next couple weeks as well.\n. My 0.9.12 install has those lines linked above @deniszh, and I can't see metrics out of carbon-cache until they are created on the filesystem either.\nEDIT: Nevermind. I think this is a different issue.\n. As someone who has previously deployed Graphite via packages (rpms, then debs, now straight python packages installed by pip), I think this thread is headed in the right direction.\nSome thoughts:\n- I have previously had success using \"python to OS package\" plugins to setup.py, bdist_rpm is built in, and bdist_deb is a short install away: https://pypi.python.org/pypi/stdeb\n- I think it's probably wise to try to \"stick to the distro\" in terms of init support. Don't switch or add supervisors on people (eg, no runit, no supervisord, bluepill, etc). This means likely writing a few init scripts. Word to the wise: carbon-cache and carbon-relay are not supported by upstart without 'hacks' such as forcing them to run in the foreground, which affects where logs are written. It might be worth doing some work on their daemonization approach in this regard.\n- I think it's also probably wise to \"stick to the distro\" in terms of Python support. I think this will be the most controversial, as I found that people sometimes want to virtualenv graphite, etc. This also means that we'd have to be very clear about which distros are supported (as @obfuscurity already laid out!). It would also indirectly prescribe which Python versions must be supported, which may or may not be something we want.\n- Carbon was the most difficult to package in my opinion, due to making sure the config files are sane. Currently Graphite generally assumes deployment into /opt/graphite - will we change that up to be more \"FHS\"-like? (You can make an argument that /opt is fine, but I think ya'll see where I'm going with that).\n- Graphite web is the second hardest. I see some discussion above about choosing to deploy under a webserver (eg, apache, nginx, whatever) or specifying a dependency. My current thinking is specify a chosen webserver as an optional dependency (eg, apache), and let the admin choose. Though this is quite different than where we are at today, it's probably wise to provide a python-native django app server if they don't choose anything, detectable by postinstall.\n- Speaking of Django and other 'probably not available' python libraries: If we're going to host this via ppa (or s3 http apt repo, which is also quite doable), I'd recommend (though not excitedly) either cloning or packaging each dependency of Graphite into it's own OS package hosted in the PPA as well.\n- Because the above is a ton of work: Is it worth building \"dependencies included\" packages of Graphite instead of separating out each Python library dependency? In the Debian world, that's a no-no, but like the way some people vendor Bottle when building a Bottle app, it might make sense to vendor all of the dependencies of graphite-web into the graphite-web package, rather than depending on them to be installed separately. It'll make the package larger (though not omnibus large), but it will dramatically simplify deployment and upgrades in the future. My gut instinct is that this is what people \"probably want\" anyways - especially if said dependencies only appear to Graphite-web/whatever.\n- Whisper is quite easy to package.\n- What will the support be for Ceres and such?\n. So, now that I've dropped a bunch of random thoughts, let me approach this a different way.\nPre-caveat: I believe strongly in OS packages wherever applicable. I don't love the proliferation of every language gets it's own package manager.\nSo here goes: What problem are we trying to solve by packaging Graphite into OS packages? As is probably obvious from my above comment, I used to invest a lot of effort into packaging Graphite. But I also invested an equal amount of effort into making sure my configuration manager understood what the package was doing so it could \"correct\" it for my deployment (eg, move /opt/graphite/storage/ to a larger partition). Each thing the package did in a roughly 'standards' way, I had to go back and tweak to make things work better for my particular deployment. Every time Graphite needed to be upgraded, I was amazed at the amount of changes that went into both packaging and configuring it vs. the previous version.\nUltimately, I ended up in a place where I just mirror the pypi tarballs and install them using pip, then perform my configuration tasks as needed. Somehow, it ends up feeling simpler, though I agree it's a lot of work repeated community wide.\nAnyways, I think it's worth being clear what goals we are trying to achieve by creating 'the way' of installing Graphite; to make sure that we actually achieve them. Packaging won't in and of itself make deploying Graphite meaningfully easier, given that much of the challenge of installing it comes after the bits are actually installed onto the machine.\nEDIT: PS, to make sure this is super clear: I am pro-binary packages for Graphite. But it'd be a good idea to align on what problems we can solve, vs which we cannot. We can likely solve \"make getting the bits on the machine\" more like what Admins are used to w/r/t to 'sudo aptitude install graphite-web' or something, but probably not make it particularly easier to install Graphite.\n. This was master, a few commits behind, but that doesn't matter because this hasn't changed since 2012: https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/remote_storage.py#L178-L196\nIn the context of wait_for_results(), the connection variable has not been set. Thus, the results cannot be retrieved from remote hosts.\n. Removed the print statements (oops, nice catch!).\nThe code on master is pretty substantially different, but I don't have any other updates for this in it's current form.\nIf you're in to porting it to master, I say goforit!\n. Oh, sad!\nI have REPLICATION_FACTOR=2, and am not having this problem. I wrote some code that attempts to prevent duplicate series from sneaking through, but perhaps I failed to consider a use case for it. Let me check the divideSeries graphs we have, and see if I can reproduce the problem.\n. Hrm, I'm not seeing this kind of error using Wildcarded series with the ones we currently dashboard.\nFor a series that looks like this:\ndivideSeries(sum(stats_counts.worker.*.REDACTED.*),sum(stats_counts.worker.*.REDACTED)), I indeed get a graph with data in the form that I would expect.\nI also tried a query with rawData=true that pulls in a variety of wildcarded metrics and that also worked.\nWhile I can't reproduce this yet, I do believe you're having a problem. I'm on Python 2.7.3 - what kind of other context can you share that might help me tickle this issue?\n. @lamont I am running a version without this commit:\nhttps://github.com/graphite-project/graphite-web/commit/fe6cda0a02fa152aa95b1798c561c3fbb4b22438\nI don't see how it could be related yet, but I am just noting it for posterity.\n. My architecture is a little simpler: 4 Graphite nodes, all behaving the same: 2-tier carbon-relay, 1 tier carbon-cache. The webapps are all configured to talk to each other (but not themselves).\nActually, configuring a webapp to talk to itself for this might be a little interesting. Can you confirm that all of your graphite web local_settings.py's CLUSTER_SERVERS on each node only looks at the appropriate non-self hosts in the cluster?\n. Actually - that could totally be the problem, and would be different with 1010 and not with 1010. In not-1010, the same loop covers both local and remote hosts. In 1010, if you loaded a metric using the LOCAL_STORE, and then again got it using the REMOTE_STORE, that could cause it to appear in the series list twice.\n. Hrm, on second read, maybe not. Worth a double-check anyways, though.\n. Good luck.. I don't really believe that commit is the issue, but since you said it was easy to try... :+1: \nI haven't come up with anything else interesting on my side yet :-(\n. PS: If it does have issues, any hacked in logging or sample data or anything like that will be super valuable. You can email me sample data direct ( my-gh-username at gmail ) if you'd prefer not to have it public (though I prefer to keep the actual discussion on the issue, since this is now marked at as blocker for 0.9.13 :-( )\n. Can you hack in some logging as to what series are being referenced?\nIE; is it 0 or 2 or ?\nAlso, just to confirm: the same exact query works fine on not-1010, correct?\n. Ah, this is indeed very interesting information. I was guessing 2+, so 0 is useful.\nFor the pre-1010 code, does it error in a different way, or does it actually have a series present?\nIf it does have a series present, do you have any indication if it's coming from the local node, a remote node, or both?\nIf it's a remote node, do you know if timeouts are in play at all?\nAlso, do you know if you accidentally have files that would satisfy the query for the divisor metric (basically all Nones) on more hosts than you expect?\nI think I can see a few ways that Line 306 here could cause us to break out of this loop at a surprising time.\n. I haven't tested it, but 1010 on a frontend to a non-1010 backend should be fine.\n. Thanks for all your help digging into this - like you, I found it to dramatically help performance, so I hope we can make it stable so everyone can get the benefit :-)\n. I made a quick branch with some debug statements, that should end up in the webapp's info.log:\nhttps://github.com/bmhatfield/graphite-web/compare/graphite-project:0.9.x...09x-debug\n. I'm not sure we should close this quite yet, I'd like to understand why 1010's behavior is different in this case a little bit better. My outer assumption is that while it's inefficient to return the same metric from each of 10 servers, it should still work roughly the same as pre-1010.\n. @lamont How many distinct series do you see in pre-1010 for the query that's reflected in the updated logs you sent me?\nThu Nov 06 01:12:57 2014 :: Fetched 'local=1&target=YOURWILDMETRICEXPR&format=pickle&from=1415167967&until=1415254367&now=1415254367' from ANIPADDR:8080, received 1 series\nThu Nov 06 01:12:57 2014 :: List contains: '['ASINGLEMETRICNAME']'\n. The ideal case would be that this gets improved to not be a gotcha :-)\n. @lamont Any chance you could hack in a log for what series are returned by what backend hosts, like what I did for remote_storage.py, but in non-1010, with memcached enabled? I have a guess as to what the problem is with 1010, and I think it's fixable in 1010, but I need a little more data :-)\n. @SEJeff is there a particular doc that you'd recommend we edit in any case? Happy to help but the request feels a bit vague at the moment :-)\n. I think we're good here. Per the earlier discussion, this works for @lamont without the shared memcached setup, and I have been using this in production for some time and am happy/confident in it's behavior.\nWe can close this issue, but I'd like to use this as an opportunity to plug #1026 as well - it's a meaningful enhancement to #1010.\n. @lamont I'm going to be checking out if this helps improve some still-slow performance I've been seeing. This may dramatically speed up your experience, given that you were previously querying from 10 hosts sequentially.\n. I'm going to un-WIP this; I have deployed it to production and I can confirm the effect is as intended.\n. Bump on this? \nI can confirm that Master is far too diverged for this to make sense in it's current form. I am happy to take the lead on carrying the concept over to master as we start thinking about how to make it the sole branch for new development, but it takes re-understanding behavior and redoing this set of work to make it happen.\nEDIT: @jraby thanks for the heads up. Reviewing 795, it appears to be building on concepts pre-1010. There's no particular need for a thread pool at this stage, because in my opinion, even a huge cluster (100 nodes?) is not likely to incur what I would call concerning thread overhead (as compared with the performance gains).\n. @toni-moreno - not yet. Master is sufficiently different that simply cross-merging won't do it, they need to be rewritten :-(\n. Oh dang, I forgot about this bad boy. I am generally interested but don't see myself being able to carve out time to do this right (ie; stand up something clustered running master and trying it out) over the next few weeks. If it's more urgent than that due to me not doing it before, I am happy to review PRs in lieu of actually writing the code.\n. Doing a quick skim of the code and @cbowman0's comment, I think that this is indeed probably 90%.\nIssuing the find requests is redundant, since calling render on a node that doesn't have the series is plenty fast and doesn't require waiting for the results of the find, so there's a speedup probably to be had there. The existing optimization of not issuing /render/ against nodes that don't have the data is probably not worth it, but they're close enough that it's probably good enough to just go with this.\n. This also looks fine to me, though I haven't tested it. It closely follows the pattern in #1026.\n. I know this just got merged, but I'm having a hard time understanding all the implications of these changes, and I recently spent a good amount of time understanding this section of the code. I'm a little nervous that this is a fairly complicated change.\n. Was this behavior experienced before #1010 and #1026? Or is it new since upgrading?\nWhen tracing this out for the issue filed with 1010, I'm not sure what would lead to this. There's an interaction somewhere that I haven't picked up on.\nHonestly, though, I have found Graphite to perform so much better by taking that memory that would be used my Memcached and just letting Linux use it for the disk cache instead (on whisper nodes), that I don't use Memcached in production.\n. Wow! :+1: \n. @gboily Thanks for your PR! We've had a few similar PRs and seem to have settled on yours to solve the problem. Would you mind dropping by this thread and rebasing so we can look towards merging this?\nThank you!\n. @gboily One last ping on this; otherwise we'll probably take over this PR / patch and make it mergable.\n@Dieterbe perhaps you'd be best positioned to turn this into a mergable PR given that you've already done it against your system?\n. @ctrochalakis go for it :-)\n. Yeah! I like this! Sorry I did not notice before.\n. This looks great to me. It took me a few moments to line up all the behaviors here, but I believe we ought to see a performance improvement and have merged series, which seems wonderful to me.\n. This looks good to me, but I am not sure if the py26 requirement is OK or not. @obfuscurity ?\n. \n. I'm in favor of this change. I know @obfuscurity is not a fan of all of these, but I think it's still OK and good to mention that the backends exist.\nI also saw a comment where someone was using this: https://github.com/InMobi/level-tsd - is it worth including?\n. Merging this on the basis of @deniszh and @jjneely's comments - we can always revert if it's inappropriate for some reason :-)\n. It took me a moment to understand this, but yes: graphite-web is only designed to be at most a 2-tier system (the \"query node\" and the \"cluster nodes\"), not a 3-tier system (\"query\" aka your global view, \"regional\" aka mid-tier query-node, \"cluster\" aka your storage-only nodes).\nAs a stop-gap, I believe Grafana can query multiple graphite clusters, but I am not sure: http://docs.grafana.org/datasources/graphite/\n. If I comment out TIME_ZONE in local_settings.py, the overriding behavior is UTC. I see that in the graph and request log:\nTue Nov 24 22:38:41 2015 :: Request TZ is None\nTue Nov 24 22:38:41 2015 :: Acting TZ is UTC\nTue Nov 24 22:38:41 2015 :: Request timerange: 2015-11-24 21:39:00+00:00, 2015-11-24 22:37:00+00:00\nIf I then specify &tz=America/New_York, I get a no data graph back, and nothing is logged in render/glyph.py.\n. I know at least that specifying &tz=UTC when TIME_ZONE was 'America/New_York' in local_settings.py worked before\n. Based upon a theory that specifying the &until was doing funny things, I let the &from fall through to default, in UTC and not:\nTue Nov 24 17:45:38 2015 :: Request TZ is None\nTue Nov 24 17:45:38 2015 :: Acting TZ is America/New_York\nTue Nov 24 17:45:38 2015 :: Request timerange: 2015-11-23 17:46:00-05:00, 2015-11-24 17:45:00-05:00\nTue Nov 24 17:45:47 2015 :: Request TZ is UTC\nTue Nov 24 17:45:47 2015 :: Acting TZ is UTC\nTue Nov 24 17:45:47 2015 :: Request timerange: 2015-11-23 17:46:00+00:00, 2015-11-24 22:45:00+00:00\n. It appears that we're manipulating the timezone of a request in more than one place:\nTue Nov 24 18:09:27 2015 :: Request TZ is UTC\nTue Nov 24 18:09:27 2015 :: Acting TZ is UTC\nTue Nov 24 18:09:27 2015 :: Request start time: 1448385000 & request end time: 1448388480\nTue Nov 24 18:09:27 2015 :: Request TZ-converted timerange: 2015-11-24 17:10:00+00:00, 2015-11-24 18:08:00+00:00\nTue Nov 24 18:09:30 2015 :: Request TZ is None\nTue Nov 24 18:09:30 2015 :: Acting TZ is America/New_York\nTue Nov 24 18:09:30 2015 :: Request start time: 1448403000 & request end time: 1448406480\nTue Nov 24 18:09:30 2015 :: Request TZ-converted timerange: 2015-11-24 17:10:00-05:00, 2015-11-24 18:08:00-05:00\n. The best info I have so far is that this https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/views.py#L292-L322 seems to be a bit early in the request cycle for manipulating timezones, or that it's happening partially (ie, one, but not the other).\n. ```\nTue Nov 24 18:39:59 2015 :: Default timezone: America/New_York\nTue Nov 24 18:39:59 2015 :: Selected Timezone: America/New_York\nTue Nov 24 18:39:59 2015 :: now: 2015-11-24 18:39:59.283623-05:00 | untilTime: 2015-11-24 18:39:59.283623-05:00 | fromTime 2015-11-24 17:39:59.283661-05:00\nTue Nov 24 18:39:59 2015 :: Request TZ is None\nTue Nov 24 18:39:59 2015 :: Acting TZ is America/New_York\nTue Nov 24 18:39:59 2015 :: Request start time: 1448404800 & request end time: 1448408340\nTue Nov 24 18:39:59 2015 :: Request TZ-converted timerange: 2015-11-24 17:40:00-05:00, 2015-11-24 18:39:00-05:00\nTue Nov 24 18:40:02 2015 :: Default timezone: America/New_York\nTue Nov 24 18:40:02 2015 :: Selected Timezone: UTC\nTue Nov 24 18:40:02 2015 :: now: 2015-11-24 23:40:02.390206+00:00 | untilTime: 2015-11-24 23:40:02.390206+00:00 | fromTime 2015-11-24 17:40:02.390241+00:00\nTue Nov 24 18:40:02 2015 :: Request TZ is UTC\nTue Nov 24 18:40:02 2015 :: Acting TZ is UTC\nTue Nov 24 18:40:02 2015 :: Request start time: 1448386860 & request end time: 1448408400\nTue Nov 24 18:40:02 2015 :: Request TZ-converted timerange: 2015-11-24 17:41:00+00:00, 2015-11-24 23:40:00+00:00\n```\n. PR added above. This also seems to be partially caused by https://github.com/Kixeye/graphite-web/pull/10\n. It's also related to https://github.com/graphite-project/graphite-web/commit/fe74b18\n. The Travis tests fail, but I think there's a chance that the test equivalence might be not correct. I'd love a second pair of eyes on this, especially @mleinart and @andrewmcgilvray (and of course @obfuscurity )\n. I did some more research into why the test was failing. I no longer believe that the test is incorrect. It appears that https://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/render/attime.py#L52 returns a datetime without a timezone, which #L48 tries to localize but doesn't succeed.\n. Closing in favor of https://github.com/graphite-project/graphite-web/pull/1409 after much research and experiments.\n. @mleinart @obfuscurity @deniszh @brutasse - this is tidied, tests updated, and ready for review.\n. We're now running this branch in our environment, and I can confirm it's working as intended :zap: \n. I think this is a step towards a great idea, but I think this PR misunderstands the problem slightly.\nGraphite-web already queries carbon-cache for datapoints in the case where it's aware a metric exists. The problem is that this awareness purely comes from globbing the file system. \nCarbon-Cache doesn't support any sort of \"list keys\" or globbing behavior, so this PR will only fix the case where the full path to the metric is specified.\nA more correct fix, I think, is to add list-keys or globbing type behavior to carbon-cache as part of the \"metric find\" functionality. Then graphite-web will know there's some uncommitted metrics in carbon-cache and will naturally attempt to retrieve them. \nOn the carbon side, that lookup will need to be performant, as it will happen frequently. \n. I can +1 this. I am not sure what version of what is breaking things - I am taking a look now.\n. Alright - it took me a little while, but I determined that the behavior here is different on more recent versions of Cairo, and SVG rendering no longer returns an error.\n. I refactored a few things in 0.9.14/15 w/r/t attime - this may already be fixed.\nEDIT: Specifically I adjusted the weird timezone math to go to 100% UTC, and as a result of that, we were able to remove some of the more fragile conversions.\n. This PR seems like a good idea - thanks for identifying a performance problem in Graphite and submitting a PR to resolve it.\nI made a couple of comments that may be imprecise; my request is to perhaps take a second pass over the changes as submitted and see if they can be simplified somewhat.\nAlso, please add some tests for these changes, since future folks working on this code may not understand why it was changed and unintentionally remove your awesome improvement!\n. This looks good to me! Thanks for tidying it up and adding tests!\n@obfuscurity happy to have a double check here, otherwise I'll merge this later this afternoon.\n. It's true: http://physics.nist.gov/cuu/Units/prefixes.html\nPlease note that you have failing tests that are looking for 'K' and now seeing 'k'. Please go through and update any/all tests that would be affected here :-)\n. Looks great, thanks.\n. Looks great, thanks.\n. I like this change. However, I think 0.9.x might have a \"no new merges\" EOL policy? I am not sure; @obfuscurity could say for certain. As someone who is also on 0.9.x, I'd personally be happy to have them.\nIn the case where this is not an official EOL, would it be possible to update/add tests for these changes?\n. What about this one? https://github.com/graphite-project/graphite-web/pull/1171\n. If the retentions for the files are the same, and both carbon-cache daemons are running with the same storage-schemas.conf, then this would imply weewxb it not receiving all of the points. Perhaps try logging updates (a setting in carbon.conf) to see if that's true?\n. Hi - I just wanted to chime in with a note of encouragement, even if you're not ready for a merge yet. Thanks for doing this, super excited to see it.\n. You are amazing. I gave this a skim but I think you are GTG at a high level, especially since the tests you added found bugs...\n. Hi,\nI took a look at these. While the tests look fine, I agree with what seems to be a general unease around understanding the implications of the changes to glyph.py. What kinds of things can you do (screenshots?) to identify before/after changes or the lack thereof?\n. I did not check that your assertions are correct, but I skimmed over the rest and it all looks great to me. In particular, as before, I love the bug fixes that are a result of you adding coverage to things. Really great work here.\n. @ctrochalakis thanks for this PR!\nYou might not be surprised to learn that there's been a few PRs around this.\nIn previous discussions, I think we'd agreed https://github.com/graphite-project/graphite-web/pull/1171 was going to be the merged version, but our fearless PR author has not returned to make it mergeable. I personally like the name \"groupByNodes\" better than the longer \"multi\" versions.\n. @ctrochalakis Thank you so much for doing this!!\nFrom my perspective, this looks like a nice improvement and low risk, and something we've been waiting to merge for some time. @obfuscurity I am good to merge this unless there's any reservations?\n. @ctrochalakis Thank you so much for doing this!!\nFrom my perspective, this looks like a nice improvement and low risk, and something we've been waiting to merge for some time. @obfuscurity I am good to merge this unless there's any reservations?\n. @nyerup heads up that your email account has posted spam to this issue a couple of times. You may want to check for breach/malware/etc.\n. I agree with updating antiquated terms, but I am curious: Are safelist / blocklist clear enough in this context? Could we use simpler language such as Allow and Deny?\nTo clarify: whitelist/blacklist are etymologically not racist terms but if there's modern connotations, then the etymology doesn't particularly matter and I am comfortable considering them antiquated. However, they do have clear meanings, so we should make sure to try and preserve the value of the specific terms by replacing them with at-least-as-clear language, to which I think safelist is particularly unclear. blocklist is closer, but that's why I am suggesting allow or perhaps AllowOnly.\n. I did a bit of work on attime on 0.9.x that probably should be carried forward if they don't already exist; notably, I got rid of all the wonky TZ code and instead did everything in UTC, then converted to the requested TZ at render time via built-in libraries.\nSee if anything in here resonates: https://github.com/graphite-project/graphite-web/pull/1409\n. Thanks for doing this! This all makes sense to me, and looks like it merged well!\n. @atnak I believe @obfuscurity was asking for a graph picture that demonstrates your problem, since we obviously don't have access to the data your query is referencing.\n. Can we do this without pulling in requests? We've been pretty good about avoiding it thus far as a requirement. Or we should go all-in on it.\nAlso, this branch seems HUGE. I imagine that the branch's base it perhaps out of date?. Ah, yes. Will update.\n. Given that 115 returns True, why would this be False here?\n. This feels like a weird if not ... or not to me. Can you talk out the intent here?\n. Doing some testing locally, it looks like defaulting REMOTE_EXCLUDE_LOCAL to 'False' is the inverse of what we have today, no?\n``` python\nhosts = ['onehost', 'twohost', 'threehost', 'localhost', 'greenhost']\ndef check_item(item):\n  print(\"Checking if localhost: \", item)\n  if item == 'localhost':\n    return True\n  else:\n    return False\nprint([host for host in hosts if not check_item(host)])\nEXCLUDE_LOCAL = True\nprint([host for host in hosts if not EXCLUDE_LOCAL or not check_item(host)])\nEXCLUDE_LOCAL = False\nprint([host for host in hosts if not EXCLUDE_LOCAL or not check_item(host)])\n```\nouputs:\n('Checking if localhost: ', 'onehost')\n('Checking if localhost: ', 'twohost')\n('Checking if localhost: ', 'threehost')\n('Checking if localhost: ', 'localhost')\n('Checking if localhost: ', 'greenhost')\n['onehost', 'twohost', 'threehost', 'greenhost']\n('Checking if localhost: ', 'onehost')\n('Checking if localhost: ', 'twohost')\n('Checking if localhost: ', 'threehost')\n('Checking if localhost: ', 'localhost')\n('Checking if localhost: ', 'greenhost')\n['onehost', 'twohost', 'threehost', 'greenhost']\n['onehost', 'twohost', 'threehost', 'localhost', 'greenhost']\nWould we not want this to be backwards compatible by default?\n. A glob technically also can have a ?, not sure if it's possible to pass it through the render API though.\n. Instead of if has_wildcard here, why not just set listdir = [ pattern ] on 81?\n. Or rather, set entries in 74-81 and skip the listdir variable altogether.\n. This timeout should be a setting, and not hardcoded.\n. Same\n. I'm a little confused about the pre-existing threading model and these changes. Are these fetches queued up / threaded in some other place? Or was this a missed opportunity that you're now fixing?\n. Why would this be the case if we only start settings.MAX_FETCH_THREADS?\n. Probably worth logging the actual exception in this case?\n. This variable is probably more clearly named pending_fetches\n. Why not just track the index in fetches_running and delete from the existing array, rather than building up a duplicate?\n. Can this except be more specific?\n. I see there's a similar pattern above with the fail() and return None. Is there some additional information you determined in debugging this taht would be helpful context here? Is None the right thing to return? Is there a richer/more correct type?\n. What was the actual exception with instantiating this class?\n. What's the difference between here and storage.py:41?\n. ",
    "obfuscurity": "@xiaods I think we'd all love to see some official distribution specs (Red Hat, Debian, etc) distributed with Graphite, but we don't have any interest in maintaining them internally. I think it would be in everyone's best interest if the current upstream maintainers submit pull requests to have the specs included in Graphite, with the understanding that they'll also be responsible for testing and keeping them up to date.\n. @Dieterbe This is probably not a best practice, in that you're writing storage files into a checkout. Until we have a better workflow around development deployments we probably shouldn't document it like this. In fact, I'd really like to see all the current content in INSTALL go away and have it point to our official docs site instead. Hmm...\n. @Dieterbe P.S. In the future please submit PRs for enhancements against the master branch. We really only want to backport fixes to 0.9.x. Thanks!\n. Closing this as a duplicate of #22. If this is not the case please elaborate. Thanks!\n. Gotcha, thanks for clarifying.\n. Frankly I think lazy-loading the graphs would suffice, i.e. load the 1st row, then the 2nd, and so on and so forth.\n. Given that nobody has stepped up to tackle this request in the 4 years since its submission, and the ubiquity of Grafana as the dashboard of choice, I think it's unlikely that we'll find spare cycles to hack on this. I'm going to close this for now but I'm happy to assist anyone that wishes to take a stab at this request.\n. Looks good to me.\n. Yes, I'm still able to reproduce this with 0.9.10. Here is a graph similar to the one above.\ntarget=derivative(sumSeries(substr(exclude(sortByMinima(resque.queue.*),\"resque.queue.*fs*\"),2)))\ntarget=alias(color(drawAsInfinite(deploy.foo.*.*),\"red\"),\"deploys\")\nareaAlpha=0.5\nfrom=-90mins\nhideLegend=true\nareaMode=stacked\nwidth=777\nheight=413\nformat=png\ntemplate=plain\n\nAs a point of reference, I also rendered the graph again without the resque metric and areaMode disabled so you can see we only have a single deploy during this window.\ntarget=alias(color(drawAsInfinite(deploy.foo.*.*),\"red\"),\"deploys\")\nareaAlpha=0.5\nfrom=-90min\nhideLegend=true\nwidth=777\nheight=413\nformat=png\ntemplate=plain\n\nI will try to get you the whisper files shortly.\n. In an effort to pare down the number of targets so I don't need to hand over ~50 whisper files, I've simplified the URL quite a bit while still being able to reproduce the issue.\ntarget=resque.queue.low\ntarget=drawAsInfinite(deploy.foo.*.*)\nfrom=-3h\nhideLegend=true\nareaMode=stacked\nwidth=777\nheight=413\n\ntarget=drawAsInfinite(deploy.foo.*.*)\nfrom=-3h\nhideLegend=true\nwidth=777\nheight=415\n\n. @mleinart I should be able to get you the whisper file from resque.queue.low but I doubt I'll be able to get you the whisper files used for deploy.foo.*.* as those number well into 6,000 individual files and contain a lot of internal deploy info I'm sure we wouldn't want shared externally. Thoughts?\n. The deploy metrics use our default policy. I assume they're using a value of 1 but I really don't know for certain if there are any exceptions.\n[default]\npattern = .*\nretentions = 10:6h,60:7d,600:5y\n. @mattpascoe It would help if you could provide more details (queries, screenshots, actual data from raw/json output). #31 has been fixed now and I don't see any of the issues you've described.\nThe closest thing to it that I've personally seen is when applying sumSeries to metrics resulting in zeroes across the bottom axis. See my blog post for an example.\n. Would you mind grabbing the output of the following query and pasting the results?\nhttp://graphite/render?from=-5minutes&target=events('*')&format=raw\n. @mattpascoe What version are you running?\n. @mattpascoe I don't know if it's fixed but I'm unable to reproduce with any of my data. Unfortunately I don't use Events which may be relevant to this issue. At any rate it doesn't sound like this is applicable to the 0.9.11 milestone. @mleinart?\n. Ok, closing this out. Apologies for the very long time-to-resolution but hopefully this makes sense now.\n. The areaBetween function is rather primitive and naive about the format it will accept series. Going to close this issue and track fixing the \"robustness\" of this function in #381.\nPlus, it looks like @mleinartas-iseatz is probably @mleinart's old user account from iSeatz. :smile:\n. @luxflux Sorry for the delay here. I'm unfamiliar with this particular specification. Is this for compatibility with a client lib?\n. Honestly this is a Rubyism inherited from Rails and the like. If it was any more complex than this I'd be :-1: but seeing as it's fairly innocuous I'm mostly :+1:. However, there are a couple changes I'd like to see.\n- comment this :poop:\n- this should probably be an if-else. I don't think we should allow both targets and target as valid params at the same time.\n. Looks ok to me. Thanks!\n. Just stumbled across this as well. Also affects yMinLeft and yMinRight (and their yMax* counterparts).\n. Works great, thanks! :+1:\n. I'd still like to see this happen but I'm not sure who's willing to take this on. /cc @esc @brutasse @gwaldo.\n. I'd also still love to see this happen. There are few docs experts among the core team so I'm not optimistic this will happen before 1.0.0, but I'm holding out hope for the next release after.\n. Fixed in #60\n. @jblaine The last three entries are missing newlines.\n. @jblaine Something go wrong with your update/merge? I don't see any new changes. Or maybe I'm assuming too much. :wink2:\n. LOL, no worries. Looks good. Thanks for the pull request. :sparkles:\n. Does this cause any regressions with lineMode=staircase? That change was introduced in https://github.com/graphite-project/graphite-web/commit/5d5192ccefcfb5ddc88cb78f5ab08b3b38a687d4.\n. I want to :+1: this but I'd like @mleinart or @drawks to review for possible regressions first.\n. Ignore\n. @mattus Would you mind resubmitting each commit as a separate pull request, each against the master branch? Enhancements like sortByTotal probably shouldn't be pulled into 0.9.x, but the other one might be. Thanks!\n. Looks good to me. :metal:\n. /cc @tmm1 @wfarr\n. Fixed in #70.\n. @Dieterbe This should be submitted against the master branch, not 0.9.x. Thanks!\n. Am I seriously looking at a PR for fixing attribution in a specific branch? Just how ridiculous is this?\n. Meh, let's try again.\n. Note to self: the RRDFile stuff that we used for the 0.9.x version of this feature was in storage.py has been refactored in master and is now found as the RRDReader class in readers.py.\n. Another note to self: Python < 2.6.0 doesn't support followlinks in os.walk. I'm not sure if we're requiring 2.6.0 for master, but something to keep in mind.\nSee #80\n. This seems very unlikely to change. If someone has the motivation to create this code and submit a PR, I'm sure we'd happily consider it for inclusion. As it stands, there's little reason to keep this issue open on the very unlikely chance it will tickle someone's fancy into hacking it up.\n. As mentioned, there's no way to support interactive hover legends natively with PNG images. This is something you could do client-side with the various charting libraries. Even with SVG you'd need to perform the hover code client-side.\n. I have to agree with @mleinart. I won't argue that @Dieterbe is wrong, only that the existing behavior is not unreasonable and is predictable for the majority of our users. Perhaps it would make sense to offer a configuration setting or render parameter to override this behavior, but I don't think we should be changing a foundational behavior such as this one.\n. Closing this one out, let's move any discussion over to #1661.\n. Would be lovely to have a 0.9.11 release coincide with Monitorama. In lieu of that, perhaps we can get some work done on closing bugs for it instead?\n. Indeed, this is a duplicate. It's also not a graphite-web bug so we'll track it in the other issue.\n. Example from /render/?width=586&height=308&target=foo.*.cpu-system.value&hideLegend=true&yDivisors=1,2\n\n. Outside of this issue, I've never heard anyone suggest this or complain about the current approach. Can you elaborate on why this is a problem?\n. Confirmed this is still an issue in master.\n\n\n. This seems broken for me in the 0.9.x. Even with connectedLimit set intentionally high it's not connecting the gap for me. Plain old connected works as expected.\n\nThe data:\ncarbon.agents.vagrant-ubuntu-trusty-64-1.updateOperations,1424563260,1424649600,60|633.0,641.0,637.0,638.0,626.0,638.0,640.0,644.0,632.0,639.0,631.0,639.0,637.0,634.0,637.0,637.0,638.0,630.0,639.0,631.0,639.0,630.0,649.0,629.0,637.0,639.0,647.0,639.0,637.0,628.0,639.0,649.0,628.0,640.0,633.0,643.0,641.0,637.0,627.0,638.0,650.0,627.0,636.0,639.0,649.0,639.0,628.0,638.0,651.0,639.0,630.0,639.0,650.0,629.0,638.0,638.0,627.0,642.0,636.0,641.0,638.0,638.0,639.0,628.0,641.0,628.0,645.0,631.0,640.0,637.0,645.0,632.0,638.0,629.0,644.0,641.0,639.0,639.0,628.0,638.0,651.0,630.0,639.0,629.0,638.0,637.0,638.0,638.0,638.0,628.0,645.0,646.0,639.0,627.0,639.0,645.0,628.0,638.0,637.0,638.0,645.0,627.0,638.0,639.0,651.0,637.0,629.0,639.0,639.0,634.0,639.0,639.0,639.0,626.0,646.0,639.0,637.0,637.0,640.0,640.0,639.0,628.0,644.0,646.0,638.0,639.0,634.0,646.0,638.0,643.0,639.0,634.0,640.0,628.0,646.0,638.0,645.0,637.0,629.0,648.0,637.0,639.0,627.0,645.0,639.0,628.0,640.0,647.0,629.0,636.0,639.0,639.0,631.0,636.0,643.0,640.0,631.0,637.0,635.0,638.0,637.0,636.0,634.0,640.0,636.0,629.0,640.0,633.0,637.0,639.0,638.0,636.0,641.0,637.0,630.0,639.0,627.0,650.0,637.0,638.0,632.0,646.0,629.0,637.0,636.0,639.0,630.0,655.0,639.0,636.0,627.0,637.0,643.0,637.0,638.0,631.0,639.0,636.0,639.0,636.0,627.0,651.0,639.0,630.0,639.0,628.0,650.0,638.0,628.0,638.0,643.0,637.0,637.0,625.0,640.0,642.0,640.0,637.0,627.0,638.0,649.0,636.0,639.0,631.0,644.0,627.0,639.0,648.0,640.0,628.0,638.0,638.0,643.0,627.0,637.0,642.0,640.0,627.0,640.0,639.0,648.0,628.0,637.0,646.0,628.0,638.0,637.0,644.0,634.0,638.0,637.0,639.0,628.0,647.0,637.0,638.0,629.0,636.0,649.0,627.0,637.0,637.0,646.0,629.0,638.0,637.0,637.0,642.0,637.0,629.0,646.0,648.0,630.0,639.0,638.0,632.0,642.0,638.0,629.0,638.0,636.0,629.0,638.0,640.0,651.0,638.0,640.0,636.0,630.0,646.0,638.0,637.0,628.0,637.0,636.0,638.0,629.0,642.0,642.0,628.0,639.0,639.0,628.0,647.0,637.0,638.0,635.0,640.0,639.0,625.0,644.0,639.0,636.0,634.0,639.0,639.0,639.0,638.0,636.0,631.0,641.0,627.0,650.0,638.0,628.0,637.0,637.0,645.0,631.0,638.0,628.0,653.0,644.0,631.0,643.0,633.0,637.0,629.0,638.0,639.0,638.0,635.0,629.0,637.0,647.0,637.0,632.0,641.0,635.0,635.0,639.0,637.0,637.0,638.0,637.0,639.0,638.0,644.0,636.0,624.0,638.0,645.0,628.0,637.0,650.0,637.0,636.0,629.0,644.0,627.0,638.0,644.0,628.0,640.0,636.0,627.0,646.0,636.0,636.0,627.0,645.0,640.0,628.0,646.0,638.0,628.0,644.0,639.0,637.0,628.0,641.0,637.0,636.0,636.0,645.0,629.0,637.0,647.0,638.0,637.0,637.0,631.0,638.0,638.0,637.0,637.0,639.0,636.0,628.0,643.0,640.0,632.0,645.0,638.0,630.0,639.0,636.0,639.0,639.0,626.0,644.0,639.0,638.0,629.0,650.0,638.0,628.0,638.0,639.0,634.0,639.0,639.0,637.0,629.0,643.0,636.0,627.0,638.0,652.0,626.0,639.0,636.0,630.0,646.0,639.0,629.0,638.0,646.0,629.0,636.0,639.0,639.0,639.0,637.0,631.0,646.0,631.0,639.0,631.0,637.0,646.0,637.0,636.0,628.0,649.0,638.0,631.0,637.0,639.0,641.0,640.0,636.0,637.0,627.0,643.0,629.0,641.0,638.0,633.0,638.0,638.0,639.0,627.0,653.0,639.0,629.0,638.0,646.0,627.0,639.0,639.0,638.0,642.0,636.0,628.0,637.0,645.0,638.0,636.0,629.0,641.0,645.0,627.0,638.0,639.0,638.0,640.0,637.0,628.0,640.0,644.0,627.0,642.0,637.0,633.0,641.0,638.0,637.0,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,235.0,642.0,633.0,638.0,634.0,626.0,659.0,640.0,638.0,628.0,651.0,634.0,636.0,645.0,644.0,647.0,643.0,657.0,628.0,637.0,638.0,634.0,645.0,638.0,634.0,638.0,626.0,658.0,636.0,637.0,642.0,648.0,639.0,648.0,634.0,636.0,636.0,650.0,648.0,625.0,636.0,635.0,640.0,652.0,638.0,634.0,634.0,630.0,649.0,636.0,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,122.0,625.0,626.0,647.0,630.0,637.0,632.0,641.0,647.0,638.0,631.0,639.0,631.0,645.0,629.0,639.0,629.0,646.0,630.0,639.0,632.0,646.0,631.0,639.0,630.0,647.0,629.0,639.0,630.0,651.0,630.0,636.0,632.0,646.0,637.0,630.0,634.0,635.0,637.0,641.0,627.0,648.0,628.0,639.0,628.0,651.0,638.0,637.0,640.0,629.0,645.0,630.0,639.0,627.0,641.0,645.0,639.0,629.0,641.0,631.0,654.0,630.0,638.0,629.0,639.0,641.0,638.0,630.0,645.0,630.0,638.0,629.0,646.0,640.0,632.0,645.0,630.0,639.0,635.0,643.0,638.0,629.0,646.0,627.0,634.0,639.0,644.0,641.0,628.0,640.0,633.0,647.0,639.0,626.0,648.0,640.0,629.0,638.0,632.0,638.0,640.0,630.0,645.0,635.0,639.0,632.0,639.0,629.0,652.0,632.0,639.0,632.0,645.0,638.0,630.0,639.0,630.0,645.0,628.0,640.0,639.0,638.0,639.0,627.0,640.0,647.0,627.0,638.0,642.0,628.0,639.0,636.0,639.0,629.0,645.0,630.0,638.0,630.0,641.0,649.0,641.0,628.0,639.0,629.0,646.0,630.0,639.0,630.0,630.0,651.0,631.0,641.0,629.0,652.0,629.0,638.0,629.0,645.0,632.0,630.0,639.0,630.0,638.0,641.0,631.0,639.0,639.0,643.0,629.0,641.0,630.0,651.0,630.0,638.0,627.0,650.0,630.0,638.0,636.0,639.0,639.0,628.0,647.0,632.0,640.0,628.0,645.0,672.0,667.0,681.0,656.0,656.0,671.0,668.0,639.0,635.0,635.0,648.0,652.0,633.0,650.0,655.0,622.0,636.0,629.0,647.0,636.0,638.0,634.0,628.0,645.0,634.0,656.0,632.0,659.0,636.0,687.0,677.0,644.0,630.0,674.0,677.0,639.0,631.0,630.0,646.0,645.0,627.0,641.0,630.0,653.0,632.0,630.0,639.0,644.0,624.0,639.0,627.0,638.0,639.0,628.0,639.0,644.0,631.0,636.0,626.0,646.0,630.0,636.0,645.0,630.0,631.0,636.0,650.0,630.0,637.0,637.0,640.0,639.0,627.0,646.0,630.0,638.0,628.0,646.0,636.0,629.0,642.0,631.0,639.0,630.0,657.0,664.0,639.0,636.0,629.0,639.0,649.0,639.0,638.0,628.0,638.0,628.0,645.0,631.0,640.0,628.0,651.0,624.0,640.0,632.0,646.0,635.0,640.0,629.0,639.0,646.0,637.0,631.0,639.0,632.0,641.0,641.0,639.0,635.0,635.0,646.0,633.0,647.0,630.0,652.0,628.0,640.0,630.0,646.0,634.0,640.0,628.0,637.0,638.0,640.0,630.0,639.0,639.0,631.0,639.0,629.0,639.0,629.0,654.0,630.0,634.0,630.0,639.0,645.0,639.0,630.0,639.0,630.0,652.0,632.0,637.0,631.0,646.0,640.0,640.0,632.0,629.0,637.0,642.0,626.0,638.0,641.0,630.0,639.0,642.0,637.0,637.0,628.0,635.0,644.0,637.0,638.0,629.0,640.0,634.0,646.0,630.0,639.0,624.0,652.0,630.0,639.0,629.0,652.0,639.0,630.0,639.0,629.0,645.0,629.0,638.0,640.0,635.0,639.0,631.0,627.0,652.0,638.0,632.0,636.0,640.0,637.0,627.0,645.0,630.0,639.0,630.0,648.0,630.0,637.0,645.0,639.0,630.0,640.0,632.0,647.0,638.0,630.0,637.0,646.0,631.0,628.0,639.0,632.0,651.0,631.0,630.0,640.0,636.0,646.0,630.0,639.0,631.0,645.0,632.0,638.0,631.0,650.0,630.0,637.0,629.0,647.0,630.0,641.0,631.0,638.0,636.0,639.0,629.0,640.0,634.0,640.0,630.0,639.0,641.0,639.0,638.0,629.0,651.0,630.0,630.0,639.0,639.0,636.0,639.0,626.0,645.0,639.0,627.0,645.0,634.0,630.0,648.0,628.0,640.0,627.0,640.0,636.0,638.0,639.0,642.0,638.0,628.0,651.0,643.0,629.0,645.0,638.0,641.0,627.0,648.0,629.0,639.0\n. Sorry, ignore me. I had a brain-fart and forgot that lineMode=connected and connectedLimit=# must be used together.\n. Closing this PR in favor of #1663. @mingbowan thanks for your contribution!\n. @Krinkle When will what be released? What version are you running? Does it include the aforementioned fix? I encourage anyone experiencing problems with 0.9.12 or older to try installing from HEAD of the 0.9.x branch.\n. Unless you can verify that the 0.9.x branch (which the next release will be tagged from) does not contain a working fix, I'd suggest opening a bug report with the Debian Graphite team. It's unclear to me what their 0.9.13+debian-1 package represents, particularly since they're actively removing upstream patches.\n. I don't have specific links, but from their changelog:\n* patches removed, included by upstream:\n    - add_maximum_returned_datapoints.patch\n    - remove_graphlot.patch\n    - django17.patch\n    - django1.6_compatibility.patch\n. Graphlot has been deprecated as of c886299f8c02917f07aa60c874847005ae87fd5a.\n. Appears to have been fixed in https://github.com/graphite-project/graphite-web/pull/487.\n. /cc @jnewland\n. See https://answers.launchpad.net/graphite/+question/166979 for more details. This appears to be a Django issue, unsure if we can / need to fix in Graphite.\n. I believe this was an issue with older versions of Django. As our master branch is now pinned to Django 1.9, this shouldn't be an issue anymore. I'm personally unable to reproduce this issue with the master branch.\nClosing this but if you can reproduce please let us know which versions of Graphite and Django and I'll be happy to investigate further.\n. Graphite does not typically run with permissions to delete files on the local filesystem. I think what @SEJeff was getting at is that since Carbon does the writes, Carbon would be the most likely place to add the requested functionality, since it already has the necessary permissions.\n. Sorry, I didn't read the issue carefully. I still agree with @brutasse that patches are going to more effective than talking about it.\n. Nice attitude, thanks for your contributions to this thread.\n. I like the intent, but I think that not showing legend elements because all values are Null is confusing. What happens when someone requests 2 metrics and one of them doesn't render because there are no datapoints? I think this has the potential to be very confusing.\ncc @mleinart for his input.\n. After discussing with @mleinart I think that the right approach is not to automatically start hiding null series (which would introduce unexpected behavior), but to add a function (e.g. \"hideNullFromLegend\") that would allow for hiding these sorts of series from the legend.\n@bryantsai Would you mind tweaking this as a function like I describe?\nNote: I'm not saying that @mleinart agrees with me, only that I discussed this with him.\n. Closing in favor of #1618.\n. @nightfly19 Can you make that minor change that @SEJeff mentioned? I'd :heart: to merge this in.\n. It occurred to me afterwards that we're missing the UI bits for this. But it needn't hold up this PR imho.\n. Thanks @nightfly19!\n:cake: :sparkles: :heart_eyes_cat: :cherries: \n. As best I can tell this pattern originated in https://github.com/graphite-project/graphite-web/commit/d622628fe6600f2979627149641bf5688e389dfc.\n. I'm not :-1: on this, but it seems like odd bedfellows (to me) to add a URL shortener into Graphite proper. I understand your hesitancy to leak URLs outside of your organization, but there's probably a better solution imho.\n. I get the usability aspect of it, but it's a slippery slope if we start integrating bits that are tangentially helpful. By that logic we could add \"send to\" buttons for email, Campfire, Facebook, etc. Just because we can doesn't mean we should. \nAgain, I'm not :-1: on this, just trying to offer a voice of reason.\n. @SEJeff Have you gotten a chance to test this? It seems the community has spoken. :smirk:\n. Clearly this is the devil's work at hand. Smite thine feature!  :fire:\n. Sorry that this has languished for so long. If someone has the time and inclination to rebase this, I'd be happy to merge it.\n. Unless anyone has objections, I'd like to merge this in today or tomorrow. /cc @esc @brutasse @SEJeff \n. Note that this is only in the master branch. I don't expect that we'd want to any significant features in the 0.9.x branch anytime soon; 0.9.13 is expected to be our last release from that branch. We'll focus on getting a 0.10 release from master after that.\n. I think a better description/request would be to \"allow setting yMin default value\" rather than making yMin=0 the default for everyone. I also think this is a dangerous precedent. The next request will be to allow setting defaults for obscure settings like yStepRight. And then worse, to set them based on metric patterns.\nI'm not going to be the anti-feature guy, but as a fellow OpenBSD fan, I think we should join forces to combat the evils of excessive knobs and buttons. :wink:\n. I hate to be hard-nosed, but I'm :-1: on adding API params to configuration files. There are probably already exceptions, but again, I'm afraid of where this road leads.\n. Any other thoughts on this, Mr. Simple? :smiling_imp: \n. Without having actually tested the code, :+1: from me.\n. This doesn't cherry-pick cleanly into 0.9.x. If someone has the time, please submit a new PR against the 0.9.x branch and we can review that separately.\n. @drawks You're absolutely right. I didn't mean to imply that it would be merged in immediately, only that we'd need a separate PR if it were to go into 0.9.x.\ns/doesn\\'t/wouldn\\'t/\n. I think you're right, thanks.\n. Per @brutasse this can be overridden using DEFAULT_CACHE_DURATION. Closing.\n. Indeed, I have not seen this in a very long time. Closing this for now but if anyone is able to reproduce please let us know.\n. Can anyone track down the related composer fix in 0.9.10?\n. The preferred workflow for adding new targets to a graph is to create a new chart on the dashboard with that metric, and then drag-n-drop the new chart over the old one to merge. This is not quite what you're asking for but is likely \"good enough\". If anyone wishes to submit a PR for this feature I'm sure we'd be happy to consider it.\n. The keyboard shortcuts have been deprecated from the composer as best I can tell. I no longer see a link for any composer help at all in 0.9.12, so I think this is no longer valid. If you're running 0.9.12 (or master) and can reproduce, please let us know.\n. I agree that we would happily consider a PR just like any other submitted patch, but the likelihood of someone stumbling across this issue and then deciding to work on this feature is almost certainly nil. Given our support for backend pluggable storage now, I see no real purpose in keeping this issue open for the indeterminate future.\n. @tmm1 IIRC this is the bug we investigated during my interview. We found that the rendering is tightly coupled to element id's in the DOM, at least afa the composer is concerned. Please correct me if I'm mistaken.\n/cc @sshirokov @sckevmit\n. Confirmed this bug still exists in 0.9.12.\nref #536 \n. Yes, this is a known bug that isn't likely to get fixed without a major redesign of the Composer UI.\n. Duplicate of #141.\n. I would like to close this as a wontfix unless someone can convince me of a good reason to keep this open. For any number of reasons that @mleinart lists above, scatterplots do not work well with Graphite's pipelining-function workflow.\n. I believe that client-side rendering with d3.js and the format=json render output is a suitable workaround. As interesting as this sort of chart might be we really don't have the cycles to dedicate to this sort of feature request in-tree. If someone wishes to do the work and submit a PR, we'll consider it of course. :smile: \n. asPercent now supports multiple divisor series in the total parameter, or you can pass the results of sumSeries(seriesList). Closing this one.\n. @ericy-jana Correct. Can you elaborate on a user case not covered by the asPercent() enhancements?\n. I'm going to say wontfix to this. There are much better ways to provide that sort of feedback, particularly with CSS.\n. ``` diff\n--- graphite-web-0.9.9/webapp/graphite/browser/views.py.orig    2012-06-19 10:36:41.000000000 +0100\n+++ graphite-web-0.9.9/webapp/graphite/browser/views.py 2012-06-19 10:37:02.000000000 +0100\n@@ -232,6 +232,7 @@\n     no_graphs.update(leafNode)\n     nodes.append(no_graphs)\n\nnodes.sort()\n   return json_response(nodes)\n\n``\n. Not a bug. The only difference is the change in scale. This is easily remedied by settingyMin(Graph Options > Y-Axis > Minimum) to0`.\nGraph using sortByMaxima(graphite-1.cpu-0.cpu-*.value):\n\nGraph using stacked(sortByMaxima(graphite-1.cpu-0.cpu-*.value)):\n\nSame graph after setting yMin=0:\n\n. Unable to reproduce in 0.9.12.\n. I don't think this is valid anymore, and even if it is, a) folks really shouldn't be running 0.9.9 anyways due to security issues and b) we have no control over RTD's infrastructure.\n. I believe that @mark-a's comment in https://github.com/graphite-project/graphite-web/issues/179#issuecomment-43505056 is a valid workaround. It doesn't appear to me that this is a Graphite issue per se, but one that affects the cairo module under wsgi. Closing.\n. Not enough info to really consider this a bug. To me it just looks like a misunderstanding of the way retentions work. The original LP bug was closed wontfix and I have to agree unless someone can provide more info such as the actual raw data output and storage schema definitions.\n. This has indeed been merged to master, but it has not been backported to 0.9.x yet.\n. Of course. It's on my to-do list for this weekend.\n. Confirmed, works fine in 0.9.12. Closing.\n. Could someone take a glance at #492 and give feedback?\n. It appears this was fixed in #492, closing.\n. /cc @gwaldo \n. Looks like it needs to be backported to 0.9.x.\nhttps://graphite.readthedocs.org/en/0.9.x/render_api.html#graphtypes\n. /cc @gwaldo - If you fix these separately, please close this issue.\n. https://github.com/graphite-project/graphite-project.github.io/issues/4\n. Honestly I don't know what should exist under \"Administering The Webapp\". I think that Graphite-web\u2019s local_settings.py should actually be renamed/moved to \"Configuring the Webapp\".\n. @ryan-williams This isn't doesn't address the documentation issue, but you might consider https://github.com/obfuscurity/synthesize if you just want to run Graphite locally and learn how the pieces work together. There's a Vagrantfile or you can one-click install to DigitalOcean from the project page.\n. Glad to hear that helped out. It actually is mentioned in the official docs now:\nhttp://graphite.readthedocs.org/en/latest/install-synthesize.html\nhttp://graphite.readthedocs.org/en/0.9.x/install-synthesize.html\nIf you want to run from source, check out the contents of Synthesize's install script or follow the instructions in http://graphite.readthedocs.org/en/latest/install-source.html#installing-in-the-default-location.\n. https://github.com/graphite-project/graphite-project.github.io/issues\n. > I've not found any documentation about how to start the web server on this repo, wikidot, readthedocs, or elsewhere on the internet\nYes, that's exactly why this issue was opened in the first place. :smile_cat: \n. It's meant to convey that many, if not all, of the issues you've brought up here (and in the other issue) are already listed in one or more issues in that repo.\n. I think this was resolved in #101, anyone care to verify?\n/cc @SEJeff @mleinart \n. Closing as fixed unless someone reports otherwise.\n. You should use Graphite Events for this sort of metric. Although we're still lacking official documentation for this functionality, you can read about it here.\nThere simply is no way to support metadata with the existing Whisper or Ceres backends.\n. duplicate of https://github.com/graphite-project/graphite-web/issues/242\n. Oh my goodness, does anyone still use pie charts in Graphite?\n. /cc @otac0n since he originally reported this issue. Are you still using Graphite pie charts?\n. /cc @otac0n since he originally reported this issue. Are you still using Graphite pie charts?\n. I'll buy you a free burrito if you stop using them and go with API -> D3 instead. :wink:\n. I was just kidding, no need to switch. I was just trying to clear out some of the cruft (issues) but if you're still using pie charts it's still valid. :heart:\n. @drawks could you take a look at this?\n. Thanks @drawks, :beers: are on me if you find the fix. :smiley_cat: \n. This may still be valid. /cc @brutasse @esc @drawks @SEJeff @gwaldo \n. Yes. See the comments about --install-lib.\n. I'm going to assume this is fixed now based on @brutasse's comment above. If that is not the case, please let us know and we'll reopen this. Thanks!\n. Closing as 0.9.10 is no longer supported or recommended.\n. If this does what I think it does, I'm very happy to see this fixed. Would you mind fixing the conflicts and I'll take a closer look?\n:boom: :cherries: :sparkles: \n. If it's easier, start a new clean PR and link to it from this one. Thanks!\n. This doesn't account for when the legend names are short enough to wrap to multiple columns, but that's probably a separate issue.\n/cc @mtodd\n. I've got a much better fix in the works. We currently forget to divide numberOfLines by columns which is necessary to figure out the real number of rows of labels we're rendering.\nBefore fix:\nWed Apr 03 14:15:08 2013 :: columns is 7.0\nWed Apr 03 14:15:08 2013 :: numRight is 1\nWed Apr 03 14:15:08 2013 :: numberOfLines is 12\nWed Apr 03 14:15:08 2013 :: columns is now 3.0\nWed Apr 03 14:15:08 2013 :: columns is finally 3.0\nWed Apr 03 14:15:08 2013 :: legendHeight is 216.0\nAfter fix:\nWed Apr 03 14:22:30 2013 :: columns is 7.0\nWed Apr 03 14:22:30 2013 :: numRight is 1\nWed Apr 03 14:22:30 2013 :: numberOfLines is 12\nWed Apr 03 14:22:30 2013 :: columns is now 3.0\nWed Apr 03 14:22:30 2013 :: columns is finally 3.0\nWed Apr 03 14:22:30 2013 :: legendHeight is 72.0\nVisual proof:\n\n. @mtodd I've applied this manually to our prod Graphite.\n. Gotcha, sorry for hijacking your issue. I'll take a closer look at this after lunch.\n. I agree that it's sensible for users that are used to the dashboard, but it's not a sensible default (for new users). Especially since that thing is just so darn well-hidden. :wink:\nI'm obliged to just say wontfix to this, but it seems like an easy fix to add a setting to override this behavior.\n. @nhooey I'm so sorry (jk).\n@SEJeff Adding this feature is on my to-do list.\n. Correct. index_json is a bit of a hack to get the list of metrics from any single node. However, it is currently deterministic in that you know exactly which metrics reside on any single server. Having any node or relay return metrics that do not originate from itself would be non-deterministic and almost certainly have some significant performance implications. Closing this as a wontfix.\n. I love @DrQz but, no. This looks like we're graphing on calamine lotion.\n\nI don't see any solarized-light option as mentioned in the issue description. I'm fine with going with something like plain as the default, but I am personally not a fan of the proposed new default.\n\n. That said, I think it would be nice to offer that as an alternative template. Just not the default. Perhaps as something named visibility, to denote its benefits to the user.\n. Ah, gotcha. Still think it would be better off named calamine. A :-1: from me in its current configuration.\n. :+1:\nOn Tue, Apr 02, 2013 at 07:55:26AM -0700, Jose Diaz-Gonzalez wrote:\n\nGiven the above feedback, I'll rename my new default to solarized-light, leave classic as is, and set plain as default. Does that seem fine? I'll squash the commits as well.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/pull/264#issuecomment-15779990\n\n\nJason Dixon\nhttp://obfuscurity.com/\nhttps://twitter.com/obfuscurity\n. I'm generally against intrusive changes like changing the active default. We have no idea what the impact will be to in-house dashboards when Graphite is upgraded.\n. I'm :+1: in its current state.\n. @DrQz I would prefer to avoid \"committee by design\". And the more I think about it, the more I think we should just go ahead and change the default. It's easy for anyone to override this with graphTemplates.conf. In fact, this might be the first exposure many users have to graph templates, which imho is a good thing.\nI'll open a separate PR for that change and link it here, after I get back from lunch. Anyone with access, feel free to merge this in at your convenience.\n. Merging this now. I would love to continue this discussion in a public forum, perhaps on the Graphite list or in LP Answers. But I think this is probably the best \"next step forward\" to maintaining a proper balance between usability and \"marketability\" (i.e. the first impression new users get).\n. Closing as a duplicate of #193 as @tmm1 mentioned.\n. I think this is applicable:\n\n. @linkslice It's not a bad idea at all, but unfortunately we already moved away from the text header to a background image with the new logo. If you can come up with a good way to link the logo (e.g. http://stackoverflow.com/a/25422362/2275058) I'd be happy to merge it in. Thanks!\n. I also don't get what this is for. Please explain?\n. :cherry_blossom: \n. :+1:\n. YEEESSSSSSSSS. :+1::boom::zap::icecream:\n. Man, I wonder how many graphs this will break.\n. Fuck it, master is for shit like this. We'll just need to make sure the change is announced LOUDLY. :confetti_ball: \n. If you've tested it, I'm :+1: on this.\n. As much as I appreciate a well-run wiki, I'd prefer to see this type of content curated and managed on the (soon to be) official website. Folks would be welcome to submit PRs to the public repo.\nIf you think I'm nuts, please reopen, but I'm going to close this one for now.\n. The dashboard uses a number of different UI elements than the composer. This is not a bug, at least not in the literal sense. :smirk: \n. Graphlot has been deprecated as of c886299f8c02917f07aa60c874847005ae87fd5a.\n. #323 \n. Please see #323 for the explanation as to why this is not being merged.\n. You should absolutely be able to save a dashboard via API, since that's what the dashboard already does. :smile_cat: \nI don't have a proper example anywhere, but you should be able to look at your web inspector to see the request as it happens. The actual code that fires the POST is here:\npython\nfunction sendSaveRequest(name) {\n  Ext.Ajax.request({\n    url: \"/dashboard/save/\" + name,\n    method: 'POST',\n    params: {\n      state: Ext.encode( getState() )\n    },\n    success: function (response) {\n               var result = Ext.decode(response.responseText);\n               if (result.error) {\n                 Ext.Msg.alert(\"Error\", \"There was an error saving this dashboard: \" + result.error);\n               }\n             },\n    failure: failedAjaxCall\n  });\n}\n. That seems reasonable. I'll re-open the issue, but I don't know that anyone is actively developing the Dashboard anymore.\n. To clarify, this is being addressed in #308.\n. @SEJeff It was, per the description (#286).\n. @SEJeff Why do the last 2 graphs look different?\n. @SEJeff It's definitely a bug. The blue clearly shouldn't cover the entire area for metricsReceived (at least, if the 1st graph is to be trusted). I wish I could feel confident this isn't introducing a regression, although it probably isn't.\nI want to merge this now, but it also should be cherry-picked into 0.9.x and I'm too tired to do this safely tonight. I'll try to remember to do this tomorrow or this weekend.\n. ref #297 \n. I'm probably out-of-date on security matters, but wouldn't it make more sense to just encode name by default?\n. Is there a PR for this fix?\n. @SEJeff Are you still interested in seeing this go in?\n. @SEJeff would you mind testing this?\n. Argh, I guess we're gonna have to do a 2-for-1 since I pulled off the 0.9.x branch of my fork. Both fixes have already been pulled into master.\n. I discussed this with @mleinart a while back. We feel it's a bad idea to start adjusting our formats to suit specific projects. Rather, we should continue to support open format standards (e.g. csv, json, jsonp) where they make sense.\nIt's a trivial exercise to transform Graphite's output to support projects like Rickshaw, as demonstrated in Tasseo.\n. #288 \n. @SEJeff is this relevant or can we close it out?\n. The reason why this didn't make it into 0.9.11 wasn't because we forgot (afair) but that there was pressure not to add new features on a release that was already well past due. The recent CVE put additional pressure to get a release cut, and quite simply, we didn't have the time or resources to add any more features before relase.\n. It would really be nice to get tests in for something like this. We're using tox now for all of our tests. @brutasse and @esc have put a lot of working into improving our coverage. :smile_cat: \nhttp://graphite.readthedocs.org/en/latest/development.html#running-the-tests\n. Actually, I just noticed that #646 has a more elegant approach, gonna go with that one. Thanks tho!\n. @SEJeff Any idea why we haven't switched to preferring cairocffi over pycairo? https://github.com/gescheit/graphite-web/blob/3eaba824df79eaf04cdd9f033bb063e01861efb8/webapp/graphite/render/glyph.py#L16-L19\n. @bitprophet Would you have time over the next week or so to add tests for this?\n. Understood. I'd really like to see this make it in 0.9.13 but @esc may be against its inclusion without tests.\n. :heart:\n. Nope, it's just that the profile stuff is out of my comfort zone. Hoping one of the other team members can review this.\n. @esc I'm not going to lose any sleep if this doesn't make it into 0.9.13, especially if the existing PR is not approved for merging.\n@brutasse You have the commit bit now. :sparkles:\n. If you're interested in this feature please help test https://github.com/graphite-project/graphite-web/pull/797 and offer feedback.\n. Thanks! :cookie:\n. Thanks! :sunglasses: \n. Thanks! :zap:\n. @joemiller I like this idea but it should maintain backwards compatibility with WHISPER_DIR. Would you be willing to rebase and extend this PR to support that?\n. Almost. local_settings.py.example should introduce WHISPER_DIRS to replace/deprecate WHISPER_DIR but both should be supported in the parsing code. Thanks!\n. I suspect this was fixed in https://github.com/graphite-project/graphite-web/commit/60706b8a53092f08cd8b411aa04bf51004af307e. Can you confirm?\n. Deprecated by #850.\n. At the very least we need to improve the function documentation so the current syntax and usage is understandable. A secondary goal should be to make the function more robust, including tests. For now let's focus on documentation.\n. Thanks! :cherries: \n. Looks good. Would you mind backporting this to 0.9.x as well? If not I'll try to get around to it this weekend.\n. @mleinart If this looks right to you (and it does to me) we should really try to get this backported to 0.9.11 asap. Or whatever our process is for \"oopsies\".\n. The search help was corrected in https://github.com/graphite-project/graphite-web/pull/1162 to match the actual (intended) behavior. Thanks for your submission though!\n. fixed by #455 \n. backported to 0.9.x in #874\n. @bryndin My sincere apologies for the length of delay in responding to your issue. I haven't been able to identify the specific fix, but I'm pretty sure this was already resolved some time ago. I'm unable to reproduce this with Graphite 0.9.15.\nIf you are indeed still encountering this exception please let us know, but for now I'm going to assume it's no longer relevant and close this issue.\n. I simply hadn't noticed this PR. Happy to merge this in if it works for others.\n@steve-dave Feel free to suggest/submit changes to this after the merge. Better to get something in than nothing. :smile_cat: \n. diffSeries is intended to be used with another series, not a constant. As @brutasse points out in #626, it would make more sense to just use offset. \n. @caipre Technically, that should work, as constantLine() returns a series.\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/functions.py#L2484-L2490\npython\n  name = \"constantLine(%s)\" % str(value)\n  start = int(epoch( requestContext['startTime'] ) )\n  end = int(epoch( requestContext['endTime'] ) )\n  step = (end - start) / 1.0\n  series = TimeSeries(str(value), start, end, step, [value, value])\n  series.pathExpression = name\n  return [series]\nIf that doesn't work then we should open an issue against that specific combination (not reopen this issue).\n. \n. :heart: \n. It would be lovely to get tests for this change but I didn't want to hold this up any longer.\n. I'm lukewarm on investing more code into the dashboard which is already crufty. I'm happy if this inspires others to maintain and innovate there, but based on the history of graphlot (and the long-deprecated but sorely missed javascript cli) I'm inclined to believe otherwise.\n. I'll try to take a look at the code later tonight. Initial UX thoughts: the navigation elements (green arrows) are different than the approach taken in the Composer (e.g. Move > Move Up). I think I like your approach a little better anyways, so NBD. But we probably shouldn't allow illegal choices (e.g. arrow up for the top element) unless that implies a wraparound action to the list.\n. @cbowman0 Sorry for the delay, looking into this now.\n. @cbowman0 Played around with it a bit. I think the feature would be much more useful if the new target field supported autocompletion. Without it, the feature feels not altogether fleshed out imho.\nIs this something you'd be willing to take a stab at?\n. Correct.\n. Fixed by #1341.\n. If someone wants to submit a PR with the cherry-pick, I'm happy to merge it into 0.9.x.\n. Thanks @pcn! :sparkles: \n. @brutasse What do you think about the suggestion by @mbogosian above to require pyparsing => 1.5.7 in setup.cfg?\n. @chrishenry Thanks for your contribution. I've moved this over to #1652 to resolve conflicts.\n. @davcamer Apologies for the terribly long response time on my part. I understand if you no longer have any interest in this change, but if you do would you mind rebasing it?\n. Given how similar this is to sumSeriesWithWildcards(), I'd much rather see that extended to support your specific use case. Although tbh I'm still not sure that it doesn't already.\n. @mleinart @SEJeff Looks ok to me, confirm?\n. I already looked at it but I'm not familiar with this particular block of code.\n. https://github.com/daccle/graphite-web/commits/f4681271c69271fa96a0560c5174092452fc0af0/webapp/graphite/util.py\n. @SEJeff @deniszh You guys mind reviewing this one?\n. I'm wondering if there's still value in this change considering that https://github.com/graphite-project/graphite-web/pull/1567 added support for multiple series in asPercent. Thoughts? /cc @cbowman0 \n. Pretty sure this change is no longer needed. Closing it out but please let me know if I've overlooked something and we can reopen.\n. @brutasse Were you planning to backport your baseUrl work in https://github.com/graphite-project/graphite-web/commit/aaac09cb6db3a1f3a5e7435624d952114beaca7f back to 0.9.x or should we go ahead and apply this fix?\n. I believe the correct fix is a combination of these backported from master:\nhttps://github.com/graphite-project/graphite-web/commit/b9694dbafca1bb08e68abcbb362f172ed78d9b48\nhttps://github.com/graphite-project/graphite-web/commit/da49960d67e43e33c02ce2ee0706b6e0d719aacf\nhttps://github.com/graphite-project/graphite-web/commit/9ac66ac8d89efddf467e81ffd367de1996de26dc\n. Fixed in #1003.\n. refs #188 \n. Replaced by #1633.\n. Why do we need a new config file for graphite-web instead of just adding new settings to local-settings.py and settings.py?\n. Unless there's a compelling reason for the new file, I think we should probably stick with the existing configs. Closing for now but if you can think of the original motivation please let us know.\n. I was in IRC when you asked about these docs. I thought you were alluding to a new page or section covering client API libraries. I think this makes a lot more sense than just tossing these in with \"Tools\".\n. This absolutely needs to be addressed. Adding it to the 0.9.13 milestone although it needs to be fixed in master as well.\n. https://github.com/graphite-project/graphite-project.github.io/issues/4\n. /cc @gwaldo \n. Ha, beat you to it. :smile_cat: \n. Ha, beat you to it. :smile_cat: \n. @bcoughlan They used the documentation up on http://graphite.wikidot.com/.\n. This might explain the behavior I'm seeing in #929. Is there any reason we're not fixing it at https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/views.py#L211 too?\n. @PrFalken Would you mind rebasing this against master?\n. @PrFalken would you be able to include tests as requested by @pcn?\n. LGTM, thanks! :zap::sparkles:\n. :heart: this. If someone has time to test it I'd like to get this in master and 0.9.x.\n. @nharkins Can you rebase this so we can merge it in cleanly?\n. Closing in favor of #1631.\n. Would anyone be willing to backport this to 0.9.x? If not I'll try to get around to it this weekend.\n. #549 \n. :+1::heart::beers:\n. Are you going to do a separate PR for master?\n. @justino lol wut?\nI was just asking if you could submit a separate PR for this feature, in the master branch.\n. Thanks! :zap: :boom: :cherries: \n. I like this, but I think it could be potentially confusing in that legends are not necessarily deterministic when using wildcards. /cc @jssjr for his experience here (refs #522).\n. Right. But have you tested how this will work when a wildcard target is in the list? I'm curious how that will affect ordering.\n. Excellent, thanks for testing that.\n. @SEJeff Seems like a no-brainer to me. You concur?\n. Can you rebase and try again in a new PR?\n. Do we want to take this opportunity to improve our rst docs around DATABASES, or just continue to link to the Django docs?\n. I'm :+1: on this otherwise.\n. I'm :+1: on this.\n@SEJeff Any concerns before I merge it?\n. Awesome, thanks! :beers:\n. Looks good, thanks! :sparkles: :cherries: :rocket:\n. Seems like a no-brainer. @SEJeff - looks ok to you?\n. @brutasse We'd like to get this merged in and start taking advantage of STORAGE_FINDERS. Do you want to cherry-pick your Travis bits into this branch so we can test a clean build before merging?\n. If there's anything I can help with please let me know. I'm on vacation this week so I have time to dedicate to Graphite stuff.\n. @brutasse Any progress on those tests. Feeling anxious over here. :smile_cat: \n. No worries, hope you had a wonderful holiday. :)\nJason Dixon\nSent from my iPhone\n\nOn Jan 5, 2014, at 11:40 AM, Bruno Reni\u00e9 notifications@github.com wrote:\n@obfuscurity sorry, was pretty much completely afk these past 2 weeks :) Normal schedule shall resume soon.\n\u2014\nReply to this email directly or view it on GitHub.\n. I have to agree with @brutasse that this seems unnecessary. However, if you'd like to submit a PR for format=png64 or something to that effect, I'm sure we'd consider it. Thanks!\n. :rocket: :icecream: :sunglasses: :boom: :sparkles: \n. @jcsp Can we get a version of this for master? :zap: :beers: :cherries: \n. I like this, but yeah, I'd like to see this against master instead (or in addition to).\n\n@SEJeff - does this look kosher to you?\n. @SEJeff @jcsp It's not so much that we're avoiding new outputs, as much as we (I?) want to avoid targeting specific 3rd party APIs outside of our control. We publish a predictable and stable json output that users are able to consume with little difficulty.\n. https://github.com/graphite-project/graphite-web/pull/1605\n. I :heart: this. Such a simple little primitive, but I can see where this would be useful. :zap: :sparkles: \n. Curious though, did you mean for this to be submitted against master?\n. Well, I was intending to backport the test for changed() from master, but it looks like we haven't made much progress for function test coverage at all. :frowning: \n. Agreed. @Dieterbe would you please add import sys in util.py to this PR and we'll get this merged in asap?\n. Sorry, you're right. I was looking at 0.9.12 instead of 0.9.x.\nThanks,\nJason\nOn Thu, Dec 19, 2013 at 02:10:55PM -0800, Dieter Plaetinck wrote:\n\ni'm confused. it already contains import sys ?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/pull/531#issuecomment-30971464\n\n\nJason Dixon\nhttp://obfuscurity.com/\nhttps://twitter.com/obfuscurity\n. Nope, no idea who set it to that. Perhaps it was the default. I've updated it for September 2014.\n. I'm not aware of any cython code in Graphite, so I'm going to assume we'd prefer pure python. I wouldn't worry too much about it running slow for this initial version.\n. Do you have a metric also named varnish_server.domain.varnish? If so, remove it and/or blacklist it. There's an old, known bug wherein you have a folder and metric of the same name at the same level. I looked at this with @tmm1 a couple years back; there's no reasonable fix short of reworking the entire front-end.\n. Did you read my response?\n. Perhaps I'm not explaining it adequately. There is a bug within graphite-web wherein if you have the following metrics:\ntest/foo.wsp\ntest/foo/bar.wsp\nYou will see the metric test.foo but you will not see the metric test.foo.bar nor the folder containing it (test.foo). This only affects folders where a metric exists within the same path and depth as a folder of the same name.\n. No worries. I'm closing this as a duplicate.\n. :heart: :zap: :rocket: :beers:\n. fixes #533 \n. @steveakers We're going to revert this for now. The numpy dependency is causing a bit of heartburn and rather than continuing to fight it in-tree we'd like to back it out and take another look at possibly introducing pluggable functions. :crying_cat_face: \n. Anyone up for getting this into 0.9.x as-is?\n. @nickryand Good stuff. Have you thought about adding this function to the composer menus?\n. :cool: I will wait on that commit before merging. :smiley_cat: \n. Thanks! :zap: :yellow_heart: :v: \n. :+1:\n. Good stuff. Would you mind expanding on the docs to explain that caveat?\n. LGTM. Thanks! :+1: :cake: :sparkles: \n. Pretty sure this change is no longer needed. Closing it out but please let me know if I've overlooked something and we can reopen.\n. @aww-yiss What a terrible idea. You're a terrible person. :wink: :heart:\nYou know I'm not an expert with the Dashboard code, but I'll add it to my list of things to look at. :smile_cat: \n. I'm still personally a fan of installing from source (either by git clone or a release tarball) since it requires less knowledge of the Python toolset (pip, virtualenv, etc). But that's just my $0.02.\n. @brutasse I agree, that would be nice.\n. Am I the only one with an emotional attachment to /opt/graphite?\n. I'm a big fan of not introducing behavior regressions; it's the right thing to support an installation prefix, but I think we should keep /opt/graphite as the default for at least the next major release. We're already going to have a significant amount of change for users, why burden them with a significant storage problem?\nWhat I mean by that of course, is that many users have designed their existing Graphite systems to require significant storage requirements for /opt/graphite/storage/{whisper,log}. It's very likely that this change, even if broadcast loudly, will be overlooked by a number of users, resulting in massive numbers of \"filesystem is full\" alerts and really painful cleanups after in-place upgrades.\nI can't stress this enough... install-option should remain defaulted to /opt/graphite for now. It's the right thing to do.\n. Let's try to keep support questions to IRC or the answers site on Launchpad, lest we get even more overwhelmed with issues than we already are. Thanks! :smiley_cat: \n. Please direct feature/support questions to IRC or the answers site on Launchpad. Thanks! :cherry_blossom: \n. This sounds like an issue with Django locales. Please try one of the solutions here and if that doesn't work, please let us know.\n. :heart: \n. Fixed in https://github.com/graphite-project/graphite-web/commit/a88ba855139b8c90dfffdbbefc7f3aa83bcda427.\n. No, there's no way to do this now. It seems like this would be very challenging to implement given that it's going to require some sort of placeholders being passed outside of the function that actually constructed them.\nIf you come up with any good solutions, please let us know. Sounds interesting.\n. I like this feature, I just wish it could be abstracted out a bit more to be more generic; like you guys mentioned in IRC, it starts to feel like MapReduce.\nIn its current state, I think a more apt name might be groupByMultiNode which is more descriptive and also sufficiently distinguishes it from groupByNode.\n. Also, would you mind adding a test?\n:chart: :cake: :smile_cat: \n. @justenwalker I'll defer to @SEJeff or @esc on suggested test patterns. Also, would you mind rebasing? Thanks!\n. Awesome, thanks. Waiting on test feedback from one of the other devs.\n. :heart::cake::beers:\n. \n. refs #538 \n. @brutasse Waiting for your :+1: here.\n. :+1:\n. @SEJeff Ready to merge?\n. Argh sorry, my view was out of date.\n. Any blockers to having this backported into 0.9.x?\n. Unable to reproduce in 0.9.12, which uses the same sumSeries. Can anyone check master to see if this is still reproducible?\n. I believe this needs to be backported to 0.9.x. However I'm currently troubleshooting an issue with DEFAULT_CACHE_DURATION that may be related.\n. I believe this needs to be backported to 0.9.x. However I'm currently troubleshooting an issue with DEFAULT_CACHE_DURATION that may be related.\n. @glafrance Sorry that this has been hanging around for a while without any activity. Is this still relevant?\n. Yeah, this is a bug. The call to Browser.trees.mygraphs.reload(); needs to happen in the callback function handleSaveMyGraphResponse(). Should be an easy fix.\nhttps://github.com/graphite-project/graphite-web/blob/916e6a8bef72d0a26aa68a50667dd28d858bd609/webapp/content/js/composer_widgets.js#L384\n. Happy to consider patches to the regex but it should include a test too.\n. I suspect the above patch still needs to be applied to 0.9.x for remote rendering to work. Adding to the 0.9.13 milestone.\n. Merged, thanks!\n. Would someone else like to confirm this is resolved before I close this issue?\n. I have not experienced any problems with Graphite performance or inability to render data from cache. Please search existing issues in both graphite-web and carbon projects before opening any new issues related to cache performance.\nI'm closing this issue as resolved for the original issue described. Thanks everyone!\n. /cc @gwaldo \n. @Ismael @ffMathy To clarify, was this actually in reference to the sample vhost? I assumed earlier this was a documentation issue but now I'm not so sure.\n. Looks like the sample vhost was updated back in https://github.com/graphite-project/graphite-web/pull/780 to address the newer access control syntax. \n. Yeah, I assume by your title that this was regarding the config example. That was indeed updated back in July 2014 (after your issue was opened).\n. Closing this due to inactivity from OP.\n. I can confirm that this is working fine in master.\n$ curl -k \"https://127.0.0.1:8443/render/?target=constantLine(21)&format=json\"\n[{\"target\": \"21\", \"datapoints\": [[21, 1469455685], [21, 1469498885], [21, 1469542085]]}\n. I agree with @brutasse, you really should be using offset here.\n. If I had to guess, your carbon server is in a timezone far enough behind that when those metrics arrive they're already hitting the next retention period and rolling up to null or zero. We'll need more information to diagnose this properly, e.g. your storage schemas.\n. I'm really surprised to see that we don't attempt to hit the cache before disk. I'd like to get @mleinart's feedback on this proposed change and why we started returning None in https://github.com/graphite-project/graphite-web/commit/764bdf739f0a742a8a3744928a7a207e2fd398c6 without checking the cache too.\n. @justanyone Sincere apologies for this PR languishing so long. While writing the Graphite book I became acutely aware of how much love Graphite's logging subsystem needs, so this patch resonates deeply with me. However, it looks like it's going to need a fair amount of rebasing work to get it in a clean state. I'll try to take a stab at it as my time permits but for now I'm going to add it to the post-1.0 milestone.\n. Any idea if this version removes the embedded SWFObject crap in ext-all.js? It would almost make it worth the effort to rip that shit out.\n. It appears that the SWFObject component has been decoupled in 4.0.0. According to this page you need to source the SWFObject library has to be embedded manually.\nHowever, if we're going to upgrade ExtJS, I think it would make sense to target the newest version (5.0.1 as of right now).\nEither way, there's a separate PR in #656 for upgrading to 4.2.1. Closing this issue and we can continue discussions over there.\n. Looks good at a glance but I don't have time to test/review this properly right now. If @SEJeff gives it a qualified :+1: I'm happy to merge.\n. \n. Would you mind rebasing this? Seems :+1: to go in otherwise.\n. Would you mind rebasing this? Seems :+1: to go in otherwise.\n. Deprecated by #1627.\n. @torkelo would you be so kind as to test this against 0.9.x and see if the issue is resolved for you?\n. Thanks @torkelo.\n. This is a duplicate of #455.\n. Confirmed, this was fixed by @g76r in #645.\n. Any content should suffice, although it depends on what you're trying to check. Personally I would query the render API to see if the internal carbon statistics are working.\nFor example:\n/render/?target=carbon.agents.*.metricsReceived&format=json\nHope this helps.\n. Any content should suffice, although it depends on what you're trying to check. Personally I would query the render API to see if the internal carbon statistics are working.\nFor example:\n/render/?target=carbon.agents.*.metricsReceived&format=json\nHope this helps.\n. @xkilian This doesn't sound crazy. It's something we (internally) have discussed for a long time now. But it takes time and resources to make it happen. As always, pull requests are welcome.\n. I think it's ludicrous to create an empty repo where no code exists, in the hopes that someone will be enticed to fill it.\nIf someone writes the code, we'll happily consider importing it to the project as a new repo (assuming that's what the author wants).\n. Sorry, I didn't mean to close this. Feel free to discuss.\n. @brutasse Dude, you're amazing. :)\n. This is effectively done but exists as a separate project, per @brutasse above. I see no reason to keep this issue open.\n. I looked at this a little closer and each of these \"features\" can easily be handled by existing functions imho. The change to offset() just makes it act like sumSeries() afaict. The upperBound and lowerBound functions act like our data filters removeAboveValue and removeBelowValue.\nClosing this for now but if I've overlooked something please let me know and we can reconsider.\n. Is there any good reason not to bump to Django 1.4 in 0.9.x?\n. Confirmed this behavior still exists in 0.9.x and is fixed by bumping to Django 1.4 in requirements.txt.\n. Should be fixed now.\n. This is definitely causing problems with pip in 0.9.13-pre1. We need to fix this for the upcoming release.\n. Yes, I remember that now. I guess at the time I didn't realize the extent of the breakage associated with keeping things the same for 0.9.13. AIUI there's no way to install via PyPI for this release.\n... well, not without also installing all the pip dependencies manually first.\n. The installation docs now formally instruct the user to set PYTHONPATH before installing via pip. Fixed in https://github.com/graphite-project/graphite-web/commit/1904348a6adbdf3eecb0c31442285fe759061f13.\n. Would you mind rebasing this PR? Also, I'm curious why you stopped short of converting the dashboard over so we could remove all the legacy ExtJS bits.\n. Would you mind rebasing this patch? It won't apply cleanly in its current state. Thanks!\n. I don't like this change but I'll defer to judgement by @brutasse @SEJeff @esc.\nP.S. You'll need to rebase this before it can be merged.\n. I think this needs to be rebased, and some tests as per @esc would be :beer:.\n. Yes, I think we all agreed some time ago to focus on cutting our next major release (0.10) from master. I'd like to see any glaring fixes applied to 0.9.x but we need to put that cow to pasture, so to speak.\nI'm happy to tag a pre-release candidate but I feel like we need to comb through the issues and PRs for any glaring fixes that should be included in a RC.\n. I would love to see a 0.9.x release come out soon and then have someone step up to take over the project leadership. Sadly I don't have much time to work on the project these days and I don't expect that to change anytime soon. Personally, I see projects like graphite-api and cyanite as the future of the Graphite community, but clearly a lot of folks would like to see something official come out of the official project for their short-term upgrade path.\nIs anyone willing to help massage the next release into shape?\n. I had intended to build up something similar to Sensu's website for Graphite but never got around to it. I have the graphiteapp.org domain registered if anyone wants to use that.\n. I'd like to propose that we pin to Django >= 1.5 and backport the fix for Django urls.\n. @brutasse I inferred that Django 1.5 introduced the urls breakage, based on this commit.\n. @brutasse :cool:, I'll work on getting a PR for that in 0.9.x today.\n. Argh, never mind. I see that the urls issue should be completely resolved in 0.9.x thanks to https://github.com/graphite-project/graphite-web/commit/ba3acd257530448e82e5654e4d74a57df9803106. Thanks to @brutasse for the correction and to @esc for the aforementioned commit.\n. I've gone through the graphite-web, carbon and whisper projects to identify any issues or PRs that should be included for the 0.9.13 milestone. Here are the current totals (not counting this issue):\n- graphite-web: 11 issues, 1 pull request\n- carbon: 11 issues, 7 pull requests\n- whisper: 1 issue\n. @gwaldo I thought I did but I was mistaken. Working on getting these for myself, will report back.\n. @esc made some notes at the top when he opened this issue.\n. @gwaldo Sorry, I was thinking you were referring to a setting in RTD. Yes, we're using the RTD service with the graphite-web project in GitHub.\n. @nox Bugs are bugs, whether they were reported 1 week ago or a year and a half ago. We appreciate your patience while we clear out the rest of the milestone in preparation for a new release. Thanks.\n. @nox Have you had a chance to try out the 0.9.x branch?\n. A quick progress report, of sorts. First, the updated count for issues and PRs:\n- graphite-web: 7 issues, 2 pull requests\n- carbon: 10 issues, 4 pull requests\n- whisper: 1 pull request\nAlthough it doesn't look like a large net difference over the last couple of weeks, it really is. There's been a bunch of activity in terms of backporting fixes from master that weren't previously reported as issues against 0.9.x.\nI'll continue to work these but I'd appreciate everyone's help, espcially with documentation fixes (see @gwaldo) and packaging (see #864). I haven't had any time to work on the new website since I've been so busy grinding through the aforementioned queue of issues and PRs.\nP.S. On a related note, I released an RC for Synthesize that lets you test out 0.9.x quickly, if you're looking for an easier sandbox to play in.\n. Welcome back @esc, it's been lonely without you. :wink:\n. Progress has ground to a screeching halt as I've switched to my new job recently. There's actually not too much work left to get graphite-web in shape for 0.9.13 but carbon still needs a lot of love. Is anyone willing to take point on getting carbon into shape for release?\n/cc @drawks @jssjr @SEJeff \nhttps://github.com/graphite-project/carbon/milestones/0.9.13\n. :heart:\n. @deniszh Would you mind updating the release notes with the bits from #1026?\n. @deniszh Thanks!\n. Release notes look pretty good. Might want to add a \"No known security issues\" under the Security Notes section. Also need to remove the UNDER DEVELOPMENT from the header.\nI feel stupid for asking, but where's the changelog? Isn't that included in the release notes?\n. In keeping with our previous releases, I suggest we tag a 0.9.13-pre1 once everything looks ready to go.\n. I've updated the changelogs, bumped the versions, and tagged 0.9.13-pre1 on the graphite-web, carbon, and whisper projects. Giving PyPI some time to see if it pulls in the new version automatically (@mleinart says it will).\n. Ok, I've generated new packages for PyPI and uploaded them. These are based on 0.9.13-pre1 although they're numbered 0.9.13. I'll just re-upload new builds as we tag new RC or final releases.\n- https://pypi.python.org/pypi/graphite-web\n- https://pypi.python.org/pypi/carbon\n- https://pypi.python.org/pypi/whisper\nI've never actually pushed a Graphite release before, so someone please correct me if I've done something incorrectly.\n. I have to step out for a few hours. Both my wife and daughter's birthdays are tomorrow, so I have a busy afternoon and evening today. I'll try to bump Synthesize to make it (even easier) for folks to start testing 0.9.13 but I would appreciate anyone on this thread to start jumping in and testing as soon as your schedule permits.\n. Synthesize has been bumped to support 0.9.13-pre1, including the one-click DigitalOcean install button.\n. Fixed documentation for pip installations in https://github.com/graphite-project/graphite-web/pull/1080.\n. Yes, there's not much left but those are clearly sticking points for a good release.\n. We will have a 0.9.13 official release when the issues in the https://github.com/graphite-project/graphite-web/milestones/0.9.13 milestone are resolved. I want this next release as badly as everyone else, but as @steve-dave mentioned, we all have personal commitments and there are no companies sponsoring the development time of Graphite right now.\n. That was the only way (afaict) to get an RC out there for general testing. Is there a good reason not to simply replace the existing version?\n. There was a valid reason for doing it like that, I'll just be damned if I can remember what it was. :tongue:\nThis should be a warning to everyone involved: get a new release out there ASAP before I get my hands on it again. :wink:\n. Can we get some eyes on #1209? I'd like to get this merged in.\n. Working on it.\n. Only a couple PRs remaining, waiting for feedback/testing: #1212 and #1295.\n. Sorry, failed to mention also a couple of issues and PRs for Carbon too: https://github.com/graphite-project/carbon/milestones/0.9.13\n. Very exciting, the graphite-web milestone is clear and the carbon milestone should be cleaned up very soon.\n. https://github.com/graphite-project/graphite-web/pull/1349 updates the release notes.\n. Our docs builds have been failing on RTD since Aug 13. @gwaldo is this something you can help take a look at?\n. @gwaldo Added you as a project admin in RTD.\n. Note that aside from the RTD docs build problem, https://github.com/graphite-project/whisper/pull/136 is our last showstopper bug before the next 0.9.x release.\n. Anyone have time to look at that RTD docs build failure for 0.9.x yet? That's the only blocker at this point.\n. \n. I don't think there's anything holding up the 0.9.14 release except for tagging and building pypi packages. I made some small improvements to the docs the other day to address what I saw was a gap for the installation process (specifically, setting up the db). We still intend to rollout a new website, but it's not ready yet, and I don't want it to hold up the new release any longer.\nAside from those things, anything else I'm overlooking?\n. I've bumped the setup.py versions, tagged the 0.9.14 release, built new source distributions, and uploaded the releases to PyPi for graphite-web, carbon, and whisper. There needs to be a formal writeup somewhere (or better yet, automated), but for the time being here are the steps I performed:\n- bump the version in setup.py\n- git tag the new version\n- push both to origin\n- launched a new Synthesize vagrant (this clones and checks out the 0.9.14 release)\n- inside the checkout for each, sudo python setup.py sdist\n- extract the PKG_INFO file from each tarball\n- login to https://pypi.python.org/pypi as myself (project admin)\n- for each project, submit PKG_INFO to https://pypi.python.org/pypi?%3Aaction=submit_form\n- upload source distribution tarball for each release (\"files\")\n. I would appreciate testing and feedback of the 0.9.14 pypi packages.\nP.S. Note that we have recently resumed work on the new website. With this will come the shutdown of the old wikidot site, improved docs (including \"quickstarts\"), and other goodies. But I felt it was in the project's best interest to not hold back 0.9.14 any longer. We will aim to ship the new site ASAP, but if it happened to coincide with a new release from master, I think that would be cool too. :wink:\nP.P.S. Thanks to everyone's hard work and persistence for helping make this release a reality.\n. P.P.P.S. I've also hidden the 0.9.13-pre1 packages on PyPi.\n. Moving the discussion on fixture migrations over to #938.\n. Note that I'm going to hide the pypi packages for 0.9.14 until a suitable upgrade path is identified.\n. Disregard, it appears that the syncdb process for upgrading works fine. I'll update the docs.\n. Updated the release notes for upgrading the database and including the release date (today). Re-tagged the release and generated a new source dist. Had to switch formats from tar.gz to zip for PyPI to allow the re-uploaded file (didn't realize this was a thing until now). Tested that pip install still works as expected.\n. Ladies and gentlemen, Graphite 0.9.14 is officially released.\n. Announced on Twitter.\n\n. For the sake of closing this issue out (oh dear god yes), I'm going to mark the \"website\" task complete even though it's clearly not. Trust me when I say we're working actively on this and I expect it to be out within the next 2-4 weeks.\n. I'm not a fan of over-parenthesization but let's get this merged.\n. Deprecated by #1629.\n. Actually, bookmarklets work perfectly fine in Chrome. I use that feature almost every day. :smile_cat:\nIs there some reason why a saved graph is insufficient for your needs?\n. Looks totally legit, thanks for all your hard work organizing and cleaning this up. :sparkling_heart: \n. I'm unable to reproduce your specific error, but it would appear that whatever functionality used to rely on this is no longer present. The references to this image were added to css/dashboard.css in https://github.com/graphite-project/graphite-web/commit/7468d0e647cbc3529aecc8d0f2f96c1c4ca2222b but I simply can't find what feature should be using this now. There is no (and never was afaict) mini-right2.gif in the tree at all. In fact, @cdavis commented out the similarly mysterious lines for mini-left2.gif back in https://github.com/graphite-project/graphite-web/commit/23dbdd965411858a29c99ee73ae32e4c1eb43419. We need to remove the entire blocks below.\nhttps://github.com/graphite-project/graphite-web/blob/c9687256f39a67cab7b36ef988cb1e118a30df38/webapp/content/css/dashboard.css#L89-L98\nhttps://github.com/graphite-project/graphite-web/blob/c9687256f39a67cab7b36ef988cb1e118a30df38/webapp/content/css/dashboard.css#L112-L121\n. FWIW I was finally able to repro thanks to help from dlloyd in freenode. I hadn't realized you were looking at the dashboard with the side navigation panel.\n. @jcua Thanks for your contribution. I'm moving this over to #1654 to rebase and make a small tweak.\n. Any updates on this? I'm also curious how you adapt graphite-web to handle OpenTSDB's reliance on tags.\n. Technically that wasn't a typo, it was an artifact. mostDeviant() used to pass the integer arg before the seriesList. That was fixed in https://github.com/graphite-project/graphite-web/commit/44524016474cbdf6d71afb9eb105dea07766d741 but it looks like the example line was overlooked.\n. Regardless, :+1:. :smile_cat: \n. @esc If this is already merged in #763 I see no reason to keep this open. Do you concur?\n. We recently merged a slightly more correct for this in https://github.com/graphite-project/graphite-web/commit/60706b8a53092f08cd8b411aa04bf51004af307e. Thanks tho! :heart:\n. @fguillen What is the output of whisper-info.py /path/to/my/key.wsp? It certainly feels to me like you're hitting a retention boundary because the data is rolling up to null.\n. :cool:, thanks for the update. :zap:\n. /cc @gwaldo \n. @drawks I've run various tests between groupByNode, sumSeriesWithWildcards, and averageSeriesWithWildcards. I'm unable to find any difference between the output (when using the proper respective indices).\nWithin the context of this particular issue, I don't necessarily see anything wrong with the documentation. They seem to operate as described... assuming you can wrap your head around the way they \"insert wildcards\" rather than aggregating on the chosen node.\nGenerally I dislike any change that would cause a usability regression for users (i.e. removing those functions), but it seems to me they've always caused confusion. Not only is groupByNodes' wildcard handling easier to grok, it's much more useful in that you can pass any(?) callback function. I would strongly advise against pulling these out of the stable 0.9.x branch, but I think a case could be made for pulling them out of 0.10 (with adequate warning in the release notes).\nRemoving from the 0.9.13 milestone.\n. Closing as duplicate of #604.\n. Seems legit but can you add a test? /cc @esc.\n. @brutasse @esc @drawks @SEJeff How's this look to you all?\n. @pabigot Would you be willing to backport this to 0.9.x?\n. I can reproduce this pretty easily although I see a UnicodeEncodeError, not a TypeError.\nSteps to reproduce:\n- install collectd\n- enable rrdtool plugin in collectd.conf\n- change Hostname to something with a unicode character, e.g. foo\u0394\n- render graph with target=collectd.foo\u0394.load.load.shortterm\nOutput:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 111, in get_response\n    response = callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 67, in renderView\n    requestKey = hashRequest(request)\n  File \"/opt/graphite/webapp/graphite/render/hashing.py\", line 33, in hashRequest\n    return compactHash(normalizedParams)\n  File \"/opt/graphite/webapp/graphite/render/hashing.py\", line 46, in compactHash\n    hash.update(string)\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\u0394' in position 30: ordinal not in range(128)\nThere appear to be other unicode bugs in graphite-web, as the /metrics/find view also fails with a 400 error when trying to navigate under the affected path.\nP.S. Note that my testing was performed under 0.9.x.\n. @deniszh It is absolutely reproducible per my previous comment. I agree with you that I think we should focus on fixing these unicode errors in master and just suffer with them for 0.9.13.\n. @esc This only affects master afaict. Removing from 0.9.13 milestone.\n. @esc I might have jumped the gun on my previous comment. Adding back to the 0.9.13 milestone as a precautionary measure so we'll remember to check this.\n. @g76r can you elaborate on \"carbon cache completion\" and how it's currently broken for you (with unpickler enabled)?\n. @g76r ping\n. Why was this reopened?\n. And let's hope it never gets opened again.\n. Seems like a reasonable change, but I'd like to see a better prefix name, e.g. MEMCACHE_KEY_PREFIX and also a brief mention in the configuration comments above.\nP.S. This also needs to be rebased.\n. Thanks!\n. This PR would need to be rebased before it will merge cleanly. That said...\nI'm sort of on the fence with this change. I think it begs for a discussion on some sort of search endpoint that offers different functions for introspection. I've never been a fan of /metrics/index.json because it performs poorly and it only has local knowledge (doesn't query remote nodes).\nAnyways, perhaps that's a separate discussion that needs to take place, but this particular PR feels like too narrow a use case. Would you mind elaborating on your proposed usage, wrt dashboarding?\n. Thanks for rebasing. As I mentioned before, I'm still on the fence about this change. I'd like to hear input from other members.\n/cc @esc @drawks @SEJeff @bitprophet @brutasse \n. @torkelo see #1630 \n. I love the work you've done but I'm generally :-1: on adding any new UI composer or dashboard elements into graphite-web. These components generally get orphaned and become unmaintained. There is so much good stuff going on in the Graphite ecosystem it just doesn't make sense imho to add in more UI cruft to a project that is increasingly aiming to become more lean and agile, focusing on its API service.\nThat said, I want to say this looks like a better alternative to Graphite's built-in Dashboard. If this were to get merged in and aim to deprecate the old Dashboard, while at the same time continue to be maintained and receive updates, I would be much more likely to be :+1:.\n. If you can add interactivity to the existing dashboard with an JS charting API that's OSS-friendly, I'd love to see it. I haven't looked around in a while so I don't know which ones are best suited to the job and meet our licensing requirements. For my personal projects I typically lean on D3.js or Rickshaw (nice abstraction on D3).\n. @SEJeff I agree, but if we're going to keep the Graphite Dashboard... and someone's willing to modernize it, I welcome the changes. My personal philosophy on {js-cli, graphlot, dashboard} is to make them better (and maintain them) or deprecate them.\n. This issue should have been opened under the https://github.com/graphite-project/carbon project, not graphite-web.\nThat said, yes, carbon-cache caches its own stats. Presumably Twisted is too busy handling your submitted metrics at the time to get to the instrumentation loop.\n. Thanks!\n. Graphlot has been deprecated as of c886299f8c02917f07aa60c874847005ae87fd5a.\n. Graphlot has been deprecated as of c886299f8c02917f07aa60c874847005ae87fd5a.\n. Graphlot has been deprecated as of c886299f8c02917f07aa60c874847005ae87fd5a.\n. Graphlot has been deprecated as of c886299f8c02917f07aa60c874847005ae87fd5a.\n. Would you be so kind as to rebase this?\n. Closing in favor of https://github.com/graphite-project/graphite-web/pull/1655.\n. @tbenk Are you saying this is no longer reproducible?\n. :cool: :+1: :zap:\n. Graphlot has been deprecated as of c886299f8c02917f07aa60c874847005ae87fd5a.\n. Graphlot has been deprecated as of c886299f8c02917f07aa60c874847005ae87fd5a.\n. Good catch, thanks!\n. Thanks @heliodor and @cbowman0, closing this one.\n. Without more information from @jraby I propose we remove this from the 0.9.13 milestone.\n. @esc I think we want to keep this open, it just needed to be removed from the 0.9.13 milestone. Reopening.\n. I've actually experienced this myself recently but hadn't looked into it closer.\n. Confirmed this patch fixes it for us. Working on a PR.\n. @brutasse I was afraid of that. In that case, would you prefer to do the honors as your schedule permits?\n. Have I mentioned that I hate timezone code?\n. Sure thing, I'll take a stab at it then so we at least have something to look at in-tree.\n. After a second glance I'm going to recuse myself from this patch. Timezone code is bad enough; it's worse when you're not familiar with most of Python's time modules. Happy to buy beers for anyone else willing to take a shot at it.\n. Closing this issue after opening #810 to address all of the DST issues comprehensively.\n. I'm also :+1: with the dict. Was there a separate issue identifying this regression for RemoteNode?\n. I'd like to get this in sooner rather than later. We have a separate issue for test coverage that hopefully will catch this later on (sorry @esc!). :sheep: :grin: \n. @esc @brutasse @drawks Does this look ok to you?\n. :beers: :rocket: :zap: \n. I'm unable to reproduce this in 0.9.12 in either Chrome or Firefox clients. I'm not sure why your installations even contain jquery.flot.time.js, because it doesn't exist as a dependency anywhere in the graphite-web tree. I have a fresh installation of 0.9.12 (via Synthesize) on this Vagrant and these are the only files for flot on the entire system:\n```\nvagrant@vagrant-ubuntu-trusty-64:/usr/local/src/graphite-web$ sudo updatedb\nvagrant@vagrant-ubuntu-trusty-64:/usr/local/src/graphite-web$ locate flot\n/opt/graphite/webapp/content/js/jquery.flot.crosshair.js\n/opt/graphite/webapp/content/js/jquery.flot.js\n/opt/graphite/webapp/content/js/jquery.flot.selection.js\n/usr/local/src/graphite-web/webapp/content/js/jquery.flot.crosshair.js\n/usr/local/src/graphite-web/webapp/content/js/jquery.flot.js\n/usr/local/src/graphite-web/webapp/content/js/jquery.flot.selection.js\n```\nCan one of you paste the section of code that's trying to load this file so I can investigate this further?\n. Removing this from the 0.9.13 milestone only because I don't believe this to be a problem in 0.9.x.\n. @darkstar Sorry, I don't follow at all. The flot libraries are included with graphite-web, they're not an external dependency (e.g. cairo) that must be installed in the server OS.\nhttps://github.com/graphite-project/graphite-web/tree/master/webapp/content/js\n. Yes, this appears to be a bug with the Ubuntu packaging. WTF do people feel the need to rip out files that we already have in-tree and replace them with external dependencies?!!! Idiotic.\nPlease report to your distro maintainers. This is not a bug with the official project.\n. /cc @hggh since he appears to work on the affected packaging.\n. @hggh It was my undisputed pleasure. :smile: \n. @hggh I saw those but haven't gotten around to digging in further.\n. It appears that Scriptaculous was only used by the js cli, which was removed in #796. The Ace editor is still in use by the dashboard (json editing) but I suspect this could be upgraded without too much pain.\n. Yep, I meant to mention that too. Thanks for the reminder.\n. \n. I like the idea but I agree this should be submitted against master instead of 0.9.x.\n. Our goal is to get 0.9.13 very soon and then focus on 0.10. You can probably tell by pulse that there's been a nice uptick recently, and not just due to the 0.9.13 push.\n. /cc @brutasse @mleinart \n. Can't, needs a rebase.\n. @kg4zow See https://github.com/graphite-project/graphite-web/pull/349#issuecomment-50447195.\n. Wow, that was :speedboat:.\n. Confirmed the # is optional. Thanks!\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/glyph.py#L211-L212\n. Have you checked carbon.relays..destinations..fullQueueDrops?\nJason\nOn Wed, Jul 30, 2014 at 01:45:25PM -0700, aaron2 wrote:\n\nsuggest logging a warning or adding a counter to the carbon metrics or incrementing error metric\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/issues/802\n\n\nJason Dixon\nhttp://obfuscurity.com/\nhttps://twitter.com/obfuscurity\n. @drawks Yes, it's tracking something different. I inferred something I probably shouldn't have.\n. @brutasse I'm going to merge this now but I'll also get started on replacing those with a pow() function. Good idea.\n. I prefer @brutasse's proposed fix in https://github.com/brutasse/graphite-api/commit/c8854f584b4ba216f9706fa5bf42597a4f9c969c#diff-1 as referenced in https://github.com/graphite-project/graphite-web/issues/753#issuecomment-50473887. We just need to find to port all of those fixes over at once.\n. I really appreciate your proposed fix, fwiw. I'd just prefer to kill all those birds with one big PR stone. :)\n. Yes, the intent is to backport to the 0.9.x branch as well. Are you\nsaying the state of the identity() function as linked in brutasse's\ncommit isn't fixed?\nJason\nOn Sat, Aug 02, 2014 at 07:28:03PM -0700, Dana Powers wrote:\n\nok, though you realize that the \"fix\" you are referencing isn't related to this bug.  admittedly brutasse fixed this bug back in march in a separate patch, unrelated to timezone issues: https://github.com/brutasse/graphite-api/commit/86e55af2f5f20fd01e0380865890f4abd98b2745\nare you planning to merge with his fork entirely?  even if yes, what about the 0.9.x branch which really should get this bug fix too?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/pull/806#issuecomment-50980293\n\n\nJason Dixon\nhttp://obfuscurity.com/\nhttps://twitter.com/obfuscurity\n. Going to go ahead and merge this now. Will address the whole of DST fixes in #810 but there's no good reason to ignore this for now.\n. I prefer @brutasse's proposed fix in https://github.com/brutasse/graphite-api/commit/c8854f584b4ba216f9706fa5bf42597a4f9c969c#diff-1 as referenced in https://github.com/graphite-project/graphite-web/issues/753#issuecomment-50473887. We just need to find to port all of those fixes over at once.\n. Going to go ahead and merge this now. Will address the whole of DST fixes in #810 but there's no good reason to ignore this for now.\n. Also refs #639 \n. Also refs #639 \n. @deniszh The two I linked in the original description.\n. Related, courtesy of @mleinart (with approval to post):\n\n. This may be fixed in 0.9.x via #1337. Would you be able to test with that patch and see if it fixes your issue?\n. Really glad to hear it's working for you now. Thanks for testing!\n. Yeah but the waiting can be excruciating. :wink:\n. Per the Python json docs:\nIf allow_nan is True (the default), then NaN, Infinity, and -Infinity will be encoded\nas such. This behavior is not JSON specification compliant, but is consistent with \nmost JavaScript based encoders and decoders. Otherwise, it will be a ValueError \nto encode such floats.\nSo we could adjust our use of json.dumps to override this default and make it compliant, but this seems to me like it would cause breakage with the numerous JS clients consuming the Graphite API.\nCurious what use case you're seeing where this is causing breakage for you.\n. Can you try setting allow_nan in the aforementioned location and see if that fixes it for you?\n. How would you suggest that we handle these invalid values? Encode them as null?\n. @drawks I agree with you. I think @torkelo would be reasonable enough to patch Grafana to work around this issue, but I hesitate to pass the buck.\n. Workaround? No.\nI think @drawks was on the right track before. I'd prefer a name like strictJson but otherwise agree with his suggestions. Anyone volunteering to submit this change? :tongue: \n. I don't know the answer off-hand. I hope you don't mind, but please consider posting your question over at the Launchpad Q&A or Librelist. Thanks!\nClosing this as a non-issue.\n. /cc @gwaldo \n. /cc @gwaldo \n. refs https://github.com/graphite-project/graphite-project.github.io/issues/4\n. refs https://github.com/graphite-project/graphite-project.github.io/issues/4\n. /cc @gwaldo \n. /cc @gwaldo \n. Some of my examples in https://github.com/obfuscurity/graphite-scripts/issues/4 may be useful as well.\n. This looks awesome. My only nitpick would be proper (and consistent) capitalization of SQLite.\n. Proper appears to be SQLite.\nOn Sat, Aug 09, 2014 at 05:58:59PM -0700, gwaldo wrote:\n\nthat's a fair nitpick; I'll look for \"proper\".  Also, Travis failed with a warn-treated-as-error.  I'll look at it.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/pull/817#issuecomment-51702879\n\n\nJason Dixon\nhttp://obfuscurity.com/\nhttps://twitter.com/obfuscurity\n. So much :heart: and :sparkles: for you.\n. docs build failed with:\nWarning, treated as error:\n/home/travis/build/graphite-project/graphite-web/docs/feeding-carbon.rst:73: WARNING: Mismatch: both interpreted text role prefix and reference suffix.\n. I have a gif in the queue, just waiting for this to go green. :gift:\n. \n. Any plans to backport this to 0.9.x? :smiling_imp: \n. @gwaldo Did you commit those straight to the 0.9.x branch? Best to issue PRs and let someone else merge them so we can get public tests and :+1:'s. Not a big deal for those recent doc changes but generally a good idea imho.\n. No worries, thanks for the clarification. :sparkles:\nOn Sun, Aug 10, 2014 at 03:25:42PM -0700, gwaldo wrote:\n\nI had a confuse, and that's what ended up happening. Didn't back it out when I realized my mistake because those had all been through CI.\nAgreed that PR is the better way.\nOn Sun, Aug 10, 2014 at 6:21 PM, Jason Dixon notifications@github.com\nwrote:\n\n@gwaldo Did you commit those straight to the 0.9.x branch? Best to issue PRs and let someone else merge them so we can get public tests and :+1:'s. Not a big deal for those recent doc changes but generally a good idea imho.\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/pull/821#issuecomment-51729298\n\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/pull/821#issuecomment-51729411\n\n\nJason Dixon\nhttp://obfuscurity.com/\nhttps://twitter.com/obfuscurity\n. Awesome, thanks! :sparkling_heart: :rocket:\n. Doesn't this also assume that we're pinning Django to >= 1.4?\n. Ok, I might be thinking of a different issue. Too many issues on the brain this weekend.\n. LGTM :+1: \n. Seems legit, but I have no experience with cache-query-bulk. Looking for review by @sidnei @drawks @jssjr.\n. Yes, I just wanted to make sure we were tracking it for 0.9.13.\n. :sparkling_heart: \n. Getting this in 0.9.x on Ubuntu 14.04:\nTraceback (most recent call last):\n  File \"/usr/bin/pip\", line 9, in <module>\n    load_entry_point('pip==1.5.4', 'console_scripts', 'pip')()\n  File \"/usr/lib/python2.7/dist-packages/pip/__init__.py\", line 185, in main\n    return command.main(cmd_args)\n  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 161, in main\n    text = '\\n'.join(complete_log)\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 45: ordinal not in range(128)\n. Oops, I think we just need to make sure to document the libffi external dep. :sheep: :grin: \n. FWIW this still doesn't fix the original issue for me. After removing python-cairo I get the cairo import error again.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 101, in get_response\n    request.path_info)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/urlresolvers.py\", line 300, in resolve\n    sub_match = pattern.resolve(new_path)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/urlresolvers.py\", line 300, in resolve\n    sub_match = pattern.resolve(new_path)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/urlresolvers.py\", line 209, in resolve\n    return ResolverMatch(self.callback, args, kwargs, self.name)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/urlresolvers.py\", line 216, in callback\n    self._callback = get_callable(self._callback_str)\n  File \"/usr/local/lib/python2.7/dist-packages/django/utils/functional.py\", line 27, in wrapper\n    result = func(*args)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/urlresolvers.py\", line 92, in get_callable\n    lookup_view = getattr(import_module(mod_name), func_name)\n  File \"/usr/local/lib/python2.7/dist-packages/django/utils/importlib.py\", line 35, in import_module\n    __import__(name)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 37, in <module>\n    from graphite.render.evaluator import evaluateTarget\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 47, in <module>\n    from graphite.render.functions import SeriesFunctions\n  File \"/opt/graphite/webapp/graphite/render/functions.py\", line 33, in <module>\n    from graphite.render.glyph import format_units\n  File \"/opt/graphite/webapp/graphite/render/glyph.py\", line 15, in <module>\n    import os, cairo, math, itertools, re\nImportError: No module named cairo\n. Fixed by #924.\n. Per https://github.com/graphite-project/carbon/pull/49#issuecomment-52258168, please add a mention in there for Windows users about using Synthesize + Vagrant for running the Graphite stack on Windows.\n. Dammit, that doesn't have the intended result. Now we're testing the entire suite against 2.6 and 2.7.\n. Hmm, not sure why job 467 appears to be using 2.7 only, while 466 uses both 2.6 and 2.7 correctly.\n/cc @brutasse @esc \n. Job 463 is another one using 2.7 only. Weird.\n. Gonna close this out for now. Doesn't look like anything I need to lose sleep over, but I welcome anyone to look into this further.\n. :cool:, thanks.\n. This works well for me on 0.9.x, no obvious regressions. Merging.\n. Edited description formatting.\n. This function was never in 0.9.x afaict.\n. I'm also fine with bringing this into 0.9.x if there are tests.\n. Yeah, that's what we meant by tests. :smiley_cat: \n. Awesome, thanks! :sparkling_heart: \n. @Krylon360 Just checking in again to see if there's been any progress on tests. :cake:\n. For your reading enjoyment:\nhttp://graphite.readthedocs.org/en/latest/development.html#running-the-tests\n:smiley_cat: \n. Should be easy enough to add another sorting test first. test_sorting_by_total already exists.\n. This should be fixed now via #1337.\n. :sparkling_heart: \n. :boom::zap::rocket::sparkles::beer:\n. Excellent change. Would you mind backporting this to 0.9.x?\n. Nice find. :zap:\n. I think you just want that last bit to be \"localhost:8443\" without the proto or trailing underscore. I'll let @gwaldo chime in here though.\n. No need to close this PR. You can just push up the changes from your fork and they'll automagically get pulled in.\nNote: it's good practice to work off a feature branch in your fork if you need to work on multiple issues within the same project.\n. So it's just the trailing underscore that broke the build? Man, I need to learn rst.\n. Thanks! :cake: :beer: :rocket:\n. We probably want to avoid unnecessary line wraps. Also, would you mind backporting this to 0.9.x afterwards?\n. I just meant that your editor seems to be wrapping into multiple lines. It should be more obvious from the colorList change. But it doesn't seem to have any affect on the final output, so I it's not a big deal.\n. @gwaldo Pretty sure he was asking about the wrapping. He's already backported a separate PR into 0.9.x. :smile_cat: \n. TIL you can add an alpha channel to the RGB value in Graphite. :boom:\n. Thanks @ojilles, good stuff. Friendly reminder about the 0.9.x backport. :smiley: \n. Thanks! Would you mind also backporting this to 0.9.x?\n. What's the reason for this change? Why not all of the other paths that use json_respnse_for? \n. Not sure I follow your scenario. Are you just talking about remote client access via AJAX?\n. Ok, makes sense. Is graphitus typically run on the same node as Graphite? I'm curious why it needs jsonp for remote clients for the metrics finder, when other dashboards like Grafana, do not.\n/cc @torkelo for his opinion\n. @torkelo Sorry, I should have been more explicit. I was hoping you could explain how Grafana pulls metric information from Graphite without using CORS/jsonp.\n. For some reason I was thinking you had to have CORS enabled in order for jsonp to work, but apparently that isn't the case.\n@kali-hernandez-sociomantic I'm not strongly against adding more jsonp support in master but I'm not crazy about adding it to our stable 0.9.x branch because of a 3rd party dashboard. Would you mind reviewing the way Grafana handles it (look for Graphite server config ) and see if that addresses your issue first?\n. @kali-hernandez-sociomantic If you're willing to port this to master in an additional PR, I'd be :+1: to let this slide in.\nI was on the fence about whether we should add jsonp to the rest of metrics/views.py but it's probably better to err on the side of caution and keep these changes separate for now.\n. Thanks @kali-hernandez-sociomantic! :sparkles: :boom:\n. :sparkling_heart: :hamburger: \n. I believe this is a duplicate of https://github.com/graphite-project/graphite-web/issues/54.\n. It does have an effect, just not on what you think it should. carbon.conf is only used for carbon daemons. To affect graphite-web you also need to set the value in local_settings.py or use the environment variable, as described in #54 that I linked above.\n. Moving this conversation over to https://github.com/graphite-project/carbon/issues/269.\n. Sorry, I don't quite understand your question. How does summarize(\"1h\") not do what you want? What is your retention policy?\n. That's the default behavior. I just created the graph below using the fllowing params:\n&from=-3days\n&target=summarize(nonNegativeDerivative(graphite-1.df.df-root.used), \"1h\")\n&lineMode=staircase\n\nI suspect that your data is being rolled up, but I can't tell because you didn't include your retention policy as I requested.\nP.S. You can also try smartSummarize if you'd like, but that is not exposed in the function menus.\n. I don't see what looks wrong with your graph.\n. I'm unable to reproduce that. What version of Graphite are you using?\n. That's ancient. Please upgrade all of your components to a stable and supported version (0.9.12).\nJason Dixon\nSent from my iPhone\n\nOn Aug 17, 2014, at 2:29 PM, Kenterfie notifications@github.com wrote:\n0.9.2 ubuntu package\n\u2014\nReply to this email directly or view it on GitHub.\n. I really like this feature. I'm curious why cache-query-bulk was never merged into master. Anyone know?\n. @deniszh FYI merging this PR does not hinge on having cache-query-bulk in master. I'm just curious why it's not there.\n. @SEJeff Any strong objections to merging this? I'd like to go ahead and bring this in now.\n. :beers: :sparkles: \n. refs #846 \n. I'm a big :+1: on this. Feel free to merge if you're happy with it.\n. Seems like such a bizarre function, I've never used it. That said, I think \"10 seconds is the default resolution for most stats stored in Carbon\" is an assumption based on your own environment. Our storage-schemas.conf.example uses 60s by default and Carbon's internal metrics are output at 60s intervals. Even collectd defaults to 60s.\n\nAs it stands I'm :-1: on this unless someone has a compelling reason to change the default here.\n. The function could easily be adapted to accept an optional step argument.\n. None of Graphite's components support SSL interactions. Your best bet is to run them on a trusted network or use IPSec or a VPN tunnel between hosts.\n. https://github.com/graphite-project/graphite-web/issues/798#issuecomment-50601148\n. Indeed, it looks like a regression in https://github.com/ptbrowne/graphite-web/commit/ac940d080980bd77267c56d1268654b7c29ae488. Would you mind also adding the same for movingMedian?\nIt would be nice to add a test for movingAverage but I won't block this fix for that.\n. Thanks! :sparkles: :boom: :zap:\n. I'd rather open an all-encompassing issue to track all functions needing tests. We know there's a lot of coverage missing, no need to open dozens(?) of issues imho.\nJason Dixon\nSent from my iPhone\n\nOn Aug 22, 2014, at 8:54 AM, Jeff Schroeder notifications@github.com wrote:\nPerhaps we can open issues for things like this that need tests written and crest a label named \"Needs Test\" or something?\n\u2014\nReply to this email directly or view it on GitHub.\n. @SEJeff refs #863 \n. Please also quote your code/logging output in the future. Not only is it much more readable, but it prevents against accidental cross-issue linking. Thanks!\n. Need more details. What is render query?\n. Yeah, aliasByMetric is not terribly smart. It just grabs the last node of any series.name. A quick and easy fix would be to look for any trailing commas and just remove everything starting there.\n. https://github.com/graphite-project/graphite-web/commit/8edd64895fb0932c98274de37b8a3201a80d4e35\n. I don't know that's feasible for 0.10 given all the other changes planned. Pluggable functions are definitely a desired feature but until we get 0.9.13 out and start really looking at what 0.10 is going to look like (never mind that megacarbon still hasn't been merged into Carbon), it's hard to see where pluggable functions fit into the big picture.\n. I think the only bits (at least in graphite-web) currently involved in packaging are setup.py, setup.cfg, and then a postinstall script at distro/redhat/misc/postinstall.\n. P.S. I think that @thorrsson has started to dig into the rpm stuff already, so please be sure to sync up before duplicating efforts.\n. I would also prefer to avoid omnibus packages, as well as using external cookbooks as our build specification. Anything we release should be managed in-tree imho.\n. It sounds like @thorrsson and @mckern should sync up on rpm work and @josephholsten is taking lead on debs. Anyone else that wants to pitch in, please do, but touch base with those gentlemen first.\n\nThanks everyone! :sparkling_heart: \nP.S. I would prefer that we target apache + mod_wsgi over mod_python unless someone has a good reason not to. Our own example vhost config uses mod_wsgi and our documentation covers mod_wsgi for virtualenv installs. We currently have no documentation (other than a passing mention) for mod_python; I have no real arguments against standardizing our packages on it, but we should also add an example config and documentation for mod_python if we wish to go down that route.\n. It seems to me that having a real build spec for debs would be a good thing. But to be honest, I haven't followed the development of fpm closely to know how well a job it does at porting one format to another.\n. On Sun, Aug 24, 2014 at 09:23:17AM -0700, Brian Hatfield wrote:\n\nSo, now that I've dropped a bunch of random thoughts, let me approach this a different way.\nPre-caveat: I believe strongly in OS packages wherever applicable. I don't love the proliferation of every language gets it's own package manager.\nSo here goes: What problem are we trying to solve by packaging Graphite into OS packages? As is probably obvious from my above comment, I used to invest a lot of effort into packaging Graphite. But I also invested an equal amount of effort into making sure my configuration manager understood what the package was doing so it could \"correct\" it for my deployment (eg, move /opt/graphite/storage/ to a larger partition). Each thing the package did in a roughly 'standards' way, I had to go back and tweak to make things work better for my particular deployment. Every time Graphite needed to be upgraded, I was amazed at the amount of changes that went into both packaging and configuring it vs. the previous version.\n\nIMHO we're trying to solve the problem of installing Graphite for users\nwho are inexperienced with Django and/or Python package/environment\ntooling (pip, virtualenv, etc).\n\nUltimately, I ended up in a place where I just mirror the pypi tarballs and install them using pip, then perform my configuration tasks as needed. Somehow, it ends up feeling simpler, though I agree it's a lot of work repeated community wide.\nAnyways, I think it's worth being clear what goals we are trying to achieve by creating 'the way' of installing Graphite; to make sure that we actually achieve them. Packaging won't in and of itself make deploying Graphite meaningfully easier, given that much of the challenge of installing it comes after the bits are actually installed onto the machine.\n\nI don't expect anything beyond this, i.e. we should not lean on our\npackaging to handle complex configurations (either vertical or\nhorizontal scaling).\n\nJason Dixon\nhttp://obfuscurity.com/\nhttps://twitter.com/obfuscurity\n. :fire:\n. @mckern I've never been fond of the python- prefixing. Is there a reason for it?\n. Can we reconcile between Fedora's prefix requirements and a clean upgrade path for existing RPM users?\n. @mckern Hmm, I don't know about the specs. I would have assumed that someone on this thread might know where to find them. @bmhatfield perhaps?\nI would love to bring in the official distro maintainers and get their $0.02 on this effort. I worry now that this is seen as duplication of their work, but I'd still rather have an officially maintained package hooked into our build (and tests /cc @jssjr) than force users to wait on the upstream maintenance cycles.\n. Can anyone take a look at https://github.com/graphite-project/carbon/issues/355 with me?\n. @sebito91 Awesome.\n. Closing for the reason @hggh mentioned above.\n. Thanks! :gift: \n. This was actually on my short list of things to hit today, so thanks for that. :smile_cat: \nP.S. Would you mind backporting this into a PR for 0.9.x too?\n. Disregard, backported above.\n. Sorry, I already backported this in #880. Thought you were :zzz: by now. :blush: \n. How is it possible this is a clean merge if I already merged mine in?\n. Can you pull/rebase against upstream and try resubmitting with just the comment fix?\n. Makes sense, thanks for the fix.\n. Nope, still broken. I just realized I have access to the RTD build logs. Here's the output for the failed build:\n```\nCould not import graphite.local_settings, using defaults!\n/var/build/user_builds/graphite/checkouts/0.9.x/webapp/graphite/settings.py:236: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security\n  warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\nTraceback (most recent call last):\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/local/lib/python2.7/site-packages/sphinx/cmdline.py\", line 253, in main\n    warningiserror, tags, verbosity, parallel)\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/local/lib/python2.7/site-packages/sphinx/application.py\", line 107, in init\n    confoverrides or {}, self.tags)\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/local/lib/python2.7/site-packages/sphinx/config.py\", line 229, in init\n    execfile_(filename, config)\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/local/lib/python2.7/site-packages/sphinx/util/pycompat.py\", line 105, in execfile_\n    exec code in _globals\n  File \"conf.py\", line 29, in \n    import sphinx_rtd_theme\nImportError: No module named sphinx_rtd_theme\nException occurred:\n  File \"conf.py\", line 29, in \n    import sphinx_rtd_theme\nImportError: No module named sphinx_rtd_theme\nThe full traceback has been saved in /tmp/sphinx-err-iZqRCb.log, if you want to report the issue to the developers.\nPlease also report this if it was a user error, so that a better error message can be provided next time.\nA bug report can be filed in the tracker at https://bitbucket.org/birkenfeld/sphinx/issues/. Thanks!\n```\n```\nsphinx\n\nRequirement already up-to-date: sphinx==1.2.2 in /var/build/user_builds/graphite/envs/0.9.x/lib/python2.7/site-packages\nRequirement already up-to-date: virtualenv==1.10.1 in /var/build/user_builds/graphite/envs/0.9.x/lib/python2.7/site-packages\nDownloading/unpacking setuptools==1.1\n  Running setup.py (path:/var/build/user_builds/graphite/envs/0.9.x/build/setuptools/setup.py) egg_info for package setuptools\nRequirement already up-to-date: docutils==0.11 in /var/build/user_builds/graphite/envs/0.9.x/lib/python2.7/site-packages\nDownloading/unpacking readthedocs-ext from git+git://github.com/ericholscher/readthedocs-sphinx-ext\n  Cloning git://github.com/ericholscher/readthedocs-sphinx-ext to /var/build/user_builds/graphite/envs/0.9.x/build/readthedocs-ext\n  Running setup.py (path:/var/build/user_builds/graphite/envs/0.9.x/build/readthedocs-ext/setup.py) egg_info for package readthedocs-ext\nwarning: no files found matching '*.css' under directory 'readthedocs_ext'\n\nInstalling collected packages: setuptools, readthedocs-ext\n  Found existing installation: setuptools 3.6\n    Uninstalling setuptools:\n      Successfully uninstalled setuptools\n  Running setup.py install for setuptools\nInstalling easy_install script to /home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/bin\nInstalling easy_install-2.7 script to /home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/bin\n\nRunning setup.py install for readthedocs-ext\nwarning: no files found matching '*.css' under directory 'readthedocs_ext'\n\nSuccessfully installed setuptools readthedocs-ext\nCleaning up...\n```\n. Still getting a docs build error for requirements on RTD.\n```\nRequirement already satisfied (use --upgrade to upgrade): Django==1.4 in /var/build/user_builds/graphite/envs/0.9.x/lib/python2.7/site-packages (from -r docs/requirements.txt (line 2))\nRequirement already satisfied (use --upgrade to upgrade): django-tagging==0.3.1 in /var/build/user_builds/graphite/envs/0.9.x/lib/python2.7/site-packages (from -r docs/requirements.txt (line 3))\nRequirement already satisfied (use --upgrade to upgrade): sphinx in /var/build/user_builds/graphite/envs/0.9.x/lib/python2.7/site-packages (from -r docs/requirements.txt (line 4))\nDownloading/unpacking sphinx-rtd-theme (from -r docs/requirements.txt (line 5))\nRequirement already satisfied (use --upgrade to upgrade): pytz in /var/build/user_builds/graphite/envs/0.9.x/lib/python2.7/site-packages (from -r docs/requirements.txt (line 6))\nRequirement already satisfied (use --upgrade to upgrade): whisper from git+git://github.com/graphite-project/whisper.git in /var/build/user_builds/graphite/envs/0.9.x/lib/python2.7/site-packages (from -r docs/requirements.txt (line 7))\nInstalling collected packages: sphinx-rtd-theme\nCleaning up...\nException:\nTraceback (most recent call last):\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/local/lib/python2.7/site-packages/pip/basecommand.py\", line 122, in main\n    status = self.run(options, args)\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/local/lib/python2.7/site-packages/pip/commands/install.py\", line 283, in run\n    requirement_set.install(install_options, global_options, root=options.root_path)\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/local/lib/python2.7/site-packages/pip/req.py\", line 1435, in install\n    requirement.install(install_options, global_options, args, *kwargs)\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/local/lib/python2.7/site-packages/pip/req.py\", line 671, in install\n    self.move_wheel_files(self.source_dir, root=root)\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/local/lib/python2.7/site-packages/pip/req.py\", line 901, in move_wheel_files\n    pycompile=self.pycompile,\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/local/lib/python2.7/site-packages/pip/wheel.py\", line 215, in move_wheel_files\n    clobber(source, lib_dir, True)\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/local/lib/python2.7/site-packages/pip/wheel.py\", line 177, in clobber\n    os.makedirs(dest)\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/lib/python2.7/os.py\", line 150, in makedirs\n    makedirs(head, mode)\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/lib/python2.7/os.py\", line 150, in makedirs\n    makedirs(head, mode)\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/lib/python2.7/os.py\", line 150, in makedirs\n    makedirs(head, mode)\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.x/lib/python2.7/os.py\", line 157, in makedirs\n    mkdir(name, mode)\nOSError: [Errno 13] Permission denied: '/opt/graphite'\nStoring debug log for failure in /home/docs/.pip/pip.log\n```\n. Yeah, good news is that the revert got the build working. And sure enough, it appears that this theme is now the default.\nhttp://graphite.readthedocs.org/en/0.9.x/functions.html\nI think we're probably good to close this for now.\n. Deprecated by #1613.\n. The initial version LGTM. Hopefully everyone understands this is a WIP and this has not actually been released today. :tongue: \n. http://graphite.readthedocs.org/en/latest/releases/0_9_13.html\nhttp://graphite.readthedocs.org/en/0.9.x/releases/0_9_13.html\n. Closing this out but we'll need to make sure to update the release notes as we go.\n. If you're getting HTML back instead of JSON, then you're hitting the wrong url. I'd suggest debugging the url in your browser until you're getting the proper response and then make those changes to your scripted dashboard.\n. \n. Nice. Would you mind porting it forward to master?\n. I also agree with @brutasse's comments, but on 2nd thought I think we should just get these pages in and then we can reevaluate content later. Sound ok with you both?\n. Sounds good. :zap:\n. Thanks @gwaldo!\n. Can you elaborate on this? If I save a graph in either 0.9.12 or 0.9.x it reloads with all of the parameters previously applied.\n. Closing as notabug. We can reopen if you have more details.\n. When you say \"latest Graphite\" you mean 0.9.12 or from master?\n. I'm unable to reproduce in 0.9.12 or 0.9.x, fwiw.\n. Very strange. I'm still unable to reproduce, at least on 0.9.x. See below.\nFresh page, no saved graphs at all:\n\nBasic chart with one metric:\n\nLet's give it a title:\n\nTitle applied successfully:\n\nLet's choose a different background color:\n\nNew background color applied. Now let's enabled connected-line mode:\n\nConnected line mode applied successfully.\n\nOk, let's go ahead and save our graph.\n\nGraph saved successfully.\n\nLet's refresh the page. There's our graph under \"My Graphs\".\n\nGraph loaded successfully, all settings successfully loaded.\n\n. I'll try again with a fresh Vagrant running 0.9.12 and report back shortly.\n. I'm also unable to reproduce on an older instance running 0.9.10.\n. Also unable to reproduce on 0.9.12. Clearly there's something going on with your server, but I've never seen any reports of this behavior before. Do you have any unusual patches or configuration changes going on?\nHave you looked through your webapp logs for any errors or exceptions?\n. Yeah, I'm not sure what else to suggest. :frowning: \n. Do you remember any errors when running your manage.py syncdb? Perhaps something got hosed with your fixtures.\n. Assuming you're using the default installation layout (/opt/graphite) you should be fine to just install over the old files. Graphite won't delete anything on install, so even your old configs will be safe.\nThat said, if you wish to back everything up, I'd suggest everything in your Whisper directory (default /opt/graphite/storage/whisper) and your configs (default /opt/graphite/conf/*).\n. Awesome! :sparkles: \n. /cc @torkelo \n. @mikhailov Can you elaborate?\n. @mikhailov I know what zooming is, but I don't know how you'd expect to do that \"for PNG\". Any zoom functionality you see on charts or images is going to be handled client-side with some sort of mapping features. If you'd like to take a stab at adding that support to Graphite's Composer or Dashboard I'm sure that would be well received, but I don't believe that there's any way you can just \"add that to a PNG\" and expect it to work in an embedded nature.\n. refs #893 \n. refs #893 \n. LGTM, thanks! :sparkles: \n. > I think I need to add tests there to calm...\nWouldn't hurt. :smile_cat: \n. Graphlot was not ripped out of graphite-web because of distro packaging. It was ripped out due to the reasons I outlined in the link above. I posted the question publicly and 100% of the responses were of the \"kill it with :fire:\" variety.\nUsers who wish to continue using Graphlot are welcome to stay on 0.9.12 (or 0.9.x previous to the merge). They're probably not exposing themselves any worse than they will in continuing to use jQuery 1.4.3. :tongue: \n\nThere's also possibly an argument for not removing features from 0.9.x if it has been decided that none will be added\n\nThe argument against adding features to 0.9.x is outdated. As you can see over the last month we've merged in a number of new features. Knock on wood, I'm happy with the current state of 0.9.x.\nI stand pretty firmly by my decision, but I'm just one committer among many. If there's a consensus to bring Graphlot back into 0.9.x I won't put up a :poop: :facepunch:.\n. It's case-by-case and mostly limited to new render functions. There's a comprehensive list over at the WIP release notes for 0.9.13.\n. @steve-dave That's very kind of you to say, but all of my work is but a blip in the grand scheme of Graphite contributor activity. I will say that I've noticed an increase in others' activity since my recent triaging work, which makes it totally worthwhile. :smile_cat: \n. @SEJeff You've made me so happy.\n. I agree that the copyright should be updated, but legally Chris should still be assigned copyright. If there are any other individuals or orgs that should be added I'm fine with that, but we can't just remove his name.\n. So would you both agree with the following:\n2008-2014, Chris Davis\n2011-2014, The Graphite Project\nThe first substantial non-Chris commits began in 2011.\n. IANAL, but it shouldn't remove his ongoing right to copyright afaik.\n. I'd actually like to get @haacked's opinion on this since he seems to know something about the topic and I respect his opinion.\n. I simply don't know much about copyright law except that you can't remove someone's notice. I have no idea if it's granted on an ongoing basis or if it stops when their contributions end.\n. @Haacked Right, that wasn't the question so much as do we have to continually update the years of his copyright assertion, or are they limited to his years of contribution?\ne.g. is it legit to say:\n2008-2012, Chris Davis\n2011-2014, The Graphite Project\n. :cool:, thanks @Haacked!\n. :+1:\n. @deniszh That's fine. Make sure to backport the new icon too.\n. Works great! :sparkles: :rocket: :boom: \n. Which storage backend are you using (Whisper, Ceres, etc)?\n. Yes, you'll get gaps in your data if both of your Whisper database files aren't in sync. Graphite-web accepts the first response to either server as the \"winner\" and renders the query based on that. If you wish to stay with Whisper, I'd suggest looking at https://github.com/jssjr/carbonate to keep your systems in sync.\n. I'd love for someone else to take a look at this with me. I've even tried hardcoding the cacheTimeout in our cache.set calls in render/views.py, and it makes no difference. With the cache enabled, Django+memcached always uses a 60s expire. I've tested this with Django 1.4, 1.5, and 1.6 against 0.9.x. I've increased verbosity on memcached and I can see the correct timeout value being sent with each set, so I'm at a loss here.\n. Fixed by #945.\n. I sure love seeing old code go :boom:. Thanks! :zap: :sparkles: :beers: \n. Fixed in https://github.com/graphite-project/graphite-web/pull/937.\n. \u2764\ufe0f Thanks @g76r!\n. Looks fine. :zap:\n. @deniszh Please add this to the release notes for 0.9.13 in both branches.\n. :zap:\n. Do we have any documentation around migrating the fixture for this?\n. @brutasse I'm not having any luck with that. Attempted a migration from a \"clean\" 0.9.12 installation to 0.9.14. Django 1.4 syncdb fails as such:\n```\n$ sudo PYTHONPATH=/opt/graphite/webapp django-admin.py syncdb --settings=graphite.settings\nCreating tables ...\nCreating table url_shortener_link\nThe following content types are stale and need to be deleted:\nauth | message\n\nAny objects related to these content types by a foreign key will also\nbe deleted. Are you sure you want to delete these content types?\nIf you're unsure, answer 'no'.\nType 'yes' to continue, or 'no' to cancel: yes\n\nInstalling custom SQL ...\nInstalling indexes ...\nProblem installing fixture 'initial_data.json': Traceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/commands/loaddata.py\", line 196, in handle\n    obj.save(using=using)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/serializers/base.py\", line 165, in save\n    models.Model.save_base(self.object, using=using, raw=True)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/models/base.py\", line 529, in save_base\n    rows = manager.using(using).filter(pk=pk_val)._update(values)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/models/query.py\", line 557, in _update\n    return query.get_compiler(self.db).execute_sql(None)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/models/sql/compiler.py\", line 986, in execute_sql\n    cursor = super(SQLUpdateCompiler, self).execute_sql(result_type)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/models/sql/compiler.py\", line 818, in execute_sql\n    cursor.execute(sql, params)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/backends/sqlite3/base.py\", line 337, in execute\n    return Database.Cursor.execute(self, query, params)\nIntegrityError: Could not load contenttypes.ContentType(pk=10): UNIQUE constraint failed: django_content_type.app_label, django_content_type.model\n```\nUpgrading to Django 1.7 offers the migrate command, but it doesn't fare any better:\n```\n$ sudo PYTHONPATH=/opt/graphite/webapp django-admin.py migrate --settings=graphite.settings\nSystem check identified some issues:\nWARNINGS:\n?: (1_6.W001) Some project unittests may not execute as expected.\n    HINT: Django 1.6 introduced a new default test runner. It looks like this project was generated using Django 1.5 or earlier. You should ensure your tests are all running & behaving as expected. See https://docs.djangoproject.com/en/dev/releases/1.6/#new-test-runner for more information.\nOperations to perform:\n  Synchronize unmigrated apps: url_shortener, account, dashboard, tagging, events\n  Apply all migrations: admin, contenttypes, auth, sessions\nSynchronizing apps without migrations:\n  Creating tables...\n    Creating table url_shortener_link\n  Installing custom SQL...\n  Installing indexes...\nTraceback (most recent call last):\n  File \"/usr/local/bin/django-admin.py\", line 5, in \n    management.execute_from_command_line()\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/init.py\", line 385, in execute_from_command_line\n    utility.execute()\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/init.py\", line 377, in execute\n    self.fetch_command(subcommand).run_from_argv(self.argv)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/base.py\", line 288, in run_from_argv\n    self.execute(args, options.dict)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/base.py\", line 338, in execute\n    output = self.handle(*args, options)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/commands/migrate.py\", line 128, in handle\n    created_models = self.sync_apps(connection, executor.loader.unmigrated_apps)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/commands/migrate.py\", line 297, in sync_apps\n    call_command('loaddata', 'initial_data', verbosity=self.verbosity, database=connection.alias, skip_validation=True, app_label=app_label, hide_empty=True)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/init.py\", line 115, in call_command\n    return klass.execute(args, defaults)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/base.py\", line 338, in execute\n    output = self.handle(*args, options)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/commands/loaddata.py\", line 61, in handle\n    self.loaddata(fixture_labels)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/commands/loaddata.py\", line 91, in loaddata\n    self.load_label(fixture_label)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/commands/loaddata.py\", line 148, in load_label\n    obj.save(using=self.using)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/serializers/base.py\", line 173, in save\n    models.Model.save_base(self.object, using=using, raw=True)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/models/base.py\", line 618, in save_base\n    updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/models/base.py\", line 680, in _save_table\n    forced_update)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/models/base.py\", line 724, in _do_update\n    return filtered._update(values) > 0\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/models/query.py\", line 600, in _update\n    return query.get_compiler(self.db).execute_sql(CURSOR)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/models/sql/compiler.py\", line 1004, in execute_sql\n    cursor = super(SQLUpdateCompiler, self).execute_sql(result_type)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/models/sql/compiler.py\", line 786, in execute_sql\n    cursor.execute(sql, params)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/backends/utils.py\", line 65, in execute\n    return self.cursor.execute(sql, params)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/utils.py\", line 94, in exit\n    six.reraise(dj_exc_type, dj_exc_value, traceback)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/backends/utils.py\", line 65, in execute\n    return self.cursor.execute(sql, params)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/backends/sqlite3/base.py\", line 485, in execute\n    return Database.Cursor.execute(self, query, params)\ndjango.db.utils.IntegrityError: Problem installing fixture '/opt/graphite/webapp/graphite/initial_data.json': Could not load contenttypes.ContentType(pk=10): UNIQUE constraint failed: django_content_type.app_label, django_content_type.model\n```\nAt this point even a manual \"dump-and-restore\" method would be acceptable.\n. Sorry, disregard. This appears to be an issue with the initial-data.json left in place by Synthesize.\n. Thanks!\n. On a related note, have you seen #929? This does not fix that behavior (although we needed this anyways).\n. What values does myproject.prod.*.*.timers.app.view.myproject.api.views.MyProjectQuery.POST.mean_99 report around that time? Perhaps we're not handling Nones properly in summarize() (I doubt it, just taking a stab in the dark).\n. It makes no sense that this happens irregularly. Can you take a look at the specific request that's failing and see if the request looks different than expected? Hint: use the web inspector.\n/cc @torkelo in case this is a Grafana issue somehow.\n. Yeah, this definitely fixes the behavior in #929. Thanks to @esc for the original fix and to @deniszh for digging it up for a backport. :smile_cat: \n. @esc Yeah, I believe currently we've only fixed it for data outputs (json, raw, etc). The other cache.set needs to be changed to cache.add (in both master and 0.9.x) to fix this condition for rendered graphs afaict.\n. :zap: \n. :boom: \n. While I appreciate the contribution, this feels like a solution trying to find a problem. It would be slightly more practical if instead it allowed you to specify the number of columns (e.g. numLegendColumns) rather than limiting it to a (seemingly arbitrary) single column.\n. It may be possible to do this with a combination of functions like mutiplySeries and something like groupByNode but I haven't tried it myself.\n. Sorry for the lengthy delay in getting around to this. Fortunately we've been using codecov for a while which seems to work pretty well, so I think this is no longer needed.\n. Love this idea. Do you have a screenshot of the before-and-after? Just curious.\n. :boom:\n. I think the proposal of adding metadata to the existing JSON output with the existence of a secondary parameter is fine and with merit. I would prefer to call the key something like measures or even more generically, metadata so that consumers understand this is not specifically intended for legends.\n. :+1: and agreed, this would be nice to have in 0.9.13.\n. Thanks @aroben! :zap: :heart: :beers:\n. LGTM\n. I don't understand why adding this metadata can't be as simple as calling a single parameter, e.g. verbose=1, and any formats (json, csv, raw, etc) can be extended independently to support that. Why the focus on arcane tertiary params?\n. I'd fine with seeing this go in considering our recent additions for format=dygraph and format=rickshaw but I'm going to simplify it a bit.\n. Closed in favor of #1656 to resolve merge conflicts. Thanks for your contribution!\n. The equivalent functionality was added in https://github.com/graphite-project/graphite-web/pull/1322. Thanks for the submission though!\n. You're missing the bits to expose it in the UI function menu.\n. @andrewbaxter Just curious, is the thinking here (vs using timeShift()) that with delay() you don't have to think about the step (interval units)?\n. P.S. I know it's completely wrong but your name keeps triggering this for me...\n\n. http://adventuretime.wikia.com/wiki/James_Baxter_the_Horse\n. Apologies in advance, the curl patching method is breaking on the first conflict so I'm going to have to re-commit the changes anew. There may be a better way to do this (command-line rebasing with git am) but I'm a little inexperienced with these.\n. Closing in favor of #1658.\n. This has been on my to-do list for a while. Never realized it was so simple. Thanks! :smiley_cat: \n. We've had conversations around numpy in the past but rejected the idea due to the dependencies.\n. What version are you running? It doesn't appear to be 0.9.12, 0.9.x, or\ncurrent master.\nOn Sat, Oct 11, 2014 at 10:48:38PM -0700, Brian Hatfield wrote:\n\nSun Oct 12 01:34:40 2014 :: Error requesting http://10.144.251.222:8180/render/?target=stats_counts.worker.blahblahlsfljljdf&format=pickle&local=1&noCache=1&from=1413090280&until=1413092080\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/remote_storage.py\", line 181, in wait_for_results\n    response = connection.getresponse()\nNameError: free variable 'connection' referenced before assignment in enclosing scope\nSun Oct 12 01:34:40 2014 :: Failed to complete subfetch\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/readers.py\", line 52, in fetch\n    results[i] = result.waitForResults()\n  File \"/opt/graphite/webapp/graphite/readers.py\", line 29, in waitForResults\n    return self.wait_callback()\n  File \"/opt/graphite/webapp/graphite/remote_storage.py\", line 207, in extract_my_results\n    for series in wait_for_results():\n  File \"/opt/graphite/webapp/graphite/remote_storage.py\", line 181, in wait_for_results\n    response = connection.getresponse()\nNameError: free variable 'connection' referenced before assignment in enclosing scope\nNoting NameError: free variable 'connection' referenced before assignment in enclosing scope, This appears to be the result of a very old (2012) merge - and I don't see how the code could have worked since that merge >2y ago.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/issues/987\n\n\nJason Dixon\nhttp://obfuscurity.com/\nhttps://twitter.com/obfuscurity\n. Oh sorry, I misread that exception as line 101.\n. @bmhatfield do you know if this is still valid?\n. Thanks @deniszh and @brutasse for getting these fixes back into graphite-web. So exciting!\n. I don't see any advantage to this. Rather, I think adding unnecessary options like this will only serve to confuse users. Regardless, thank you for the submission and my apologies that it took this long to review.\n. Please remove the parenthesized link. It's sufficient that it's referenced in this PR.\n. Not sure what this is supposed to be, but I'm closing it.\n. How do I put this kindly? Hmm, no.\n. It's unclear to me what you're hoping to do although I think it's centered on user authorization. There are a bunch of regressions in your diff, not the least of which are breaking the baseUrl fixes and removing copyright notices.\n. I don't think it's a bad idea, but it would be nice to include a specification and tests (and documentation) for what you're hoping to achieve with this.\n. I prefer the approach taken in #1435. It's not ready to merge yet but I think it's a more mature implementation. Closing this one.\n. @leizha Do you have any written verification of Uber using Graphite? All I was able to find was a job posting.\n. @leizha Do you have any written verification of Uber using Graphite? All I was able to find was a job posting.\n. If Graphite is working fine for your other metrics, I don't see how this is a Graphite bug. Did a Whisper file get created for the metric?\n. If Graphite is working fine for your other metrics, I don't see how this is a Graphite bug. Did a Whisper file get created for the metric?\n. Graphite doesn't know anything about \"raw\" (or even counters or gauges for that matter). All it does it store your values as floats.\nClosing this issue since it's not really a Graphite bug. But feel free to jump in #graphite on freenode IRC or ask your question over at https://answers.launchpad.net/graphite/ if you still need further assistance.\n. @deniszh I think you misunderstand the patch, at least the way I'm reading it. With this change we only request metrics from CarbonLink that match the shortest step (secondsPerPoint) / highest precision. This makes sense imho since we only cache the raw datapoints, and I suspect this will address a number of issues currently open regarding data inconsistencies.\n. I'm merging this in and will attempt to port it to 0.9.x.\n. Would you mind adjusting the order? The tools are currently listed alphabetically in each section. It should come after Cabot and before rearview.\n. No worries, thanks for that. I should have been more explicit, would you mind also putting the link in the proper alphabetical order too? It should appear between Graphios and Graphite-Tattle. :smile_cat: \n. Awesome, thank you!  :heart::zap::sparkles:\nIf you'd like this to be included in our next release (0.9.13), please consider backporting this change to the 0.9.x branch.\n. @ssgelm Can you elaborate on \"looks like this is a problem in the current git HEAD\"? Which version are you claiming is broken? As @brutasse points out, the links to the images are there, and the assets all exist in https://github.com/graphite-project/graphite-web/tree/master/webapp/content/js/ext/resources/icons/fam. Closing this out for now but if you're still having issues please let me know and we can reopen.\n. :heart:\n. I missed this merge before and only stumbled across it because of https://answers.launchpad.net/graphite/+question/260408. Besides the lack of documentation for this Vagrantfile, it doesn't actually install Carbon.\nI would like to revert this merge unless a solid case can be made for keeping it.\n. I missed this merge before and only stumbled across it because of https://answers.launchpad.net/graphite/+question/260408. Besides the lack of documentation for this Vagrantfile, it doesn't actually install Carbon.\nI would like to revert this merge unless a solid case can be made for keeping it.\n. @SEJeff thoughts?\n. Are you suggesting that integral() isn't working as expected?\n. I haven't looked at it lately but why haven't you simply tried it? And even if it doesn't, then just use transformNull() first and you should be fine.\n. Closing this because it really isn't a bug. Happy to continue the discussion however, please report back your findings.\n. @gwaldo Do you have a few mins to tackle this doc bug?\n. Number 1.\n. It's already in requirements.txt so it shouldn't be an issue unless you try to install manually.\n. @gwaldo Do you have time to finish this up anytime soon?\n. Thanks!\n. This was already fixed in https://github.com/graphite-project/graphite-web/pull/1107.\n. Still, it's good to recognize these use cases. There seems to be some confusion about the best way to incorporate memcached among Graphite clusters.\nIt appears that the \"best practice\" in multi-node setups is to use a shared memcached among workers on the same frontend graphite-web, but not at all on backend nodes.\n. I'm pretty sure this is resolved. I think any confusion stems from a lack of proper guidelines around using memcached with multiple nodes.\n. @lamont @bmhatfield You guys have any time this week for any final tests so we can mark this closed? This is effectively the last issue blocking 0.9.13.\n. Are you running incompatible versions on a backend node?\n. If I understand you correctly, this has nothing to do with those functions and is solely a byproduct of DST. When the switchover occurs, we \"lose\" an hour, so the right-side of your graph (\"until\") is one hour \"ahead\". We have a number of DST fixes in master and 0.9.x but there's at least one bug still hanging around.\n. For example, the first graph is correct when the \"from\" starts at 2am (EDT ends) but the second graph is broken if we shift it back an hour to 1am EDT.\n\n\nTo be clear, it's a bug but I'm not sure it's a fixable bug. At the DST boundaries we're always going to gain or lose an hour, and I doubt that Graphite will ever be smart enough to logically resolve that in a rendered chart. I mean, how do you reconcile in a line chart that the moment it becomes 2am it's actually 1am? \n. @bmhatfield I've invited you to the committers team. Merry Christmas. :wink::christmas_tree::gift: \nYour first official duty is to open an issue to port this functionality to master. :grin: \n. We should backport this to 0.9.x, no?\n. Sorry, wasn't necessarily asking you to do it (thx for the fix), just confirming that it needs to be backported. :)\nJason Dixon\nSent from my iPhone\n\nOn Nov 14, 2014, at 6:45 AM, Bruno Reni\u00e9 notifications@github.com wrote:\nI'm not going to do it but if somebody's up for it, sure :)\n\u2014\nReply to this email directly or view it on GitHub.\n. I prefer the bit savings in #1480 but the logic seems ok either way.\n. Merged #1480.\n. Thanks!\n. Bonus points if you submit a PR for this.\n. Looks right to me, thanks!\n. No updates or responses to @brutasse's question, closing.\n. LGTM.\n\nP.S. Hi @aroben! :smile: \n. I suspect this is related to #1341 and not a holtWintersAberration-specific bug, but there's really not enough detail here to be sure. Closing this as resolved but please let us know if this isn't the case.\n. Thanks again! :smile_cat: \n. Sorry for the delay in reviewing this one. I'd like to get this merged in but first I'm going to make sure we add a test for this and also get it ported over to the master branch.\n. JFC, not a bug.\nWe're going to kill our wikidot site soonish anyways, but we don't control the wikidot service. Take up your issue with them, please.\n. Yes, when the new site goes live.\n. Gonna mark this closed. @JarleB if you try 0.9.x and find that it doesn't address your issues, please reopen. Thanks!\n. @JarleB Awesome, thanks for the update.\n. I think the ROI for something like this is pretty low, when all you're really doing is creating a macro for from and until.\n. /cc @toni-moreno\n. @bmhatfield I would buy you so many :beers: if you would be inclined to push this along. :smile: \n. @bmhatfield any chance you'd be willing to pick this work up again? I personally consider this a \"must-have\" for a 1.0 release.\n. I would be willing to sponsor this work.\n. This feature reeks of antipattern but maybe it's just me.\n. @steve-dave Can you elaborate on the value this change adds? What's a good use case for someone running these \"remote hosts\" locally?\n. Seems to me that something like Vagrant or even Docker would be a better way to manage that use case.\n. I defer to @deniszh and @drawks here.\n. I defer to @deniszh and @drawks here.\n. I agree with @deniszh, this needs to be an option, disabled by default. Otherwise this could fail spectacularly for people in unexpected ways.\n. Just to be clear, I think it's a good option. Just not a sane default.\n. Instead of supplying a string value (GET or POST), I would change the setting to a boolean with a default state to False. This makes the conditional logic simpler as well.\n``` python\nREMOTE_STORE_HTTP_METHOD_POST = False # Use POST instead of GET for remote requests\n``\n. Nice. :v:\n. Nice. :v:\n. Has this been ported to master?\n. Has this been ported to master?\n. We've got to cut off any more commits to0.9.xASAP but if this results in a much better experience for the user then I'm tentatively ok with it.\n. After0.9.13is tagged and shipped, there should be nothing except for bugfixes going into0.9.x.\n. @SEJeff I'm an idiot when it comes to threaded Python (or any threaded code, tbh) but if it looks ok to you, I'm ok with it being merged.\n. @jraby Would you mind addingPREFETCH_REMOTE_DATAto local_settings.py with an explanation?\n. Mostly because it hasn't been tested en masse and I really despise regressions. Once it's had time to bake, I'm fine with flipping the default if we feel confident that it's not going to cause any headaches. This close to release I think I'm justified in my line of thinking but I'm open to coercion by other team members.\n. Well,0.9.13will be our last release from the0.9.x` branch, so it's now or never. :wink:\nP.S. @jraby I've been wondering, can you say where you work / this cluster is running?\n. Restarting the failed job.\n. Thanks @johanek, your kind feedback is greatly appreciated. :smile_cat: \n. @SEJeff This was the PR for master. #1076 is the one for 0.9.x.\n. Thanks!\n. Fixed in https://github.com/graphite-project/graphite-web/pull/1080.\n. /cc @brutasse @deniszh \n. The cairocffi and libffi bits should also be ported to master if this looks :+1: but I wanted to get peer review first.\n. Yeah, I guess it wouldn't hurt to change that to 0.9.13 now since we're not planning to do any more 0.9.x releases.\n. @brutasse This was actually via source installation. Steps to reproduce on Ubuntu 14.10:\n```\n$ sudo su -\napt-get update\napt-get install -y libcairo2-dev libffi-dev pkg-config python-dev python-pip fontconfig apache2 libapache2-mod-wsgi git-core memcached\ncd /usr/local/src/\ngit clone https://github.com/graphite-project/whisper.git\ngit clone https://github.com/graphite-project/carbon.git\ngit clone https://github.com/graphite-project/graphite-web.git\ncd whisper/\ngit checkout 0.9.13-pre1\npython setup.py install\ncd ../carbon/\ngit checkout 0.9.13-pre1\npip install -r requirements.txt\npython setup.py install\ncd ../graphite-web/\ngit checkout 0.9.13-pre1\npip install -r requirements.txt\npython check-dependencies.py\n```\n. Reproduced on Ubuntu 14.04.\n. @bitprophet Looks lik you're onto something. Tested with CentOS 7, unable to reproduce.\n```\nls -l /opt/graphite/lib/\ntotal 8\ndrwxr-xr-x. 3 root root 4096 Jan  5 17:21 carbon\n-rw-r--r--. 1 root root  291 Jan  5 17:21 carbon-0.9.13-py2.7.egg-info\ndrwxr-xr-x. 3 root root   20 Jan  5 17:21 twisted\npython check-dependencies.py\n[WARNING]\nUnable to import the 'ldap' module, do you have python-ldap installed for python 2.7.5?\nWithout python-ldap, you will not be able to use LDAP authentication in the graphite webapp.\nAll necessary dependencies are met.\n1 optional dependencies not met. Please consider the warning messages before proceeding.\n. Also unable to reproduce on Debian 7.7. This appears to be an Ubuntu-ism.\n. This is probably a stab in the dark, but perhaps @warsaw might have some insight into what's causing this issue.\n. Unable to reproduce in Ubuntu 13.10. Whatever is causing this appears to have surfaced in Ubuntu 14.04.\n. @warsaw Thanks for the feedback. The problem isn't that Carbon is being installed to `/opt/graphite/lib`, but rather than random(?) dependencies are being installed into `/opt/graphite/lib/python2.7/site-packages/` and the rest are going to the standard system location. See my original comment at the top.\n.\nvagrant@packer-debian-7:~$ pip --version\npip 1.1 from /usr/lib/python2.7/dist-packages (python 2.7)\n``\n. Yeah, it looks like every distro with a pip that includes Wheels also works well with--no-use-wheel` for our purposes. Others return a usage error.\nWould it be reasonable to check for the specific version of pip that started using Wheels and base our logic around that?\n. Yeah, it looks like every distro with a pip that includes Wheels also works well with --no-use-wheel for our purposes. Others return a usage error.\nWould it be reasonable to check for the specific version of pip that started using Wheels and base our logic around that?\n. @SEJeff Can you give an example how this would work?\n. Thanks @arthru. Would you mind also backporting this to the 0.9.x branch?\n. I don't think there's any need to backport the README changes since most users are only going to read it when they visit the master branch on GitHub, but since you've already done the work... :)\n. On second thought, I think this backport is unnecessary. Closing.\n. That sounds right, and I don't think it would make sense to include the absolute path since we're aggregating anyways. Ok to close?\n. Yes, readthedocs regenerates automatically based on the content in this repo. I don't think we need specific instructions in the docs for improving the docs.\n. Would you mind also backporting this to 0.9.x?\n. Restarted the failed job.\n. Not at all. But in this case it makes sense to have it in both master and 0.9.x.\n. I don't know why that TOXENV=py27-django16-pyparsing1 keeps failing due to the image but I'm probably going to merge it anyways.\n. Oops, I mean TOXENV=docs. Hmm.\n. Oops, I mean TOXENV=docs. Hmm.\n. On second thought, we need to fix whatever's breaking the docs build or it will never pass again.\n@ryan-williams Check out the failed build at https://travis-ci.org/graphite-project/graphite-web/jobs/46499107 and see if you can figure out how to fix the local image issue. @gwaldo might be able to offer assistance here.\n. On second thought, we need to fix whatever's breaking the docs build or it will never pass again.\n@ryan-williams Check out the failed build at https://travis-ci.org/graphite-project/graphite-web/jobs/46499107 and see if you can figure out how to fix the local image issue. @gwaldo might be able to offer assistance here.\n. Ok, it looks like the proper fix is to refer to the local image. Fortunately we already have it at webapp/content/img/overview.png so you'll just need to point to that instead. Please change this in #1092 as well.\n. Ok, it looks like the proper fix is to refer to the local image. Fortunately we already have it at webapp/content/img/overview.png so you'll just need to point to that instead. Please change this in #1092 as well.\n. @ryan-williams The following patch should fix it for you. I can't test it locally because there's something broken about libcairo on the Mac.\n```\ndiff --git a/docs/faq.rst b/docs/faq.rst\nindex 5150a35..1777748 100644\n--- a/docs/faq.rst\n+++ b/docs/faq.rst\n@@ -75,7 +75,7 @@ Is there a diagram of Graphite's architecture?\n ----------------------------------------------\n There sure is! Here it is:\n-.. image:: https://raw.githubusercontent.com/graphite-project/graphite-web/master/webapp/content/img/overview.png\n+.. image:: ../webapp/content/img/overview.png\n.. _Django: http://www.djangoproject.com/\n```\n. P.S. Please update #1092 with the same change if this one passes tests.\n. Cool, thanks.\n. Looks good. http://graphite.readthedocs.org/en/latest/faq.html\n. Blocking on a build issue in #1091.\n. Blocking on a build issue in #1091.\n. We don't have a docs-specific build here, I forget the reason.\n. From the latest build:\nCould not import graphite.local_settings, using defaults!\n/var/build/user_builds/graphite/checkouts/0.9.13-pre1/webapp/graphite/settings.py:244: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security\n  warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\n/var/build/user_builds/graphite/checkouts/0.9.13-pre1/docs/functions.rst:17: WARNING: autodoc: failed to import module u'graphite.render.functions'; the following exception was raised:\nTraceback (most recent call last):\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/0.9.13-pre1/local/lib/python2.7/site-packages/sphinx/ext/autodoc.py\", line 335, in import_object\n    __import__(self.modname)\n  File \"/var/build/user_builds/graphite/checkouts/0.9.13-pre1/webapp/graphite/render/functions.py\", line 24, in <module>\n    from graphite.render.datalib import TimeSeries\n  File \"/var/build/user_builds/graphite/checkouts/0.9.13-pre1/webapp/graphite/render/datalib.py\", line 22, in <module>\n    from graphite.storage import STORE, LOCAL_STORE\n  File \"/var/build/user_builds/graphite/checkouts/0.9.13-pre1/webapp/graphite/storage.py\", line 4, in <module>\n    import whisper\nImportError: No module named whisper\n/var/build/user_builds/graphite/checkouts/0.9.13-pre1/docs/releases/0_9_11.rst:2: WARNING: Explicit markup ends without a blank line; unexpected unindent.\n/var/build/user_builds/graphite/checkouts/0.9.13-pre1/docs/releases/0_9_11.rst:115: WARNING: Title underline too short.\n. So it appears that the .. automodule:: graphite.render.functions in functions.rst is failing because the build has no permissions for /opt/graphite so it can't install the Whisper dependency for functions.py.\nI'm a bit out of my element here with the RTD build environment. Anyone have clues?\n/cc @mleinart @brutasse @drawks \n. P.S. 0.9.12 is the last 0.9.x tag with a working functions list. I hesitate to rebuild that one because it's probably going to fail for the same reason.\n. Ok, I think I fixed this over at RTD. We use virtualenv for building docs, but 0.9.x still has the hardcoded prefix problem. So I enabled the following setting and builds are passing now.\n\nI don't know if this is an actual fix or just a workaround but it WFM for now.\n. related: #236 \n. Only your frontend render host(s) should use memcached. You should not use it with any of your backend graphite-web instances. If you're running multiple frontend webs, use a shared memcached across all of them.\n. @deniszh @bmhatfield You should expect inconsistencies when using memcached on backend nodes (in a multi-node cluster). This is not surprising. We should absolutely have documentation explaining how to use memcached properly but this report does not signify any sort of regression in my mind. \n. Actually, Synthesize still installs Graphite 0.9.x + Django 1.4 (per requirements.txt) which doesn't use the collectstatic command.\n. Reformatted your original question so it's more legible.\n. It would be helpful if you could narrow down the scope of your question to the bare minimum, e.g. \"this basic Graphite query isn't working as expected\". I've re-read your description numerous times and I can't wrap my head around the real issue because you have some Grafana-isms mixed in; I don't use Grafana so it's causing additional confusion.\nI'd like to help but your queries are really awkward for me to parse. Maybe I just need more sleep. :)\nI did notice that some of your examples use 5, 6 as index arguments to multiplySeriesWithWildcards(), but you only have six nodes (zero-indexed).\n/cc @torkelo in the hopes that he might be able to translate some of the Grafana stuff.\n. Sure, although we'll need to really be careful about regressions here since we're close to release.\n. This is a really unusual edge case. It smells of antipattern because someone wasn't collecting their metrics properly.\n. Ok, I think I grok the use case a little better now. There are still a few issues we'll need to address before we can merge it.\n- [x] documentation needs an example\n- [x] needs to be wired into the UI\n- [x] needs test(s) please :smile: \n. Do you know how to squash commits via rebase? It would be nice to clean some of that up.\n. I'm not sure the \"Useful when...\" explanation will make sense to readers that aren't following this PR. Perhaps something like this instead?\nUseful for filtering part of a series among a wider range of data.\n. Cool, thanks for hammering this into shape. :smile_cat: \n. We switched to preferring cairocffi over pycairo a while back. Fortunately it appears that the latter has always supported unicode for text_path() so I think it's safe to merge this in without any dependency changes (e.g. requirements.txt).\n. Why are you setting maxValue to 0?\n. You don't need to set it at all, it's optional.\n. It's fine, I was just wondering why it was set. It's not obvious to me what the use case is for maxValue anyways. The code in question is a little unusual:\npython\nelif maxValue is not None and maxValue >= val:\n  newValues.append( (maxValue - prev) + val  + 1 )\nSay we have a series of [940, 980, 1000, 900, -500] with a maxValue=0. When we get to the next-to-last value, prev=1000 and val=900, resulting in the following:\n``` python\n\n\n\n(0 - 1000) + 900 +1\n-99\n```\n\n\n\nAnd then we get to the final value:\n``` python\n\n\n\n(0 - 900) + -500 + 1\n-1399\n```\n\n\n\nIf maxValue is not set, we just get a None as intended. I'm not sure what the use case is for that expression, but it seems like we need to check the value is >= 0 or return None.\nFor the time being I'd strongly recommend not setting maxValue at all. /cc @torkelo \n/cc @SEJeff @mleinart for a logic check here.\n. So I'm an idiot when it comes to electricity. But it sounds like you have a counter value for wattage. If you want to view the current rate, just use the derivative() or nonNegativeDerivative() functions around that metric. If that doesn't get you what you're expecting to see, please elaborate on your results for each attempt.\n. This is awesome. I noticed this as well when writing the current chapter of my book. Would you mind backporting this to the 0.9.x branch as well?\n. Thanks! :ice_cream: \n. Ha, nice.\n. :+1: \n. I resisted this sort of opinionated formatting a while back but I'm softening to the idea if it helps make everyone's lives easier.\n. @chicagobuss We would also need documentation patches.\n. Yes, I've made it no secret that I'd like to see pluggable functions as a first-tier citizen within graphite-web. This would also theoretically make it easier to expose Graphite's capabilities through the API for consumers like Grafana.\n. @toni-moreno Please don't hijack this PR with an existing conversation taking place in #980.\n@gingerlime You read a couple comments on HN that the project is dead? Wow, I guess we better close up shop. :wink:\nFrankly, I'm happy to continue helping people be productive with storage and rendering features that actually work. It may not be the glitziest project, and the release cycles may run longer than we'd like, but people actually use it for production loads at significant scale, every day. It has a clustering design that works quite well and isn't subject to the sort of data loss you read about routinely in \"that other project\".\n. @chicagobuss We're in favor of this change as long as there's documentation and tests.\n. Moving this change over to #1605.\n. That function was only added to the master branch. Many of the 0.9.* releases have been cut from the 0.9.x branch. After the upcoming 0.9.13 release we'll be focusing on cutting new releases from master once again.\nIf you really need that function you can patch it manually; it was submitted in https://github.com/graphite-project/graphite-web/pull/365.\n. Are you certain this is a regression with 0.9.13? Did you run a previous version of Graphite in these same operating environments?\n. Also, can you report the output of the following?\n```\n$ python\n\n\n\nimport sys\nsys.float_info.epsilon\n``\n. I feel confident this has nothing to do with Graphite directly but I'm not sure which of the dependencies might be causing this discrepancy. I'm unable to reproduce this on any of my systems. Can you upload the wsp file somewhere I can download it?\n. I don't think this scale value is a bug per se but a product of the precision, possibly compounded by the fact we're looking atNonevalues preceding it. I wonder if someone like @MathYourLife might be able to explain this.\n. @brutasse Agreed, I'm just surprised it hasn't surfaced (or at least, _been reported_) before.\n. To clarify, my surprise is not that it's reporting in notation at times, but rather that it _only_ does it when the range includesNonevalues. If you change the range (and adjust the yMin and yStep accordingly), zero reports as 0.\n. Thanks. Would you like to cherry-pick this into the0.9.xbranch?\n. You're hitting a [limitation within SQLite](http://stackoverflow.com/questions/7106016/too-many-sql-variables-error-in-django-witih-sqlite3), not Graphite. My suggestion would be to move to a real RDBMS.\n- http://obfuscurity.com/2013/12/Why-You-Shouldnt-use-SQLite-with-Graphite\n- http://obfuscurity.com/2013/12/Migrating-Graphite-from-SQLite-to-PostgreSQL\n. Yep, thanks for this one. Sorry for the delay! :cake: \n. Thanks!\n. I don't have an opinion on this, deferring to @drawks and @SEJeff.\n. I don't have an opinion on this, deferring to @drawks and @SEJeff.\n. I have to agree with @deniszh, there's no reason to insert an arbitrary delay here.\n. Fixed in https://github.com/graphite-project/graphite-web/pull/1122.\n. Would be nice to link to the reporting issue and/or commit explicitly.\n.scaleToSeconds()doesn't work for you?\n. Also, don't take this the wrong way :heart:, butgraphite-dashboardis a really confusing name for a cli tool used to manage the native Graphite dashboard, don't you think? \n. Sure, but graphite-dashboard is almost certainly going to confuse users that don't realize that Graphite's built-in dashboard is already distributed with graphite-web.\n. Couldn't you call it something like graphite-dash-cli instead?\n. @blacked Thanks! :sparkles: :beer: \n. @MFAnderson Do you think you'd be up to porting a similar fix to0.9.x?\n. @MFAnderson Sorry for the slow response on this, would you mind rebasing this one?\n. Awesome, thanks.\n. This looks like a duplicate of #1139, which was resolved via #1122. If I'm incorrect please let me know and we can reopen.\n. Thank you for your help with this and especially for the tests. :zap:\n. Thanks! :smile_cat: \n. Restarting errored build (thanks Obama).\n. fixes #1340 \n. WFM, thanks for the contribution! \ud83c\udf34 \ud83c\udf66 \u2728 \n. Yoinks, good catch.\n. @steve-dave Yeah, :+1: on your proposed change. That seems like the \"least ugly\" of all choices. But we need some very solid test coverage for this merged function to cover both use cases.\n. @robertclancy We'd need to first rebase it, then resolve the issues reported by @gboily and put it through some actually test use.\n. @kokje Seems like a nice addition. At the very least would you mind adding it to the [Graph Parameters](https://github.com/graphite-project/graphite-web/blob/master/docs/render_api.rst#graph-parameters) section of the documentation. Bonus points for a new test.\n. Replaced by https://github.com/graphite-project/graphite-web/pull/1612.\n. This doesn't sound like a RHEL/EPEL bug to me, but a side-effect of upstream policy changes by Django. I'm not familiar with the EPEL package; do they rely on the developmentrunserveror does the same thing happen when run via Apache/Nginx/etc?  /cc @brutasse  \n. Looks good, thanks!\n.0.9.13` hasn't been tagged nor released.\n. That's actually a pre-release. Sorry for the confusion.\n. Thanks!\n. @brutasse @SEJeff any idea why this is failing the lint test? I grok the error:\n\n\n\n/home/travis/build/graphite-project/graphite-web/webapp/graphite/render/attime.py:18:1: F401 'timezone' imported but unused\n... but that seems incorrect. I see where we're calling timezone in two different places. We can add # noqa to the import but then what's the point of testing flake8?\n. Gonna go ahead and noqa that for now. If it can be fixed \"correctly\" someone let me know.\n. Oh :poop:, you're right. I was looking at @kokje's master which was almost certainly behind. I'll fix that right now. thanks.\n. I'm pleased that this fixes it for you but I think there are some larger issues wrt consistency. I'm looking into this and will let you know what I find out.\n. Would you mind reverting your change and trying out the patch in #1337 to see if that fixes it for you?\n. There have been a number of recent fixes, specifically to the merging of CarbonLink and Whisper data, that should address this issue. Closing this out but please let us know if you're still experiencing with the issue with the in-tree fixes.\n. Thanks!\n. Should fix #1173.\n. Thanks @brutasse, LGTM. :+1: \n. Those timestamps are 60s apart. Your schema doesn't specify any retentions with that precision. That wouldn't explain why your values are changing, but it feels like you're leaving out some important details (or you don't know your own system).\nThe likeliest answer is that something is overwriting your data with zeroes. I can't tell anything else based on the information you've provided.\n. Closing. The most likely conclusion (since no one else has reported this) is that the data was being overwritten. No additional information has been provided.. https://github.com/brutasse/graphite-api is a separate project.\n. @jssjr @tmm1 @f3ew Are you guys using this endpoint with any of your stuff?\n. I'm a big :+1: in favor of removing cruft. Thanks for this!\n. @slackhappy Folks should be using https://github.com/graphite-project/graphite-web/blob/master/bin/build-index.sh via cron for that.\n. I would love to see 0.9.13 out ASAP but I've personally been overwhelmed with other work and projects. Hoping someone else will step up and help bring us to the finish line.\nRelated: https://github.com/graphite-project/graphite-web/issues/677\n. @deniszh you're welcome :wink:\n. @deniszh If you're trying to upgrade to Django 1.8, it's no longer Synthesize. Synthesize uses requirements.txt, which is frozen to 1.4.\n. I've dug into django-tagging to better understand what's going on here. Sadly, it looks like both with_any() and with_all() (what graphite-web uses) behave similarly, returning only unions. What we really need is something that returns an intersection, e.g. get_intersection_by_model(). We may have to monkey patch django-tagging to make this work intuitively.\n. From the freshly updated events doc:\n'The set parameter accepts an optional union or intersection argument to determine the behavior for filtering sets of tags (i.e. inclusive or exclusive). By default, Graphite uses a \u201clazy union\u201d that will return any matching events for a given tag in a list of tags. This behavior is not intuitive and will therefore be deprecated in a future release.'\n. @deniszh We seem to be dealing with two different problems. In #1279 we avoid negatively caching empty responses. Here in #1212 we're trying to avoid conflicting responses among memcached peers for the same metric.\n. :boom:\n. Why should this only apply to JSON? Also, the change to parseOptions() only affects line graphs, not JSON afaict. Don't you want to change https://github.com/Squarespace/graphite-web/blob/cache-policy/webapp/graphite/render/views.py#L239 as well?\n. @klynch Sorry for letting this slide so long. After further review, I see that I originally misinterpreted the way this worked. I like that you're doing this on the request parsing. Thanks for this. :icecream: \n. @klynch Sorry for letting this slide so long. After further review, I see that I originally misinterpreted the way this worked. I like that you're doing this on the request parsing. Thanks for this. :icecream: \n. You shouldn't need to authenticate to view events, only to manage them in the /admin/ section. And your POST should work as expected, not return HTML. It sounds like maybe your web configuration (Apache?) is messed up.\nEither way, it would help if you provided more information about your system (distro, release), which version of Graphite you're running, and how you installed it. Oh, and your actual web config.\nTo contrast, these activities work as expected for me with a fresh Synthesize install. You can see the configurations here:\nhttps://github.com/obfuscurity/synthesize/blob/master/templates/apache/graphite.conf\nhttps://github.com/obfuscurity/synthesize/blob/master/templates/graphite/webapp/local_settings.py\n. Based on this it looks like you're running Django > 1.3. The method raw_post_data was changed to body in Django 1.4. You can upgrade your Graphite to 0.9.13rc1, although I suspect it'll be easier for you to just downgrade Django.\n. The version of django-tagging pinned in 0.9.x (0.3.1) works fine with the version of Django pinned (1.4). If you need to run a newer version of Django then you may need to update django-tagging, but it's not something that's currently mismatched as found in the source tree.\n. :+1:\n. Not to mention that we're not planning to cut any non-maintenance releases from 0.9.x moving forward.\n. @g76r I don't know the SVG code very well; it was written by @tmm1 but he's not really active anymore. Regardless, it seems like you seem to know what you're doing so... :boom::wink:\n. @g76r I don't know the SVG code very well; it was written by @tmm1 but he's not really active anymore. Regardless, it seems like you seem to know what you're doing so... :boom::wink:\n. Should be fixed by #1224.\n. Should be fixed by #1224.\n. Thank you! :gift: \n. Thank you! :gift: \n. This is a known issue but unfortunately probably won't get fixed anytime soon without a complete rewrite of the way the nodes are handled in the view. Closing as a duplicate of https://github.com/graphite-project/graphite-web/issues/193.\n. There could be any number of things causing this, including a misunderstanding of the storage schema. Please include more details if you're still experiencing an issue, including sample graph output, the query you're running, and the retention policy for the affected series.\nClosing this out for now but will reopen if you're still having this issue.\n. Agreed, this appears to fix the issue. I suspect the reason for this hack was fixed upstream and is no longer necessary (part of it, anyways).\n. I proposed an answer to your(?) similar question over on Launchpad.  https://answers.launchpad.net/graphite/+question/267173\n. @gsaray101 If you can provide me with a link to a Vagrant image or something similar for your OS version, I can take a look at this for you tonight.\n. Looks good, thanks! :sparkles: \n. @theromis We'd be happy to consider the revised change as suggested by @brutasse and @SEJeff. Until then we're going to go ahead and close this one out. Thanks!\n. This report is old enough that it's probably not relevant anymore but I wanted to at least comment here before closing it out. Although we'd love to support \"version foo\" of Django, django-tagging, etc., the reality is that this continues to get harder with each release (especially with Django). Your older version of 0.9.10 was pinned to Django 1.3. Graphite 0.9.15 (stable) is pinned to Django 1.4. Bleeding-edge Graphite master is pinned to 1.9. Each of these versions has their own incompatibilities which makes an already challenging situation with Python software dependencies that much more painful.\nI know this doesn't help you much and I imagine you already fixed your installation months ago by downgrading your django-tagging, but I figured I'd take the opportunity to vent a bit about Django. :wink: \n. Thanks!\n. Seems like an idea worth of inclusion to 0.9.x, but would you mind adding it to local_settings.py.example with some comments or documentation?\n. Good stuff, thanks! :cake: \n. @llicour I have the same response here as in https://github.com/graphite-project/carbon/pull/419; this looks like a neat addition but it belongs in master. I'm going to close these PRs down but I'd really like to see them submitted again against the master branch.\n. @deniszh Please try to cherry-pick them from @llicour's branch, if possible.\n. Merged #1034 which should fix this in 0.9.x.\n. @redice I believe this has been fixed in 0.9.x, at least it looks that way to me.\n```\n\n\n\ndatetime.datetime.utcfromtimestamp(time.time())\ndatetime.datetime(2015, 9, 27, 2, 21, 28, 594272)\ndatetime.datetime.utcfromtimestamp(1431607046)\ndatetime.datetime(2015, 5, 14, 12, 37, 26)\n```\n\n\n\nIf you have reason to believe otherwise please let me know and we can reopen this one.\n. @cbowman0 I've been meaning to organize all of the various PRs and issues related to brace expansion. Here's what I have so far:\n- https://github.com/graphite-project/graphite-web/pull/1290 (this one)\n- https://github.com/graphite-project/graphite-web/pull/483 (presumably deprecated by #1290)\n- https://github.com/graphite-project/graphite-web/pull/1531 (against 0.9.x but a nice universal approach)\n- https://github.com/graphite-project/graphite-web/issues/1457 (brace sorting issue)\n- https://github.com/graphite-project/graphite-web/issues/256 (ceres issue addressed by this patch)\n- https://github.com/graphite-project/graphite-web/issues/1218 (another ceres issue mentioned above)\n- https://github.com/graphite-project/graphite-web/pull/550 (closed by me because it was ceres-specific)\n- https://github.com/graphite-project/graphite-web/issues/472 (another \"no data\" report)\n- https://github.com/graphite-project/graphite-web/issues/1289 (another \"no data\" report)\nI was hoping that brace expansion could be addressed in a more universal manner but I think the introduction of storage finders has made that impractical.\n. I think we need to move forward with brace expansion for the standard and ceres finders but I haven't had time to sort through the existing PRs/issues to see which make sense. If you're happy with this Ceres change, feel free to merge it. Would you mind also following up to get the same added for the standard finder?\n. \ud83d\udc4d \n. Would you mind posting a gif of the new feature? We've started doing this at work for any UI changes and it works really well. :smile_cat: \n. Thanks. Useful feature and I'll go ahead and merge it in. Any thoughts to naming it \"From Target\" instead of \"From Metric\"?\n. Has this / can this be submitted against master too?\n. @jjneely I hate to be a pest, but would you be willing to add a test for this? I'd like to get this into 0.9.x and adding a new test would be enough justification for me to pull the trigger. :wink:\n. @deniszh Would you be willing and/or able to port this to master?\n. You all realize the operational implications here, right? And I mean that in a good way. :smile: \nBefore merging this, can anyone think of any situations where this breaks a \"relied on\" behavior? I can't think of any but I just woke up from a nap so my neurons aren't firing at :100: yet.\n. I think this is a significant enough change to the existing (admittedly sucky but still \"as intended\") design that it warrants a knob, at least in 0.9.x. And yes, we should enable it by default. Thanks @deniszh!\n. That's an invalid character for metric names. Better to use something like pct.\n. Hmm, that's unexpected. Sorry, I'll reopen this so we can investigate further.\n. @deniszh Are the benefits worth the added complexity to 0.9.x? If we can't enforce a minimum version of whitenoise (because of python/django dependencies and conflicts) in setup.py, I wonder if many users will know to take advantage of it.\n. Bah, humbug.\n. Thanks!\n. Indeed, this has been fixed in master.\n. Closing in favor of #1611.\n. Thanks! :zap:\n. Python 2.4 is still the dependency for 0.9.x. That being the case, and with support for remote store fetch timeouts in 0.9.x, I think this is probably not going to work for 0.9.x. It seems like we have adequate support for remote fetch timeouts in master, so I'm going to close this one for now. Thanks though, @jjneely :sparkles:.\n. @jjneely Can you add the proposed changes by @deniszh?\n. Closing this one in lieu of #1330. Thanks @jjneely @deniszh! :beers:\n. The \"0.9.13\" from pip is a release candidate, not the final version. You didn't specify what sort of \"time gap issues\" you're having, but there were two recent PRs merged that address time issues. See https://github.com/graphite-project/graphite-web/pull/1157 and https://github.com/graphite-project/graphite-web/pull/1034 for details.\n. @ravibhooshan As mentioned in my previous comment, there were some fixes that went into the 0.9.x branch to address time issues. If you're able, please try the 0.9.x branch. If not, it should be fixed in the next stable release.\n. I defer to @SEJeff here, even more than normal. I hate time code. :trollface: \n. Is this related to https://answers.launchpad.net/graphite/+question/271258? If so, can't you please exercise some patience and let me investigate it before opening up a completely separate discussion?\n. Similar in what sense... that they're both reporting EAGAIN? That's a normal Python symbol telling the client to try again.\n. Have you tcpdump'ed the traffic to analyze the behavior? The output you've pasted suggests normal behavior on the part of the application.\nNote: edited your original comment to add code formatting.\n. LGTM since we require Python 2.6 now. My only gripe would be better naming on the setting, maybe CLUSTER_SERVERS_USE_HTTPS or CLUSTER_SERVERS_SECURE.\n. It would be helpful to see the actual data. Can you upload a copy of your Whisper file somewhere?\n. @schancel Would you mind testing with the 0.9.x branch (HEAD) and let me know if you're still seeing this? We've had some recent fixes that affect merging of CarbonLink and Whisper archives which I think may be related to your issue.\n. All nulls represent \"no data\". Under what circumstance would a null represent zero?\n. @atnak dare I ask, are you using Graphite to study DNA strands?\n. Oh, I didn't need another example, I was just curious. At this point I'm trying to decide if this is a feature unique to a single user (you) or if it's going to be generally applicable to others.\n. Aside from those small issues, LGTM.\n. Duplicate of https://github.com/graphite-project/graphite-web/issues/1191.\n. As someone who's blessedly ignorant of LDAP, I don't understand the advantages/disadvantages of this approach vs setting LDAP_BASE_USER and LDAP_BASE_PASS. Can we add any more comments to the local_settings.py.example that will help users?\nOtherwise, this looks benign.\n. Yes please, that would be helpful.\n. Yup, could you just fix the spelling typo (\"templat\") and move it above the line? It's ok to add an extra line above for separation, e.g.\n```\nLDAP_USER_QUERY = \"(username=%s)\"  #For Active Directory use \"(sAMAccountName=%s)\"\n\nUser DN template to use for binding (and authentication) against the LDAP server.\n%(username) is replaced with the username supplied at login.\nLDAP_USER_DN_TEMPLATE = \"CN=%(username)s,OU=users,DC=mycompany,DC=com\"\n```\n. Thanks!\n. Nice, thanks!\n. :+1: \n. @jjneely Thanks for the patch in #1308, and thanks to @deniszh for making it 2.6 friendly here.\n. Can you paste the relevant section of your carbon.conf?\n. Thanks!\n. Good catch, looks like a botched copypasta from the 0.9.x branch. Maybe. :smile: \n. Yes, this PR is indeed #1337. :wink: \n. Awesome, thanks for testing. This should resolve a lot of issues. :smile_cat: \n. Anything that adds test coverage sounds good to me but I'm pretty ignorant of Graphite's test innards. Looking for input from @SEJeff @brutasse.\n. @SEJeff I defer to you and @brutasse here in matters of python testing. Please advise.\n. @JeanFred If we merge this in, can we count on you to update Carbon and Whisper to use CodeCov too?\n. Just FYI, a little more background on why I started down this path. I've been testing 0.9.x in Synthesize and recently started seeing the following exception when starting or stopping Carbon. I can't find any evidence that this is a regression caused by recent versions of Twisted or by the existence of different Twisted versions on the same host, but it goes away for me when we remove the Twisted requirement for graphite-web.\n```\nStopping carbon-cache-1: Unhandled Error\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/python/usage.py\", line 382, in \n    fn = lambda name, value, m=method: m(value)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/application/app.py\", line 508, in opt_reactor\n    installReactor(shortName)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/application/reactors.py\", line 79, in installReactor\n    for installer in getReactorTypes():\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/plugin.py\", line 209, in getPlugins\n    allDropins = getCache(package)\n---  ---\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/plugin.py\", line 167, in getCache\n    provider = pluginModule.load()\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/python/modules.py\", line 383, in load\n    return self.pathEntry.pythonPath.moduleLoader(self.name)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/python/reflect.py\", line 464, in namedAny\n    topLevelPackage = _importAndCheckStack(trialname)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/plugins/twisted_core.py\", line 5, in \n    from twisted.internet.endpoints import _SystemdParser, _TCP6ServerParser, _StandardIOParser\nexceptions.ImportError: cannot import name _SystemdParser\nUnhandled Error\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/application/app.py\", line 595, in parseOptions\n    usage.Options.parseOptions(self, options)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/python/usage.py\", line 226, in parseOptions\n    for (cmd, short, parser, doc) in self.subCommands:\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/application/app.py\", line 607, in subCommands\n    for plug in plugins:\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/plugin.py\", line 209, in getPlugins\n    allDropins = getCache(package)\n---  ---\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/plugin.py\", line 167, in getCache\n    provider = pluginModule.load()\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/python/modules.py\", line 383, in load\n    return self.pathEntry.pythonPath.moduleLoader(self.name)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/python/reflect.py\", line 464, in namedAny\n    topLevelPackage = _importAndCheckStack(trialname)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/plugins/twisted_core.py\", line 5, in \n    from twisted.internet.endpoints import _SystemdParser, _TCP6ServerParser, _StandardIOParser\nexceptions.ImportError: cannot import name _SystemdParser\n```\n. @deniszh Try the following Synthesize patch to test this PR, works for me.\n$ GRAPHITE_RELEASE=0.9.x vagrant up\n```\ndiff --git a/install b/install\nindex f1af04d..46102f9 100755\n--- a/install\n+++ b/install\n@@ -30,7 +30,8 @@ RUNLEVEL=1 apt-get install -y libcairo2-dev libffi-dev pkg-config python-dev pyt\n# Download source repositories for Graphite/Carbon/Whisper and Statsite\n cd /usr/local/src\n-git clone https://github.com/graphite-project/graphite-web.git\n+#git clone https://github.com/graphite-project/graphite-web.git\n+git clone https://github.com/obfuscurity/graphite-web.git\n git clone https://github.com/graphite-project/carbon.git\n git clone https://github.com/graphite-project/whisper.git\n git clone https://github.com/armon/statsite.git\n@@ -38,7 +39,8 @@ git clone https://github.com/armon/statsite.git\n # Build and install Graphite/Carbon/Whisper and Statsite\n cd whisper; git checkout ${GRAPHITE_RELEASE}; python setup.py install\n cd ../carbon; git checkout ${GRAPHITE_RELEASE}; pip install -r requirements.txt; python setup.py install\n-cd ../graphite-web; git checkout ${GRAPHITE_RELEASE}; pip install -r requirements.txt; python check-dependencies.py; python setup.py install\n+#cd ../graphite-web; git checkout ${GRAPHITE_RELEASE}; pip install -r requirements.txt; python check-dependencies.py; python setup.py install\n+cd ../graphite-web; git checkout fix/remove_twisted_dependency; pip install -r requirements.txt; python check-dependencies.py; python setup.py install\n cd ../statsite; pip install --egg SCons; make; cp statsite /usr/local/sbin/; cp sinks/graphite.py /usr/local/sbin/statsite-sink-graphite.py\n# Install configuration files for Graphite/Carbon and Apache\n```\n. @deniszh Any last-minute objections before I merge this?\n. Good catch, thanks @brutasse.\n. I'm really excited to see this go in, but I have a couple issues.\nOne, I'd prefer to see @jjneely given credit for his original work. Can you submit a new PR with his original commits + your new setting? Second, I think the setting name should reflect that we're merging remote store results, perhaps REMOTE_STORE_MERGE_RESULTS (and group it in each settings file accordingly)?\nP.S. @deniszh Thanks again for the visual proof of the tests, very cool. :sparkles: \n. No worries, I didn't think you were trying to hijack anything. I'd just like to make sure everyone gets attribution where possible, especially for a nice change like this. :smile_cat: \nThis should get you a new working branch with his commits. You should see his commits from August in your git log.\n$ git checkout 0.9.x\n$ git pull git@github.com:graphite-project/graphite-web.git 0.9.x\n$ git checkout -b jjneely_merge_timeseries 0.9.x\n$ git pull https://github.com/jjneely/graphite-web.git 0.9.x-merge-timeseries\nThe last command will require you to commit the merge. Then make your changes, commit, and git push origin jjneely_merge_timeseries to your fork. You should then be able to submit a new PR that includes @jjneely's original commits and your changes.\n. Gonna go ahead and merge this in. We can discuss in #677.\n. @deniszh @jjneely We absolutely need to get this into master. Is there anyone willing to take this on if we find a sponsor?\n. First, you should never run mismatched versions of Graphite components. Second, there have been a lot of fixes for this type of behavior in the 0.9.x branch leading up to our next release. You should try the 0.9.x branch and please let us know if that doesn't fix your issues.\n. It's very unrealistic that someone will use decade as a time unit. The Graphite project hasn't even been around for one decade. Statistically, the likelihood of someone trying that is virtually zero, so I have no concerns with trying to support that literal unit. If someone really wants to query \"20 years\" they can use the year time unit.\n. People can type any nonsense in there but it doesn't make it any more valid. See, I can type from=-2shits, which will render seconds and just so happens to represent exactly how many things I don't give about this issue. :wink:\nPoint being, yes, you are expected to use the product as intended. If you choose to insert nonsense time units, then be prepared to be disappointed.\n. Will you port this to master's version of the 0.9.13 release notes?\n. Thanks!\n. Is it possible your theme is using a white font against a white background?\n. Awesome, glad to hear it. I suspected it might have been a fonts issue (this seems to be common with Red Hat systems) but I figured we'd start with the easy stuff. :wink:\n. The browser search functionality relies on the index db existing. There should be a build-index.sh script (typically in /opt/graphite/bin) that can be cron'd to rebuild your index file. HTH.\n. Nice idea but I think it would make more sense to extend sortByName() to support an optional arg (e.g. natural=false) rather than an entirely new function. What do you think?\n. LGTM!   :sparkles: :zap: :+1: :icecream: \nDefault/normal sort (natural=False):\n\nNatural sort (natural=True):\n\n. @addshore see https://github.com/graphite-project/graphite-web/pull/1414\n. This was originally removed in https://github.com/graphite-project/graphite-web/issues/886 because of the same permission denied errors for the build. Hmm.\n. @brutasse I'm still seeing build failures with this fix. https://readthedocs.org/projects/graphite/builds/3414550/\n. :fire:\n. Thanks!\n. Indeed, great work @deniszh @brutasse. Nice to have this fixed (for a while anyways, lol).\n. Thanks!\n. :heart: \n. What happens if you use a timeshift in terms of minutely units, e.g. 10080 mins instead of 7d?\n. If this looks ok I can port it to master.\n. For the first part, it sounds like your MEMCACHE_KEY_PREFIX is too long.\nYou can disable the cache logging by setting LOG_CACHE_PERFORMANCE = False. If you'd like me to investigate the behavior of the caching itself, I'll need to know which version of Graphite you're running here.\n. Closing this out for now but I'm happy to reopen if you're still looking for resolution.\n. It's probably fine to capitalize Ceres. Even the project README capitalizes it throughout.\n. Going to merge this in its current state. level-tsd looks like it's not under active development and even specifies that you have to use the megacarbon branch.\n. @deniszh look ok to you?\n. Although now that I think about it, the real problem tends to be the release notes. Maybe once the website is up we can just move them over there.\n. We can fix it in 0.9.x, but we have no plans to cut a new release from that branch.\n. Marking fixed by #1390.\n. The problem is not with nonNegativeDerevative, it's with your use of weightedAverage. More specifically, it looks like sumSeries is returning an empty result in line 679; probably because productList is empty. We try to grab the first result ([0]) which causes the IndexError: list index out of range.\nThis isn't a bug per se, in that we can't return a valid series where data doesn't exist, but it feels like we could raise a better exception message. Can you provide the actual render query you're trying to run?\n. Yes, I'm pretty sure this was fixed already in 0.9.14. Going to close this issue, but please let us know if you're still experiencing the problem after upgrading.\n. I don't see anything wrong with that code. It's easy enough to verify for yourself, just add a print statement to dump results['datapoints'] or len(results['datapoints']).\n. To clarify, this only gets triggered once (per restart) when the code path is hit, and is logged by the webserver (e.g. Apache error.log), not the application logs. Seems like a reasonable fix. Only affects 0.9.x.\n. Thanks @ShalomCohen! :sparkles: :boom: :cake: \n. This will happen for any function that attempts to call an array item explicitly. See https://github.com/graphite-project/graphite-web/issues/1389. There's nothing we can do there to prevent bad data, but it's one of my goals to improve the exception handling / logging.\n. I don't see how continue helps here. It's giving you bad data.\n. @toote Sorry if my response was too abbreviated. continue will certainly bypass the error, but it's my opinion that failing and logging a useful message is more helpful than silently continuing and returning bad data. I'm open to suggestions.\n. Yeah, that's a valid point. Reopening.\n. @mhagger @cbowman0 invited you both to the organization, really appreciate all the effort you've been putting into this. \ud83c\udf70 \n. I don't have any feedback on the code itself right now. The glyph stuff is so hairy it'll take me a bit of time to ramp up on it again. Or did you just mean afa aesthetic preferences?\n. Frankly they're all improvements as far as I'm concerned. The rendering stuff is hard to get perfect for all scenarios but I think this is generally better than where we started. Specifically wrt \"should we present floats or integers\" I think the current behavior is ok. I would typically argue in favor of consistency (\"either all floats or none at all\") but here we're using K units where I think it makes sense to be show some significant digits.\nDoes that address everything? \ud83d\ude1b \n. Thanks for all your work on this @mhagger it's greatly appreciated. \ud83c\udf70 \u2764\ufe0f \ud83d\udd25 \ud83c\udf66 \n. Thanks! :sparkles: \n. Thanks!\n. Thanks!\n. Thanks!\n. While working on the book, I remembered this issue and tested/verified the behavior for myself. I feel strongly that we should add a flag to allow the user to override this behavior from the layer performing the remote fetch. It seems like an easy addition; I'll take a stab as time permits unless someone beats me to it.\n. /cc @jmanero who was chatting with me about it yesterday :smile: \n. oh and @tomc603, sorry Tom. :wink: \n. /cc @brutasse @deniszh too\n. Nice, thanks @bmhatfield! :icecream: \n. Thanks!\n. :beers::heart::beers:\n. Merging this in even though the docs build is failing; something has broken outside of this diff.\n. Note that I stopped short of requiring 2.7 in check-dependencies.py since 0.9.x is more likely to be running on older hosts.\n. The docs build is broken, everything else passes. Merging.\n. Possibly related, we're also seeing broken docs tests in master related to the Django models, resulting in empty functions documentation. :cry: \nRunning my tests locally:\n```\n...\n...\nreading sources... [ 95%] tools\nreading sources... [ 97%] whisper\nreading sources... [100%] who-is-using\nWarning, treated as error:\n/usr/local/src/graphite-web/docs/functions.rst:17: WARNING: autodoc: failed to import module u'graphite.render.functions'; the following exception was raised:\nTraceback (most recent call last):\n  File \"/usr/local/src/graphite-web/.tox/docs/local/lib/python2.7/site-packages/sphinx/ext/autodoc.py\", line 385, in import_object\n    import(self.modname)\n  File \"/usr/local/src/graphite-web/webapp/graphite/render/functions.py\", line 26, in \n    from graphite.events import models\n  File \"/usr/local/src/graphite-web/webapp/graphite/events/models.py\", line 4, in \n    from tagging.managers import ModelTaggedItemManager\n  File \"/usr/local/src/graphite-web/.tox/docs/local/lib/python2.7/site-packages/tagging/managers.py\", line 5, in \n    from django.contrib.contenttypes.models import ContentType\n  File \"/usr/local/src/graphite-web/.tox/docs/local/lib/python2.7/site-packages/django/contrib/contenttypes/models.py\", line 159, in \n    class ContentType(models.Model):\n  File \"/usr/local/src/graphite-web/.tox/docs/local/lib/python2.7/site-packages/django/db/models/base.py\", line 94, in new\n    app_config = apps.get_containing_app_config(module)\n  File \"/usr/local/src/graphite-web/.tox/docs/local/lib/python2.7/site-packages/django/apps/registry.py\", line 239, in get_containing_app_config\n    self.check_apps_ready()\n  File \"/usr/local/src/graphite-web/.tox/docs/local/lib/python2.7/site-packages/django/apps/registry.py\", line 124, in check_apps_ready\n    raise AppRegistryNotReady(\"Apps aren't loaded yet.\")\nAppRegistryNotReady: Apps aren't loaded yet.\nERROR: InvocationError: '/usr/local/src/graphite-web/.tox/docs/bin/sphinx-build -W -b html -d /usr/local/src/graphite-web/.tox/docs/tmp/doctrees . /usr/local/src/graphite-web/.tox/docs/tmp/html'\n______ summary ________\nERROR:   docs: commands failed\n``\n. /cc @Dieterbe who mentioned the functions docs issue in IRC.\n. Locking in docs builds to Django==1.8 fixes that particular issue. I hate to do this but I think we need to limit our Django to>=1.4,<1.9` for the time being.\n. @wawrzek Yes indeed, that fixes it for me with Django 1.9. Thank you! \ud83d\udcaf \ud83d\ude0d \ud83c\udf70 \n@brutasse Your suggestion works for me with Django 1.8. I'm going to go ahead and use 1.9 going forward though. \ud83d\ude1c \n. The build is green aside from an unrelated broken docs build. Merging.\n. @AnthonySteele Was it not clear that's the whole point of this issue? :stuck_out_tongue_closed_eyes: \n. :boom::heart::sparkles::bow:\n. Seems fine. It'd be nice to know why we have cached data that old (i.e. we should purge it before we try to use it) but this looks like a reasonable fix for now.\n. @cosm0s Seems like this could be a very welcome addition but none of the comments have been addressed yet. Are you still able to contribute to this PR?\n. WFM, no apparent regressions.\nSample rendered with:\nrender/\n?width=586\n&height=308\n&target=collectd.graphite.load.load.*\n&graphType=pie\n&valueLabelsMin=30\n&pieLabels=rotated\n&areaAlpha=0.4\n&valueLabelsColor=white\n\nP.S. God have mercy on my soul for constructing this chart.\n. Sorry, but we don't have the resources or expertise to support Icinga's particular configuration of Graphite. Please contact Icinga support.\n. @iain-buclaw-sociomantic The tests are still failing, can you fix this up and we'll get it merged in?\n. Would you mind also updating the comment to reflect that it supports multiple series?\n. Thanks!\n. No, there's nothing built-in that I'm aware of. There may be something in ExtJS to help with this, but it's more likely that you'll need to construct the JS manually.\n. Thanks!\n. Graphite does not support datapoints in the future.\n. @jjneely Sorry for the slow response. This looks similar to https://github.com/graphite-project/graphite-web/pull/1431/commits/492ef8f8b7d49c13f5168a36cb9770f6efb05150 which merged in recently to master. Which approach do you prefer?\n. Sounds good, I'll whip up a PR.\n. Oops, already fixed in https://github.com/graphite-project/graphite-web/pull/1672.\n. This is probably a dumb question, but does it make sense to default startSourceAt to the from parameter?\n. Does Mackerel base their product (hosted monitoring) on Graphite, or you're saying you use it internally to support your operations?\n. Very cool.\n. Thanks!\n. What do you mean by one data point in Archive-3 is having a scientific value?\n. I assume you're hitting the performance limitations of your particular configuration. The short answer is to decrease your I/O bottleneck by increasing the number of carbon-cache processes and putting a carbon-relay in front of them; or in your case, having your carbon-c-relay distribute among your carbon-cache pool.\nI'm closing this issue since this is not a bug, but I encourage you to post your question to https://answers.launchpad.net/graphite which is more suitable for community/support discussions at length. Thank you.\n. You missed https://github.com/graphite-project/graphite-web/blob/master/docs/conf.py#L83.\n. Thanks!\n. I assume you mean should not eat? :smile: \n. +1 to @mleinart's comment\n. It's fine gentlemen, the issue is filed with the correct project. Let's move on. :heart:\n. Sounds like you're running an incompatible version of Django. Graphite-web 0.9.15 requires Django 1.4.\n. Good catch, thanks!\n. Thanks!\n. Sorry, I didn't see this one earlier (or forgot about it). Seems like a good idea. \ud83d\udc4d \n@cbowman0 feel free to merge this one if you'd like, you're on the team now. \ud83d\ude38 \n. Other than the grammar/documentation edits I've suggested, it looks generally ok to me and I can certainly see the utility.\n. Thanks!\n. Thanks to @cbowman0 for reminding me this one still needs to be merged.\n. Thanks to @cbowman0 for reminding me this one still needs to be merged.\n. @virtualdxs Which version of Graphite? Can you reproduce with the master branch?\n. @JeanFred Can you elaborate on what you've done in the past day or so? It looks like you added \"GitHub releases\" to the 0.9.14 and 0.9.15 tags from last year. Just want to clarify that you didn't push anything on top of the 0.9.x releases by accident.\n. Yeah it's fine, it just caused a bit of kerfuffle. Some folks saw the \"new release\" and got confused. Yes, I've deleted all \"releases\" since they're generally unused within the project. We tag releases and ship them to PyPI, and document the release notes accordingly.\nI'm totally :+1: on taking charge and being forward-thinking, but just keep in mind that operating \"rogue\", especially when you haven't done much administratively with the project, may cause some raised eyebrows. Best to open an issue first (as you've done here) or discuss with folks in IRC before making any non-code type changes (as a committing team member).\n. P.S. Added you back to the committers team. \n. I agree with the sentiment.\n. Yes, it appears accurate. The relevant bits are here and here. The latter (traditional template field) has an explicit match for the dot, the former (<<expanding>> field) lacks this delimiter.\nAt least, that's my hot take on this code. :wink: \n. I like the consistency of the annotation-style but I also like the simplicity of inline. \u00af(\u30c4)/\u00af\n. I'm fine with either.\n. Comments added.\n. I haven't tested it but it LGTM. Ok to merge @deniszh. \ud83d\udc4d \n. You're using a very outdated version of Graphite. I strongly encourage you to upgrade to 0.9.15 and try again. I don't think the problem you're seeing is with the summarize function at all, but rather with the way we were retrieving data in the first place. This was fixed in recent versions.\nNote that although I feel pretty confident about this, I haven't actually dug through your problem and the associated code deeply because you're running such an old version of the Graphite stack.\n. It sounds like this does effectively the same thing as transformNull() or drawNullAsZero(). In that case I'm :-1:.\n. @ckolbeck Yeah, seems reasonable and I'm happy to merge. Would you consider adding a test for this?\n. It's not merged yet. See my previous comment/question about adding a test.\n. @ckolbeck No worries, hope you had a nice vacation. Whenever you have time.\n. Here's a possible test I've been working on for this but I can't seem to get it working. It keeps returning the full original expression instead of the altered one. Browser (format=json) works as expected so I'm probably overlooking something stupid.\n@cbowman0 see anything obvious?\n```\ndiff --git a/webapp/tests/test_functions.py b/webapp/tests/test_functions.py\nindex c50342c..06a46e7 100644\n--- a/webapp/tests/test_functions.py\n+++ b/webapp/tests/test_functions.py\n@@ -1559,6 +1559,22 @@ class FunctionsTest(TestCase):\n         result = functions.countSeries(request_context, seriesList)\n         self.assertEqual(result, expectedResult)\n\ndef test_empty_countSeries(self):\nseriesList = [\nTimeSeries('removeAboveValue(constantLine(1),0)',0,600,60,[None,None,None,\n]\nfor series in seriesList:\nseries.pathExpression = series.name\nexpectedResult = [\nTimeSeries('countSeries(removeAboveValue(1, 0))',0,600,60,[0,0,0,0,0,0,0,0\n]\nfor series in expectedResult:\nseries.options = {}\n+\nrequest_context = {}\nresult = functions.countSeries(request_context, seriesList)\nself.assertEqual(result, expectedResult)\n+\n     def test_group(self):\n         seriesList = [\n             TimeSeries('collectd.test-db1.load.value',0,600,60,[1,2,3,4,5,6,7,8,9,10])\n```\n\n```\nFAIL: test_empty_countSeries (tests.test_functions.FunctionsTest)\n\nTraceback (most recent call last):\n  File \"/usr/local/src/graphite-web/webapp/tests/test_functions.py\", line 1576, in test_empty_countSeries\n    self.assertEqual(result, expectedResult)\nAssertionError: Lists differ: [TimeSeries(name=countSeries(r... != [TimeSeries(name=countSeries(r...\nFirst differing element 0:\nTimeSeries(name=countSeries(removeAboveValue(constantLine(1),0)), start=0, end=600, step=60)\nTimeSeries(name=countSeries(removeAboveValue(1, 0)), start=0, end=600, step=60)\n\n\n[TimeSeries(name=countSeries(removeAboveValue(constantLine(1),0)), start=0, end=600, step=60)]\n?                                               ------------- -\n\n\n[TimeSeries(name=countSeries(removeAboveValue(1, 0)), start=0, end=600, step=60)]\n?                                                 +\n\n\n\nRan 609 tests in 3.392s\n``\n. @cbowman0 Thanks, that makes sense. Replacing this PR with #1608 so we can add the test.\n. I can confirm this behavior, and it started with pip version 7.0.0. This can be worked around with the--no-binary=:all:` pip argument (to disable wheels).\nRelated, from the pip version 7.0.0 release notes:\n- Support --install-option and --global-option per requirement in requirement files (PR #2537)\n- The use of --install-option, --global-option or --build-option disable the use of wheels, and the autobuilding of wheels. (PR #2711) Fixes #2677\nAt the very least we should mention this issue in our install docs.\n. @jblaine So we can't \"fix\" the docs for 0.9.15 since they're associated with the tag, which is associated with the pypi package, etc. Yay for python packaging once again. For the time being please make sure to use --no-binary=:all: when installing 0.9.15 and pip >= 7.0.0.\n. @jvanasco If you're still interested, I'd appreciate your feedback with the latest versions of the install documents and updated setuptools for pip.\nhttp://graphite.readthedocs.io/en/latest/install.html\nhttp://graphite.readthedocs.io/en/latest/install-virtualenv.html\nhttp://graphite.readthedocs.io/en/latest/install-pip.html\nThis process has been updated so that anyone installing from these (\"latest\") docs should get the proper versions of each Graphite component (currently on version 0.10.0-rc1). It should also handle all required Python package dependencies, and where it doesn't, should document the needed vendor packages (e.g. Python or Cairo headers).\nIt's certainly possible there are still some rough edges, but I've run through this a couple times now and knocked out all of the obvious bugs.\nIf you're not interested in testing this out or you've moved on to a different installation process (or entirely different TSDB), that's ok too, just let me know so we can close this issue. Sincerely, thanks for your feedback.\n. Awesome, thanks @jvanasco. I hope you don't mind, I'm going to close this issue for now but I'm looking forward to hearing back from you.\n. I'm not sure I agree with this addition. There are other locked dependency versions (e.g. django-tagging) and we have the mechanisms in place (e.g. requirements.txt) to help enforce these. I appreciate the contribution but I'm going to politely decline this one for now. \nP.S. FWIW I think a better approach would be to use something like pip freeze.\n. Closing due to inactivity.\n. Thanks @cbowman0 \ud83d\udc4d \n. Have you tried using http://graphite.readthedocs.io/en/latest/functions.html#graphite.render.functions.grep to only match and return the metrics if the intended path exists?. I don't believe rrdtool has anything to do with the original exception. @skilledmonster are you still running into this issue? Apologies for the slow response.\n. Graphite-web 0.9.15 is intended to be run with Django 1.4. Please try that version and report back.. Cool. I'm going to close this issue since it appears to be a dependency versioning issue.. @arielnh56 Thank you for all of your work into this fix. Much appreciated. \ud83c\udf70 \ud83d\udcaf \ud83c\udf6d \ud83d\udc4d \ud83d\ude0e \n. Thanks!\n. Thanks!\n. Merging this because why not, right? :wink: \nBut seriously, we still don't intend to cut another 0.9.x release but I don't see any harm in getting it in the branch, especially since we're likely to merge it into master too via https://github.com/graphite-project/graphite-web/pull/1529.\n. refs #1521 \n. @cbowman0 How well has this been working for you in production (against master)?\n. @cbowman0 I haven't had a time to review this yet. Sorry, been hands-full with $newjob and Monitorama. Will try to look closer while I'm traveling tomorrow.\n. @cbowman0 I haven't had a time to review this yet. Sorry, been hands-full with $newjob and Monitorama. Will try to look closer while I'm traveling tomorrow.\n. @cbowman0 Sorry for the delay. Been HAM on Monitorama lately. This looks sane and we already merged a version of this in 0.9.x, so there's no reason not to get this in master and let people hammer away at it. Thanks for this fix.\n. Thank you for adding these tests. Dumb question but why not just use %f instead of %g?\n. Disregard, %g definitely looks more suitable here.\n. LGTM. @deniszh @SEJeff @cbowman0 Any concerns?\n. I'm fine with merging this into 0.9.x but we should have an accompanying patch to merge it into master.\n. @dnayyl Have you done any troubleshooting on weewxb to ensure that it's actually sending data every 5 minutes?\n. @dnayyl Sounds good, thanks for the update. Closing.\n. :heart: \n. Hi @1337newbee, I'm trying to understand your use case but I'm not a Seyren user so I'm not sure where the problem lies. Can you explain what sort of series you need for your Seyren trigger to fire as intended?\n. @1337newbee I'd love to help you out but you're not really giving me enough information to work with. It would help to know what sort of data you're storing, and what you want Graphite to convert it to.\n. That's precisely what transformNull() is for, not sure what else you're looking for.\n. Long story short, I've been distracted by my work at a commercial monitoring company the last couple years, not to mention the other usual stuff (family, writing a book, etc). I don't want to make grand promises I can't keep, but Graphite is becoming a big focus for me again. I want to get a new release cut from master within the next 2-3 months.\n. Good stuff. Mind cleaning up those little bits? I'm \ud83d\udc4d otherwise.\n. Oh sorry, I jumped the gun there. Carry on. \ud83d\ude04 \n. Thanks! \ud83d\udc4d \n. Note that we're no longer evolving the 0.9.x branch except in rare circumstances. Django-tagging has been pinned to version 0.4.3 in the master branch and it works as intended. Closing this issue.\n. \ud83d\udc4d \n. Hi @jbergler! Rest assured we will make sure there is a reasonable path forward for 0.9.x -> 1.0 users when 1.0 is considered ready for general consumption. That is to say, when it's fully documented and the warts are fully understood/acknowledged.\nGood news, Graphite development has been picking up recently but it's about to get a full-on boost of my time. I'm starting employment at raintank next week. They're huge fans of Graphite and intend to continue investing in the Graphite ecosystem, in particular with my time working on the project.\nP.S. Thank you for your PR bringing in sortByTotal to 0.9.x. \n. LGTM. \ud83d\udc4d \n. Looks like there are some additional ones we could include. Mind comparing to https://github.com/graphite-project/carbon/commit/e9904725e371974036ea8312231b092715a4b88c as a baseline?\n. Nice, thanks! \ud83c\udf70 \ud83c\udf34 \ud83c\udf67 \n. LGTM otherwise. Would be nice to get another set of \ud83d\udc40 on it though.\n. Yeah I think this is a safe merge. The other similar PR (for glyph.py) probably demands more scrutiny because... it's glyph.py. \ud83d\ude1b \n. https://github.com/graphite-project/graphite-web/pull/1548 was the one I was thinking of.\n. We don't want to lock Graphite to an old version of Django. If there are problems with newer versions we should fix those issues. Thank you for the proposed change though.\n. We don't want to lock Graphite to an old version of Django. If there are problems with newer versions we should fix those issues. Thank you for the proposed change though.\n. This PR now officially supercedes #1171. Adding @gboily's original description here for context.\n```\nAdd a new function 'groupByNodes' that support multiple indexes like the function 'aliasByNode'.\nThis avoids to add a new graph target for each new metric/node created.\nEx:\n&target=groupByNodes(ganglia.server*.*.cpu.load*,\"sumSeries\",1,4)\nWill \"generate\" the following formula : \n- sumSeries(ganglia.server1..cpu.load5)\n- sumSeries(ganglia.server1..cpu.load10)\n- sumSeries(ganglia.server1..cpu.load15)\n- sumSeries(ganglia.server2..cpu.load5)\n- sumSeries(ganglia.server2..cpu.load10)\n- sumSeries(ganglia.server2..cpu.load15)\n- ...\nWill produce the following legends : \n- server1.load5\n- server1.load10\n- server1.load15\n- ...\n- server2.load5\n- server2.load10\n- server2.load15\n- ...\n```\n. :shipit: \n. :shipit: \n. :zap::boom::sparkles::ice_cream:\n. :zap::boom::sparkles::ice_cream:\n. @johnseekins Thanks for this. I've opened #1573 to make sure we track a long-term fix for this issue.\n. Looks like @wawrzek's tip in https://github.com/graphite-project/graphite-web/issues/1425#issuecomment-194905689 may allow us to use 1.9 again.\n. @brutasse There's so much volatility in the Django release cycles that I think we're going to have to decouple ourselves from it sooner rather than later. It's become a nightmare to try and support all of the different versions >= 1.4 and <= 1.9.\nI would love to see graphite-web evolve to something like graphite-api (rendering) with pluggable support for events & users, and the UI stuff (composer, dashboard) broken off into their own projects. I think bringing graphite-api into the main project would be a great step forward but there's still a ton of work to break out the other bits, make them compatible, merge in graphite-web enhancements back into graphite-api, etc.\nI don't have a conclusive opinion here but I'd love to have the conversation continue.\n. :boom: \n. @brunorey Would it be possible for you to try the same query on a newer Graphite, e.g. 0.9.15 (or the 0.9.x branch)? I know there were some fixes in the code path for combining series of different precision levels. Alternatively, could you upload your Whisper files somewhere that I could test against 0.9.15?\n. Huge \ud83d\udc4d for removing an unnecessary knob.\n. \n. So much cleaner, love the arg name change. \ud83d\udc4d \n. @kamaradclimber Sorry, we merged some other submissions. Would you mind rebasing?\n. @kamaradclimber Looks like the test commit got swallowed somewhere? I'm only seeing the refactor commit.\n. Looks good. I ran some basic benchmarks at https://repl.it/C9kE/2 and can confirm the new algorithm works as advertised.\n. I'm :100: on this although the PR should aim to be backwards compatible with the old naming, at least as far as the code (not docs). As far as the new names, I prefer block over deny but I don't have any strong argument for it. I just think blocked_metrics.conf sounds better than denied_metrics.conf. \ud83d\ude1d \n. Cool, yeah a startup message to the logs would be good.\nYes, those names make sense to me.\n. USE_METRIC_FILTERS ?\nJason Dixon\nSent from my iPhone\n\nOn Jul 2, 2016, at 5:27 PM, gwaldo notifications@github.com wrote:\nFor reference, I'm replacing the general term (env vars, etc) USE_WHITELIST with USE_METRICSLIST.\nIt feels a little clunky, but I think it implies what we're going for. I'm open to a better name.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @DaxDupont Let's keep the conversation focused on the objective bits. Whether this should or shouldn't change isn't up for discussion.\n. @jpscaletti It doesn't matter what the original intent was. People do have a negative reaction to these terms. There is no justification for keeping it. Let's keep the discussion focused to the objective merits of the pull request.\n. As explained previously, please refrain to objective commentary on the proposed changes.\n\nJason Dixon\nSent from my iPhone\n\nOn Aug 3, 2016, at 6:40 PM, Juan-Pablo Scaletti notifications@github.com wrote:\n\"People\"? Who are these people? Are they contributors or users of the project? How many people must have a negative reaction to a word (any word) to merit a change?\nAlso, if whether this shouldn't change isn't up for discussion... Why the issue is still open?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. I didn't even know you could do that. Perfect. \ud83d\udc4d \n. @gwaldo Got time to rebase?\n. Closing in favor of #1700.\n. I'm not opposed to the idea, I just wonder how much effort will be required by the user to use them effectively (e.g. submit their metrics in the proper format).\n. /cc @Dieterbe since he's done a far amount of work thinking about histogram support for Graphite.\n. @iksaif I think you're probably right. /cc @torkelo :trollface: \n. I think the moral of the story is that we'd love to have Graphite support histograms as a first-class feature, but this patch doesn't really get us any further than what's already possible using the native primitives. Dedicated support for histograms within Graphite means adding a new storage component.\n\nI'm \ud83d\udc4e on this PR as it exists today. I'm closing the task but I'd be happy to see the discussion continue towards something fruitful down the line.\n. @cbowman0 why the change from unpickle to pickle? The latter is intended as a safe alternative to the insecure pickle.\nhttps://github.com/graphite-project/graphite-web/blob/af7c0c13889c59a192debcb3833643df3d394597/webapp/graphite/util.py#L159-L212\n. Ah right, we're loading from a file, not a string. Looks like a typo from https://github.com/graphite-project/graphite-web/commit/c198e5836970f0970b96498fcbe6fa83d90110cf. Yeah, this change looks good.\n\ud83d\udc4d \n. @cbowman0 Yeah, good idea.\n. Hi @stefanosala! Unfortunately we don't currently support \"pluggable functions\". Coincidentally, we recently merged support for integralByInterval() which sounds like it does the same thing. \ud83d\ude38 \n. LGTM \ud83d\udc4d \n. refs https://github.com/graphite-project/graphite-project.github.io/issues/10\n. LGTM, merging.\n. @nathanbower Can you elaborate on this \"limit\" you're hitting? I'm not aware of any fixed limit on the number of targets that can be returned. Here's a sample graph I just threw together showing 34 different metrics using wildcards, and then again with distinct targets (clicking one at a time).\n\n. Glad to hear it. Definitely let us know if something seems weird again. Closing.\n. @cbowman0 Agreed, and let's get that other one merged. :)\n. @cbowman0 Agreed, and let's get that other one merged. :)\n. Thanks!\n. Thanks!\n. LGTM, feel free to merge. \ud83d\udc4d :shipit: \n. Did you test this against Python 2.7?\n. Ok, just an FYI that we won't merge this until it's been tested for regressions.\n. Tested this with Python 2.7 on Graphite 0.9.15 (Synthesize). No visible regressions.\n. Ok with me if it's ok with @SEJeff.\n. Gonna go ahead and merge this but if anyone has objections please let me know and we can reconsider.\n. I love how clean that TimeSeries __eq__ change in #1590 made everything. Huge win. \ud83d\udc4d \n. Good catch and thanks as always for the new tests. \ud83d\udc4d \n. Yes, please! \ud83d\udc4d \n. @JeanFred I agree that we should definitely look to refactor views.py (and frankly, glyph.py needs a lot of TLC). But for now I'd like to get these in and we can refactor as time permits.\n@cbowman0 @SEJeff Do these tests look adequate to you? They're basically a copy of the existing format=json tests, but they already uncovered some bugs in the original implementation. \n. Merging this in but please feel free to correct me if the tests are insufficient.\n. I assume you mean in the 0.9.x branch. In that case, no. If you need it I encourage you to try running from the master branch instead. It's actually quite stable and our current focus is preparing a 1.0 release.\n. Fixes #1495.\n. @1337newbee If you're running master, just pull the latest changes and redeploy. If you're running a 0.9.x-based release you'll need to patch manually. It's a pretty small and self-contained change so it should be simple enough.\n. @1337newbee Can you elaborate on what parts of the documentation are not up-to-date? The installation instructions for master should be accurate. Otherwise it's a bug and we should treat it as such.\nhttp://graphite.readthedocs.io/en/latest/install.html\nAlternatively, you can try the Synthesize 3.0.0RC pre-releases to play with or install master, or just study the install script to see how it performs an installation from the master branch.\n. You didn't specify which link you're referring to. Either way if you're trying to install master then you must have done something wrong. At least with Synthesize 3.0.0RC you should see graphite_web-0.10.0_alpha-py2.7.egg-info in there.\n. No, if you're doing that then you're almost certainly installing 0.9.15. It should work if you do this instead:\npip install https://github.com/graphite-project/whisper/tarball/master\npip install https://github.com/graphite-project/carbon/tarball/master\npip install https://github.com/graphite-project/graphite-web/tarball/master\nNote that you can always check your graphite-web version by visiting http://<graphite>/version/.\n. No, if you're doing that then you're almost certainly installing 0.9.15. It should work if you do this instead:\npip install https://github.com/graphite-project/whisper/tarball/master\npip install https://github.com/graphite-project/carbon/tarball/master\npip install https://github.com/graphite-project/graphite-web/tarball/master\nNote that you can always check your graphite-web version by visiting http://<graphite>/version/.\n. I'd love to help debug this but we really need more information. Starting with the Graphite version you're running, but ideally I'd also like to see your actual request/response headers and the same responses as raw or json output to see if it's happening there as well.\n. Good catch, thanks! /cc @kamaradclimber \n. Note that I'm while not enthusiastic about implementing this in the render view, I don't see a better place for it since the output formats are so diverse in the way they handle timestamps. It also makes it more challenging to devise tests.\nAnyways, I think this is safe to go in. If there are any objections please let me know.\n. ref #1110 since we definitely want to prefer cairocffi if we're going to merge that too.\n. Awesome!\n. Awesome!\n. Anyone have any idea why the lint job is failing? Passes for me locally. \n(env)root@vagrant:/usr/local/src/graphite-web# git branch\n* landlord11-interpolate\n  master\n(env)root@vagrant:/usr/local/src/graphite-web# tox -e lint\nGLOB sdist-make: /usr/local/src/graphite-web/setup.py\nlint inst-nodeps: /usr/local/src/graphite-web/.tox/dist/graphite-web-0.10.0-alpha.zip\nlint installed: flake8==2.6.2,graphite-web==0.10.0a0,mccabe==0.5.0,pycodestyle==2.0.0,pyflakes==1.2.3\nlint runtests: PYTHONHASHSEED='4262694904'\nlint runtests: commands[0] | flake8 /usr/local/src/graphite-web/webapp/graphite\nlint runtests: commands[1] | flake8 /usr/local/src/graphite-web/webapp/tests\n_______________________________________ summary _______________________________________\n  lint: commands succeeded\n  congratulations :)\n/cc @SEJeff @brutasse @cbowman0 \n. I'm going to merge this since all other tests are passing but we'll need to figure out what's going on with lint. \ud83d\ude1c \n. Thanks!\n. This doesn't surprise me at all since we've never enforced pep8 indentation. I'll try and review later tonight but it would be good to have some extra sets of \ud83d\udc40 on this.\n/cc @graphite-project/committers \n. This doesn't surprise me at all since we've never enforced pep8 indentation. I'll try and review later tonight but it would be good to have some extra sets of \ud83d\udc40 on this.\n/cc @graphite-project/committers \n. That might be a good idea. Although most of them seem to require rebasing anyways, I think it would make it a fair bit more challenging to throw that into the mix.\n. Not true, I've been doing this work myself.\nJason Dixon\nSent from my iPhone\n\nOn Jul 27, 2016, at 8:37 AM, Jeff Schroeder notifications@github.com wrote:\nAnd a lot of the old 1-2 year old PRs likely never will be debased unfortunately.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. My vote would be to hold off until we can get all the PRs in milestone 1.0.0 rebased and merged. I'm :100: on the spirit of this change, but I also dread the notion of losing git blame.\n. P.S. At least until we reach the 1.0.0 milestone. :wink: \n. LGTM \ud83d\udc4d \n. Yeah, restarting the job cleared it up.\n. It was hurting my brain for a bit so I ran through it manually with some sample sets. LGTM \ud83d\udc4d\n. Note, this was a bit of a nasty merge due to subsequent TZ fixes by @brutasse in https://github.com/graphite-project/graphite-web/commit/72099b17490a772043d0974d823ec4956759f427 but it works for me and the tests look good.\n. refs #1627 \n. @cbowman0 I thought the existing tests were adequate but if you have suggestions I'll take a stab at it.\n. @brutasse Great feedback, thanks. Please review.\n. Note that removing the str interpolation (which was mistakenly left in during debugging) also fixed JSON validation.\n\n```\ncurl -s -k \"https://127.0.0.1:443/metrics/find/?query=carbon...*&format=nodelist&position=2\" | json_pp\n{\n   \"nodes\" : [\n      \"vagrant-1\"\n   ]\n}\n$ curl -s -k \"https://127.0.0.1:443/metrics/find/?query=carbon...*&format=nodelist&position=-1\" | json_pp\n{\n   \"nodes\" : [\n      \"activeConnections\",\n      \"avgUpdateTime\",\n      \"blacklistMatches\",\n      \"cache\",\n      \"committedPoints\",\n      \"cpuUsage\",\n      \"creates\",\n      \"droppedCreates\",\n      \"errors\",\n      \"memUsage\",\n      \"metricsReceived\",\n      \"pointsPerUpdate\",\n      \"updateOperations\",\n      \"whitelistRejects\"\n   ]\n}\n```\n. Squashed.\n. Awesome, thanks.\nJason Dixon\nSent from my iPhone\n\nOn Aug 2, 2016, at 9:13 AM, Christopher Bowman notifications@github.com wrote:\n@jdixon I'll send you a pull request with working tests within a couple hours.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @cbowman0 Nope, looks like the evaluator might need patching to support this pattern. I'm guessing @nharkins has some changes to evaluator that weren't pushed up. Gonna remove the horizontalLine() bits from my branch.\n. Good catch. That doesn't surprise me, there are a zillion different approaches in there. I'll int() those and push up.\n. @cbowman0 Not sure what's going on now, looks like TravisCI is running builds against a different commit.\n. Oh never mind, something ate my int() fixes.\n. Thanks to @russell for the contribution. \ud83c\udf81 \n. Definitely not. Thanks for catching that, I'll fix it now.\n. Is this a duplicate of #1210?\n. Looks like django-tagging has a default MAX_TAG_LENGTH of 50 characters. This can be overridden by adding MAX_TAG_LENGTH=255 to your local_settings.py and restarting the Graphite webapp.\n\nhttp://django-tagging.readthedocs.io/en/develop/#max-tag-length\n. Seems fine. Should we do something similar for label? \ud83d\udc4d \n. Er, never mind. Yeah this is fine to merge.\n. @brutasse Thanks for the feedback, I think these have been addressed now. We should definitely aim to replace other uses of json.dumps as we come across them going forward.\n. @brutasse @cbowman0 Now that master is pinned to Django==1.9, would you agree that we're ok pulling out the other tox envs? The following are complaining due to the use of JsonResponse which wasn't introduced until Django 1.7.\npy27-django14-pyparsing2\npy27-django15-pyparsing2\npy27-django16-pyparsing2\npy27-django16-pyparsing1\n. @brutasse Do we want to keep requirements loose?\n. Humbly presented as evidence: https://github.com/graphite-project/graphite-web/pull/1555\n. @brutasse In order to preserve sanity for 1.0.0 I'll go ahead and introduce JsonResponse into graphite-web for now.\n. \ud83d\udc4d \n. @torkelo This would add a set parameter to the events get_data endpoint. The parameter would support two new filter sets: union and intersection. Without specifying set, the behavior will not change.\nYes, the plan is to remove Django from the scenario post-1.0 but that doesn't preclude us from adding new functionality now to fix bugs.\n. How stupid am I to use a reserved function name for a variable. Fixed that. Also, I feel dirty but this change to implement manual filtering for intersections was the best non-yucky way to fix tagging's broken implementation.\n. This feels ready to merge. Any last minute objections?\n. Nice, thanks!\n. Please re-open this issue under the https://github.com/graphite-project/carbon project.\n. My apologies, I misunderstood your request.\n. Fixed by #1723.\n. I don't grok why this is failing unless something has changed for the build environment (on TravisCI's side). Nothing else has changed in 0.9.x since our last successful build for #1432.\n. I'm guessing this has something to do with TravisCI's new container-based build infra.\n. Really, TravisCI?\n. I've tested this pretty extensively in the composer. Gonna merge it in but someone please \ud83d\udc40 it if you get a moment.\n. Note that this works well for branch nodes, but it doesn't seem to pick up any additions to the root nodes. Investigating.\n. Awesome work, thanks @deniszh!\n. The function needs to be exposed to the Composer UI in https://github.com/graphite-project/graphite-web/blob/master/webapp/content/js/composer_widgets.js#L1067 and it would be great to see a test included.\n. @leoleovich Just fyi in case you haven't noticed but the test is failing.\n. @deniszh Yes but there's still a broken test.. @deniszh Yes but there's still a broken test.. Note that this is a bit of a band-aid imho. I would rather move this logic to a custom model, which would also allow us to add a real test case. But for now I'd like get this merged and we can circle back later.\n. Note that what I mean is that by inserting the graph directly via the ORM, we bypass thhe view logic, e.g.:\nuser.profile.mygraph_set.create(name=u'test_graph', url='target=secondYAxis(carbon.agents.test-db1.memUsage)')\nresponse = self.client.get(url, {'path': ''})\nself.assertEqual(response.status_code, 200)\n[leaf] = json.loads(response.content)\nself.assertEqual(leaf['graphUrl'], u'target=secondYAxis%28carbon.agents.test-db1.memUsage%29')\nThe result comes back without encoding since we added it directly via the model.\n[   {   u'allowChildren': 0,\n        u'expandable': 0,\n        u'graphUrl': u'target=secondYAxis(carbon.agents.test-db1.memUsage)',\n        u'id': u'2a812540041e8cef62b37dec5f8c6196',\n        u'leaf': 1,\n        u'text': u'test_graph'}]\nNote that I've also tried adding it with the test client get but it fails to save the graph for an unknown reason.\nresponse = self.client.get(url, {'action': 'save', 'graphName': 'test_graph', 'url': 'target=secondYAxis(carbon.agents.test-db1.memUsage)'})\n[   {   u'allowChildren': 0,\n        u'expandable': 0,\n        u'id': u'no-click',\n        u'leaf': 1,\n        u'text': u'No saved graphs'}]\nThis is all just FYI for later.\n. Proposed test:\ndiff --git a/webapp/tests/test_browser.py b/webapp/tests/test_browser.py\nindex f271720..5e9c2c8 100644\n--- a/webapp/tests/test_browser.py\n+++ b/webapp/tests/test_browser.py\n@@ -62,3 +62,19 @@ class BrowserTest(TestCase):\n         self.assertEqual(response.status_code, 200)\n         [leaf] = json.loads(response.content)\n         self.assertEqual(leaf['text'], u'f\u00f2o')\n+\n+    def test_encoded_graph_name(self):\n+        url = reverse('browser_my_graph')\n+        user = User.objects.create_user('test', 'test@example.com', 'pass')\n+        self.client.login(username='test', password='pass')\n+        response = self.client.get(url, {'path': ''})\n+        self.assertEqual(response.status_code, 200)\n+\n+        response = self.client.get(url, {'action': 'save', 'graphName': 'test_graph', 'url': 'target=secondYAxis(carbon.agents.test-db1.memUsage)'})\n+        self.assertEqual(response.status_code, 200)\n+\n+        response = self.client.get(url, {'path': ''})\n+        self.assertEqual(response.status_code, 200)\n+        [leaf] = json.loads(response.content)\n+        self.assertEqual(leaf['graphUrl'], u'target=secondYAxis%28carbon.agents.test-db1.memUsage%29')\n+\n. Updated your report to use a blockquote.\n. I don't personally run Graphite with NGINX but there's an obvious syntax error in your output. Have you checked that?\n. You haven't mentioned what version you're running so I can't intuit what your wsgi.py looks like, nor have you posted the file itself. I'd be happy to take a look but I need more details. :)\n. That's from whitelist.conf.example from the Carbon project. That's not a valid wsgi configuration.\n. Also, you haven't mentioned what version of Graphite you have installed. I'm going to assume 0.9.15, but it would be helpful if you got the specific version from pip.\n. Here's the sample graphite.wsgi file included with Graphite 0.9.15. Alternatively you could look at this gist by @drawks with his wsgi setup for NGINX.\n. @anirbanroydas Sorry, I don't use NGINX. Did you look at @drawks' gist that I linked earlier?\nAlso note that I suspect you're mixing instructions for the master branch with the 0.9.15 pip packages. Please ensure that you're following the correct installation docs for 0.9.15.\n. A general note that this addresses performance regressions for REMOTE_STORE_MERGE_RESULTS introduced in https://github.com/graphite-project/graphite-web/pull/1657.\n. I'd like to see the setup.py changes submitted separately if you don't mind.\n. @iksaif Can you share your before-and-after benchmarking results?\n. Sounds like your Graphite-Web isn't able to retrieve \"hot\" metrics from the Carbon cache(s). Check your local_settings.py to make sure CARBONLINK_HOSTS is set properly.\n. It looks like your clients aren't sending at regular intervals. This can cause gaps like that, especially if your retention policy doesn't match. Note that the [carbon] retention policy you pasted above only covers Carbon's internal reporting stats; it wouldn't affect the servers.* metrics you're showing in these graphs.\nThings to check:\n- First, make sure that your CARBONLINK_HOSTS setting above matches the [cache] definition(s) in your carbon.conf. If it doesn't, then Graphite-Web (the rendering API) will be unable to pull those \"hot metrics\" from cache memory.\n- You should have a schema in your storage-schemas.conf for the affected servers.* metrics (or just the [default] schema), what does it look like?\n- Note that for the time being, you can at least \"bandaid\" your graphs by turning on Graphite's lineMode=connected. I don't recall where this setting resides in Graphite but it shouldn't be too hard to find.\n. Paste your carbon.conf here so we can help you identify what your CARBONLINK_HOSTS setting should be.\nThe lineMode=connected comment refers to the lineMode feature of Graphite's render API. This will be exposed somewhere within the Grafana UI but I don't know where offhand.\n. Your [cache] block starts right there at the top. Looks like a standard configuration, so your CARBONLINK_HOSTS should be fine. Your retention policy looks fine. So it just seems like your clients are sending servers.* metrics at infrequent intervals. They need to send them at 1s intervals or you're going to end up with gaps (based on your current 1s:1d precision).\n. I don't see any obvious regressions for Python 2.x but I wonder why we'd want this in there? Are you running Graphite successfully with Python 3.x?\n. Maybe something wonky with your python environment? IMHO you should be developing and running your tests in a virtualenv anyways.\n. Regardless I don't see any problems with this going in for now with the expectation that One Day :tm: we'll support Python 3. \ud83d\ude1c \n. P.S. Have you tested package builds with this change?\n. @iksaif I'm tempted to merge it, but given that it's largely an unnecessary change and that the same hasn't been submitted for the other projects, I'm going to pass on this for now. \n@deniszh You have any strong opinion on this?\n. Ok I'm closing this one now but I'm gonna hold onto it like that last piece of bubble gum for when we really need it. :wink: \n. Modern systems or not, Python 2.x is still a requirement for Graphite. Our own tox.ini requires Python 2.7 as the basepython.\n. Did you include the fix from https://github.com/graphite-project/graphite-web/pull/1666 in your test?\n. Do you have any graphs that demonstrate the regression? I'd like to compare it against @iksaif's results in https://github.com/graphite-project/graphite-web/pull/1666#issuecomment-243656795.\n. wut\n. I was hoping you could show a graph that visually corroborates your regression claims, similar to what @iksaif did in the comment I linked earlier.\n. @atnak I typically use collectd's tail plugin to measure retrieval time as first demonstrated by @jssjr here.\n. @iksaif My main concern here isn't so much the slowdown for remote fetches as that we're slowing down at all for local fetches. As @atnak mentioned, why should anyone hit this if they're not using clusters?\n. @iksaif Sorry, this isn't your problem; you worked on a performance fix, not the original timeseries merge code.\n. It's looking for more test coverage for the patch. Close enough, I wouldn't worry about that.\n. As @deniszh mentions, this is a pretty basic configuration setting for Apache. I'm sure either of us would be happy to assist if you'd like to post more configuration details but there's no need to keep this issue open any longer.\n. If I understand your problem, why not just filter out the spikes with removeAbovePercentile() or removeAboveValue()?\n. @jonkerj Agreed, I can see how this may be a little confusing. I'd consider changes to this but I'd like to ensure it doesn't break expectations for existing users of this function.\n. You should be able to query it just fine (i.e. the render API) but the composer will not support browsing them in the metrics tree. There's a long-standing bug around this that would not be resolved without a complete redesign.\nJason Dixon\nSent from my iPhone\n\nOn Sep 5, 2016, at 7:45 AM, Vichan Chutitaporn notifications@github.com wrote:\nI have 2 following metrics\ntest.count\ntest.count.max\nWhisper files are stored corectly with 2 whisper files.\n/opt/graphite/whisper/test/count.wsp\n/opt/graphite/whisper/test/count/max.wsp\nCan I query both metric in this way.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Not offhand. Are you getting any server errors or exceptions?\n\nJason Dixon\nSent from my iPhone\n\nOn Sep 5, 2016, at 9:29 AM, Vichan Chutitaporn notifications@github.com wrote:\nStrange, the render api cannot query test.count. Is there any know issue or setting I should consider?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @toopa2002 I hate to say it, but 0.9.12 is way too old. Please upgrade to a supported version (0.9.15 or from the master branch) and let us know if you still encounter this issue.\n. I assume you want to close this one in favor of #1680?\n. The difference here is so minimal (to the other PR) I'd prefer to see it added on top of #1680.\n. Just fyi, we (I?) can retrigger the tests on Travis anytime.\n. Looked through the impact on useCache, seems sane. \ud83d\udc4d \n. @ellipses Thanks for the contribution. Looks like the tests are failing, can you take a look into that?\n. You have a couple things going on here. The exception global name 'carbon' is not defined is a bug that was fixed in https://github.com/graphite-project/carbon/commit/6aeb5b94274a36f41a2c32a74753a3d05b0cacc2. You should apply that fix or upgrade to 0.9.15.\n\nThe log is also telling you that your carbon-cache has hit MAX_CACHE_SIZE. That is to say, your cache is holding the maximum number of datapoints allowed. Short-term you can increase the MAX_CACHE_SIZE, but you should plan on scaling your node(s) vertically (more caches + relay) or horizontally (more nodes).\n. My pleasure!\n. There's no way you should be seeing that log entry with MAX_CACHE_SIZE = inf. Your system is lying to you; either you're not running the settings you think you are, or something else is going on.\nhttps://github.com/graphite-project/carbon/blob/0.9.14/lib/carbon/cache.py#L46-L53\n. It looks right but that's your CM config, not your generated carbon.conf. I have no idea if this has actually been applied to your running system and that the carbon-cache processes have been restarted.\nP.S. Note that I updated your comment to use triple backticks for code blocks, please use these in the future.\nP.P.S. Also note that it's a good practice to set your MAX_CACHE_SIZE to a non-inf value. I touch on the reasons why in this blog post as well as the Scaling chapter of this book. Long story short, Carbon will only perform batch writes effectively when you limit the cache size. It's fine to start with inf but you should observe your cache size (carbon.agents.*.cache.size) and then set MAX_CACHE_SIZE (and MAX_UPDATES_PER_SECOND) to a reasonable value.\n. No problem!\n. I'm \ud83d\udc4e on this. Icinga's inclusion of Graphite doesn't really qualify it as a \"tool\" imho. I appreciate the contribution but we'll pass on this one.\n. The url link and section are still wrong, fixed in #1686.\n. LGTM, thanks!\n. @redbaron Can you provide a diagram of your setup?\n. I don't know if anyone noticed this the first time, but @cbowman0's https://github.com/graphite-project/graphite-web/issues/1063#issuecomment-240763266 appears to demonstrate a 2s latency in cluster requests in the best case and in one case (datastore_3), it looks like 40s.\n@bmhatfield does that look correct to you?\n. Ok that's fair. I think it's still fair to acknowledge we have some performance issues afa parallel cluster requests. I expect to be fairly occupied over the next couple days but I'll try to setup a test cluster scenario this weekend.\n. Just a general note that this is a hard blocker for the next release.\n. I've been looking at this for a while and I'm not able to reproduce anything that suggests a threading performance or deadlock issue. @Kenterfie Is it possible that the pickle rendering (or network transit) is taking longer than expected?\nHere are some sample datapoints requesting one large query at a time. There's a single frontend with three backend storage nodes.\nC0 - laptop client\nB0 - backend storage\nB1 - backend storage\nB2 - backend storage\nF0 - frontend render\n```\n(C0) $ curl -k \"https://${F0}/render/?target=collectd..cpu-.cpu-*&format=json\" 2>&1 >/dev/null\n(B0) Wed Oct 05 15:12:25 2016 :: Retrieval of collectd..cpu-.cpu- took 0.364455\n(B0) Wed Oct 05 15:12:25 2016 :: Total pickle rendering time 0.424098\n(B1) Wed Oct 05 15:12:25 2016 :: Retrieval of collectd..cpu-.cpu- took 0.556552\n(B1) Wed Oct 05 15:12:25 2016 :: Total pickle rendering time 0.634059\n(B2) Wed Oct 05 15:12:23 2016 :: Retrieval of collectd..cpu-.cpu- took 0.185500\n(B2) Wed Oct 05 15:12:23 2016 :: Total pickle rendering time 0.206219\n(F0) Wed Oct 05 15:12:25 2016 :: Retrieval of collectd..cpu-.cpu- took 1.124653\n(F0) Wed Oct 05 15:12:27 2016 :: Total json rendering time 3.208207\n(B0) Wed Oct 05 15:14:24 2016 :: Retrieval of collectd..cpu-.cpu- took 0.334817\n(B0) Wed Oct 05 15:14:24 2016 :: Total pickle rendering time 0.392128\n(B1) Wed Oct 05 15:14:24 2016 :: Retrieval of collectd..cpu-.cpu- took 0.467109\n(B1) Wed Oct 05 15:14:24 2016 :: Total pickle rendering time 0.541729\n(B2) Wed Oct 05 15:14:22 2016 :: Retrieval of collectd..cpu-.cpu- took 0.166958\n(B2) Wed Oct 05 15:14:22 2016 :: Total pickle rendering time 0.186709\n(F0) Wed Oct 05 15:14:24 2016 :: Retrieval of collectd..cpu-.cpu- took 0.802444\n(F0) Wed Oct 05 15:14:26 2016 :: Total json rendering time 2.822548\n(B0) Wed Oct 05 15:50:51 2016 :: Retrieval of collectd..cpu-.cpu- took 0.307690\n(B0) Wed Oct 05 15:50:51 2016 :: Total pickle rendering time 0.366291\n(B1) Wed Oct 05 15:50:51 2016 :: Retrieval of collectd..cpu-.cpu- took 0.498106\n(B1) Wed Oct 05 15:50:51 2016 :: Total pickle rendering time 0.573213\n(B2) Wed Oct 05 15:50:49 2016 :: Retrieval of collectd..cpu-.cpu- took 0.170795\n(B2) Wed Oct 05 15:50:49 2016 :: Total pickle rendering time 0.190676\n(F0) Wed Oct 05 15:50:51 2016 :: Retrieval of collectd..cpu-.cpu- took 0.888467\n(F0) Wed Oct 05 15:50:53 2016 :: Total json rendering time 2.889416\n```\nNote that I believe that something is causing a performance regression for you guys, but it's been very difficult to reproduce the conditions in such a way that will point to precisely where we should begin looking.\n. Apologies, I misunderstood the problem. I thought we were trying to track down a gap in time between the backend retrieval finished and the rendering began. I know see that we're talking about how long retrieval takes on the frontend node for local metrics when clustering is enabled.\n. Regardless, I'm still not seeing any \"regression\" between master and 0.9.15. Both systems behave identically for the tests I've performed.\n. @Kenterfie My results do not correlate to your findings. The Retrieval timer reported by the frontend represents the total (or at least, the time required to merge the required union) of all CLUSTER_SERVERS (and local) retrievals. Hence, it's only natural that disabling CLUSTER_SERVERS on the frontend would result in drastically reduced query times (it's only retrieving locally).\nThis is what I find so frustrating... I'm unable to reproduce anything that resembles your or @reyjrar's results.\n. It would appear that the remote fetches are not happening in parallel, but sequentially. Note the results in the gist below: the first few (one for each of my CLUSTER_SERVERS) represent the initial connection setup; subsequent fetches reuse these connections and are much faster, but all of them are still blocking in order.\n@Kenterfie would you mind applying the datalib.py patch here and gisting your results?\nhttps://gist.github.com/obfuscurity/9f5f6c59d700a6ec1ec41585bc8ecaf8\n. @cbowman0 I absolutely agree that the per-node fetches are serialized. However, if you look at my logs, it would appear that all of the fetches are serial, according to the timestamps (one does not start until the previous is finished). I have a hard time believing that none of these fetches would have staggered run times. Please correct me if you think I'm misreading the output.\n. Also, agreed on the http connection startup time. I wasn't implying that anything is wrong there, just highlighting it.\n. @Kenterfie Were you running 0.9.x with REMOTE_PREFETCH_DATA enabled or not?\n. @Kenterfie ping ^\n. I've looked at this for the better part of 3 weeks now, attempting to forward port the parallel fetch and prefetch designs from 0.9.x to master. I'm not embarrassed to admit that I'm not smart enough to figure it out on my own.\nI've been pushing very hard to release 1.0.0, but this behavioral regression is simply too important to ignore. For the time being I'm planning to bump setup.py to 0.10.0-rc2 and ask folks to test while we continue to look into this cluster performance issue.. The wonderful @DanCech has been kind enough to hack on this for a while. He has a branch up that seems to be fixing the clustering performance issue. @Kenterfie @reyjrar @redbaron would you be so kind as to try out his branch and see if that improves things for you?\nhttps://github.com/graphite-project/graphite-web/compare/master...DanCech:clustering\n@graphite-project/committers This is going to be a significant patch so it would be great to get eyes on it sooner rather than later.. Per conversations with @DanCech, it seems to be the only http lib that's thread-safe.. refs #1509 \n. refs #1509 \n. Any useful results from your tests?\n. I also just ran a basic test against a 1:1 cluster. Like @Kenterfie, I'm also seeing even worse performance with this patch applied.\n. I'm pushing this back to the next milestone for now. I would like to see this go in but after working on it for a day I feel like there's room for improvement in terms of naming clarity. I've gone through it a few times and caught myself getting confused with some of the functions/settings.\n. refs https://github.com/graphite-project/carbon/pull/591\n. @reyjrar Can you elaborate on your comment? It sounds like you're saying the existing implementation in settings.py is broken... but then you closed your issue?\n. Have you seen minXStep?\n. Sorry, misunderstood your question. No, there is currently no way to override this via the render API. You could change to labelStep=1 here to achieve the desired effect.\nIt would be nice to expose labelStep in the render API.\n. LGTM \ud83d\udc4d . Thanks!\n. The 2nd and last lines are telling you there's a syntax error in your graphite.wsgi file.\n. The last line is telling you that you're missing the cairo libraries needed for cairocffi.\n. @torkelo Would this cause any problems for Grafana?. Note that I want to merge this in but not until #1690 is resolved. There is a related branch that's already pretty complicated that I'd like to see go in first, then we can rebase this one.. Going to merge this now. Discussed with @DanCech, he said he'd rebase this into his ongoing work related to #1690.. I see no reason why URL_PREFIX shouldn't continue to work as intended. Note that this is a Graphite-only setting developed specifically to support non-root URLs.\nConversely, STATIC_URL is a Django setting which has more to do with how the URL is interpolated within templates.\n. Can't say without you providing more details, e.g. the actual 404 log entry (which URLs is it trying to load), what your wsgi configuration looks like, etc.\n. Can we also include some new tests?\n. Thanks for adding the tests. @deniszh if you feel confident about this and #1531, feel free to merge both.\n. As per the official docs, Graphite is not supported natively on Windows. However, you can run Graphite in a virtual machine on Windows. The Graphite website has a quickstart guide for doing precisely this using the Synthesize project.\n. I don't see how this makes the documentation any clearer. @cbowman0 ?\n. Thanks!\n. @iain-buclaw-sociomantic Went ahead and merged this in so we have at least something in there. It would be great to add those missing sections as your time permits.. Thanks!\n. This is not a Carbon issue per se, but an upstream problem with pyOpenSSL. See https://github.com/pyca/pyopenssl/issues/392 and https://github.com/pyca/cryptography/pull/2501. You should be able to get around this by explicitly removing python-openssl (which IIRC is installed by something unrelated) or forcing it to use a newer version with the pyca fix.\n. @deniszh Did you see my full comment? I had replacement text with formatting for that paragraph.\n. Hint: it was inside the blockquote. \ud83d\ude04 \n. Thanks!\n. Thanks!\n. Seems fine, only affects RRD.\n. @edlitmus Do you know if this fixes https://github.com/graphite-project/graphite-web/issues/361?\n. Good catch, I was thinking of the release series instead of the specific version(s). I didn't mean to lock it into 1.9(.0).\n. I'm not an encoding expert, but would it make more sense to include this in settings.py instead since then it's certain to get pulled in by default?\n. Good catch, thanks. Fixing in #1729.\n. LGTM\n. Wouldn't it make more sense to just source the setting directly in remote_storage.py rather than passing it from the view (it's not a query param)?\n. Sorry, I had a total brain fart. I forgot we were pulling the headers out of the request in the view. As you were. :wink: \n. Any feedback from anyone else @graphite-project/committers?\n. It would be far better to support protobufs or something similarly efficient. JSON is far too inefficient for inter-node data transfer imho. See https://github.com/graphite-project/carbon/issues/275 and https://github.com/graphite-project/carbon/pull/601 for related discussions.\n. Yes, I was just giving that as an example. JSON is a horribly inefficient format for exchanging data between nodes.\n. Don't get me wrong, I'm not a JSON hater. I just don't think it's appropriate for this use case.\n. That said, if someone wants to add JSON support and show benchmarks to support its inclusion, I'd be happy to consider the related PRs.\n. Yeah but that's a 1:1 query. We're talking backend clusters which are by nature one-to-many.\n. /cc @DanCech who's working on related\nJason Dixon\nSent from my iPhone\n\nOn Nov 30, 2016, at 8:43 AM, Ben Burry notifications@github.com wrote:\nI hear your concerns, and had overlooked that master now has custom storage backend support.\nWithout adjusting the API, making the request context available is achievable either by using a threadlocal (which has enough negative/conflicting opinions to make it an undesirable approach) or ... I'll have a think\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Also \ud83d\udc4d \n. Job restarted.\n. \ud83d\udc4d \n. Always a fan of your tests. \ud83d\udc4d \n. Thanks!\n. Thanks!\n. Because the documentation for functions is derived from the inline comments among the code, it would a) involve tons of research and b) really muck up the git blame.\n\nI'm open to suggestions on how this would best be accomplished without diverting precious developer resources and cluttering up our commit history.. Yes, that's a handy format for sure. Not sure that it addresses any of my concerns. Are you proposing we rip all of the documentation out and make it standalone? Because that's even more of a resource demand than simply adding comments to the existing documentation source.. I totally understand and empathize. If you look at http://graphite.readthedocs.io/en/0.9.15/functions.html instead of http://graphite.readthedocs.io/en/latest/functions.html then you're seeing the correct docs for your version.. @deniszh I don't think it makes enough difference to warrant mentioning here. I've added a lot more, everything look ok otherwise?. Gonna go ahead and merge this but we can adjust as needed.. Yeah you need to update .travis.yml too.. @cbowman0 @SEJeff @brutasse Any concerns with removing these?. @SEJeff Do you have a link to that versioning feature?. Python, thank you for my fevered dreams tonight. \ud83e\udd12 . This breaks pip requirements for older (e.g. system pip on 14.04).\n```\nvagrant@vagrant:~$ lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 14.04.2 LTS\nRelease:    14.04\nCodename:   trusty\nvagrant@vagrant:~$ pip -V\npip 1.5.4 from /usr/lib/python2.7/dist-packages (python 2.7)\n\n+ cd ../graphite-web\n+ git checkout master\nAlready on 'master'\nYour branch is up-to-date with 'origin/master'.\n+ pip install -r requirements.txt\nException:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 122, in main\n    status = self.run(options, args)\n  File \"/usr/lib/python2.7/dist-packages/pip/commands/install.py\", line 262, in run\n    for req in parse_requirements(filename, finder=finder, options=options, session=session):\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 1632, in parse_requirements\n    req = InstallRequirement.from_line(line, comes_from, prereleases=getattr(options, \"pre\", None))\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 173, in from_line\n    return cls(req, comes_from, url=url, prereleases=prereleases)\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 71, in init\n    req = pkg_resources.Requirement.parse(req)\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2667, in parse\n    reqs = list(parse_requirements(s))\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2605, in parse_requirements\n    line, p, specs = scan_list(VERSION,LINE_END,line,p,(1,2),\"version spec\")\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2573, in scan_list\n    raise ValueError(\"Expected \"+item_name+\" in\",line,\"at\",line[p:])\nValueError: ('Expected version spec in', 'Django~=1.9.0', 'at', '~=1.9.0')\nStoring debug log for failure in /home/vagrant/.pip/pip.log\n``. This LGTM. In case anyone else (like me) was unaware,django.conf.url.patterns()` was deprecated in 1.8 and removed in 1.10.. Anyone else in @graphite-project/committers want to review this before merging?. Thanks!. Yup, makes sense. Thanks!. Your \"fix\" describes the old behavior that was changed in https://github.com/graphite-project/graphite-web/pull/1627. I assume you're using a 0.9 release.\nThe URL fix is correct, would you like to rework the patch for that only?. Thanks!. @howfree All metrics are recorded as floats in Graphite. There are no additional storage formats for numbers.. We don't package Graphite for any distributions. In fact, I have no idea what graphite-manage even is. Please open a bug report with the vendor.\nAs a point of reference, here are the official database setup instructions:\nhttp://graphite.readthedocs.io/en/0.9.15/config-database-setup.html. @DanCech Are you saying this is related to the issues reported in #1690? Because the FloatEncoder changes weren't merged into master until https://github.com/graphite-project/graphite-web/pull/1710, 12 days ago.. LGTM, any concerns or feedback from @rehevkor5 or @torkelo?. @torkelo Yeah, fixing a perf regression like that is a no-brainer. I cc'd you more for any concern about compatibility with Grafana. \ud83d\ude3a . Is the intent to remove the FloatEncoder class at some point in the near future? Why not remove it now?. @DanCech Assuming it fixes the performance hit, I'm leaning towards #1785 since it fully reverts the FloatEncoder class. Any concerns?. @DanCech Makes sense to me, thank you for the detailed explanation. I'm :+1: in favor of your approach. Anyone else want to chime in before I merge this?. @Marek77 The only image link that works for me is the first one. Instead of linking externally, please embed the images in your original comment using GitHub attachments (see the help at the bottom of the comment editor window).. Ok, I agree with the intent of the change. Would you put the parentheses back (in the right places) to remove any ambiguity in the math?. Yeah, I know. I just really like the added clarity cuz I iz stupid. :wink: . Thanks @Marek77!. Yes, my assumption (upon reading your tweet) is that this is due to the condensing of data necessary to fit a fixed number of points within the rendered image, and not \"exactly the problem\" that you alluded to when dealing with percentiles.\nMy suggestion would be to either widen your graph to a point where consolidation is unnecessary or use the consolidateBy function with the max or min arguments to preserve your peaks.. If you're curious in the consolidation code, you can find the respective bits in HEAD here.. Oh I wasn't annoyed so much, I just get frustrated with conjecture... especially with a lack of details. Anyways, I'm glad that we narrowed down the problem and gave you a suitable fix. \ud83d\udc4d . @Dieterbe As @nickstenning made clear early on, he was experiencing datapoint consolidation within the same archive, not rollups.. Thanks! \ud83c\udf70 . Can you please provide more details? At the very least, a link to the incorrect documentation?. Agreed, we need to be consistent within the same page. Thanks!. Generally LGTM. What's the advantage to using random.shuffle()?\n/cc @DanCech . \ud83d\udc4d . Can you add any tests for this?. Very nice change. I forgot grammar was even there. \ud83d\ude04 \nLGTM. \ud83d\udc4d . I like this in general, but I would prefer to call it uniqueSeries or simply unique.\nShould the function attempt to do any reconciliation of series with different contents (or determine preference)?. refs #1779 . #1690 is blocking release of 0.10.0. Presumably work on this has been delayed due to the holiday season but hopefully we should see progress on it again soon.. Would you mind adding tests for this?. Closing this one in favor of #1768. Regardless, @rehevkor5 thank you for the original change and for your input on fixing the regression.. I'm probably being naive, but why not just convert nulls to zero, sum up all the series, and look for anything > 0 (or the actual value, if that's interesting)?. @Roguelazer Yes, we've talked about adding the concept of \"pluggable functions\" to Graphite. We'd all agree that it would be a fantastic addition, someone just has to find the time and resources to make it happen.. @deniszh I don't really have a strong opinion on it anymore.. I'm curious about the new gitignore entries. I like the added tests. Everything else looks generally good although I haven't scrutinized it yet.. Thanks!. I'm not crazy about the user_util naming but I'm not going to lose sleep on it either. I would recommend breaking the user_util tests out to their own file though.. Sadly, I do not. Naming is hard. I wouldn't worry about that too much.. @iksaif Did you mean to push the new file?. No problem, git gremlins are the worst. :)\nOn Thu, Jan 26, 2017 at 12:25 PM, Corentin Chary notifications@github.com\nwrote:\n\nSomehow the file got lost, even on my filesystem. I'll push it again on\nmonday. Sorry for that\n\u2014\nYou are receiving this because you are on a team that was mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1798#issuecomment-275451802,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAeLAuHn3dMP07a9c_tH2419PrvMteqnks5rWNb6gaJpZM4LtgE1\n.\n. :+1: . Are you sure that your collectd metrics are being emitted every 10s? For example, if they're only being emitted every 60s, they would not fulfill your xFilesFactor ratio and hence be rolled up to nulls in the second archive.\n\nI explain it a bit more in this blog post.. Glad to hear it, no problem! :heart: . Perhaps https://github.com/graphite-project/graphite-web/pull/1662?. I'm very much in favor of this approach, with Graphite continuing to default to FULL.. Agreed, we should have at least a commented-out entry in local_settings.py so users know it exists.. No objections here.. :+1: . :+1: thanks. I haven't tested but I like the approach.. Would be nice to have someone benchmark reads after this change.. Dangit, I was looking specifically for that and missed it. Thanks for catching it in hindsight.. Haven't tested but LGTM. Anyone have time to test against both backend versions?. P.S. Great idea @DanCech. :smile_cat: . :+1: . I'm generally :+1: in favor of this but I'd also like to see #1736 go in first and then add the supporting changes for #1818.. /cc @graphite-project/committers . Apologies, REMOTE_STORE_MERGE_RESULTS already exists in master. I think I meant to refer to parallel fetch but I suspect the proposed cluster changes will incorporate this functionality. Does this sound right to you @DanCech?. @iksaif http://graphite.readthedocs.io/en/latest/releases/0_10_0.html#upgrading\nYes, a wiki page should be created, but I'd like to see it put together by whoever manages this release. The notes I added above are a couple years old and iirc there were some problems with the pip packages in 0.9.15 so they should be taken with a grain of salt.. @deniszh Is there an authoritative page I can point users to or is it assumed they'll figure out how to install the RC?. https://twitter.com/obfuscurity/status/846417810804948992. It's already in the queue. \ud83d\ude04. @shivagopalan Just use a separate URL for each graph.. @shivagopalan The events argument must be a quoted string, e.g. events('*').. Let's clarify what we're talking about. It's absolutely possible to render a virtually infinite number of graphs on the same page by sourcing each of them individually, e.g.:\n<img src=\"http://graphite/render?target=foo.a\" />\n<img src=\"http://graphite/render?target=foo.b\" />\n<img src=\"http://graphite/render?target=foo.c\" />\nIf you want a dashboard, use a dashboard. But it sounded to me like @shivagopalan was trying to compose a custom web page with multiple graphs.. Eh? http://obfuscurity.com/2012/04/Unhelpful-Graphite-Tip-1. The link was just to demonstrate that you could apply colors to a drawAsInfinite() series. It doesn't address your specific use case.. Some of this might be duplication of content at http://graphiteapp.org/ (https://github.com/graphite-project/graphite-project.github.io), which is fine as long as we keep stuff in sync.. Nope, I've never created anything on dockerhub.\nOn Sun, Mar 26, 2017 at 1:53 PM, Denis Zhdanov notifications@github.com\nwrote:\n\nHello!\nI'm personally not a great fan of Docker, especially on PROD, but I can\nadmit that's a great packer tool, especially for packages with multiple\ndependencies.\nSo, I think we need to create official docker images for 1.0.0 too.\nCurrent issues:\n\nI can't add 'graphite' or 'graphiteproject' organization on\n   dockerhub for some reason. Now I created graphiteapp. @obfuscurity\n   https://github.com/obfuscurity - did you created one? I can't find\n   it using search, though.\nCurrent most popular graphite images which we have on dockerhub:\n   https://github.com/hopsoft/docker-graphite-statsd\n   https://github.com/nickstenning/docker-graphite\n   Should we fork them or create separate one?\nShould we include statsd? or Grafana?\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1874, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAeLAr_2JwJtTyXH23cfNBlb12-3g_X7ks5rpqY3gaJpZM4Mpg7k\n.\n. I'd love to see downloads incorporated into the http://graphiteapp.org/ website. /cc @mattttt @bulletfactory @DanCech. @iksaif It's the https://github.com/graphite-project/graphite-project.github.io project using GitHub Pages. Everyone in @graphite-project/committers has access.. Not sure I understand your question @deniszh.. @mleinart @SEJeff and myself are all wikidot admins for the old Graphite site.. Oracle definitely uses Graphite, but it would be nice to know this is being submitted by an actual Oracle employee.. I'm a very strong :-1: on this.. Having worked with both Graphite and Prometheus in production lately, I\nhave some pretty strong feelings about what a next generation Graphite\nstack would look like. Unfortunately I don't think any of this is an\nevolution of the existing stack.\n\nJason\nOn Sun, Jan 27, 2019 at 11:02 AM Corentin Chary notifications@github.com\nwrote:\n\n\nTarget small to medium installations (big installs can be covered by\n   Metrictank, Biggraphite, and Clickhouse).\n\nI think it would be nice to have something that works easilly for smalls\ninstalls but still can be extended for big installs. Ideally with the same\ncomponents.\n\nOfficially deprecate python carbon daemon and replace it with\n   go-carbon https://github.com/lomik/go-carbon. Please note, that\n   go-carbon is a separate project with own maintainers, not sure should\n   we use it as is and contribute to its development or fork it.\n\nWhich is why I'm a bit afraid about that... Plus it makes it harder to\nshare code between graphite and carbon. But considering the workforce that\nwe have I do not have better suggestions.\n\nGet rid of Django - it was (and still) a constant source of\n   incompatibilities and installation issues. (OTOH - will we have the same\n   issues with e.g. Flask then? Or maybe we just strictly should use LTS\n   versions?)\nGet rid of state in graphite-web - or make it at optional, at least\n   (i.e. separate rendering and dashboards/tree-view, as was done in\n   graphite-api).\n\nI'm all in to simplify graphite-web. It's nice to keep a minimialistic\nconsole view to debug it though but I don't think dashboards are super\nuseful these days.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2418#issuecomment-457930126,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAeLApJHQCRw9wrBv3bmNYBV3CKSDNjPks5vHc2SgaJpZM4aUsjj\n.\n\n\n-- \nJason Dixon\nhttp://obfuscurity.com/\nhttps://twitter.com/obfuscurity\n. I don't really want to elaborate at length because it's always theoretical\nuntil someone scratches that itch and code magically appears. :-P\nLet's just say that I feel pretty strongly that one of Graphite's biggest\nweaknesses is around metrics naming discipline and the lack of\nauth{entication,authorization} around metrics ingestion. If that is\naddressed properly, it would also conceivably lessen the\nimportance/necessity of metrics node manipulation in chained queries.\nJason\nOn Sun, Jan 27, 2019 at 11:23 AM Denis Zhdanov notifications@github.com\nwrote:\n\n@obfuscurity https://github.com/obfuscurity : I'm not sure that we need\nto go that path either, just thinking out loud here. Maybe you can share\nyour thoughts, Jason?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2418#issuecomment-457931896,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAeLAoT5t8ScUUEd1aVgwCuDwsbV73s3ks5vHdJ4gaJpZM4aUsjj\n.\n\n\n-- \nJason Dixon\nhttp://obfuscurity.com/\nhttps://twitter.com/obfuscurity\n. Agreed, easy deployment and sandboxing is a given for me. Getting started\nwith Prometheus is trivial. However it does have a lot of\noperational/ease-of-use trade-offs later on, which I think is where\nGraphite shines by comparison.\nJason\nOn Sun, Jan 27, 2019 at 12:16 PM Piotr Popieluch notifications@github.com\nwrote:\n\nI've got a lot of ideas but it all depends on people having time to\nimplement them, which I don't think anyone has, therefore I don't think it\nmakes much sense to write them all down.\nThis brings me to the point of how to get Graphite more appealing to\ncontribute to for new developers. I think making it easier to install would\nhelp. I still sometimes struggle to get a dev environment working on a\nclean linux install, and I've been using this for over 6 years. I can\nimagine it turns potential contributors off. Other thing is that there is a\nlot of old ugly code...\nI'm not in favour of any big revolutionary changes, Graphite's strongest\npoint imo is that \"it just works\" and has a large user base.\nWhisper has it's downsides, but it is simple and it is still the only\ndatabase backend which I have seen work stable in large installations. Only\nfeature I was missing was automatic rebuilding of failed cluster nodes (but\nthat would probably make it complex and less stable).\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2418#issuecomment-457936372,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAeLArZWFLxIPhe6rTANqqh7L0NHN8EWks5vHd8KgaJpZM4aUsjj\n.\n\n\n-- \nJason Dixon\nhttp://obfuscurity.com/\nhttps://twitter.com/obfuscurity\n. I personally care less about any standardization and am thinking about this\npurely in terms of user experience and system ownership. Yes, OpenMetrics\ncertainly overlaps in many regards but I don't frame the conversation\naround a standard.\nJason\nOn Sun, Jan 27, 2019 at 2:05 PM Denis Zhdanov notifications@github.com\nwrote:\n\n@obfuscurity https://github.com/obfuscurity :\nLet's just say that I feel pretty strongly that one of Graphite's biggest\nweaknesses is around metrics naming discipline and the lack of\nauth{entication,authorization} around metrics ingestion.\nWell, agreed. But please note that even in current state Graphite is\ntechnically ready for OpenMertrics\nhttps://prometheus.io/docs/instrumenting/exposition_formats/#text-based-format\nsupport. OTOH, Openmetrics doesn't have anything about auth part either...\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2418#issuecomment-457944754,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAeLAlwbCKxh-CbVycIIozdJ2xoDgE8yks5vHfh2gaJpZM4aUsjj\n.\n\n\n-- \nJason Dixon\nhttp://obfuscurity.com/\nhttps://twitter.com/obfuscurity\n. P.S. I'm actually glad to see we don't all agree on all of these items but\nthat we're inclusive of differing opinions.\nP.P.S. Nice to be in a place (Obs @ Fastly) where I can be involved in\nthese sorts of discussions again. :)\nJason\nOn Sun, Jan 27, 2019 at 7:54 PM Dan Cech notifications@github.com wrote:\n\nI've been thinking along the same lines @deniszh\nhttps://github.com/deniszh and had some good discussions with @Dieterbe\nhttps://github.com/Dieterbe, I just wish I had more time to dedicate to\nthe project. The biggest thing I'd add to your list would be focusing on\nease of installation and sensible defaults, I know @piotr1212\nhttps://github.com/piotr1212 has been doing some work in that direction\nand ditching django would also help in that area.\n@lomik https://github.com/lomik I'd love to hear your\nthoughts/suggestions/reaction to the idea of adopting go-carbon as the\nofficial daemon\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2418#issuecomment-457970380,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAeLApOtMwnnyVLgGBM4Yn6Q_NgeEIQAks5vHkpLgaJpZM4aUsjj\n.\n\n\n-- \nJason Dixon\nhttp://obfuscurity.com/\nhttps://twitter.com/obfuscurity\n. Let's avoid inline styles going forward. Please remove the STYLE from here and add it to the HEAD styles instead (hint: insert at L38).\n.header .title a {\n  text-decoration: none;\n  color: #fff;\n}\n. Something about this looks weird, but perhaps it's my inexperience with RST. What is the :doc: tag? Why the extra whitespace before it? What's up with the closing </tools> element?\n. Punctuation police here; please end your sentence with a period. :smile_cat: \nHave a good day, sir. :doughnut: \n. whitespace\n. That should be https://github.com/graphite-project/ceres.git.\n. Would it read better as \"defined by a common groupNode and filtered by metricNode\"?\n. That and s/common node/common groupNode/.\n. The phrase \"running a unix as a VM\" seems a bit off to me, not quite sure what you're shooting for here. I'd also capitalize UNIX.\n. Probably want to capitalize UNIX here.\n. That's part of it. I'm also using bad list syntax. I have it working\nnow, about to push up the changes.\nOn Sat, Aug 23, 2014 at 07:39:35PM -0700, mark olson wrote:\n\n\n@@ -341,3 +341,15 @@ def test_invert(self):\n                     continue\n                 expected_value = math.pow(original_value, -1)\n                 self.assertEqual(value, expected_value)\n+\n-    def test_changed(self):\n-        config = [\n-            ([1,2,3,4,4,5,5,5,6,7], [0,0,0,0,1,0,1,1,0,0]),\n-            ([None,None,None,None,0,0,0,None,None,1], [0,1,1,1,0,1,1,0,1,0])\n-        ]\n\nFor why the test is failing: I think that that the first expected values be [0,1,1,1,0,1,0,0,1,1], and the 2nd to be [0,0,0,0,0,0,0,0,0,1] or[0,1,0,0,1,0,0,1,0,1]`, depending on transformNull/drawNullAsZero\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/pull/866/files#r16633904\n\n\nJason Dixon\nhttp://obfuscurity.com/\nhttps://twitter.com/obfuscurity\n. Don't feel bad, it confused me at first too. :smile_cat: \n. We're tracking a bunch (and a planned page for the new website) over at https://github.com/graphite-project/graphite-project.github.io/issues/11.\n. P.S. 160k metrics/minute \"ain't shit\" anymore. I can benchmark 100k metrics/sec on a single i2.4xlarge (non-persistent SSDs), but I'd rather get a number from someone like Booking.com in there, who uses it in a very large production architecture.\n. Agreed, that entire paragraph needs to be modernized imho.\n. I hate to nitpick, but can you elaborate here, i.e. explain the difference between the old and new behavior? TIA\n. That looks perfect, thanks.\n. Can you move this comment to a line above by itself?\n. I think you need an empty line between the bullets and the line before them.\n. Need to remove this.\n. It's a small detail, but would you mind changing those to single-quotes?\n. Just realized the comments were not updated to reflect the removal of total. @cbowman0 would you mind fixing this?\n. Does the indention look off here or is it my imagination?\n. Just noticed this doesn't report the actual metric name. Dang.\n. :100: \n. Shouldn't we include the keySeries in this output, e.g.:\nseries.name = \"transformNull(%s,%g,%s)\" % (series.name, default, keySeries)\n. Some grammatical typos here. Some are inherited from the previous version, some are new. Would you mind fixing these?\nwild card should be wildcard\n. wildcard seriesList\n. Let's switch Nulls to nulls for consistency with the rest of the document.\n. drawNullAsZero flag to the drawNullAsZero function and text only to text-only\n. Yeah, about this. The 0.9.x build is still not working. The master build was fixed but that was all thanks to @brutasse. Either way, I don't think it warrants a mention in the Release Notes since it's technically related to infrastructure.\n. Don't we need to manipulate host here for ipv6?\n. Ok, I'm trusting you here since I have little practical ipv6 experience to compare against. :smile_cat: \n. I don't think that \"uniquified\" is a real word, perhaps \"normalized\" has the same connotation you're looking for?\n. First, a simple nit - this sentence is missing a period at the end.\nI'm pretty sure I grok the intent of this function, but perhaps you could elaborate by giving a series of intermediate examples demonstrating how each series would be interpolated into the templateFunction?\n. \"see a problem and take the time\"\n. \"However, we would love if you could take a moment to search through the existing issues first. It's entirely possible that the issue has already been reported, or even better, fixed and resolved.\"\n. \"When opening a new issue, please write a descriptive report.\"\n. Should we give examples how to identify the version when running from source? (e.g. graphite-web has /version/)\n. \"We are always looking to improve our test coverage. New tests will be appreciated and quickly reviewed for inclusion.\"\n. I don't think we should overemphasize tests, but @SEJeff may disagree with me. :wink: \n. s/is/are/;\n. \"other than the default\"?\n. I just meant that we have a dedicated Tests section literally preceding this one.\n. No, I just meant that it sounds weird to me without the word \"other\" in there.\n. Do we want to leave this cruft?\n. Same here.\n. Nice catch here.\n. Is this still a WIP?\n. No need to apologize, your work here is greatly appreciated.\n. Add a period here?\n. Do you think we should expand on this for people who might not read ahead to what consolidateBy does?\n. \ud83d\udcaf \n. Yeah I'm not a dygraph user so I assumed \ud83d\ude32 the contributor knew how many significant digits to use.\n. @brutasse I definitely prefer your approach (works fine here) but while the XHR request comes back sorted properly, the UI displays them in descending order. Easy fix, I'll open a separate PR for the full change.\n. Note that this was addressed in https://github.com/graphite-project/graphite-web/pull/1567.\n. @cbowman0 Any idea why this keeps returning one match instead of zero? The curl equivalent of this test works as expected so I'm not sure why it keeps returning an unwanted result.\n. You're right, my CLI tests were invalid. It does indeed return a set even with unknown tags. I'll look deeper into this today, may require another patch to tagging.\n. Shouldn't we check for isdir here too?\n. Ah ok, missed that.\n. Why import all of urllib2 when we're still only using urllib2.urlopen?\n. Note that from urllib2 import urlopen works fine for me. I've used index.json extensively in the past and it's never been an issue afaik.\n. Shouldn't this be >=? Technically there's no reason why you couldn't have a single CLUSTER_SERVERS on the backend.\n. Ended up importing all of urllib2 so we can raise an exception too.\n. Note this should be delay instead of scale here. I'll fix this in my branch.\n. Also missing the delay() in the expected TimeSeries name. Fixing.\n. Good catch, that's an artifact from when I listed the members explicitly.\n. @cbowman0 You're a scholar and a gentleman. I'll wait for you to take a closer look later.\n. Great comment here, thanks.\n. Did you mean to pull these in too?\n. Oops, disregard. I see your explicit commit. :smile: \n. Is the trailing comma intentional?\n. Please move this down in the correct alphabetical order (below Shinken).\n. Would be useful to link to the external project/docs. Also, what does instance refer to here?\n. Can we get an explicit list of supported values?\n. s/support of/support for/\n. We should document all supported values.\n. I would prefer an explicit if else here unless this is considered more pythonic.\n. I would propose this instead, also removing the 2nd paragraph:\nThe default `carbon_ch` is Graphite's traditional consistent-hashing implementation. Alternatively, you can use the `fnv1a_ch`, which supports the Fowler\u2013Noll\u2013Vo hash function (FNV-1a) hash implementation offered by the `carbon-c-relay relay <https://github.com/grobian/carbon-c-relay>`_ project.\n. Let's take a similar approach here to what I suggested above.\n. Sorry, one more fix (my fault)... s/you can use the/you can use/. Also in the settings example below. Thanks!\n. Agreed, but then let's go ahead and make it a code-block.\n. Almost. Need to remove those trailing backticks.\n. Minor nit, adding a space after the #.\n. See https://github.com/graphite-project/graphite-web/pull/1726 for why this is the way it is.. The behavior has not changed with regards to querying. You still separate tags in the events() function with a space. We should remove this and just elaborate on L36, e.g. \"a combination of tags separated by whitespace\".. Ok, this still needs to be cleaned up. I really dislike that sentence being slapped onto the set paragraph, it doesn't belong there. And I don't think we need that Note since it's referring to the output. Nowhere else do we explicitly document the format of each key/value output. As long as we have an accurate example (which we do, here), I don't see the point.\nTL;DR please remove the changes in this snippet.. Would this be more clear as Total rendering time?. lenght. I don't think we use \"stat\" anywhere else, perhaps \"metric\" here?. ",
    "thesamet": "The web UI uses Django's default time zone. You can override it in webapp/local_settings.py\n. Sorry, I missed that in your original post. I just came across this bug\nwhile trying to fix the same problem for myself. For me, setting the\nTIME_ZONE variable in local_settings.py and restarting the stack was enough.\nOn Tue, Sep 25, 2012 at 8:58 AM, Dieter Plaetinck\nnotifications@github.comwrote:\n\n@thesamet https://github.com/thesamet there is no\nwebapp/local_settings.py, only webapp/graphite/local_settings.py and like I\nsaid in my opening post, I configured TIME_ZONE in there.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/issues/19#issuecomment-8859543.\n\n\n-Nadav\n. ",
    "chino": "What about changing it for graphlot ?\nOr how do I force graphlot to notice the change ?\nOn Mon, Sep 24, 2012 at 11:58 AM, Nadav Samet notifications@github.comwrote:\n\nThe web UI uses Django's default time zone. You can override it in\nwebapp/local_settings.py\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/issues/19#issuecomment-8823955.\n. Restarting Apache with wsgi setup should work I would imagine?\nOn Sep 25, 2012 12:13 PM, \"Nadav Samet\" notifications@github.com wrote:\nSorry, I missed that in your original post. I just came across this bug\nwhile trying to fix the same problem for myself. For me, setting the\nTIME_ZONE variable in local_settings.py and restarting the stack was\nenough.\nOn Tue, Sep 25, 2012 at 8:58 AM, Dieter Plaetinck\nnotifications@github.comwrote:\n\n@thesamet https://github.com/thesamet there is no\nwebapp/local_settings.py, only webapp/graphite/local_settings.py and\nlike I\nsaid in my opening post, I configured TIME_ZONE in there.\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/graphite-project/graphite-web/issues/19#issuecomment-8859543>.\n\n\n-Nadav\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/issues/19#issuecomment-8860064.\n. \n",
    "rorist": "Hi there, \nI've applied this patch to actually display the date passed in argument..\nHere it is in 47649448255ac8f43f1133651bf91b659f1d2758\nRegards.\n. Hello, \nff8e1a5 is problematic as stated, when you save a graph with a specific date range it replaces the %3a (\":\") which is then loaded as %253a and makes the date invalid.\nmleinart@1985a89 fix it all for me and this particular issue, on my 0.9.x version.\nThanks\n. ",
    "supre": "Is there a way to find out active metrics and not all metrics. We have a lot of metrics which we don't use anymore but they do appear in the list with /metrics/find?query=*. I only want to see the metrics that are being actively used right now. \n. ",
    "keen99": "@supre http://graphite-api.readthedocs.io/en/latest/api.html#the-metrics-api and  https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/metrics/views.py#L87 suggests the from parameter, fwiw.\n. this appears to have been fixed in https://github.com/graphite-project/graphite-web/commit/1a826949732c394b03c7e2e94e65af82762b9103 btw - april 2012. :)\n. ",
    "sidnei": "A bigger issue is that if you scale out your frontend by running multiple graphite-web instances doesn't make loading graphs faster, because even though Apache has spare cycles to burn there's an upper limit in parallel requests from the same domain.\nWhat I'd really like to see is to have alias domains for the same graphite instance configurable in settings.py, and have graphite shard the requests across those aliases, to get around the limitation of parallel requests that the browser has (eg: http://yuiblog.com/blog/2007/04/11/performance-research-part-4/)\n. Cool, thanks for catching that.\n. Looks correct to me (btw, when are we going to start adding tests?) :+1: \n. Since write_index defaults to settings.CERES_DIR if it's unset, you could default to None here instead.\n. Same here.\n. Seems like DJANGO_SETTINGS_MODULE was not required by the previous script, so instead of silently defaulting it here, I suggest complaining if it's unset instead.\n. This leaks an fd. Instead of throwing it away and using the name only, use:\nfd, tmp = mkstemp()\n  ...\n  tmp_index = os.fdopen(fd, 'wt')\n  ...\n  finally:\n     tmp_index.close()\n     os.unlink(tmp)\n. Ok, that's fine then.\n. I understand. OTOH, if you have a custom DJANGO_SETTINGS_MODULE and you don't set the environment variable pointing to it, it will silently use graphite's default paths, which will be wrong, so if you are running this script from cron and upgrade it will silently break.\nWhat I'm proposing is that you check for 'if \"DJANGO_SETTINGS_MODULE\" in os.environ' as the very first thing in the script, before trying to import from graphite. If it's unset, complain loudly and exit.\n. ",
    "luciotorre": "looks good.\n. ",
    "mattpascoe": "I can confirm having this same problem.  Fix please and thank you!\n. Here is the query I have to generate the images below.  The first image is basically how I use it.  You can see the small red bumps in the lower line.  The dark line is over top of it.  When the line width is 1 its mostly fine (unless the red line was shallower).  I increase the line width to help show the issue better in the second image.  If course I wont be using a linewidth of 5 in normal use but it would be nice to not have the horizontal line at the bottom at all for the drawasinfinite lines.  Hope that helps.\nhttp://graphite/render?from=-360minutes&until=11:04_20130426&width=500&height=250&bgcolor=white&fgcolor=black&majorGridLineColor=darkgrey&drawNullAsZero=1&title=Title&yMin=0&target=cactiStyle(alias(color(maxSeries(requests.mean),'blue'),'Mean'))&target=cactiStyle(alias(color(maxSeries(requests.p_98),'green'),'98th'))&target=cactiStyle(alias(color(maxSeries(requests.upper),'red'),'Worst'))&target=alias(color(dashed(constantLine(1000),10),'orange'),'1s+SLA')&target=alias(color(dashed(constantLine(2000),10),'red'),'2s+SLA')&fontBold=true&target=lineWidth(alias(color(drawAsInfinite(events('*')),%20'black'),'Events'),5)\n\n\n. Here you go\nevents(*),1367005515,1367005815,1|None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None,None\n. I built it from github master branch awhile back.  It reports itself as 0.10.0-alpha.. Looks like it was around Oct 17 2012 timeframe.\n. If this problem seems to be fixed in current branches then I'd say we can close it.  I'm certainly on older code and have not had opportunity to update.  If I find that the issue still exists when I do upgrade then I could reopen the issue.  It's not a show stopper for me, just wanted it out there for someone to look at.  Thanks all.\n. I have re-confirmed that this looks to be a problem still in the 0.9.12 code.  It should be simple to reproduce by adding a few events and then making a graph with the following target \ntarget=lineWidth(alias(color(drawAsInfinite(events('*')),%20'black'),'Events'),10)\nEDIT:  I just noticed that it has something to do with drawnullaszero=1... I had that in my urls and when I remove it the horizontal line goes away.\n. ",
    "pcn": "Has anyone taken a look at this yet?  It's been really useful here (I work with jordanlewis) and it seems like a clear win to add it.\n. What would be the impact if we coudln't say:\npython\nif is_leaf(node):\n    [blah]\nelse:\n    [is a branch]\nIt seems like it'd mean re-defining a node to have a property, meaning \"contains_data\" to separate the purpose of navigating the tree from returning data.\n. :heart:\nDoes this rely on any other PRs?\n. +1 on including it.\n. This looks like it should work.  Tests would be appreciated for this case.\n. Re: https://github.com/graphite-project/graphite-web/issues/428\n. Can this be 2 PRs?  One for the curlies, and 1 for the **.  I'm completely on board with expanding {..}, but I'm not sure about modifying the finder in a way that is inconsistent between ceres and whisper and rrd if there's a way to avoid it.\n. +1 to making the install process simpler.\n. The [carbon] section matches metrics starting with \"carbon\". The match is based on the pattern.  If you're not familiar with regular expresions, the \"^\" means \"at the beginning of a string\" so it matches strings that start with \"carbon\".  This is for internal metrics relating to graphite.\nThe [default_1min_for_1day] section matches everything else.  The regular expression \".\" means any character (the \".\") zero or more times (the qualifier \"\" gives it this meaning).\nYou could keep a 1 minute resolution for new metrics by changing the default to \"60s:180d\" if you're OK with 6 months being 180 days, which is more-or-less approximately right.\nHowever, metrics that you've already created will need to have their whisper files altered since carbon-cache doesn't change retention info, it only sets it when it's created.  You can either delete the existing files, or you can use the whisper-resize script that comes with the whisper package to resize the file that you need.\n. Makes sense.\n. Thanks. :thumbsup: from me.\n. Please ask for troubleshooting help on IRC, that'll get you results in real-time.\n. OK, so we need to look into https://docs.djangoproject.com/en/1.5/topics/cache/ (for instance - it looks like 1.4, 1.5, and development 1.6 haven't introduced changes in the docs that stand out from a brief glimpse).\n. If I'm reading the configuration correctly, using django >= 1.5, this configuration should work:\npython\nCACHE_MIDDLEWARE_SECONDS = 60\nCACHES = {\n    'default': {\n        'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',\n        'LOCATION': [\n            '127.0.0.1:11211'\n        ]\n    }\n}\nMy vagrant dev environment is not in any shape to test this.  @msn if you have a chance to try this that'd be helpful.\n. :+1:\n. @drawks you know @brutasse - he loves to build up the suspense :smile:\nJust kidding!\n. @kds119 my friend @roymarantz had the interesting idea of pointing kairosdb at an opentsdb implementation to make it read-only.\nI've started a personal branch on master to try that out, but I need his tsdb installation to try it out.\n. This is a valid question about graphite, but it is not an issue (e.g. a bug, or documentation that leads to a non-functioning system).  Since it's not an issue, I'm going to close this issue.\nPlease ask this question on the mailing list or in IRC.\n. The easiest suggestion is to fail if libcairo.so.2 can't be loaded.  Adding \ncdll.LoadLibrary('libcairo.so.2')\nas a test should determine whether it's installed correctly, failure can be dealt with by printing the instructions for the specific distro/platform.\n. Has anyone taken a stab at the modularization?\n. :+1:\n. Can you add an example of what the expected input and output is to illustrate the use of this for the reader?\n. ",
    "daveconcannon": "We've merged this into hostedgraphite.com without issue, works well.\n. ",
    "jordanlewis": "great news! thanks.\n. ",
    "garo": "I've just traced the issue a bit. Here's the steps which I found will reproduce the bug:\nCreate a graph with two items. Apply alias(your.item, \"name with %\") to both the items. If you save that graph it wont be loaded because the browser will crash into a Malformed URI exception in composer.js at line\nvar params = Ext.urlDecode( this.queryString );\nThere's the setQueryString function in the composer.js, which has this line:\nthis.queryString = qs.replace(/%/,\"%25\").replace(/#/,\"%23\");\nnow the problem looks like that the replace functions will replace only the first occurrence of the characters. I rewrote it to use the g-modifier:\nthis.queryString = qs.replace(/%/g,\"%25\").replace(/#/g,\"%23\");\nand that looks like it fixes the problem so that the graph wont load. \n. There's similar problem in the Dashboard.\nfile dashboard.js line 949 can be rewritten to:\nvar fullUrl = decodeURIComponent(targetUrl).replace(/%/g,\"%25\").replace(/#/g,'%23');\nand it now allows to load the saved graph I described in my previous comment to this issue.\n. ",
    "diegovar": "I believe the decodeUriComponent of the targetUrl shouldn't be there. The url in the JSON object is not encoded per se, only its querystring parameters are. Removing the decodeURIComponent part fixes the fact that trying to load any saved graph whose targets include the \" character fails.\n. This is related to https://github.com/graphite-project/graphite-web/issues/101\n. Can't do so easily as I'm no longer working with Graphite, sorry. \n. It's the version of django that comes with graphite 0.9.10.\n. ",
    "mzupan": "found a bug... closing till i fix it\n. Do we want to have the files live in the same directories across platform or stick to debian/ubuntu standards vs redhat standards. Sometimes they don't differ much but mainly configs they differ. \n. ",
    "kurtabersold": "I stumbled on this trying to get around having certain datapoints in the legend. After striking out with the docs, I just started trying stuff. Using the alias function with a blank string seemed to do the trick.\nalias(someArbitraryData),\"\")\n. ",
    "cbowman0": "Yes.\n. This appears to fix it in my testing:\npython\ndiff --git a/webapp/graphite/render/functions.py b/webapp/graphite/render/functions.py\nindex ab258a8..9aa7f6c 100644\n--- a/webapp/graphite/render/functions.py\n+++ b/webapp/graphite/render/functions.py\n@@ -1989,7 +1989,7 @@ def constantLine(requestContext, value):\n   start = timestamp( requestContext['startTime'] )\n   end = timestamp( requestContext['endTime'] )\n   step = (end - start) / 2.0\n-  series = TimeSeries(str(value), start, end, step, [value, value])\n+  series = TimeSeries(str(value), start, end, step, [value, value, value])\n   return [series]\n. RemoteReader.fetch() has pieces that don't make logical sense.\nThe clean_cache() routine only cleans entries when the cache reaches capacity.  The check to preemptively return data out of the cache doesn't check if the data is old.\nIf a cached entry is found, it is pulled apart and returned as (time_info, series['values']).  In other parts of the routine the return value is the entire contents of the specific entry in the cache.\nThe connection object is not shared between threads.  The same thread that gets the request_lock is not guaranteed to get the wait_lock.  \nThe net of the above is something is causing the completion event to not fire before the timeout (which I set to 60 seconds during some of my testing).\n. The cache is ok, since the urls to the remote hosts includes the unix timestamp that the query is for.\nAnd missed the call to extract_my_results in the return of FetchInProgress.  The duplication of that code is confusing.\nStill tracking down the thread synchronization.\n. The log event is:\nMon Aug 27 21:28:33 2012 :: Wait For Results - Error requesting http://xx.xx.xx.xx/render/?target=%2A.carbon-daemons.%2A.cache%2A.writer.write_ratelimit_exceeded&format=pickle&local=1&noCache=1&from=1346016512&until=1346102912\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/remote_storage.py\", line 233, in wait_for_results\n    response = self.connection.getresponse()\nAttributeError: 'NoneType' object has no attribute 'getresponse'\nLine number doesn't match the git repo.  Line 233 is inside of wait_for_results():\n          response = self.connection.getresponse()\n. It appears that adding in an Event() for the connection completing resolves this.\nI currently also have a connection dictionary as a part of the RemoteReader class.  Since I'm not convinced that there's a guarantee that the same thread that made the request is the one that will get the write lock.\nIf this runs through tomorrow then I'll commit a fix in my branch.\n. I cherry picked this into my branch and it appears to be usable after these two commits:\nhttps://github.com/cbowman0/graphite-web/commit/bb776c6f3fdb0e49d32fbc11f2bc8d37c00e9045\nhttps://github.com/cbowman0/graphite-web/commit/a5efba47079f9968facb4b331ab8b074ada268e3\nI will be testing more.\n. Merged into master via #1639.\n. Changed applied to master in #2100.. This fixes it in my branch: https://github.com/cbowman0/graphite-web/commit/cb89c8249c2ad1ad6beae942c017564ad6d7fb4b\n. Looks like this also covers #405 \n. Thanks @jamesjuran.\n. You can have multiple target entries ( target=abs(...)&target=derivative(...) ).  Are you restricted to only one target=?\n. I do not know.\nMy pull requests are just sitting with no input from people with commit permissions.  I'd be happy to make changes to get things merged if that's what is required.\n. This was rebased against master.  Please let me know if there's anything else that's needed.\n. Your initial response made no indication that the existing state with regards to the flot libraries was known but subpar.  I was not aware the existing setup had caused such distress and if I had known this I wouldn't have proposed the change I did.\nBased on the tone of the first response and the closing of the pull request, I am unsure if this feature as a whole is wanted.  Can you clarify if resolving the jquery/flot situation will allow this to move forward or if you are unilaterally declaring this feature unwanted.\n. Ok.  I suspected as such.\nWhere is the larger discussion going to happen about the dependency/inclusion stuff?  Do I need to start that discussion or will it happen in the background with the other project leaders and you'll come back to this pull request when the decision has been made?\n. I do not mind.\n. Let me know if that works for you.\n. Welp, this is useless now.  Closing to clear the noise.\n. @SEJeff I did a rebase.\n. Ok.  I did that by adding an optional config file with options, such as this one, to influence the rendering.  I'll submit a pull request for that now.\n. For @obfuscurity: \n\n. Looks like it's 4 space instead of 2 space.  I'll correct it.\n. Can I get a peer review of this?\n. Do you mean in the pop-up after clicking a graph, choosing new target and it adds a blank target line?\n. I'm not sure where to start with adding auto-completion to the add new target box.  Do others believe this is required to merge this in?. @DanCech or @deniszh, any objections to merging this as is?. #2097 created.  Merging now.. @Mariano-gon Take an existing dashboard from your environment.  Replace some part(s) of the target(s) with VALUE and save it as a Template.  Then load the template and it'll replace the VALUE with the value you provide on load.\n. I made the class syntax change as requested and rebased against master.\n. Rebased against master.\n. It made sense to me 1.5 years ago when I wrote it up.  I'm attempting to remember that logic.  From what I recall, there aren't any similar items in settings.py or local-settings.py that manipulate content, so I mimicked graphTemplates.conf and dashboard.conf.  Whether that made sense then or makes the most sense now, I'll let you guys tell me.  If the consensus is we put them in settings.py, then that's a-ok with me.\n. I cannot reproduce on master.\n. Does #1565 resolve this?\n. We don't have tests for raw output?  Bummer.  I took a stab at it in #1624.\n. Does this improve upon the fetch call immediately returning with a callback method that is implemented in master?\n. I capture network traffic to confirm what I believed to be happening in master right now.\nThis was run with one graphite-web instance with no local data and 9 servers holding data.  All running the master branch of graphite-web.  The /metrics/find requests happen in quick succession without waiting for a response from each server.  The responses are then processed before issuing the /render requests to get data, which happen in the same method.  Here's timestamps and action (manually pulled together for readability):\n```\n15:01:38.771883 IP ui_instance.48944 > datastore_1.http: GET /metrics/find/?local=1&format=pickle&query=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&from=1471531598&until=1471532498 HTTP/1.1\n15:01:38.773039 IP ui_instance.41471 > datastore_2.http: GET /metrics/find/?local=1&format=pickle&query=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&from=1471531598&until=1471532498 HTTP/1.1\n15:01:38.774019 IP ui_instance.36074 > datastore_3.http: GET /metrics/find/?local=1&format=pickle&query=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&from=1471531598&until=1471532498 HTTP/1.1\n15:01:38.774955 IP ui_instance.54046 > datastore_4.http: GET /metrics/find/?local=1&format=pickle&query=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&from=1471531598&until=1471532498 HTTP/1.1\n15:01:38.775897 IP ui_instance.45051 > datastore_5.http: GET /metrics/find/?local=1&format=pickle&query=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&from=1471531598&until=1471532498 HTTP/1.1\n15:01:38.777053 IP ui_instance.52152 > datastore_6.http: GET /metrics/find/?local=1&format=pickle&query=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&from=1471531598&until=1471532498 HTTP/1.1\n15:01:38.778014 IP ui_instance.46540 > datastore_7.http: GET /metrics/find/?local=1&format=pickle&query=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&from=1471531598&until=1471532498 HTTP/1.1\n15:01:38.778999 IP ui_instance.48222 > datastore_8.http: GET /metrics/find/?local=1&format=pickle&query=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&from=1471531598&until=1471532498 HTTP/1.1\n15:01:38.780049 IP ui_instance.39282 > datastore_9.http: GET /metrics/find/?local=1&format=pickle&query=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&from=1471531598&until=1471532498 HTTP/1.1\n15:01:39.469023 IP datastore_2.http > ui_instance.41471: Find Response\n15:01:39.506322 IP datastore_8.http > ui_instance.48222: Find Response\n15:01:39.506967 IP datastore_9.http > ui_instance.39282: Find Response\n15:01:39.507394 IP datastore_5.http > ui_instance.45051: Find Response\n15:01:39.508467 IP datastore_1.http > ui_instance.48944: Find Response\n15:01:39.521411 IP datastore_3.http > ui_instance.36074: Find Response\n15:01:39.557917 IP datastore_6.http > ui_instance.52152: Find Response\n15:01:39.558094 IP datastore_7.http > ui_instance.46540: Find Response\n15:01:39.560281 IP datastore_4.http > ui_instance.54046: Find Response\n15:01:39.573574 IP ui_instance.52156 > datastore_6.http: GET /render/?target=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&format=pickle&local=1&noCache=1&from=1471531598&until=1471532498 HTTP/1.1\n15:01:39.574377 IP ui_instance.36082 > datastore_3.http: GET /render/?target=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&format=pickle&local=1&noCache=1&from=1471531598&until=1471532498 HTTP/1.1\n15:01:39.575092 IP ui_instance.39285 > datastore_9.http: GET /render/?target=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&format=pickle&local=1&noCache=1&from=1471531598&until=1471532498 HTTP/1.1\n15:01:39.575997 IP ui_instance.41482 > datastore_2.http: GET /render/?target=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&format=pickle&local=1&noCache=1&from=1471531598&until=1471532498 HTTP/1.1\n15:01:39.576893 IP ui_instance.48228 > datastore_8.http: GET /render/?target=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&format=pickle&local=1&noCache=1&from=1471531598&until=1471532498 HTTP/1.1\n15:01:39.577581 IP ui_instance.45061 > datastore_5.http: GET /render/?target=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&format=pickle&local=1&noCache=1&from=1471531598&until=1471532498 HTTP/1.1\n15:01:39.578506 IP ui_instance.46549 > datastore_7.http: GET /render/?target=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&format=pickle&local=1&noCache=1&from=1471531598&until=1471532498 HTTP/1.1\n15:01:39.579782 IP ui_instance.48960 > datastore_1.http: GET /render/?target=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&format=pickle&local=1&noCache=1&from=1471531598&until=1471532498 HTTP/1.1\n15:01:39.583148 IP ui_instance.54060 > datastore_4.http: GET /render/?target=carbon.carbon-daemons.%2A.cache%2A.writer.datapoints_written&format=pickle&local=1&noCache=1&from=1471531598&until=1471532498 HTTP/1.1\n15:01:40.545555 IP datastore_9.http > ui_instance.39285: Render Response\n15:01:40.551027 IP datastore_8.http > ui_instance.48228: Render Response\n15:01:40.552385 IP datastore_7.http > ui_instance.46549: Render Response\n15:01:40.695801 IP datastore_5.http > ui_instance.45061: Render Response\n15:01:40.696058 IP datastore_6.http > ui_instance.52156: Render Response\n15:01:40.759602 IP datastore_4.http > ui_instance.54060: Render Response\n15:02:18.642834 IP datastore_3.http > ui_instance.36041: Render Response\n```\nSo, data to represent that the code in master does issue requests without waiting for the response.\n. I rebased this in #1639.  Very minor changes needed and those were primarily in test_finders, since that file had some structural changes.\n. #1639 was merged.\n. Are you using the Ceres or Whisper file format?\n. This applies cleanly now and has test cases.  Is this the acceptable solution to this?\n. I agree.  The two storage finders in the tree perform differently.  I don't currently see a universal approach that doesn't have performance implications for one or the other.\n. What's our direction on this now?\n. I believe it already works with the standard finder.  I have dashboards that utilize braces via the standard finder which failed to load when I was testing Ceres a year ago.\n. OK.  Sure.\n\n. I have noticed that the Y-Axis has an alignment issue between the 0 and other listed values:\n\n. I tried to boil down the differences in label layout before/after this change.\nNo patch:\n\nWith patch:\n\nExisting has some inconsistencies:\n1. In graph a between <1 and >=1 values.\n2. In graph b when the label has a prefix and when it doesn't.\n3. In graph d, the prefix has no spacing after the prefix.\nI think there's an opportunity to normalize the spacing between the label and the graph.\nWith these changes to your makeLabel:\n--- glyph.py    2016-05-12 14:29:26.415428819 +0000\n+++ glyph.py.bak    2016-05-12 14:28:22.361647434 +0000\n@@ -226,19 +226,17 @@\n   def makeLabel(self, value):\n     value, prefix = format_units(value, self.step, system=self.unitSystem)\n     span, spanPrefix = format_units(self.span, self.step, system=self.unitSystem)\n-    if (prefix != ''):\n-      prefix = \"%s \" % prefix\n     if value < 0.1:\n       return \"%g %s\" % (float(value), prefix)\n     elif value < 1.0:\n       return \"%.2f %s\" % (float(value), prefix)\n     if span > 10 or spanPrefix != prefix:\n       if type(value) is float:\n-        return \"%.1f %s\" % (value, prefix)\n+        return \"%.1f %s \" % (value, prefix)\n       else:\n-        return \"%d %s\" % (int(value), prefix)\n+        return \"%d %s \" % (int(value), prefix)\n     elif span > 3:\n-      return \"%.1f %s\" % (float(value), prefix)\n+      return \"%.1f %s \" % (float(value), prefix)\n     elif span > 0.1:\n       return \"%.2f %s \" % (float(value), prefix)\n     else:\nThe labels now look like:\n\n. Thanks @obfuscurity .\nCan you weigh in on @mhagger most recent questions about presentation?\n. I was asking about the aesthetics preferences on the graph images.\n. This appears to have been corrected via #1487.  Closing this request because of that.\n. Anyone have thoughts on this?  We've been running this for a few years without any complaints from my users.\n. OK to push this?\n. Based on how the changes to countSeries() are written, this is what will work for a test:\n```\n--- a/webapp/tests/test_functions.py\n+++ b/webapp/tests/test_functions.py\n@@ -1559,6 +1559,18 @@ class FunctionsTest(TestCase):\n         result = functions.countSeries(request_context, seriesList)\n         self.assertEqual(result, expectedResult)\n\ndef test_empty_countSeries(self):\nexpectedResult = [\nTimeSeries('0',0,600,300,[0,0,0]),\n]\n+\nrequest_context = {\n'startTime': datetime(1970, 1, 1, 0, 0, 0, 0, pytz.timezone(settings.TIME_ZONE)),\n'endTime': datetime(1970, 1, 1, 0, 10, 0, 0, pytz.timezone(settings.TIME_ZONE)),\n}\nresult = functions.countSeries(request_context)\nself.assertEqual(result, expectedResult)\n+\n\n```\n. To expand, it expects there are NO passed in seriesList.  Also, without resetting name on the new series, it'll keep the name from constantLine, hence the '0' and constantLine() uses the startTime and endTime of the request context to build the step.\n. I know this exact problem.  This is the patch on master that deals with it by querying all caches: https://github.com/graphite-project/graphite-web/commit/48bbfbe073df7852625b9462907ac56f9d65a297\n. I will attempt to get test coverage on these once I work out how to simulate the request object.  If someone has any ideas, please let me know.\n. @obfuscurity I'll let you know early next week.  I deployed the changes to the primary cluster today.\n. No complaints from my users.\n. Any objections to merging this?\n. I have no concerns with it.\n. I have something going for this, but to really test the edge cases more will be necessary.\nAlso, some views in graphite.metrics don't seem to work with Django 1.9.  I'll be pushing a fix after I transfer trains.\n. grumble returned json isn't sorted.\n. I think this is as far as I'm going to get for now.  I'm ready to merge after someone else has had a chance to look over it.\nThere's a few cases (get/set metadata) where I don't know how to invoke a valid request.  I have not deduced a valid key from a short amount of digging.\nOverall, I think I have covered the majority of the requests variations, but that's where the coverage may be lacking so if someone sees something that was missed let me know.\n. I don't want to leave those bits in\nI was abusing the pull request to get codecov.io to show what code paths are not covered, but I've now worked out how to generate that data locally.  I'll push an update within a day or 2 that includes those checked flushed out or removed.\n. Ok.  I finished this round.  Please confirm this is ok to merge.\n. @JeanFred Is this more in line with best practice?\n. I haven't a clue why the lint checks are failing.  I didn't touch anything in settings.py.\nWould declaring those two variables outside of the conditionals work?\n```\n@@ -211,6 +211,7 @@ if 'sqlite3' in DATABASES.get('default',{}).get('ENGINE','') \\\n   DATABASES['default']['NAME'] = join(STORAGE_DIR, 'graphite.db')\n# Caching shortcuts\n+CACHES={}\n if MEMCACHE_HOSTS:\n     CACHES['default'] = {\n         'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',\n@@ -223,6 +224,7 @@ if MEMCACHE_HOSTS:\n if USE_LDAP_AUTH and LDAP_URI is None:\n   LDAP_URI = \"ldap://%s:%d/\" % (LDAP_SERVER, LDAP_PORT)\n+AUTHENTICATION_BACKENDS=[]\n if USE_REMOTE_USER_AUTHENTICATION or REMOTE_USER_BACKEND:\n   MIDDLEWARE_CLASSES += ('django.contrib.auth.middleware.RemoteUserMiddleware',)\n   if REMOTE_USER_BACKEND:\n```\n. Thanks @JeanFred \nI suspect it's the update to pyflakes (1.0.0 -> 1.2.3) that caused those errors to now show up.  flake8 assigned the new pyflakes warnings with error code F405.  I was able to reproduce locally by removing the .tox dir in my tree.  This is a rabbit hole of fun...\n. This fixes issues seen in #1544 with newer versions of flask8/pyflakes.\n. @mhagger Do you have feedback on these unit tests of some of the changes you recently made to render/glyph.ph?\n@graphite-project/committers Feedback?\n. Thank you, @mhagger.\nI'm going to close this pull request and will open a new one later with the suggested changes and some visual confirmations.\n. OK.  I'm done with correcting those remaining items.  Can someone else look it over?\n. Yes.  Agreed on extra people checking out #1548 .  That's some code that others should understand.\n. I'll check it out.\n. This looks good to me.\n. I gave this a test drive on my cluster and saw no issues with it.  I sporadically have the errors described and agree with the logic here to fix them.\n. Graphs with these changes:\n\nGraphs without these changes:\n\n. Rebased against master and manually merged the tests that were recently merged to master and that were duplicated in this branch.\n. I believe the tests for movingAverage() that were added as part of #1529 cover these changes.\n. What uses the lists that graphite-web is able to add/remove from in graphite.whitelist?\nI don't find anything that interacts with them besides the add/remove urls in composer.  I find mentions  of the whitelist lists in carbon, but didn't find where carbon uses the data (USE_WHITELIST and whitelist.conf and blacklist.conf are there, but different from what I can see)\nBesides that, I don't believe the code in master works.  The load_whitelist() method calls unpickle.load on a file handle.  The unpickle class has no load method and the only method it does have is loads and that don't take a file handle as an argument.\nDo we need the whitelist endpoint in graphite-web?\n. @gwaldo can you rebase from master?  I can help resolve any issues with the unit tests after that, unless you wish to do them yourself.\n. I adjusted the testing to be deterministic.  I would like someone else from @graphite-project/committers to confirm the changes to load_whitelist() is ok.\nOnce that is merged, I will help @gwaldo with adjusting the test cases to work with his wording changes, if required.\n. @obfuscurity Because it currently doesn't work.  Calls to load_whitelist() error with:\nAttributeError: type object 'SafeUnpickler' has no attribute 'load'\n. Alternatively, we can do:\nbuffer = open(settings.WHITELIST_FILE, 'rb').read()\n   whitelist = unpickle.loads(buffer)\nand add set to the PICKLE_SAFE set.  I can do that quickly, if that's preferred.\n. Done.  Once automated checks go green again, I'll merge it.\n. I cherry-picked those changes to get us consistency between 0.9.x and master in #1579 .  If those get merged, then I'll rebase this against that and verify this still runs clean.  Although, the differences between 0.9.10 and master is minimal, so I'm not sure where some issue would occur with leap-year...\n. There are additional differences between master and the 0.9.x branch.  Most are innocuous, but there are a few that should be analyzed.\n```\n$ git diff upstream/0.9.x graphite/render/attime.py | cat\ndiff --git a/webapp/graphite/render/attime.py b/webapp/graphite/render/attime.py\nindex 10405e3..9f9845e 100644\n--- a/webapp/graphite/render/attime.py\n+++ b/webapp/graphite/render/attime.py\n@@ -12,30 +12,24 @@ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\"\"\"\n+import pytz\n from datetime import datetime,timedelta\n from time import daylight\n-from django.utils import timezone\n-\n-try: # See if there is a system installation of pytz first\n-  import pytz\n-except ImportError: # Otherwise we fall back to Graphite's bundled version\n-  from graphite.thirdparty import pytz\n-\n+from django.conf import settings\nmonths = ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']\n weekdays = ['sun','mon','tue','wed','thu','fri','sat']\n-def parseATTime(s, tzinfo=None, now=None):\n+def parseATTime(s, tzinfo=None):\n   if tzinfo is None:\n-    tzinfo = timezone.get_current_timezone()\n-\n+    tzinfo = pytz.timezone(settings.TIME_ZONE)\n   s = s.strip().lower().replace('_','').replace(',','').replace(' ','')\n   if s.isdigit():\n     if len(s) == 8 and int(s[:4]) > 1900 and int(s[4:6]) < 13 and int(s[6:]) < 32:\n       pass #Fall back because its not a timestamp, its YYYYMMDD form\n     else:\n       return datetime.fromtimestamp(int(s),tzinfo)\n-  elif ':' in s:\n+  elif ':' in s and len(s) == 13:\n     return tzinfo.localize(datetime.strptime(s,'%H:%M%Y%m%d'), daylight)\n   if '+' in s:\n     ref,offset = s.split('+',1)\n@@ -50,6 +44,7 @@ def parseATTime(s, tzinfo=None, now=None):\ndef parseTimeReference(ref):\n   if not ref or ref == 'now': return datetime.now(pytz.utc)\n+\n   #Time-of-day reference\n   i = ref.find(':')\n   hour,min = 0,0\n@@ -109,7 +104,7 @@ def parseTimeReference(ref):\n     elif ref[-1:].isdigit():\n       refDate = refDate.replace(day= int(ref[-1:]))\n     else:\n-      raise Exception, \"Day of month required after month name\"\n+      raise Exception(\"Day of month required after month name\")\n   elif ref[:3] in weekdays: #DayOfWeek (Monday, etc)\n     todayDayName = refDate.strftime(\"%a\").lower()[:3]\n     today = weekdays.index( todayDayName )\n@@ -118,8 +113,7 @@ def parseTimeReference(ref):\n     if dayOffset < 0: dayOffset += 7\n     refDate -= timedelta(days=dayOffset)\n   elif ref:\n-    raise Exception, \"Unknown day reference\"\n-\n+    raise Exception(\"Unknown day reference\")\n   return refDate\n@@ -164,4 +158,4 @@ def getUnitString(s):\n   if s.startswith('w'): return 'weeks'\n   if s.startswith('mon'): return 'months'\n   if s.startswith('y'): return 'years'\n-  raise Exception, \"Invalid offset unit '%s'\" % s\n+  raise Exception(\"Invalid offset unit '%s'\" % s)\n```\n. I believe this duplicates #1482. Would you agree?\n. OK now?\n. OK now?\n. I resolved the merge conflict caused by #1585 \nThis is now clean.  Any objections?\n. Ok.  I pushed changes to that pull request to clean up and/or fix the existing unit tests.\n. Rebased against master.\nI fixed the failing unit tests and cleaned up some more that were doing extra or useless tests now that there is an equality operator for TimeSeries.\n. I believe this is ready to go.\n. I have an idea on how to do unit tests for glyph.cc, but it pretty much involves static image comparisons...\n. Ignore point 2.  I was testing a nested list by accident ([range(0,100)] doesn't do what I thought.  Maybe we should check for that in TimeSeries?  The values should always be a list of numbers?\n. Tests look sufficient to me!\n. If I'm reading the code in master accurately, it is not doing any choosing of duplicate series beyond removing duplicate empty ones.  Does that read accurately?  If so, is the expectation that in this block code similar to the 0.9.x branch would be added to perform the merging?\n. Additionally, I think this may exist in MultiReader already...\n. Not yet.  I'm able to reproduce locally.  Probably because of flake8==3.0.1\n. Splitting into multiple pull requests can be done.  I'll attempt it this morning.\nI did attempt to add a [flake8] section to tox.ini but was unsuccessful in having it affect flake8 runs.\n. Turns out that's where the [flake8] is defined in graphite.  https://github.com/graphite-project/graphite-web/blob/master/setup.cfg#L19\nBy the looks of things, it isn't being respected.\n. Here's the bug in flake8 https://gitlab.com/pycqa/flake8/issues/181  Fixed in 3.0.2 which was released last night.\nSince we're already headed down the path of being pep8 compliant let's keep going for now.\n. Yup.  Looks like it.  Consistency is good.\nI opened #1623 for a more staged approach at converting the code base to pep8 compliance.  Please review that one.\nI'm closing this without merging as it accomplished what was intended.\n. I'm of the opinion that we make the change now.  We have the most test coverage we ever have and the backlog is as small as I've ever seen it.\n. Looks good to me\n. Rebasing will be not worth it.  I'll re-run the pep8 routines and submit a new PR.. That test failure smells of a timing issue.  It's not in code that this PR touched.\n. \ud83d\udc4d \n. The legend output with units is nice.\nIt looks like the PR is missing test cases for the changes to both cactiStyle() and format_units(), though.  Are those planned?\n. I sent you a PR to your branch for some unit tests for this.  I hope that's ok.\nThe cactiStyle() ones were straightforward.\nThe format_units() ones were written with a rather limited scope, so I expanded them some more.  Those should be sufficient for this change.\n. @jdixon I'll send you a pull request with working tests within a couple hours.\n. PR sent.\nDoes the horizontalLine() function work?  I tried to invoke it from a test case with the same arguments as threshold() and received an error: TypeError: horizontalLine() takes exactly 0 arguments (2 given)\n. Another thing I noticed was that the start and end times were returned as floats. I don't remember other functions returning non integers.\n. I tested on my backup cluster and this appears to work as expected.  I did have some timeouts when I searched across all 9 of the backend servers in the cluster for something that would return a very large result set, but I think that's to be expected.\n@obfuscurity do you want to try this out before we merge?\n. I will attempt to find time this week.. What was the exception that was thrown?\n. I see the issue and can reproduce.  Let's go with this until a better solution comes up, if ever.\n. That cluster wasn't in the best of states when I ran those tests.  It's my disaster recovery/standby cluster that needed/needs reconfiguration to be viable.  It has full input load for carbon, but essentially no read load so I could do isolated traffic capture of graphite-web requests.\nMore than a couple carbon caches on at least datastore_3 were at MAX_CACHE_SIZE and that seems to cause the carbon link lookups to timeout frequently on my carbon build (old megacarbon fork).  Since local fetches to whisper files are done serially, and thus carbonlink lookups for reach one, that request was expected to be extra long.  Essentially, those numbers are probably closer to worse case than best.\nI've been meaning to upgrade graphite-web to after the merge result set changes were merged. I will attempt to get some timings of some queries to see if I can see any difference in performance in my setup.\n. I don't think that is how it works, @obfuscurity.  My understanding is that the loop starting with for node, results in fetches: is serializing the consumption of the results of the fetch requests.  Those will always happen in sequential order and the first one (or few) should take longer than the rest, since they are going to block on the remote store responding.  To calculate how long each remote fetch takes (along with the start/stop times), the instrumentation would need to be added to node.fetch() or within the fetches = [(node, node.fetch(startTime, endTime)) for node in matching_nodes if node.is_leaf] line as that's when the start of the remote request happens.\nThe datalib.py code in master is rather different from the one in the 0.9.x tree, but the major parts I see are:\n1. In 0.9.x, STORE.find is only called for the local store.  All of the remote nodes are sent the fetch request without sending a find() first.  In master, we reduce the remote stores that we issue the fetch request to by first sending a find request in the line that assigns matching_nodes.\n2. In 0.9.x the carbonlink supports a bulk query option, but master does not.\n3. 0.9.x has as prefetch of remote data, but master does not.\nWith regards to # 1, I think that's a wash because the subsequent fetch() will result in a local find() call anyways on the remote nodes and that one should be cached and/or faster because the data would be in kernel's disk cache.  We could test it bypassing those find calls by converting STORE.remote_stores into the datatype that matching_nodes list contains.\nFor # 2, I'm not sure where in the master code to reimplement that feature.  Maybe in datalib.py:fetch_data() we can loop over matching_nodes and extract all metrics found, do the bulk carbonlink and pass that cached data to every WhisperReader.fetch(), as an example? \nFor # 3, I don't know enough about the implementation of that to know if it would make a difference here.\n. I have not had a chance to test any of the performance issues or the affects of this patch on them.  I am not sure if I'll have time this week.\n. I do not mind.  #1818 superceded it.. If someone else has the time to work through this, then please go ahead.\nRight now this is what I'm thinking are conditions that need to be considered for merging the cached values in:\n1. How to determine what the step or interval is for the cached values?\n2. What if the step or interval of the cached values differs from the lowest retention period?\n  a. If it's smaller, I'm not exactly sure what whisper does, but I think the it chooses the most recent value within the interval and stores that. \n  b. If it's bigger, but aligns with a multiple of the step or interval, do we just use xFilesFactor and run it through the aggregations?\n3. The metadata for whisper gives the aggregation methods, but what do we do with Ceres?\nAssuming we know the answers above, just run the data through the aggregation method for each retention interval until we reach the appropriate one.\n. I'm ok with this documentation change.  It makes it the intention clearer to me.\n. It was doing the opposite, though.\nI suspect there is a big misunderstanding on what the FIND_TOLERANCE actually does.  How would you describe the purpose of it?\n. Ah.  We are in the same boat.\nMy understanding is that the purpose was to pad the latest time of the data found on disk by the tolerance window.  This would compensate for the cached/unwritten data during find requests.\nAt this point I feel the need to put some logging in to confirm what the code is actually doing.  I'm hoping I have the time to do it soon.\n. I read through the code and feel that I have an understanding of the logic.  I will attempt to walk through it and we can decide how the REMOTE_STORE_MERGE_RESULTS should interact with it.\nLet's assume settings.FIND_TOLERANCE is set to 10 minutes.\nThis is a tad bit simplified, but here goes: \n1. Build up a list of nodes, local and remote, and store in nodes_remaining.  This is effectively the list of CLUSTER_SERVERS and any local Finders defined in settings.\n1. For each local node in nodes_remaining, if the data is found on disk then add to minimal_node_set and remove from nodes_remaining.\n2. For each node in nodes_remaining:\n   a. Compute how many intervals of data this node can provide, but this time ignore any intervals that reside in between now-FIND_TOLERANCE and now.\n   b. Add each node that has any data to the minimal_node_set\n3. If minimal_node_set is empty (no remote nodes had data and the local finder had NO data on disk), then choose the node where the difference between the node's most recent data and the start time of the query is less than the value of FIND_TOLERANCE.\nI question the need for the ignoring of intervals within the FIND_TOLERANCE window, but that can be discussed later somewhere else if needed.\nI have a few questions:\n1. Why do we need REMOTE_STORE_MERGE_RESULTS to even touch anything in the find logic IF the logic already there in step 3 will merge in multiple remote nodes have any relevant data?\n1. If you still think we do, then the base case (step 4) needs to be moved outside of the if settings.REMOTE_STORE_MERGE_RESULTS as a catch-all for when the requested interval is newer than data written to disk and the local host is the one that probably has it unwritten in carbon's cache.\n2. Before, with REMOTE_STORE_MERGE_RESULTS, there was a claim that it affected render times if all leaf_nodes were added.  Was that substantiated?\n. I proposed changes in #1743 that seem to correct this.  I would prefer everyone take a look to make sure this is ok.\nAlso, let's get the test cases expanded to include anything everything we should and should not accept.  I think I covered all the important cases, but what other ways do people use curly braces?\nFixed test case script can be here: https://gist.github.com/cbowman0/3f857db0e30ebb0f6c2948f504523381\nOutput of the tests:\nPattern: {foo}\nPre_1713 match_entries : ['foo']\nPost_1730 match_entries: []\nProposed match_entries : ['foo']\nPattern: foo\nPre_1713 match_entries : ['foo']\nPost_1730 match_entries: ['foo']\nProposed match_entries : ['foo']\nPattern: baz\nPre_1713 match_entries : []\nPost_1730 match_entries: []\nProposed match_entries : []\nPattern: {foo,garbage}\nPre_1713 match_entries : ['foo']\nPost_1730 match_entries: ['foo']\nProposed match_entries : ['foo']\nPattern: f{o,r,x}o\nPre_1713 match_entries : ['foo']\nPost_1730 match_entries: ['foo']\nProposed match_entries : ['foo']\nPattern: {fo{o}}\nPre_1713 match_entries : []\nPost_1730 match_entries: []\nProposed match_entries : ['foo']\nPattern: {f{o}o}\nPre_1713 match_entries : []\nPost_1730 match_entries: []\nProposed match_entries : ['foo']\nPattern: foo{}\nPre_1713 match_entries : ['foo']\nPost_1730 match_entries: []\nProposed match_entries : ['foo']\nPattern: fo{o}}\nPre_1713 match_entries : []\nPost_1730 match_entries: []\nProposed match_entries : []\nPattern: {foo}se\nPre_1713 match_entries : []\nPost_1730 match_entries: []\nProposed match_entries : []\nPattern: {foo,bar}\nPre_1713 match_entries : ['foo', 'bar']\nPost_1730 match_entries: ['foo', 'bar']\nProposed match_entries : ['foo', 'bar']\nPattern: {fo,ba}{o}\nPre_1713 match_entries : []\nPost_1730 match_entries: []\nProposed match_entries : ['foo']\nPattern: {fo,ba}{o,z}\nPre_1713 match_entries : []\nPost_1730 match_entries: ['foo']\nProposed match_entries : ['foo']\n. Pull request #1743 was merged.\n. Thanks.\nThe acceptance of {} used to work.  This is copied from a comment in issue #1740.\nFixed test case script can be here: https://gist.github.com/cbowman0/3f857db0e30ebb0f6c2948f504523381\nOutput of the tests:\nPattern: {foo}\nPre_1713 match_entries : ['foo']\nPost_1730 match_entries: []\nProposed match_entries : ['foo']\nPattern: foo\nPre_1713 match_entries : ['foo']\nPost_1730 match_entries: ['foo']\nProposed match_entries : ['foo']\nPattern: baz\nPre_1713 match_entries : []\nPost_1730 match_entries: []\nProposed match_entries : []\nPattern: {foo,garbage}\nPre_1713 match_entries : ['foo']\nPost_1730 match_entries: ['foo']\nProposed match_entries : ['foo']\nPattern: f{o,r,x}o\nPre_1713 match_entries : ['foo']\nPost_1730 match_entries: ['foo']\nProposed match_entries : ['foo']\nPattern: {fo{o}}\nPre_1713 match_entries : []\nPost_1730 match_entries: []\nProposed match_entries : ['foo']\nPattern: {f{o}o}\nPre_1713 match_entries : []\nPost_1730 match_entries: []\nProposed match_entries : ['foo']\nPattern: foo{}\nPre_1713 match_entries : ['foo']\nPost_1730 match_entries: []\nProposed match_entries : ['foo']\nPattern: fo{o}}\nPre_1713 match_entries : []\nPost_1730 match_entries: []\nProposed match_entries : []\nPattern: {foo}se\nPre_1713 match_entries : []\nPost_1730 match_entries: []\nProposed match_entries : []\nPattern: {foo,bar}\nPre_1713 match_entries : ['foo', 'bar']\nPost_1730 match_entries: ['foo', 'bar']\nProposed match_entries : ['foo', 'bar']\nPattern: {fo,ba}{o}\nPre_1713 match_entries : []\nPost_1730 match_entries: []\nProposed match_entries : ['foo']\nPattern: {fo,ba}{o,z}\nPre_1713 match_entries : []\nPost_1730 match_entries: ['foo']\nProposed match_entries : ['foo']\n. That timing issue in test_render bit again.  Maybe I'll track that down sometime.  Can someone with perms ask Travis CI to try again?\n. \ud83d\udc4d . I was thinking we should, so no concerns from me.. Looks good to me.  \ud83d\udc4d . Yes.  So far that the only set I'm able to reproduce it on.. @deniszh I do have REMOTE_STORE_MERGE_RESULTS = False.  Sometime around Nov of last year time frame, something in that code path caused issues in me getting data.  I believe it may have been related to #1739.  So much has changed since then, though, that i'll be probably worthless to track back to why at this point.\n@DanCech Yep.\nI added some logging in there. \nContents of leaf_nodes:\n2017-03-21,13:07:31.886 :: leaf_nodes: [<LeafNode[7f4e2409c350]: carbon.carbon-daemons.datastore1.instance1.metrics_received (<RemoteReader[7f4e380b3e68]: 10.0.0.100>)>, <LeafNode[7f4e240d1dd0]: carbon.carbon-daemons.datastore1.instance1.metrics_received (<RemoteReader[7f4e240c4ba8]: 10.0.0.102>)>]\nNode and contents of intervals of the first node passed to distance_to_requested_interval:\n2017-03-21,13:07:31.886 :: node: <LeafNode[7f4e2409c350]: carbon.carbon-daemons.datastore1.instance1.metrics_received (<RemoteReader[7f4e380b3e68]: 10.0.0.100>)>\n2017-03-21,13:07:31.886 :: node.intervals: []\n. The issue does not happen if REMOTE_STORE_MERGE_RESULTS is True.\nIf REMOTE_STORE_MERGE_RESULTS = False, then adding return if node.intervals is empty like\nif not node.intervals:\n              return float('inf')\nprior to the latest = sorted(node.intervals, key=lambda i: i.end)[-1] line keeps results in graphs and no errors.. Could this be because I'm running Django 1.5?. Yes.  I've been running on master with these specific packages for a while.  It still works \ud83e\udd37\u200d\u2642\ufe0f .. I don't know.  I haven't run the 0.9.x line in a very long time.. I'd say close this.. I understand the code to need to know all of the cache instances in order to properly perform the consistent hashing that carbon relays does.  So, set the value of CARBONLINK_HOSTS to include the same set of cache hosts as the relay but with the ports changed to match the cache configs.  Based on your example, if I didn't mess it up, it would be CARBONLINK_HOSTS=['10.0.0.1:7002:a', '10.0.0.1:7102:b', '10.0.0.2:7002:a', '10.0.0.2:7102:b', '10.0.0.3:7002:a', '10.0.0.3:7102:b']\nIf I'm wrong, I'm sure it will be pointed out quickly.. I understand the code to need to know all of the cache instances in order to properly perform the consistent hashing that carbon relays does.  So, set the value of CARBONLINK_HOSTS to include the same set of cache hosts as the relay but with the ports changed to match the cache configs.  Based on your example, if I didn't mess it up, it would be CARBONLINK_HOSTS=['10.0.0.1:7002:a', '10.0.0.1:7102:b', '10.0.0.2:7002:a', '10.0.0.2:7102:b', '10.0.0.3:7002:a', '10.0.0.3:7102:b']\nIf I'm wrong, I'm sure it will be pointed out quickly.. Timing related failures that didn't happen locally.  I'll work on eliminating that.. I'll clean up a few things I noticed and merge afterwards.. Support for Django 1.8 was added to 1.0.x and master branches.  Next release(s) will contain support unless some major issue is found.. I have found no issues in my testing\u200b so far.  I want try try a few more items before merging.. I've found no issues.  Does anyone have anything specific to check?\n. I submitted PR #1975 for the 1.0.x branch.  OK to merge here?. Graphs work if I set REMOTE_PREFETCH_DATA = False.. It looks like the prefetch code is buried under the elif requestOptions['graphType'] == 'line': section in webapp/graphite/render/views.py. I don't see where that would happen.  Should it be in datalib.py's fetchData or _fetchData?. Making this change gets pie charts back to working: https://github.com/cbowman0/graphite-web/commit/1510d0afb587d5746876f032b5fcf4004fb64c78. No worries.  I didn't it missing until I reviewed the patches to my local test build this morning.. The propose is to support symbolic links.  Between 0.9 and 1.x, the functionality was expanded to support symlinks beyond the filename in #1738.  https://github.com/graphite-project/graphite-web/pull/1738\n. For now, use the docs from the 1.0.2 release: http://graphite.readthedocs.io/en/1.0.2/functions.html. I do not know how the docs work.  I can look later unless someone else gets to it.. This appears to function properly.\nCan you add a test case for when the symlink is inside of the metric path as opposed to the base path itself: /opt/graphite/storage/whisper/foo/bar is a symlink to /opt/graphite/storage/whisper/baz/\nget_real_metric_path function should return baz.blah.. I agree with @iksaif. For functions.evaluateTarget?. Right.  I used the manual patching to keep with the other methods.  Mock makes more sense.  I'll rework this and the other cases that were similar to mock fetchData.. Some comments added back.\nAlso, unless I missed something, added tests for the final 2 functions with no coverage. . Github was slow with processing the change removing that import.  https://status.github.com/messages shows queue delays.  I'll check out #2076 now.. Yes.  I think it would be wrong to remove the dashboard.  We also use grafana but the use cases are different.\nOf course I also patch back in graphlot support since that's used here, too.  . I can't argue with your logic or plan.  That's what perSecond is supposed to be.. Maybe using environment markers in requirements.txt? https://www.python.org/dev/peps/pep-0508/#environment-markers. My email is my github username at gmail.com.\nhttps://stackoverflow.com/questions/27644586/how-to-set-up-travis-ci-with-multiple-languages suggests a way to make it work.  At least we can attempt it, but an ESLint config file can be used by Codacy, so it's not useless.. Start of work described in #2144 . Anything allowing this to be dynamic would be nice: https://github.com/graphite-project/graphite-web/blob/master/webapp/content/js/composer_widgets.js#L1072-L1216. Codacy appears to have a 50 issues per file max.  I removed some known issues by converting quotes, so new ones were \"discovered\".. I vote for a 1.1.x. branch.. Additionally to this quick documentation, I was thinking it would be good to describe/document what dependencies are needed to have an environment where the tests would run.  I believe they currently need Python tox, coverage, a redis server on localhost and a mysql server on localhost.   Maybe an Ubuntu/CentOS VM?  Are there other ways that are simpler/easier?. OK.  I changed the words to highlight the environments to test in and added them to the examples.  I'm done changing the words for now.. This breaks the completer in the dashboard UI.. Was this an intended change from REMOTE_FETCH_TIMEOUT to REMOTE_FIND_TIMEOUT?\n. Thanks for the feedback!\n. It wasn't intended to be, but that's definitely not done.  I'll clean it up tomorrow during my train ride.  Sorry for the extra garbage.\n. Sure.\n. I can copy in some of the verbiage about what consolidateBy does.  That would make it cleaner.\n. FYI - The dygraph output has the same number of digits as raw did before this change.\n. It's returning this data [{\"what\": \"A final thing happened\", \"tags\": [\"foo\", \"bar\"], \"id\": 3, \"data\": \"yet even more info\", \"when\": 1470740187}]\nThe tags [u'foo', u'bar', u'nope'] are being passed all the way down into the find_events method.  I haven't checked past there, but it appears that the with_intersection call isn't working as expected.\n. https://github.com/flebel/django-tagging/issues/157 ?\nThat's just an export from code.google.com.  Also, from 2008.  I'll dig more.\n. I tested independently and only saw directories show up.\nOutput is only directories:\n$ python2.7 test.py | xargs -L1 -I{}  stat -c%F {} | uniq -c\n  27177 directory\nIt did take 1.1 seconds to just run the osdir on this host, but it is at about 65% IO utilization all the time.\n```\n$ time python2.7 test.py  | wc -l\n27177\nreal    0m1.149s\nuser    0m0.675s\nsys 0m0.478s\n```\nTest script:\n```\n$ cat test.py\nimport os\nimport operator\ncurrent_dir='/opt/graphite/storage/whisper/carbon/'\nmatching_subdirs = map(operator.itemgetter(0), os.walk(current_dir))\nfor i in matching_subdirs:\n  print i\n``\n. Therange(20)` call returns a list.  No need for the braces around that.\n. For anyone following along that is curious how I debug/write tests, I add the following code (or something similar) after the execution of the function to see what is being returned vs what was coded as expected:\nfor index, series in enumerate(result):\n          print series.getInfo()\n          print expectedResults[index].getInfo()\nThis gets removed before checking in.  Running \nIt may be worthwhile to make TimeSeries.__repr__ return more data than it currently does, so this is not necessary\n. @obfuscurity If the test was changed like this, it returns success, but I'm not 100% sure that it does as expected.  The fact that certain inputs can result in no data being returned is concerning.  Also, these inputs only hit the positive case on the if (len(series) < expectedSamples * 0.95): on line 1190 of functions.py\n```\ndiff --git a/webapp/tests/test_functions.py b/webapp/tests/test_functions.py\nindex 5494b7d..84814a7 100644\n--- a/webapp/tests/test_functions.py\n+++ b/webapp/tests/test_functions.py\n@@ -868,19 +868,19 @@ class FunctionsTest(TestCase):\n def test_smooth(self):\n     seriesList = [\n\n\nTimeSeries('collectd.test-db1.load.value',0,1,1,[range(20)]),\nTimeSeries('collectd.test-db1.load.value',0,20,60,range(0, 1200, 60)),\n         ] def mock_evaluateTokens(reqCtx, tokens, replacements=None):\n     seriesList = [\n\n\nTimeSeries('collectd.test-db1.load.value',0,1,1,[range(20)]),\n\nTimeSeries('collectd.test-db1.load.value',0,100,12,range(0, 1200, 12)),\n         ]\n         for series in seriesList:\n             series.pathExpression = series.name\n         return seriesList\nexpectedResults = [\n-            TimeSeries('smooth(collectd.test-db1.load.value, 5)',0,1,1,[None,None,None,None,2,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n+            TimeSeries('smooth(collectd.test-db1.load.value, 5)',60,100,12,range(0,1177,12)),\n ]\nwith patch('graphite.render.functions.evaluateTokens', mock_evaluateTokens):\n@@ -889,13 +889,16 @@ class FunctionsTest(TestCase):\n             'template': {},\n             'args': ({},{}),\n             'startTime': datetime(1970, 1, 1, 0, 0, 0, 0, pytz.timezone(settings.TIME_ZONE)),\n-                    'endTime': datetime(1970, 1, 1, 0, 9, 0, 0, pytz.timezone(settings.TIME_ZONE)),\n+                    'endTime': datetime(1970, 1, 1, 0, 20, 0, 0, pytz.timezone(settings.TIME_ZONE)),\n             'localOnly': False,\n-                    'width': 400,\n+                    'width': 100,\n             'data': []\n         },\n         seriesList, 5\n```\n. Also, based on looking at the output of coverage (https://coverage.readthedocs.io) on my test vm via:\n\n\n\n\ncbowman@cbo-virtualbox:~/git/graphite-web/webapp$ coverage html --include=\"graphite/*\"\ncbowman@cbo-virtualbox:~/git/graphite-web/webapp$ firefox htmlcov/graphite_render_functions_py.html\nthose changes don't hit all of the resample code.  So, only keeping that one test is insufficient.  I can look more later tonight/tomorrow if need be.\n. The existing model is asynchronous fetches to remote stores (cluster servers) and serial fetches for local (whisper files).  The asynchronous responses were then serially handled in-line with the local fetches, if any existed.\nI put this patch together because I was looking at long fetch times for serially opening whisper files + carbonlink over thousands of datafiles.  This is an extreme case, but something I was looking to see if this helped out.  I do not have number yet to confirm if it does.\nIf this does help, then we should probably remove the async callback from remote fetches and just use threading, if threading is enabled, and not mix the two styles.\n. Sure.\n. I started with for i, fetch_thread in enumerate(fetches_running), but when doing a remove on the index, but that's broken logic.  So, I went with this, but am not tied to it. \n. It should never be greater than settings.MAX_FETCH_THREADS, so that's just defensive coding.  If it's equal, then I was thinking it was better to not spin the main process waiting for a worker thread to finish.\nWas there another question?\n. Yes.  The question that I hadn't yet answered was what value to use.  I didn't want to create another variable, but to use an existing it'd be dependent on the type of fetch (local or remote) that it was.\n. Agreed.\n. Yep.  Agreed.\n. Actually, that comment needs help in clarity, too.  It was to remind me to add that code there, but that was done.\n. I used the if bool(whisper) because that was how it was done in the classes themselves, so it's consistent with the rest of the file.\n. This was how it was done in the other similar test.  Other than that, I don't know of a reason off hand to choose this way over mock.patch().\nI think we're pretty inconsistent with how we mock things in the tests.  It may be useful to define some standards \u00af_(\u30c4)_/\u00af\n. Is this extra line intentional?. Singular of series is still series.  At least for the error message, can you change it to be series?. Also, I don't think this failure condition is tested in the tests provided.  Can you add another test or two to test the failure conditions?\nThe rest looks good!. Ah.  I thought it would be something like that.  English isn't always logical.... Yes.  Done.. Ah.  Yes.  Still need to have a local variable with the lower() value for comparison.  I think it's cleaner to read, though, so I changed it to that.. Ok.  That makes sense.  Done.. They were erroneously copied from other checks when I created them this week.  I'll see if I can put comments there that do make sense.. Yep.  Will fix.. Thanks for reading through it.. Yes.. ",
    "tubit": "good job. saved my setup. :)\n. ",
    "luxflux": "Now it just have to be merged :)\n. Hi @obfuscurity, faraday does this as default (https://github.com/technoweenie/faraday/issues/78)... I found this when I implemented faraday with team_dashboard (https://github.com/fdietz/team_dashboard/issues/4).\nIt's not really a specification, it's more just a \"common practice\". At least for jQuery (http://api.jquery.com/jQuery.param/), Rails and PHP. So I'd suggest to support this notation as well.\nIMHO it's anyway more intuitive as you override a value when you specify it multiple times, normally.\n. Okay, I will do some updates next week.\n. Added some comments and changed the code to use if and elif. What do you think about it?\n. ",
    "rngtng": "yeah - good one! +1 \n. Awsm! Thx\n. ",
    "gwaldo": "I think it's a great idea, but surprise, there be yaks.\nFrom my perspective, the layers are:\n- Get rst / sphinx working on my box; I'd had this working when I contributed some docs stuff to MongoDB, but it has since gone sideways.  When I get this done, I've got a pull request to fill in some initial blank spots just about ready, but without Sphinx working, I can't test.\n- ~~Finish the work to migrate info in wikidot to the RTD site, so we can kill wikidot. https://github.com/graphite-project/graphite-project.github.io/issues/4~~\n- Breakout (this ticket https://github.com/graphite-project/graphite-web/issues/46)\n- Start incorporating some of the tribal knowledge / wiki posts into the docs.\n. This appears to have already been fixed last year.\ndf330c3b (Jeff Schroeder    2013-05-09 22:12:39 -0500  562) graphType\n. Backported.\n. :+1:  Assign this to me.\n. What do we actually want/need to say about \"Administering\" the webapp?\n. (Trackpad whoopsie.)\n. Yes, it's become apparent to me that it's all a mess.  There is a lot of reorganization, consolidation, consistency, etc that really should be done.  I'm assuming that there is going to be duplicate (and possibly conflicting) info in there, but to fix that will take a solid 'refactor' of the whole, as a whole.\nBut for now I think the best things to do are capture what is missing (missing feature/explanations, as well as wikidot data) before we do anything else first (docs-wise)\n. \n. \n. I should have time to work on this tomorrow.  Thank you for trying, Denis\nOn Thu, Oct 16, 2014 at 3:30 PM, Denis Zhdanov notifications@github.com\nwrote:\n\nJust tried to repeat steps from manual, LGTM:\nroot@vagrant-ubuntu-precise-64:~# pip install carbon --install-option=\"--prefix=/srv/graphite\" --install-option=\"--install-lib=/srv/graphite/lib\"\nDownloading/unpacking carbon\n  Downloading carbon-0.9.12.tar.gz (47Kb): 47Kb downloaded\n  Running setup.py egg_info for package carbon\npackage init file 'lib/twisted/plugins/__init__.py' not found (or not a regular file)\nwarning: no files found matching '*' under directory 'conf/'\nwarning: no files found matching '*' under directory 'distro/'\nwarning: no previously-included files found matching 'conf/*.conf'\nRequirement already satisfied (use --upgrade to upgrade): twisted in /usr/lib/python2.7/dist-packages (from carbon)\nDownloading/unpacking txamqp (from carbon)\n  Downloading txAMQP-0.6.2.tar.gz\n  Running setup.py egg_info for package txamqp\nInstalling collected packages: carbon, txamqp\n  Running setup.py install for carbon\n    package init file 'lib/twisted/plugins/init.py' not found (or not a regular file)\n    changing mode of build/scripts-2.7/carbon-client.py from 644 to 755\n    changing mode of build/scripts-2.7/carbon-relay.py from 644 to 755\n    changing mode of build/scripts-2.7/carbon-aggregator.py from 644 to 755\n    changing mode of build/scripts-2.7/carbon-cache.py from 644 to 755\n    changing mode of build/scripts-2.7/validate-storage-schemas.py from 644 to 755\nwarning: no files found matching '*' under directory 'conf/'\nwarning: no files found matching '*' under directory 'distro/'\nwarning: no previously-included files found matching 'conf/*.conf'\nchanging mode of /srv/graphite/bin/carbon-client.py to 755\nchanging mode of /srv/graphite/bin/carbon-relay.py to 755\nchanging mode of /srv/graphite/bin/carbon-aggregator.py to 755\nchanging mode of /srv/graphite/bin/carbon-cache.py to 755\nchanging mode of /srv/graphite/bin/validate-storage-schemas.py to 755\nRunning setup.py install for txamqp\nSuccessfully installed carbon txamqp\nCleaning up...\nroot@vagrant-ubuntu-precise-64:~# ls -al /srv/graphite\ntotal 24\ndrwxr-xr-x 6 root root 4096 Oct 16 19:23 .\ndrwxr-xr-x 3 root root 4096 Oct 16 19:23 ..\ndrwxr-xr-x 2 root root 4096 Oct 16 19:23 bin\ndrwxr-xr-x 2 root root 4096 Oct 16 19:23 conf\ndrwxr-xr-x 7 root root 4096 Oct 16 19:23 lib\ndrwxr-xr-x 6 root root 4096 Oct 16 19:23 storage\nroot@vagrant-ubuntu-precise-64:~# ls -al /srv/graphite/bin/\ntotal 32\ndrwxr-xr-x 2 root root 4096 Oct 16 19:23 .\ndrwxr-xr-x 6 root root 4096 Oct 16 19:23 ..\n-rwxr-xr-x 1 root root 1085 Oct 16 19:23 carbon-aggregator.py\n-rwxr-xr-x 1 root root 1085 Oct 16 19:23 carbon-cache.py\n-rwxr-xr-x 1 root root 4321 Oct 16 19:23 carbon-client.py\n-rwxr-xr-x 1 root root 1085 Oct 16 19:23 carbon-relay.py\n-rwxr-xr-x 1 root root 2236 Oct 16 19:23 validate-storage-schemas.py\nroot@vagrant-ubuntu-precise-64:~# ls -al /srv/graphite/lib\ntotal 28\ndrwxr-xr-x 7 root root 4096 Oct 16 19:23 .\ndrwxr-xr-x 6 root root 4096 Oct 16 19:23 ..\ndrwxr-xr-x 3 root root 4096 Oct 16 19:23 carbon\ndrwxr-xr-x 2 root root 4096 Oct 16 19:23 carbon-0.9.12-py2.7.egg-info\ndrwxr-xr-x 3 root root 4096 Oct 16 19:23 twisted\ndrwxr-xr-x 2 root root 4096 Oct 16 19:23 txAMQP-0.6.2-py2.7.egg-info\ndrwxr-xr-x 3 root root 4096 Oct 16 19:23 txamqp\nroot@vagrant-ubuntu-precise-64:~# ls -al /srv/graphite/lib/twisted/\ntotal 12\ndrwxr-xr-x 3 root root 4096 Oct 16 19:23 .\ndrwxr-xr-x 7 root root 4096 Oct 16 19:23 ..\ndrwxr-xr-x 2 root root 4096 Oct 16 19:23 plugins\nroot@vagrant-ubuntu-precise-64:~# ls -al /srv/graphite/lib/twisted/plugins/\ntotal 32\ndrwxr-xr-x 2 root root 4096 Oct 16 19:23 .\ndrwxr-xr-x 3 root root 4096 Oct 16 19:23 ..\n-rw-r--r-- 1 root root  675 Oct 16 19:23 carbon_aggregator_plugin.py\n-rw-r--r-- 1 root root 1168 Oct 16 19:23 carbon_aggregator_plugin.pyc\n-rw-r--r-- 1 root root  643 Oct 16 19:23 carbon_cache_plugin.py\n-rw-r--r-- 1 root root 1126 Oct 16 19:23 carbon_cache_plugin.pyc\n-rw-r--r-- 1 root root  641 Oct 16 19:23 carbon_relay_plugin.py\n-rw-r--r-- 1 root root 1124 Oct 16 19:23 carbon_relay_plugin.pyc\nroot@vagrant-ubuntu-precise-64:~#\nDid I understand something wrong?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/237#issuecomment-59416713\n.\n. I think I figured out the (or at least \"a\") problem.  On vanilla Ubuntu 12.04 and 14.04 Vagrant boxes, the command pip install carbon --install-option=\"--prefix=/srv/graphite\" --install-option=\"--install-lib=/srv/graphite/lib\" will fail saying twisted/runner/portmap.c:10:20: fatal error: Python.h: No such file or directory\n\nThe problem isn't that pip won't install the prerequisite packages, but rather that a system package (not a pip package) is missing: python-dev (in Debian-land, anyway; it looks like it's python-devel in Hat-land).\nI'll submit PRs for Master and 0.9.x shortly.\n. PR: https://github.com/graphite-project/graphite-web/pull/1005\nBackport PR: https://github.com/graphite-project/graphite-web/pull/1006\n. Assign this to me, too.\n. Assign this to me, too.\n. I'm closing this as a duplicate of https://github.com/graphite-project/graphite-web/issues/227\n. Unassigning myself in case someone else wants to pick this up before I get to it.\n. Does anyone have rights to the RTD site?  I've seen it updated automagically, and as an Ops-guy, I thoroughly appreciate that.\nI poked around RTD's RTD, and discovered their Build Process doc.\nCan someone with rights to our project drop in the Settings menu, and check whether we have a Webhook configured?  I don't have rights to modify settings (nor do I need them.)\n. Speaking of docs, does anyone have a doc / notes / scripts / omnious-puzzle-box that helps with releases?\n\n. @obfuscurity Actually, it was both.\nI took the notes at the top to be \"I don't know how it is supposed to be done, but here's what I would do\".\nAs far as RTD, I know we're using it, but I was curious if anyone had credentials to RTD, and if we have a webhook set up on the repo.\n. I've unassigned myself from a couple of documentation-tagged tickets in case anyone happens to see one that they want to pick up.  Mainly the ones where I have little thought/expertise/involvement.\nFor instance, to solve sumSeriesWithWildcard and averageSeriesWithWildcard have insufficient documentation I will have to dive into the code.  It's not a problem for me to do it, but it will require time.\n. Sure, I'll try to take a look today.\nOn Sun, Oct 11, 2015 at 11:19 PM, Jason Dixon notifications@github.com\nwrote:\n\nOur docs builds have been failing on RTD since Aug 13\nhttps://readthedocs.org/projects/graphite/builds/3236334/. @gwaldo\nhttps://github.com/gwaldo is this something you can help take a look at?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/677#issuecomment-147281643\n.\n. Looking into this for a little bit, the most plain errors are /opt/graphite can't be created and that django isn't being imported.\n\nLooking at the last build to succeed, it looks like a very different style of information being presented.\nI just signed up for a readthedocs.org account.  Does anything need to be done to add me to the RTD for Graphite so that I can see what options are available to manage it or engage with the RTD team?\n. There is a significant gut-and-rebuild of the docs that I want to do, but there's no reason to hold this up.  I'll be happy to see this release.\n. @brutasse Thank you.  I have fixed it.\n. /bump\n. Apparently I've been failing at git the last day or two.  I've pulled, merged, resolved and pushed up.  @brutasse thought that it looked good before, but I'd love to hear other opinions.\n. Sure thing.  Thanks for your work on the project and community!\n. I'll assign to myself and take a look.\n. Unassigning myself in case someone else wants to pick this up before I get to it.\n. What do you mean by this?  Are you suggesting a port to py3?\n. PR https://github.com/graphite-project/graphite-web/pull/821 /cc @obfuscurity \n. I was reading that and the Federating Graphite posts earlier today.  My company has almost nothing, the adoption has gone much better than I'd expected, and the MVP boxes that I'd stood up quickly are no longer sufficient.\nYup, I'll be the living test of these docs.  May Cthulhu be merciful and quick.\n. :100: \n. Also, for quick reference:\nhttp://rcrowley.org/articles/federated-graphite.html\nand\nhttp://grey-boundary.com/the-architecture-of-clustering-graphite/\n. that's a fair nitpick; I'll look for \"proper\".  Also, Travis failed with a warn-treated-as-error.  I'll look at it.\n. oops, already forgot the SQLite.  Don't merge yet\n. D'awww...\n. \ns/girl\\ in\\ tap\\ shoes/TravisCI\\ and\\ rST/;\nI no longer care tonight.  Going to bed.\n. gif away, sir.\n. \n. Backported\nOn Sun, Aug 10, 2014 at 1:10 PM, Jason Dixon notifications@github.com\nwrote:\n\nAny plans to backport this to 0.9.x? :smiling_imp:\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/pull/821#issuecomment-51720661\n. I had a confuse, and that's what ended up happening. Didn't back it out when I realized my mistake because those had all been through CI.\n\nAgreed that PR is the better way.\nOn Sun, Aug 10, 2014 at 6:21 PM, Jason Dixon notifications@github.com\nwrote:\n\n@gwaldo Did you commit those straight to the 0.9.x branch? Best to issue PRs and let someone else merge them so we can get public tests and :+1:'s. Not a big deal for those recent doc changes but generally a good idea imho.\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/pull/821#issuecomment-51729298\n. After this weekend, I don't think I can ever question your commitment to :sparkles: Nation!\n. Altogether this is a lot of work.  I've broken this up into discrete tickets:\n\nhttps://github.com/graphite-project/graphite-web/issues/833\nhttps://github.com/graphite-project/graphite-web/issues/834\nhttps://github.com/graphite-project/graphite-web/issues/835\nhttps://github.com/graphite-project/graphite-web/issues/836\n. Link would be helpful: https://github.com/obfuscurity/synthesize/\n. Contributed as https://github.com/graphite-project/graphite-web/pull/837\n. PR 837 merged.  Closing\n. I love that you're contributing documentation!  :+1: :beer: \nUnfortunately I'm not familiar with that feature, so I'll let someone else merge.\nThank you!\n. Actually, @deniszh, I think you're right to have the protocol for the web interface, but the protocol needs to be \"https\".\nGood catch!  I'll be happy to accept this when you resubmit.\n. @obfuscurity I hadn't seen the build status.  rST seriously seems to be inconsistent at times.  I don't think anybody knows it... :-p\nHence my sputtering the other night\n. :heart: \n. @ojilles By 'backport to 0.9.x', we mean that you checkout the 0.9.x branch from your forks origin/0.9.x, then cherry-pick these commits into the local 0.9.x branch, push up to your origin (github), and send a PR for it.\nWhen I'm feeling lazy, I do this in SourceTree.\n. lol, oops.  Good thing I have today blocked out to write a presentation...\n\n. LGTM!  :beer: \n. LGTM\nThanks!\n. @deniszh The hw-cookbooks/graphite cookbook is what I intend on using as the recommendation for the 'install-via-chef' section of the docs, though this is heavily caveated.\nFirst, is that last I checked a week or two ago, they are looking to cut a new release that is much more library/provider oriented than the current version.\nSecond, is that if we get the packaging more sane, that could simplify the cookbooks needed overall.\n. PR: https://github.com/graphite-project/graphite-web/pull/899\n. At first it seemed that the .. _list-of-functions : reference was missing, but after the RTD site didn't get fixed, the remaining difference was the lack of a tab character on the line after that.  Because rST, I suppose.  I've submitted https://github.com/graphite-project/graphite-web/pull/887\n. Does the theme need to be defined there? It seems to be using the theme regardless.\n. I offer a confused :beer: :ok_hand: \n. This is to hopefully address https://github.com/graphite-project/graphite-web/issues/886\n. :heart: \n. Will be backported to 0.9.x\n. Thanks for the comments, @brutasse !  This was actually just a quick stab at porting the page over for the most part, as part of the chipping-away at the wikidot site.\nThese are all fine comments.\n. Normally I'd rather go with your second thought, but in this case, I think these changes should be quick, so I'll do them now.\n. I actually like the \"who-is-using\" idea, so I'm not going to get into that here.\nI've added a couple more commits re: @brutasse 's comments\n. @brutasse Would you care to merge this and https://github.com/graphite-project/graphite-web/pull/910 ?  @obfuscurity says that he's going to be away for a bit.\n. :heart: \n. Also this, @obfuscurity \n. So if we're agreed...\n2008-2012, Chris Davis\n2011-2014, The Graphite Project\nYea / Nay?\nI plan do do a little work tonight, so I should be able to knock this out.\n. \nOn it.\n. Addresses https://github.com/graphite-project/graphite-web/issues/237\n. You're absolutely correct.  I'll modify the doc\n. Backport of fix for issue https://github.com/graphite-project/graphite-web/issues/237\n. :heart: :+1: \n. @obfuscurity Which would you say is the doc bug?\n1. pytz isn't installed, and should be a prereq\n2. Timezone should be set to zulu or local\n3. something about the relative times (I'd need help with an explanation on that)\n. @obfuscurity I'll add to the docs.\nCan / Should I add it to the code so that it's done automagically (w/ Twisted, et al.)?\n. Yes, I can do that tonight.\nApologies for my inattention.  End-of-year madness at both home & work.\n. That looks fabulous, @deniszh :sparkles: \nAre you doing the same for Master, or shall I?\n. I'll take a look in a little while, but I've got nothing off of the top of my head.\nOn Sat, Jan 10, 2015 at 9:41 PM, Jason Dixon notifications@github.com\nwrote:\n\nP.S. 0.9.12 is the last 0.9.x tag with a working functions list. I hesitate to rebuild that one because it's probably going to fail for the same reason.\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/issues/1097#issuecomment-69481261\n. Apologies for the late reply.  There is a lot of documentation work to be done, and I simply have not had time to contribute much lately.\n\nThere are a few excellent blog posts out there (sadly more of lesser quality) that I use as reference for whisper-based graphite clustering:\n- BitProphet's \"Clustering Graphite\"\n- Richard Crowley's \"Federated Graphite\"\n- The Architecture of Clustering Graphite\n. From a quick read it looks ok, but I don't have the bandwidth to test it.\n. Can we merge now and rename the property before release?\n. :+1: Let's try it\n. I'm fine with including other backends.  This was just a list off of the few that I'd had in recent memory and seemed most-common alternates (for some value of \"most\").\n. Good point, @obfuscurity Thanks!\n. Thanks!\nI also backported this in https://github.com/graphite-project/graphite-web/pull/1382\n. What timezone is your server set to?\n. What about your clients?\nIf you run date +%s on the graphite cluster machines, and on your emitter's machines, do they have the same idea of what time it is?\n. We don't have a solid time, but here are a couple recent blog posts:\nhttp://obfuscurity.com/2015/10/Graphite-the-Phoenix-Release\nand\nhttp://obfuscurity.com/2015/11/Graphite-0.9.14-the-Highlights\nP.S. I'm glad that some time and effort has been put into improving what was there, Mr. Ladd.\n. @danielladd 0.9.14 is released\nhttp://graphite.readthedocs.org/en/0.9.x/releases/0_9_14.html\n. Thanks, @chrisheerscap, that information is actually helpful. We didn't know when it had stopped working.\n. :heart: \n. It's a documentation ticket.  Docs live in this repo.\n. Sorry, I don't mean to sound like an ass, @drawks; It's just stupid busy here, and I noticed a gap in the docs, and wanted to record the problem as quickly as possible.\n. Thanks, @pra85 !\n. @graphite-project/committers Can someone with more domain expertise have a look at this?\n@Sinkmanu Do you have any tests that you could provide that we could use to to confirm the fix?\n. I agree that the wording here is goofy, but I'd rather use other wording.  Thank you for bringing it up, though!\n. Thanks for the contribution!\n. Thanks!\n. Didn't mean to, but this got rolled into https://github.com/graphite-project/graphite-web/pull/1501\nNote to self: This is what Branches are for.\nWould @graphite-project/committers have a look, comment, etc\n. Can any @graphite-project/committers comment on the accuracy of this doc?\n. Yea, I intend to switch to one consistently, and prefer the inline, but don't know if anyone cares. (Doubtful, come to think of it...)\n. Comments incorporated and/or responded to with more comments.  Thanks!\n. \nJust for you, @obfuscurity \n. Thanks for the fix\n. I'm down as long as the tests are updated.\n. Do you work at Pandora and use Graphite yourself?\n. Thank you! Is there any chance you'd be willing to participate in a case study (or refer someone there who is to me)?\n. I love that you're putting some effort into testing!  I'm not really competent with python, but having useful tests makes me more confident to make changes.\n. The problem is that you are running with a default password for Django (which requires it), but since this is an open-source product and the password is widely-known, we provide a default value, but then check if this password is set to the default, and force you to change it so that you are less likely to be hacked via it.\nThis is purely for your benefit.  It is working as expected.\n. :heart:\n. As far as changes to visual elements, we could incorporate a perceptual-diff tool (idea cribbed from https://www.youtube.com/watch?v=1wHr-O6gEfc)\n. I've opened https://github.com/graphite-project/carbon/issues/567 for the Carbon PR, but I'd like the main discussion to be here.  The graphite-web PR will be linked against this Issue.\n. Heh, I'd been thinking about those terms, too.  :-)\nI think allow/deny makes a lot more sense.  What is thought of allowed_metrics.conf and denied_metrics.conf\n. Yes, I have no intention of ripping the carpet out from anyone.  The idea is that the old files will still be allowed, but perhaps throw a deprecation message out to the logs?\nI intend for the docs to reflect the change.  Whitelist/Blacklist will be listed as pending deprecation (something to that effect), and directing people to use the new names.\nDoes allowed_metrics.conf + blocked_metrics.conf make sense?\n. For reference, I'm replacing the general term (env vars, etc) USE_WHITELIST with USE_METRICSLIST.\nIt feels a little clunky, but I think it implies what we're going for.  I'm open to a better name.\n. That's better.  Thanks, @obfuscurity.\n. @cbowman0 I honestly wasn't sure if I was missing something, whether features I hadn't used or noticed, or that there was some by-convention references / magic.\nI had wanted to play around with them while testing at home, but limited available time was exacerbated by a sysadmin yak-shave.\n. Could you clarify, @DaxDupont?  \n\n...Seriously?\n\nDoesn't convey much other than you might be incredulous about something...\n. I haven't had a chance to test, but I think that this is all of the edits that I need to make.  @graphite-project/committers please have a look.\n. I haven't had time/energy to work on this, so yes, @obfuscurity (per our chat), I would be perfectly happy if you would be able to finish it.\n. Thank you for this, @cbowman0 \n. This is perfect!  Thanks, @esacteksab \n. \u2764\ufe0f \n. I hadn't heard of SO-Docs, so thanks for bringing it up!\nI can start a \"getting started\" page off the Overview or FAQ sections that\nbasically points to Synthesize, and strongly points out that this isn't for\nprod.\nOn Sun, Mar 26, 2017 at 1:14 PM, Denis Zhdanov notifications@github.com\nwrote:\n\nHi All,\nWe reach 5 votes on Stack Overflow documentation site, so, now we have\nGraphite section there - https://stackoverflow.com/documentation/graphite\nPlease share your ideas what we need to put there.\n/cc @DanCech https://github.com/DanCech @iksaif\nhttps://github.com/iksaif @gwaldo https://github.com/gwaldo\n@graphite-project/committers\nhttps://github.com/orgs/graphite-project/teams/committers\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1873, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AANKc_MK-ZzhlGSX2DNvPbZJpeW4pPgoks5rpp0EgaJpZM4Mpfkr\n.\n. Those look like welcome additions to the Docs.  I'm happy with it.  Are there any objections to a merge, @graphite-project/committers ?. The clarification is helpful!  Thank you, @wfhu !\n\nI think that the two sentences can be combined into one, but I'm ok with the change as-is.. Thank you, @bitfur!. PR Looks good.  Going to attempt to re-run the Travis job.. Builds are failing now because Ubuntu Deb repos are unavailable at the moment.  We'll need to try again later.. Thanks for the contribution!. Thank you, @earthgecko !. I'm very much pro-tests, but I'm not great at it.\nThe idea of emphasizing tests, aside from the \"less likely to break things\" also opens a category of contributions from QA/QE-minded folks who might not be comfortable writing a feature.\n. I see what you did there... working with me when I'm not good at the words... :-p\n. Wanted to call out that Whisper is the default.  What's your thinking on this?\n. My thinking is that unless tests are unwelcome, we should welcome them.  Would you care to discuss?  I can be available for a few minutes.\n. ah, I see.  I removed \"other\" because it seemed redundant to \"alternate\".\nWhat about It is possible to use an alternate storage layer rather than the default,\nWhisper, in order to accommodate specific needs. ?\n. ",
    "piotr1212": "\nthe install fails (due to /opt/graphite not being writable).\n\nhttps://github.com/graphite-project/graphite-web/pull/2409. I won't be finishing this, Grafana does this way better than I could ever built in Graphite.\n. I cannot reproduce this (running latest git version). Setting a negative Y-axis minimum from the composer works fine here.\n. I see I mixed up two commits. I'm still new to git I think I messed up with branching. I will clean it up and let you know. \n. I've created a new clean pr https://github.com/graphite-project/graphite-web/pull/308 this one can be closed\n. https://github.com/graphite-project/graphite-web/pull/249 this pullrequest addresses this issue, I'm cleaning up the the commit and conflicts.\n. I can't reproduce the 'new' bug. I've checked with and without the patch. The metrics order doesn't filp here.\n. I used your url and tried a different one as well. I'm using carbon 0.9.9 I\ncan try with different carbon this weekend (if needed)\nOn Fri, May 3, 2013 at 5:08 PM, Jeff Schroeder notifications@github.comwrote:\n\n@piotr1212 https://github.com/piotr1212 Using the same url I did and\ngraphite-web master with a 0.9.10 carbon? It might be a bug in\nnon-megacarbon carbon that has already been fixed in upstream carbon.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/307#issuecomment-17399380\n.\n. Offset does not work when I specify the first argument as constant.\ne.g.\noffset(100,some.metric)\n\nThe other way around offset(some.metric,100) does work\n. I do understand that offset is not intended for that purpose. I wanted to illustrate that offset is not a viable alternative for some specific diffSeries() use-cases.\n. I do understand that offset is not intended for that purpose. I wanted to illustrate that offset is not a viable alternative for some specific diffSeries() use-cases.\n. > I should note that for us, some metrics current values are retrievable from the cache, while others aren't. I don't know if this is because of a hashing problem or because of the sheer number of metrics in the cache at any given time. But in either case, the fact that it retrieves some of them means it shouldn't be a big deal to update the code to retrieve any of them, right? In our case, we're not using wildcards, if that makes a difference.\nThis issue is about metrics for which a .wsp file does not exist yet. They never return results. I don't get why it would sometimes return data for you. In https://github.com/graphite-project/carbon/issues/782#issuecomment-396078840 you indicated that the metrics you are having trouble with already have a .wsp file. Is this another issue you are experiencing?\n. @takluyver While I was working on porting carbon https://github.com/graphite-project/carbon/pull/656 it turned out a lot more complicated than to just run some automated tools. Issues which you'll run into: str<->unicode mess, functions returning different types, missing testcases, imported libraries which are not py3 compatible yet. . @ArturGajowy Do you mean 1.0.2 in EPEL 7 repository? For that to work Django will need to be updated first.. @ArturGajowy I'm still not really sure which packages you are referring to.\n. > I tried installing from pip, but got this:\nI'm quite sure pip does not install in /usr/lib/python2.7/site-packages/graphite by default. Make sure to remove old installations.. This happens on Fedora 21 and RHEL6. \n. I've copied the wsp to a 0.9.12 rhel6 system and it shows the same error... \n. Python 2.6.6 (r266:84292, Nov 21 2013, 10:50:32)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport sys\nsys.float_info.epsilon\n2.2204460492503131e-16\n. That was on rhel6,\n\n\n\nf21 gives:\nPython 2.7.8 (default, Nov 10 2014, 08:19:18) \n[GCC 4.9.2 20141101 (Red Hat 4.9.2-1)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport sys\nsys.float_info.epsilon\n2.220446049250313e-16\n. https://piotrp.fedorapeople.org/graphite/cpuUsage.wsp\n. btw, this is carbon's cpuUsage metric. It starting at -0.25 is probably a (minor) bug too.\n. another reason for https://github.com/graphite-project/graphite-web/pull/2409. fixed  in  https://github.com/graphite-project/graphite-web/pull/1343. My request is for graphite-web to support jump_fnv1a so I can use graphite-web in combination with carbon-c-relay in jump_fnv1a_ch mode. Now graphite-web's carbonlink queries fail because they query the wrong cache. \n\n\n\nFrom what I can tell the hashring (used for carbonlink queries) is implemented here:\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/hashing.py\nI could open a second issue on carbon to request it implemented in carbon-relay, but I personally don't need it as I don't use carbon-relay.  I'm not sure if I understand why this issue is closed.\n. Thinking more about it, it would make sense to have the ConsistentHashRing class not duplicated in the two projects. ConsistentHashRing could be defined in carbon and imported in graphite-web (unless you want to prevent dependencies between the projects). \nA quick check between https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/hashing.py#L45-L100 and https://github.com/graphite-project/carbon/blob/master/lib/carbon/hashing.py#L8-L62 shows that there are slight differences. I haven't looked into details so don't know if there are issues, but I would expect it to be identical.\n. Amazing!! thanks\n. Just tried it on a quite clean centos7 and it just works for me. Are you sure you cd into the correct directory?\nbash\nyum install python-virtualenv libffi-devel python-devel gcc\nvirtualenv /opt/venv\nsource /opt/venv/bin/activate\npip install -U setuptools\npip install -U carbon\npip install -U graphite-web\ncd /opt/graphite/webapp\nPYTHONPATH=/opt/graphite/webapp/ django-admin migrate --settings=graphite.settings --run-syncdb. Ok, but why am I getting 404's on any request I do when setting URL_PREFIX? And also why am I getting prefixed urls with only setting WSGIScriptAlias and no URL_PREFIX. Is this an issue with my system?\n. Looking at it again it got me even more confused. \nWhen configuring:\nWSGIScriptAlias / /usr/share/graphite/graphite-web.wsgi\nURL_PREFIX = '/graphite'\nGraphite works when requesting http://localhost/graphite/ (only with trailing slash)\nWith:\nWSGIScriptAlias /graphite /usr/share/graphite/graphite-web.wsgi\nURL_PREFIX = '/'\nGraphite also works when requesting http://localhost/graphite (works without trailing slash)\nBut when I configure both settings, as suggested here https://github.com/graphite-project/graphite-web/blob/master/examples/example-graphite-vhost.conf#L48-L58\nWSGIScriptAlias /graphite /usr/share/graphite/graphite-web.wsgi\nURL_PREFIX = '/graphite'\nI get 404's I can get access to the webinterface on http://localhost/graphite/graphite/ but the static content is not served.\nhttpd config https://paste.fedoraproject.org/442760/55295581\nlocal_settings.py https://paste.fedoraproject.org/442761/29603147\naccess log https://paste.fedoraproject.org/442762/29716147\n. @brutasse thank you for commenting. It all makes sense to me now ;)\n. What do you want to have on the download link? Preferred way to install is using pip install xx or docker run xx. . Can't we just get rid of the wikidot page. It all seems dated which is only confusing users.. Launchpad also has a lot of info in the answers section. Sometimes people need to download an older version, so not sure if that needs to be removed.\nAnyway, I see no point in keeping the Wikidot. Who is admin there?. I see that @mleinart is site admin at wikidot. .  I'm the Fedora/EPEL Graphite package maintainer. If not too much trouble I would be quite happy with 1.8 support in Graphite. Django packaging is a bit of a pain in EPEL. Supporting 1.8 would make it easier to get Graphite 1.x in EPEL 7. . https://github.com/graphite-project/graphite-web/pull/2409. Which version of graphite-web and carbon are you using?\nYou say if you select one month, you get a resolution of 300s. The archive 300:26298, is only 26298/(60*24) = 18,2625 days. This doesn't make sense to me, it should show a 900s interval on one -1month. \n. You do actually have that timezone info file installed? On my system it is in:\n/usr/share/zoneinfo/America/New_York. > but the local.setttings.py doesn't seem to recompile.\nNot sure if this is a typo, but it should be local_settings.py (underscore instead of dot).\nAlso check if there is an local_settings.pyc or .pyo in the same directory, check if the server user has write access to those files and/or delete them/set correct permissions.. Well to find out, set a completely different TIME_ZONE, and check which error message it will spawn then.. So it doesn't pick up your local_settings.py\nWhat do you exactly mean with \"recompile the server\"?. Then I think it is looking in a different path for the local_settings.py. You can check if you have more than one local_settings on your system or strace the httpd server to see what it actually opens\n. Hashing is not used for security purpose but for distributing metrics to different nodes in a cluster. Just replacing the hashing algorithm will break metrics distribution in relay's (carbon-relay, carbon-c-relay and carbon-relay-ng). Latest graphite-web, carbon, and carbon-c-relay do support fnv1a, you might try to use that (but you will need python 2.7). Or just disable the policy as it doesn't make sense in this context.. Yes, OS policy. \nIf you are on graphite 1.0.2 you can switch from carbon_ch md5 based hashing to  fnv1a_ch, see: CARBONLINK_HASHING_TYPE. You'll need to update your relay config as well.. Yup this is a Red Hat specific issue with Django, see: https://code.djangoproject.com/ticket/28401. Nothing we can do about it. I cannot reproduce this specific exception on clean 1.0.2 and master (I do get carbonlink exceptions though). I've recreated your path structure and using the same curl call. Are you using some specific configuration?. Are you running in apache httpd and mod_wsgi?\nIf so, could you please check if you get the same error when you add:\nexport LANG='en_US.UTF-8'\nexport LC_ALL='en_US.UTF-8'\nto /etc/apache2/envvars\nI don't know if casting 'unicode' objects to str is the right thing to do in this case.. \nedit: formatting\n. > Yes, I use apache. But setting locale not fix the issue in my environment...\n\nAlso, I've tested this case at docker-graphite-statsd container and all work as expecting!\nIt looks like there really is a problem in the locale. I will continue to investigate.\n\nYes I too think it is something with the locale. Just can't figure out what, still cannot reproduce your issue (in ubuntu docker image). You might try adding lang='en_US.UTF-8' locale='en_US.UTF-8' to your WSGIDaemonProcess directive in your httpd vhost config. \nBut remember, even if you get this fixed you will run into other UTF-8 issues. My advise would be to just use ASCII unless you are willing to spend a lot of time fixing issues.. I've looked into making unicode work a while back. There happens a lot of unicode to str casting which it shouldn't. This should be not that hard to fix. \nThe harder part I run into is the font rendering in render/glyph.py. The current implementation uses \"Cairo's toy text API\" which should be sufficient for most users of latin and cyrillic fonts. The toy API cannot handle more complex situations, for example when a font does not contain a specific glyph it cannot fallback to a different font which does have it. It doesn't support fonts with different writing directions like Chinese and Arabic and doesn't support variable width fonts like Indic and Thai.\nThe solution would be to implement Pango font rendering. I've did an attempt at this and got it basically working but run into many spacing and positioning issues. I think glyph.py would need some mayor refactoring to get it working properly.. Will do, I think I solved this specific issue in a different way. . > Is there a way to indicate carbon to keep one value per month ?\nYes, you can configure the storage schema accordingly, see:\nstorage-schemas.conf\n\nThis kpi is calculated on another system and cannot be computed using daily values ( nb of distinct user using the service: one customer can use the services 2 diff\u00e9rents days and must be show as 1 customer ).\n\nNot sure if I understand this, why can't you compute that value more often? eg (pseudo code), select distinct count(username) from logins where login_date > day(-31) and schedule that every minute. . @YouriAndropov Yes it is. We would prefer to add support to Graphite-web than to just disable it in carbon. To support utf-8 properly would take a lot of time. Maybe it would be better to fix separate issues incrementally.. Might be some missing fonts. You could try to yum install dejavu-sans-fonts dejavu-serif-fonts to see if that helps.. Wouldn't it make sense to drop all the dasboarding features in favor of Grafana (and just keep the composer)? I guess there is a dozen more XSS bugs.. Yes, I still use the \"composer\" tree based view. I really miss that in Grafana, it is really usefull if you are browsing metrics when you don't know what you are looking for. The dashboarding part always felt clumsy and buggy in Graphite.. I guess this should help too https://github.com/graphite-project/graphite-web/pull/2409. Cool, you can add your tool to: https://github.com/graphite-project/graphite-web/blob/master/docs/tools.rst for extra visibility. \nI know some people doing ML with graphite as source, but that are mostly company internal projects.\n. There are known issues with non-ascii characters in Graphite-web. . graphite-api is a webservice too which also does rendering. You can use whisper-fetch to query whisper files locally.\nIt could be improved, I think the carbonlink protocol could be extended. But that would introduce some challenges (globbing of metricnames and determining which cache to query).  Just nobody had time to do this, pr's are welcome.\n. | ImportError: No module named wsgi\nYou either have not installed the wsgi python module or your pythonpath is\nincorrect.\nOn Thu, Mar 29, 2018 at 9:01 AM, Abid1986 notifications@github.com wrote:\n\nHi,I am running it in chrome browser.\nIn /opt/graphite/storage/log/webapp error.log i can find below error as\nno module named wsgi.\n[Sun Mar 25 02:24:02.226890 2018] [:error] [pid 2647] Traceback (most\nrecent call last):\n[Sun Mar 25 02:24:02.226910 2018] [:error] [pid 2647] File\n\"/opt/graphite/conf/graphite.wsgi\", line 4, in\n[Sun Mar 25 02:24:02.226957 2018] [:error] [pid 2647] from graphite.wsgi\nimport application\n[Sun Mar 25 02:24:02.226973 2018] [:error] [pid 2647] ImportError: No\nmodule named wsgi\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2267#issuecomment-377141827,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACxrWlmro8nLumS2xRNV2sHQrJhzn2ynks5tjIbFgaJpZM4S-uk7\n.\n. Closing for inactivity, if you still have problems feel free to reopen.. Think this is a duplicate https://github.com/graphite-project/whisper/issues/230. apache httpd cannot find the graphite python module. Most likely the PYTHONPATH variable in /usr/share/graphite-web/graphite.wsgi is incorrect.. Try: yum install dejavu-sans-fonts dejavu-serif-fonts\n\nOn Thu, 10 May 2018, 09:38 keyboardfann, notifications@github.com wrote:\n\nHi,\nDoes anyone meet the issue, the x and y label become special character.\n[image: image]\nhttps://user-images.githubusercontent.com/1110451/39858026-e8a405a4-5467-11e8-8f54-6af83e9574f0.png\nENV:\nRHEL 7\nPython Version:\nPython 2.7.5\nGraphite Version:\n0.9.15\nPIP list:\nansible (2.0.1.0)\nasn1crypto (0.22.0)\nattrs (17.2.0)\nBabel (0.9.6)\nbackports.ssl-match-hostname (3.4.0.2)\ncairocffi (0.8.0)\ncertifi (2018.1.18)\ncffi (1.11.5)\nchardet (3.0.4)\ncommon (0.1.2)\nconfigobj (4.7.2)\nconstantly (15.1.0)\nconstants (0.6.0)\ncryptography (2.2.2)\ndecorator (3.4.0)\nDjango (1.6.11)\ndjango-tagging (0.3.6)\necdsa (0.13)\nenum34 (1.1.6)\nethtool (0.8)\nhttplib2 (0.7.7)\nidna (2.6)\nincremental (17.5.0)\niniparse (0.4)\nipaddress (1.0.22)\nJinja2 (2.7.2)\nlxml (3.2.1)\nM2Crypto (0.21.1)\nMarkupSafe (0.11)\nMySQL-python (1.2.5)\nparamiko (1.15.1)\npciutils (1.7.3)\nperf (0.1)\npip (8.1.2)\nply (3.4)\npyasn1 (0.1.6)\npyasn1-modules (0.0.9)\npycairo (1.17.0)\npycparser (2.14)\npycrypto (2.6.1)\npycurl (7.19.0)\npygobject (3.14.0)\npygpgme (0.3)\npyhash (0.8.2)\npyliblzma (0.5.3)\npyOpenSSL (0.13.1)\npython-dateutil (1.5)\npython-dmidecode (3.10.13)\npython-keyczar (0.71rc0)\npython-ldap (3.0.0)\npython-memcached (1.59)\npytz (2016.10)\npyudev (0.15)\npyxattr (0.5.1)\nPyYAML (3.10)\nrhnlib (2.5.65)\nrhsm (1.15.4)\nservice-identity (17.0.0)\nsetuptools (0.9.8)\nsix (1.9.0)\nslip (0.4.0)\nslip.dbus (0.4.0)\ntagging (0.2.1)\nTwisted (16.5.0)\ntxAMQP (0.8.2)\nurlgrabber (3.10)\nwhisper (0.9.15)\nyum-metadata-parser (1.1.4)\nzope.interface (4.5.0)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2287, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACxrWsm5-JEE2b0eVbC56s3poENXs0oZks5tw-6FgaJpZM4T5ffe\n.\n. I really don't know. As far as I can remember these fonts have always been\na requirement. Maybe they were installed as a dependency of something else\non your previous installs.\n\nOn Thu, 10 May 2018, 11:38 keyboardfann, notifications@github.com wrote:\n\nHi @piotr1212 https://github.com/piotr1212 ,\nThank you very much and it works. It's wierd that I install graphite-web\nbefore and don't install dejavu-sans-fonts dejavu-serif-fonts still works\nnormal. Is it because new version library use it?\nFann\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2287#issuecomment-388005310,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACxrWg0W8aK6xxEVVPL5AF0MfBJQr_6kks5txAqrgaJpZM4T5ffe\n.\n. With \"node case sensitive\", do you mean the names of the servers in DESTINATIONS in consistent hashing  or the names of the metrics?. You can use -1 as timestamp to have it generated at ingestion time.. > Adding -1 as timestamp worked. Somehow I missed that part of documentation, but thank you.\n\nYou didn't miss it, it just wasn't documented. ;). Are you sure this only happens with maxDataPoints? I've seen strange float rounding errors before which I couldn't reproduce. See also this one: https://github.com/graphite-project/graphite-web/issues/1124 (don't know if related).. Yes, most likely overloaded carbon process, try running more processes as they don't do multiprocessing.\nclosing, feel free to reopen if your issue is not resolved.. Rechecked and seems to work with Django 1.8. Not sure what I did wrong before... Where did you find that doc? It should be updated, it now is manage.py instead of graphite-admin.py. lgtm, Could you please add it to the tests? somewhere here https://github.com/graphite-project/graphite-web/blob/master/webapp/tests/test_functions.py#L1443-L1449\n. I think you should use StringIO instead of BytesIO\nps. run tox to run all tests on all python versions locally.. Besides of the minor grammar errors I think it is ok. Not sure if you expect me to cross check all PR's?. \nDo you got any info in the logs? Are the metrics in the wsp files or only in cache (check with whisper-dump,  whisper-fetch? What happens when you disable zipper (so that only metrics from disk are being read).\n. I've only seen select statements changed and assumed the schema will stay the same.. I always thought that was the actual behaviour. Having an optional parameter would be great.. Tested and works great thanks. \nOnly found that we have an issue in the code on Python 3. The issue was already there, so I suggest to first merge this and then fix the issue later.\nTraceback (most recent call last):                                 \n  File \"/home/piotr/venv37/lib/python3.7/site-packages/django/core/handlers/exception.py\", line 34, in inner\n    response = get_response(request)                                                                          \n  File \"/home/piotr/venv37/lib/python3.7/site-packages/django/core/handlers/base.py\", line 126, in _get_response\n    response = self.process_exception_by_middleware(e, request)                                       \n  File \"/home/piotr/venv37/lib/python3.7/site-packages/django/core/handlers/base.py\", line 124, in _get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)                           \n  File \"/home/piotr/venv37/lib/python3.7/site-packages/graphite/render/views.py\", line 118, in renderView\n    data.extend(evaluateTarget(requestContext, targets))                                                  \n  File \"/home/piotr/venv37/lib/python3.7/site-packages/graphite/render/evaluator.py\", line 28, in evaluateTarget\n    result = evaluateTokens(requestContext, target)                                           \n  File \"/home/piotr/venv37/lib/python3.7/site-packages/graphite/render/evaluator.py\", line 57, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression, replacements)                             \n  File \"/home/piotr/venv37/lib/python3.7/site-packages/graphite/render/evaluator.py\", line 93, in evaluateTokens\n    return func(requestContext, *args, **kwargs)                                                      \n  File \"/home/piotr/venv37/lib/python3.7/site-packages/graphite/render/functions.py\", line 2171, in nonNegativeDerivative     \n    delta, prev = _nonNegativeDelta(val, prev, maxValue)                                                \n  File \"/home/piotr/venv37/lib/python3.7/site-packages/graphite/render/functions.py\", line 2192, in _nonNegativeDelta\n    if maxValue is not None and val > maxValue:                                                         \nTypeError: '>' not supported between instances of 'NoneType' and 'int'\n. What are you trying to do? Is this related to Graphite? . So this patch breaks Python 2 compatibility because .decode() returns an 'unicode' object while rrdtool is expecting an Py 2 'str'.\nAlso note that the return type of os.path.realpath depends on the parameter, so if skip the encode we don't need to decode.\nThis should work on all Python versions https://github.com/piotr1212/graphite-web/commit/bff2a27ff84d992160bb036c1c4e63af6d4d2ccd . I'll try to make some tests this evening.. Well, I tested it manually and it worked in Py2 and Py3, so I think it is OK to merge. The tests are more broken than I expected, so I might not be ready today.. > graphite wsgi is consuming CPU only for read load, not for write load\nUnless you are using tags. @balasenthil-d are you using tags?. @balasenthil-d did changing to a RDBMS solve your issue?. So, to be clear you describe the following two issues, correct?\n\ndiffSeries of a datapoint with null and a number returns a number instead of null.\ndiffSeries with multiple series and the first series is empty, the operations happen on the first series in the list which have values.\n\nFor point 1 I guess that is wrong behaviour as \"null\" basically means that the value is unknown, assuming it is 0 is wrong. IMO if users want that they just have to use transformNull on the series before feeding it to aggregate. But changing this breaks the api, I think we could add an option to the function. \nWill need to look into point 2.\n. Null != 0, doing it in other functions too does not make it correct. I'm definitely not suggesting to change the default but Null means that the value is unknown. \nLets say the measurement dropped for whatever reason. For example, if you take the difference in temperature between Amsterdam and the Hague (in summer), if one measurement drops because of a network outage then it doesn't mean the difference is 25 degrees as we both know the weather is evenly bad in the whole country.\n. I don't think the database can be migrated from 0.9.x to 1.x, You'll probably need to create a new one.. This has been fixed in https://github.com/graphite-project/graphite-web/issues/2320. > version which i am using is:\npython2-django-1.11.15-1\ngraphite-web-1.1.4-1\npython-carbon-1.1.4-1\npython-whisper-1.1.4-1\nWhere did you get those package from? Did you rebuilt my Fedora specfile or are you using some internal Red Hat packages?. Could you check if your django-tagging module is out of date? I suspect it needs to be updated.. Are you 100% sure? See: https://github.com/Fantomas42/django-tagging/blob/v0.4.5/tagging/models.py#L10 does not try to import generic. Maybe there is some issue with your pythonpath?. check /usr/local/lib/python2.7/site-packages/tagging/models.py and see what is being imported, if it is from django.contrib.contenttypes.generic import GenericForeignKey then there is something off. It mus not contain the .generic part.. why are you running the devel-server? that is intended for developers and needs some extra actions for serving the static content.. There are some issue with the way that the Django internal server handles static content, I can't remember the details, I think there was some security setting blocking this.\nBut why do you want to run the Django server? The recommended way is to use one of: apache+mod_wsgi, gunicorn, or uwsgi+nginx. as described in the documents https://graphite.readthedocs.io/en/latest/config-webapp.html . The original issue as reported seems to be solved, this is getting off-topic, closing. If you still have troubles please open a new issue.. Probably installed in a different path, find / -name 'manage.py' 2>/dev/null if you don't know where. Closing as this doesn't seem an issue. Feel free to reopen if needed.. Could you please copy/paste the full error message? Do you have any exceptions in the graphite-web logs?\nWhat are the parameters you gave? Any special characters in them? (*?|/). Seems like there are some lines missing from the stack trace, quite hard to say what is wrong without them.\nWould also be helpful if you would post more info about your environment, os, python version, django version, apache version... I'm not finished yet but made some changes to the docs which I would like to get some feedback for. I've tried to simplify the docs, IMO there were too much separate pages which you had to jump back and forward to.\nI've changed the default install (settings.py) so that running collectstatic is not needed. The static files can be served directly from the app with whitenoise. Serving from whitenoise should be fast enough for most installations. this eliminates the need for configuring the static dir in the webserver (simplifies installation). From what I've read the whole purpose of collectstatic is for organisations which run multiple django apps and have separated their static files from code (in repo), so that they can update static files without having to deploy code and vice versa. As graphite's static files haven't changed in years and they are in the code repo I don't see a point to require collectstatic in the default install. Users can still run collectstatic if they want/need.\n. > I think a rebase went a bit wrong, you ended up with a copy of the commit \"fix dashboard graph metric list icon paths with URL_PREFIX\" from the master branch\nin master: 0a037db\nin this branch: 1ba4da5\nI think I merged instead of rebased. Anyway, cleaned up now.. good point. I'll have a look. But busy at the moment with more higher prio stuff.. If it breaks other functions as written here https://github.com/graphite-project/graphite-web/issues/1814#issuecomment-439843600 then I don't think this should be merged.. \nHave you tried replacing https://github.com/graphite-project/graphite-web/pull/2411/files#diff-e87e8c4fda2bf408f4864aad6741df42L207\nnumberOfDataPoints = timeRange/series.step with numberOfDataPoints = len(series). I think I once run into an issue that this was not the same (which it should..).. > I believe that the reason timeRange is used is so that all series get consolidated to the same interval, rather than series that are only active for part of the query time range being returned at a higher resolution.\nHow does that work out if data has series with different step?\n\nAnyway, I'm not sure how to fix this in a normal way. We don't use a lot of aggregation functions, so probably we can't test it well\n\nI think the real issue is that sometimes the number of datapoints in a serie does not match the expected number (timerange/step). I've run into this before and couldn't find where it is caused. I guess this happens somewhere deeper in whisper or in the code that merges whisper and carbonlink data. Not sure..\n@rickrian does it show only one datapoint for series which would otherwise not work at all or for all series (which did work before the patch)?\n. ok, for me. @DanCech What do you think?. You confused chmod with chown.\nOn Wed, 23 Jan 2019, 01:42 aussiearef <notifications@github.com wrote:\n\nThanks @deniszh https://github.com/deniszh\nNo offense. I have been a Windows user for 20 years hence struggling with\nUbuntu. I would like to install Graphite myself rather than using Docker\nfor some reasons so please bear with me.\nI will try your solution, but I am pretty sure I granted permissions to\n/var as well.\nOne question. Is it the Apache user that runs carbon-cache or is it the\nuser who started the service?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2413#issuecomment-456621748,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACxrWvUWu1QXbfXqXJqVd-dzC2vzctOaks5vF7ANgaJpZM4aNm9y\n.\n. > One question. Is it the Apache user that runs carbon-cache or is it the user who started the service?\n\nReally depends on how you started it, just run ps -ef | grep carbon. @aussiearef The permission issue and and the version incompatibility are packaging issues and should be reported to the package maintainer downstream so they can fix it. Reporting issues with Ubuntu packages can be done at: https://bugs.launchpad.net/ubuntu. it is in webapp/graphite/settings.py but you could also:\npip list | grep graphite\ncurl http://graphite/version/\n. I'm assuming this answered your question thus closing this issue, feel free to reopen if it is still not clear.. > FileNotFoundError: [Errno 2] No such file or directory: '/opt/graphite/lib/python3.6/storage/log/webapp/info.log'\nhave you tried creating that directory? That should fix it. \nThat said, your GRAPHITE_ROOT seems a bit strange, just set it to /opt/graphite in local_settings.py\n. Probably path issue, hope we can improve it with https://github.com/graphite-project/graphite-web/pull/2409\n. I've got a lot of ideas but it all depends on people having time to implement them, which I don't think anyone has, therefore I don't think it makes much sense to write them all down.\nThis brings me to the point of how to get Graphite more appealing to contribute to for new developers. I think making it easier to install would help. I still sometimes struggle to get a dev environment working on a clean linux install, and I've been using this for over 6 years. I can imagine it turns potential contributors off. Other thing is that there is a lot of old ugly code... \nI'm not in favour of any big revolutionary changes, Graphite's strongest point imo is that \"it just works\" and has a large user base. \nWhisper has it's downsides, but it is simple and it is still the only database backend which I have seen work stable in large installations. Only feature I was missing was automatic rebuilding of failed cluster nodes (but that would probably make it complex and less stable).. That fully depends on how you named it. Graphite is \"just\" a database for metrics, it has the names which you put into it.. In my attempt to make installation easier, I'm trying to get rid of the whole collectstatic mess and serve static files with whitenoise from the application as default. Then you also won't need to configure the webserver for serving static files, which makes it even easier.\nWe'll still have an option to serve static files from a webserver, but using whitnoise as default should be good enough for most installations, especially since I expect most users running grafana by now and not using graphite dashboards.\nhttps://github.com/graphite-project/graphite-web/pull/2409. Not sure but I think that carbonlink returns all datapoints in the cache.\nIf the number increases that would mean your cache size is increasing.\nCheck if your cache size is increasing in the graphite carbon metrics. If\nso, you could try to increase your max_updates in carbon conf.\nOn Mon, 4 Feb 2019, 20:35 JosephFY <notifications@github.com wrote:\n\nHello,\nUsing graphite_web-1.1.3-py2.7.egg-info\nI am pushing data every one minute. I have a dashboard with a time period\nof the last 5 minutes in Grafana. The query inspector on Grafana shows only\n5 data points returned when I refresh the dashboard.\nEverytime I refresh the dashboard for the metrics on Grafana the count of returned\nx datapoints increments the value by one in cache.log\nCarbonLink cache-query request for xxxx.xxxx.xxx returned 27 datapoints\nI am setting up a big query and it's taking long time (which is expected),\nMy concern if there is more data is being requested that's making the query\ntaking more than it should.\nIs this the intended behaviour ? it just doesnt make sense to me.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2423, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACxrWjO8uDkXCUXMH4GTe5OEjClHPM4_ks5vKItxgaJpZM4ah4yT\n.\n. How many caches are you running?. I'd double that, to the same as the number of cores, they'll have more time responding to your queries. You now got 128 graphite-web threads querying 8 carbon threads (actually more but only one can run at the same time).\n\nOr try out go-carbon.\n. > I will try doubling and get back with results, so you think the bottleneck is on the carbon side not the graphite-web side ?\nI don't know, just run top and see how much cpu they use. due to Python's GIL they cannot use more than one core.\n\nYOu think adding more process/workers for WSGIDaemonProcess would help ?\n\nYou only know when you try but my guess would be no. 128 seems a lot already. \n\nIs go-carbon ready for production environment?\n\nBooking is using it on production for some years already, they store a few million metrics/second.\n. mysqlclient 1.4 changed some internal api's which Django < 2 was depending on, that is why those tests fail. This will be restored in the next mysqlclient release so didn't bother add a constraint for that time.. I'm wondering all the time if we could automate this stuff on app startup? Just like Grafana does.\n. Static files are missong. Did you run collectstatic?\nOn Tue, 19 Feb 2019, 20:13 Shirish Kale <notifications@github.com wrote:\n\nLet me further expand on my issue.\nI have a POC machine on which I have a 4 node causal cluster, 3 core + 1\nread replica. I have followed instructions to install Carbon followed by\nwhisper as well as graphite-web. Once the neo4j causal cluster is started\nand carbon startup. I see 4 directories populated under\n\"/data/graphite/storage/whisper\" named as core1,core2,core3 and readrep1\nspecifying 4 node metrics captured by prefix which I have added in the\nneo4j.conf for the respective nodes of my causal cluster.\nThis was my proof that Carbon and whisper seems to be working? Is there\nany reason that should not be the case?\nSimilarly I have installed graphite web, and as per the URL specified when\nI go there I see this blank graphite website.\n[image: image]\nhttps://user-images.githubusercontent.com/7253980/53040848-749d3a80-3450-11e9-9dac-f85cbe40a590.png\nAs per the installation instructions this should have displayed me my\nmetrics but it does not.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2427#issuecomment-465270583,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACxrWsmRSwA_0YFj4c7kiqGnzostGyIrks5vPEzugaJpZM4bDX-W\n.\n. Then your Pythonpath is broken. I need to make some time to finish this https://github.com/graphite-project/graphite-web/pull/2409 should solve PYTHONPATH and collectstatic issues.. You have to install whisper and it must be in your pythonpath (also in the pythonpath of the process running the webapp). make sure you have django installed, it is a supported version and that it is in your pythonpath.. Did you try installing whisper?\n\nOn Thu, 28 Feb 2019, 19:34 Shirish Kale, notifications@github.com wrote:\n\nInstalling Django again versio 1.11.20 ran the graphite server but the\nerror log file still has\nWARNING: whisper module could not be loaded, whisper support disabled\nand no metrics is getting displayed in graphite-web.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2428#issuecomment-468385501,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ACxrWvfoD0AbLEcYlGqHQKVgcgr2ffAJks5vSCEngaJpZM4bXD6E\n.\n. is your whisper module installed in /data/graphite/webapp?. If your modules are installed in /data/graphite/lib why are you specifying /data/graphite/webapp as your pythonpath?. Well, you installed your modules in a location which is not known by Python. Now Python cannot find them.. you'll need to tell Python where to find its modules. hint. Python searches exactly the paths specified in its PYTHONPATH for modules. You can run python -c 'import sys; print(sys.path)' to find out the specified path. If using a custom path it would look something like PYTHONPATH=/tmp/foo/bar python -c 'import sys; print(sys.path)'. see https://docs.python.org/3.7/library/sys.html#sys.path and https://docs.python.org/3.7/using/cmdline.html#envvar-PYTHONPATH for more info.\n\nAnyway, I've closed this as this is not an issue. I think I gave you enough pointers to figure it out yourself. I can't help you any further.\n. \"bugfix release for the stable Graphite branch\", and: \"backported from the master branch\". \"if you are using\".  .decode(sys.getfilesystemencoding()) will be better. Although I think rrdtool should accept bytes... This should be changed to prefix, Ignore this commit, I'll change/rebase later.. thanx. cleaned up. Not sure what happens wit the .keep vs dummy.txt when updating. Will leave it like this for now.. ",
    "jblaine": "Example:\nhttp://i.imgur.com/TtICh.jpg\n. Weird. I fixed it locally, did a 'git commit --amend' (I had done nothing else to jblaine/graphite-web since this commit), and pushed. Commit just above this comment still shows the old file.\n. My git-fu is super weak :(\n. How embarassing. I need to just stop trying to help OSS projects until I thoroughly understand git. I always embarass myself.\n. Are you using https? I had the same problem ~2 years ago and never could figure it out. The suspicion was that there was intermittently not enough entropy for the SSL connections' encryption.\n. As additional info, everything is correct when not using pip and instead building from source.\n. That works.\n(graphite) webapp:etc-metrics# pwd\n/opt/graphite/webapp\n(graphite) webapp:etc-metrics# rm -rf graphite\n(graphite) webapp:etc-metrics# pip install graphite-web --install-option=\"--prefix=/opt/graphite\" --install-option=\"--install-lib=/opt/graphite/webapp\"\n/opt/graphite/lib/python2.7/site-packages/pip/commands/install.py:180: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n  cmdoptions.check_install_build_global(options)\nCollecting graphite-web\n  Downloading graphite-web-0.9.15.tar.gz (2.3MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.3MB 540kB/s\nSkipping bdist_wheel for graphite-web, due to binaries being disabled for it.\nInstalling collected packages: graphite-web\n  Running setup.py install for graphite-web ... done\nSuccessfully installed graphite-web\n(graphite) webapp:etc-metrics# ls graphite\naccount           composer      local_settings.py.example  metrics             settings.pyc  url_shortener  version\napp_settings.py   dashboard     logger.py                  remote_storage.py   storage.py    urls.py        views.py\napp_settings.pyc  events        logger.pyc                 remote_storage.pyc  storage.pyc   urls.pyc       views.pyc\nbrowser           __init__.py   manage.py                  render              templates     util.py        whitelist\ncli               __init__.pyc  manage.pyc                 settings.py         thirdparty    util.pyc\n(graphite) webapp:etc-metrics#\n. FWIW\n(graphite) webapp:etc-metrics# pip --version\npip 8.1.1 from /opt/graphite/lib/python2.7/site-packages (python 2.7)\n(graphite) webapp:etc-metrics#\n. Hasty close IMO.\ngunicorn is in the graphite-web requirements.txt, so apparently it is part of the graphite stack indirectly.\nhttps://github.com/graphite-project/graphite-web/blob/master/requirements.txt#L45\nAs you can see above in the output I pasted, the Python I am using is Python 2.7.\nSo, gunicorn is a dependency of graphite-web right now, is not pinned to any specific version of gunicorn, and gunicorn being fetched without that pin does not work with Python 2.7 therefore graphite-web has a bug in its overall source/dep-pinning/whatever.. ",
    "manderdev": "It looks to me like it extends halfway across the width of the chart, nothing to do with blocking lines. I'm having the same issue on a chart with threshold lines, but all the other lines on my chart are well below the threshold.\n. ",
    "bclermont": "I just make html and it look good\n. +1\n. ",
    "nleskiw": "Christopher Bowman notifications@github.com wrote:\n\nRemoteReader.fetch() has pieces that don't make logical sense.\nThe clean_cache() routine only cleans entries when the cache reaches capacity.  The check to preemptively return data out of the cache doesn't check if the data is old.\nIf a cached entry is found, it is pulled apart and returned as (time_info, series['values']).  In other parts of the routine the return value is the entire contents of the specific entry in the cache.\nThe connection object is not shared between threads.  The same thread that gets the request_lock is not guaranteed to get the wait_lock.  \nThe net of the above is something is causing the completion event to not fire before the timeout (which I set to 60 seconds during some of my testing).\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/issues/51#issuecomment-8069279\n. Multiple time ranges are not supported on one dashboard.  The idea was you change the time range on all the graphs at the same time, using the toolbar at the top.  I know this may seem narrow minded but at the time it was written all we had was the composer, so it was pretty freaking cool.  \"Dashboards\" were all hacked together  tags in static HTML files served from a separate Apache server.\n\nThe whole dashboard needs a new revision to fix all the UI feature requests, and I'm working on it.  I have to work on this in my spare time (no OSS work allowed at my current job) so, yeah, don't hold your breath.\nBryan Berry notifications@github.com wrote:\n\nFor example, I may want 1 graph \"http request latency for that last 24 hrs\" and another \"http request latency for the last week\" and even \"http request latency for the last 30 days\" all on the same dashboard\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/issues/88\n. Graphite returns a PNG image, not HTML/CSS.  So this is not currently\npossible.\n\nYou may want to check out the Graphlot interface for this kind of\nfunctionality.\nOn Oct 30, 2012 9:11 AM, \"leonidlm\" notifications@github.com wrote:\n\nI think it will be much easier if I could see the current metric value\nwhen I mouse over the relevant metric graph.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/issues/90.\n. I think that if there's at least 1 value something should be returned. safeSum of [1, 2, 3,] and [None, 2, 3] would be [1, 4, 6] \n\nAnyone concur / disagree with that? \n. The whole point of safeSum is to never return none values.  I'd recommend making an unsafeSum function that does what Dieterbe is asking for; return None as soon as the value becomes unknown.\n. This is probably just because its a float being printed to 2 decimal\nplaces.  I'll check glyph.py for the culprit.\nOn Nov 15, 2012 1:28 PM, \"Dieter Plaetinck\" notifications@github.com\nwrote:\n\nanother effect of this, is that for scale(...,5) the output becomes\nscale(...,5.0) , even though case isn't as bad, I would still argue the\nreturned target string should display the factor how I choose to display it\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/issues/103#issuecomment-10421581.\n. Hi,\n\nbonsai.js is a GPLv2 file.\nYou cannot add it to this project.\nRead this for more information:\nhttp://www.apache.org/licenses/GPL-compatibility.html\n-Nick\nOn Jan 28, 2013 9:45 AM, \"spellik\" notifications@github.com wrote:\n\nHello.\nHistory functions don`t need any GPL scripts.\nWith best regards, Oleg.\n2013/1/28 Jeff Schroeder notifications@github.com\n\nUnfortunately, the javascript file you include is GPL licensed and\ngraphite is Apache licensed. This would effectively change the license\nof\ngraphite to GPL and I don't think that is going to pass muster.\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/graphite-project/graphite-web/pull/123#issuecomment-12786516>.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/123#issuecomment-12787762.\n. Please remove it, then.\n\n-Nick\nOn Jan 29, 2013, at 12:33 AM, spellik notifications@github.com wrote:\n\nHello \nbonsai.js need for zoom. History work without external JS \n2013/1/29 Nicholas Leskiw notifications@github.com \n\nHi, \nbonsai.js is a GPLv2 file. \nYou cannot add it to this project. \nRead this for more information: \nhttp://www.apache.org/licenses/GPL-compatibility.html \n-Nick \nOn Jan 28, 2013 9:45 AM, \"spellik\" notifications@github.com wrote: \n\nHello. \nHistory functions don`t need any GPL scripts. \nWith best regards, Oleg. \n2013/1/28 Jeff Schroeder notifications@github.com \n\nUnfortunately, the javascript file you include is GPL licensed and \ngraphite is Apache licensed. This would effectively change the license \nof \ngraphite to GPL and I don't think that is going to pass muster. \n\u2014 \nReply to this email directly or view it on GitHub< \n\nhttps://github.com/graphite-project/graphite-web/pull/123#issuecomment-12786516>. \n\u2014 \nReply to this email directly or view it on GitHub< \nhttps://github.com/graphite-project/graphite-web/pull/123#issuecomment-12787762>. \n\n\u2014 \nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/123#issuecomment-12807671. \n\u2014\nReply to this email directly or view it on GitHub.\n. Is it at all possible for you to integrate this into master branch as well?   I looked at it and it appears to be non-trivial. \n\n\nOtherwise we may lose this functionality in future versions.\n-Nick\n. ",
    "danielbeardsley": "This seems like a really good idea, is there a reason this hasn't been merged in?\n. Been using this in production, works great!\n. I'll admit the docs are pretty spartan, but I thought the comments above each function were the docs so that's what I updated / edited.\nAre docs for each function stored somewhere else?\n. I've improved and re-written the doc-strings for moving{Average,Median}\n. We've been using these functions in production for a long time now and they've been great and address a real issue: speeding up slow calculations with large numbers of points, and time-scale independent smoothing of a series over a given number of pixels.\nThree things I would like to find the time to do:\n- Properly rewrite the series names so they come out as smooth(blah) instead of movingAverage(resample(blah))\n- Add some tests\n- Have the smooth() function respect a query param like smoothPixels=5 so smoothing can be adjusted on the whole graph at once instead of editing the argument in each data series.\n. ",
    "brutasse": "Maybe it hasn't always been the case but for graphite-web, 2) and 3) are supplementary. If you alter local_settings, there is no need to touch the environment variable and vice versa.\ncarbon and graphite-web being separate projects that can be deployed in separate places, I don't see their config files being merged soon. I'll close the issue for now.\n. @obfuscurity which upstream patches are being removed?\n. I read it the same way as you did but it is only poor wording for \"patches made redundant by upstream changes\". Apparently these patches were applied to their 0.9.12 package and are not required anymore with 0.9.13.\nCurrent list of patches: https://sources.debian.net/src/graphite-web/0.9.13%2Bdebian-1/debian/patches/\nPatches applied to 0.9.12: https://sources.debian.net/src/graphite-web/0.9.12%2Bdebian-6/debian/patches/\n. An actual patch would be more useful than +1's :)\n. It'd be nice to mention this change in the 0.10 release notes.\n. @virtuald the one-minute limitation is here because of the default cache timeout, which is set to 60 seconds. You can set it to a lower value in your settings (DEFAULT_CACHE_DURATION) or disable cache entirely to get updated graphs at a greater frequency.\n. Since the creation of this issue Django gained static (app assets) and media (user uploads/data) separation and master now properly uses STATIC_URL. Closing.\n. @rezib yes. This requires a merge of #580 and then some additional work in a bunch of templates and static files. There are existing PRs related to the issue (#328, #349) but they're not ready.\n. To everyone who upvoted this issue, please give #797 a try. Feedback is welcome.\n. Fixed with #797\n. Fixed with #797\n. Pull request #480 fixes what appears to be the same issue. The patch needs a tiny bit of additional work.\n. @Finkregh not only 0.9.11 but 0.9.12 have been released some time ago:\nhttps://pypi.python.org/pypi/graphite-web/\nFor all releases: https://pypi.python.org/simple/graphite-web/\n. Not like this. It's been fixed in a better & more extensive way in master: https://github.com/graphite-project/graphite-web/commit/9c10e9e74c195010a07bafe765b68c8bc811dfb5\n. @esc if you want a proper fix, backport the patch that was applied to master :)\n. Fixed with #797\n. I do not think this should be merged as it is.\n- URLs like ../metric/find/ are necessary because the composer view is available under /, /composer and /composer/ URLs. Switching to metric/find/ will break /composer/. Same for the dashboard view\n- Static files should be updated to django's staticfiles handling, see my last PR about it (#580).\nThe very good part here is the addition of named URL patterns. Raw URLs in templates are not robust and switching from <a href=\"/dashboard/\" to <a href=\"{% url \"dashboard\" %}\" is something that's highly desirable because it expands to absolute URLs and raises errors if the URL can't be resolved.\nWould it be possible to restrict this PR to only touch the URL resolving part and removing raw links from the HTML/JS source code? I'm addressing static files in #580.\nOnce #580 is merged and we use {% url %} everywhere it will be very easy to fix #327 properly.\n. Ok let's start again :)\nSay you have graphite deployed at https://graphite.mycompany.com\nYou visit https://graphite.mycompany.com/ and end up on the graphite browser view.\nbrowser.js makes an AJAX request to metrics/find/ which resolves as https://graphite.mycompany.com/metrics/find/. All good.\nNow you visit https://graphite.mycompany.com/browser which is also a valid URL for the browser view.\nAgain, an AJAX request to metrics/find/ resolves as https://graphite.mycompany.com/metrics/find/.\nNow you visit https://graphite.mycompany.com/browser/ (note the trailing slash)\nAn AJAX request to metrics/find/ resolves as https://graphite.mycompany.com/browser/metrics/find/ and raises a 404.\nIf we keep ../metrics/find/, all the cases explained above are covered without any 404.\nThe current state is clearly not robust but changing paths like this makes things worse.\nWhat I'm suggesting instead is to always use URLs that start with a / (so /metrics/find/) but to resolve them instead of hardcoding them because resolving them would lead to a simple fix for #327.\nChanges I'm suggesting:\n- Use {% url %} in .html templates instead of hardcoded paths (partly done here)\n- Provide these URLs to the javascript code as well with data- attributes. For instance with <body data-metrics-find-url=\"{% url \"metrics_find\" %}\"> the JS code can make an AJAX request using the info in document.body.dataset.metricsFindUrl.\nThis way we're 100% sure all URLs are valid. Then we can introduce a prefix in the graphite URLs (empty by default) for namespacing a graphite deployment (example.com/graphite/ instead of graphite.example.com/ for instance) and URLs will keep working just fine everywhere.\n. @titilambert not for me, things like url(../content/img/\u2026\") should not be changed yet -- see my previous comment.\n. Was this only fixed in 0.9.x? Looks like master would benefit from the same patch\n. Please see my last comment on #328, merging this will break some views\u2026\n. Yes, it's not ready. And it's something that should be done in master IMO.\n. @esc I've discussed this rather extensively on https://github.com/graphite-project/graphite-web/pull/328#issuecomment-32132896 and a proper fix needs to be done in a couple of steps. The required changes are:\n- Update to use django's staticfiles app\n- Use named urls, reverse and {% url %} everywhere\n- Introduce an URL prefix\nI have an open pull request (#580) against master for the first of these steps but since I don't have commit bit I can't move this forward. And I'm not very motivated to work on subsequent steps while the first isn't in yet.\nFurthermore since 0.9.x and master have diverged a bit it's probably going to be painful to port the changes between the two branches. I'd rather improve master than spend lots of time backporting stuff to a branch that's going to be abandoned sooner or later :)\n. @obfuscurity :sparkles: indeed!\nI'll move forward with my plan. I'd really call it a feature and not bother with 0.9.x.\n. The issue is that constantLine returns a 2-datapoint series, which usually gives a much lower resolution than the other series. That breaks normalize() and if it didn't it'd be hard to diff datapoints 2 by 2.\nconstantLine is useful for drawing lines but for calculations it'd be nicer to just support constants when appropriate. If you were to \"fix\" constantLine, how would you choose the step of the created series? There's no way to know what resolution the other series is at.\n. @g76r your python and django versions are fine. It must be something else\u2026\n. @mbogosian do the two machines share the same whisper dataset?\n. I'm not sure 1.5.7 is the minimal version we support. 1.5.6 works, for instance. If we find actual incompatibilities I'm all for restricting options but I don't see a good reason for requiring >=1.5.7.\n. Welp, should have read the backlog. Sorry\u2026 I ran the graphite-api tests with 1.5.6 and everything went fine so I assumed 1.5.6 worked but maybe I'm not covering this case. I'll have a closer look.\nIn the meantime, count my comment above as a +1 for >=1.5.7 :) I think some OS still have 1.5.6 so it might be worth trying to support >= 1.5.6 if that's doable.\n. No, not planning to backport the url prefix feature.\nThat said, if (\"{{queryString|escapejs}}\") should be written differently. A data-attribute and a check against document.body.dataset.queryString would be much safer and wouldn't need |escapejs.\nAnd instead of adding |escape in templates we should be removing all occurrences of {% autoescape off %}.\n. This has been fixed in #589. Could somebody close the issue?\n. @tsabirgaliev looks good!\n. @esc great!\n. @esc do you think this should be documented? A note in the 0.10 release notes would probably be useful. Other than that, looks OK to me.\n. I just added back an explanation about preloading the search index in graphite.wsgi.\n. Travis is still green, see https://travis-ci.org/graphite-project/graphite-web/builds/15673978\n. @SEJeff updated the code to use stuff or settings.STUFF.\nThe cyanite code isn't in the branch itself anymore. But I'll add __slots__, thanks.\n. @obfuscurity will do! I talked with @SEJeff a bit and I also need to add tests+docs to the branch.\nI'm less often in front of a computer at that time of the year but I'm hoping to get this done in the next couple of days.\n. @obfuscurity sorry, was pretty much completely afk these past 2 weeks :) Normal schedule shall resume soon.\n. Ok, I added a basic test that demonstrates custom finders and a new section in the docs about available finders + how to write custom finders.\nLet me know if anything's missing. I couldn't help but fix missing imports while I was here so there is a bit of noise not so related to the PR\u2026\n. @Dieterbe yes, that's the expected behavior.\n. This shouldn't be necessary: the graphite composer already has an \"auto-refresh\" mode that updates the image. It just relies on a common trick which is to inject a querystring parameter in the image URL to make the browser think the image has changed. Using a timestamp is common for this. That's what the composer does:\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/content/js/composer.js#L104-L113\n. Fixed with #797\n. :thumbsup: from me, FWIW :)\n. @obfuscurity what workflow would be broken by adding dependencies to setup.py? python setup.py install from a release tarball would keep working and pull deps automatically. And it would work just fine in a virtualenv (well, not completely because of the prefix in setup.cfg)\nWhat do you do when installing from source? python setup.py install && pip install -r requirements.txt?\nBeing able to have a full graphite environment with pip install graphite-web would be a nice win IMO.\n. @mleinart the only place where I see deps is in the bdist_rpm section here. I'm not familiar at all with RPM generation though.\nCurrently if you pip install graphite-web as a user in a virtualenv it fails with a permission error (trying to write to /opt/graphite) and as root it works but doesn't pull any dependency.\nIt's not ready for merge but I have played with packaging in a branch of my fork: https://github.com/brutasse/graphite-web/compare/master...feature;packaging\n. glad to hear, @mleinart @drawks :)\nRe. pyparsing, it seems the latest versions supports both python 2 and 3: https://pypi.python.org/pypi/pyparsing/2.0.1\n. @obfuscurity I'd be fine with the prefix if it could be disabled. There is no way to disable it via the command line. The only option if you don't want it is to edit setup.cfg and remove it.\nIf it gets removed you can always get it back with pip install graphite-web --install-option='--prefix=/opt/graphite'. But once it's there, there is no way back.\nThis prefix prevents very legitimate workflows like installing in a virtualenv or in the global site packages. I have never seen anyone familiar with the python ecosystem not pestering while packaging/installing graphite :)\nHere's what a more standard layout would look like following the principle of least surprise:\n- code in /usr/share/python/graphite if isolated from the system site-packages, otherwise in the site-packages (the user decides)\n- config in /etc/graphite\n- data in /srv/graphite\n- logs in /var/log/graphite\nEverything can still be configurable but these would be much saner defaults.\nObviously this would take more effort than simply removing the prefix from setup.cfg but I do think it'd be easier for users/admins this way.\n. @esc The prefix is set in setup.cfg. It's not dynamic, not overridable\u2026\n. @esc I don't think so but I'd love to be proven wrong\n. That's been changed in Django 1.3\nThe proper fix would be to convert the layout to work with contrib.staticfiles and add a section about the collectstatic step in the deployment docs.\n. :thumbsup: \n. python manage.py is equivalent to django-admin.py --settings=graphite.settings.\nI guess there are a couple of options:\n- Document django-admin.py --settings instead of manage.py\n- Add a proper console entry point to bin/ that acts as a wrapper similar to manage.py.\n. @abhinav-upadhyay would you be able to provide a complete example of HTTP request that fails for you? Including headers, body (if any), params\u2026 everything.\n. Hah, it happened to me as well.\nI tried replacing the gunicorn process with a straight django-admin.py runserver and I cannot reproduce the problem anymore. Switching back to gunicorn, getting mixed pngs/json again. Is everyone here using gunicorn? If so, please post versions / proxy info (nginx?). Try running it with runserver (only for trying this obviously), see if the problem disappears and please post some information about your web stack.\n. Correction, it seems to be a caching issue. I managed to reproduce the issue reliably :)\nA workaround for now is to disable caching completely (django uses local memory by default). Add to local_settings.py:\npython\nCACHES = {\n    'default': {\n        'BACKEND': 'django.core.cache.backends.dummy.DummyCache',\n    },\n}\nThe issue most likely lies in the way graph cache keys are derived from incoming requests. I'll work on a fix for this but not until next week though.\n. I found something.\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/hashing.py#L24\nChange request.GET to request.REQUEST. POST parameters are basically ignored at the moment when computing the cache key. I don't know if request.REQUEST.lists() is a thing but that could be a start if anyone wants to try.\n. I believe I found a solution for tox's incompatibility with the install prefix :) See https://github.com/brutasse/graphite-web/commit/d231f2f7f6bd377acba96f5db5d8e88bd6b97f20\nBasically, the install section is manipulated in setup.py. By default it sets the prefix to /opt/graphite like the current behavior. However, if an environment variable is present (GRAPHITE_NO_PREFIX) it won't set the prefix to keep installers' default behavior.\nTox sets that environment variable so graphite-web can be successfully installed in all the tox environments.\nWhile the codebase isn't ready for a prefix removal (conf/storage/log paths may end up in the site packages right now), this makes an opt-in solution already possible and local testing is now really seamless.\nRefs #555 \n. Just merged the current master into this PR. Anything else needed for this to get in?\nThe docs build target is nice to have on its own as pull requests over the past month introduced a bunch of sphinx warnings :)\n. @SEJeff should be mergeable now\n. What's the status of this? Anything I need to do to help getting it merged? People are working on the same issue, I'd be nice to avoid duplicate effort and conflicting PRs\u2026\n. Branch updated, should be clean to merge again.\n. There's something weird here: https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/remote_storage.py#L167-L177\nIf the condition L167 returns False, connection is never created and you end up with the NameError L182.\n. Thanks for the merge :) :thumbsup: \n. I can't reproduce this with graphite-api. Could anyone try it and let me know if the issue is still here?\nhttp://graphite-api.readthedocs.org/en/latest/\n. Ah there are notes for 0.9.12 but not in master:\nhttps://github.com/graphite-project/graphite-web/compare/0.9.11...0.9.12\n. Just cherry-picked the 0.9.12 release notes\n. That should almost be possible. You're talking about the /render/ endpoint, right? It supports POST requests, although not in JSON but as HTML form data (application/x-www-form-urlencoded or multipart/form-data).\nSo technically the graphite server can already accept data as POST. However, it'll be tricky to display graphs this way since they're just images with src set to /render/?<graph params> and there's no way to tell browsers to make POST requests when displaying images :)\nI see 2 possible workarounds:\n- Fetch data as JSON and render graphs on the client\n- Add a base64 image format to the /render/ endpoint and instead of just setting image URLs, set their src attribute to data:image/png;base64,\u2026.\n. Yeah but most web servers don't read GET request bodies. Django ignores it so there's extra work to do to support that in Graphite-web.\n. They have a request body but graphite uses requests.REQUEST which comes from the request querystring or the request body if the request is a POST. GET bodies are never parsed.\nIn any case when it comes to the original issue there is no way to have an <img src=\"/render/\"> tag enclose a body in the HTTP request it generates so we still have to change the way graphs are displayed in order to support that use case.\n. Support for JSON data is present in graphite-api\n. Your workaround isn't going to work. You'll want to install the real pytz and pyparsing instead:\nhttps://pypi.python.org/pypi/pyparsing/2.0.1\nhttps://pypi.python.org/pypi/pytz\n. https://github.com/graphite-project/whisper/commit/a6e2176eb624f0c09df399b4f8464a5a08789bd6 needs to be applied to whisper's 0.9.x branch. In the meantime you should be able to use whisper's master branch.\n. Was <!doctype html> not enough to get out of quirks mode?\n. Travis doesn't support Python 2.5. Which, considering the age of 2.5, is not surprising :). It'll become harder and harder to spot regressions for 2.5 considering how old it is and that people usually develop with 2.7 or sometimes 2.6 nowadays.\nI dropped it completely with graphite-api because supporting python 2 and 3 with a single codebase is much easier when you require 2.6 or above. I would totally support dropping official support for 2.5 in all graphite projects.\n. Yes, it's been fixed for the 0.9 branch in #531. The patch should also be applied to master.\n. graphite-web 0.9.12 doesn't contain the fix -- the patch is 2 months old and 0.9.12 was released in august 2013. Same for evernote graphite-web -- it looks pretty much out of date and doesn't contain the fix.\nYou need to install from source using the 0.9.x branch.\n. I can't find it right now but I have opened a pull request for this.\nOn Tuesday, 4 February 2014 at 01:44, Peter N wrote:\n\nIf I'm reading the configuration correctly, using django >= 1.5, this configuration should work:\nCACHE_MIDDLEWARE_SECONDS = 60 CACHES = { 'default': { 'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache', 'LOCATION': [ '127.0.0.1:11211' ] } }  \nMy vagrant dev environment is not in any shape to test this. @msn (https://github.com/msn) if you have a chance to try this that'd be helpful.\n\u2014\nReply to this email directly or view it on GitHub (https://github.com/graphite-project/graphite-web/issues/612#issuecomment-33973329).\n. Fixed in #589. Could somebody close the issue?\n. Closing as it's been fixed in master.\n. @esc I did 2 comments above -- https://github.com/graphite-project/graphite-web/issues/612#issuecomment-38070387\n. Could somebody close this? It's been fixed in https://github.com/graphite-project/graphite-web/commit/a46163c80a68586c0395670de3689208585c742a\n. Django 1.4 (which is the minimum required version) is 2.5+ as well. Current master doesn't support python 2.4\n. Try disabling graphite-web's internal cache. Graphs are cached in memory by default. This is how you disable caching:\n\nhttps://github.com/graphite-project/graphite-web/issues/576#issuecomment-36246428\n. What about closing the two issues given that offset() does the job here?\n. There's some great stuff here, but some definite overlap with existing pull requests\u2026\nRequestContext + custom context processor is not needed with proper use of contrib.staticfiles -- see #580 which addresses the static files part by using django's standard static file handling and allows extra niceties such as cache busting and properly serving the admin's static files.\nThe part where you resolve graphite URLs using {% url foo %} is good. I would probably have done it differently than an inline <script> though, by using data-attributes on the <body> tag:\nhtml\n<body data-metric-find-url=\"{% url \"metric_find\" %}\" data-metric-expand-url=\"{% url \"metric_expand\" %}\" [etc etc]>\nAnd then you can use that info in the JS files:\njs\nnew Ext.tree.TreeLoader({\n    url: document.body.dataset.metricFindUrl,\n    // etc etc\n})\nWhen you use url(\u2026) in an url pattern it's good to provide a name= keyword argument so that you can do {% url \"given_name\" %} instead of {% url \"graphite.full.path.to.actual.view\" %}.\nI would suggest submitting separate pull requests for the 2 fixes you made (the pickle one and the json one) because they would get merged easily (I'm not a graphite committer though, so take my word with a grain of salt ;). More substantial changes need time and review, it's best to keep things separate.\nIf you could:\n- Build this on top of #580 and remove the use of RequestContext\n- Add name= to your URLs and call them by name instead of dotted path\n- Use document.body.dataset instead of a global object\n- Investigate why the build is failing on Django 1.4\n\u2026 it would be fantastic :)\nReferencing related issues / PRs I found so far:\n- #580 (static files),\n- #223, #327, #349, #524, (prefixed graphite installs),\n- #328 (relative URL paths)\nPrefixed graphite installs can be addressed fully in another PR once this is merged in. Doing things incrementally might be slower but has a better chance of success :)\nThanks again!\n. Addressed in #797\n. Nice :)\nLast time I noted there were also imports from the md5 module as fallbacks in the absence of hashlib.\n. :thumbsup: \n. @SEJeff :thumbsup: from me about requiring 2.6. Would also make it easier to support python 2 and 3 with the same codebase.\n. @johntdyer definite bug -- I've submitted #636 which makes the API saner.\n. @johntdyer so right now tags is always treated as a string, whether it's at creation time or serialization time. If you want multiple tags you can separate them with a space \"tags\": \"deploy foo\" and they will properly be created. That's also what you'll get back from get_data so you need to split on spaces if you want arrays.\nAnyway, an array-only API seems more like what people will expect.\n. FYI I have fixed this issue in graphite-api.\nSee https://github.com/brutasse/graphite-api/commit/c8854f584b4ba216f9706fa5bf42597a4f9c969c\nfrom/until are resolved using the provided timezone and legends are displayed in the requested timezone as well.\n. I have a working proof-of-concept. It uses Flask instead of Django and only provides the following API URLs:\n- /metrics/find\n- /metrics/expand\n- /metrics/search\n- /render\n(which should be enough for compatibility with existing 3rd-party dashboards).\nIt has pluggable storage backends like graphite-web master and I added pluggable functions while I was at it.\nI developed it using python 3 and the goal is to support Python 2.6 and above. Whisper needed a couple of adjustments but it was pretty simple.\nI still need to clean it up but I'll put this up online soon if there's interest :)\n. Here you go :) https://github.com/brutasse/graphite-api\nThe build is failing only because coveralls.io is broken. It'll get fixed at some point :)\n. I guess this fix shouldn't have made it to the 0.9 branch and should be reverted.\nOtherwise I support having a minimal requirement of Django 1.4 for next release, which is already the case: 1.3 is EOL'ed and Django 1.4 has been labelled as \"LTS\" by the Django core developers, meaning it'll get security updates for longer than usual.\nThe next Ubuntu LTS is due in a month and should provide Django 1.6.\n. @obfuscurity fixing this is just a matter of removing the /opt/graphite prefix in setup.cfg. But it's a big backwards-incompatible change and messes all paths in graphite's settings. Last time we talked about this, the conclusion was to keep the prefix in 0.9 and make a possibly breaking change in 0.10 to end up with a more common directory layout.\n. Note that URL handling has changed since this pull request was created. See the changes made in #797\n. Duplicate of #597 \n. Ah, right. My bad.\n. I was about to comment on #606:\nA proper fix would use os.path.relpath. It gives the same output, whether there is a trailing slash or not.\n```\n\n\n\nos.path.relpath('/opt/graphite/storage/whisper/systems/u/ubuntu', '/opt/graphite/storage/whisper/')\n'systems/u/ubuntu'\nos.path.relpath('/opt/graphite/storage/whisper/systems/u/ubuntu', '/opt/graphite/storage/whisper')\n'systems/u/ubuntu'\n```\n\n\n\nThen you can safely .replace(os.sep, '.') and end up with a clean metric name.\n. Also worth noting that build_index could query the storage backends instead of the filesystem paths. That's what I did in graphite-api for a storage-independent indexer. It's ridiculously simple:\nhttps://github.com/brutasse/graphite-api/blob/master/graphite_api/app.py#L205-L223\n. :thumbsup: \n. When dumping the data, exclude contenttypes and use the --natural flag:\ndjango-admin.py dumpdata --settings=graphite.settings -e contenttypes --natural > dump.json\nThen try the loaddata command again.\n. Multiple cactiStyle() calls are not aware of each other. You need to have a single function call.\nTry this as the only graph target: cactiStyle(aliasByMetric(casa.test.nono.system.memory.*), \"binary\")\n. @PrFalken that would require some pretty deep refactoring of how the grammar works\u2026 Certainly not impossible but it's something that implies significant changes outside of the cactiStyle function itself.\n. :thumbsup: \nIt'd be nice to have a release branch, like with 0.9.X. That way you create the branch and can do all release-related things there (RC, etc) without having to freeze master. When fixing things, fix in master and backport to the release branch. I fear that some issues will reappear with the release as some fixes only made it to 0.9.X and not master.\nRegarding the docs, they are built automatically on readthedocs: http://graphite.readthedocs.org/ (someone with enough permissions please replace the project URL with that, the repo home on GitHub still points to wikidot). So IMO no action is required here. Just mark the 0.10.0 release notes as final when cutting the release.\nCarbon & whisper releases are needed as well, with some pull-requests cleanup on these projects. Ceres has never been released so far so I don't know what the status is.\n. Some fixes even made it to 0.9.x but not to master. I tried to resync things once but gave up\u2026 +1 for bugfix-only branches.\n. @obfuscurity why the need for Django >= 1.5? There is very little django in graphite-web, it's easy to support 1.3 all the way to 1.7 (unreleased but coming at some point).\n. @obfuscurity with this pattern:\npython\ntry:\n    from django.conf.urls import patterns, url, include\nexcept ImportError:\n    from django.conf.urls.defaults import patterns, url, include\nYou get Django 1.3+ support with no warning at all.\nThe order is currently reversed, meaning Django 1.3 and 1.4 work but get a warning.\nhttps://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/urls.py#L15-L18\n. @nox here's the milestone for 0.9.13: https://github.com/graphite-project/graphite-web/milestones/0.9.13\n. This is related to #639. Basically timezone handling is fairly limited. I fixed this in graphite-api (development version). If someone wants to port the patch feel free to do so.\n. @jhwbz what did you end up doing?\n. That's probably not enough because master still does a bunch of things incorrectly.\nHere are the two relevant graphite-api commits:\nhttps://github.com/brutasse/graphite-api/commit/c8854f584b4ba216f9706fa5bf42597a4f9c969c\nhttps://github.com/brutasse/graphite-api/commit/a0ae47023cce8fe7dc2815c8ed46ffc16f3bdefe\n. @PrFalken you can't. JSON rendering only includes the datapoints.\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/views.py#L130-L167\n. In what case would it be a requirement to follow graphite's png colors? (honest question, while I don't see the need for it there might be a use case that I'm not aware of)\nWhat dashboards usually do is that they just have their own color palettes and rotate the colors between the different metrics returned by the API.\n. @gwaldo could you have a look at the Sphinx warning?\nhttps://travis-ci.org/graphite-project/graphite-web/jobs/23003950\n. @gwaldo great. I can't merge this myself but that looks good!\n. @gwaldo great. I can't merge this myself but that looks good!\n. That's a useful feature but I feel the configuration exposed is too complicated. Ideally you'd just look at a ALLOWED_ORIGINS setting and configure everything. That's what I've done with graphite-api -- a very basic CORS setup is more than enough.\n. @esc I was rather suggesting an update to the patch to simplify configuration\n. @kds119 graphite-web and graphite-api support pluggable metrics stores so it should be pretty easy to publish TSDB support using that.\nSee http://graphite.readthedocs.org/en/latest/storage-backends.html#custom-finders for more information.\nThere are already a couple of 3rd-party backends: cyanite that stores metrics in Cassandra and also influxdb / kairosdb backends in the works.\n. @kds119 what I was saying is that there is a way to integrate with non-whisper metrics storage in both graphite-api and graphite-web. And it works the same way with the 2.\nSo if you want TSDB in graphite-web, the easiest way is to write a storage finder. It'll also work in graphite-api. And you won't have to wait for a graphite-web pull request to be approved for it to be usable by other people.\n. @drawks ah right. That's a direction where we want to go with Cyanite as well so I'll definitely look into adding that kind of feature to the storage finders API.\n. @esc patch looks good -- merging.\n. Try django-admin.py syncdb --settings=graphite.settings. Or setting the DJANGO_SETTINGS_MODULE environment variable to graphite.settings.\n. What about the 2nd option? The two should be equivalent but still\u2026\ndjango-admin.py syncdb is a generic command and need to be aware of the \"project\" you want to work on, hence the DJANGO_SETTINGS_MODULE variable. If you set it and still see the same error there's probably something weird with your graphite installation but it's hard to tell from here.\n. :thumbsup: \n. Closing this as it's not really an issue. @toni-moreno feel free to submit a pull request to add it to docs/tools.rst.\n. 'consolidationFunc' is apparently the underlying exception but I don't see where it comes from.\nCan you change graphite/render/datalib.py to remove the try\u2026 except block (just keep what's in the try block) and paste the traceback again? It's here, line numbers might not match exactly what you have in your installation: https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/datalib.py#L135-L145\n. great, thanks @g76r \n. unpickle needs to be changed but not replaced here. @g76r can you provide a traceback that demonstrates the issue?\n. It seems to me we should rather improve /metrics/find to allow searching for webservers.production.*.exim.* instead of adding another API call.\n. Like @obfuscurity, I still think this change is a bit too narrow and we should rather improve other node-discovery APIs than add other very specialized API calls.\n. Fixed in #765 \n. <graphite-url>/version/ gives you the graphite-web version. Or rather, the content of settings.WEBAPP_VERSION. You could play with this and do in your local_settings.py\n``` python\nimport pkg_resources, json\nversion = lambda d: pkg_resources.get_distribution(d).version\nWEBAPP_VERSION = json.dumps({pkg: version(pkg) for pkg in ['graphite-web', 'carbon', 'whisper']})\n```\n(this requires python 2.7 which provides dict comprehensions but it can be adapted to older pythons)\n. We shouldn't expect much breakage with this change. +1 from a technical point of view but really, just use the json format :)\nI'm -1 on the change that manipulates sys.path in the settings though\u2026 Why did you add it?\n. @esc sure, +1\n. FTR I've fixed timezone handling in graphite-api with these 2 commits:\nhttps://github.com/brutasse/graphite-api/commit/c8854f584b4ba216f9706fa5bf42597a4f9c969c\nhttps://github.com/brutasse/graphite-api/commit/a0ae47023cce8fe7dc2815c8ed46ffc16f3bdefe\nSo there are a bunch of things to update for timezone issues to completely go away.\n. Who doesn't?\nMy schedule probably won't allow a fix before 2nd half of August (I'll be away starting this thursday and have a bunch of things to do before that). If anyone wants to port the patches, feel free to do so\u2026\n. Looks good. And yes, stripControlCharacters can safely be removed -- just note that it's used twice in the file.\n. Why are there docs for 1.x on RTD? Master is 0.10.\nThe table is still here in the master docs: https://graphite.readthedocs.org/en/latest/render_api.html#from-until\n. What's the issue with Django 1.6?\n. The code in the master branch has been updated accordingly. The build passes with Django 1.6 : https://travis-ci.org/graphite-project/graphite-web\n. @esc instead of inserting None values to later remove them, it doesn't take them into account in the first place. Sounds like a sane optimization.\nHowever\u2026 @powo I see you ran into the issue with 1s resolution but are you actually sending data to carbon at that resolution? If you have 1s retention but you send points every 30s, the consolidation function wastes a lot of time reading null values and discarding them. This also wastes disk space. Your lowest resolution should match the interval at which you're sending data in.\n. @Dieterbe can you elaborate? It's a weird way to prune a list but it doesn't look incorrect to me.\n. +1\n. I have the rebase locally, just need to make sure everything works fine with the recently-added URL shortener.\n. Looking good! Merging.\n. Pattern number 14 is wrong, fix incoming in #974\n. @angelatlarge can you update to latest master and try again?\n. URLs look good now. Next thing you need to do is setup your web server to serve /graphite/static/* properly. For apache, it'll be something like Alias /graphite/static/ \"/path/to/your/graphite/static\". Or with nginx:\nlocation /graphite/static/ {\n    alias /path/to/your/graphite/static/;\n}\n. Django usually redirects if the trailing slash is missing. Not sure why it doesn't here, maybe you can setup a redirection in your web server directly.\n. @dipthegeezer no, the changes are too significant to be easily ported to 0.9.x. The patch would need to be almost completely rewritten since master and 0.9.x are quite different.\n. What version of pyparsing do you have ?\n. Can you try a more up-to-date version? Latest version is 2.0.2.\n. Yes, I fixed this issue in master a couple of hours ago. Please get the latest master and things should work fine now :)\n. The development server won't serve static assets with DEBUG = False. You need to run django-admin.py collectstatic and serve them with a proper web server like Apache or nginx.\nSee the 0.10.0 release notes and the STATIC_ROOT section in the docs: http://graphite.readthedocs.org/en/latest/config-local-settings.html#filesystem-paths\nYou shouldn't have to touch any settings to run collectstatic.\n. We might want to add a pow() function and use that for invert -- pow(x, -1) -- and square root -- pow(x, 0.5).\nIn the meantime, LGTM\n. @deniszh there were 2 commits in graphite-api. The first one was incomplete, make sure you take both into account.\n. Do we know if 3rd party dashboard actually rely on that? I'd be more in favor of fixing the issue. Most dashboards using format=json being javascript-based [citation needed], we'd better make sure JSON.parse works with the returned data :)\nI'd translate +inf to 1e9999, -inf to -1e9999 and nan to null.\n. The easiest way to have working cairo bindings is to use cairocffi. It's pip-installable (requires libcairo2 of course) and works with modern python versions.\nWe should recommend that instead of the py2cairo / pycairo bindings. That's also the approach we currently use for travis.\n. Both are already supported, but I think the py2cairo requirement in requirements.txt should be replaced with cairocffi for a better default.\n. @obfuscurity the indication on travis is misleading :) Python 2.7 is used to install tox, and when running tox -e $TOXENV the appropriate python version is chosen (travis instances have all the pythons available). So the first 3 lines (py26-django1*) are for python 2.6 and we're properly covering 2.6 and 2.7. It's easier to integrate travis and tox that way than using travis's python versions handling.\n. Travis happy, merging.\n. The master branch dropped support for Python <= 2.5. You need Python 2.6 or 2.7.\n. Feel free to have a look at what's done in graphite-api. I'm using coverage.py + coveralls.\n. Graphite is popular enough that it's already provided in official distro packages (at least for debian and ubuntu). IMO, if we want to provide distro package files we have\u00a0to work with debian / ubuntu / centos people to write packaging scripts they're happy with directly in our repos. This way distro packages and user-built packages will look the same and it'll be easer for people to switch from an official package to a custom one. So, can we get in touch whith Debian / Ubuntu / Centos developers to have them bless the results of what we're doing? Or look at what they've done in official repos and start from there? We need to get to a point where distro developers don't have to maintain packaging patches. Also let's hear what they say about supporting older OS versions than current stable/LTS\u2026 I'm not sure 12.04 / debian 6 is doable, might have to check.\n(and if they're all in favor of dropping the /opt/graphite prefix, let's do it asap)\nWhat I did for graphite-api is an omnibus package (fpm) that provides all dependencies, completely isolated from system packages and with an init script that starts a service with gunicorn. I left apache/nginx configuration instructions in the docs. Debian people seem to be interested in having it packaged in sid+backports, in which case I'll merge whatever pull request they make to get a debian/ directory they're happy with. (and they probably won't go the omnibus way :)\n. Indeed, would be nice to have that in a pull request checklist :)\nPerhaps you could do what I did for 0.10.0 and add a big fat UNDER DEVELOPMENT note in the title? (and no date (what's this date format anyway :innocent:))\nhttp://graphite.readthedocs.org/en/latest/releases/0_10_0.html\n. :thumbsup: \n. :ship: it!\n. Yep, simply add a line below Chris's.\n. @andrewbaxter sure, feel free to send a pull request.\nI usually just override DATABASES in local_settings.py but that's a more elegant solution.\n. It's hard to review such a change\u2026 It looks like you've wrapped views in try\u2026 except blocks but it's hard to guarantee that nothing else changed :)\nDjango offers a mechanism for what you're trying to achieve. A middleware's process_exception method would be more appropriate: https://docs.djangoproject.com/en/1.7/topics/http/middleware/#process-exception\nJust log the exception and make sure you return None from process_exception.\n. Addressed by #954 \n. Addressed by #954 \n. The issue is, now any DATABASES definition in local_settings.py will be overwritten by this one. In production setups you'll want another database than sqlite, which is done by setting DATABASES in local_settings.py.\nI'd advise a different approach. Where DATABASES is currently defined, do\npython\nDATABASES = None\nAnd at the place where you moved DATABASES, do\npython\nif DATABASES is None:\n    DATABASES = {\n        # ...\n    }\nBetween these two blocks, there is the local_settings import which will set DATABASES to something else than None if local_settings contain a database definition.\n. @obfuscurity you mean for upgrading? It's just a new table so django-admin.py syncdb creates it.\n. Great, LGTM\n. Awesome, LGTM!\n. Awesome, LGTM!\n. Travis happy, :ship: it\n. Thank you!\n. Yeah, I plan to port the multi fetch to graphite-web at some point.\nRegarding your graphite-api PR, I'd like to see it in graphite-web too. If everyone's happy with it we can merge it in the two projects at once.\n. You can already pass timestamps manually. event.get('when', time.time()) tries to find a when field in the JSON data and falls back to the current time. How is it not working for you?\n. Indeed. It's a shame that there is no clear error message. Also since the input is JSON I'm not sure if we should allow quoted floats or refuse them. I guess changing that code to float(event.get('when', time.time())) wouldn't hurt.\n. The default test runner changed in Django 1.6, which is why DiscoverRunner is used before Django 1.6. Since you replaced that, these tests don't use the correct test runner anymore.\nYou can simply use coverage with something like DJANGO_SETTINGS_MODULE=graphite.settings coverage runwhich django-admin.pytest. Feel free to have a look at what I've done in graphite-api, there is only one build target that measures coverage data.\n. The proposed parameters don't look very obvious to me. \"jsondpfmt\", \"jsonseriemd\" and \"XmsY\" feel weird.\nI'd like to have opinions from other team members about this but the API is currently relatively simple, it'd be nice if this could be simplified. `\nOther than that, I'll have some comments on the code and we'll need tests to merge this but let's agree on the API first.\nDo we really need multiple metadata formats? Instead of ?jsonseriemd=basicstats I'd just go with ?metadata=true for all metadata and ?metadata=avg&metadata=max for selecting only the aggregation functions you need. And instead of ?jsondpfmt=XmsY I'm thinking of a new format, something like ?format=json+xmsy (or something else, not a big fan of xmsy).\n. @toni-moreno I understand your reasoning behind the API you designed but like I said, it doesn't feel obvious.\nI maintain that ?metadata=true and metadata=avg&metadata=bar is simpler and more future-proof than jsonseriemd=basicstats (will we add jsonseriemd=advancedstats in the future? Here it'd be easy to add metadata=ft or metadata=linreg or whatever).\nAnd I would like to find a more elegant solution for the jsondpformat case.\nI you want to make a separate pull request for metadata I'll be happy to review it and merge it once it's ready (unless other team members disagree).\n. @toni-moreno that's not exactly what I meant. What I'd like is:\nhttp://graphite-url/render?target=foo&metadata=avg&metadata=cur\nto give the following output:\njson\n[{\n    \"target\": \"foo\",\n    \"datapoints\": [...],\n    \"metadata\": {\n        \"avg\": 5.704,\n        \"cur\": 0\n    }\n}]\nAnd with ?metadata=true, include all stats:\njson\n[{\n    \"target\": \"foo\",\n    \"datapoints\": [...],\n    \"metadata\": {\n        \"avg\": 5.704,\n        \"cur\": 0,\n        \"max\": 1092.25,\n        \"min\": 0,\n        \"sum\": 2047.99,\n    }\n}]\nThat'd be much simpler for API clients.\n. @toni-moreno regarding metadata, if you address https://github.com/graphite-project/graphite-web/pull/980#issuecomment-59705231 and make it a separate pull request that includes docs and tests, I'm happy to consider it for inclusion. JSON format = 1 pull request, metadata = another pull request.\n. @toni-moreno we're not going to rush a feature into an open source project because you have a customer requirement :) You can always maintain your fork while we're iterating on the feature.\njsondpfmt, XmsY and YXs still look too cryptic to me. No other committers have commented so I don't know how they feel about that change.\nIn any case this will also need tests, and since it touches a public HTTP API, docs need to be updated.\n. I've done it in graphite-api, in a way that's similar to the storage backends feature. It's quite easy to load them dynamically from a user-defined value. I'd recommend requiring them to be available in the python path and not loaded from a random directory. There could be a graphite-numpy package that you can pip install, add a line in graphite-web's settings and you'd have numpy-optimized functions.\nhttps://github.com/brutasse/graphite-api/blob/master/graphite_api/config.py#L120-L122\n. Apart from the two settings, that looks correct. Two other comments:\n- We'll need the same PR agains the master branch\n- A test really would be useful here. Simply make a request with two different timezones and bounds that are supposed to be the same (07:00 to 08:00 in UTC and 08:00 to 09:00 in Europe/Berlin for instance) and make sure the epochs returned in the response actually match.\n. Ok, done in master and 0.9.x\n. That looks good! Not sure why Travis isn't building though\u2026\n. This is kind of a sensitive feature so it'll take some effort to get it into a mergeable state. That means:\n- robust, idiomatic code\n- extensive tests\n- documentation\n- modifications to the user interface to take restrictions into account. If a user doesn't have the permission to see a dashboard, it shouldn't be visible at all to them in the dashboard finder. Same applies for edits / deletes (no permission, button needs to be disabled) and for metrics (shouldn't be visible in metrics tree).\nThe current implementation probably works when you have dozens of metrics but it's going to be hard to manage in setups with hundred or thousands of metrics and lots of users. That's a non-trivial feature to implement and I'm not sure what the graphite team will choose between a) merging a partial solution to a problem and b) keeping the codebase and feature set simple (thus not merging).\nA solution that scales with large amounts of data would need at least patterns for metrics instead of exact matches and permissions tied to groups instead of single users.\nThere are also some edge cases in functions.py that would allow to bypass permission checks. Chech useSeriesAbove but there is an evaluateTarget call that could allow a user to fetch any metric as long as they have access to one metric.\n. This is kind of a sensitive feature so it'll take some effort to get it into a mergeable state. That means:\n- robust, idiomatic code\n- extensive tests\n- documentation\n- modifications to the user interface to take restrictions into account. If a user doesn't have the permission to see a dashboard, it shouldn't be visible at all to them in the dashboard finder. Same applies for edits / deletes (no permission, button needs to be disabled) and for metrics (shouldn't be visible in metrics tree).\nThe current implementation probably works when you have dozens of metrics but it's going to be hard to manage in setups with hundred or thousands of metrics and lots of users. That's a non-trivial feature to implement and I'm not sure what the graphite team will choose between a) merging a partial solution to a problem and b) keeping the codebase and feature set simple (thus not merging).\nA solution that scales with large amounts of data would need at least patterns for metrics instead of exact matches and permissions tied to groups instead of single users.\nThere are also some edge cases in functions.py that would allow to bypass permission checks. Chech useSeriesAbove but there is an evaluateTarget call that could allow a user to fetch any metric as long as they have access to one metric.\n. LGTM as well.\n. That's actually a requirement for all installations with pip regardless of the location. pip install carbon also requires Python.h.\n. @fmenabe regarding the use of JSON.parse in applyState in your patch: what version are you currently running? 0.9.12 or an installation from git master or 0.9.x branches?\n@Roguelazer any news?\n. LGTM\n. What are the image URLs that are 404'ing?\n. FWIW icons were changed recently. Did you run django-admin.py collectstatic when updating your installation?\n. Ah, there they are: https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/templates/dashboard.html#L18-L24\n. Hmm actually, they're all here:\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/content/js/ext/resources/icons/fam/cog_edit.png\nWhat graphite-web version are you running?\n. Do you have pytz installed? It's a requirement for proper timezone handling. We should probably add it / document this if that's not already the case.\n. If this gets merged in 0.9.x it really needs to go to master too. Usually the workflow is to commit in master first then backport to 0.9.x. It's easier to ensure branches don't diverge too much with this process.\n. I just pushed a fix in master, please give it a try!\nUSE_TZ should really be left at True. There is no other sane value :)\n. We force USE_TZ = True in the global settings. So even if a user sets it to False in local_settings.py it'll be forced at True. Graphite users shouldn't have to bother with this setting :)\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/settings.py#L234\n. Travis says :shipit:, merging!\n. I'm not going to do it but if somebody's up for it, sure :)\n. 0.9.x doesn't use Django named URLs so you reverse('events') in tests doesn't work. You'll need to use the dotted path to the view, e.g. reverse('graphite.events.views.view_events') or hardcode the URL instead of using reverse().\n. @deniszh not really, these warnings mean that naive timestamps are stored in the DB which is what we're trying to avoid :). Can you have a look at the parsed timestamp in events/views.py and see why it's seen as naive?\n. You can read the release notes for 0.10 (master). They're updated as new features get added.\nhttp://graphite.readthedocs.org/en/latest/releases/0_10_0.html\n. You can read the release notes for 0.10 (master). They're updated as new features get added.\nhttp://graphite.readthedocs.org/en/latest/releases/0_10_0.html\n. @jkur great, thank you!\n. There was no swap with Django 1.7. Looking at the code I don't see the offending line either in master or in 0.9.x. Which version are you running?\n. I followed the instructions as updated in #1080 and couldn't replicate this behavior. pytz ended up in the global site-packages as expected, as all other dependencies (/usr/local/lib/python2.7/dist-packages/ on an ubuntu 14.04 instance).\n. That doesn't make sense\u2026 When you pip install -r something from a directory containing a setup.cfg file, I'm 99% sure pip doesn't take setup.cfg into account. @dstufft should be able to confirm.\n@obfuscurity what's the pip version on the debian 7.7 you tried?\n. +1 for documenting --no-use-wheel for now.\n. Fixed in #1233 \n. It's probably a resolution issue. What's the time resolution of your data? You're probably seeing 2000 / (number of datapoints in an hour).\n. And 20 seconds is also the resolution of your whisper files?\nWhat happens if you take the result of your derivative and do scale(xxx, 180)? (180 being the number of datapoints you have in 1h)\n. Looks good! Not sure why you're seeing these aberrations though.\n. https://github.com/graphite-project/graphite-web/pull/1116#issuecomment-70630825\n. It's not needed :) Python has native sqlite support since version 2.5.\nhttps://docs.python.org/2/library/sqlite3.html\n. I'm :+1: for the same reason as @obfuscurity but it'd be nice to have tests for new features :)\n. @toni-moreno I like ?format=json_dygraph more than ?format=json&jsondpfmt=something. It's more explicit. I also just added a comment to #980.\n. This isn't misconfiguration, just a change in carbon's internal. Cache queries now returns datapoints as a dictionnary.\n. Whisper uses floats internally, so this isn't particularly surprising.\n. In carbon's master branch, cached datapoints are stored as a dict whereas in the 0.9.x branch they're a list-like structure (deque IIRC).\nThis is how I account for the two possible return values in graphite-api: https://github.com/brutasse/graphite-api/blob/master/graphite_api/finders/whisper.py#L122-L123\n. That's not intended behavior. The issue is here:\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/attime.py#L68\ntimezone.now() takes the project timezone regardless of what was passed. The timezone should be passed all the way to parseTimeReference instead.\n. You don't need manage.py. Please try:\nPYTHONPATH=/opt/graphite/webapp django-admin.py syncdb\n. Static files are served runserver only in DEBUG mode. Otherwise they're not served at all (considered insecure), hence the collectstatic step. Given that collectstatic's behavior depends on configuration, it's not easily done at packaging time or even installation time.\nWe could probably serve static files directly in the graphite webapp, using something like whitenoise (probably needs more work than what's in the quickstart docs, though).\n. Static files are served runserver only in DEBUG mode. Otherwise they're not served at all (considered insecure), hence the collectstatic step. Given that collectstatic's behavior depends on configuration, it's not easily done at packaging time or even installation time.\nWe could probably serve static files directly in the graphite webapp, using something like whitenoise (probably needs more work than what's in the quickstart docs, though).\n. There is a fix in master: static files are served by the webapp directly, no need to make an alias or anything. Note that whitenoise must be installed as a new dependency. The patch should be easy to port to 0.9.x if needed.\n. @gsaray101 do you have whitenoise installed?\n. Just using DjangoWhiteNoise isn't enough -- see what's done here: https://github.com/graphite-project/graphite-web/blob/1db428c17789fe914e2027c03d5829734389c324/webapp/graphite/wsgi.py#L21-L29\nIf you have the secret_key warning, it means your local_settings.py aren't found by graphite.\n. get_current_timezone() returns a proper timezone if pytz is installed. Are you actually seeing a LocalTimezone being returned with pytz installed?\n. manage.py is still useful to have for development, it's a nice shortcut.\n. And in this installation guide /srv/www/graphite contains no exploitable Python / config file.\n. @resnostyle can you provide the exact URL for this query?\n. Can you run in a django shell (DJANGO_SETTINGS_MODULE=graphite.settings django-admin.py shell)\n``` python\nfrom django.utils import timezone\nprint(timezone.now())\nfrom django.conf import settings\nprint(settings.TIME_ZONE)\n``\n. @toni-moreno do you havepytzinstalled? It's really strange thattimezone.now()gives you a datetime in UTC.\n. Ok nevermind,timezone.now()` always returns a date in UTC. The problem is somewhere else:\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/functions.py#L3001-L3002\nWith these lines start points are always multiples of \"interval\", without taking UTC offset into account.\n. Right -- see the source I linked in my previous comment. Without aligntofrom, start time is \"rounded\" to your interval length. But graphite only deals with epoch timestamps at this time in rendering, which is why you went back to UTC without alignToFrom.\n. :+1: \n. Thanks!\n. Thanks!\n. That's because of the install prefix: carbon and graphite-web are installed under /opt/graphite by default and pip doesn't know about that. Whisper doesn't have a hardcoded prefix.\nYou should be able to override the prefixed installation by passing --install-option=\"--prefix=/usr\" --install-option=\"--install-lib=/usr/lib/python2.7/site-packages/\" as options to the pip install command (or change to whatever prefix/python version you need).\n. A fix was committed for this issue, you should only have to update your graphite installation to the latest master: https://github.com/graphite-project/graphite-web/commit/9f73f67c77d17013446b4a2f0231b5dbba7a7da0\n. You're probably using graphite 0.9.x, in which case the example wsgi file can be found in that branch: https://github.com/graphite-project/graphite-web/blob/0.9.x/conf/graphite.wsgi.example\ngraphite.wsgi is new in 0.10.\n. @gsaray101 master is the same as 0.10, but I don't think there's anything built-in for opentsdb.\n. Can you run in a python shell\npython\nfrom graphite import settings\nprint(settings.WEBAPP_VERSION)\n. You need to provide more information about the error. Here it's impossible to understand where it happens.\nWhere did you put local_settings.py?\n. Can you provide the full error traceback? There's still no way to find out where it happens from here.\n. I guess that's good enough for 0.9.x\u2026 although the bundled version is pretty old :)\n. Thanks!\n. LGTM\n. Closing as a duplicate of #1240 \n. Graphite-web doesn't even need twisted. Only carbon uses it. The entry in requirements.txt is only here for convenience for people who run carbon and graphite-web on the same machine.\nI'd be in favor of letting carbon deal with the twisted dependency and remove everything twisted-related from graphite-web (requirements.txt, check-dependencies.py etc.)\n. I don't think graphite-web is affected by this Django vulnerability. In any case Django 1.7.11 addresses the issue and is supported. Django 1.8 is supported in master and it might just work with 0.9.x (I haven't tried).\n. That's backwards-incompatible -- graphite-web supports API calls via POST as well as GET, which is what request.REQUEST allows :)\nWe probably need to introduce a wrapper that does request.GET.get(param_name, request.POST.get(param_name, default)) instead.\n. The first error is probably because you're using Python 2.6 with Django 1.8. The last version to support Python 2.6 is Django 1.7.\nThe second error is due to a django-tagging/Django incompatibility. In this case you probably need to downgrade django-tagging as well.\n. Ah yes sorry, 1.7 already doesn't work with Python 2.6. You need Django 1.6 or lower.\nSee https://docs.djangoproject.com/en/1.8/faq/install/#what-python-version-can-i-use-with-django\n. Ghaa\u2026 Now that's whitenoise that isn't compatible with python 2.6.\nDo you have 2.7 available? Otherwise you'll probably have to stick to graphite-web 0.9.x.\n. @toni-moreno great, thanks for investigating and letting us know!\n. Yeah 0.3.5 seems to have disappeared from PyPI. We probably need 0.3.6 instead, but dependencies are looking more and more like a matrix with python/django version combinations (django-tagging 0.4 requires a more modern Django than 1.4 for instance).\n. Could you instead extend the REQUIREMENTS environment variable? Something like REQUIREMENTS=\"Django<1.5 django-discover-runner django-tagging<0.4\". (I prefer <0.4 than ==0.3.6). This would allow testing agains the latest Django (1.8) with an extra line like REQUIREMENTS=\"Django<1.9 django-tagging\".\n. :+1: \n. With 0.9.x branch it should be here\u00ad\u2026 What file structure do you have in /opt/graphite/webapp/graphite?\n. :shipit:\n. There is a pull request #997 but it hasn't been updated in months. The comment I left stills sums up my opinion about the feature.\nWhat does your patch look like?\n. Enforcing permissions in STORE.find() is the right approach. Per-user permissions are a good start but I think it'd be useful to also have group permissions (define sales permissions once, add people to the sales group as they come and leave). Django's contrib.auth app provides groups and memberships so it'll be rather easy to do.\nThere are a couple of things in the code that are not idiomatic or that will cause issues but an in-depth review will be easier to do in a pull request.\nThe fact that the feature is behind a setting is nice because there won't be a performance impact for people who don't need permission checks.\n. @toni-moreno sure, open a PR and I'll comment inline :)\nAlso I meant groups additionally to users, not as a replacement\u2026 Both are useful IMO.\n. @obfuscurity LGTM, but I haven't touched 0.9.x in ages :)\n. Sorry for the wait\u2026 :+1: from me!\n. zope-interface isn't needed by graphite-web either :)\n. :+1: \n. > An easy workable solution could be to be able to set WEBAPP_DIR in local_settings.py\nThat's already the case, at least in master. Which version are you using?\n. settings.py defines WEBAPP_DIR then imports local_settings so it really should be taken into account when overridden in local_settings.\n. :sob: \n. @deniszh great, I forgot that setup.cfg didn't have the [install] section anymore in master. Looks good, there's only a try/finally block missing around the setup() call: see what's done in master, we need to make sure the add/remove dance doesn't alter the file.\n. @deniszh :+1: \n. Build passed. Thanks @deniszh! :sparkles: :zap: \n. Travis agrees, merging.\n. I'm fixing  the docs build, but the issue with syncdb comes from synthetize which provides a JSON fixture a bit too early it seems. I'd recommend doing django-admin.py migrate then django-admin.py loaddata with the path to the fixture. Using another name than initial_data prevents Django from picking it up during migrate.\n. It's back! http://graphite.readthedocs.org/en/latest/functions.html\n. +1 to everything @SEJeff said:\n- thank you for working on this!\n- tests are important :)\nI'll add a couple of comments inline as well.\n. @JeanFred do it :)\n. That's great, thanks @rvernica! :+1: from me.\n. Thanks for the report! Fix incoming in #1487 \n. Which version are you running? I don't see how this would happen with latest master or 0.9.x.\n. Django is required primarily for database interaction for users management and storing events. A flask migration is pointless if no solution is found for these, since it will still require a dependency such as sqlalchemy & schema migrations handling. For events, there is https://github.com/brutasse/graphite-api/pull/182 which sounds like an interesting approach (pluggable events backend). For user management, I chose to drop that part completely from graphite-api\u2026\nHappy to bring graphite-api under the graphite-project umbrella if that's desired, it could share a libgraphite dependency with graphite-web for core functionality & graphite-web/graphite-api would implement the HTTP layer, plus some additional persistence features for graphite-web.\n. I usually put a [flake8] section in setup.cfg. But other locations seem to be supported\u2026 http://flake8.pycqa.org/en/latest/user/configuration.html#configuration-locations\n. The only issue is that so much code changes that all currently open pull requests will have to be rebased or redone. I think that was the main counter-argument so far, not opposition to adopting more widely used standards.\n. @obfuscurity one option to keep requirements loose would be to add JsonResponse in Graphite directly, it's only 10-15 lines.\n. @obfuscurity it's a maintenance pain\u2026 but it also allows packagers to avoid patching if the Django required isn't the Django version provided with e.g. Debian stable.\nAnswering also in #1555 since I missed that discussion :)\n. I'm not sure how this is supposed to work with Apache. I developed this patch with proxy setups in mind, with a dedicated WSGI server like gunicorn. Typically this would be an nginx+gunicorn deployment.\nWhen setting URL_PREFIX, STATIC_URL is updated accordingly: https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/settings.py#L208-L210\nIf setting WSGIScriptAlias+STATIC_URL and not URL_PREFIX works, the Apache documentation needs to be updated accordingly. This is a perfectly valid approach. URL_PREFIX needs to stay for proxy deployments.\n. There must be a duplicate Django installation somewhere in your path that takes higher priority. django.utils.importlib exists in Django up to 1.8 (included).\nWhere/how did you install Django and the other dependencies?\n. Your Django version is too new\u2026 django.contrib.contenttypes.generic was removed in Django 1.9, so you need Django 1.8.x max. It can be installed with pip install \"Django>=1.8,<1.9\"\n. @ElsaHuang you're trying to install Graphite with Python 3, which isn't supported. Can you try with Python 2.7?. I remember having this error when working on https://github.com/graphite-project/graphite-web/pull/1960, but I'm pretty sure I fixed it.\nWhat Django version do you have? Is your 'current master' less than 6 days old?. Just filed a pull request to fix the issue, #1981 \nPython 2 has full unicode support, but it requires more attention than with Python 3. Django handles unicode in requests/responses and in database models, so if there is a unicode bug it's very likely that it's in the Graphite-web codebase\u00ad\u2026\nUnicode support isn't going to come for free with Python 3, it's rather the opposite: Python 3 support will come for free when we fix the unicode sandwich in Graphite-web :). I might be biased but I'd favor the fix in #1981 :). @DanCech it depends on the backend being used, but for mysql/postgres it's not graphite-web's responsibility, and Django does its best to handle encoding properly: https://docs.djangoproject.com/en/1.11/ref/unicode/#creating-the-database.\nHere the thing that trips utf-8 up is all the casting to str(), which are bytes in Python 2. str(<some-unicode-string>) uses ASCII encoding, so it works most of the time but breaks whenever the unicode string has characters out of the ASCII range.\nInstead, if bytes are really needed, one should call .encode('utf-8') on unicode strings.\nPython 2's bytes type is str, and its actual string type is unicode. Python 3 clears this confusion by having a str type which is proper unicode, and a bytes type. This means however that str has a different meaning across Python versions, and should only be used as last resort. Encoding strings and decoding bytes should be the norm.. > I don't know if casting 'unicode' objects to str is the right thing to do in this case..\nIn Python 2, 'str' objects are bytes. To convert them to unicode strings, the proper way is to call .decode('<encoding>') on them. To convert unicode strings to bytes, the opposite operation is .encode('<encoding>'). str(some_unicode_string) is bound to lead to a unicode error\u2026\nusually the encoding to use is 'utf-8' :). Yes, str() calls will add more breakage in Python 2 and do nothing in Python 3\u2026 . I think simplejson can go: it probably dates from when JSON support wasn't so great in the standard library. Nowadays import json behaves consistently across python versions >=2.6 so this is dead code.. Just return tokens.string[1:-1] is enough\n. string.encode('utf-8') is more common but I guess this is mostly cosmetic :)\n. You can make the database sort the dashboards directly:\npython\nfor dashboard in Dashboard.objects.order_by('name'):\n    # etc etc\nname is a primary key here so this will even be properly optimized at the database level.\n. Mostly because from a user perspective, just creating a wsgi file that imports graphite.wsgi triggers the code that preloads the index. I could add it back to the graphite wsgi if that's desired but that comment doesn't make sense in the .example file anymore.\n. If Django 1.6 throws warnings, these will become errors in 1.7 or 1.8. Theses shouldn't be silenced but fixed instead :) Only internal warnings (are there any?) can safely be silenced.\nIf Graphite supports more than 2 Django versions, in some cases we might need a compatibility layer for supporting old versions.\n. About that\u2026 What about removing the insecure default and documenting it in local_settings.py? Django doesn't boot with an empty secret key because it's insecure, and a default key is just as insecure :)\n. Could you switch the order of the imports? from django.conf.urls is the future-proof one so it'd be nicer to have it first.\n. if key not in metaSeries: is enough, no need to call .keys()\n. you can remove the .keys() here. It's not a syntax error :)\n```\n\n\n\nmetaSeries = {}\n'a' in metaSeries\nFalse\nmetaSeries['a'] = 'foo'\n'a' in metaSeries\nTrue\n``\n. Mostly cosmetic but multiple imports per line are fine if they come from the same namespace:from mock import patch, call, MagicMock. @SEJeff this way the order is correct. 0.9.x versions weren't in the correct order on http://graphite.readthedocs.org/en/latest/releases.html\n. You have a missings`. \"utils\".\n\n\n\nSame in attime.py and glyph.py below.\n. That doesn't do anything and should be replaced with an admin.py file to register the model in the admin.\n. @SEJeff no, that works :)\nIt'd be nice if the tests were integrated in the existing test suite. There's also #577 that adds a tox.ini\n. I think this was meant as \"one or several metrics\"\n. @ifdattic yes, can you update your patch?\n. Uh yeah I meant 2.6. Fixing :)\n. Could you move the import statement to the top of the file with all the other imports?\n. I didn't want to touch the file too much so I did that\u2026 It's not too bad :) We can always revisit this later.\n. Why /opt/graphite:webapp?\n. points can be removed from that sentence. There's also only one l in \"actual\".\n. Specific use case but not very niche anymore, no?\nI also wouldn't say it's overkill for a couple hundred metrics :)\n. Maybe mention SSDs for people who want to send lots of metrics to whisper?\n. What about saying lots of people do and making a list of companies we know instead?\n- GitHub (not sure if they still do)\n- Vimeo\n- Booking.com\n- Etsy (I think?)\n- Daemonware\n- \u2026\nAnd deprecate that page while we're at it: http://graphite.readthedocs.org/en/latest/who-is-using.html\n. Can you move that to app_settings.py instead? Since it's not conditional it belongs directly to the MIDDLEWARE_CLASSES declaration.\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/app_settings.py#L55-L61\n. Also here it'd be more useful to return a message. log.exception() automatically adds sys.exc_info to the log record so the traceback is already there.\nPerhaps something like\npython\nlog.exception('{0} {1}'.format(request.method, request.build_absolute_uri()))\n. Django<1.7 tests Django 1.6. We'll want to keep this line, can you add it again? Django<1.8 is a new target.\n. Instead of datetime.utcnow().replace(tzinfo=pytz.utc) you can do\npython\nfrom django.utils import timezone\ntimezone.now()  # returns the tz-aware current datetime\nYou should also add USE_TZ = True to webapp/graphite/settings.py\n. And a default timezone, TIME_ZONE = 'UTC' would be a saner default than Django's America/Chicago.\n. Same, with django.utils.timezone you can do timezone.now().replace(hour=hour,minute=min,second=0).\n. class Admin: pass doesn't do anything since Django 1.0 (released Sep 2008)\n. Here you're making this view vulnerable to SQL injection.\nDjango has an ORM, use it when querying data. Only in some very rare cases you actually have to use a raw database cursor.\nInstead of this line, has_restrictions = dashboard.admindashboard_set.exists() will tell you whether this dashboard has restrictions and dashboard.admindashboard_set.filter(user=request.user) gives you the list of permissions for the currently logged-in user.\n. I'd use a ForeignKey to the User model instead. You don't actually need the profile and in your code you're mixing up profile IDs and user IDs\u2026 They can be different.\n. That error message will be confusing and make things hard to debug if an exception actually happens here. If you don't know what to expect, don't wrap your code in a try\u2026 except. The view will crash with a proper traceback and this will give much more information for debugging.\nAlso, except: clauses need to specify the type of exception. A bare except: like this catches SystemExit, KeyboardInterrupt which usually are things you don't want to catch silently. You need to be specific or remove the try\u2026 except.\n. Same as above: use the ORM to query the database, be specific about the exceptions you're expecting.\n. Idem\n. Same comments as above apply here as well. Also note that this view is cached (see L61 to L68) and a cache hit will bypass your permission check. A possible solution would be to include the user ID in the cache key.\nYou're also doing a SQL query inside of a double for loop, which can lead to a big performance issue when you have a large number of metrics and restrictions. Instead, you should:\n- compute a list of metric names (names = [series.name for series in data])\n- identify which metrics have restrictions (restrictions = UserMetric.objects.filter(name__in=names).values_list('name', flat=True).distinct())\n- when you have restrictions, fetch the current user's permissions and remove forbidden series from the dataset.\n. Hmm, that's what needs to be fixed instead.\n. What version of graphite-web are you running? 0.9.12 has this bug but It looks like it was already fixed in both master and 0.9.x branches:\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/templates/dashboard.html#L27\nhttps://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/templates/dashboard.html#L26\nTo compare with 0.9.12:\nhttps://github.com/graphite-project/graphite-web/blob/0.9.12/webapp/graphite/templates/dashboard.html#L26\n. That line can be moved outside of the if/else clause\n. Could you use \"URL-prefixed install\" instead of \"non-root\"?\nThere's also some mixed indentation in the lines you've added. Other than that, LGTM!\n. See the comment a couple of lines above: it looks like here target= can be repeated multiple times, meaning you'd have to parse/split/etc. to handle all targets. I feel like the escaping should be done earlier than in this function.\n. That's a rather odd way to generate JSON. Could you refactor that to read result = json.dums(some_structure) with native types (lists / dicts) rather than string interpolation?\n. This is provided by default by Django, so there's no need to specify an id\n. Can you use another name that's not a Python builtin? Perhaps pattern?\n. The best practice in Django land in Python 2 is to define a __unicode__ method (not __str__) and make it return a unicode string. Also it's fine to use def.\n. For model fields with choices, Django provides a get_FOO_display method on the instance. So you can use self.get_type_filter_display() in your model methods.\n. You can use related_name and do user.metric_filters.all() instead of having to define a method for getting filters for a given user.\n. Are users supposed to have at most one UserMetricFilter? You have a ForeignKey to User but if a user have multiple filters, calling .get() as done here will raise an exception.\nGiven that a UserMetricFilter has an m2m relation to MetricFilter, it seems that it'd be enough to make the UserMetricFilter - User relation a OneToOneField instead of a ForeignKey. Same logic for GroupMetricFilter.\n. When user isn't provided, user=None breaks the nodeFilter function. I think you probably want to make user a required argument and update all STORE.find() occurrences to provide user.\n. nodeFilter probably needs to be applied one level up:\npython\nfor node in nodes:\n  if not nodeFilter(user, node.path):\n      continue\n  if node.is_leaf:\n    \u2026\nThis avoids changing the logic below: only allows nodes end up in leaf_nodes and there is no need for the minimal_node_set_filtered calculation L137-146.\n. No need for membership test, simply .add() and the set takes care of duplicates\n. Doesn't this line have an extra level of indentation?\nsorted(found) is enough, a set is already an iterable.\nIs the format meant to be JSON? Or something else? Serialization shouldn't be done with string interpolation.\n. @obfuscurity dedent this line one level? It's only needed at after the loop is done. After that, LGTM :+1: :)\n. The 404 case isn't covered for JSON requests. It'd be nice to return a 404 JSON response if Event.DoesNotExist is raised.\n. Django>=1.7 has a built-in JSON response, perhaps it'd be useful to have it in graphite-web to avoid calling json.dumps and passing content-type every time JSON is returned.\n. You could even do \npython\nfor name in Dashboard.objects.order_by('name').values_list('name', flat=True):\n    name = \u2026. IIRC this was done on purpose: an epoch is a fixed point in time, whereas a naive datetime can have multiple interpretations. I'm not sure using settings.TIME_ZONE as default fallback is always correct.\nThe absence of fallback is a way to raise issues when naive datetime are passed around. The fix is to make these datetimes aware (as you did \ud83d\udc4d), but I'd keep epoch() fail rather than being at risk of being off by X hours.. ",
    "slackhappy": "This still causes problems (originally introduced by ff8e1a5e7db25193d3bbb1b891494ec2aefc5a27) when trying to load the composer via url.\ne.g. if this renders:\nhttp://my-graphite/render/?target=sumSeries%28my.metric%29\nswitching \"render\" to \"composer\" in the url, which is a supported feature (though sadly broken since 0.9.10 by ff8e1a5e)\nit will show up in the composer as:\nsumSeries%28my.metric%29\nbefore applying this patch, it was\nsumSeries%28my.metric)\nwhich is arguably worse, since it was inconsistently broken\n. I've tried searching for code outside the project (there is none within) that calls metrics/search, or uses the keep_query_pattern parameter, but didn't find much.  As a concrete example, grafana uses metrics/find, but not metrics/search.  The last update to the behavior of search_view was 19ed37ae10 in 2011, which seemed to add some commented out code, and remove a parameter.\nI'm not sure how to figure out if there are projects out there that use it, but if there are, it would be possible to bring it back.\nIt has a response format that isn't very helpful for someone using it for search.\nImagine this sample index file:\na.b.leaf\na.b.c.d.leaf1\na.b.c.d.leaf2\na.x.leaf\nleaf\nand this session (the view just jsonifies the results, so the shell will do):\n```\npython2.7 manage.py shell\n\nfrom graphite.metrics.search import searcher\ndict(metrics=list(searcher.search(\"a\")))\n{'metrics': [{'is_leaf': False, 'path': None}]}\n```\n\nhmm, not very useful, it didn't even return the child nodes of a\nusing keep_query_pattern is even worse:\n```\n\ndict(metrics=list(searcher.search(\"a\", keep_query_pattern=True)))\nAttributeError: 'NoneType' object has no attribute 'split'\n```\n\nthe only interesting results occur when you search for a node that has leaves:\n```\n\ndict(metrics=list(searcher.search(\"a.b\")))\n{'metrics': [{'is_leaf': False, 'path': None},\n  {'is_leaf': True, 'path': 'a.b.leaf'}]}\ndict(metrics=list(searcher.search(\"a.b.c.d\")))\n{'metrics': [{'is_leaf': False, 'path': None},\n  {'is_leaf': True, 'path': 'a.b.c.d.leaf1'},\n  {'is_leaf': True, 'path': 'a.b.c.d.leaf2'}]}\n```\n\nIt seems like this hints that you could use it for querying target globs expansion if you already knew the exact targets you were looking for, but then:\n```\n\ndict(metrics=list(searcher.search(\"..c.d.*\")))\n{'metrics': [{'is_leaf': False, 'path': None},\n  {'is_leaf': True, 'path': 'a.b.c.d.leaf1'},\n  {'is_leaf': True, 'path': 'a.b.c.d.leaf2'},\n  {'is_leaf': True, 'path': 'leaf'}]}\n```\n\nWhat is the leaf path doing at the end of that list?   It doesn't match the given pattern!  (Perhaps a longstanding bug, failing to check match occurs before the end of the pattern?)\nI was thinking of suggesting implementing metrics/search in terms of metrics/find, but I don't see a specification, and my attempts to derive a useful one have failed.\n. One thing i noticed, which probably depends on your configuration, is that you'll need something that kicks off graphite.util.write_index, which wsgi.py was effectively doing.\n. Agreed. Just wanted to point out.  Thanks for merging.\n. ",
    "ojilles": "Hi,\nWas running into the same or similar issue. Fixed it with the following change. Let me know if this is a way that is interesting, will send in a small pull request.\n\nNeeds a change in graphite webapp:\n    file: /opt/graphite/webapp/graphite/templates/composer.html\n```\nThis section:\n       / Direct graph loading /\n        if (\"{{queryString}}\") {\n          Composer.loadURL(\"?{{queryString}}\");\n        }\nNeeds to be changed into:\n       / Direct graph loading /\n        if (\"{{queryString}}\") {\n          Composer.loadURL(decodeURIComponent(\"?{{queryString}}\"));\n```\n. +1\n. Being European and all, I think @mleinart 's change is a more sensible default and makes due with less precious pixels.\nAgreed that his should be a config setting. The easiest way I can think of is some 12/24 hr switch. Would that be good enough? Making all of xAxisConfigs' dict's configurable is overkill in my opinion.\n(I could probably make a PR for that). \n. Ran into this myself, too. Decided to look into the code a bit, and this (paraphrased) gave me the result intended:\n/render/?&target=areaBetween(metricname.{median,95percentile})&areaMode=stacked\nSeems the implementation is rather naive. You need to give both metrics in one go (hence the {xx,yy}), the first one MUST be the lower metric, and you need to do areaMode=stacked.\nLike you i was suspecting I could do areaBetween(minSeries(some..stuff.), maxSeries(some.stuff.)), but alas.\nCode here: https://github.com/graphite-project/graphite-web/blob/1eb5c7bfbd1d3d21edf643c43826764e75e90cc6/webapp/graphite/render/functions.py#L1116\nLiterally, it will just stack both, declare the first one invisible, resulting in a band that is colored (with everything else underneath that invisible).\n. I'll take a stab at it tonight (CET). \n\nOn Aug 14, 2014, at 15:59, Jason Dixon notifications@github.com wrote:\nAt the very least we need to improve the function documentation so the current syntax and usage is understandable. A secondary goal should be to make the function more robust, including tests. For now let's focus on documentation.\n\u2014\nReply to this email directly or view it on GitHub.\n. Just wanted to chime in and say thanks for all the hard work folks!\n. See #842 for the backport, skips the small update for reduceSeries as explained over there.\n. Was firing up the trusty editor, all ready to do what you asked, but realized I don't know what you mean? (Looks consistent with the rest of the document to me) Please enlighten, and I'll update :)\n\n(Will backport ofc. Also, found typo s/transparant/transparent/)\n. Sorry, was still mucking about with the line endings and how to have a clean commit for the PR. Nvm then, will backport as is to 0.9.x. \nAn \"TIL\" by @obfuscurity due to my checkin, makes my day :-D\n. Oh sorry about that, was git-wrestling :)\nBut you'd probably want to take mine, includes a correction in the documentation! (correction for master coming now as well). Sorry for the noise!\n. Yeah hold on :)\n. Reason why that other commit would have applied cleanly is b/c we put the new function in different places in the functions.py file, I suspect. \n. ",
    "nhooey": "Can someone consider merging this change?\n. How about also allowing a percentage of data points in moving{Average,Median}?\n. I stopped using Graphite over a year ago. I've been using Datadog.\n\nOn Aug 14, 2014, at 19:06, Jeff Schroeder notifications@github.com wrote:\n@nhooey Want to take a go at adding a setting where the default is the same behavior as currently, and you can toggle a boolean setting to enable this feature as you've suggested?\n\u0081\\\nReply to this email directly or view it on GitHub.\n. Heh, Datadog has it's own barrel of issues. Just the way the cards were dealt when I switched jobs. Good luck.\nOn Aug 14, 2014, at 20:34, Jason Dixon notifications@github.com wrote:\n@nhooey I'm so sorry (jk).\n@SEJeff Adding this feature is on my to-do list.\n\u0081\\\nReply to this email directly or view it on GitHub.\n. \n",
    "mattsn0w": "I do not see any regression when I append lineMode=staircase to graphs with this change.\n. Perfect! Thank you!\n. ",
    "jhenry82": "I accomplished this using ripienaar's Gdash dashboard tool (I'm sure others would work, too)  and some Apache rewrite rules based on the node name. Our hierarchy in graphite is set up such that every node contains the name of its datacenter (let's call them dc1, dc2, etc). Given that, I can use mod_proxy and mod_rewrite to forward requests to the right graphite instance and render the results all on one page. In Apache it looks something like this:\n``` apache\nRewriteEngine On\nRewriteCond %{QUERY_STRING} ^(.dc1_companyname_com)$\nRewriteRule ^/render/$ http://graphite.dc1.companyname.com/render/?%1 [NC,NE,L,P]\n```\nGdash composes graphs using Graphite's URL API. If you're trying to graph the 15 minute load average of a host, this might be saved in graphite as hostname_dc1_company_com.load.longterm. Gdash requests http://localhost/render?target=hostname_dc1_company_com.load.longterm.\nThe rewrite rule I specified notes \"dc1_companyname\" in the query string, proxies the request off to the graphite instance running in dc1, and returns the graph.\nMake sense?\n. ",
    "drawks": "This seems a bit contrary to whole rest of graphite/carbon which is full based around graphing time series data. A working patchset that adds this would probably be welcomed, but I wouldn't hold your breathe waiting for someone to add it since it wouldn't be trivial and is something which would be more easily addressed by using a different toolset.\n. The consolidation portion of this for raw/json outputs appears to be addressed in 7adc7f4 I think that offering a sample mode vs avg could be useful though. I'll tag this for the 0.10.0 milestone. I'd like to see some tests for this before merging though.\n. Of course it will appear twice, you've included it twice in your targets definitions. The targets are independent lists of timeseries deduplicating the final result would make no sense since your inner target definitions may have different outer functions. This is not a bug.\n. It doesn't work like that since the containing function could return different results depending on the series list it contains. What you are asking for is just not doable. Targets are independent from one another. If you'd like to ensure that you don't have duplicate entries from overlapping matches then either come up with a better naming/organizing scheme, or use symlinks to organize your targets on the back end to make them available through a more sensible target description.\n. Yeah, we've got a ton of commits in the 0.9.x branch, can we call feature freeze over the long Thanksgiving weekend and do a bughunt and version tag next week?\n. To be clear, the 0.9.11-pre1 version is tagged, so please only bother with pull requests for bug fixes and not new features.\n. Can you collapse down this pull request to only include the actual changes? It is a bit hard to evaluate it the way it is. Also, it is probably worth mentioning that we have official debian maintainers now and the current \"debian\" subdir should probably be stripped from our repo; at a minimum I'll not that this current debian postinst doesn't appear to be ideal in that it presumes that graphite-web is run via apache, it doesn't properly check for the context in which the script is being run, it copies files into /etc without marking them as config files, etc... This /may/ work on some debian installs but it falls short of what is required to be policy compliant in the Debian world and is almost certainly being thrown away whole cloth in the official debian packages and packages of other derivative distributions.\nThat all said, I'm gonna close this without merging, if you want to resubmit a merge request for your import change alone that would be appreciated.\n. 16327f54544cff12f1acde76925e3936f74ae264 Fixed!\n. I couldn't care less about inclusion or not of this feature, but would like to ensure that we take into account the difference in valid characters in the key side of the has between eval-able javascript (required by jsonp) and pure json. Who ever finally commits to implementing this would do good by the project to ensure special case handling for the few characters which are legal in json but aren't in javascript.\nThis link explains:\nhttp://timelessrepo.com/json-isnt-a-javascript-subset\n. The above example looks fine, I'm not sure if it is preferable to honor the last occurrence or first occurrence of a timeseries in the pathexpression in the case that there is a duplicate. At any rate, I'm going to close this pull request because the fix introduces it's own bugs with allowing duplicates in the pathexpression.\n. 0.9.x isn't supposed to be getting new features until after a 0.9.11 \nfinal tag happens.\n-Dave\nOn 04/25/2013 09:00 AM, Jason Dixon wrote:\n\nThis doesn't cherry-pick cleanly into |0.9.x|. If someone has the time,\nplease submit a new PR against the |0.9.x| branch and we can review that\nseparately.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/170#issuecomment-17016322.\n. This issue is filed in the incorrect project. This appears to be a bug report for carbon.\n. sorts aren't free, a function is implemented to handle this.\n. it really depends on the number of metrics, but... since there is already a function to do the sorting for you.....\n. I'll take a look at the code, but most likely this is not a simple fix since there is not a dependable way to anticipate what the seriesnames will look like that arrive at the function. Its been a while since I wrote this though so maybe there is something obvious that I'm missing.\n. This looks like a relatively simple feature request. I've tagged it for the .10 milestone since I expect the 0.9.13 may be the last version in the 0.9.x run.\n. This looks like an issue for the carbon project not graphite-web\n. Its been a while since I've looked at this code, but glancing at it now I don't see where any coercion from float to int is happening. Actually all the format strings and everything appear to be setup to do floats with 2 decimal places..\n. This looks like a problem in graphite-web.render.glyph.format_units, some sort of abuse of math.floor that seems to be rounding everything off.\n. or maybe not... hrm, looking\n. Commit 457ec24 fixed this in \n. Sorry that took a while to get done. Thanks SuminAndrew for finding the bug, I swear I stared at that for at least an hour and didn't see it.\n. As far as I can tell this is a pretty typical pattern for testing django codebases on travis. The caveat is that it of course only tests a single db backend implementation. Which means if the django model ever includes DB features which behave differently or aren't supported on one of the different backends that could bite us.\n\nThat said the amount of the django db ORM that is used in graphite is trivial and not complex.\nI have no objection to the merge of the syncdb stuff.\n. This is super messy, if you had problems with requiring an upgrade of a related library and it isn't reflected already in the documentation or packaging please update one or both of those things. Including the library in graphite-web is only going to block this project from acceptance into debian/redhat/ubuntu etc as nearly every distribution maintainer requires that the system version of required libraries is used and that 3rd party software which may be bound by a different license or rights holder be kept seperate. \n. Yes, the already included portions already cause a bit of consternation in that downstream maintainers have to patch them out to prefer system provided versions. Perhaps the answer here is mark jquery/flot etc as submodules in the git project such that we recognize they are related and required, but we avoid having the 3rd party code directly included.\n. Ack, my bad I hastily closed the pull thinking that it was only including the flot library. I had neglected to see the larger feature addition of flot as dashboard renderer. I'll reopen this one the request and leave the above discussion with regards to third party library inclusion in place. If someone wants to address the library dependency/inclusion stuff I 100% believe that we should have a solution where we can stake dependency at a given version and perhaps track as sub modules in the main head.\nI think the library stuff must be resolved and once it is that should paint a clear path as to how to integrate further features that depend on them; including this one.\nedit:\nMea culpa, lesson learned, drink coffee FIRST! Then answer pulls requests.\n. Alright, after a bit of a chat on IRC I withdraw my objection, but I don't have time to test this so I'm not gonna merge it. If someone else wants to own testing/merging I'm all for it.\n. Please refactor this to be optional and revert to the existing behavior as default. It is unexpected for datapoints to be dropped from a render silently and per previous discussions with regards to this topic, the current behavior is \"correct and expected\". This is a neat feature, but definitely needs to be something that a user should have to enable intentionally.\n. 0.9.12 is the latest version. It has the CAN_FALLOCATE attribute needed.\n. This is really faster than native array? You didn't happen to keep all your timing results?\n. How is this different than groupByNode?\n. This works fine for me, I'd like to get at least one other person to test it before I merge.\n. This works fine for me, I'd like to get at least one other person to test it before I merge.\n. I agree, graphite-web fails to start without ceres available, that seems to have jumped from optional to required. Merge seems innocuous.\n. I don't know much about it or really have time to test it since it doesn't really cause me any performance problems. If you want to avoid that code you can go ahead and set the READTHEDOCS variable in environment and the whole block will be skipped. Seems like a workable solution for people complaining of a performance problem at the moment until someone can read through and explain the current behavior OR whomever already understands the motivation can explain it.\n. cactiStyle already does this. Try adding an argument like \"si\" or \"binary\" to the function like:\ncactiStyle(scale(carbon.agents.a.updateOperations,1024000),\"si\")\nI had added configurable precision at one point too, but I seem to have never merged it back. At any rate The idea is appreciated, but seems to duplicate existing behavior.\n. also it looks like your variable precision formatter is broken...\n. This configuration is possible using shared storage.\n. The intention here is good and I really like the flexibility, but I think the level of verbosity of the required config changes are a little out of hand. IMHO it would be ideal if there were some middle ground where some sane syslog default or a sane flat file default could be selected with a single config option. The explicit config could be something that is optional if people have some ultra-specific requirements, but I think that most people do not want 80 lines of their config to be a giant block of code including imports, flow control statements and nested dicts.\nIf we kept the handful of existing booleans:\n``` python\nLOG_RENDERING_PERFORMANCE = True\nLOG_CACHE_PERFORMANCE = True\nLOG_METRIC_ACCESS = True\n```\nand added a few simple settings like:\n``` python\nENABLE_STANDARD_LOGGING = True\nENABLE_SYSLOG = False\nIf desired a custom logger can be defined. See docs/custom_log.example for details\nCUSTOM_LOGGER = None\nSet the following to \"0\" to disable rotation\nSTANDARD_LOGGING_ROTATE_PERIOD = 24\n```\nI think that would cover the majority of users' needs. Users that have more complicated needs could define and assign a custom logger definition if wanted, but that should probably be moved to a standalone example document or something to keep the basic example config concise and not scare away newbs.\n. This should probably just be implemented as a render function and not be the default behavior. Aside from not having to incur the cost of the sort if it isn't desired it would allow things like alpha sort on aliased metric names and specifying the direction of the sort.\n. hot. +1\n. From the example carbon.conf\n```\nIf set true, metric received will be forwarded to DESTINATIONS in addition to\nthe output of the aggregation rules. If set false the carbon-aggregator will\nonly ever send the output of aggregation.\nFORWARD_ALL = True\n```\nAlso, FWIW in the future can you try and file issues under the correct project? graphite-web has absolutely zero to do with carbon configuration.\n. with setuputils you can specify depends in setup.py and pin versions using normal version comparison strings. For example I have the following in another project I've been working on:\n``` python\nfrom setuptools import setup\nsetup(\n...\n    install_requires=[\n        'Beaker>=1.6.4',\n        'CherryPy>=3.2.4',\n        'Jinja2>=2.6',\n        'PyYAML>=3.10',\n        'SQLAlchemy>=0.8.0',\n        'bottle>=0.11.6',\n        'iso8601>=0.1.4',\n        'psycopg2>=2.4.6',\n        'pyOpenSSL>=0.13',\n        'python_ldap>=2.4.10',\n        'requests>=1.2.0',\n        'uWSGI>=1.9.8'\n        ],\n    tests_require=[\n        'coverage>=3.6',\n        'mock>=1.0.1',\n        'nose>=1.3.0',\n       ],\n    setup_requires=['nose>=1.3.0'],\n...\n)\n```\nThis allows specifying dependencies that will be pulled in automatically and when manually runnign setup.py if you specify the install vs test targets you get the appropriate dependencies for each.\n. Also, I'm 100% in favor of getting rid of the ridiculous /opt/graphite as the project root. It is dumb, it has always been dumb...\n. mock the callback and test for call count and args passed.\n. Honestly I've not written any tests in this project; I was just suggesting that mocking the callback would be a workable test pattern.\n. /usr/lib64/nagios/plugins/check_carbon_aggregator_updates.py < is some random nagios plugin. not provided by graphite-web, carbon, whisper or any orther graphite-project software package as far as I know.\n. I'm all for layout changes that make graphite less of a unique snowflake and generally eliminate surprises.\n. Nice commit! Thanks for the work.\n. No reason this has to be a post. GET requests can also have a body as part of the request. I've used this approach in another API I wrote.\n. I just did a very quick minimal test in django and it looks like GET requests have a request body just like any other request. I know for certain that this pattern works with nginx +  uwsgi, I'd be surprised if it didn't work most places.\n. I'm no big django-phile, but just for the sake of leaving a crumbtrail to a solution....\n- in django request.REQUEST is scheduled to be deprecated in 1.7\n- request.REQUEST is simply a convenience attribute that present the contents of request.GET and request.POST\n-  request.GET is a querydict of query args sent in the url of any type of http request (it is poorly named) and never includes any parsed request.body regardless of content-type headers\n- request.POST is a querydict of the parsed request.body of POST requests which contain form encoded key/value information.\n- To implement a general solution\n  - queryargs and body should probably be considered mutually-exclusive to avoid having to evaluate both and apply some inherit order of operation\n  - each request of any type having a body would first need to have it's content-type headers checked to evaluate how  request body should be parsed.\n    - for formencoded values a direct mapping to querydict is possible since it natively support non-unique keys\n    - for json encoded data; non-unique keys are valid in json but not natively supported in pythons json parser. So...\n      - Custom json parser that goes directly from json > querydict with multiple same-name top level keys evaluating into a single querydict key with multiple values\n      - Require json encoded bodies be formatted as a single namespace dict with unique key names and values which are lists of values.\n- Once ^ is done, bob's your uncle\n- For simple <img src=\"/render/\"> usage I think you are correct that neither POST nor GET actions apply and that the trade of in expressiveness of the render api is that the display code must be more complicated. Generally solutions which would send a body are going to be implemented in JS or another full language, not in HTML or other similarly limited markups\n. @Dieterbe Personally I think that the various http verbs should have some reflection on the actual action which is to be taken. If I want to get a graph it seems weird that I POST parameters instead of GET. As for which encoding/serialization the parameters are specified as seems purely stylistic. Perhaps the framework which @glafrance is using make json easier for him? At any rate if both behaviors can be trivially implemented and the API show proper respect for content-type headers when interpreting requests it seems, to me, like this is a win/win situation or minimally a win/noop. \n. +1 to @brutasse Dependencies exist for a reason. \n. INSTALL indicates that we support python 2.4 or greater. collections.defaultdict is  2.5+\n. Seems like this a documentation bug then...\n. Looks good to me.\n. If the X axis isn't time then it isn't a time series... Graphite is a graphing tool for time series data.\n. +1 good patch, please add test\n. Splitting the two doesn't sound too crazy to me... But then again I have basically zero commits to rendering/web code so having a clean API only codebase is pretty appealing.\n. Rad\n. Where's the code?\n. This needs tests. Also, while the intent of the function seems decently conceived, the code is quite tersely written and difficult to read. I'd much prefer a solution that is either very plainly written OR heavily commented.\n. Well, if you are asking for a merge of code you don't understand, isn't commented and doesn't come with tests I would say that it is a poor candidate for merging. I mean, no offense intended, but we are trying to improve the overall quality of this project and blindly merging big blobs of unclear code is not going to do that.\nI'm going to close this for now. Feel free to open a new PR if/when you've addressed these concerns.\n. The repo organization aspect of this is really the most difficult part to sort out in my opinion. I'm not sure how to dig out of the current mess, but the end result should look like:\n1. Active development is all based against Master and no feature branches should be maintained in the official git project.\n2. Major versions should cut be cut into separate maint. branches for continued support in the form of bugfixes and once they are ~2 majors behind they should either be abandoned or be kept around only for critical security fixes.\n3. All PRs for merge into master should be squashed into single commits and should include a concise description of their intended behavior AND a unit test to match. Only PRs which are pure refactors of code which is already covered by existing tests should be accepted without tests.\n4. Some sort of automated/scripted procedure should be adopted for cutting new versions which should handle the work of updating all relevant version strings AND ideally updating a CHANGELOG document which would be initially synthesized from commit messages, but should be massaged into something readable by whoever is cutting the release.\n5. All of the above along with any suggested workflow should be codified in a CONTRIBUTING file at the base of the project to provide guidance for contributors. Additionally we might consider adding a FAQ section to this document to address questions which have been asked and answered before i.e. Why isn't this project PEP8 style compliant?\nedit: This is all just a suggestion from my point of view and one that I've made before in other channels. None of the above should be construed as me dictating new policy or assuming a role of leadership in the project; I've got too much other stuff going on to bite off a bigger chunk of graphite.\n. @brutasse Your point is well made, but without seeing exactly how tsdb has been integrated it may be an oversimplification to point @kds119 at the pluggable backend. IIRC from last time I looked into OTSDB one of it's strong points was pushing some of the \"render\" transforms back at the storage layer. Housing the time series data in a hadoop cloud has some advantages for being able to do map/reduce/filter operations inside hadoop. Of course to take advantage of any of this would require either overloading the existing storage finder api OR extending it.\n. This pretty neat!\n. Grafana is missing a few significant features from the above list, but I too see it as probably more fruitful to add dashboarding/viewing features into a different frontend project like grafana vs doing further feature development in graphite-web.\nI personally would find it spectacular if grafana got features like shared crosshair (although that might be a flot issue) and actual user access controls.\n. +1 to everything @steve-dave said. If there is no existing test coverage please add some.\n. :+1: This looks like a really sensible change and I think that dropping support for 2.5 and simplifying the docs to keep from having multiple sources of \"truth\" is a great move for making installation and post-installation support simpler for everyone.\n. https://github.com/graphite-project/graphite-web/blob/master/requirements.txt#L46\nLooks like this newer version of pyparsing isn't listed in requirements.txt\n. @obfuscurity I think fullQueueDrops is tracking something entirely different. I suspect that @aaron2 is correct that there is no log line for packets received that exceed the max dp per message. If so, that should be fixed.\n. Looks good with the exception of my one comment. But, since @obfuscurity and I discussed it offline I'm fine with it as is and we can always address it later if it is decided a different solution is preferable.\n. Well, I think really that the solution here should probably preserve backwards compatible behavior (at least for some period to transition if that is desirable) likely you'd want to have a queryarg like \"strict=true\" and that could do some combination of the following options.\n- render \"inf\" as \"1e+9999\" and \"-inf\" as \"-1e+9999\" to to overflow the parser's float type and result in proper inf and -inf at the end of decoding\n- render \"nan\" as false\n- render both infinity and negative infinity as \"true\" and \"nan\" as \"false\", leaving \"null\" as the correct response for no value at a given datapoint.\n- fully drop empty datapoint from the json and use \"null\" to represent \"nan\" and use true/false to represent inf/-inf respectively.\nHonestly I think this is probably not a really worthwhile endevour as most json decoders aren't as strict as the native decoders in most javascript engines. AND inf, -inf, nan are all valid in javascript but technically not valid in json.\nThat said I'd be fine with a patch to implement some combination of the above behavior when strict json is requested. As long as the code is clean and comes with tests :)\n. To be clear I'm 100% against changing the current default behavior, even if it isn't technically correct.\n. both cairocffi and py2cairo have identical (at least for our purpose) API. py2cairo is notoriously hard to build install from source, but is available from upstream distro maintainers on more platforms. Cairocffi is typically easier to install but has an additional non-python dependency of libffi. I think we should just support either implementation through the normal \"try-import as-except-import as\" pattern... just my .02\n. Seems like that is a issue to open against grafana. I appreciate @torkelo 's work on that project, but it is probably a bug to expect functions from the underlying datasource that haven't been present in a release version of the datasource software :) Honestly I think that grafana and graphite-web should support some method for enumerating the available functions in the backend and then using that result to populate the list if functions and their signatures in the front end.\nI /think/ maybe this bug is originated because torkelo codes against graphite-api and not graphite-web. graphite-api does include sortbyname :)\n. I'm not opposed to merging this as an exception to the feature freeze in 0.9.x, but I'd like to see a test included too :)\n. Try using the \"group\" function to build your serieslist.\nareaBetween(group(minSeries(a.*.min),maxSeries(a.*.max)))\n. I wouldn't call it a consensus, but... The last in depth conversation WRT inclusion of numpy seemed to conclude with wanting to modularize the rendering functions first and then it would be easy to provide numpy based functions for systems where the dependency isn't a problem. FWIW numpy is much faster than the inbuilt pure-python solutions for several of the core functions.\n. @pcn  I just added #988 to the milestones for 1.0.0\n. Looks good to me, not sure why it can't be automerged. Can you do a quick rebase against current master?\n. I don't think it is really our place to dictate how people architect their installation. FWIW I think almost the opposite change should be made. The \"detection of loopbacks\" was made to enable reuse of generic config right? Which is an IMHO \"magical behavior\" which depends on a rather dubious heuristic for determining whether the IP is \"local\" or not.\nlocal_ip = sock.getsockname()[0]\n....\nif local_ip == host:\n    return True\nIt is exactly the type of architectures that @jraby is describing that I recall having cited in my original objection to this behavior. That said I'm against this commit and would rather there just a be a switch around filtering with \"is_local_interface\" when building self.remote_stores in the Store class.\n. Yeah I'm :-1: on this, but would be in favor of just making an explicit \"enable/disable\" switch around the entire \"loopback prevention\" codepath. I think it [the original code] is poorly conceived and should either be removed whole cloth or minimally have a full switch around it.\n. The intention of this is good, but I've got some reservations about baking our own \"natural sort\" and then making it the default behavior. Among other concerns the strategy of using a fixed regex for pulling the numeric portions seems overly simplistic. The natsort is intended for this exact use case and takes into account many other factors such as locale and speed when sorting large sets.\nThat said, my primary objection here is a lack of test coverage for the change ;)\n. patch looks fine to me, but should include a test case\n. OrderedDict was introduced in python 2.7, requirements for this project are 2.6+\n. thanks\n. let me give you a hand.... :hand: \n. Yeah the typical usecase for the holtwinters stuff is to run it against past periods and use it to draw a trend line over the current period so you can see over/undershoot \n. the semantic argument is silly, it is called forecast because that is the name of the mathematical model used.\n. #1455 Should address this issue...\n. I am using cross version nodes, however the compatibility between versions was trivial to implement AND the pattern by which the exception was silently swallowed is bad practice. So, two birds with one stone. This PR corrects for the exception handling and opens the door for cross version metrics/find which allows for arbitrary federation between dissimilar versions.\n. FWIW this PR returns the exact same test failures as 0.9.x, those failures should be corrected, but I'm not sure I have a stomach for the archeology to find where they were introduced.\n. @deniszh Yah, thanks. I fixed it, well at least in the PR, I'm not entirely certain if it is a good idea to amend the commit message of a branch. I think editing history is bad practice?\n. The safe unpickler in 0.9.15 ALREADY referenced graphite.intervals, so it appears that it was the intention of that code for it to already be available. I've not done the archeology to find when/why intervals appeared in master or what the alternative to that type in a pickle packet should be or how that mapping should be implemented. \nI will say that if such a trivial change brings cross version api compat in a way that make federation work that it'd be a big win for facilitating changing versions in production without trying to cut everything over at once. It seems, to me, a good value from a marginally distasteful change.\n. So whats the consensus here? I'm fine running this patch out of tree to scratch my own itch, as it were. I still think this looks like the right way to handle this in tree. If bringing intervals from master is really that distasteful it seems that the unpickler code in 0.9.x should also have references to it scrubbed.\n. K, fair. I'll rejigger this PR when I have a few moments free.\n. Sounds like you're talking about carbon not graphite-web. Please file issue with appropriate project.\nhttps://github.com/graphite-project/carbon/issues\n. mea culpa! \n. git is hard.... this pr replaces #1455 \n. Looks good to me as well. My only niggling complaint would be that I'd like to see the test and the refactor done as two different pulls. Since this is a refactor id' be nice to see the test green on the current code path before replacing it with the refactored one.\nI mostly just watch from the cheap seats, but that seems (to me) a tad bit saner as far as a flow for refactoring.\n. settings.py contains settings for configuring logging, presumably if your local_settings are meant to configure logging AND cause an exception there is no gurantee about the state of your logging config. It would be a real pickle to raise an error about your logging config using a broken logger. When in doubt fail back to the lowest common denominator. Just my .02, perhaps I'm missing something here.. Nice work everyone!\n. Historically there has been a decent amount of resistance to adding a dependency on numpy. The general suggestion to feature requests of this nature has been to perhaps build out better capability for making render functions pluggable and allowing plugins to to have various 3rd party dependencies.\nI'd personally really like to see maybe a third way, using setuptools' extras capability to have numpy be an optional dependency and then offer some numpy optimized versions of inbuilt render functions as well as potentially some additional functions which would become enabled only when numpy support is enabled.. Historically there has been a decent amount of resistance to adding a dependency on numpy. The general suggestion to feature requests of this nature has been to perhaps build out better capability for making render functions pluggable and allowing plugins to to have various 3rd party dependencies.\nI'd personally really like to see maybe a third way, using setuptools' extras capability to have numpy be an optional dependency and then offer some numpy optimized versions of inbuilt render functions as well as potentially some additional functions which would become enabled only when numpy support is enabled.. \n*Begin tangential discussion***\n\n\nPluggable functions is interesting, and I've been thinking a little about an endpoint to be able to advertise to tools like Grafana which functions are supported by the graphite instance.\n\nAt this point I think the quality of the code base of graphite-api is such that it would be my preference if that became the reference implementation and was pulled under the graphite project and then have graphite-web be skeletonized into a minimal composer/dashboard interface on top of graphite-api.\nAs a tool for rapidly browsing data and prototyping various shaping/graphing of the data graphite-web's front end has always been a nice feature for me. But, the grafana project has far surpassed it in capabilities for dashboarding.\nIf the api had an endpoint for enumerating and describing render functions such that the front end layer could be agnostic that would be awesome. . > Quick googling shows that pure python FFT is very slow and/or limited, everybody recommends using numpy or scipy version.\nI just wanted to jump in here and point out that the more typical use case for FFT is for stream processing applications. So the concept of pure python being \"very slow\" or \"limited\" may not be entirely applicable when looking at [relatively] small chunks of time series. It could be that a fallback to a pure python implementation is \"fast enough\" for most typical graphite use cases. And having numpy as an optional dependency may provide more speed for those who need or want it.. I need to set it explicitly here so that the default value will display in the usage statement. It is/was a convenience for users so that they can understand what graphite thinks the config is and what they can expect to happen.\n. Without setting it you can't import graphite.util which imports django.settings for some sort of weird django-ness. This seems safe to default since without setting it the subsequent import will fail with some sort of crazy django exception. The previous version didn't require this because it expected you to feed it settings via the environment. Which, if you are keeping score, seems the odder of the two conventions in light of how the rest of the project works.\n. I think that would break /more/ people, since I expect that far fewer people explicitly set DJANGO_SETTINGS_MODULE than those who leave it unset. Seems like this is a relatively innocuous mode of breakage too, in that the index file is a completely ephemeral bit of data. Failure to generate a new index would be pretty apparent in web frontend when new metrics fail to appear, attempts to create a new index at a non-valid location would result in an exception and an email from cron.\n. Is this meant to be \"2.7\" or \"2.6\". The two lines in this commit don't agree with one another.\nLooks like everything else in this PR is \"2.6\" and that seems sensible.\n. I sorta think this should return NaN or something instead of silently eating the domain error.\n. Seems like this should probably be collapsed to a lambda instead of a full first class function declaration since there isn't any obvious case where you'd be reusing that inner function.\n. Any reason not to just import all of pyparsing? Seems like we're importing a goodly portion of what the module provides. Would it not be more clear if we just imported pyparsing and then explicitly called pyparsing.foo for these 17 objects?\n. It is different than the current default behavior, which is intended. Not sure how best to handle communicating that change. Perhaps make it default on now with a comment that the we intend to reverse the default behavior in a future release? Honestly I think the change will affect very few people, and the current default behavior seems questionably implemented at best.\n. ",
    "yee379": "re: contrary to time series data: yeah, i was surprised when i found out that pie charts were supported.\n. ",
    "cbliard": "You're welcome. It really is a minor update.\n. ",
    "tmm1": ":fire:\n. /cc #193 \n. I added these back in https://github.com/graphite-project/graphite-web/commit/f1d9992454e59dc190a65280aa0a42635f96431f because groupByNode only supports wildcard on  a single node, whereas these functions can take multiple position arguments.\n. > I'd strongly prefer there be unit tests for this function before it is merged. Would you mind taking care of that please?\nDefinitely, wasn't planning to merge this without tests and docs.\nI've got some basic tests going in here, but would like to add more that actually operate on series. However I'm not seeing an easy way to create dummy metrics to exercise the seriesLists paths.. do you have any ideas?\n. Alright, added some docs and figured out how to create fake metrics during the test to exercise the pathExpression substitution cases.\n. Need another test here that passes numeric args via the query params\n. Added the new tests and made sure numbers passed via query parameters are parsed correctly.\n. ",
    "rowanu": "Ceres is not required, even though some of the log messages suggests it might be (and I'm guessing one day it will be).\nThe only requirement is that you have Whisper OR Ceres available, so that there is at least one back-end for storing metrics.\n. Yeah, that's why I did it - to get a third party lib to work like it did\nwith Whisper back-end - I agree it's a hack.\nOn Jan 29, 2013 11:51 PM, \"Michael Leinartas\" notifications@github.com\nwrote:\n\nThanks for this - index.json is kindof a hack but there are some tools\nthat depend on it so it's good to extend this support. At some future point\nwe may want to actually use the Finder for this so that it can include\nremote data or other datasources, but for now let's get it in\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/130#issuecomment-12874219.\n. \n",
    "patrickshan": "Yes. We have come across the same issue while we need to display one graph with different time range. \n. ",
    "leonidlm": "Good point Nicholas, thank you for pointing it out.\nI checked the Graphlot interface, and it seems very immature. What are the plans for it? should it be an alternative interface for Graphite or is it planned to merge them together? Asking, because maybe I can help ...\n. ",
    "dakotacody": "The specific problem that brought this to our attention was  doing a diff between total memory and memory committed.  Currently, if data is missing from one of the 2 series you just get the value of that series which can produce some very misleading graphs if data is missing sporadically due to network or other issues.  Everywhere else in graphite a 'None' is not interpreted as a 0 but instead as a gap or unknown value.  If this solution won't work perhaps a different function could be added or a flag that could be toggled to allow the user to decide how to handle missing data. \n. ",
    "client9": "FYI --\n@Dieterbe Right now changing the width of a graph (&width) has no effect on JSON output.  Therefore consolidateBy will also have no effect, as the resampling for images is done in another path, only for images.\nWith a large number of points, I'm able to crash a few browsers using client-side rendering, so a re-sampling would be great.\nIf width and current resampling code could be moved out and re-used in the json output (i.e. no width = current system, if width, then resample to that many number of points.).  Then consolidate by could be re-used, and no new functions are needed.\nthoughts?   I might be able to find some time to do this.\nnickg\n. See also #153 \n. @denniskong thanks for posting that.  Reverting that commit worked for me too.\n@mleinart any thoughts here?  It would be nice to make HEAD work, but I'm not sure what the intent was of keyword args.\n. ",
    "SEJeff": "@Dieterbe Will you close this pull request if you think the code is too hairy?\n. @diegovar Any chance you know what version of django exhibited this error so I can reproduce (and fix) it?\n. Unfortunately, the javascript file you include is GPL licensed and graphite is Apache licensed. This would effectively change the license of graphite to GPL and I don't think that is going to pass muster.\n. @captsens This is the right place to send pull requests. Unfortunately, the maintainers have a $real_job, so can't always go through things super fast. Also, on IRC, please idle for awhile. Sometimes it takes someone a bit to respond. They can't if you disconnect :)\nI'm just responding to your IRC comment asking if this is the right place.\n. Unfortunately I don't think this is a graphite change so much as a big carbon change. Also, it would require backend changes to both carbon's pluggable back ends and graphite-web's pluggsble back ends.\nI suspect that for this to be merged, someone will have to implement it and send us a pull request. Not trying to be a dick but I'm going to close this. If you'd like to start a serious conversation about this functionality, open a bull request with code to add it and we can go from there!\n. Looks good\n. Haven't had a chance to test this yet, but I love the idea\n. It does suck quite a bit to send graphite urls in emails or IM especially to users with brain dead email clients that get confused by urls with () in them. It also sucks in markdown. External URL shorteners no way, but a simple and integrated one would be much appreciated by some users.\nWhen sending links to less technical users, this is very valuable.\n. @obfuscurity I've not yet, but am going to shortly when I get home (travelling right now) at the end of the week. I'm in Boston at the moment\n. Please do, I for sure would use this functionality. I'd probably implement it differently, but this is entirely reasonable\n. +1\n. +1\n. The code looks pretty sane to me as well. I'd also LOVE to see this with giraffe\n. This depends on #278 for starters\n. @tabletcorry Thanks for the heads up!\n. This issue is fixed by #296 \n. This fix is already in e4f62ddc4e8c948\n. \n. Would have to test, but I agree that this should work as said. Hardcoding /opt/graphite and not allowing overriding that without code changes is a really shitty default\n. May I suggest you take a look at ceres. It was written to fix whisper scalability limitations and works precisely how you mention in bold.\n. @nhooey Want to take a go at adding a setting where the default is the same behavior as currently, and you can toggle a boolean setting to enable this feature as you've suggested?\n. Great success! :+1: \n. @tait just curious, what do you use this for? Why is it more useful than a get request? Do you do this via jquery or something?\n. @mleinart I pulled down this branch and merged it into the current master. After applying the fix from #313, this appears to work perfectly fine.\nGranted I didn't do heavy duty testing, but did actually test the ui and the dashboard out for a few minutes. I also verified it was pulling the new javascript ie: `ext-base-debug.js by going into Incognito Mode (chrome) and opening up the javascript it is pulling down (via the Network tab) into a new tab to make sure the version said Extjs 3.4.0\n:+1:\n. honeybadger don't care\n. Thanks for the bug report @syepes and thanks for the fix @gingerlime \n. Would you mind making a fork of graphite-web and redoing this as a commit against the master branch of graphite? If not, I can do it later on, but it would be better so you get proper credit for you to do it.\n. This should be closed.\n. Thanks for the heads up!\n. It doesn't seem unreasonable to make the post data as similar as sanely possible to the dashboards so it is easier to create them. Thoughts?\n. Yes, I just need a chance to test it. Today or tomorrow most likely\n. @Fak3 Thanks for your contributions!\n. @adrienbrault merged. I'll just manually refactor and fix the comment that was never addressed by the submitter.\n. Forgive my stupidity, does this need to be forward ported into master as well?\n. So I can confirm this bug using graphite-web master with the old 0.9.10 carbon and graphite-web 2d63bbbe2b387f6 using this url\n/render/?width=586&height=308&target=carbon.agents.*.{updateOperations,metricsReceived}&areaMode=all\nWhen hitting refresh 5-10x for that same graph url, I got 2 different results. It was not consistent, but here is what the 2 different images look like (saved in a private gist):\n\n\nYou can quite clearly see the bug that is exhibited in the second image. After applying this patch, it indeed is fixed:\n\n\nSo... based on the testing @obfuscurity asked me to do, LGTM :+1: \n. @obfuscurity Notice the metric order flipped and 1 is ontop of the other covering it up. Not beautiful, but I think that is another bug and unrelated to this one.\n. I see 2 issues here outside of this specific bug:\n1. My thoughts are that the metrics and colors applied to them should be consistent. Perhaps the series isn't sorted or something before\n2. There are stacking problems where one metric with &areaMode=all will cover others behind it. This doesn't happen consistently, but is noticeable in the pre and post patch images.\nDue to 2, it doesn't seem that this patch introduces regressions. It only exposes yet another bug that needs to be fixed. Do you want me to file a separate issue from the above?\n. @piotr1212 Using the same url I did and graphite-web master with a 0.9.10 carbon? It might be a bug in non-megacarbon carbon that has already been fixed in upstream carbon.\n. Confirmed this patch along with the patch in #307 applied before it do indeed improve things. I've got a test metric that is set for 1 second resolution. Then in a terminal, I did something like:\nbash\n$ while true; do echo \"test.metric1 $(($RANDOM % 200)) $(date +%s)\" | tee test.log | nc localhost 2003; sleep 1; done\n<CTRL>-c\n$ while true; do echo \"test.metric1 -$(($RANDOM % 200)) $(date +%s)\" | tee test.log | nc localhost 2003; sleep 1; done\nThen I pulled up the graphs and they look much better with the patch:\n\n\nThe gap is from me hitting <CTRL>-c\nLGTM :+1: \n. @obfuscurity Yup, I'm not sure why it wasn't referenced originally. Very strange.\n. Oh an FYI: master is completely unusable out of the box without this fix (KeyError). This is kind of an important change.\n. Fixes #313 \n. Sure. It might take a day or two\n. Can you please run:\nbash\ngit remote add upstream git://github.com/graphite-project/graphite-web.git\ngit pull --rebase upstream master\nIt really ticks off my inner OCD seeing completely pointless merge commits.\n. @boopathi Would you mind if I rebase squashed these into 1 commit to remove the add/remove pointless commits and the merge commits?\n. Good call! Works like a champ now.\nbash\n[jschroeder@desktopmonster graphite-web]$ ./check-dependencies.py \n[REQUIRED] Unable to import the 'pyparsing' module, do you have pyparsing module installed for python 2.7.3?\n[OPTIONAL] Unable to import the 'memcache' module, do you have python-memcached installed for python 2.7.3? This feature is not required but greatly improves performance.\n[OPTIONAL] Unable to import the 'ldap' module, do you have python-ldap installed for python 2.7.3? Without python-ldap, you will not be able to use LDAP authentication in the graphite webapp.\n[OPTIONAL] Unable to import the 'txamqp' module, this is required if you want to use AMQP as an input to Carbon. Note that txamqp requires python 2.5 or greater.\n[OPTIONAL] Unable to import the 'python-rrdtool' module, this is required for reading RRD.\n3 optional dependencies not met. Please consider the optional items before proceeding.\n1 necessary dependencies not met. Graphite will not function until these dependencies are fulfilled.\n[jschroeder@desktopmonster graphite-web]$ echo $?\n1\n. I don't see any major problems with this, but I'm curious, what is the point of this? Are you moving graphite to a specific part of a url ie http://somedomain/graphite?\n. @Seb-Solon or @titilambert ping.\n. @titilambert If you can clean this up to work against the latest master, I'll finally merge it in.\n. @Seb-Solon ping :)\n. I'm pretty sure this would fix issues where graphite-web on a relay doesn't show any metrics at all. I've not tested it, but like the idea at least.\n. I like the idea of this conceptually, but I think this would most likely be a better fit in the megacarbon branch of carbon, where the backend (ceres, whisper, vertica?) are abstracted and the existing webapp would require 0 changes to use vertica.\n. @tkuhlman I like the idea however, do you mind if I close this pull request? I would like to see another pull request against the megacarbon branch of carbon adding a vertica backend however. Then there would be exactly 0 changes required for graphite-web, only some config tweaks\n. Thanks @gingerlime!\n. Manually merged via: 353916ab893a09\n. No preference from me. Which is easier to install from pip would be my first question. Generally I'd preference that onr\n. Please file these issues as you find them! I've been super busy with traveling and family in town but will have a spurt of free time coming up soon to take care of low hanging fruit like this. Please do file all the issues (meme style) so we can continue to improve graphite!\n. @bitprophet unit test the graphs, but via &rawData=true, not the pngs, that would be a bit too tricky I suspect.\n. @bitprophet Please take a shot at writing some initial nose tests if you don't mind. I accepted a unittest based set of tests for whisper, but might take a shot at migrating to nose eventually.\n. Fantastic contribution, thanks! This is a perfect example of how to do it right:\n1. List the problem\n2. List the solution with context\n3. Include a patch that implements 2\n4. Profit!\n. Can you push some documentation examples to the same branch so that this pull request is updated?\n. Long docstrings are fine so long as they are relevant. I like the writeup you did about how you use this quite a bit. It is good to give people ideas.\n. Thanks for making graphite better. Feel free to keep the improvements coming :+1: \n. Thanks!\n. @diyan Mind rebasing this against the latest 0.9.x branch?\n. Keep the fixes coming!\n. @lukevenediger Sorry it took so long! All of the graphite maintainers (including myself!) are extremely busy with $real_life. We really appreciate your contribution and hopefully it won't be your last!\n. Fixed in #360, thanks @DazWorrall!\n. @burr86 Would you mind updating this to work against the latest master?\n. @bcap Very good stuff! Perhaps you can do some more like this. That would be awesome :+1: \n. As much as I like this patch (my ~/.vimrc auto-strips all whitespace when I save a python file), it will make merging the zillion other PRs harder. So we'll have to do this later.\n. Absolutely love some of the work from this, but unfortunately, it can't be merged. I'm going to perhaps base some stuff off of the ideas from this at a future date.\n. FYI: This bug is exhibited in 0.9.x as well. Should be fixed before release as it is an easy fix.\n. @ziegeer answered that correctly. If you want that behavior, you'll have to use ceres. Ceres is totally fine and I've used it in production for a long time. Also, so has sears holdings fwiw.\n. Thanks!\n. great stuff @Dieterbe  thanks!\n. @cbowman0 if you wouldn't mind rebasing against the latest master, I'll get this merged right in. So sorry for it taking forever. We are all volunteers and my time has been a pretty precious commodity. \n. Awesome, thankyou!\n. Do you mind adding this to the webui as well? It is in dashboard.js I believe.\n. @cbowman0 Do you mind rebasing against the latest master please?\n. @obfuscurity or @mleinart What do you think of this? I'll not have a chance to test this for roughly a day or two, but conceptually really being able to draw graphs client-side if possible.\n. @cbowman0 I feel like a broken record here... but would you mind rebasing this against master? If you could do that, I'd love to merge this. Thanks!\n. @Dieterbe thanks\n. @cbowman0 Thanks!\n. Would you mind rebasing this against master? I'd like to take a shot at getting this merged in.\n. @marco-hoyer Mind rebasing against master?\n. Fixed\n. Thanks!\n. Thanks!\n. @fdChasm ping?\n. Manually merged.\n. Nah, I just wanted to manually merge in the branch myself and run make html\nlocally to look at the rendered sphinx. Then I pushed it to my own github\nand closed the pr manually. Then I realized I was an idiot and pushed it to\nthe upstream graphite-project repo :)\nIts all good, you're here:\nhttps://github.com/graphite-project/graphite-web/commits/master\nPersonally, I'd have done a rebase squash for these 3 commits into 1, but\nit isn't the end of the world. All is well!\nOn Sun, Jan 12, 2014 at 5:23 PM, Chasm notifications@github.com wrote:\n\nI take it that having the revert and merge from parent in the history made\nit cleaner to do a manual merge?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/469#issuecomment-32137883\n.\n\n\nJeff Schroeder\nDon't drink and derive, alcohol and analysis don't mix.\nhttp://www.digitalprognosis.com\n. @esc will you please rebase this against the latest master so it can be merged in?\n. Thanks gentlemen!\n. Thanks!\n. Ah sorry, fixed in #481. Please keep them coming however!\n. Thanks!\n. Thanks!\n. Thanks!\n. Thanks! These types of things are some of the most difficult to notice.\n. Let's add tests to keep this from being a regression. Just sending this so I don't forget to do it myself if someone doesn't beat me to it first.\nText by Jeff, typos by iPhone\n\nOn Dec 20, 2013, at 1:13, Jason Dixon notifications@github.com wrote:\nMerged #505.\n\u2014\nReply to this email directly or view it on GitHub.\n. Thanks!\n. Huge fan, lets improve our docs AND link to the django docs in a followup commit\n. Provided the travis config change works, this seems like an obvious no brainer to me.\n. Hey, I've got a patch in this! Seems only fair that this should be merged. Thanks @xiongchiamiov!\n. Enough people have asked for this feature that it absolutely makes sense to merge.\n. With a bit of testing, lets just merge this one into 0.9.x and #328 into master. Then this feature, which has been requested many times, can be finally done.\n. @jcsp What do you think of us merging in #349 in place of this one? It seems a bit more complete\n. @jcsp Could you make it work with the code from both prs? Ideally for big stuff like this, it will go into master (not 0.9.x). If you're ok with doing the work, that would be awesome and we'll finally merge it all in for you. Thanks for your patience\n. Hi @jcsp thanks for the contribution, but we (@mleinart, @obfuscurity, and the graphite contributors) try to publish a \"stable output format\" for our json and support that. Other outputs were added in the past, but going forward we are shying away from new output formats. We will most definitely merge #523 when one of us has time to test the code, but I'm afraid we can't merge this one. I hope you understand.\n. you sir rock\n. Yes, there is a 0.9.13 milestone\n. Or make it optional and use cython if available, otherwise fallback to the pure python version.\n\nText by Jeff, typos by iPhone\n\nOn Dec 18, 2013, at 7:06, Steve Akers notifications@github.com wrote:\nYeah, I had to add it to the dependencies to make it work. I'll remove it and go back to my original version. The only other route would be a patch users could apply after installing graphite.\n\u2014\nReply to this email directly or view it on GitHub.\n. Thanks much!\n. Looking at what you sent, do not use commas as a delimeter:\n\nvarnish_server,domain,varnish.backend_conn\nShould be:\nvarnish_server.domain.varnish.backend_conn\nText by Jeff, typos by iPhone\n\nOn Dec 16, 2013, at 7:18, Mariano-gon notifications@github.com wrote:\nvarnish_server,domain,varnish.backend_conn\n. Thanks!\n. Awesome stuff! If you want to be super fancy, could you add tests for this to:\n\nhttps://github.com/graphite-project/graphite-web/tree/master/webapp/tests somewhere?\n. You can find the magic voodoo to run them in the root of the repo's .travis.yml.\nThanks!\nText by Jeff, typos by iPhone\n\nOn Dec 20, 2013, at 9:03, Steve Akers notifications@github.com wrote:\nDoh! I was just thinking that this morning. I definitely want to be fancy. :) I\u2019ll try to get those in today. \nOn December 19, 2013 at 11:53:48 PM, Jeff Schroeder (notifications@github.com) wrote: \nAwesome stuff! If you want to be super fancy, could you add tests for this to: \nhttps://github.com/graphite-project/graphite-web/tree/master/webapp/tests somewhere? \n\u2014 \nReply to this email directly or view it on GitHub.\n\u2014\nReply to this email directly or view it on GitHub.\n. Travis likes it and I like it. Very nice work @brutasse \n. @esc https://github.com/graphite-project/graphite-web/blob/master/.travis.yml\n. @brutasse  you sir are on a roll! Please keep it up\n. @mleinart or @obfuscurity what needs to be done to merge this?\n. I'm a fan of killing legacy grossness and pretty much loathe /opt/graphite as it simply was a bad decision to begin with. That being said, we are stuck with it now and I somewhat agree with @obfuscurity that we need to come up with some sort of better migration plan. I'm going to merge this PR and think harder about the /opt/graphite issue.\n. Thanks!\n. Thanks @octo. I'm a huge fan of collectd as well FYI.\n. :thumbsup: \n. MOAR TESTS!\n. Mind rebasing against master?\n. @brutasse Can you kindly rebase this once more? Then we will merge it I promise.\n. None I can think of. Makes a ton of sense to do. More ease in testing == more tests\n. @justenwalker Nice touch adding tests as you add a function. Please send us more\n. I'm sorry, this is the bug tracker and is the wrong place to ask. Plenty of people run graphite on a shared host. So long as you aren't expecting to send it hundreds of thousands of metrics per second, there should be no issue. Next time try using an IRC client and asking in #graphite on irc.freenode.net.\n. I've got to spend to spend a bit of time and test this, but conceptually, I really like this approach.\n. Please fix the one tyop you introduced and this is good to merge. Thanks!\n. Thanks!\n. Thanks @esc! I hope you're feeling better now\n. '''\"Showing 21 changed files with 5 additions and 811 deletions.\"\"\" - Graphite needs more commits like this one!\n. Thanks @torkelo \n. Thanks!\n. I'm so sorry you have to use IE, but thanks for fixing this bug!\n. Hi @josephmc5 and thanks for the contribution, the problem is that we are dropping python 2.4 support in graphite-web master. I'd be happy to put this in the 0.9.x branch, but unless we want to use django 1.3 (we don't), then we have to drop python 2.4.\n\nWe will be working on getting all of the necessary stuff for 0.10 in EPEL5, so you can install python 2.6 and the graphite stack from EPEL if you're on RHEL/CentOS/SCL 5.x.\n. @justanyone Hey thanks for the contribution! However, this can't be accepted as is. Please rebase this down a bit to remove some of the pointless commits and don't break the tests. Bonus if you add working tests :)\n. Please whittle these down a bit more and we can go again.\n. I like this change, but I don't like bare excepts, which catch everything, even KeyboardInterrupt. Would it be possible to catch a specific exception or at the very least, do:\nexcept Exception as exc:\n    log.... str(exc)\n. Thanks!\n. Mind sending a PR? Seems like a no brainer\n. Thanks! I'm actually leaning towards making python 2.6+ required for 0.10\n. Thanks!\n. Hey thanks for the contribution! I there any way you could add a test for this new function so that we don't break it on accident in the future when refactoring things?\nAll of the functions.py tests can be found here and an example of how we run them (via travis-ci) is here\n. Would you mind adding a test for this to make sure we don't break this functionality? Then I'd be happy to merge this, thanks for your contribution\n. Looks reasonable, would you mind adding a test for this please? We are slowly trying to fix the lack of tests and it would be much appreciated if you wouldn't mind. Take a look here, and you can get an idea of how to run them from here.\nThanks!\n. I'll have from Saturday through Thursday evenings completely uninterrupted to go through and merge a lot of PRs. I'll make a special point to look through all of your outstanding ones. Sorry but we've been busy with real life things unfortunately.\n. @PrFalken I think you should look at giraffe, which uses Rickshaw.\n. I don't think it makes sense to break the existing json format.\n. We would love to see a backend tab builds on the pluggable backend support @brutasse did for both graphite-web AND graphite-api, but I suspect we would not merge a backend specific UI component to build in things like tags and whatnot. Thoughts from everyone else?\n. Legit\n. Unless the high charts license has changed, last I recall it was free but most certainly not OSS.\nLooking at it, CC NC is most certainly not going to work for graphite. This should be an external tool. http://shop.highsoft.com/highcharts.html\nSorry: \n. You're basically describing grafana. Not sure it makes sense to reproduce.\n. Thanks @esc!\n. @jraby Please open a pull request against master and we'll merge it in. Thanks\n. @macolu If this affects master, would you mind sending us a PR against master as well?\n. @macolu If this affects master, would you mind sending us a PR against master as well?\n. Thanks @steve-dave \n. Thanks @steve-dave \n. Awesome, I've used apache for a very long time and didn't know you could do the ifVersion trick, very nice. THanks @loisaidasam \n. Merging all\n\n. @steve-dave This would be kind of nice to add a test for when you get a chance, but this seems like a simple enough fix.\n. @hashar thanks!\n. @deniszh Please take a shot at re-doing this for master, this is a great feature\n. @obfuscurity exactly 0. I'm always a fan of obvious optimizations like this. Please merge.\n. Thanks for this one @brutasse, really great stuff as usual\n. I like @obfuscurity's suggestion of an optional step parameter.\n. Perhaps we can open issues for things like this that need tests written and crest a label named \"Needs Test\" or something?\n. I'll drink to that! Would you mind doing the honors sir?\nText by Jeff, typos by iPhone\n\nOn Aug 22, 2014, at 16:58, Jason Dixon notifications@github.com wrote:\nI'd rather open an all-encompassing issue to track all functions needing tests. We know there's a lot of coverage missing, no need to open dozens(?) of issues imho. \nJason Dixon \nSent from my iPhone \n\nOn Aug 22, 2014, at 8:54 AM, Jeff Schroeder notifications@github.com wrote: \nPerhaps we can open issues for things like this that need tests written and crest a label named \"Needs Test\" or something? \n\u2014 \nReply to this email directly or view it on GitHub.\n\u2014\nReply to this email directly or view it on GitHub.\n. @esc want to take a shot at adding that to Travis?\n. Will you tell graphana upstream about this and possibly they could add it to their documentation? Funnily enough, I opened the graphana bug asking them to support native png rendering.\n. Fuck yeah!\n. Good riddance\n. :sparkle:\n. If you create a new function, just please add tests\n. Awesome, thanks\n. Thanks!\n. @obfuscurity gah my bad. You're absolutely right. Go for it\n. Perhaps you could send us a PR to update the docs with this as a gotcha?\n. I believe your PR should address this, if you disagree please reopen this with a reason why. Thanks everyone!\n. Thanks, please feel free to send us more contributions! This is likely my fault. Perhaps we should rung graphite through one of the xss proxy pen testing apps to try to find them all.\n. @carlcrott Thanks! Please keep the documentation cleanups coming.\n. Thanks!\n. Thanks!\n. I'm +meh on it as well, but equally agree with @drawks. The existing code could use some sprucing up\n. I'd also really like to see this against master\n. What are your thoughts on merging this into 0.9.x @obfuscurity?\n. +1 what @obfuscurity said\n. Will you make another PR for master?\n. Nope, just put the pip version in the requirements and I believe it will do the right thing. For several internal webapps at $real_job we deploy via wheels and have to do this.\n. I agree with @brutasse here\n. It works in the 0.9.12 release and doesn't on the latest 0.9.23-pre1? I should be able to bust out some git bisect run later today and find/fix the culprit.\n. django-admin.py is installed with django. If you used pip to install graphite, synthesize, or operating system packages it should be available. Feel free to send a pull request making the documentation more clear but since this isn't an issue, I'm going to go ahead and close it.\n. Would you mind adding a blurb about using offset() to do what people obviously assume they should be able to do with diffSeries please?\n. Any chance you'd mind doing this in 0.9.x as well so it makes it into 0.9.13, our last 0.9.x releases?\n. I wish more of our users were like you :)\n\n\nAs @mleinart slowly merges the megacarbon bits back into master, it is going to be shaping up into a very nice thing indeed\n. Nice!\n. From my phone I can't tell if this is against master or 0.9.x branches. Can you also forward or back port this to the other branch so this fix makes it into both 0.9.13 and 0.10 please?\n. Doing the transforms in JavaScript is quite trivial but I'm not strongly against this either.\n. Sort of analogous to this, what does everyone think conceptually of making functions.py pluggable in a similar way to @brutasse's work on the storage finders?\n- default functions.py\n- magical list setting to include import paths for more functions\nWe could then have a graphite-functions-json that supported dyngraphs, highcharts, rickshaw, flot, etc.\ncc: @gingerlime\n. @gingerlime: someone is wrong on the Internet? Nooooooo: https://news.ycombinator.com/item?id=8907875\n. Thanks!\n. I concur with @drawks, please add test coverage for this change. Also, please add a setting and make this disabled by default. Changing defaults should normally not be done lightly as it could mess up expected \"features\" users currently rely on.\n. @JeanFred: What do you think of this? Seems relevant to #1276 \n. @obfuscurity You sure about that?\ncheck this out\nNo use of timezone\n. Is it possible there was a bad merge or something weird? When I clicked view full file in the changes section of this PR, it does show timezone being used. However, the current code in master clearly does not.\n. I'm all for burning legacy crud, but am somewhat certain we have external graphite dashboards and tooling that uses this endpoint. If that is the case, I'd be pretty strongly against removing it and more in favor of fixing it to use a cache or something\n. @graphite-project/committers thoughts?\n. I'd strongly prefer there be unit tests for this function before it is merged. Would you mind taking care of that please?\n. You guys really kick ass and help make graphite better. I can see it being useful in some cases and see the worth in merging it.\nThanks @tmm1\n. This looks reasonable to me, although I'm not the biggest fan of skipping using a web server like Apache or nginx.\nPersonally, it shouldn't be a warning in the check deps script, perhaps just an INFO message?\n. Thanks!\n. \n. What do you think @brutasse or @graphite-project/committers? Is putting a 30 day \"month\" alright? Other than that small niggle, I'm happy with this patch. Just want someone else's take before merging it.\n. It shall be so.\n. Thanks!\n. Thanks!\n. Thanks!\n. @theromis yes, please write a backwards compatible wrapper function or something and update this PR. Then it could be merged.\n. The upstream changelog shows >= django-tagging 0.3.5 supports Django 1.8 officially.\n. Thanks for taking the time to make graphite better\n. Thanks @languitar \n. Do you mind making some bs commit to kick it in the head? There unfortunately isn't a wonderful way to have it re-run that I'm aware of as a project maintainer. I can't manually re-run the hook as I'm not an admin, only a collaborator. Perhaps @obfuscurity could?\n. Good call @brutasse. This looks sane and @jstangroome did appear to address your feedback. Merging\n. @jjneely Please forward port this to master if you would be so kind. Thanks for making graphite better, it is much appreciated.\n. Oh and that error that caused travis to blow up should be fixed by the recently merged #1273 \n. @JeanFred This looks fantastic and I'm always a fan of more automated testing, however it fails on a few of our build targets.\nIs there any chance you could update the code to work on those build targets? I'd be happy to merge it assuming all goes well.\n. Feel free to update this yet again to make unittest2 a dependency, even in py2.6. It is just that much better and I'm ok with that.\nJust do the little import trick something akin to:\npython\ntry:\n    import unittest2 as unittest\nexcept ImportError:\n    import unittest\n. You're truly a wonderful person. If you're ever in Chicago, or at a conference I'm at, I'll buy you some :beer:.\nThanks!\n. It depends on the backend in use, if you're using the default whisper backend, it would depend on your filesystem and how many files can be stored in a single directory.\nThe metric foo.bar.baz would come out to:\n$GRAPHITE_STORAGE_DIR/foo/bar/baz.wsp\nBut it really depends on the storage backend. Some of them use Cassandra, Riak, or OpenTSDB and can hold quite a few sets.\nIf this didn't answer your question feel free to reopen it\n. I was thinking that this code looked familiar and then just realized I wrote some of those tests when my wife was driving home for turkey day a year or two ago. These look great and if I ever meet you @JeanFred or you're near Chicago, I owe you :beers:\n. Feel free to write more, we need them :)\n. Thanks!\n. I suspect seeing th coverage will just lead me to drink... More. This is totally fine by me but we are already using coveralls on whisper, so I'm not sure if we should switch them both or just use a single service.\n. Just remove th coveralls bits in whisper. Coveralls is a bit annoying\n. Seems like if we like codecov, we can just switch everything to it and only it.\n. 0.9.14 wfm\n. Beat me :)\n. Yay!\n. I'm a pretty big fan of this feature as it is something I could have used long ago. However, I can't stress enough that a large new feature, which could result in leaking information if regressed, needs tests so we don't break in the future. That is my only serious nit, is no tests. The rest are mostly just stylistic to improve the code.\nThanks for doing this @cosm0s \n. Good stuff, thanks!\n. Tests are always appreciated, thanks!\n. Thanks much!\n. Comparison is generally __cmp__() in Python I think the terminology you want is equality. I like the correctness of a TimeSeries.__eq__() and see it as the right approach from a \"pythonic\" perspective.\nI'd say fix the tests and let's merge your #1590\n. Yup, I love how much crud this cleans up in the tests.\n. Whoa we are ok with supporting general Python coding standards?! One of the things that always made me sad hacking on graphite was the very anti-pythonic coding style. I'm +1000 on any patches which make graphite project code look like the overwhelming majority of Python projects you see out there.\n. And a lot of the old 1-2 year old PRs likely never will be debased unfortunately. \n. :boom: \n. @obfuscurity: my preference is to use Django~=1.9.0 as it is a bit more elegant. As to removing old unsupported and security problematic versions of Django, strong :+1:. Yessir! It is in PEP-0440. ~=1.9 matches 1.* whereas ~=1.9.0 matches 1.9.*. That is how the operator works. The pep was cowritten by the pip author as well.. Missing test coverage sucks. Can we get older pip for distros we support in the Travis config?. Thanks!. This is not a graphite problem. This is a setup problem with your install. You need to run python manage.py migrate to create the tables.\nPlease read the install document to setup your system properly first.. Send a pull request please! Open source is all about community \nText by Jeff, typos by iPhone\n\nOn Nov 24, 2013, at 2:16, \"Scott M. Likens\" notifications@github.com wrote:\nIn docs/tools.rst:\n\n@@ -319,6 +323,7 @@ annotated events, etc). Supports Graphite, flot, rickshaw and anthracite.\n .. _Rickshaw: http://code.shutterstock.com/rickshaw/\n .. _Rocksteady: http://code.google.com/p/rocksteady/\n .. _Seyren: https://github.com/scobal/seyren\n+.. _Sentu: http://sensuapp.org/\nI believe this should be.\n\n.. _Sensu: http://sensuapp.org/ ? Sorry to be a spell checker :(\n\u2014\nReply to this email directly or view it on GitHub.\n. Question, why did you remove this big comment?\n. @obfuscurity That is indeed your inexperience with rST :)\n\n:doc:printed text <name_of_linked_file_sans_dot_rst> is how you link to internal project docs using rST.\n. I'll take a shot at this, but it would involve setting up a custom error message page to make it obvious for new users.\n. @brutasse Instead, what do you think of an approach vaguely like this one?\nSometing kind of like:\n```\n\n\n\n''.join([random.SystemRandom().choice(string.printable) for i in range(100)]).replace(' ', '')\n'kAgDA\\t($ez5kiYQ<;m^@]p1\"4mCwEYC;!_U{[,L\\x0c1H.\\x0bzEPH7zyWR\"\\'4BYMtH05u(20G%gw#qd%s9;;mI.X?a3\\r:V!F*2v,xm'\n```\n\n\n\nThat automatically sets the secret key each time the application is first started. Thoughts?\n. Yeah I think that is a separate discussion we should have at the summit.\n. These are fixed in #567 \n. The main requirements list Django>=1.4 FYI\n. Use new style classes please. For new code please inherit all classes from object. We'll fix the existing stuff slowly as it is changed.\n. Care to change this to:\nexport PYTHONPATH=/opt/graphite/webapp:$PYTHONPATH\n. s/comning/coming/\n. Why hardcode this?\n. typo here? :)\n. Pedantic I know, but do you mind changing prefix to graphite?\n. With pyparsing in specific, this is normally how it is done (much like with Django)\n. Please remove these two stray print statements, rebase, and update this pr. They shouldn't be in committed code.\n. Good man, adding tests is always a very welcome thing to see.\n. Isn't that kind of cheating? Not all months are 30 days.\n. Better would be to left indent all of the code below and instead make this:\npython\nif not setitngs.ALLOW_METRICFILTER_BY_USER:\n    return True\nif user.id is not None:\n...\n. Minor nit, but please don't ever use variables that override builtin python functions (such as filter). It often leads to much pain and gnashing of teeth when debugging down the road.\n. Same as above, please don't use variable names that override stdlib builtin functions. I'm almost certain a linter such as flake8 would pick this up as a no-no.\n. Since there is no way to fall through from a return ie the return matching or return False, you can change:\npython\n    return False\n  else:\n    return True\nTo simply:\npython\n  return False\nreturn True\n. Would you mind setting these as constants instead of magic numbers? I had to go up to the model reference to know this was include vs exclude. For constants like INCLUDE_FILTER or EXCLUDE_FILTER I see no reason it would hurt for them to be globals, but up to you.\n. Same as above, the else can go away and please left indent the return None. Redundant code is extra code to maintain, which makes things harder to maintain.\n. Agreed, I wasn't a huge fan of a lambda for this as it does sacrifice maintainability and readability.\n. Slightly nicer but equally effective:\nif whisper is False:\n. Nice! :+1: \n. This is entirely reasonable, but stupid question. Why not just use mock.patch() with a context manager?\n. It tends to be bad python, but fair enough, that's an excellent reason.\n. More test coverage is better than perfectly consistent tests. Thanks for your help!\n. I think the best way would be using the compatible release operator here actually. Strangely no one on that mailing list thread realized you can use it:\nDjango~=1.9.0\n\nIs equivalent to asking for Django 1.9.*. ",
    "sodabrew": "@diegovar Can you capture the complete SQL query that is throwing the error?\n. +1 -- would really appreciate periodic bugfix releases rather than waiting to jump major version rewrite hurdles.\n. You are a god amongst men, sir!\n. Ping for getting this into the 0.9.x branch.\n. Ping @SEJeff ?\n. ",
    "kesor": ":+1: \n. ",
    "josephholsten": "Heck I'll gladly deploy head on my prod data in the interest of bug hunting. So long as it helps get a release out.\n\nhttp://josephholsten.com\nOn Nov 21, 2012, at 17:09, Dave Rawks notifications@github.com wrote:\n\nYeah, we've got a ton of commits in the 0.9.x branch, can we call feature freeze over the long Thanksgiving weekend and do a bughunt and version tag next week?\n\u2014\nReply to this email directly or view it on GitHub.\n. Awesome. Got any existing bugs you want closed we can help with?\n\nOn Feb 4, 2013, at 16:42, Michael Leinartas notifications@github.com wrote:\n\nThanks for your patience all, we should have an 0.9.11-pre1 cut this week. From there we can see where the bugs take us :)\n\u2014\nReply to this email directly or view it on GitHub.\n. ATM we've got 20 open pull requests. To anyone with copious free time: could you pick one to review & test?\n. I'd prefer to provide the debs via a launchpad ppa. 90% of the challenge there is just following debian packaging guidelines. I'm already acquainted with debhelper and dh-golang, and I think just grokking dh_python(1) should be enough to get us where we want to be.\n\nFor the debian stack, I recommend we focus on Ubuntu 14.04 first and try to get into Utopic (14.10). Then we can figure out a backporting strategy for Ubuntu 12.04, Debian 7 & Debian 6.\nDo we want to support both apache and nginx webservers? mod_python? uwsgi? gunicorn? I've personally liked boring apache+mod_python, but I understand that's not the coolest tech.\nFor carbon, we'll probably want to support sysv-init, upstart & systemd. It'll almost certainly need tweaked ulimits . Otherwise they should be relatively boring.\n. Oh, I'm working on debhelperizing for ubuntu 14.04. I'll be using apache httpd & mod_python.\n. got it, mod_wsgi it is.\n. I'm working on the debian packaging in #865. My debian-specific notes will be going there.\n. according to https://www.debian.org/doc/manuals/maint-guide/dreq.en.html#customrules, the current dh tooling is provided by dh_python2 \n. ok, I've got it building into /opt/graphite, now I've just got to get it hooked up to apache.\n. ok, I can put it in place with an addition to debian/rules, but probably want to consider the options mentioned in https://wiki.debian.org/ConfigPackages:\n- config-package-dev\n- debconf\n- ucf\n. ",
    "lreed": "I concur with  josephholsten.  Could deploy head or happy to take a prerelease bundle and do some bug hunting.\n. ++1 .\nI know a few folks have been looking at http://rpy.sourceforge.net/rpy2_documentation.html.\ncould be very interesting to have those hooks!\nI've been wanting to start using R with Graphite but have not made much progress yet myself.\n. ",
    "ghost": "+1 - please?\n. Not sure it helps, but: I encountered this bug on and off for years and I finally got rid of it (it seems) since moving my retention policy from 7d to 8d for my charts showing the last week. That graphite doesn't necessarily play nice at the fringes is no secret and I assume the problem happens when microseconds determine which scaling to use, so that one series is still fully rendered within the 7d policy and the next series already within the larger one, leading to a scaling difference whenever the stars/microseconds align right.. +1. \nNo important modifications needed to enable this. But, it will be usefull to get it into config.\n. Works also here. Thanks for the push.\n. Thanx a lot, guys! We've sold it trying keepLastValue()\n. Are there any problems with porting it to python3 (except work needs to be done by someone)?. Thanks!  It works fine when I try to execute as below:\nPYTHONPATH=/opt/graphite/webapp /usr/lib/python2.6/site-packages/django/bin/django-admin.py syncdb --settings=graphite.settings\n. @deniszh does it worth rebasing now? the original PR was closed \"until we reach the 1.0.0 milestone. \". ",
    "mckern": "+1 - please and thank you?\n. First read through, those don't look crazy. That list is really only supporting two platforms, with minor variations among the flavors of those platforms (init scripts, dependency package names, versions on older releases like EL5 and Ubuntu 12.04 LTS). We'd need to work out the dependencies, what's provided by the vendor already, what's provided but too old to use, and then how to resolve that. I haven't looked at any existing binary packages in about 12-18 months but I can do that this weekend. Any documentation about how they're built right now?\n. :+1: to not building omnibus packages if it can be avoided. It'll inflate packages considerably (where do you dry the line in the stack?) and it'll increase complexity. @deniszh Have you already started digging into deb packages?\n. I think we should start w/ HTTPD + mod_python if it's the simplest supportable configuration; it doesn't need to be cool. It's easier to pick one target and increment than to try to shotgun them all. It'll also make reusing logic a little easier, so that even though we can't reuse post/pre scripts we can make sure that the packages do the same things on different platforms.\n. Oh, and (unfortunately) :+1: to supporting sysv-init, upstart, and systemd. It's a pain, but it's an upfront pain. \nQuick status check for the names that have chimed in, who's working on what specifically already? Would like to avoid duplicating effort or competing implementations if we can suss out who will tackle what.\n. HTTPD + mod_wsgi sounds like a plan. @thorrsson, how's things on your end?\n. @thorrsson Have you got this work in a branch somewhere? And have you got the .spec files you've created available anywhere?\n. @thorrsson I've forked carbon to my namespace, and checked out your rpm_spec branch. I've got an updated carbon.spec that runs through mock on EL7. Here's a list of the files that this packages:\n[root@redrobot build]# rpm -qp --filesbypkg ./RPMS/carbon-0.9.12-1.noarch.rpm \ncarbon                    /usr/bin/carbon-aggregator.py\ncarbon                    /usr/bin/carbon-cache.py\ncarbon                    /usr/bin/carbon-client.py\ncarbon                    /usr/bin/carbon-relay.py\ncarbon                    /usr/bin/validate-storage-schemas.py\ncarbon                    /usr/lib64/carbon-0.9.12-py2.7.egg-info\ncarbon                    /usr/lib64/carbon/__init__.py\ncarbon                    /usr/lib64/carbon/__init__.pyc\ncarbon                    /usr/lib64/carbon/__init__.pyo\ncarbon                    /usr/lib64/carbon/aggregator/__init__.py\ncarbon                    /usr/lib64/carbon/aggregator/__init__.pyc\ncarbon                    /usr/lib64/carbon/aggregator/__init__.pyo\ncarbon                    /usr/lib64/carbon/aggregator/buffers.py\ncarbon                    /usr/lib64/carbon/aggregator/buffers.pyc\ncarbon                    /usr/lib64/carbon/aggregator/buffers.pyo\ncarbon                    /usr/lib64/carbon/aggregator/receiver.py\ncarbon                    /usr/lib64/carbon/aggregator/receiver.pyc\ncarbon                    /usr/lib64/carbon/aggregator/receiver.pyo\ncarbon                    /usr/lib64/carbon/aggregator/rules.py\ncarbon                    /usr/lib64/carbon/aggregator/rules.pyc\ncarbon                    /usr/lib64/carbon/aggregator/rules.pyo\ncarbon                    /usr/lib64/carbon/amqp0-8.xml\ncarbon                    /usr/lib64/carbon/amqp_listener.py\ncarbon                    /usr/lib64/carbon/amqp_listener.pyc\ncarbon                    /usr/lib64/carbon/amqp_listener.pyo\ncarbon                    /usr/lib64/carbon/amqp_publisher.py\ncarbon                    /usr/lib64/carbon/amqp_publisher.pyc\ncarbon                    /usr/lib64/carbon/amqp_publisher.pyo\ncarbon                    /usr/lib64/carbon/cache.py\ncarbon                    /usr/lib64/carbon/cache.pyc\ncarbon                    /usr/lib64/carbon/cache.pyo\ncarbon                    /usr/lib64/carbon/client.py\ncarbon                    /usr/lib64/carbon/client.pyc\ncarbon                    /usr/lib64/carbon/client.pyo\ncarbon                    /usr/lib64/carbon/conf.py\ncarbon                    /usr/lib64/carbon/conf.pyc\ncarbon                    /usr/lib64/carbon/conf.pyo\ncarbon                    /usr/lib64/carbon/events.py\ncarbon                    /usr/lib64/carbon/events.pyc\ncarbon                    /usr/lib64/carbon/events.pyo\ncarbon                    /usr/lib64/carbon/exceptions.py\ncarbon                    /usr/lib64/carbon/exceptions.pyc\ncarbon                    /usr/lib64/carbon/exceptions.pyo\ncarbon                    /usr/lib64/carbon/hashing.py\ncarbon                    /usr/lib64/carbon/hashing.pyc\ncarbon                    /usr/lib64/carbon/hashing.pyo\ncarbon                    /usr/lib64/carbon/instrumentation.py\ncarbon                    /usr/lib64/carbon/instrumentation.pyc\ncarbon                    /usr/lib64/carbon/instrumentation.pyo\ncarbon                    /usr/lib64/carbon/log.py\ncarbon                    /usr/lib64/carbon/log.pyc\ncarbon                    /usr/lib64/carbon/log.pyo\ncarbon                    /usr/lib64/carbon/management.py\ncarbon                    /usr/lib64/carbon/management.pyc\ncarbon                    /usr/lib64/carbon/management.pyo\ncarbon                    /usr/lib64/carbon/manhole.py\ncarbon                    /usr/lib64/carbon/manhole.pyc\ncarbon                    /usr/lib64/carbon/manhole.pyo\ncarbon                    /usr/lib64/carbon/protocols.py\ncarbon                    /usr/lib64/carbon/protocols.pyc\ncarbon                    /usr/lib64/carbon/protocols.pyo\ncarbon                    /usr/lib64/carbon/regexlist.py\ncarbon                    /usr/lib64/carbon/regexlist.pyc\ncarbon                    /usr/lib64/carbon/regexlist.pyo\ncarbon                    /usr/lib64/carbon/relayrules.py\ncarbon                    /usr/lib64/carbon/relayrules.pyc\ncarbon                    /usr/lib64/carbon/relayrules.pyo\ncarbon                    /usr/lib64/carbon/rewrite.py\ncarbon                    /usr/lib64/carbon/rewrite.pyc\ncarbon                    /usr/lib64/carbon/rewrite.pyo\ncarbon                    /usr/lib64/carbon/routers.py\ncarbon                    /usr/lib64/carbon/routers.pyc\ncarbon                    /usr/lib64/carbon/routers.pyo\ncarbon                    /usr/lib64/carbon/service.py\ncarbon                    /usr/lib64/carbon/service.pyc\ncarbon                    /usr/lib64/carbon/service.pyo\ncarbon                    /usr/lib64/carbon/state.py\ncarbon                    /usr/lib64/carbon/state.pyc\ncarbon                    /usr/lib64/carbon/state.pyo\ncarbon                    /usr/lib64/carbon/storage.py\ncarbon                    /usr/lib64/carbon/storage.pyc\ncarbon                    /usr/lib64/carbon/storage.pyo\ncarbon                    /usr/lib64/carbon/util.py\ncarbon                    /usr/lib64/carbon/util.pyc\ncarbon                    /usr/lib64/carbon/util.pyo\ncarbon                    /usr/lib64/carbon/writer.py\ncarbon                    /usr/lib64/carbon/writer.pyc\ncarbon                    /usr/lib64/carbon/writer.pyo\ncarbon                    /usr/lib64/twisted/plugins/carbon_aggregator_plugin.py\ncarbon                    /usr/lib64/twisted/plugins/carbon_aggregator_plugin.pyc\ncarbon                    /usr/lib64/twisted/plugins/carbon_aggregator_plugin.pyo\ncarbon                    /usr/lib64/twisted/plugins/carbon_cache_plugin.py\ncarbon                    /usr/lib64/twisted/plugins/carbon_cache_plugin.pyc\ncarbon                    /usr/lib64/twisted/plugins/carbon_cache_plugin.pyo\ncarbon                    /usr/lib64/twisted/plugins/carbon_relay_plugin.py\ncarbon                    /usr/lib64/twisted/plugins/carbon_relay_plugin.pyc\ncarbon                    /usr/lib64/twisted/plugins/carbon_relay_plugin.pyo\ncarbon                    /var/carbon/conf/aggregation-rules.conf.example\ncarbon                    /var/carbon/conf/blacklist.conf.example\ncarbon                    /var/carbon/conf/carbon.amqp.conf.example\ncarbon                    /var/carbon/conf/carbon.conf.example\ncarbon                    /var/carbon/conf/relay-rules.conf.example\ncarbon                    /var/carbon/conf/rewrite-rules.conf.example\ncarbon                    /var/carbon/conf/storage-aggregation.conf.example\ncarbon                    /var/carbon/conf/storage-schemas.conf.example\ncarbon                    /var/carbon/conf/whitelist.conf.example\ncarbon                    /var/carbon/storage/lists\ncarbon                    /var/carbon/storage/log\ncarbon                    /var/carbon/storage/rrd\ncarbon                    /var/carbon/storage/whisper\nCheck out the diff between the autogenerated spec and the one I've tweaked, and let's figure out:\n- How to ensure that all the scripts work correctly when prefix is set to /usr (spoiler alert: sed all up ins)\n- How to safely move configuration out of /var/carbon/conf and into /etc/carbon\n- How to get it working if we install this package\n. Here's something that Fedora is cooking up:\nhttp://pkgs.fedoraproject.org/cgit/graphite-web.git/tree/\n. Taking a look at the Fedora RPM spec files today, and experimenting to see if they're of use/benefit here. Will try building across all the requested RPM platforms, and will report back.\n. @obfuscurity thoughts on the use in Fedora of the python- prefix for carbon & whisper?\n. Usually denotes that something is specifically going to drop libraries into your python libpath 9r requires python to be of any use (much like the ruby- and rubygem- prefixes). Here's the relevant details from Fedora.\n. Where can I find some existing non-Fedora RPMs or spec files? Using metadata like Requires or Obsoletes, we could craft packages that simply replace existing packages as part of a known upgrade but I'd need to know what sort of metadata the existing packages provide.\nOr am I misunderstanding, and you're proposing that we make it possible to upgrade from Fedora/EPEL graphite/carbon/whisper packages to ones provided first-hand by the Graphite project?\n. @obfuscurity Any followup to the outstanding questions ^^ ?\n. I may have misunderstood; I was working from the assumption that there were already pre-existing but less-than-immaculate packages.\n. ",
    "extremeunix": ":+1: \n. ",
    "dwerder": "When does it come? Is there a release date? :)\n. Hi @toni-moreno ,\nthere is already an issue where things like that are discussed. See: https://github.com/graphite-project/graphite-web/issues/677\n. ",
    "wdomburg": "I don't suppose this is forthcoming?\n. ",
    "KlavsKlavsen": "I have the same issue :(\n. but ceres/megacarbon has been deprecated, right?\n. ",
    "wldevries": "Duplicate of https://github.com/graphite-project/carbon/issues/428\n. ",
    "wfarr": "tested and working \n. ",
    "harveyzh": "I am sorry.\nThe \"above\" in description should also change to \"below\".\n. ",
    "derpston": "I forgot to mention that these changes were requested by Hosted Graphite users, and they are already in production.\n. Fixing a bug in the connected lineMode where a line would falsely be drawn from the top left of the graph area to the first datapoint. It is helpful to have an index of the series while drawing datapoints from it. My changes to keepLastValue() were not affected by this bug.\n. That might bring up a usability problem. Could the behaviour of these two options be changed to prevent this confusion in future?\nFor instance, if the user specifies connectedLimit and doesn't specify lineMode the lineMode could be changed to connected. I'm not sure whether that's a good solution, just a suggestion to get the discussion started.\n. ",
    "felixbarny": "+1\n. :+1: \n. ",
    "gdubicki": "+1\n. ",
    "Krinkle": "When will this be released? I'm still experiencing this bug quite often. It also affects Grafana customers.\nIt's especially odd if the point to which all data points were aggregated is later excluded through exclude() or highestCurrent(), in which case there will be a single data point way off the chart (virtually speaking), causing making the resulting graph to be far less useful as it may force the scale to be expanded.\n. @obfuscurity What => The bug fix for movingAverage discussed in this issue.\nWikimedia upgraded to graphite-web 0.9.13+debian-1, but this bug still exists in that version.\nSee also https://phabricator.wikimedia.org/T104536#1448881.\n. See also #677.\n. ",
    "spellik": "Hello.\nHistory functions don`t need any GPL scripts.\nWith best regards, Oleg.\n2013/1/28 Jeff Schroeder notifications@github.com\n\nUnfortunately, the javascript file you include is GPL licensed and\ngraphite is Apache licensed. This would effectively change the license of\ngraphite to GPL and I don't think that is going to pass muster.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/123#issuecomment-12786516.\n. Hello\n\nbonsai.js need for zoom. History work without external JS\n2013/1/29 Nicholas Leskiw notifications@github.com\n\nHi,\nbonsai.js is a GPLv2 file.\nYou cannot add it to this project.\nRead this for more information:\nhttp://www.apache.org/licenses/GPL-compatibility.html\n-Nick\nOn Jan 28, 2013 9:45 AM, \"spellik\" notifications@github.com wrote:\n\nHello.\nHistory functions don`t need any GPL scripts.\nWith best regards, Oleg.\n2013/1/28 Jeff Schroeder notifications@github.com\n\nUnfortunately, the javascript file you include is GPL licensed and\ngraphite is Apache licensed. This would effectively change the license\nof\ngraphite to GPL and I don't think that is going to pass muster.\n\u2014\nReply to this email directly or view it on GitHub<\n\nhttps://github.com/graphite-project/graphite-web/pull/123#issuecomment-12786516>.\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/graphite-project/graphite-web/pull/123#issuecomment-12787762>.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/123#issuecomment-12807671.\n. Removed.\n. \n",
    "gingerlime": "I'm trying to use the events/get_data to retrieve event annotation data for giraffe graphite dashboard. It would be useful to be able to pull data using jsonp rather than just plain-json. This is my first graphite pull request, so hope I'm sticking to the coding standards of the project. I'm also not really sure if this pull request should go into 0.9.x or master branch (or both?).\n. First of all, thanks for the interesting link. I learned something new and quite interesting. \nA couple of comments:\n- I'm not sure how this pull request makes the situation any different. Perhaps it uses the same implementation as where jsonp is used in render, which means if later this needs to be fixed it would need to be fixed in two places?\n- I just did a quick test using the text with the whitespace from the link, and then using the standard json module as well as simplejson (either of those as far as I see are used by graphite in utils). Perhaps I'm wrong, but it looks to me like both these modules handle the conversion correctly already:\n```\nPython 2.7.3 (default, Aug  1 2012, 05:14:39) \n[GCC 4.6.3] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\ns = '{\"JSON\":\"ro\u2028cks!\"}'\ns\n'{\"JSON\":\"ro\\xe2\\x80\\xa8cks!\"}'\nimport json\njson.dumps(s)\n'\"{\\\"JSON\\\":\\\"ro\\u2028cks!\\\"}\"'\nimport simplejson\nsimplejson.dumps(s)\n'\"{\\\"JSON\\\":\\\"ro\\u2028cks!\\\"}\"'\n``\n. +1 - this looks like a very good idea, and would improve performance of external dashboards that pick up json data rather than perform the rendering inside graphite-web.\n. Thanks for merging @obfuscurity . Does this also go to0.9.xbranch / pypi ? or would that require a separate pull request? in other words, when/how does this go into thepip install`ed version?\n\n\n\n(have to admit, I don't understand the graphite project organization / workflow...)\n. I've created a pull request for 0.9.x, hope it can get accepted. I tested it and it works as expected. The code / change is identical to the one that went into master.\n. I would say it would make most sense to remove the safe tag from the template - this probably applies to other elements which might be at risk of XSS if marked as safe... However, I'm wondering if there's a genuine reason to mark them so?\n. Whilst this feature would be nice for projects like Giraffe (which uses Graphite and Rickshaw), I tend to agree with @obfuscurity on this. It's easy enough to convert the format using simple json manipulation (the mapping itself is rather simple).\nThis keeps the graphite project focused and loosely-coupled with any front-end.\n. any feedback / comments? It would be nice to get this merged into 0.9.x. The code is exactly the same as in #170\n. @obfuscurity @SEJeff - any particular reason why this got excluded and didn't get merged into 0.9.11 ??\n. ok, thanks @mleinart \nI was really looking forward to having this officially in... but I've waited long enough, I can survive a while longer I suppose :)\n. ok, that makes sense. It's really far from an urgent feature, so I wouldn't squeeze it into a release at the last minute either. I totally understand.\nThat said, this has been around for a while, it's already in master for even longer, and I've tested it thoroughly and am using it in production. Would be nice to have it merged in at some point.\n. ok, that makes sense. It's really far from an urgent feature, so I wouldn't squeeze it into a release at the last minute either. I totally understand.\nThat said, this has been around for a while, it's already in master for even longer, and I've tested it thoroughly and am using it in production. Would be nice to have it merged in at some point.\n. Thanks for merging this!!\n. @jaimegago - I believe it was merged after 0.9.12 actually. If you install from the 0.9.x it should be included. I'm not a core contributor on this project, so can't decide when this goes in and when it gets updated on pypi unfortunately.\n. ... and much less important but docs say 1.3 pretty much all over the place.\nbut I totally agree with @astanway - seems sensible to avoid compatibility break for a minor release if possible.\n. Thanks for cc'ing me @SEJeff \nTo contribute my rather not-that-well-thought-out 2 cents (and I'm not sure if my opinion should really count for much, so feel free to ignore everything I say):\nI also think that those transformations are pretty easy to do client-side. This definitely wasn't something I felt was a pain-point. \nPluggable functions seems like a good idea. I can even imagine building a completely different query language a-la influxDB / fnordmetric with an SQL-like query... But I have no clue about the implementation challenges of such parsing to be honest.\nAdding metadata (as proposed on #980 / #976) has a definite appeal to me. One of the pain-points I did feel was when combining different targets on a single HTTP request - it was hard to know which results matches which target on the response. (I noticed a similar problem with influxDB btw). Forcing to use an alias was the only workaround and you can still end up with collisions if I remember correctly. Some kind of an identifier would be nice that matches the request / metadata.\nAlso agree with @toni-moreno about backwards compatibility despite it looking less elegant / explicit.\nCompletely OT, and I don't mean to hijack the issue, but I've seen a couple posts on HN declaring graphite \"practically dead\" (backend in particular). Does this have any merit?\n. Thanks for the update / link. And you did a really good job with your response @SEJeff. Very reassuring.\n. ",
    "corywright": "I also would like to see jsonp available where ever json currently is supported.\n. ",
    "jbardin": "sorry I forgot to report back, since I had a fix in production locally.\nDEFAULT_CACHE_DURATION reduced the quantization, but can't eliminate it. What happens I think is that since I need the timespan [t0, t10], that cache will always be created at some t0+delta, and that delta will always increase over time. This gives you a shifting window of data wich doesn't line up with the actual time range being requested.\n. ",
    "colinbjohnson": "The problem stems from the rewrite of the protocol used to access the site - in this case, I initially access the site using https and when logging in the requests after login are sent to http. In my case, the site only answers at https.\n. ",
    "captsens": "Hi Jeff\nMany thanks for the reply.\nI was just trying to get some feedback on whether my pull request was\nheading in the right direction - I hadn't been able to find any discussions\non webapp security among the developers, and I was wondering if there were\nany agreed directions or ideas for implementing it.  I didn't want to be\npushing a model that wasn't generally acceptable.\nAnd sorry for not lurking longer on IRC, but here in Australia it was\n$bed_time :)  Next time I'll try to connect earlier in my day.\nThanks again ............................ John\nOn 20 February 2013 00:59, Jeff Schroeder notifications@github.com wrote:\n\n@captsens https://github.com/captsens This is the right place to send\npull requests. Unfortunately, the maintainers have a $real_job, so can't\nalways go through things super fast. Also, on IRC, please idle for awhile.\nSometimes it takes someone a bit to respond. They can't if you disconnect :)\nI'm just responding to your IRC comment asking if this is the right place.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/143#issuecomment-13776439.\n\n\n\nPaddling hard to escape the mainstream.\n. I've now added a second commit, which I'm hoping provides a pretty good set of security options for the dashboard.\nThis commit builds on the first one, and leaves the dashboard with a choice of 4 security models:\n1. No security (still the default)\n2. Only logged-in users are allowed to modify and delete dashboards\n3. Only users in a specified group are allowed to modify and delete dashboards\n4. Only users with the \"change dashboard\" permission (assigned in the Admin app) can modify dashboards, and similarly with the \"delete dashboard\" permission.\nOther functions, e.g. login/logout and disabling of menu items, are unchanged from the previous commit.\nI'd also like to provide some documentation for these options, but I haven't worked how to submit pull requests for that...\nCheers ................................................ John\n. Sneaky, hiding the documentation in such an obvious place :)\nSo, I've added notes on the authorisation settings to config-local-settings.rst.\n. I discovered that the local settings config documentation wasn't linked from the index.html, so I've added that too.\n. Thanks for reviewing this, @mleinart.\nYou make a good point about the config - I wasn't quite happy about it, and should have gone back and played some more.  \nI prefer your option 2 - authentication is required for all 3 security models, so it makes a good on-off switch, and the other 2 settings (group name and requiring permission) add to this: they can be used together, separately, or not at all.  It all makes sense.\nI'm thinking that I should rename DASHBOARD_EDIT_GROUP to DASHBOARD_REQUIRE_EDIT_GROUP, to fit in with the other option names.  What do you think?\nI'll make the changes tonight (Australian time).\n. Config changed as discussed.\n. ",
    "jjdeluxe": ":thumbsup: \n. ",
    "kovyrin": "+1\n. So, how exactly was it a valid decision to close the totally legitimate feature request issue? Especially when it was based on absolutely invalid understanding of the request.\n. Ugh, writing pull commit messages late at night is not always a good idea :-)\nIt should read like this: So, in this small commit I've updated the code of all functions that touch timeseries data to update pathExpression along with the time series name.\n. @anatolijd the issue is easily reproducible on the master:\n1) Take any growing metric (network interface bytes sent, successfull logins, anything that constantly grows)\n2) Use a derivative function on it to get throughput values\n3) Use scaleToSeconds(foo, 1) on it to convert the value to per-second metric\n4) Use movingAverage(foo, 10) on it and you'll see the spike at the beginning of the graph\n. It is definitely reproducible on the latest 0.9.x branch.\n. ",
    "carmandrew": "+1, I want to be able to add a new metric and then just type in what I want. Rather than having to add a new metric via drag and drop, and then edit it to something else. Similar with removing a metric.\n. ",
    "pguth66": "+1\n. ",
    "kaverne": "+1\n. ",
    "cryptographrix": "+1\n. Wait, a UI change to this box is a \"big carbon change\"? https://www.dropbox.com/s/unp8075orhy3fcv/Screenshot%202015-06-26%2015.19.13.png?dl=0\nWe're talking about adding a button to support removal of metrics in graphs under /dashboard/...\n. We're talking about adding and removing metrics from the graph definitions that are stored in the sqlite DB.\nAs per your own \"readthedocs\" documentation, \"Graphite uses n SQLite database file located at STORAGE_DIR/graphite.db\".\nIndeed, a \"select * from dashboard_dashboard;\" from within sqlite client of that file produces a list of all of the dashboards in question complete with the json definitions of their contents.\n```\nls -lah /opt/graphite/\ntotal 14M\ndrwxr-xr-x  6 apache apache 4.0K Jun 26 01:07 .\ndrwxr-xr-x  9 apache apache 4.0K Dec 21  2013 ..\n-rw-r--r--  1 root   root      4 Oct 22  2014 carbon-cache-a.pid\n-rw-r--r--  1 apache apache 118K Dec  5  2014 graphite.db\n```\nThis is not a carbon change.  \nThis is stored within the django-based SQLite DB and would be a UI change, likely within js/ext/resources/css/ext-all.css, which is an external library, or another UI element.\nedit: I was harsh and ext-all is an external dep.\n. sqlite> select * from dashboard_dashboard where name='network';\nnetwork|{\"name\": \"network\", \"defaultGraphParams\": {\"width\": \"300\", \"from\": \"-12hours\", \"until\": \"now\", \"height\": \"230\"}, \"refreshConfig\": {\"interval\": 60000, \"enabled\": false}, \"graphs\": [[\"target=pings.*.*.*.*.duration\", {\"from\": \"-12hours\", \"target\": [\"pings.*.*.*.*.duration\"], \"until\": \"now\"}, \"/render?width=300&from=-12hours&until=now&height=230&target=pings.*.*.*.*.duration&_uniq=0.8197899542283267&title=pings.*.*.*.*.duration\"], [\"target=pings.*.*.*.*.returned\", {\"areaMode\": \"stacked\", \"from\": \"-12hours\", \"target\": [\"pings.*.*.*.*.returned\"], \"until\": \"now\"}, \"/render?width=300&from=-12hours&until=now&height=230&areaMode=stacked&target=pings.*.*.*.*.returned&_uniq=0.9330033755395561&title=pings.*.*.*.*.returned\"], [\"target=scale(derivative(snmp.10.0.0.1.eth2.ifBytesIn)%2C0.016667)&target=scale(derivative(snmp.10.0.0.1.eth2.ifBytesOut)%2C0.016667)\", {\"areaMode\": \"all\", \"until\": \"now\", \"from\": \"-12hours\", \"target\": [\"scale(derivative(snmp.10.0.0.1.eth2.ifBytesIn),0.016667)\", \"scale(derivative(snmp.10.0.0.1.eth2.ifBytesOut),0.016667)\"], \"title\": \"Internal: Default\"}, \"/render?width=300&from=-12hours&until=now&height=230&areaMode=all&title=Internal%3A%20Default&target=scale(derivative(snmp.10.0.0.1.eth2.ifBytesIn)%2C0.016667)&target=scale(derivative(snmp.10.0.0.1.eth2.ifBytesOut)%2C0.016667)&_uniq=0.9130394915118814\"], [\"target=scale(derivative(snmp.10.0.0.1.eth2.30.ifBytesIn)%2C0.016667)&target=scale(derivative(snmp.10.0.0.1.eth2.30.ifBytesOut)%2C0.016667)\", {\"areaMode\": \"all\", \"until\": \"now\", \"from\": \"-12hours\", \"target\": [\"scale(derivative(snmp.10.0.0.1.eth2.30.ifBytesIn),0.016667)\", \"scale(derivative(snmp.10.0.0.1.eth2.30.ifBytesOut),0.016667)\"], \"title\": \"Internal: DMZ\"}, \"/render?width=300&from=-12hours&until=now&height=230&areaMode=all&title=Internal%3A%20DMZ&target=scale(derivative(snmp.10.0.0.1.eth2.30.ifBytesIn)%2C0.016667)&target=scale(derivative(snmp.10.0.0.1.eth2.30.ifBytesOut)%2C0.016667)&_uniq=0.9641848232131451\"], [\"target=scale(derivative(snmp.10.0.0.1.eth2.20.ifBytesIn)%2C0.016667)&target=scale(derivative(snmp.10.0.0.1.eth2.20.ifBytesOut)%2C0.016667)\", {\"areaMode\": \"all\", \"until\": \"now\", \"from\": \"-12hours\", \"target\": [\"scale(derivative(snmp.10.0.0.1.eth2.20.ifBytesIn),0.016667)\", \"scale(derivative(snmp.10.0.0.1.eth2.20.ifBytesOut),0.016667)\"], \"title\": \"Internal: Public\"}, \"/render?width=300&from=-12hours&until=now&height=230&areaMode=all&title=Internal%3A%20Public&target=scale(derivative(snmp.10.0.0.1.eth2.20.ifBytesIn)%2C0.016667)&target=scale(derivative(snmp.10.0.0.1.eth2.20.ifBytesOut)%2C0.016667)&_uniq=0.5761637438554317\"], [\"target=scale(derivative(snmp.10.0.0.1.eth2.60.ifBytesIn)%2C0.016667)&target=scale(derivative(snmp.10.0.0.1.eth2.60.ifBytesOut)%2C0.016667)\", {\"until\": \"now\", \"from\": \"-12hours\", \"target\": [\"scale(derivative(snmp.10.0.0.1.eth2.60.ifBytesIn),0.016667)\", \"scale(derivative(snmp.10.0.0.1.eth2.60.ifBytesOut),0.016667)\"], \"title\": \"Internal: Nick's\"}, \"/render?width=300&from=-12hours&until=now&height=230&title=Internal%3A%20Nick's&target=scale(derivative(snmp.10.0.0.1.eth2.60.ifBytesIn)%2C0.016667)&target=scale(derivative(snmp.10.0.0.1.eth2.60.ifBytesOut)%2C0.016667)&_uniq=0.3786822578404099\"], [\"target=scale(derivative(snmp.10.0.0.1.eth1.ifBytesOut)%2C0.016667)&target=scale(derivative(snmp.10.0.0.1.eth1.ifBytesIn)%2C0.016667)\", {\"areaMode\": \"all\", \"until\": \"now\", \"from\": \"-12hours\", \"target\": [\"scale(derivative(snmp.10.0.0.1.eth1.ifBytesOut),0.016667)\", \"scale(derivative(snmp.10.0.0.1.eth1.ifBytesIn),0.016667)\"], \"title\": \"External: FiOS\"}, \"/render?width=300&from=-12hours&until=now&height=230&areaMode=all&title=External%3A%20FiOS&target=scale(derivative(snmp.10.0.0.1.eth1.ifBytesOut)%2C0.016667)&target=scale(derivative(snmp.10.0.0.1.eth1.ifBytesIn)%2C0.016667)&_uniq=0.732207843568176\"], [\"target=scale(derivative(snmp.10.0.0.1.tun0.ifBytesOut)%2C0.016667)&target=scale(derivative(snmp.10.0.0.1.tun0.ifBytesIn)%2C0.016667)\", {\"areaMode\": \"all\", \"until\": \"now\", \"from\": \"-12hours\", \"target\": [\"scale(derivative(snmp.10.0.0.1.tun0.ifBytesOut),0.016667)\", \"scale(derivative(snmp.10.0.0.1.tun0.ifBytesIn),0.016667)\"], \"title\": \"External: IPV6\"}, \"/render?width=300&from=-12hours&until=now&height=230&areaMode=all&title=External%3A%20IPV6&target=scale(derivative(snmp.10.0.0.1.tun0.ifBytesOut)%2C0.016667)&target=scale(derivative(snmp.10.0.0.1.tun0.ifBytesIn)%2C0.016667)&_uniq=0.46815102454274893\"]], \"timeConfig\": {\"startDate\": \"2014-10-18T23:27:16\", \"endTime\": \"5:00 PM\", \"endDate\": \"2014-10-18T23:27:16\", \"relativeStartQuantity\": \"12\", \"relativeUntilQuantity\": \"\", \"startTime\": \"9:00 AM\", \"relativeStartUnits\": \"hours\", \"type\": \"relative\", \"relativeUntilUnits\": \"now\"}, \"graphSize\": {\"width\": \"300\", \"height\": \"230\"}}\n. What we're talking about is the ability to remove a metric from graphs in the dashboard definition located at - for instance - https://graphitesystem.host.com/dashboard/#main.\nAdding is easy - drag and drop, or merge, or rename...but there is no way to remove a metric from the graph except by destroying the graph and recreating it without that metric.\nWhen you're working on complex graphs that may contain many different transformations, that's the last thing you want to do.\n. \n. Ah, the \"send a PR\" response that's quickly becoming the inverse of \"Let me Google that for you\" on Github.\nSure - as someone that has absolutely no direct experience with the codebase you've built or the Django framework, we'll get right on that.\nGot a \"donate\" link we could put money into to prioritize this change instead?\n. same.\n. I think what I mean to suggest are alternative support models for distributed org/corp-less projects like Graphite that target enterprises.\nI feel the contributor's pain too - How do issues get prioritized, by technical merit or issue advocacy/demand? Which issues are more important than others? Why should \"advocated issue 357\" get prioritized over \"big technical issue 3\"?\nIn enterprise-support world, there's a customer support/account rep that handles this prioritization.\nThis is a two+ year old issue that looks (\"looks\") like a relatively \"easy\" fix within the UI for somebody that knows Django and the graphite-web codebase...and it really makes graphite-web look abandoned for it not to have been fixed in the time it was open.\nBut to have it closed after two years...\nIf it really did require a carbon change etc - sure, I totally would've let it drop there.\nBut my 10 minutes of investigation showed that it didn't (above).\n@kovyrin asked the important question without my attitude.\n. ",
    "matlockx": "+1\n. ",
    "AnthonySteele": "+1 it's silly that the basic operations of adding and removing lines from a graph is so roundabout and undiscoverable.\n. Docs are empty of content :disappointed: \ni.e. the list of functions as per 'docs/functions.rst' and at http://graphite.readthedocs.org/en/latest/functions.html#list-of-functions is empty. (consider this a +1 on the issue)\nYou can find an older copy at: https://graphite.readthedocs.org/en/0.9.10/functions.html\n. @obfuscurity was it not clear, that's just a +1 on the issue?\n. ",
    "juancb": "+1\n. ",
    "TheDeveloper": "+1, for now I make do by replacing the metric with gobbledigook. The graph will ignore metrics that don't exist.\n. ",
    "markhu": "Is this even possible? \n\nit's silly that the basic operations of adding and removing lines from a graph is so roundabout and undiscoverable.\n. \n",
    "deniszh": "It seems that everybody using alternative dashboards, and nobody really care enough to provide that functionality. As far as I understand, good amount of code needs to be written. For simple dashboards it's not needed too, so... \n. Using that patch (backported to 0.9.12) in production, haven't any troubles, even on sqlite\n. Wow, nice debugging, @brianmcdonnell , thanks a lot!\ndatalib code was refactored in 1.1.x, but whisper.fetch() still not accepting now - https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/readers/whisper.py#L57\nBut fix should be quite easy - just propagate now through https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/readers/whisper.py#L54\nAm I right, @DanCech ?\n. Not very good as I see - https://github.com/graphite-project/graphite-web/pull/551 needs to be rebased and tests need to be added (IIRC no function changes w/o testing accepted).\n. @obfuscurity - on my to-do list on weekend too. :)\nPhew, did that properly at last.\n. Just tried to repeat steps from manual, LGTM:\n``` bash\nroot@vagrant-ubuntu-precise-64:~# pip install carbon --install-option=\"--prefix=/srv/graphite\" --install-option=\"--install-lib=/srv/graphite/lib\"\nDownloading/unpacking carbon\n  Downloading carbon-0.9.12.tar.gz (47Kb): 47Kb downloaded\n  Running setup.py egg_info for package carbon\npackage init file 'lib/twisted/plugins/__init__.py' not found (or not a regular file)\nwarning: no files found matching '*' under directory 'conf/'\nwarning: no files found matching '*' under directory 'distro/'\nwarning: no previously-included files found matching 'conf/*.conf'\n\nRequirement already satisfied (use --upgrade to upgrade): twisted in /usr/lib/python2.7/dist-packages (from carbon)\nDownloading/unpacking txamqp (from carbon)\n  Downloading txAMQP-0.6.2.tar.gz\n  Running setup.py egg_info for package txamqp\nInstalling collected packages: carbon, txamqp\n  Running setup.py install for carbon\n    package init file 'lib/twisted/plugins/init.py' not found (or not a regular file)\n    changing mode of build/scripts-2.7/carbon-client.py from 644 to 755\n    changing mode of build/scripts-2.7/carbon-relay.py from 644 to 755\n    changing mode of build/scripts-2.7/carbon-aggregator.py from 644 to 755\n    changing mode of build/scripts-2.7/carbon-cache.py from 644 to 755\n    changing mode of build/scripts-2.7/validate-storage-schemas.py from 644 to 755\nwarning: no files found matching '*' under directory 'conf/'\nwarning: no files found matching '*' under directory 'distro/'\nwarning: no previously-included files found matching 'conf/*.conf'\nchanging mode of /srv/graphite/bin/carbon-client.py to 755\nchanging mode of /srv/graphite/bin/carbon-relay.py to 755\nchanging mode of /srv/graphite/bin/carbon-aggregator.py to 755\nchanging mode of /srv/graphite/bin/carbon-cache.py to 755\nchanging mode of /srv/graphite/bin/validate-storage-schemas.py to 755\n\nRunning setup.py install for txamqp\nSuccessfully installed carbon txamqp\nCleaning up...\nroot@vagrant-ubuntu-precise-64:~# ls -al /srv/graphite\ntotal 24\ndrwxr-xr-x 6 root root 4096 Oct 16 19:23 .\ndrwxr-xr-x 3 root root 4096 Oct 16 19:23 ..\ndrwxr-xr-x 2 root root 4096 Oct 16 19:23 bin\ndrwxr-xr-x 2 root root 4096 Oct 16 19:23 conf\ndrwxr-xr-x 7 root root 4096 Oct 16 19:23 lib\ndrwxr-xr-x 6 root root 4096 Oct 16 19:23 storage\nroot@vagrant-ubuntu-precise-64:~# ls -al /srv/graphite/bin/\ntotal 32\ndrwxr-xr-x 2 root root 4096 Oct 16 19:23 .\ndrwxr-xr-x 6 root root 4096 Oct 16 19:23 ..\n-rwxr-xr-x 1 root root 1085 Oct 16 19:23 carbon-aggregator.py\n-rwxr-xr-x 1 root root 1085 Oct 16 19:23 carbon-cache.py\n-rwxr-xr-x 1 root root 4321 Oct 16 19:23 carbon-client.py\n-rwxr-xr-x 1 root root 1085 Oct 16 19:23 carbon-relay.py\n-rwxr-xr-x 1 root root 2236 Oct 16 19:23 validate-storage-schemas.py\nroot@vagrant-ubuntu-precise-64:~# ls -al /srv/graphite/lib\ntotal 28\ndrwxr-xr-x 7 root root 4096 Oct 16 19:23 .\ndrwxr-xr-x 6 root root 4096 Oct 16 19:23 ..\ndrwxr-xr-x 3 root root 4096 Oct 16 19:23 carbon\ndrwxr-xr-x 2 root root 4096 Oct 16 19:23 carbon-0.9.12-py2.7.egg-info\ndrwxr-xr-x 3 root root 4096 Oct 16 19:23 twisted\ndrwxr-xr-x 2 root root 4096 Oct 16 19:23 txAMQP-0.6.2-py2.7.egg-info\ndrwxr-xr-x 3 root root 4096 Oct 16 19:23 txamqp\nroot@vagrant-ubuntu-precise-64:~# ls -al /srv/graphite/lib/twisted/\ntotal 12\ndrwxr-xr-x 3 root root 4096 Oct 16 19:23 .\ndrwxr-xr-x 7 root root 4096 Oct 16 19:23 ..\ndrwxr-xr-x 2 root root 4096 Oct 16 19:23 plugins\nroot@vagrant-ubuntu-precise-64:~# ls -al /srv/graphite/lib/twisted/plugins/\ntotal 32\ndrwxr-xr-x 2 root root 4096 Oct 16 19:23 .\ndrwxr-xr-x 3 root root 4096 Oct 16 19:23 ..\n-rw-r--r-- 1 root root  675 Oct 16 19:23 carbon_aggregator_plugin.py\n-rw-r--r-- 1 root root 1168 Oct 16 19:23 carbon_aggregator_plugin.pyc\n-rw-r--r-- 1 root root  643 Oct 16 19:23 carbon_cache_plugin.py\n-rw-r--r-- 1 root root 1126 Oct 16 19:23 carbon_cache_plugin.pyc\n-rw-r--r-- 1 root root  641 Oct 16 19:23 carbon_relay_plugin.py\n-rw-r--r-- 1 root root 1124 Oct 16 19:23 carbon_relay_plugin.pyc\nroot@vagrant-ubuntu-precise-64:~#\n```\nDid I understand something wrong?\n. Can we merge PRs and close issue? It's still in release milestone.\n. Can we merge PRs and close issue? It's still in release milestone.\n. I can be wrong, but IIRC 0.9.x stops working with 0.9.12 after that commit - https://github.com/graphite-project/graphite-web/commit/28bbd7f9abf9daef102601f673bd6fdac679542a - it's almost 1 year ago.\nYou can try use graphite-web from 0.9.x branch - might be just drop-in replacement for 0.9.12\n. Or try to apply https://github.com/gingerlime/graphite-web/commit/5b1015df91b7740a96299594a3a1727238517c20 to 0.9.12 - also might work.\n. It should be rebased and altered to support backward compatibility with WHISPER_DIR.\n@joemiller - do you still have an interest in this?. Hi All,\nJust my five cents - just checked 0.9.11 on Ubuntu 12.04 LTS and found\nthat's not working for Django 1.3 there. It fails with \"ViewDoesNotExist:\nCould not import graphite.events.views. Error was: No module named\ntimezone\".\nThanks.\n2013/8/21 gingerlime notifications@github.com\n\n... and much less important but docs[\nhttps://github.com/graphite-project/graphite-web/blob/0.9.x/docs/releases/0_9_11.rst]\nsay 1.3 pretty much all over the place.\nbut I totally agree with @astanway https://github.com/astanway - seems\nsensible to avoid compatibility break for a minor release if possible.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/399#issuecomment-22996812\n.\n. I didn't try this, but I'm think if you define environment variable READTHEDOCS (for example SetEnv  READTHEDOCS True in httpd.conf) it will help\n. Or check that PR - https://github.com/graphite-project/graphite-web/pull/445\n. @brutasse, you can provide step as additional parameter for constantLine(), as was already have - for example in time(), sin(), and randomWalk() functions - https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/functions.py#L3105\n. Other option is adding collections.deque to unpickler:\n\n'collections': set(['deque']),\nafter line 84 and 107 in webapp/graphite/util.py\n(so, both sections will be look like\n```\nPICKLE_SAFE = {\n  'copy_reg': set(['_reconstructor']),\n  'builtin': set(['object']),\n'collections': set(['deque']),\n}\n```\n)\nWorks for me, but not sure is it safe to do so...\n2013/9/9 Nelson Elhage notifications@github.com\n\nI'm running with nelhage/carbon@e5eea1bhttps://github.com/nelhage/carbon/commit/e5eea1b4e3a747e31daa037af87f433ba0f45011as a workaround -- it makes carbon return a\nlist instead of deque. Since graphite immediately calls list() on the\noutput from carbon-cache anyways, this has no functional effect, but lets\nthe data pass through the safe unpickler.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/issues/423#issuecomment-24099728\n.\n. Maybe https://github.com/graphite-project/graphite-web/commit/fbcc3ef9d9989a524921031115b9e76ecee4863a which is already in 0.9.12 branch already fixes this issue? Could someone try?\n. BTW - remote rendering is not working also because of this. If you need that - make your PICKLE_SAFE looking like this \n\nPICKLE_SAFE = {\n      'copy_reg': set(['_reconstructor']),\n      '__builtin__': set(['object', 'list']),\n      'collections': set(['deque']),\n      'graphite.render.datalib': set(['TimeSeries']),\n      'graphite.intervals': set(['Interval', 'IntervalSet']),\n    }\n. It really depends on how it was installed, @susampath . Can we merge PRs and close issue? It's still in release milestone.\n. Can we merge PRs and close issue? It's still in release milestone.\n. I can backport it to 0.9.x, but please note that we have no plans to release 0.9.x anymore.\n. @jkandula: No, not yet. Made it a part of 0.9.16 milestone.\nIt's in the master branch, so, will be part of 1.0 release too - when it is done.\n. For working remote rendering you need to add 'graphite.render.datalib': set(['TimeSeries']) to the list:\nPICKLE_SAFE = {\n      'copy_reg': set(['_reconstructor']),\n      '__builtin__': set(['object', 'list']),\n      'collections': set(['deque']),\n      'graphite.render.datalib': set(['TimeSeries']),\n      'graphite.intervals': set(['Interval', 'IntervalSet']),\n    }\n(lines 90-96 and 116-122 in webapp/graphite/util.py)\n. Backported that fix to master branch too, just in case, please merge if needed.\n. Maybe @szibis can confirm?\n. @xkilian, if this error is not fixed (cache is not working because of broken unpickling) performance of Graphite is awful. IMO caching of internal Graphite metrics is definitely another issue.\n. @ocervell - unfortunately not yet.\nWe still need to attack this issue.\n. It completely slips off our radars. Developers on my work hitting that constantly, I even using dirty hack for 0.9.x\nWill try to make a fix for a master, but adding 1.1.0 milestone just in case.\n. @ocervell :  There're 2 main reasons why this issue is not in active development yet. First - that logic buried quite deep in whisper itself, and fix for this is quite big and massive, as @DanCech said.\nOn the other hand - if you will use SSDs and increase the number of your caches delay can be quite low and acceptable. I'm not saying that we should fix it, just explaining why even 3rd party graphite implementations still have this issue. :(. Milestone 1.2.0 means only 'not now'. Currently, we have no solution for that problem, and not actively working on it.\nSorry.. @Comradin, \nIIRC it's already done in https://github.com/graphite-project/graphite-web/pull/989 and later fixes.\n. Good question. Maybe nobody checked if issue is really gone in master / 0.9.x ?\n@torkelo, could you please test it?\nAnd please note that master and 0.9.x are different branches.\n. @g76r - yep, but it's not ready to merge now :(\n. Looks good, tests are there, merging.\n. Ubuntu 12.04 still in use, although it's not latest LTS anymore...\n. :+1: what @brutasse said - I also see no way how it could be fixed in 0.9.13 w/o breaking many things. Maybe fixing it in 1.0-dev is much better idea? \n. Sorry, I'm agree with @obfuscurity here - not really sure that we need that complexity here.\nAny other opinions? \n@brutasse @SEJeff @esc @bmhatfield @cbowman0 ?\n. Will try to merge this.. Tests are green. @nox, please try 0.9.x branch, as was suggested by @obfuscurity. If you install carbon from 0.9.x and graphite-beb from 0.9.x, and also has Django 1.4 or later it must work out of the box. And if you need rpm or deb package you can easily built it using marvellous https://github.com/jordansissel/fpm\n. @jssjr :+1: \n. @obfuscurity -\nOn my way!\n. @obfuscurity - here you are - \n1064\n1065\n. I have an idea - as we have couple of issues with current 0.9.13-pre1 (like https://github.com/graphite-project/carbon/issues/383 or https://github.com/graphite-project/carbon/issues/355) when pypy decides that 0.9.13 is already released - maybe we need to rename current release to 0.9.14 then?\n. Looks like https://github.com/graphite-project/graphite-web/pull/1188 is a good thing to have in upcoming release. Will try to port it.\n. It looks like I found something, but I do not know how to fix it properly. I'm absolute novice in RTD, so...\n\nIt seems that pip on RTD now honors setup.cfg settings, if it's located in same directory -  that's why when RTD tries to run\n...pip install --exists-action=w -rdocs/requirements.txt it tries to write to '/opt/graphite' (because we still have\n[install]\nprefix=/opt/graphite\nin setup.cfg for 0.9.x - and fails.\n@obfuscurity tried to fix that with https://github.com/graphite-project/graphite-web/commit/8539ebb1c72351a091a2825b3943acae3c1a8b87, but problem that installing docs dep's step running before running pip setup.py install...\nI found no ways how we can change pip behaviour, so, we need to change setup.py for 0.9.x to similar setup as we have in master. I'll create PR for that.\nPlease test.\n. You can use some external dashboard with authorization (e.g. Grafana http://grafana.org ) and limit Graphite access only from that Grafana host (localhost, if installing Grafana on the same host as Graphite)\n. A I only one who doesn't get what the problem is?\nFor me all graphs are look completely sane. \nAs documentations of derivative() says \" This function does not normalize for periods of time, as a true derivative would. Instead see the perSecond() function to calculate a rate of change over time.\"\n. @sravanireddyl:\nWhat you mean \"i need the fixed Y-axis value without fractional values(exact count of metric)\"? \nYou need logins per second/minute/hour ? Use perSecond() and scale it as documentation says.\nOr, if perSecond() is not in your Graphite - use scaleToSeconds(nonNegativeDerivative(...),1) - which is exactly the same. But it still be fractional too, I think, because of rounding errors.\n. scaleToSeconds(nonNegativeDerivative(...),60) will give you logins per minute, scaleToSeconds(nonNegativeDerivative(...),3600) - per hour/etc\n. scaleToSeconds(nonNegativeDerivative(...),60) will give you number of logins per minute. But I don't think it will be exact integer, because of nature of derivative and rounding errors. \n. Then it's depends on your storage schema. By default retention function is avg, but for counters it's need to be sum. Check this article for details - http://www.franklinangulo.com/blog/2014/5/20/graphite-series-3-whisper-storage-schemas-aggregations\n. @bilal-fazlani - it's not Graphite issue, it's your storage schema / aggregation issue. Could you please check links above -  http://www.franklinangulo.com/blog/2014/5/20/graphite-series-3-whisper-storage-schemas-aggregations ?\nWhat's your /opt/graphite/conf/storage-schemas.conf and /opt/graphite/conf/storage-aggregation.conf looks like?\n. It's not that easy as cherry-pick btw... Need to test that before moving to PR.\n. Trying to test my change, but can't get RRD read working at all, even on clean 0.9.x\nI'm creating test rrd file and put it to /opt/graphite/storage/rrd/blah/test.rrd but have no success to get bla.test metric in web interface...\nAm I doing that right or not? Does anyone has experience with Graphite + RRD ?\n. @pabigot is right - it's another Unicode bug, but it's not about RRD and unicode filepath (not metric's name) as #552. Still can't reproduce it to test my fix and - as we seems have ton of other Unicode bug as well - not even sure if we need to fix that at all in 0.9.x...\n. @pabigot is right - it's another Unicode bug, but it's not about RRD and unicode filepath (not metric's name) as #552. Still can't reproduce it to test my fix and - as we seems have ton of other Unicode bug as well - not even sure if we need to fix that at all in 0.9.x...\n. @pabigot \nI can't get RRD working on 0.9.x at all, maybe I'm doing that wrong. I'm creating test rrd file and put it to /opt/graphite/storage/rrd/blah/test.rrd but have no success to get bla.test metric in web interface.\n. @pabigot \nI can't get RRD working on 0.9.x at all, maybe I'm doing that wrong. I'm creating test rrd file and put it to /opt/graphite/storage/rrd/blah/test.rrd but have no success to get bla.test metric in web interface.\n. At least my logic is OK. Will test other things one more time tonight.\nThanks! \n. Ok, I spent some time on that and for 0.9.x backport of this patch I'm officially want to drop this as WONTFIX. I'll better to focus on other  backports and fixes. :smiling_imp:\n. Hi @g76r,\nis your version of graphite-web has that additions - \nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/util.py#L144\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/util.py#L170 ? \n. It could. Master contains it and works fine, also 0.9.x \n. @bmhatfield, 0.9.12 was released with broken cache because broken unpickle module,  and some of this lines was added later, in 0.9.x and master branch. \n. Just make PICKLE_SAFE looks exact  like in current 0.9.x or even try current 0.9.x version - on Django >= 1.4 it works like drop-in replacement - but better to check release notes first, of course. \n. @g76r, my response was for @bmhatfield, 'cause he complains about 0.9.x. But for master it will work too - just cherry-pick that PICKLE_SAFE fix - It's quite isolated from other code. \n. @stanjer : \"except the work\" - no, I don't think so.\nBut please note that we should stay to be compatible with 2.7 too,  so, it should be dual compatible.. I think yes.. @mbethke : First, it should be rebased. Did you really try that change? Is it really a big improvement?\nSecond - that's a good question about tests in this particular case. It's a change of TimeSeries class, main core thing of whole Graphite. Do we really need a unit test for such case? For what then - for a private function? \nThoughts? /cc @DanCech @iksaif @brutasse ?. @Jessicalx - unfortunately we have no releases for master branch, you need to install it from GitHub directly. Current master version is 0.10.0\n. Hi @esc,\nCould you please point to these commits please? :)\n. Nah, my fix is not working, @mleinart was right :)\npython\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 111, in get_response\n    response = callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 53, in renderView\n    (graphOptions, requestOptions) = parseOptions(request)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 305, in parseOptions\n    startTime = min(fromTime, untilTime)\nTypeError: can't compare offset-naive and offset-aware datetimes\nIt seems time handling in Flask is much better, or @brutasse also fix this in different way... \n. Hm. It's working now, but not sure if I patched fetchData() right. @brutasse, could you please check ?\n. More fixes after @brutasse advices. Will port to master w/proper testing today or on weekend.\n. Why? For me it's working on 1.3\n. Kindly asking @obfuscurity , @cdavis and/or @mleinart to review, much appreciated!\n. Made a quick look for master branch - not even sure if this thing still relevant there, seems it's not.\n. Ah, I was too sleepy/dumb yesterday, and made logic mistake during merging my patch with current 0.9.x branch. Now it's working as intended, I also added logging to check:\n\nSo, it seems really need to be reviewed more thoroughly when someone will have a minute.\nThanks a lot!\n. Ah, it seems I screwed that pull request, will create new one.\n. And it seems that I found bug in code btw, so good that I screw it. Will think about testing that first. \n. Since we use that function I can try to make tests for it instead of @Krylon360 \nBut I found that functions tests for 0.9.x in in functions_test.py \nCould someone give me a hint how to run tests from there? @obfuscurity ? @drawks ?\nThanks!\n. Since we use that function I can try to make tests for it instead of @Krylon360 \nBut I found that functions tests for 0.9.x in in functions_test.py \nCould someone give me a hint how to run tests from there? @obfuscurity ? @drawks ?\nThanks!\n. I'll try to create proper test for that, then it will be definitely ready for merging.\n. Ah, screw it. @obfuscurity, as preparation to 0.9.13 - could you bear with me and just merge it? It's clear backport of https://github.com/graphite-project/graphite-web/pull/522 form master and many users vote for it. One less patch to backport to my own branch ;) \nPlease?\n. :+1: \nI backported that to my branch, but was disttracted from backporting tests, sorry...\n. I think using shared memcache is quite obvious, I never faced this issue because if this. We are also ignoring last value in Nagios checks BTW. \nMaybe we need just to recommend using shared cache in documentation? \n. I removed it from 0.9.13 milestone for now (with @obfuscurity approval :smile_cat: )\n. Ah, you're right. Will rework that, sorry.\n. Yep, got that, thanks. That's why I just screwed both of my PRs :)\nWill rework properly today, with feature branches.\n. Already backported and merged by @gwaldo. Thanks!\n. Caching code at master was reworked a lot, I didn't find how to put cache-query-bulk there at a glance. I'll make a second try. :)\n. @obfuscurity I know, but AFAIR there was agreement to not put new functionality to 0.9.x, so, I tried to put it to master first.\n. @SEJeff, yep, already working on this.\nPlease merge this one to 0.9.x at least. :)\n. I can say that support 0.9.13 in Ubuntu 12.04 and Debian 6 will be quite hard. AFAIK Ubuntu 12.04 has only Django 1.3 - http://packages.ubuntu.com/precise/python-django and Debian 6 has Django 1.2 - https://packages.debian.org/squeeze/python-django and 0.9.13 requires Django 1.4\nAnd AFAIR I faced with many other issues with python packages even on 0.9.10.\n. IMHO https://github.com/hw-cookbooks/graphite is good candidate for filling 'Install by Chef' docs section. Binary package implies installation of some amount deb or rpm packages and that's all. You can transfer these packages to own repo and install it even without Internet access - and it must work after that. \n. Currently I'm installing Graphite on my prod envs using deb packages built\nby FPM. Is it good idea or it's better to stick with standard Debian build\nsystem?\n+1 for Apache + mod_wsgi\nAlso maybe we could use supervisord   for starting daemons? It's pythonish\nand cross-platform.\n24 \u0430\u0432\u0433. 2014 \u0433. 1:14 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \"Ryan McKern\" notifications@github.com\n\u043d\u0430\u043f\u0438\u0441\u0430\u043b:\n\n[image: :+1:] to not building omnibus packages if it can be avoided.\nIt'll inflate packages considerably (where do you dry the line in the\nstack?) and it'll increase complexity. @deniszh\nhttps://github.com/deniszh Have you already started digging into deb\npackages?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/864#issuecomment-53169896\n.\n. @brutasse, only 14.04 and 14.10 ubuntu has current graphite packages. RHEL / CentOS / Fedora / Debian 6 / Debian 7 / Ubuntu 12.04 are not. And yep, in Ubuntu there's no /opt/graphite prefix, all packages are installing to appropriate (by distro) places. /etc/graphite/local_settings.py looks odd to me, but good from distro perspective.\n. sigh\nI think I need to add tests there to calm #863 ...\n. BTW, according our per-case backport rules, is it a good idea to have Short\nURL functionality in 0.9.x? I did it for 0.9.12 in our Graphite\ninstallation already and will do it for 0.9.13 after release anyway, so, is\nit good idea to share that patch?\n01 \u0441\u0435\u043d\u0442. 2014 \u0433. 23:59 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \"Jason Dixon\" notifications@github.com\n\u043d\u0430\u043f\u0438\u0441\u0430\u043b:\nMerged #923 https://github.com/graphite-project/graphite-web/pull/923.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/923#event-159773862\n.\n. Hi @obfuscurity,\nsorry, I didn't confirm that. Just checked my backport of #589 to 0.9.x on synthesize and found out that cache works fine.\nDEFAULT_CACHE_DURATION=10:\nInitial request:\nSun Sep 07 16:35:48 2014 :: Request-Cache miss [1ea283ba3983b76b81fb1fcf55202eff]\nReload graph once, after 4 seconds - hit\nSun Sep 07 16:35:52 2014 :: Request-Cache hit [1ea283ba3983b76b81fb1fcf55202eff]\nSecond reload, 4 more seconds - hit\nSun Sep 07 16:35:56 2014 :: Request-Cache hit [1ea283ba3983b76b81fb1fcf55202eff]\nThird reload, 11 sec > 10 sec = miss\nSun Sep 07 16:35:59 2014 :: Request-Cache miss [1ea283ba3983b76b81fb1fcf55202eff]\n\nIncreasing DEFAULT_CACHE_DURATION up to 120:\nInitial request - \nSun Sep 07 16:37:33 2014 :: Request-Cache miss [1ea283ba3983b76b81fb1fcf55202eff]\nReloading graph many times - \nSun Sep 07 16:37:38 2014 :: Request-Cache hit [1ea283ba3983b76b81fb1fcf55202eff]\nSun Sep 07 16:37:40 2014 :: Request-Cache hit [1ea283ba3983b76b81fb1fcf55202eff]\nSun Sep 07 16:37:42 2014 :: Request-Cache hit [1ea283ba3983b76b81fb1fcf55202eff]\nSun Sep 07 16:38:03 2014 :: Request-Cache hit [1ea283ba3983b76b81fb1fcf55202eff]\nSun Sep 07 16:38:27 2014 :: Request-Cache hit [1ea283ba3983b76b81fb1fcf55202eff]\nSun Sep 07 16:38:50 2014 :: Request-Cache hit [1ea283ba3983b76b81fb1fcf55202eff]\n103 seconds after - still hit \nSun Sep 07 16:39:16 2014 :: Request-Cache hit [1ea283ba3983b76b81fb1fcf55202eff]\n131 sec gone - miss as expected.\nSun Sep 07 16:39:44 2014 :: Request-Cache miss [1ea283ba3983b76b81fb1fcf55202eff]\nJust check with 'python manage.py diffsettings' that your settings was really changed, and if not - try to remove local_settings.pyc or recompile that with 'python -m py_compile local_settings.py' - by default sysnthesize not allows apache to overwrite root-owned files in /opt/graphite/webapp/graphite - maybe that was the reason of problems.\n. Done - #939 and #940 \n. For 0.9.x we even have no testing code for make requests - as far as I understand. Will try to do the same for master and port proper test from graphite-api then.\nThanks, @brutasse for review BTW!\n. Yep, it seems tests are helpful - this is really doesn't work, checking.\n. Tests passed, ready to merge! :smiley_cat: \n. D-oh, my fault. Nice catch @bmhatfield, thanks @brutasse for fix!\n. I ran test using tox, it works fine. Trying to do the same for 0.9.x, still struggling.\n. Maybe it silently builds? Just fixed corresponding 0.9.x patch - passed tests FINALLY. So, both PRs are ready to merge.\n. Oops, my fault, forgot to fix that on master.\n. ..since we are require Django 1.4 for 0.9.x anyway.\n. I'm not a guru for Graphite internals but IIUC after this patch graphite-web will ask carbon cache only for lowest resolution values which IMO instantly kill even slightly loaded instance... \nAnd rational behind that patch still not clear to me. Roman, could you please explain bug more? Maybe it can be fixed in less drastic way... \n. I asked Roman for bug explanation and make some tests on my testing cluster (0.9.x but code is question is the same there). Problem lies in these lines:\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/readers.py#L174-L181\n(for 0.9.x it is mergeResults function in datalib.py).\nSometimes, values from Carbon-cache really becomes last value after merging, and AFAIU in that case that last value can be from \"raw\", un-aggregated data from client. It looks very similar to  https://github.com/graphite-project/graphite-web/issues/832\n@penpen, are you using memcaches for caching?\nAnyway, @penpen's solution for this bug is maybe relevant for hicount() function and his usecase but unfortunately will kill Graphite performance for most users.\n. Sorry, have no idea how to fix that in normal way in graphite-web. :(\nMaybe we need to aggregate data when it comes in carbon-cache (in RAM, not on disk - they're aggregating now only in whisper, right)?\n@jssjr ? \n. Sorry, have no idea how to fix that in normal way in graphite-web. :(\nMaybe we need to aggregate data when it comes in carbon-cache (in RAM, not on disk - they're aggregating now only in whisper, right)?\n@jssjr ? \n. @penpen - at least cache merging code present there for years and it's in use for many users. Your patch will change that behaviour significantly. Fresh cache data is not \"wrong\", it \"could be wrong\", and I think in many cases is better to have that data instead of Nones or stale data.\nBut maybe I'm wrong, we need to discuss that.\nAnyway - IMO you also can make some config parameter in settings to make this patch configurable, disabled by default. \nFor https://github.com/graphite-project/graphite-web/issues/832 - it's still not clear what is cause or solution for that bug. For me it looks like very similar.\n. OK, problem is clear. But how we need to fix that? @penpen's patch fixes that problem but will break caching, which is not very good. I personally will prefer to have last wrong value than break caching and I suspect I'm not alone. At least we need some trigger to enable/disable fix.\n. Hi,\nThat's my whole point - it's not 'slightly stale data vs invalid one' - it's 'invalid-value-sometimes-if-you-are-using-sum-aggregation-only vs not-working graphite'. E.g. in my case turning off cache for lowest resolution means turning off cache almost completely which kills my graphite instantly - and I do not really care that latest value could be wrong sometimes - graphs looks OK and our scripts will ignore last value anyway.\nMaybe we really need to fix that, but not with turning cache off for lowest retention.\n. Hm. It seems you are right, @obfuscurity !\nI need to apologize to @penpen and @LukaszStrzelecki then - sorry, Roman and Lukasz, I completely misread your patch! :-1: \nBut I have a couple of other questions then.\n1. Do we need same for Ceres fetcher too? (it's in same file)\nBut you can merge it and I can add Ceres later.\n2. If I understand correctly 0.9.x code is different but it will be equivalent - during merge it will pick only values which match to interval -  https://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/render/datalib.py#L462-L482 (despite last value, which was tried to adress at https://github.com/graphite-project/graphite-web/issues/832)\nAm I right? Because for 0.9.x we also have some issues with consistency, like https://github.com/graphite-project/graphite-web/issues/812 - but maybe not related with that thing. \n. Checked with synthesize, looks good to me.\n@obfuscurity @brutasse ? Could you please check ?\n. :+1: \n. Tried that on our 0.9.12 w/patches - works great. If you are using clustering - must have, render time drops very significantly (if you are using globs - up to 2-3 times).\nJust replaced debug print stanzas with log.metric_access.\n. @bmhatfield, do you have some additional patches for that PR? Or can I try to port it to master?\n. How about to merge it?\nIn clustering environments it gives 2-3 times (!) faster response than previous code - I think it's a huge improvement and it's good to have it in 0.9.x and 0.9.13 release.\nStill want to wrap up my mind around master to port it and QUERY_BULK btw.\n@obfuscurity? @brutasse? @jssr?\n. How about to merge it?\nIn clustering environments it gives 2-3 times (!) faster response than previous code - I think it's a huge improvement and it's good to have it in 0.9.x and 0.9.13 release.\nStill want to wrap up my mind around master to port it and QUERY_BULK btw.\n@obfuscurity? @brutasse? @jssr?\n. Hello @lamont,\nThere was a bug with TZ processing, maybe you need to apply that commit - https://github.com/graphite-project/graphite-web/commit/8f41a2d899c29b882d37e38aae6446ab5e5956c2 ? \n. You're welcome,  but it was fix from @brutasse, not from me. :) \n. 0.9.x has similar patch already applied btw - https://github.com/graphite-project/graphite-web/pull/1270\n. @gwaldo, just created #1061 - is that enough? If not - yw to create another one. :smile_cat: \n. Simple cherry-pick - https://github.com/graphite-project/graphite-web/pull/1062\n. @billowqiu - yes, that will also work, but please note that development server is for development only, and not for production load.\n. @lamont, pity to hear that. Unfortunately I have REPLICATION_FACTOR=1 and can't test it - but maybe @bmhatfield has one?\n. @bmhatfield - or maybe we need to web bulk-fetch configurable like QUERY_BULK with default = false ?\n. Cool, guys, it was really impressive debugging session!\nBtw, @lamont, maybe it's a good idea to close issue then?\n. @lamont, any progress with testing? I'm still wondering why shared memcache killing your queries...\nDid you try local memcache (i.e. on localhost) instead of shared one?\n. :+1: for #1026 too\n. There's no bug - timeShift() just shift data backwards for specific number of seconds - and it's not a problem of timeShift if your clock was artificially skewed. All our graphs with timeShift(-1w) had this '1 hour diff' for whole week after switchover (which was in Europe at October 26).\nI do not think that we need to over-complicate timeShift() to mitigate that.\n. Hi @philbooth,\nwhat is your storage schema? (cat /opt/graphite/conf/storage-schemas.conf) ?\n. @philbooth \nthanks, and storage-aggregation.conf please - forgot to ask.\n. @philbooth, thanks.\nAs you can see you have \n[count]\npattern = \\.count$\nxFilesFactor = 0\naggregationMethod = sum\nSo graphite writes new value for \"blah.blah.count\" metric into wsp file into first archive (archive 0) and when you request value for last 720 minutes (12 hours in storage schema) graphite will get it from this unaggregated archive.\nBut second retention is \"1min:7d\", so - every 60 sec - 6 metrics from Archive 0 - Graphite will aggregate metrics and put them in Archive 1. But your aggregationMethod for count is sum(), so it will summarize that 6 metrics for every minute. That's exactly what you can see on your graph - 150 = 25 * 6\n. Maybe my explanation is not very good, sorry. Please check this thread on stack exchange then - https://stackoverflow.com/questions/17474505/getting-accurate-graphite-stats-counts\n. You're welcome! Just do not forget to recreate wsp files after changing aggregation.\n. Just tested this PR, works fine, I think we still need to merge it before 0.9.13 release. It's quite complimentary to #1010 too. @jraby's approach is nice too, maybe we need to port it to master though...\n. I'm suspecting that master is already too much diverged. At least fetch\ncode logic is completely different from 0.9.x\nI tried to port carbon query bulk and failed.\nMy point that 0.9.13 is last release in 0.9.x, right? After 0.9.13 all\ndevelopment will be switched to master and 0.9.x will be frozen and all\nvaluable PRs there will be frozen too. So, why not let users of current\nproduction to get speed-up improvement now and not\n\"i-do-not-know-how-many-month-later-after-first-1.0.0-release\"?\nIMO current \" branch split\" situation is really poisonous for development\nprocess.\n8:32, \u043f\u043d, 08.12.2014, Bruno Reni\u00e9 notifications@github.com:\n\nIf this gets merged in 0.9.x it really needs to go to master too. Usually\nthe workflow is to commit in master first then backport to 0.9.x. It's\neasier to ensure branches don't diverge too much with this process.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1026#issuecomment-66033224\n.\n. Correspodent fix for 0.9.x is not merged yet - https://github.com/graphite-project/graphite-web/pull/1034\nbecause tests are failing. Will try to check / fix that this weekend.\n. No, my bad - completely forgot about it from Nov. 2014 ;)\n. We didn't using events (still good old drawAsInfinite) but if no-one picks that I'll did that. :)\n. Yep, or this or #1480 - both looks fine. Pick one, ditch another, merge. \ud83d\udc4d \n. Something definitely wrong with Github or my hands today....\n. Hi @brutasse, thanks, just fixed!\nTests are green now, but Travis says:\n\"test_timezone_handling (tests.test_events.EventTest) ... /home/travis/virtualenv/python2.6.9/lib/python2.6/site-packages/django/db/models/fields/init.py:808: RuntimeWarning: DateTimeField received a naive datetime (1970-01-01 00:00:00) while time zone support is active.\n  RuntimeWarning)\n/home/travis/virtualenv/python2.6.9/lib/python2.6/site-packages/django/db/models/fields/init.py:808: RuntimeWarning: DateTimeField received a naive datetime (2014-11-16 22:04:02.948218) while time zone support is active.\n  RuntimeWarning)\n/home/travis/virtualenv/python2.6.9/lib/python2.6/site-packages/django/db/models/fields/init.py:808: RuntimeWarning: DateTimeField received a naive datetime (2014-11-16 22:04:02.951622) while time zone support is active.\n  RuntimeWarning)\n/home/travis/virtualenv/python2.6.9/lib/python2.6/site-packages/django/db/models/fields/init.py:808: RuntimeWarning: DateTimeField received a naive datetime (2014-11-16 22:04:02.956400) while time zone support is active.\n  RuntimeWarning)\nok\"\nIs it OK?\n. Ah, I suspected that's not OK :) Ok, I'll try to find out what's happening.\n. It's nice to have this fix before 0.9.13, will add it to milestone and try to fix tests.\n. BTW - I tried to find why tests generate some warnings but failed - same code on master works fine, but generate workings on 0.9.x.\nBut looks like commit works and fixes issue anyway.\n. Hi @fessyfoo,\nOne of our cluster is exactly the same as your - one-layer graphite-web with shared memcached and I see no troubles. After implementing bulk web queries speed of rendering was dramatically increased, and I got many kudos from users.\nSnippets from current config:\n\nMEMCACHE_HOSTS = [\"graph001:11211\",\"graph002:11211\"]\nCLUSTER_SERVERS = [\"graph001:80\",\"graph002:80\"]\nCARBONLINK_HOSTS = [\"127.0.0.1:7102\", \"127.0.0.2:7202\", \"127.0.0.3:7302\", \"127.0.0.4:7402\"]\nDid you tried to disable memcache caching?\n. Hi @fessyfoo,\nOne of our cluster is exactly the same as your - one-layer graphite-web with shared memcached and I see no troubles. After implementing bulk web queries speed of rendering was dramatically increased, and I got many kudos from users.\nSnippets from current config:\nMEMCACHE_HOSTS = [\"graph001:11211\",\"graph002:11211\"]\nCLUSTER_SERVERS = [\"graph001:80\",\"graph002:80\"]\nCARBONLINK_HOSTS = [\"127.0.0.1:7102\", \"127.0.0.2:7202\", \"127.0.0.3:7302\", \"127.0.0.4:7402\"]\nDid you tried to disable memcache caching?\n. @fessyfoo, did you tried not remove memcache but split it per node? I.e. using \nMEMCACHE_HOSTS = ['host1:11211'] for host1\nMEMCACHE_HOSTS = ['host2:11211'] for host2\nI'm doubt that shared memcache will give significant performance gain comparable to dedicated. \n. Ah, stupid me, disregard my last message, sorry. If you need to return only local results from node you need to use localOnly=true, as described in API docs - http://graphite.readthedocs.org/en/latest/render_api.html\n\"localOnly\nDefault: False\nSet to prevent fetching from remote Graphite servers, only returning metrics which are accessible locally\".\nAll other requests COULD return non-local metrics, noCache=true is not enough.\n. Hello @JarleB,\nProblem here lies not in size of metrics, but in metrics itself - you are using globs in your metrics and rendering of metrics with globs is not really optimized in current graphite.\nWhich version are you using? You can try latest 0.9.x branch of graphite-web which contains @bmhatfield's patch (https://github.com/graphite-project/graphite-web/pull/1010) for bulk glob rendering. You also can try to test parallel improvement of this patch in https://github.com/graphite-project/graphite-web/pull/1026 or even try to use @datacratic branch - https://github.com/graphite-project/graphite-web/issues/1045\nFor me #1010 + #1026 works quite fine.\n. #1010 is already merged into 0.9.x btw\n10:02, \u043f\u043d, 22.12.2014, JarleB notifications@github.com:\n\nThanks for the reply @deniszh https://github.com/deniszh. My wording\nwas perhaps not correct, but yes I'm aware that the issue becomes evident\nbecause of the number of metrics returned by the glob and not the size of\neach metric. Seems #1010\nhttps://github.com/graphite-project/graphite-web/pull/1010 is\naddressing this in particular. Will check it out.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1058#issuecomment-67815279\n.\n. Do we need to port it into master? I mean is pytz requred for master too? @brutasse ?\n. :+1: \n. Maybe, need to test that. \n. Looks like @cbowman0 is right. Master has megacarbon branch merged, and has async fetcher, for find and read requests. Not sure which way is works better, though - but IMO that can be enough for now. \n. I think we can close it\n\nOn Mon, 20 Mar 2017 at 07:22, Corentin Chary notifications@github.com\nwrote:\n\nIs this still an issue with the latest HEAD ?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1063#issuecomment-287685252,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51nEPQeOI6TsJwo2_Ou8UiByPu2XCks5rnhsYgaJpZM4DLSw3\n.\n. @SEJeff, could you please also merge same PR for 0.9.x branch - https://github.com/graphite-project/graphite-web/pull/1067 ?\nThanks!\n. :+1: for having a switch :)\nwo 31 dec. 2014 \u0432 18:23, Dave Rawks notifications@github.com:\nI don't think it is really our place to dictate how people architect their\ninstallation. FWIW I think almost the opposite change should be made. The\n\"detection of loopbacks\" was made to enable reuse of generic config right?\nWhich is an IMHO \"magical behavior\" which depends on a rather dubious\nheuristic for determining whether the IP is \"local\" or not.\nlocal_ip = sock.getsockname()[0]\n....\nif local_ip == host:\n    return True\nIt is exactly the type of architectures that @jraby\nhttps://github.com/jraby is describing that I recall having cited in my\noriginal objection to this behavior. That said I'm against this commit and\nwould rather there just a be a switch around filtering with\n\"is_local_interface\" when building self.remote_stores in the Store class.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1069#issuecomment-68455793\n.\n. IMO this change needs to be optional, configurable by local_settings.py and\nnot enabled by default. I never faced any problems with GET and prefer to\nstick to it as it much easier to log and debug.\nJust my 5 cents.\nwo 31 dec. 2014 \u0432 01:39, steve-dave notifications@github.com:\n@obfuscurity https://github.com/obfuscurity another small, high value\nchange. Do you think it could be in 0.9.13?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1071#issuecomment-68414431\n.\n. Didn't test it on our clusters (because of vacations and production freeze), but looks perfectly legit to merge (and complement to #1026). :+1: \n. Hi @jraby,\nI'm little bit confused - we already have #1010 implemented, so -\n1) why your patch is better ?\n2) is it OK to apply your patch over #1010 - I mean is it have any sense ?\n. And I think it will be good to add PREFETCH_REMOTE_DATA and explanation to local_settings.py too\n. Ah, stupid me. Thanks, @jraby, for explanation. Looks like good addition to latest changes then.\n. Just tested manual on fresh Ubuntu 12.04 and 14.04 VMs, looks good! :+1: \nBTW, requirements.txt for graphite-web in 0.9.x contains \n\ngit+git://github.com/graphite-project/whisper.git@0.9.x#egg=whisper\nDo we need to change it to\ngit+git://github.com/graphite-project/whisper.git@0.9.13#egg=whisper\nlater?\n. So, what's preffered fix? pinnin pip to <= 1.4.x (not sure how it will work when we have system pip version >= 1.5) or just documenting \"--no-use-wheel\" (IMO better for now) ?\n@SEJeff ? @obfuscurity ? @dstufft ?\n. :+1: \nUseful note.\n. Oops, wrong project :-1:\n. It was simple cherry-pick anyway, np. :+1: \n. I think we need to include that warning in release notes for 0.9.13, I'l\nmake a PR\nI'm also wondering why local memcached is not working. @bmhatfield, what do\nyou think?\n6:59, \u0432\u0442, 13.01.2015, Jason Dixon notifications@github.com:\n\nClosed #1102\nhttps://github.com/graphite-project/graphite-web/issues/1102.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1102#event-217880987\n.\n. @bmhatfield - as far as I understand @arma26 have same problems as @lamont in https://github.com/graphite-project/graphite-web/issues/1022. I'm also see no difference between enabling and disabling memcache, but if it breaks something after upgrade to 0.9.13 we need to warn users then...\n@obfuscurity - I was quite sure that we need to disable shared memcached and single memcached instance works fine. But if not - it's also OK, we just need to include something like \"Change in clustering: if you are using clustering graphite with REPLICATION_FACTOR >= 2 you need to disable memcache caching\" in release notes.\nIs it good idea to do so? I think it will spare some questions and github issues after release...\n. Sorry, but why? IMO graphite is quite slow by itself :)\nLooks like you trying to solve some other problem, which one?\n. @urtzurd:\nLooks like same issue,  but it was fixed in master and 0.9.x branch from end of February.\nWhich version are you using - latest master from github?\n. Hello @steve-dave,\nSorry for delay! We usually agree that default for 0.9.x should not change existing behaviour.\nCould you please rebase this change? Much appreciated!. Hi Lukasz,\n\nIn same situation I configured single carbon instance on rendering server just for that purpose. \n. If I remember correctly this error happening if mod_wsgi and python versions are different (e.g 2.6 vs 2.7 or 2.7 vs 3.x). Strange, it seems my tree was on some strange state... 67 commits and only 2 are mine.\n. @bmhatfield - Looks what I found!\n. Blergh... will create clear one.\n. @bmhatfield , please check that. :)\n. Added to 0.9.13 milestone. It's a nice fix, without it you got no errors in logs when your remote fetch queries starts timeouting. I spent couple of hours debugging this. :)\n. Hello Tony,\nIt's not for master, it's for 0.9.x. It's included in milestone, so, I\nhope, it will be included in upcoming release.\nOn Tue 15 Sep 2015 at 08:30 Toni Moreno notifications@github.com wrote:\n\nHi guys.\nThere is any plans to merge this PR in master ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1208#issuecomment-140293001\n.\n. Thanks, Jason, for merging this!\n. @brutasse , @obfuscurity - please check, thanks!\n. @esc: Very good question, indeed. I just found that I do not know. Synthesize working properly on Django 1.4-1.7 and there's no need to setup /static alias at all.... :confused:\n. @obfuscurity: yep, that's good in general, but not in that specific case.\nBTW latest Django 1.8 doesn't work with synthesize, gives strange error, will try to find out what's going on.\n. According to https://github.com/graphite-project/graphite-web/issues/1252#issuecomment-113413285 and whitenoise docs it doesn't work with python 2.6 and 0.9.13 must working with it.\nMaybe we need to import whitenoise only if python version is 2.7 ?\n. Add that change and squashed commits.\n. @SEJeff - will change it, thanks!\n. Is anyone has idea why Travis is b0rking on that change for all Django's except 1.8 ?\n. Ah, screw it. Will make another commit, this one is too distant from 0.9.x\n. Really wondering if it's still needed after #1279 ...\n. So, then we need to merge it. \nIt looks good and have tests, and it's last issue in next release's milestone for graphite-web, yay!\n@obfuscurity ? @jssjr ?\n. :+1: \n\n. Unfortunately, this patch is for master and I really doubt that it could be cherry-picked to 0.9.x - need to be ported.\n. @gsaray101, it's hard to set up something from source on unsupported OS. Your problem is not Graphite related, it's OS' problem. It seems you need to install Graphite from scratch on another server, maybe virtual one. There's ton of manuals out there for installing Graphite, you just need to have working OS first. \n. :+1: for that @brutasse said.\n. Yes, Django 1.8 - 1.11 are supported in current codebase.. Maybe removing whitenoise (\"pip uninstall whitenoise\") module helps? \n. Maybe we need to import whitenoise only if python version is 2.7 ?\n. @toni-moreno , please try replace wsgi.py with this version - https://gist.github.com/deniszh/8863e5412df438150a48\n. yes\n. Yes, will make PR then.\n. https://github.com/graphite-project/graphite-web/pull/1254\n. @evansd - in perfect world - yes, but in reality we need to extend check logic IMO:\nif python == 2.6 and whitenoise >= 2.0.1 OR python == 2.7...\n. @evansd - yep. But thanks for support of python 2.6 anyway - it will make life happier :+1: :)\n. @evansd - maybe you can advise me do we have a way how we can check whitenoise version?\nIf I understand correctly - whitenoise.version will not work, right?\n. Hello @jstangroome,\nCould you please include some documentation for this feature, e.g. from https://github.com/graphite-project/graphite-web/issues/1268 ?. Hello @jstangroome,\nCould you please include some documentation for this feature, e.g. from https://github.com/graphite-project/graphite-web/issues/1268 ?. @DanCech - if @jstangroome still interested in this feature we can port it to master too.\nBut I just found out that this thing separates only whisper dirs, with disabled Carbonlink. Not sure if viable solution.... Thanks for explanation, @jstangroome !\nIMO that's a nice feature to have. But we have 2 issues:\n1) Lack of documentation. I think you need to add your explanation how it works and warning for indirect Carbonlnk support. You can add new page under https://graphite.readthedocs.io/en/latest/config-webapp.html and add link from local_settings.py explanation\n2) We can add this feature to 0.9.x, but it will be abandoned very soon, and next version (1.0.0) will be derived from master very soon. So, if you want to make it part of Graphite you need to port it to master branch. Is that OK?. I'm \ud83d\udc4d for merging this if no objections.\n\n@jstangroome: Thank you for your contribution, but please do not forget to port that change to master, otherwise, it will be gone soon.\nI created a separate issue #1869  about 0.9.16 release.. Hi @sergedu,\n0.10-alpha is version of master branch, and WEBAPP_VERSION = '0.9.13' is already fixed in 0.9.x branch - https://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/settings.py\n0.9.13 will be released soon :tm: from that branch.\n. It was not bumped when 0.9.13-rc1 was packed. Now it's bumped, (so, no need for PR), and WEBAPP_VERSION = '0.9.13' will be present in 0.9.13 when it will be finally released soon. :tm:\n. Tried both calls with corresponding Carbon patch - LGTM, both works. :+1: \n. @llicour - I'm using this functionality internally anyway... Will you mind if I port it to master ?\n. Are you using shared memcached? It's known issue then. Please switch to local memcached instance for every node. \n. Adding empty list to cache is harmless btw, just turn off shared memcached. \n. Not sure if that's enough to fix shared memcached problem. IMO PR #1212 is more general fix for this. \n. I also faced different issues with shared memcached and replication factor = 1. My point is that #1212 will add hostname to caching key, so, it will mitigate this issue too, probably. But it's just my hypothesis, didn't test that yet. Maybe I'm wrong and your patch is needed too. \n. Tried your patch on test environment, works fine, not sure if helps with shared memcached - still didn't enable that in my env.\n. This problem was fixed in master but not in 0.9.x yet. Please check https://github.com/graphite-project/graphite-web/issues/1028, I'll try to fix that this weekend.\n. Hi @franklupo,\nIf you mean https://github.com/graphite-project/graphite-web/pull/1435 - no, no news. Somebody need to work on this PR, there's a ton of corrections but the author (@cosm0s) have no time to work on it. Also, code probably is quite old and need to rebased/adapted for modern codease now.. > I believe it already works with the standard finder.\nUnfortunately, only single braces. I still need to spend 10 minutes to forward port https://github.com/graphite-project/graphite-web/pull/1531 to master.\n. IMO it's a very nice feature for Graphite clustering and it's quite good to have test. Will try to create one tomorrow, to include that in release.\n. @obfuscurity - I'll try to. My knowlege of master is still scarce. :imp: \n. IIRC current bulk-fetch implementation done mostly by @bmhatfield with some improvements by @jraby.\nWe can open issue for porting that to master first.\n. I found #1010 master-port issue BTW - https://github.com/graphite-project/graphite-web/issues/1063\n. I'll try to test it on my cluster for weekend. If it works - maybe we can create separate test for that, just for inclusion in upcoming 0.9.14 ? \nWe only need to test data merge itself, right - and not any other functionality here - other things are already merged and works.\n. Tested that on simple 2 node cluster. My lowest retention is 60 seconds.\n1. Creating local.random.diceroll2 metric on server1 - with values 1..5:\nshell\n$ for x in 1 2 3 4 5; do echo \"local.random.diceroll2 ${x} `date +%s`\" | nc -q0 127.0.0.1 2103; sleep 60; done\nAfter finishing that script run similar on node 2\n2. Creating local.random.diceroll2 metric on server2 with values 6..10:\nshell\n$ for x in 1 2 3 4 5; do echo \"local.random.diceroll2 ${x} `date +%s`\" | nc -q0 127.0.0.1 2103; sleep 60; done\n1. Checking Graph in Grafana:\n   \n   As you can see graph is perfectly merged.\n2. Dumping individual values:\n   Server1:\nshell\n$ whisper-dump.py /opt/graphite/storage/whisper/local/random/diceroll2.wsp | grep -A 10 'Archive 0 data'\nArchive 0 data:\n0: 1444580340,          1\n1: 1444580400,          2\n2: 1444580460,          3\n3: 1444580520,          4\n4: 1444580580,          5\n5: 0,          0\n6: 0,          0\n7: 0,          0\n8: 0,          0\n9: 0,          0\nServer2:\nshell\n$ whisper-dump.py /opt/graphite/storage/whisper/local/random/diceroll2.wsp | grep -A 10 'Archive 0 data'\nArchive 0 data:\n0: 1444580640,          6\n1: 1444580700,          7\n2: 1444580760,          8\n3: 1444580820,          9\n4: 1444580880,         10\n5: 0,          0\n6: 0,          0\n7: 0,          0\n8: 0,          0\n9: 0,          0\nI'm advocating for merging it before 0.9.14 release. It's very nice addition to have for graphite clustering, but not very easy to make test for that.\n@obfuscurity @bmhatfield @jssjr @mleinart @SEJeff @drawks ?\n(sorry in advance for poking you, guys :) )\n. Hm. I can do that thing optional, as I did for cabon's https://github.com/graphite-project/carbon/pull/135, for example. Still thinking that's need to be enabled by default, but it's nice to have a trigger to turn that off.\n. Done, will test tomorrow.\nAlthough it's lot looks more clear now... :imp: \n. Closing in the favor of https://github.com/graphite-project/graphite-web/pull/1352\n. @SEJeff - rebased https://github.com/graphite-project/graphite-web/pull/1209, could you please check?\n. Latest whitenoise supports 2.6, it's just more, like, precaution. IMO will help in some cases.\n:+1:\nThanks!\n. :+1: \nThanks, @evansd !\n. Backported that solution to 0.9.x in https://github.com/graphite-project/graphite-web/pull/1295\n. No, I backported it only to 0.9.x, master branch needs to be fixed with this PR. Thanks, @evansd !\n. :+1:\nDidn't have such big queries, but will try this patch tomorrow. Thanks! \n. Yep,  it's better to add this commit to #1293 and close this one. \n. :+1:\nIIRC Python 2.6 is official requirement for 0.9.13 for long time ago.\nBut I'm wondering is this patch obsoletes my fix in https://github.com/graphite-project/graphite-web/pull/1208 or not...\n. If I understand correctly my fix from https://github.com/graphite-project/graphite-web/pull/1208 also helps with that issue.\nAnd still not understand why you remove timeout from https://github.com/jjneely/graphite-web/blob/0.9.x-timeouts-buffers/webapp/graphite/render/datalib.py#L351 ... Timeout in thread will not raise exception, you need to check fetch_thread.is_alive() explicitly - thats whole point of https://github.com/graphite-project/graphite-web/pull/1208. Maybe we need to merge your fix with mine.\n. Maybe we can create some wrapper or check for fallback on 2.6 ? \n2.6 is still official requirement for 0.9.x branch\n. I'll try to do that today.\n. Just create https://github.com/graphite-project/graphite-web/pull/1330, please review - @jjneely @obfuscurity \nThanks!\n. Agreed, will make PR. Thanks!. Yep, your'e right - 'carbon' metrics not cached at all (not sure if because of wrong CARBONLINK request or because of no cache request even being made) and it's not fixed yet.\n. Hi @rlueckl,\nIf \"latest metrics would always be available on the disk and graphite would show them 'up-to-date'\" - why we need carbonlink and cache then?\nNo, that's a problem exactly - metrics on disk are delayed, because disk is slow, and carbon daemon flushes its cache periodically. In my case it finish flush cycle for 30-40 minutes, so carbon metrics for last hour are inconsistent. If I turn off cache completely - all my metrics will be delayed.\n. And BTW problem lies somewhere in carbon daemon - graphite-web doesn't know nothing about CARBON_METRIC_PREFIX before https://github.com/graphite-project/graphite-web/commit/48bbfbe073df7852625b9462907ac56f9d65a297 \n. As I found just now this commit was never backported to 0.9.x branch, so, it's not exist in 0.9.13 yet. \nBut I have this patch backported to my own branch and I still have same problems as you describe - so, I do not think that's solution for that problem (or maybe I backported it wrong). Real problem lies somewhere in carbon daemon, and I'm almost migrated to https://github.com/lomik/go-carbon which doesn't have this problem...\n. PS: I migrated not because of this issue - it seems that issue or not relevant for majority of users (and nobody cares) or maybe code in master branch really fixes that...\n. We finally dropped python 2.6 support from the master. So, now it can be much simplified.\n. LGTM!\nWill put it to our testing cluster and check how it affects performance.\n. Just did that - works fine, no signs of performance problems and graphs still looks sane. :smiling_imp: \n. :+1: \n. Just hit the same thing, testing my PRs on Synthesize. :)\n.  Works now, thanks!\n. Not from my side, LGTM!\n\n. Yay, @brutasse is right. \n. Master looks much cleaner then 0.9.x...\n:+1: \n. I was glad to help but it's not directly Graphite-related question, can be easilly googled - http://bfy.tw/29Kp \n\"Use the -u flag to include a username, and curl will prompt for a password:\ncurl -u username http://example.com\nYou can also include the password in the command, but then your password will be visible in bash history:\ncurl -u username:password http://example.com\"\n. Tested that on simple 2 node cluster - works now:\n1. Creating local.random.diceroll3 metric on server1 - with values 1..5:\nshell\ndzhdanov@graph001:/$ for x in 1 2 3 4 5; do echo \"local.random.diceroll3 ${x} `date +%s`\" | nc -q0 127.0.0.1 2103; sleep 60; done\ndzhdanov@graph001:/$ whisper-dump.py /opt/graphite/storage/whisper/local/random/diceroll3.wsp | grep -A 10 'Archive 0 data'\nArchive 0 data:\n0: 1444641360,          1\n1: 1444641420,          2\n2: 1444641480,          3\n3: 1444641540,          4\n4: 1444641600,          5\n5: 0,          0\n6: 0,          0\n7: 0,          0\n8: 0,          0\n9: 0,          0\nAfter finishing that script run similar on node 2\n2. Creating local.random.diceroll3 metric on server2 with values 6..10:\nshell\ndzhdanov@graph002:~$ for x in 6 7 8 9 10; do echo \"local.random.diceroll3 ${x} `date +%s`\" | nc -q0 127.0.0.1 2103; sleep 60; done\ndzhdanov@graph002:~$ whisper-dump.py /opt/graphite/storage/whisper/local/random/diceroll3.wsp | grep -A 10 'Archive 0 data'\nArchive 0 data:\n0: 1444641840,          6\n1: 1444641900,          7\n2: 1444641960,          8\n3: 1444642020,          9\n4: 1444642080,         10\n5: 0,          0\n6: 0,          0\n7: 0,          0\n8: 0,          0\n9: 0,          0\n1. Applying patch with MERGE_TIMESERIES=True\n2. Checking Graph in Grafana:\n   \n3. Switching to MERGE_TIMESERIES=False on both nodes:\n   \nPlease check and review - @jjneely @bmhatfield @obfuscurity @brutasse @SEJeff ?\n. PS: will fix release notes if we'll merge this\n. Hi Jason,\nWill change variable now, but it's not very clear what I need to do with @jjneely commits?\nI had no intention to hijack his contribution, I just trying to make this PR introduced in 0.9.14 for broad audience but TBH I had no idea how I can extract his commits from his PR and put it to another PR to apply my changes... :(\n. Ah, will try that now. Tried to do the same through patches, but 2nd patch failing for some reason...\n. Abandoning that in favour of https://github.com/graphite-project/graphite-web/pull/1352\n. 0.9.14 ftw!\n. Tested that on simple 2 node cluster - works now:\n1. Creating local.random.diceroll3 metric on server1 - with values 1..5:\nshell\ndzhdanov@graph001:/$ for x in 1 2 3 4 5; do echo \"local.random.diceroll3 ${x} `date +%s`\" | nc -q0 127.0.0.1 2103; sleep 60; done\ndzhdanov@graph001:/$ whisper-dump.py /opt/graphite/storage/whisper/local/random/diceroll3.wsp | grep -A 10 'Archive 0 data'\nArchive 0 data:\n0: 1444641360,          1\n1: 1444641420,          2\n2: 1444641480,          3\n3: 1444641540,          4\n4: 1444641600,          5\n5: 0,          0\n6: 0,          0\n7: 0,          0\n8: 0,          0\n9: 0,          0\nAfter finishing that script run similar on node 2\n2. Creating local.random.diceroll3 metric on server2 with values 6..10:\nshell\ndzhdanov@graph002:~$ for x in 6 7 8 9 10; do echo \"local.random.diceroll3 ${x} `date +%s`\" | nc -q0 127.0.0.1 2103; sleep 60; done\ndzhdanov@graph002:~$ whisper-dump.py /opt/graphite/storage/whisper/local/random/diceroll3.wsp | grep -A 10 'Archive 0 data'\nArchive 0 data:\n0: 1444641840,          6\n1: 1444641900,          7\n2: 1444641960,          8\n3: 1444642020,          9\n4: 1444642080,         10\n5: 0,          0\n6: 0,          0\n7: 0,          0\n8: 0,          0\n9: 0,          0\n1. Applying patch with REMOTE_STORE_MERGE_RESULTS=True\n2. Checking Graph in Grafana:\n   \n3. Switching to REMOTE_STORE_MERGE_RESULTS=False on both nodes:\n   \nPlease check and review - @jjneely @bmhatfield @obfuscurity @brutasse @SEJeff ?\n. Yes. The completely different master code in \"data retrieval\" area. Still need to be ported there - but it will require more patches to port...\nSad but true.\n. @cout @wooparadog @redbaron - does someone want to make a PR? Or I can do that.\n. Hi @obfuscurity - please check.\n. IIRC /opt/graphite prefix unfortunately hardcoded and you can't easily change that. But maybe I'm wrong. \n. @sylr :\nYou can check example of boolean function parameters for example at hitCount() function.\nSo, it could be something like\npython\ndef sortByName(requestContext, seriesList, natural = False):\n  \"\"\"\n  Takes one metric or a wildcard seriesList.\n  Sorts the list of metrics by the metric name.\n  Natural sorting allows names containing numbers to be sorted more naturally, e.g:\n     Normal sorting: server1, server11, server12, server2\n     Natural sorting: server1, server2, server11, server12\n  \"\"\"\n  if natural:\n    def natName(name):\n       return re.sub(\"(\\d+)\", lambda x: \"{0:010}\".format(int(x.group(0))), name)\n    def compare(x,y):\n       return cmp(natName(x.name), natName(y.name))\n  else:\n    def compare(x,y):\n      return cmp(x.name, y.name)\n  seriesList.sort(compare)\n  return seriesList\nJust an example, it can be written more efficient and elegant ofc...\n. Doc build still failed:\nWarning, treated as error:\n/home/travis/build/graphite-project/graphite-web/webapp/graphite/render/functions.py:docstring of graphite.render.functions.sortByName:4: ERROR: Unexpected indentation. in https://travis-ci.org/graphite-project/graphite-web/jobs/86390850\n. https://readthedocs.org/projects/graphite/builds/3430383/ :dancers: :boom: \n. Wow! \n:+1:, much appreciated!\n. @obfuscurity @gwaldo @brutasse - could you please check - does it looks good and does it fix RTD build (merge/revert maybe) ?\n. Now looks better, @brutasse ?\n. Cool! We are ready to finally \"push the button\"\n\n. Similar issue - https://github.com/graphite-project/graphite-web/issues/1024\nNot much to add here, at least from my side.\n. Unfortunately no good solution exist for that bug currently - except running your servers in UTC (which is easy if you doing that from start, but quite hard if you need to migrate there).\nEven if we (over)complicate timeOffset() function to check DST boundaries it will work only when both (\"from\" and \"to\") boundaries is after or before switch time. \nI see no way how we can fix graph above - timeShift() value must be the same for all graph values, it can't be 10080m before DST moment, and 10020 / 10140 after, on same graph....\n. I think it's need to be rewritten like this to not bring any overhead where dstTweak=false. And maybe make it false by default....\n``` python\n  if dstTweak:\n    def localDST(dt):\n      return time.localtime(time.mktime(dt.timetuple())).tm_isdst\nreqStartDST = localDST(requestContext['startTime'])\nreqEndDST   = localDST(requestContext['endTime'])\nmyStartDST  = localDST(myContext['startTime'])\nmyEndDST    = localDST(myContext['endTime'])\n\ndstOffset = timedelta(hours=0)\n# If the requestContext is entirely in DST, and we are entirely NOT in DST\nif ((reqStartDST and reqEndDST) and (not myStartDST and not myEndDST)):\n    dstOffset = timedelta(hours=1)\n# Or if the requestContext is entirely NOT in DST, and we are entirely in DST\nelif ((not reqStartDST and not reqEndDST) and (myStartDST and myEndDST)):\n    dstOffset = timedelta(hours=-1)\n# Otherwise, we don't do anything, because it would be visually confusing\nmyContext['startTime'] += dstOffset\nmyContext['endTime'] += dstOffset\n\n``\n. Of course, why not? It's a nice addition and best fix of DST timeShift problem so far.\nPlease create PR for that - https://help.github.com/articles/creating-a-pull-request/\nThen project owners and collaborator can review and merge your patch easy.\n. How do you calculate \"graphite response time\" graph? I mean, what is source of these data?\n. GraphiteXX nodes also haveCLUSTER_SERVERS = [\"graphite02\", \"graphite03\", \"graphite05\" , \"graphite07\"]` in local_settings.py ?\n. Do you use Grafana only of PNG rendering too? In your setup only proxy node will run rendering, graphite nodes will run only pickles' rendering.\n. Then it's quite logical though - proxy need to ask all 4 nodes one by one to get all metrics first, then run all functions and give result to user - and graphite nodes only running simple \"gimme-that-metric-from-cache-or-disk\" task.\n0.9.14 has \"parallel fetch\" patch, still need to be ported to master. But in general I see no problem here - 'proxy' node is more (and differently) loaded than normal nodes. \n. @toni-moreno: it depends. I think only 50 times slower is even quite optimistic.\nUnfortunately master and 0.9.x is quite different, so, it's quite hard to port that functionality to master. That's a issue for tracking that - https://github.com/graphite-project/graphite-web/issues/1063\nAnd I'm not sure that I understand your note. You are using master already - how stopping of 0.9.x will affect you and \"make me difficult manage updates from a stable release\"?\n. > I also saw a comment where someone was using this: https://github.com/InMobi/level-tsd - is it worth including?\nYep, yesterday I saw question in Graphite-dev mailing list about it - it's a first time when I saw person using that. Maybe need to include to doc though...\n. Yep, fixed in  master and upcoming 0.9.14 too\n. Yep. It's quite simple to merge, even w/o testing. :+1:\nOn Sat, 7 Nov 2015 at 05:35, Jason Dixon notifications@github.com wrote:\n\n@deniszh https://github.com/deniszh look ok to you?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1384#issuecomment-154619115\n.\n. s/testing/test code/, sorry\nOn Sat, 7 Nov 2015 at 06:12, Denis Zhdanov denis.zhdanov@gmail.com wrote:\nYep. It's quite simple to merge, even w/o testing. :+1:\nOn Sat, 7 Nov 2015 at 05:35, Jason Dixon notifications@github.com wrote:\n\n@deniszh https://github.com/deniszh look ok to you?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1384#issuecomment-154619115\n.\n. Hmm... I just copied example from master branch, IIRC it's working there w/o problems...\nIs adding line above helps with running graphite successfully? \n. Yep, looks like master do not require installation to /opt/graphite, so, that's why it was excluded there. I can do PR but IIRC 0.9.14 is latest 0.9.x release... \nCan we release 0.9.14.1 or 0.9.15 (damn you, semver!) or ...?\n@obfuscurity @brutasse ? \n. We faced similar issue. I thought it was fixed in https://github.com/graphite-project/graphite-web/commit/ff091fb4c766858bd6dcb28501eb1109995881d3 - merging will be done only when cache resolution equal with whisper resolution.\n. Should be fixed in current master, please see https://github.com/graphite-project/graphite-web/issues/1156#issuecomment-144720336\n.  Nice catch, indeed! \n. WOW. Didn't check patch yet, just want to say that is look MASSIVE! Much appreciated!\nNot sure if I can help with testing... My testing skills are weak. :(\n. That's true, when server makes queries to CLUSTER_SERVERS it will always add \"local=1\" to query, to make local requests only, for avoiding loops. AFAIK it's mandatory and can't be changed. So, you need to include only servers with CARBONLINK_HOSTS enabled in CLUSTER_SERVERS.\nBut I'm still not really understand your setup and why you need \"recursive\" cluster requests... \n. Do you still have time to check it, @DanCech, or should I move to post-1.1 improvements?. Cool, closing this.\nThanks, @DanCech! \n@xneo64 - you can check the latest master - or wait to 1.1.0, which should be released soon.. :+1:\n\n\nOn Fri, 27 Nov 2015 at 19:06, Jason Dixon notifications@github.com wrote:\n\n\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/graphite-project/graphite-web/pull/1415\nCommit Summary\n- release notes for 0.9.15\nFile Changes\n- M docs/releases.rst\n  https://github.com/graphite-project/graphite-web/pull/1415/files#diff-0\n  (1)\n- A docs/releases/0_9_15.rst\n  https://github.com/graphite-project/graphite-web/pull/1415/files#diff-1\n  (74)\nPatch Links:\n- https://github.com/graphite-project/graphite-web/pull/1415.patch\n- https://github.com/graphite-project/graphite-web/pull/1415.diff\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1415.\n. An interesting feature, thanks, @wooparadog!\nNot sure about realisation too... The separate reader looks like bit hackish, indeed.\n\nTests are broken because of docs again... we need to find out why...\n. @wooparadog - you can also check these PRs:\nhttps://github.com/graphite-project/carbon/pull/419\nhttps://github.com/graphite-project/graphite-web/pull/1277\nBoth need to be ported to master, though - but I tested that on 0.9.x - at least metric flushing works.\n. Will try to look...\n. :+1: \nLooks like it's broken only in 1.9, which was released a week ago ...\n. Also, link section should be sorted alphabetically. Sorry for nit-picking. :)\n. Nah, just quiet time between holidays. :)\nBut Moira is a nice project, indeed - thanks for adding.\n. @JeanFred - are you gonna merge that or I ...?\n. s/1554/1454/g ?\n. As you can see in https://github.com/graphite-project/graphite-web/blob/fe04c9e64d53057877a25cbc78edd70dc8127759/webapp/graphite/util.py#L161-L167 collections class is included in PICKLE_SAFE variable, so, it should unpickle safely.\nSo, my idea is that you're not running 0.9.15\nCould you please check your local webapp/graphite/util.py file - does it looks similar?\n. Please show output of \npip show -f graphite-web\n. Location: /usr/lib/python2.6/site-packages\nSo, it's in under /usr/lib/python2.6/site-packages and not in /opt/graphite\nTry to reinstall it in /opt/graphite:\npip uninstall graphite-web\npip install graphite-web --install-option=\"--prefix=/opt/graphite\" --install-option=\"--install-lib=/opt/graphite/webapp\"\nMaybe you need to do the same for carbon and whisper too.\n. yw\nNot sure why default install not going to /opt/graphite\npip is odd sometimes, and developing quite fast, though.\n. IIRC graphite never guaranteed any specific metric order. It's really hard to say from where sorting came out suddenly in 0.9.15, code change between 0.9.12 and 0.9.15 is really huge... \n. As a comment above says sort is needed because 'Stabilize the order of the results by ordering the resulting series by name. This returns the result ordering to the behavior observed pre PR#1010'. \nWithout sort, results will be inconsistent during refreshes.\nAm I right, @bmhatfield ?. @lildude :\nsortByName() will not solve the problem if you need to wrap every graph on it. :)\nI'm suspecting that sort is connected with getting a consistent list of metrics during parallel fetches. So, if I should choose - sorted and consistent list and unsorted inconsistent - I'll choose first. It's a pity that we're changed sort behavior in 0.9.15, but I do not think that we need to change it back. \nIf you find an optional way how to enable previous behavior (with some config variable in local_settings.py) then we can include that in upcoming 1.0.0 (maybe in 0.9.16 too, but I'm still not sure that we should release it, maybe just left it as a patched 0.9.x branch on Github). Hello @ChrisHeerschap and @lildude ,\nI'm understanding your concerns, but as I said before we're not going to revert to old behavior and I mostly interested in good performance and stable clustering more. We can include some code which can optionally revert to old behavior, not sure if it's possible though.. @DazWorrall - something is definitely wrong with your cache, but config looks perfectly fine. Try to disable CARBONLINK_QUERY_BULK  - but I'm just 'shooting in the dark'.\n. Nah, CARBONLINK_QUERY_BULK was just a guess. In both cases the reason is the same - cache is not working and graphite reads data from disk, and that's slow. But what's an issue - hard to say. :(\n. @toni-moreno: You can try to set DIVERSE_REPLICAS=False for your carbon (if your version has DIVERSE_REPLICAS implemented) - if you have more than one host and REPLICATION_FACTOR >1\n. @toni-moreno: That's fine - DIVERSE_REPLICAS  can interfere with cache behaviour, but if you don't have it - then you have nothing to worry about.\nAnd if your problem is not persistent and happening only when queue grows  - then, I'm afraid, your installation just can't keep up with load.\n. @FractalizeR : That's a slightly different issue - metric will not be visible until whisper file will be created on disk. That's also known, but unfortunately quite hard to fix in the current codebase.. Looks like fixed for master branch in https://github.com/graphite-project/graphite-web/pull/1471\n. Absolutely great, agreed! :+1: \n. Could you please attach your local_settings.py ?\n. Hi @kennedyoliveira,\nhave no idea tbh. Something is definitely wrong with your installation. :(\nI can only advice to try to install https://github.com/obfuscurity/synthesize on DO or in Virtualbox (try video if stuck - https://www.youtube.com/watch?v=-ql3X5Dbsdo) and check configs there and compare with yours.\nYou can compare configs even w/o installing, of course - https://github.com/obfuscurity/synthesize/tree/master/templates\n. If configs are the same (I suspect that) - compare versions of Django and python modules.\n. OR\ntry this wsgi script - https://gist.github.com/deniszh/ba79af2728f8ba23afcb\nor - if do not work - https://gist.github.com/deniszh/133befd85d2e5ada2de4\n. whitenoise is a python library which helps django app to serve static files. Without it, you need to set up Apache properly to serve static files instead.\nWhy wsgi1.py isn't working? In wsgi2.py I removed whitenoise part completely, then you need to take all Apache setup burden. In wsgi1.py I tried to setup whitenoise static prefix manually, instead of (in your case) broken autodetect.\n. I found out that Nginx + Gunicorn setup nowadays looks much simpler than Apache + mod_wsgi.\nYou can also try that - http://www.kinvey.com/blog/108/graphite-on-ubuntu-1204-lts-8211-part-ii-gunicorn-nginx-and-supervisord\n. If it's working for you in the same setup as in synthesize - that's great. whitenoise should help with setup, not bring other troubles.\nNginx + gunicorn doesn't need static files setup and wsgi too. Maybe gunicorn serve static w/o additional  setup.\n. 0.9.12 only supports 1.3, maybe 1.4 IIRC\nIt's really old\n2016-03-11 13:58 GMT+01:00 Kennedy Oliveira notifications@github.com:\n\nHey @wawrzek https://github.com/wawrzek - Yeah, i'm using WhiteNoise, i\nhad to turn it off to work too. I'm installing 0.9.12.\nAbout Django, what version do you recommend?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1472#issuecomment-195354244\n.\n. LGTM!\nbut noting that fact that we will have no 0.9.x releases anymore - could you please port that change to master branch too?\n. Yep, separate PR is needed for master. \nNot sure about releasing bugfixes for 0.9.x, but doubt so. Am I right, @obfuscurity ?\n\nSVG rendering is another PITA. Maybe that pesky tag is dependent on Cairo version, or something?\n. Yep, looks like need to be fixed in 0.9.x and master too\n. :+1:\n. :+1:\n. :+1:\n. Hi guys,\nMerging code use number on Nones for merging, so, if both files have different retentions that could go terribly wrong. :(\nDifferent xFilesFactor can cause a different number of Nones too.\n. BTW. Maybe need to create separate issue for that...\nMaybe we need some merging policy too? E.g. \n- how much reviewers need to check new PR?\n- do we need review from specific \"elder\" person to press merge?\n  etc.\n. More/less - https://github.com/graphite-project/graphite-web/blob/master/CONTRIBUTING.md. Could you please advice which Java client you using? And which Graphite version?\nThanks!\n. Graphite 0.9.12 is quite old, this issue (https://github.com/graphite-project/graphite-web/issues/608) was fixed in 0.9.13 and later.\n. Also please note that 0.9.15 maybe not compatible with Django 1.9 at all\n. This PR https://github.com/graphite-project/graphite-web/pull/1471 add suport of Django 1.9 to master branch, but not 0.9.x\n. :+1: \n. LGTM, does anyone have concerns about merging this? \n@obfuscurity @brutasse @bmhatfield @mleinart ?\nWill merge it otherwise. :)\n. I made a quick test, seems working. Merging. \ud83c\udf56 \nThanks, @squarebracket, for your contribution, sorry for merge delay! \ud83d\udc4d \n. @jblaine,\nI saw similar issue before, I suspect it's because of last changes in pip.\nCould you please test is adding location options helps, like in https://graphite.readthedocs.org/en/0.9.15/install-pip.html#installing-carbon-in-a-custom-location but for /opt/graphite instead of /srv/graphite, e.g.\npip install whisper --install-option=\"--prefix=/opt/graphite\" --install-option=\"--install-lib=/opt/graphite/lib\"\npip install carbon --install-option=\"--prefix=/opt/graphite\" --install-option=\"--install-lib=/opt/graphite/lib\"\npip install graphite-web --install-option=\"--prefix=/opt/graphite\" --install-option=\"--install-lib=/opt/graphite/webapp\"\nIf yes - then we need to update docs.\n. Cool! Thanks a lot for testing!\nWe have another pip install issue, it looks like pip didn't follow any deps and requirements automatically. Will try to find out what's going on.\n. Yep, pypi version (sic!) has following in setup.cfg\n[install]\nprefix = /opt/graphite\ninstall-lib = %(prefix)s/webapp\nNot sure how that section can do exactly opposite thing and bring package to /usr/local.... I'm wondering what's result has '--no-binary' flag on pip.. @Ajedi32 - I mean, it's known issue, see previous comment - https://github.com/graphite-project/graphite-web/issues/1508#issuecomment-244383013. Hi @rsvancara \nWhich versions of Django did you try? What's your Graphite version is?\n. Hi @thinhduckhoi,\n1. Please do not use 0.9.13-pre1 branch, use 0.9.15 or 0.9.x instead\n2. It's just WARNINGS, you can safely proceed.\n. Yep, it's optional components.\n. Please note that it's only for carbon.* metrics - it's processed in special way.\n. Until now, master was not released, it's always bleeding edge. All releases were done from 0.9.x branch.\nBut next major release will be 1.0 from master branch, still not clear when though.\n. Hello @charlesdunbar,\nWe tagged 1.0.0-rc1 from master now, please test it . IIRC, it's more like \"no more releases from 0.9.x\". IMO we can still accept valuable patches to 0.9.x.\nBut tests are really needed, though.\nAnd maybe this patch need to be ported to master too? \n. Yep, known issue - https://github.com/graphite-project/graphite-web/issues/1508\nLet's continue there.\n. :+1:\n. This patch is working on our production cluster w/o any issues. As the description says, it allows expanding multiple (including nested) curly braces in regexes.\nNot sure, what's policy for merging 0.9.x branch.\nIMO we can at least merge useful patches it to current 0.9.x, w/o making releases through GitHub and/or pypi. \nOpinions? @graphite-project/committers ?\n. Mater PR https://github.com/graphite-project/graphite-web/pull/1713 just merged, merging this too.\n. Same here. :+1:\nNeed to do the same, maybe will learn that too from your code. Much appreciated!\n. https://github.com/django-extensions/django-extensions/issues/705#issuecomment-194036134\nTry to remove old *.pyc files\n. Try to upgrade django-tagging to the latest version, 0.3.6 is quite old.\nAccording to our test suite django 1.9.6 + tagging 0.4.3 was working fine - https://travis-ci.org/graphite-project/graphite-web/jobs/134935515\n. We have https://github.com/graphite-project/graphite-web/blob/0.9.x/requirements.txt\nWe could change django-tagging==0.3.1 to django-tagging>=0.3.1,<0.4 but not sure if needed.\nWe also have django-tagging<0.4 in https://github.com/graphite-project/graphite-web/blob/0.9.x/.travis.yml\n. IMO cherry-pick is fine. The test is also there. Any complaints? If not - I'll merge it soon.\n. I would prefer to replace Django with Flask or Bottle, indeed. But I think it's target for e.g., Graphite 2.0 version. \nIMO we need to support at least LTS releases (now 1.8) and version in current LTS distros - e.g. Ubuntu 12.04 / 14.04, CentoOS 6 / 7, etc. - but now pinning to <1.9 is fine.\n. Hello @genericgithubuser,\nIt seems your graphite setup is wrong. You don't need to have separate storage dirs for different carbon caches - they can (and should) share single whisper storage.\n. @vinhlh - yes, documentation is a bit scarce... But what exact document do you mean btw?\nOfficial documentation now lives in http://graphite.readthedocs.io/en/latest/ and open to PRs here - https://github.com/graphite-project/graphite-web/tree/master/docs\n. Nice catch, @nyerup ! \nLGTM \ud83d\udc4d \n. Looks good at first sight, but I do not running master in PROD.\nAnyone else want to check @graphite-project/committers ?\n. Hi @redbaron,\nlint is not happy about your change:\n/home/travis/build/graphite-project/graphite-web/webapp/graphite/remote_storage.py:4:1: F401 'threading.Event' imported but unused\n/home/travis/build/graphite-project/graphite-web/webapp/graphite/remote_storage.py:163:93: F821 undefined name 'url'\n/home/travis/build/graphite-project/graphite-web/webapp/graphite/remote_storage.py:169:45: F821 undefined name 'url'\nERROR: InvocationError: '/home/travis/build/graphite-project/graphite-web/.tox/lint/bin/flake8 /home/travis/build/graphite-project/graphite-web/webapp/graphite\n. Did you see any performance regressions w/o cache, @redbaron ?\n. OK, let's give it a try.\n. Another great work, @cbowman0! \ud83d\udc4d \n. LGTM, but maybe someone more knowledgeable in movinAverage() function could take a look? Maybe @arielnh56 ? Or anybody else? @bmhatfield @cbowman0 ?\n. Cool! Thanks, @kamaradclimber!\nAny objections to merging it? @drawks ?\n. I limited conversation here to collaborators. Is it OK, @obfuscurity ?\n. Fixed by https://github.com/graphite-project/graphite-web/pull/1560\nApply it to 0.9.15 locally or upgrade to 0.9.x branch please. \n. Looks you're right, @cbowman0 \nWill try to dive into that on the weekend, maybe it will work w/o backporting of additional patches.\n. I have a progress:\n\nit's a quick hack, will finish it tomorrow. :)\n. I have a progress:\n\nit's a quick hack, will finish it tomorrow. :)\n. PR is done, let me check couple of other things.\n. PR is done, let me check couple of other things.\n. Yep, works with local merges too, ready to merge. :)\n. It's because of nature of this patch, though. As you can see in the code, if merging is enabled - I'm disabling calculation minimal node set completely - https://github.com/deniszh/graphite-web/blob/bc18f9730b102cd5ab3ba1409b6760471f4b43ea/webapp/graphite/storage.py#L83\nSo, it will ask all leaf nodes for potential data to merging. I see no other way how we can decide which node can contain valuable data.\nIf someone has ideas how to do so - will glad to fix it.\n. Unfortunately, I'm not running master on any significant load. I could test it on some artificial load, though.\n. @iksaif - cool, thanks a lot!\nIs del self.inflight_requests[url] good as a permanent fix? Or we need  a better solution? \n. @iksaif \nThanks a lot! Let me know if I can help with something.\n. Thanks, @iksaif !\nMaybe we need to disable cache explicitly when REMOTE_STORE_MERGE_RESULTS disabled?\n. Ah, right, cool then!\n. Does anyone want to rebase this? @cbowman0 or anyone else?. Does anyone want to rebase this? @cbowman0 or anyone else?. Which backend do you use for Django, @scosist? Default one, SQLite? Just shooting in the dark, though.\n. Both carbon and graphite-web has support of user keyfunc for hashing, not sure if that enough for proper fnv1a support, suspecting that's not...\n. I added FNV1a support in PR https://github.com/graphite-project/graphite-web/pull/1723\n. Hello, @56quarters !\n1) If it gives 10% less overhead, even in tests - IMO it's a good candidate for optimization.\nOTOH enablePackrat() should speed up parsing - so, maybe it takes more CPU time but response time is better when it's enabled? (just speculating here, my parsers' knowledge is scarce, I have no idea, tbh)\n2) PR is always a good idea. Just make it against a master.\n3) USE_GRAMMAR_PARSE_CACHE is OK\n4) We can change default behavior in master, but if we want to disable cache we need to be sure that enablePackrat() is evil in 100% of cases.\n. Also tested with two nodes cluster and HTTPS.  So, it works with local storage too.\nBoth servers have\nCLUSTER_SERVERS = [\"192.168.50.2:443\", \"192.168.50.3:443\"]\nINTRACLUSTER_HTTPS = True\nResults:\n\nLogs:\ngraph001:\n==> /opt/graphite/storage/log/webapp/info.log <==\nSun Aug 21 18:36:47 2016 :: find_view query=local.random.diceroll3 local_only=1 matches=1\nSun Aug 21 18:36:47 2016 :: received remote find request: pattern=local.random.diceroll3 from=1471783007 until=1471804607 local_only=1 format=pickle matches=1\nSun Aug 21 18:37:03 2016 :: FindRequest.send(host=192.168.50.2:443, query=<FindQuery: local.random.diceroll3 from Sun Aug 21 18:04:42 2016 until Sun Aug 21 18:37:28 2016>) called\nSun Aug 21 18:37:03 2016 :: FindRequest.send(host=192.168.50.3:443, query=<FindQuery: local.random.diceroll3 from Sun Aug 21 18:04:42 2016 until Sun Aug 21 18:37:28 2016>) called\nSun Aug 21 18:37:03 2016 :: find_view query=local.random.diceroll3 local_only=1 matches=1\nSun Aug 21 18:37:03 2016 :: received remote find request: pattern=local.random.diceroll3 from=1471802682 until=1471804648 local_only=1 format=pickle matches=1\nSun Aug 21 18:37:03 2016 :: find_view query=local.random.diceroll3 local_only=1 matches=1\nSun Aug 21 18:37:03 2016 :: received remote find request: pattern=local.random.diceroll3 from=1471802682 until=1471804648 local_only=1 format=pickle matches=1\nSun Aug 21 18:37:03 2016 :: ReadResult :: requesting http://192.168.50.3:443/render/?target=local.random.diceroll3&format=pickle&local=1&noCache=1&from=1471802682&until=1471804648\nSun Aug 21 18:37:03 2016 :: ReadResult :: requesting http://192.168.50.2:443/render/?target=local.random.diceroll3&format=pickle&local=1&noCache=1&from=1471802682&until=1471804648\ngraph002:\n==> /opt/graphite/storage/log/webapp/info.log <==\nSun Aug 21 18:37:03 2016 :: FindRequest.send(host=192.168.50.2:443, query=<FindQuery: local.random.diceroll3 from Sun Aug 21 18:04:42 2016 until Sun Aug 21 18:37:28 2016>) called\nSun Aug 21 18:37:03 2016 :: FindRequest.send(host=192.168.50.3:443, query=<FindQuery: local.random.diceroll3 from Sun Aug 21 18:04:42 2016 until Sun Aug 21 18:37:28 2016>) called\nSun Aug 21 18:37:03 2016 :: find_view query=local.random.diceroll3 local_only=1 matches=1\nSun Aug 21 18:37:03 2016 :: received remote find request: pattern=local.random.diceroll3 from=1471802682 until=1471804648 local_only=1 format=pickle matches=1\nSun Aug 21 18:37:03 2016 :: find_view query=local.random.diceroll3 local_only=1 matches=1\nSun Aug 21 18:37:03 2016 :: ReadResult :: requesting http://192.168.50.2:443/render/?target=local.random.diceroll3&format=pickle&local=1&noCache=1&from=1471802682&until=1471804648\nSun Aug 21 18:37:03 2016 :: received remote find request: pattern=local.random.diceroll3 from=1471802682 until=1471804648 local_only=1 format=pickle matches=1\n. Have no idea what to do with tests / coverage, though.\n. Looks like should be rebased for having working tests.... Will try to carefully merge this. Tests are looks green. Hello @obfuscurity,\nAre you still interested in that functions? I can try to rebase, we can still put that to 1.0.0 :) . Ah, true. Maybe @cbowman0 will find some time to check this? I'm weak at tests. Or anyone else? /cc @graphite-project/committers ?. Not sure do we still need this, but looks promising.\nThis is original description from #92 is \n\n\"Functions: add resample() for performance and sanity\nDrastically speeds up further calcuations on the returned series\nMakes it much easier to have a consistent datapoints / pixels ratio\nfor movingAverage() and friends.\nFunctions: add smooth() as a movingAverage over pixels\nInternally does a movingAverage(resample()).\nProvides consistent smoothing over a given number of graph pixels,\ninstead of a number of datapoints of arbitrary time width.\nStill provides consistent smoothing when there are fewer datapoints\nthan pixels.\". @anirbanroydas - which Django do you using? 0.9.15 doesn't support 1.9, try downgrade it to 1.8\n. Django 1.9 is not compatible with Graphite 0.9.15, it's 100%\n. @anirbanroydas: IIRC 1.4 - 1.8 is fine.\nSorry, I'm not on IRC now. \n. dod you run syncdb command?\nPYTHONPATH=$GRAPHITE_ROOT/webapp django-admin.py syncdb --settings=graphite.settings \nIt's mandanory.\n. According to http://stackoverflow.com/questions/29442323/django-centos-7-cannot-import-name-col/29612399#29612399 error above could be caused by improper downgraded Django.\nTry to \"uninstall Django, remove the remaining Django directories under site-packages, then reinstall it\", as suggested.\n. Does setup.py changes intended?\n. Ah, it's in a separate commit, missed that, sorry.\nFrom my perspective, it looks good.\nTested on your prod, @iksaif ?\nI think we can merge it if no other objections.\nAnd then repeat perftest, @obfuscurity . :)\n. \ud83d\udc4d \n. @iksaif - Could you please check if this fix still relevant after merging PR #1674 ?\nI suspect that it still, because we still not filtering remote nodes, but maybe worth to check.\n. Thanks!\n. \ud83d\udc4d \n. @obfuscurity - I also have no idea why this change is needed for @iksaif \nSo - \u00af(\u30c4)/\u00af\n. That's sucks. I still didn't test fix...\n. @atnak \nIf you not using clustering, of course, you can disable REMOTE_STORE_MERGE_RESULTS.\nStill wondering why fix worked for @iksaif ..\n. @atnak - what's a slowdown (in seconds) for your graph above was introduced?\n\nCould you please check render time when REMOTE_STORE_MERGE_RESULTS enabled and REMOTE_STORE_MERGE_RESULTS disabled?\nIs is still worse than before if REMOTE_STORE_MERGE_RESULTS is disabled?\n. @obfuscurity - yes, I was also surprised that my patch affects local fetches.\n. Thanks, @atnak !\nIndeed, my patch shouldn't affect local queries.\nBack to the drawing board!\n\n. I'm suspecting that fix should be simple as https://github.com/deniszh/graphite-web/commit/7f3c01b0088f3d8c70881aedfb08865ce709989e, testing it now.\n. @atnak \nYes, that's exactly what I'm testing now. And it looks like local merging works too. :)\nPreparing PR.\n. @atnak - could you please test my fix in PR #1674 ?  I tested it only on synthesize - unfortunately I had no production installation on master yet.\nMuch appreciated!\n. Fixed in PR #1674 with many help of @atnak \n. Nice catch! Yes, please create same for the master, much appreciated!\n. @iliapolo - up to you. Issue edit is fine.\n. \ud83d\udc4d \nAppreciating the test too!\n. Good question, @iliapolo !\nI'm not really sure TBH. Please ignore it for now. :)\n. Merging\n. I made similar test setup which I used in PR #1657: Synthesize with 3 servers.\nBut now I generated not only local.random.diceroll3 but also local.random.diceroll4 - same, but decreasing time series, for testing wildcards.\nTest nr. 1 - \"Dedicated frontend\".\nMaster is dedicated frontend, graph001 (ip 192.168.50.2) and graph002 (ip 192.168.50.3) hold local metrics.\nMaster has \nCLUSTER_SERVERS = [\"192.168.50.2\", \"192.168.50.3\"]\nINTRACLUSTER_HTTPS = True\nREMOTE_STORE_MERGE_RESULTS=True\ngraph00x servers run on default setup (REMOTE_STORE_MERGE_RESULTS=True)\nResult: https://snapshot.raintank.io/dashboard/snapshot/rtttjdYshCteQAj4zVOiqGdXjD2S9Rrn\n\n\nLogs on master:\nSat Sep 03 15:48:09 2016 :: FindRequest.send(host=192.168.50.2, query=<FindQuery: local.random.diceroll3 from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:48:09 2016 :: FindRequest.send(host=192.168.50.2, query=<FindQuery: local.random.* from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:48:09 2016 :: FindRequest.send(host=192.168.50.3, query=<FindQuery: local.random.diceroll3 from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:48:09 2016 :: FindRequest.send(host=192.168.50.3, query=<FindQuery: local.random.* from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:48:09 2016 :: ReadResult :: requesting http://192.168.50.2/render/?target=local.random.diceroll3&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nSat Sep 03 15:48:09 2016 :: ReadResult :: requesting http://192.168.50.3/render/?target=local.random.%2A&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nSat Sep 03 15:48:09 2016 :: ReadResult :: requesting http://192.168.50.3/render/?target=local.random.diceroll3&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nSat Sep 03 15:48:09 2016 :: ReadResult :: requesting http://192.168.50.2/render/?target=local.random.%2A&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nSat Sep 03 15:48:09 2016 :: FindRequest.send(host=192.168.50.2, query=<FindQuery: local.random.diceroll4 from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:48:09 2016 :: FindRequest.send(host=192.168.50.3, query=<FindQuery: local.random.diceroll4 from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:48:09 2016 :: ReadResult :: requesting http://192.168.50.2/render/?target=local.random.diceroll4&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nSat Sep 03 15:48:09 2016 :: ReadResult :: requesting http://192.168.50.3/render/?target=local.random.diceroll4&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nTest nr. 2 - \"Clustered frontend\"\ngraph001 (ip 192.168.50.2) and graph002 (ip 192.168.50.3) hold local metrics, and also both have\nCLUSTER_SERVERS = [\"192.168.50.2\", \"192.168.50.3\"]\nINTRACLUSTER_HTTPS = True\nREMOTE_STORE_MERGE_RESULTS=True\nResult: https://snapshot.raintank.io/dashboard/snapshot/SxF1I84u096DunhvXqVlAOTLIcPPdsfl\n\n\nLogs graph001:\nSat Sep 03 15:51:35 2016 :: FindRequest.send(host=192.168.50.2, query=<FindQuery: local.random.* from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:51:35 2016 :: FindRequest.send(host=192.168.50.2, query=<FindQuery: local.random.diceroll3 from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:51:35 2016 :: find_view query=local.random.diceroll3 local_only=1 matches=1\nSat Sep 03 15:51:35 2016 :: received remote find request: pattern=local.random.diceroll3 from=1472911957 until=1472914884 local_only=1 format=pickle matches=1\nSat Sep 03 15:51:35 2016 :: FindRequest.send(host=192.168.50.3, query=<FindQuery: local.random.diceroll3 from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:51:35 2016 :: find_view query=local.random.diceroll3 local_only=1 matches=1\nSat Sep 03 15:51:35 2016 :: received remote find request: pattern=local.random.diceroll3 from=1472911957 until=1472914884 local_only=1 format=pickle matches=1\nSat Sep 03 15:51:35 2016 :: find_view query=local.random.* local_only=1 matches=2\nSat Sep 03 15:51:35 2016 :: received remote find request: pattern=local.random.* from=1472911957 until=1472914884 local_only=1 format=pickle matches=2\nSat Sep 03 15:51:35 2016 :: FindRequest.send(host=192.168.50.3, query=<FindQuery: local.random.* from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:51:35 2016 :: find_view query=local.random.* local_only=1 matches=2\nSat Sep 03 15:51:35 2016 :: received remote find request: pattern=local.random.* from=1472911957 until=1472914884 local_only=1 format=pickle matches=2\nSat Sep 03 15:51:35 2016 :: ReadResult :: requesting http://192.168.50.3/render/?target=local.random.diceroll3&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nSat Sep 03 15:51:35 2016 :: ReadResult :: requesting http://192.168.50.2/render/?target=local.random.diceroll3&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nSat Sep 03 15:51:35 2016 :: ReadResult :: requesting http://192.168.50.3/render/?target=local.random.%2A&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nSat Sep 03 15:51:35 2016 :: ReadResult :: requesting http://192.168.50.2/render/?target=local.random.%2A&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nSat Sep 03 15:51:35 2016 :: FindRequest.send(host=192.168.50.2, query=<FindQuery: local.random.diceroll4 from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:51:35 2016 :: FindRequest.send(host=192.168.50.3, query=<FindQuery: local.random.diceroll4 from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:51:35 2016 :: find_view query=local.random.diceroll4 local_only=1 matches=1\nSat Sep 03 15:51:35 2016 :: received remote find request: pattern=local.random.diceroll4 from=1472911957 until=1472914884 local_only=1 format=pickle matches=1\nSat Sep 03 15:51:35 2016 :: ReadResult :: requesting http://192.168.50.2/render/?target=local.random.diceroll4&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nSat Sep 03 15:51:35 2016 :: find_view query=local.random.diceroll4 local_only=1 matches=1\nSat Sep 03 15:51:35 2016 :: received remote find request: pattern=local.random.diceroll4 from=1472911957 until=1472914884 local_only=1 format=pickle matches=1\nSat Sep 03 15:51:35 2016 :: ReadResult :: requesting http://192.168.50.3/render/?target=local.random.diceroll4&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nLogs graph002:\nSat Sep 03 15:51:35 2016 :: FindRequest.send(host=192.168.50.2, query=<FindQuery: local.random.* from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:51:35 2016 :: FindRequest.send(host=192.168.50.2, query=<FindQuery: local.random.diceroll3 from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:51:35 2016 :: FindRequest.send(host=192.168.50.3, query=<FindQuery: local.random.* from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:51:35 2016 :: FindRequest.send(host=192.168.50.3, query=<FindQuery: local.random.diceroll3 from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:51:35 2016 :: find_view query=local.random.* local_only=1 matches=2\nSat Sep 03 15:51:35 2016 :: received remote find request: pattern=local.random.* from=1472911957 until=1472914884 local_only=1 format=pickle matches=2\nSat Sep 03 15:51:35 2016 :: find_view query=local.random.diceroll3 local_only=1 matches=1\nSat Sep 03 15:51:35 2016 :: received remote find request: pattern=local.random.diceroll3 from=1472911957 until=1472914884 local_only=1 format=pickle matches=1\nSat Sep 03 15:51:35 2016 :: ReadResult :: requesting http://192.168.50.2/render/?target=local.random.%2A&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nSat Sep 03 15:51:35 2016 :: ReadResult :: requesting http://192.168.50.3/render/?target=local.random.%2A&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nSat Sep 03 15:51:35 2016 :: find_view query=local.random.diceroll3 local_only=1 matches=1\nSat Sep 03 15:51:35 2016 :: received remote find request: pattern=local.random.diceroll3 from=1472911957 until=1472914884 local_only=1 format=pickle matches=1\nSat Sep 03 15:51:35 2016 :: find_view query=local.random.* local_only=1 matches=2\nSat Sep 03 15:51:35 2016 :: received remote find request: pattern=local.random.* from=1472911957 until=1472914884 local_only=1 format=pickle matches=2\nSat Sep 03 15:51:35 2016 :: ReadResult :: requesting http://192.168.50.3/render/?target=local.random.diceroll3&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nSat Sep 03 15:51:35 2016 :: ReadResult :: requesting http://192.168.50.2/render/?target=local.random.diceroll3&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nSat Sep 03 15:51:35 2016 :: FindRequest.send(host=192.168.50.2, query=<FindQuery: local.random.diceroll4 from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:51:35 2016 :: find_view query=local.random.diceroll4 local_only=1 matches=1\nSat Sep 03 15:51:35 2016 :: received remote find request: pattern=local.random.diceroll4 from=1472911957 until=1472914884 local_only=1 format=pickle matches=1\nSat Sep 03 15:51:35 2016 :: FindRequest.send(host=192.168.50.3, query=<FindQuery: local.random.diceroll4 from Sat Sep  3 14:12:37 2016 until Sat Sep  3 15:01:24 2016>) called\nSat Sep 03 15:51:35 2016 :: find_view query=local.random.diceroll4 local_only=1 matches=1\nSat Sep 03 15:51:35 2016 :: received remote find request: pattern=local.random.diceroll4 from=1472911957 until=1472914884 local_only=1 format=pickle matches=1\nSat Sep 03 15:51:35 2016 :: ReadResult :: requesting http://192.168.50.3/render/?target=local.random.diceroll4&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nSat Sep 03 15:51:35 2016 :: ReadResult :: requesting http://192.168.50.2/render/?target=local.random.diceroll4&format=pickle&local=1&noCache=1&from=1472911957&until=1472914884\nSo, both local and remote merging are working, and shouldn't be any local regression anymore.\n. Very strange... Really puzzled here. local should be True for local metrics. \n@atnak, could you please share your /opt/graphite/storage/log/webapp/info.log for periods above with me privately? My email is denis.zhdanov@gmail.com\n. @atnak - please disregard my request, checking another solution\n. @atnak - please test current PR - should work now. If not - please give me your /opt/graphite/storage/log/webapp/info.log when you enable REMOTE_STORE_MERGE_RESULTS\n. Finally!\nThanks a lot, @atnak - you made that I didn't! I removed my implementation and provide yours.\nPlease check the latest code - if it works I will squash and merge, mentioning you as an author.\nOr you can create own PR and I'll ditch this one out.\nMuch appreciated!\n. > Since we found that local nodes have redundant intervals, could remote nodes be the same?\n\nIn that case, it might be required to reduce nodes to the best measure_of_added_coverage(node, False) per path per remote host.\n\nYes, maybe. Now it's reducing per path and removing relevant data if merging enabled. Let's address that in separate PR.\n. I'm going to squash and merge it, to have properly working code ASAP, @obfuscurity \n. Yes, this PR is not an ideal, patches are welcome. :) Also not sure how it works with intervals, maybe completely wrong/non-effective - but merges are working and doesn't bring any troubles for local queries. Sorry, but I can't agree that 900% performance penalty for local queries (which should not be affected at all) is \"small\".\nNot sure do we need to enable merging behavior enabled by default, but I would like to do that if we able to.\n. Test results:\nShared frontend:  https://snapshot.raintank.io/dashboard/snapshot/EbbM9e4k01C9czIzgx1AiD8qMLvsznSa\nDedicated frontend:\nhttps://snapshot.raintank.io/dashboard/snapshot/xgfh6rHj0ovNckEWSOw34zgATkbi91KP\nVagrant: https://github.com/deniszh/synthesize/tree/feature/multimaster\n. I'm suspecting we're talking about different problems, @iksaif \n\nThe performance penalty here is totally fine, it's only a few milliseconds and that's expected if you set REMOTE_STORE_MERGE_RESULTS because you explicitly want to fill gaps by querying multiple replicas (= a least a remote one). While initially, this setting would create an additional query per metric, with my fix it only does one query per glob (usually one).\n\nI'm not talking about performance penalty of querying multiple replicas. That PR was triggered by @atnak 's problem. As you can see on the graph above when he enables REMOTE_STORE_MERGE_RESULTS he got 30000 ms rendering time instead of 8000-10000 ms. It's not 900%, but it's 300-400%. But the main problem was not that, he has NO CLUSTER HOSTS configured. That's totally unacceptable, and I suspected that problem can also happen on hosts with remote nodes with some massive wildcard queries.\nAfter this PR we still have working merging and NO IMPACT on local queries.\n\nThe most common usecase for this is when you have REPLICATION_FACTOR>1 and a node goes down and come back up with holes. These holes will take hours to be fixed by carbonate, and in the meantime you want to make sure that you're service the correct date. The old implementation of REMOTE_STORE_MERGE_RESULTS + my fix did exactly that with < 10% performance penalty on my setup.\n\nI know, that was a whole idea behind REMOTE_STORE_MERGE_RESULTS change. It was working on 0.9.x and I believe that new implementation doing exactly the same.\n\nMaybe we should have an additional setting (REMOTE_STORE_FETCH_REPLICAS = x) to state that we one to fetch multiple replicas at any time ? I can write a patch for that, it would be set to 1 by default.\n\nSorry, I'm totally lost and confused here. It will fetch multiple replicas now, no?\nCould you please maybe show problems with current code on test (or your) environment?\n. Thanks for testing, @iksaif !\n. Hello @dantate,\nThere's no graphite-web.conf config exists for Graphite, unfortunately. If you want to change graphite-web port you need to do that in your webserver config.\nWhich webserver do you use, Apache?\n. You need to add \"Listen 81\" directive somewhere near \"Listen 80\". Usually it is in /etc/apache2/ports.conf file and restart Apache.\nIt's not directly Graphite-related though. :)\n. On Centos this file is named /etc/httpd/conf/httpd.conf\n. Not really understand what you mean... Anyway, there is a ton of articles how to add change Apache port on the Internet, this question is not specific to Graphite, for example, http://www.cyberciti.biz/faq/fedora-rhel-centos-configure-httpd-listen-multipleports/\n. If you change port to 81 in Virtualhost section - but didn't add \"Listen 81\" directive - Graphite (or any application which is under that Virtualhost section) will not load, that's intended behaviour. Please configure your Apache properly.\n. Sorry, @dantate - but your question is not related to Graphite. If it's Icinga related - it's better to check Icinga page for support - https://www.icinga.org/community/ .\nIf it's general Linux question - it's better to ask that on general Linux forums, on Stackoverflow / Serverfault.\n. @toopa2002 - no, and that's exactly what @obfuscurity talking about.\nYou will not get test.count in browser tree, but can get results using render api.\nTry to create graph of test.count.max metric, then edit metric name manually - you will get graph for test.count\n. LGTM\n. But, but, but... they're changing same lines in same file - how both could be applied..? \nCompare \n  https://github.com/redbaron/graphite-web/blob/d3293c185a02316f518b697708642065b07a51c6/webapp/graphite/render/views.py#L377-L383 and\n  https://github.com/redbaron/graphite-web/blob/345b5c20ba528bf41963d8582a821403c2878a43/webapp/graphite/render/views.py#L378-L380\n. Close/open to retrigger tests. Sometimes they're sporadically fails...\n. Ah, true, just find that. Thanks, @obfuscurity !\n. Hello @ellipses,\nSorry for delay, could you please rebase test code? I'm \ud83d\udc4d to merge this!\nThanks!. @strajansebastian - you forgot to add URL link. See http://docutils.sourceforge.net/docs/ref/rst/restructuredtext.html#hyperlink-references\n. When you say 'latest git' is it really latest, i.e. 2aee2bc ? And it includes 1f56f515 and 71cfdac ?\nEdit: include right commits.\n. > Small request needs >1000ms on webapp render host\n\nWhen we call the same render request over wget from the render host it needs only 180ms not 1000ms. \n\nSorry, looks really confusing, do not really understand which request takes what. Could you maybe give exact queries?\n. > I have everything updated 3 hours ago, so yes. The latest commit is 2aee2bc, but the problem was with 0.9.15 the same.\nAh, then maybe it's something else.  Did you have REMOTE_STORE_MERGE_RESULTS enabled on 0.9.15 ?\n. Another (probably stupid) question - are you sure that you really running on master code now, and not on still 0.9.15 ?\n. Graphite clustering is quite complicated. I never tried to measure timings and compare them between frontend and backend. 0.9.15 works quite good, I didn't try it on master though, but other people using clustering on master too.\nIs that regression visible only on summarize() or globs or on any requests?\n. @Kenterfie - did you try 0.9.x branch instead of 0.9.15?\nDid I understand you right - cluster on a current master is still much slower than on 0.9.15?\n\nI dont know, why no stable version is released since month to fix the most annoying bugs in 0.9.15\n\nIIRC that 0.9.15 should be last 0.9.x release. If we had proper semver semantic that 0.9.16 should be bugfix release - but we do not have it.\nMaybe releasing 0.9.16 is a good idea. What do you think, @obfuscurity ?\n. Just test that on 0.9.15 (with proper waiting for 10 minutes between requests to mitigate the cache), for format=json and quite complicated target with 3 globs -  aliasByNode(nonNegativeDerivative(groupByNode(node*.*.blah*.value, 1, 'sumSeries')), 0)\"\nResults - \nBackend 1 - 1.5 sec\nBackend 2 - 1.6 sec\nFrontend (with just 2 backends above) - 15.3 sec \nAs I said, clustering is hard. If you can avoid that - do that.\nWith a second request (when results are still in cache), it's much better - \nBackend 1 - 0.9 sec\nBackend 2 - 0.8 sec\nFrontend (with just 2 backends above) - 1.7 sec \n. @redbaron: it's a nice thing to have dedicated caches for graphite-web, but that's not mandatory. You can run cluster w/o any problems up to some extent on shared caches.\n. That's really nice thought experiment, but I have 0 issues on my graphite clusters, at least with load 0.5 - 1.5 M metrics/min write and 2000-5000 K metrics/min read.\nSo, no, that's not mandatory. :)\n. So, that's my point - YMMV. Nice to try separate pools in case of deadlocks, but 99% of users will never hit that problem.\nAlso, IIRC you have quite a big load, around 6M/min, right? even hitting cache problems in go-carbon - maybe that's case.\n. Wondering also how you can hit deadlocks on twisted-based daemon... Maybe because of some race conditions or other bugs in carbon or twisted, under some high load.\n. > 8 nodes, 12M metrics/minute received with replication factor of 2, so 24M /minute persisted\n24M per 8 nodes,  i.e. 3M per node, right?\n\nthere are not that many requests coming from browsers: 1200-1500 / minute, but graphs are quite big, with json response sizes being as high as 500KB (10% are >225KB).\n\nDo you have maxDataPoints implemented btw? Just checked - even on 6-month graph I see 100-200K responses max. \n\nI configure apache with 2 WSGI process groups on ports 8000 and 8010, both pointing to the same graphite install with same with CLUSTER_SERVERS in local_settings.py pointing to port 8010.\n\nAh, you mean separate pools of graphite-web in WSGI config for a client and intra-cluster setup? Got it. I thought separate carbon daemon for carbonlink queries and writes.\nI never have this problem on my load, nor on Apache + WSGI nor on Nginx + Gunicorn - but I think you can simply increase a number of workers on port 8000. \n. > Render times are noticeably through the roof. I didn't change the config and had CLUSTER_SERVERS enabled in the past to merge results.\n@reyjrar, could you please test does REMOTE_STORE_MERGE_RESULTS = False make things better for master or not?\nI'm trying to find is that a mail culprit now or it's because of lack of parallel requests.\nHave plans to test rendering on master too on one of my clusters, but not sure if will have time this week...\n. @reyjrar - could you please test this PR https://github.com/graphite-project/graphite-web/pull/1699 ?\nI tried to tag you there but seems misread your github handle...\n. I have plans to use newly-opensourced pyflame profiler http://eng.uber.com/pyflame/ to find where culprit is.\n. @OrangeDog - no, it's not connected with #1768 . Graphite 0.9.16 and 1.0.0 are released. Please report any performance issue (and other issues too, of course). @reyjrar @stan-sack :\nCould you please open a new issue for that, to increase its visibility? It's probably not related to original (and closed) issue anyway.\nThanks a lot!. Retriggered test, green now.\n@Kenterfie @reiyjar @bmhatfield - please check it out.\n. I think https://github.com/graphite-project/graphite-web/pull/1818 superseding this. Closing, I hope you do not mind, @cbowman0 ?. \ud83d\udc4d\nLooks plausible.\n. Running tests on python3 and app itself on 2.7... ?\n. Dublicate of #1867.\n@saifromeo : Try to run\ncd /opt/graphite/webapp/\nPYTHONPATH=/opt/graphite/webapp django-admin.py migrate --settings=graphite.settings --run-syncdb. I suspecting it's because of some Centos-specific things but I have no idea which things exactly.... which version of Django are you trying, @cixelsyd ?. @mknornschild: Looks like a completely different issue to me. Sorry, I can't read German, but looks like your manual is broken somehow. Not sure which version of Graphite is in Ubuntu 18.04 packets, also Django 1.4 is probably too old.\nWe officially recommend to install Graphite from sources or use Docker image.. @andChow : you installed python part (cairocffi) but probably missing cairo libraries. Which linux distribution do you use?. LGTM.\nI would prefer separate commits for docs and functionality itself, but maybe I'm nitpicking here.\nAny other reviewers? /cc @graphite-project/committers ?\n. Maybe can be merged with https://github.com/graphite-project/graphite-web/pull/1290\n/cc @cbowman0 \n. Will do.\n. Tests are added, looks better now.\n. Thanks, merging both.\n. Strange, Django 1.7 should have django.utils.importlib - according to https://stackoverflow.com/questions/32761566/django-1-9-importerror-for-import-module\nPlease note that Django 1.7 dropped python 2.6 support. Do you have python 2.6 or 2.7?\n. On 2.7 and Django 1.7 it should work. Or you running Django 1.7 on python 2.6 or Django version is different.\nTry to downgrade Django to 1.6\n. Looks like your Django install running on Python 2.6, which is a default for centos 6.5 and installing python to /opt/python is not enough.\n. @jack17529 : try to add \n```\nInstall pyopenssl\nRUN set -x \\\n    && pip install pyopenssl\n```\njust before # Install cairocffi.\n. But 'python-twistedis required for carbon.\n. I can add more tests if needed - now I added only for changed functions. Tests data is taken fromcarbon-c-relay` output, so, it should be compatible.\n. Tried to fix the docs, @obfuscurity \nNot sure if good now. Suggestions are welcome. :)\n. Done, @obfuscurity !\n. Oops, was blind, sorry :)\nLooks better now?\n. NP, fixed!\n. Will squash and merge that EOD\n. @elsmorian ,\nAre you sure that we're compatible with Django 1.9. IIRC we had some issues with that.\n. @spektre1 \nAccording to https://github.com/graphite-project/graphite-web/blob/master/requirements.txt - Django should be strictly 1.9.\nPlease try to \npip uninstall Django\npip install \"Django>=1.9,<1.9.99\n. @shahmaulikn - what's you r Django version? Should be 1.9.x. @spektre1 - could you please share details what's exactly should be patched?. Wow, indeed, @iain-buclaw-sociomantic !\nThanks for suggestions!\n. Merging for master and 0.9.x\n. Implemented in https://github.com/graphite-project/graphite-web/pull/1808. Merging!. smart_str() will work both, for python3 and python2. It's built in and should be safe to use. But I'm not sure how it will be compatible with carbon and 3rd party tools.\n@eagleliang : You can create PR, it's a good thing - but we need to test it how it works with different combinations of pythons/djangos/carbons/relays etc. . LGTM\n. Yes, REMOTE_STORE_MERGE_RESULTS=True removing all FIND_TOLERANCE logic completely. Not sure how it can cause 'No data' situation, because it should make more requests but get data anyway. \nWe also have a long flush cycle in carbon, but TBH I see no sense in such big cache / tolerance window (7200 is 2 hours, right?). \n. Indeed. I have no good knowledge how find() in master branch works and why FIND_TOLERANCE is there at all. 0.9.x doesn't have that.\n. I mean, It will be great if you explain master's find() logic here, I'm\nreally not sure that I read source right :)\n. It will ask data through normal Graphite API, so, for existing (on disk) metrics it will return data, and for non-existing - it will return nothing. I see no sense to check something in past for such logic (and FAULT_TOLERANCE describes tolerance window in past: tolerance_window = now - settings.FIND_TOLERANCE ).\nMy understanding of FIND_TOLERANCE logic - we could have different find results for a different time interval. The current code of master trying to optimize merging of all these intervals and in a case of REMOTE_STORE_MERGE_RESULTS we can safely bypass all these optimizations.\nBut maybe I'm completely wrong. :)\n. > Why do we need REMOTE_STORE_MERGE_RESULTS to even touch anything in the find logic IF the logic already there in step 3 will merge in multiple remote nodes have any relevant data?\nI think I already tried to remove all tolerance find_logic from remote merge in https://github.com/graphite-project/graphite-web/commit/bc18f9730b102cd5ab3ba1409b6760471f4b43ea#diff-57c0c4ffa18833a2e2ea55aab4309ffc - and a result was terrible.\nMaybe I did it wrong. :)\n\nIf you still think we do, then the base case (step 4) needs to be moved outside of the if settings.REMOTE_STORE_MERGE_RESULTS as a catch-all for when the requested interval is newer than data written to disk and the local host is the one that probably has it unwritten in carbon's cache.\n\nYes, maybe we need to try that first.\n\nBefore, with REMOTE_STORE_MERGE_RESULTS, there was a claim that it affected render times if all leaf_nodes were added. Was that substantiated?\n\nIIRC I tried to fix that with https://github.com/graphite-project/graphite-web/commit/71cfdac927465497581756ddedf23a13b6fadba0\n. Fixed in master, I think. Hmmm. And pre-#1730 not working too. And {fo{o}}' and '{f{o}o}' not matching too. Not good...\n. Totally \ud83d\udc4d on this change!\nWill backport to 0.9.x after merge.\n. \ud83d\udc4d \n. \ud83d\udc4d . @gogich77 : which project dropped? Graphite itself is alive and kicking.\nExact Django migration command depends on Django version, current Graphite support Django 1.8.xx - 2.1.xx, which are quite different.\nWe always recommend install everything from source and do not use built-in packages - or use Docker image.. We can reuse @dgryski's info from https://github.com/dgryski/carbonapi/blob/master/COMPATIBILITY.md#functions . LGTM. LGTM @DanCech @obfuscurity ?\nLink to commit - https://github.com/rehevkor5/graphite-web/commit/277bce2633aa934f70efb2ab181616535e09c5d6. It's codecov patch status - https://github.com/codecov/support/wiki/Commit-Coverage-Status\nIn your case you can ignore that.. PR is merged. Hello @yati-sagade,\nWould you mind to rename your function to unique? It will have nice match with grep, exclude etc.. Not sure if a test is needed here. unquote_plus() in known function and fix is quite minor.. Thanks! Merging. @Roguelazer - I'm with @obfuscurity here. Do you have some usecase where his proposal will not work?\nI'm generally \ud83d\udc4d for adding more functions to Graphite but let's not bloat it over the top.. Hm. Looks legit. What do you think, @obfuscurity ?\nOn Sat, 18 Mar 2017 at 17:58, James Brown notifications@github.com wrote:\n\nThe particular metric I'm concerned with this is \"time since queue X was\nempty\". I guess I could divide the series by itself to normalize all\nnon-zeros to one and then integrate that? Presuming 0/0 equals 0, which I\ndon't think is strictly speaking true.\n--\nJames Brown,\ncurrently mobile\n\nOn Mar 18, 2017, at 09:18, Denis Zhdanov notifications@github.com\nwrote:\n@Roguelazer - I'm with @obfuscurity here. Do you have some usecase where\nhis proposal will not work?\nI'm generally \ud83d\udc4d for adding more functions to Graphite but let's not\nbloat it over the top.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1789#issuecomment-287559224,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51tqrAsM0jJIUITsa2g-c8sa__Klkks5rnA0dgaJpZM4LjU4q\n.\n. @Roguelazer - yes, you're completely right, it should be implemented. We already have issue #988 for that, which is added to v1.1.0 milestone. Maybe we can try to add it in same way as graphite-api does\n\nCould you please rebase test file? IMO your functions can be useful for monitoring.\nThanks a lot!. It's a known issue in graphite 0.9.x and it fixed in master now - https://github.com/graphite-project/graphite-web/commit/c6cf8244862ec15bf25b4cb4b01a29e45f59e2eb\nNot sure if possible to backport that to 0.9.x because of a codebase in the master is quite different.\n. When it will be ready, @Pita123 \nNow we still have major issues with it, unfortunately - https://github.com/graphite-project/graphite-web/milestone/2\n. BTW 1.0.0 is at RC-phase now, please test it, if you want, @Pita123 . Cool! LGTM!\nAnyone else wants to check? /cc @graphite-project/committers . Not sure if explanation of the function is needed. Maybe link to wikipedia? https://en.wikipedia.org/wiki/Moving_average#Exponential_moving_average. Could you please rebase, @cmaurer ? I'm gonne merge it if no objections. :). Did you finish, @cmaurer ? Can we review it already?. I'll merge after another review.. Cool! Thanks for reformatting test functions, @cmaurer !\nMerging.. Reverting, tests are become real flacky now.\n```\nFAIL: test_find_view (tests.test_metrics.MetricsTester)\n\nTraceback (most recent call last):\n  File \"/home/travis/build/graphite-project/graphite-web/webapp/tests/test_metrics.py\", line 333, in test_find_view\n    self.assertEqual(int(data[0]['intervals'][0]['start']), ts_minus_sixty_seconds)\nAssertionError: 1487678717 != 1487678716\n``. Tried 3 times, failed 3 of 3 attempts. :(. Reverted. Not sure why it's doing that. Do you have ideas, @cmaurer ?\nOr @iksaif ?. I think you should create new PR, @cmaurer :(\nI see latest commit 15 hours ago. Sorry for that.. LGTM\n\ud83d\udc4d . Maybe you mixed upmasterand0.9.xbranches a bit. Search functionality was quite crappy and was completely removed frommasterbranch. So, https://github.com/graphite-project/graphite-web/commit/3213dc6a18d6ec01e1f14b3125751cb9e9cea0c1 does not containsearch.py` at all.\nBut for 0.9.x you're right. Not sure do we need to fix it though.... Agreed, having sense.\nMerging.. LGTM,  merging. LGTM, but quite massive, it's better to ask another pair of eyes to check.\n/cc @graphite-project/committers ?. Looks perfect for me. @obfuscurity ?. @louwrentius : looks like majority of users using 3rd party dashboards now. Yes, graphite developers are agreed to release 0.10.0 branch as graphite 1.0.0, when it will be ready to release. And, indeed, #1690 is the main blocker now.. Try to use currentAbove(transformNull(...,0),0). Sorry, @ajaycsse, I don't really understand what you trying to achieve. :(\nCould you please explain a bit more?. No examples in config? :(. Looks good on the first sight. /cc @obfuscurity ?\nDid you try to run graphite on Django 1.10, @kviktor ?. Looks good, will merge if no objections follow.. Merging then. Maybe bug in pyparsing? Try to upgrade or downgrade it. . Does 2.0 working fine? Then we need to pin it, instead of 1.5.7. @ybizeul \n\nPyparsing developer came back to me and was very helpful explaining the major difference with multi threading with 2.2 that didn't exists in 2.1, it seems pretty clear 2.1 needs to be pinned.\n\nDid you mean 2.1 and 2.0 - or 2.2 and 2.1 ? \npyparsing 2.2 doesn't exist. Maybe could be simplified by merging movingAvegare() and movingSum()... myeah, good enough for me. \nThanks for the tests!\nAny other of @graphite-project/committers wants to check?. Thanks for your contribution, @benbc !. Cool! Just remove unused imports, it breaks tests.. Agreed - parsing metrics' grammar is not most heavyweight operation on graphite. But I agreed with @obfuscurity - we should benchmark that, at least on separate test.. So, is that enough to merge? @graphite-project/committers ?. Yes, looks like it was overlooked. Thanks!. Absolutely great idea, indeed!\nSame here, did not tested, but looks good.\nDid you tried that code, @DanCech ?. I think that approach is fine. Merging.. @q2dg - no need to install old Django version, vice versa - Django should be updated.\nGraphite 1.1.x requires at least Django 1.8, or newer. . @q2dg : You can install Graphite from source, that's not that hard. Or you can use old Graphite, of course, but then you need to downgrade Django to 1.6, indeed.. @ponikrf: sorry, we can't guarantee that all current and future versions of all Linux distros will be compatible with Graphite. If you installing Graphite not from the package the please install Django and other dependencies also not from package.. Wow! Will try to review this beast on the weekend. Or it's too early?. Looks very nice at the first glance, indeed!\nLet's test that! \ud83e\udd47 . Who's brave to merge this?!\nLet's do that and proceed to 1.0.0rc1 :shipit: . Nice improvement, thanks a lot! Let me (and others) review it?. In general, I'm very \ud83d\udc4d about this patch. Graphite was always quite hungry about disk operations and any optimizations in this area are quite valuable. OTOH this patch makes scandir module as mandatory dependence, which is probably fine because it's a master branch.\nIt's quite easy to make this patch to use os module if scandir is not present - but I'm not sure if that needed.\nWhat other committers say? @obfuscurity @iksaif @DanCech \n/cc @graphite-project/committers . Thanks for reviews, guys! Merging.\nAnd thank @olevchyk for your contribution!. Nice thing, quite clear.\nLGTM \ud83d\udc4d . Cool, looks much better now!. Merging. It's not really clear what relation of this patch to #1818 - is it required for it?\nAlso tests are broken. :(. Strange, test was flaky, done after second run. I think we should do that after merging that massive changes, indeed.. Going forward with RC w/o merging #1818 ?. According to milestone major blocker is #1690 - but we have good news - fix is in #1818  and under review now.\n@kkdk5535 - you can try to test master branch with #1818 applied and share your results  - it will make 1.0 release much faster. \ud83d\ude38 \n. @iksaif : Indeed, 0.9.13-pre1 created many confusions in the past.. Not sure if I influential, but will try to ask my 600 followers to do that.\n@obfuscurity, is it possible for you to ask your 5K followers to do so too, please? :). @obfuscurity,\nGood question, indeed. \nShould we create a separate page or install current master is enough? @iksaif @DanCech \nThen we need to point users to https://graphite.readthedocs.io/en/latest/install-source.html#installing-from-source, and I just found out that download link there is broken - we should point it to Github instead of launchpad - #1880 \ud83d\udc7f . Looks like we have no proper download page. Maybe we should fix that before release too. :(\nOr even asap, and put RC there.. Or maybe just point user on https://graphite.readthedocs.io/en/latest/install-pip.html#installing-in-the-default-location to test the last master - that's the fastest way. Thoughts?. Thanks, @obfuscurity! I retweet with citation: https://twitter.com/deniszh/status/846437032582877184\nAnd congrats on a finishing line of the book, Jason!. @iksaif: Yes, for Graphite web - but we have whisper and carbon too.. I think second one, master is a bleeding edge, and we're cherry-picking\ncommits to release branch and tag releases there\nOn Wed, 29 Mar 2017 at 07:22, Corentin Chary notifications@github.com\nwrote:\n\nRelated to 1.0.x...master\nhttps://github.com/graphite-project/graphite-web/compare/1.0.x...master,\nwhat is our current merge path ?\nMerge PR to 1.0.x if they qualify, then merge back to master ?\nMerge everything to master, and cherry-pick to 1.0.x ?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1823#issuecomment-289985649,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51prn-Ri-UEBOT1l-DO5Wet8tO3tXks5rqep-gaJpZM4L-VCD\n.\n. I wanted to propose Thursday, but Wednesday is even better. Let me sync latest changes and put date first.. I set April 12 as the release date for 1.0.0 and 0.9.16 and push changes to 0.9.x, 1.0.x and master branches.. OK, I tagged releases already, prepared PKG_INFO and going to upload packages to PyPI. A little bit out of schedule, but...\nIt's the last chance to fix something, though.\nAny objections, corrections, etc? @obfuscurity @iksaif @DanCech . OK, everything is finished now. It's almost midnight in CET, so, we're almost on time.\nLet's promote release tomorrow, in twitter and chats.\nAnd let's celebrate, of course!. Thanks, @iksaif ! We're all made this possible.\nJason, could you please make announce in your twitter? Or should I do? /cc @obfuscurity . That's fine IMO. :) Thanks!. I hope that @obfuscurity and @mjulian will mention release in Monitoring weekly too ;). According to code it should work as expected.. You can use transformNull(..,0) to transform Nones to 0.. Hello @mayurmahajan,\nCould you please elaborate this PR? I mean what you trying to achieve here.\n\nThanks! . That's a really strange issue. Which python version do you use? \nGraphite Synthesize has master support and it works fine - https://github.com/obfuscurity/synthesize/blob/feature/graphite-master/install. That's a really strange issue. Which python version do you use? \nGraphite Synthesize has master support and it works fine - https://github.com/obfuscurity/synthesize/blob/feature/graphite-master/install. Using requirements.txt is a documented way of installing Graphite IIRC. We should pin pyparsing version in requirements.txt too.\nBut in general, my knowledge of python packaging is vague. I should carefully read https://stackoverflow.com/questions/8295644/pypi-userwarning-unknown-distribution-option-install-requires and http://python-notes.curiousefficiency.org/en/latest/pep_ideas/core_packaging_api.html \u00af_(\u30c4)/\u00af\nIf someone from @graphite-project/committers knows how to fix that properly - please speak up.. Using requirements.txt is a documented way of installing Graphite IIRC. We should pin pyparsing version in requirements.txt too.\nBut in general, my knowledge of python packaging is vague. I should carefully read https://stackoverflow.com/questions/8295644/pypi-userwarning-unknown-distribution-option-install-requires and http://python-notes.curiousefficiency.org/en/latest/pep_ideas/core_packaging_api.html \u00af_(\u30c4)/\u00af\nIf someone from @graphite-project/committers knows how to fix that properly - please speak up.. Yes, that's a known issue in Python world, you should use virtualenv to create a separate environment for installing Graphite together with other python software. Graphite has strong Django version requirements (>=1.9,<1.9.99), but you do not worry if some software brings Django 1.6 or 1.10, right? We can pin pyparsing version in a same way.\nIn the modern world, when we're using Docker and VMs we're usually installing every piece of software separately...\n. Maybe I'm wrong, but as far as I inderstand install_requires is setuptools options anyway and it's not possible to pin dependencies in setup.py w/o using setuptools.\nSo, I think we should merge #1810 or pin pyparsing version in requirements.txt. . I would say that derivative here is name of math function, and not Graphite function. But semanically you're right ofc.\nMerging.. Merging because it's already reviewed in #1792.\nRan tests couple of times - looks stable now.\n@DanCech - could you please check possible solutions for whisper tests? Thanks!\nThanks for your contribution, @cmaurer !. OK, seems legit. :)\nThanks!. @fauzankhanak - after updating storage schema you should delete old metrics and recreate wsp files.. @NetaNir - I'm agreed that's a bug, but i'm not sure is it applicable to master branch and upcoming 1.0.0 release. 0.9.x is (almost) legacy now.. Because we're not going to release 0.9.x versions anymore, next version will be 1.0.0, released from a current master branch. I think it will be soon, we almost ready to testing RC.\nWe could release 0.9.16 as a final (\"bugfix\") release of 0.9.x branch, though.. Closed as duplicate of https://github.com/graphite-project/graphite-web/issues/1838. No, I was wrong sorry. Misleading title though.\n. Looks like something is wrong with your pip installation:\ndistutils.errors.DistutilsError: Could not find suitable distribution for Requirement.parse('cffi>=1.1.0'). Please see https://github.com/graphite-project/graphite-web/issues/1721#issuecomment-253279181\nRemove system python-openssl package or upgrade it if it's needed - pip install pyopenssl. Hello Mayur,\nCould you please explain a bit why do you using this change?. Any objections to merge this?. @hexian55 - your graphite should work despite error above. That's a different issue then. What's symptoms? What's in the logs?. What's your Graphite and Django versions are?. Strange, it should work. Sorry, I have no good ideas here. The version is OK, migrate command should create all tables.. > Django must be 1.8.15!!! it's ok\nFor 0.9.15 Django 1.9 is not supported, 1.8 is fine.\nFor master Django 1.9 is required.. Is this done now, @iksaif, or should I move this to post-1.1 milestone?. I'm \ud83e\udd37\u200d\u2642\ufe0f about that change. Any opinions? /cc @graphite-project/committers ?. OK, will merge tomorrow if looks good. Ok, merging that. Ident is not so critical in JS, we can fix it later.. Hello Raju,\nWhat's exactly became slow in your Graphite installation?\nYou can try database upgrade using 'python manage.py syncdb', but I'm doubt that it helps. . It's hard to say from this error, maybe your instance is just overloaded. What's your hardware, which load average do you have, how many metrics are you processing? . Looks like everybody proposing to refactor moving* functions, but nobody really wants to do so. :)\n(no offense intended, just fact of life \ud83d\ude08 ). So, I opened issue #1854 for that and added it to v1.1.0 milestone.\nI think we should merge this PR now and proceed with refactor later. Any bjections?\n@DanCech @iksaif @AlexAkulov ?. Yes, we should migrate to taggit, but unfortunately it's not drop-in replacement, we should migrate there.\nFor now installing django-tagging is only way to make this work.. Hello @jstangroome,\nAh, I know what's going on. According to code if you have less then 100 datapoints - it should enable consolidation function - and by default timeseries have 'avg' as consolidation function. \nBut if you have maxDataPoints >=50 valuesPerPoint is rounded to 1,  so, making no consolidation.\nSo, consolidation starts working only when valuesPerPoint becoming less then 50, and consolidation is always \"average\".. So, my point is that is working as expected - maxDataPoints consolidation was 'avg' all the time and I'm not sure then we should fix that (and how to do so w/o breaking compatibility). > The series iterator checks valuesPerPoint and consolidationFunc to apply the chosen sum aggregation to produce the returned data: https://github.com/graphite-project/graphite-web/blob/0.9.15/webapp/graphite/render/datalib.py#L77\nYes, it checks. But I do not see code where consolidation funcion propagates. By default it's 'avg'.\nMaybe I wrong, @Dieterbe also mentioned that runtime consolidation should obey to consolidateBy but I do not see that in the code, at least for maxDataPoints.\n\nAlso, numberOfDataPoints == 100 and the consolidation works correctly for all values of maxDataPoints from 50 to 101 inclusive. Only when maxDataPoints <= 49 do incorrect results occur.\n\nMaybe, you're right. maxDataPoints <= 49 triggering change of valuesPerPoint from 2 to 3 and it can affect nudge logic.\n\nI wonder whether the ability to disable the nudge logic would be a useful render query parameter, especially for JSON queries where the results are often used for purposes other than drawing line charts.\n\nIf it helps - sure, why not. Try to comment out line 159 and check . It will not hurt, I think.. Hi @Himanshu-pupneja ,\nYou can use pickle protocol or AMQP. Are you trying to install Graphite on python 3 ? It will not work, only python 2.7 is supported now. Hello @zhangsm,\nWas it really python 3 or other issue?. It's even worse. @ElsaHuang trying to install https://pypi.python.org/pypi/graphite/0.71 - I do not even know what it is. \n@ElsaHuang - please follow install instruction from https://graphite.readthedocs.io/en/latest/install.html. Does it help, @ElsaHuang ?\n@zhangsm ?. I think it's done, @DanCech ?. Thanks, @ellipses !\nCould I please get more reviews from @graphite-project/committers ?. All Graphite packaging are made by corresponding distro's maintainers. We're not making RPM, DEB, PKG or any other packages except python one.\nYou can use fpm to build own RPM if needed, as described e.g. in this article.. PR for 0.9.x https://github.com/graphite-project/graphite-web/pull/1879 is created.\nAs we should sync docs for both releases let's discuss and fix all issues there first and I port that to master.. Right now I see couple of issues there.\n1. We should release carbonate 1.0.0 or exclude it from release\n2. We should or exclude Ceres and mark it unsupported or include it in 1.0.0 release. 1. Tagged as 1.0.0-pre1 now - https://github.com/graphite-project/carbonate/releases\n2. OK. I set April 12 as the release date for 1.0.0 and 0.9.16 and push changes to 0.9.x, 1.0.x and master branches.. Feel free to merge #1858 or this one. :). Hi @Kenterfie ,\nCould you please show an example of logs which you have? I do not see any enabled by default logging in master.... Hello @Kenterfie,\nDid you find source of excessive logging? As far as I see in the code by default logging should be disabled.. OK, this should be addressed.. @shivagopalan:\nIn current codebase, it's not possible to render multiple graphs on the same SVG or PNG image. Please use dashboard software to achieve this, e.g. Grafana. Grafana also can help you with showing annotations. . Sure! I'm not going to downplay your explanation, @obfuscurity. It really depends on the task, maybe for @shivagopalan's task using dashboard, especially such complicated as Grafana could be clear overkill.. And I don't know how you can color drawAsInfinite() graph btw.. I mean, as @shivagopalan wants - with '' as event and color list.. @cbowman0 : It's only for carbon. metrics?  . Ah, according to code you have REMOTE_STORE_MERGE_RESULTS = False. Right, @cbowman0 ?\nThen I fall back to old code - https://github.com/graphite-project/graphite-web/blame/3b7ccf79b6a47860deeca80aa7fa1b1e3a218c43/webapp/graphite/storage.py#L83-L132 - which is there for 5 years already. Hmmmmm.... PR is prepared, looks good.. ...and merged.. Are you running master on 1.5 ? How's that possible?. We can add TEMPLATE_DIRS fix, if still supported in latest Django ofc.. Yes, I think it's because of Django 1.5. Master including https://github.com/graphite-project/graphite-web/blob/89d21b73b410cc51f4c66fab6ac38c71c00c06fd/webapp/graphite/app_settings.py which contains TEMPLATE variable, which was introduced in Django 1.8. Not sure should we fix that, @cbowman0 . Maybe for 0.9.16 only?. I think, it's the same as https://github.com/graphite-project/graphite-web/issues/1957 and it's fixed now by adding parameter for bootstrap interval. Hi @neeocf - thanks for reporting!\n@iksaif @DanCech - are we OK with simply ignoring IO errors accessing whisper? Or it's better to report it in some way?. How 'bout https://github.com/graphite-project/graphite-web/pull/1878 @DanCech ?. @worrathep :\nWhat's in $GRAPHITE_ROOT?\nPlease try to run PYTHONPATH=/opt/graphite/webapp django-admin.py migrate --settings=graphite.settings --run-syncdb. Yes, you need to install scandir module too, it's required now.. Hello @worrathep,\ndid you succeeded? . \ud83d\udc4d . Yes, known bug. But unfortunately we can't fix that retroactively.\nMaybe we should release 0.9.16 as final release in 0.9.x series.... I should fix documentation for 0.9.x (mentioning 1.0.0 and legacy status) and prepare changelog too.\nWill try to do that this weekend.. PR for 0.9.x https://github.com/graphite-project/graphite-web/pull/1879 is created.\nAs we should sync docs for both releases let's discuss and fix all issues there first and I port that to master.. I set April 12 as the release date for 1.0.0 and 0.9.16 and push changes to 0.9.x, 1.0.x and master branches.. IMO more documentation is better than less documentation. :)\nBut more burden to make all in sync ofc.. OK, I just pushed this - https://github.com/graphite-project/docker-graphite-statsd and this - https://hub.docker.com/r/graphiteapp/graphite-statsd/. I asked @hopsoft's permission to reuse his code.\nAny suggestions / patches etc. are welcome!\n. I want to include small snippet to http://graphiteapp.org/quick-start-guides/\nLike\n```\nTry Graphite in Docker and have it running in seconds:\ndocker run -d\\\n --name graphite\\\n --restart=always\\\n -p 80:80\\\n -p 2003-2004:2003-2004\\\n -p 2023-2024:2023-2024\\\n -p 8125:8125/udp\\\n -p 8126:8126\\\n graphiteapp/graphite-statsd\nCheck https://github.com/graphite-project/docker-graphite-statsd for details.\nThis is portable, fast and easy to use.\n```. PR for website - https://github.com/graphite-project/graphite-project.github.io/pull/35\nNot sure should we push it or mentioning in Install section should be enough.. Yes, we have docker image and it has open repo now - https://github.com/graphite-project/docker-graphite-statsd. Current issues (TODO):\n1. Ceres deprecation (if needed) in master\n2. Add whisper changes in master. OK, I fixed issues here.\nLet's pick a date for release, fix date here, cherry-pick this to 1.0.0 and master and finally make a release!\n@iksaif @DanCech @obfuscurity . We can insert date later, of course. But pick one and make it on-time is more exciting. :)\nI'm fine with both variants. Will port that to master change today.. Ah, will change it a bit.. If I understand correctly it's more about design, right, @obfuscurity ?. Yep, first I also mistakenly decided that only @mattttt @bulletfactory @DanCech have access to graphiteapp.org\nNow we know that everybody has access, not sure if everybody has enough experience with web design to put properly looking download page to graphiteapp.org.... @piotr1212 : if you google \"graphite download\" second result will bring you to http://graphite.wikidot.com/downloads, which say \"Tarballs for the latest release can be downloaded from https://launchpad.net/graphite/+download\"\nAnd https://launchpad.net/graphite/+download has very dated tarballs.\nSo, initial idea was to put tarballs there - but it can be just page with links pointing to tarball in Github.\nBut not sure if that relevant, though.. we probably can, but it will not remove https://launchpad.net/graphite/+download\nand we still using mailing list there (on launchpad). Why not merge all branch?. Cool!\n\ud83d\udc4d . They're counters, indeed. When you running script from command line it's\nobviously not - but when running from collectd values should be\nmonotonically increased. You can enable debug and check all output in\nsystem logs.\nDo you have Graphite connected to Collectd or only rrd?\nOn Fri, 31 Mar 2017 at 20:53, 7 notifications@github.com wrote:\n\nI have a graphite cluster that has 2 shards (data are different on these\ntwo nodes based on consistent hashing). And we also have Master\nGraphite-Web talk with these two shards and merge the results. But.....\nthere are some chance (very low, about 1/60) that Master graphite-web just\nreturn data from one shard. but i can't find any exception from log around\nthis. Does this fix the issue ? any idea ?\n\u2014\nYou are receiving this because you modified the open/close state.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1884#issuecomment-290798369,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51oAl8nbgK_gp7I13mIkoshlfSJjwks5rrUu0gaJpZM4MsgnW\n.\n. Fixed in master and 1.0.x. I'm not sure that an API officially, and I'm not sure that many people using that out there, so I'm OK with that.. POOL_WORKERS_PER_BACKEND = 1, that's fine.\nBut does reducing POOL_WORKERS to 1 making all pool thing useless, @iksaif?\n          . >Remember that the total number of threads is POOL_WORKERS_PER_BACKEND * len(CLUSTER_SERVERS) + POOL_WORKERS.\n\nCool! Maybe it's worth to mention that somewhere in https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/local_settings.py.example#L248-L262 ?. Cool, thanks! \ud83d\udc4d . \ud83d\udc4d \nMaybe it's worth to mention the same on https://github.com/graphite-project/ceres\nWill fix release notes too.. >I'll probably use the same singleton.py in carbon too as there are a few singleton that could benefit from it\nYep, that's a good idea ! \ud83d\udc4d \nThanks!. Hi @UKNC ,\nCould you please elaborate what you mean when saying \"server\"? Is it separate instance of Carbon and Graphite-web, right? Or something else?. Ah, it's a problem with graphite 'client' not server. I can imagine firewall or something like that. You can use tcpdump or ngrep on graphite server to check is really data coming from the problematic server. That's a good article about solving that type of issues - https://theholyjava.wordpress.com/2014/05/05/graphite-shows-metrics-but-no-data-troubleshooting/. Good catch, @chriwill !\n/cc @iksaif . Hi @tonychoe,\nSorry for asking, but how do you know that Oracle using Graphite?. Cherry picking #1879 to 1.0.x branch. Graphite is not designed to use remote CARBONLINK connections, it's an anti-pattern. You should install carbon-caches only on localhost, and include 'em in local_settings.py, and then include remote graphite-web in CLUSTER_SERVERS (and not remote carbon daemons).\nSo, include only local CARBONLINK_HOSTS to each local_settings.py (in same order as in relay), and include all other graphite-web servers (except local) to CLUSTER_SERVERS (in any order).. Sorry, @blafrisch, but I'm not getting your way of thinking or your setup. Could you please elaborate it a little bit more?. REPLICATION_FACTOR should be defined for picking a right instance of carbon-cache for graphite-web. Graphite-web should know only about local caches, it will reach remote caches through corresponding remote graphite-web. I got your idea, but Graphite just not designed to work that way, unfortunately.\nEven if you remove line 40 your graphite-web will not able to match caches properly, effectively disabling them. Your DESTINATIONS should match with CARBONLINK_HOSTS for all servers (port should be different, ofc). You can try that, but then you need to comment out CLUSTER_SERVERS. If your ping between nodes is low - that could work.. Or you can use reference implementation:\n1. Two tier relay - one relay (tier0) send data to 3 hosts, with e.g. RF=2. Second relay (tier1) just taking metrics from tier0 relay and spreading them across 2-4 local caches per host (for scalability).\n2. Graphite-web installed on each host, having in CARBONLINK_HOSTS only local caches (from tier1) and 2 other graphite-web in  CLUSTER_SERVERS.. Just use teir0 relay info to config carbonate - it will work, no need to write anything. . No, and there's no need to having separate data directories for different instances - hashing should care for that, but you can use locking for whisper files to be sure.. Indeed. You're using multiple hosts for redundancy, right? . Thanks, @deejay1 !. @deejay1, yes, please. We can release some hotfix release then.\nRelease often, release early! :). Hello @jvanasco,\n\ndjango-admin.py is located in $GRAPHITE_ROOT/bin/\n\nSorry, but that's not true. django-admin.py is part of Django and it's location depends on your Django installation path - it could be '/usr/bin', '/usr/local/bin' etc. :\n$ which django-admin.py\n/usr/local/bin/django-admin.py\nUsually this path is belong to system path, that's why documentation just ask to run django-admin.py - without full path.. Good catch!. Thanks!. Starting from 0.9.15 graphite-web supporting REMOTE_STORE_MERGE_RESULTS parameter, which will do exactly that - merge results from different cluster nodes.. Yes, it's only included in config example, but the default is True in latest releases.. The official plan - make Graphite better, without breaking it. If you're proposing some radical changes - we can merge it to master and release later, in some 1.1.x release. If your changes fixing some problem and are not bringing troubles - we can even include that in a current stable release.\n. Ah, one more thing - all breaking changes should be configurable by config file and disabled by default.. @wy92 - sorry for bugging you, but did you have a chance to test @DanCech's patch?. LGTM, but is it really gives much gain in performance?. @DanCech - should we merge it to master?. For me it's fine \ud83d\udc4d \n. Yes, Graphite on Windows is totally not-supported. Not even sure if it will work at all. Patches are welcome. :). Fixed in master and 1.0.x branches.. It really depends on how often your netdata metrics coming. Retention 15s:7d means that data should come at least every 15 seconds, otherwise you will get null. . Then you need to have retention = 10s, instead of 15s - this could help. But if \"it might take more from time to time\" - then you will get nulls anyway - that's how Graphite works. . Maybe we should extend transformNull() function to remove nulls using previous data, but in general, it is an antipattern. . Thanks for reporting!\nFixed. Fixed. Thanks, @leoleovich, merged!. First, I want to mention that there are a many ways how you can run Graphite - gunicorn is only one option.\nBut I'm also running Graphite with gunicorn, so, I know an answer. :)\nI'm running it like this (with latest Gunicorn):\nPYTHONPATH=/opt/graphite/webapp\ncd /opt/graphite\n/usr/local/bin/gunicorn wsgi -b127.0.0.1:8000 -w2 --preload .... Thanks @b13n1u !. Hi @leoleovich,\nIt happened after https://github.com/graphite-project/graphite-web/commit/b71f0e01b7b8b90f8124fabcc591c1f77e04ffc3 which was made by @natbraun \nMaybe @iksaif knows more.\n. which bug are you referring, @admin-pro ? if what @leoleovich mentioning - yes, and it's not a bug but changed function semantics in 1.0.x. Codacy warning is nitpicking there.\nLGTM.. More reviewers needed, please.. Thanks, @cbowman0 !. Fixed. Thanks for reporting!. Hello @mageo,\nThanks for your contribution, much appreciated!\nBut we have strong policy about testing functions - would you mind to add tests to your function?\nYou can check examples in webapp/tests/test_functions.py or check other PRs, e.g. https://github.com/graphite-project/graphite-web/pull/1928. Looks good. Codacy is too picky for lambdas, can be ignored.\nLet us review it.. Thanks, @mageo, merged!. @DanCech :\nYes, you're right, I was too fast with merging that, sorry. Usually, I wait a couple of days before merging, but these was approved so fast and looks OK...\nWe can fix it, though - or even revert - it's still in master, which is subject to change.\nCould you please elaborate what you mean \"be more powerful if it used sprintf syntax for building the alias\" btw?. Ah, got your point. Much better now. \ud83d\udc4d . Hello @erikmack,\nAgreed, CLUSTER_SERVERS just silently implicate that it contains part of URL, maybe we should make that explicit (e.g. introduce CLUSTER_SERVERS_URLS), but, for now, I think we just need to fix documentation somewhere around CLUSTER_SERVERS and/or URL_PREFIX\nThanks for reporting that!. Quick googling shows that pure python FFT is very slow and/or limited, everybody recommends using numpy or scipy version.\nI'm also concerned about pluggable functions and fragmenting API a bit - but I do not see how you can implement really powerful math w/o something like numpy.\nI like @drawks idea about setup extras more.. Quick googling shows that pure python FFT is very slow and/or limited, everybody recommends using numpy or scipy version.\nI'm also concerned about pluggable functions and fragmenting API a bit - but I do not see how you can implement really powerful math w/o something like numpy.\nI like @drawks idea about setup extras more.. I'm \ud83d\udc4d for making Graphite UI separate, but \ud83d\udc4e on deprecating it - it still has its userbase, and if you talk about simple browse of metrics tree - it's still VERY useful.\nI'm also not sure about graphite-api, I also had an idea to in-source it to Graphite project first - but I'm not sure should we do that and support two codebases separately, because graphite-api lacks so many parts. \nOTOH - maybe periodic copying functions.py and some patches back and forth is not that tremendous effort?\nBut we're drifting out from my initial topic - should we try to include numpy as optional dependency and try to port ASAP smoothing?. I really suspect that pure python fft will be slow even for relatively small chunks of time series.  But agreed that's pure theoretizing, indeed. We can test and fix pure python part later.\nIf you need help with code or testing, @DanCech - let me know!. https://github.com/deniszh/graphite_asap\nWill try to create a numpy-free version using JS code above, but not sure if it will fast enough for practical usage, probably not.. I'm very \ud83d\udc4d for idea of splitting finders/readers, much appreciated!\nNot sure why lint has so much complains about importing, though.... Grafana's annotations are only showing annotations, it still relies on some storage. Graphite events API is used by many people, it's already there, we should support it in the backward compatible way IMO. We can introduce another API for events, sure, though.\nI don't really like that it's stored in Django database, we should find some pluggable/configurable way where we can store it - OTOH current API is quite verbose and requiring to store bunch of strings data (message and list of tags, tags should be searchable) - so, I doubt that can be easy done with most of KV/NoSQL storages.. @DanCech : Ah, didn't know that. Maybe it's in beta? Annotation documentation says nothing about it, and I see nothing in latest stable Grafana.\nWe're still using normal Graphite annotations (like drawAsInfinite(my.releases)) but still planning a move to Graphite events or ES, because it has tags or ability to add some text mark to it.\nI'm not sure what we going to do here. \nNormal Graphite annotations are very limited but working for every backend, even now.\nEvents API looks great but has a burden of SQL storage.\nWe can introduce different storage, but it will require that storage instead of SQL for events.\nIdeal solution IMO - left Events API as is, but make it works with pluggable backends, but not Django related (e.g. most popular, MySQL / PostgreSQL / Cassandra / Riak / Mongo, with migration tool from current backend).\nIf we're OK to proceed - I can spend some time on it, we would like to use something similar.. OTOH - doing so many backends w/o django db abstraction layer could be... not quite easy task.\nI do not know any existing sql/nosql db abstraction layer for python.... @hapx101 - are you using master? Try to use 1.0.x branch\n@iksaif, could you please check ?. @yesoreyeram - did #1946 fix problem for you?. Fixed now.. Looks great, thanks!. \ud83d\udc4d . Hello @pengyusong,\nThanks for reporting! I remember this or a similar bug. What's your version of graphite and pyparsing?. Yep, we had similar issues in https://github.com/graphite-project/graphite-web/issues/1806 and it was fixed in https://github.com/graphite-project/graphite-web/pull/1810\n. Could you please check how pyparsing 2.1 and/or pyparsing 1.5.x working?. OK, thanks! Then we should fix that, indeed. \nSetting lock around is not an option, right?\nAsking for committers opinion here. /cc @iksaif @DanCech @graphite-project/committers . Yep, what @DanCech says.  What's a problem with maxDataPoints? it's not mandatory, just not use it if not needed.. >I don't with it's for the width of the graph. Isn't this controlled by from/until ? For me it was just to avoid drowing / tranmitting points that would not be useful.\nNo, from/until will limit your timespan, but not a number of data points. If you have from=-1y and second resolution (without aggregation ofc) you'll get json with 31536000 points, which definitely kill your browser, which will try to fit it into 1000 pixels on screen width anyway. maxDataPoints was introduced in graphite-api for work with Grafana because of that issue.\n\nThe current code doesn't seem to work if there is an empty timeseries.\n\nWhy? Could you please elaborate a bit more? The current code/logic is working in Graphite starting from 0.9.14, graphite-api and carbonapi, nobody complaint about much about it before.\nWe can introduce another smoothing algorithm for maxDataPoints ofc, but IMO it should work as intended - return not more than maxDataPoints. :). Looks like it's caused by https://github.com/graphite-project/graphite-web/commit/2e940a34d9aae0a27d1629dd1db8255763aeef51\nwhich was backport of fix for issue #220 from a master.\nSo, I think version 1.0.x also contain this. \nNot sure how that patch could cause that, though. series[i] is defined, otherwise it should fail on line 1975 instead, so it's caused by bad upperBand[i] - but it was there before and reverting this patch doesn't help.\nAnd I do not see any other changes in holtWintersAberration.\nSo, I'm suspect that you just getting NULLs in your data when getting older data, maybe because of wrong aggregation policy. Could you please check that? just remove holtWintersAberration and check rest of your data.. Hmm... Could you please check if reverting https://github.com/graphite-project/graphite-web/commit/2e940a34d9aae0a27d1629dd1db8255763aeef51 helps then?\nIf not - then it's some other change, probably in  groupByNode() then.. Thanks, looks like the problem lies somewhere else. I checked scaleToSeconds() and groupByNode() functions too - there was no changes there in 0.9.16 either.. Ah, you're right, completely missed that.\nIt's coming from https://github.com/graphite-project/graphite-web/pull/1523 by @arielnh56 \nHe mentioned that there's problem with 7 days selection, but 9 days should not be affected:\n\"holtWinters functions old version had a discontinuity around a 7 day\ntime selection. 167 hours significantly different from 169 hours. This\ndiscontinuity is now gone, but the results for shorter time windows are\nnow significantly changed. I'm not a statistician but I suspect these\nare intended for long time windows and the discontinuity was a bug\nbrought on by mixing two whisper ranges together.\". If previewSeconds is a problem - we can make it configurable, it's easy and backward compatible.\nA key component of Holt-Winters method is seasonality, so, I think 7 days was chosen as a minimal period which should have good seasonality.. PS: Very nice explanation of what Holt-Winters  is in Grisha's blog - Part I, Part II, Part III. @bemeyert : Let me know if changing previewSeconds helps in your particular case.\nI'll submit PR for making it configurable.. Yep, will do exactly that.. Change should be as simple as https://github.com/graphite-project/graphite-web/pull/1961\nEasy portable to 1.0.x and even 0.9.x.\nLet me check it first, though.. Or it's better to make it seconds instead of days? @bemeyert @DanCech (fractional days will work, of course, it only matters of convenience).. @bemeyert : maybe hours is better then?. @bemeyert - does that looks better - https://github.com/graphite-project/graphite-web/pull/1965 ?\nIt will accept any Graphite compatible time string as bootstrap parameter. Default is '7d', you can use e.g. '15h'.. @bemeyert - thanks! Let me know is it useful, I'll cherry-pick it to 1.0.x and 0.9.x branches.. @bemeyert Will cherry pick it now to 1.0.x branch, so, release 1.0.2, I think. Together with Django 1.8 support, (which should be in the same release), it should make porting it to RHEL/CentOS 7 quite easy.. Yes, 0.9.16 was final. I can backport patch to 0.9.x branch, just for sake of completeness, but we're not going to release it.  . Done - merged to master and 1.0.x branch, so, should be included in next release.. Looks good, @iksaif !\nPlease go on! \ud83d\udc4d . I think we should release 1.1.0 after such massive changes btw. An @piotr1212 's python3 compatibility work.. No, not yet. Will try tomorrow. But proper test requires quite a bit cluster, it's hard to cover all corner cases with simple test setup.... Hi @brutasse,\nGreat stuff, as usual! \ud83d\udc4d \nI'm very \ud83d\udc4d for supporting Django LTS and dropping 1.9 after next release. . Merging, if no objections.. Agreed, will make a fix now.. @DanCech, https://github.com/graphite-project/graphite-web/pull/1964 - please review. will make another change, with settings instead of days.. @DanCech: Ah, very good suggestion, thanks!. Thanks!\nShould be backported to 1.0.x too.... Thanks!\nShould be backported to 1.0.x too.... Yes, agreed. Now all logic in HW bootstrap is based on seconds, so, decided to not put more complexity there, at least now.. It's fixing the issue, so, merging.. It slipped out between releases - too old for 1.0.x but too new for 0.9.x.\nI'm totally \ud83d\udc4d for supporting it but have no idea TBH what need to be done.\nMaybe @brutasse knows?. But does it work with Django 1.8 at all btw?\nThis should be ported to 1.0.x branch then.. I think we can merge it.\nNot sure do we need to backport to 1.0.x and release 1.0.2 or release 1.1.0 instead?\nMaybe 1.0.2 is better, master is still very fresh.. Generally looks good, but tests are still broken\n. Yes, that's flaky one, just need to be restarted.. Indeed - I would prefer 1.1.0 release, but I want to merge tagging support\nfirst. Sorry for being quiet lately, I'm still on vacation.\nOn Mon, 28 Aug 2017 at 17:46, Dan Cech notifications@github.com wrote:\n\n@deniszh https://github.com/deniszh After thinking about this a little\nmore I think the time may have come to release 1.1.x, since there are a\nbunch of other changes in master that it would be good to get into a stable\nrelease. What do you think?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1970#issuecomment-325391795,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51ofOnju7yNvkPt2OlTey2cF2dL3pks5scuDIgaJpZM4OAgeE\n.\n. Try to install like\npip install --prefix=/opt/graphite --install-lib=/opt/graphite/lib graphite-web. Did it help, @mizhdi ?. Try the latest master. Should be fixed now. Or you can use 1.0.x branch too.. >Any idea what causes this?\n\nprobably changes in pip.\n\nIt also doesn't seem to work 100% of the time.\n\nCould you please explain what exactly broken?\n. No need to open more than one issue. Merging. Release is done - https://graphite.readthedocs.io/en/latest/releases/1_0_2.html, \nrelease documentation also updated - https://github.com/graphite-project/graphite-summit/wiki. Simple cherry-pick of my commits for #1965. Merging. Looks good, will merge tomorrow and backport to 1.0.x. merging. Ah, good to know, thanks!\n. Hello @fidergo-stephane-gourichon,\nThanks for reporting this, but dashboard functionality of graphite-web is not really on top priority right now. Most of the people are using or simple graphs or 3rd party dashboards like 'Grafana'. If you have a proper fix - we would like to include it to current code, but I suspect that proper solution will require a quite big effort - maybe we can fix it later when we'll migrate to python3 and will have proper  UTF-8 support. . You can use django-admin or python -m django instead  - see  https://docs.djangoproject.com/en/1.11/ref/django-admin/\nPlease also note that 1.0.1 supports Django 1.9, upcoming 1.0.2 should support 1.8 (current LTS) too and master supporting 1.8-1.11\nI.e. just updating Graphite version is not enough, dependencies are also updated.. Looks like you're trying to build docker image. You can use official one now - https://hub.docker.com/r/graphiteapp/graphite-statsd/. Hello @scraly ,\nWhat's version of your Graphite?. Try to ask '/version/' on Graphite URL.\nE.g.\n$ curl https://graphite.my.org:8080/version/\n0.9.12. Grafana is not 'using' Graphite, it's separate dashboard. Somebody installed Graphite on your system and connect Grafana to it, but that's completely separate projects.. You can find Graphite url in Datasources list in Grafana.. I would say that's broken whisper file.  Please check the size of your whisper files, do you see some really small files, less than 1KB? Maybe disk with whisper file was full some time ago? (it's effectively corrupt whisper files). Unfortunately, not. According to code it's really looks like broken wsp files, though. Hello @rajats105,\nCould you please show contents of storage-schemas.conf and storage-aggregation.conf ?. Unfortunately,  I see no easy explanation for this behaviour, having configs above. Did you change retentions and/or storage schemas after creating metrics?\nCould you please show output of whisper-info.py collectd/production/ec2/ip-w-x-y-z/cpu-0.wsp command?. Sorry, then I have no explanation. Maybe we have some bug in sumserieswithwildcards() but I can't reproduce it.. Which version of Graphite are you using? 1.0.x supports only Django 1.9, not 1.10\nMaster branch should support 1.10 though. something wrong with your local_settings.py. Could you please show it (w/o comments).. Try to put timezone in quotes - \nTIME_ZONE = 'America/New_York'\nNot sure if related, though - but all strings are quoted in my config.. Merging that to master and cherry-picking to 1.0.x\nLast call for changes in 1.0.2!\n/cc @graphite-project/committers . Sorry, completely missed this when picking patches for 1.0.2, merging.. Which Graphite version are you using? Looks like quite old, new versions are using '/static' prefix.\nFor modern version, you need to set e.g.\nSTATIC_URL = '/static/'\nURL_PREFIX = '/graphite'\nThen you need to make your webserver to serve static files from static_url above, e.g.\nAlias /static/ /opt/graphite/static\n. That's not a Graphite issue per se, that's a Django issue. django_content_type is not specific Graphite table, that's common Django table. So, you need to skip migrations, but not all, just initial - you need to use --fake-initial instead of --fake - check https://stackoverflow.com/a/29760818/1139639 and https://docs.djangoproject.com/en/1.8/ref/django-admin/#django-admin-option---fake-initial\n. That's not a Graphite issue per se, that's a Django issue. django_content_type is not specific Graphite table, that's common Django table. So, you need to skip migrations, but not all, just initial - you need to use --fake-initial instead of --fake - check https://stackoverflow.com/a/29760818/1139639 and https://docs.djangoproject.com/en/1.8/ref/django-admin/#django-admin-option---fake-initial\n. The real error is quite hidden, it is:\nc/_cffi_backend.c:15:17: fatal error: ffi.h: No such file or directory\nTry to install dev package - libffi-dev on Debian/Ubuntu.. Cool, good catch!\nNot sure about the fix, though. Now setting FIND_CACHE_DURATION=0 will move start and end time to -1 second. It's propably harmless, but not needed side effect IMO. @DanCech @iksaif ?. Oh, stupid me,  mod 1 is 0, not 1 -  indeed! \nputs on sackcloth and ashes. Merging then, to hide my shame! :). Are you sure are you using graphite? IIRC \"data points outside time range\" is influxdb error - e.g.\n https://github.com/grafana/grafana/issues/1913\nI never saw similar error in Graphite graphs on Grafana.. Thanks, @msk610, please check my comments. Thanks for your contribution!. @msk610 :\nNow it still complaining about events function: \ngraphite-web/webapp/graphite/render/functions.py:docstring of graphite.render.functions.events:5: ERROR: Unexpected indentation.\nPlease restore events() function as it was before - wil all spaces and empty lines - they're important.. Going to merge that too. More reviews, please?\n. Ahem... I doubt that fix is that simple. I'm not aware of what get_real_metric_path() doing though. @iksaif @DanCech ?. Ah, cool. Thanks, @cbowman0 ! Maybe you could make a fix then?. The line above is a part of logtime decorator which is calling from many places. But it needs to be fixed, though.. @replay - https://github.com/graphite-project/graphite-web/commit/e5aa02407f9750fd9962d5b100c47beb5aa556a5 ?. I thought we included it into 1.0.2 though.... No looks like we didn't. sigh. Yes, let's release it in 1.1.0 :)\nOn Fri, 25 Aug 2017 at 18:24, Dan Cech notifications@github.com wrote:\n\n@deniszh https://github.com/deniszh this is the same issue I'm\nreferring to in #1970\nhttps://github.com/graphite-project/graphite-web/pull/1970\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2015#issuecomment-324969517,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51jTKcH45p68tj3S0mEEKxtsVU1swks5sbvUigaJpZM4O0kox\n.\n. IIRC Events API has no support in UI, it provides only API. Grafana has\nEvents support, but I'm not sure if it renders URLs in info field.\nUnfortunately I'm AFK for some time now, but you can test it too.\n\nOn Thu, 24 Aug 2017 at 14:31, Paul Gilligan notifications@github.com\nwrote:\n\nI have a use case where a typical systems transactions might fail during\nprocesses and there is a URL you could click to retry the transaction.\nHad a good read through the docs but it is not clear how an event sent to\ngraphite could contain a URL that could be \"clicked\" from the graphite UI\nand hence re-try the transaction (jumps away from Graphite's windows maybe).\nAny thoughts ideas?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2016, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51mDy_pxxdwkscMFyMcZmgAsjwgUcks5sbW0JgaJpZM4PBT-W\n.\n. Yes, according to https://github.com/grafana/grafana/issues/1588 Grafana supporting that now.. Hi @DavidCallahan,\nDo you have wsgi.py file in /opt/graphite/webapp ?\nIf not - you can copy it from https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/wsgi.py . gunicorn is not part of Graphite stack - why would we pin its version?\nLooks like latest gunicorn became not compatible with python 2.6 - but latest Graphite requires python 2.7 too.... Ugh... Sorry for that. Gunicorn is not a part of stack in any way (because you can run Graphite without it). Will remove in from requirements asap.. Ah, nvm. It's pip problem - https://github.com/benoitc/gunicorn/issues/982\nAnd gunicorn even working after that btw, so it's purely cosmetic.\nBut gunicorn should be removed from requirements anyway, though.. After some thinking I would prefer to left it like this - it's a minor cosmetic issue with old pip/gunicorn, not sure if it's worth to spend time on it.. Yes, switching to carbon-c-relay and fnv1a_ch should help.. CARBONLINK_HASHING_TYPE will only change \"carbonlink\" settings, i.e. how graphite-web looking for  graphite caches. We sharing some code between carbon and graphite-web obviously, but we have no support for fnv1a hash in default (python) version of relay. \nSo, fnv1a_ch hashing will work only with \"carbon-c-relay\" and not with normal relay.. Also please note that \"fnv1a_ch\" has was introduced in graphite 1.0.0 and it will not work in older versions.. > ok, can you please have a look above traceback, is it Django not working with FIPS ?\nI am seeing this error when I try to run initialization on freshmachine:\nPYTHONPATH=$GRAPHITE_ROOT/webapp/ /usr/lib/python2.7/site-packages/django/bin/django-admin.py migrate --settings=graphite.settings --run-syncdb\n\nAh, that's completely different issue. This is a part of Django, and not a graphite. Then you probably need to run this command on different server and copy graphite.db file on new server. I traced Django code - hashlib.md5() is there for all supported versions.\n\nis there any document on how to configure carbon-relay\n\nYes, on carbon-c-realy repo - https://github.com/grobian/carbon-c-relay. I'm \ud83d\ude10 about this change - but weeks with some specified dow could be useful. @luigiberrettini - could you please clean up white spaces in lines below? Lint is complaining - \n/home/travis/build/graphite-project/graphite-web/webapp/graphite/render/attime.py:165:50: W292 no newline at end of file\n/home/travis/build/graphite-project/graphite-web/webapp/graphite/render/functions.py:3725:1: W293 blank line contains whitespace\nThanks a lot!. @luigiberrettini - yes, looks like flaky test. It's green now.\nMaybe someone else wants to look? @DanCech @iksaif ?. @bartekgb ,\nThanks for reporting! \nAre you sure that no more test/x directories exist, only test/1 ?. Yes, that's clear - but dirs are empty (except 1), right?. Thanks a lot, @zasca !\nCould you please maybe make a PR from it?. Just want to note that Graphite officially doesn't support unicode metrics, only ASCII - https://github.com/graphite-project/graphite-web/issues/243\nBut your efforts are appreciated, of course.\nAnd, you're right, grammar.py is completely not ready for utf-8 - https://github.com/graphite-project/graphite-web/blob/400057dd6e3b8aa0d7116c4f2b82175236219da2/webapp/graphite/render/grammar.py. @piotr1212 - could you please review it?. Yes, that's what I remembered. Thanks!. It was not merged, just closed. Probably not ready yet.. \ud83d\udc4d LGTM\nThanks!. Huh? Did we lose functions on master again? wtf...\nDo you know something  about it, @cbowman0 ?\nOr anydody? @iksaif @DanCech ?\nTests for docs looks perfect... sigh. Strange. I just tried to rebuild docs locally - it worked fine:\n```\n[dzhdanov:~/git/graphite-web] master+ \u00b1 mkdir docs-html\n[dzhdanov:~/git/graphite-web] master+ \u00b1 pip2 install Sphinx sphinx_rtd_theme scandir\nRequirement already satisfied: Sphinx in /usr/local/lib/python2.7/site-packages\nRequirement already satisfied: sphinx_rtd_theme in /usr/local/lib/python2.7/site-packages\nRequirement already satisfied: scandir in /usr/local/lib/python2.7/site-packages\nRequirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python2.7/site-packages (from Sphinx)\nRequirement already satisfied: setuptools in /usr/local/lib/python2.7/site-packages (from Sphinx)\nRequirement already satisfied: imagesize in /usr/local/lib/python2.7/site-packages (from Sphinx)\nRequirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python2.7/site-packages (from Sphinx)\nRequirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python2.7/site-packages (from Sphinx)\nRequirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python2.7/site-packages (from Sphinx)\nRequirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python2.7/site-packages (from Sphinx)\nRequirement already satisfied: docutils>=0.11 in /usr/local/lib/python2.7/site-packages (from Sphinx)\nRequirement already satisfied: typing; python_version < \"3.5\" in /usr/local/lib/python2.7/site-packages (from Sphinx)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/site-packages (from Sphinx)\nRequirement already satisfied: requests>=2.0.0 in /usr/local/lib/python2.7/site-packages (from Sphinx)\nRequirement already satisfied: Pygments>=2.0 in /usr/local/lib/python2.7/site-packages (from Sphinx)\nRequirement already satisfied: pytz>=0a in /usr/local/lib/python2.7/site-packages (from babel!=2.0,>=1.3->Sphinx)\nRequirement already satisfied: MarkupSafe in /usr/local/lib/python2.7/site-packages (from Jinja2>=2.3->Sphinx)\n[dzhdanov:~/git/graphite-web] master+ \u00b1 sphinx-build -b html docs docs-html\nRunning Sphinx v1.6.3\nCould not import graphite.local_settings, using defaults!\n/Users/dzhdanov/git/graphite-web/webapp/graphite/settings.py:286: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security\n  warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\nWARNING: while setting up extension conf.py: directive 'autofunction' is already registered, it will be overridden\nloading pickled environment... not yet created\nloading intersphinx inventory from http://docs.python.org/objects.inv...\nintersphinx inventory has moved: http://docs.python.org/objects.inv -> https://docs.python.org/2/objects.inv\nbuilding [mo]: targets for 0 po files that are out of date\nbuilding [html]: targets for 47 source files that are out of date\nupdating environment: 47 added, 0 changed, 0 removed\nreading sources... [100%] who-is-using\nlooking for now-outdated files... none found\npickling environment... done\nchecking consistency... done\npreparing documents... done\nwriting output... [100%] who-is-using\ngenerating indices... genindex py-modindex\nwriting additional pages... search\ncopying images... [100%] ../webapp/content/img/overview.png\ncopying static files... done\ncopying extra files... done\ndumping search index in English (code: en) ... done\ndumping object inventory... done\nbuild succeeded, 1 warning.\n[dzhdanov:~/git/graphite-web] master+* \u00b1 ls -al docs-html/functions.html\n-rw-r--r--  1 dzhdanov  dzhdanov  115571 Sep 10 21:34 docs-html/functions.html\n```. Travis job for docs looks fine - https://travis-ci.org/graphite-project/graphite-web/jobs/273504741\nBut TBH I have no idea howit comes to https://graphite.readthedocs.io/\nWill ask Jason :). >Update : graphite-web seems to work when I use the following URL : curl http://localhost:80/graphite/\nA trailing slash seems to be needed, no error log is generated this way...\nIIRC it's a matter of your web server setup, Graphite is fine with either URL.. You can use e.g. RedirectMatch directive:\nRedirectMatch ^/graphite$ /graphite/\n. @YouriAndropov : What's issue? You said earlier that trailing slash is needed, why do you trying now without it?. If you mean access errors then try to add to config\n<Directory /opt/graphite/static/>\n            Require all granted\n        </Directory>. I think Grafana should follow redirects but that's better to ask on Grafana issues.. Unfortunately,  it's not possible in the current codebase. TBH I don't know why from and until parameters present in find call. Theoretically, they can be used in some external finder, but now nor Whisper nor Ceres storage not testing file creation date and not comparing from with it. Also, it will be quite hard to calculate until date for a particular metric - you need to check all archives and find \"data edge\" - it's can be quite complicated and resource intensive.\nSo, for now, I would say - it's only present for external storages, in current code it has no meaning.. You can find that info in carbon.log (looks like ' [creates] creating database file /opt/graphite/storage/whisper/blah.wsp (archive=[(60, 525600), (600, 518400)] xff=None agg=None)') and you can use something like https://github.com/eBayClassifiedsGroup/graphite-news to expose this info.. I'm not sure that RemoteFinder's way is portable enough. Probably we need to backport fetch_multi() from graphite-api.. No contradiction, just evolution :)\nSeriously speaking it's just how it was implemented - initial implementation picked only first result from remote servers, which make scaling up quite hard. Then new behavior was implemented, but as any breaking change in minor release it was disabled by default in 0.9.14\nBut in 1.0.0 release this feature is enabled by default (still can be disabled btw) but nobody fixed docs in config file.. Thanks, LGTM.\nMore reviews? /cc @DanCech @iksaif ?. Stable release is currently 1.0.2, you can use release from Github or 1.0.x branch. You can also use master, it's usually stable enough to use it in production too.\nPlease note that graphite.wsgi do not contain graphite.metrics.search for long time already (starting 1.0.0 release).. But in general, I'm very \ud83d\udc4d \ud83d\udc4d \ud83d\udc4d about that change, very nice addition to function syntax.\nLet's properly test and document this first, though.. Also not sure if ctime/mtime is reliable enough. I would not rely on that for making requests anyway. If you just need to get this info for analytical purposes you can use what I proposed in https://github.com/graphite-project/graphite-web/issues/2036#issuecomment-329467099. @DanCech : sure, looks interesting!. Still wondering when we should 'cut' 1.1.0\nYou still have something to add, @DanCech, right?. @DanCech : And documentation for new finders, right? \ud83d\ude09 \nThanks!. Just FYI for curious - we're tracking progress to 1.1.0 in corresponding milestone - https://github.com/graphite-project/graphite-web/issues?q=is%3Aopen+is%3Aissue+milestone%3A1.1.0. I can cut 1.1.0-rc1 in Github releases very easy, but I'm not really sure that we need to put it (non-final version, I mean) on pip/documentation page etc. After that, it becomes full-fledged release anyway, not sure why it needs that -rc tag then.\nAlso, documentation is important ofc, but that's a minor thing. Changing compatibility of tag implementation (e.g. after https://github.com/graphite-project/graphite-web/issues/2122 ) - that's completely different story. Implementing federated TagDB - that's a completely new thing again.\nIMO we should fully use semver now and make releases more often, incrementing subversion not only for bugfixes, but for some new feature too.\nSo, 1.1.0 is a major milestone, with tagging and functions chaining, but we can put federated tagdb  to 1.1.1, and handful of other small things - to 1.1.2 etc. Put them to issues and let's have a proper milestone then.\nAny thoughts? @iksaif @obfuscurity @cbowman0 ?\n. PS: Also please note that we should release not only graphite-web but carbon and whisper too.\nTagging 1.1.0-pre1 now.. Also, for proper release I need to prepare release notes, changelog etc. Will not include that in -pre release yet.. I just tagged 1.1.0-pre1 and pushed it to docker hub too.. OK, will release pre2 today. But I need to compile at least minimal \"what's new\" to advertise it properly. :). Thanks, Dan, pushed 1.1.0-pre2, let's try to advertise it tomorrow!. Oops, overlooked boo-boo with latest carbon commit in 1.1.0-pre2. Fixed and released 1.1.0-pre3 instead.. 1.1.0-rc released as \"final release candidate\". So, I'm going to \n- prepare release notes / what's new\n- fork 1.1.0 as a separate branch\n- upgrade component's versions in master\n@DanCech - it's a good time to finish plugin dev docs ;). @admin-pro ,\nDo you have any specific problems during upgrade or just interest in the process?\nIt's quite dependant on your current OS and installation type (from packgage or source, using virtualenv or not).. Sorry for not updating this issue btw.\nCurrent status - 1.1.1 release is done, I want to backport some patches from master and release 1.1.2. @admin-pro,\nWhat's your Django version and how it's installed? Graphite 1.1.x requires Django 1.8 or newer. If you do not have it - then you can upgrade only to 0.9.16 and not on 1.0.x/1.1.x unfortuntely. Yes, but if you have Django installed using package upgrade is not trivial - you can't easily replace newer django from source in this case - https://unix.stackexchange.com/questions/278468/should-we-let-python-pip-overwrite-modules-that-were-installed-by-rpm. @admin-pro,\nFrom the source you can easily upgrade that, indeed. Please note that from yum you will install only v0.9.16 from EPEL repo, newer is only can be installed from source, because of dependencies.. I would recommend using Nginx + Gunicorn instead. You can peek how it's done in official Graphite image - https://github.com/graphite-project/docker-graphite-statsd or 3rd paty images, like - https://hub.docker.com/r/yesoreyeram/graphite-nginx-gunicorn/~/dockerfile/.\nIt's mostly simple proxying Nginx (like https://gist.github.com/bhang/3003978) and gunicorn runner (like https://github.com/graphite-project/docker-graphite-statsd/blob/master/conf/etc/service/graphite/run)\nBut you probably right, we need to collect deployment docs in something like GraphiteAPI has - https://github.com/brutasse/graphite-api/blob/master/docs/deployment.rst. @admin-pro,\nSomething like pip freeze | grep graphite should give you version. Also try go to http:///version/. @admin-pro \nYour command looks OK, but of course, not me nor anyone else could give you any guarantees what's happened when you run it on your production server. It probably will run fine, but it can destroy all your config and data or anything else.\nTry testing server, make backups and good luck with the upgrade!. @Serphentas,\nThanks! Documentation is in the same repo - https://github.com/graphite-project/graphite-web/tree/master/docs. I'm going to release bugfix 1.1.2 release soon BTW, will open a separate issue for that.. IIRC populate() isn't reentrant is not a real problem, it' just a symptom. You can try to use special wsgi file for find proper error - https://stackoverflow.com/a/30968197/1139639. PS: Also this happening when you mixed up Apache and mod_uwsgi versions - they're should be the same.. Master Graphite (and current stable release too) support Django 1.8 - 1.11, 1.6 is too old.\n. @DanCech \n\nThe latest commit changes the behavior when there are entries in seriesList that don't have a corresponding series in total and vice-versa. Previously they were ignored, now they produce output series that have MISSING in the corresponding part of their name and values that are all None. They can be filtered out if desired by piping through |exclude('MISSING')\n\nNice feature! But maybe we should mention that in docs section?. @DanCech : Change is good, but I think you meant #771 and not #2040 ?. Could you please elaborate what is \"maxDataPoints=1\", and why it could be processed that way?\nI think this should be included also in documentation - https://graphite.readthedocs.io/en/latest/render_api.html#maxdatapoints. OK, agreed. Does it really happens only for maxDataPoints=1 or just for low maxDataPoints? \n. \ud83d\udc4d ,  but I'm wondering should we include some documentation for Redis tagdb or HTTP tagdb in local_settings.py.example or documentation maybe?. For testing - yes, we can mock HTTP tagdb separately, but it will be not very easy and not sure if have any sense.... Something is wrong with your fonts. Which OS are you using?. Nope, it's only matter of font packages. Please try to install dejavu-sans-fonts & dejavu-serif-fonts packages.. PS: \n\nGraphs rendered by already installed older versions are displayed correctly.\n\nDid I understand correctly that old version is installed on another server, right?. Kicked stuck test.\nAgreed, the previous check was quite naive.. I'm not a big Django expert, but I'm suspecting that it will be quite hard to implement behavior which described above. It's much easier to tune web server for a bigger timeout. @cbowman0 @DanCech ?. With all respect, +1 will not help to fix the issue, but PR will. Documentation is located in this repo too - https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/functions.py#L2150-L2163 :). Sorry, if I sounding harsh, it was not intended. Just added info if someone wants to make a contribution to the project. Documentation is the best thing to start.\nIf no volunteers - I'll do that later, but usually I'm retracting myself from doing documentation because I'm not a native English speaker.. Hi Caroline,\nThanks a lot for using Graphite, we're here to help you. :)\nmaxValue was there from start, but for some reason, it was never documented. I'm not aware of it either, need to check code to understand it completely.. Change is great, but codacy and lint are complaining about unused imports.... Thanks!. Thanks, @olevchyk !\nI though we fixed that issue in https://github.com/graphite-project/graphite-web/pull/1337 already, but looks not. Please review that - /cc @iksaif @DanCech @cbowman0 . That's much better, thank you!. I think it's done now, right, @DanCech ?. Ah, OK. I just thought its already done in the current implementation.. Need to spend more time on review but in general is very very \ud83d\udc4d \n. Maybe we should release 1.1 before merging this and then merge it to master and give users some time to change 3rd party plugins?. Well, all existing plugins are used \"1.0.x\" plugin api AKA \"master\" plugin api, because there's no other api exist, right? :)\nI know at least 3 more-less actively used in a wild - https://github.com/InfluxGraph/influxgraph, https://github.com/brutasse/graphite-cyanite and https://github.com/yandex/graphouse/tree/master/src/main/pySources and a bunch of more-less outdated. But probably not a big deal, though.\nI mostly worried about documentation here. This - https://graphite.readthedocs.io/en/latest/storage-backends.html - is mostly outdated now, right? Can we fix that before releasing this? \nAnd or we need some working custom finder there, probably big-graphite is enough.. >At this point I think the best way to move forward is to update the docs so people have a guide to implementing the more efficient fetch path, but use the code in this branch as-is.\nIf old plugins will work - then that's great, let's proceed! \ud83d\udc4d \nWe never had fetch_multi support anyway.\nThanks for digging that, @DanCech !. \"Documentation\" link goes to http://graphite.readthedocs.io/, which automatically redirects to http://graphite.readthedocs.io/en/latest/ - that's how ReadTheDocs works.\nWe have current version variable WEBAPP_VERSION in https://github.com/graphite-project/graphite-web/blob/8ba0cb0c96beefbeffe71d5fe792505f837fad2f/webapp/graphite/settings.py#L29\nWe just need to properly fix DOCUMENTATION_URL in https://github.com/graphite-project/graphite-web/blob/8ba0cb0c96beefbeffe71d5fe792505f837fad2f/webapp/graphite/settings.py#L116 to support \"latest\" and releases.. Thanks for fixing that, @wridgers !\nMerging.. Probably your disk is overloaded if fetching 200 series takes 412 seconds.... Unfortunately current graphite-web doesn't support non-ascii metrics. Maybe we can make a quick fix for standart finder? /cc @piotr1212 @brutasse . Hi @innerx,\nProblem is not in python code but in the environment.\nWhat's your OS and cairo package version?. Yes, @piotr1212 says about http:/<GRAPHITE>/dashboard/ (https://github.com/graphite-project/graphite-web/tree/master/webapp/graphite/dashboard in sources), not about tree view, right?\nI also not used that, but I think we have a some users who still using that. Probably, some \"dashboard-to-grafana\" migration script could help.\n. IIRC for example @cbowman0 using dashboards.. Ah, good catch. Thanks!. Should be closed in favor of #2121, right?. Wow, cool stuff!. Same here. Looks like a great plan! \ud83d\udc4d . Hi @kira510,\nSorry for late response, I missed your question somehow.\nFirst, it's quite hard to track connections in graphite cluster, unfortunately. You can enable access logging and you will see metrics and source IP of the requestor in the log.\nFor the second question - yes, what @wolfzhaoshuai said. Graphite has blacklist feature too, but for cluster-wide blocking, you may prefer do it on relay level. 3rd party carbon-c-relay or graphite-ng relays are able to do that.. Thanks, but already fixed in https://github.com/graphite-project/graphite-web/pull/2111/. I'm not really sure that you'll get such huge performance gain from changing hash functions.\nAlso please note that it will be better if hash function should be available \"out of the box\" from major python distribution, so, IMO no need to bring some 3rd party module for that or I doubt that python-only xxhash or murmur3 will be faster than hashlib.sha256...\nFor dot replacement - _DOT_ looks good enough for me. :). Discussed that offline with @Civil. IMO ideal solution - configurable hash - sha256 for out-of-the-box solution, and something very fast  like cityHash or xxHash for speed.. I think you need to run \nPYTHONPATH=$GRAPHITE_ROOT/webapp django-admin.py migrate auth --settings=graphite.settings\nfirst - it will create auth relations, then --run-syncdb will work well.. Well... I'm not a big Django expert, unfortunately. If migrate doing the same as migrate auth and migrate --run-syncdb then it's simpler - because it's a single command.\nMaybe @brutasse knows?. I spent some time with python3 and django 1.11. Looks like migrate auth and migrate behaves little better.. @AlejandroRivera:\nNo, disregard that. What's your Django version?\nLooks like migrate auth and migrate --run-syncdb works fine for 1.8.x, but cause troubles for 1.11. TBH I didn't reproduce cases when --run-sync-db doesn't work, but simple migrate does. I'll prefer to keep  --run-sync-db in documentation, because it works well on empty db, but upgrade manual is still open question.. LGTM!. Good idea, but it expects urllib3.connectionpool.HTTPConnectionPool() in result.. Probably, yes - https://launchpad.net/ubuntu/+source/graphite-web\nUnfortunately, we're not packing graphite into packages, distro owners doing that. Create deb or rpm for python package is easy, but many dependencies should be upgraded too, so, it's not that straightforward process, as expected. That's why 1.0.2 is presented only in upcoming (Bionic Beaver) ubuntu.. I made a quick hack to test python3 in docker -  https://github.com/graphite-project/docker-graphite-statsd/tree/python3 aka graphiteapp/graphite-statsd:1.1.0-pre4-py3\nIt mostly works, after fixing some dependencies:\nroot@9aca3511819c:/# ps awx | grep python3\n    1 ?        Ss     0:00 /usr/bin/python3 -u /sbin/my_init\n   26 ?        S      0:00 /usr/bin/python3 /usr/local/bin/gunicorn wsgi --workers=4 --bind=127.0.0.1:8080 --log-file=/var/log/gunicorn.log --preload --pythonpath=/opt/graphite/webapp/graphite\n   31 ?        S      0:00 /usr/bin/python3 /opt/graphite/bin/carbon-aggregator.py start --debug\n   34 ?        Sl     0:00 /usr/bin/python3 /opt/graphite/bin/carbon-cache.py start --debug\n   61 ?        S      0:00 /usr/bin/python3 /usr/local/bin/gunicorn wsgi --workers=4 --bind=127.0.0.1:8080 --log-file=/var/log/gunicorn.log --preload --pythonpath=/opt/graphite/webapp/graphite\n   62 ?        S      0:00 /usr/bin/python3 /usr/local/bin/gunicorn wsgi --workers=4 --bind=127.0.0.1:8080 --log-file=/var/log/gunicorn.log --preload --pythonpath=/opt/graphite/webapp/graphite\n   63 ?        S      0:00 /usr/bin/python3 /usr/local/bin/gunicorn wsgi --workers=4 --bind=127.0.0.1:8080 --log-file=/var/log/gunicorn.log --preload --pythonpath=/opt/graphite/webapp/graphite\n   64 ?        S      0:00 /usr/bin/python3 /usr/local/bin/gunicorn wsgi --workers=4 --bind=127.0.0.1:8080 --log-file=/var/log/gunicorn.log --preload --pythonpath=/opt/graphite/webapp/graphite\nSo, will try to find out why ceres test failing. I'm afraid ceres is not python3 ready, probably we should exclude it from tests then.. FYI here: Current requirements.txt is not compatible with Python 3. Now I just include it with docker image explicitly - https://github.com/graphite-project/docker-graphite-statsd/blob/python3/Dockerfile#L35-L48, but I'm wondering - how do we should work on that?\nHaving something like requirements3.txt?\nIdeas? @DanCech @iksaif @piotr1212 @brutasse ?. @iksaif \n\nWhy is our requirements.txt not compatible with python 3 ?\n\nMostly because pinned versions of modules are not python3 compatible.\npython-memcached==1.47\ntxAMQP==0.4\nsimplejson==2.1.6\npip3-freeze giving me:\npython-memcached==1.58\ntxAMQP==0.7.0\nsimplejson==3.13.2. @iksaif \n\nDo they really need to be pinned ? \n\nGood question. Tried to run on latest modules - simplejson looks good, but I didn't test AMQP and memcached though :)\n\nIsn't pip smart enough to see that some versions don't support Python2 ?\n\nNo, it's quite dumb.. Well. I think we can unpin txamqp, memcached and simplejson and fix any (possible) issues later.. >Yup, I agree. Do you want that removed as part of this PR?\nYes, it's fine, I think.. Great! Let's merge this!\nWill cut 1.1.0-pre5 (now with Python 3 support!) . Thanks a lot for your contribution, @takluyver , it was impressive!. Hello @AsenZahariev ,\nThis is part of rather new PR https://github.com/graphite-project/graphite-web/pull/2136\nCiting it here:\n\nCurrently pickle & msgpack using lots of small reads to decode the responses from remote hosts, which causes slowdowns for the remote host and can drastically increase the time taken to receive large responses.\nThis PR adds a new BufferedHTTPReader class that can be used to wrap the result before passing it to load(). It reads from the underlying response object in chunks to keep memory usage reasonable without slowing down the producer.\n\nREMOTE_BUFFER_SIZE is a default buffer size for http calls to remote hosts in cluster. Default is 1024 * 1024 = 1048576, i.e. 1MB - should be fine for most clusters.. Are you experiencing any issues, or just curious?. Cool, thanks for sharing!\nPlease note that (IIRC) this behavior is enabled by default now, and doesn't require any additional variables. \nREMOTE_PREFETCH_DATA and REMOTE_STORE_USE_POST are useful for cluster tuning ofc.. IIRC is an acronym for \"If I Recall(or Remember) Correctly\".\nAnd looks like I'm not really because REMOTE_PREFETCH_DATA is not used now (after https://github.com/graphite-project/graphite-web/pull/2093). REMOTE_STORE_USE_POST can be useful but not mandatory for features above, POST has no limit for size request (contrary to GET).. For 0.9.15 PREFETCH is still valid, of course.. 1. We all three - @iksaif @DanCech and me. I can add you too - give me your (github account) email please. Or give me list of files which we should exclude.\n2. Very \ud83d\udc4d . But how you going to run it?\n3. Fine for me.\n4. My Travis experience is quite limited, but I think it's currently not supported - https://github.com/travis-ci/travis-ci/issues/4090. >My email is my github username at gmail.com.\nPlease check now - https://www.codacy.com/. It's \ud83d\udc4d , but could you please elaborate what is group attribute and how grouping functionality works ?. Ah, now I got it, thanks! \nNot sure if current mapping method is very convenient, though. OTOH we should do it only once..... LGTM!\nI didn't really like function describe format, but now looks better, and @DanCech made all hard work already. :)\n(I'm still busy with release notes, pre-Christmas days are always hard...). Thanks!. Absolutely must! Thanks, @DanCech !. @chrim5: could you please elaborate which version and how are you trying to install? Thanks!. @DanCech : probably it's related to moving parts of functions.py to functions/__init__.py ?\nThen it should affect other places too:\n[dzhdanov:~/git/graphite-web] master+* \u00b1 grep -r 'from graphite.functions import' *\nwebapp/graphite/functions/views.py:from graphite.functions import SeriesFunctions, SeriesFunction, PieFunctions, PieFunction, functionInfo\nwebapp/graphite/render/evaluator.py:from graphite.functions import SeriesFunction\nwebapp/graphite/render/functions.py:from graphite.functions import SeriesFunction, ParamTypes, Param\nwebapp/graphite/render/views.py:from graphite.functions import loadFunctions, PieFunction\nwebapp/tests/test_functions.py:from graphite.functions import _SeriesFunctions, loadFunctions\nOr it's something Django related?. Ah, completely forgot that. Thanks, @DanCech !\n@chrim5 - could you please test now?. Hello @mlausch,\nPlease note that we're not going to make any 0.9.x releases. But you can use 0.9.x branch, of course, after merging fix in #2156 . @mlausch: Fix is merged, please test.. Nice work, @DanCech!\nMuch appreciated!. Thanks!\nI have one more question btw.\nShould we fork 1.1.x branch or just tag 1.1.0 release in master and continue development there?\nPreviously we had incompatible master and 0.9.x branches, so, it was logical to fork 1.0.x.\nBut now we're in the different situation.\nOTOH stable fork giving us much more freedom for incompatible changes in master, but then we will be forced to merge all master changes back to the stable branch. \nIt's also possible to fork 1.1.x e.g. for graphite-web and carbon (where we have a majority of changes) and keep whisper/carbonate as tagged.\nWhat do you guys think?\n/cc @DanCech @iksaif @cbowman0 @obfuscurity \n. OK, submitting that, then forking and releasing. ;). yes, me too. But pip install carbon picks 1.0.2 :(. Cool, thanks a lot!. For me #2168 looks good, thanks, @DanCech !. IronDB is a good software but IMO it's hard to be named as \"Storage Backend Alternative\" for Graphite, it's compatible commercial alternative, though. It's also closed-sourced and commercial only (InfluxDB is open-sourced and the commercial version is only for clustering). OTOH we're not saying explicitly that we accepting only OSS additions to the list.\nWhat do you guys think? /cc @iksaif @DanCech @obfuscurity ?. Sorry, @redhotpenguin, I agreed with @DanCech and @obfuscurity here. As I said, IronDB is a good software, but it's\n - closed-source\n - commercial (not open-core as it's not open-source).\nAll other services on the list are open-source, even InfluxDB - which is open-core.\nTake our sincere apologies, but we're believing in OSS and will promote it in a first place.. That's strange - if that be true then cache was not working completely, right? \nI saw quite big carbon caches in memory and never experience this issue. Which version do you running, @cout ?. #629 is a different issue, not sure why #602 is there too - typo in issue number?\n1460 is a complete mystery to me too, looks like some cache misconfiguration.. Maybe it's worth to share your config too. Thanks!. Looks good!\nWe should apply similar patch to carbon too, right?. Will backport this to 1.1.x, and release 1.1.2, but I think in 2018 - let's wait for some other fixes.. Cool! I didn't know that such tooling even exist :P\nCould you please put the same for carbon and whisper repo too?. Still relevant, @cbowman0 ?\nTests should be fixed, though.. Oh, thanks a lot, @piotr1212 !\nMerging.. I think build-index program should be removed, together with documentation. Index are handled by workers now.. Very cool, indeed!\nThanks, @cbowman0 !. Did you check it, @DanCech ?. Ah, you referring to tests. I also faced this with python 35, looks like temporary glitches on Travis. Merging that now, will try to extend with different installation methods.\nThanks, @Serphentas !. No additional config needed for Sentry?\nMaybe we also should mention it in docs? (maybe not, just thinking out loud). Needs to be rebased now, @iksaif  :(\nDid you check it, @DanCech ?. Cool, thanks!\n@DanCech, could you please review it when you have time?. The release window is going to close. :)\nMore reviews, please?. Not sure if rebase is needed - if Github not complaining then it should find a way how to merge it properly.. I think it's already rebased, so https://github.com/iksaif/graphite-web/blob/06a1fafde6c96fa6c250767f7a69fb8adddf5a14/webapp/graphite/settings.py#L68-L72 is looks exactly as it is in master - https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/settings.py#L68-L71\nOr what do you mean, @DanCech ?. Codacy is not happy. :|. @iksaif : so, this PR can be closed now, right?\nOr it's useful to have FetchInProgress per se?. Hi @wangzaiqqhop,\nCould you please explain more about your setup? \nDo you use cluster or single server? \nWhat's version of Graphite? \nDo you have only 3 categories, A, B, and C?\n  . fixed in master, backported to stable branch, will be released soon.. That 'u' means representation of Unicode string. Not sure why it's happening only in tags.... @ahoffpauir - try to set up tags as space separated list - does it works that way?. Backporting important master commits back to 1.1.x, to not making 1.1.x too diverged.. @iksaif @DanCech \nLooks like get_nodes() change is not backward compatible, all tests are failed. Then we need to roll it back in carbon too.... This works, cool! :) . You not using clustering and CLUSTER_SERVERS = [] in config?\nHmm... Does setting USE_WORKER_POOL=False fixing that?. CLUSTER_SERVERS = [] is on settings.py, so, should be fine. Try to set USE_WORKER_POOL=False \nIf it helps then we can fix it like\nif settings.USE_WORKER_POOL & len(CLUSTER_SERVERS): in https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/storage.py#L86. @DanCech \nSo, REMOTE_FETCH_TIMEOUT applying to local fetches too now?  That's new for me. :)\nI think we need to introduce a new variable for local fetches then, REMOTE_FETCH_TIMEOUT is confusing. And probably local fetch timeout should be much less then remote fetch timeout.. Yep, I remember similar report, but I was not able to reproduce that.\nWhat's version of OS, python and pip?. That's funny. I was not able to reproduce on exact same config. Will try again.. Cool, nice feature!. More is better in this case. \ud83d\udc4d \n. Thanks, Dan, looks good!. @surendarchandra \nWhole code looks like this:\nif known_nones > candidate_nones and len(series):\n        if settings.REMOTE_STORE_MERGE_RESULTS:\n          # This series has potential data that might be missing from\n          # earlier series.  Attempt to merge in useful data and update\n          # the cache count.\n          log.debug(\"Merging multiple TimeSeries for %s\" % known.name)\n          for i, j in enumerate(known):\n            if j is None and series[i] is not None:\n              known[i] = series[i]\n              known_nones -= 1\n          # Store known_nones in our cache\n          series_best_nones[known.name] = known_nones\n        else:\n          # Not merging data -\n          # we've found a series better than what we've already seen. Update\n          # the count cache and replace the given series in the array.\n          series_best_nones[known.name] = candidate_nones\n          seriesList[known.name] = series\nSo, it's part of code which merge duplicate data from different sources. If series[i] generating Index not found error then it means that two series with same name have different number of datapoints, i.e. usually different step, which shouldn't happening normally. OTOH generating error is not good idea, of course.\nWe can change it as you proposing, but I think in this case result of data merge will be garbage anyway.  Or we can wrap it with try... except IndexError: pass, then we'll return  at least one serie from two. Not sure what result is better.\nIdeas? \n@DanCech @iksaif @cbowman0 ?\nThanks!. @surendarchandra:\nWhat do you mean, a1-metric and b1-metric ? Different names? No, this code works only for merging same name metric from different sources, in this case, every storage should return the same amount of data points.\n. Well, in case of one datasource returns e.g. 600 datapoints and another for some reason returns 601, we can easily ignore the last datapoint, as you proposing. But if one datasource return 600 datapoints and another returns e.g. 3600 datapoints for same metric and time period - it will be quite hard to merge both in this case.. Yep, agreed, thanks, Dan - https://github.com/graphite-project/graphite-web/pull/2198. Merged, will backport to 1.1.x. Backport PR - https://github.com/graphite-project/carbon/pull/732. Even on localhost it should have same length, right?. Backport script still doesn't work. Oh well.... Oops, wrong branch. Will open a new one.. >Looks good, not sure how up to date those carbon-api benchmarks are.\nYep, that's why I put \"In some benchmarks\". Benchmarks are lie, everybody knows that.\n\nAFAIK graphite-remote-adapter doesn't (yet) support tags so I wouldn't call it \"fully featured\"\n\nI hope they will finish it soo, right, @iksaif ? :). We can add some fixes later, of course.. backport command is still working, until 10-20 commits or so. Weird.. Ah, I suspected that. Then we need to that earlier.. Cool, thanks for testing that, @wridgers !. Sorry, I thought it was already merged, my bad. Done.. Yes, confirmed.. Hmmm... I thought I included https://github.com/graphite-project/graphite-web/pull/2207/commits/1bb3542f0789ab2ace71392e139e703e24a0e29e in my docs PR.. Sorry for a mess, will sqashmerge it.. Travis has issues now, but local tests went fine.. Travis has issues now, but local tests went fine.. Broken. Hi @yuribit,\nDid you configure remote user auth properly - https://docs.djangoproject.com/en/2.0/howto/auth-remote-user/ ?\nDid your web server really set REMOTE_USER variable?. Yes, thanks for clarification, @wfhu !\nLGTM, but I'm not native English speaker. Maybe someone else can review? @DanCech @cbowman0 ?. You need to set URL_PREFIX = '/graphite' in local_settings.py and restart Graphite.. @mcbsdEU : Thanks for reporting!\n@DanCech @iksaif : It's about a scenario when one node of the cluster is down.\nPrevious fix of @iksaif is https://github.com/graphite-project/graphite-web/commit/c6cf8244862ec15bf25b4cb4b01a29e45f59e2eb\nDo you  have an ideas how to fix that properly in current codebase?\n. >This is really a philosophical issue, should we return incomplete results or let the user know that it's not possible to compute a complete result?\nGraphite is supporting replication, but it depends on metric name, so, real problem here is that we're do not know will result for specific query be complete or not if particular cluster node is down.\nSo, agreed with @DanCech here - this should be configurable option, but I can see valid usecases where we should return data even if one of the cluster node is down.. Hmm... if I understand it correctly #2303 doing exact opposite thing and not really fixing this issue.... Nothing, except careful backporting and preparing release notes. But not sure if I will make it soon, I was thinking about the end of August. Will try this weekend, but no promises.. @funtiik: but that\u2019s exactly what\u2019s expected - it breaks on error, otherwise graphite will swallow unreachable backend error and produce wrong result.\nNot really clear what you trying to achieve, could you please elaborate?. Hello @t4ko,\nDid you test migration to new schema btw? does it pick up metrics in old format too?. Or 1.1.x branch\nOn Thu, 1 Feb 2018 at 19:30, Dan Cech notifications@github.com wrote:\n\nPlease use master, there are issues with 1.1.1\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2220#issuecomment-362358569,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51tZr8DgSkXTuJDmOLLPWhgytZT7Kks5tQgK8gaJpZM4R2EWm\n.\n. @raghavan20 : You have single 1s:14d archive in whisper. That means \"Store data every 1 second and keep last 14 days of data\". So, Graphite by default takes 14 days from current date and renders data - but there's no data for last 14 days - then it shows nothing.\nThe proper solution for that situation - change your storage schema. If you need to keep more that 14 days of data, please change it accordingly  - or increase retention or add another archive.\nBut if you need to just render data for that period once - then you can trick Graphite to think that today is 16 july 2017 - add  now=14%3A00_20170716 to your render request, but it will work only in Graphite 1.1 or newer.. That's strange. Are you sure that you mount all volumes properly? Can you render other targets? (e.g. carbon or statsd statistics). Ah, that's how None coming instead of []. Thanks!\nWill backport it to 1.1.x and prepare release notes of 1.1.2, now we have more then enough patches for it.. Then I'm not understanding this - https://github.com/iksaif/graphite-web/blob/3d12bf728bc1d7141d2efa63033fa1908a42f4d8/webapp/tests/test_functions.py#L5147-L5150\nSo, before it was 1 time series, now it's 2 time series.\nI'm confused now. :|\nAnd I do not see pattern in tests either.... Ah, nvm, I'm blind. Friday \ud83e\udd37\u200d\u2642\ufe0f . Hello @DineshGuptaa,\nYou need to set up Django database for Graphite. Please see documentation - https://graphite.readthedocs.io/en/latest/config-database-setup.html\n```\nTo set up a new database and create the initial schema, run:\n\nPYTHONPATH=$GRAPHITE_ROOT/webapp django-admin.py migrate --settings=graphite.settings --run-syncdb\n``. @Abid1986 : which version of django are you using?. @Abid1986 \nYou probably using olddjango-tagging` packgage, you should use v0.4.6 - https://github.com/graphite-project/graphite-web/blob/master/requirements.txt#L43. >In my head it would be something like this, but it does not work\nWhat exactly it's not working?. >The problem is that aggregateLine produces a series with only 3 points, so it's not really going to work for this kind of thing.\nAh, indeed - https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/functions.py#L4071\nProbably we can add step as additional parameter to constantLine() and aggregateLine(). Oops.\nWhat docs version does?. Strange, still see no difference. Where's this title should be visible?. '|' has a special meaning now - it's function separator.  Not sure if you can escape it, probably not.\nMaybe @DanCech have some ideas?. @rwyse - could you please show your apache config? Did it base on https://github.com/graphite-project/graphite-web/blob/master/examples/example-graphite-vhost.conf ?. @rwyse,\nCould you please show me opt/graphite/conf/graphite.wsgi'?\nDoes it looks like https://github.com/graphite-project/graphite-web/blob/master/conf/graphite.wsgi.example ?. @rwyse \nWell. Something is really wrong with your system, I'm just trying to find what exactly. \nHow do you install graphite-web - usingpip install? did you use virtualenv?\nPlease show output ofls -al /opt/graphite/webapp/graphite/render/`. Graphite's file permissions are quite simple - files under /opt/graphite/storage/whisper should be writable by carbon process, and readable by apache process. If you using sqlite database - it should be readable and writable by apache process. Simplest solution - run carbon process with apache permissions too.\nFiles under /opt/graphite/webapp/ can be only readable by apache process, write permission is not mandatory.\n\nWeb interface is working properly however I am still not able to expand metrics and see any thing in the tree.\n\nWhat's in the error logs now?\n. @rwyse: Sorry, but it looks like your installation is severely broken, maybe recreating from scratch is a good idea. E.g. I'm very puzzled how you can get AttributeError: 'Job' object has no attribute 'exception_info' if Job object definitely has this attribute. Maybe permissions is bad again and it just could not import it?\n. Hello Ryan,\nDo you still have AttributeError: 'Job' object has no attribute 'exception_info' error?\nIf yes - could you please show me output of\nls -al /opt/graphite/webapp/graphite\nls -al /opt/graphite/webapp/graphite/worker_pool\ncommands.\nAnd if you using some manual for installation - it would be nice to have a link, maybe it's outdated.. let's close this then :). I also not really like more Django, but it's better to have some metrics then no metrics, right?\nMaybe default prefiux should be switched to graphite.django. ?. Yes, but now we have hard dependency on prometheus_client which make me sad.. Hmm... No other errors in log file ? I do not see any issues in files above, unfortunately.. Did you upgrade Django when upgrading Graphite to 1.1.2 ? Which was the old version?. @jdblack :\nVery strange. Looks somewhat similar to https://github.com/graphite-project/graphite-web/issues/2243 but totally different call.\nDo you have cluster configured? Or it's single node? \nBlocking tags makes it working for uwsgi, right?\nAre you using tags btw? If not - you can try to disable it completely, using TAGDB=None.. @jdblack \nIt looks similar to https://github.com/graphite-project/graphite-web/issues/2243\nCould you please show how your CLUSTER_SERVERS looks like?\nIf something like\nCLUSTER_SERVERS=[`x.x.x.x`, `y.y.y.y`]\nthen you need or apply https://github.com/graphite-project/graphite-web/pull/2244 or try\nCLUSTER_SERVERS=[`http://x.x.x.x?noTags=1`,`http://y.y.y.y?noTags=1`]\nPprobably we should release 1.1.3 with PR#2244 and document CLUSTER_SERVERS format, described in https://github.com/graphite-project/graphite-web/pull/2123 and https://github.com/graphite-project/graphite-web/commit/4c5a7c44bb1b12dd3724a50569b4d9c45b85ca20. Hi @pinkynrg ,\nI can be wrong, but I suspecting that template can't contain '.'\nTry this:\ntemplate(timeStack(summarize(portal.ovomec.santa_maria_di_sala.1308756.ovomec_smart_one.$prefix.0,'10minutes','max'),'24h',0,10))&template[prefix]=food_distributed_today\n(i know that's not what you want to achive, just for test).\nBTW, PR for template - https://github.com/graphite-project/graphite-web/pull/1205, it also contains some tests, which are still working fine.. Ok, got it. Could you please elaborate how exactly it doesn't work?. Sorry for stupid question - but does timeStack(summarize(...)) working fine w/o templating?. Well, everything is possible, but I would say in this case effort is not worth the outcome.\nIMO it's a clear edge case, and it's should be documented, like 'please use template() as  close do metrics as possible, otherwise it will not work with functions which re-evaluates queries, e.g. timeStack()'.. Yes, some additional info is needed, 'cause testing code is still there and works - https://github.com/graphite-project/graphite-web/blob/53b9f97ef1f393dbc31cde2c681617e55c249dc5/webapp/tests/test_functions.py#L3794-L3816 . Yes, right name of the function is round - https://github.com/graphite-project/graphite-web/blob/53b9f97ef1f393dbc31cde2c681617e55c249dc5/webapp/graphite/render/functions.py#L5368\nWe should rename it, probably.. @pinkynrg:\nSorry, but it's still not clear what's wrong is with rounding in  round(minSeries(metrics.temperature_probe.{0,1,2}),2)\nCould you please elaborate?\n. Yes, maxDatapoints making another aggregation, after round() - that's why @DanCech asked about it too - https://graphite.readthedocs.io/en/latest/render_api.html?highlight=maxDatapoints#maxdatapoints. @pinkynrg \nNot sure if it's possible in the current codebase. :( consolidateBy have no support for round, only \u2018sum\u2019, \u2018average\u2019, \u2018min\u2019, \u2018max\u2019, \u2018first\u2019 & \u2018last\u2019...\nBut I'm wondering why maxDataPoints doing aggregation even if you asking points for 1 hour and limit it to 500. Have you more than 500 points per hour? Because maxDataPoints should do nothing if the number of points is less then maxDataPoints.... Default storage schema is https://github.com/graphite-project/docker-graphite-statsd/blob/master/conf/opt/graphite/conf/storage-schemas.conf\nDefault aggregation schema is https://github.com/graphite-project/docker-graphite-statsd/blob/master/conf/opt/graphite/conf/storage-aggregation.conf\nAs you can see, default retention is 10s:6h,1m:6d,10m:1800d, and default aggregation is average with xFilesFactor = 0.3.\nThat means that for every minute you need to send data at least 60 / 10 * 0.3 = 1.8 ~ 2 times, otherwise, Graphite will think that you have a gap in data, and it should not be propagated to next archive.\nYou can set xFilesFactor = 0 in https://github.com/graphite-project/docker-graphite-statsd/blob/master/conf/opt/graphite/conf/storage-aggregation.conf#L41 but then you need to delete and recreate your wsp files in /opt/graphite/storage/whisper dir.. Yes, that was an idea behind graphite-api, indeed. But nowadays graphite-api become so diverted from rest of the code, so, it's almost impossible to do anything with that.\nI do not think that having local API is a good idea - everybody is doing REST or gRPC nowadays.\nIt's possible to create something like graphite-api - rendering core, and convert current dashboard and web interface code to clients of that API - but scope of this task is quite big, that's \"scheduled\" for graphite 2.0 or something.. I can be wrong, but I'm suspecting that maxDataPoints removes Null values when doing any consolidation  - https://github.com/graphite-project/graphite-web/blob/e83977bfa40fb13e226a54031704f814693501cc/webapp/graphite/readers/utils.py#L39-L53\nbut probably noNullPoints can remove nulls if maxDataPoints >= numberOfDataPoints...\n. Backport of #2221, #2234, #2244, #2252, #2254 and #2257 to 1.1.x. Thanks!\nWell, uwsgi config is just a stab, compare it to for example config for gunicorn from same link.\nIt can be extended, of course, and documentation is in the same repo btw - https://github.com/graphite-project/graphite-web/blob/master/docs/config-webapp.rst. You have retention 1s with 60 points in archive 0, but sending data every 60 seconds - so, it goes to 2nd archive directly. Works as designed, IMO. . @Tushar1983,\nI think this issue should belong to https://github.com/graphite-project/docker-graphite-statsd repo, but OK.\nThat's a known issue, probably should be fixed. If you mount the empty directory as storage Graphite creates all data there, but then its need to be restarted to pick up changes. For now, it's better to restart container or create whisper directory and graphite.db in advance.. @Abid1986 : unfortunately, the information above is not enough to proper diagnosing of issue. \nWhat browser showing to you? What's in the error log, access log of webserver, of graphite itself?\n. I'm not aware of any reasoning, looks like it's just \"because of historical reasons\".. >If it hasn't been implemented yet there must be a reason. Is it such a bad idea to build an API to rename metrics?\nI do not see any useful use cases for such functionality TBH. Could you please elaborate why such functionality is needed?\nThis can be implemented, of course, but I see a couple of limitations here. \nFirst - graphite-web is not altering data at all,  only carbon doing that - but carbon has no API for that and it will be quite hard to implement that without introducing some new API - REST or gRPC. If you going to change that - that's fine, but how it will propagate these changes for federated graphite-web servers?\n. Cool, very nice, thanks a lot, @piotr1212 !. Something is stopping your carbon daemon. I would say it's OOM killer but it looks like it's terminating normally, with TERM signal, like something doing carbon stop. \nOr maybe it's not relevant log too.. index file is completely optional. Make it empty and root-owned - it will stop its population.. Hard to say. Something is definitely wrong, but information from logs above is not enough. What's your Graphite and twisted version?. Latest Carbon requires Twisted 13.2.0 or newer - https://github.com/graphite-project/carbon/blob/master/requirements.txt#L1 - maybe that's the case.. \ud83d\udc4d \nGood to know, thanks!. 1423594000 is Tuesday, February 10, 2015 6:46:40 PM GMT, so, time long in the past. Default graphite from Docker have following schema - https://github.com/graphite-project/docker-graphite-statsd/blob/master/conf/opt/graphite/conf/storage-schemas.conf#L26:21\nretentions = 10s:6h,1m:6d,10m:1800d\nSo, data which is 3 years old will go directly to 3rd archive - 10m:1800d, which have resolution 10 minutes. That's why your timestamps will be changed, \nAlso, usually Graphite line protocol is much easier to handle then pickle and it's also have same speed - so, it's recommended to use it and not pickle.. Fixed in https://github.com/graphite-project/graphite-web/pull/2284\nThanks!. This can happen or because of different python version between mod_wsgi and python or because of hardware problems.. TBH I was thinking that Graphite always was case-sensitive, at least on case-sensitive filesystems.\nI just tested oldest version in docker repo - 1.0.2 - it's case-sensitive as well.. what's your metric interval?. Ah, got it. Graphite probably is not accepting \"3minutes\" as valid interval - https://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/render/attime.py#L159-L167\nTry to use \"3min\" or just 180. That's very strange. Code is very simple here:\n1. \"integer division or modulo by zero\" means that interval variable is zero - https://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/render/functions.py#L2555-L2558\n2. interval = delta.seconds + (delta.days * 86400), so delta should be zero - https://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/render/functions.py#L2546\n3. delta = parseTimeOffset(intervalString) - https://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/render/functions.py#L2545:3\n4. intervalString is coming directly from your request, and parseTimeOffset should work fine.\n```\n\n\n\nparseTimeOffset(\"3minutes\")\ndatetime.timedelta(0, 180)\nparseTimeOffset(\"3min\")\ndatetime.timedelta(0, 180)\nEven \"3minutes\" should work. \nI'm confused TBH. \ud83d\ude15 \n. You can use Query editor in Grafana to peek how it send request to Graphite btw.. @gms1234 \nIf you run\ncd $GRAPHITE_ROOT/webapp\nls -al graphite/settings.py\n```\nwhat is saying?\n. Sorry, @gms1234, but we're not supporting Icinga2, please try to ask support from Icinga community. I do not know how Icinga getting metrics and how it integrates with Graphite.. well, technically it's a bug, but I would say empty search string is technically incorrect anyway.... Ah, that makes sense. Well, error 400 will also generate a warning in Grafana. We can fix it, should be quite simple fix.. Ah, returning something there is hard - because we need to create proper response for every output format. But proper error message is easy - https://github.com/graphite-project/graphite-web/pull/2295\nNot sure will it help with Grafana warning, though. >The configuration clearly says: \"Note that this database is only used to store Django models such as saved graphs, dashboards, user preferences, etc.\"\n\n\n\nYes, this statement is clearly wrong - it's from \"pre-tag\" era, indeed.\n\nIs it normal that I have such bottleneck between Graphite and Postgres?\n\nYes, storing tags in PostgreSQL is made through Django ORM, which is far from ideal - if you talking about performance. Redis will do that task much better.\n\nIs there any recommendations for current configuration?\n\nYes, use Redis and not relational DB for storing tags. You can also disable tags support completely.\n\nAlso, I don't use the tags feature at all, is there any ways to disable it?\n\nSure, set ENABLE_TAGS = FALSE in carbon.conf and TAGDB = None in local_settings.py. Well, this patch will fix that specific use case, indeed.\nBut I'm just wondering what exact use case it is? n in lowest() is not a time interval, it's a number. Graphite is written in Python, in Python string and integer is different entities, so, why on earth someone putting a string into integer field? :)\nNot saying that it shouldn't be implemented, just curious. Some Javascript code doing that?. Ah, right. OK, thanks.. @deejay1 : no worries, fix is merged and will be released soon. Hello @surajbora007,\nCurrently, it's not possible to provide user-level authentication for Graphite. You can put some authentication e.g. to Apache, as described e.g. in https://serverfault.com/a/837031/104117, but it will be global - e.g. it's not possible to limit access to some metric subtree or something.. Not sure what are you trying to fix here - user's browser will have access to user credentials, by design. If you worrying about credentials sniffing you need to put your Graphite on the SSL-protected website.. @surajbora007 : You can bind graphite-web to localhost and install Graphana on same server. Or you can limit access to graphite-web (again, using Apache) to allow requests only from Grafana server.\nThen you need to ssl or auth for graphite-web itself. . @JeKa0111 : unfortunately, no one is working or planning to work to implement this feature. The current workaround is still the same - use 3rd party dashboard tools (e.g. Grafana) for managing dashboards if you need more granular security policy. But please note that even that way will be no limitation for graphite metrics itself - only to dashboards.. Hello @ptonelli \n\nSorry to annoy you, but I believe a warning should be added in the documentation of the docker image not to use it for prod\n\nprod readiness is a bit philosophical question, I would say...\n\nit has an sshd daemon listening inside\n\nNo, it's not true. Phusion baseimage has support for ssh (you can check their reasoning in their docs) and it's disabled by default.\n\nstatsd is downloaded at runtime from the web, which means it cannot run on an isolated machine\n\nYes, if you mount /opt/statsd volume. But that's easily fixable - put statsd archive into container and unpack it to /opt/statsd instead of downloading it. Please create an issue for that in https://github.com/graphite-project/docker-graphite-statsd\n\nI am currently searching for a lighter version which would contain the minimal components and be autonomous, or maybe a docker-compose of the necessary components (graphite...)\n\nThat's a valid concern too - but it will require some work.\nFirst, you need to create separate docker images for every component. A most obvious place for that /docker directory in every component repo. But phusion baseimage is overkill for that, something like alpine python should be used, but then you need to find a way how to build e.g. cffi and cairo support.\nAfer that you can create docker compose config for running the whole stack and put it to graphite-docker-statsd repo as an alternative to the monolithic image - or even in the separate repo.\nI still have these plan above on my TODO list, but can't promise anything.\nOf course, PRs are welcome.\n\nI do not know if this should be put as a bug report / feature request, so please close if irrelevant.\n\nI would say it's a feature request, but not for this repo - and I created it a long time ago already - https://github.com/graphite-project/docker-graphite-statsd/issues/29 \n. Hello @clusterfudge \n\nThe immediate repercussion of this is that a node is only considered available if it has recently failed.\n\nLooks like it. @DanCech ?\nNot sure how it works if nodes never failed, though.. Yes, that's known. Currently noNullPoints is render-dependent and implemented currently only for json rendering.\nOf course, you can copy implementation above to all renders, but that's a terrible solution IMO.. I'm \ud83d\udc4d but not sure does default should be True here. It's changing existing behavior and other users use replication, so, current behavior can be acceptable.\nAlso \"All requests failed\" error message is misleading here, probably it's better to introduce separate if with distinct error message.. I'm agreed with @DanCech  here that Graphite should not lie to a user and return broken data to a user by default. But it can be viable option to enable because e.g. if you copy your metrics to 2 servers in parallel and one of the servers gone - you still can use another copy - it's a perfectly valid use case. Unfortunately, it's almost impossible to be sure if all your data valid in all cases, so, IMO, we can have an option in properties to enable this behavior - if a user knows what he/she's doing. \n. Looks good - thanks!\nI'm not a big JS guru, though.... it's documented now. Cool, nice addition to docs! Was not aware of this feature too... :). You need to install cairocffi python module. Which OS are you using and how exactly do you install graphite?. Easiest fix then - install python-cairocffi package from epel repo - https://centos.pkgs.org/7/epel-x86_64/python-cairocffi-0.5.4-1.el7.noarch.rpm.html. You need to increase fetch timeout instead. In latest Graphite variable named FETCH_TIMEOUT but probably in 1.1.2 it's still named REMOTE_FETCH_TIMEOUT.. @clusterfudge @DanCech @iksaif - what we going to do with this change?\nReminding that is related to the problem described in https://github.com/graphite-project/graphite-web/issues/2313#issuecomment-401876110\nI also notice some performance degradation on big requests but I can't prove it with numbers, though.\nOTOH I don't really like consequences of mandatory REMOTE_BUFFER_SIZE. Can we probably make avoiding streaming deserialization optional somehow? I.e if REMOTE_BUFFER_SIZE > 0 use it, if not - fallback to stream implementation? . Hi @clusterfudge ,\nCould you please fix this PR soon, if you want that change still be included in upcoming 1.1.4 release. Thanks! . Soon \u2122\ufe0f :)\nI.e. when it's ready, but let's not spend weeks there. . @clusterfudge : OK, you mentioned before that if request size is more then REMOTE_BUFFER then it's almost guaranteed to fail, but I'm not really getting why. Is it still true?\nIf yes - then we need to inform users about changing behavior.. Ah, that's what I suspect - but thanks for the clarification.\nMerging.. Sorry, I overlooked this question. If you running only single carbon instance then it's probably overloaded, try to add relay and multiple carbon instances (4-8).. @cbowman0 - could you please take a peek on this PR, please? Thanks a lot!. Merged. Thanks!. Ah, you continue to develop it, nice!\nLGTM. Well, change looks good, but something is happening with Travis CI integration, checking.. Where did you see that, @yadsirhc ?. Yes, please try that. Thanks!\nOn Sat, 28 Jul 2018 at 01:04, Chris Day notifications@github.com wrote:\n\n@deniszh https://github.com/deniszh Should I try closing this PR and\nopening a new one? My gut tells me that the initial PR for the change\nfollowed later by adding the unit test may be why it was rejected by Travis\nCI.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/2321#issuecomment-408550144,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51oPrDxmhzxm3m5OgCtaipwRZorb8ks5uK46JgaJpZM4VbvwI\n.\n. Is it OK to merge that? @DanCech @iksaif ?. Thanks! But I will have PC and stable internet in a week or so... Maybe be\n@DanCech could check?\n\nOn Fri, 3 Aug 2018 at 20:16, Chris Day notifications@github.com wrote:\n\nFix for #2320\nhttps://github.com/graphite-project/graphite-web/issues/2320\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/2326#issuecomment-410319273,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51kgXi9YRFpJ1O7Dhbtke2A9HVxxzks5uNIVYgaJpZM4VpZ-L\n.\n. Can I get more eyes on this PR, pretty please? @DanCech @iksaif @cbowman0 @piotr1212 ?\nThanks!. Your fetch queries giving only \u2018None\u2019, I.e. data is absent - hence not\nshowing in Graphite\n\nOn Wed, 1 Aug 2018 at 08:56, rejkube notifications@github.com wrote:\n\nA Setup with Graphite, whisper db when populated with 1s granular data\n;retention config for 1s:30d [archive0]. whisper-fetch.py shows the data\npoints recorded ,while the Graphite webapp and dashboard fails to show the\nmetric itself !.The final level of the schema itself is not discovered by\nwebapp .There are no issues for the same setup when fed with 1min\ngranular-data or higher\nIs it that Graphite cant show 1 sec data points ? or Any fine tuning 1\nmissed ?\nDjango 1.4.22\nwhisper 0.9.13\nwhisper-fetch.py --pretty key.wsp |tail\nWed Aug  1 00:55:00 2018  None\nWed Aug  1 00:55:01 2018  None\nWed Aug  1 00:55:02 2018  None\nWed Aug  1 00:55:03 2018  None\nWed Aug  1 00:55:04 2018  None\nWed Aug  1 00:55:05 2018  None\nWed Aug  1 00:55:06 2018  None\nWed Aug  1 00:55:07 2018  None\nWed Aug  1 00:55:08 2018  None\nWed Aug  1 00:55:09 2018  None\nwhisper-info.py key.wsp\nmaxRetention: 2592000\nxFilesFactor: 0.0\naggregationMethod: max\nfileSize: 31104028\nArchive 0\nretention: 2592000\nsecondsPerPoint: 1\npoints: 2592000\nsize: 31104000\noffset: 28\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2327, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51o1xZXOO5AOyGkL36XaFHpTqXE04ks5uMULvgaJpZM4Vp4yv\n.\n. Sorry, I see only \u2018Nones\u2019 above, and no values. Or you see no metric at all?\nDo you have directory named \u2018key\u2019 on same level as key.wsp ?\n\nOn Thu, 2 Aug 2018 at 09:07, rejkube notifications@github.com wrote:\n\nthere are values which are present [pls see the ones above]. The key\nitself is not getting detected in graphite-web. expected\nhostname->iops->disks->key\nall but key is displayed\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2327#issuecomment-409816499,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51kAKUsLcQfCAz2txuuZS5mha2EW7ks5uMpcvgaJpZM4Vp4yv\n.\n. Sorry, not really understand what the problem here.\nGraphite is working with 1s resolution, though.\nNot also sure why your whisper-fetch.py giving a list of Nones - are you sure that it really has data in timespan which you asking?. Not sure what's happening there. Maybe it's broken upgrade, old dependencies or some bug in 1.0.2\nDefinitely, that's not happening on the latest version.. Cool, thanks to adding this info.\nStrictly speaking, leading dot should never work, because it will cause .wsp files, which not visible in query editor (but probably worked in queries earlier).. Agreed with @DanCech - documentation is needed.. @zasca - could you please fix the docs, if you want that change still be included in upcoming 1.1.4 release. Thanks! . @zasca \nI saw that before from time to time but I'm still not getting why it happens.\nFor newer Django try something like\nPYTHONPATH=/opt/graphite/webapp django-admin.py makemigrations --settings=graphite.settings\nPYTHONPATH=/opt/graphite/webapp django-admin.py migrate auth --settings=graphite.settings\nPYTHONPATH=/opt/graphite/webapp django-admin.py migrate --run-syncdb --settings=graphite.settings. I'm wondering if moving settings.py to settings/__init__.py affect all django admin commands. Then we need to change all references in documentation and also old Django commands will stop working after applying this change. I would not apply that to minor release like 1.1.4 then.. Same here - I do not see why this is better than #2358\nIs #2358 not good enough? . Going to merge this to fix master. Yes, fixed in https://github.com/graphite-project/graphite-web/pull/2331 - just pinning to latest 3.x for now. Fixed in 1.1.4. Why Django 1.10 is required? Is it needed for whitenoise 4 ?. Yes, we have the similar issue - https://github.com/graphite-project/graphite-web/issues/1801\nI was thinking that breaking only existing graphs, not the new...\nLooks like this issue was introduced after https://github.com/graphite-project/graphite-web/pull/1662 and looks like #1662 not fixing all XSS issues anyway.\nCurrently, only one workaround exist - use some 3rd party dashboard, like Grafana, for saving graphs.\n  . According to Google search if django-admin.py saying pkg_resources.ResolutionError: No script named 'django-admin.py' that means some Django installation error, e.g. mixing different version of Django installed with different ways (e.g. through pip and easy_instyall). I didn't find any solution for that case, though.\nBut you can try what @piotr1212 said - use manage.py or python -m django instead - https://docs.djangoproject.com/en/2.1/ref/django-admin/. Merged to master, backported to 1.1.x, will be released in 1.1.5. Thanks, @eachirei !\nBut please add tests, as @piotr1212 said.. Superceded by https://github.com/graphite-project/graphite-web/pull/2380. Part 1 of 1.1.4 backport. Sorry, it's not fit in one change because of Github API limitation :). will merge that, because part 3 is not mergeable until I merge this to 1.1.x. Part 2 of 1.1.4 backport. will merge that, because part 3 is not mergeable until I merge this to 1.1.x. part 3 of backport changes to 1.1.4\nPhew. Next time I will do that more often, sorry - that amount of changes is almost unbearable.. @clusterfudge : did you test proper msgpack module, like msgpacku?\nJson just can't be faster then msgpack, that's completely counter-intuitive. :) . root@osboxes:~# python test.py\nSerializer: test_json_buffer Payload Size: 80000\n0.0744750499725\nSerializer: test_json_stream Payload Size: 80000\n0.0723919868469\nSerializer: test_pickle_buffer Payload Size: 90006\n0.594494104385\nSerializer: test_pickle_stream Payload Size: 90006\n18.9840340614\nSerializer: test_umsgpack_buffer Payload Size: 90003\n0.61233997345\nSerializer: test_umsgpack_stream Payload Size: 90003\n4.59184384346\nSerializer: test_msgpacku_buffer Payload Size: 90003\n0.0142819881439\nSerializer: test_msgpacku_stream Payload Size: 90003\n0.0173029899597. Sorry, not really getting what you mean, @clusterfudge \nPure python version of modules is good for fallback, but performance sucks, obviously. \nIf you need speed - just install an optimized version of the module.\nmsgpacku is the same drop-in replacement for msgpack, as ujson for json.\n\n. Serializer: test_json_buffer Payload Size: 80000\n0.0708241462708\nSerializer: test_json_stream Payload Size: 80000\n0.0717329978943\nSerializer: test_ujson_buffer Payload Size: 70001\n0.0287759304047\nSerializer: test_ujson_stream Payload Size: 70001\n0.0259568691254\nSerializer: test_umsgpack_buffer Payload Size: 90003\n0.641237020493\nSerializer: test_umsgpack_stream Payload Size: 90003\n4.80740189552\nSerializer: test_msgpacku_buffer Payload Size: 90003\n0.012020111084\nSerializer: test_msgpacku_stream Payload Size: 90003\n0.0133380889893. NB: I'm not in any case against json support for clustering - I'm completely fine with it. \nE.g. even normal json is faster then usmgpack in 10-70 times, so, probably good use case.\nJust wanted to note that I would be very surprised if json will outperform msgpack, even in case of python modules...\n. > msgpacku is the same drop-in replacement for msgpack, as ujson for json.\nOoh, I was wrong, disregard that. msgpack is proper implementation, umsgpack is pure pyton, msgpacku is utf-8 wrapper.. Just did serialize benchmark one more time - looks like json is still fastest serializer from default Python distribution, both for 2 and 3. Msgpack is fastest when msgpack module is installed, with cbor on 2nd place, but CBOR also suffering from stream deserialization, same as pure-python umsgpack...\npy2\nSerializer: test_json_buffer Payload Size: 80000\n0.111679077148\nSerializer: test_json_stream Payload Size: 80000\n0.0857601165771\nSerializer: test_pickle_buffer Payload Size: 90006\n0.968269109726\nSerializer: test_pickle_stream Payload Size: 90006\n19.1805100441\nSerializer: test_msgpack_buffer Payload Size: 90003\n0.0109009742737\nSerializer: test_msgpack_stream Payload Size: 90003\n0.0110709667206\nSerializer: test_umsgpack_buffer Payload Size: 90003\n0.940215110779\nSerializer: test_umsgpack_stream Payload Size: 90003\n4.4617190361\nSerializer: test_cbor_buffer Payload Size: 90003\n0.0151782035828\nSerializer: test_cbor_stream Payload Size: 90003\n3.91015386581\npy3\nSerializer: test_json_buffer Payload Size: 80000\n0.11116572699999999\nSerializer: test_json_stream Payload Size: 80000\n0.10955180199999998\nSerializer: test_pickle_buffer Payload Size: 90026\n0.040406074000000014\nSerializer: test_pickle_stream Payload Size: 90026\n3.492091802\nSerializer: test_msgpack_buffer Payload Size: 90003\n0.018487617999999983\nSerializer: test_msgpack_stream Payload Size: 90003\n0.01901640300000018\nSerializer: test_umsgpack_buffer Payload Size: 90003\n1.0347651600000005\nSerializer: test_umsgpack_stream Payload Size: 90003\n4.593411390999999\nSerializer: test_cbor_buffer Payload Size: 90003\n0.022226440000000736\nSerializer: test_cbor_stream Payload Size: 90003\n3.972988451000001. Well, IIRC cluster transport is an internal thing and was never published as a public API. Before Graphite 1.x only picklie was supported, which is python internal and schemaless, then @DanCech implement msgpack for speed and Metrictank interoperability...\nTheoretically, msgpack should look similar enough to json.. Are you still interested in implementing this, @clusterfudge ?. Can I get more reviews for this, pretty please? I want to release 1.1.4 tonight. :)\n/cc @piotr1212 @DanCech @iksaif @gwaldo \nThanks a lot!. @piotr1212 : no, of course not - I wanted more eyes with grammar, indeed. Will fix now.\n. You need to populate directory with static files and serve them using Apache.\nPlease check STATIC_ROOT  in https://graphite.readthedocs.io/en/latest/config-local-settings.html#filesystem-paths . Huh, I didn't realize that REMOTE_BUFFER_SIZE is used in find code too - \nhttps://github.com/graphite-project/graphite-web/blob/17d4804c961741d0562cb8d574ce9ed796240fb1/webapp/graphite/finders/remote.py#L110-L115\nNot sure if it will work with reverted logic (when REMOTE_BUFFER_SIZE == 0 disables buffering) by default, am I right, @clusterfudge ?\nBTW, according to my tests installing python-msgpack module making msgpack unpacking blazingly fast, even with default streaming deserialize.. Fixed in master, will  be released in 1.1.5. just FYI: trying to create simple docker compose to test cluster from 3 nodes. No objections. :). @clusterfudge : you fixing only find/remote.py here, right? And render/remote.py  will still have old code?\nIMO we should did it in similar way as @DanCech did in #2354, but probably move deserializer in some shared file - user_util.py or something else.\nThoughts? /cc @DanCech @iksaif . According to https://github.com/graphite-project/graphite-web/pull/2128 it should work out of the box. Right, @DanCech ?. IIRC vhost is RabbitMQ specific thing, AMQP support was introduced in graphite 0.9.6 in 2010, when RabbitMQ probably has no vhost support.\nConfig is still part of carbon repo - https://github.com/graphite-project/carbon/blob/master/conf/carbon.amqp.conf.example - but it should be included in the documentation, indeed.. Hello @dreddynarahari ,\nCould you please describe your installation in detail -\nHow do you install django-tagging module?\nWhat's your Django version? How it's installed? . Maybe you have more than one python installed and pip is not installing python modules on the same path where is Apache looking for it? . Sorry, I'm out of ideas then. Something is broken with your python installation. Graphite-web should be still compatible with Django 1.8. @dreddynarahari : This is because https://github.com/go-graphite/carbonzipper/issues/72\nThe second error is also probably because of that.\nOr rollback Graphite to 1.0.x or upgrade zipper to 1.0.x-alpha.. Hello @caldavid,\nUnfortunately, it will probably not work. Carbonlink was not designed as clustering protocol. But OTOH you don't need a dedicated rendering server, you can use only two graphite-web instances and point them to each other. . Yes, indeed. You can probably remove local part completely, so, CLUSTER_SERVERS=['xxx.xxx.xxx.xxx:80'] - local cluster should use carbonlink and direct access to whisper files.. CARBONLINK is for connecting graphite-web to carbon cache, usually localhost and port 700x.\nCLUSTER_SERVERS is for clustering graphite-web instances, protocol is http - so they are not interchangeable but ofc can be used together.\n. Put Transformnull(x,0) before countseries ?\nOn Wed, 17 Oct 2018 at 00:00, JosephFY notifications@github.com wrote:\n\nHello,\nWe are using 1.1.2 in a one node setup\nUsing Grafana with graphite as data source and one of our metrics looks\nlike the following\nservers.serverX.status\nThe status metric can have 3 values, 0,1,2 each one of these metrics means\nsomething to us. have about 10000 servers for this query\nI am trying to get counts for every number on a single stat panel in\nGrafana, for example\ncountSeries(currentAbove(servers.*.status, 1.1)) to get us count of\nservers reporting 2.\nThis is working just fine when there is some servers reporting 2 but when\nthere is no servers reporting 2, count function seems to have a problem\nbecause currentabove is returning no data points so the single stat is\ngiving server error because it's trying to count a \"no data point' results\nAny thoughts on this ? any work around that I am not aware off? we do a\nlot of those aggregation functions and in many cases\nThanks in advance\nThank you\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2367, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51mdalYx9_t6YLosaJoyVsARsTqD1ks5ullb6gaJpZM4XiaXG\n.\n. Yes, you're right, didn't realize that first.\n\n17 \u043e\u043a\u0442. 2018 \u0433. 10:18 PM \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044c \"Dan Cech\" notifications@github.com\n\u043d\u0430\u043f\u0438\u0441\u0430\u043b:\n@deniszh https://github.com/deniszh that won't help if the seriesList is\nempty, but it could be done with something like:\nfallbackSeries(countSeries(currentAbove(servers.*.status, 1.1)),\nconstantLine(0))\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2367#issuecomment-430773434,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51r2blnbLBSjvciZwj5EK8xT0CN_6ks5ul5ChgaJpZM4XiaXG\n.\n. Did I understand correctly that it will be not compatible with previous version? Should we provide migration script then?. Ah, right, hash was there, but not used for selecting. Cool, looks good then. :). Hi @andreasferber ,\nValid concern, indeed.. Hello @uwemaurer ,\nYou should run \"collectstatic\" command from Django to put files in /opt/graphite/static. That's a standart way how Django app works.\n. I'm fine with that option, why not. You can implement some optional parameter for that, with default value as false for backward compatibility.\nThanks!. Merged in 1.1.5. Can I haz more reviews for this? @piotr1212 @DanCech @iksaif ?\nThanks!. Nice catch, @piotr1212 !\nI opened https://github.com/graphite-project/graphite-web/issues/2384 for that.. I'm doubting that. If it's related to Django only and not Graphite per se, @Mohamedghalab, please ask support in Django community. For installing Graphite you do not need to run startproject command. Please refer to installation docs - https://graphite.readthedocs.io/en/latest/install.html. @piotr1212, could you please check this? I'm not sure that it's a proper fix.... Ah, test is there but it tests pretty much nothing, d-oh - https://github.com/graphite-project/graphite-web/blob/master/webapp/tests/test_readers_rrd.py\nWill fix that.. Thanks! I fixed a patch, will try to create RRD file and create a proper test.. Well, I still have a hope to create proper RRD test, but will do that otherwise.. @piotr1212 : it will be great! Not sure if I will have enough free time to implement that soon, was hesitating to merge it as is :(. Closed in the favor of https://github.com/graphite-project/graphite-web/pull/2383. Hello @dengshaochun ,\nPlease downgrade whitenoise to 2.0.6 or lower - newer version doesn't support Django 1.4.\nPlease also note that 0.9.16 is very old and 0.9.x branch is not supported anymore (i.e. we're not planning to make any 0.9.x releases). OK, agreed - https://github.com/graphite-project/graphite-web/pull/2379. Merged. Looks good! but should avg_zero be mentioned somewhere in docs?\nNot sure where do we need to put it though... Initial PR for whisper - https://github.com/graphite-project/whisper/pull/92 . @piotr1212 : could you please check this PR, if possible? Thanks!. Some patches from current master still need to be properly tested, and merged, e.g. https://github.com/graphite-project/carbon/pull/817\nI'm understanding your concern but I do not want to bring more issues in the next release then fixing.. yep, closing this. Thanks a lot, @piotr1212 ! Merging this.. graphite wsgi is consuming CPU only for read load, not for write load. Do you have some read load? how much? If yes - then wsgi can be just overloaded, you can try to tune number of wsgi threads and workers first - use any general WSGI / Django tuning documentation for that, e.g. https://stackoverflow.com/questions/30794363/how-to-improve-the-performance-of-apache-with-mod-wsgi  or https://serverfault.com/questions/592646/tuning-django-based-site-deployed-using-apache-wsgi-for-better-performance. Ah, very good point, @piotr1212 ! You can disable tags in carbon using ENABLE_TAGS = False if tags are not used, @balasenthil-d \n. sqllite db is not recommended for production usage, but in case of single host and no tagging it's usually working fine. In case of tagging you probably should use Redis / MySQL / Postgres.. IMO 1) is logic behavior (we're doing that assumption, I mean, NULL=0 quite often in other functions too) but we can make it optional.\n2) should be fixed, indeed.. IIRC search functionality was VERY ineffective (it just dumps all metric to the text file and run grep on it) and was disabled before 1.0.0 release.. Hello @idling11,\nUnfortunately, no such functionality currently exists out of box. You can use some suggestions from https://github.com/graphite-project/graphite-web/issues/26, e.g. write some script which will call /metrics/find?query=* url, then walk the tree. Or use find /opt/graphite/storage/whisper -type f -name *.wsp locally on Graphite server to get list of metrics.. I can't find that in docs now (d-oh), but I definitely remember that somewhere we have a metric description, and then it was mentioned that metric name should start with alphanumeric character.\nAgreed that we need to deny it on carbon level.  \ud83d\udc4d \n. Exact migration command really depends on source and destination Django and Graphite version, I'm afraid, it will be quite hard to create a really generic solution.\nBut still open for any suggestions or even PRs.. That's not a Graphite issue per se, that's a Django issue. django_content_type is not specific Graphite table, that's common Django table. So, you need to skip migrations, but not all, just initial - you need to use --fake-initial instead of --fake - check https://stackoverflow.com/a/29760818/1139639 and https://docs.djangoproject.com/en/1.8/ref/django-admin/#django-admin-option---fake-initial. That's not a Graphite issue per se, that's a Django issue. django_content_type is not specific Graphite table, that's common Django table. So, you need to skip migrations, but not all, just initial - you need to use --fake-initial instead of --fake - check https://stackoverflow.com/a/29760818/1139639 and https://docs.djangoproject.com/en/1.8/ref/django-admin/#django-admin-option---fake-initial. @GowthamShanmugam : \ncould you please elaborate which Graphite version do you use and how you trying to install it? \nThanks!. Then you not really upgrading Graphite on old server and still using 0.9.15. Please note that you can't install graphite with RPMs and then upgrade it with installation from source - probably RPMs using different paths (probably /usr/something), source installation using /opt/graphite.\nYou can find Graphite version if check /version url. If it's giving 404 - it's less then 1.0.0. My hypothesis is still the same, you still using old version somehow. Fresh installation work fine, right?\nWhat curl -v http://<your-graphite-url>/version says?. Ah, selinux. Good to know, @GowthamShanmugam , thanks!\n. Documentation looks sane. What do you mean when saying \"graphite doesn't start up\"? graphite-web doesn't start? apache doesn't start? carbon doesn't start? what's in the logs?. @njchandu : unfortunately, scripts above (e.g. bin/service.sh start ) are not part of Graphite and we can't say anything about them or Ansible automation. \nBut from information above I can say that you didn't apply migrations, please do that using something like \ncd /opt/graphite\nsudo PYTHONPATH=/opt/graphite/webapp/ django-admin.py syncdb --settings=graphite.settings\nor\ncd /opt/graphite\nsudo PYTHONPATH=/opt/graphite/webapp/ django-admin.py migrate --settings=graphite.settings --run-syncdb. @njchandu : this script is not part of Graphite installation and not required for its work, and we not able to provide help about it.\nGraphite-web is working in port 8080, try to open that port in web browser. Is it working? If not - what in the logs?. @njchandu : no need to touch script for now, just use browser.... Usually it's in /opt/graphite/webapp/log but it's configurable.. You can replace\npython ./manage.py syncdb --noinput\nwith\npython django-admin.py migrate --settings=graphite.settings --run-syncdb\n- that's a same command but for later django version.. Sorry, @njchandu, I'm really confused. 3 days ago you said that command was there and you ran it successfully - https://github.com/graphite-project/graphite-web/issues/2391#issuecomment-446809009\nEven when you started this thread django-admin.py was there...\ndjango-admin.py is part of Django installation if it's not there - then Django not installed properly.\nProbably, you need to describe your setup and what you trying to achieve more thoroughly. . @njchandu:\nyou need to run collectstatic command python django-admin.py collectstatic --settings=graphite.settings. .tarvis.yaml changes from #2383 are lost somehow...\nwill redo that. Hello!\nYour database file is owned by user graphite and group root hence not\nreadable by Apache user (not sure which one, it\u2019s OS dependent).\nYou need to make Apache user able to read and write graphite.db file, e.g.\nby chown command.\nOn Mon, 10 Dec 2018 at 07:48, Chris Jefferies notifications@github.com\nwrote:\n\nI'm trying to create a repeatable install with Ansible. None of the\nexisting Ansible modules (roles) work in my environment so integrating\nvarious pieces. The end game is to collect metrics and get them into\nGrafana.\nMost of it has come together - I have Apache working with mod_wsgi 4.6.5,\nCarbon is receiving metrics and storing it in Whisper, Grafana launches\nbeautifully... but can't get past this database error. This one is killin'\nme.\nAny tips mercifully appreciated. Thanks.\nUsing:\nApache 2.4,\nmod_wsgi 4.6.5,\nGraphite, Carbon, Whisper 1.1.3,\nPython 2.7,\nDjango 1.11.17,\ngeerlingguy.apache (ansible role),\nnsg.graphite (forked and modifying)\nConfigured in debug mode (set in\nopt/graphite/webapp/graphite/local_settings.py):\nRequest Method:   GET\nRequest URL:  http://tinaja-gr:8080/composer\nDjango Version:   1.11.17\nException Type:   OperationalError\nException Value:\nunable to open database file\nException Location:   /usr/local/lib/python2.7/dist-packages/django/db/backends/sqlite3/base.py in execute, line 328\nPython Executable:    /usr/bin/python\nPython Version:   2.7.13\nPython Path:\n['/usr/lib/python2.7',\n '/usr/lib/python2.7/plat-arm-linux-gnueabihf',\n '/usr/lib/python2.7/lib-tk',\n '/usr/lib/python2.7/lib-old',\n '/usr/lib/python2.7/lib-dynload',\n '/usr/local/lib/python2.7/dist-packages',\n '/usr/lib/python2.7/dist-packages',\n '/opt/graphite/webapp']```\nThe graphite.db file:\n```# ls -la /opt/graphite/storage/\ntotal 336\ndrwxr-xr-x 7 graphite root       4096 Dec  9 23:27 .\ndrwxr-xr-x 8 root     root       4096 Dec  9 21:33 ..\n-rw-r--r-- 1 graphite graphite      3 Dec  9 23:27 carbon-cache-a.pid\ndrwxr-xr-x 3 graphite root       4096 Dec  9 21:33 ceres\n-rwxrwxr-x 1 www-data www-data 311296 Dec  9 21:34 graphite.db\ndrwxr-xr-x 2 graphite root       4096 Dec  9 21:33 lists\ndrwxr-xr-x 4 graphite root       4096 Dec  9 21:33 log\ndrwxr-xr-x 3 graphite root       4096 Dec  9 21:33 rrd\ndrwxr-xr-x 5 graphite root       4096 Dec  9 21:34 whisper\nThe database layout in local_settings.py\nDATABASES = {\n    'default': {\n        'NAME': '/opt/graphite/storage/graphite.db',\n        'ENGINE': 'django.db.backends.sqlite3',\n        'USER': '',\n        'PASSWORD': '',\n        'HOST': '',\n        'PORT': ''\n    }\n}\nStack trace:\nFile \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/exception.py\" in inner\n  41.             response = get_response(request)\nFile \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\" in _get_response\n  187.                 response = self.process_exception_by_middleware(e, request)\nFile \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\" in _get_response\n  185.                 response = wrapped_callback(request, callback_args, *callback_kwargs)\nFile \"/opt/graphite/webapp/graphite/composer/views.py\" in composer\n  35.   profile = getProfile(request)\nFile \"/opt/graphite/webapp/graphite/user_util.py\" in getProfile\n  25.     return default_profile()\nFile \"/opt/graphite/webapp/graphite/user_util.py\" in default_profile\n  41.                                       'password': '!'})\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/manager.py\" in manager_method\n  85.                 return getattr(self.get_queryset(), name)(args, *kwargs)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/query.py\" in get_or_create\n  466.             return self._create_object_from_params(lookup, params)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/query.py\" in _create_object_from_params\n  500.                 obj = self.create(**params)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/query.py\" in create\n  394.         obj.save(force_insert=True, using=self.db)\nFile \"/usr/local/lib/python2.7/dist-packages/django/contrib/auth/base_user.py\" in save\n  80.         super(AbstractBaseUser, self).save(args, *kwargs)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/base.py\" in save\n  808.                        force_update=force_update, update_fields=update_fields)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/base.py\" in save_base\n  838.             updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/base.py\" in _save_table\n  924.             result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/base.py\" in _do_insert\n  963.                                using=using, raw=raw)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/manager.py\" in manager_method\n  85.                 return getattr(self.get_queryset(), name)(args, *kwargs)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/query.py\" in _insert\n  1079.         return query.get_compiler(using=using).execute_sql(return_id)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/sql/compiler.py\" in execute_sql\n  1112.                 cursor.execute(sql, params)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/backends/utils.py\" in execute\n  79.             return super(CursorDebugWrapper, self).execute(sql, params)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/backends/utils.py\" in execute\n  64.                 return self.cursor.execute(sql, params)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/utils.py\" in exit\n  94.                 six.reraise(dj_exc_type, dj_exc_value, traceback)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/backends/utils.py\" in execute\n  64.                 return self.cursor.execute(sql, params)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/backends/sqlite3/base.py\" in execute\n  328.         return Database.Cursor.execute(self, query, params)\nException Type: OperationalError at /composer\nException Value: unable to open database file\nGraphite.settings:\nSetting | Value\n-- | --\nABSOLUTE_URL_OVERRIDES | {}\nADMINS | []\nALLOWED_HOSTS | ['']\nALLOW_ANONYMOUS_CLI | True\nAPPEND_SLASH | False\nAUTHENTICATION_BACKENDS | ['django.contrib.auth.backends.ModelBackend']\nAUTH_PASSWORD_VALIDATORS | u'****'\nAUTH_USER_MODEL | u'auth.User'\nAUTO_REFRESH_INTERVAL | 60\nCACHES | {'default': {'BACKEND': 'django.core.cache.backends.dummy.DummyCache'}}\nCACHE_MIDDLEWARE_ALIAS | u'default'\nCACHE_MIDDLEWARE_KEY_PREFIX | u'***'\nCACHE_MIDDLEWARE_SECONDS | 600\nCARBONLINK_HASHING_KEYFUNC | u'**'\nCARBONLINK_HASHING_TYPE | 'carbon_ch'\nCARBONLINK_HOSTS | ['127.0.0.1:7002']\nCARBONLINK_RETRY_DELAY | 15\nCARBONLINK_TIMEOUT | 1.0\nCARBON_METRIC_PREFIX | 'carbon'\nCERES_DIR | '/opt/graphite/storage/ceres/'\nCLUSTER_SERVERS | []\nCONF_DIR | '/opt/graphite/conf'\nCSRF_COOKIE_AGE | 31449600\nCSRF_COOKIE_DOMAIN | None\nCSRF_COOKIE_HTTPONLY | False\nCSRF_COOKIE_NAME | u'csrftoken'\nCSRF_COOKIE_PATH | u'/'\nCSRF_COOKIE_SECURE | False\nCSRF_FAILURE_VIEW | u'django.views.csrf.csrf_failure'\nCSRF_HEADER_NAME | u'HTTP_X_CSRFTOKEN'\nCSRF_TRUSTED_ORIGINS | []\nCSRF_USE_SESSIONS | False\nDASHBOARD_CONF | '/opt/graphite/conf/dashboard.conf'\nDASHBOARD_REQUIRE_AUTHENTICATION | False\nDASHBOARD_REQUIRE_EDIT_GROUP | None\nDASHBOARD_REQUIRE_PERMISSIONS | False\nDATABASES | {'default': {'ATOMIC_REQUESTS': False,              'AUTOCOMMIT': True,              'CONN_MAX_AGE': 0,              'ENGINE': 'django.db.backends.sqlite3',              'HOST': '',              'NAME': '/opt/graphite/storage/graphite.db',              'OPTIONS': {},              'PASSWORD': u'**',              'PORT': '',              'TEST': {'CHARSET': None,                       'COLLATION': None,                       'MIRROR': None,                       'NAME': None},              'TIME_ZONE': None,              'USER': ''}}\nDATABASE_ROUTERS | []\nDATA_UPLOAD_MAX_MEMORY_SIZE | 2621440\nDATA_UPLOAD_MAX_NUMBER_FIELDS | 1000\nDATETIME_FORMAT | u'N j, Y, P'\nDATETIME_INPUT_FORMATS | [u'%Y-%m-%d %H:%M:%S',  u'%Y-%m-%d %H:%M:%S.%f',  u'%Y-%m-%d %H:%M',  u'%Y-%m-%d',  u'%m/%d/%Y %H:%M:%S',  u'%m/%d/%Y %H:%M:%S.%f',  u'%m/%d/%Y %H:%M',  u'%m/%d/%Y',  u'%m/%d/%y %H:%M:%S',  u'%m/%d/%y %H:%M:%S.%f',  u'%m/%d/%y %H:%M',  u'%m/%d/%y']\nDATE_FORMAT | '%m/%d'\nDATE_INPUT_FORMATS | [u'%Y-%m-%d',  u'%m/%d/%Y',  u'%m/%d/%y',  u'%b %d %Y',  u'%b %d, %Y',  u'%d %b %Y',  u'%d %b, %Y',  u'%B %d %Y',  u'%B %d, %Y',  u'%d %B %Y',  u'%d %B, %Y']\nDEBUG | True\nDEBUG_PROPAGATE_EXCEPTIONS | False\nDECIMAL_SEPARATOR | u'.'\nDEFAULT_CACHE_DURATION | 60\nDEFAULT_CACHE_POLICY | []\nDEFAULT_CHARSET | u'utf-8'\nDEFAULT_CONTENT_TYPE | u'text/html'\nDEFAULT_EXCEPTION_REPORTER_FILTER | u'django.views.debug.SafeExceptionReporterFilter'\nDEFAULT_FILE_STORAGE | u'django.core.files.storage.FileSystemStorage'\nDEFAULT_FROM_EMAIL | u'webmaster@localhost'\nDEFAULT_INDEX_TABLESPACE | u''\nDEFAULT_TABLESPACE | u''\nDEFAULT_XFILES_FACTOR | 0\nDISALLOWED_USER_AGENTS | []\nDJANGO_VERSION | (1, 11, 17, u'final', 0)\nDOCUMENTATION_URL | 'https://graphite.readthedocs.io/en/1.1.3/'\nDOCUMENTATION_VERSION | '1.1.3'\nEMAIL_BACKEND | u'django.core.mail.backends.smtp.EmailBackend'\nEMAIL_HOST | u'localhost'\nEMAIL_HOST_PASSWORD | u'**'\nEMAIL_HOST_USER | u''\nEMAIL_PORT | 25\nEMAIL_SSL_CERTFILE | None\nEMAIL_SSL_KEYFILE | u'**'\nEMAIL_SUBJECT_PREFIX | u'[Django] '\nEMAIL_TIMEOUT | None\nEMAIL_USE_LOCALTIME | False\nEMAIL_USE_SSL | False\nEMAIL_USE_TLS | False\nFETCH_TIMEOUT | 6.0\nFILE_CHARSET | u'utf-8'\nFILE_UPLOAD_DIRECTORY_PERMISSIONS | None\nFILE_UPLOAD_HANDLERS | [u'django.core.files.uploadhandler.MemoryFileUploadHandler',  u'django.core.files.uploadhandler.TemporaryFileUploadHandler']\nFILE_UPLOAD_MAX_MEMORY_SIZE | 2621440\nFILE_UPLOAD_PERMISSIONS | None\nFILE_UPLOAD_TEMP_DIR | None\nFIND_CACHE_DURATION | 300\nFIND_TIMEOUT | 3.0\nFIND_TOLERANCE | 600\nFIRST_DAY_OF_WEEK | 0\nFIXTURE_DIRS | []\nFLUSHRRDCACHED | ''\nFORCE_SCRIPT_NAME | None\nFORMAT_MODULE_PATH | None\nFORM_RENDERER | u'django.forms.renderers.DjangoTemplates'\nFUNCTION_PLUGINS | []\nGRAPHITE_ROOT | '/opt/graphite'\nGRAPHITE_WEB_APP_SETTINGS_LOADED | True\nGRAPHTEMPLATES_CONF | '/opt/graphite/conf/graphTemplates.conf'\nIGNORABLE_404_URLS | []\nINDEX_FILE | '/opt/graphite/storage/index'\nINSTALLED_APPS | ('graphite.account',  'graphite.browser',  'graphite.composer',  'graphite.dashboard',  'graphite.events',  'graphite.functions',  'graphite.metrics',  'graphite.render',  'graphite.tags',  'graphite.url_shortener',  'graphite.whitelist',  'django.contrib.auth',  'django.contrib.sessions',  'django.contrib.admin',  'django.contrib.contenttypes',  'django.contrib.staticfiles',  'tagging')\nINTERNAL_IPS | []\nINTRACLUSTER_HTTPS | False\nJAVASCRIPT_DEBUG | False\nLANGUAGES | [(u'af', u'Afrikaans'),  (u'ar', u'Arabic'),  (u'ast', u'Asturian'),  (u'az', u'Azerbaijani'),  (u'bg', u'Bulgarian'),  (u'be', u'Belarusian'),  (u'bn', u'Bengali'),  (u'br', u'Breton'),  (u'bs', u'Bosnian'),  (u'ca', u'Catalan'),  (u'cs', u'Czech'),  (u'cy', u'Welsh'),  (u'da', u'Danish'),  (u'de', u'German'),  (u'dsb', u'Lower Sorbian'),  (u'el', u'Greek'),  (u'en', u'English'),  (u'en-au', u'Australian English'),  (u'en-gb', u'British English'),  (u'eo', u'Esperanto'),  (u'es', u'Spanish'),  (u'es-ar', u'Argentinian Spanish'),  (u'es-co', u'Colombian Spanish'),  (u'es-mx', u'Mexican Spanish'),  (u'es-ni', u'Nicaraguan Spanish'),  (u'es-ve', u'Venezuelan Spanish'),  (u'et', u'Estonian'),  (u'eu', u'Basque'),  (u'fa', u'Persian'),  (u'fi', u'Finnish'),  (u'fr', u'French'),  (u'fy', u'Frisian'),  (u'ga', u'Irish'),  (u'gd', u'Scottish Gaelic'),  (u'gl', u'Galician'),  (u'he', u'Hebrew'),  (u'hi', u'Hindi'),  (u'hr', u'Croatian'),  (u'hsb', u'Upper Sorbian'),  (u'hu', u'Hungarian'),  (u'ia', u'Interlingua'),  (u'id', u'Indonesian'),  (u'io', u'Ido'),  (u'is', u'Icelandic'),  (u'it', u'Italian'),  (u'ja', u'Japanese'),  (u'ka', u'Georgian'),  (u'kk', u'Kazakh'),  (u'km', u'Khmer'),  (u'kn', u'Kannada'),  (u'ko', u'Korean'),  (u'lb', u'Luxembourgish'),  (u'lt', u'Lithuanian'),  (u'lv', u'Latvian'),  (u'mk', u'Macedonian'),  (u'ml', u'Malayalam'),  (u'mn', u'Mongolian'),  (u'mr', u'Marathi'),  (u'my', u'Burmese'),  (u'nb', u'Norwegian Bokm\\xe5l'),  (u'ne', u'Nepali'),  (u'nl', u'Dutch'),  (u'nn', u'Norwegian Nynorsk'),  (u'os', u'Ossetic'),  (u'pa', u'Punjabi'),  (u'pl', u'Polish'),  (u'pt', u'Portuguese'),  (u'pt-br', u'Brazilian Portuguese'),  (u'ro', u'Romanian'),  (u'ru', u'Russian'),  (u'sk', u'Slovak'),  (u'sl', u'Slovenian'),  (u'sq', u'Albanian'),  (u'sr', u'Serbian'),  (u'sr-latn', u'Serbian Latin'),  (u'sv', u'Swedish'),  (u'sw', u'Swahili'),  (u'ta', u'Tamil'),  (u'te', u'Telugu'),  (u'th', u'Thai'),  (u'tr', u'Turkish'),  (u'tt', u'Tatar'),  (u'udm', u'Udmurt'),  (u'uk', u'Ukrainian'),  (u'ur', u'Urdu'),  (u'vi', u'Vietnamese'),  (u'zh-hans', u'Simplified Chinese'),  (u'zh-hant', u'Traditional Chinese')]\nLANGUAGES_BIDI | [u'he', u'ar', u'fa', u'ur']\nLANGUAGE_CODE | 'en-us'\nLANGUAGE_COOKIE_AGE | None\nLANGUAGE_COOKIE_DOMAIN | None\nLANGUAGE_COOKIE_NAME | u'django_language'\nLANGUAGE_COOKIE_PATH | u'/'\nLDAP_BASE_PASS | u'**'\nLDAP_BASE_USER | ''\nLDAP_PORT | 389\nLDAP_SEARCH_BASE | ''\nLDAP_SERVER | ''\nLDAP_URI | None\nLDAP_USER_DN_TEMPLATE | None\nLDAP_USER_QUERY | ''\nLDAP_USE_TLS | False\nLEGEND_MAX_ITEMS | 10\nLOCALE_PATHS | []\nLOGGING | {}\nLOGGING_CONFIG | u'logging.config.dictConfig'\nLOGIN_REDIRECT_URL | u'/accounts/profile/'\nLOGIN_URL | u'/account/login'\nLOGOUT_REDIRECT_URL | None\nLOG_CACHE_PERFORMANCE | False\nLOG_DIR | '/opt/graphite/storage/log/webapp'\nLOG_FILE_CACHE | 'cache.log'\nLOG_FILE_EXCEPTION | 'exception.log'\nLOG_FILE_INFO | 'info.log'\nLOG_FILE_RENDERING | 'rendering.log'\nLOG_RENDERING_PERFORMANCE | False\nLOG_ROTATION | True\nLOG_ROTATION_COUNT | 1\nMANAGERS | []\nMAX_FETCH_RETRIES | 2\nMAX_TAG_LENGTH | 50\nMEDIA_ROOT | ''\nMEDIA_URL | ''\nMEMCACHE_HOSTS | []\nMEMCACHE_KEY_PREFIX | u'**'\nMEMCACHE_OPTIONS | {}\nMESSAGE_STORAGE | u'django.contrib.messages.storage.fallback.FallbackStorage'\nMETRICS_FIND_FAILURE_THRESHOLD | inf\nMETRICS_FIND_WARNING_THRESHOLD | inf\nMIDDLEWARE | ('graphite.middleware.LogExceptionsMiddleware',  'django.middleware.common.CommonMiddleware',  'django.middleware.gzip.GZipMiddleware',  'django.contrib.sessions.middleware.SessionMiddleware',  'django.contrib.auth.middleware.AuthenticationMiddleware',  'django.contrib.auth.middleware.SessionAuthenticationMiddleware',  'django.contrib.messages.middleware.MessageMiddleware')\nMIDDLEWARE_CLASSES | [u'django.middleware.common.CommonMiddleware',  u'django.middleware.csrf.CsrfViewMiddleware']\nMIGRATION_MODULES | {}\nMONTH_DAY_FORMAT | u'F j'\nNUMBER_GROUPING | 0\nPASSWORD_HASHERS | u'**'\nPASSWORD_RESET_TIMEOUT_DAYS | u'**'\nPOOL_MAX_WORKERS | 10\nPREPEND_WWW | False\nREMOTE_BUFFER_SIZE | 1048576\nREMOTE_EXCLUDE_LOCAL | False\nREMOTE_FETCH_TIMEOUT | None\nREMOTE_FIND_TIMEOUT | None\nREMOTE_RENDERING | False\nREMOTE_RENDER_CONNECT_TIMEOUT | 1.0\nREMOTE_RETRY_DELAY | 60.0\nREMOTE_STORE_FORWARD_HEADERS | []\nREMOTE_STORE_MERGE_RESULTS | True\nREMOTE_STORE_USE_POST | False\nREMOTE_USER_BACKEND | ''\nREMOTE_USER_MIDDLEWARE | ''\nRENDERING_HOSTS | []\nREPLICATION_FACTOR | 1\nROOT_URLCONF | 'graphite.urls'\nRRD_CF | 'AVERAGE'\nRRD_DIR | '/opt/graphite/storage/rrd/'\nSECRET_KEY | u'**'\nSECURE_BROWSER_XSS_FILTER | False\nSECURE_CONTENT_TYPE_NOSNIFF | False\nSECURE_HSTS_INCLUDE_SUBDOMAINS | False\nSECURE_HSTS_PRELOAD | False\nSECURE_HSTS_SECONDS | 0\nSECURE_PROXY_SSL_HEADER | None\nSECURE_REDIRECT_EXEMPT | []\nSECURE_SSL_HOST | None\nSECURE_SSL_REDIRECT | False\nSERVER_EMAIL | u'root@localhost'\nSESSION_CACHE_ALIAS | u'default'\nSESSION_COOKIE_AGE | 1209600\nSESSION_COOKIE_DOMAIN | None\nSESSION_COOKIE_HTTPONLY | True\nSESSION_COOKIE_NAME | u'sessionid'\nSESSION_COOKIE_PATH | u'/'\nSESSION_COOKIE_SECURE | False\nSESSION_ENGINE | u'django.contrib.sessions.backends.db'\nSESSION_EXPIRE_AT_BROWSER_CLOSE | False\nSESSION_FILE_PATH | None\nSESSION_SAVE_EVERY_REQUEST | False\nSESSION_SERIALIZER | u'django.contrib.sessions.serializers.JSONSerializer'\nSETTINGS_MODULE | 'graphite.settings'\nSHORT_DATETIME_FORMAT | u'm/d/Y P'\nSHORT_DATE_FORMAT | u'm/d/Y'\nSIGNING_BACKEND | u'django.core.signing.TimestampSigner'\nSILENCED_SYSTEM_CHECKS | ['urls.W002']\nSMTP_SERVER | 'localhost'\nSTANDARD_DIRS | ['/opt/graphite/storage/whisper/']\nSTATICFILES_DIRS | ('/opt/graphite/webapp/content',)\nSTATICFILES_FINDERS | [u'django.contrib.staticfiles.finders.FileSystemFinder',  u'django.contrib.staticfiles.finders.AppDirectoriesFinder']\nSTATICFILES_STORAGE | u'django.contrib.staticfiles.storage.StaticFilesStorage'\nSTATIC_ROOT | '/opt/graphite/static'\nSTATIC_URL | '/static/'\nSTORAGE_DIR | '/opt/graphite/storage'\nSTORAGE_FINDERS | ('graphite.finders.remote.RemoteFinder',  'graphite.finders.standard.StandardFinder')\nTAGDB | 'graphite.tags.localdatabase.LocalDatabaseTagDB'\nTAGDB_AUTOCOMPLETE_LIMIT | 100\nTAGDB_CACHE_DURATION | 60\nTAGDB_HTTP_AUTOCOMPLETE | False\nTAGDB_HTTP_PASSWORD | u'****'\nTAGDB_HTTP_URL | ''\nTAGDB_HTTP_USER | ''\nTAGDB_REDIS_DB | 0\nTAGDB_REDIS_HOST | 'localhost'\nTAGDB_REDIS_PORT | 6379\nTEMPLATES | [{'APP_DIRS': True,   'BACKEND': 'django.template.backends.django.DjangoTemplates',   'DIRS': ['/opt/graphite/webapp/graphite/templates'],   'OPTIONS': {'context_processors': ['django.contrib.auth.context_processors.auth',                                      'django.template.context_processors.debug',                                      'django.template.context_processors.i18n',                                      'django.template.context_processors.media',                                      'django.template.context_processors.static',                                      'django.template.context_processors.tz',                                      'django.contrib.messages.context_processors.messages']}}]\nTEMPLATE_DEBUG | False\nTEST_NON_SERIALIZED_APPS | []\nTEST_RUNNER | u'django.test.runner.DiscoverRunner'\nTHOUSAND_SEPARATOR | u','\nTIME_FORMAT | u'P'\nTIME_INPUT_FORMATS | [u'%H:%M:%S', u'%H:%M:%S.%f', u'%H:%M']\nTIME_ZONE | 'America/Chicago'\nURL_PREFIX | ''\nUSE_ETAGS | False\nUSE_I18N | True\nUSE_L10N | False\nUSE_LDAP_AUTH | False\nUSE_REMOTE_USER_AUTHENTICATION | False\nUSE_THOUSAND_SEPARATOR | False\nUSE_TZ | True\nUSE_WORKER_POOL | True\nUSE_X_FORWARDED_HOST | False\nUSE_X_FORWARDED_PORT | False\nWEBAPP_DIR | '/opt/graphite/webapp'\nWEBAPP_VERSION | '1.1.3'\nWEB_DIR | '/opt/graphite/webapp/graphite'\nWHISPER_DIR | '/opt/graphite/storage/whisper/'\nWHITELIST_FILE | '/opt/graphite/storage/lists/whitelist'\nWSGI_APPLICATION | None\nX_FRAME_OPTIONS | u'SAMEORIGIN'\nYEAR_MONTH_FORMAT | u'F Y'\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2393, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51pLtBEVZDUC4elawsYLh4nVO7nnLks5u3gO3gaJpZM4ZKwPR\n.\n. You can make file owned by graphite user and www-data group, and ofc\nwritable both for user and group - should work.\nNo magic here - Graphite-web is generic web application, which run under\nuser of web server.\n\nOn Mon, 10 Dec 2018 at 08:04, Denis Zhdanov denis.zhdanov@gmail.com wrote:\n\nHello!\nYour database file is owned by user graphite and group root hence not\nreadable by Apache user (not sure which one, it\u2019s OS dependent).\nYou need to make Apache user able to read and write graphite.db file, e.g.\nby chown command.\nOn Mon, 10 Dec 2018 at 07:48, Chris Jefferies notifications@github.com\nwrote:\n\nI'm trying to create a repeatable install with Ansible. None of the\nexisting Ansible modules (roles) work in my environment so integrating\nvarious pieces. The end game is to collect metrics and get them into\nGrafana.\nMost of it has come together - I have Apache working with mod_wsgi 4.6.5,\nCarbon is receiving metrics and storing it in Whisper, Grafana launches\nbeautifully... but can't get past this database error. This one is killin'\nme.\nAny tips mercifully appreciated. Thanks.\nUsing:\nApache 2.4,\nmod_wsgi 4.6.5,\nGraphite, Carbon, Whisper 1.1.3,\nPython 2.7,\nDjango 1.11.17,\ngeerlingguy.apache (ansible role),\nnsg.graphite (forked and modifying)\nConfigured in debug mode (set in\nopt/graphite/webapp/graphite/local_settings.py):\nRequest Method:  GET\nRequest URL: http://tinaja-gr:8080/composer\nDjango Version:  1.11.17\nException Type:  OperationalError\nException Value: \nunable to open database file\nException Location:  /usr/local/lib/python2.7/dist-packages/django/db/backends/sqlite3/base.py in execute, line 328\nPython Executable:   /usr/bin/python\nPython Version:  2.7.13\nPython Path: \n['/usr/lib/python2.7',\n '/usr/lib/python2.7/plat-arm-linux-gnueabihf',\n '/usr/lib/python2.7/lib-tk',\n '/usr/lib/python2.7/lib-old',\n '/usr/lib/python2.7/lib-dynload',\n '/usr/local/lib/python2.7/dist-packages',\n '/usr/lib/python2.7/dist-packages',\n '/opt/graphite/webapp']```\nThe graphite.db file:\n```# ls -la /opt/graphite/storage/\ntotal 336\ndrwxr-xr-x 7 graphite root       4096 Dec  9 23:27 .\ndrwxr-xr-x 8 root     root       4096 Dec  9 21:33 ..\n-rw-r--r-- 1 graphite graphite      3 Dec  9 23:27 carbon-cache-a.pid\ndrwxr-xr-x 3 graphite root       4096 Dec  9 21:33 ceres\n-rwxrwxr-x 1 www-data www-data 311296 Dec  9 21:34 graphite.db\ndrwxr-xr-x 2 graphite root       4096 Dec  9 21:33 lists\ndrwxr-xr-x 4 graphite root       4096 Dec  9 21:33 log\ndrwxr-xr-x 3 graphite root       4096 Dec  9 21:33 rrd\ndrwxr-xr-x 5 graphite root       4096 Dec  9 21:34 whisper\nThe database layout in local_settings.py\nDATABASES = {\n    'default': {\n        'NAME': '/opt/graphite/storage/graphite.db',\n        'ENGINE': 'django.db.backends.sqlite3',\n        'USER': '',\n        'PASSWORD': '',\n        'HOST': '',\n        'PORT': ''\n    }\n}\nStack trace:\nFile \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/exception.py\" in inner\n  41.             response = get_response(request)\nFile \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\" in _get_response\n  187.                 response = self.process_exception_by_middleware(e, request)\nFile \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\" in _get_response\n  185.                 response = wrapped_callback(request, callback_args, *callback_kwargs)\nFile \"/opt/graphite/webapp/graphite/composer/views.py\" in composer\n  35.   profile = getProfile(request)\nFile \"/opt/graphite/webapp/graphite/user_util.py\" in getProfile\n  25.     return default_profile()\nFile \"/opt/graphite/webapp/graphite/user_util.py\" in default_profile\n  41.                                       'password': '!'})\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/manager.py\" in manager_method\n  85.                 return getattr(self.get_queryset(), name)(args, *kwargs)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/query.py\" in get_or_create\n  466.             return self._create_object_from_params(lookup, params)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/query.py\" in _create_object_from_params\n  500.                 obj = self.create(**params)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/query.py\" in create\n  394.         obj.save(force_insert=True, using=self.db)\nFile \"/usr/local/lib/python2.7/dist-packages/django/contrib/auth/base_user.py\" in save\n  80.         super(AbstractBaseUser, self).save(args, *kwargs)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/base.py\" in save\n  808.                        force_update=force_update, update_fields=update_fields)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/base.py\" in save_base\n  838.             updated = self._save_table(raw, cls, force_insert, force_update, using, update_fields)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/base.py\" in _save_table\n  924.             result = self._do_insert(cls._base_manager, using, fields, update_pk, raw)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/base.py\" in _do_insert\n  963.                                using=using, raw=raw)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/manager.py\" in manager_method\n  85.                 return getattr(self.get_queryset(), name)(args, *kwargs)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/query.py\" in _insert\n  1079.         return query.get_compiler(using=using).execute_sql(return_id)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/models/sql/compiler.py\" in execute_sql\n  1112.                 cursor.execute(sql, params)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/backends/utils.py\" in execute\n  79.             return super(CursorDebugWrapper, self).execute(sql, params)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/backends/utils.py\" in execute\n  64.                 return self.cursor.execute(sql, params)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/utils.py\" in exit\n  94.                 six.reraise(dj_exc_type, dj_exc_value, traceback)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/backends/utils.py\" in execute\n  64.                 return self.cursor.execute(sql, params)\nFile \"/usr/local/lib/python2.7/dist-packages/django/db/backends/sqlite3/base.py\" in execute\n  328.         return Database.Cursor.execute(self, query, params)\nException Type: OperationalError at /composer\nException Value: unable to open database file\nGraphite.settings:\nSetting | Value\n-- | --\nABSOLUTE_URL_OVERRIDES | {}\nADMINS | []\nALLOWED_HOSTS | ['']\nALLOW_ANONYMOUS_CLI | True\nAPPEND_SLASH | False\nAUTHENTICATION_BACKENDS | ['django.contrib.auth.backends.ModelBackend']\nAUTH_PASSWORD_VALIDATORS | u'****'\nAUTH_USER_MODEL | u'auth.User'\nAUTO_REFRESH_INTERVAL | 60\nCACHES | {'default': {'BACKEND': 'django.core.cache.backends.dummy.DummyCache'}}\nCACHE_MIDDLEWARE_ALIAS | u'default'\nCACHE_MIDDLEWARE_KEY_PREFIX | u'***'\nCACHE_MIDDLEWARE_SECONDS | 600\nCARBONLINK_HASHING_KEYFUNC | u'**'\nCARBONLINK_HASHING_TYPE | 'carbon_ch'\nCARBONLINK_HOSTS | ['127.0.0.1:7002']\nCARBONLINK_RETRY_DELAY | 15\nCARBONLINK_TIMEOUT | 1.0\nCARBON_METRIC_PREFIX | 'carbon'\nCERES_DIR | '/opt/graphite/storage/ceres/'\nCLUSTER_SERVERS | []\nCONF_DIR | '/opt/graphite/conf'\nCSRF_COOKIE_AGE | 31449600\nCSRF_COOKIE_DOMAIN | None\nCSRF_COOKIE_HTTPONLY | False\nCSRF_COOKIE_NAME | u'csrftoken'\nCSRF_COOKIE_PATH | u'/'\nCSRF_COOKIE_SECURE | False\nCSRF_FAILURE_VIEW | u'django.views.csrf.csrf_failure'\nCSRF_HEADER_NAME | u'HTTP_X_CSRFTOKEN'\nCSRF_TRUSTED_ORIGINS | []\nCSRF_USE_SESSIONS | False\nDASHBOARD_CONF | '/opt/graphite/conf/dashboard.conf'\nDASHBOARD_REQUIRE_AUTHENTICATION | False\nDASHBOARD_REQUIRE_EDIT_GROUP | None\nDASHBOARD_REQUIRE_PERMISSIONS | False\nDATABASES | {'default': {'ATOMIC_REQUESTS': False,              'AUTOCOMMIT': True,              'CONN_MAX_AGE': 0,              'ENGINE': 'django.db.backends.sqlite3',              'HOST': '',              'NAME': '/opt/graphite/storage/graphite.db',              'OPTIONS': {},              'PASSWORD': u'**',              'PORT': '',              'TEST': {'CHARSET': None,                       'COLLATION': None,                       'MIRROR': None,                       'NAME': None},              'TIME_ZONE': None,              'USER': ''}}\nDATABASE_ROUTERS | []\nDATA_UPLOAD_MAX_MEMORY_SIZE | 2621440\nDATA_UPLOAD_MAX_NUMBER_FIELDS | 1000\nDATETIME_FORMAT | u'N j, Y, P'\nDATETIME_INPUT_FORMATS | [u'%Y-%m-%d %H:%M:%S',  u'%Y-%m-%d %H:%M:%S.%f',  u'%Y-%m-%d %H:%M',  u'%Y-%m-%d',  u'%m/%d/%Y %H:%M:%S',  u'%m/%d/%Y %H:%M:%S.%f',  u'%m/%d/%Y %H:%M',  u'%m/%d/%Y',  u'%m/%d/%y %H:%M:%S',  u'%m/%d/%y %H:%M:%S.%f',  u'%m/%d/%y %H:%M',  u'%m/%d/%y']\nDATE_FORMAT | '%m/%d'\nDATE_INPUT_FORMATS | [u'%Y-%m-%d',  u'%m/%d/%Y',  u'%m/%d/%y',  u'%b %d %Y',  u'%b %d, %Y',  u'%d %b %Y',  u'%d %b, %Y',  u'%B %d %Y',  u'%B %d, %Y',  u'%d %B %Y',  u'%d %B, %Y']\nDEBUG | True\nDEBUG_PROPAGATE_EXCEPTIONS | False\nDECIMAL_SEPARATOR | u'.'\nDEFAULT_CACHE_DURATION | 60\nDEFAULT_CACHE_POLICY | []\nDEFAULT_CHARSET | u'utf-8'\nDEFAULT_CONTENT_TYPE | u'text/html'\nDEFAULT_EXCEPTION_REPORTER_FILTER | u'django.views.debug.SafeExceptionReporterFilter'\nDEFAULT_FILE_STORAGE | u'django.core.files.storage.FileSystemStorage'\nDEFAULT_FROM_EMAIL | u'webmaster@localhost'\nDEFAULT_INDEX_TABLESPACE | u''\nDEFAULT_TABLESPACE | u''\nDEFAULT_XFILES_FACTOR | 0\nDISALLOWED_USER_AGENTS | []\nDJANGO_VERSION | (1, 11, 17, u'final', 0)\nDOCUMENTATION_URL | 'https://graphite.readthedocs.io/en/1.1.3/'\nDOCUMENTATION_VERSION | '1.1.3'\nEMAIL_BACKEND | u'django.core.mail.backends.smtp.EmailBackend'\nEMAIL_HOST | u'localhost'\nEMAIL_HOST_PASSWORD | u'**'\nEMAIL_HOST_USER | u''\nEMAIL_PORT | 25\nEMAIL_SSL_CERTFILE | None\nEMAIL_SSL_KEYFILE | u'**'\nEMAIL_SUBJECT_PREFIX | u'[Django] '\nEMAIL_TIMEOUT | None\nEMAIL_USE_LOCALTIME | False\nEMAIL_USE_SSL | False\nEMAIL_USE_TLS | False\nFETCH_TIMEOUT | 6.0\nFILE_CHARSET | u'utf-8'\nFILE_UPLOAD_DIRECTORY_PERMISSIONS | None\nFILE_UPLOAD_HANDLERS | [u'django.core.files.uploadhandler.MemoryFileUploadHandler',  u'django.core.files.uploadhandler.TemporaryFileUploadHandler']\nFILE_UPLOAD_MAX_MEMORY_SIZE | 2621440\nFILE_UPLOAD_PERMISSIONS | None\nFILE_UPLOAD_TEMP_DIR | None\nFIND_CACHE_DURATION | 300\nFIND_TIMEOUT | 3.0\nFIND_TOLERANCE | 600\nFIRST_DAY_OF_WEEK | 0\nFIXTURE_DIRS | []\nFLUSHRRDCACHED | ''\nFORCE_SCRIPT_NAME | None\nFORMAT_MODULE_PATH | None\nFORM_RENDERER | u'django.forms.renderers.DjangoTemplates'\nFUNCTION_PLUGINS | []\nGRAPHITE_ROOT | '/opt/graphite'\nGRAPHITE_WEB_APP_SETTINGS_LOADED | True\nGRAPHTEMPLATES_CONF | '/opt/graphite/conf/graphTemplates.conf'\nIGNORABLE_404_URLS | []\nINDEX_FILE | '/opt/graphite/storage/index'\nINSTALLED_APPS | ('graphite.account',  'graphite.browser',  'graphite.composer',  'graphite.dashboard',  'graphite.events',  'graphite.functions',  'graphite.metrics',  'graphite.render',  'graphite.tags',  'graphite.url_shortener',  'graphite.whitelist',  'django.contrib.auth',  'django.contrib.sessions',  'django.contrib.admin',  'django.contrib.contenttypes',  'django.contrib.staticfiles',  'tagging')\nINTERNAL_IPS | []\nINTRACLUSTER_HTTPS | False\nJAVASCRIPT_DEBUG | False\nLANGUAGES | [(u'af', u'Afrikaans'),  (u'ar', u'Arabic'),  (u'ast', u'Asturian'),  (u'az', u'Azerbaijani'),  (u'bg', u'Bulgarian'),  (u'be', u'Belarusian'),  (u'bn', u'Bengali'),  (u'br', u'Breton'),  (u'bs', u'Bosnian'),  (u'ca', u'Catalan'),  (u'cs', u'Czech'),  (u'cy', u'Welsh'),  (u'da', u'Danish'),  (u'de', u'German'),  (u'dsb', u'Lower Sorbian'),  (u'el', u'Greek'),  (u'en', u'English'),  (u'en-au', u'Australian English'),  (u'en-gb', u'British English'),  (u'eo', u'Esperanto'),  (u'es', u'Spanish'),  (u'es-ar', u'Argentinian Spanish'),  (u'es-co', u'Colombian Spanish'),  (u'es-mx', u'Mexican Spanish'),  (u'es-ni', u'Nicaraguan Spanish'),  (u'es-ve', u'Venezuelan Spanish'),  (u'et', u'Estonian'),  (u'eu', u'Basque'),  (u'fa', u'Persian'),  (u'fi', u'Finnish'),  (u'fr', u'French'),  (u'fy', u'Frisian'),  (u'ga', u'Irish'),  (u'gd', u'Scottish Gaelic'),  (u'gl', u'Galician'),  (u'he', u'Hebrew'),  (u'hi', u'Hindi'),  (u'hr', u'Croatian'),  (u'hsb', u'Upper Sorbian'),  (u'hu', u'Hungarian'),  (u'ia', u'Interlingua'),  (u'id', u'Indonesian'),  (u'io', u'Ido'),  (u'is', u'Icelandic'),  (u'it', u'Italian'),  (u'ja', u'Japanese'),  (u'ka', u'Georgian'),  (u'kk', u'Kazakh'),  (u'km', u'Khmer'),  (u'kn', u'Kannada'),  (u'ko', u'Korean'),  (u'lb', u'Luxembourgish'),  (u'lt', u'Lithuanian'),  (u'lv', u'Latvian'),  (u'mk', u'Macedonian'),  (u'ml', u'Malayalam'),  (u'mn', u'Mongolian'),  (u'mr', u'Marathi'),  (u'my', u'Burmese'),  (u'nb', u'Norwegian Bokm\\xe5l'),  (u'ne', u'Nepali'),  (u'nl', u'Dutch'),  (u'nn', u'Norwegian Nynorsk'),  (u'os', u'Ossetic'),  (u'pa', u'Punjabi'),  (u'pl', u'Polish'),  (u'pt', u'Portuguese'),  (u'pt-br', u'Brazilian Portuguese'),  (u'ro', u'Romanian'),  (u'ru', u'Russian'),  (u'sk', u'Slovak'),  (u'sl', u'Slovenian'),  (u'sq', u'Albanian'),  (u'sr', u'Serbian'),  (u'sr-latn', u'Serbian Latin'),  (u'sv', u'Swedish'),  (u'sw', u'Swahili'),  (u'ta', u'Tamil'),  (u'te', u'Telugu'),  (u'th', u'Thai'),  (u'tr', u'Turkish'),  (u'tt', u'Tatar'),  (u'udm', u'Udmurt'),  (u'uk', u'Ukrainian'),  (u'ur', u'Urdu'),  (u'vi', u'Vietnamese'),  (u'zh-hans', u'Simplified Chinese'),  (u'zh-hant', u'Traditional Chinese')]\nLANGUAGES_BIDI | [u'he', u'ar', u'fa', u'ur']\nLANGUAGE_CODE | 'en-us'\nLANGUAGE_COOKIE_AGE | None\nLANGUAGE_COOKIE_DOMAIN | None\nLANGUAGE_COOKIE_NAME | u'django_language'\nLANGUAGE_COOKIE_PATH | u'/'\nLDAP_BASE_PASS | u'**'\nLDAP_BASE_USER | ''\nLDAP_PORT | 389\nLDAP_SEARCH_BASE | ''\nLDAP_SERVER | ''\nLDAP_URI | None\nLDAP_USER_DN_TEMPLATE | None\nLDAP_USER_QUERY | ''\nLDAP_USE_TLS | False\nLEGEND_MAX_ITEMS | 10\nLOCALE_PATHS | []\nLOGGING | {}\nLOGGING_CONFIG | u'logging.config.dictConfig'\nLOGIN_REDIRECT_URL | u'/accounts/profile/'\nLOGIN_URL | u'/account/login'\nLOGOUT_REDIRECT_URL | None\nLOG_CACHE_PERFORMANCE | False\nLOG_DIR | '/opt/graphite/storage/log/webapp'\nLOG_FILE_CACHE | 'cache.log'\nLOG_FILE_EXCEPTION | 'exception.log'\nLOG_FILE_INFO | 'info.log'\nLOG_FILE_RENDERING | 'rendering.log'\nLOG_RENDERING_PERFORMANCE | False\nLOG_ROTATION | True\nLOG_ROTATION_COUNT | 1\nMANAGERS | []\nMAX_FETCH_RETRIES | 2\nMAX_TAG_LENGTH | 50\nMEDIA_ROOT | ''\nMEDIA_URL | ''\nMEMCACHE_HOSTS | []\nMEMCACHE_KEY_PREFIX | u'**'\nMEMCACHE_OPTIONS | {}\nMESSAGE_STORAGE | u'django.contrib.messages.storage.fallback.FallbackStorage'\nMETRICS_FIND_FAILURE_THRESHOLD | inf\nMETRICS_FIND_WARNING_THRESHOLD | inf\nMIDDLEWARE | ('graphite.middleware.LogExceptionsMiddleware',  'django.middleware.common.CommonMiddleware',  'django.middleware.gzip.GZipMiddleware',  'django.contrib.sessions.middleware.SessionMiddleware',  'django.contrib.auth.middleware.AuthenticationMiddleware',  'django.contrib.auth.middleware.SessionAuthenticationMiddleware',  'django.contrib.messages.middleware.MessageMiddleware')\nMIDDLEWARE_CLASSES | [u'django.middleware.common.CommonMiddleware',  u'django.middleware.csrf.CsrfViewMiddleware']\nMIGRATION_MODULES | {}\nMONTH_DAY_FORMAT | u'F j'\nNUMBER_GROUPING | 0\nPASSWORD_HASHERS | u'**'\nPASSWORD_RESET_TIMEOUT_DAYS | u'**'\nPOOL_MAX_WORKERS | 10\nPREPEND_WWW | False\nREMOTE_BUFFER_SIZE | 1048576\nREMOTE_EXCLUDE_LOCAL | False\nREMOTE_FETCH_TIMEOUT | None\nREMOTE_FIND_TIMEOUT | None\nREMOTE_RENDERING | False\nREMOTE_RENDER_CONNECT_TIMEOUT | 1.0\nREMOTE_RETRY_DELAY | 60.0\nREMOTE_STORE_FORWARD_HEADERS | []\nREMOTE_STORE_MERGE_RESULTS | True\nREMOTE_STORE_USE_POST | False\nREMOTE_USER_BACKEND | ''\nREMOTE_USER_MIDDLEWARE | ''\nRENDERING_HOSTS | []\nREPLICATION_FACTOR | 1\nROOT_URLCONF | 'graphite.urls'\nRRD_CF | 'AVERAGE'\nRRD_DIR | '/opt/graphite/storage/rrd/'\nSECRET_KEY | u'**'\nSECURE_BROWSER_XSS_FILTER | False\nSECURE_CONTENT_TYPE_NOSNIFF | False\nSECURE_HSTS_INCLUDE_SUBDOMAINS | False\nSECURE_HSTS_PRELOAD | False\nSECURE_HSTS_SECONDS | 0\nSECURE_PROXY_SSL_HEADER | None\nSECURE_REDIRECT_EXEMPT | []\nSECURE_SSL_HOST | None\nSECURE_SSL_REDIRECT | False\nSERVER_EMAIL | u'root@localhost'\nSESSION_CACHE_ALIAS | u'default'\nSESSION_COOKIE_AGE | 1209600\nSESSION_COOKIE_DOMAIN | None\nSESSION_COOKIE_HTTPONLY | True\nSESSION_COOKIE_NAME | u'sessionid'\nSESSION_COOKIE_PATH | u'/'\nSESSION_COOKIE_SECURE | False\nSESSION_ENGINE | u'django.contrib.sessions.backends.db'\nSESSION_EXPIRE_AT_BROWSER_CLOSE | False\nSESSION_FILE_PATH | None\nSESSION_SAVE_EVERY_REQUEST | False\nSESSION_SERIALIZER | u'django.contrib.sessions.serializers.JSONSerializer'\nSETTINGS_MODULE | 'graphite.settings'\nSHORT_DATETIME_FORMAT | u'm/d/Y P'\nSHORT_DATE_FORMAT | u'm/d/Y'\nSIGNING_BACKEND | u'django.core.signing.TimestampSigner'\nSILENCED_SYSTEM_CHECKS | ['urls.W002']\nSMTP_SERVER | 'localhost'\nSTANDARD_DIRS | ['/opt/graphite/storage/whisper/']\nSTATICFILES_DIRS | ('/opt/graphite/webapp/content',)\nSTATICFILES_FINDERS | [u'django.contrib.staticfiles.finders.FileSystemFinder',  u'django.contrib.staticfiles.finders.AppDirectoriesFinder']\nSTATICFILES_STORAGE | u'django.contrib.staticfiles.storage.StaticFilesStorage'\nSTATIC_ROOT | '/opt/graphite/static'\nSTATIC_URL | '/static/'\nSTORAGE_DIR | '/opt/graphite/storage'\nSTORAGE_FINDERS | ('graphite.finders.remote.RemoteFinder',  'graphite.finders.standard.StandardFinder')\nTAGDB | 'graphite.tags.localdatabase.LocalDatabaseTagDB'\nTAGDB_AUTOCOMPLETE_LIMIT | 100\nTAGDB_CACHE_DURATION | 60\nTAGDB_HTTP_AUTOCOMPLETE | False\nTAGDB_HTTP_PASSWORD | u'****'\nTAGDB_HTTP_URL | ''\nTAGDB_HTTP_USER | ''\nTAGDB_REDIS_DB | 0\nTAGDB_REDIS_HOST | 'localhost'\nTAGDB_REDIS_PORT | 6379\nTEMPLATES | [{'APP_DIRS': True,   'BACKEND': 'django.template.backends.django.DjangoTemplates',   'DIRS': ['/opt/graphite/webapp/graphite/templates'],   'OPTIONS': {'context_processors': ['django.contrib.auth.context_processors.auth',                                      'django.template.context_processors.debug',                                      'django.template.context_processors.i18n',                                      'django.template.context_processors.media',                                      'django.template.context_processors.static',                                      'django.template.context_processors.tz',                                      'django.contrib.messages.context_processors.messages']}}]\nTEMPLATE_DEBUG | False\nTEST_NON_SERIALIZED_APPS | []\nTEST_RUNNER | u'django.test.runner.DiscoverRunner'\nTHOUSAND_SEPARATOR | u','\nTIME_FORMAT | u'P'\nTIME_INPUT_FORMATS | [u'%H:%M:%S', u'%H:%M:%S.%f', u'%H:%M']\nTIME_ZONE | 'America/Chicago'\nURL_PREFIX | ''\nUSE_ETAGS | False\nUSE_I18N | True\nUSE_L10N | False\nUSE_LDAP_AUTH | False\nUSE_REMOTE_USER_AUTHENTICATION | False\nUSE_THOUSAND_SEPARATOR | False\nUSE_TZ | True\nUSE_WORKER_POOL | True\nUSE_X_FORWARDED_HOST | False\nUSE_X_FORWARDED_PORT | False\nWEBAPP_DIR | '/opt/graphite/webapp'\nWEBAPP_VERSION | '1.1.3'\nWEB_DIR | '/opt/graphite/webapp/graphite'\nWHISPER_DIR | '/opt/graphite/storage/whisper/'\nWHITELIST_FILE | '/opt/graphite/storage/lists/whitelist'\nWSGI_APPLICATION | None\nX_FRAME_OPTIONS | u'SAMEORIGIN'\nYEAR_MONTH_FORMAT | u'F Y'\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2393, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51pLtBEVZDUC4elawsYLh4nVO7nnLks5u3gO3gaJpZM4ZKwPR\n.\n\n\n. Probably. It\u2019s a generic Linux, so read and execute rights for directories\nshould be propagated. Check directory attributes and inheritance in\nhttps://wiki.archlinux.org/index.php/File_permissions_and_attributes\nE.g. if file is located in /opt/graphite/storage, /opt and /opt/graphite\ncan be owned by root and should have r-x for others (usually it has by\ndefault), /opt/graphite/storage can  be owned by graphite user and www-data\ngroup and should have rwxrwxr-x permission\n\nOn Mon, 10 Dec 2018 at 08:11, Chris Jefferies notifications@github.com\nwrote:\n\nhmm - it looks like this as shown above:\n-rwxrwxr-x 1 www-data www-data 311296 Dec 9 21:34 graphite.db\nwww-data is the apache user on Raspbian (Debian for Raspberry Pi). Does\nthe directory need these settings also?\nThank you,\nChris.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2393#issuecomment-445709528,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51hqiSpnGHipWr2lYgN93JT0D-Szsks5u3gkKgaJpZM4ZKwPR\n.\n. Apache should be configured to serve these files. Please check example\nconfig in\nhttps://graphite.readthedocs.io/en/latest/config-webapp.html\n\nOn Mon, 10 Dec 2018 at 08:57, Chris Jefferies notifications@github.com\nwrote:\n\nLooking at the graphite log, I see a dozen or so entries using this:\ntail -f /opt/graphite/storage/log/webapp/error.log\nclient denied by server configuration: /opt/graphite/static, referer:\nhttp://tinaja-gr:8080/composer?\nI'm not sure about the reference to composer; I'm using the URL:\nhttp://tinaja-gr:8080\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2393#issuecomment-445723518,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51tslznvMYxtoPTnHKTQ-apqY3nCTks5u3hPXgaJpZM4ZKwPR\n.\n. I'm not a lawyer, but I think fair use of logo, of course, allowed. \nWhat do you think, @DanCech ?. welll... I'll try to backport it in portions. A-ha, everything OK, except linter. Merging that and proceeding.. Strange, that went fine. Merging too.. Graphite has e.g. aliasSub() but I just tested - it's not helping in this case, for some reason Grafana not accepting variables from aliasSub() function.. All Graphite functions are for display purpose, but some functions operate on data values, some - on metric names. aliasSub() operates on metric names, so, theoretically, you can use aliasSub() when providing variable in Grafana, to replace all \"|\" with \"/|\" in metric name - but for some reason, it's not working on Grafana side (it works on Graphite).. I did only generic test, not \"|\" specific. But @DanCech is right, Grafana use find requests for getting metric variables, which not supporting graphite functions.. Hi @blesaffre ,\nSorry, I don't really understand what's the issue is from your explanation, and Graphite have no time() function, but if it's Grafana-related you need to ask it on Grafana repo? . @blesaffre : Oh, sorry, forgot about that function.\nBut works for me: I'm running Grafana 5.2.4 and Graphite 1.1.4:\n\n. @blesaffre : According to error you calling function like timeFunction(stats.sets.stats.kamailio.marek.customer..registration.....200.count, '\"The.time.series\"'), i.e. providing it with second parameter string \"The.time.series\".\nAccording to documentation, the second parameter should be step and it should be integer, not string.. @blesaffre : Ah, I know what's going on. \nIn Graphite function is accepting time series and usually returns time series - https://graphite.readthedocs.io/en/latest/functions.html\nYou can chain function calls, but normally you doing that directly. E.g. if you have series stats.sets.stats.kamailio.marek.customer.registration.200.count, and you need to apply e.g. time() and alias() function, you need to do it like this:\nalias(time(stats.sets.stats.kamailio.marek.customer.registration.200.count),'blah')\nGrafana assuming that you know how Graphite functions work, so, it was hiding that semantic in the user interface, showing that you applying chain of functions to metric name. But in reality, it's good old Graphite under the hood, and first argument is always coming from metric name or previous function call.\nYou can check real function call if you toggle edit mode (three stripes button -> Toggle Edit mode) or go to Query Inspector and check request.data (but then you need to apply some url decoder).\nSo, in your case you just need to call timeFunction function, without parameters. \"The.time.series\" in the example is only example of series name.. Ah, I'm wrong. Indeed, time() function IS NOT accepting time series as parameter, so, you can't (and shouldn't) include it in function call chain later. You can use it only as first function in the call chain.. Maybe it will be easier if you describe what are you trying to achieve?. Ok, then I'll change the date for tomorrow and will make a release now!. That issue \"Target WSGI script cannot be loaded as Python module\" repeating one time after another, usually on CentOS. I'm suspecting that happening when mixing different python versions for Apache.. Graphite 0.9.16 was released in 2015 and it supports Django 1.4 only - https://github.com/graphite-project/graphite-web/blob/0.9.16/requirements.txt#L32\nOr donwngrade Django to 1.4 or upgrade Graphite to latest release (1.1.5).. Thanks for finally sorting this, @ploxiln !\nWe'll need to fix docs. Unfortunately, it's not easy for already released versions, but at least, we can do that in master. @aussiearef: please do not take this as an offense, but please read this link first - https://www.tutorialspoint.com/unix/unix-file-permission.htm\nI found that surprisingly big amount of Linux users do not really understand how file and directory permissions really work.\nNext step - check permissions on the whole hierarchy for /var/log/graphite/info.log, i.e.\nls -al /var\nls -al /var/log\nls -al /var/log/graphite\nls -al /var/log/graphite/info.log\nFor Apache user (in Ubuntu it is indeed www-data), you should have write permissions for /var/log/graphite/info.log file, but also read and execute rights for other users for all directories - /var, /var/log and /var/log/graphite.\n\nYou can also use Docker installation from your link, it's much simpler.. Hello @aussiearef \n\nI have been a Windows user for 20 years hence struggling with Ubuntu\n\nIs it separate Ubuntu or you trying to install Graphite in WSL?\n\nIs it the Apache user that runs carbon-cache or is it the user who started the service?\n\nAs @piotr1212 said - it depends on the installation. But if it's coming from graphite.wsgi - it probably not carbon but graphite-web, trying to write logs.\n. @aussiearef \n\nUsing chown and chomod then rebooting the whole machine I will get rid of the permission denied error.\n\nReboot is not needed, sudo systemctl stop apache2 + sudo systemctl start apache2 should be enough.\n\nBut then after that I always have this:\nFile \"/usr/share/graphite-web/graphite.wsgi\", line 14, in \napplication = get_wsgi_application()\nFile \"/usr/lib/python2.7/dist-packages/django/init.py\", line 27, in setup\napps.populate(settings.INSTALLED_APPS)\n\nUnfortunately, it's a truncated error message, could you post the whole error message here?\n\nBut there is no user called \"_graphite\". Is this user created by Graphite?? Do I have to create it?!!\n\nProbably, debian packages doing that during install. We're not packing Graphite for Ubuntu or Redhat, package maintainers doing that.\n. Hello @aussiearef ,\n\n1- The problem was that the user \"_graphite\" had to have access to info.log and exception.log and now www-data user. So I granted access and then I could see some proper errors in the logs.\n\nWell, as I said before, we're not packing Graphite in packages, maintainers of distros doing that. IMHO it's not a good idea to have same file for logging from carbon and graphite-web - but looks like maintainers think differently.\nIt's a quite hard problem with the distribution of complex software products, it's hard to solve it without some generic distribution mechanism, like e.g. Docker ;)\n\n2- There is an exception raised by Django saying \"context must be dict not Context\". It is because \"Context is deprecated but it is used in views.py file. I edited this file and removed the Context and so this error went away too. (I see that this is already fixed in master\n\nYes, that's because of Django 1.11, it was fixed not only in master but in 1.1.0 and newer versions. Again, dependencies are another complex issue.. Graphite logs looks fine. You didn\u2019t publish nginx config and logs, though.\nCould you please do that?\nOn Thu, 24 Jan 2019 at 19:00, Chandan Javaregowda notifications@github.com\nwrote:\n\nHere's what I'm trying to do.\nsudo yum install gcc jq python-pip python-devel cairo cairo-deve pycairo pycairo-devel xfsprogs libffi-devel\nsudo pip install python-memcached twisted django==1.11.18 whisper=1.1.5 graphite-web==1.1.5 carbon==1.1.5\nsudo yum install https://github.com/lomik/go-carbon/releases/download/v0.13.0/go-carbon-0.13.0-1.x86_64.rpm\nsudo mkdir -p /opt/graphite\nsudo mkdir -p /var/log/go-carbon\nsudo mkdir -p /etc/cron.healthcheck\nsudo mkdir -p /mnt/data/graphite\nsudo chmod -R 755 /var/log/go-carbon\nsudo chown -R ec2-user:ec2-user /mnt/data/\nsudo vi /usr/local/etc/go-carbon.conf\nsudo vi /etc/cron.hourly/whispercleanup.sh\nsudo vi /etc/cron.healthcheck/selfhealthcheck.sh\nsudo vi /etc/cron.d/healthcheck\nsudo vi /etc/cron.daily/s3backup.sh\nsudo vi /etc/cron.d/dxedge_elb_cron\nsudo vi /usr/local/bin/dxedge_elb_count.sh\nsudo cp dashboard/conf/graphite/conf/ /opt/graphite/conf\nsudo cp dashboard/conf/graphite/conf/ /opt/graphite/conf\nsudo cp dashboard/conf/graphite/webapp/graphite/local_settings.py /opt/graphite/webapp/graphite/local_settings.py\nsudo mv /opt/graphite/storage /mnt/data/graphite/storage\nsudo ln -s /mnt/data/graphite/storage /opt/graphite/storage\nsudo PYTHONPATH=/opt/graphite/webapp /usr/local/bin/django-admin.py migrate --settings=graphite.settings --run-syncdb\nsudo PYTHONPATH=/opt/graphite/webapp /usr/local/bin/django-admin.py collectstatic --settings=graphite.settings\nsudo yum install nginx\nsudo pip install gunicorn\nsudo touch /var/log/nginx/graphite.access.log\nsudo touch /var/log/nginx/graphite.error.log\nsudo chmod 640 /var/log/nginx/graphite.\nsudo chown www-data:www-data /var/log/nginx/graphite.\nsudo touch /var/log/nginx/graphite.access.log\nsudo touch /var/log/nginx/graphite.error.log\nsudo chmod 640 /var/log/nginx/graphite.access.log\nsudo chmod 640 /var/log/nginx/graphite.error.log\nsudo chown ec2-user:ec2-user /var/log/gunicorn.log\nsudo chown ec2-user:ec2-user /var/log/nginx/graphite.error.log\nsudo ln -s /etc/nginx/sites-available/graphite /etc/nginx/sites-enabled\nsudo rm -f /etc/nginx/sites-enabled/default\nsudo vi /etc/nginx/sites-available/graphite\nsudo mkdir -p /etc/nginx/sites-available\nsudo vi /etc/nginx/sites-available/graphite\nsudo service nginx reload &\nsudo chown -R ec2-user:ec2-user /mnt/data/graphite/storage\nsudo chown -R ec2-user:ec2-user /opt/graphite/storage\nsudo go-carbon -config /usr/local/etc/go-carbon.conf -daemon\nPYTHONPATH=/opt/graphite/webapp gunicorn wsgi --workers=4 --bind=127.0.0.1:8080 --log-file=/var/log/gunicorn.log --preload --pythonpath=/opt/graphite/webapp/graphite &\nWhen I try to hit the ip/dashboard the page doesn't load.\nHere's what I see in the gunicorn.log\n[ec2-user@ip-172-17-70-84 log]$ cat gunicorn.log\n[2019-01-24 05:53:44 +0000] [40453] [INFO] Starting gunicorn 19.9.0\n[2019-01-24 05:53:44 +0000] [40453] [INFO] Listening at: http://127.0.0.1:8080 (40453)\n[2019-01-24 05:53:44 +0000] [40453] [INFO] Using worker: sync\n[2019-01-24 05:53:44 +0000] [40462] [INFO] Booting worker with pid: 40462\n[2019-01-24 05:53:44 +0000] [40463] [INFO] Booting worker with pid: 40463\n[2019-01-24 05:53:44 +0000] [40464] [INFO] Booting worker with pid: 40464\n[2019-01-24 05:53:44 +0000] [40465] [INFO] Booting worker with pid: 40465```\nThanks in advance.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2416, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51u6F5_N8Iyq_cci0VvMbC8_iuPLCks5vGfTXgaJpZM4aRaXl\n.\n. Hello @njchandu ,\n\nSorry, but it looks like some generic problem with Nginx installation on Amazon, and not relate to Graphite itself. Please check some documentation on how to set up Nginx, proxying config for Graphite looks fine, though.. Hello @fuzzball5000,\nUnfortunately, Django 1.7 is not supported. Graphite 1.0.x and newer require Django >=1.8, Graphite 0.9.x supports only Django 1.4 . @obfuscurity : I'm not sure that we need to go that path either, just thinking out loud here.\nMaybe you can share your thoughts, Jason, pretty please?. @iksaif \n\nI think it would be nice to have something that works easilly for smalls installs but still can be extended for big installs. Ideally with the same components.\n\nOk, let me rephase it. \"Target small to medium installations as we doing now\", because for big installations whisper is not a best option IMO. But current implementation (or go-graphite) can be scaled to thousands of servers, Booking.com doing that - but if you planning something big right now I do not think yoiu should choose whisper.\n\nWhich is why I'm a bit afraid about that... \n\nIndeed. I do not like loosing control over own code. But we can always fork.\n\nPlus it makes it harder to share code between graphite and carbon. \n\nIMO sharing nothing can be good in that case. We have quite big amount of code duplication in current implementation. . @obfuscurity : \n\nLet's just say that I feel pretty strongly that one of Graphite's biggest\nweaknesses is around metrics naming discipline and the lack of\nauth{entication,authorization} around metrics ingestion.\n\nWell, agreed. But please note that even in current state Graphite is technically ready for OpenMertrics support. OTOH, Openmetrics doesn't have anything about auth part either.... Yep, I just mean OpenMetrics will help with naming metrics discipline - not as a standard, but as a different way of naming metrics.. Hello @Dieterbe, \nsorry, was little busy lately. Will try to answer your questions.\n\nThis notion that \"the official graphite stuff is merely a reference implementation\": is rarely explained, if at all. adding much to the confusion (even the frontpage of https://graphiteapp.org/ claims booking.com uses \"graphite\" and then goes on to list \"graphite's components\" which simply lists the stock python stuff\n\nWell, that's true - but why should we care and promote 3rd party implementations if it's not officially supported and not part of a project, right? Booking was using python stuff some time ago, they completely migrated to Go stack not that long time ago.\n\nre \"go-carbon is a separate project with own maintainers\". Who maintains it now, if lomik left it?\n\nHe didn't leave - just not using it in prod anymore. He's still accepting patches and making releases. \n\nDo the goals of the project align with the goals of the official graphite-project? In the pursuit of simplifying stuff, just bringing the project under the graphite umbrella seems to make more sense.\n\nThere's no official goal exist on go-carbon page. But I can assume it has the same goal as \"go-graphite\" project - reimplement Graphite in Go. And that's why I'm still not convinced should we accept \"go-carbon\" in python Graphite. \nGo-graphite project does exist, there's not much movement there lately, its goal is clear.\nMaybe we could officially approve \"Go-graphite\" as a project and participate there, fixing documentation on how to deploy it - and then keep Python Graphite as is, even officially deprecate it if/when we reach enough compatibility.\nI don't know. That's why I decided to ask the community here.\nAlso, carbonapi just got tag support, but TagDB still implemented only in python stack.\n\nIs it drop-in backwards compatible? (e.g. config syntax and feature completeness).\n\nThat's another source of my hesitation. It can read whisper files and compatible with schema and aggregation files, but:\n config is not drop-in compatible (i.e. completely different) \n relaying is not supported\n* no blacklisting / whitelisting\nAlso, have some additional features:\n Metric ingestion through Kafka and/or HTTP\n Carbonlink-like GRPC api (aka \"carbonserver\")  \nSo, if we want to adopt it we should do something with relaying - i.e. officially accept carbon-relay-ng. But I still think that's another reason to do that in \"go-graphite\" and not an official project.\n\nI think the graphite project should either adopt a statsd implementation (and it should become \"the reference implementation\") or even merge it into a carbon relay.\n\nGood idea\n\nfirst class support for units (track units along with timeseries, change unit strings when processed with functions, provide units in render responses so dashboards like grafana can automatically put right unit on axis labels etc)\n\nAlso agreed. We can just add support of TYPE from Openmetric format.\n\n@deniszh what did you mean with \"IMO lack of flexible aggregators\" ?\n\nWell, exactly that. IMO all existing implementations are buggy and/or slow - python, carbon-c-relay and graphite-relay-ng (didn't try aggregations in NG personally, just word-of-mouths).\n\nwhat are the downsides to carbonserver compared to our current cluster method? do we lose any functionality or desirable characteristics?\n\nI'm not aware of any downsides, but TBH I didn't check its implementation thoroughly. That's old repo, before merging code to go-carbon - https://github.com/grobian/carbonserver. Bioyino is a great project, but not all users are in Booking.com or Avito scale, i.e. targeting distributed aggregations as a hard requirement, IMHO.. @vaibhavpujari89 \nYou need to install metric collector on target host, and point it to Graphite input (usually port 2003).\nGood collectors - Collectd or Diamond.. If you using official Docker image - it have admin user root/root.\nBut if you just have a fresh install of Graphite - it will not have any users. Create admin user by using command django-admin.py createsuperuser.. Or you can use 3rd party dashboards app, like Grafana for storing dashboards / graphs.. Sorry, @sbancal , but that's wrong. Not sure about which Nginx config do you mean, but /opt/graphite/static/ is right path, jou just need to run collectstatic Django command, as described in documentation - see STATIC_ROOT in https://graphite.readthedocs.io/en/latest/config-local-settings.html. Thanks a lot! Merged. @ploxiln : will it work for all Django versions? Even if we upgrading from Graphite 0.9.x and/or Django 1.4 ?\nBecause --run-syncdb was added because normal migrate command did nothing before.... @ploxiln : maybe that should be mentioned in docs too? Could you please amend it? Thanks!. For me, it looks great. Maybe I could ask @gwaldo to check?. OK, I'm merging this for now.. LGTM. Should I merge it, @DanCech ?. Hi @shirishkale ,\nCould you elaborate an issue, please? Is Graphite web interface working at all?\nIf yes - is any other metric visible?\nWhat is in Graphite logs?\n. PS: whisper files are preallocated, so, have fixed size and not increasing in size.. Are you running this command in /opt/graphite directory?\nYou can also install whitenoise module. Check this answer on stackowerflow\n-\nhttps://stackoverflow.com/a/44734510\nOn Tue, 19 Feb 2019 at 20:53, Shirish Kale notifications@github.com wrote:\n\nNo. But when I try to run, it complains there is no module like\ngraphite.settings.\ndjango-admin.py collectstatic --noinput --settings=graphite.settings\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2427#issuecomment-465284876,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51jPd9dXocvwozW_a79ILGTwbwpWyks5vPFYmgaJpZM4bDX-W\n.\n. Did you check link which I mentioned?\nYou need to set up serving for static files from webserver or you can try\nwhitenoise module.\n\nOn Tue, 19 Feb 2019 at 21:18, Shirish Kale notifications@github.com wrote:\n\nThen your Pythonpath is broken\nOnce I fixed the PYTHONPATH and ran collectstatic. Are there any other\nsteps. I restarted graphite-web after collectstatic and still there is no\nmetrics displayed in graphite-web\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2427#issuecomment-465293695,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51rKE7dGi4rG1rFawuv6EcGO8VmSXks5vPFwVgaJpZM4bDX-W\n.\n. @shirishkale : something is really wrong with your PYTHONPATH. I'm not sure how properly detect that problem or fix it. It's far beyond Graphite per se, it's a generic issue with your python installation.\nHow do you trying to install Graphite? From source or packages? Do you use virtualenv? . Hi @dumbdonkey ,\n\nSorry, I'm not really sure what you trying to achieve in general.\nE.g., dividing two series of same length makes sense - divide [1,1,1] to [2,2,2] will give you [0.5,0.5,0.5]\nBut how you can divide two series of different length?\nWhat you're expecting when trying to delete [1,1,1,1] to [2,2] ? Graphite has not matrix or vector operations...\nAlso, all your asterisks are gone, so, it's quite hard to understand your original question. Maybe dividing different series list is not even needed...\n. As far as I understand, it was done the same way as sortByMinima() and sortByMaxima() did. Do we need to collapse all 3 or left it as is?\n. @Krylon360 , that's clear. As far as I understand @drawks proposed use\nseriesList.sort(lambda x,y: cmp(x.name, y.name)) instead of using compare() function, and my question was - do we need to make that transition to other functions which use same compare() function or left it as is.\n. You can (http://stackoverflow.com/a/9542768) use 'next' to replace\npython\nself.local_host = [host for host in remote_hosts if is_local_interface(host)]\nif len(self.local_host) == 1:\n  self.local_host = self.local_host[0]\nelse:\n  self.local_host = 'local'\nwith\npython\nself.local_host = next((host for host in remote_hosts if is_local_interface(host)),'local')\nIf I understand logic correctly...\n. You also need to fix tests in https://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/tests/test_render.py#L97-L102 btw - just humble reminder. :)\n. Here is quite easy fix for 2.6 support:\nMaybe do something like\ntry: # Python 2.7+, use buffering of HTTP responses\n    response = connection.getresponse(buffering=True)\n  except TypeError: # Python 2.6 and older\n    response = connection.getresponse()\nAssert will catch everything else.\n. For supporting python 2.6 it can be rewritten like\n```\n    try:\n      response = self.connection.getresponse(buffering=True)\n    except TypeError:\n      response = self.connection.getresponse()\ntry:\n  assert response.status == 200, \"received error response %s - %s\" % (response.status, response.reason)\n\n...\n``\n. Ok, removing then.\n. Missing :\n. Yes, that's better\n. You mean,if not \",\" in sub:.\nNot sure which one is better. :)\n. No,open_brace, close_brace = m.span(1)it's not the same. :(\n. Oh, I'm blind. Of course,if \",\" in sub:, and it's much better.\n. Ah, in that sense. Yes, sorry, didn't get your point from start. Yes, it works, and more readable now.\n. Is thattry..exceptblock really needed? I mean, when it could fail?. Good, that's clear now.. We already removed pyparsing version binding, could you please remove it too?. Thanks!. \ud83d\udc4d for multiprocessing.Pool. \ud83d\udc4d for not pinning version of urllib3 here. It's master code now, we can start with any version until we hitting some problems with it.. Looks much better thensock.connect( (host, 4242) )\ud83d\udc4d . How will it work iffromis timestamp? Maybe we should make some shortcut for this ?. Ah, nvm. Probably it's fine.. Ahem.... Why do you kill half ofevents()` function?. Not sure if we want to test function like this. Please check other tests. Usually, you need to pick 2-3-5 etc predefined series, apply your function and compare results with right one.\nFunction below is just comparing your function with some reference implementation - but if we know that reference implementation is fine - why do we need to check that at all?. Could you please elaborate how pipedArg is passed here?. Ah, missed that, sorry. Thanks!. \ud83d\udc4d . \ud83d\udc4d too. Does it applicable here?. Why removing this and comments below? Just curious. nice catch. should it be before raise Exception?\n. should we raise exception here? It will not be caught, right?. Same here. Pipeline changes look good. \ud83d\udc4d \n. \ud83d\udc4d \n. missing/extra quote?. Looks like extra.. Travis-ci ?. Should you include percentiles' aggregation on this list too?. >My PR concerns carbon-aggregator aggregation methods, which I believe to be distinct from whisper storage aggregation methods that are defined here.\nYes, I got that - but then maybe the warning should be moved from line 107 to somewhere after line 176, i.e. after aggregator's aggregation methods?. should it be something like prometeus-metrics/ ? Thinking about possible (logical) clash with https://github.com/graphite-project/graphite-web/pull/2195. Not sure if an empty line is safe/needed here.... I think it should be log.debug - here, and in other places too.. Agreed with Codacy here - please specify exception type here . it's better to use something like\nSETTINGS_MODULE = os.environ.get('GRAPHITE_SETTINGS_MODULE', 'graphite.local_settings')\nhere. ",
    "garrettwilkin": ":+1: I would love to have this feature while exploring my data.\n. ",
    "steve-dave": "@cryptographrix I wish it was as easy as a donate box, but unfortunately that's a whole can of worms in its own right...  How much needs to be donated before the feature gets done and what if there is a disagreement about that between the donors and developer(s)?  Which developer(s) actually implements, do they need to be from the core team or can anyone do it?  Who holds the funds in trust and who decides when they should be paid out?  You have to remember that if there is not a company/organization (or managers with delegation powers) behind a project then the project collaborators quite likely have never met in person and are physically distributed all over the globe.\nI feel your pain.  I, and I'm sure some of the others, would likely happily implement the feature if we had the time.  I have not even been able to work on my own list of wanted features for quite some time...  The only thing I could suggest is advertising on a freelancers board or something like that, there's nothing stopping you (or a group) from independently hiring a developer to implement features for open source projects.\n. I've opened a pull request for this: https://github.com/graphite-project/graphite-web/pull/735\n. @esc is it already there: https://github.com/graphite-project/graphite-web/commit/9febb4ecabc57e6fc7ada18467696450baa7379f\n. @esc I believe so.\n. @esc I believe it does but was waiting for it to be finalized before backporting it.\n. @esc I made a suggestion that I thought might have been slightly cleaner. It's a while ago now, if no one concurs with my suggestion then I'm happy for my comment to be disregarded.\n. It appears this has been resolved in the master branch.  Does it need to be backported or is it just a nice to have?\n. @esc do you know how this \"automagically\" became merged?  Maybe obvious, I'm relatively new to GitHub...\n. @esc @avnivamsikrishna I'm not sure about this one.  I don't believe it's ready for a PR.  Hold off if you can and I'll come back to it.\n. @esc @avnivamsikrishna I have a fix for this in the pipeline and will submit PRs tomorrow (it affects both 0.9.x and master)\n. Don't think it's quite worth a +:100: :-), it's actually quite a simple fix...\n0.9.x:  https://github.com/graphite-project/graphite-web/pull/784\nmaster:  https://github.com/graphite-project/graphite-web/pull/785\n. I've opened a PR for this: https://github.com/graphite-project/whisper/pull/80\n. At this stage I won't comment on how to structure releases.  There are a lot of good ideas here but I'm new to the project and wouldn't feel confident in suggesting how to go forward in that respect.\nHowever, I would like to see at least one more 0.9.x release.  As far as I know it is considered the \"stable\" release, but currently Graphite things just don't work if 0.9.x is checked out from all 3 repos.  I have opened a number of PRs across the 3 repos that get the branch to the point where it does run and it would be great if they could be merged and then a new, possibly final, 0.9.x release tagged.  I have one more 0.9.x branch in the works that doesn't have a PR here, but it is only a backport of a performance improvement so it wouldn't matter so much if it wasn't accepted here - my main interest is in getting 0.9.x across all three repos running out of the box.\nIt's entirely possible that master is almost ready for a 0.10.x, but I'm not sure that it would be ready to be the new \"stable\" release as it has quite a bit of new functionality in it that may not be complete or well tested.  For this reason, IMHO, it would be best to keep 0.9.x going at least there is a new \"stable\".\nThanks!\n. I am happy to be involved in the next (which obviously may be the final) release(s) of 0.9.x, but I'm not familiar enough with master to know if I have the bandwidth to be (heavily) involved in 0.10.x.  I have a number of PRs pending for 0.9.x (various fixes and backports to get the branch back to running out of the box) that could be the makings of 0.9.13-pre and I'd be happy investigate bug reports relating to the pre-release once it has been tagged.  Of course I'd be happy to help review any other PRs that it is thought should be included as well, but perhaps anything other than bugfixes could be postponed till at least 0.9.14?\n. Thanks Jeff.  Totally understand the real life side of it, was more offering help than expecting anything.  Once again, if I can help, let me know.\n. I agree it's quite concerning how far the branches have diverged.  I still feel that 0.9.x should be maintained until 0.10.x is released and becomes the official stable, perhaps put a line in the sand at 0.10.1 or 0.10.2?  Any fixes that are applicable to both branches should be applied to both branches until 0.9.x is final (certainly not just to 0.9.x) - I'm not sure the preferred procedure for this but I have just been opening a PR for each branch rather than leaving it to the person that merges to have to port it.  If someone else doesn't get to it first I will review the fixes that have been applied to 0.9.x and port them to master where applicable.\n. @esc FWIW we are currently running the 0.9.x branch in production with a handful of patches that I have created pull requests for to be considered for merging.\nWould you like the 0.9.13 candidate pull requests highlighted here in this thread, tag those with permissions in the actual PRs, or highlight them in some other way?  I apologise if this is obvious, but I am new to both this project and Github.  Also, should I include carbon and whisper PRs or are they handled by different people?\n. @esc Is there a deadline or target date?  I've lost a couple of days to miscellaneous issues and haven't been able to go through the PRs yet...\n. +1\n. @esc it does... https://github.com/graphite-project/graphite-web/pull/761\n. Sorry wrong branch\n. Sorry wrong branch\n. Answered in another thread :-)\n. Out of interest, what constitutes a large scale Graphite system?  Do you know much about how the bigger users are using it?\n. Thanks Marc.  It's interesting to hear about others are using it.  So far our instances are all <1,000,000 metrics/min - good to know we've got a bit of room to move.\n. Thanks, exactly what I needed!\n. @esc ?\n. Looks sane to me, but no idea how much if any breakage.  Presumably it's just a complete coincidence that python decided to use 12 decimal places when using str() to cast floats and they've obviously removed that truncation for later versions anyway.  It would be strange but not impossible for a consumer to break with higher precision.  IMO if something breaks because of this, it needs to be fixed/improved anyway.\n. @jraby Out of curiosity, are you running the latest 0.9.x source or a release?\n. @jraby I've submitted a PR for a possible fix:  https://github.com/graphite-project/graphite-web/pull/775\nAre you able to test?\n. AFAIK Graphite doesn't do this out of the box.  You would need to overlay an authorisation layer or, if you can afford the time, build authorisation into Graphite itself.  This is a huge can of worms though...\n. Or please advise if there's a better way to do this :-)\n. @esc \nVersion for master: https://github.com/graphite-project/graphite-web/pull/760\nNot sure how how best to be tested, but I can confirm it is being used in production here and it is a very basic change.\n. @esc yes, same\n. @esc thank you\n. No worries, in case you're wondering you didn't miss it, you just reminded me to open it :-)\n. Of course, and I won't be offended if someone else beats me too it :-).. IMO the main thing is it's tagged with milestone so it doesn't get forgotten. I will get to it but have a few more bugs to fix first.\n. Thanks @esc !\n. Thanks @macolu .\n. Disclaimer:  I am not a project collaborator so this is all just IMHO.\nFirstly, FWIW, I like the way you have refactored it.  It looks like at first glance that your code should return the same output more efficiently.  That said, are there pre-existing tests and are they sufficient?  If either not the case expect to be asked for some tests to be added to the PR (and that is not a bad thing).\nSecondly, in both the existing code and your proposed replacement, if there are no values in \"buf\" after running out of values the generator will return None before raising \"StopIteration\"... do you know if this is necessary or should it be replaced with (current code):\nyield None\n-    while None in buf: buf.remove(None)\n-    if buf: yield self.__consolidate(buf)\n-    else: yield None\n+    if buf:\n+      while None in buf: buf.remove(None)\n+      if buf: yield self.__consolidate(buf)\n+      else: yield None\n     raise StopIteration\nor (your patch):\nyield None\n-    if buf:\n-      yield cf(buf)\n-    else:\n-      yield None\n+    if valcnt:\n+      if buf:\n+        yield cf(buf)\n+      else:\n+        yield None\n     raise StopIteration\nThirdly, and I honestly don't know the answer to this question, would it be (fractionally) more efficient to move the __consolidation_functions dict inside __consolidatingGenerator or would this actually be less efficient or bad practice:\n-  __consolidation_functions = {\n-    'sum': sum,\n-    'average': lambda usable: float(sum(usable) / len(usable)),\n-    'max': max,\n-    'min': min,\n-  }\n   def __consolidatingGenerator(self, gen):\n+    __consolidation_functions = {\n+      'sum': sum,\n+      'average': lambda usable: float(sum(usable) / len(usable)),\n+      'max': max,\n+      'min': min,\n+    }\n     try:\n-      cf = self.__consolidation_functions[self.consolidationFunc]\n+      cf = __consolidation_functions[self.consolidationFunc]\nFinally, to keep my sanity, is it just me for is the first line of the __consolidate function in the current code completely redundant?  Given that __consolidatingGenerator has already filtered \"None\"s, is it necessary to iterate over the passed values?\n. @esc master definitely needs to be reviewed for the \"now\" related changes... It's been sitting in the back of my mind but an issue should probably be opened so it's not forgotten. I'll review this PR in a few hours if no one beats me too it.\n. @esc \nI see your PR and raise you https://github.com/graphite-project/graphite-web/pull/782\nBasically the now parameter needed to be added to the request and I removed the endtime and now defaulting to be consistent with local storage fetch (see https://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/storage.py#L307) where the defaulting is done further downstream (see https://github.com/graphite-project/whisper/blob/0.9.x/whisper.py#L716) as it is in the webapp render API (see https://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/render/attime.py#L31).\n. @loisaidasam although I'm not against the ifVersion trick I'm kind of interested in what your issue was as I am using the old syntax without issue on Apache 2.4.7 with mod_wsgi...\n```\n$ apache2 -v\nServer version: Apache/2.4.7 (Ubuntu)\nServer built:   Jul 22 2014 14:36:38\n$ grep -B 1 \"Allow from all\" /etc/apache2/sites-available/default-ssl.conf\n                Order deny,allow\n                Allow from all\n--\n                Order deny,allow\n                Allow from all\n$ grep -i \"Require all granted\" /etc/apache2/sites-available/default-ssl.conf || echo \"String not found!\"\nString not found!\n```\n. @esc replaces https://github.com/graphite-project/graphite-web/pull/779\n. @esc I'm not sure how API calls are normally tested.  @jraby are you able to test?\n@esc I've removed your commit.  Didn't feel right removing it without your blessing and I was using one line out of it :-)\n. Thanks @jraby .  A testament to peer review and not making post-testing \"simple and harmless refactors\".  Sorry for that.\nI've changed the list to a dict, anyone against that idea?  Personally just feel the code is easier on the eye as @jraby suggested.  If it is preferred, I will change the other query_params list (in FindRequest.send()) to a dict as well.\n. @jraby @esc @obfuscurity sorry to act like I'm losing my mind but I've gone back to a list.  It just makes more sense when dealing with GET variables because multiple variables are allowed to have the same key.  Still needs tests though.  Unfortunately I've been tied up with \"real life\" recently, but will continue to give as much time as I can.\n. I wonder if this would be a breaking change for consumers that parse the output and now expect it to be misspelled?  I'm not completely against it, but it would need to be well advertised in documentation/release notes (if there are any).\n. @esc they haven't been forgotten :)\n. @SEJeff totally agree.  It's still on my list :-)\n. :+1: \n. Don't believe it is applicable to master.\n. @obfuscurity, @SEJeff:\nCan I suggest that Graphlot be left in 0.9.x?  I understand the frustration of it being messed up downstream during packaging, etc. but that is not the fault of Graphlot.  Graphlot works as is in 0.9.x if you don't mess with it, and it may well still have active users.\nWhile we use Grafana dashboarding mostly, we do still embed Graphlot graphs in some cases.  Yes, we maintain our own fork so we can easily just keep Graphlot for our own purposes, but I'm raising my hand as if we use it there may well be other users out there.  Also agree that there are better alternatives to Graphlot, but a user may want to get to benefit of bugfixes in 0.9.13 without having to refactor their uses of Graphlot.\nThere's also possibly an argument for not removing features from 0.9.x if it has been decided that none will be added...  Just my $0.02...\nPS.  Totally :+1:  for removing it from 0.10.x\n. @obfuscurity (sorry for the mistag @esc ) didn't mean to down play the argument for removing it, mainly just mentioning it may not be completely redundant.\nAs for the adding new features to 0.9.x point, that is fair...  I've been a bit out of touch for a while, are new features allowed as a rule or on a case-by-case :+1: basis?\n. @obfuscurity BTW (without discrediting all contributors - there's been significant activity recently), your level of contribution recently has been mind-blowing!  We are all in your debt!\n. Absolutely!\n. IANAL but :+1: either.  Even if we go with \"2008-2012, Chris Davis\" his Copyright remains valid for a long time anyway, but I don't think there'd be any issue with \"2008-2014, Chris Davis\" as this would imply the project is granting his Copyright on an ongoing basis...\n. @miloszszymczak @mlowicki sorry if I've missed it somewhere above but can you please advise the exact graphite-web version you are running?\n. @obfuscurity how do you feel about this being merged in before 0.9.13?  it's a relatively simple change but I believe it does add value.\n. :+1: for switch worst-case, but theoretically it's also possible that you may want the benefit of the auto-detection as well as deliberate loopbacks.\nIs the change I have proposed horribly inefficient or is there another reason it cannot be merged as-is?\n. @obfuscurity basically it just adds some flexibility without any real complication.  It allows for remotehost targeting to be exported to a reverse proxy, load balancer or port forward, etc. as well as the multiple instances on a single host scenario.\n. @drawks @obfuscurity if you're still against this idea I'm happy to close the PR... don't want to clog up the queue.\n. @drawks @obfuscurity if you're still against this idea I'm happy to close the PR... don't want to clog up the queue.\n. Closed in favour of https://github.com/graphite-project/graphite-web/pull/1134\n. I see this has broken tests.  I'll look into it as soon as I can.\n. Tests updated and passing.  Now just depends on https://github.com/graphite-project/graphite-web/pull/1069 being accepted.\n. Closed in favour of https://github.com/graphite-project/graphite-web/pull/1135\n. @obfuscurity another small, high value change.  Do you think it could be in 0.9.13?\n. Now configurable :smiley: \n. @obfuscurity do you think it's OK to merge now?\n. @obfuscurity done.\n. Not yet.  Master is too different to allow a quick cherry-pick but I agree it needs to be done.\n. Not yet.  Master is too different to allow a quick cherry-pick but I agree it needs to be done.\n. :+1: looks sane to me.  Do tests need to be updated?\n. @g76r sorry, what I should have said is that if there are existing tests they need to be updated to complete this patch.  If there are existing tests they are not sufficient as they are passing without this fix, but it's possible you're right and there are no tests.  I'll review when I can unless someone more familiar can get to it first.\n. Ping @obfuscurity\n. ping @drawks @SEJeff \n. Closing as I no longer have commit on the head fork. Moving to #1167 \n. ping @drawks @SEJeff \n. Closing as I no longer have commit on the head fork.  Moving to https://github.com/graphite-project/graphite-web/pull/1166\n. @obfuscurity @SEJeff @gwaldo 0.9.x is broken without this fix but don't want to merge my own code.\n. @obfuscurity apologies, failed in haste.  Breaking commit is: https://github.com/graphite-project/graphite-web/commit/a835a884953e148134c619c852f70c8911271b00\nBug was reported by email:\nI'm merging upstream into our local repo and ran into this. It caused the webapp to blow up in our configuration.\nc2b14600 webapp/graphite/remote_storage.py (Chris Davis       2009-10-19 01:20:01 -0500 136)     connection = HTTPConnectionWithTimeout(self.store.host)\n797a5cd2 webapp/graphite/remote_storage.py (Chris Davis       2011-03-16 01:22:21 -0500 137)     connection.timeout = settings.REMOTE_STORE_FETCH_TIMEOUT\na835a884 webapp/graphite/remote_storage.py (Dave Ertel        2014-10-03 14:13:21 +1000 138)     if settings.REMOTE_STORE_USE_POST:\na835a884 webapp/graphite/remote_storage.py (Dave Ertel        2014-10-03 14:13:21 +1000 139)       self.connection.request('POST', '/render/', query_string)\na835a884 webapp/graphite/remote_storage.py (Dave Ertel        2014-10-03 14:13:21 +1000 140)     else:\na835a884 webapp/graphite/remote_storage.py (Dave Ertel        2014-10-03 14:13:21 +1000 141)       self.connection.request('GET', '/render/?' + query_string)\n71d395ee lib/graphite/clustering.py        (Chris Davis       2009-06-22 19:12:17 -0500 142)     response = connection.getresponse()\nIn this class (RemoteNode) connection is a local var , not a class var. This is inconsistent with the previous function (FindRequest) where you added the same code.\nI removed the \"self.\" from the \"connection.request\" calls in my local version to work around the issue. I suspect the \"right\" fix would be to make both functions consistent?\nthanks for supporting graphite - it's a great tool. \n. :+1: can't see any reason that this should not be merged!\n. :+1: can't see any reason that this should not be merged!\n. @bmhatfield @drawks do you think this is mergeable?  The change of default definitely needs to be documented and how to do that is definitely the question.  That said, master is technically not released yet and when it does get released there are possibly quite a few backwards incompatibilities with 0.9.x.  How about for #1167 (0.9.x port) the default should be unchanged (I'll modify the patch if we are in agreement) and a comment added to the config highlighting the fact that the behaviour will change from 1.x (master) onwards?  It also needs to be in 1.x release notes as well I would expect, but as far as I can tell there isn't even a placeholder for 1.x release notes yet.\n. @drawks test added.\n. @drawks @bmhatfield so do we leave this as is or flip the default so that it doesn't change the current behavior?\n. ping @drawks @bmhatfield \n. I can see how this would be useful, but could it be achieved by adding the functionality to groupByNode() and maybe aliasing groupByNodes() to groupByNode()?\n. Possibly use *args and require that the callback always be the last parameter or alternatively do it the way you have proposed, but strip the code from groupByNode() and convert it to a wrapper of groupByNodes() so that it's backwards compatible but we avoid code duplication.\nWhat do you think @obfuscurity ?\n. @gboily can you please convert groupByNode() to a wrapper of groupByNodes() and ensure there are good tests?\n. Have I misunderstood your question?  I think the config option you are asking for already exists as LOG_DIR...\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/local_settings.py.example#L86\nhttps://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/local_settings.py.example#L78\n. :+1: \n. Relates to https://github.com/graphite-project/graphite-web/pull/586\n@SEJeff @brutasse \n. @blacked check out https://github.com/Kixeye/graphite-web/tree/0.9.x-deletemetrics if you're running 0.9.x... Not sure if it's still a clean merge but pretty sure it was working just not QA'd.\n. Would it work to leave the the old replace in and put it inside this conditional?  As in replace this block with:\n```\nIf subs made, replace the first  with , and close out the last  at the end\nif subsMade > 0:\n  svgData = svgData.replace('\"\nsvgData = svgData.replace(' data-header=\"true\"','')\n```\n. For the host to be queried graphite-web must think it is not local, otherwise it is ignored.  I believe this was loopback prevention so that config can be shared between hosts in a cluster, but sometimes it can be useful to run multiple instances on a single host and have them query one another.\nSee here: https://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/storage.py#L34\n. @bmhatfield would you prefer a if not (... or ...) or do you just want an explanation of the logic?\nBasically it's just including hosts if either is_local_interface() is not enabled by config (REMOTE_EXCLUDE_LOCAL) or is_local_interface() returns false.\n. ",
    "kayvonr": "+1000000\n. ",
    "nightfly19": "@obfuscurity Fixed.\n. Oops, updated to include the new function in the composer.\n. ",
    "Finkregh": "another example where the green graph has data from right now and the blue one is summarized over 24 hours (with its first point in the future):\n\n. thanks for the quick fix :-)\n. @banzayats have a look at d3js and eventually dashboards that use that...\n. :+1: release early, release often :)\n. where is the release? oO?\n. http://graphite.wikidot.com/installation references https://launchpad.net/graphite/+download/\nanother broken link: https://pypi.python.org/pypi/graphite-web/ to http://graphite-project.github.com/\n;)\n. :+1: thanks @piotr1212 \n. ",
    "panthomakos": "I am seeing the same issue with Staircase Line and any Area Mode (All or Stacked).  This does not happen when the line mode is set to something else (Connected or Sloped).  I am running version 0.9.10.\n. ",
    "lusis": "Curious about this myself. Bump\n. ",
    "Krylon360": "old bump; I know; oh well. Not seeing this issue on our env. We are cleaning the sample before sending it over through powershell though.\n```\nRemoving Beginning Backslashes\"\n$cleanNameOfSample = $cleanNameOfSample -replace '^\\\\', ''\nReplacing Backslashes After ServerName With dot\"\n$cleanNameOfSample = $cleanNameOfSample -replace '\\\\', '.'\n```\n. seeing the same inconsistancy. I'm using gw 10.0-alpha source though; and carbon/whisper 0.9.x (0.9.12 tag) source.\nKeep getting the file already exists exception.\n. ah; you're right. derpage on my part. to many PR's doing the same thing; I saw Pull #522 and got confused. Either way; the function is still needed in 0.9.x or it will create a lot of Issue requests in @torkelo grafana issue log if the function is called in Grafana.\n. Works find in composer:\n\nWorks in Graphlot:\n\nWorks in Grafana:\nI had to snip the screen cap. 99 servers on one graph is a bit extreme when enabling the legend. heh.\n\nGrafana with sortByName() Query.\n\nIf you'd like me to built it into the functions_test.py; let me know.\n. @obfuscurity Yeah; I figured that was the case right after I posted. I'll see if I can get some time today to add in the function to the funtions_test module; and add it to the pr\n. Sorry; haven't had a ton of free time the past week or so. I finished our\nGraphite+Grafana POC and had to jump right into our SCOM to Zabbix\nMigration POC. I'll try to get a test created this weekend. need to read\nthe Test Creation wiki doc Tagging myself for a reminder email. @Krylon360\nOn Fri, Aug 22, 2014 at 12:31 PM, Jason Dixon notifications@github.com\nwrote:\n\n@Krylon360 https://github.com/Krylon360 Just checking in again to see\nif there's been any progress on tests. [image: :cake:]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/831#issuecomment-53102808\n.\n\n\nBryce T. Walter\n. thanks @deniszh I've been tasked with a new role at work; and putting in 14 hrs just to cross-train, manage our cloud, and adding new instances to Graphite + Grafana Dashboards. Time has all but stopped. When it rains; it pours.. but; in this case; it's a much needed rain.\n. thanks @deniszh I've been tasked with a new role at work; and putting in 14 hrs just to cross-train, manage our cloud, and adding new instances to Graphite + Grafana Dashboards. Time has all but stopped. When it rains; it pours.. but; in this case; it's a much needed rain.\n. sortByMinima() / sortByMoxima() sorts by the Values. sortByName sorts by\nthe Metric Name. ex: Production.Servers.ABServer.LogicalDisk.C.FreeSpace;\nProduction.Servers.CDServer.LogicalDisk.C.FreeSpace.\nProduction.Servers are hard coded into the Agg. ABServer/CDServer is the\n1st Metric Name.... with the sortByName Function; it would sort it\nAlphaNuma so AB would display 1st. This comes in VERY USEFUL when you are\nhave 300 Servers in 1 graph (100 Hyper-V Windows GO Agent Servers, 100\nVMWare Windows GO Agent Servers, and 100 Linux GO Agent Servers.) In our\ncase, PRDSLHWGOAxxx, PRDSLSLGOAxxx, PRDSLVWGOAxxx; where xxx is a number\nOn Thu, Sep 11, 2014 at 9:35 AM, Denis Zhdanov notifications@github.com\nwrote:\n\nIn webapp/graphite/render/functions.py:\n\n@@ -1557,6 +1557,18 @@ def limit(requestContext, seriesList, n):\n   \"\"\"\n   return seriesList[0:n]\n+def sortByName(requestContext, seriesList):\n-  \"\"\"\n-  Takes one metric or a wildcard seriesList.\n  +\n-  Sorts the list of metrics by the metric name.\n-  \"\"\"\n-  def compare(x,y):\n-    return cmp(x.name, y.name)\n\nAs far as I understand, it was done the same way as sortByMinima() and\nsortByMaxima() did. Do we need to collapse all 3 or left it as is?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/831/files#r17428141\n.\n\n\nBryce T. Walter\n. ",
    "seveas": "Here's a screenshot of it in action:\n\n. Thanks for considering. It's already proving pretty useful here, so I'm going to add links to create short urls to graphplot pages etc. as well.\n. It's certainly possible to use an external url shortener app, but it's the integration with the graphite UI that made it 1000% more useful for us than an external app. So my definition of 'better' is more integration :)\n. I just rebased the patches onto current master and pushed the result.\n. Just a datapoint as to how useful it is for us: in the 4 months since we added this, 666 short urls were created.\n. Hmm, it's entirely possible that I did a manual alter table after syncdb, table definition here is:\nsql\nurl_shortener_link | CREATE TABLE `url_shortener_link` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `url` text NOT NULL,\n  `date_submitted` datetime NOT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=1123 DEFAULT CHARSET=utf8\n. I've pushed reworked commits based on todays master that include @cbowman0's changes.\n. ",
    "chjohnst": "Will give this a try tmo awesome stuff!\nOn Sunday, February 24, 2013, Dennis Kaarsemaker wrote:\n\nLinks to graphite images are completely unpastable to jabber/irc/mail as\nclients mangle the humongous urls. These commits add a url shortener that\ncan\nshorten any graphite url from /what/ever/.... to /S/.\nIt can be used manually by prepending /s to the url path, and I added a\nbutton\nto the graphite composer that does this for you.\nYou can merge this Pull Request by running\ngit pull https://github.com/seveas/graphite-web url_shortener\nOr view, comment on, or merge it at:\nhttps://github.com/graphite-project/graphite-web/pull/158\nCommit Summary\n- Add url shortening views\n- Integrate the short url functionality with the composer\nFile Changes\n- A webapp/content/img/browser.pnghttps://github.com/graphite-project/graphite-web/pull/158/files#diff-0(0)\n- M webapp/content/js/composer_widgets.jshttps://github.com/graphite-project/graphite-web/pull/158/files#diff-1(15)\n- M webapp/graphite/app_settings.pyhttps://github.com/graphite-project/graphite-web/pull/158/files#diff-2(1)\n- A webapp/graphite/url_shortener/init.pyhttps://github.com/graphite-project/graphite-web/pull/158/files#diff-3(0)\n- A webapp/graphite/url_shortener/baseconv.pyhttps://github.com/graphite-project/graphite-web/pull/158/files#diff-4(58)\n- A webapp/graphite/url_shortener/models.pyhttps://github.com/graphite-project/graphite-web/pull/158/files#diff-5(5)\n- A webapp/graphite/url_shortener/views.pyhttps://github.com/graphite-project/graphite-web/pull/158/files#diff-6(24)\n- M webapp/graphite/urls.pyhttps://github.com/graphite-project/graphite-web/pull/158/files#diff-7(2)\nPatch Links:\n- https://github.com/graphite-project/graphite-web/pull/158.patch\n- https://github.com/graphite-project/graphite-web/pull/158.diff\n. We were able to reproduce a similar issue when using apache benchmark.  Might be worth testing this patch it looks fairly sane.\n. \n",
    "rolandow": "How do I get this button to fetch the URL in my Graphite? I only have the first three buttons shown in the screenshot, missing the last two.\n. ",
    "jmason": "sniping from the bikesheds: fwiw, I've seen a similar feature in Amazon's internal graphite-like system to deal with unpasteably humungous URLs.\n. \\m/\n. awesome. thanks guys, looking forward to this ;)\n. argh! Sorry -- I just tried it, and it works fine. totally a PEBKAC error. apologies!\n. ",
    "devopsrick": "++interest in this feature!  Any chance it will be merged?\n. ",
    "lukevenediger": "I've seen what @jmason is talking about: their graphing front-end auto-shortifies the link and displays it on the page. As an interim step, it would be awesome if the \"Direct URL\" button on the Dashboard gave me a complete link, as opposed to everything except the scheme and host. Extra points for auto-highlighting the whole line as it appears.\n. @anatolijd where I work we have 100+ people accessing the dashboards via Graphite every day, so this interface is hardly a prototyping tool. We have dashboards showing on large monitors all around the offices. I think it's fair for @mjulian to ask for this feature to be available in the interface, and helps the dashboard become an all-in-one solution.\n. @anatolijd ok I understand, thanks.\n. @SEJeff no probs! Thanks for merging it in :)\n. Thank you!\nOn Monday, December 16, 2013, Jakob Aar\u00f8e Dam wrote:\n\n\nYou can merge this Pull Request by running\ngit pull https://github.com/jakobadam/graphite-web master\nOr view, comment on, or merge it at:\nhttps://github.com/graphite-project/graphite-web/pull/537\nCommit Summary\n- Complete instructions for setting up a dev environment.\nFile Changes\n- M requirements.txthttps://github.com/graphite-project/graphite-web/pull/537/files#diff-0(11)\nPatch Links:\n- https://github.com/graphite-project/graphite-web/pull/537.patch\n- https://github.com/graphite-project/graphite-web/pull/537.diff\n. Thanks, I tried the first option (--settings=graphite.settings) and it fails with the same error.\n\nSent from my iPhone\n\nOn 07 May 2014, at 11:20 PM, Bruno Reni\u00e9 notifications@github.com wrote:\nTry django-admin.py syncdb --settings=graphite.settings. Or setting the DJANGO_SETTINGS_MODULE environment variable to graphite.settings.\n\u2014\nReply to this email directly or view it on GitHub.\n. The second option also fails for me. I've tried this on ubuntu and centos 6.5.\n\nI got this working on centos by doing the following:\n$ export PYTHONPATH=/opt/graphite/webapp/\n$ django-admin.py syncdb --settings=graphite.settings\nAnd then I was able to create the sqlite db.\n. ",
    "alistairking": "I'd also love to see this added. Was just about to code it myself when i saw this.\n. I've been using this PR for a couple of months now and it has been really great - with one exception:\nIt seems that the default length for the URLField field type used in the model is 200 characters (https://docs.djangoproject.com/en/dev/ref/models/fields/#urlfield), a limit that I easily ran over with Graphite render URLs. \nIn fairness I only came across this problem after switching to a MySQL backend, so perhaps SQLite does not enforce this constraint?\nTo get around the problem I changed the url field type in url_shortener/models.py to \"TextField\" and then followed the alter table workaround described at http://code.djangoproject.com/ticket/2495 to trick MySQL and Django into playing nicely together when using a TextArea as part of a unique key.\nI'm pretty new to django so I'm hoping there a better way to fix this.\n. ",
    "whilp": "Hm; I'm going to have to think about these concerns for a bit. Thanks for detailing them!\n. Updated to drop ensure_ascii only if the request indicates that utf-8 is acceptable. harrumph\n. Wrong base, rePRing.\n. Tanks!\n. ",
    "daniellawrence": "This is a duplicate of https://github.com/graphite-project/graphite-web/pull/165\n. ",
    "vaxvms": "And also a duplicate of #161 \n. ",
    "jbrucenet": "I found this while trying to use sin(), time(), and randomWalk() with Tasseo.\n. ",
    "jyoo": "+1 +1 +1 please! This would be wonderful for sending a fixed number of points for long time periods - an autosummary if you will. \n. ",
    "tomerpeled": "Hi,\nI think we should improve this maxDataPoints behaviour or create another type lastXDataPoints (which won't be consolidated).\nThere are cases when you'll want that Graphite will calculate some function (such as HoltsWinter) on some large period of time, but only to get the last x data points. \nFor example: I'm using the holtWintersConfidenceBands function on several days period (for precise calculation) but only interested in the last several minutes results. Notice that in this case it is important to run holtWintersConfidenceBands function on some large period but it's not important to get all the data points from this period. By returning only the last x data points (without any consolidation) we will help to improve the calling client performance a lot. \nWe can add another filter type ,such as: lastXDataPoints .\nThe code should look like this:\nif 'lastXDataPoints' in requestOptions and any(data):\n        lastXDataPoints = requestOptions['lastXDataPoints']\n    for series in data:\n       timestamps = range(series.start, series.end, series.step)\n           datapoints = zip(series[-lastXDataPoints:], timestamps[-lastXDataPoints:])\n       series_data.append( dict(target=series.name, datapoints=datapoints) )\nWhat do you think?\n. Hi,\nI think we should improve this maxDataPoints behaviour or create another type lastXDataPoints (which won't be consolidated).\nThere are cases when you'll want that Graphite will calculate some function (such as HoltsWinter) on some large period of time, but only to get the last x data points. \nFor example: I'm using the holtWintersConfidenceBands function on several days period (for precise calculation) but only interested in the last several minutes results. Notice that in this case it is important to run holtWintersConfidenceBands function on some large period but it's not important to get all the data points from this period. By returning only the last x data points (without any consolidation) we will help to improve the calling client performance a lot. \nWe can add another filter type ,such as: lastXDataPoints .\nThe code should look like this:\nif 'lastXDataPoints' in requestOptions and any(data):\n        lastXDataPoints = requestOptions['lastXDataPoints']\n    for series in data:\n       timestamps = range(series.start, series.end, series.step)\n           datapoints = zip(series[-lastXDataPoints:], timestamps[-lastXDataPoints:])\n       series_data.append( dict(target=series.name, datapoints=datapoints) )\nWhat do you think?\n. ",
    "clodio": "It does not solve your problem but it solve mine. \nFor the date rendering, i change the /opt/graphite/webapp/graphite/render/glyph.py to match the date format I want :  02/28--> 28/02\nchanged in the file :  format=\"%m/%d\" to format=\"%d/%m\"\n. ",
    "jamesjuran": "I believe this is fixed by #499 -- can this issue be closed?\n. This could be the same cause as #448 .\n. Try increasing CARBONLINK_TIMEOUT in local_settings.py. It defaults to 1.0 seconds. If your cache is really big, you may need a longer time.\n. Thank you @cbowman0; I'm using this patch in my installation. Would you like to submit a pull request for this, or would you like me to submit one in the hopes that it makes this easier to get into the next release?\n. Also seeing this with EPEL 6 packages of graphite 0.9.12. Have you figured out any workarounds?\n. I added 'collections': set(['deque']) as described by @deniszh and it is working. I too am unsure if this is the correct fix. @mleinart, as the author of the pickle security fix, do you have a thought on the correct way to solve this?\n. I like this idea; I'll try integrating it with my installation next time I update it.\n. I'm now using syslog to capture these logs correctly, even from multiple WSGI / Apache processes. As a side benefit, this also lets me use logrotate to rotate the files.\nIf there is interest in this and PR #464 gets merged, I'll create a pull request for the graphite-web internal changes (part 2 below).\nI don't love consuming 5 of the 7 local syslog facilities, so if someone has a better way to do this, please let me know.\nYou need:\n1) PR #464\n2) Additional changes to graphite-web to make syslog work correctly when so configured: https://github.com/jamesjuran/graphite-web/compare/django_logging_v3...syslog\n3) Put this in your syslog configuration (/etc/rsyslog.d/graphite.conf for me):\nlocal2.*                                                /var/log/graphite-web/info.log\nlocal3.*                                                /var/log/graphite-web/exception.log\nlocal4.*                                                /var/log/graphite-web/cache.log\nlocal5.*                                                /var/log/graphite-web/rendering.log\nlocal6.*                                                /var/log/graphite-web/metricaccess.log\n4) Put the following in your local_settings.py:\n\nLogging Configuration\n\nLOG_RENDERING_PERFORMANCE = True\nLOG_CACHE_PERFORMANCE = True\nLOG_METRIC_ACCESS = True\nOnly change this if you want to change where logs go\nimport os\nimport logging\nfrom graphite.logger import UTFFixedSysLogHandler\nLOGGING = {\n  'version': 1,\n  'disable_existing_loggers': False,\n  'formatters': {\n    'standard': {\n        'format':  '%(message)s',\n    },\n  },\n  'handlers': {\n    'info':{\n      'formatter': 'standard',\n      'class': 'graphite.logger.UTFFixedSysLogHandler',\n      'address': '/dev/log',\n      'facility': logging.handlers.SysLogHandler.LOG_LOCAL2,\n    },\n    'exception':{\n      'formatter': 'standard',\n      'class': 'graphite.logger.UTFFixedSysLogHandler',\n      'address': '/dev/log',\n      'facility': logging.handlers.SysLogHandler.LOG_LOCAL3,\n    },\n  },\n  'loggers': {\n    'info': {\n      'handlers': ['info'],\n      'level': 'INFO',\n    },\n    'exception': {\n      'handlers': ['exception'],\n    },\n  }\n}\nif LOG_CACHE_PERFORMANCE:\n  LOGGING['handlers']['cache'] = {\n    'formatter': 'standard',\n    'class': 'graphite.logger.UTFFixedSysLogHandler',\n    'address': '/dev/log',\n    'facility': logging.handlers.SysLogHandler.LOG_LOCAL4,\n  }\n  LOGGING['loggers']['cache'] = {\n    'handlers': ['cache'],\n    'level': 'INFO',\n  }\nif LOG_RENDERING_PERFORMANCE:\n  LOGGING['handlers']['rendering'] = {\n    'formatter': 'standard',\n    'class': 'graphite.logger.UTFFixedSysLogHandler',\n    'address': '/dev/log',\n    'facility': logging.handlers.SysLogHandler.LOG_LOCAL5,\n  }\n  LOGGING['loggers']['rendering'] = {\n    'handlers': ['rendering'],\n    'level': 'INFO',\n  }\nif LOG_METRIC_ACCESS:\n   LOGGING['handlers']['metric_access'] = {\n    'formatter': 'standard',\n    'class': 'graphite.logger.UTFFixedSysLogHandler',\n    'address': '/dev/log',\n    'facility': logging.handlers.SysLogHandler.LOG_LOCAL6,\n   }\n   LOGGING['loggers']['metric_access'] = {\n    'handlers': ['metric_access'],\n    'level': 'INFO',\n   }\n. Good suggestion, and it also solves the question of how to get syslog configuration into the standard release, that I hadn't yet figured out. I'll try to do this and update the PR.  Thanks for the quick review.\n. Close for now -- this needs more work to merge, and I'm unlikely to get it in the near future. If someone else wants to take this and run with it, feel free.\nAlso looks like #499 may have solved this in a better way.\n. ",
    "cosimo": "Not sure the segfault and the + \"</g>\" is the same issue here.\nI'm happily running graphite under mod_wsgi with no segfaults, but my browser refuses to display the svg file because of an unbalanced </g> closing tag.\nThe reported fix, removing the + \"</g>\" works, even if I have no idea why :-)\n. Please see my comment here: https://github.com/graphite-project/graphite-web/issues/179#issuecomment-16386658\n. ",
    "mark-a": "Had the same problem; cloud fix it with \"WSGIApplicationGroup %{GLOBAL}\" in the apache config file.\nRelated: http://stackoverflow.com/questions/4236045/django-apache-mod-wsgi-hangs-on-importing-a-python-module-from-so-file\n. ",
    "virtuald": "I know this is old, but what are the chances of getting this done anytime soon? Looks like there is a related pull request (https://github.com/graphite-project/graphite-web/pull/691), but according to a comment on this bug (https://github.com/graphite-project/graphite-web/issues/285) and my own experience graphite won't give you a new graph if you refresh more than a minute.\nAlternatively, if you can point me at the right part of the code, I'd be happy to go in and fix it too.\n. Excellent, good to know! I'll have to try that tomorrow. \n. ",
    "pfreund": "+1\nIt should be a great functionnality, especially when we have 2 graphs like eth1 and eth0 and metrics are not in the same order.\n. ",
    "Marenz": "The most desirable way to fix this would be to be able to set it in a config file\n. ",
    "dkling": "+1\n. ",
    "RReverser": "+1\n. ",
    "aeriff": "+1\n. ",
    "RichieB2B": "This is very silly. All formats of the xAxisConfig use %H except for the two most often used ones which use %l%p. Until this is configurable @mleinart 's fix should be the default IMHO. \n. ",
    "taschauer1": "+1\n. ",
    "sanga": "I appear to be effected by this also. None of the keyboard shortcuts the composer in the dashboard seem to do anything at all. Some detail: Firefox 23 on debian 6 with Graphite 0.9.10. Likely not browser specific though, as the same thing repros in current Chrome (v28)\n. also getting bitten by this relatively often.\n. ",
    "mihneadb": "Hello, are there any workarounds for this? Thanks.\n. ",
    "languitar": ":+1: hit this as well\n. I just stumbled across this as well.\nI think there is a definition problem now whether reduce should call the reduction function with a series list or with multiple arguments. But since series lists as the first argument are generally supported by nearly all functions, using multiple arguments would be much more useful because many functions do not support a series list as their second argument.\n. Same here\n. Oops, right, print statements are gone.\n. Might be that I hit a problem with retention policies here. So I'll close this for now.\n. ",
    "frlinux": "We hit this too, still present on 0.9.15 from EPEL. \n. Hello, is there any news on this one, we are still hit by that one a fair bit :/\n. Not a bug so closing this, sorry for the noise.\n. ",
    "mdeeks": "I'm also encountering this rendering bug in 0.9.12. It only seems to affect my areaMode=stacked graphs. It happens randomly, maybe 1 out of every 4 or 5 tries, if I keep reloading. \nCorrect graph:\nhttp://imgur.com/zXCk8wR\nIncorrect graph:\nhttp://imgur.com/GFm5s0g\nIncorrect graph 2:\nhttp://imgur.com/25V1FOc\nMy render URL:\n/render?hideLegend=false\n&yMax=100\n&format=svg\n&areaMode=stacked\n&title=CPU+Usage\n&height=210\n&width=420\n&template=plain\n&from=-6h\n&uniqueLegend=true\n&target=color(alias(averageSeries(system.db-3.cpu-.cpu-user),%22user%22),%22blue%22)\n&target=color(alias(averageSeries(system.db-3.cpu-.cpu-system),%22system%22),%22orange%22)\n&target=color(alias(averageSeries(system.db-3.cpu-.cpu-wait),%22iowait%22),%22red%22)\n&target=color(alias(averageSeries(system.db-3.cpu-.cpu-steal),%22steal%22),%22black%22)\n&target=color(alias(averageSeries(system.db-3.cpu-.cpu-interrupt),%22irq%22),%22brown%22)\n&target=color(alias(averageSeries(system.db-3.cpu-.cpu-softirq),%22sirq%22),%22gray%22)\n&target=color(alias(averageSeries(system.db-3.cpu-*.cpu-nice),%22nice%22),%22green%22)\n&_rnd=1391551467076\n. ",
    "brianmcdonnell": "I've been experiencing the same/similar problem too and can add some details on the cause of the issue. I'm running graphite 1.0.1\nAs a test, I was running a query over the last 30 days, with the same target plotted 5 times. Almost every rendered graph would erroneously render 1 of the 5 series at a lower level to the 4 others correct series.  The magnitude of the error would change on every load (I disabled request caching).\ne.g. request:\nhttp://graphite.example.com/render/width=913&height=456\n  &from=-30days\n  &target=sumSeries(stats.counters.servers.*.request.count)\n  &target=sumSeries(stats.counters.servers.*.request.count)\n  &target=sumSeries(stats.counters.servers.*.request.count)\n  &target=sumSeries(stats.counters.servers.*.request.count)\n  &target=sumSeries(stats.counters.servers.*.request.count)\nThe bug is easy to spot when you request multiple identical targets because you'd expect the plotted series to always overlap.  It also increases the chances of the bug occurring on a single request because there are multiple opportunities for it to occur.\nHowever, multiple plots are not required to produce the problem.  The following request will also occasionally render the series at incorrect scale.\nhttp://graphite.example.com/render/width=913&height=456\n  &from=-30days\n  &target=sumSeries(stats.counters.servers.*.request.count)\nAs others have touched on, this bug comes up when the relative \"from\" date aligns with a retention policy threshold. Mine is as follows:\n[stats]\npattern = ^stats.*\nretentions = 1m:6d,10m:30d,1h:1y\nWhen the relative \"from\" date aligns with a retention policy threshold AND the target involves loading multiple whisper files, some of the whisper files can load from one archive (in my case 10m:30d) while the remainder will load from the coarser grained archive (in my case 1h:1y).\nIn my example target above, the sumSeries function will receive results with a mixture of retention rates. The docs describe how sumSeries handles this:\n\nIf metrics with different retention rates are combined, the coarsest metric is graphed, and the sum of the other metrics is averaged for the metrics with finer retention rates.\n\nSo sumSeries will roll my 10m counts into 60m counts by averaging 6 10min values.  In my use-case the correct aggregation of these counts would have been to use sum rather than average.  Meaning metrics/whisper files that returned 10m values are under-represented by a factor of 1/6th in the final result.\nSo why does whisper sometimes return data from one archive and other times from another archive? The short answer is that it's a race condition that depends on how long it takes graphite to load all the whisper files needed for the request.\nAssume this target matches 10 different metrics:\nhttp://graphite.example.com/render/width=913&height=456\n  &from=-30days\n  &target=sumSeries(stats.counters.servers.*.request.count)\nA whisper file for each of the following keys must be read.\nstats.counters.servers.server01.request.count\nstats.counters.servers.server02.request.count\nstats.counters.servers.server03.request.count\n...\nstats.counters.servers.server10.request.count\nIn pseudocode, here's what happens:\nmatching_nodes = finders.findNodes(target)\n  for node in matching_nodes:\n    node.fetch(startTime, endTime, now, requestContext) # Link 1\nLink 1: https://github.com/graphite-project/graphite-web/blob/1.0.1/webapp/graphite/render/datalib.py#L125\nnode.fetch will in turn call WhisperReader.fetch.  Crucially, it does not pass now as WhisperReader.fetch does not accept this parameter.\nFinally WhisperReader calls whisper.fetch() but it cannot pass now as it doesn't have this value. whisper.fetch supports an optional now argument, but instead is forced to calculate now on the fly by getting the system time.\nWhen working through the list of whisper files the system time can advance to the next second.  Given that our query was literally on the threshold of 30day retention period, when reading the next whisper file, the from date will be 30days + 1 second old and whisper.py will be forced to use the coarser-grained archive for the current and all subsequent whisper files.\nThis causes the mixed retention period data, and in turn triggers the unwanted 'averaging' normalization behaviour by sumSeries.  The random scale of the error from one request to another is caused by the randomness of where/when the second will tick over as graphite loads the list of whisper files.\nThe Fix\nThe solution, is to pass now all the way down to whisper.py, so it doesn't change from the loading of one whisper file to the next.\n. ",
    "DanCech": "What I've actually been considering is to have whisper use the file mtime when calculating available retentions rather than now, though that would potentially cause similar problems if files have differing mtime values.  Passing now through would definitely be a big improvement since whisper now supports it.. until=6:00pm+tomorrow will work, because graphite (currently) looks for X:XXpm.  In general the parsing is pretty rigid about acceptable formats and could be made a lot more flexible.\nAlso note that + is correct here because it will be url-decoded to a space (until=6:00pm_tomorrow is also supported, as are until=6:00pm,tomorrow and until=6:00pmtomorrow).  until=6:00pm%2Btomorrow is not correct because it will decode to 6:00pm+tomorrow and +tomorrow isn't a valid offset.. Side note: until=+2d doesn't work because it decodes to <space>2d, which isn't a valid specification.  until=%2B2d decodes to +2d and is thus valid (same thing as now+2d).. It's definitely an improvement, also not sure where to start with auto-complete as I haven't done any dev on the graphite-web ui.  I'd suggest opening an issue for adding auto-complete to it, then going ahead with the merge.. In order to implement this, keepLastValue would need to be updated to check whether the first value of each series was None and if so get the previous limit values (would need to decide on a sensible maximum when limit = INF) to try and find the previous non-None value to start from, much like movingWindow does to bootstrap the aggregation.  We wouldn't want to do it unless the first value is None, because making the additional sub-request to get those earlier values requires another query to the data store.. This is outdated by #1818 . The big thing that would be needed for this to work would be the ability for carbon to respond to a find request, since right now if the whisper file doesn't exist then the standard finder is never going to find the series and the code won't even get as far as calling the whisper reader.\nIf carbon were to provide a find method then it seems like it should be possible to move all the cache functionality out into a finder and reader for the cache, and handle merging the cached data into the final results via new the MultiReader mechanism.  The biggest issue there seems to be aligning the data from the cache since it's stored with the raw timestamps and not aligned to a step.. This idea dovetails well with the work in #2093 so I went ahead and made a branch that follows on from those changes.\nI'll open a PR once #2093 has been merged and I've had a chance to add some tests, but in the meantime the diff is at https://github.com/DanCech/graphite-web/compare/backend-failures...DanCech:metrics-index?expand=1\nI changed the name of the function to get_index (since it doesn't return nodes) and added a base implementation so it'll work out of the box with any existing finders (but finder authors can override get_index with a more efficient implementation if desired).\nThe other change I did make was to remove the old cluster parameter to /metrics/index.json and replace it with the same local behavior as the other find & render endpoints (as well as adding support for pretty).  Supporting cluster is possible, but would require some messy code in STORE.get_index, it could be done if we think it's important to maintain strict BC though it's much cleaner and more consistent to use local IMHO.. Closing in favor of #2103 . The patch itself looks reasonable, adding unit tests is definitely a good idea, the suggested patch for checking valcnt makes sense to me.  I'll check it out.. I just made my own rebased branch and it seems to work ok, test coverage actually isn't bad but the tests will need to be updated to be able to fix the issue with the extra None values.  I'll dig into it more tomorrow.. Closing in favor of #2052 . This was implemented in #2146 . @deniszh at this point wouldn't we be adding this to master rather than 0.9.x?. At first glance it appears that the way the prefix add/remove code is structured is too permissive, and would cause problems if a user defined a metric that happened to start with the same node(s) as the prefix.. In my opinion, if we're going to support a setting like this it needs to be enforced properly, with the prefix being added to every request and any results returned from the remote node that don't start with the prefix discarded.. The only real change here is to not pass the local=1 flag when dispatching remote find and fetch requests, 1.x will handle this with nothing more than addition of a config flag to toggle the local param to 0 in the remote finder and reader.\nThe config flag is needed to avoid creating potential loops in a scenario where cluster servers might refer to each other (eg if you have an http load balancer in front of a cluster where host a has CLUSTER_SERVERS=b,c, b has a,c and c has a,b).. It hasn't been implemented, I can maybe look at it next week.. I'm working on a patch to add support for msgpack as an alternative to pickle (since my testing shows it's up to 10x faster for large payloads), and should be able to roll a local toggle into that also since I have to add the capability to specify that msgpack should be used instead of pickle.  I'm hoping to have that ready for review tomorrow, made some good headway today but still have some more work to do.. I'm going to see if I can use urllib3 directly, since that's what requests uses anyway.  httplib and urllib2 aren't thread safe.. bear in mind the current patch is very much a work in progress, I do intend to migrate the http calls for delegated rendering etc to the same lib. @Kenterfie I just pushed some updates, can you try again with DEBUG = True and LOG_RENDERING_PERFORMANCE = True and post the output of tail -qF info.log exception.log rendering.log for a single request?. can you grab the latest code? we identified a couple of issues today.  Also can you make the backend IPs distinguishable? 0.0.0.0 0.0.0.1 etc vs all 0.0.0.0 would be great.. @Kenterfie are you sure? the log lines like:\nTue Nov 29 22:53:37 2016 :: FindRequest.send(host=0.0.158.22:1444, query=<FindQuery: statsite.anonym.data.* from Tue Nov 29 21:53:37 2016 until Tue Nov 29 22:53:37 2016>) called\nshould include a raw timestamp at the end like:\nTue Nov 29 17:39:01 2016 :: FindRequest.send(host=1.2.3.4:80, query=<FindQuery: carbon.agents.*.* from Wed Nov 23 10:12:46 2016 until Thu Nov 24 10:12:46 2016>) called at 1480459141.4. @stan-sack if you can share some logs that would help us figure out what's happening in your case. In my clustering branch I'm passing requestContext down to the readers and using it to implement forward headers.  I haven't updated the finders to use that also but I do intend to do that.  Reading from the request context is thread safe, there isn't any locking needed unless you're writing to the same key from multiple threads.. Personally I don't see an issue with making the headers available, as they could well be used by custom storage backends.  @benburry if you want to add that entry to __slots__ and rebase this on master I'd support merging it.. Solving the issue of node.fetch getting access to the headers so it can pass them to RemoteReader is somewhat complicated.  In #1818 we're passing requestContext (and now) through since RemoteReader needs access to requestContext, but since this branch doesn't have now that complicates things slightly.\nI can cut another branch to make a minimal patch for handling now, which should then let us cleanly update this branch and get requestContext down to RemoteReader so it can grab the headers in a way that won't later conflict with #1818. Ok, this should be pretty much ready for merge. @obfuscurity @deniszh you guys want to take a look?. This is quite out of date now since the updates to add python 3.x support.. hmm, looks like travis-ci must have a hardcoded list of tox envs to run, since it's not happy about me nuking the unused ones.. @SEJeff thanks for the tip, I'll update the PR. All I'm saying is that the FloatEncoder change made JSON rendering an order of magnitude slower than using json.dumps without the custom encoder.  For people with large data sets it's quite possible with FloatEncoder for the JSON rendering phase to take the majority of the time spent servicing a request.  For reference, the time spent retrieving the result set (find & fetch) in my examples above was less than 0.4s.\nI didn't really start looking at that part of the pipeline until after the change was merged, but it does jive with what I was seeing in initial testing (before the merge) where it didn't seem to be a bottleneck.  The way the time taken was reported previously (including the fetch time) also made it difficult to understand exactly how much time was spent rendering the JSON because the duration logged included the find & fetch duration.. @rehevkor5 thanks for detailed comment!\nI has the same concern as you about unintentional replacements, which is why I first tried:\n - using default() doesn't help us, since it is only used to encode custom objects.  We could maybe wrap every value into a custom object but I haven't tried that yet, since we'd be adding a lot of overhead to build up the list of \"value\" objects to then pass them into the encoder.\n - a raw string manipulation solution, I got it down to 2x the speed of raw json.dumps but I'm not sure how much better that that I could make it.\nAs for simplejson, the code in FloatEncoder actually looks a lot like https://github.com/simplejson/simplejson/blob/master/simplejson/encoder.py#L299 so I doubt that would help much.\nReplacing the values before encoding won't work because then you'd end up with \"null\" in the output instead of null, We could possibly pre-process the values array and replace None, NaN & +/-Infinity values with placeholders like ###NULL### or similar, then do string replacement on those strings, but I feel that it would add complexity and processing time for little benefit.\nWhat I did include to help prevent the incorrect replacement scenario you described in that the replacement isn't None but None, etc.  That should make it less likely that there will be an incorrect replacement.  We could take that a step further by doing .replace('[None, ', '[null, ') etc (with or without the trailing space), though we would require an extra call to replace -Infinity since right now the single infinity replacement matches both positive and negative infinity.\nAs far as generating a large dataset, you can just take a target that matches a good number of series and use a long time range or include it multiple times.\nI absolutely agree that the JSON output should be correct, so I hope we can arrive at a solution people can support.. The concern I'd have with this approach would be that it's creating a seemingly-random (and long; an issue when you have a lot of them) proxy value for the Inf/-Inf values, and as far as I can tell it'll still output None rather than null for None and NaN values.\nI initially left the FloatEncoder class in for benchmarking, it and the associated import and if/else should be removed before any merge.. Personally I'm not in favor of the 1.7976931348623157e+308 and 2.2250738585072014e-308 values for Inf and -Inf, which have the disadvantage that json.loads('[2.2250738585072014e-308]') returns [2.2250738585072014e-308] rather than [-inf] (as json.loads('[-1e9999]') does) as well as being ~3x as many bytes.  The actual values used are also dependent on sys.float_info.max / sys.float_info.min which are platform/implementation dependent and for cPython are based on the values for the c constants DBL_MAX / DBL_MIN.\nI know that doing string manipulation on the JSON is also not a perfect solution, but I don't see a better one.  It lets us use a compact and platform-independent representation for these values, which will get correctly decoded by python.\nI can update this PR to totally remove FloatEncoder very easily, it's only in there for comparison purposes.. code changes look reasonable to me, need to figure out the failing test though.. Closing in favor of #2063 . @deniszh I suspect it's to do with the way that the whisper library use time.time() when determining the covered intervals.  We never saw it before because there was no test coverage, but itooks like those test are affected by how long it is between the whisper files being written and when they're queried.  For now I'd recommend commenting out those lines in the test that assert on 'intervals' values, and I'll try to figure out what a solution might be.  The reliance on time.time() in the whisper finder seems like a bug to me. In whisper itself time.time() is present but seems to only be used as a fallback if a \"proper\" value isn't provided. . @xneo64 Can you share any exception logs?  The changes in #1810 should have resolved the issue that was previously causing problems with 2.1+. This looks good, but I'm not sure what the implications are of disabling packrat. @ybizeul do you mean on chat or something you can link us to?. @ybizeul doesn't appear to mention packrat, or is that what he's referring to about multiple threads?. After reading The pyparsing FAQ on packrat I think that as long as performance isn't affected adversely we shouldn't see any other issues from not using packrat.. Very nice! Can you share the benchmarking script?. This could be done via a relatively small change in webapp/graphite/urls.py to select which routes get added to graphite_urls. Interesting.  The issue is in the code that handles maxDataPoints, and from the sounds of things it's somehow coming up with a number of points to be removed that's greater than the number of points in one of the returned series.. @obfuscurity @deniszh yes, I've been using this code as part of the cluster performance testing so I can run both 0.9.x and master front-end nodes against the same set of 0.9.x back-end nodes to eliminate back-end node performance as a source of variation.. The alternative approach would be to change the master find output to match the 0.9.x format, since I'm not sure that there's any real benefit to the change in format.. You are running an old version of Graphite, please update to the current release which will solve the problem.. @deniszh definitely still very early, you're welcome to take a look though :). There is still an issue when using prefetch:\n2017-02-10,21:26:45.422 :: Got an exception when fetching data! Try: 1 of 2. Root cause:\nTraceback (most recent call last):\n  File \"/home/dcech/graphite-project/graphite-web-master/webapp/graphite/render/datalib.py\", line 170, in fetchData\n    seriesList = _fetchData(pathExpr, startTime, endTime, requestContext, seriesList)\n  File \"/home/dcech/graphite-project/graphite-web-master/webapp/graphite/render/datalib.py\", line 240, in _fetchData\n    for path, results in result_queue:\n  File \"/home/dcech/graphite-project/graphite-web-master/webapp/graphite/render/datalib.py\", line 211, in result_queue_generator\n    for key, fetch in fetches.iteritems():\nRuntimeError: dictionary changed size during iteration\nTraceback (most recent call last):\n  File \"/home/dcech/graphite-project/graphite-web-master/webapp/graphite/render/datalib.py\", line 170, in fetchData\n    seriesList = _fetchData(pathExpr, startTime, endTime, requestContext, seriesList)\n  File \"/home/dcech/graphite-project/graphite-web-master/webapp/graphite/render/datalib.py\", line 240, in _fetchData\n    for path, results in result_queue:\n  File \"/home/dcech/graphite-project/graphite-web-master/webapp/graphite/render/datalib.py\", line 211, in result_queue_generator\n    for key, fetch in fetches.iteritems():\nRuntimeError: dictionary changed size during iteration. @deniszh @obfuscurity I think this is ready for some more testing now \ud83e\udd1e . Let me see if I can carve out some time today . @iksaif @deniszh I think this is ready for another round of feedback\n@reyjrar did you get a chance to give this a workout?. Looking good \ud83d\udc4d . Good call @deniszh, added tests for the json format and expanded the pickle tests. @deniszh missed the tests, just needed a couple of tweaks from clustering_skip_find.  All sorted now.\nSo the relationship is that the functionality in #1818 requires that requestContext be passed down into the readers.  This patch implements that change with minimal changes to the core functionality itself, so we can reduce the overall size of the #1818 patch and keep things more \"bite-sized\".\nI didn't realize when doing this that #1736 already existed, so at this point I'd suggest that we first look at merging that PR, then I can rebase this branch and the patch set here will be reduced to the structural changes needed to support #1818 as well as some assorted cleanups.\nAlso of note, the large change in datalib.py is due to moving _fetchData out of fetchData, the logic changes involved there are minimal and can be seen more clearly with whitespace changes disabled https://github.com/graphite-project/graphite-web/pull/1821/files?w=1#diff-9cfc4413865fc2a2f6942d9a215fc4e1. Actually I can open a separate PR just for moving _fetchData, which should clarify things here also.. @deniszh this should be a lot clearer now. @obfuscurity we can do that, but it'll need to be rebased and have the requestContext stuff wired up properly since right now it doesn't forward headers for render requests because of the missing link in node.fetch. This is now just various small refactors to support #1818. See https://github.com/graphite-project/graphite-web/pull/1822/files?w=1 to ignore the outdenting of _fetchData. @obfuscurity yes, the optimizations discussed in #1063 are included in #1818. I'd be very hesitant to call it an rc without #1818 or similar.  I'm on vacation this week but will be back in action Monday and can push through any remaining issues with #1818. . @ybizeul can you answer my question on #1810 about packrat?. @mayurmahajan can you resolve the conflicts?. What I'd suggest is to use summarize(summarize(<series>, '1h', 'avg'), '10y', 'sum') which will give you a single value that is the total Wh for the period you're looking at.\nThis works by first taking an average W for each hour, which will be the Wh for that hour, then summing them up to get the total for the period.. seems reasonable enough at first glance. Python 3.x is now supported. I'll see if I can take a crack at this.. Yup \ud83d\udc4d . Looks good. Seems that error would happen if node.intervals was an empty list. @leochen4891 upgrade to a supported django, 1.8+. Since it's an edge case I'd suggest it should at least be logged. I'm very much in favor of a bugfix release in the 0.9.x branch. I'd propose a small fleet of images that are designed to work together, with carbon and graphite-web as the core, and potentially modified versions of the grafana and/or statsd/collectd images to help people get going quickly.  Once they're published then it would be relatively easy to give people a docker-compose.xml that could fire up a complete stack based on the individual images.. @replay could use your eyes here also. I spoke to @replay and he's going to look into it, reducing the number of worker threads is an option.. You can do this using the clustering support if you have one graphite-web node querying the folder containing the \"current\" data and another the \"archived\" data.. Looking at summarize there are definitely some gains to be made internally since right now it uses a multi-step process and generates an intermediate list of buckets.  I have a rough patch to reduce that so it only walks the data once, you can try that out in https://github.com/graphite-project/graphite-web/tree/summarize if you're interested.  In my testing I'm seeing about a 40% reduction in the time spent in summarize(). @deniszh it depends on how many data points you have, with a large number of points I was seeing about a 40% decrease in runtime compared to master which could easily be several seconds difference.. If it looks good to you and @iksaif I don't see why not.. It's been a while, IIRC I was just running requests using summarize over large amounts of data from collectd and collecting timing information.. At first glance this looks really good :). I do have to agree that this change is confusing.  Part of the issue really is that the way functions produce aliases for their output makes it difficult to say that there's a single \"right\" way to implement aliasByNode(), though the BC break for dashboard does worry me.  In the short term the simplest solution for the OP would be to use aliasSub(), but I do think this warrants further discussion.. We're continuing to run into issues with the new aliasByNode, with dashboards that worked on 0.9 failing with parse errors etc.  I'm going to dig deeper.. @Dieterbe you need to update to 1.1.1. I was going to say that I like this a lot but feel that it'd be more powerful if it used sprintf syntax for building the alias vs regex replacement  That would allow things like formatting the value properly if it's a float, etc.. @deniszh as a more general comment, we need to be cognizant that once a new function is added to the API we're making a commitment to supporting it going forward and we should allow time for discussion to happen before we merge something like this.. @deniszh cool, I'll throw together a PR to demo what I mean.. More or less, I have a patch that uses series.name = newName % current since that syntax is much closer to the standard c-style printf syntax.\nhttps://docs.python.org/2/library/stdtypes.html#string-formatting-operations\nhttps://en.wikipedia.org/wiki/Printf_format_string\nI'm just expanding the docs a bit and I'll open a PR for that.. #1938 . I have started a patch to support http(s):// as a prefix on the entries to signal whether to use http or https for communication with the backend host, so officially supporting a path also would make sense imhoOn May 23, 2017 1:07 PM, Denis Zhdanov notifications@github.com wrote:Hello @erikmack,\nAgreed, CLUSTER_SERVERS just silently implies that it contains part of URL, maybe we should make that explicit (e.g. introduce CLUSTER_SERVERS_URLS), but, for now, I think we just need to fix documentation somewhere around CLUSTER_SERVERS and/or URL_PREFIX\nThanks for reporting that!\n\u2014You are receiving this because you are subscribed to this thread.Reply to this email directly, view it on GitHub, or mute the thread.. You beat me to opening this!  I'm fine with adding a dependency, especially since we could then start looking at places where numpy might be able to speed up existing functions.. Pluggable functions is interesting, and I've been thinking a little about an endpoint to be able to advertise to tools like Grafana which functions are supported by the graphite instance, but there's a lot of work there, and I am concerned about fragmenting the api out into 3rd-party functions that then make moving the core forward more difficult and make it harder for users to find out what functions are out there.\nThat said, the optional dependency idea seems to be a good one, and looking at the asap codebase it's actually only using the numpy.fft fft and ifft functions, so it seems like it would be reasonable to add a pure-python implementation of those to use as a fallback if numpy isn't installed.. Pluggable functions is interesting, and I've been thinking a little about an endpoint to be able to advertise to tools like Grafana which functions are supported by the graphite instance, but there's a lot of work there, and I am concerned about fragmenting the api out into 3rd-party functions that then make moving the core forward more difficult and make it harder for users to find out what functions are out there.\nThat said, the optional dependency idea seems to be a good one, and looking at the asap codebase it's actually only using the numpy.fft fft and ifft functions, so it seems like it would be reasonable to add a pure-python implementation of those to use as a fallback if numpy isn't installed.. @deniszh yes, I was only suggesting it as a better option than having the function be totally unavailable if we wanted to make numpy/scipy an optional dependency.\n@drawks there are a lot of use-cases that graphite-api just doesn't handle, and it doesn't have any of the threading or clustering support that's present in graphite-web 1.0.  I do like the idea of deprecating the ui in favor of Grafana, and am pushing to make that possible by improving the abilities of Grafana as a tool for exploring the metrics hierarchy.. @iksaif @deniszh sounds like we're on the same page inre the UI, need to think a bit about the best way to do that without introducing a lot more work.  One thing we could do which would be simple is to take functions.py and break that out into its own package, then graphite-api and similar projects could import and use that if they wanted rather than forking it.\n@denish yes, I'm all for getting ASAP smoothing into the codebase.  I would like to see numpy as an optional dependency, with a pure-python fft fallback if it isn't installed.  https://gist.github.com/wilem/9824988 might be something to build from there.. When you say events are you referring to things like annotations?  I'd argue that expanding into that arena is something that doesn't add a lot of value for most users, especially since Grafana now includes annotation functionality, and as you say a lot of folks are already using ES etc.  What do you see as the advantage for the user of having events in graphite?. Actually Grafana now has support for storing annotations in its own database.. @iksaif I thought the updated annotation system had been merged, but it's still in development.  I just reached out to @torkelo for more info on the state of things.. https://github.com/grafana/grafana/issues/1286#issuecomment-294281268\nhttps://github.com/grafana/grafana/pull/8197. LGTM. Yeah, that seems like the least-invasive way to sideskirt any thread-safety issues in pyparsing.. I think the idea is that the maxDataPoints value is usually set to the width of the graph in pixels (Grafana does that unless you manually override), so you want the total number of intervals in the result (across all series) to equal that number to end up with an interval per pixel.  If you do it per-series then you'll end up with more points than you have pixels to plot them on if the time ranges don't all align.. The best solution is likely to add previewSeconds as an additional optional parameter on the holt-winters functions.. Looking good, I'll do some testing as soon as I can.  @replay how does this look to you?. I haven't actually had a chance to test this yet, @deniszh did you do any testing?. @deniszh I'd suggest the same syntax as movingAverage and summarize use, there's already a dedicated parser for it.. You can use aliasSub(groupByNodes(some.series, 1), \"^.*\", \"\\1 Derivative\")\nhttp://graphite.readthedocs.io/en/latest/functions.html#graphite.render.functions.aliasSub. Yeah, we still need to do more testing on the refactored clustering before I'd consider it stable enough to cut 1.1.0, so I'd merge it to master then backport to 1.0.x for release. Please do. Looking good now. @deniszh this fixes an issue in 1.0.2 where timeShift doesn't work properly with remote prefetch (it returns non-shifted data), what do you think about merging it to 1.0.x and cutting 1.0.3?. @deniszh After thinking about this a little more I think the time may have come to release 1.1.x, since there are a bunch of other changes in master that it would be good to get into a stable release.  What do you think?. What character set do we use for the database? That's typically the thing that trips utf-8 up.. Sounds like an issue with the code that checks whether the remote data was prefetched or not. Yes, but the code that does the actual fetches should see that it wasn't prefetched and fetch it normally, regardless of the prefetch setting. Yeah, the whisper file must not have any archive entries in the header.. Can you provide information on the query you're using, the whisper file, the data you're expecting, and the data you're actually receiving?. Yes, that would do it.  The config file is a python script so it needs to be valid python syntax, including string quoting.. LGTM. @iksaif not sure I follow, there is already support for pluggable TagDB backends, and the mapping of tagged series names to actual files is handled by the finders.  What else would you want to customize?. Because there isn't a 1:1 relationship between TagDB and a specific finder, the TagDB operates one level up the stack.. @iksaif did you have a chance to take a look at this yet?. @iksaif hope you had/are having a great trip, would love to get your feedback when you're back.\n@RichiH interesting stuff! One of the goals of the format I came up with was to retain as much compatibility as possible with existing carbon-based systems like relays, etc by sticking as much as possible to the existing set of allowed characters for carbon (the only change being allowing =), while supporting the OpenMetrics format would require using {, } and \" (in addition to =) which are all currently considered special characters in graphite.. @iksaif I think I've hit most everything here also.  For the moment I'm punting on creation of a shared graphite-common lib, but we can do that at any point in the future.\n@deniszh could use your feedback here also. Ok, @iksaif @deniszh @obfuscurity @cbowman0 I think everything brought up so far has been addressed.  Can I get a show of hands in favor of merging this beast?. @deniszh yes, that's been on my todo list for a while, I'll go ahead and get something put together.. I'm going to merge this, then work on the documentation in a new branch. Looks good go me, anything mod 1 is going to be zero so it'll work well.On Jul 28, 2017 6:42 PM, Denis Zhdanov notifications@github.com wrote:Cool, good catch!\nNot sure about the fix, though. Now setting FIND_CACHE_DURATION=0 will move start and end time to -1 second. It's propably harmless, but not needed side effect IMO. @DanCech @iksaif ?\n\u2014You are receiving this because you were mentioned.Reply to this email directly, view it on GitHub, or mute the thread.. I'm not clear on the exact purpose, but it seems that the problem would be solved by using:\nrelative_fs_path = metric_path.replace('.', os.sep) + '.wsp'. @deniszh this is the same issue I'm referring to in #1970 . Handling this properly is going to be a non-trivial exercise, especially while maintaining support for both python 2.x and 3.x.  Under 2.x this seems like it would still result in UnicodeEncodeError getting raised when it tries to convert unicode strings coming back from the filesystem into ascii.. @melnikk Why was this merged? Looking at the diff it doesn't do anything.. @deniszh ah yes, that last commit threw me for a loop.  It's not needed any more since we have movingSum, movingMin and movingMax now anyway.. One function call? I very much doubt it.. I'll take a look. Yeah same here, might be a transient problem with whatever builds docs for the site? I just pushed a minor docs tweak to master so I guess we'll see if the build is still failing. Appears to be an issue with importing scandir:\n/home/docs/checkouts/readthedocs.org/user_builds/graphite/checkouts/latest/docs/functions.rst:17: WARNING: autodoc: failed to import module u'graphite.render.functions'; the following exception was raised:\nTraceback (most recent call last):\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/latest/lib/python2.7/site-packages/sphinx/ext/autodoc.py\", line 551, in import_object\n    __import__(self.modname)\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/latest/lib/python2.7/site-packages/graphite/render/functions.py\", line 33, in <module>\n    from graphite.storage import STORE\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/latest/lib/python2.7/site-packages/graphite/storage.py\", line 17, in <module>\n    from graphite.finders.utils import FindQuery\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/latest/lib/python2.7/site-packages/graphite/finders/utils.py\", line 6, in <module>\n    from graphite.readers.utils import wait_for_result, FetchInProgress\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/latest/lib/python2.7/site-packages/graphite/readers/__init__.py\", line 3, in <module>\n    from graphite.readers.whisper import WhisperReader, GzippedWhisperReader  # noqa # pylint: disable=unused-import\n  File \"/home/docs/checkouts/readthedocs.org/user_builds/graphite/envs/latest/lib/python2.7/site-packages/graphite/readers/whisper.py\", line 9, in <module>\n    from scandir import scandir, stat  # noqa # pylint: disable=unused-import\nImportError: No module named scandir\nfrom https://readthedocs.org/projects/graphite/builds/5956726/\nDigging into what it will take to make that available on readthedocs. Found it, readthedocs uses docs/requirements.txt which doesn't include scandir.  I just added it, hopefully that'll do the trick.. All fixed \ud83d\udc4d. PR #2093 allows finders to expose find_multi and fetch methods, you can read the details in BaseFinder and an example implementation of fetch in RemoteFinder and RemoteReader in my branch.\nThe implementation differs from graphite-api in that it allows the finder to totally skip the find cycle and instead just perform the fetch directly.  Note that find_multi is only used by fetch at present, the find HTTP API only supports specifying a single path so it only uses find_nodes.   Because of this RemoteFinder doesn't re-implement find_multi since its fetch implementation doesn't use it.. Obviously this needs docs before it would be merge-able, I'd also like to write some tests for both the parser and evaluator since coverage is pretty weak there right now.  Feedback would be appreciated before I go ahead with that though.. The problem with using ctime to determine timestamps is that they aren't always going to be reliable, but at the same time they should be better than just assuming that the last modified time is now.. The simplest way to do that right now will be to call render on the series you're interested in and walk the result to find the last non-null value.. @deniszh I tweaked things a little to make the function name less verbose and to sync the implementation with groupByNodes, which is very similar but accepts the nodes that match for each group rather than the nodes that are different between each group.  I'll update the docs so that each function refers to the other to make the relationship clearer.. I have a few things still cooking, might ask you to hold off one more week depending on how things go.  Here are a few things that are on the hit list right now:\n\nhandling the case when all remote reads have failed (and there's no local finder), right now an empty result is returned but it really should be an error\nmiddleware to properly format render error responses as JSON when using format=json\nsupport for matching series by node/tag(s) in asPercent\nan endpoint to return a list of supported functions. Yeah #2103 is outstanding right now, we can go ahead with the release once that's merged I think. I don't want to hold the release up while I try to find time to finish the error format stuff.. Haha yeah that too. Given the amount of new stuff in this release, I think we should tag a 1.1.0beta1 or similar release from master ASAP to get as much feedback as possible on the new features etc.\n\nThe major outstanding items I have for a 1.1.0 final release are the finder documentation and an update I'm working on to add support for federating the tagdb so we can scale that out across a cluster (Pr coming soon), but there are a handful of other small things I have in various stages of development also.. @deniszh I'd like to get it into 1.1.0, it's not going to break any existing interfaces but will make it much easier for folks to upgrade their existing sharded clusters since they'd be able to have each carbon-cache send tags to the corresponding graphite-web vs having to use a single central tagdb.  Let me get the patch finished and open the PR, then we can decide what to do in terms of releases.. I just realized that I haven't gotten around to the change from _ to _DOT_ in tagged series filenames, I'll get that done really quick then we should drop pre2. Some high-level items:\n- Tag support\n- Updated cluster request handling\n- \"pipe\" function chaining syntax\n- xFilesFactor support\n- overhauled aggregation functions\n- msgpack support. The tag index deals with the logical names, the encoded names in the whisper file hierarchy are only used to make sure that we distribute the files properly on the filesystem.\nCan you share the list of series you have in the index and the output of find storage/whisper/_tagged -type f?. That all looks correct, the whisper finder is responsible for handling the encoding back into the _tagged format, since it may be handled differently by different readers.  You can see that happens at https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/finders/standard.py#L33-L37\nI wonder if there is some encoding issue that's resulting in a different hash and therefore it can't find the files on disk.... Are you sure you have your whisper finder enabled?\nI just did a quick test here:\n```\nsha256sum, this should match the file path\n$ echo -n 'tags.diceroll2;foo=bar5;label=value' | sha256sum\n20097e6e32bbddca626bbbfe325e1399367128a05c8764333df08e4362767d54  -\nsend a point to carbon so it creates the whisper file and tagdb entry\n$ echo 'tags.diceroll2;foo=bar5;label=value 1 '$(date +%s) | nc localhost 2003\nvalidate that the whisper file is where we expect it\n$ find storage/whisper -type f -name 'diceroll'\nstorage/whisper/_tagged/200/97e/tags-diceroll2;foo=bar5;label=value.wsp\nverify that the entry is present in the TagDB\n$ curl 'http://localhost:8000/tags/findSeries?expr=label=value'\n[\"tags.diceroll2;foo=bar5;label=value\"]\nquery the series\n$ curl 'http://localhost:8000/render?format=json&target=seriesByTag(\"label=value\")|sortByName()&from=-1min'\n[{\"datapoints\": [[null, 1507214640]], \"target\": \"tags.diceroll2;foo=bar5;label=value\", \"tags\": {\"foo\": \"bar5\", \"name\": \"tags.diceroll2\", \"label\": \"value\"}}]\n``. That would explain it ;). The latest commit changes the behavior when there are entries inseriesListthat don't have a corresponding series intotaland vice-versa.  Previously they were ignored, now they produce output series that haveMISSINGin the corresponding part of their name and values that are allNone.  They can be filtered out if desired by piping through|exclude('MISSING'). Good call, I'll add it to the docs. Thanks @mbethke I have a version I'm going to publish shortly that includes updates for all the tests to includeconsolidate(3). @iksaif @cbowman0 @obfuscurity any input on the change to not always appendNone` when the consolidation interval divides evenly into the number of raw points? I looked through the history and it seems that behavior has been around since the initial import by @cdavis but it does seem incorrect to me.. Cool, I'll go ahead and merge this \ud83d\udc4d \nI have what's turned into a decent-sized patch coming shortly that wires xFilesFactor support through all the remaining functions that aggregate across series or intervals, just needs a little more polish and some work on test coverage.. @iksaif not sure I understand the question, there is no change in behavior unless you set a non-zero xFilesFactor. @deniszh the most common use for setting maxDataPoints=1 is the singlestat panel in Grafana, where doing that means that Graphite only needs to return a single data point to Grafana (instead of returning a list and having Grafana do the aggregation client-side, or hacking it in Graphite with |summarize(\"10y\") or similar).\nThe problem with current master is that if you specify maxDataPoints=1 then typically you're going to throw out quite a lot of the points due to the \"nudging\", which makes the result more or less useless (and quite confusing to the end user).  If you look at the diff for c1752ff the tests show what happens to the values, the sum over the interval under the old scheme is quite a bit lower than you'd expect because most of the values are being thrown away.\nI'm not really sure what we should put in the docs, to really explain it you have to first get into a discussion about how the \"nudging\" works to align the intervals when using maxDataPoints.  The way I look at it, with this patch the user is going to get back results that don't need to be explained, where current master you definitely need to understand what's going on \"under the hood\" to see why your value isn't what you'd expect.  Any suggestions?. The effect is there for other low maxDataPoints, but it's less pronounced and I can't think of any real-world scenarios besides 1 where you'd use less than 10 or so.. Good call, I'll see about adding some detail to the docs.. I figured out how to test it, will check out docs tomorrow. Did you run manage.py migrate ?. It should be at /opt/graphite/webapp/manage.py. You may also want to try PYTHONPATH=/opt/graphite/webapp django-admin.py syncdb --settings=graphite.settings  If you're using a virtualenv then you'll need to have that activated.. It shouldn't be required, that getattr call should have a default I guess.  That said, you really should be importing and extending graphite.finders.utils.BaseFinder which will give you that attribute as well as local. See #2062. I need to check, but I think this will break parsing for specifications like 8:50 when now is specified since that would now go through line 41 instead of line 82, and so with this patch would be relative to the current wall clock time rather than the now timestamp.. @hzy001 do you have any examples of time specifications you're seeing that aren't being parsed properly?. @hzy001 I'm talking about the url parameter &now= which is fed into parseATTime as the parameter now.  I'll take a look at adding tests to make that clear, and whether we can relax the restriction without breaking things.. Closing in favor of #2070. Any reason not to use mock?. Yeah, I see that there are a few places in test_functions.py with the manual patching, and other places that use with patch('graphite.render.evaluator.fetchData', mock_data_fetcher): for scenarios like this.. Closing in favor of #2070 . Getting some docs together now. Looks good, just need to lose the unneeded import.  Any thoughts on #2076 ?. The correct consolidation for counters is last not sum.  Whisper doesn't perform any aggregation when you send multiple points within the secondsPerPoint window, it simply uses the most recent one.. Why do you say that sum makes more sense for counters?  The correct aggregation for counters is last and should then be used with perSecond to give rates for graphing etc.. Yeah those are per-period counts not ever-increasing counters.. yep, will check out the PR.  We should also maybe think about making that default config clearer, with a .counter section and some comments about the difference.. I suspect that the logic there can be simplified, will experiment here a little.. Thanks!. This is an interesting idea, I'm actually working on a refactor of the way that queries are dispatched through the finders (PR coming soon) which should make it possible to expose the complete queries to the finders.. It's not available right now, but we could add it to the requestContext in evaluateTarget. @iksaif I'd love to get your feedback on this. @iksaif I'd love to get your feedback on this. Hmm, that is an interesting question.  I'd really like to avoid maintaining that stuff moving forward, especially since it isn't the easiest thing in the world to follow.  Let's think about what that would look like, since we'd also need to re-add code in the store to run wait_for_results or similar.. Well, I doubt there are many (any besides biggraphite?) plugins using the 1.x \"plugin api\", so I'd rather get it released ASAP than have things drag on.  At this point anyone running 1.0.x should really upgrade to 1.1.x anyway, unless we want to backport some stuff and cut a 1.0.3. @deniszh all that documentation is still correct, I didn't change compatibility with any 0.9.x-style plugins.  Looking at the code influxgraph, graphite-cyanite & graphouse will all work.  Right now there is no support for the graphite-api-style fetch_multi call so it won't take advantage of that for influxgraph or cyanite, but I think that it should be possible with a little refactoring in BaseFinder.  I will check that out and update the plugin API docs with the 1.1+ spec later this week.. Looking deeper the issue with trying to support fetch_multi is that it doesn't return which path expression each result is related to, which is required for the prefetch mechanism to work properly.  It also doesn't support passing now or requestContext, so I don't think trying to support it is a good idea.\nAt this point I think the best way to move forward is to update the docs so people have a guide to implementing the more efficient fetch path, but use the code in this branch as-is.. Cool, I'm going to go ahead with this merge so that I can open a PR for the index.json update, will open an issue for updating the docs to include the new capabilities that are currently only documented in the code of BaseFinder and BaseReader.. ConfigParser converts all option names to lowercase, it looks like you may have an older version of the carbon library installed since in master options.get('xfilesfactor') only occurs once in carbon/lib/storage.py.. Thanks for the info, seems like we should pin the cairocffi requirement. I need to work on test coverage still, but this is working.. Ok, I think this is ready for review now \ud83d\ude03 . I realized that with the changes it only makes sense to have a worker thread per finder because each python process will handle only a single request at a time, so we'll only ever dispatch a job for each finder at any given time.  Knowing that, I removed the POOL_WORKERS and POOL_WORKERS_PER_BACKEND settings, since they are no longer used.  I also removed the dependency of worker_pool on settings, the check for USE_WORKER_POOL is now handled in STORE.\nThe other big change is that when not using the worker pool we check the timeout value before scheduling a job, that way if the timeout is exceeded during execution of the last job it will still be returned rather than dropping all the completed work on the floor and returning an error.. You mean to use it as a maximum if someone has a really large number of finders?\nThat's not what it does in the current codebase, if we wanted to have a setting for that we'd need to add a new config item like MAX_POOL_WORKERS.. @iksaif that what you were thinking of?. I think it's all set.. How many series does that match? Can you paste the log output?. I mean how many series does the selector newapi.rc.*.request.*.sqlite.*.count match?  sumSeries is always going to boil down the series you pass into a single result, but that could take a while if there are a huge number of underlying series for it to process.\nFor logs, you should enable LOG_RENDERING_PERFORMANCE = True and LOG_CACHE_PERFORMANCE = True in your local_settings.py, then look in /opt/graphite/storage/log/webapp for entries in rendering.log, cache.log and info.log.. See #2183 #2185 . The biggest thing missing in Grafana today is a tree-style explorer for finding metrics, but if that can be added then I think it makes sense.. From IRC:\n\nI'm not a huge fan of forcing people to import autocomplete which then imports settings anyway and forcing people to put that boilerplate into each tagdb\nthinking it might make more sense to have the store provide settings and cache to the tagdb via init when it sets it up\nthen we don't need to have the hoops for autocomplete and it having to go back out and import STORE to get the caching, etc\nthe tagdb just gets passed settings and cache and uses them without needing any imports\nbasically https://en.wikipedia.org/wiki/Dependency_injection. That is interesting, I'll have to see if anything has changed in normalize between the versions.  Not sure what would be causing the memory allocation, possibly the backtrace.. Good point, it does seem like that trailing None may have been there to avoid this kind of issue by providing a bogus value if you iterated past the end in a scenario like this.  I'm digging deeper to try and figure out a reasonable solution. . @xtus can you try out that PR and verify that it fixes the issue?. Another wrinkle that has come up as I dig into this; in perSecond it tries to \"bridge\" None values by keeping track of the last-seen value and accumulating step values to try and produce a value when it sees the next non-None value.  There are a few issues with this, and we already had to fix one bug in #2029, but I'd argue that it's still not the right thing to do, since you end up with a graph that shows a value for an interval where we really don't know what happened because it's based on taking the last \"known\" value which may have been some time ago and we have no idea if the increase in the counter value happened evenly across the intervening intervals (as the code currently assumes) or if it was not evenly distributed over time.\n\nIn addition, though we assume that the rate was constant over that time period we only produce a value for the last interval so the rate produces for intervals during the \"unknown\" period doesn't reflect that assumption.  Finally, this approach is inconsistent with nonNegativeDerivative which does no such \"special\" handling.\nFor all those reasons I'm proposing removing this logic from perSecond, so that to produce a rate for a given interval it requires 2 consecutive non-None values.  This will make perSecond(x) equivalent to scaleToSeconds(nonNegativeDerivative(x), 1).. Ah, codacy doesn't like filter because it's the name of a builtin.  I'll change that back.. Yes, I think so.  We implemented the dot handling change, and can make the hash function configurable later if desired.. 14577ba also includes docs updates for the changes in #2093. Yes, it would.. I suspect the correct command varies by django version :( I'll do some digging and see what I can find out. Ok, I think I've hit all the outstanding issues here.  Much happier now that we're using BytesIO consistently.. @deniszh @iksaif care to give this a final once-over?. I'm working on an updated patch, will open a PR shortly. Very cool! Will check this out . I wouldn't unpin, but I'd definitely be in favor of updating the pin to the latest versions if they support both 2 and 3.. We should be able to handle txamqp the same way as we do in carbon, try the import and ignore it if it fails.. Ok, then we can either remove it or have the output say that AMQP support won't be available. Good call @brutasse , there are quite a few other pieces of code that are leftovers from pre-2.7 support and can go also, like handling lack of cPickle and/or cStringIO. As far as the ceres test, after mucking with it a bit I'm of the opinion that we shouldn't really be reaching into the ceres innards from within graphite-web at all.  The number of times it calls os.listdir is really an implementation detail that should be handled in the ceres test suite.  I'll update takluyver/graphite-web#1 shortly removing the references to os.listdir. @deniszh how does this look to you?  I'm thinking that it makes sense to squash merge this one. With the pipe syntax in 1.1 this can be written as aliasSub(series,old1,new1)|aliasSub(old2,new2).... Once we create an eslint config file codacy will use it. The easiest way is likely just to make another matrix entry that overrides all the defaults, installs node & eslint and runs it. single quotes all the way \ud83d\udc4d . @deniszh In Grafana the functions are organized hierarchically into groups, so the group functionality here is intended to help organize the functions and to support Grafana being able to pull a list of supported functions to build the UI so that we can get to a point where the Grafana UI can automatically show all supported functions (including any defined in plugins).\nI also added a params block and fleshed out some examples of what that might look like.  I still need to update the docs to describe the structure for that but you can see the examples in the diff.\n@torkelo would love to get your input on this!\n. Side note: Once we have the built-in functions categorized into groups we could think about breaking up graphite.render.functions into multiple graphite.functions.<group> files to help keep things manageable.. @cbowman0 If you want to take a crack at the frontend that would be awesome ;) I'm also interested in any feedback/suggestions to the api output or the way the params are structured that would make that easier.. All functions are now documented with a group and parameter info, I'm open to suggestions for improvements to the current groups.  I also have Grafana able to load and use the definitions \ud83d\ude01 but still have to update the docs showing how the params definition works and documenting the available options.\nAdditional updates:\n The _*_key functions added in #2139 have been replaced with a single function keyFunc that works with all safe* functions, e.g. seriesList.sort(key=keyFunc(safeLast))\n All aggregation functions are centralized in aggFuncs (& aggFuncAliases), and aggregate, movingWindow, legendValue, filterSeries (see below), aggregateLine, groupByNodes, smartSummarize, summarize now all support the complete set of aggregations.\n* New filterSeries function added, it consolidates handling of maximumAbove, minimumAbove, maximumBelow, minimumBelow, currentAbove, currentBelow, averageAbove & averageBelow into a single function that supports all defined aggregation functions and a complete list of comparisons (=, !=, >, >=, <, <=).  As part of this update averageAbove and currentAbove were changed to be consistent with maximumAbove and minimumAbove in using the > comparison (all the *Below functions were already consistent in using <=).\n. Good point, I'll add that. @iksaif I think it's much clearer now with Param & ParamTypes, and the definitions are much more compact too.\n@deniszh I think this is ready for a final review :). Ah, we may need to add the new files to a manifest for packaging, let me check that. c974645d146d1fd07e0c5e32af267328f70f20dd. I haven't looked into the packaging side of things before, it seems like we could switch to using find_packages() to avoid running into this problem in the future.  I'll dig into it a little.. Hmm, distutils doesn't provide find_packages() so we'd either need to vendor it in or start requiring setuptools.  I've forgotten too much about python packaging to know what the best way forward is off-hand, going to have to dig deeper.. FWIW those django \"URL pattern\" warnings will go away if you update to 1.11 (which I recommend).. See #2156 . You can use the maxValue parameter for nonNegativeDerivative to exclude nonsensical values.. Ah, I thought you were graphing raw counters.  Since you're using data that is already a rate you'll want to use removeAboveValue to exclude the spikes.. Good question, I haven't done enough release management with git to know the right answer but I'd tend to say that we should fork a 1.1.x branch and use it only for bugfixes, any new features should get released as 1.2.x anyway (as per https://semver.org/). I see 1.1.0 at https://pypi.python.org/pypi/carbon. I agree, we're not looking to advertise a closed-source proprietary system that promotes itself as a replacement for Graphite.. @deniszh yes, we definitely should.  Looking at hashing.py in carbon I see that actually there are quite a lot of differences besides the fnv1a_ch hashing that we should really resolve to be sure that it's doing the same thing as graphite-web.. @deniszh sure thing. It appears that INDEX_FILE is still used by graphite.browser.views.search, so maybe we should bring back write_index but update it to use the code that generates index.json instead of trying to walk the directories itself.. Actually if you run tox with no additional parameters right now I think it's going to try and run a huge number of different environments (check out the output of tox -l) so we should probably be more prescriptive there.  I usually run py27-django111-pyparsing2 and py35-django111-pyparsing2 locally along with lint.\nAs far as requirements, those only need a local redis server.  If you want to run -mysql or -postgresql locally then you'll need to have servers running , database users, etc.  For 99.9% of development that isn't required though, only if you're hacking on the tag db.. Fixes #2175. It seems to work when you restart the job, not sure what the story is with that.\nThanks for the contribution, will check it out ASAP!. Seems reasonable at first glance, I want to go through the logging and error message stuff in more detail. Haven't had a chance to look in depth yet :( will try to find some time this week.. I think this will be ok, one concern is that it uses 'Fetch' which is captialized vs 'find' etc which aren't.  There seems to only be one log entry that places the job/context description at the beginning of the output, so I'd suggest switching that to lowercase and potentially rewording that entry so it's within the line.\nThis also needs to be rebased, since the changes in config don't make sense after #2196 . They won't make sense because the comment in settings.py refers to settings that have been changed.. The same goal can be achieved by implementing fetch in the finder, but if we think it's going to be a common pattern we could include it here so authors don't have to deal with it.. @wangzaiqqhop when you use * the finder has to search every folder that matches, so if you have 30k folders but only want A, B, & C it still has to go through each of the 30k to see if they contain a hx_subCategory_Q19_daily_info1_count folder.  You can do it in a single query like sumSeries(api.*.QOSController.{A,B,C}.hx_subCategory_Q19_daily_info1_count). Yes, I just checked the finder code and it isn't smart enough (right now) to see that it can locate all matches for {A,B,C} without scanning the entire directory :(  It's that scan that's slow.\nLooking at the code it seems like it can be optimized, but in the meantime you're best off with sumSeries(api.*.QOSController.A.hx_subCategory_Q19_daily_info1_count,api.*.QOSController.B.hx_subCategory_Q19_daily_info1_count,api.*.QOSController.C.hx_subCategory_Q19_daily_info1_count). https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/finders/standard.py#L97-L170\nIt's a little hard to follow since it operates recursively, but you're interested in has_wildcard on line 103-104, that is what triggers it to either look for a specific file/directory or scan the folder.\nI think that it'd be possible to restructure the whole thing to make it more efficient by using the objects returned by scandir, and possibly to see whether an expression like {A,B,C} has a finite set of matches and if so testing them instead of scanning.. We use threads for multiple reasons, if your whisper files can't be read within the timeout period then you'll want to either increase the timeout or tell graphite-web not to use the worker pool via the existing setting.  Switching the worker pool off altogether when not using remote servers isn't a good idea.. It's a single timeout, we just didn't change the name yet to avoid people having to change their configs.  It's really just a sanity check, so there's no need (and it would be complicated) to have separate local and remote timeouts.  To avoid confusion I'll put together a PR that changes the canonical name but keeps backward compatibility for the old setting.. #2196 . This isn't a good solution, one thing that might make sense is to only use the pool if we have more than one finder.  I'll look into that.. Once I add native support in carbon we won't need an adapter at all for the standard use-case, though looking at graphite-remote-adapter it seems like it would be useful for cases where more control over the final series names etc are needed.. Indeed, and if the lengths aren't the same that may indicate that the resolution of the data is different, in which case trying to merge them would be counter-productive.  The best answer might be to switch to something like:\nif settings.REMOTE_STORE_MERGE_RESULTS and len(series) == len(known):. Looks good, not sure how up to date those carbon-api benchmarks are.  AFAIK graphite-remote-adapter doesn't (yet) support tags so I wouldn't call it \"fully featured\". https://github.com/sqren/backport/blob/master/src/lib/github.js has a hard-coded limit of 20 commits it seems. I'm concerned about the potential errors when setting STORE.tagdb to None, since there are a number of places in the code that assume it will be a valid TagDB object.  I'd be more comfortable with a subclass of BaseTagDB that ignores input and returns empty results and/or emits log entries saying that the TagDB was called while not configured.. I just don't want the whole thing to blow up when it tries to call a method on None \ud83d\ude09 It shouldn't be too hard to make a dummy TagDB to use for that case.. The whisper files just have to be moved to the right location, currently your example file would be /opt/graphite/storage/whisper/measurement/tag1/tag2/tag3/field1.wsp and you need to move it to /opt/graphite/storage/whisper/measurement/tag4/tag5/field2.wsp. Good point, it may make sense to update aggKey to do this automatically.  Something like this may do the trick:\ndef aggKey(series, nodes, pathExpression=None):\n  # if series.name looks like it includes a function, use the first path expression\n  if pathExpression is None and series.name[-1] == ')':\n    pathExpression = _getFirstPathExpression(series.name)\n  return '.'.join([getNodeOrTag(series, n, pathExpression) for n in nodes])\nWith that patch aliasByNode would no longer need to pass pathExpression to aggKey either.. I think if we're going to change it then the best thing to do is to change it globally vs having the behavior be different depending on the function that you're using.  I'm not a fan of aliasByNode being different from the other functions today and don't want to compound that issue by adding more functions that use _getFirstPathExpression without applying it equally across all functions.\nWith the change to use getNodeOrTag we fixed some of the issues that would previously occur if the user specified a node that couldn't be found, etc so I think we're in a much better position to make this change now than we were in the past.. This is really a philosophical issue, should we return incomplete results or let the user know that it's not possible to compute a complete result?  There are a lot of scenarios where trying to continue even though a shard is unavailable will cause the results to be incorrect in ways that are non-obvious, a good example is when using sumSeries as the sum will just appear to be lower than normal.\nMy personal opinion is that in this case returning an error is the right thing to do.  If we want to enable this functionality it should be gated behind a configuration setting so the administrator of the system can acknowledge that they want the system to continue when a backend node is down and return most-likely-incorrect results.\nThe correct way to achieve high availability is to run backend shards as load-balanced pairs, so that the system can operate correctly when either one of each pair is unavailable.. Yes, that would be a scenario where skipping a failed host would make sense.. If anyone wants to write a PR I'll be happy to review it.. Currently the filenames created are in a human-readable format, but it would be possible to add a configuration setting to support using the hash instead which would work around this problem by changing the behavior of TaggedSeries.encode().  That would need to be done in both carbon and graphite-web, and the 2 would need to be kept in sync.. See #2221. Please use master, there are issues with 1.1.1.  1.1.2 will be released early next week.. You need carbon from 1.1.x to get the fix, graphite-web 1.1.1 or 1.1.x will be fine.. Yes, the interaction between async and non-async code causes shutdown problems as well as problems with blocking the main thread.  In 1.1.x the metric and tag writers both operate as synchronous processes in dedicated threads to avoid those problems.. Right now whisper always calculates available data from the current time, so it won't be able to retrieve data older than 14 days.  You can work around it via the render API if you're running master or the 1.1.x branch by passing now=14%3A00_20170716 in your render request.. I've been able to duplicate this error when the whisper file exists but whisper.fetch returns None, will open a PR to fix that shortly. Looking much better, the last thing that needs to be done is to update the parameter definitions, you'll want to add an entry for seasonality after the Param('bootstrapInterval', ParamTypes.interval, default='7d', suggestions=['7d', '30d']), line for each function.  This is used to expose the parameter to make the new parameter available in systems like Grafana.. Thanks @LTMXcitrus \ud83c\udf89 . @deniszh as far as I can see the function signature didn't change, it still accepts a single series list.. It's a single series list, before it contained a single series, now it contains 2 series.. The problem is that aggregateLine produces a series with only 3 points, so it's not really going to work for this kind of thing.. The simplest change may be simply to have aggregateLine produce the same step as the input series, which would be quite a trivial patch.  I'm trying to think of any applications where that change would cause a problem.... The one issue I can see right now is that currently aggregateLine totally ignores the start and end of the series and instead uses the graph period, which would make the behavior with this change somewhat different when applied to a series where start and end don't match the graph period.  Not sure whether the best solution is to add a flag to control whether you get the legacy or the \"new\" behavior, that seems like the safest approach.  I have 1/2 a patch here, will see if I can find some time to polish it up tomorrow.. @pinkynrg was simpler than I thought \ud83d\ude42 . Title for the docs. You could use:\nsome.series|transformNull(-1)|removeAboveValue(0)|transformNull(1)|removeBelowValue(0)\nThat will take your values, assign any null points the value -1, remove all the values above 0 (setting them to null), transform those nulls to 1, then set the -1 values we set earlier back to null.\nThe total number of minutes above zero will be the sum of the series.. I like the idea a lot, but I'm much less keen on coupling ever more tightly with django, and I would like to see instrumentation of at least the basic steps in the find, render and processing pipeline so we can get some of the same stats that right now are just in the logs.. Yeah, those metric names are horrible also.  I think that it makes more sense for us to build this ourselves or to use a more graphite-friendly library.. The changes in #2244 seem like they should solve the problem you have currently where each host is trying to forward the tag requests to the others in an infinite loop.\nAn alternative approach if you're using a single redis tagdb for all 4 instances would be to specify noTags=1 in your CLUSTER_HOST definitions, which will cause the graphite-web host that initially receives the query to resolve the tags and pass the resolved names to each of the non-local graphite-web instances.. Yup, that's it exactly.  For remote servers defined with noTags=1 a graphite-web server will resolve the seriesByTag(...) calls to a list of tagged series and pass that resolved list to the remote servers rather than simply passing through the seriesByTag(...) request and letting the remote host resolve the call itself.  Direct tag API calls like autocomplete will not be forwarded to remote hosts that have noTags=1 specified.\nIn your scenario with a single shared TagDB you'll likely see better performance that way since you'll cut the number of requests to the TagDB and avoid the clustering HTTP requests for tag API calls.\nIf you had the TagDB sharded with each graphite-web host having a separate DB that deals with only the series stored on that host then you would not want to use the noTags=1 option and instead allow each graphite-web to query its own TagDB.. See https://github.com/grafana/grafana/pull/11002. The percentage of values that are 1 in a series of 1 and 0 will be the average value of the series, so if you're using a singlestat panel in Grafana you can just use isNonNull(local.random.diceroll) with Stat: Average and Unit: percent (0.0-1.0).  If you don't want/need to show the sparkline you can also set maxDataPoints: 1 in the query options so the average will be computed in Graphite and a single point sent to Grafana.. the ?noTags=1 setting just controls what happens when a query that contains seriesByTag calls is received.  By default the seriesByTag call is passed through to the cluster servers, with that option it is instead resolved against the local TagDB and the resulting series are requested from the cluster servers.  If you aren't using seriesByTag there won't be any difference.\nBasically ?noTags=1 is just meant to be used when you have a cluster server that doesn't have its own TagDB, doesn't support, or can't resolve seriesByTag calls.. yes if you're using a shared mysql then that will likely be more efficient since you'll just do the tag lookup once on the frontend graphite-web. Have you tried moving the template call inside the timeStack?\nhttp://graphite.il.faccousa.net/render?format=json&from=20180215&until=20180301&target=timeStack(template($ctrl.food_distributed_today.0),'24h',0,10)&template[ctrl]=this.is.my.controller. timeStack re-evaluates the query passed to it with the context adjusted for the different time periods, so with template outside it's evaluating $ctrl.food_distributed_today.0 not template($ctrl.food_distributed_today.0). If you'd like to take a stab at a PR to change the behavior as you're describing I'd be happy to review it.. Please give us something to work with. What are the results you are getting vs what you expect? Are you using maxDatapoints?\nround is an alias for roundFunction. It may be possible to have round set a  value in the request context like the consolidateBy function does, so that the maxDataPoints code could honor it.. @pinkynrg I'm describing development work that could be done, it's not supported right now.. Yes, it looks like regular division should be used there.. Please go ahead and switch it to use isinstance. I had the same thought, it should be a relatively small refactor in the render code.. If you are using 1.1.2 you can provide a set of nodes to asPercent that it will use to match the series.\nhttp://graphite.readthedocs.io/en/latest/functions.html#graphite.render.functions.asPercent. why do you have URL_PREFIX = '/graphite'?  If you comment that out it should work.. You can make custom functions that return the curves\nhttp://graphite.readthedocs.io/en/latest/functions.html#function-plugins. this may be a side effect of the performance improvements for the standard finder, though imho any previous case insensitivity would have been a bug rather than a feature.. yes, since you're using \" to enclose the entire url you need to use ' around the values because otherwise bash interprets it as 3 strings concatenated together.. It can happen in Grafana if you try to use a template variable to make the threshold select-able, since it wraps the value with quotes by default.. Thanks @Dieterbe . I agree with Denis, with those changes this should be merge-able.. The example setting in local_settings.py should be the default afaik, you will also want to add a test case to cover the new path.. Actually, thinking about this a little more the name of the config setting should probably be changed since it's actually applicable to all stores.  One possible option would be STORE_FAIL_ON_ERROR but I'm open to any suggestions.. After getting feedback from users it seems like we should be a bit more prescriptive as far as what characters are supported.  Things like ' cause problems when trying to query from Grafana, and values that start with ~ are a problem when querying.  The other thing I've been thinking about is support for escaping \"special\" characters.. Can you post your complete schema and aggregation config?. The problem is that when using hashed filenames it is difficult to determine which entry in the tagdb corresponds to a given whisper file.  Right now the only option would be to go through each tagdb entry and check whether the corresponding whisper file is present and if not then remove the entry.\nThe localdatabase tagdb does store a hash which will correspond to the filename (it is used internally to work around an issue with unique index length in MySQL), but the series cannot currently be looked up or deleted by hash.  We could possibly add support for that, but we would need to also update the redis and http tagdb implementations to support it.. Yes, you could definitely loop over the records in redis and calculate the hashed filename for each one, then if the file doesn't exist go ahead and issue a delete via the http api.. The hashed filenames are calculated here:\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/tags/utils.py#L69-L110\nThe process used to delete a series from redis is here:\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/tags/redis.py#L242-L259. What you're most likely seeing here is that when using maxDataPoints what happens is that the underlying points are divided up into groups and aggregated, but when reaching the end of the series the final grouping may not have the same number of points as the rest.\nFor example if you have a series with values 1,1,1,1,1,1,1 and ask for maxDataPoints=4 and consolidateBy('sum') that will get broken up into (1,1) (1,1) (1,1) (1) and consolidate to 2,2,2,1\nIf you want to avoid that you can add |xFilesFactor(1) to your query and you'll only get values for intervals that have enough points, so instead you'd get 2,2,2,null. @piotr1212 yeah, floating point math is another potential source of excitement.  The only ways I know of to get around that would be to use https://docs.python.org/3.6/library/decimal.html (which would slow down all calculations compared to float), or to use the round function to truncate to the desired precision and discard any leftover floating point math artifacts.\nRight now the math done in the maxDataPoints consolidation doesn't have any way to specify the desired precision, but that's something we could look into.. This is interesting work, it's been a while now but I seem to recall doing benchmarking at the time that didn't show anything like a 500x slowdown.  If you can share your benchmarking setup I'd be keen to see if we can figure out what's going on and whether there's a way to have the best of both worlds.. I'd recommend using asPercent with the nodes parameter, and smartSummarize to get the right alignment (as it's timezone-aware), something like:\naliasByNode(scale(asPercent(smartSummarize(prefix.*.user, '1day', 'sum', 'days'), smartSummarize(prefix.*.income, '1day', 'sum', 'days'), 1), 0.01), 1)\nThe reason for using asPercent rather than applyByNode is that it doesn't have to re-evaluate the query like applyByNode does, so it should be a much faster.  The scale(0.01) is there because asPercent outputs the ratio as 0-100 rather than 0-1 so we multiply by 0.01 to get the result you're expecting.. Need to dig into this a bit and make sure there's no performance regression . Is there any reason to push this forward given that we have #2358 ?. This doesn't seem to be worth the potential issues with backward compatibility.. Yes, that makes sense.  Shouldn't be too hard to implement. Someone will need to find time to work on it, I'd be happy to review a PR. This looks good, we should add avg_zero to the consolidateBy function docs and also add it to aggFuncs in functions.py so that it can be used consistently throughout the system.. On further review, this only adds support for merge_with_cache consolidation at read time for whisper files using avg_zero consolidation.\nTo support avg_zero in consolidateBy it would need to be added to __consolidatingGenerator in TimeSeries which will take some effort as that is geared entirely toward discarding None values.  The test passes because it just verifies that series.consolidationFunc is set, not that it actually functions.\nTo support avg_zero in functions that perform consolidation is possible, and doesn't require too much work.\nAt this point I'd accept this as a bugfix if the change in webapp/tests/test_functions.py is removed, then support for avg_zero in consolidateBy and the aggregation functions could be added separately.. Thanks @stembrain \ud83d\udc4d . There is an existing PR for this #2338, see the commentary there for what else is needed if you want to expand this PR. Is there a particular reason for wanting to use json over say msgpack?  I ask because the payloads are much larger with json so performance is likely to suffer quite a lot for larger queries.. This seems reasonable to me. You may be running into a situation where the backends are getting marked as bad for some reason, try setting REMOTE_RETRY_DELAY=0 and see whether that fixes the problem.  If that is the case you should also see a log entry indicating that a query to the backend has failed.. The tags stuff also uses the finder request method, so I'm not going to try and shoehorn the de-serialization in there also at the moment.. As far as I can tell this is working correctly, any objections to merging it @deniszh ?. When I implemented filterSeries to replace the old implementations I set it up that way to duplicate the previous behavior.  If we want to change the semantics of minimumBelow that would be a breaking change to the API, and we'd need to carefully consider the impact it might have on existing users of the function.\nAs you're no doubt aware, you can use filterSeries(<seriesList>, 'min', '<', <n>) to get the result you're looking for.. It looks like you need to run the database migration to create those missing tables.  The exact command will depend on your django version.. That should work, what is the output you get?\nThe other method you could try would be to use the manage.py script bundled with graphite, run ./manage.py migrate. The remote reader already uses self.finder.request, the only real difference is that in the reader it has retry logic, I suspect that the deserialize could be done inline within RemoteFinder.request and work just fine.  I do want to see more test coverage for both buffered and unbuffered use, so I'll incorporate that in #2354. Superseded by #2354 . This appears to be the issue described in #1054 and fixed in #1055, are you testing against the 1.x release series?. Thanks for the confirmation \ud83d\udc4d , if you run into any other issues please let us know.. you need to use an underscore, not a dash.  That will be (one of) the reason(s) that graphite-web can't load your local_settings.py file.. Currently the way it works is that the /tags/autoComplete/tags and /tags/autoComplete/values calls are fanned out across the cluster, but /tags and /tags/:tag are not.\nIn general for exploration purposes you will want to use the autoComplete functions rather than trying to get the entire tag list.. The first log entry you posed indicates that the cluster server is returning a 500 (Internal Error) response code, you'll need to debug that, which you can do by trying to load the referenced url http://0.0.0.0:8888/render/?format=pickle&local=1&noCache=1&from=1538069856&until=1538070756&target=stg.mon.UP.rg.mon01.services.kafka.kafka_server.total_lag&now=1538070756 and checking the logs on that host.. That is a good question, I have a trivial patch to make countSeries \"work\" for empty result sets, but a drawback is that it doesn't have an easy way to tell what the output alias should be, due to the internal implementation of series lists in graphite functions not keeping track of the pathExpression for a given list.  It should be possible to fix that problem by implementing a custom SeriesList type based on list (similar to the existing TimeSeries type) so that it could carry pathExpression and thus support proper aliasing when empty.. @deniszh that won't help if the seriesList is empty, but it could be done with something like:\nfallbackSeries(countSeries(currentAbove(servers.*.status, 1.1)), constantLine(0)). Hi Joseph,\n\nFirst, big thanks to @DanCech for the nice webinar yesterday, was really helpful. I was the guy asking questions here and there :)\n\nThanks for the kind words \ud83d\ude03 \n\n2. `1.1.3` The Singlstat throw `no value` BUT it doesn't show the Internal Server Error. But of course putting this in a an actual graph returns no data points\n\n\nYep, that makes sense.\n\nWe don't care about the 0.9.13 as we are migrating from it. We also trying to get historical data of those counts, I usually do them in a combination of scale(0) and offset(1) count() functions.\n\nAlso makes sense.\n\nI will be testing the fallbackSeries in the next few minutes and get back to you with results.\n\nCool.. To make this work over time, I would use a different approach:\ntransformNull(aggregate(removeAboveValue(removeBelowValue(servers.*.status, 2), 2), 'count'))\nThat is going to take each of your series, and for each one replace any values less than 2 or more than 2 with null, then for each interval it'll count the remaining non-null values (the series that had value=2 for that interval), then it'll transform the result for any intervals that didn't have any non-null values from null to 0.. That may be due to a flake8 update, I'll check it out. Can you add documentation for minValue and maxValue to the docblock?. I'm not sure that this is correct, the traceback in the linked issue isn't super helpful and doesn't include the actual error being thrown.  This patch assumes that os.path.realpath() is returning a utf8-encoded binary string, and decoding it into a python text type.  There is a lot going on there and the existing code dates back to #552 which was also trying to work around an encoding issue.\nReading the rrdtool module https://github.com/commx/python-rrdtool/blob/master/rrdtoolmodule.c it seems that it expects an str under both python 2.x and 3.x.  As these are different types, I suspect that we actually need to handle 2.x and 3.x differently.\nUnder 2.x I think the current behavior is correct, under 3.x we would want to decode the path from bytes to str, most likely using sys.getfilesystemencoding() rather than \"utf-8\".\nI haven't tested that, and of course I could be totally wrong.  In any case I this seems like something we need a test case for so we can be sure we're not breaking things.. Ok, you may want to explore using MySQL or Postgres instead of the sqlite graphite.db.  Another thing you can do is to increase the TAG_UPDATE_INTERVAL setting in carbon to reduce how often it sends updates to the TagDB, I'd suggest increasing it to 1000 and seeing whether that makes a difference.. Did you try setting the SECRET_KEY and running the migrations as instructed in the log output above?. Yes, that is absolutely fair use, we're always happy for people to talk about how they use Graphite \ud83d\udc4d . In 1.1.x the pipe symbol is now part of the query syntax, so to use it in series names it has to be escaped.\nhttps://graphite.readthedocs.io/en/latest/render_api.html#examples shows how it is used.. Grafana uses find queries to resolve variables, so it can't use aliasSub unfortunately.  The likely solution will be to add support in Grafana for escaping the pipe character, or to use a different separator in the series names.. Right now the spec is overly permissive, which is partly how we got into this situation. I commonly use underscore or dash in this situation as those are definitely safe.. I believe that the reason timeRange is used is so that all series get consolidated to the same interval, rather than series that are only active for part of the query time range being returned at a higher resolution.. > > I believe that the reason timeRange is used is so that all series get consolidated to the same interval, rather than series that are only active for part of the query time range being returned at a higher resolution.\n\nHow does that work out if data has series with different step?\n\nThe step of the underlying series will just impact how many raw points go into each aggregated interval.\n\n\nAnyway, I'm not sure how to fix this in a normal way. We don't use a lot of aggregation functions, so probably we can't test it well\n\nI think the real issue is that sometimes the number of datapoints in a serie does not match the expected number (timerange/step). I've run into this before and couldn't find where it is caused. I guess this happens somewhere deeper in whisper or in the code that merges whisper and carbonlink data. Not sure..\n@rickrian does it show only one datapoint for series which would otherwise not work at all or for all series (which did work before the patch)?\n\nI suspect it could happen if you have a series that returned points for only part of the interval.  If you look at lines 197-199 it's trying to figure out the total time range covered by all of the series and setting that as timeRange.\n. I don't think this is a good idea as it stands, because of the change it introduces in how the final step is calculated.. I've been thinking along the same lines @deniszh and had some good discussions with @Dieterbe, I just wish I had more time to dedicate to the project.  The biggest thing I'd add to your list would be focusing on ease of installation and sensible defaults, I know @piotr1212 has been doing some work in that direction and ditching django would also help in that area.\n@lomik I'd love to hear your thoughts/suggestions/reaction to the idea of adopting go-carbon as the official daemon. Good catch.  I'm thinking that integer is actually not accurate either for a lot of these params, and that it should actually be float.. Is your carbon-cache able to flush data points to disk?  CarbonLink is responsible for returning points that have not yet been flushed to disk, so if that number is going up then it would indicate that there is a problem preventing carbon-cache from updating the whisper files.. Ok, so then you are seeing what you'd expect, carbonlink returning the points for the requested series that haven't been flushed to disk yet.. Because the carbonlink protocol is quite simple, graphite-web asks carbon-cache for all the datapoints it hasn't flushed to disk yet for series X, and it returns them.  You're artificially creating a scenario that in production would indicate that there is a problem with your carbon-cache configuration, and in any case the delta on returning 5 or 50 points is going to be lost in the noise of tcp overhead and round-trip latency.. Is there any harm in always recommending users specify --fake-initial, ie having it in the example command?. You should check out https://graphite.readthedocs.io/en/latest/functions.html#graphite.render.functions.asPercent \nBy passing the nodes you want to aggregate on it will handle the situation where there are series missing.. that is interesting, and somewhat weird.  I'll update this patch to align them all to be that way. this isn't needed. 'headers' should be added here. It's needed to support readers that don't accept the new parameters, we could go through and update all the readers and tests to accept them, but that would still leave people with 3rd-party readers needing to update them for no good reason.. That was the latest version at the time, we can certainly test with 1.20. I agree, I've used https://docs.openstack.org/developer/pbr/ in the past with good success.  That's outside the scope of this PR though.. Again outside the scope of this PR. It's used for both local and remote, if we're happy with the functionality of the pool it can probably be removed since there's little reason to not want it.. Yeah, prefetchRemoteData could certainly be moved.. This whole function was a direct copy from 0.9.x, but I think the comments are valid.  We'll take a look at it.. It looks like the moving functions share 99% of their code, differing only in the aggregation function applied (safeMin, safeMax, etc) and the name of the new series, so at first glance it seems like it should be possible to create a helper function that takes a seriesList, period, aggregation function and name template that all the moving fructions can use.. Shouldn't this be inside the loop?. This seems like it's going to trigger find requests to all the remote hosts.  Avoiding those extra requests when using prefetch was part of the optimization we did to improve performance in 1.x. We want to wait for these results in the main loop, so that they don't block any local finds and fetches from being kicked off. Should we have ,<1.11.99 here and in requirements.txt in case of breaking changes in future releases?. These will allow the django19 env to pull in pre-release versions of 1.10 etc, we want to keep the same requirement pattern as we had before.. Can we add ,<1.11.99 here to match setup.py?. Since we're refactoring this, let's promote extractPathExpression to a standalone function extractPathExpression(target) and have it parse the target into tokens internally rather than doing that in extractPathExpressions. isn't this change going to mean that an empty result will always overwrite the seriesList entry down in the else clause?. Might be worth profiling this against:\nfor node in matching_nodes:\n  result_queue.append((node.path, node.fetch(startTime, endTime, now, requestContext))). We should be careful here, since when we pull the data back out we use the explicit prefetched = requestContext['prefetched'].get((startTime, endTime, now), None), if timebounds() is updated to return any additional items then the lookup will fail.  This should likely be requestContext['prefetched'][(startTime, endTime, now)] = PrefetchedData(results). This does use the thread pool, in RemoteReader._fetch_list_locked(). I'm not 100% clear on what this is doing, it seems like if we're using prefetch but the remote nodes didn't return any results then it's going to try to query them again.. Nope, I was looking at line 222, you're right.. Fair enough, it's definitely more involved than I realized with the way it handles pathExpressions. Looking good, let's just move this inside the if to avoid doing the extra work if we don't need it.. Yeah, I left it in there to be consistent with the code in list_tags, and on the off chance that there might be a tag with no values.. Yep, parseATTime supports timestamps. The local database implementation is really just a \"starter\" config I think, I envisioned redis (or something else once there are more options available) being the most-common configuration people  would use in production.. No, I removed = from the symbols list to allow it to occur in metric names, so it has to be re-added here to avoid breaking existing queries that pass it escaped with a backslash like \\= rather than as a bare =.. Performance, this is avoiding all the python overhead that the ORM brings.. Good catch, sqlite doesn't support regular expression matching at all, and pgsql requires a different syntax.  I'll update that.. Will do. I'm not sure that it's even possible to implement the logic in find_series_query using the ORM, because of the complex joins it uses, but I'm not a django ORM expert.  The same is true for the multi-value insert ignore statements used in tag_series, which afaik in the ORM would have to be implemented with separate queries for each tag and each value.   If you know the ORM I'd be keen to see how it could be implemented, but I'm much more confident in saying that the raw SQL will work as correctly and efficiently as I can make it.. sqlite actually does support REGEXP via a UDF provided by django, so we should be GTG now.  I haven't tested it on postgres yet, will do that as soon as I have a chance.. added. I'll add a function in TaggedSeries so it can be consistent here and in carbon, which is the part that is critical for everything to work properly.. Good call, will update. pipedArg is passed when evaluating the rightmost piped function, the value passed in will be the parse tree for the remaining part of the expression.  You can see on line 30 & 31 that the rightMost piped call is pop()ed off the stack and tokens is then passed as pipedArg when it's evaluated.  This mechanism is how the piped calls are \"converted\" back to a regular nested set of function calls.. No worries, it's definitely the most non-obvious part of the change.  I'll add some comments to explain what's going on there.. Comments added. All of them. If you want more fine-grained control over the xff for individual cases you can use the new function to set it exactly as needed. I think it's fine as is, as you note the docs themselves are pretty clear about what it affects.  If it bothers you I'd be happy to accept a PR.. It is linked on line 92. Same question, looks great otherwise. I understand what you're saying, but at the same time a hard failure is a pretty nasty thing.  I think that using the configured timezone is a pretty reasonable approach, and if we're worried about it then I can easily add a \"warning non-timezone aware datetime passed to epoch\" log entry there.. This is just an improvement to the efficiency of the tests, because coverage isn't going to collect info on files outside of graphite only to discard it when generating the reports.. This method centralizes the code used to make requests to remote hosts, and is also used by RemoteReader. Retry logic for remote fetches was moved here, which allows each remote host to be retried individually.. This change ensures that template arg values are scalars, to be consistent with documentation and match a bugfix in extractPathExpressions which didn't handle templates properly.. This function was moved here because it is so closely related to evaluateTokens. This change is what allows a finder module to create multiple finders, and is responsible for adding fetch and find_multi to legacy finders that don't extend BaseFinder.. This refactor simplifies use of @logtime, and makes it more extensible in future by adding functionality to the Timer class.. These variables are renamed to make them accessible for testing.. The pool now accepts and returns Job objects, which makes it easier to keep track of what's going on, and to deal with functions that raise exceptions.. pool_exec takes care of a few things that were previously duplicated in different places that used pool_apply.  It takes care of tracking timeouts (and adds timeout checking between jobs if the worker pool isn't used), it also allows a caller to just iterate over the result instead of having to deal with the queue directly.. This patch is needed to avoid the real prefetchData being called when patching fetchData in the test.. This is a bugfix, previously startTime was getting defined as a tuple because of the trailing comma.  It didn't directly affect these test but became obvious in test_prefetchData. This test just verifies the TagDB behavior when there is a cached result for a query.. good call, will update. re.match is implicitly anchored. yeah fixed that in the rebase. That is some testing code that got through, will clean it up when I get the auto-complete tests fleshed out.. It would get caught in tags/views.py and output as a json error. I have a few changes for the auto-complete stuff as I'm working on the tests for it, right now I'd really appreciate feedback on the render pipeline changes.. Afaik python 2.7+ supports io.StringIO, we shouldn't need the try/except.\nhttps://docs.python.org/2/library/io.html#io.StringIO. Can we use graphite.dashboard.send_graph here and skip the __future__ import?. consolidate to a single line. is_leaf is already defined in Node.__slots__. We can eliminate the try/except here since we don't need to support <2.7. consolidate to a single line. eliminate unneeded try/except. This whole branch can be eliminated, every supported python has cPickle. What is the second set of brackets for?. We should make the output consistent between the 2 versions instead, I'd be in favor of dropping the u prefix in py2 output. Instead of having these through the whole file, can we add a helper function to use throughout the file?\ndef json_dumps(obj, *args, **kwargs):\n  if sys.version_info[0] >= 3:\n    return json.dumps(obj, *args, **kwargs).encode('utf-8')\n  return json.dumps(obj, *args, **kwargs). See takluyver/graphite-web#1 for more on this, we should be able to handle this all in graphite.util.json. Interesting, definitely a case where I'd like to see a comment explaining what's going on. I think it makes sense to just use the absolute import and not bother with the future import since the tests will all fail for py3.x if someone tries to use an implicit relative import. Technically since the last 4 tests don't pass pretty=1 we should be expecting non-indented json (right now they pass because the indented json is only a single line).. They're broadly similar, but there are a lot of small differences (log messages, job parameters).  I could try to abstract it away into a helper but I'm not sure how complex it would get trying to handle the differences.. @takluyver I came up with this as an alternative to having to worry about passing strings to StringIO, does it make sense to you?. I see that here we don't have the same python version check as in carbon, we should make them consistent.  The best solution may be to use big_hash = int(fnv32a(six.binary_type(key))) for both 2.x and 3.x.. Actually that won't work because on 3.x bytes required an encoding parameter.  Likely the simplest solution is to use the same arrangement as in carbon.. I realized that the carbon approach doesn't properly handle multibyte characters either, if #2168 makes sense to you @iain-buclaw-sociomantic @deniszh then I'll port it over to carbon also.. This is a port of graphite-project/carbon#196. missing space?. was this throwing an exception?. Aha, fair enough. return None. return None. return True. return series or something like raise Exception('Tag support disabled'). This comment doesn't make sense any more.. This doesn't seem like it would actually change the behavior of the function. Ah ok, I didn't realize that was a problem on 3.x. It's fine, not sure why this file had the trailing whitespace but it isn't needed (or desired).. where is result['values'] coming from? assuming this should be values. this line should be removed. This may be a problem, as pathExpression is not always going to be the same as name.  We may want to expand the json format to include it with a fallback like we do for the other formats (adding step, start & end wouldn't hurt either).. pathExpression is the actual query that the series matched, it's used as the key to get results back out of the requestContext['prefetched'] data structure.. It should, but as far as I can tell it doesn't. suggestion\n        # Only Python 2 'unicode' needs to be converted to str/bytes. ",
    "banzayats": "Are there any news about scatter plotting in graphite?\nMayby some external dashbords can do this?\nI want to draw a graph: \n\nlike this:\n\nCan it be done in graphite or maybe some external dashboards can do this?\n. There is no validMetricChars in grammar.py, but webapp still doesn't work with utf8 characters. \nAny news on this issue?\n. Maybe someone can help me implement double seasonal forecast? This is usefull for data with multiple periodicity (for example: 1 day and 1 week). Thus, we can get the confidence bands for 2 different seasonalities.\nYou can read about double seasonality forcasts here: http://www.hindawi.com/journals/jcnc/2012/192913/\n. ",
    "tjsousa": "+1\n. +1! Saw the same error today while testing 0.9.12 on our staging data. 0.9.10 worked, though.\n. > I saw this lately too. Worse, it seems to happen for keys that do exist if the first datapoint is None.\n@davcamer This would be really useful! \n. Hi guys! We make use of keepLastValue to identify status changes (graph rendered by client-side Javascript lib), which clearly shows this effect at the begining.\nIt would be great if a workaround could be found for this. \n\n. ",
    "omarmarquez": "+1\n. ",
    "jgpaiva": "+1\n. +1\n. ",
    "punnie": "+1\n. +1\n. ",
    "mathias-baumann-sociomantic": "+1 from me for this feature\n. No, it works as is :)\n. We discovered a bug in this pull request and are on it to fix it, don't merge it yet.\n. Taking the example of the documentation:\n```\nreduce(map(servers..disk.,1),3,\"asPercent\",\"bytes_used\",\"total_bytes\") =>\nasPercent(servers.server1.disk.bytes_used,servers.server1.disk.total_bytes),\nasPercent(servers.server2.disk.bytes_used,servers.server2.disk.total_bytes),\n...\nasPercent(servers.serverN.disk.bytes_used,servers.serverN.disk.total_bytes)\n\n```\nAs far as I understand it, this will take the series-list given by map and calls \"asPercent\" for each series, giving it as parameters .bytes_used and .total_bytes from the common beginning that was given to map.\nI am yet not sure how exactly map works though.\n. ",
    "leandro-lucarella-sociomantic": "\nDoes this rely on any other PRs?\n\nNope :)\n. Sorry, I didn't have the take to get into this again (I'm not regularly working with this so I will have to dive into graphite again to rebase and add tests, so it might not happen very soon :-/ ).\n. I worked on this without realising it was already addressed by #416 and #447. Nevertheless, I think the solutions presented in this PR are a bit more clean and simple (compared to #416 this plays better with the current style and compared to #447 it just extends divideSeries instead of creating a new function).\n. OK, the problem is fixed, but I'm still not sure if the normalization is correct in the case multiple series of totals are used. If you can test against something you know how it is supposed to look, it would be appreciated. Any other testing for this pull request is very welcome too.\nI would say to still wait a bit before merging this...\n. Anyone interested in reviewing this? I would happily fix the conflicts if this have any chance to be merged. Thanks!\n. I'll have a look at it when I have some time. Thanks for the feedback!\n. ",
    "corradio": "Status here?\n. Is there another workaround right now?\n. ",
    "jemshad": "looking forward to this feature! :)\n. ",
    "eyablonowitz": "Thanks @obfuscurity, asPercent works great.  But the original issue description and subsequent comments also included divideSeries, but it looks like only asPercent was actually addressed?  I looked but failed to find a separate issue for divideSeries.  Would it make sense to create one or reopen this?\n. @obfuscurity I am trying to graph I/O latency from stats fed to Graphite by a Sensu check reading /proc/diskstats (https://github.com/sensu-plugins/sensu-plugins-disk-checks/blob/master/bin/metrics-disk.rb).  \nI am trying to divide the time spent doing I/O over the sample period by the number of I/O operations during the same time period.  This works ok if I am graphing a single host, but fails when using variables/wildcards to attempt to graph multiple hosts.  This same pattern used to fail in a similar way with asPercent prior to .10 but now works great (e.g. for graphing CPU utilization %).  \nBelow is the targets array from the Grafana panel json to further illustrate what I am trying to do.  Apologies if I am missing an obvious solution.  I am new to Graphite.\nEdit - After typing this up a colleague made the suggestion that I could just use the asPercent function and scale it by 0.01 which works fine.\n\"targets\": [\n    {\n      \"target\": \"nonNegativeDerivative(linux.io.$cluster.$host.*.ioTime)\",\n      \"hide\": true\n    },\n    {\n      \"target\": \"groupByNodes(#A, 'sum', 2,3)\",\n      \"textEditor\": true,\n      \"hide\": false\n    },\n    {\n      \"target\": \"nonNegativeDerivative(linux.io.$cluster.$host.*.{reads,writes})\",\n      \"hide\": true\n    },\n    {\n      \"target\": \"groupByNodes(#C, 'sum', 2,3)\",\n      \"textEditor\": true,\n      \"hide\": false\n    },\n    {\n      \"target\": \"divideSeries(#B,#D)\",\n      \"hide\": false,\n      \"textEditor\": true\n    }\n  ],\n. ",
    "robinsmidsrod": "Shouldn't this issue technically be on the carbon project?\n. ",
    "tabletcorry": "This was fixed in 7f8d9fd9.\nI would close it, but the migration seems to have made that impossible.\n. ",
    "amalloy": "I have the same problem. Would love to see a version that only uses cairo on a single thread.\n. ",
    "aaronfc": "This is also happening to me. Could the NoneCheck be added?\n. I re-opened the pull-request and it got automatically to \"Merged\" status.\n. I was also surprised by the automatic merge, thought it was already fixed by a previous commit as this commit was relatively old and I re-opened it.\n. ",
    "akafred": "Putting a transformNull() on the series before applying holtWintersAberration() is a workaround.\n. ",
    "nox": "Ping?\n. @aaronfc Oh right. When is planned the next release? I don't see that fix in any tag. Could a new version be properly released?\n. Could it be?\n. So, still no new release? No new 0.9.x either with the new backports\u2026?\n. That's not an ETA, is it? The holtWintersAberration bug have been reported for ages and makes Graphite mostly unusable for forecasts. Why should a release including it be stalled for bugs reported a couple of weeks ago?\n. ",
    "esc": "@nox planning for the next release is here:\nhttps://github.com/graphite-project/graphite-web/issues/677\n. What needs to be done here? Spin up a vanilla box and ensure all the instructions still work as advertised?\n. This might be related: using relative references into the future might also be bust:\n&until=+2d\nLeads to:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/django/core/handlers/base.py\", line 115, in get_response\n    response = callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 48, in renderView\n    (graphOptions, requestOptions) = parseOptions(request)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 287, in parseOptions\n    untilTime = parseATTime(queryParams['until'], tzinfo)\n  File \"/opt/graphite/webapp/graphite/render/attime.py\", line 43, in parseATTime\n    return tzinfo.localize(parseTimeReference(ref), daylight) + parseTimeOffset(offset)\n  File \"/opt/graphite/webapp/graphite/render/attime.py\", line 117, in parseTimeReference\n    raise Exception(\"Unknown day reference\")\nException: Unknown day reference\n. @fjorgemota yeah, I think that is what I ended up doing (using %2B instead of +.\n. What is the state of this, it still has the 0.9.13 milestone, does it need to go in?\n. @brutasse in that case, what is your recommendation to proceed with this?\n. fuuuuuuuuuuuuuuu\n. @steve-dave oh cool, can I close this one then?\n. @brutasse do you concur?\n. Closing as per advice from @brutasse on IRC, thanks!\n. FWIW: we do have a test suite in-place, since:\nhttps://github.com/graphite-project/graphite-web/pull/575\nWould it be feasible to add tests for this PR using that setup? Regarding the unit tests for the graph drawing, perhaps you could use mock to check that correct drawing commands are called?\n. @brutasse what is the status of this? It seems like it isn't ready yet.\n. @brutasse why will this not be needed for 0.9.x? Is it too much of a 'feature'?\n. The initial contribution is over a year old. Does anyone actually need this to be in 0.9.13?\n. Closing this as wontfix and looking forward to an implementation in master.\n. @jhwbz has the question been answered, can this be closed?\n. Does this apply to 0.9.x?\n. @steve-dave @ajablonski what needs to be finalized for this to move forward?\n. Quite a lot of copy and paste, don't you think?\n. #522 was a duplicate of this one. Currently sortByName exists twice in master:\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/functions.py#L1703\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/functions.py#L1768\nAlso, it is defined twice in the SeriesFunctions:\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/functions.py#L3097\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/functions.py#L3102\nSuggest reverting this.\n. Revert avail. in #663\n. What symptoms were you experiencing? Stacktraces returned over HTTP? I am asking because we may have encountered a similar issue.\n. I can't reproduce this using a40794603aa4165d8124c8bb0749acf6c95d37d0, which is the master from Tue 18th Mar 2014. In my simple tests both alias and constantLine works as expected.\n. Ping.\n. Ping.\n. @SEJeff @marco-hoyer will rebase against master and merge\n. The equivalent of this is already in current master, was introduced with a888a4990d4 by someone with the following credentials:\ngraphite graphite@lead.flightlookup.com\n. @SEJeff will rebase to current master, adress issues and then merge.\n. merged with #559 so closing\n. Not sure what happend here, seems a like rebase that went pear-shaped\n. I think the two important ones are in pull-requests #442 and #441 which correspond to 063a476 and  54e286a. Hence closing this. Please reopen if I have missed something.\n. i.e. closing w/o merge.\n. +1\n. I should probably note, that this fixes the fact that graphite someties spews out python tracebacks over HTTP and one sees that instead of a graph.\n. Ping.\n. Sure, I'll rebase and merge on monday.\n. did someone fix the whitespace?\n. Rebased version on #584 \n. How about statsmodels + pandas for a more native solution?\n. Or am I doing it wrong?\n. I also tried using the instructions from:\nhttps://docs.djangoproject.com/en/dev/topics/testing/overview/\n```\nzsh\u00bb python manage.py test\nCould not import graphite.local_settings, using defaults!\n/data/home/vhaenel/git-working/graphite-web/webapp/graphite/settings.py:217: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security\n  warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\nCreating test database for alias 'default'...\n...............................................................F.................................................................F................s...........................................................................................................................x...................................................................................................................................\n======================================================================\nFAIL: testLoginRequired (django.contrib.auth.tests.decorators.LoginRequiredTestCase)\n\nTraceback (most recent call last):\n  File \"/data/home/vhaenel/virtualenv/graphite/local/lib/python2.7/site-packages/django/contrib/auth/tests/decorators.py\", line 37, in testLoginRequired\n    self.assertTrue(login_url in response['Location'])\nAssertionError: False is not true\n======================================================================\nFAIL: test_unknown_user (django.contrib.auth.tests.remote_user.RemoteUserNoCreateTest)\n\nTraceback (most recent call last):\n  File \"/data/home/vhaenel/virtualenv/graphite/local/lib/python2.7/site-packages/django/contrib/auth/tests/remote_user.py\", line 145, in test_unknown_user\n    self.assertTrue(response.context['user'].is_anonymous())\nAssertionError: False is not true\n\nRan 402 tests in 4.264s\nFAILED (failures=2, skipped=1, expected failures=1)\nDestroying test database for alias 'default'...\n```\nAnd this looks fine at first sight, but when I add the following patch to check if the tests do actually run:\n```\ndiff --git i/webapp/graphite/render/functions_test.py w/webapp/graphite/render/functions_test.py\nindex f57bda1371..8a054e271b 100644\n--- i/webapp/graphite/render/functions_test.py\n+++ w/webapp/graphite/render/functions_test.py\n@@ -19,6 +19,9 @@ import graphite.render.functions as functions\nclass FunctionsTest(unittest.TestCase):\n\ndef testFaile(self):\nself.assertFalse(True)\n+\n     def testHighestMax(self):\n         config = [ 20, 50, 30, 40 ]\n         seriesList = [range(max_val) for max_val in config]\n```\n\nThe failure is not reported, inidicating this does not actually run the tests...\n. I'll close this one myself now, since by now I have figured out how to run them.\nFYI:\n$ cd webapp\n$ python manage.py test --settings=tests.settings\n[...]\n. Ping. Can this be merged?\n. Thanks for merge.\n. http://martinfowler.com/bliki/TwoHardThings.html\n. I just had another look at the whole caching code and that ought to be flagged for refactor, gives me a headache every time.\n. Can you test this? IIRC functions are tested quite well.\n. @steve-dave, no idea TBH. I looked at the graph and there were no merges 8 days ago. My guess is that the PR was re-opened 8 days ago and then closed automatically, since it had already been merged.\n. @aaronfc yeah, it seems to be as I wrote above. BTW: feel free to ignore or implement my comment about tests ;)\n. @brutasse this was written before the unit testing you did became available, IIRC. My plan is to rebase onto current master and integrate the changes as you suggested.\n. @brutasse this has been rebased. I cleaned up a little and also removed the tox config. Travis is green too. Think it is ready to be merged?\n. Also, I should mention, that this does not require any changes to the config.\n. @brutasse the release notes thing is probably a good idea.\n. @brutasse the changelog entry has been added.\n. If there are no further objections, I'll merge this tomorrow.\nThis would then also be a stable basis for  the comments suggested in:\nhttps://github.com/graphite-project/graphite-web/pull/464\n. A loose definition of 'tomorrow'\n. @steveakers we started looking at simple linear regression, for example to predict filling drives. I don't really understand LOWESS yet, could you give some intuition about the algorithm and potential use-cases for the types of data (i.e. monitoring data) that might commonly be stored in graphite?\n. @avnivamsikrishna does this still apply? Did you end up finding a fix for it?\n. @avnivamsikrishna we are currently working on a 0.9.13 release of graphite-web and this issue is tagges with the 0.9.13 milestone. Do you think it should be included?\n. Can someone fix this up and submit as a pull-request.\n. @steve-dave that would be +:100:\n. can be close when #784 is merged\n. I'll give it a shot.\n. Backport for 0.9.x available in #575\n. This is great. But how do you actually run the tests?\n. The functions.py have tests, IIRC, could you perhaps add some?\n. It is a bit unconventional but you could make an environment variable GRAPHITE_INSTALL_DIR which you could use to override the installation directory in setup.py.\n. yes, but could we move it into the setup.py?\n. Is there anything that should go on top of this?\n. @SEJeff thanks for fixing up the import order.\n. We can close this one soon, then I take it.\n. #764 fixes this for 0.9.x and #765 for master\n. Thanks for everyone who worked on this one!\n. I just had a look at this, after installing libcffi-dev locally, I managed to execute tox.\nI like the use of tox, because it allows us to check python2.6 compatability BEFORE pushing and testing via Travis-CI.\nWhat I don't like about this PR, is that it \"munges\" several features:\n- adding tox support\n- development instructions\n- fixes in setup.py to alter the installation location (TOX needs this?)\n- Various compatability fixes for Django\n- Fixing up deprecation warnings\nI don't think this is too drastic though, so I would be in favour of merging this.\n. Since three people already upvoted this PR, I'll go ahead and merge it. @brutasse do you think it should be mentioned in changelog?\n. rebased version of #476\n. The diff of the rebased version looks somewhat different, would like to run sanity tests.\n. We deployed this to our testing cluster today and it seems to be working fine.\n. This patch has survived the night, it can be considered as \"ready\"\n. @SEJeff thanks for the merge. Feeling better now, almost ready to go back to work!\n. I reset the dependencies in the requirements.txt:\nhttps://github.com/graphite-project/graphite-web/pull/749\nWe can close this if #749 gets merged, I suppose.\n. #749 was merged, closing this.\n. This should probably be fixed as and when some kind of mechanism for specifying the install path becomes available.\n. @lakshmi-kannan it would perhaps depend on how your base_path is configured.\nThe config example here:\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/local_settings.py.example#L79\nSuggests to write WHISPER_DIR w/o a trailing slash.\nWhile here, in settings.py we can see it with a trailing slash.\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/settings.py#L164\nI think that simply deploying with this patch, might cause some regressions for people who configured their Graphite using the example settings, i.e. WHISPER_DIR w/o slash.\nOne solution might be, to check the last character of the base_path and use +1 on the length  if it is a slash (or better yet, the standard separator for the target platform) and just the length if it is not.\n. I can confirm that this would break our deployment and prefix all of our metrics with an additional ..\n. can you like the commit, just for the record.\n. @brutasse thank you, my bad.\n. Yeah, I can reproduce this using a40794603aa4165d8124c8bb0749acf6c95d37d0\n. I'll add this to the 0.10 milestone.\n. I chose this path, because this is what is done here:\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/metrics/views.py#L56\nBut maybe relpath is better.\n. Fixed it as suggested.\n. @brutasse thanks for the feedback, merging now.\n. Will merge this as it seems pretty trivial.\n. Merci!\n. How about some tests?\n. Also, regarding the issue you have with Grafana, would noCache help?\n. :beers: indeed\n. I would prefer to prep and cut the release from master. Release branches are useful if you want to continue development during a release time. Also, I think it is simpler to just use master, no need to backport or forwardport.\nRegarding the project url, I have no ownership over th organization. I think it needs to be someone who does have that. @obfuscurity would you do use the honours? :)\n. @obfuscurity I agree that as many glaring things should be applied as possible. However, there are really a ton of PRs and Issues and sometimes it is better to release (ship) something that is adequate, rather than wait until it is perfect.\nFor sorting issues, I would suggest to use the 0.10 milestone to flag anything that should absolutely go into 0.10:\nhttps://github.com/graphite-project/graphite-web/issues?milestone=2&page=1&state=open\nSo currently there are 8 issues remaining. Does that mean: a) there are only 8 left to go? or b) that we still need to sift through the remaining 278 issues and 72 PRs?\nIn the Numpy project the releases usually start with a request on the mailinglist, for anyone to speak up about issues that they thing should absolutely be resolved before the next release. This is nice, because it doesn't require the maintainers to sift through each and every one of the issues, which in the case of graphite(-web) is quite a daunting task.\nSo, perhaps we can find a common ground where we say, the following issues will be fixed within a pre-defined time-period that we can commit to, and everything else will be sorted later. Also, for any major issues, security, grave functionality impediments, we can always push maintenance releases of the form 0.10.X.\n. I gather that it isn't clear if we should release a 0.9.x maintenance release or/and a 0.10.x release. We have been using the current master (plus some additional feature branches) in production since April w/o any issues. What are you guys using in production?\nThe problem with having a 0.9.x and a 0.10.x simultaneously that it is a bit of a split brain situation, and having to supply two branches with fixes and updates is somewhat of a maintenance nightmare.\nAs you can see here:\nzsh\u00bb git log --oneline master..0.9.x | wc -l\n200\nzsh\u00bb git log --oneline 0.9.x..master | wc -l\n641\nthe two branches have diverged quite a\nlot. Note however that this does not take into account any commits that have been cherry picked.\nIf we do decide to to go for a 0.9.x, for which I do sense some momentum, we should use the 0.9.13 milestone, for which there are already some issues:\nhttps://github.com/graphite-project/graphite-web/issues?milestone=4&page=1&state=open\n. BTW: tests for 0.9.x are currently passing:\nhttps://travis-ci.org/graphite-project/graphite-web/builds/19817617\nAlso, we should favor bugfixes only for 0.9.x as @steve-dave suggested and additionally reject anything that isn't tested in one way or another.\n. I agree that cleaning up the repo structure in the long run is something to aim at. Also having a nice a website under a good domain-name is something to aspire to. However, given the recent dilemma we are in, I would like to focus on getting a release out the door that fulfills the minimum requirements -- something like a minimum viable release.\nFrom what I gather here, maybe a 0.9.13 release would be a good interim solution, but I still don't have an idea what you guys use in production. Anyway, to move towards that release, we will only merge bug-fixes that have tests.\n@steve-dave you said you had some pending pull requests? I would like to take up your offer of looking into those for the 0.9.x release. Perhaps you could point the people with appropriate privileges that are reading this thread: (@esc, @obfuscurity, @SEJeff, @drawks. others?) to any bugfixes you would like to have included. Of course this also goes for anyone else reading this thread, who has commits that they feel need to be applied to 0.9.x\nFrom a maintainers / release managers point of view, here is a small checklist for evaluating if a commit is suitable:\n- [] can the commit be merged / cherry-picked cleanly to 0.9.x?\n- [] is it a bug-fix?\n- [] does it have tests?\n- [] releationship to master? Is it backported from master? Does it need to be applied there?\nIf you encounter a commit which you can not deal with immediately, please set the 0.9.13 milestone, to make sure we have a list of things that need to be done:\nhttps://github.com/graphite-project/graphite-web/issues?milestone=4&page=1&state=open\nAlso, again, what do you guys use in production?\nAlso does anyone feel like we need, or would prefer to have a mailinglist for this project. I sometimes miss being able to answer in-line when discussing with github issues?\n. No problem, by highlighting, i meant, just writing the name, e.g. @esc in the issue. That way you will draw mine (our) attention to this pull-request so that it can be considered.\nRegarding whisper and carbon, I realized earlier today, that the current 0.9.x branch of graphite-web actually needs a whisper from its master. This would indicate that maybe new releases of both projects would also be required.\n. @steve-dave not sure yet who or how whisper and carbon will be handled.\n. No, as long as we keep momentum, it will be ready when it is ready. Importantly, motion towards the release should not stagnate.\n. @toni-moreno thanks, your help is much appreciated. We are currently focusing on a 0.9.x release and as such we need to handle all the issues in:\nhttps://github.com/graphite-project/graphite-web/issues?milestone=4&page=1&state=open\nAnd potentially tag more with the 0.9.13 milestone so that we can track open things better.\nWe have pretty much agreed that we will only add bugfixes -- no new features -- to the 0.9.13 release and that any bug fixes really should include some tests. \nIf you would like to help out, take a look at the issues in the 0.9.13 milestone and see if you can help out with any of those. Also, if there are any issues or pull-requests you see, that should be part of the 0.9.13, you can highlight me using @esc and I'll add them to the milestone.\nI am hoping that once the 0.9.13 release is out -- which should hopefully be the last one in the 0.9.x series -- we as a community can focus on moving forward, and get 0.10  released.\n. FYI: I am also available on freenode as esc in case you need me in (semi-)realtime.\n. @toni-moreno we use current master in production too, with a number of patches, of course. Regarding the 0.10 release, please be patient, we are quite understaffed as a community, but I hope that will get around to it soon.\n. I have updated the 0.9.13 to be due by Jul 11 2014, it's just an arbitrary date in the future, if we make an rc1 by then, good, if not, we can always extend it.\n. Adding to 0.9.13 milestone to track the release, cc @obfuscurity \n. @obfuscurity :dancer: \n. Hey all, I have been away from the machine for about four weeks and have just returned. If you are still awaiting a response of mine please ping me in the issue/pull-request.\n. Aweseome!\n. :sparkle: :sparkle: :sparkle:\n. Let us know about any tweets you send out, would love to retweet this!\n. A 0.9.13-pre was released last december:\nhttps://pypi.python.org/pypi/graphite-web/0.9.13\nThis issue should have been closed then, so closing now.\n@timbunce and @frejsoya please try the latest available release from PyPi.\n. Nice!\nOn 8 November 2015 03:43:36 CET, Jason Dixon notifications@github.com wrote:\n\nClosed #677.\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/issues/677#event-458046628\n\n\nSent from my Android device with K-9 Mail. Please excuse my brevity.\n. This looks sane.\n. @brutasse close as wontfix?\n. @brutasse sounds good to me, @dene14 could you take a look?\n. The docstring of the mostDeviant seems incorrect too:\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/functions.py#L1804\nWould that need to be fixed also?\n. We had recently agreed that this should be:\nhttp://graphite.readthedocs.org\nI'll fix 0.9.x and master, but add this issue to the 0.9.13 milestone. I guess it can be closed when 0.9.13 is released because then, the URL on PyPi will be fixed.\n. Fixed in master with #762 \n. Fixed in 0.9.x with #763\n. To be closed when 0.9.13 is released.\n. It's not fixed on PyPi yet, so someone might hit this again. But I guess the chance is quite low, so I don't mind closing this.\n. thanks.\n. Haven't used RRD but from a glance the code looks sane, but hard to tell w/o tests.\n. 0.9.13 milestone?\n. Yeah, PING!!! :grin: \n. Thanks for getting this one closed!\n. Do this need/have a backport?\n. This was recently fixed by:\nhttps://github.com/graphite-project/graphite-web/commit/c4653f7ac2c994d00aa2edee16051d8a71762652\nMaybe yourself and @bbrown want to discuss what the best wording might be? Closing this one, but feel free to submit a new pull-request with a tweaked description should you reach an agreement.\n. it is pointing at the 0.9.12 tag. Since we are currently prepping a release from the 0.9.x branch I deem this to be cruft and will delete it.\n. I have deleted it, thanks @steve-dave for pointing this out.\n. Can you test this somehow? Do you think it might break existing dashboards? I think the JSON interface may be used as a machine readable interface to graphite-web.\n. Waiting to hear from tarvis.\n. All tests passed, will merge.\n. This looks legit. Anyone to confirm?\n. @brutasse thanks\n. There is not enough information here to blindly apply the patch. Can you write a unit test to expose the problem?\n. @obfuscurity yeah, agreed, I'll tag it with 0.9.x so that we don't loose track of it for a potential 0.9.14.\n. @obfuscurity sure, I think I might have closed it accidentally\n. We recently patched fixed the corresponding whisper:\nhttps://github.com/graphite-project/graphite-web/issues/597\nSounds to me like this may need to be applied to 0.9.x? Also does this apply to master also?\n. None of the readers in master seem to have this applied? Do you have a stack trace, maybe?\n. @szibis whisper 0.9.x should be compatible with graphite-web 0.9.x.\n. I made a PR for this patch in #779, please review.\n. closing in favour of #782 \n. Bug for 0.9.x or feature for future release? I would say feature.\n. Sounds like a critical bug to be fixed in 0.9.13, adding to that milestone.\n. It 'is in progress':\nhttps://github.com/graphite-project/graphite-web/issues/677\nYou are welcome to help, if you like, the current list of TODOs is here:\nhttps://github.com/graphite-project/graphite-web/issues?milestone=4&state=open\n. I'll add this to the 0.9.13 milestone.\n. Is there a corresponding version for master?\n. Also, is it possible to test this in any way?\n. Anyone else to :+1: this?\n. Alright, here we go.\n. Same thing, eh?\n. @steve-dave thanks!\n. Thanks, adding to 0.9.13 milestone.\n. Can I ask about tests?\n. :dancer: \n. the other line is https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/hashing.py#L38\n. @rmca thanks, I'll patch everything together and make some clean PRs for master and 0.9.x\n. Closing in favour of:  #764\n. Clean version of #764 waiting on Travis, will then merge.\n. @rmca there was some trailing whitespace in the tests, but other than that, :bomb: away\n. @rmca no worries, graphite-web is still quite far away from pep8 compliance.\n. Closing as deemed unnecessary, please re-open if you feel the need. \n. Thanks, I have retroactively added this to the 0.9.13 milestone.\n. +1 for test coverage\n. Oups, sorry for close-reopen, hit the wrong button.\n. Also, can you explain what it does?\n. Ok, why not!\n. Adding to 0.9.13 milestone, to keep track of what was merged.\n. OK, i guess.\n. Corresponding PR for #754, tagging for 0.9.13.\n. Not sure if this should be applied to master too, RemoteNode doesn't exist there anymore?\n. closing in favour of #782 \n. something caused travis to have a hickup, restarting.\n. Okay, it was just a hickup, thanks @bbrown \n. Thanks for the review @steve-dave, much appreciated. I think this is better than the original patch suggestion. Is there any way to perhaps test this?\nAlso the first commit I made can probably be discarded.\n. I am fine with the dict. \n. @obfuscurity i think there have been several iterations of this issue:\nhttps://github.com/graphite-project/graphite-web/issues/754\nhttps://github.com/graphite-project/graphite-web/pull/779\nSome tests would be nice if possible. Even if you just use mock to do dependency injection to test the \"happy trail\".\n. Acknowledged!\n. added to the 0.9.13 milestone.\n. Where are the tests, yo! :smile: \n. Can you provide more details? Adding to 0.9.13 milestone for now.\n. anyone else, opinions on this?\n. The milestone is now a label. The milestone specifies the target version and the label (there is one for backport too) specifies that an issue or fix needs to be backported.\n. It is gone, closing this issue.\n. Yeah, only support what we test via travis.\n. Not sure, this seems like a feature not a bugfix for 0.9.x I would tend to not include this in 0.9.13.\n. +1\n. In case anyone is wondering, the two graphite-api commits need to be backported to fix the three issues.\n. cairocffi installs via pip no problem on debian stable with libcairo2 installed.\n. So yeah, what @brutasse suggested seems the correct thing to do.\n. Also the tox.ini has cairocffi as dependency\n. Probably affects both master and 0.9.x\n. 0.9.13 : #920\nmaster: #919\n. I guess the label should be needs forward port, but anyway, just trying to circumvent that we can only have one milestone per issue.\n. I guess #919 doesn't close this for 0.9.x...\n. But #920 does!\n. Bummer, seems there is no test coverage for this, hence travis thought it was good to go. :-/\n. Seems like wee need to tweak the imports, meh:\nhttp://pythonhosted.org/cairocffi/overview.html#importing\n. So, on master this is already fixed:\nhttps://github.com/graphite-project/graphite-web/blob/master/check-dependencies.py#L31\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/glyph.py#L16\n. @obfuscurity  please try #924 to see if it resolves the issue.\n. So, we used to have this issue also, at least when it comes to the last value being incorrect (None or not None, in fact), but it was happening during subsequent requests of the same metric for the same time interval. We eventually tracked it down to there being several wsgi workers each having their own local cache. Once this cache contained the null value, it was unable to fetch an updated value from disk/carbon-cache. IIRC we fixed this by using a memcached instance shared by the many wsgi workers. That at least made the results consistent and IIRC we ignore the last value anyway, e.g. for Icinga checks.\n. What should we do with this one? Is there an easy way to reproduce it?\n. Thanks man, much appreciated.\n. I agree with @obfuscurity , pluggable functions are a highly desirable mid-term goal, but 0.9.13 comes first.\n. Also, do we have automatic coverage measurement tied into the tests? I looked at https://graphite.readthedocs.org/en/latest/development.html but couldn't find anything.\n. @SEJeff sure, I have some experience with coverage.py i vaguely remember having used it with graphite manually at some point.\n. :+1: on pruning \n. Needs a backport, adding label.\n. Maybe:\n2008-2012, Chris Davis\n2011-2014, The Graphite Project\nSince the last commit was in 2012, if I am not mistaken.\n. I think either one is fine, as long as it changes from what we have currently.\n. Aye!\n. Fixed by https://github.com/graphite-project/graphite-web/pull/941 closing.\n. what about the other occurance of set vs. add?\n. Close in favour of #948\n. Close in favour of #947 \n. fixed by #952\n. Let's see what travis says.\n. Yay, travis worked!\n. alright, I'll merge.\n. Should probably have removed the whitelist_externals in the tox config.\n. Scrap that, the docs need it, at least for now.\n. I think the most recent versions of Django-Coverage don't work with older versions of Django anymore, perhaps.\n. rebased to run a single coverage target from tox\n. deniszh: how do I go about checking this?\n. ",
    "MichaelHierweck": "This breaks connecting webapp instances via stunnel, too.\n. ",
    "bryankennedy": "I'd second that. I just tried to setup Graphite with an Apache ProxyPass, but the web code doesn't seem to be able to make references relative to a subdirectory.\n. ",
    "blackrosezy": "+1\n. ",
    "nikicat": "+1\n. +1\n. Hi @Dieterbe, we have faced the problem with graph-explorer and sharded graphite too. This @legion-github's patch supplements this patch for graph-explorer to solve it.\n. +1\n. ",
    "adrianlzt": "+1\n. ",
    "rezib": "+1\nMay it require changes anywhere else than webapp/graphite/urls.py?\n. +1\nMay it require changes anywhere else than webapp/graphite/urls.py?\n. ",
    "dimovelev": "+1\n. I have the same problem (probably different version). Any ideas?\nThu Feb 13 14:29:20 2014 :: Exception Caught\nTraceback (most recent call last):\n  File \"/apps/pbgmon/graphite/webapp/graphite/render/datalib.py\", line 231, in fetchData\n    cachedResults = CarbonLink.query(dbFile.real_metric)\n  File \"/apps/pbgmon/graphite/webapp/graphite/render/datalib.py\", line 140, in query\n    results = self.send_request(request)\n  File \"/apps/pbgmon/graphite/webapp/graphite/render/datalib.py\", line 166, in send_request\n    result = self.recv_response(conn)\n  File \"/apps/pbgmon/graphite/webapp/graphite/render/datalib.py\", line 181, in recv_response\n    return unpickle.loads(body)\n  File \"/apps/pbgmon/graphite/webapp/graphite/util.py\", line 108, in loads\n    return pickle_obj.load()\n  File \"/apps/pbgmon/graphite/webapp/graphite/util.py\", line 97, in find_class\n    raise pickle.UnpicklingError('Attempting to unpickle unsafe module %s' % module)\nUnpicklingError: Attempting to unpickle unsafe module collections\n. ",
    "deltamualpha": "Not to pile on, but I would love to see this as well.\n. ",
    "yinchuan": "+1\n. ",
    "metzlar": "+1\n. ",
    "ajmath": "+1\n. ",
    "sstarcher": "+1\n. LGTM\n. ",
    "sseveran": "+1\n. ",
    "g76r": "using sortByName() as in #419 solves the issue\n. Likely to be fixed by #646\n. @piotr1212 \nI think offset is viable in most cases.\nSimple example: offset(my.metric,-1).\nOr event diffSeries and then offset: offset(diffSeries(my.{a,b}.metrics),-1)\n. Rationale: when graphing several series with only one target, e.g. target=aliasByNode(orders.server_.ok.count,1) the series are displayed in a random number, therefore the series orders and color will be different in a second graph using e.g. target=aliasByNode(orders.server_.ko.count,1).\nThis new functions make possible to graph ok and ko series in the same order (and with the same colors) on a dashboard with e.g. target=sortByName(aliasByNode(orders.server*.ok.count,1))\n. +1\nanother stack trace example:\n(...)\nevaluator.py, line 31, in evaluateTokens\n    return func(requestContext, _args, *_kwargs)\nTypeError: alias() takes exactly 3 arguments (1 given)\nI propose a pull request to fix this until de kwargs features is debuged: #643\n. I still experience the issue with a407946 and 0c210d3.\nMaybe it's a matter of Python or Django version. I use 2.6.4 and Django 1.4.10.\nI'm not a Python expert, maybe more recent versions of Python tolerate calling functions with extra arguments, or something like that.\n. Works also for me since I upgraded pyparsing to 1.5.7 (I cannot use 2.x since I'm still using Python 2.6)\n. For me the API works fine with current master (0c210d3). I didn't checked within graphlot.\nAlso, a coworker of mine made patches to make the API accept mixed syntaxes between from and until, see #644.\n. @obfuscurity: I didn't talk to @mleinart but I had a look in the code to change that behaviour by myself.\nIMHO it should be interesting to search the cache for series not yet present on disk, however it needs some changes since carbon cache store series in a hashtable, therefore resolving glob-expression will need another memory structure in addition to the hashtable (or, less likely, instead of the hashtable). This is probably why Graphite-web does not request carbon cache first.\n. I'm not sure but I think patch 0742236 I propose in #645 is slightly more secure because it casts step to int. However I'm not enough experienced with Python to be sure.\n. Newers comments in #429 suggest that it can be fixed w/o reverting the commit but rather by upgrading pyparsing version.\n. w/ test function\n. Is it ok for merging ?\n. I rewrote the ZeroDivisionError-when-only-target-is-constantLine fix because the one by a coworker of mine had performance issues (it generated series with 1 data point per second).\nNow I think it's ready for merging, however I'm not sure what you will prefer to merge or not among other PR and issues currently open and tagged \"constantLine\" by @obfuscurity. I've just read #641 and now that I've rewritten initial patch, my patch is almost the same.\n. Hi, is someone interested in reviewing and/or merging these changes ?\nBTW, happy new year :-)\n. Hi, is someone interested in reviewing and/or merging these changes ?\n. Hi, is someone interested in reviewing and/or merging these changes ?\n. Hi, is someone interested in reviewing and/or merging these changes ?\n. Hi, is someone interested in reviewing and/or merging these changes ?\n. rebased\nironicaly the conflict was probably due to #935 merge :-)\n. well... rebasing a 2 years old PR without fixing tests was not a good idea...\nfixed. sorry.\n. Hi, is someone interested in reviewing and/or merging these changes ?\n. thanks!\n. w/ test function\n. Is it ok for merging ?\n. Is it ok for merging ?\n. Is it ok for merging ?\n. I rebased against master to fix merge conflict.\nIs it ok for merging ?\n. Hi, is someone interested in reviewing and/or merging these changes ?\nBTW, happy new year :-)\n. Hi, is someone interested in reviewing and/or merging these changes ?\n. Hi, is someone interested in reviewing and/or merging these changes ?\n. Hi, is someone interested in reviewing and/or merging these changes ?\n. Hi, is someone interested in reviewing and/or merging these changes ?\n. @ everyone:\nWell, I didn't read the code for a long while and find it not clear enough too... I apologize.\nI will rewrite it.\n@deniszh Not sure what you call \"complexity\".\nIf you speak of the code: I will rewrite it soon.\nIf you speak of passing a consolidation hint all the line from whisper file to render lib: I think it's better to have this complexity than to leave the not-always-technical user alone and take time to teach him that he must add consolidateBy() methods on almost every target all over his dashboards although they can be guessed very often (e.g. when a user group series using maxSeries() or if a series is agregated on disk using 'max' function). I think it's better to add value than to leave the user alone. And yes dashboards with consolidateBy() all over the targets are complex too.\n@bmhatfield Ok about everything.\nI can add before/after screenshots but they will be exactly the same than when you use consolidateBy() on a metric or you don't.\n. Screenshot without the patch (nor explicit consolidateBy() in graphite target):\n\nScreenshot with the patch (or explicit consolidateBy() in graphite target):\n\nHave a look on the blue spikes on the left.\nThe example is choosen on purpose: in one case disk activity spikes are hidden when shorter than the duration of a pixel, and this is not what is expected by a normal human being for whisper series with aggregationMethod=max or for series aggregated using maxSeries().\nObviously the spikes would not be hidden if the graph was drawed with one pixel per data sample (or more). The issue only occurs when there are more than one sample per pixel (see http://graphite.readthedocs.io/en/latest/functions.html#graphite.render.functions.consolidateBy)\nRewritten code and reworded commits incoming (in a few hours probably).\n. I rewritten the whole patch taking into account @bmhatfield concerns and clarifying the code changes in the commit comment.\nHowever I not at ease about the right way to add tests (and BTW comply with codecov checks) and would need some help, for the following reasons:\n1. I cannot have the tests run successfuly on my dev environment and therefore need to push in the pull request branch to have the test run by travis (when I run PYTHONPATH=. python manage.py test --settings=tests.settings I've got a bunch of failures, about timezones, even on master branch)\n2. I don't think I'm python fluent enough to code the test case that would create at less 2 whisper files with different aggregationMethods and then generate to csv or json and parse returned csv or json to check expected values, and I don't find a test case I could copy/paste/modify for this purpose\n3. data passed through pickle accross several graphite instance seems hard to me to test, altough there are probably smart pythonic hacks to use pickle in test cases without requiring several processes (but I don't know about these hacks)\nI tested the change by hand on development and staging environments of mine, and it works fine. (\"by hand\" is far from CI test cases, of course). My in staging environment has 2 graphite instances on 2 servers and I checked that a graph can be drawn on one of the instance with data stored on the other one. The original (non-rewritten) commits are running on my production environment for about 2 years (again this is not a CI test case, but it make me fill rather confident).\n. Hello @JeanFred \n\nArf :-/ Aptitude is usually the best way to ensure compatibility with your system though :-/ I\u2019m not a\nDebian user (running Ubuntu here) though so unsure how much I can help. Maybe installing tox via pip?\n\nI'm clearly very intersted in the consistency of my system, therefore I won't mix package managers on my runtime used computer.\nI installed a brand new debian8 virtual machine, apt-get install python-pip then pip install tox.\nThe result is different but not better:\n```\nroot@debian:~/graphite-web# TOXENV=py27-django17-pyparsing2 tox\nGLOB sdist-make: /root/graphite-web/setup.py\npy27-django17-pyparsing2 create: /root/graphite-web/.tox/py27-django17-pyparsing2\npy27-django17-pyparsing2 installdeps: coverage, cairocffi, django-tagging, pytz, mock, git+git://github.com/graphite-project/whisper.git#egg=whisper, git+git://github.com/graphite-project/ceres.git#egg=ceres, pyparsing, Django>=1.7,<1.8\nERROR: invocation failed (exit code 1), logfile: /root/graphite-web/.tox/py27-django17-pyparsing2/log/py27-django17-pyparsing2-1.log\nERROR: actionid: py27-django17-pyparsing2\nmsg: getenv\ncmdargs: [local('/root/graphite-web/.tox/py27-django17-pyparsing2/bin/pip'), 'install', 'coverage', 'cairocffi', 'django-tagging', 'pytz', 'mock', 'git+git://github.com/graphite-project/whisper.git#egg=whisper', 'git+git://github.com/graphite-project/ceres.git#egg=ceres', 'pyparsing', 'Django>=1.7,<1.8']\nenv: {'SSH_CLIENT': '192.168.79.1 61276 22', 'LOGNAME': 'root', 'USER': 'root', 'HOME': '/root', 'PATH': '/root/graphite-web/.tox/py27-django17-pyparsing2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin', 'LANG': 'en_US.UTF-8', 'TERM': 'xterm', 'SHELL': '/bin/bash', 'GRAPHITE_NO_PREFIX': 'true', 'SHLVL': '1', 'PYTHONHASHSEED': '270926094', 'TOXENV': 'py27-django17-pyparsing2', 'PYTHONPATH': '/root/graphite-web/webapp', 'SSH_AUTH_SOCK': '/tmp/ssh-RkTdFRKuYe/agent.1096', 'VIRTUAL_ENV': '/root/graphite-web/.tox/py27-django17-pyparsing2', '_': '/usr/local/bin/tox', 'SSH_TTY': '/dev/pts/1', 'OLDPWD': '/root', 'PWD': '/root/graphite-web', 'DJANGO_SETTINGS_MODULE': 'tests.settings', 'MAIL': '/var/mail/root', 'SSH_CONNECTION': '192.168.79.1 61276 192.168.79.237 22'}\nCollecting coverage\n  Using cached coverage-4.1.tar.gz\nCollecting cairocffi\n  Using cached cairocffi-0.7.2.tar.gz\n    Complete output from command python setup.py egg_info:\n    c/_cffi_backend.c:2:20: fatal error: Python.h: No such file or directory\n     #include \n                        ^\n    compilation terminated.\n    Traceback (most recent call last):\n      File \"\", line 1, in \n      File \"/tmp/pip-build-kfHaDf/cairocffi/setup.py\", line 65, in \n        **cffi_args\n      File \"/usr/lib/python2.7/distutils/core.py\", line 111, in setup\n        _setup_distribution = dist = klass(attrs)\n      File \"/root/graphite-web/.tox/py27-django17-pyparsing2/local/lib/python2.7/site-packages/setuptools/dist.py\", line 269, in init\n        self.fetch_build_eggs(attrs['setup_requires'])\n      File \"/root/graphite-web/.tox/py27-django17-pyparsing2/local/lib/python2.7/site-packages/setuptools/dist.py\", line 313, in fetch_build_eggs\n        replace_conflicting=True,\n      File \"/root/graphite-web/.tox/py27-django17-pyparsing2/local/lib/python2.7/site-packages/pkg_resources/init.py\", line 826, in resolve\n        dist = best[req.key] = env.best_match(req, ws, installer)\n      File \"/root/graphite-web/.tox/py27-django17-pyparsing2/local/lib/python2.7/site-packages/pkg_resources/init.py\", line 1092, in best_match\n        return self.obtain(req, installer)\n      File \"/root/graphite-web/.tox/py27-django17-pyparsing2/local/lib/python2.7/site-packages/pkg_resources/init.py\", line 1104, in obtain\n        return installer(requirement)\n      File \"/root/graphite-web/.tox/py27-django17-pyparsing2/local/lib/python2.7/site-packages/setuptools/dist.py\", line 380, in fetch_build_egg\n        return cmd.easy_install(req)\n      File \"/root/graphite-web/.tox/py27-django17-pyparsing2/local/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 665, in easy_install\n        return self.install_item(spec, dist.location, tmpdir, deps)\n      File \"/root/graphite-web/.tox/py27-django17-pyparsing2/local/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 695, in install_item\n        dists = self.install_eggs(spec, download, tmpdir)\n      File \"/root/graphite-web/.tox/py27-django17-pyparsing2/local/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 876, in install_eggs\n        return self.build_and_install(setup_script, setup_base)\n      File \"/root/graphite-web/.tox/py27-django17-pyparsing2/local/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 1115, in build_and_install\n        self.run_setup(setup_script, setup_base, args)\n      File \"/root/graphite-web/.tox/py27-django17-pyparsing2/local/lib/python2.7/site-packages/setuptools/command/easy_install.py\", line 1103, in run_setup\n        raise DistutilsError(\"Setup script exited with %s\" % (v.args[0],))\n    distutils.errors.DistutilsError: Setup script exited with error: command 'x86_64-linux-gnu-gcc' failed with exit status 1\n----------------------------------------\n\nCommand \"python setup.py egg_info\" failed with error code 1 in /tmp/pip-build-kfHaDf/cairocffi/\nERROR: could not install deps [coverage, cairocffi, django-tagging, pytz, mock, git+git://github.com/graphite-project/whisper.git#egg=whisper, git+git://github.com/graphite-project/ceres.git#egg=ceres, pyparsing, Django>=1.7,<1.8]; v = InvocationError('/root/graphite-web/.tox/py27-django17-pyparsing2/bin/pip install coverage cairocffi django-tagging pytz mock git+git://github.com/graphite-project/whisper.git#egg=whisper git+git://github.com/graphite-project/ceres.git#egg=ceres pyparsing Django>=1.7,<1.8 (see /root/graphite-web/.tox/py27-django17-pyparsing2/log/py27-django17-pyparsing2-1.log)', 1)\n___________ summary ___________\nERROR:   py27-django17-pyparsing2: could not install deps [coverage, cairocffi, django-tagging, pytz, mock, git+git://github.com/graphite-project/whisper.git#egg=whisper, git+git://github.com/graphite-project/ceres.git#egg=ceres, pyparsing, Django>=1.7,<1.8]; v = InvocationError('/root/graphite-web/.tox/py27-django17-pyparsing2/bin/pip install coverage cairocffi django-tagging pytz mock git+git://github.com/graphite-project/whisper.git#egg=whisper git+git://github.com/graphite-project/ceres.git#egg=ceres pyparsing Django>=1.7,<1.8 (see /root/graphite-web/.tox/py27-django17-pyparsing2/log/py27-django17-pyparsing2-1.log)', 1)\nroot@debian:~/graphite-web# pip install coverage\nRequirement already satisfied (use --upgrade to upgrade): coverage in /usr/lib/python2.7/dist-packages\nCleaning up...\nroot@debian:~/graphite-web# python --version\nPython 2.7.9\nroot@debian:~/graphite-web# dpkg -l python-django\nDesired=Unknown/Install/Remove/Purge/Hold\n| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\n|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\n||/ Name                                Version                Architecture           Description\n+++-===================================-======================-======================-===========================================================================\nii  python-django                       1.7.7-1+deb8u4         all                    High-level Python web development framework (Python 2 version)\n```\nIf you think I need an ubuntu vm to run tox, I would agree to install one. You prefer 16.04 LTS or another one ?\nAnyway this would luckily solves points 1. of my comment, not 2. and 3.\n. @JeanFred thanks, it works (when installing libffi-dev in addition to python-dev).\n______________________________________________________________________________ summary ______________________________________________________________________________  py27-django14-pyparsing2: commands succeeded\n  py27-django15-pyparsing2: commands succeeded\n  py27-django16-pyparsing2: commands succeeded\n  py27-django17-pyparsing2: commands succeeded\n  py27-django18-pyparsing2: commands succeeded\n  py27-django19-pyparsing2: commands succeeded\n  py27-django16-pyparsing1: commands succeeded\n  lint: commands succeeded\n  docs: commands succeeded\n  congratulations :)\n. Is it ok for merging ?\n. Is it ok for merging ?\n. I understand.\nHowever Florent has left and therefore I would need to take time to dive inside his code by myself, and am not likely to do that very soon.\n. I'm asking for merging code that works in production, which is quite a good candidate for me. However I understand that it's not enough for you (it wouldn't be for me either, if I was you). I'll open a new PR later.\n. The callbackToNode function in this PR is no longer usefull, since #1478 introduced an applyToNode method which does almost the same thing, see http://graphite.readthedocs.io/en/latest/functions.html#graphite.render.functions.applyByNode\nTherefore I wont rewrite my coworker code to make this PR mergeable.\n. We've just discovered a bug in the function, therefore I close the PR until fixed.\n. Fixed.\n. no longer needed since #1551 has been merged\n. carbon-cache.py is part of carbon project, not graphite-web project.\nsee https://github.com/graphite-project/carbon\n. I don't remember. I test again w/o patch today or tomorrow and tell you more.\n. With this Configuration:\n- graphite-web 1c0bd67\n- Python 2.6.5\n- Django 1.4.10\n- RHEL 5\nHaving carbon cache completion enabled, i.e. having this line in local_settings.py\nCARBONLINK_HOSTS = [\"127.0.0.1:25212:cc1\"]\nI get this output string instead of a nice PNG:\nTraceback (most recent call last):\n  File \"/usr/local/applications/eagle/exec/eagle-graphite-2/deps/django/django/core/handlers/base.py\", line 111, in get_response\n    response = callback(request, *callback_args, **callback_kwargs)\n  File \"/usr/local/applications/eagle/exec/eagle-graphite-2/webapp/graphite/render/views.py\", line 110, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/usr/local/applications/eagle/exec/eagle-graphite-2/webapp/graphite/render/evaluator.py\", line 10, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/usr/local/applications/eagle/exec/eagle-graphite-2/webapp/graphite/render/evaluator.py\", line 21, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression)\n  File \"/usr/local/applications/eagle/exec/eagle-graphite-2/webapp/graphite/render/evaluator.py\", line 24, in evaluateTokens\n    return fetchData(requestContext, tokens.pathExpression)\n  File \"/usr/local/applications/eagle/exec/eagle-graphite-2/webapp/graphite/render/datalib.py\", line 160, in fetchData\n    raise Exception(\"Failed after %i retry! See: %s\" % (settings.MAX_FETCH_RETRIES, e))\nException: Failed after 2 retry! See: 'consolidationFunc'\nWith this patch, I've got a nice PNG again: c4827b9998ee41838fac3c46fb7a0971d97a73b9\n. I apologize, this is not an actual issue, it only appears when I change graphite version on one node but not the other ones (this morning I did a git checkout 1c0bd67 && apachectl restart on only one server).\nI suppose the issue occurs between two graphites rather than between a graphite and its local carbon cache, but anyway with consistent versions accross graphite nodes I cannot reproduce the issue.\nThanks @brutasse : with your last comment you've make me understand that the issue only appear when the version is not the same on every graphite nodes.\n. Hi, in fact, it's really an issue for me. Just I did not re-checked it the right way because I did not remember the right context to reproduce it.\nTo reproduce it you need to have data in carbon cache that di not yet hit the disk, which can be done easier with MAX_UPDATES_PER_SECOND = 1 in carbon cache's carbon.conf.\nWithout my commit c4827b9998ee41838fac3c46fb7a0971d97a73b9 I've got a png without last minutes data, and this error in cache.log:\nFri Sep 12 18:23:48 2014 :: Request-Cache miss [e1e690e664b653ea9eead8d6fdb09ba1]  \nFri Sep 12 18:23:48 2014 :: Data-Cache miss [c6d9898769cb0390b997368286a4ecb3]\nFri Sep 12 18:23:48 2014 :: CarbonLink creating a new socket for ('127.0.0.1', 'cc1')\nFri Sep 12 18:23:48 2014 :: CarbonLink sending request for stats.null to ('127.0.0.1', 'cc1')\nFri Sep 12 18:23:48 2014 :: Exception getting data from cache ('127.0.0.1', 'cc1'): Attempting to unpickle unsafe module collections\nFri Sep 12 18:23:48 2014 :: CarbonLink cache-query request for stats.null returned 0 datapoints\nWith my patch it's ok both in rendered png and in cache.log:\nFri Sep 12 18:25:47 2014 :: Request-Cache miss [e1e690e664b653ea9eead8d6fdb09ba1]\nFri Sep 12 18:25:47 2014 :: Data-Cache miss [4e8380682f3ec5aae9bc67901af170b5]\nFri Sep 12 18:25:47 2014 :: CarbonLink creating a new socket for ('127.0.0.1', 'cc1')\nFri Sep 12 18:25:47 2014 :: CarbonLink sending request for stats.null to ('127.0.0.1', 'cc1')\nFri Sep 12 18:25:47 2014 :: CarbonLink finished receiving stats.null from ('127.0.0.1', 'cc1')\nFri Sep 12 18:25:47 2014 :: CarbonLink cache-query request for stats.null returned 9 datapoints\n. When testing on series that have yet been written on disk, the exception is not logged in cache.log (I think because there are no datapoints to return the thus the pickle message is not the same) and - of course - the rendered data is ok.\n. Hi @deniszh,\nNo: https://github.com/graphite-project/graphite-web/blob/1c0bd6746fd147aec89f05ca08f92008de181122/webapp/graphite/util.py\nYou think this would help ?\n. @deniszh I don't wan't to try 0.9.x versions, I did in the past and it was a headache both to contribute and to get new features in production, I prefer master, but didn't fetch-rebase for a while.\nI'll try on monday. And will first of all cherry-pick the commit with the lines you told me to add and keep this issue informed.\nThanks.\n. Nice thanks!\n@deniszh's 4d854a76d1f087ba3d687b4c22f4b625578dac63 fixes the issue\n. yes hope so :-)\n. @brutasse Yes I added a comment in #712 \n. @SEJeff right !\n. Rebased (and by the way squashed)\n. Thanks.\n. I rebased against master to fix merge conflict.\nIs it ok for merging ?\n. Hi, is someone interested in reviewing and/or merging these changes ?\nBTW, happy new year :-)\n. Hi, is someone interested in reviewing and/or merging these changes ?\n. Hi, is someone interested in reviewing and/or merging these changes ?\n. I've just rebased the PR against up to date master.\nIs someone interested in reviewing and/or merging these changes ?\n. I've just rebased the PR against up to date master.\nIs someone interested in reviewing and/or merging these changes ?\n. thanks\n. Since constantLine() produces a TimeSeries with pathExpression (I suppose thanks to this commit: c1a04ec5), this exception no longer occurs. However diffSeries still does not really work with constantLine since constantLine only produces 2 or 3 values (2 with current master, 3 with #645) and diffSeries produces null values whenever one of the diffed series is null.\nIMHO diffSeries or constantLine should not be fixed to work together, but one of this option should rather be choosen:\n- You can use offset with negative value instead. Example: offset(my.metric,-1). Or event diffSeries and then offset. Example: offset(diffSeries(my.{a,b}.metrics),-1)\n- Or diffSeries should be modified to accept either series or constants as parameters, see issues #415 and #626. This would be more user-friendly, but, IMHO, not so easy to do. These issues are currently closed since offset does the job.\nI would propose to close this issue since using offset solves it.\n. Selecting series with * or event with {used,total} does not guarantee any order. Moreover Selecting series like server.*.cpu.{used,total} does not guarantee that servers won't be mixed.\nThis is exactly why a coworker of mine created a non-merged \"callbackToNode\" method seen in #661.\nThis is a kind of generalized multiplySeriesWithWildcards.\nHowever the way the function was coded/documented is not acceptable as is for merging and I did not yet take time to rewrite it.\nIf it can help you this works nice for us with asPercent() and diffSeries(). It is likely to work with multiplication or division either.\nHowever we do not use Grafana, only Graphite with Whisper backend.\n. @steve-dave \nWell... I'm not sure what you mean by \"need\".\nThis patch did not broke the tests, therefore no test case MUST be updated.\nHowever, AFAIK the modified method is not tested at all, therefore it SHOULD be a good idea in the future to add some tests on graph output (or at less on json/csv output, since svg/png are probably not so easy to test because of changes in cairo and other libraries), regardless of this pull request.\n. Thx for the merge.\nDo you want me to backport the same commit to 0.9.x ?\n. @SEJeff done.\n. @SEJeff  Done. #1108 \n. BTW I wonder how one can still use 0.9.x, I switched to master-in-production for a while ;-)\n. Hi, is someone interested in reviewing and/or merging these changes ?\n. Hi, is someone interested in reviewing and/or merging these changes ?\n. Hi, since this PR is only a backport of #1106, IMHO maybe it can be merged or rejected (anyway #1106 has been merged, therefore it will be released one day).\n. np thx\n. Yes, I think with such one-line patches I may be awarded some Nobel prize or at less the Turing Award. ;-)\n. Hi, is someone interested in reviewing and/or merging these changes ?\n. thanks.\n. ",
    "eugenea": "g76r oh no, we need to edit every single graph to work properly? Why it cannot be default behavior?\n. Err how expensive can that be?\n. ",
    "ryan-williams": "any updates here? I've been getting quite lost in the various/incomplete pieces of graphite documentation I've found around the web; I've still not successfully run graphite locally after a few days of trying.\n. thanks @obfuscurity, that DO recipe worked like a charm.\nCan that be added somewhere where newcomers might see it?\nIs there a minimal set of commands that would let me run Graphite from source? bin/carbon-cache.py start from a carbon checkout seems like it would be a member of such a set (and is not documented in the carbon README.md); what command starts the web server? Is there a whisper command that needs to be run as well?\n. thanks, I tried to work through the readthedocs site but got to the blank pages around starting up the web server documented in this issue: http://graphite.readthedocs.org/en/latest/admin-webapp.html\n. I've not found any documentation about how to start the web server on this repo, wikidot, readthedocs, or elsewhere on the internet\n. Is the link to https://github.com/graphite-project/graphite-project.github.io/issues meant to suggest that it is the place where one should file issues with the readthedocs site?\n. Cool, yea I was confused by that repo because I did not see the activity in the issues; it has no commits in >2yrs and a title link to a nonexistent page. \nIs \"the new site\" that you've referenced a few times the existing-but-incomplete http://graphite.readthedocs.org/en/latest/, or is it a newer site that will live at http://graphite-project.github.io/?\n. heh, I also just noticed the \"Edit on Github\" badges on the upper-right of every page. fair enough!\n. OK. Should I do that in addition to or in lieu of sending PRs to master?\n. sorry for dropping this, applying your fix now, thanks\n. weirdly the build didn't originally fail here, I guess it didn't attempt to build the docs as of 0.9.x?\n. ",
    "geogdog": "I have the same problem, but it doesn't go away when I use the wildcard.  I actually found it whilst using a wildcard.\nI get a broken image and an assertion error page when I attempt to read volume information from my NetApp filer.  Some of the volume contain (1) or (2), etc at the end of the name.\n. ",
    "otac0n": "Yes, absolutely.\n. Well, they are just so convenient.  I guess we can convert to something else, but that's extra effort, when we can just use the workaround.\nIs there some reason you want to kill pie charts?\n. Ok, well, we need 10 seconds, and this function doesn't (and can't) do it.  How would you suggest I change this to allow 10 second resolution?\n. ",
    "jdanbrown": "Thanks @ralphm! This bug has been bothering me for a while on 0.9.10. (Merged into my own fork.)\n. ",
    "lVlayhem": "I have same bug when title has non ASCII characters\n```\nRequest Method: GET\nRequest URL: http://127.0.0.1:8080/render/?width=586&height=308&_salt=1367637239.914&target=carbon.agents.ip-10-250-161-221-a.cpuUsage&from=-1221hours&title=%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D0%BA\nDjango Version: 1.4.5\nPython Version: 2.7.3\nTraceback:\nFile \"/home/sergbol/statistics/.env/lib/python2.7/site-packages/django/core/handlers/base.py\" in get_response\n  111.                         response = callback(request, callback_args, *callback_kwargs)\nFile \"/home/sergbol/statistics/.env/opt/graphite/webapp/graphite/render/views.py\" in renderView\n  58.     requestKey = hashRequest(request)\nFile \"/home/sergbol/statistics/.env/opt/graphite/webapp/graphite/render/hashing.py\" in hashRequest\n  33.   return compactHash(myHash)\nFile \"/home/sergbol/statistics/.env/opt/graphite/webapp/graphite/render/hashing.py\" in compactHash\n  52.   hash.update(string)\nstring  u'from=-1221hours,height=308,target=carbon.agents.ip-10-250-161-221-a.cpuUsage,title=\\u0433\\u0440\\u0430\\u0444\\u0438\\u043a,width=586'\nException Type: UnicodeEncodeError at /render/\nException Value: 'ascii' codec can't encode characters in position 83-88: ordinal not in range(128)\n```\n. ",
    "zwily": ":thumbsup:\n. :thumbsup: \n. I'm seeing the same rounding issue here... Since upgrading to 0.9.12, cactiStyle() is only showing integers.\n. ",
    "minaguib": "My team has learnt to look for telltale overlaps in stacked graphs (or avoid them completely) to deal with this bug in graphite.\nIt would be kind of awesome to trust stacked graphs output by graphite as correct.\n. ",
    "mingbowan": "what about file system paths?  like \"/usr/local\"? You cannot escape \"/\" in file path\n. ",
    "davewongillies": "+1\n. Adding a \"me too\" to this, thought I was going quietly insane. I'm seeing this on 0.9.12.\n. ",
    "nacyot": "+1\n. ",
    "karmicnewt": "+1\n. ",
    "KevinJMao": "+1\n. ",
    "sebito91": "As SEJeff mentioned, the Ceres solution provides assistance with the precise issue you've mentioned. To leverage this new method with graphite-web you should also look at the megacarbon branch of the Carbon code.\n. Yes, that's definitely something that would be interesting.\nWe converted from graphite to metrilyx (from ticketmaster) but it's not\nas full featured as graphite, at least not in terms of transform\ncapabilities.\nAre you planning to open source it?\nOn Tuesday, April 22, 2014, Kamaldeep Singh notifications@github.com\nwrote:\n\nHi,\nCurrently graphite does not support integration with TSDB. It uses the\nwhisper database which is not as scalable as TSDB.\nWe at Ebay have developed a solution for integrating Graphite with TSDB.\nIs it something the developer community at graphite would be willing to\nreview ?\nThanks,\nKamaldeep Singh,\nEbay\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/issues/693\n.\n. There is actually a branch on manolama's GitHub that you can check out for\ncomparison which has some hooks for graphite-web built in.\n\nhttps://github.com/manolama/graphite-web/tree/opentsdb_scratch\nOn Tuesday, April 22, 2014, Kamaldeep Singh notifications@github.com\nwrote:\n\nYes we are planning to open source it . We wanted to first measure the\ninterest that the community has in this initiative .\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/issues/693#issuecomment-41032387\n.\n. ```\nHappy\n      to start looking at the issue.\n\n  Quick update, mentioned pacakaging graphite-web for release\n  during the Fedora-admin meeting this week and the team is\n  stoked. They're keen to start testing it out on the\n  infrastructure servers as soon as we can get it ready!\n\nSebastian Borza\n```\n\nPGP: EDC2 BF61 4B91 14F2 AAB4  06C9 3744 7F3F E411 0D3E\n      On 1/12/15 11:25 PM, Jason Dixon wrote:\n@sebito91 That would be great, and\n    now's the time to pitch in. Per graphite-project/carbon#355,\n    it seems that our build tarball is missing some distro stuff.\n    Admittedly, I built it on a Mac, but I'm surprised that would\n    matter. We can't very well make multiple tarballs available, one\n    per distro/platform, for a single release (for PyPI).\n  \u2014\n    Reply to this email directly or view\n      it on GitHub.\n. ",
    "mtodd": "This issue is very pronounced when I have a secondary axis with more than a dozen legend keys on the left.\nHere's what graphite just rendered for me, with 1144x415 dimensions specified:\n\nThat is unmodified. You'll notice the large white gap at the bottom.\n. ",
    "daa": "actually my issue is about this (i've got part of your image):\n\nyou see too much padding added\nlegend height is calculated as if we add 5px padding to each color annotation but that doesn't happen and gap between lines is 1px, so there is huge gap at the bottom\n. ",
    "siboulet": "+1\n. Please review https://github.com/graphite-project/whisper/pull/72 which I believe is related to this. It's not clear to me in the first place why now is used to determine the archive to use from the Whisper file. Why don't we rely on fromTime and untilTime? \n. ",
    "jablo": "+1\nI was looking for this function in despair. I have icinga firing an alarm when one of a number of graphs in a wildcard series goes below a certain threshold. Thus I need to display a dashboard with all graphs below that threshhold.\n. Sounds great. In the mean time I am using a workaround:\nI multiply the graphs by -1, then use the highest max function and multiply the resulting graphs by -1 again. This is equivalent to taking the lowest min. Though not as readable.\n. ",
    "damm": "This issue might be better Handled over there\n. Here's my only wish, it was not in the example but actually in the defaults.  I'm totally aware that the defaults are a Dict hardcoded.\n. @joemiller what's wrong with using STANDARD_DIRS ? That's how I made graphiteweb read my 12 different whisper paths. \n. Wish the tree was more configurable; if it was me I would remove my Graphs and User Graphs and just have the metrics in the tree.\nI don't know many people who use the user graphs :( must be just me.\n. Any reason you can't Omnibus Graphite? there's only really a few targets and trying to address many versions will only complicate life.\n. I believe this should be.\n.. _Sensu: http://sensuapp.org/ ? Sorry to be a spell checker :( \n. ",
    "kaarelk": "Yes,\nhttps://github.com/graphite-project/carbon/issues/89\n. It is the same if you only have two wildcards in the query, but it returns more series if there's more than two wildcards. I.e it works like sumSeriesWithWildcards.\nLet's assume we gather metrics for requests in the form:\n$server.$httpType.$requestUrl.rate (e.g server1.GET.index-php.rate). I would like to draw a line for each server for each requestUrl (ignoring httpType). I could do it like that:\nsumSeriesWithWildcards(..*.rate, 1). Can I do the same with groupByNode?\n. ",
    "falkenbt": ":+1:\n. +1\n. +1\n. Ping \nThis is hitting us as well since the codahale metrics library initializes values with -infinity, so graphs containing metrics that never got any update in the viewed timeframe will always break, which is not pretty. \n. :+1: \n. ",
    "marco-hoyer": ":+1:\n. +1, helped us solving strange behavior.\n. ",
    "bwheatley": "+1\n. ",
    "fjorgemota": "@esc \nTry to use the URL encoded +, which is %2B actually.\nNote that this dont solve the problem of allowing Graphite to show data in the future, something that I need and want to know too (for allowing visibility of goals in graph with multiple data series (with some goals, effectively)). See the issue that I opened at https://github.com/graphite-project/whisper/issues/68\n. ",
    "josegonzalez": "@damm wat\nAlso, @obfuscurity if you don't pull this in, the good Doctor WILL revoke your thought leader certification!\n. The solarized-light options are written into the default setting.\nAlso, this version wards away chicken pox.\n. Given the above feedback, I'll rename my new default to solarized-light, leave classic as is, and set plain as default. Does that seem fine? I'll squash the commits as well.\n. Pull request updated. Do we need to duplicate the new default stanza under plain?\n. So then should I close this pull request, or is there something you'd like me to change about it?\n. Would be nice if this was merged at some point.\n. ",
    "DrQz": "Thanks to @obfuscurity for the heads-up and the opportunity to have a nano-rant about gothic graphs.\nThere were really 2 main points I tried to get across in my keynote slides:\n1. Use a neutral background color. Some shade of gray is generally considered best for luminance contrast of color lines and areas. Pale bisque is not a shade of gray. :) [A pale green background is considered optimal for black text.]\n2. Use coordinated colors (not pastels) on a neutral background. The ColorBrewer pkg in R is a tool to help do that; although it was originally aimed at maps.\nIt's difficult to be too categorical about all this b/c of the variation in human perception and variation in display quality and calibration. The final arbiter is the user's level of comfort. Therefore, the best option is to allow the user to set it along with some well-known guidelines. --End of rant (maybe)\n. Like the corporate IT race toward offshoring to reduce costs and taxes, it's not so much the transition that is socially harmful but the rate of transition. And maintaining the status quo is not the best option either. What to do?\nIf by \"active default\" we're talking about the Gothic black background for graphite plots, then I would propose that the following steps be taken as a more progressive transition to ease the user community into it:\n1. Warn the users that the default will change eventually.\n2. See if there's any significant blow back.\n3. Eventually enable a user-selectable choice from black to white or light gray (not bisque).\n4. After some even longer period, make the selectable background the default background.\nOr something like that.\n. ",
    "mblair": "Alright, now that I've got a bit more context, this may be helpful:\nThe graph that shows this symptom has 2 metrics on each y-axis, and their aliases are around 12 characters each. It only manifests itself when the graph is enlarged (or maximised).\n. ",
    "oldmantaiter": "I'm going to close this, was something to get around my lack of real programming knowledge.\n. Didn't even think about the max URL length issue when I submitted this, I hadn't hit that over here.\nAs for the explicit JSON input check, that was so I could decode it with lazy programming.\n. ",
    "manolama": "RE: the series request, we don't have a means of wildcard matching in OpenTSDB yet, but you could use the search plugin to perform that search, then pull a list of TSUIDs and pass that to the query. This applies to 2.0 of course.\n. RE: the POST data, the default is JSON but I do have code for an ASCII serializer that could use to POST a number of TELNET style lines to store data. I just have to clean it up and add it to the repo if enough folks are interested in it.\n. Ah sheesh, sorry, I thought I was on a diff page. goes back to sleep\n. @kds119 Yes please please please publish your Graphite work! The hack I slapped together as @sebito91 pointed out needs a lot of love. @tungstencarbide has also done some awesome work that adds tag selection to Graphite. We're hoping to get that public soon and maybe, someday integrated into Graphite-Web.\n@brutasse Unfortunately Graphite is limited to a heirarchical naming structure (web01.sys.cpu.0) whereas the newest wave of time series data stores (OpenTSDB, KairosDB, InfluxDB, TempoDB, etc) support key/value naming. Graphite-Web requires a fair amount of work to support such queries properly but I think it would be well worth it since the rendering capabilities are so rich in Graphite. \n. ",
    "benmathews": "Is this difficult to fix? I'd like to use Graphite, but our security folks won't allow it while XSS is possible.\n. I'm happy with the proposed fix.\n. ",
    "optumstockwell": "This fix is available in optumstockwell/graphite-web fork of this repository. A pull request has been generated.\n. ",
    "stevebanik": "I have increased CARBONLINK_TIMEOUT to 30.0 and I'm still seeing the timeout in datalib.py.  'iostat' shows my RAID5 SSD %util at about 98% at all times.  I have 98GB of RAM.  Are there any tips/tricks on how to effectively balance in-memory vs on-disk reads, writes and lookups? \n. @pcn Your suggestion worked for me.  We have Django==1.5.4 and my settings.py reads:\nif MEMCACHE_HOSTS:\nCACHE_MIDDLEWARE_SECONDS = 60\n    CACHES = {\n        'default': {\n            'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',\n            'LOCATION': [\n                '127.0.0.1:11211'\n            ]\n        }\n}\nMake sure the indentation is correct so that Python doesn't complain.\n. ",
    "gnemoug": "i think graphite-web had better make a conf file that can change something like this.\n. ",
    "sbimikesmullin": "for me this increased the frequency of calls to /render but the resulting image/png was the same over a minute. so it didn't work for me. it looks like a graphite image rendering limitation.\nmay be related: https://answers.launchpad.net/graphite/+question/189806\n. ",
    "Igor-Misic": "Hi everyone.\nDid you guys try this with this \"change 'var interval = 60;' to 'var interval = 1;' in the function toggleAutoRefresh\"?\nI use graphite on ubuntu and my path is here:\n/usr/share/graphite-web/static/js/composer_widgets.js\nIf I change something inside \"composer_widgets.js\",  nothing has happened in graphite web. Do you know why? \nEdit: Ok, I'm find out. Problem was with Google chrome. He has cached all in memory. When I opened it in IE i show changes.  And this 1sec work very well. \n. ",
    "jhaubrich": "Will do!\n. This commit is messed up. #fixing\n. ",
    "jbochi": "Any news on this issue? I have tested it and i can confirm that it works on CPython and PyPy as well\n. ",
    "tegge": "As far as i know, this problem isn't actualy in the current version. But im not sure.\n. ",
    "guidoiaquinti": "Yes @obfuscurity my workaround was setting up a script for the POST request. The problem is that the dashboard JSON definition != POST data expected by graphite.\n. ",
    "erowan": "+1\n. ",
    "Fak3": "so... can this be merged?\n. ",
    "gjesse": "closing - will redo work in branches then re-request.\n. eh sorry, I lost track of this. There's a bunch of places in the file where iterations are done using .keys(). If that's going to be addressed it should be done separately, otherwise I tend toward maintaing consistency. \n. ",
    "adrienbrault": "ping\n. Having the same issue\n. ",
    "jwheare": "Django templates have a filter escapejs[1] exactly for this purpose. Much better to use that instead of incomplete string replacing in the view.\n[1] https://docs.djangoproject.com/en/dev/ref/templates/builtins/#escapejs\n. Since you're marking all those values as |safe and putting them in a string, there's nothing in the template that's stopping them from breaking out of the JSON.parse context, by having a ') in them. I prefer to use a |json_encode custom filter and just pass in raw dict data to the template to avoid this. Had to roll my own filter though.\n. ",
    "anatolijd": "What's the benefit of using two webapps at the same host ? And how did you managed to install two webapps at the same host, please ?\n. What is the practical usage of it ? \nI see it can be quite useful as an API-extension if you want to get a list of  available metrics in a cluster, without manual iteration thru the the CLUSTER_SERVERS.\n. i'm +1 for it.\nhttps://github.com/obfuscurity/therry will benefit from it too.\n. Hi mjulian, I think that this is probably the same story as with '&graphType=pie' and other API functions, and it is right, imho.\nGraphite Composer primary purpose is fast graph prototyping. margin function is rarely used so, it is good not to overload Composer UI with unneeded stuff.\n. @lukevenediger , I said just Graphite Composer not to be overloaded, interface where you initially design the graph. And you are talking about Graphite Dashboard, where you view graphs. \nI also use Graphite Dashboard at some place where other people create many graphs and I can't use gdash. \nIn my case, I put number of graphs at the Graphite Dashboard, and I can set margin for all graphs in Dashboard Menu->Graphs->Edit Default Parameters 'margin=80', which sets the same margin to all the graphs at this Dashboard.\nIf I need to set margin just to one individual graph at Dashboard (what never actually happen to me) - I'd go Menu->Dashboad->Edit Dashboard, and add \"margin\":\"160\" (or \"graphType\":\"pie\") to individual graph .\nThat's why I think there is no needs to add this function to Graphite Composer.\n. Oleksiy, what graphite-web version you're at ? Is it any tagged release ?\nLooking at http://snap.kovyrin.net/skitched-20130506-120110.gif, it should be easy to reproduce it, but it is not. At least, I can't see that spike at my graph on the same function on similar metric (I see some another glitch, though).\nIf you could share more examples of unexpected results it might give - please share. \n. hi, there are cases when  is_local_interface() may return wrong result: https://github.com/graphite-project/graphite-web/issues/222\n. in most cases, keepLastValue() helps, try to use it this way : averageSeries(keepLastValue(_.Queue.2207,*_.Queue.2207))\n. Looks like values for for 2nd graph are aggregated (60 vs 360 resolution). \nWhat are retention intervals for for this graph ?\nIf you have a graphite cluster - are all nodes time syncronized ?\nIs it still reproducible if you remove &tz=CET parameter ?\n. what is WHISPER_DIR value for your uber graphite-web local_settings.py ?\nWhat is LOCAL_DATA_DIR value for carbon-cache at this instance (carbon.conf) ? \nDoes directory /home/graphiteweb/storage/whisper/ exist ?\n. ",
    "RobK998": "Hi,\nSorry for not replying sooner.\nWe have multiple sites in different timezones but we want to store all\nGraphite data at a single site. I have setup a cluster for each site\nconsisting of a carbon, a relay and a webapp. Remotely I have a relay\nrunning at each site.\nThe reason for having a webapp for each cluster is so the time zone can be\nset correctly so the graphs show the local time when viewed by the local\nteam. By including all the webapps in CLUSTER_SERVERS we also have a nice\nside effect that we can view another sites data using the local webapp and\nsee the same graphs but with local time.\nIs there another way I can configure the clusters that would not require\nmultiple webapps but still have the correct time on the graphs?\nThanks\nRob\nOn 17 May 2013 16:05, \"Anatoliy D.\" notifications@github.com wrote:\n\nWhat's the benefit of using two webapps at the same host ? And how did you\nmanaged to install two webapps at the same host, please ?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/issues/312#issuecomment-18066838\n.\n. \n",
    "beevee": "+1 on this problem.\nI intend to run a cluster with three tiers in production, as described in this scheme:\nhttp://us-east.manta.joyent.com/jalquiza/public/grey-boundary/graphite-cluster.png\nI can't test this configuration on a single machine. Top webapp refuses to gather metrics from secondary webapps thinking that they are local. It means that I must run at least two servers for test environment.\n. Of course, there are many solutions for this problem:\n- run at least two machines for any clustered Graphite environment\n- patch Graphite to remove is_local_interface check from https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/storage.py#L32\n- patch is_local_interface to filter out only explicit declarations like 'localhost' or '127.0.0.1'\nBut what is the reason to disallow local webapps in the first place?\n. I had the same problem, and it disapperead after I switched gunicorn from gevent back to default sync workers. I don't feel very inclined to debug gevent-related problems, but maybe somebody else is.\n. For example, if event occurs at 14:30 (UTC+3), in sqlite it becomes 11:30 (UTC), on /events/ page it is 14:30, but in /events/get_data timestamp is for 08:30 (UTC).\n. I tried turning USE_TZ on and off in local_settings - doesn't help.\n. As of django 1.5, default value for USE_TZ is False. Maybe we should set it explicitly in local_settings.py.example?\n. Problem is resolved now, by the way. Thanks.\n. I'm also experiencing a different bug that should be fixed by hashing cache keys.\nGrafana dashboard sometimes sends render requests with invalid metric name (appends select%20metric in the end). This causes graphite-web to crash with 500 response code due to \"control character\" (space) in cache key:\nException encountered in <GET http://graphite/metrics/find/?query=some.query.select%20metric>\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/django/core/handlers/base.py\", line 115, in get_response\n    response = callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite-front/webapp/graphite/metrics/views.py\", line 128, in find_view\n    matches = list( STORE.find(query, fromTime, untilTime, local=local_only) )\n  File \"/opt/graphite-front/webapp/graphite/storage.py\", line 41, in find\n    remote_requests = [ r.find(query) for r in self.remote_stores if r.available ]\n  File \"/opt/graphite-front/webapp/graphite/remote_storage.py\", line 24, in find\n    request.send()\n  File \"/opt/graphite-front/webapp/graphite/remote_storage.py\", line 57, in send\n    self.cachedResult = cache.get(self.cacheKey)\n  File \"/usr/lib/python2.7/site-packages/django/core/cache/backends/memcached.py\", line 64, in get\n    val = self._cache.get(key)\n  File \"/usr/lib/python2.7/site-packages/memcache.py\", line 898, in get\n    return self._get('get', key)\n  File \"/usr/lib/python2.7/site-packages/memcache.py\", line 847, in _get\n    self.check_key(key)\n  File \"/usr/lib/python2.7/site-packages/memcache.py\", line 1062, in check_key\n    \"Control characters not allowed\")\nMemcachedKeyCharacterError: Control characters not allowed\n. @bitprophet could you resolve merge conflicts, please?\n. ",
    "SuminAndrew": "that's really simple\nhttps://github.com/graphite-project/graphite-web/blob/476d484f044551d6a4089eaa30b21ad784a0bee8/webapp/graphite/render/functions.py#L1088\nshould be \"%.2f\" instead of \"%2.f\"\n. @drawks, any hope for a bugfix in the nearest future?\n. ",
    "pha6d": "Hello, \nI ran into a similar issue.\nGraphite-web doesn't show the metric in the GUI, list is cached on client side. An update of the datasource file will not be shown untill refresh of graphite gui.\nI suggest updating readers.py > (Datasource) > get_intervals \nwith:\n   end = max( os.stat(self.fs_path).st_mtime, start + 1)\nAvoiding then that end = start and returned list being empty.\n. ",
    "boopathi": "done .. !! \n. Nope!!\n. ;) .. did that in the next commit .. !! \n. ",
    "bjcubsfan": "@SEJeff, @jhaubrich and I put our changes together in this pull request.\n. ",
    "influenza": "Yeah! Thanks :+1: \n. ",
    "swills": "This needs to be addressed, django-tagging doesn't work with django 1.8 and is unmaintained, at least according to https://www.djangopackages.com/grids/g/tagging/\n. Also see #326 though I may have been wrong about django-tagging not working with django 1.8.\n. Defaulting to FULL means not functioning properly out of the box, no? And also requiring that users potentially reduce security in order for graphite web to function properly, if I'm understanding right. I would hope there is a better solution.. ",
    "kelbergs": "+1\nFacing the same issue. \nGraphite is not the only thing running on the server, and I cant use an alternative port due to firewall restrictions in my organization\n. ",
    "Seb-Solon": "@kelbergs Have a look to the #328 pull request, you may find what you need ;)\n. Well, you can take the commits if you want. With good apache configuration you can make it work :)\n. Yep exactly :)\n. Ok I manage to merge #346 and #349 to our repo and merge it to the current HEAD. The pull request can be automatically merged :)\nThanks @diyan and @perher :)\n. I'll have a look this week end / monday to merge it. I guess there is a bunch of issue. I keep you updated.\n. Hi! I've update the PR. Please review the dashboard one, I have skipped some code enhancement as I was not sure on how to merge\n@brutasse I dont get your composer thing. If I do a GET on myserver/, will I have graphite? If so, this is not the wanted behavior. The / should be free for other app. \n. Yeah ! :dancers: \n. You're welcome :tada: ! \n. ",
    "Alives": "I started with templates/browser.html and am now working my way through the js files... saw this after getting stumped with the dashboard text entry section.  Any progress here?\n. ",
    "titilambert": "Hello !\nAn other question, what about graphlot ? Is it the future of graphite ?\n. Hello !\nThe pull request was updated. Could you merge it ?\nThanks !\n. @SEJeff, @brutasse Is it good for you ?\n. ",
    "sykp241095": "@anatolijd In graph-explorer( https://github.com/vimeo/graph-explorer ), it will download all metrics to \"metrics.json\", then it will build another two data structures based on metrics.json. \n. :-)\n. very nice.\n. ",
    "LanselotOz": "There is a problem if i use https access to graphite web installations in my cluster. \nI read this comments and realize that my cluster is broken now :) \nAll nodes doesn't see anyone, because I switch to https. \n. ",
    "tkuhlman": "Vertica as a dependency of course would be a bad idea, I realize this code is of use to a relatively small group. This doesn't do that it is just optional to connect to vertica. The os level pyodbc setup must be done independently.\nI didn't know about the megacarbon branch of carbon, I will check it out when I get some free cycles. This code I submitted has a number of workarounds in it that I don't like but didn't have time to do better so I am glad to hear there is potentially a better way to get it done.\n. ",
    "jaimegago": "@gingerlime This has not been merged into the python package that gets installed with \"pip install graphite-web\" despite the fact that the package claims to be 0.9.12...I \"merged\" the PR manually by editing /opt/graphite/webapp/graphite/render/views.py and then I got maxDataPoints to work as expected.\n. ",
    "mikhailov": "did you merge branches recently? This page referenced to the PR and it became outdated:\n\"Alternative if you use 0.9.12 you can update just render/views.py with this https://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/render/views.py, and render/attime.py with https://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/render/attime.py\"\nThe files (views.py, attime.py) stopped working with newer graphite-web.\n. @deniszh that works, thanks!\n. That's amazing thing! :thumbsup:\n. guys, would that be useful to have zooming functionality for PNG?\n. @obfuscurity we move towards Grafana as a UI replacement for Graphite. Not sure, if native Graphite supports zooming functionality, but RRDTool (we used to have before with Cacti) provides you an interface to magnify a certain range between two date points, please see an example below:\n\nIt helps to pan zoom between two exact date points, makes sense to see what happens straight after deployments or configuration's update for instance.\nThe reason why I ask about it, Flot (canvas) is the Grafana default rendering mechanism which is fine for small date ranges but totaly unacceptable for longer period because of O(n) complexity. A bigger number of diagrams with longer dates range both produce very slow response and browser crash. It is also impossible to use Flot (canvas) via the mobile browser by the same reason. Probably the best place to discuss is Grafana issues on Github, but it would be great to know your opinion.\n. @torkelo you are the best, thanks for quick response!\n. ",
    "bitprophet": "I actually pondered how one would integration-test Graphite, or at least the stuff I touched here (graph drawing). Think we'd have to start with unit tests because \"testing\" PNG output feels tricky (though I've never tried testing anything non-text-based.)\nWe have zero tests now, huh. I'd be willing to throw initial unit tests at the API call(s) this PR touches, but not sure about consensus re: test lib. I like to use spec for my projects but I'm admittedly biased and suspect a \"regular\" use of Nose would be more palatable to the average contributor.\n. Wouldn't that only spit out the actual data (which isn't the problem) and not the draph drawing values (X/Y axis min/max, etc)?\n. @obfuscurity Apparently not :( :( sorry! I am taking some time over the next few weeks to catch up on my OSS, so I might be able to get to it at some point, but my own projects take priority at the moment.\n. I'll tackle this today, FWIW :)\n. Welp that only took most of the day :D it's partly my fault for lacking a solid local test-ready environment (long story) and then the usual \"how exactly can I get all the right levers flipped and dials spun so this test lines up right?\".\nTested with the fix reverted, got the wrong answer, put fix back in, got right answer. :sparkles:\n. And boo, something odd is going on re: the test db. When I temporarily up verbosity it's clearly creating the table causing the asplosion. Digging further...\nOK, I see, my added imports are getting their fingers into a part of the stack I think the previous ones did not - which ends up requesting a DB table at import time. (It's graphite/util.py which tries obtaining a user object at load time - that feels kinda bad.)\n. Looks like this particular wrinkle was fixed in #527 which I am guessing never got backported to 0.9.x? Feels like the only real options here are:\nA) suck it up, make a default empty db in travis.yml such that this level of import doesn't kaboom (I had to do this locally, come to think of it)\nor\nB) backport that PR or otherwise tweak util.py such that it more gracefully handles this problem.\nI am leaning towards option A, it's easy, fast, and has few downsides given the actual test function bodies will get run against a separate, clean test DB.\nEDIT: submitting a commit that implements A. Also leaving the verbosity flag on the real test - I always find that useful in CI contexts. Can revert if desired.\n. Tests pass  will let one of the other devs merge as a way of +1'ing my approach re: the db problem. Thanks!\n. cc @obfuscurity @esc in case they've got this issue muted :)\n. Confirmed that this appears broken in the 0.9.x series and also (going by the code) master.\n. #629 may be related to this, hard to hell exactly from your description :) (#629 refers specifically to entire new metric paths not showing up immediately - not datapoints for pre-existing metrics.)\n. This seems to be a dupe of #415 :)\n. FTR this is the traceback one will hit in the wild when this bug is present:\nTraceback (most recent call last):\n  File \"/mnt/services/graphite/lib/python2.7/site-packages/django/core/handlers/base.py\", line 111, in get_response\n    response = callback(request, *callback_args, **callback_kwargs)\n  File \"/mnt/services/graphite/lib/graphite/render/views.py\", line 115, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/mnt/services/graphite/lib/graphite/render/evaluator.py\", line 10, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/mnt/services/graphite/lib/graphite/render/evaluator.py\", line 21, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression)\n  File \"/mnt/services/graphite/lib/graphite/render/evaluator.py\", line 24, in evaluateTokens\n    return fetchData(requestContext, tokens.pathExpression)\n  File \"/mnt/services/graphite/lib/graphite/render/datalib.py\", line 142, in fetchData\n    raise Exception(\"Failed after %i retry! See: %s\" % (settings.MAX_FETCH_RETRIES, e))\nException: Failed after 2 retry! See: Key length is > 250\nAnd the \"real\" traceback when one temporarily nukes the try/except there:\nTraceback (most recent call last):\n  File \"/mnt/services/graphite/lib/python2.7/site-packages/django/core/handlers/base.py\", line 111, in get_response\n    response = callback(request, *callback_args, **callback_kwargs)\n  File \"/mnt/services/graphite/lib/graphite/render/views.py\", line 115, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/mnt/services/graphite/lib/graphite/render/evaluator.py\", line 10, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/mnt/services/graphite/lib/graphite/render/evaluator.py\", line 21, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression)\n  File \"/mnt/services/graphite/lib/graphite/render/evaluator.py\", line 24, in evaluateTokens\n    return fetchData(requestContext, tokens.pathExpression)\n  File \"/mnt/services/graphite/lib/graphite/render/datalib.py\", line 136, in fetchData\n    seriesList = _fetchData(pathExpr,startTime, endTime, requestContext, seriesList)\n  File \"/mnt/services/graphite/lib/graphite/render/datalib.py\", line 99, in _fetchData\n    fetches = [(node, node.fetch(startTime, endTime)) for node in matching_nodes if node.is_leaf]\n  File \"/mnt/services/graphite/lib/graphite/storage.py\", line 24, in find\n    remote_requests = [ r.find(query) for r in self.remote_stores if r.available ]\n  File \"/mnt/services/graphite/lib/graphite/remote_storage.py\", line 25, in find\n    request.send()\n  File \"/mnt/services/graphite/lib/graphite/remote_storage.py\", line 58, in send\n    self.cachedResult = cache.get(self.cacheKey)\n  File \"/mnt/services/graphite/lib/python2.7/site-packages/django/core/cache/backends/memcached.py\", line 57, in get\n    val = self._cache.get(key)\n  File \"/mnt/services/graphite/lib/python2.7/site-packages/memcache.py\", line 793, in get\n    return self._get('get', key)\n  File \"/mnt/services/graphite/lib/python2.7/site-packages/memcache.py\", line 761, in _get\n    self.check_key(key)\n  File \"/mnt/services/graphite/lib/python2.7/site-packages/memcache.py\", line 954, in check_key\n    % self.server_max_key_length)\nMemcachedKeyLengthError: Key length is > 250\n. Hrm I apparently pushed this without actually updating my source branch (I develop patches against my workplace's internal, stable-thus-old checkout - but usually remember to then rebase against public master before PR'ing), git log was showing me the previous commit was from early 2013. Hilarious?\nJust merged to latest master, the diff looks identical to me still (though yes - git did think there was some sort of \"both updated\" situation) so we'll see what Travis says.\n. Given I barely contribute, I'd like to merge this but would :heart: a simple :+1: from any other @graphite-project/committers first to make sure I'm not committing a faux pas. An emoji will do! A day or two of silence may be taken as consent :)\n. Recreated on an Ubuntu 14.10 Rackspace Cloud VPS:\n- Whisper install: whisper ends up in /usr/local/lib/python2.7/dist-packages (not site-packages).\n  - I forget when/why dist-packages vs site-packages is a thing offhand, but I know I have seen it occasionally in the past.\n  - Installing whisper doesn't create /opt/graphite.\n- Carbon requirements: Twisted does at least some C compilation, as does zope.interface; however they & the other pure-Python requirements all end up in /usr/local/lib as with Whisper.\n- Carbon install: this is what creates /opt/graphite; it also appears to copy itself & a portion of Twisted into /opt/graphite/lib/ but not into any site-packages.\n- Graphite requirements: this seems to be the primary culprit. Afterwards, things like Django and cairocffi end up in /usr/local/lib alongside their friends (again, this is always dist-packages not site-packages but I doubt this matters) but as noted above, gunicorn, pygments etc are in /opt/graphite/lib/python2.7/site-packages/.\n  - Nothing in the pip output seems to explain the discrepancy, nor are any patterns jumping out, eg some sub-dependencies end up in both places, some things requiring C compilation end up in both places.\nAll output of my pip/python commands are in this gist: https://gist.github.com/bitprophet/794bb907dfa30460b08e\nI'm wondering if perhaps this has something to do with individual packages' sensitivity to the local setup.cfg files (which specify prefix = /opt/graphite) but that's a long shot; I/we need to poke their individual setup.py files to check on this. It would also have likely affected some carbon deps, but those were all fine.\nI also noticed that some of the /opt/graphite installed packages mention explicit /opt/graphite/bin/xxx paths in their RECORD metadata, so I thought perhaps it had to do with installing non-lib elements, but sadly no - a couple of these 'misplaced' packages lack any such elements.\nShould also try to reproduce on a non Ubuntu platform to see if it's related to Ubuntu/Debian specific Python shenanigans, as I've run into that before. I have a CentOS 6.5 instance cooking now.\n. Yea I remember having wackadoodle Python issues on Ubuntu back when I opsed on it (which is  now a few years ago).\nQuickly peeped some setup.py files and both 'good' and 'bad' packages do things like require setuptools, so I'm largely out of ideas here and would probably just be Googling terms related to ubuntu, python and prefixes until something rang a bell \u00af\\_(\u30c4)_/\u00af\nComedy option: have the checker test for this specific case, either VERY specific (\"if ubuntu -> look in /opt/graphite/<prefix> for some things\"), or just \"if not found in expected location, check some common trees inside /opt/graphite in case this issue is present\".\n. Tentative :+1:, I have the same vague concerns as @SEJeff had in his original reply, but I just checked quickly on my internal fork and don't see us using this anywhere, nor are any of our dashboardy bits (gdash, tessera etc) apparently using it.\nSo that's one data point in favor of removal, at least.\n. FWIW, while you can never fully escape the \"docs/changelogs want to exist in some form in released tarballs\" problem, I've found having the canonical versions of that data be the Web-accessible versions helps me care less about minor oopses in the packaged copies.\n(Though as I tweeted at you earlier, I am also becoming a fan of \"well, 2.1.6 had an oops in its packaging/docs? Just cut 2.1.7 a few minutes later and literally nobody will end up downloading 2.1.6 anyways\".)\nE.g. http://paramiko.org/changelog.html happens to be generated from https://raw.githubusercontent.com/paramiko/paramiko/master/sites/www/changelog.rst but I routinely find minor mistakes and just patch them so they end up online (and in subsequent releases, like a bugfix).\nSimilarly, I've ended up posting my docs in per-release-line 'versions' such as \"the docs for 1.2.x\" (example - an older release line that can still sometimes get updates) so I don't have to worry as much about version 1.2.3's docs having some stupid inaccuracy for all time as long as I update the 1.2 branch sometime.\n(This approach works best when paired with said reliable, canonical changelog, plus remembering to use .. versionadded:: / .. versionchanged:: directives [no, I don't always myself, sigh] so a user can quickly figure out if 1.2.3 has that bugfix they're after or if it was added in 1.2.5, etc.)\n\n. Shameless plug that http://contribution-guide.org might also be worth linking to or cribbing off of.\n. Upon review, I think the root bug here is actually in normalize(), and given the stacktrace in #1495, it looks like it has received a fix sometime after the HEAD that I, being an asshole, apparently left Cory with. Said fix is: 9da1d8a896ae08deaa36c2c97ea5a5710ae24c0d - with that in play I'm guessing countSeries works better.\n(Which is odd because I am 95% sure I updated the Graphite install he's using, well after 2013...but maybe something something master isn't release branch something? I'll hash it out with him.)\n\nThat said, if there's additional bugs in countSeries re: the empty-list-of-seriesLists scenario, I would argue it needs fixing and is not subject to transformNull et al, as those appear to be solely fixated on null values within single seriesList objects, which isn't the problem here.\n. @ckolbeck hm, so is something in countSeries still being poopy with an empty list (after the normalize)? That sounds like it falls under \"additional bugs\" to me. (In which case - I think this changeset here is still pretty cool so :+1: from me. the count of an empty series list certainly is 0.)\n. ",
    "arkaitzj": "Hi, I am affected by this bug as well I suspect, my stacked graphs do not fit inside the picture! Is this going to get merged?\n. ",
    "diyan": "Hi all,\nWe also need this functionality in order to host Graphite in non-root path behing Nginx.\nUnfortunately we have only single domain for monitoring-related projects such as Nagios, Graphite and Sentry.\nWould like to give thanks for @perher @titilambert and @Seb-Solon for their effort on this topic.\nWe will try to test this codebase in our environment.\n. ",
    "mprovost": "Sure is there any format that you'd like them in? It looks like the\nfunction documentation just comes out of the autodocs, but those are pretty\nshort. I can write up something longer but I'm not sure where it should go.\nOn 14 June 2013 12:42, Jeff Schroeder notifications@github.com wrote:\n\nCan you push some documentation examples to the same branch so that this\npull request is updated?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/347#issuecomment-19433782\n.\n. How's this?\n\nOn 14 June 2013 13:07, Jeff Schroeder notifications@github.com wrote:\n\nLong docstrings are fine so long as they are relevant. I like the writeup\nyou did about how you use this quite a bit. It is good to give people ideas.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/347#issuecomment-19434479\n.\n. \n",
    "perher": "I've also verified the additional changes by diyan and that worked for me as well.\n. I've also verified the additional changes by diyan and that worked for me as well.\n. ",
    "02strich": "Works like a charm for me as well, why not merge it?\n. ",
    "swehner": "Sorry - I have just seen that this has been fixed in https://github.com/graphite-project/graphite-web/commit/0c2b2670ec41c85d63b275e47e221894c3aca8b6\n. ",
    "burr86": "Done! Not entirely sure why there was a merge conflict but fixed. Try now?\n. ",
    "kpumuk": ":+1: Fixed the issue for us (Scribd)\n. ",
    "joemiller": "closing due to reverting newer code. will submit a new PR against master\n. When I tried that, I did not get the view I wanted. \nI am working on federated setup where each carbon-cache instance stores in a unique dir based on its instance name, eg:  carbon-cache:01 -> /data/whisper/01, carbon-cache:02 -> /data/whisper/02, etc. In order to make it easier to determine which shards to move when expanding the cluster.\nwith the current graphite-web code and settings:\nWHISPER_DIR = /data/whisper\nSTANDARD_DIRS = [ /data/whisper/01, /data/whisper/02, ...]\nThe graphite-web UI would render a tree that looked like:\ngraphite\n |_ 01\n   |_ servers \n      |_ server01 \\ ...\n |_ 02\n    |_ servers\n       |_server03 \\ ...\nThis is difficult to navigate, especially with the current setup of 32 shards.\nI would rather have a combined view such as:\ngraphite\n   |_ servers \n      |_ server01 \\ ...\n       |_server03 \\ ...\n. @obfuscurity Yeah I'll give it a shot. I figure my approach would be to retain the existing WHISPER_DIR variable but extend the parsing to accept a string or list of strings. Does that sound OK?\n. I don't have much time to work on this at the moment. Last I picked it up I\ngot bogged down in getting a working local dev environment up. If someone\nis interested in picking up this work and taking it across the finish line\nthat would be awesome. I don't anticipate having time to do this on my own\ntime soon, and at $dayjob we're considering outsourcing our graphite so I'm\nunlikely to have work-sponsored time either.\nOn Thu, May 18, 2017 at 12:39 AM Denis Zhdanov notifications@github.com\nwrote:\n\nIt should be rebased and altered to support backward compatibility with\nWHISPER_DIR.\n@joemiller https://github.com/joemiller - do you still have an interest\nin this?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/372#issuecomment-302326239,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAXDA_r0oq9-2ZLr73rL-pHwYzPoDs4eks5r6_W_gaJpZM4A1Q3A\n.\n. \n",
    "diceone": "Is there any update on this feature? . I would NOT include grafana or statsd/collectd. It would blow up the container image. There is an official grafana image.. ",
    "cdegroot": "Closing PR as it is probably obsolete by now.\n. ",
    "theaeolianmachine": "Yarg, this is determined statically initially, so the docs are correct. That's what I get for sending this at 2am my time...\n. ",
    "aeirola": "After having a look at the code for the renderer view, the problem seems to be that json format handler isn't expecting float values for the timeseries start, end and step fields; but instead directly feeds them to the range function which throws an exception.\nAs far as I can see, the issue can be fixed in any of these three places:\n1. Change the json handler in view.py to convert any floats to integers before passing them to range()\n2. Change start, end and step calculation for the affected functions in functions.py\n3. Change TimeSeries.__init__() to convert all values for start, end and step to integers.\nAs I'm not previously familiar with the graphite-web renderer codebase, I'm not sure which of the fixes would be the most suitable one. But the first alternative seems to be the simplest one. I'll put up a pull-request for this later on.\n. Unfortunately I don't have a testing environment set up to tyr out the fix. But looking at the code it seems to fix the problem properly. Closing the issue as fixed.\n. ",
    "yohcop": "Actually, it seems that it's not working as I thought. Please correct me if I'm wrong, or if there is another way to set things up.\nWhen Graphite gets the render request, it first checks if the metric exists by reading the CERES_DIR on disk, then tries to read as much data as possible from that location, and finally check the Carbon instances for even more fresh data.\nIf the CERES_DIR is on a network storage shared with Carbon, it works. But if this network storage is really really slow, it gets painful to render graphs. It seems that it would be much faster to get all the data and metric names from the Carbon instances instead, giving them a lot of RAM to cache a lot of data, and replicating them as much as needed. \nMaybe having dedicated \"reader\" Carbon instances with a bunch of RAM - Although this is the same as a Graphite instance that caches a lot of data too. But using the writer carbon instance would mean never have to read from disk, only writing archives (and few reads for historical archives), but most recent data would be in ram.\nThe alternative would be to periodically rsync the network storage to a local directory on the Graphite hosts, but I'm not sure that may require a lot of local disk.\nIs there a better way? What is the typical distributed setup?\n. ",
    "ralphm": "Thanks for backporting to 0.9.x, @obfuscurity. I was on holiday :-)\n. To be fair, this function was added only in the master branch, not in 0.9.x. But I do have the same question, why where the references to that function in the 0.9.x removed from docstrings, instead of backporting the function? This is the single most used function in my dashboards.\n. I looked at the original commit (3877366221ee93bfccabce736d69a4b11da25ea5) and the comments there refer to a mailing list message and that no new features will be merged into 0.9.x. However, that's from quite a while ago, and 0.9.11/12 have a received a bunch of new features.\n. ",
    "ziegeer": "No, that is what the ceres branch of graphite is intended to do.  With the default whisper files you can't do this and just have to make sure all the data for a given metric is in a single file on a single server.  You can try the ceres branch but I'm under the impression it is not yet complete and/or production ready.\n. ",
    "rkrombho": "This actually introduced a dependency to Django >= 1.4 (and by that to python >= 2.5).\nThis is sad as the release notes for Graphite 0.9.11 still state that Django 1.3 (and above) and python 2.4 should work fine.\nWithout knowing anythin about the code: is there a way to avoid the timezone module (which does not exist in Django 1.3)?\n. ",
    "astanway": "Before you merge, requirements.txt also needs to be changed. I just got into bed though so I'll add the commit in the morning. \nHowever: are we sure we totally want to drop support for Django 1.3? Seems like it might be a bit heavy handed when a simple try block on the URL imports can give us backwards compatibility.\n. ",
    "kisoku": "Any chance you ever figured out how to resolve this ? \n. ",
    "baites": "It seems there is not interest on this so I am closing the request.\n. It seems the problem did not go away. Initially looked it did so I am back to square one. I am closing this pull request until I understand the problem a bit more.\n. I am testing a new solution where I am not using a lock by rather an event (connection_event) that is set once the connection is stablished and the GET operation successful. Then I wait before line 182 for the connection_event to be set before reading the response, see\nhttps://github.com/baites/graphite-web/blob/remote_storage_fix2/webapp/graphite/remote_storage.py#L167\nLet me test this for a few days more before I make a pull request so I do not wasting anybody time.\n. Ok the new implementation seems to be working. I added a new pull request #592 with it.\n. I am close this request because I have an improved version that reduce the number of sync locks and events.\n. Thanks legionus, I had that done in my graphite repository but some reason it did not make it to the branch.\nhttps://github.com/graphite-server/graphite-web/blob/master/webapp/graphite/remote_storage.py\n. ",
    "biyanisuraj": "Got it fixed by using python 2.6\n. ",
    "abhishekwy": "+1 \n. ",
    "lyrixx": "We have also this issue.\nAnyway the commit https://github.com/graphite-project/graphite-web/commit/540fec4ccb7869a2eebec7dd234f49a055f38d5d is not in a release. Could you tag a new release.\n. ",
    "davidgiesberg": "This patch works for me on the graphs that I had been getting this error.\n. ",
    "russell": "Thanks @ajablonski this patch also worked for me!\n. Thanks for implementing the fix @jssjr.  This issue should be closed if it's fixed in master.\n. ",
    "benh57": "I have verified this works fine here, too. Fixed my issue with rendering 'no data' SVGs.\n. The perfect is the enemy of the good. Perhaps if you submitted a pull request with the 'cleaner' version, they could accept that one easily - rather than nothing happening at all due to the organizers ignoring a PR with pending 'improvements'.\n. ",
    "kuno": "@cbowman0 \nI dont know, I think it is up to graphite api, right?\nI try the multiple target entries, but graphite api return an error\npython\nTraceback (most recent call last):\n  File \"/opt/virtualenvs/graphite/local/lib/python2.7/site-packages/django/core/handlers/base.py\", line 111, in get_response\n    response = callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 104, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 10, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 21, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 29, in evaluateTokens\n    return func(requestContext, *args)\n  File \"/opt/graphite/webapp/graphite/render/functions.py\", line 1992, in exclude\n    regex = re.compile(pattern)\n  File \"/opt/virtualenvs/graphite/lib/python2.7/re.py\", line 190, in compile\n    return _compile(pattern, flags)\n  File \"/opt/virtualenvs/graphite/lib/python2.7/re.py\", line 229, in _compile\n    p = _cache.get(cachekey)\nTypeError: unhashable type: 'list'\n. sorry, I am wrong.\nIt works, thanks.\nClosing.\n. ",
    "ericachelis": "+1 - I have faced the same issue and can readily replicate.\n. I was able to work around this issue by using the offset() function and passing a negative constant:\n``` python\ndef offset(requestContext, seriesList, factor):\n  \"\"\"\n  Takes one metric or a wildcard seriesList followed by a constant, and adds the constant to\n  each datapoint.\nExample:\n    &target=offset(Server.instance01.threads.busy,10)\n  \"\"\"\n```\nExample:\noffset(path.to.metric,-100)\nThis will draw the metric with 100 subtracted from each data point.\n. ",
    "caipre": "@obfuscurity: Curious, shouldn't constantLine work? (I tried, it doesn't)\neg: diffSeries(foo.bar.baz, constantLine(1))\nPerhaps it doesn't generate a series?\n. Patient: \"It hurts when I do this.\"\nDoctor:  \"Don't do that.\"\n=)\nThe documentation doesn't seem to indicate that it would work the other way around, and the function doesn't appear to be written to support it either: https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/functions.py#L834-852\n. Patient: \"It hurts when I do this.\"\nDoctor:  \"Don't do that.\"\n=)\nThe documentation doesn't seem to indicate that it would work the other way around, and the function doesn't appear to be written to support it either: https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/functions.py#L834-852\n. Closing this; I was misunderstanding how the function works. Movement to the right is correct for moving backward in time (relative to the data of the original timeseries). It's something like this:\nf(t) = y            // raw data\ng(t, f, n) = f(t-n) // timeshifted\n. Agreed, offset seems fine.\n. ",
    "thefab": "Very useful pull request ! It works ok for me.\nWith it, it's really easy to build and plot \"memory/cpu usage percent per host\" series from collectd metrics. \nThanks !\n. The same idea would be really useful with divideSeries ?\n. ",
    "aaron2": "see pull req #447 for divideSeries implementation\n. possibly. seems to work ok for me but apparently this feature has been added recently and I should update my graphite install. please disregard\n. I am not using a relay. this is in carbon-cache\n. from client.py:\ndef takeSomeFromQueue(self):\n    datapoints = self.queue[:settings.MAX_DATAPOINTS_PER_MESSAGE]\n    self.queue = self.queue[settings.MAX_DATAPOINTS_PER_MESSAGE:]\n    return datapoints\n. based on the 2 lines of code above it would seem the remaining items are left in the queue and not dropped. which makes this setting name and its docs kind of misleading but it also means the problem I am experiencing is somewhere else. I appreciate the fast response and humbly withdraw my bug report\n. ",
    "zsprackett": "What's needed to get this merged?\n. ",
    "nelhage": "I'm running with nelhage/carbon@e5eea1b4e3a747e31daa037af87f433ba0f45011 as a workaround -- it makes carbon return a list instead of deque. Since graphite immediately calls list() on the output from carbon-cache anyways, this has no functional effect, but lets the data pass through the safe unpickler.\n. ",
    "bzed": "0.9.12 didn't fix it for me, I've added deque to PICKLE_SAFE as described above.\n. ",
    "codecov-io": "Codecov Report\n\nMerging #424 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster     #424   +/-\n=======================================\n  Coverage   71.78%   71.78%         \n=======================================\n  Files          78       78         \n  Lines        7574     7574         \n  Branches     1480     1480         \n=======================================\n  Hits         5437     5437         \n  Misses       1881     1881         \n  Partials      256      256\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b9a39bb...238b265. Read the comment docs.\n. ## Current coverage is 37.67%\nMerging #645 into master will not change coverage\n\ndiff\n@@             master       #645   diff @@\n==========================================\n  Files            51         51          \n  Lines          5771       5771          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1117       1116     -1   \n==========================================\n  Hits           2174       2174          \n  Misses         3400       3400          \n  Partials        197        197\n\nPowered by Codecov. Last updated by 51118d7...3f6a775\n. ## Current coverage is 37.65%\nMerging #660 into master will decrease coverage by <.01%\n\ndiff\n@@             master       #660   diff @@\n==========================================\n  Files            51         51          \n  Lines          5771       5782    +11   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1116       1120     +4   \n==========================================\n+ Hits           2174       2177     +3   \n- Misses         3400       3405     +5   \n- Partials        197        200     +3\n\nPowered by Codecov. Last updated by 1be88cd...1c2e2bf\n. ## Current coverage is 37.38%\nMerging #935 into master will increase coverage by 0.53%\n\ndiff\n@@             master       #935   diff @@\n==========================================\n  Files            51         51          \n  Lines          5729       5752    +23   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1116       1120     +4   \n==========================================\n+ Hits           2111       2150    +39   \n+ Misses         3422       3406    -16   \n  Partials        196        196\n\nPowered by Codecov. Last updated by c4ff0b2...0b94cdc\n. ## Current coverage is 66.07% (diff: 100%)\nMerging #1290 into master will increase coverage by 0.30%\n\ndiff\n@@             master      #1290   diff @@\n==========================================\n  Files            52         53     +1   \n  Lines          5977       6010    +33   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1174       1180     +6   \n==========================================\n+ Hits           3931       3971    +40   \n+ Misses         1839       1832     -7   \n  Partials        207        207\n\nPowered by Codecov. Last update a447075...e5d6aae\n. ## Current coverage is 65.30% (diff: 100%)\nMerging #1439 into master will increase coverage by 0.03%\n\ndiff\n@@             master      #1439   diff @@\n==========================================\n  Files            52         52          \n  Lines          5842       5848     +6   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1131       1133     +2   \n==========================================\n+ Hits           3813       3819     +6   \n  Misses         1827       1827          \n  Partials        202        202\n\nPowered by Codecov. Last update 093999f...637a018\n. ## Current coverage is 36.96%\nMerging #1512 into master will increase coverage by +<.01%\n\ndiff\n@@           master      #1512   diff @@\n========================================\n  Files          51         51          \n  Lines        5712       5712          \n  Methods         0          0          \n  Branches     1111       1111          \n========================================\n  Hits         2111       2111          \n+ Misses       3410       3407     -3   \n- Partials      191        194     +3\n1. File .../render/functions.py (not in diff) was modified. more \n   - Misses -3 \n   - Partials +3 \n   - Hits 0\n\nPowered by Codecov. Last updated by b30f46c\n. ## Current coverage is 36.96%\nMerging #1515 into master will not change coverage\n\ndiff\n@@             master      #1515   diff @@\n==========================================\n  Files            51         51          \n  Lines          5712       5712          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1111       1111          \n==========================================\n  Hits           2111       2111          \n  Misses         3407       3407          \n  Partials        194        194\n\nPowered by Codecov. Last updated by d9dd787...9c36880\n. ## Current coverage is 37.67%\nMerging #1524 into master will not change coverage\n\ndiff\n@@             master      #1524   diff @@\n==========================================\n  Files            51         51          \n  Lines          5771       5771          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1117       1116     -1   \n==========================================\n  Hits           2174       2174          \n  Misses         3400       3400          \n  Partials        197        197\n\nPowered by Codecov. Last updated by 51118d7...488b0f9\n. ## Current coverage is 49.40%\nMerging #1529 into master will increase coverage by 3.53%\n\ndiff\n@@             master      #1529   diff @@\n==========================================\n  Files            52         52          \n  Lines          5790       5779    -11   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1116       1111     -5   \n==========================================\n+ Hits           2656       2855   +199   \n+ Misses         2927       2713   -214   \n- Partials        207        211     +4\n\nPowered by Codecov. Last updated by c44feea...b4fe3b2\n. ## Current coverage is 37.79%\nMerging #1530 into master will increase coverage by 0.08%\n\ndiff\n@@             master      #1530   diff @@\n==========================================\n  Files            51         51          \n  Lines          5771       5771          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1116       1116          \n==========================================\n+ Hits           2174       2181     +7   \n+ Misses         3400       3393     -7   \n  Partials        197        197\n\nPowered by Codecov. Last updated by 1be88cd...dc1bf89\n. ## Current coverage is 40.86%\nMerging #1533 into master will increase coverage by 3.19%\n\ndiff\n@@             master      #1533   diff @@\n==========================================\n  Files            51         51          \n  Lines          5771       5777     +6   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1116       1116          \n==========================================\n+ Hits           2174       2361   +187   \n+ Misses         3400       3201   -199   \n- Partials        197        215    +18\n\nPowered by Codecov. Last updated by 1be88cd...50eea61\n. ## Current coverage is 38.29%\nMerging #1534 into master will increase coverage by 0.62%\n\ndiff\n@@             master      #1534   diff @@\n==========================================\n  Files            51         51          \n  Lines          5771       5771          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1116       1116          \n==========================================\n+ Hits           2174       2210    +36   \n+ Misses         3400       3361    -39   \n- Partials        197        200     +3\n\nPowered by Codecov. Last updated by 1be88cd...695043f\n. ## Current coverage is 44.88%\nMerging #1537 into master will increase coverage by 3.39%\n\ndiff\n@@             master      #1537   diff @@\n==========================================\n  Files            51         51          \n  Lines          5777       5781     +4   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1116       1116          \n==========================================\n+ Hits           2397       2595   +198   \n+ Misses         3162       2964   -198   \n- Partials        218        222     +4\n\nPowered by Codecov. Last updated by e876fba...355176a\n. ## Current coverage is 44.54%\nMerging #1540 into master will increase coverage by 2.92%\n\ndiff\n@@             master      #1540   diff @@\n==========================================\n  Files            51         51          \n  Lines          5777       5781     +4   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1116       1116          \n==========================================\n+ Hits           2404       2575   +171   \n+ Misses         3155       2977   -178   \n- Partials        218        229    +11\n\nPowered by Codecov. Last updated by 46feba1...40236b3\n. ## Current coverage is 45.50%\nMerging #1542 into master will increase coverage by 0.96%\n\ndiff\n@@             master      #1542   diff @@\n==========================================\n  Files            51         52     +1   \n  Lines          5781       5788     +7   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1116       1116          \n==========================================\n+ Hits           2575       2634    +59   \n+ Misses         2977       2940    -37   \n+ Partials        229        214    -15\n\nPowered by Codecov. Last updated by 41b7bcc...3678479\n. ## Current coverage is 45.87%\nMerging #1544 into master will not change coverage\n\ndiff\n@@             master      #1544   diff @@\n==========================================\n  Files            52         52          \n  Lines          5790       5790          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1116       1116          \n==========================================\n  Hits           2656       2656          \n  Misses         2927       2927          \n  Partials        207        207\n\nPowered by Codecov. Last updated by 1e54b89...0970b08\n. ## Current coverage is 45.82%\nMerging #1546 into master will decrease coverage by 0.03%\n\ndiff\n@@             master      #1546   diff @@\n==========================================\n  Files            52         52          \n  Lines          5788       5790     +2   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1116       1116          \n==========================================\n- Hits           2654       2653     -1   \n- Misses         2927       2929     +2   \n- Partials        207        208     +1\n\nPowered by Codecov. Last updated by 42d6b4f...e70f2a4\n. ## Current coverage is 45.87%\nMerging #1547 into master will not change coverage\n\ndiff\n@@             master      #1547   diff @@\n==========================================\n  Files            52         52          \n  Lines          5790       5790          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1116       1116          \n==========================================\n  Hits           2656       2656          \n  Misses         2927       2927          \n  Partials        207        207\n\nPowered by Codecov. Last updated by 8e206f9...02f5b99\n. ## Current coverage is 49.41%\nMerging #1548 into master will increase coverage by 3.54%\n\ndiff\n@@             master      #1548   diff @@\n==========================================\n  Files            52         52          \n  Lines          5790       5800    +10   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1116       1119     +3   \n==========================================\n+ Hits           2656       2866   +210   \n+ Misses         2927       2725   -202   \n- Partials        207        209     +2\n\nPowered by Codecov. Last updated by c44feea...aa738dc\n. ## Current coverage is 47.20%\nMerging #1549 into master will increase coverage by 1.33%\n\ndiff\n@@             master      #1549   diff @@\n==========================================\n  Files            52         52          \n  Lines          5790       5792     +2   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1116       1118     +2   \n==========================================\n+ Hits           2656       2734    +78   \n+ Misses         2927       2854    -73   \n+ Partials        207        204     -3\n\nPowered by Codecov. Last updated by c44feea...1220a77\n. ## Current coverage is 47.55%\nMerging #1551 into master will increase coverage by 0.34%\n\ndiff\n@@             master      #1551   diff @@\n==========================================\n  Files            52         52          \n  Lines          5792       5796     +4   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1118       1120     +2   \n==========================================\n+ Hits           2734       2756    +22   \n+ Misses         2854       2833    -21   \n- Partials        204        207     +3\n\nPowered by Codecov. Last updated by 0ddff54...f7d9981\n. ## Current coverage is 47.20%\nMerging #1552 into master will not change coverage\n\ndiff\n@@             master      #1552   diff @@\n==========================================\n  Files            52         52          \n  Lines          5792       5792          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1118       1118          \n==========================================\n  Hits           2734       2734          \n  Misses         2854       2854          \n  Partials        204        204\n\nPowered by Codecov. Last updated by 0ddff54...aafa977\n. ## Current coverage is 47.20%\nMerging #1552 into master will not change coverage\n\ndiff\n@@             master      #1552   diff @@\n==========================================\n  Files            52         52          \n  Lines          5792       5792          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1118       1118          \n==========================================\n  Hits           2734       2734          \n  Misses         2854       2854          \n  Partials        204        204\n\nPowered by Codecov. Last updated by 0ddff54...aafa977\n. ## Current coverage is 48.75%\nMerging #1554 into master will increase coverage by 1.20%\n\ndiff\n@@             master      #1554   diff @@\n==========================================\n  Files            52         52          \n  Lines          5796       5796          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1120       1120          \n==========================================\n+ Hits           2756       2826    +70   \n+ Misses         2833       2775    -58   \n+ Partials        207        195    -12\n\nPowered by Codecov. Last updated by 9424b43...82844af\n. ## Current coverage is 47.55%\nMerging #1555 into master will not change coverage\n\ndiff\n@@             master      #1555   diff @@\n==========================================\n  Files            52         52          \n  Lines          5796       5796          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1120       1120          \n==========================================\n  Hits           2756       2756          \n  Misses         2833       2833          \n  Partials        207        207\n\nPowered by Codecov. Last updated by 9424b43...f6677b4\n. ## Current coverage is 47.50%\nMerging #1556 into master will decrease coverage by 0.04%\n\ndiff\n@@             master      #1556   diff @@\n==========================================\n  Files            52         52          \n  Lines          5796       5801     +5   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1120       1120          \n==========================================\n  Hits           2756       2756          \n- Misses         2833       2838     +5   \n  Partials        207        207\n\nPowered by Codecov. Last updated by 9424b43...0478beb\n. ## Current coverage is 47.68%\nMerging #1559 into master will increase coverage by 0.13%\n\ndiff\n@@             master      #1559   diff @@\n==========================================\n  Files            52         52          \n  Lines          5796       5796          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1120       1120          \n==========================================\n+ Hits           2756       2764     +8   \n+ Misses         2833       2826     -7   \n+ Partials        207        206     -1\n\nPowered by Codecov. Last updated by 9424b43...d2f9672\n. ## Current coverage is 47.65%\nMerging #1561 into master will increase coverage by 0.10%\n\ndiff\n@@             master      #1561   diff @@\n==========================================\n  Files            52         52          \n  Lines          5796       5796          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1120       1120          \n==========================================\n+ Hits           2756       2762     +6   \n+ Misses         2833       2826     -7   \n- Partials        207        208     +1\n\nPowered by Codecov. Last updated by 9424b43...db3c749\n. ## Current coverage is 47.55%\nMerging #1563 into master will not change coverage\n\ndiff\n@@             master      #1563   diff @@\n==========================================\n  Files            52         52          \n  Lines          5796       5796          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1120       1120          \n==========================================\n  Hits           2756       2756          \n  Misses         2833       2833          \n  Partials        207        207\n\nPowered by Codecov. Last updated by 9424b43...c05ac81\n. ## Current coverage is 47.55%\nMerging #1564 into master will increase coverage by <.01%\n\ndiff\n@@             master      #1564   diff @@\n==========================================\n  Files            52         52          \n  Lines          5796       5800     +4   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1120       1121     +1   \n==========================================\n+ Hits           2756       2758     +2   \n- Misses         2833       2835     +2   \n  Partials        207        207\n\nPowered by Codecov. Last updated by bd2bd0f...49b2646\n. ## Current coverage is 47.73%\nMerging #1565 into master will increase coverage by 0.18%\n\ndiff\n@@             master      #1565   diff @@\n==========================================\n  Files            52         52          \n  Lines          5796       5780    -16   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1120       1113     -7   \n==========================================\n+ Hits           2756       2759     +3   \n+ Misses         2833       2814    -19   \n  Partials        207        207\n\nPowered by Codecov. Last updated by 2b02527...b02678c\n. ## Current coverage is 51.01%\nMerging #1566 into master will increase coverage by 3.46%\n\ndiff\n@@             master      #1566   diff @@\n==========================================\n  Files            52         52          \n  Lines          5796       5808    +12   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1120       1123     +3   \n==========================================\n+ Hits           2756       2963   +207   \n+ Misses         2833       2635   -198   \n- Partials        207        210     +3\n\nPowered by Codecov. Last updated by 2b02527...6e6c77a\n. ## Current coverage is 56.25%\nMerging #1567 into master will not change coverage\n\ndiff\n@@             master      #1567   diff @@\n==========================================\n  Files            52         52          \n  Lines          5781       5781          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1111       1111          \n==========================================\n  Hits           3252       3252          \n  Misses         2328       2328          \n  Partials        201        201\n\nPowered by Codecov. Last updated by d0295a2...34f7a9b\n. ## Current coverage is 56.31%\nMerging #1568 into master will increase coverage by 0.06%\n\ndiff\n@@             master      #1568   diff @@\n==========================================\n  Files            52         52          \n  Lines          5781       5789     +8   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1111       1113     +2   \n==========================================\n+ Hits           3252       3260     +8   \n  Misses         2328       2328          \n  Partials        201        201\n\nPowered by Codecov. Last updated by d0295a2...3fe38bd\n. ## Current coverage is 56.31%\nMerging #1568 into master will increase coverage by 0.06%\n\ndiff\n@@             master      #1568   diff @@\n==========================================\n  Files            52         52          \n  Lines          5781       5789     +8   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1111       1113     +2   \n==========================================\n+ Hits           3252       3260     +8   \n  Misses         2328       2328          \n  Partials        201        201\n\nPowered by Codecov. Last updated by d0295a2...3fe38bd\n. ## Current coverage is 56.25%\nMerging #1570 into master will not change coverage\n\ndiff\n@@             master      #1570   diff @@\n==========================================\n  Files            52         52          \n  Lines          5781       5781          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1111       1111          \n==========================================\n  Hits           3252       3252          \n  Misses         2328       2328          \n  Partials        201        201\n\nPowered by Codecov. Last updated by d0295a2...e49cf78\n. ## Current coverage is 56.73%\nMerging #1572 into master will increase coverage by 0.47%\n\ndiff\n@@             master      #1572   diff @@\n==========================================\n  Files            52         52          \n  Lines          5781       5780     -1   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1111       1111          \n==========================================\n+ Hits           3252       3279    +27   \n+ Misses         2328       2300    -28   \n  Partials        201        201\n\nPowered by Codecov. Last updated by d0295a2...cddfe40\n. ## Current coverage is 56.78%\nMerging #1576 into master will not change coverage\n\ndiff\n@@             master      #1576   diff @@\n==========================================\n  Files            52         52          \n  Lines          5780       5780          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1111       1111          \n==========================================\n  Hits           3282       3282          \n  Misses         2295       2295          \n  Partials        203        203\n\nPowered by Codecov. Last updated by 155345f...1b02d77\n. ## Current coverage is 56.78%\nMerging #1577 into master will not change coverage\n\ndiff\n@@             master      #1577   diff @@\n==========================================\n  Files            52         52          \n  Lines          5780       5780          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1111       1111          \n==========================================\n  Hits           3282       3282          \n  Misses         2295       2295          \n  Partials        203        203\n\nPowered by Codecov. Last updated by 1b46bbd...16e653f\n. ## Current coverage is 56.78%\nMerging #1579 into master will not change coverage\n\ndiff\n@@             master      #1579   diff @@\n==========================================\n  Files            52         52          \n  Lines          5780       5780          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1111       1111          \n==========================================\n  Hits           3282       3282          \n  Misses         2295       2295          \n  Partials        203        203\n\nPowered by Codecov. Last updated by e95555a...427052e\n. ## Current coverage is 56.78%\nMerging #1580 into master will not change coverage\n\ndiff\n@@             master      #1580   diff @@\n==========================================\n  Files            52         52          \n  Lines          5780       5780          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1111       1111          \n==========================================\n  Hits           3282       3282          \n  Misses         2295       2295          \n  Partials        203        203\n\nPowered by Codecov. Last updated by e95555a...d70da8a\n. ## Current coverage is 56.84%\nMerging #1583 into master will not change coverage\n\ndiff\n@@             master      #1583   diff @@\n==========================================\n  Files            52         52          \n  Lines          5788       5788          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1113       1113          \n==========================================\n  Hits           3290       3290          \n  Misses         2295       2295          \n  Partials        203        203\n\nPowered by Codecov. Last updated by ea2e103...d8dab8a\n. ## Current coverage is 56.84%\nMerging #1584 into master will increase coverage by <.01%\n\ndiff\n@@             master      #1584   diff @@\n==========================================\n  Files            52         52          \n  Lines          5788       5789     +1   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1113       1113          \n==========================================\n+ Hits           3290       3291     +1   \n  Misses         2295       2295          \n  Partials        203        203\n\nPowered by Codecov. Last updated by 02832d4...e23e434\n. ## Current coverage is 56.96%\nMerging #1585 into master will increase coverage by 0.10%\n\ndiff\n@@             master      #1585   diff @@\n==========================================\n  Files            52         52          \n  Lines          5786       5786          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1113       1113          \n==========================================\n+ Hits           3290       3296     +6   \n+ Misses         2293       2287     -6   \n  Partials        203        203\n\nPowered by Codecov. Last updated by 07cd9bf...277836f\n. ## Current coverage is 56.96%\nMerging #1585 into master will increase coverage by 0.10%\n\ndiff\n@@             master      #1585   diff @@\n==========================================\n  Files            52         52          \n  Lines          5786       5786          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1113       1113          \n==========================================\n+ Hits           3290       3296     +6   \n+ Misses         2293       2287     -6   \n  Partials        203        203\n\nPowered by Codecov. Last updated by 07cd9bf...277836f\n. ## Current coverage is 58.98%\nMerging #1586 into master will increase coverage by 2.02%\n\ndiff\n@@             master      #1586   diff @@\n==========================================\n  Files            52         52          \n  Lines          5786       5786          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1113       1113          \n==========================================\n+ Hits           3296       3413   +117   \n+ Misses         2287       2170   -117   \n  Partials        203        203\n\nPowered by Codecov. Last updated by b4eb6cf...112268b\n. ## Current coverage is 58.98%\nMerging #1586 into master will increase coverage by 2.02%\n\ndiff\n@@             master      #1586   diff @@\n==========================================\n  Files            52         52          \n  Lines          5786       5786          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1113       1113          \n==========================================\n+ Hits           3296       3413   +117   \n+ Misses         2287       2170   -117   \n  Partials        203        203\n\nPowered by Codecov. Last updated by b4eb6cf...112268b\n. ## Current coverage is 58.99%\nMerging #1590 into master will increase coverage by 0.01%\n\ndiff\n@@             master      #1590   diff @@\n==========================================\n  Files            52         52          \n  Lines          5786       5790     +4   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1113       1114     +1   \n==========================================\n+ Hits           3413       3416     +3   \n- Misses         2170       2171     +1   \n  Partials        203        203\n\nPowered by Codecov. Last updated by e4e02cd...2529831\n. ## Current coverage is 58.98%\nMerging #1592 into master will not change coverage\n\ndiff\n@@             master      #1592   diff @@\n==========================================\n  Files            52         52          \n  Lines          5786       5786          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1113       1113          \n==========================================\n  Hits           3413       3413          \n  Misses         2170       2170          \n  Partials        203        203\n\nPowered by Codecov. Last updated by 774c9aa...539dd41\n. ## Current coverage is 58.99%\nMerging #1593 into master will not change coverage\n\ndiff\n@@             master      #1593   diff @@\n==========================================\n  Files            52         52          \n  Lines          5790       5790          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1114       1114          \n==========================================\n  Hits           3416       3416          \n  Misses         2171       2171          \n  Partials        203        203\n\nPowered by Codecov. Last updated by 3350864...5a4e284\n. ## Current coverage is 60.22%\nMerging #1594 into master will increase coverage by 1.22%\n\ndiff\n@@             master      #1594   diff @@\n==========================================\n  Files            52         52          \n  Lines          5790       5790          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1114       1114          \n==========================================\n+ Hits           3416       3487    +71   \n+ Misses         2171       2104    -67   \n+ Partials        203        199     -4\n\nPowered by Codecov. Last updated by 92bc8d3...abc6338\n. ## Current coverage is 60.06%\nMerging #1599 into master will not change coverage\n\ndiff\n@@             master      #1599   diff @@\n==========================================\n  Files            52         52          \n  Lines          5804       5804          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1118       1118          \n==========================================\n  Hits           3486       3486          \n  Misses         2118       2118          \n  Partials        200        200\n\nPowered by Codecov. Last updated by 79d54c8...b33266d\n. ## Current coverage is 60.54%\nMerging #1600 into master will increase coverage by 0.48%\n\ndiff\n@@             master      #1600   diff @@\n==========================================\n  Files            52         52          \n  Lines          5804       5804          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1119       1119          \n==========================================\n+ Hits           3486       3514    +28   \n+ Misses         2118       2092    -26   \n+ Partials        200        198     -2\n\nPowered by Codecov. Last updated by 904071b...ae4e8f1\n. ## Current coverage is 61.00%\nMerging #1602 into master will increase coverage by 0.94%\n\ndiff\n@@             master      #1602   diff @@\n==========================================\n  Files            52         52          \n  Lines          5804       5804          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1119       1119          \n==========================================\n+ Hits           3486       3541    +55   \n+ Misses         2118       2065    -53   \n+ Partials        200        198     -2\n\nPowered by Codecov. Last updated by 904071b...f343a7e\n. ## Current coverage is 61.73%\nMerging #1603 into master will increase coverage by 1.18%\n\ndiff\n@@             master      #1603   diff @@\n==========================================\n  Files            52         52          \n  Lines          5804       5804          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1119       1119          \n==========================================\n+ Hits           3514       3583    +69   \n+ Misses         2092       2026    -66   \n+ Partials        198        195     -3\n\nPowered by Codecov. Last updated by 44326ec...73b6198\n. ## Current coverage is 65.17%\nMerging #1604 into master will increase coverage by 2.94%\n\ndiff\n@@             master      #1604   diff @@\n==========================================\n  Files            52         52          \n  Lines          5804       5804          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1119       1119          \n==========================================\n+ Hits           3612       3783   +171   \n+ Misses         1998       1822   -176   \n- Partials        194        199     +5\n\nPowered by Codecov. Last updated by 3e04c5f...bfcbdfb\n. ## Current coverage is 65.26%\nMerging #1605 into master will increase coverage by 0.08%\n\ndiff\n@@             master      #1605   diff @@\n==========================================\n  Files            52         52          \n  Lines          5804       5836    +32   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1119       1128     +9   \n==========================================\n+ Hits           3783       3809    +26   \n- Misses         1822       1825     +3   \n- Partials        199        202     +3\n\nPowered by Codecov. Last updated by cb7414b...09f5da3\n. ## Current coverage is 65.29%\nMerging #1607 into master will increase coverage by <.01%\n\ndiff\n@@             master      #1607   diff @@\n==========================================\n  Files            52         52          \n  Lines          5839       5840     +1   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1129       1130     +1   \n==========================================\n+ Hits           3812       3813     +1   \n  Misses         1825       1825          \n  Partials        202        202\n\nPowered by Codecov. Last updated by 1b09219...1a6a54c\n. ## Current coverage is 65.19%\nMerging #1608 into master will increase coverage by 0.01%\n\ndiff\n@@             master      #1608   diff @@\n==========================================\n  Files            52         52          \n  Lines          5804       5807     +3   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1119       1120     +1   \n==========================================\n+ Hits           3783       3786     +3   \n  Misses         1822       1822          \n  Partials        199        199\n\nPowered by Codecov. Last updated by cb7414b...2a86d72\n. ## Current coverage is 65.28%\nMerging #1610 into master will not change coverage\n\ndiff\n@@             master      #1610   diff @@\n==========================================\n  Files            52         52          \n  Lines          5839       5839          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1129       1129          \n==========================================\n  Hits           3812       3812          \n  Misses         1825       1825          \n  Partials        202        202\n\nPowered by Codecov. Last updated by 1b09219...532c544\n. ## Current coverage is 65.27% (diff: 100%)\nMerging #1611 into master will decrease coverage by 0.02%\n\ndiff\n@@             master      #1611   diff @@\n==========================================\n  Files            52         52          \n  Lines          5840       5848     +8   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1130       1134     +4   \n==========================================\n+ Hits           3813       3817     +4   \n- Misses         1825       1827     +2   \n- Partials        202        204     +2\n\nPowered by Codecov. Last update ea0d895...6106bc8\n. ## Current coverage is 65.14% (diff: 100%)\nMerging #1612 into master will decrease coverage by 0.12%\n\ndiff\n@@             master      #1612   diff @@\n==========================================\n  Files            52         52          \n  Lines          5842       5853    +11   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1131       1137     +6   \n==========================================\n  Hits           3813       3813          \n- Misses         1827       1836     +9   \n- Partials        202        204     +2\n\nPowered by Codecov. Last update 093999f...d07cd5b\n. ## Current coverage is 65.16% (diff: 100%)\nMerging #1613 into master will not change coverage\n\ndiff\n@@             master      #1613   diff @@\n==========================================\n  Files            52         52          \n  Lines          5867       5867          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1143       1143          \n==========================================\n  Hits           3823       3823          \n  Misses         1838       1838          \n  Partials        206        206\n\nPowered by Codecov. Last update aad8787...7a20a1e\n. ## Current coverage is 65.12% (diff: 0.00%)\nMerging #1614 into master will decrease coverage by 0.03%\n\ndiff\n@@             master      #1614   diff @@\n==========================================\n  Files            52         52          \n  Lines          5867       5867          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1143       1143          \n==========================================\n- Hits           3823       3821     -2   \n- Misses         1838       1840     +2   \n  Partials        206        206\n\nPowered by Codecov. Last update b6cad02...a16c767\n. ## Current coverage is 64.89% (diff: 100%)\nMerging #1620 into master will not change coverage\n\ndiff\n@@             master      #1620   diff @@\n==========================================\n  Files            52         52          \n  Lines          5893       5893          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1152       1152          \n==========================================\n  Hits           3824       3824          \n  Misses         1863       1863          \n  Partials        206        206\n\nPowered by Codecov. Last update f852903...08ca585\n. ## Current coverage is 65.16% (diff: 100%)\nMerging #1622 into master will increase coverage by 0.27%\n\ndiff\n@@             master      #1622   diff @@\n==========================================\n  Files            52         52          \n  Lines          5893       5893          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1152       1152          \n==========================================\n+ Hits           3824       3840    +16   \n+ Misses         1863       1845    -18   \n- Partials        206        208     +2\n\nPowered by Codecov. Last update 0aebf6b...544f63b\n. ## Current coverage is 65.16% (diff: 100%)\nMerging #1622 into master will increase coverage by 0.27%\n\ndiff\n@@             master      #1622   diff @@\n==========================================\n  Files            52         52          \n  Lines          5893       5893          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1152       1152          \n==========================================\n+ Hits           3824       3840    +16   \n+ Misses         1863       1845    -18   \n- Partials        206        208     +2\n\nPowered by Codecov. Last update 0aebf6b...544f63b\n. ## Current coverage is 65.09% (diff: 63.23%)\nMerging #1623 into master will decrease coverage by 0.06%\n\ndiff\n@@             master      #1623   diff @@\n==========================================\n  Files            52         52          \n  Lines          5893       5948    +55   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1152       1172    +20   \n==========================================\n+ Hits           3840       3872    +32   \n- Misses         1845       1863    +18   \n- Partials        208        213     +5\n\nPowered by Codecov. Last update b09349f...a62067c\n. ## Current coverage is 65.29% (diff: 100%)\nMerging #1624 into master will increase coverage by 0.13%\n\ndiff\n@@             master      #1624   diff @@\n==========================================\n  Files            52         52          \n  Lines          5893       5893          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1152       1152          \n==========================================\n+ Hits           3840       3848     +8   \n+ Misses         1845       1838     -7   \n+ Partials        208        207     -1\n\nPowered by Codecov. Last update b09349f...5c28f6b\n. ## Current coverage is 65.35% (diff: 100%)\nMerging #1626 into master will increase coverage by 0.05%\n\ndiff\n@@             master      #1626   diff @@\n==========================================\n  Files            52         52          \n  Lines          5893       5903    +10   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1152       1155     +3   \n==========================================\n+ Hits           3848       3858    +10   \n  Misses         1838       1838          \n  Partials        207        207\n\nPowered by Codecov. Last update 2b8709d...bf869f8\n. ## Current coverage is 65.32% (diff: 100%)\nMerging #1627 into master will increase coverage by 0.02%\n\ndiff\n@@             master      #1627   diff @@\n==========================================\n  Files            52         52          \n  Lines          5893       5898     +5   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1152       1154     +2   \n==========================================\n+ Hits           3848       3853     +5   \n  Misses         1838       1838          \n  Partials        207        207\n\nPowered by Codecov. Last update 2b8709d...9780d57\n. ## Current coverage is 65.32% (diff: 100%)\nMerging #1628 into master will not change coverage\n\ndiff\n@@             master      #1628   diff @@\n==========================================\n  Files            52         52          \n  Lines          5898       5898          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1154       1154          \n==========================================\n  Hits           3853       3853          \n  Misses         1838       1838          \n  Partials        207        207\n\nPowered by Codecov. Last update 05e00f4...75f3be9\n. ## Current coverage is 65.40% (diff: 100%)\nMerging #1629 into master will increase coverage by 0.08%\n\ndiff\n@@             master      #1629   diff @@\n==========================================\n  Files            52         52          \n  Lines          5898       5912    +14   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1154       1161     +7   \n==========================================\n+ Hits           3853       3867    +14   \n  Misses         1838       1838          \n  Partials        207        207\n\nPowered by Codecov. Last update ad8848a...97b7206\n. ## Current coverage is 65.47% (diff: 100%)\nMerging #1630 into master will increase coverage by 0.06%\n\ndiff\n@@             master      #1630   diff @@\n==========================================\n  Files            52         52          \n  Lines          5912       5923    +11   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1161       1163     +2   \n==========================================\n+ Hits           3867       3878    +11   \n  Misses         1838       1838          \n  Partials        207        207\n\nPowered by Codecov. Last update 44abc6b...a92a278\n. ## Current coverage is 65.57% (diff: 100%)\nMerging #1631 into master will increase coverage by 0.09%\n\ndiff\n@@             master      #1631   diff @@\n==========================================\n  Files            52         52          \n  Lines          5923       5940    +17   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1163       1166     +3   \n==========================================\n+ Hits           3878       3895    +17   \n  Misses         1838       1838          \n  Partials        207        207\n\nPowered by Codecov. Last update df344b9...68f03ea\n. ## Current coverage is 65.47% (diff: 100%)\nMerging #1632 into master will not change coverage\n\ndiff\n@@             master      #1632   diff @@\n==========================================\n  Files            52         52          \n  Lines          5923       5923          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1163       1163          \n==========================================\n  Hits           3878       3878          \n  Misses         1838       1838          \n  Partials        207        207\n\nPowered by Codecov. Last update df344b9...aa360ec\n. ## Current coverage is 65.48% (diff: 100%)\nMerging #1633 into master will increase coverage by 0.01%\n\ndiff\n@@             master      #1633   diff @@\n==========================================\n  Files            52         52          \n  Lines          5923       5925     +2   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1163       1163          \n==========================================\n+ Hits           3878       3880     +2   \n  Misses         1838       1838          \n  Partials        207        207\n\nPowered by Codecov. Last update 577765e...2e88083\n. ## Current coverage is 65.48% (diff: 100%)\nMerging #1634 into master will not change coverage\n\ndiff\n@@             master      #1634   diff @@\n==========================================\n  Files            52         52          \n  Lines          5925       5925          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1163       1163          \n==========================================\n  Hits           3880       3880          \n  Misses         1838       1838          \n  Partials        207        207\n\nPowered by Codecov. Last update cea02d9...438db5e\n. ## Current coverage is 65.48% (diff: 100%)\nMerging #1636 into master will not change coverage\n\ndiff\n@@             master      #1636   diff @@\n==========================================\n  Files            52         52          \n  Lines          5925       5925          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1163       1163          \n==========================================\n  Hits           3880       3880          \n  Misses         1838       1838          \n  Partials        207        207\n\nPowered by Codecov. Last update 2f0172f...72f457e\n. ## Current coverage is 65.61% (diff: 100%)\nMerging #1637 into master will increase coverage by 0.04%\n\ndiff\n@@             master      #1637   diff @@\n==========================================\n  Files            52         52          \n  Lines          5940       5947     +7   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1166       1169     +3   \n==========================================\n+ Hits           3895       3902     +7   \n  Misses         1838       1838          \n  Partials        207        207\n\nPowered by Codecov. Last update b8e0211...ab7e902\n. ## Current coverage is 65.67% (diff: 74.07%)\nMerging #1638 into master will increase coverage by 0.06%\n\ndiff\n@@             master      #1638   diff @@\n==========================================\n  Files            52         52          \n  Lines          5947       5970    +23   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1169       1172     +3   \n==========================================\n+ Hits           3902       3921    +19   \n- Misses         1838       1839     +1   \n- Partials        207        210     +3\n\nPowered by Codecov. Last update 92bd68a...c06148a\n. ## Current coverage is 65.69% (diff: 100%)\nMerging #1639 into master will increase coverage by 0.08%\n\ndiff\n@@             master      #1639   diff @@\n==========================================\n  Files            52         52          \n  Lines          5947       5953     +6   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1169       1171     +2   \n==========================================\n+ Hits           3902       3911     +9   \n  Misses         1838       1838          \n+ Partials        207        204     -3\n\nPowered by Codecov. Last update 92bd68a...8f496e2\n. ## Current coverage is 65.68% (diff: 100%)\nMerging #1642 into master will increase coverage by <.01%\n\ndiff\n@@             master      #1642   diff @@\n==========================================\n  Files            52         52          \n  Lines          5970       5971     +1   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1172       1172          \n==========================================\n+ Hits           3921       3922     +1   \n  Misses         1839       1839          \n  Partials        210        210\n\nPowered by Codecov. Last update 7f7254c...b2da214\n. ## Current coverage is 65.82% (diff: 78.12%)\nMerging #1643 into master will increase coverage by 0.14%\n\ndiff\n@@             master      #1643   diff @@\n==========================================\n  Files            52         53     +1   \n  Lines          5971       5990    +19   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1172       1178     +6   \n==========================================\n+ Hits           3922       3943    +21   \n+ Misses         1839       1838     -1   \n+ Partials        210        209     -1\n\nPowered by Codecov. Last update 00c961b...d734078\n. ## Current coverage is 65.68% (diff: 100%)\nMerging #1644 into master will not change coverage\n\ndiff\n@@             master      #1644   diff @@\n==========================================\n  Files            52         52          \n  Lines          5971       5971          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1172       1172          \n==========================================\n  Hits           3922       3922          \n  Misses         1839       1839          \n  Partials        210        210\n\nPowered by Codecov. Last update 00c961b...d8133c2\n. ## Current coverage is 65.85% (diff: 0.00%)\nMerging #1649 into master will decrease coverage by 0.05%\n\ndiff\n@@             master      #1649   diff @@\n==========================================\n  Files            53         53          \n  Lines          5996       6001     +5   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1180       1184     +4   \n==========================================\n  Hits           3952       3952          \n- Misses         1838       1843     +5   \n  Partials        206        206\n\nPowered by Codecov. Last update c895525...a563644\n. ## Current coverage is 65.85% (diff: 100%)\nMerging #1651 into master will not change coverage\n\ndiff\n@@             master      #1651   diff @@\n==========================================\n  Files            53         53          \n  Lines          6001       6001          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1184       1184          \n==========================================\n  Hits           3952       3952          \n  Misses         1843       1843          \n  Partials        206        206\n\nPowered by Codecov. Last update 4399116...e5d75a4\n. ## Current coverage is 65.85% (diff: 100%)\nMerging #1652 into master will not change coverage\n\ndiff\n@@             master      #1652   diff @@\n==========================================\n  Files            53         53          \n  Lines          6001       6001          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1184       1184          \n==========================================\n  Hits           3952       3952          \n  Misses         1843       1843          \n  Partials        206        206\n\nPowered by Codecov. Last update 03a57b1...c3ab093\n. ## Current coverage is 66.15% (diff: 100%)\nMerging #1653 into master will not change coverage\n\ndiff\n@@             master      #1653   diff @@\n==========================================\n  Files            54         54          \n  Lines          6034       6034          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1190       1190          \n==========================================\n  Hits           3992       3992          \n  Misses         1836       1836          \n  Partials        206        206\n\nPowered by Codecov. Last update 424f298...128670e\n. ## Current coverage is 66.16% (diff: 100%)\nMerging #1654 into master will increase coverage by <.01%\n\ndiff\n@@             master      #1654   diff @@\n==========================================\n  Files            54         54          \n  Lines          6034       6035     +1   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1190       1190          \n==========================================\n+ Hits           3992       3993     +1   \n  Misses         1836       1836          \n  Partials        206        206\n\nPowered by Codecov. Last update 22ddff8...14945a9\n. ## Current coverage is 66.21% (diff: 87.50%)\nMerging #1655 into master will increase coverage by 0.05%\n\ndiff\n@@             master      #1655   diff @@\n==========================================\n  Files            54         54          \n  Lines          6035       6042     +7   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1190       1190          \n==========================================\n+ Hits           3993       4001     +8   \n  Misses         1836       1836          \n+ Partials        206        205     -1\n\nPowered by Codecov. Last update 99bf233...3a115ff\n. ## Current coverage is 66.14% (diff: 12.50%)\nMerging #1656 into master will decrease coverage by 0.07%\n\ndiff\n@@             master      #1656   diff @@\n==========================================\n  Files            54         54          \n  Lines          6042       6049     +7   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1190       1193     +3   \n==========================================\n  Hits           4001       4001          \n- Misses         1836       1842     +6   \n- Partials        205        206     +1\n\nPowered by Codecov. Last update 3a80832...307de1c\n. ## Current coverage is 65.60% (diff: 10.16%)\nMerging #1657 into master will decrease coverage by 0.54%\n\ndiff\n@@             master      #1657   diff @@\n==========================================\n  Files            54         54          \n  Lines          6049       6064    +15   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1193       1198     +5   \n==========================================\n- Hits           4001       3978    -23   \n- Misses         1842       1884    +42   \n+ Partials        206        202     -4\n\nPowered by Codecov. Last update 3b7ccf7...bc18f97\n. ## Current coverage is 65.60% (diff: 10.16%)\nMerging #1657 into master will decrease coverage by 0.54%\n\ndiff\n@@             master      #1657   diff @@\n==========================================\n  Files            54         54          \n  Lines          6049       6064    +15   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1193       1198     +5   \n==========================================\n- Hits           4001       3978    -23   \n- Misses         1842       1884    +42   \n+ Partials        206        202     -4\n\nPowered by Codecov. Last update 3b7ccf7...bc18f97\n. ## Current coverage is 65.69% (diff: 100%)\nMerging #1658 into master will increase coverage by 0.09%\n\ndiff\n@@             master      #1658   diff @@\n==========================================\n  Files            54         54          \n  Lines          6064       6081    +17   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1198       1201     +3   \n==========================================\n+ Hits           3978       3995    +17   \n  Misses         1884       1884          \n  Partials        202        202\n\nPowered by Codecov. Last update 6063737...c0ac337\n. ## Current coverage is 65.68% (diff: 90.00%)\nMerging #1659 into master will decrease coverage by <.01%\n\ndiff\n@@             master      #1659   diff @@\n==========================================\n  Files            54         54          \n  Lines          6081       6100    +19   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1201       1204     +3   \n==========================================\n+ Hits           3995       4007    +12   \n- Misses         1884       1890     +6   \n- Partials        202        203     +1\n\nPowered by Codecov. Last update aca8f0c...672456e\n. ## Current coverage is 65.61% (diff: 100%)\nMerging #1660 into master will decrease coverage by 0.08%\n\ndiff\n@@             master      #1660   diff @@\n==========================================\n  Files            54         54          \n  Lines          6081       6081          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1201       1201          \n==========================================\n- Hits           3995       3990     -5   \n- Misses         1884       1888     +4   \n- Partials        202        203     +1\n\nPowered by Codecov. Last update aca8f0c...6193f25\n. ## Current coverage is 65.51% (diff: 38.09%)\nMerging #1662 into master will decrease coverage by 0.10%\n\ndiff\n@@             master      #1662   diff @@\n==========================================\n  Files            54         54          \n  Lines          6081       6101    +20   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1201       1205     +4   \n==========================================\n+ Hits           3990       3997     +7   \n- Misses         1888       1900    +12   \n- Partials        203        204     +1\n\nPowered by Codecov. Last update 68bb54b...8862db3\n. ## Current coverage is 65.51% (diff: 33.33%)\nMerging #1663 into master will not change coverage\n\ndiff\n@@             master      #1663   diff @@\n==========================================\n  Files            54         54          \n  Lines          6101       6101          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1205          \n==========================================\n  Hits           3997       3997          \n  Misses         1900       1900          \n  Partials        204        204\n\nPowered by Codecov. Last update 9d2013d...5fedd66\n. ## Current coverage is 65.51% (diff: 100%)\nMerging #1664 into master will not change coverage\n\ndiff\n@@             master      #1664   diff @@\n==========================================\n  Files            54         54          \n  Lines          6101       6101          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1205          \n==========================================\n  Hits           3997       3997          \n  Misses         1900       1900          \n  Partials        204        204\n\nPowered by Codecov. Last update c939cb8...9715581\n. ## Current coverage is 65.47% (diff: 0.00%)\nMerging #1666 into master will decrease coverage by 0.04%\n\ndiff\n@@             master      #1666   diff @@\n==========================================\n  Files            54         54          \n  Lines          6101       6105     +4   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1205          \n==========================================\n  Hits           3997       3997          \n- Misses         1900       1904     +4   \n  Partials        204        204\n\nPowered by Codecov. Last update 1ad8331...a870356\n. ## Current coverage is 65.47% (diff: 100%)\nMerging #1668 into master will not change coverage\n\ndiff\n@@             master      #1668   diff @@\n==========================================\n  Files            54         54          \n  Lines          6105       6105          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1205          \n==========================================\n  Hits           3997       3997          \n  Misses         1904       1904          \n  Partials        204        204\n\nPowered by Codecov. Last update 1f56f51...3a7adac\n. ## Current coverage is 65.47% (diff: 100%)\nMerging #1669 into master will not change coverage\n\ndiff\n@@             master      #1669   diff @@\n==========================================\n  Files            54         54          \n  Lines          6105       6105          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1205          \n==========================================\n  Hits           3997       3997          \n  Misses         1904       1904          \n  Partials        204        204\n\nPowered by Codecov. Last update 1f56f51...c3d6660\n. ## Current coverage is 65.63% (diff: 60.00%)\nMerging #1673 into master will increase coverage by 0.16%\n\ndiff\n@@             master      #1673   diff @@\n==========================================\n  Files            54         54          \n  Lines          6105       6102     -3   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1204     -1   \n==========================================\n+ Hits           3997       4005     +8   \n+ Misses         1904       1893    -11   \n  Partials        204        204\n\nPowered by Codecov. Last update 0683107...31407f5\n. ## Current coverage is 65.78% (diff: 68.18%)\nMerging #1674 into master will increase coverage by 0.31%\n\ndiff\n@@             master      #1674   diff @@\n==========================================\n  Files            54         54          \n  Lines          6105       6109     +4   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1206     +1   \n==========================================\n+ Hits           3997       4019    +22   \n+ Misses         1904       1883    -21   \n- Partials        204        207     +3\n\nPowered by Codecov. Last update 0683107...f647a60\n. ## Current coverage is 65.94% (diff: 0.00%)\nMerging #1679 into master will decrease coverage by 0.01%\n\ndiff\n@@             master      #1679   diff @@\n==========================================\n  Files            54         54          \n  Lines          6106       6107     +1   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1204     -1   \n==========================================\n  Hits           4027       4027          \n- Misses         1872       1873     +1   \n  Partials        207        207\n\nPowered by Codecov. Last update 2cc2a2a...345b5c2\n. ## Current coverage is 65.92% (diff: 33.33%)\nMerging #1680 into master will decrease coverage by 0.02%\n\ndiff\n@@             master      #1680   diff @@\n==========================================\n  Files            54         54          \n  Lines          6106       6110     +4   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1205          \n==========================================\n+ Hits           4027       4028     +1   \n- Misses         1872       1874     +2   \n- Partials        207        208     +1\n\nPowered by Codecov. Last update 2cc2a2a...d171767\n. ## Current coverage is 65.95% (diff: 100%)\nMerging #1685 into master will not change coverage\n\ndiff\n@@             master      #1685   diff @@\n==========================================\n  Files            54         54          \n  Lines          6106       6106          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1205          \n==========================================\n  Hits           4027       4027          \n  Misses         1872       1872          \n  Partials        207        207\n\nPowered by Codecov. Last update 2cc2a2a...13d00e4\n. ## Current coverage is 65.95% (diff: 100%)\nMerging #1686 into master will not change coverage\n\ndiff\n@@             master      #1686   diff @@\n==========================================\n  Files            54         54          \n  Lines          6106       6106          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1205          \n==========================================\n  Hits           4027       4027          \n  Misses         1872       1872          \n  Partials        207        207\n\nPowered by Codecov. Last update 2cc2a2a...03beab7\n. ## Current coverage is 65.95% (diff: 100%)\nMerging #1687 into master will not change coverage\n\ndiff\n@@             master      #1687   diff @@\n==========================================\n  Files            54         54          \n  Lines          6106       6106          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1205          \n==========================================\n  Hits           4027       4027          \n  Misses         1872       1872          \n  Partials        207        207\n\nPowered by Codecov. Last update 43d3e71...7f3d5b1\n. ## Current coverage is 65.92% (diff: 100%)\nMerging #1689 into master will not change coverage\n\ndiff\n@@             master      #1689   diff @@\n==========================================\n  Files            54         54          \n  Lines          6110       6110          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1205          \n==========================================\n  Hits           4028       4028          \n  Misses         1874       1874          \n  Partials        208        208\n\nPowered by Codecov. Last update 722a3df...e4eeae0\n. ## Current coverage is 65.92% (diff: 100%)\nMerging #1692 into master will not change coverage\n\ndiff\n@@             master      #1692   diff @@\n==========================================\n  Files            54         54          \n  Lines          6110       6110          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1205          \n==========================================\n  Hits           4028       4028          \n  Misses         1874       1874          \n  Partials        208        208\n\nPowered by Codecov. Last update 2aee2bc...e0084fa\n. ## Current coverage is 65.92% (diff: 100%)\nMerging #1693 into master will not change coverage\n\ndiff\n@@             master      #1693   diff @@\n==========================================\n  Files            54         54          \n  Lines          6110       6110          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1205          \n==========================================\n  Hits           4028       4028          \n  Misses         1874       1874          \n  Partials        208        208\n\nPowered by Codecov. Last update ee2b9df...1904348\n. ## Current coverage is 65.72% (diff: 100%)\nMerging #1695 into master will decrease coverage by 0.19%\n\ndiff\n@@             master      #1695   diff @@\n==========================================\n  Files            54         54          \n  Lines          6110       6110          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1205          \n==========================================\n- Hits           4028       4016    -12   \n- Misses         1874       1882     +8   \n- Partials        208        212     +4\n\nPowered by Codecov. Last update 4c31446...c277781\n. ## Current coverage is 65.92% (diff: 100%)\nMerging #1696 into master will not change coverage\n\ndiff\n@@             master      #1696   diff @@\n==========================================\n  Files            54         54          \n  Lines          6110       6110          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1205          \n==========================================\n  Hits           4028       4028          \n  Misses         1874       1874          \n  Partials        208        208\n\nPowered by Codecov. Last update 4c31446...59c354a\n. ## Current coverage is 65.72% (diff: 100%)\nMerging #1697 into master will not change coverage\n\ndiff\n@@             master      #1697   diff @@\n==========================================\n  Files            54         54          \n  Lines          6110       6110          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1205          \n==========================================\n  Hits           4016       4016          \n  Misses         1882       1882          \n  Partials        212        212\n\nPowered by Codecov. Last update 32bf376...749265e\n. ## Current coverage is 65.72% (diff: 100%)\nMerging #1697 into master will not change coverage\n\ndiff\n@@             master      #1697   diff @@\n==========================================\n  Files            54         54          \n  Lines          6110       6110          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1205       1205          \n==========================================\n  Hits           4016       4016          \n  Misses         1882       1882          \n  Partials        212        212\n\nPowered by Codecov. Last update 32bf376...749265e\n. ## Current coverage is 65.39% (diff: 31.57%)\nMerging #1699 into master will decrease coverage by 0.22%\n\ndiff\n@@             master      #1699   diff @@\n==========================================\n  Files            54         54          \n  Lines          6122       6158    +36   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1210       1222    +12   \n==========================================\n+ Hits           4017       4027    +10   \n- Misses         1892       1916    +24   \n- Partials        213        215     +2\n\nPowered by Codecov. Last update 20d443b...900fc7e\n. ## Current coverage is 65.61% (diff: 95.23%)\nMerging #1700 into master will not change coverage\n\ndiff\n@@             master      #1700   diff @@\n==========================================\n  Files            54         54          \n  Lines          6122       6122          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1210       1210          \n==========================================\n  Hits           4017       4017          \n  Misses         1892       1892          \n  Partials        213        213\n\nPowered by Codecov. Last update 20d443b...a32b395\n. ## Current coverage is 65.75% (diff: 100%)\nMerging #1705 into master will increase coverage by 0.13%\n\ndiff\n@@             master      #1705   diff @@\n==========================================\n  Files            54         54          \n  Lines          6122       6146    +24   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1210       1218     +8   \n==========================================\n+ Hits           4017       4041    +24   \n  Misses         1892       1892          \n  Partials        213        213\n\nPowered by Codecov. Last update a76f0e5...ead31dc\n. ## Current coverage is 65.61% (diff: 100%)\nMerging #1706 into master will not change coverage\n\ndiff\n@@             master      #1706   diff @@\n==========================================\n  Files            54         54          \n  Lines          6122       6122          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1210       1210          \n==========================================\n  Hits           4017       4017          \n  Misses         1892       1892          \n  Partials        213        213\n\nPowered by Codecov. Last update a76f0e5...cd9b9bf\n. ## Current coverage is 65.65% (diff: 73.17%)\nMerging #1710 into master will increase coverage by 0.03%\n\ndiff\n@@             master      #1710   diff @@\n==========================================\n  Files            54         55     +1   \n  Lines          6122       6161    +39   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1210       1222    +12   \n==========================================\n+ Hits           4017       4045    +28   \n- Misses         1892       1899     +7   \n- Partials        213        217     +4\n\nPowered by Codecov. Last update 6811d20...a971bcd\n. ## Current coverage is 65.65% (diff: 90.00%)\nMerging #1713 into master will increase coverage by 0.04%\n\ndiff\n@@             master      #1713   diff @@\n==========================================\n  Files            54         54          \n  Lines          6122       6135    +13   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1210       1212     +2   \n==========================================\n+ Hits           4017       4028    +11   \n- Misses         1892       1893     +1   \n- Partials        213        214     +1\n\nPowered by Codecov. Last update 6811d20...6e7f5ae\n. ## Current coverage is 65.61% (diff: 100%)\nMerging #1715 into master will not change coverage\n\ndiff\n@@             master      #1715   diff @@\n==========================================\n  Files            54         54          \n  Lines          6122       6122          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1210       1210          \n==========================================\n  Hits           4017       4017          \n  Misses         1892       1892          \n  Partials        213        213\n\nPowered by Codecov. Last update 6811d20...39f5217\n. ## Current coverage is 65.61% (diff: 100%)\nMerging #1716 into master will not change coverage\n\ndiff\n@@             master      #1716   diff @@\n==========================================\n  Files            54         54          \n  Lines          6122       6122          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1210       1210          \n==========================================\n  Hits           4017       4017          \n  Misses         1892       1892          \n  Partials        213        213\n\nPowered by Codecov. Last update 6811d20...44e86e8\n. ## Current coverage is 65.63% (diff: 100%)\nMerging #1717 into master will increase coverage by 0.01%\n\ndiff\n@@             master      #1717   diff @@\n==========================================\n  Files            54         54          \n  Lines          6122       6125     +3   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1210       1210          \n==========================================\n+ Hits           4017       4020     +3   \n  Misses         1892       1892          \n  Partials        213        213\n\nPowered by Codecov. Last update f2cb984...2e58230\n. ## Current coverage is 65.59% (diff: 45.45%)\nMerging #1719 into master will decrease coverage by 0.04%\n\ndiff\n@@             master      #1719   diff @@\n==========================================\n  Files            54         54          \n  Lines          6125       6135    +10   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1210       1211     +1   \n==========================================\n+ Hits           4020       4024     +4   \n- Misses         1892       1897     +5   \n- Partials        213        214     +1\n\nPowered by Codecov. Last update e76a8d6...c0eb83f\n. ## Current coverage is 65.63% (diff: 100%)\nMerging #1720 into master will not change coverage\n\ndiff\n@@             master      #1720   diff @@\n==========================================\n  Files            54         54          \n  Lines          6125       6125          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1210       1210          \n==========================================\n  Hits           4020       4020          \n  Misses         1892       1892          \n  Partials        213        213\n\nPowered by Codecov. Last update e6fa81f...b14c93c\n. ## Current coverage is 65.50% (diff: 0.00%)\nMerging #1722 into master will decrease coverage by 0.12%\n\ndiff\n@@             master      #1722   diff @@\n==========================================\n  Files            54         54          \n  Lines          6125       6137    +12   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1210       1210          \n==========================================\n  Hits           4020       4020          \n- Misses         1892       1904    +12   \n  Partials        213        213\n\nPowered by Codecov. Last update e76a8d6...0f8bd9a\n. ## Current coverage is 65.59% (diff: 88.46%)\nMerging #1723 into master will increase coverage by 0.09%\n\ndiff\n@@             master      #1723   diff @@\n==========================================\n  Files            54         54          \n  Lines          6137       6168    +31   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1210       1215     +5   \n==========================================\n+ Hits           4020       4046    +26   \n- Misses         1904       1908     +4   \n- Partials        213        214     +1\n\nPowered by Codecov. Last update dddbe3e...a89bd05\n. ## Current coverage is 65.59% (diff: 100%)\nMerging #1724 into master will not change coverage\n\ndiff\n@@             master      #1724   diff @@\n==========================================\n  Files            54         54          \n  Lines          6168       6168          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1215       1215          \n==========================================\n  Hits           4046       4046          \n  Misses         1908       1908          \n  Partials        214        214\n\nPowered by Codecov. Last update 61f62ee...1367b18\n. ## Current coverage is 65.59% (diff: 0.00%)\nMerging #1725 into master will not change coverage\n\ndiff\n@@             master      #1725   diff @@\n==========================================\n  Files            54         54          \n  Lines          6168       6168          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1215       1215          \n==========================================\n  Hits           4046       4046          \n  Misses         1908       1908          \n  Partials        214        214\n\nPowered by Codecov. Last update b70a005...3c665ec\n. ## Current coverage is 65.59% (diff: 100%)\nMerging #1726 into master will not change coverage\n\ndiff\n@@             master      #1726   diff @@\n==========================================\n  Files            54         54          \n  Lines          6168       6168          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1215       1215          \n==========================================\n  Hits           4046       4046          \n  Misses         1908       1908          \n  Partials        214        214\n\nPowered by Codecov. Last update 3244b1d...5d139bd\n. ## Current coverage is 65.59% (diff: 100%)\nMerging #1728 into master will not change coverage\n\ndiff\n@@             master      #1728   diff @@\n==========================================\n  Files            54         54          \n  Lines          6168       6168          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1215       1215          \n==========================================\n  Hits           4046       4046          \n  Misses         1908       1908          \n  Partials        214        214\n\nPowered by Codecov. Last update 337eacf...299c5fc\n. ## Current coverage is 65.59% (diff: 100%)\nMerging #1729 into master will not change coverage\n\ndiff\n@@             master      #1729   diff @@\n==========================================\n  Files            54         54          \n  Lines          6168       6168          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1215       1215          \n==========================================\n  Hits           4046       4046          \n  Misses         1908       1908          \n  Partials        214        214\n\nPowered by Codecov. Last update 337eacf...149c420\n. ## Current coverage is 65.57% (diff: 87.50%)\nMerging #1730 into master will decrease coverage by 0.01%\n\ndiff\n@@             master      #1730   diff @@\n==========================================\n  Files            54         54          \n  Lines          6168       6165     -3   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1215       1215          \n==========================================\n- Hits           4046       4043     -3   \n  Misses         1908       1908          \n  Partials        214        214\n\nPowered by Codecov. Last update 77fed6e...3dce0f4\n. # Codecov Report\nMerging #1736 into master will decrease coverage by -0.04%.\nThe diff coverage is 62.85%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1736      +/-\n==========================================\n- Coverage   66.24%   66.21%   -0.04%   \n==========================================\n  Files          55       55            \n  Lines        6348     6363      +15   \n  Branches     1255     1258       +3   \n==========================================\n+ Hits         4205     4213       +8   \n- Misses       1927     1932       +5   \n- Partials      216      218       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 74.35% <100%> (+0.16%) | :white_check_mark: |\n| webapp/graphite/storage.py | 53.78% <100%> (\u00f8) | :white_check_mark: |\n| webapp/graphite/metrics/views.py | 79.67% <100%> (+0.25%) | :white_check_mark: |\n| webapp/graphite/render/datalib.py | 71.66% <100%> (\u00f8) | :white_check_mark: |\n| webapp/graphite/render/views.py | 46.51% <28.57%> (-0.24%) | :x: |\n| webapp/graphite/remote_storage.py | 38.7% <57.89%> (+0.29%) | :white_check_mark: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 348bac9...b381f9a. Read the comment docs.. ## Current coverage is 65.68% (diff: 100%)\nMerging #1738 into master will increase coverage by 0.10%\n\ndiff\n@@             master      #1738   diff @@\n==========================================\n  Files            54         54          \n  Lines          6165       6166     +1   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1215       1215          \n==========================================\n+ Hits           4043       4050     +7   \n+ Misses         1908       1903     -5   \n+ Partials        214        213     -1\n\nPowered by Codecov. Last update 8b142c8...3e924fb\n. ## Current coverage is 65.70% (diff: 66.66%)\nMerging #1743 into master will increase coverage by 0.02%\n\ndiff\n@@             master      #1743   diff @@\n==========================================\n  Files            54         54          \n  Lines          6166       6170     +4   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1215       1216     +1   \n==========================================\n+ Hits           4050       4054     +4   \n  Misses         1903       1903          \n  Partials        213        213\n\nPowered by Codecov. Last update f5a44c0...c9d3f64\n. ## Current coverage is 65.73% (diff: 67.85%)\nMerging #1744 into master will increase coverage by 0.03%\n\ndiff\n@@             master      #1744   diff @@\n==========================================\n  Files            54         54          \n  Lines          6170       6193    +23   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1216       1224     +8   \n==========================================\n+ Hits           4054       4071    +17   \n- Misses         1903       1909     +6   \n  Partials        213        213\n\nPowered by Codecov. Last update 64698ce...b130c05\n. ## Current coverage is 65.86% (diff: 66.66%)\nMerging #1745 into master will increase coverage by 0.15%\n\ndiff\n@@             master      #1745   diff @@\n==========================================\n  Files            54         54          \n  Lines          6170       6172     +2   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1216       1217     +1   \n==========================================\n+ Hits           4054       4065    +11   \n+ Misses         1903       1893    -10   \n- Partials        213        214     +1\n\nPowered by Codecov. Last update 64698ce...ea6a610\n. ## Current coverage is 65.89% (diff: 100%)\nMerging #1747 into master will not change coverage\n\ndiff\n@@             master      #1747   diff @@\n==========================================\n  Files            54         54          \n  Lines          6195       6195          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1225       1225          \n==========================================\n  Hits           4082       4082          \n  Misses         1899       1899          \n  Partials        214        214\n\nPowered by Codecov. Last update 11a814f...88ee461\n. ## Current coverage is 65.89% (diff: 100%)\nMerging #1747 into master will not change coverage\n\ndiff\n@@             master      #1747   diff @@\n==========================================\n  Files            54         54          \n  Lines          6195       6195          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1225       1225          \n==========================================\n  Hits           4082       4082          \n  Misses         1899       1899          \n  Partials        214        214\n\nPowered by Codecov. Last update 11a814f...88ee461\n. ## Current coverage is 66.29% (diff: 100%)\nMerging #1749 into master will increase coverage by 0.40%\n\ndiff\n@@             master      #1749   diff @@\n==========================================\n  Files            54         54          \n  Lines          6195       6195          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1225       1225          \n==========================================\n+ Hits           4082       4107    +25   \n+ Misses         1899       1868    -31   \n- Partials        214        220     +6\n\nPowered by Codecov. Last update 44717c3...0e71a31. ## Current coverage is 66.37% (diff: 100%)\nMerging #1752 into master will increase coverage by 0.08%\n\ndiff\n@@             master      #1752   diff @@\n==========================================\n  Files            54         55     +1   \n  Lines          6195       6228    +33   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1225       1234     +9   \n==========================================\n+ Hits           4107       4134    +27   \n- Misses         1868       1872     +4   \n- Partials        220        222     +2\n\nPowered by Codecov. Last update 26c2de1...1b7f391. ## Current coverage is 66.37% (diff: 100%)\nMerging #1752 into master will increase coverage by 0.08%\n\ndiff\n@@             master      #1752   diff @@\n==========================================\n  Files            54         55     +1   \n  Lines          6195       6228    +33   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1225       1234     +9   \n==========================================\n+ Hits           4107       4134    +27   \n- Misses         1868       1872     +4   \n- Partials        220        222     +2\n\nPowered by Codecov. Last update 26c2de1...1b7f391. ## Current coverage is 66.28% (diff: 100%)\nMerging #1753 into master will decrease coverage by 0.01%\n\ndiff\n@@             master      #1753   diff @@\n==========================================\n  Files            54         54          \n  Lines          6195       6193     -2   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1225       1224     -1   \n==========================================\n- Hits           4107       4105     -2   \n  Misses         1868       1868          \n  Partials        220        220\n\nPowered by Codecov. Last update 26c2de1...be7fca6. ## Current coverage is 66.35% (diff: 100%)\nMerging #1754 into master will increase coverage by 0.05%\n\ndiff\n@@             master      #1754   diff @@\n==========================================\n  Files            54         54          \n  Lines          6195       6191     -4   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1225       1223     -2   \n==========================================\n+ Hits           4107       4108     +1   \n+ Misses         1868       1865     -3   \n+ Partials        220        218     -2\n\nPowered by Codecov. Last update 26c2de1...bf8e53b. ## Current coverage is 66.29% (diff: 100%)\nMerging #1755 into master will not change coverage\n\ndiff\n@@             master      #1755   diff @@\n==========================================\n  Files            54         54          \n  Lines          6195       6195          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1225       1225          \n==========================================\n  Hits           4107       4107          \n  Misses         1868       1868          \n  Partials        220        220\n\nPowered by Codecov. Last update 26c2de1...24398ad. ## Current coverage is 66.28% (diff: 100%)\nMerging #1756 into master will decrease coverage by 0.01%\n\ndiff\n@@             master      #1756   diff @@\n==========================================\n  Files            54         54          \n  Lines          6195       6193     -2   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1225       1224     -1   \n==========================================\n- Hits           4107       4105     -2   \n  Misses         1868       1868          \n  Partials        220        220\n\nPowered by Codecov. Last update 26c2de1...acb2e01. ## Current coverage is 66.28% (diff: 100%)\nMerging #1757 into master will not change coverage\n\ndiff\n@@             master      #1757   diff @@\n==========================================\n  Files            54         54          \n  Lines          6193       6193          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1224       1224          \n==========================================\n  Hits           4105       4105          \n  Misses         1868       1868          \n  Partials        220        220\n\nPowered by Codecov. Last update d6069e7...7283f2c. ## Current coverage is 66.28% (diff: 100%)\nMerging #1758 into master will not change coverage\n\ndiff\n@@             master      #1758   diff @@\n==========================================\n  Files            54         54          \n  Lines          6193       6193          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1224       1224          \n==========================================\n  Hits           4105       4105          \n  Misses         1868       1868          \n  Partials        220        220\n\nPowered by Codecov. Last update e873e73...095ce17. ## Current coverage is 66.34% (diff: 100%)\nMerging #1759 into master will not change coverage\n\ndiff\n@@             master      #1759   diff @@\n==========================================\n  Files            54         54          \n  Lines          6189       6189          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1222       1222          \n==========================================\n  Hits           4106       4106          \n  Misses         1865       1865          \n  Partials        218        218\n\nPowered by Codecov. Last update 89d21b7...929c513. ## Current coverage is 66.33% (diff: 100%)\nMerging #1762 into master will not change coverage\n\ndiff\n@@             master      #1762   diff @@\n==========================================\n  Files            55         55          \n  Lines          6238       6238          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1235       1235          \n==========================================\n  Hits           4138       4138          \n  Misses         1877       1877          \n  Partials        223        223\n\nPowered by Codecov. Last update 6696a95...2aff698. ## Current coverage is 66.46% (diff: 100%)\nMerging #1763 into master will not change coverage\n\ndiff\n@@             master      #1763   diff @@\n==========================================\n  Files            55         55          \n  Lines          6262       6262          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1243       1243          \n==========================================\n  Hits           4162       4162          \n  Misses         1877       1877          \n  Partials        223        223\n\nPowered by Codecov. Last update 7a5359d...0c7a204. ## Current coverage is 65.73% (diff: 100%)\nMerging #1768 into master will decrease coverage by 0.77%\n\ndiff\n@@             master      #1768   diff @@\n==========================================\n  Files            55         54     -1   \n  Lines          6276       6263    -13   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1247       1238     -9   \n==========================================\n- Hits           4174       4117    -57   \n- Misses         1878       1932    +54   \n+ Partials        224        214    -10\n\nPowered by Codecov. Last update 4fe3474...11b52c7. ## Current coverage is 66.46% (diff: 0.00%)\nMerging #1769 into master will not change coverage\n\ndiff\n@@             master      #1769   diff @@\n==========================================\n  Files            55         55          \n  Lines          6262       6262          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1243       1243          \n==========================================\n  Hits           4162       4162          \n  Misses         1877       1877          \n  Partials        223        223\n\nPowered by Codecov. Last update ed692e0...3e775d1. ## Current coverage is 66.46% (diff: 100%)\nMerging #1771 into master will not change coverage\n\ndiff\n@@             master      #1771   diff @@\n==========================================\n  Files            55         55          \n  Lines          6262       6262          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1243       1243          \n==========================================\n  Hits           4162       4162          \n  Misses         1877       1877          \n  Partials        223        223\n\nPowered by Codecov. Last update 5638adf...ef6f825. ## Current coverage is 66.46% (diff: 100%)\nMerging #1773 into master will not change coverage\n\ndiff\n@@             master      #1773   diff @@\n==========================================\n  Files            55         55          \n  Lines          6262       6262          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1243       1243          \n==========================================\n  Hits           4162       4162          \n  Misses         1877       1877          \n  Partials        223        223\n\nPowered by Codecov. Last update f7634f3...1c4fb5b. ## Current coverage is 65.68% (diff: 57.89%)\nMerging #1774 into master will decrease coverage by 0.77%\n\ndiff\n@@             master      #1774   diff @@\n==========================================\n  Files            55         55          \n  Lines          6262       6275    +13   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1243       1245     +2   \n==========================================\n- Hits           4162       4122    -40   \n- Misses         1877       1936    +59   \n+ Partials        223        217     -6\n\nPowered by Codecov. Last update f7634f3...41cc96d. ## Current coverage is 66.50% (diff: 86.66%)\nMerging #1775 into master will increase coverage by 0.04%\n\ndiff\n@@             master      #1775   diff @@\n==========================================\n  Files            55         55          \n  Lines          6262       6276    +14   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1243       1247     +4   \n==========================================\n+ Hits           4162       4174    +12   \n- Misses         1877       1878     +1   \n- Partials        223        224     +1\n\nPowered by Codecov. Last update f7634f3...b71f0e0. ## Current coverage is 66.50% (diff: 100%)\nMerging #1777 into master will not change coverage\n\ndiff\n@@             master      #1777   diff @@\n==========================================\n  Files            55         55          \n  Lines          6276       6276          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1247       1247          \n==========================================\n  Hits           4174       4174          \n  Misses         1878       1878          \n  Partials        224        224\n\nPowered by Codecov. Last update 4fe3474...03edddf. ## Current coverage is 66.55% (diff: 100%)\nMerging #1780 into master will increase coverage by 0.04%\n\ndiff\n@@             master      #1780   diff @@\n==========================================\n  Files            55         55          \n  Lines          6276       6285     +9   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1247       1250     +3   \n==========================================\n+ Hits           4174       4183     +9   \n  Misses         1878       1878          \n  Partials        224        224\n\nPowered by Codecov. Last update 73bc058...907f252. ## Current coverage is 66.50% (diff: 0.00%)\nMerging #1784 into master will not change coverage\n\ndiff\n@@             master      #1784   diff @@\n==========================================\n  Files            55         55          \n  Lines          6276       6276          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1247       1243     -4   \n==========================================\n  Hits           4174       4174          \n  Misses         1878       1878          \n  Partials        224        224\n\nPowered by Codecov. Last update 73bc058...eeaa965. ## Current coverage is 66.60% (diff: 78.94%)\nMerging #1785 into master will increase coverage by 0.09%\n\ndiff\n@@             master      #1785   diff @@\n==========================================\n  Files            55         54     -1   \n  Lines          6276       6255    -21   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1247       1239     -8   \n==========================================\n- Hits           4174       4166     -8   \n+ Misses         1878       1869     -9   \n+ Partials        224        220     -4\n\nPowered by Codecov. Last update 73bc058...0546308. ## Current coverage is 65.88% (diff: 100%)\nMerging #1789 into master will increase coverage by 0.15%\n\ndiff\n@@             master      #1789   diff @@\n==========================================\n  Files            55         55          \n  Lines          6289       6317    +28   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1245       1253     +8   \n==========================================\n+ Hits           4134       4162    +28   \n  Misses         1937       1937          \n  Partials        218        218\n\nPowered by Codecov. Last update 4005a0e...963346f. # Codecov Report\nMerging #1792 into master will increase coverage by 0.14%.\nThe diff coverage is 89.74%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1792      +/-\n==========================================\n+ Coverage   66.22%   66.36%   +0.14%   \n==========================================\n  Files          55       55            \n  Lines        6365     6404      +39   \n  Branches     1258     1267       +9   \n==========================================\n+ Hits         4215     4250      +35   \n- Misses       1932     1934       +2   \n- Partials      218      220       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.6% <89.74%> (\u00f8) | :white_check_mark: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ee15e37...e46d80e. Read the comment docs.. ## Current coverage is 65.71% (diff: 0.00%)\nMerging #1793 into master will decrease coverage by 0.02%\n\ndiff\n@@             master      #1793   diff @@\n==========================================\n  Files            55         55          \n  Lines          6289       6291     +2   \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1245       1246     +1   \n==========================================\n  Hits           4134       4134          \n- Misses         1937       1939     +2   \n  Partials        218        218\n\nPowered by Codecov. Last update 4005a0e...5eb528a. ## Current coverage is 65.71% (diff: 100%)\nMerging #1797 into master will not change coverage\n\ndiff\n@@             master      #1797   diff @@\n==========================================\n  Files            55         55          \n  Lines          6291       6291          \n  Methods           0          0          \n  Messages          0          0          \n  Branches       1246       1246          \n==========================================\n  Hits           4134       4134          \n  Misses         1939       1939          \n  Partials        218        218\n\nPowered by Codecov. Last update 3213dc6...587a48c. # Codecov Report\nMerging #1798 into master will increase coverage by 0.02%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1798      +/-\n==========================================\n+ Coverage   65.92%   65.94%   +0.02%   \n==========================================\n  Files          54       55       +1   \n  Lines        6297     6301       +4   \n  Branches     1247     1247            \n==========================================\n+ Hits         4151     4155       +4   \n  Misses       1932     1932            \n  Partials      214      214\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/util.py | 76.15% <\u00f8> (-2.21%) | :x: |\n| webapp/graphite/metrics/views.py | 77.58% <100%> (+0.09%) | :white_check_mark: |\n| webapp/graphite/render/views.py | 46.83% <100%> (+0.13%) | :white_check_mark: |\n| webapp/graphite/composer/views.py | 24.69% <100%> (\u00f8) | :white_check_mark: |\n| webapp/graphite/browser/views.py | 49.41% <100%> (+0.29%) | :white_check_mark: |\n| webapp/graphite/account/views.py | 25% <100%> (\u00f8) | :white_check_mark: |\n| webapp/graphite/user_util.py | 95.23% <95.23%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5f6e561...ad88d44. Read the comment docs.. # Codecov Report\nMerging #1815 into master will not impact coverage by -0.06%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1815      +/-\n==========================================\n- Coverage   65.94%   65.88%   -0.06%   \n==========================================\n  Files          55       55            \n  Lines        6301     6308       +7   \n  Branches     1247     1248       +1   \n==========================================\n+ Hits         4155     4156       +1   \n- Misses       1932     1938       +6   \n  Partials      214      214\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/remote_storage.py | 38.85% <10%> (-1.03%) | :x: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7d0e077...1ca7be3. Read the comment docs.. # Codecov Report\nMerging #1818 into master will decrease coverage by 0.32%.\nThe diff coverage is 46.32%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1818      +/-\n==========================================\n- Coverage   66.36%   66.04%   -0.33%   \n==========================================\n  Files          55       56       +1   \n  Lines        6404     6529     +125   \n  Branches     1267     1301      +34   \n==========================================\n+ Hits         4250     4312      +62   \n- Misses       1934     1994      +60   \n- Partials      220      223       +3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 75.15% <100%> (+0.79%) | :white_check_mark: |\n| webapp/graphite/util.py | 79.06% <100%> (+2.14%) | :white_check_mark: |\n| webapp/graphite/remote_storage.py | 30.54% <22.05%> (-8.17%) | :x: |\n| webapp/graphite/render/datalib.py | 61.37% <30.3%> (-10.53%) | :x: |\n| webapp/graphite/render/views.py | 46.34% <45.45%> (-0.31%) | :x: |\n| webapp/graphite/storage.py | 57.34% <77.77%> (+3.17%) | :white_check_mark: |\n| webapp/graphite/worker_pool/pool.py | 89.47% <89.47%> (\u00f8) | |\n| webapp/graphite/render/evaluator.py | 76.38% <92.85%> (+3.97%) | :white_check_mark: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 80c2a29...c3e7f0c. Read the comment docs.. # Codecov Report\nMerging #1819 into master will increase coverage by 0.03%.\nThe diff coverage is 76.92%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1819      +/-\n==========================================\n+ Coverage   65.88%   65.92%   +0.03%   \n==========================================\n  Files          55       55            \n  Lines        6308     6318      +10   \n  Branches     1248     1248            \n==========================================\n+ Hits         4156     4165       +9   \n- Misses       1938     1939       +1   \n  Partials      214      214\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/standard.py | 68.53% <100%> (+1.09%) | :white_check_mark: |\n| webapp/graphite/readers.py | 40.16% <57.14%> (+1.01%) | :white_check_mark: |\n| webapp/graphite/render/views.py | 46.71% <\u00f8> (-0.12%) | :x: |\n| webapp/graphite/render/datalib.py | 71.9% <\u00f8> (\u00f8) | :white_check_mark: |\n| webapp/graphite/events/views.py | 78.2% <\u00f8> (+0.57%) | :white_check_mark: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8e61ee2...5a8cff5. Read the comment docs.. # Codecov Report\nMerging #1820 into master will increase coverage by 0.35%.\nThe diff coverage is 100%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1820      +/-\n==========================================\n+ Coverage   65.88%   66.24%   +0.35%   \n==========================================\n  Files          55       55            \n  Lines        6308     6348      +40   \n  Branches     1248     1255       +7   \n==========================================\n+ Hits         4156     4205      +49   \n+ Misses       1938     1927      -11   \n- Partials      214      216       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/metrics/views.py | 79.42% <100%> (+1.83%) | :white_check_mark: |\n| webapp/graphite/intervals.py | 50% <100%> (+12.5%) | :white_check_mark: |\n| webapp/graphite/remote_storage.py | 38.41% <\u00f8> (-0.44%) | :x: |\n| webapp/graphite/render/datalib.py | 71.66% <\u00f8> (-0.24%) | :x: |\n| webapp/graphite/render/views.py | 46.75% <\u00f8> (-0.09%) | :x: |\n| webapp/graphite/render/attime.py | 100% <\u00f8> (\u00f8) | :white_check_mark: |\n| webapp/graphite/events/views.py | 78.2% <\u00f8> (+0.57%) | :white_check_mark: |\n| webapp/graphite/util.py | 76.92% <\u00f8> (+0.76%) | :white_check_mark: |\n| webapp/graphite/finders/standard.py | 68.53% <\u00f8> (+1.09%) | :white_check_mark: |\n| ... and 3 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8e61ee2...7e07fab. Read the comment docs.. # Codecov Report\nMerging #1821 into master will increase coverage by 0.01%.\nThe diff coverage is 76.47%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1821      +/-\n==========================================\n+ Coverage   66.21%   66.22%   +0.01%   \n==========================================\n  Files          55       55            \n  Lines        6363     6366       +3   \n  Branches     1258     1258            \n==========================================\n+ Hits         4213     4216       +3   \n  Misses       1932     1932            \n  Partials      218      218\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/remote_storage.py | 38.7% <100%> (\u00f8) | :white_check_mark: |\n| webapp/graphite/storage.py | 54.16% <100%> (+0.38%) | :white_check_mark: |\n| webapp/graphite/render/views.py | 46.65% <100%> (+0.13%) | :white_check_mark: |\n| webapp/graphite/render/datalib.py | 71.9% <50%> (+0.23%) | :white_check_mark: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 07a0ac3...ca53f11. Read the comment docs.. # Codecov Report\nMerging #1822 into master will not change coverage.\nThe diff coverage is 31.57%.\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1822   +/-\n=======================================\n  Coverage   65.88%   65.88%         \n=======================================\n  Files          55       55         \n  Lines        6308     6308         \n  Branches     1248     1248         \n=======================================\n  Hits         4156     4156         \n  Misses       1938     1938         \n  Partials      214      214\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/datalib.py | 71.9% <31.57%> (\u00f8) | :white_check_mark: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8e61ee2...6735a7b. Read the comment docs.. # Codecov Report\nMerging #1824 into master will increase coverage by 0.01%.\nThe diff coverage is 84.37%.\n\n```diff\n@@            Coverage Diff            @@\nmaster   #1824      +/-\n=========================================\n+ Coverage   65.88%   65.9%   +0.01%   \n=========================================\n  Files          55      55            \n  Lines        6311    6326      +15   \n  Branches     1248    1251       +3   \n=========================================\n+ Hits         4158    4169      +11   \n- Misses       1939    1942       +3   \n- Partials      214     215       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/attime.py | 100% <100%> (\u00f8) | :white_check_mark: |\n| webapp/graphite/util.py | 76.92% <100%> (+0.76%) | :white_check_mark: |\n| webapp/graphite/node.py | 88.88% <100%> (+1.93%) | :white_check_mark: |\n| webapp/graphite/render/datalib.py | 71.66% <100%> (-0.24%) | :x: |\n| webapp/graphite/remote_storage.py | 38.41% <33.33%> (-0.44%) | :x: |\n| webapp/graphite/readers.py | 39.14% <50%> (\u00f8) | :white_check_mark: |\n| webapp/graphite/render/views.py | 46.75% <75%> (+0.03%) | :white_check_mark: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6c8a80b...50b7a90. Read the comment docs.. # Codecov Report\nMerging #1829 into master will not change coverage.\nThe diff coverage is n/a.\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1829   +/-\n=======================================\n  Coverage   66.22%   66.22%         \n=======================================\n  Files          55       55         \n  Lines        6365     6365         \n  Branches     1258     1258         \n=======================================\n  Hits         4215     4215         \n  Misses       1932     1932         \n  Partials      218      218\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.59% <\u00f8> (\u00f8) | :white_check_mark: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ee15e37...7bc643d. Read the comment docs.. # Codecov Report\nMerging #1832 into master will increase coverage by 0.14%.\nThe diff coverage is 89.74%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1832      +/-\n==========================================\n+ Coverage   66.22%   66.36%   +0.14%   \n==========================================\n  Files          55       55            \n  Lines        6365     6404      +39   \n  Branches     1258     1267       +9   \n==========================================\n+ Hits         4215     4250      +35   \n- Misses       1932     1934       +2   \n- Partials      218      220       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.6% <89.74%> (\u00f8) | :white_check_mark: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5cdacf3...a52fa8e. Read the comment docs.. # Codecov Report\nMerging #1833 into master will not change coverage.\nThe diff coverage is n/a.\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1833   +/-\n=======================================\n  Coverage   66.36%   66.36%         \n=======================================\n  Files          55       55         \n  Lines        6404     6404         \n  Branches     1267     1267         \n=======================================\n  Hits         4250     4250         \n  Misses       1934     1934         \n  Partials      220      220\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fea2e21...03cc7f4. Read the comment docs.. # Codecov Report\nMerging #1839 into master will decrease coverage by -0.01%.\nThe diff coverage is 60%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1839      +/-\n==========================================\n- Coverage   66.36%   66.35%   -0.01%   \n==========================================\n  Files          55       55            \n  Lines        6404     6409       +5   \n  Branches     1267     1269       +2   \n==========================================\n+ Hits         4250     4253       +3   \n- Misses       1934     1935       +1   \n- Partials      220      221       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/views.py | 46.81% <60%> (+0.16%) | :white_check_mark: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update bd551fa...e2af88f. Read the comment docs.. # Codecov Report\nMerging #1845 into master will not change coverage.\nThe diff coverage is n/a.\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1845   +/-\n=======================================\n  Coverage   66.36%   66.36%         \n=======================================\n  Files          55       55         \n  Lines        6404     6404         \n  Branches     1267     1267         \n=======================================\n  Hits         4250     4250         \n  Misses       1934     1934         \n  Partials      220      220\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update bd551fa...a2fccb8. Read the comment docs.. # Codecov Report\nMerging #1847 into master will increase coverage by 0.29%.\nThe diff coverage is 100%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1847      +/-\n==========================================\n+ Coverage   66.36%   66.65%   +0.29%   \n==========================================\n  Files          55       55            \n  Lines        6404     6460      +56   \n  Branches     1267     1281      +14   \n==========================================\n+ Hits         4250     4306      +56   \n  Misses       1934     1934            \n  Partials      220      220\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.92% <100%> (+0.32%) | :white_check_mark: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 97898ea...9e61499. Read the comment docs.. # Codecov Report\nMerging #1850 into master will not change coverage.\nThe diff coverage is n/a.\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1850   +/-\n=======================================\n  Coverage   66.36%   66.36%         \n=======================================\n  Files          55       55         \n  Lines        6404     6404         \n  Branches     1267     1267         \n=======================================\n  Hits         4250     4250         \n  Misses       1934     1934         \n  Partials      220      220\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 97898ea...a58269b. Read the comment docs.. # Codecov Report\nMerging #1852 into master will not change coverage.\nThe diff coverage is n/a.\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1852   +/-\n=======================================\n  Coverage   66.36%   66.36%         \n=======================================\n  Files          55       55         \n  Lines        6404     6404         \n  Branches     1267     1267         \n=======================================\n  Hits         4250     4250         \n  Misses       1934     1934         \n  Partials      220      220\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 97898ea...097f5a1. Read the comment docs.. # Codecov Report\nMerging #1855 into master will decrease coverage by 0.04%.\nThe diff coverage is 100%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1855      +/-\n==========================================\n- Coverage   66.36%   66.32%   -0.05%   \n==========================================\n  Files          55       55            \n  Lines        6404     6396       -8   \n  Branches     1267     1266       -1   \n==========================================\n- Hits         4250     4242       -8   \n  Misses       1934     1934            \n  Partials      220      220\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.55% <100%> (-0.05%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 80c2a29...418e09e. Read the comment docs.. # Codecov Report\nMerging #1858 into master will not change coverage.\nThe diff coverage is n/a.\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1858   +/-\n=======================================\n  Coverage   66.35%   66.35%         \n=======================================\n  Files          56       56         \n  Lines        6596     6596         \n  Branches     1317     1317         \n=======================================\n  Hits         4377     4377         \n  Misses       1996     1996         \n  Partials      223      223\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3f24ba7...46acfeb. Read the comment docs.. # Codecov Report\nMerging #1859 into master will not change coverage.\nThe diff coverage is n/a.\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1859   +/-\n=======================================\n  Coverage   66.35%   66.35%         \n=======================================\n  Files          56       56         \n  Lines        6596     6596         \n  Branches     1317     1317         \n=======================================\n  Hits         4377     4377         \n  Misses       1996     1996         \n  Partials      223      223\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3f24ba7...c331817. Read the comment docs.. # Codecov Report\nMerging #1864 into master will decrease coverage by 0.02%.\nThe diff coverage is 0%.\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1864      +/-\n==========================================\n- Coverage   66.35%   66.33%   -0.03%   \n==========================================\n  Files          56       56            \n  Lines        6596     6598       +2   \n  Branches     1317     1318       +1   \n==========================================\n  Hits         4377     4377            \n- Misses       1996     1998       +2   \n  Partials      223      223\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/storage.py | 56.55% <0%> (-0.8%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2ed7d96...5f736a8. Read the comment docs.. # Codecov Report\nMerging #1870 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1870   +/-\n=======================================\n  Coverage   66.33%   66.33%         \n=======================================\n  Files          56       56         \n  Lines        6598     6598         \n  Branches     1318     1318         \n=======================================\n  Hits         4377     4377         \n  Misses       1998     1998         \n  Partials      223      223\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3e6c9f5...331820f. Read the comment docs.. # Codecov Report\nMerging #1875 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1875   +/-\n=======================================\n  Coverage   66.33%   66.33%         \n=======================================\n  Files          56       56         \n  Lines        6598     6598         \n  Branches     1318     1318         \n=======================================\n  Hits         4377     4377         \n  Misses       1998     1998         \n  Partials      223      223\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 053b918...0bb5a0e. Read the comment docs.. # Codecov Report\nMerging #1876 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1876   +/-\n=======================================\n  Coverage   66.33%   66.33%         \n=======================================\n  Files          56       56         \n  Lines        6598     6598         \n  Branches     1318     1318         \n=======================================\n  Hits         4377     4377         \n  Misses       1998     1998         \n  Partials      223      223\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 053b918...4609bfe. Read the comment docs.. # Codecov Report\nMerging #1878 into master will decrease coverage by 0.02%.\nThe diff coverage is 40%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1878      +/-\n==========================================\n- Coverage   66.33%   66.31%   -0.03%   \n==========================================\n  Files          56       56            \n  Lines        6598     6602       +4   \n  Branches     1318     1318            \n==========================================\n+ Hits         4377     4378       +1   \n- Misses       1998     2001       +3   \n  Partials      223      223\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/readers.py | 41.15% <40%> (-0.28%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 053b918...c9dd03c. Read the comment docs.. # Codecov Report\nMerging #1882 into master will increase coverage by 0.01%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1882      +/-\n==========================================\n+ Coverage   66.33%   66.34%   +0.01%   \n==========================================\n  Files          56       56            \n  Lines        6598     6606       +8   \n  Branches     1318     1322       +4   \n==========================================\n+ Hits         4377     4383       +6   \n+ Misses       1998     1993       -5   \n- Partials      223      230       +7\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/remote_storage.py | 30.54% <0%> (\u00f8) | :arrow_up: |\n| webapp/graphite/storage.py | 57.51% <0%> (+0.96%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 550a047...014a59a. Read the comment docs.. # Codecov Report\nMerging #1884 into 1.0.x will decrease coverage by <.01%.\nThe diff coverage is 50%.\n\n\n```diff\n@@            Coverage Diff             @@\n1.0.x    #1884      +/-\n==========================================\n- Coverage   66.34%   66.34%   -0.01%   \n==========================================\n  Files          56       56            \n  Lines        6606     6597       -9   \n  Branches     1322     1317       -5   \n==========================================\n- Hits         4383     4377       -6   \n- Misses       1993     1997       +4   \n+ Partials      230      223       -7\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/remote_storage.py | 30.54% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/readers.py | 41.25% <0%> (-0.18%) | :arrow_down: |\n| webapp/graphite/settings.py | 75.15% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/storage.py | 57.34% <0%> (-0.18%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 014a59a...32159d7. Read the comment docs.. # Codecov Report\nMerging #1888 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #1888   +/-\n======================================\n  Coverage    66.3%   66.3%         \n======================================\n  Files          56      56         \n  Lines        6603    6603         \n  Branches     1318    1318         \n======================================\n  Hits         4378    4378         \n  Misses       2002    2002         \n  Partials      223     223\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 75.15% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5fe6ab3...258ea17. Read the comment docs.. # Codecov Report\nMerging #1889 into 1.0.x will decrease coverage by 0.04%.\nThe diff coverage is 44.44%.\n\n\n```diff\n@@            Coverage Diff            @@\n1.0.x   #1889      +/-\n=========================================\n- Coverage   66.34%   66.3%   -0.05%   \n=========================================\n  Files          56      56            \n  Lines        6597    6603       +6   \n  Branches     1317    1318       +1   \n=========================================\n+ Hits         4377    4378       +1   \n- Misses       1997    2002       +5   \n  Partials      223     223\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/storage.py | 56.55% <0%> (-0.8%) | :arrow_down: |\n| webapp/graphite/settings.py | 75.15% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/readers.py | 40.98% <40%> (-0.27%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 422acce...67c1860. Read the comment docs.. # Codecov Report\nMerging #1890 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #1890   +/-\n======================================\n  Coverage    66.3%   66.3%         \n======================================\n  Files          56      56         \n  Lines        6603    6603         \n  Branches     1318    1318         \n======================================\n  Hits         4378    4378         \n  Misses       2002    2002         \n  Partials      223     223\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e0f3b5f...0462608. Read the comment docs.. # Codecov Report\nMerging #1891 into master will decrease coverage by 0.38%.\nThe diff coverage is 72.41%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1891      +/-\n==========================================\n- Coverage    66.3%   65.91%   -0.39%   \n==========================================\n  Files          56       57       +1   \n  Lines        6603     6645      +42   \n  Branches     1318     1321       +3   \n==========================================\n+ Hits         4378     4380       +2   \n- Misses       2002     2043      +41   \n+ Partials      223      222       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/carbonlink.py | 30.72% <59.45%> (-20.74%) | :arrow_down: |\n| webapp/graphite/readers.py | 41.46% <80%> (+0.47%) | :arrow_up: |\n| webapp/graphite/singleton.py | 82.22% <82.22%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update bbfab10...33e4906. Read the comment docs.. # Codecov Report\nMerging #1893 into master will increase coverage by 0.57%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1893      +/-\n==========================================\n+ Coverage   65.91%   66.48%   +0.57%   \n==========================================\n  Files          57       57            \n  Lines        6645     6645            \n  Branches     1321     1321            \n==========================================\n+ Hits         4380     4418      +38   \n+ Misses       2043     2001      -42   \n- Partials      222      226       +4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/carbonlink.py | 53.61% <100%> (+22.89%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 36aebcd...0f77579. Read the comment docs.. # Codecov Report\nMerging #1894 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1894   +/-\n=======================================\n  Coverage   65.91%   65.91%         \n=======================================\n  Files          57       57         \n  Lines        6645     6645         \n  Branches     1321     1321         \n=======================================\n  Hits         4380     4380         \n  Misses       2043     2043         \n  Partials      222      222\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 36aebcd...4111e0b. Read the comment docs.. # Codecov Report\nMerging #1899 into 1.0.x will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff          @@\n1.0.x   #1899   +/-\n=====================================\n  Coverage   66.3%   66.3%         \n=====================================\n  Files         56      56         \n  Lines       6603    6603         \n  Branches    1318    1318         \n=====================================\n  Hits        4378    4378         \n  Misses      2002    2002         \n  Partials     223     223\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9c794a1...69b91ec. Read the comment docs.. # Codecov Report\nMerging #1901 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1901   +/-\n=======================================\n  Coverage   66.48%   66.48%         \n=======================================\n  Files          57       57         \n  Lines        6645     6645         \n  Branches     1321     1321         \n=======================================\n  Hits         4418     4418         \n  Misses       2001     2001         \n  Partials      226      226\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 48e96b9...5391c0b. Read the comment docs.. # Codecov Report\nMerging #1902 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1902   +/-\n=======================================\n  Coverage   66.48%   66.48%         \n=======================================\n  Files          57       57         \n  Lines        6645     6645         \n  Branches     1321     1321         \n=======================================\n  Hits         4418     4418         \n  Misses       2001     2001         \n  Partials      226      226\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7ea6d42...756d7ef. Read the comment docs.. # Codecov Report\nMerging #1903 into 1.0.x will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff          @@\n1.0.x   #1903   +/-\n=====================================\n  Coverage   66.3%   66.3%         \n=====================================\n  Files         56      56         \n  Lines       6603    6603         \n  Branches    1318    1318         \n=====================================\n  Hits        4378    4378         \n  Misses      2002    2002         \n  Partials     223     223\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ede7330...7e3e09b. Read the comment docs.. # Codecov Report\nMerging #1906 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1906   +/-\n=======================================\n  Coverage   66.48%   66.48%         \n=======================================\n  Files          57       57         \n  Lines        6645     6645         \n  Branches     1321     1321         \n=======================================\n  Hits         4418     4418         \n  Misses       2001     2001         \n  Partials      226      226\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/metrics/views.py | 79.67% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5e2f9ae...83e72d4. Read the comment docs.\n. # Codecov Report\nMerging #1907 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1907   +/-\n=======================================\n  Coverage   66.48%   66.48%         \n=======================================\n  Files          57       57         \n  Lines        6645     6645         \n  Branches     1321     1321         \n=======================================\n  Hits         4418     4418         \n  Misses       2001     2001         \n  Partials      226      226\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5e2f9ae...53682d2. Read the comment docs.\n. # Codecov Report\nMerging #1908 into master will decrease coverage by 0.01%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1908      +/-\n==========================================\n- Coverage   66.48%   66.47%   -0.01%   \n==========================================\n  Files          57       57            \n  Lines        6645     6646       +1   \n  Branches     1321     1322       +1   \n==========================================\n  Hits         4418     4418            \n- Misses       2001     2002       +1   \n  Partials      226      226\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/datalib.py | 60.95% <0%> (-0.43%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5e2f9ae...132c35d. Read the comment docs.\n. # Codecov Report\nMerging #1909 into master will decrease coverage by 0.01%.\nThe diff coverage is 12.5%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1909      +/-\n==========================================\n- Coverage   66.47%   66.46%   -0.02%   \n==========================================\n  Files          57       57            \n  Lines        6646     6647       +1   \n  Branches     1322     1322            \n==========================================\n  Hits         4418     4418            \n- Misses       2002     2003       +1   \n  Partials      226      226\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/remote_storage.py | 30.39% <12.5%> (-0.15%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a8bff5b...766503c. Read the comment docs.\n. # Codecov Report\nMerging #1910 into 1.0.x will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@          Coverage Diff          @@\n1.0.x   #1910   +/-\n=====================================\n  Coverage   66.3%   66.3%         \n=====================================\n  Files         56      56         \n  Lines       6603    6603         \n  Branches    1318    1318         \n=====================================\n  Hits        4378    4378         \n- Misses      2001    2002    +1   \n+ Partials     224     223    -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/metrics/views.py | 79.67% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/remote_storage.py | 30.54% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7421750...78a9575. Read the comment docs.\n. # Codecov Report\nMerging #1911 into 1.0.x will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\n1.0.x    #1911   +/-\n=======================================\n  Coverage   66.28%   66.28%         \n=======================================\n  Files          56       56         \n  Lines        6605     6605         \n  Branches     1319     1319         \n=======================================\n  Hits         4378     4378         \n  Misses       2004     2004         \n  Partials      223      223\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e41d3ca...e59ea21. Read the comment docs.\n. # Codecov Report\nMerging #1916 into master will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1916      +/-\n==========================================\n+ Coverage   69.21%   69.23%   +0.02%   \n==========================================\n  Files          78       78            \n  Lines        7507     7512       +5   \n  Branches     1458     1460       +2   \n==========================================\n+ Hits         5196     5201       +5   \n  Misses       2037     2037            \n  Partials      274      274\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 88.05% <100%> (+0.03%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3f2112a...bae5937. Read the comment docs.\n. # Codecov Report\nMerging #1920 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1920   +/-\n=======================================\n  Coverage   66.46%   66.46%         \n=======================================\n  Files          57       57         \n  Lines        6647     6647         \n  Branches     1322     1322         \n=======================================\n  Hits         4418     4418         \n  Misses       2003     2003         \n  Partials      226      226\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 622830d...f7966e0. Read the comment docs.\n. # Codecov Report\nMerging #1921 into 1.0.x will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\n1.0.x    #1921   +/-\n=======================================\n  Coverage   66.28%   66.28%         \n=======================================\n  Files          56       56         \n  Lines        6605     6605         \n  Branches     1319     1319         \n=======================================\n  Hits         4378     4378         \n- Misses       2003     2004    +1   \n+ Partials      224      223    -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/remote_storage.py | 30.39% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 479516a...d0f9bce. Read the comment docs.\n. # Codecov Report\nMerging #1924 into master will decrease coverage by 0.01%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1924      +/-\n==========================================\n- Coverage   66.46%   66.44%   -0.02%   \n==========================================\n  Files          57       57            \n  Lines        6647     6649       +2   \n  Branches     1322     1323       +1   \n==========================================\n  Hits         4418     4418            \n- Misses       2003     2005       +2   \n  Partials      226      226\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/remote_storage.py | 30.09% <0%> (-0.3%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8ab9719...26ccb39. Read the comment docs.\n. # Codecov Report\nMerging #1926 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1926   +/-\n=======================================\n  Coverage   66.44%   66.44%         \n=======================================\n  Files          57       57         \n  Lines        6649     6649         \n  Branches     1323     1323         \n=======================================\n  Hits         4418     4418         \n  Misses       2005     2005         \n  Partials      226      226\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 75.15% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8ce51a1...c7ebc2a. Read the comment docs.\n. # Codecov Report\nMerging #1927 into 1.0.x will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\n1.0.x    #1927   +/-\n=======================================\n  Coverage   66.28%   66.28%         \n=======================================\n  Files          56       56         \n  Lines        6605     6605         \n  Branches     1319     1319         \n=======================================\n  Hits         4378     4378         \n  Misses       2004     2004         \n  Partials      223      223\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 75.15% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 79daefa...aa2fc1f. Read the comment docs.\n. # Codecov Report\nMerging #1928 into master will increase coverage by 0.07%.\nThe diff coverage is 90%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1928      +/-\n==========================================\n+ Coverage   66.44%   66.51%   +0.07%   \n==========================================\n  Files          57       57            \n  Lines        6649     6669      +20   \n  Branches     1323     1327       +4   \n==========================================\n+ Hits         4418     4436      +18   \n- Misses       2005     2006       +1   \n- Partials      226      227       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.87% <90%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 25bcbdd...1cb5e91. Read the comment docs.\n. # Codecov Report\nMerging #1933 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1933   +/-\n=======================================\n  Coverage   66.44%   66.44%         \n=======================================\n  Files          57       57         \n  Lines        6649     6649         \n  Branches     1323     1323         \n=======================================\n  Hits         4418     4418         \n  Misses       2005     2005         \n  Partials      226      226\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6619657...ccee6e8. Read the comment docs.\n. # Codecov Report\nMerging #1935 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1935   +/-\n=======================================\n  Coverage   66.44%   66.44%         \n=======================================\n  Files          57       57         \n  Lines        6649     6649         \n  Branches     1323     1323         \n=======================================\n  Hits         4418     4418         \n  Misses       2005     2005         \n  Partials      226      226\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.87% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6619657...6a9f439. Read the comment docs.\n. # Codecov Report\nMerging #1937 into master will increase coverage by 0.09%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1937      +/-\n==========================================\n+ Coverage   66.44%   66.54%   +0.09%   \n==========================================\n  Files          57       57            \n  Lines        6649     6680      +31   \n  Branches     1323     1330       +7   \n==========================================\n+ Hits         4418     4445      +27   \n- Misses       2005     2007       +2   \n- Partials      226      228       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.82% <100%> (-0.05%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2a5511a...a43e320. Read the comment docs.\n. # Codecov Report\nMerging #1938 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1938   +/-\n=======================================\n  Coverage   66.54%   66.54%         \n=======================================\n  Files          57       57         \n  Lines        6680     6680         \n  Branches     1330     1330         \n=======================================\n  Hits         4445     4445         \n  Misses       2007     2007         \n  Partials      228      228\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.82% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 504154d...e4fc9bf. Read the comment docs.\n. # Codecov Report\nMerging #1939 into master will decrease coverage by 0.03%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #1939      +/-\n=========================================\n- Coverage   66.54%   66.5%   -0.04%   \n=========================================\n  Files          57      57            \n  Lines        6680    6684       +4   \n  Branches     1330    1332       +2   \n=========================================\n  Hits         4445    4445            \n- Misses       2007    2011       +4   \n  Partials      228     228\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/remote_storage.py | 29.52% <0%> (-0.58%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 504154d...2d1f18b. Read the comment docs.\n. # Codecov Report\nMerging #1942 into master will increase coverage by 0.14%.\nThe diff coverage is 48.73%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1942      +/-\n==========================================\n+ Coverage    66.5%   66.65%   +0.14%   \n==========================================\n  Files          57       65       +8   \n  Lines        6684     6717      +33   \n  Branches     1332     1331       -1   \n==========================================\n+ Hits         4445     4477      +32   \n- Misses       2011     2012       +1   \n  Partials      228      228\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/remote_storage.py | 100% <100%> (+70.47%) | :arrow_up: |\n| webapp/graphite/readers/__init__.py | 100% <100%> (\u00f8) | |\n| webapp/graphite/finders/ceres.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/http_pool.py | 100% <100%> (\u00f8) | |\n| webapp/graphite/readers/remote.py | 18.48% <18.48%> (\u00f8) | |\n| webapp/graphite/readers/utils.py | 35.84% <35.84%> (\u00f8) | |\n| webapp/graphite/readers/rrd.py | 36.2% <36.2%> (\u00f8) | |\n| webapp/graphite/readers/ceres.py | 45.16% <45.16%> (\u00f8) | |\n| webapp/graphite/finders/remote.py | 48.35% <48.35%> (\u00f8) | |\n| webapp/graphite/finders/utils.py | 50% <50%> (\u00f8) | |\n| ... and 13 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1e5cf9f...e4cba2d. Read the comment docs.\n. # Codecov Report\nMerging #1946 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1946   +/-\n=======================================\n  Coverage   66.65%   66.65%         \n=======================================\n  Files          65       65         \n  Lines        6717     6717         \n  Branches     1331     1331         \n=======================================\n  Hits         4477     4477         \n  Misses       2012     2012         \n  Partials      228      228\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9c6dc88...588ba5d. Read the comment docs.\n. # Codecov Report\nMerging #1947 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1947   +/-\n=======================================\n  Coverage   66.65%   66.65%         \n=======================================\n  Files          65       65         \n  Lines        6717     6717         \n  Branches     1331     1331         \n=======================================\n  Hits         4477     4477         \n  Misses       2012     2012         \n  Partials      228      228\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9b6726b...f00fa61. Read the comment docs.\n. # Codecov Report\nMerging #1948 into master will decrease coverage by <.01%.\nThe diff coverage is 76.19%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1948      +/-\n==========================================\n- Coverage   66.65%   66.64%   -0.01%   \n==========================================\n  Files          65       65            \n  Lines        6717     6730      +13   \n  Branches     1331     1334       +3   \n==========================================\n+ Hits         4477     4485       +8   \n- Misses       2012     2015       +3   \n- Partials      228      230       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 75.46% <100%> (+0.3%) | :arrow_up: |\n| webapp/graphite/storage.py | 56.77% <63.63%> (-0.08%) | :arrow_down: |\n| webapp/graphite/logger.py | 83.33% <85.71%> (-1.45%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fabd783...946f8f6. Read the comment docs.\n. # Codecov Report\nMerging #1949 into master will increase coverage by 2.52%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1949      +/-\n==========================================\n+ Coverage   66.64%   69.16%   +2.52%   \n==========================================\n  Files          65       65            \n  Lines        6730     6730            \n  Branches     1334     1334            \n==========================================\n+ Hits         4485     4655     +170   \n+ Misses       2015     1831     -184   \n- Partials      230      244      +14\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/readers/rrd.py | 41.37% <0%> (+5.17%) | :arrow_up: |\n| webapp/graphite/intervals.py | 60.71% <0%> (+10.71%) | :arrow_up: |\n| webapp/graphite/readers/whisper.py | 89.55% <0%> (+23.88%) | :arrow_up: |\n| webapp/graphite/readers/ceres.py | 93.54% <0%> (+48.38%) | :arrow_up: |\n| webapp/graphite/readers/utils.py | 90.56% <0%> (+54.71%) | :arrow_up: |\n| webapp/graphite/readers/remote.py | 76.47% <0%> (+57.98%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cd44980...bf6628d. Read the comment docs.\n. # Codecov Report\nMerging #1950 into master will decrease coverage by 0.02%.\nThe diff coverage is 65.62%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1950      +/-\n==========================================\n- Coverage   66.64%   66.61%   -0.03%   \n==========================================\n  Files          65       65            \n  Lines        6730     6794      +64   \n  Branches     1334     1345      +11   \n==========================================\n+ Hits         4485     4526      +41   \n- Misses       2015     2035      +20   \n- Partials      230      233       +3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/glyph.py | 31.89% <\u00f8> (-0.2%) | :arrow_down: |\n| webapp/graphite/wsgi.py | 41.37% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/app_settings.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/functions.py | 89.73% <\u00f8> (-0.1%) | :arrow_down: |\n| webapp/graphite/events/models.py | 82.35% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/singleton.py | 82.22% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/readers/utils.py | 35.84% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/compat.py | 77.27% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/storage.py | 56.77% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/events/compat.py | 75% <\u00f8> (\u00f8) | :arrow_up: |\n| ... and 32 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cd44980...b44d90c. Read the comment docs.\n. # Codecov Report\nMerging #1953 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1953   +/-\n=======================================\n  Coverage   69.16%   69.16%         \n=======================================\n  Files          65       65         \n  Lines        6730     6730         \n  Branches     1334     1334         \n=======================================\n  Hits         4655     4655         \n  Misses       1831     1831         \n  Partials      244      244\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d8cdaa7...9e706a0. Read the comment docs.\n. # Codecov Report\nMerging #1954 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1954   +/-\n=======================================\n  Coverage   69.16%   69.16%         \n=======================================\n  Files          65       65         \n  Lines        6730     6730         \n  Branches     1334     1334         \n=======================================\n  Hits         4655     4655         \n  Misses       1831     1831         \n  Partials      244      244\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0c168cd...2918e48. Read the comment docs.\n. # Codecov Report\nMerging #1956 into master will decrease coverage by 0.01%.\nThe diff coverage is 50%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1956      +/-\n==========================================\n- Coverage   69.16%   69.15%   -0.02%   \n==========================================\n  Files          65       65            \n  Lines        6729     6730       +1   \n  Branches     1334     1335       +1   \n==========================================\n  Hits         4654     4654            \n  Misses       1831     1831            \n- Partials      244      245       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/app_settings.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/settings.py | 75.15% <50%> (-0.31%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update bca7145...3c2c718. Read the comment docs.\n. # Codecov Report\nMerging #1958 into master will decrease coverage by 0.21%.\nThe diff coverage is 64.02%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1958      +/-\n==========================================\n- Coverage   69.16%   68.95%   -0.22%   \n==========================================\n  Files          65       66       +1   \n  Lines        6730     6902     +172   \n  Branches     1334     1348      +14   \n==========================================\n+ Hits         4655     4759     +104   \n- Misses       1831     1910      +79   \n+ Partials      244      233      -11\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/node.py | 92.59% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/settings.py | 75.46% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/finders/standard.py | 68.88% <100%> (+0.34%) | :arrow_up: |\n| webapp/graphite/readers/ceres.py | 92% <100%> (-1.55%) | :arrow_down: |\n| webapp/graphite/remote_storage.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/finders/ceres.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/readers/whisper.py | 88.13% <100%> (-1.42%) | :arrow_down: |\n| webapp/graphite/readers/__init__.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/logger.py | 84% <100%> (+0.66%) | :arrow_up: |\n| webapp/graphite/render/views.py | 45.72% <33.33%> (-0.62%) | :arrow_down: |\n| ... and 12 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update aa312df...be670dc. Read the comment docs.\n. # Codecov Report\nMerging #1959 into master will decrease coverage by 0.01%.\nThe diff coverage is 50%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1959      +/-\n==========================================\n- Coverage   69.16%   69.15%   -0.02%   \n==========================================\n  Files          65       65            \n  Lines        6729     6730       +1   \n  Branches     1334     1335       +1   \n==========================================\n  Hits         4654     4654            \n  Misses       1831     1831            \n- Partials      244      245       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/app_settings.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/settings.py | 75.15% <50%> (-0.31%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update bca7145...3c2c718. Read the comment docs.\n. # Codecov Report\nMerging #1960 into master will increase coverage by 0.19%.\nThe diff coverage is 95.94%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1960      +/-\n==========================================\n+ Coverage   68.93%   69.13%   +0.19%   \n==========================================\n  Files          66       70       +4   \n  Lines        6902     6953      +51   \n  Branches     1349     1352       +3   \n==========================================\n+ Hits         4758     4807      +49   \n- Misses       1910     1912       +2   \n  Partials      234      234\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/urls.py | 90.9% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/account/models.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/middleware.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/events/views.py | 77.92% <100%> (-0.29%) | :arrow_down: |\n| webapp/graphite/events/migrations/0001_initial.py | 100% <100%> (\u00f8) | |\n| webapp/graphite/url_shortener/views.py | 52.38% <100%> (+7.93%) | :arrow_up: |\n| .../graphite/url_shortener/migrations/0001_initial.py | 100% <100%> (\u00f8) | |\n| webapp/graphite/account/views.py | 30.76% <100%> (+5.76%) | :arrow_up: |\n| webapp/graphite/account/migrations/0001_initial.py | 100% <100%> (\u00f8) | |\n| ...bapp/graphite/dashboard/migrations/0001_initial.py | 100% <100%> (\u00f8) | |\n| ... and 9 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update af76801...f05d4a2. Read the comment docs.\n. # Codecov Report\nMerging #1961 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1961   +/-\n=======================================\n  Coverage   68.93%   68.93%         \n=======================================\n  Files          66       66         \n  Lines        6902     6902         \n  Branches     1349     1349         \n=======================================\n  Hits         4758     4758         \n  Misses       1910     1910         \n  Partials      234      234\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.82% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update af76801...7b03ccc. Read the comment docs.\n. # Codecov Report\nMerging #1962 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1962      +/-\n==========================================\n+ Coverage   68.93%   68.94%   +<.01%   \n==========================================\n  Files          66       66            \n  Lines        6902     6903       +1   \n  Branches     1349     1349            \n==========================================\n+ Hits         4758     4759       +1   \n  Misses       1910     1910            \n  Partials      234      234\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 75.3% <100%> (+0.14%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update af76801...b01da76. Read the comment docs.\n. # Codecov Report\nMerging #1962 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1962      +/-\n==========================================\n+ Coverage   68.93%   68.94%   +<.01%   \n==========================================\n  Files          66       66            \n  Lines        6902     6903       +1   \n  Branches     1349     1349            \n==========================================\n+ Hits         4758     4759       +1   \n  Misses       1910     1910            \n  Partials      234      234\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 75.3% <100%> (+0.14%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update af76801...b01da76. Read the comment docs.\n. # Codecov Report\nMerging #1963 into 1.0.x will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\n1.0.x    #1963      +/-\n==========================================\n+ Coverage   66.28%   66.28%   +<.01%   \n==========================================\n  Files          56       56            \n  Lines        6605     6606       +1   \n  Branches     1319     1319            \n==========================================\n+ Hits         4378     4379       +1   \n  Misses       2004     2004            \n  Partials      223      223\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 75.3% <100%> (+0.15%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3c90fe2...5a327ce. Read the comment docs.\n. # Codecov Report\nMerging #1964 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1964   +/-\n=======================================\n  Coverage   69.14%   69.14%         \n=======================================\n  Files          70       70         \n  Lines        6954     6954         \n  Branches     1352     1352         \n=======================================\n  Hits         4808     4808         \n  Misses       1912     1912         \n  Partials      234      234\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a8908ea...ecf8325. Read the comment docs.\n. # Codecov Report\nMerging #1965 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1965      +/-\n==========================================\n+ Coverage   69.14%   69.14%   +<.01%   \n==========================================\n  Files          70       70            \n  Lines        6954     6956       +2   \n  Branches     1352     1352            \n==========================================\n+ Hits         4808     4810       +2   \n  Misses       1912     1912            \n  Partials      234      234\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.83% <100%> (+0.01%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a8908ea...79a9d5f. Read the comment docs.\n. # Codecov Report\nMerging #1968 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1968   +/-\n=======================================\n  Coverage   69.14%   69.14%         \n=======================================\n  Files          70       70         \n  Lines        6954     6954         \n  Branches     1352     1352         \n=======================================\n  Hits         4808     4808         \n  Misses       1912     1912         \n  Partials      234      234\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 254820a...c21e1b9. Read the comment docs.\n. # Codecov Report\nMerging #1969 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1969   +/-\n=======================================\n  Coverage   69.14%   69.14%         \n=======================================\n  Files          70       70         \n  Lines        6954     6954         \n  Branches     1352     1352         \n=======================================\n  Hits         4808     4808         \n  Misses       1912     1912         \n  Partials      234      234\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/utils.py | 41.37% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 254820a...9f92b65. Read the comment docs.\n. # Codecov Report\nMerging #1970 into master will decrease coverage by 0.04%.\nThe diff coverage is 52.85%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1970      +/-\n==========================================\n- Coverage   69.14%   69.09%   -0.05%   \n==========================================\n  Files          70       71       +1   \n  Lines        6954     6964      +10   \n  Branches     1352     1355       +3   \n==========================================\n+ Hits         4808     4812       +4   \n- Misses       1912     1918       +6   \n  Partials      234      234\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/evaluator.py | 72.41% <\u00f8> (-3.98%) | :arrow_down: |\n| webapp/graphite/readers/remote.py | 82.57% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/views.py | 46.56% <100%> (+0.84%) | :arrow_up: |\n| webapp/graphite/storage.py | 48.42% <14.28%> (-0.63%) | :arrow_down: |\n| webapp/graphite/render/datalib.py | 64.19% <41.86%> (-7.68%) | :arrow_down: |\n| webapp/graphite/finders/remote.py | 48.61% <50%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/utils.py | 93.33% <93.33%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 52f2d00...e5aa024. Read the comment docs.\n. # Codecov Report\nMerging #1974 into 1.0.x will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\n1.0.x    #1974   +/-\n=======================================\n  Coverage   66.28%   66.28%         \n=======================================\n  Files          56       56         \n  Lines        6606     6606         \n  Branches     1319     1319         \n=======================================\n  Hits         4379     4379         \n  Misses       2004     2004         \n  Partials      223      223\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 71d41d2...405dc54. Read the comment docs.\n. # Codecov Report\nMerging #1975 into 1.0.x will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\n1.0.x    #1975   +/-\n=======================================\n  Coverage   66.28%   66.28%         \n=======================================\n  Files          56       56         \n  Lines        6606     6606         \n  Branches     1319     1319         \n=======================================\n  Hits         4379     4379         \n  Misses       2004     2004         \n  Partials      223      223\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 71d41d2...16cff61. Read the comment docs.\n. # Codecov Report\nMerging #1977 into 1.0.x will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\n1.0.x    #1977      +/-\n==========================================\n+ Coverage   66.28%   66.29%   +0.01%   \n==========================================\n  Files          56       56            \n  Lines        6606     6608       +2   \n  Branches     1319     1319            \n==========================================\n+ Hits         4379     4381       +2   \n  Misses       2004     2004            \n  Partials      223      223\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.88% <100%> (+0.01%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7d5d1f6...f31d195. Read the comment docs.\n. # Codecov Report\nMerging #1978 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #1978   +/-\n======================================\n  Coverage    69.1%   69.1%         \n======================================\n  Files          71      71         \n  Lines        6966    6966         \n  Branches     1355    1355         \n======================================\n  Hits         4814    4814         \n  Misses       1918    1918         \n  Partials      234     234\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 36ec377...f9edf00. Read the comment docs.\n. # Codecov Report\nMerging #1980 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #1980   +/-\n======================================\n  Coverage    69.1%   69.1%         \n======================================\n  Files          71      71         \n  Lines        6966    6966         \n  Branches     1355    1355         \n======================================\n  Hits         4814    4814         \n  Misses       1918    1918         \n  Partials      234     234\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 829db8e...800a93c. Read the comment docs.\n. # Codecov Report\nMerging #1981 into master will increase coverage by 0.41%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1981      +/-\n==========================================\n+ Coverage    69.1%   69.52%   +0.41%   \n==========================================\n  Files          71       71            \n  Lines        6966     6966            \n  Branches     1355     1355            \n==========================================\n+ Hits         4814     4843      +29   \n+ Misses       1918     1882      -36   \n- Partials      234      241       +7\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/browser/views.py | 66.47% <100%> (+17.05%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 829db8e...5c653de. Read the comment docs.\n. # Codecov Report\nMerging #1982 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #1982   +/-\n======================================\n  Coverage    69.1%   69.1%         \n======================================\n  Files          71      71         \n  Lines        6966    6966         \n  Branches     1355    1355         \n======================================\n  Hits         4814    4814         \n  Misses       1918    1918         \n  Partials      234     234\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/dashboard/views.py | 99.25% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 829db8e...426eaf1. Read the comment docs.\n. # Codecov Report\nMerging #1987 into master will decrease coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #1987      +/-\n=========================================\n- Coverage   69.52%   69.5%   -0.03%   \n=========================================\n  Files          71      71            \n  Lines        6966    6961       -5   \n  Branches     1355    1356       +1   \n=========================================\n- Hits         4843    4838       -5   \n  Misses       1882    1882            \n  Partials      241     241\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/util.py | 78.94% <100%> (-0.6%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 37980e1...5b70edd. Read the comment docs.\n. # Codecov Report\nMerging #1989 into master will decrease coverage by 0.04%.\nThe diff coverage is 92%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #1989      +/-\n==========================================\n- Coverage    69.5%   69.45%   -0.05%   \n==========================================\n  Files          71       71            \n  Lines        6961     6953       -8   \n  Branches     1356     1357       +1   \n==========================================\n- Hits         4838     4829       -9   \n  Misses       1882     1882            \n- Partials      241      242       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/views.py | 46.56% <0%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/attime.py | 99.13% <95.83%> (-0.87%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c9207d0...90fe80b. Read the comment docs.\n. # Codecov Report\nMerging #1990 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1990   +/-\n=======================================\n  Coverage   69.45%   69.45%         \n=======================================\n  Files          71       71         \n  Lines        6953     6953         \n  Branches     1357     1357         \n=======================================\n  Hits         4829     4829         \n  Misses       1882     1882         \n  Partials      242      242\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ac103b1...2645b82. Read the comment docs.\n. # Codecov Report\nMerging #1992 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1992   +/-\n=======================================\n  Coverage   69.45%   69.45%         \n=======================================\n  Files          71       71         \n  Lines        6953     6953         \n  Branches     1357     1357         \n=======================================\n  Hits         4829     4829         \n  Misses       1882     1882         \n  Partials      242      242\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ac103b1...4dda257. Read the comment docs.\n. # Codecov Report\nMerging #1993 into 1.0.x will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\n1.0.x    #1993   +/-\n=======================================\n  Coverage   66.29%   66.29%         \n=======================================\n  Files          56       56         \n  Lines        6608     6608         \n  Branches     1319     1319         \n=======================================\n  Hits         4381     4381         \n  Misses       2004     2004         \n  Partials      223      223\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 47f529c...19ff850. Read the comment docs.\n. # Codecov Report\nMerging #1994 into 1.0.x will increase coverage by 0.07%.\nThe diff coverage is 90%.\n\n\n```diff\n@@            Coverage Diff             @@\n1.0.x    #1994      +/-\n==========================================\n+ Coverage   66.29%   66.36%   +0.07%   \n==========================================\n  Files          56       56            \n  Lines        6608     6628      +20   \n  Branches     1319     1323       +4   \n==========================================\n+ Hits         4381     4399      +18   \n- Misses       2004     2005       +1   \n- Partials      223      224       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.88% <90%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 47f529c...3702a71. Read the comment docs.\n. # Codecov Report\nMerging #1995 into 1.0.x will decrease coverage by 0.01%.\nThe diff coverage is 50%.\n\n\n```diff\n@@            Coverage Diff             @@\n1.0.x    #1995      +/-\n==========================================\n- Coverage   66.29%   66.28%   -0.02%   \n==========================================\n  Files          56       56            \n  Lines        6608     6609       +1   \n  Branches     1319     1320       +1   \n==========================================\n  Hits         4381     4381            \n  Misses       2004     2004            \n- Partials      223      224       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/app_settings.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/settings.py | 75% <50%> (-0.31%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 47f529c...9160cde. Read the comment docs.\n. # Codecov Report\nMerging #1996 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #1996   +/-\n=======================================\n  Coverage   69.45%   69.45%         \n=======================================\n  Files          71       71         \n  Lines        6953     6953         \n  Branches     1357     1357         \n=======================================\n  Hits         4829     4829         \n  Misses       1882     1882         \n  Partials      242      242\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ac216de...ee0b918. Read the comment docs.\n. # Codecov Report\nMerging #1997 into 1.0.x will decrease coverage by 0.05%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff            @@\n1.0.x   #1997      +/-\n=========================================\n- Coverage   66.35%   66.3%   -0.06%   \n=========================================\n  Files          56      56            \n  Lines        6629    6634       +5   \n  Branches     1324    1325       +1   \n=========================================\n  Hits         4399    4399            \n- Misses       2005    2010       +5   \n  Partials      225     225\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/views.py | 45.78% <0%> (-0.56%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 46dfd2e...29a16eb. Read the comment docs.\n. # Codecov Report\nMerging #1998 into 1.0.x will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@          Coverage Diff          @@\n1.0.x   #1998   +/-\n=====================================\n  Coverage   66.3%   66.3%         \n=====================================\n  Files         56      56         \n  Lines       6634    6634         \n  Branches    1325    1325         \n=====================================\n  Hits        4399    4399         \n  Misses      2010    2010         \n  Partials     225     225\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/dashboard/views.py | 99.25% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8880b9f...09e2cfb. Read the comment docs.\n. # Codecov Report\nMerging #1999 into 1.0.x will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff          @@\n1.0.x   #1999   +/-\n=====================================\n  Coverage   66.3%   66.3%         \n=====================================\n  Files         56      56         \n  Lines       6634    6634         \n  Branches    1325    1325         \n=====================================\n  Hits        4399    4399         \n  Misses      2010    2010         \n  Partials     225     225\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2600f4a...b860e5c. Read the comment docs.\n. # Codecov Report\nMerging #2001 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2001   +/-\n=======================================\n  Coverage   69.45%   69.45%         \n=======================================\n  Files          71       71         \n  Lines        6953     6953         \n  Branches     1357     1357         \n=======================================\n  Hits         4829     4829         \n  Misses       1882     1882         \n  Partials      242      242\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 93c8a5b...c8c3b05. Read the comment docs.\n. # Codecov Report\nMerging #2002 into master will decrease coverage by 0.31%.\nThe diff coverage is 68.43%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2002      +/-\n==========================================\n- Coverage    69.5%   69.19%   -0.32%   \n==========================================\n  Files          71       78       +7   \n  Lines        7012     7503     +491   \n  Branches     1372     1456      +84   \n==========================================\n+ Hits         4874     5192     +318   \n- Misses       1890     2037     +147   \n- Partials      248      274      +26\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/urls.py | 90.9% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/app_settings.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/grammar.py | 82.69% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/settings.py | 75.84% <100%> (+0.55%) | :arrow_up: |\n| webapp/graphite/tags/migrations/0001_initial.py | 100% <100%> (\u00f8) | |\n| webapp/graphite/tags/urls.py | 100% <100%> (\u00f8) | |\n| webapp/graphite/tags/views.py | 28% <28%> (\u00f8) | |\n| webapp/graphite/finders/standard.py | 66.66% <33.33%> (-2.23%) | :arrow_down: |\n| webapp/graphite/finders/ceres.py | 87.87% <42.85%> (-12.13%) | :arrow_down: |\n| webapp/graphite/render/utils.py | 75% <42.85%> (-18.34%) | :arrow_down: |\n| ... and 15 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a363273...f24425b. Read the comment docs.\n. # Codecov Report\nMerging #2007 into master will decrease coverage by <.01%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2007      +/-\n==========================================\n- Coverage   69.45%   69.44%   -0.01%   \n==========================================\n  Files          71       71            \n  Lines        6953     6954       +1   \n  Branches     1357     1357            \n==========================================\n  Hits         4829     4829            \n- Misses       1882     1883       +1   \n  Partials      242      242\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/remote.py | 48.27% <0%> (-0.34%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a54167c...dfc34a3. Read the comment docs.\n. # Codecov Report\nMerging #2010 into master will decrease coverage by 0.02%.\nThe diff coverage is 63.15%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2010      +/-\n==========================================\n- Coverage   69.44%   69.41%   -0.03%   \n==========================================\n  Files          71       71            \n  Lines        6954     6971      +17   \n  Branches     1357     1361       +4   \n==========================================\n+ Hits         4829     4839      +10   \n- Misses       1883     1888       +5   \n- Partials      242      244       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/metrics/views.py | 78.32% <63.15%> (-1.35%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c499a7b...460fcf7. Read the comment docs.\n. # Codecov Report\nMerging #2011 into master will decrease coverage by 0.02%.\nThe diff coverage is 70.58%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2011      +/-\n==========================================\n- Coverage   69.44%   69.41%   -0.03%   \n==========================================\n  Files          71       71            \n  Lines        6954     6988      +34   \n  Branches     1357     1366       +9   \n==========================================\n+ Hits         4829     4851      +22   \n- Misses       1883     1890       +7   \n- Partials      242      247       +5\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.66% <70.58%> (-0.18%) | :arrow_down: |\n| webapp/graphite/metrics/views.py | 78.32% <0%> (-1.35%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c499a7b...d215dc1. Read the comment docs.\n. # Codecov Report\nMerging #2014 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2014      +/-\n==========================================\n+ Coverage   69.41%   69.42%   +<.01%   \n==========================================\n  Files          71       71            \n  Lines        6971     6973       +2   \n  Branches     1361     1362       +1   \n==========================================\n+ Hits         4839     4841       +2   \n  Misses       1888     1888            \n  Partials      244      244\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/hashing.py | 96.59% <100%> (+0.07%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3eded1d...4d5d449. Read the comment docs.\n. # Codecov Report\nMerging #2021 into master will increase coverage by 0.07%.\nThe diff coverage is 96.15%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2021      +/-\n==========================================\n+ Coverage   69.41%   69.49%   +0.07%   \n==========================================\n  Files          71       71            \n  Lines        6971     6992      +21   \n  Branches     1361     1366       +5   \n==========================================\n+ Hits         4839     4859      +20   \n  Misses       1888     1888            \n- Partials      244      245       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/attime.py | 99.18% <100%> (+0.04%) | :arrow_up: |\n| webapp/graphite/render/functions.py | 89.86% <94.11%> (+0.02%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3eded1d...86f7cdc. Read the comment docs.\n. # Codecov Report\nMerging #2025 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #2025   +/-\n======================================\n  Coverage    69.5%   69.5%         \n======================================\n  Files          71      71         \n  Lines        7011    7011         \n  Branches     1372    1372         \n======================================\n  Hits         4873    4873         \n  Misses       1890    1890         \n  Partials      248     248\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/standard.py | 68.88% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6de60c9...cc6332a. Read the comment docs.\n. # Codecov Report\nMerging #2026 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #2026      +/-\n=========================================\n+ Coverage    69.5%   69.5%   +<.01%   \n=========================================\n  Files          71      71            \n  Lines        7011    7012       +1   \n  Branches     1372    1372            \n=========================================\n+ Hits         4873    4874       +1   \n  Misses       1890    1890            \n  Partials      248     248\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.69% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6de60c9...2430da1. Read the comment docs.\n. # Codecov Report\nMerging #2029 into master will increase coverage by <.01%.\nThe diff coverage is 96.42%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #2029      +/-\n=========================================\n+ Coverage    69.5%   69.5%   +<.01%   \n=========================================\n  Files          71      71            \n  Lines        7011    7012       +1   \n  Branches     1372    1372            \n=========================================\n+ Hits         4873    4874       +1   \n  Misses       1890    1890            \n  Partials      248     248\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.69% <96.42%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6de60c9...9fb53b9. Read the comment docs.\n. # Codecov Report\nMerging #2030 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2030      +/-\n==========================================\n+ Coverage    69.5%   69.52%   +0.01%   \n==========================================\n  Files          71       71            \n  Lines        7011     7015       +4   \n  Branches     1372     1374       +2   \n==========================================\n+ Hits         4873     4877       +4   \n  Misses       1890     1890            \n  Partials      248      248\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 89.68% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/datalib.py | 65.06% <100%> (+0.86%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6de60c9...35bffb4. Read the comment docs.\n. # Codecov Report\nMerging #2032 into master will decrease coverage by 0.38%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2032      +/-\n==========================================\n- Coverage   69.23%   68.84%   -0.39%   \n==========================================\n  Files          78       78            \n  Lines        7512     7406     -106   \n  Branches     1460     1432      -28   \n==========================================\n- Hits         5201     5099     -102   \n+ Misses       2037     2035       -2   \n+ Partials      274      272       -2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 87.58% <100%> (-0.47%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4b77dde...dcfd6eb. Read the comment docs.\n. # Codecov Report\nMerging #2034 into master will increase coverage by 0.84%.\nThe diff coverage is 83.07%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2034      +/-\n==========================================\n+ Coverage   68.84%   69.69%   +0.84%   \n==========================================\n  Files          78       78            \n  Lines        7406     7440      +34   \n  Branches     1432     1440       +8   \n==========================================\n+ Hits         5099     5185      +86   \n+ Misses       2035     1969      -66   \n- Partials      272      286      +14\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/urls.py | 90.9% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/urls.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/redis.py | 89.87% <100%> (+8.86%) | :arrow_up: |\n| webapp/graphite/tags/utils.py | 78.16% <100%> (+0.77%) | :arrow_up: |\n| webapp/graphite/tags/views.py | 55.26% <61.53%> (+27.26%) | :arrow_up: |\n| webapp/graphite/tags/localdatabase.py | 90.15% <86.95%> (+26.15%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3d16358...acb3d83. Read the comment docs.\n. # Codecov Report\nMerging #2037 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2037   +/-\n=======================================\n  Coverage   69.69%   69.69%         \n=======================================\n  Files          78       78         \n  Lines        7440     7440         \n  Branches     1440     1440         \n=======================================\n  Hits         5185     5185         \n  Misses       1969     1969         \n  Partials      286      286\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 87.58% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ffb2eac...d509f38. Read the comment docs.\n. # Codecov Report\nMerging #2040 into master will decrease coverage by 0.07%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #2040      +/-\n=========================================\n- Coverage   71.78%   71.7%   -0.08%   \n=========================================\n  Files          78      78            \n  Lines        7574    7575       +1   \n  Branches     1480    1480            \n=========================================\n- Hits         5437    5432       -5   \n- Misses       1881    1887       +6   \n  Partials      256     256\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/__init__.py | 94.23% <100%> (+0.11%) | :arrow_up: |\n| webapp/graphite/tags/localdatabase.py | 87.8% <0%> (-2.93%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b9a39bb...bfc5bd0. Read the comment docs.\n. # Codecov Report\nMerging #2042 into master will increase coverage by 0.03%.\nThe diff coverage is 92.85%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2042      +/-\n==========================================\n+ Coverage   69.69%   69.72%   +0.03%   \n==========================================\n  Files          78       78            \n  Lines        7440     7448       +8   \n  Branches     1440     1442       +2   \n==========================================\n+ Hits         5185     5193       +8   \n  Misses       1969     1969            \n  Partials      286      286\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 87.58% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/evaluator.py | 75% <100%> (+2.58%) | :arrow_up: |\n| webapp/graphite/render/grammar.py | 83.33% <80%> (+0.64%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1769f81...350923f. Read the comment docs.\n. # Codecov Report\nMerging #2044 into master will increase coverage by 0.03%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2044      +/-\n==========================================\n+ Coverage   69.72%   69.76%   +0.03%   \n==========================================\n  Files          78       78            \n  Lines        7448     7418      -30   \n  Branches     1442     1432      -10   \n==========================================\n- Hits         5193     5175      -18   \n+ Misses       1969     1963       -6   \n+ Partials      286      280       -6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 88.04% <100%> (+0.45%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8ea3ef0...934d79d. Read the comment docs.\n. # Codecov Report\nMerging #2047 into master will increase coverage by 0.07%.\nThe diff coverage is 86.77%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2047      +/-\n==========================================\n+ Coverage   69.76%   69.83%   +0.07%   \n==========================================\n  Files          78       78            \n  Lines        7418     7489      +71   \n  Branches     1432     1462      +30   \n==========================================\n+ Hits         5175     5230      +55   \n- Misses       1963     1977      +14   \n- Partials      280      282       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/datalib.py | 64.97% <0%> (-0.75%) | :arrow_down: |\n| webapp/graphite/tags/localdatabase.py | 90.73% <100%> (+0.57%) | :arrow_up: |\n| webapp/graphite/finders/remote.py | 42.46% <100%> (-5.82%) | :arrow_down: |\n| webapp/graphite/finders/utils.py | 42.37% <100%> (+0.99%) | :arrow_up: |\n| webapp/graphite/storage.py | 49.69% <66.66%> (+0.61%) | :arrow_up: |\n| webapp/graphite/tags/redis.py | 88.63% <84.88%> (-1.24%) | :arrow_down: |\n| webapp/graphite/tags/utils.py | 80.45% <0%> (+2.29%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cc5b069...2b99573. Read the comment docs.\n. # Codecov Report\nMerging #2050 into master will increase coverage by 0.22%.\nThe diff coverage is 98.27%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2050      +/-\n==========================================\n+ Coverage   69.83%   70.06%   +0.22%   \n==========================================\n  Files          78       78            \n  Lines        7489     7546      +57   \n  Branches     1462     1481      +19   \n==========================================\n+ Hits         5230     5287      +57   \n+ Misses       1977     1976       -1   \n- Partials      282      283       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/grammar.py | 83.63% <100%> (+0.3%) | :arrow_up: |\n| webapp/graphite/render/functions.py | 88.38% <100%> (+0.34%) | :arrow_up: |\n| webapp/graphite/render/evaluator.py | 75.75% <50%> (+0.75%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cb5b865...12f2805. Read the comment docs.\n. # Codecov Report\nMerging #2051 into master will decrease coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2051      +/-\n==========================================\n- Coverage   69.83%   69.83%   -0.01%   \n==========================================\n  Files          78       78            \n  Lines        7489     7481       -8   \n  Branches     1462     1457       -5   \n==========================================\n- Hits         5230     5224       -6   \n  Misses       1977     1977            \n+ Partials      282      280       -2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/datalib.py | 64.49% <100%> (-0.48%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cb5b865...6b941b9. Read the comment docs.\n. # Codecov Report\nMerging #2052 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2052      +/-\n==========================================\n+ Coverage   69.83%   69.83%   +<.01%   \n==========================================\n  Files          78       78            \n  Lines        7489     7483       -6   \n  Branches     1462     1458       -4   \n==========================================\n- Hits         5230     5226       -4   \n  Misses       1977     1977            \n+ Partials      282      280       -2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/datalib.py | 64.91% <100%> (-0.06%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update cb5b865...66c627c. Read the comment docs.\n. # Codecov Report\nMerging #2053 into master will increase coverage by 0.58%.\nThe diff coverage is 99.5%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2053      +/-\n==========================================\n+ Coverage   70.06%   70.65%   +0.58%   \n==========================================\n  Files          78       78            \n  Lines        7540     7555      +15   \n  Branches     1477     1483       +6   \n==========================================\n+ Hits         5283     5338      +55   \n+ Misses       1976     1937      -39   \n+ Partials      281      280       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/views.py | 46.95% <100%> (+0.12%) | :arrow_up: |\n| webapp/graphite/settings.py | 75.97% <100%> (+0.13%) | :arrow_up: |\n| webapp/graphite/render/hashing.py | 96.59% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/datalib.py | 66.27% <100%> (+1.36%) | :arrow_up: |\n| webapp/graphite/render/functions.py | 90.37% <99.41%> (+1.98%) | :arrow_up: |\n| webapp/graphite/tags/utils.py | 82.75% <0%> (+2.29%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ed32ba6...e6b19f6. Read the comment docs.\n. # Codecov Report\nMerging #2054 into master will increase coverage by 1.08%.\nThe diff coverage is 90.41%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2054      +/-\n==========================================\n+ Coverage   70.65%   71.73%   +1.08%   \n==========================================\n  Files          78       78            \n  Lines        7555     7568      +13   \n  Branches     1483     1485       +2   \n==========================================\n+ Hits         5338     5429      +91   \n+ Misses       1937     1882      -55   \n+ Partials      280      257      -23\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/views.py | 66.98% <90.41%> (+20.02%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f6c262c...c1752ff. Read the comment docs.\n. # Codecov Report\nMerging #2055 into master will increase coverage by 0.04%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #2055      +/-\n=========================================\n+ Coverage   70.65%   70.7%   +0.04%   \n=========================================\n  Files          78      78            \n  Lines        7555    7561       +6   \n  Branches     1483    1478       -5   \n=========================================\n+ Hits         5338    5346       +8   \n+ Misses       1937    1936       -1   \n+ Partials      280     279       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 90.51% <100%> (+0.13%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f6c262c...5224e31. Read the comment docs.\n. # Codecov Report\nMerging #2057 into master will increase coverage by 0.26%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2057      +/-\n==========================================\n+ Coverage   71.78%   72.04%   +0.26%   \n==========================================\n  Files          78       79       +1   \n  Lines        7574     7620      +46   \n  Branches     1480     1486       +6   \n==========================================\n+ Hits         5437     5490      +53   \n+ Misses       1881     1878       -3   \n+ Partials      256      252       -4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 76.37% <100%> (+0.39%) | :arrow_up: |\n| webapp/graphite/readers/remote.py | 83.33% <100%> (+0.75%) | :arrow_up: |\n| webapp/graphite/tags/http.py | 100% <100%> (\u00f8) | |\n| webapp/graphite/finders/__init__.py | 94.23% <0%> (+0.11%) | :arrow_up: |\n| webapp/graphite/tags/localdatabase.py | 92.68% <0%> (+1.95%) | :arrow_up: |\n| webapp/graphite/tags/views.py | 60.52% <0%> (+5.26%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b9a39bb...e37f178. Read the comment docs.\n. # Codecov Report\nMerging #2060 into master will increase coverage by 0.17%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2060      +/-\n==========================================\n+ Coverage   72.04%   72.22%   +0.17%   \n==========================================\n  Files          79       79            \n  Lines        7620     7622       +2   \n  Branches     1486     1487       +1   \n==========================================\n+ Hits         5490     5505      +15   \n+ Misses       1878     1870       -8   \n+ Partials      252      247       -5\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/events/views.py | 94.93% <100%> (+17.01%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 02e7b61...1e87044. Read the comment docs.\n. # Codecov Report\nMerging #2062 into master will not change coverage.\nThe diff coverage is 33.33%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2062   +/-\n=======================================\n  Coverage   72.04%   72.04%         \n=======================================\n  Files          79       79         \n  Lines        7620     7620         \n  Branches     1486     1486         \n=======================================\n  Hits         5490     5490         \n  Misses       1878     1878         \n  Partials      252      252\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/datalib.py | 66.27% <0%> (\u00f8) | :arrow_up: |\n| webapp/graphite/storage.py | 49.69% <50%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 02e7b61...9c2cf9c. Read the comment docs.\n. # Codecov Report\nMerging #2063 into master will increase coverage by 0.03%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2063      +/-\n==========================================\n+ Coverage   72.22%   72.25%   +0.03%   \n==========================================\n  Files          79       79            \n  Lines        7622     7631       +9   \n  Branches     1487     1490       +3   \n==========================================\n+ Hits         5505     5514       +9   \n  Misses       1870     1870            \n  Partials      247      247\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 90.55% <100%> (+0.04%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e0a48b8...8a98369. Read the comment docs.\n. # Codecov Report\nMerging #2064 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2064   +/-\n=======================================\n  Coverage   72.25%   72.25%         \n=======================================\n  Files          79       79         \n  Lines        7631     7631         \n  Branches     1490     1490         \n=======================================\n  Hits         5514     5514         \n  Misses       1870     1870         \n  Partials      247      247\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/attime.py | 99.18% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 13a06b4...3187aae. Read the comment docs.\n. # Codecov Report\nMerging #2065 into master will increase coverage by 0.16%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2065      +/-\n==========================================\n+ Coverage   72.25%   72.42%   +0.16%   \n==========================================\n  Files          79       79            \n  Lines        7631     7636       +5   \n  Branches     1490     1491       +1   \n==========================================\n+ Hits         5514     5530      +16   \n+ Misses       1870     1860      -10   \n+ Partials      247      246       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/logger.py | 78.43% <0%> (-5.57%) | :arrow_down: |\n| webapp/graphite/settings.py | 76.88% <0%> (+0.5%) | :arrow_up: |\n| webapp/graphite/render/functions.py | 91.29% <0%> (+0.73%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 13a06b4...a6eb1f7. Read the comment docs.\n. # Codecov Report\nMerging #2066 into master will decrease coverage by 0.02%.\nThe diff coverage is 75%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2066      +/-\n==========================================\n- Coverage   72.25%   72.23%   -0.03%   \n==========================================\n  Files          79       79            \n  Lines        7631     7636       +5   \n  Branches     1490     1491       +1   \n==========================================\n+ Hits         5514     5516       +2   \n- Misses       1870     1872       +2   \n- Partials      247      248       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 76.88% <100%> (+0.5%) | :arrow_up: |\n| webapp/graphite/logger.py | 78.43% <66.66%> (-5.57%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 13a06b4...d6717c3. Read the comment docs.\n. # Codecov Report\nMerging #2071 into master will increase coverage by 1.37%.\nThe diff coverage is 88.88%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2071      +/-\n==========================================\n+ Coverage   73.52%   74.89%   +1.37%   \n==========================================\n  Files          79       79            \n  Lines        7652     7660       +8   \n  Branches     1496     1498       +2   \n==========================================\n+ Hits         5626     5737     +111   \n+ Misses       1775     1663     -112   \n- Partials      251      260       +9\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/glyph.py | 39.69% <88.88%> (+7.6%) | :arrow_up: |\n| webapp/graphite/render/views.py | 68.86% <0%> (+1.88%) | :arrow_up: |\n| webapp/graphite/tags/localdatabase.py | 92.68% <0%> (+2.92%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8ca156b...efdfff9. Read the comment docs.\n. # Codecov Report\nMerging #2072 into master will decrease coverage by 0.03%.\nThe diff coverage is 84%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2072      +/-\n==========================================\n- Coverage   74.89%   74.85%   -0.04%   \n==========================================\n  Files          79       79            \n  Lines        7660     7665       +5   \n  Branches     1498     1500       +2   \n==========================================\n+ Hits         5737     5738       +1   \n- Misses       1663     1664       +1   \n- Partials      260      263       +3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 95.58% <84%> (-0.2%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fe0beac...0a87f0d. Read the comment docs.\n. # Codecov Report\nMerging #2074 into master will increase coverage by 0.05%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2074      +/-\n==========================================\n+ Coverage   74.85%   74.91%   +0.05%   \n==========================================\n  Files          79       79            \n  Lines        7665     7675      +10   \n  Branches     1500     1502       +2   \n==========================================\n+ Hits         5738     5750      +12   \n+ Misses       1664     1662       -2   \n  Partials      263      263\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/readers/whisper.py | 92.3% <100%> (+4.17%) | :arrow_up: |\n| webapp/graphite/render/datalib.py | 67.04% <100%> (+0.76%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update f297be7...06597c9. Read the comment docs.\n. # Codecov Report\nMerging #2075 into master will increase coverage by 0.22%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2075      +/-\n==========================================\n+ Coverage   74.91%   75.14%   +0.22%   \n==========================================\n  Files          79       79            \n  Lines        7675     7675            \n  Branches     1502     1502            \n==========================================\n+ Hits         5750     5767      +17   \n+ Misses       1662     1644      -18   \n- Partials      263      264       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/datalib.py | 76.7% <0%> (+9.65%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 338c903...7eda0d4. Read the comment docs.\n. # Codecov Report\nMerging #2076 into master will increase coverage by <.01%.\nThe diff coverage is 81.48%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2076      +/-\n==========================================\n+ Coverage   74.91%   74.92%   +<.01%   \n==========================================\n  Files          79       79            \n  Lines        7675     7680       +5   \n  Branches     1502     1507       +5   \n==========================================\n+ Hits         5750     5754       +4   \n- Misses       1662     1664       +2   \n+ Partials      263      262       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/views.py | 68.67% <100%> (-0.2%) | :arrow_down: |\n| webapp/graphite/render/functions.py | 95.83% <100%> (+0.25%) | :arrow_up: |\n| webapp/graphite/render/utils.py | 57.69% <33.33%> (-17.31%) | :arrow_down: |\n| webapp/graphite/render/evaluator.py | 76.92% <78.94%> (+1.16%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 338c903...5d46231. Read the comment docs.\n. # Codecov Report\nMerging #2077 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2077   +/-\n=======================================\n  Coverage   75.14%   75.14%         \n=======================================\n  Files          79       79         \n  Lines        7675     7675         \n  Branches     1502     1502         \n=======================================\n  Hits         5767     5767         \n  Misses       1644     1644         \n  Partials      264      264\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9cc1e57...4835c8d. Read the comment docs.\n. # Codecov Report\nMerging #2078 into master will increase coverage by 0.35%.\nThe diff coverage is 90%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2078      +/-\n==========================================\n+ Coverage   75.69%   76.05%   +0.35%   \n==========================================\n  Files          80       80            \n  Lines        7711     7785      +74   \n  Branches     1512     1536      +24   \n==========================================\n+ Hits         5837     5921      +84   \n+ Misses       1613     1609       -4   \n+ Partials      261      255       -6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/tags/urls.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/views.py | 100% <100%> (+39.47%) | :arrow_up: |\n| webapp/graphite/tags/localdatabase.py | 93.17% <100%> (+0.48%) | :arrow_up: |\n| webapp/graphite/tags/base.py | 83.51% <82.22%> (+3.08%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 370ab87...308fcd7. Read the comment docs.\n. # Codecov Report\nMerging #2079 into master will increase coverage by 0.18%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2079      +/-\n==========================================\n+ Coverage   75.14%   75.32%   +0.18%   \n==========================================\n  Files          79       79            \n  Lines        7675     7675            \n  Branches     1502     1502            \n==========================================\n+ Hits         5767     5781      +14   \n+ Misses       1644     1630      -14   \n  Partials      264      264\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/future.py | 70% <0%> (+3.33%) | :arrow_up: |\n| webapp/graphite/render/datalib.py | 84.09% <0%> (+7.38%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9cc1e57...7d39d4a. Read the comment docs.\n. # Codecov Report\nMerging #2081 into master will decrease coverage by 0.01%.\nThe diff coverage is 78.26%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2081      +/-\n==========================================\n- Coverage   75.32%   75.31%   -0.02%   \n==========================================\n  Files          79       79            \n  Lines        7680     7696      +16   \n  Branches     1507     1513       +6   \n==========================================\n+ Hits         5785     5796      +11   \n- Misses       1632     1636       +4   \n- Partials      263      264       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/readers/whisper.py | 92.53% <100%> (+0.22%) | :arrow_up: |\n| webapp/graphite/readers/utils.py | 90.83% <76.19%> (-3.18%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d1acd6e...97d35bc. Read the comment docs.\n. # Codecov Report\nMerging #2082 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2082      +/-\n==========================================\n+ Coverage   75.32%   75.34%   +0.01%   \n==========================================\n  Files          79       79            \n  Lines        7680     7685       +5   \n  Branches     1507     1508       +1   \n==========================================\n+ Hits         5785     5790       +5   \n  Misses       1632     1632            \n  Partials      263      263\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/readers/utils.py | 94.21% <100%> (+0.19%) | :arrow_up: |\n| webapp/graphite/readers/whisper.py | 92.42% <100%> (+0.11%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update de81a03...da2d1fd. Read the comment docs.\n. # Codecov Report\nMerging #2083 into master will increase coverage by 0.03%.\nThe diff coverage is 87.32%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2083      +/-\n==========================================\n+ Coverage   75.32%   75.36%   +0.03%   \n==========================================\n  Files          79       80       +1   \n  Lines        7680     7703      +23   \n  Branches     1507     1510       +3   \n==========================================\n+ Hits         5785     5805      +20   \n- Misses       1632     1633       +1   \n- Partials      263      265       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/tags/http.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/settings.py | 77% <100%> (+0.12%) | :arrow_up: |\n| webapp/graphite/util.py | 79.55% <100%> (+0.61%) | :arrow_up: |\n| webapp/graphite/tags/localdatabase.py | 92.68% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/utils.py | 85.45% <100%> (+2.69%) | :arrow_up: |\n| webapp/graphite/tags/redis.py | 88.54% <100%> (-0.09%) | :arrow_down: |\n| webapp/graphite/tags/base.py | 80.43% <80.43%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update de81a03...1d3f0d9. Read the comment docs.\n. # Codecov Report\nMerging #2084 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2084   +/-\n=======================================\n  Coverage   75.32%   75.32%         \n=======================================\n  Files          79       79         \n  Lines        7680     7680         \n  Branches     1507     1507         \n=======================================\n  Hits         5785     5785         \n  Misses       1632     1632         \n  Partials      263      263\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update de81a03...dd5fec4. Read the comment docs.\n. # Codecov Report\nMerging #2085 into master will increase coverage by 0.19%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2085      +/-\n==========================================\n+ Coverage   75.32%   75.52%   +0.19%   \n==========================================\n  Files          79       79            \n  Lines        7680     7681       +1   \n  Branches     1507     1507            \n==========================================\n+ Hits         5785     5801      +16   \n+ Misses       1632     1620      -12   \n+ Partials      263      260       -3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/datalib.py | 92.65% <100%> (+8.56%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b0dd486...97cd281. Read the comment docs.\n. # Codecov Report\nMerging #2086 into master will increase coverage by 0.11%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2086      +/-\n==========================================\n+ Coverage   75.57%   75.69%   +0.11%   \n==========================================\n  Files          80       80            \n  Lines        7709     7709            \n  Branches     1511     1511            \n==========================================\n+ Hits         5826     5835       +9   \n+ Misses       1621     1613       -8   \n+ Partials      262      261       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/storage.py | 50.9% <0%> (+1.21%) | :arrow_up: |\n| webapp/graphite/render/datalib.py | 96.61% <0%> (+3.95%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fba5f70...3ac472e. Read the comment docs.\n. # Codecov Report\nMerging #2087 into master will increase coverage by 0.11%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2087      +/-\n==========================================\n+ Coverage   75.57%   75.69%   +0.11%   \n==========================================\n  Files          80       80            \n  Lines        7709     7709            \n  Branches     1511     1511            \n==========================================\n+ Hits         5826     5835       +9   \n+ Misses       1621     1613       -8   \n+ Partials      262      261       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/storage.py | 50.9% <0%> (+1.21%) | :arrow_up: |\n| webapp/graphite/render/datalib.py | 96.61% <0%> (+3.95%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fba5f70...154aece. Read the comment docs.\n. # Codecov Report\nMerging #2088 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #2088      +/-\n=========================================\n+ Coverage   75.69%   75.7%   +0.01%   \n=========================================\n  Files          80      80            \n  Lines        7709    7714       +5   \n  Branches     1511    1512       +1   \n=========================================\n+ Hits         5835    5840       +5   \n  Misses       1613    1613            \n  Partials      261     261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 95.83% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/util.py | 80.1% <100%> (+0.54%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 38380c1...b68ff48. Read the comment docs.\n. # Codecov Report\nMerging #2089 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2089      +/-\n==========================================\n+ Coverage   75.69%   75.69%   +<.01%   \n==========================================\n  Files          80       80            \n  Lines        7709     7711       +2   \n  Branches     1511     1512       +1   \n==========================================\n+ Hits         5835     5837       +2   \n  Misses       1613     1613            \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 95.83% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 38380c1...e36a80f. Read the comment docs.\n. # Codecov Report\nMerging #2091 into master will decrease coverage by 0.01%.\nThe diff coverage is 25%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2091      +/-\n==========================================\n- Coverage   75.69%   75.68%   -0.02%   \n==========================================\n  Files          80       80            \n  Lines        7711     7714       +3   \n  Branches     1512     1513       +1   \n==========================================\n+ Hits         5837     5838       +1   \n- Misses       1613     1615       +2   \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 76.31% <25%> (-0.69%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 587a313...0302198. Read the comment docs.\n. # Codecov Report\nMerging #2092 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2092   +/-\n=======================================\n  Coverage   75.69%   75.69%         \n=======================================\n  Files          80       80         \n  Lines        7711     7711         \n  Branches     1512     1512         \n=======================================\n  Hits         5837     5837         \n  Misses       1613     1613         \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/tags/http.py | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 587a313...5e0171b. Read the comment docs.\n. # Codecov Report\nMerging #2094 into master will decrease coverage by 0.07%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2094      +/-\n==========================================\n- Coverage   75.69%   75.61%   -0.08%   \n==========================================\n  Files          80       80            \n  Lines        7711     7711            \n  Branches     1512     1512            \n==========================================\n- Hits         5837     5831       -6   \n- Misses       1613     1619       +6   \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/tags/localdatabase.py | 89.75% <0%> (-2.93%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 370ab87...8ed946d. Read the comment docs.\n. # Codecov Report\nMerging #2096 into master will decrease coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2096      +/-\n==========================================\n- Coverage   76.05%   76.04%   -0.01%   \n==========================================\n  Files          80       80            \n  Lines        7793     7791       -2   \n  Branches     1538     1538            \n==========================================\n- Hits         5927     5925       -2   \n  Misses       1611     1611            \n  Partials      255      255\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 95.83% <100%> (-0.01%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c5dc26b...5512881. Read the comment docs.\n. # Codecov Report\nMerging #2098 into master will increase coverage by 0.11%.\nThe diff coverage is 96.87%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2098      +/-\n==========================================\n+ Coverage   76.04%   76.16%   +0.11%   \n==========================================\n  Files          80       80            \n  Lines        7791     7813      +22   \n  Branches     1538     1544       +6   \n==========================================\n+ Hits         5925     5951      +26   \n+ Misses       1611     1609       -2   \n+ Partials      255      253       -2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 76.56% <100%> (+0.24%) | :arrow_up: |\n| webapp/graphite/tags/views.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/http.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/base.py | 88.65% <92.85%> (+5.14%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 016f3a1...6a300b0. Read the comment docs.\n. # Codecov Report\nMerging #2099 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2099   +/-\n=======================================\n  Coverage   76.16%   76.16%         \n=======================================\n  Files          80       80         \n  Lines        7813     7813         \n  Branches     1544     1544         \n=======================================\n  Hits         5951     5951         \n  Misses       1609     1609         \n  Partials      253      253\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8ba0cb0...5ffc1ab. Read the comment docs.\n. # Codecov Report\nMerging #2100 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2100   +/-\n=======================================\n  Coverage   76.16%   76.16%         \n=======================================\n  Files          80       80         \n  Lines        7813     7813         \n  Branches     1544     1544         \n=======================================\n  Hits         5951     5951         \n  Misses       1609     1609         \n  Partials      253      253\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8710967...613650e. Read the comment docs.\n. # Codecov Report\nMerging #2102 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2102      +/-\n==========================================\n+ Coverage   76.16%   76.17%   +<.01%   \n==========================================\n  Files          80       80            \n  Lines        7813     7814       +1   \n  Branches     1544     1544            \n==========================================\n+ Hits         5951     5952       +1   \n  Misses       1609     1609            \n  Partials      253      253\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 76.68% <100%> (+0.12%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3d43c8a...cd176d0. Read the comment docs.\n. # Codecov Report\nMerging #2103 into master will increase coverage by 0.15%.\nThe diff coverage is 85.45%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2103      +/-\n==========================================\n+ Coverage   78.88%   79.03%   +0.15%   \n==========================================\n  Files          78       78            \n  Lines        7626     7633       +7   \n  Branches     1515     1513       -2   \n==========================================\n+ Hits         6016     6033      +17   \n+ Misses       1383     1372      -11   \n- Partials      227      228       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 76.43% <100%> (-0.13%) | :arrow_down: |\n| webapp/graphite/util.py | 76.38% <100%> (-4.16%) | :arrow_down: |\n| webapp/graphite/worker_pool/pool.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/finders/remote.py | 97.02% <100%> (+0.36%) | :arrow_up: |\n| webapp/graphite/metrics/views.py | 85.34% <100%> (+7.01%) | :arrow_up: |\n| webapp/graphite/finders/utils.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/finders/standard.py | 66.66% <48%> (\u00f8) | :arrow_up: |\n| webapp/graphite/finders/ceres.py | 86.66% <83.33%> (-1.22%) | :arrow_down: |\n| webapp/graphite/storage.py | 84.28% <97.43%> (+2.16%) | :arrow_up: |\n| webapp/graphite/tags/base.py | 91.01% <0%> (-4.5%) | :arrow_down: |\n| ... and 3 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2121eca...8eef60d. Read the comment docs.\n. # Codecov Report\nMerging #2103 into master will increase coverage by 0.15%.\nThe diff coverage is 85.45%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2103      +/-\n==========================================\n+ Coverage   78.88%   79.03%   +0.15%   \n==========================================\n  Files          78       78            \n  Lines        7626     7633       +7   \n  Branches     1515     1513       -2   \n==========================================\n+ Hits         6016     6033      +17   \n+ Misses       1383     1372      -11   \n- Partials      227      228       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 76.43% <100%> (-0.13%) | :arrow_down: |\n| webapp/graphite/util.py | 76.38% <100%> (-4.16%) | :arrow_down: |\n| webapp/graphite/worker_pool/pool.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/finders/remote.py | 97.02% <100%> (+0.36%) | :arrow_up: |\n| webapp/graphite/metrics/views.py | 85.34% <100%> (+7.01%) | :arrow_up: |\n| webapp/graphite/finders/utils.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/finders/standard.py | 66.66% <48%> (\u00f8) | :arrow_up: |\n| webapp/graphite/finders/ceres.py | 86.66% <83.33%> (-1.22%) | :arrow_down: |\n| webapp/graphite/storage.py | 84.28% <97.43%> (+2.16%) | :arrow_up: |\n| webapp/graphite/tags/base.py | 91.01% <0%> (-4.5%) | :arrow_down: |\n| ... and 3 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2121eca...8eef60d. Read the comment docs.\n. # Codecov Report\nMerging #2107 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2107   +/-\n=======================================\n  Coverage   78.88%   78.88%         \n=======================================\n  Files          78       78         \n  Lines        7626     7626         \n  Branches     1515     1515         \n=======================================\n  Hits         6016     6016         \n  Misses       1383     1383         \n  Partials      227      227\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 15f16a3...5231d27. Read the comment docs.\n. # Codecov Report\nMerging #2114 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2114   +/-\n=======================================\n  Coverage   78.88%   78.88%         \n=======================================\n  Files          78       78         \n  Lines        7626     7626         \n  Branches     1515     1515         \n=======================================\n  Hits         6016     6016         \n  Misses       1383     1383         \n  Partials      227      227\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 95.83% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8847188...8e19e56. Read the comment docs.\n. # Codecov Report\nMerging #2115 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2115   +/-\n=======================================\n  Coverage   78.88%   78.88%         \n=======================================\n  Files          78       78         \n  Lines        7626     7626         \n  Branches     1515     1515         \n=======================================\n  Hits         6016     6016         \n  Misses       1383     1383         \n  Partials      227      227\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 95.83% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8847188...d9e6ea0. Read the comment docs.\n. # Codecov Report\nMerging #2117 into master will decrease coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2117      +/-\n==========================================\n- Coverage   78.88%   78.86%   -0.03%   \n==========================================\n  Files          78       78            \n  Lines        7626     7616      -10   \n  Branches     1515     1514       -1   \n==========================================\n- Hits         6016     6006      -10   \n  Misses       1383     1383            \n  Partials      227      227\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 95.81% <100%> (-0.03%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ea4974a...c79b748. Read the comment docs.\n. # Codecov Report\nMerging #2118 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2118      +/-\n==========================================\n+ Coverage   78.86%   78.87%   +0.01%   \n==========================================\n  Files          78       78            \n  Lines        7616     7622       +6   \n  Branches     1514     1514            \n==========================================\n+ Hits         6006     6012       +6   \n  Misses       1383     1383            \n  Partials      227      227\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/evaluator.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/http.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/functions.py | 95.81% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/redis.py | 88.54% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/base.py | 95.69% <100%> (+0.19%) | :arrow_up: |\n| webapp/graphite/tags/views.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/localdatabase.py | 93.17% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 830367b...52ec7c7. Read the comment docs.\n. # Codecov Report\nMerging #2120 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2120   +/-\n=======================================\n  Coverage   79.07%   79.07%         \n=======================================\n  Files          78       78         \n  Lines        7629     7629         \n  Branches     1512     1512         \n=======================================\n  Hits         6033     6033         \n  Misses       1370     1370         \n  Partials      226      226\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 506f499...146af83. Read the comment docs.\n. # Codecov Report\nMerging #2121 into master will increase coverage by 0.21%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2121      +/-\n==========================================\n+ Coverage   79.07%   79.29%   +0.21%   \n==========================================\n  Files          78       78            \n  Lines        7629     7669      +40   \n  Branches     1512     1527      +15   \n==========================================\n+ Hits         6033     6081      +48   \n+ Misses       1370     1365       -5   \n+ Partials      226      223       -3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/tags/views.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/utils.py | 87.23% <\u00f8> (+1.77%) | :arrow_up: |\n| webapp/graphite/storage.py | 84.57% <100%> (+0.29%) | :arrow_up: |\n| webapp/graphite/tags/base.py | 100% <100%> (+4.3%) | :arrow_up: |\n| webapp/graphite/tags/http.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/redis.py | 91.77% <100%> (+3.22%) | :arrow_up: |\n| webapp/graphite/tags/localdatabase.py | 93.36% <100%> (+0.19%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 506f499...739882d. Read the comment docs.\n. # Codecov Report\nMerging #2123 into master will decrease coverage by 1.79%.\nThe diff coverage is 49.1%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2123     +/-\n=========================================\n- Coverage   79.26%   77.47%   -1.8%   \n=========================================\n  Files          78       79      +1   \n  Lines        7669     8146    +477   \n  Branches     1527     1672    +145   \n=========================================\n+ Hits         6079     6311    +232   \n- Misses       1366     1576    +210   \n- Partials      224      259     +35\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/views.py | 69.3% <100%> (+0.62%) | :arrow_up: |\n| webapp/graphite/metrics/views.py | 86.55% <100%> (+1.2%) | :arrow_up: |\n| webapp/graphite/readers/remote.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/finders/remote.py | 99.15% <100%> (+2.12%) | :arrow_up: |\n| webapp/graphite/umsgpack.py | 42.92% <42.92%> (\u00f8) | |\n| webapp/graphite/util.py | 76.12% <64.28%> (-0.26%) | :arrow_down: |\n| webapp/graphite/tags/base.py | 100% <0%> (+1.9%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update dabbf02...14577ba. Read the comment docs.\n. # Codecov Report\nMerging #2124 into master will decrease coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2124      +/-\n==========================================\n- Coverage   79.26%   79.25%   -0.02%   \n==========================================\n  Files          78       78            \n  Lines        7669     7674       +5   \n  Branches     1527     1529       +2   \n==========================================\n+ Hits         6079     6082       +3   \n- Misses       1366     1367       +1   \n- Partials      224      225       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 95.82% <100%> (+0.01%) | :arrow_up: |\n| webapp/graphite/tags/redis.py | 89.24% <0%> (-2.54%) | :arrow_down: |\n| webapp/graphite/tags/base.py | 100% <0%> (+1.9%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update dabbf02...7e0dec1. Read the comment docs.\n. # Codecov Report\nMerging #2126 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2126   +/-\n=======================================\n  Coverage   77.63%   77.63%         \n=======================================\n  Files          79       79         \n  Lines        8195     8195         \n  Branches     1684     1684         \n=======================================\n  Hits         6362     6362         \n  Misses       1575     1575         \n  Partials      258      258\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b73cc91...3ea588f. Read the comment docs.\n. # Codecov Report\nMerging #2127 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2127      +/-\n==========================================\n+ Coverage   77.63%   77.64%   +0.01%   \n==========================================\n  Files          79       79            \n  Lines        8195     8201       +6   \n  Branches     1684     1684            \n==========================================\n+ Hits         6362     6368       +6   \n  Misses       1575     1575            \n  Partials      258      258\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 95.83% <100%> (+0.01%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b73cc91...8a0c009. Read the comment docs.\n. # Codecov Report\nMerging #2128 into master will increase coverage by 0.75%.\nThe diff coverage is 98.41%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2128      +/-\n==========================================\n+ Coverage   77.96%   78.71%   +0.75%   \n==========================================\n  Files          80       80            \n  Lines        8331     8472     +141   \n  Branches     1874     1908      +34   \n==========================================\n+ Hits         6495     6669     +174   \n+ Misses       1573     1543      -30   \n+ Partials      263      260       -3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 95.62% <\u00f8> (-0.04%) | :arrow_down: |\n| webapp/graphite/render/datalib.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/storage.py | 90.61% <100%> (+5.54%) | :arrow_up: |\n| webapp/graphite/render/grammar.py | 84.21% <100%> (+0.57%) | :arrow_up: |\n| webapp/graphite/tags/views.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/util.py | 92.13% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/glyph.py | 41.64% <100%> (+1.95%) | :arrow_up: |\n| webapp/graphite/render/evaluator.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/finders/remote.py | 99.33% <100%> (+0.18%) | :arrow_up: |\n| webapp/graphite/finders/utils.py | 96.22% <60%> (-3.78%) | :arrow_down: |\n| ... and 4 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9b07842...31fabde. Read the comment docs.\n. # Codecov Report\nMerging #2129 into master will decrease coverage by 0.03%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2129      +/-\n==========================================\n- Coverage   77.64%   77.61%   -0.04%   \n==========================================\n  Files          79       79            \n  Lines        8201     8206       +5   \n  Branches     1684     1685       +1   \n==========================================\n+ Hits         6368     6369       +1   \n- Misses       1575     1577       +2   \n- Partials      258      260       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/tags/utils.py | 88.46% <100%> (+1.22%) | :arrow_up: |\n| webapp/graphite/tags/redis.py | 89.24% <0%> (-2.54%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c68ab72...0209a3d. Read the comment docs.\n. # Codecov Report\nMerging #2131 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2131   +/-\n=======================================\n  Coverage   77.64%   77.64%         \n=======================================\n  Files          79       79         \n  Lines        8201     8201         \n  Branches     1684     1684         \n=======================================\n  Hits         6368     6368         \n  Misses       1575     1575         \n  Partials      258      258\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/storage.py | 84.57% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c68ab72...cf2afbf. Read the comment docs.\n. # Codecov Report\nMerging #2132 into master will decrease coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2132      +/-\n==========================================\n- Coverage   77.66%   77.64%   -0.02%   \n==========================================\n  Files          79       79            \n  Lines        8206     8210       +4   \n  Branches     1685     1686       +1   \n==========================================\n+ Hits         6373     6375       +2   \n- Misses       1575     1576       +1   \n- Partials      258      259       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 95.84% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/redis.py | 90.5% <0%> (-1.27%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d4eb9cc...7ed9055. Read the comment docs.\n. # Codecov Report\nMerging #2134 into master will decrease coverage by 0.13%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2134      +/-\n==========================================\n- Coverage   77.67%   77.53%   -0.14%   \n==========================================\n  Files          79       79            \n  Lines        8210     8213       +3   \n  Branches     1686     1687       +1   \n==========================================\n- Hits         6377     6368       -9   \n- Misses       1575     1584       +9   \n- Partials      258      261       +3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/utils.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/localdatabase.py | 88.62% <0%> (-5.69%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 61ac7f2...abc3ca3. Read the comment docs.\n. # Codecov Report\nMerging #2135 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2135   +/-\n=======================================\n  Coverage   77.67%   77.67%         \n=======================================\n  Files          79       79         \n  Lines        8210     8210         \n  Branches     1686     1686         \n=======================================\n  Hits         6377     6377         \n  Misses       1575     1575         \n  Partials      258      258\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/logger.py | 78.43% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 61ac7f2...dc6f944. Read the comment docs.\n. # Codecov Report\nMerging #2136 into master will increase coverage by 0.05%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2136      +/-\n==========================================\n+ Coverage   77.67%   77.73%   +0.05%   \n==========================================\n  Files          79       79            \n  Lines        8210     8231      +21   \n  Branches     1686     1689       +3   \n==========================================\n+ Hits         6377     6398      +21   \n  Misses       1575     1575            \n  Partials      258      258\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/remote.py | 99.15% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/util.py | 78.85% <100%> (+2.72%) | :arrow_up: |\n| webapp/graphite/readers/remote.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/settings.py | 76.56% <100%> (+0.12%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 61ac7f2...913b00b. Read the comment docs.\n. # Codecov Report\nMerging #2137 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2137      +/-\n==========================================\n+ Coverage   77.73%   77.75%   +0.01%   \n==========================================\n  Files          79       79            \n  Lines        8234     8241       +7   \n  Branches     1690     1692       +2   \n==========================================\n+ Hits         6401     6408       +7   \n  Misses       1575     1575            \n  Partials      258      258\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/tags/http.py | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5a58e4f...d19a95a. Read the comment docs.\n. # Codecov Report\nMerging #2139 into master will increase coverage by 0.65%.\nThe diff coverage is 88.7%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2139      +/-\n==========================================\n+ Coverage   77.75%   78.41%   +0.65%   \n==========================================\n  Files          79       79            \n  Lines        8241     8292      +51   \n  Branches     1692     1871     +179   \n==========================================\n+ Hits         6408     6502      +94   \n+ Misses       1575     1527      -48   \n- Partials      258      263       +5\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/remote.py | 99.14% <100%> (-0.01%) | :arrow_down: |\n| webapp/graphite/composer/views.py | 24.69% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/node.py | 92.59% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/http.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/browser/views.py | 66.27% <100%> (-0.2%) | :arrow_down: |\n| webapp/graphite/storage.py | 85.06% <100%> (+0.48%) | :arrow_up: |\n| webapp/graphite/worker_pool/pool.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/readers/utils.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/events/views.py | 95% <100%> (+0.06%) | :arrow_up: |\n| webapp/graphite/readers/multi.py | 92.15% <100%> (\u00f8) | :arrow_up: |\n| ... and 18 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 019bcf1...1ee9c83. Read the comment docs.\n. # Codecov Report\nMerging #2140 into master will decrease coverage by 0.01%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2140      +/-\n==========================================\n- Coverage   78.41%   78.39%   -0.02%   \n==========================================\n  Files          79       79            \n  Lines        8292     8285       -7   \n  Branches     1871     1871            \n==========================================\n- Hits         6502     6495       -7   \n  Misses       1527     1527            \n  Partials      263      263\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/glyph.py | 39.69% <\u00f8> (-0.15%) | :arrow_down: |\n| webapp/graphite/render/functions.py | 95.65% <\u00f8> (-0.01%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e83977b...7a68dc8. Read the comment docs.\n. # Codecov Report\nMerging #2141 into master will decrease coverage by 0.43%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2141      +/-\n==========================================\n- Coverage   78.39%   77.96%   -0.44%   \n==========================================\n  Files          79       80       +1   \n  Lines        8285     8331      +46   \n  Branches     1871     1874       +3   \n==========================================\n  Hits         6495     6495            \n- Misses       1527     1573      +46   \n  Partials      263      263\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/account/ldapBackend.py | 0% <0%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9f65e7c...db86996. Read the comment docs.\n. # Codecov Report\nMerging #2145 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2145   +/-\n=======================================\n  Coverage   77.96%   77.96%         \n=======================================\n  Files          80       80         \n  Lines        8331     8331         \n  Branches     1874     1874         \n=======================================\n  Hits         6495     6495         \n  Misses       1573     1573         \n  Partials      263      263\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9b07842...48c6ac0. Read the comment docs.\n. # Codecov Report\nMerging #2146 into master will increase coverage by 0.83%.\nThe diff coverage is 98.07%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2146      +/-\n==========================================\n+ Coverage   78.71%   79.55%   +0.83%   \n==========================================\n  Files          80       85       +5   \n  Lines        8472     8808     +336   \n  Branches     1908     1888      -20   \n==========================================\n+ Hits         6669     7007     +338   \n+ Misses       1543     1540       -3   \n- Partials      260      261       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/urls.py | 90.9% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/compat.py | 81.81% <\u00f8> (+4.54%) | :arrow_up: |\n| webapp/graphite/events/views.py | 94.93% <100%> (-0.07%) | :arrow_down: |\n| webapp/graphite/functions/__init__.py | 100% <100%> (\u00f8) | |\n| webapp/graphite/render/evaluator.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/views.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/util.py | 93.39% <100%> (+1.26%) | :arrow_up: |\n| webapp/graphite/errors.py | 100% <100%> (\u00f8) | |\n| webapp/graphite/functions/urls.py | 100% <100%> (\u00f8) | |\n| webapp/graphite/render/attime.py | 100% <100%> (\u00f8) | :arrow_up: |\n| ... and 11 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b7a9020...0878305. Read the comment docs.\n. # Codecov Report\nMerging #2148 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2148   +/-\n=======================================\n  Coverage   77.96%   77.96%         \n=======================================\n  Files          80       80         \n  Lines        8331     8331         \n  Branches     1874     1874         \n=======================================\n  Hits         6495     6495         \n  Misses       1573     1573         \n  Partials      263      263\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e55e6c3...44724db. Read the comment docs.\n. # Codecov Report\nMerging #2149 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2149   +/-\n=======================================\n  Coverage   77.96%   77.96%         \n=======================================\n  Files          80       80         \n  Lines        8331     8331         \n  Branches     1874     1874         \n=======================================\n  Hits         6495     6495         \n  Misses       1573     1573         \n  Partials      263      263\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c7ae662...7f247c0. Read the comment docs.\n. # Codecov Report\nMerging #2150 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2150   +/-\n=======================================\n  Coverage   78.71%   78.71%         \n=======================================\n  Files          80       80         \n  Lines        8472     8472         \n  Branches     1908     1908         \n=======================================\n  Hits         6669     6669         \n  Misses       1543     1543         \n  Partials      260      260\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4c5a7c4...af7540f. Read the comment docs.\n. # Codecov Report\nMerging #2151 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2151   +/-\n=======================================\n  Coverage   78.71%   78.71%         \n=======================================\n  Files          80       80         \n  Lines        8472     8472         \n  Branches     1908     1908         \n=======================================\n  Hits         6669     6669         \n  Misses       1543     1543         \n  Partials      260      260\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8649f6f...a557a31. Read the comment docs.\n. # Codecov Report\nMerging #2153 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2153   +/-\n=======================================\n  Coverage   79.55%   79.55%         \n=======================================\n  Files          85       85         \n  Lines        8808     8808         \n  Branches     1888     1888         \n=======================================\n  Hits         7007     7007         \n  Misses       1540     1540         \n  Partials      261      261\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c974645...146c96a. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (0.9.x@2bb157b). Click here to learn what that means.\nThe diff coverage is 37.5%.\n\n\n```diff\n@@           Coverage Diff           @@\n0.9.x   #2156   +/-\n=======================================\n  Coverage         ?   33.5%         \n=======================================\n  Files            ?      57         \n  Lines            ?    8668         \n  Branches         ?    1588         \n=======================================\n  Hits             ?    2904         \n  Misses           ?    5413         \n  Partials         ?     351\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/storage.py | 41.53% <37.5%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2bb157b...90d1ab2. Read the comment docs.\n. # Codecov Report\nMerging #2157 into master will increase coverage by 0.05%.\nThe diff coverage is 50%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2157      +/-\n==========================================\n+ Coverage   79.48%   79.53%   +0.05%   \n==========================================\n  Files          85       85            \n  Lines        8808     8807       -1   \n  Branches     1888     1888            \n==========================================\n+ Hits         7001     7005       +4   \n+ Misses       1546     1541       -5   \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/dashboard/urls.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/events/urls.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/version/urls.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/account/urls.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/metrics/urls.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/whitelist/urls.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/urls.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/composer/urls.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/functions/urls.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/browser/urls.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| ... and 3 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 63a9621...fc698a4. Read the comment docs.\n. # Codecov Report\nMerging #2158 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2158   +/-\n=======================================\n  Coverage   79.53%   79.53%         \n=======================================\n  Files          85       85         \n  Lines        8807     8807         \n  Branches     1888     1888         \n=======================================\n  Hits         7005     7005         \n  Misses       1541     1541         \n  Partials      261      261\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4a27e48...f467d30. Read the comment docs.\n. # Codecov Report\nMerging #2159 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2159   +/-\n=======================================\n  Coverage   79.53%   79.53%         \n=======================================\n  Files          85       85         \n  Lines        8807     8807         \n  Branches     1888     1888         \n=======================================\n  Hits         7005     7005         \n  Misses       1541     1541         \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 76.8% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6b91f29...5145a61. Read the comment docs.\n. # Codecov Report\nMerging #2160 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2160   +/-\n=======================================\n  Coverage   79.53%   79.53%         \n=======================================\n  Files          85       85         \n  Lines        8807     8807         \n  Branches     1888     1888         \n=======================================\n  Hits         7005     7005         \n  Misses       1541     1541         \n  Partials      261      261\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 911a0e1...049637c. Read the comment docs.\n. # Codecov Report\nMerging #2161 into 1.1.x will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\n1.1.x    #2161   +/-\n=======================================\n  Coverage   79.53%   79.53%         \n=======================================\n  Files          85       85         \n  Lines        8807     8807         \n  Branches     1888     1888         \n=======================================\n  Hits         7005     7005         \n  Misses       1541     1541         \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 76.8% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2d5f15c...0337750. Read the comment docs.\n. # Codecov Report\nMerging #2162 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2162      +/-\n==========================================\n+ Coverage   79.53%   79.55%   +0.01%   \n==========================================\n  Files          85       85            \n  Lines        8807     8814       +7   \n  Branches     1888     1888            \n==========================================\n+ Hits         7005     7012       +7   \n  Misses       1541     1541            \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/remote.py | 99.33% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/views.py | 70.88% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/datalib.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/readers/remote.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/metrics/views.py | 86.66% <100%> (+0.05%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 11631c1...3b05015. Read the comment docs.\n. # Codecov Report\nMerging #2163 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2163      +/-\n==========================================\n+ Coverage   79.55%   79.55%   +<.01%   \n==========================================\n  Files          85       85            \n  Lines        8814     8815       +1   \n  Branches     1888     1888            \n==========================================\n+ Hits         7012     7013       +1   \n  Misses       1541     1541            \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/app_settings.py | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 018356b...16b9fe0. Read the comment docs.\n. # Codecov Report\nMerging #2164 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2164   +/-\n=======================================\n  Coverage   79.55%   79.55%         \n=======================================\n  Files          85       85         \n  Lines        8814     8814         \n  Branches     1888     1888         \n=======================================\n  Hits         7012     7012         \n  Misses       1541     1541         \n  Partials      261      261\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 018356b...b9120fe. Read the comment docs.\n. # Codecov Report\nMerging #2165 into master will decrease coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2165      +/-\n==========================================\n- Coverage   79.55%   79.55%   -0.01%   \n==========================================\n  Files          85       85            \n  Lines        8815     8813       -2   \n  Branches     1888     1887       -1   \n==========================================\n- Hits         7013     7011       -2   \n  Misses       1541     1541            \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/hashing.py | 96.51% <100%> (-0.08%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b3365c0...2f4c2d6. Read the comment docs.\n. # Codecov Report\nMerging #2166 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2166   +/-\n=======================================\n  Coverage   79.55%   79.55%         \n=======================================\n  Files          85       85         \n  Lines        8815     8815         \n  Branches     1888     1888         \n=======================================\n  Hits         7013     7013         \n  Misses       1541     1541         \n  Partials      261      261\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b3365c0...9275056. Read the comment docs.\n. # Codecov Report\nMerging #2168 into master will increase coverage by 0.02%.\nThe diff coverage is 91.3%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2168      +/-\n==========================================\n+ Coverage   79.55%   79.58%   +0.02%   \n==========================================\n  Files          85       85            \n  Lines        8815     8822       +7   \n  Branches     1888     1892       +4   \n==========================================\n+ Hits         7013     7021       +8   \n+ Misses       1541     1539       -2   \n- Partials      261      262       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/hashing.py | 97.89% <91.3%> (+1.3%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b3365c0...b65ec59. Read the comment docs.\n. # Codecov Report\nMerging #2169 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2169   +/-\n=======================================\n  Coverage   79.58%   79.58%         \n=======================================\n  Files          85       85         \n  Lines        8822     8822         \n  Branches     1892     1892         \n=======================================\n  Hits         7021     7021         \n  Misses       1539     1539         \n  Partials      262      262\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.03% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d2f7fd5...b98e54c. Read the comment docs.\n. # Codecov Report\nMerging #2170 into 1.1.x will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\n1.1.x    #2170   +/-\n=======================================\n  Coverage   79.53%   79.53%         \n=======================================\n  Files          85       85         \n  Lines        8807     8807         \n  Branches     1888     1888         \n=======================================\n  Hits         7005     7005         \n  Misses       1541     1541         \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.03% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0456360...c248c0f. Read the comment docs.\n. # Codecov Report\nMerging #2171 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2171   +/-\n=======================================\n  Coverage   79.58%   79.58%         \n=======================================\n  Files          85       85         \n  Lines        8822     8822         \n  Branches     1892     1892         \n=======================================\n  Hits         7021     7021         \n  Misses       1539     1539         \n  Partials      262      262\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 60cc9d6...0acf12c. Read the comment docs.\n. # Codecov Report\nMerging #2172 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2172   +/-\n=======================================\n  Coverage   79.58%   79.58%         \n=======================================\n  Files          85       85         \n  Lines        8822     8822         \n  Branches     1892     1892         \n=======================================\n  Hits         7021     7021         \n  Misses       1539     1539         \n  Partials      262      262\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.03% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9a1660f...fdda1cb. Read the comment docs.\n. # Codecov Report\nMerging #2173 into 1.1.x will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\n1.1.x    #2173   +/-\n=======================================\n  Coverage   79.53%   79.53%         \n=======================================\n  Files          85       85         \n  Lines        8807     8807         \n  Branches     1888     1888         \n=======================================\n  Hits         7005     7005         \n  Misses       1541     1541         \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.03% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 6141eba...6eceee8. Read the comment docs.\n. # Codecov Report\n:exclamation: No coverage uploaded for pull request base (master@a597925). Click here to learn what that means.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster    #2174   +/-\n=========================================\n  Coverage          ?   80.13%         \n=========================================\n  Files             ?       85         \n  Lines             ?     8895         \n  Branches          ?     1899         \n=========================================\n  Hits              ?     7128         \n  Misses            ?     1501         \n  Partials          ?      266\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a597925...c087240. Read the comment docs.\n. # Codecov Report\nMerging #2181 into master will decrease coverage by 0.12%.\nThe diff coverage is 95.89%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2181      +/-\n==========================================\n- Coverage   79.59%   79.47%   -0.13%   \n==========================================\n  Files          85       85            \n  Lines        8881     8841      -40   \n  Branches     1899     1887      -12   \n==========================================\n- Hits         7069     7026      -43   \n- Misses       1549     1550       +1   \n- Partials      263      265       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 77.27% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/worker_pool/pool.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/metrics/views.py | 86.72% <100%> (+0.05%) | :arrow_up: |\n| webapp/graphite/storage.py | 88.85% <95.08%> (-2.26%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c698848...c095384. Read the comment docs.\n. # Codecov Report\nMerging #2185 into master will increase coverage by 0.03%.\nThe diff coverage is 76.47%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2185      +/-\n==========================================\n+ Coverage   79.63%   79.66%   +0.03%   \n==========================================\n  Files          85       85            \n  Lines        8847     8851       +4   \n  Branches     1895     1896       +1   \n==========================================\n+ Hits         7045     7051       +6   \n+ Misses       1540     1539       -1   \n+ Partials      262      261       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/standard.py | 70.16% <76.47%> (+2.66%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 561c899...191a8fe. Read the comment docs.\n. # Codecov Report\nMerging #2188 into 1.1.x will decrease coverage by 0.03%.\nThe diff coverage is 77.77%.\n\n\n```diff\n@@            Coverage Diff            @@\n1.1.x   #2188      +/-\n=========================================\n- Coverage   79.53%   79.5%   -0.04%   \n=========================================\n  Files          85      85            \n  Lines        8807    8820      +13   \n  Branches     1888    1888            \n=========================================\n+ Hits         7005    7012       +7   \n- Misses       1541    1546       +5   \n- Partials      261     262       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/remote.py | 99.33% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/views.py | 70.88% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/readers/remote.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/datalib.py | 98.51% <50%> (-1.49%) | :arrow_down: |\n| webapp/graphite/metrics/views.py | 86% <71.42%> (-0.61%) | :arrow_down: |\n| webapp/graphite/umsgpack.py | 49.08% <0%> (-0.46%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a996c58...1f537f0. Read the comment docs.\n. # Codecov Report\nMerging #2189 into 1.1.x will increase coverage by 0.12%.\nThe diff coverage is 87.36%.\n\n\n```diff\n@@            Coverage Diff             @@\n1.1.x    #2189      +/-\n==========================================\n+ Coverage   79.53%   79.66%   +0.12%   \n==========================================\n  Files          85       85            \n  Lines        8807     8851      +44   \n  Branches     1888     1896       +8   \n==========================================\n+ Hits         7005     7051      +46   \n+ Misses       1541     1539       -2   \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/remote.py | 99.33% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/readers/remote.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/views.py | 70.88% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/metrics/views.py | 86.66% <100%> (+0.05%) | :arrow_up: |\n| webapp/graphite/storage.py | 91.11% <100%> (+0.49%) | :arrow_up: |\n| webapp/graphite/render/datalib.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/app_settings.py | 90.9% <71.42%> (-9.1%) | :arrow_down: |\n| webapp/graphite/finders/standard.py | 70.16% <76.47%> (+3.49%) | :arrow_up: |\n| webapp/graphite/render/hashing.py | 97.89% <91.3%> (+1.3%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a996c58...ec22e8e. Read the comment docs.\n. # Codecov Report\nMerging #2190 into master will not change coverage.\nThe diff coverage is 0%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2190   +/-\n=======================================\n  Coverage   79.66%   79.66%         \n=======================================\n  Files          85       85         \n  Lines        8851     8851         \n  Branches     1896     1896         \n=======================================\n  Hits         7051     7051         \n  Misses       1539     1539         \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/app_settings.py | 90.9% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update bd60ff6...ca984ea. Read the comment docs.\n. # Codecov Report\nMerging #2191 into master will decrease coverage by 0.03%.\nThe diff coverage is 72.72%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2191      +/-\n==========================================\n- Coverage   79.66%   79.63%   -0.04%   \n==========================================\n  Files          85       85            \n  Lines        8851     8862      +11   \n  Branches     1896     1899       +3   \n==========================================\n+ Hits         7051     7057       +6   \n- Misses       1539     1542       +3   \n- Partials      261      263       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/hashing.py | 93.39% <72.72%> (-4.5%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 38690cb...43810ce. Read the comment docs.\n. # Codecov Report\nMerging #2195 into master will decrease coverage by 3.56%.\nThe diff coverage is 7.95%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2195      +/-\n==========================================\n- Coverage   79.63%   76.07%   -3.57%   \n==========================================\n  Files          85       90       +5   \n  Lines        8862     9315     +453   \n  Branches     1899     1916      +17   \n==========================================\n+ Hits         7057     7086      +29   \n- Misses       1542     1965     +423   \n- Partials      263      264       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/urls.py | 80% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/prometheus/gogo_pb2.py | 0% <0%> (\u00f8) | |\n| webapp/graphite/prometheus/types_pb2.py | 0% <0%> (\u00f8) | |\n| webapp/graphite/prometheus/urls.py | 100% <100%> (\u00f8) | |\n| webapp/graphite/prometheus/views.py | 24% <24%> (\u00f8) | |\n| webapp/graphite/prometheus/remote_pb2.py | 5.26% <5.26%> (\u00f8) | |\n| webapp/graphite/render/views.py | 71.02% <77.77%> (+0.13%) | :arrow_up: |\n| ... and 3 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d483c17...658a6df. Read the comment docs.\n. # Codecov Report\nMerging #2196 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2196      +/-\n==========================================\n+ Coverage   79.63%   79.64%   +<.01%   \n==========================================\n  Files          85       85            \n  Lines        8862     8866       +4   \n  Branches     1899     1899            \n==========================================\n+ Hits         7057     7061       +4   \n  Misses       1542     1542            \n  Partials      263      263\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/readers/remote.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/http.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/finders/remote.py | 99.33% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/storage.py | 91.11% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/settings.py | 77.27% <100%> (+0.46%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d483c17...3cfa36a. Read the comment docs.\n. # Codecov Report\nMerging #2198 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2198   +/-\n=======================================\n  Coverage   79.63%   79.63%         \n=======================================\n  Files          85       85         \n  Lines        8862     8862         \n  Branches     1899     1899         \n=======================================\n  Hits         7057     7057         \n  Misses       1542     1542         \n  Partials      263      263\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/datalib.py | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d483c17...440195e. Read the comment docs.\n. # Codecov Report\nMerging #2199 into 1.1.x will decrease coverage by 0.02%.\nThe diff coverage is 80%.\n\n\n```diff\n@@            Coverage Diff             @@\n1.1.x    #2199      +/-\n==========================================\n- Coverage   79.66%   79.64%   -0.03%   \n==========================================\n  Files          85       85            \n  Lines        8851     8866      +15   \n  Branches     1896     1899       +3   \n==========================================\n+ Hits         7051     7061      +10   \n- Misses       1539     1542       +3   \n- Partials      261      263       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/remote.py | 99.33% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/http.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/readers/remote.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/app_settings.py | 90.9% <0%> (\u00f8) | :arrow_up: |\n| webapp/graphite/storage.py | 91.11% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/settings.py | 77.27% <100%> (+0.46%) | :arrow_up: |\n| webapp/graphite/render/datalib.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/hashing.py | 93.39% <72.72%> (-4.5%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 18ccc97...93b751e. Read the comment docs.\n. # Codecov Report\nMerging #2201 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2201   +/-\n=======================================\n  Coverage   79.64%   79.64%         \n=======================================\n  Files          85       85         \n  Lines        8866     8866         \n  Branches     1899     1899         \n=======================================\n  Hits         7061     7061         \n  Misses       1542     1542         \n  Partials      263      263\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 419a926...bdcea94. Read the comment docs.\n. # Codecov Report\nMerging #2202 into 1.1.x will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\n1.1.x    #2202   +/-\n=======================================\n  Coverage   79.64%   79.64%         \n=======================================\n  Files          85       85         \n  Lines        8866     8866         \n  Branches     1899     1899         \n=======================================\n  Hits         7061     7061         \n  Misses       1542     1542         \n  Partials      263      263\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1c22bf6...1612a04. Read the comment docs.\n. # Codecov Report\nMerging #2204 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2204   +/-\n=======================================\n  Coverage   79.64%   79.64%         \n=======================================\n  Files          85       85         \n  Lines        8866     8866         \n  Branches     1899     1899         \n=======================================\n  Hits         7061     7061         \n  Misses       1542     1542         \n  Partials      263      263\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ded6a2d...00f2a0e. Read the comment docs.\n. # Codecov Report\nMerging #2206 into master will decrease coverage by 0.04%.\nThe diff coverage is 65%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2206      +/-\n==========================================\n- Coverage   79.64%   79.59%   -0.05%   \n==========================================\n  Files          85       85            \n  Lines        8866     8881      +15   \n  Branches     1899     1899            \n==========================================\n+ Hits         7061     7069       +8   \n- Misses       1542     1549       +7   \n  Partials      263      263\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/datalib.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/storage.py | 91.11% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/base.py | 94.44% <53.33%> (-5.56%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d83a548...c4dfc1a. Read the comment docs.\n. # Codecov Report\nMerging #2208 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2208   +/-\n=======================================\n  Coverage   79.64%   79.64%         \n=======================================\n  Files          85       85         \n  Lines        8866     8866         \n  Branches     1899     1899         \n=======================================\n  Hits         7061     7061         \n  Misses       1542     1542         \n  Partials      263      263\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ded6a2d...eda1eef. Read the comment docs.\n. # Codecov Report\nMerging #2209 into 1.1.x will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\n1.1.x    #2209   +/-\n=======================================\n  Coverage   79.64%   79.64%         \n=======================================\n  Files          85       85         \n  Lines        8866     8866         \n  Branches     1899     1899         \n=======================================\n  Hits         7061     7061         \n  Misses       1542     1542         \n  Partials      263      263\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/readers/whisper.py | 92.42% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 150ed88...379ba7b. Read the comment docs.\n. # Codecov Report\nMerging #2210 into 1.1.x will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\n1.1.x    #2210   +/-\n=======================================\n  Coverage   79.64%   79.64%         \n=======================================\n  Files          85       85         \n  Lines        8866     8866         \n  Branches     1899     1899         \n=======================================\n  Hits         7061     7061         \n  Misses       1542     1542         \n  Partials      263      263\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/readers/whisper.py | 92.42% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 150ed88...7c597aa. Read the comment docs.\n. # Codecov Report\nMerging #2212 into 1.1.x will decrease coverage by 0.17%.\nThe diff coverage is 89.24%.\n\n\n```diff\n@@            Coverage Diff             @@\n1.1.x    #2212      +/-\n==========================================\n- Coverage   79.64%   79.47%   -0.18%   \n==========================================\n  Files          85       85            \n  Lines        8866     8841      -25   \n  Branches     1899     1887      -12   \n==========================================\n- Hits         7061     7026      -35   \n- Misses       1542     1550       +8   \n- Partials      263      265       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 77.27% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/datalib.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/metrics/views.py | 86.72% <100%> (+0.05%) | :arrow_up: |\n| webapp/graphite/worker_pool/pool.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/base.py | 94.44% <53.33%> (-5.56%) | :arrow_down: |\n| webapp/graphite/storage.py | 88.85% <95.16%> (-2.26%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 50ce36d...7a4cc3c. Read the comment docs.\n. # Codecov Report\nMerging #2213 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2213      +/-\n==========================================\n+ Coverage   79.47%   79.47%   +<.01%   \n==========================================\n  Files          85       85            \n  Lines        8841     8842       +1   \n  Branches     1887     1888       +1   \n==========================================\n+ Hits         7026     7027       +1   \n  Misses       1550     1550            \n  Partials      265      265\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.04% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2a897cf...bc6ade3. Read the comment docs.\n. # Codecov Report\nMerging #2215 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2215   +/-\n=======================================\n  Coverage   79.47%   79.47%         \n=======================================\n  Files          85       85         \n  Lines        8842     8842         \n  Branches     1888     1888         \n=======================================\n  Hits         7027     7027         \n  Misses       1550     1550         \n  Partials      265      265\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0f11d54...97de802. Read the comment docs.\n. # Codecov Report\nMerging #2216 into 1.1.x will decrease coverage by 0.03%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\n1.1.x    #2216      +/-\n==========================================\n- Coverage   79.47%   79.43%   -0.04%   \n==========================================\n  Files          85       85            \n  Lines        8841     8842       +1   \n  Branches     1887     1888       +1   \n==========================================\n- Hits         7026     7024       -2   \n- Misses       1550     1553       +3   \n  Partials      265      265\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.04% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/hashing.py | 90.56% <0%> (-2.84%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 2d6b41b...cf4f58a. Read the comment docs.\n. # Codecov Report\nMerging #2221 into master will decrease coverage by 0.9%.\nThe diff coverage is 84.93%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2221      +/-\n==========================================\n- Coverage   79.48%   78.57%   -0.91%   \n==========================================\n  Files          85       85            \n  Lines        8847     8851       +4   \n  Branches     1890     1723     -167   \n==========================================\n- Hits         7032     6955      -77   \n- Misses       1550     1630      +80   \n- Partials      265      266       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/tags/utils.py | 88.46% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/finders/ceres.py | 95.55% <100%> (+8.88%) | :arrow_up: |\n| webapp/graphite/finders/standard.py | 75.93% <82.25%> (+5.77%) | :arrow_up: |\n| webapp/graphite/middleware.py | 77.77% <0%> (-22.23%) | :arrow_down: |\n| webapp/graphite/url_shortener/views.py | 42.85% <0%> (-9.53%) | :arrow_down: |\n| webapp/graphite/app_settings.py | 81.81% <0%> (-9.1%) | :arrow_down: |\n| webapp/graphite/util.py | 85.84% <0%> (-7.55%) | :arrow_down: |\n| webapp/graphite/umsgpack.py | 42.92% <0%> (-6.63%) | :arrow_down: |\n| webapp/graphite/render/hashing.py | 86.79% <0%> (-6.61%) | :arrow_down: |\n| webapp/graphite/tags/localdatabase.py | 88.62% <0%> (-5.69%) | :arrow_down: |\n| ... and 9 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4a92f52...f3daba9. Read the comment docs.\n. # Codecov Report\nMerging #2223 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2223      +/-\n==========================================\n+ Coverage   79.47%   79.47%   +<.01%   \n==========================================\n  Files          85       85            \n  Lines        8842     8845       +3   \n  Branches     1888     1889       +1   \n==========================================\n+ Hits         7027     7030       +3   \n  Misses       1550     1550            \n  Partials      265      265\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/utils.py | 96.42% <100%> (+0.2%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a01dbe9...f275242. Read the comment docs.\n. # Codecov Report\nMerging #2224 into 1.1.x will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\n1.1.x    #2224      +/-\n==========================================\n+ Coverage   79.47%   79.47%   +<.01%   \n==========================================\n  Files          85       85            \n  Lines        8842     8845       +3   \n  Branches     1888     1889       +1   \n==========================================\n+ Hits         7027     7030       +3   \n  Misses       1550     1550            \n  Partials      265      265\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/utils.py | 96.42% <100%> (+0.2%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5d3baa1...9015872. Read the comment docs.\n. # Codecov Report\nMerging #2225 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2225      +/-\n==========================================\n+ Coverage   79.47%   79.48%   +<.01%   \n==========================================\n  Files          85       85            \n  Lines        8845     8846       +1   \n  Branches     1889     1889            \n==========================================\n+ Hits         7030     7031       +1   \n  Misses       1550     1550            \n  Partials      265      265\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.04% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b8f9aed...5abfd4c. Read the comment docs.\n. # Codecov Report\nMerging #2226 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2226      +/-\n==========================================\n+ Coverage   79.48%   79.48%   +<.01%   \n==========================================\n  Files          85       85            \n  Lines        8846     8847       +1   \n  Branches     1889     1890       +1   \n==========================================\n+ Hits         7031     7032       +1   \n  Misses       1550     1550            \n  Partials      265      265\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.04% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ba4b77d...3d12bf7. Read the comment docs.\n. # Codecov Report\nMerging #2228 into 1.1.x will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\n1.1.x    #2228      +/-\n==========================================\n+ Coverage   79.47%   79.48%   +<.01%   \n==========================================\n  Files          85       85            \n  Lines        8845     8847       +2   \n  Branches     1889     1890       +1   \n==========================================\n+ Hits         7030     7032       +2   \n  Misses       1550     1550            \n  Partials      265      265\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.04% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d04efb4...8807f1f. Read the comment docs.\n. # Codecov Report\nMerging #2229 into 1.1.x will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\n1.1.x    #2229   +/-\n=======================================\n  Coverage   79.47%   79.47%         \n=======================================\n  Files          85       85         \n  Lines        8845     8845         \n  Branches     1889     1889         \n=======================================\n  Hits         7030     7030         \n  Misses       1550     1550         \n  Partials      265      265\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 77.27% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d04efb4...b097495. Read the comment docs.\n. # Codecov Report\nMerging #2230 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2230   +/-\n=======================================\n  Coverage   79.48%   79.48%         \n=======================================\n  Files          85       85         \n  Lines        8847     8847         \n  Branches     1890     1890         \n=======================================\n  Hits         7032     7032         \n  Misses       1550     1550         \n  Partials      265      265\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 10cf578...eed289c. Read the comment docs.\n. # Codecov Report\nMerging #2231 into 1.1.x will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\n1.1.x    #2231   +/-\n=======================================\n  Coverage   79.48%   79.48%         \n=======================================\n  Files          85       85         \n  Lines        8847     8847         \n  Branches     1890     1890         \n=======================================\n  Hits         7032     7032         \n  Misses       1550     1550         \n  Partials      265      265\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b7e51b4...ce8f6e4. Read the comment docs.\n. # Codecov Report\nMerging #2233 into 1.1.x will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\n1.1.x    #2233   +/-\n=======================================\n  Coverage   79.48%   79.48%         \n=======================================\n  Files          85       85         \n  Lines        8847     8847         \n  Branches     1890     1890         \n=======================================\n  Hits         7032     7032         \n  Misses       1550     1550         \n  Partials      265      265\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7ac4586...f653336. Read the comment docs.\n. # Codecov Report\nMerging #2234 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2234      +/-\n==========================================\n+ Coverage   79.48%   79.48%   +<.01%   \n==========================================\n  Files          85       85            \n  Lines        8847     8849       +2   \n  Branches     1890     1891       +1   \n==========================================\n+ Hits         7032     7034       +2   \n  Misses       1550     1550            \n  Partials      265      265\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.04% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0cee578...654b950. Read the comment docs.\n. # Codecov Report\nMerging #2240 into master will decrease coverage by 0.02%.\nThe diff coverage is 71.87%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2240      +/-\n==========================================\n- Coverage    79.6%   79.58%   -0.03%   \n==========================================\n  Files          85       87       +2   \n  Lines        8857     8889      +32   \n  Branches     1897     1900       +3   \n==========================================\n+ Hits         7051     7074      +23   \n- Misses       1545     1551       +6   \n- Partials      261      264       +3\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/instrumentation/__init__.py | 100% <100%> (\u00f8) | |\n| webapp/graphite/urls.py | 85.71% <100%> (+5.71%) | :arrow_up: |\n| webapp/graphite/settings.py | 77.61% <100%> (+0.33%) | :arrow_up: |\n| webapp/graphite/app_settings.py | 80% <50%> (-10.91%) | :arrow_down: |\n| webapp/graphite/instrumentation/apps.py | 68.75% <68.75%> (\u00f8) | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d6e45fa...b2328a9. Read the comment docs.\n. # Codecov Report\nMerging #2244 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #2244   +/-\n======================================\n  Coverage    79.6%   79.6%         \n======================================\n  Files          85      85         \n  Lines        8857    8857         \n  Branches     1897    1897         \n======================================\n  Hits         7051    7051         \n  Misses       1545    1545         \n  Partials      261     261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/remote.py | 99.33% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/views.py | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update d6e45fa...9e20e18. Read the comment docs.\n. # Codecov Report\nMerging #2252 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #2252   +/-\n======================================\n  Coverage    79.6%   79.6%         \n======================================\n  Files          85      85         \n  Lines        8857    8857         \n  Branches     1897    1897         \n======================================\n  Hits         7051    7051         \n  Misses       1545    1545         \n  Partials      261     261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.04% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8ad9ad4...17e115b. Read the comment docs.\n. # Codecov Report\nMerging #2254 into master will increase coverage by <.01%.\nThe diff coverage is 14.28%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2254      +/-\n==========================================\n+ Coverage    79.6%   79.61%   +<.01%   \n==========================================\n  Files          85       85            \n  Lines        8857     8858       +1   \n  Branches     1897     1897            \n==========================================\n+ Hits         7051     7052       +1   \n  Misses       1545     1545            \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/glyph.py | 41.69% <14.28%> (+0.04%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1de9f70...63594f9. Read the comment docs.\n. # Codecov Report\nMerging #2257 into master will decrease coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2257      +/-\n==========================================\n- Coverage   79.61%   79.58%   -0.03%   \n==========================================\n  Files          85       85            \n  Lines        8858     8848      -10   \n  Branches     1897     1894       -3   \n==========================================\n- Hits         7052     7042      -10   \n  Misses       1545     1545            \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/datalib.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/views.py | 69.95% <100%> (-0.94%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 42226ae...eb73620. Read the comment docs.\n. # Codecov Report\nMerging #2258 into 1.1.x will increase coverage by 0.1%.\nThe diff coverage is 85.71%.\n\n\n```diff\n@@            Coverage Diff            @@\n1.1.x    #2258     +/-\n=========================================\n+ Coverage   79.48%   79.58%   +0.1%   \n=========================================\n  Files          85       85           \n  Lines        8847     8848      +1   \n  Branches     1890     1894      +4   \n=========================================\n+ Hits         7032     7042     +10   \n+ Misses       1550     1545      -5   \n+ Partials      265      261      -4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/remote.py | 99.33% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/finders/ceres.py | 95.55% <100%> (+8.88%) | :arrow_up: |\n| webapp/graphite/tags/utils.py | 88.46% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/functions.py | 96.04% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/datalib.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/views.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/views.py | 69.95% <100%> (-0.94%) | :arrow_down: |\n| webapp/graphite/render/glyph.py | 41.69% <14.28%> (+0.04%) | :arrow_up: |\n| webapp/graphite/finders/standard.py | 75.75% <82.25%> (+5.59%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 0c3802a...9ccd2a0. Read the comment docs.\n. # Codecov Report\nMerging #2259 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2259   +/-\n=======================================\n  Coverage   79.58%   79.58%         \n=======================================\n  Files          85       85         \n  Lines        8848     8848         \n  Branches     1894     1894         \n=======================================\n  Hits         7042     7042         \n  Misses       1545     1545         \n  Partials      261      261\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update a1e6020...6960f24. Read the comment docs.\n. # Codecov Report\nMerging #2262 into master will decrease coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2262      +/-\n==========================================\n- Coverage   79.58%   79.57%   -0.02%   \n==========================================\n  Files          85       85            \n  Lines        8848     8842       -6   \n  Branches     1894     1893       -1   \n==========================================\n- Hits         7042     7036       -6   \n  Misses       1545     1545            \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.03% <100%> (-0.02%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 085e60c...362eb39. Read the comment docs.\n. # Codecov Report\nMerging #2264 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2264   +/-\n=======================================\n  Coverage   79.58%   79.58%         \n=======================================\n  Files          85       85         \n  Lines        8848     8848         \n  Branches     1894     1894         \n=======================================\n  Hits         7042     7042         \n  Misses       1545     1545         \n  Partials      261      261\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 085e60c...362a68a. Read the comment docs.\n. # Codecov Report\nMerging #2266 into master will increase coverage by 0.04%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2266      +/-\n==========================================\n+ Coverage   79.58%   79.62%   +0.04%   \n==========================================\n  Files          85       85            \n  Lines        8848     8846       -2   \n  Branches     1894     1892       -2   \n==========================================\n+ Hits         7042     7044       +2   \n+ Misses       1545     1543       -2   \n+ Partials      261      259       -2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.23% <100%> (+0.18%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 085e60c...65d3714. Read the comment docs.\n. # Codecov Report\nMerging #2270 into 1.1.x will increase coverage by 0.02%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\n1.1.x    #2270      +/-\n==========================================\n+ Coverage   79.58%   79.61%   +0.02%   \n==========================================\n  Files          85       85            \n  Lines        8848     8840       -8   \n  Branches     1894     1891       -3   \n==========================================\n- Hits         7042     7038       -4   \n+ Misses       1545     1543       -2   \n+ Partials      261      259       -2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.22% <100%> (+0.17%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 94d4200...1c5892e. Read the comment docs.\n. # Codecov Report\nMerging #2271 into 1.1.x will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\n1.1.x    #2271   +/-\n=======================================\n  Coverage   79.58%   79.58%         \n=======================================\n  Files          85       85         \n  Lines        8848     8848         \n  Branches     1894     1894         \n=======================================\n  Hits         7042     7042         \n  Misses       1545     1545         \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 77.27% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 94d4200...47a2507. Read the comment docs.\n. # Codecov Report\nMerging #2276 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2276   +/-\n=======================================\n  Coverage   79.61%   79.61%         \n=======================================\n  Files          85       85         \n  Lines        8840     8840         \n  Branches     1891     1891         \n=======================================\n  Hits         7038     7038         \n  Misses       1543     1543         \n  Partials      259      259\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4e381d3...470218f. Read the comment docs.\n. # Codecov Report\nMerging #2277 into master will not change coverage.\nThe diff coverage is 0%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2277   +/-\n=======================================\n  Coverage   79.61%   79.61%         \n=======================================\n  Files          85       85         \n  Lines        8840     8840         \n  Branches     1891     1891         \n=======================================\n  Hits         7038     7038         \n  Misses       1543     1543         \n  Partials      259      259\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/account/ldapBackend.py | 0% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c0b4ea9...0f2af92. Read the comment docs.\n. # Codecov Report\nMerging #2278 into master will increase coverage by 0.01%.\nThe diff coverage is 90.9%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2278      +/-\n==========================================\n+ Coverage   79.61%   79.62%   +0.01%   \n==========================================\n  Files          85       85            \n  Lines        8840     8846       +6   \n  Branches     1891     1892       +1   \n==========================================\n+ Hits         7038     7044       +6   \n  Misses       1543     1543            \n  Partials      259      259\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/app_settings.py | 90.9% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/user_util.py | 96.15% <100%> (+0.91%) | :arrow_up: |\n| webapp/graphite/dashboard/views.py | 99.25% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/account/views.py | 30.76% <50%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c0b4ea9...88f4de0. Read the comment docs.\n. # Codecov Report\nMerging #2281 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2281   +/-\n=======================================\n  Coverage   79.62%   79.62%         \n=======================================\n  Files          85       85         \n  Lines        8846     8846         \n  Branches     1892     1892         \n=======================================\n  Hits         7044     7044         \n  Misses       1543     1543         \n  Partials      259      259\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9c44f96...d7455b4. Read the comment docs.\n. # Codecov Report\nMerging #2285 into master will decrease coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2285      +/-\n==========================================\n- Coverage   79.62%   79.62%   -0.01%   \n==========================================\n  Files          85       85            \n  Lines        8846     8845       -1   \n  Branches     1892     1891       -1   \n==========================================\n- Hits         7044     7043       -1   \n  Misses       1543     1543            \n  Partials      259      259\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.22% <100%> (-0.01%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 07b2ed6...e6a0c64. Read the comment docs.\n. # Codecov Report\nMerging #2291 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2291   +/-\n=======================================\n  Coverage   79.62%   79.62%         \n=======================================\n  Files          85       85         \n  Lines        8845     8845         \n  Branches     1891     1891         \n=======================================\n  Hits         7043     7043         \n  Misses       1543     1543         \n  Partials      259      259\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/hashing.py | 93.39% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3a796b0...ca4ba6c. Read the comment docs.\n. # Codecov Report\nMerging #2294 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2294   +/-\n=======================================\n  Coverage   79.62%   79.62%         \n=======================================\n  Files          85       85         \n  Lines        8845     8845         \n  Branches     1891     1891         \n=======================================\n  Hits         7043     7043         \n  Misses       1543     1543         \n  Partials      259      259\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.22% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 3a796b0...b49e0d3. Read the comment docs.\n. # Codecov Report\nMerging #2295 into master will decrease coverage by 0.01%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #2295      +/-\n=========================================\n- Coverage   79.62%   79.6%   -0.02%   \n=========================================\n  Files          85      85            \n  Lines        8845    8847       +2   \n  Branches     1891    1892       +1   \n=========================================\n  Hits         7043    7043            \n- Misses       1543    1544       +1   \n- Partials      259     260       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/metrics/views.py | 86% <0%> (-0.72%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update fb469e5...e5911ee. Read the comment docs.\n. # Codecov Report\nMerging #2300 into master will not change coverage.\nThe diff coverage is 50%.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #2300   +/-\n======================================\n  Coverage    79.6%   79.6%         \n======================================\n  Files          85      85         \n  Lines        8847    8847         \n  Branches     1892    1892         \n======================================\n  Hits         7043    7043         \n  Misses       1544    1544         \n  Partials      260     260\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/hashing.py | 93.39% <0%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/datalib.py | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 79eadd7...7b8bcf2. Read the comment docs.\n. # Codecov Report\nMerging #2301 into master will not change coverage.\nThe diff coverage is 0%.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #2301   +/-\n======================================\n  Coverage    79.6%   79.6%         \n======================================\n  Files          85      85         \n  Lines        8847    8847         \n  Branches     1892    1892         \n======================================\n  Hits         7043    7043         \n  Misses       1544    1544         \n  Partials      260     260\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/carbonlink.py | 53.33% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 79eadd7...f705c4f. Read the comment docs.\n. # Codecov Report\nMerging #2303 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2303      +/-\n==========================================\n+ Coverage    79.6%   79.62%   +0.01%   \n==========================================\n  Files          85       85            \n  Lines        8847     8853       +6   \n  Branches     1892     1894       +2   \n==========================================\n+ Hits         7043     7049       +6   \n  Misses       1544     1544            \n  Partials      260      260\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 77.38% <100%> (+0.11%) | :arrow_up: |\n| webapp/graphite/storage.py | 89.02% <100%> (+0.17%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e15593a...b4df156. Read the comment docs.\n. # Codecov Report\nMerging #2303 into master will increase coverage by 0.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2303      +/-\n==========================================\n+ Coverage    79.6%   79.62%   +0.01%   \n==========================================\n  Files          85       85            \n  Lines        8847     8853       +6   \n  Branches     1892     1894       +2   \n==========================================\n+ Hits         7043     7049       +6   \n  Misses       1544     1544            \n  Partials      260      260\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 77.38% <100%> (+0.11%) | :arrow_up: |\n| webapp/graphite/storage.py | 89.02% <100%> (+0.17%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e15593a...b4df156. Read the comment docs.\n. # Codecov Report\nMerging #2304 into master will decrease coverage by 0.03%.\nThe diff coverage is n/a.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2304      +/-\n==========================================\n- Coverage    79.6%   79.57%   -0.04%   \n==========================================\n  Files          85       85            \n  Lines        8847     8847            \n  Branches     1892     1892            \n==========================================\n- Hits         7043     7040       -3   \n- Misses       1544     1547       +3   \n  Partials      260      260\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/hashing.py | 90.56% <0%> (-2.84%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e15593a...f2288a6. Read the comment docs.\n. # Codecov Report\nMerging #2307 into master will decrease coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #2307      +/-\n=========================================\n- Coverage    79.6%   79.6%   -0.01%   \n=========================================\n  Files          85      85            \n  Lines        8847    8841       -6   \n  Branches     1892    1891       -1   \n=========================================\n- Hits         7043    7038       -5   \n  Misses       1544    1544            \n+ Partials      260     259       -1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/__init__.py | 95.65% <100%> (+1.42%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4798ae9...555453f. Read the comment docs.\n. # Codecov Report\nMerging #2309 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff           @@\nmaster   #2309   +/-\n======================================\n  Coverage    79.6%   79.6%         \n======================================\n  Files          85      85         \n  Lines        8847    8847         \n  Branches     1892    1892         \n======================================\n  Hits         7043    7043         \n  Misses       1544    1544         \n  Partials      260     260\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 4798ae9...4250644. Read the comment docs.\n. # Codecov Report\nMerging #2317 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2317   +/-\n=======================================\n  Coverage   79.62%   79.62%         \n=======================================\n  Files          85       85         \n  Lines        8847     8847         \n  Branches     1893     1893         \n=======================================\n  Hits         7044     7044         \n  Misses       1544     1544         \n  Partials      259      259\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 250742f...1e72eee. Read the comment docs.\n. # Codecov Report\nMerging #2319 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2319   +/-\n=======================================\n  Coverage   79.62%   79.62%         \n=======================================\n  Files          85       85         \n  Lines        8847     8847         \n  Branches     1893     1893         \n=======================================\n  Hits         7044     7044         \n  Misses       1544     1544         \n  Partials      259      259\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 250742f...e617a0f. Read the comment docs.\n. # Codecov Report\nMerging #2322 into master will decrease coverage by 0.01%.\nThe diff coverage is 60%.\n\n\n```diff\n@@            Coverage Diff            @@\nmaster   #2322      +/-\n=========================================\n- Coverage   79.62%   79.6%   -0.02%   \n=========================================\n  Files          85      85            \n  Lines        8847    8852       +5   \n  Branches     1893    1896       +3   \n=========================================\n+ Hits         7044    7047       +3   \n- Misses       1544    1545       +1   \n- Partials      259     260       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/datalib.py | 98.57% <60%> (-1.43%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 5d3a3c9...e150af4. Read the comment docs.\n. # Codecov Report\nMerging #2326 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2326   +/-\n=======================================\n  Coverage   79.62%   79.62%         \n=======================================\n  Files          85       85         \n  Lines        8847     8847         \n  Branches     1893     1893         \n=======================================\n  Hits         7044     7044         \n  Misses       1544     1544         \n  Partials      259      259\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/__init__.py | 95.65% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7244a2c...6c0be51. Read the comment docs.\n. # Codecov Report\nMerging #2330 into master will decrease coverage by 0.07%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2330      +/-\n==========================================\n- Coverage   79.62%   79.54%   -0.08%   \n==========================================\n  Files          85       85            \n  Lines        8847     8868      +21   \n  Branches     1893     1899       +6   \n==========================================\n+ Hits         7044     7054      +10   \n- Misses       1544     1551       +7   \n- Partials      259      263       +4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings/__init__.py | 77.94% <100%> (\u00f8) | |\n| webapp/graphite/app_settings.py | 77.41% <0%> (-13.49%) | :arrow_down: |\n| webapp/graphite/wsgi.py | 38.7% <0%> (-2.67%) | :arrow_down: |\n| webapp/graphite/render/datalib.py | 98.57% <0%> (-1.43%) | :arrow_down: |\n| webapp/graphite/dashboard/views.py | 98.51% <0%> (-0.75%) | :arrow_down: |\n| webapp/graphite/account/models.py | 100% <0%> (\u00f8) | :arrow_up: |\n| webapp/graphite/account/views.py | 30.76% <0%> (\u00f8) | :arrow_up: |\n| webapp/graphite/account/ldapBackend.py | 0% <0%> (\u00f8) | :arrow_up: |\n| webapp/graphite/composer/views.py | 24.69% <0%> (\u00f8) | :arrow_up: |\n| webapp/graphite/functions/views.py | 94.28% <0%> (\u00f8) | :arrow_up: |\n| ... and 20 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7244a2c...6a6118c. Read the comment docs.\n. # Codecov Report\nMerging #2331 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2331   +/-\n=======================================\n  Coverage   79.62%   79.62%         \n=======================================\n  Files          85       85         \n  Lines        8847     8847         \n  Branches     1893     1893         \n=======================================\n  Hits         7044     7044         \n  Misses       1544     1544         \n  Partials      259      259\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7244a2c...56c19ec. Read the comment docs.\n. # Codecov Report\nMerging #2333 into master will decrease coverage by 0.05%.\nThe diff coverage is 33.33%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2333      +/-\n==========================================\n- Coverage   79.62%   79.56%   -0.06%   \n==========================================\n  Files          85       85            \n  Lines        8847     8858      +11   \n  Branches     1893     1896       +3   \n==========================================\n+ Hits         7044     7048       +4   \n- Misses       1544     1550       +6   \n- Partials      259      260       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/wsgi.py | 38.7% <0%> (-2.67%) | :arrow_down: |\n| webapp/graphite/app_settings.py | 77.41% <44.44%> (-13.49%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7b688f1...66e1de3. Read the comment docs.\n. # Codecov Report\nMerging #2336 into master will not change coverage.\nThe diff coverage is 50%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2336   +/-\n=======================================\n  Coverage   79.55%   79.55%         \n=======================================\n  Files          85       85         \n  Lines        8863     8863         \n  Branches     1899     1899         \n=======================================\n  Hits         7051     7051         \n  Misses       1551     1551         \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/dashboard/models.py | 90% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/functions/__init__.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/functions/params.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/events/compat.py | 75% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/account/views.py | 30.76% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/functions/views.py | 94.28% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/models.py | 77.77% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/account/models.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/account/admin.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/dashboard/views.py | 99.25% <\u00f8> (\u00f8) | :arrow_up: |\n| ... and 21 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 824800a...c2906cc. Read the comment docs.\n. # Codecov Report\nMerging #2338 into master will decrease coverage by 0.01%.\nThe diff coverage is 60%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2338      +/-\n==========================================\n- Coverage   79.55%   79.54%   -0.02%   \n==========================================\n  Files          85       85            \n  Lines        8863     8868       +5   \n  Branches     1899     1902       +3   \n==========================================\n+ Hits         7051     7054       +3   \n- Misses       1551     1552       +1   \n- Partials      261      262       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/readers/utils.py | 97.01% <60%> (-2.99%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 824800a...57fa464. Read the comment docs.\n. # Codecov Report\nMerging #2339 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2339   +/-\n=======================================\n  Coverage   79.55%   79.55%         \n=======================================\n  Files          85       85         \n  Lines        8863     8863         \n  Branches     1899     1899         \n=======================================\n  Hits         7051     7051         \n  Misses       1551     1551         \n  Partials      261      261\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 87240bf...4b6ce14. Read the comment docs.\n. # Codecov Report\nMerging #2340 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2340   +/-\n=======================================\n  Coverage   79.55%   79.55%         \n=======================================\n  Files          85       85         \n  Lines        8863     8863         \n  Branches     1899     1899         \n=======================================\n  Hits         7051     7051         \n  Misses       1551     1551         \n  Partials      261      261\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 15b4372...0e4339e. Read the comment docs.\n. # Codecov Report\nMerging #2341 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2341   +/-\n=======================================\n  Coverage   79.55%   79.55%         \n=======================================\n  Files          85       85         \n  Lines        8863     8863         \n  Branches     1899     1899         \n=======================================\n  Hits         7051     7051         \n  Misses       1551     1551         \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.22% <\u00f8> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 15b4372...d6fdbac. Read the comment docs.\n. # Codecov Report\nMerging #2342 into master will decrease coverage by 0.02%.\nThe diff coverage is 0%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2342      +/-\n==========================================\n- Coverage   79.55%   79.52%   -0.03%   \n==========================================\n  Files          85       85            \n  Lines        8863     8866       +3   \n  Branches     1899     1901       +2   \n==========================================\n  Hits         7051     7051            \n- Misses       1551     1553       +2   \n- Partials      261      262       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/readers/utils.py | 95.38% <0%> (-4.62%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c96420c...d62b863. Read the comment docs.\n. # Codecov Report\nMerging #2343 into 1.1.x will decrease coverage by <.01%.\nThe diff coverage is 83.33%.\n\n\n```diff\n@@            Coverage Diff            @@\n1.1.x   #2343      +/-\n=========================================\n- Coverage   79.61%   79.6%   -0.01%   \n=========================================\n  Files          85      85            \n  Lines        8840    8847       +7   \n  Branches     1891    1892       +1   \n=========================================\n+ Hits         7038    7043       +5   \n- Misses       1543    1544       +1   \n- Partials      259     260       +1\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/account/ldapBackend.py | 0% <0%> (\u00f8) | :arrow_up: |\n| webapp/graphite/metrics/views.py | 86% <0%> (-0.72%) | :arrow_down: |\n| webapp/graphite/render/hashing.py | 93.39% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/user_util.py | 96.15% <100%> (+0.91%) | :arrow_up: |\n| webapp/graphite/app_settings.py | 90.9% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/dashboard/views.py | 99.25% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/functions.py | 96.22% <100%> (-0.01%) | :arrow_down: |\n| webapp/graphite/account/views.py | 30.76% <50%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 496b2aa...130fc9d. Read the comment docs.\n. # Codecov Report\nMerging #2344 into 1.1.x will increase coverage by 0.01%.\nThe diff coverage is 77.77%.\n\n\n```diff\n@@            Coverage Diff             @@\n1.1.x    #2344      +/-\n==========================================\n+ Coverage   79.61%   79.62%   +0.01%   \n==========================================\n  Files          85       85            \n  Lines        8840     8846       +6   \n  Branches     1891     1893       +2   \n==========================================\n+ Hits         7038     7044       +6   \n  Misses       1543     1543            \n  Partials      259      259\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/carbonlink.py | 53.33% <0%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/hashing.py | 93.39% <0%> (\u00f8) | :arrow_up: |\n| webapp/graphite/storage.py | 89.02% <100%> (+0.17%) | :arrow_up: |\n| webapp/graphite/settings.py | 77.38% <100%> (+0.11%) | :arrow_up: |\n| webapp/graphite/render/datalib.py | 100% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 496b2aa...12baae3. Read the comment docs.\n. # Codecov Report\nMerging #2345 into 1.1.x will decrease coverage by 0.07%.\nThe diff coverage is 74.64%.\n\n\n```diff\n@@            Coverage Diff             @@\n1.1.x    #2345      +/-\n==========================================\n- Coverage   79.62%   79.54%   -0.08%   \n==========================================\n  Files          85       85            \n  Lines        8853     8897      +44   \n  Branches     1894     1901       +7   \n==========================================\n+ Hits         7049     7077      +28   \n- Misses       1544     1558      +14   \n- Partials      260      262       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/functions/__init__.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/models.py | 77.77% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/dashboard/views.py | 99.25% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/functions/views.py | 94.28% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/account/admin.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/dashboard/models.py | 90% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/functions/params.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/tags/views.py | 100% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/url_shortener/baseconv.py | 36.66% <\u00f8> (\u00f8) | :arrow_up: |\n| webapp/graphite/events/compat.py | 75% <\u00f8> (\u00f8) | :arrow_up: |\n| ... and 26 more | |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 326137a...67d0997. Read the comment docs.\n. # Codecov Report\nMerging #2347 into 1.1.x will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\n1.1.x    #2347   +/-\n=======================================\n  Coverage   79.54%   79.54%         \n=======================================\n  Files          85       85         \n  Lines        8897     8897         \n  Branches     1901     1901         \n=======================================\n  Hits         7077     7077         \n  Misses       1558     1558         \n  Partials      262      262\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 77.38% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 40dcdf2...ad66037. Read the comment docs.\n. # Codecov Report\nMerging #2348 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2348   +/-\n=======================================\n  Coverage   79.54%   79.54%         \n=======================================\n  Files          85       85         \n  Lines        8897     8897         \n  Branches     1901     1901         \n=======================================\n  Hits         7077     7077         \n  Misses       1558     1558         \n  Partials      262      262\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update eb19ceb...31c001b. Read the comment docs.\n. # Codecov Report\nMerging #2349 into 1.1.x will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\n1.1.x    #2349   +/-\n=======================================\n  Coverage   79.54%   79.54%         \n=======================================\n  Files          85       85         \n  Lines        8897     8897         \n  Branches     1901     1901         \n=======================================\n  Hits         7077     7077         \n  Misses       1558     1558         \n  Partials      262      262\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 11076ed...efa9883. Read the comment docs.\n. # Codecov Report\nMerging #2350 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2350      +/-\n==========================================\n+ Coverage   79.54%   79.54%   +<.01%   \n==========================================\n  Files          85       85            \n  Lines        8897     8899       +2   \n  Branches     1901     1901            \n==========================================\n+ Hits         7077     7079       +2   \n  Misses       1558     1558            \n  Partials      262      262\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/remote.py | 99.34% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 778eb07...c3e33e1. Read the comment docs.\n. # Codecov Report\nMerging #2354 into master will decrease coverage by 0.01%.\nThe diff coverage is 82.92%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2354      +/-\n==========================================\n- Coverage   79.54%   79.53%   -0.02%   \n==========================================\n  Files          85       85            \n  Lines        8899     8887      -12   \n  Branches     1901     1899       -2   \n==========================================\n- Hits         7079     7068      -11   \n+ Misses       1558     1557       -1   \n  Partials      262      262\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/readers/remote.py | 100% <100%> (+8.98%) | :arrow_up: |\n| webapp/graphite/finders/remote.py | 95.62% <82.5%> (-3.72%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 17e2e1f...6c275a9. Read the comment docs.\n. # Codecov Report\nMerging #2366 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2366   +/-\n=======================================\n  Coverage   79.55%   79.55%         \n=======================================\n  Files          85       85         \n  Lines        8902     8902         \n  Branches     1901     1901         \n=======================================\n  Hits         7082     7082         \n  Misses       1558     1558         \n  Partials      262      262\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update ee3a14d...c939100. Read the comment docs.\n. # Codecov Report\nMerging #2368 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2368      +/-\n==========================================\n+ Coverage   79.55%   79.55%   +<.01%   \n==========================================\n  Files          85       85            \n  Lines        8902     8904       +2   \n  Branches     1901     1901            \n==========================================\n+ Hits         7082     7084       +2   \n  Misses       1558     1558            \n  Partials      262      262\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/tags/localdatabase.py | 94.36% <100%> (+0.05%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update e19fc64...7396dff. Read the comment docs.\n. # Codecov Report\nMerging #2370 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2370      +/-\n==========================================\n+ Coverage   79.62%   79.62%   +<.01%   \n==========================================\n  Files          85       85            \n  Lines        8892     8893       +1   \n  Branches     1899     1899            \n==========================================\n+ Hits         7080     7081       +1   \n  Misses       1551     1551            \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/finders/standard.py | 75.93% <100%> (+0.18%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 183cfca...08c1ac7. Read the comment docs.\n. # Codecov Report\nMerging #2371 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2371   +/-\n=======================================\n  Coverage   79.62%   79.62%         \n=======================================\n  Files          85       85         \n  Lines        8892     8892         \n  Branches     1899     1899         \n=======================================\n  Hits         7080     7080         \n  Misses       1551     1551         \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/evaluator.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/functions.py | 96.22% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 183cfca...0f30228. Read the comment docs.\n. # Codecov Report\nMerging #2372 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2372   +/-\n=======================================\n  Coverage   79.62%   79.62%         \n=======================================\n  Files          85       85         \n  Lines        8892     8892         \n  Branches     1899     1899         \n=======================================\n  Hits         7080     7080         \n  Misses       1551     1551         \n  Partials      261      261\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1134385...901968f. Read the comment docs.\n. # Codecov Report\nMerging #2373 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2373   +/-\n=======================================\n  Coverage   79.62%   79.62%         \n=======================================\n  Files          85       85         \n  Lines        8892     8892         \n  Branches     1899     1899         \n=======================================\n  Hits         7080     7080         \n  Misses       1551     1551         \n  Partials      261      261\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 1134385...a382331. Read the comment docs.\n. # Codecov Report\nMerging #2375 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2375      +/-\n==========================================\n+ Coverage   79.85%   79.86%   +<.01%   \n==========================================\n  Files          85       85            \n  Lines        8895     8899       +4   \n  Branches     1899     1901       +2   \n==========================================\n+ Hits         7103     7107       +4   \n  Misses       1526     1526            \n  Partials      266      266\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.22% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 705d302...05859b2. Read the comment docs.\n. # Codecov Report\nMerging #2375 into master will increase coverage by <.01%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2375      +/-\n==========================================\n+ Coverage   79.85%   79.86%   +<.01%   \n==========================================\n  Files          85       85            \n  Lines        8895     8899       +4   \n  Branches     1899     1901       +2   \n==========================================\n+ Hits         7103     7107       +4   \n  Misses       1526     1526            \n  Partials      266      266\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/functions.py | 96.22% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 705d302...05859b2. Read the comment docs.\n. # Codecov Report\nMerging #2377 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2377   +/-\n=======================================\n  Coverage   79.62%   79.62%         \n=======================================\n  Files          85       85         \n  Lines        8893     8893         \n  Branches     1899     1899         \n=======================================\n  Hits         7081     7081         \n  Misses       1551     1551         \n  Partials      261      261\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/readers/rrd.py | 46.66% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 839b443...22a971f. Read the comment docs.\n. # Codecov Report\nMerging #2379 into 0.9.x will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@          Coverage Diff          @@\n0.9.x   #2379   +/-\n=====================================\n  Coverage   33.5%   33.5%         \n=====================================\n  Files         57      57         \n  Lines       8668    8668         \n  Branches    1588    1588         \n=====================================\n  Hits        2904    2904         \n  Misses      5413    5413         \n  Partials     351     351\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 9c603a3...a18af30. Read the comment docs.\n. # Codecov Report\nMerging #2380 into master will decrease coverage by 0.03%.\nThe diff coverage is 73.33%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2380      +/-\n==========================================\n- Coverage   79.62%   79.59%   -0.04%   \n==========================================\n  Files          85       85            \n  Lines        8893     8904      +11   \n  Branches     1899     1904       +5   \n==========================================\n+ Hits         7081     7087       +6   \n- Misses       1551     1554       +3   \n- Partials      261      263       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/readers/utils.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/render/functions.py | 96.13% <33.33%> (-0.1%) | :arrow_down: |\n| webapp/graphite/render/datalib.py | 96.55% <75%> (-2.02%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 839b443...cdedb9c. Read the comment docs.\n. # Codecov Report\nMerging #2383 into master will increase coverage by 0.22%.\nThe diff coverage is 100%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2383      +/-\n==========================================\n+ Coverage   79.62%   79.85%   +0.22%   \n==========================================\n  Files          85       85            \n  Lines        8893     8895       +2   \n  Branches     1899     1899            \n==========================================\n+ Hits         7081     7103      +22   \n+ Misses       1551     1526      -25   \n- Partials      261      266       +5\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/readers/rrd.py | 85.48% <100%> (+38.81%) | :arrow_up: |\n| webapp/graphite/settings.py | 76.61% <0%> (-1%) | :arrow_down: |\n| webapp/graphite/finders/standard.py | 75.18% <0%> (-0.76%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 570265e...eed617a. Read the comment docs.\n. # Codecov Report\nMerging #2399 into 1.1.x will increase coverage by 0.17%.\nThe diff coverage is 82.85%.\n\n\n```diff\n@@            Coverage Diff             @@\n1.1.x    #2399      +/-\n==========================================\n+ Coverage   79.62%   79.79%   +0.17%   \n==========================================\n  Files          85       85            \n  Lines        8892     8910      +18   \n  Branches     1899     1906       +7   \n==========================================\n+ Hits         7080     7110      +30   \n+ Misses       1551     1537      -14   \n- Partials      261      263       +2\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/evaluator.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/readers/utils.py | 100% <100%> (\u00f8) | :arrow_up: |\n| webapp/graphite/finders/standard.py | 71.42% <100%> (-4.33%) | :arrow_down: |\n| webapp/graphite/readers/rrd.py | 45.16% <60%> (-1.51%) | :arrow_down: |\n| webapp/graphite/render/datalib.py | 96.55% <75%> (-2.02%) | :arrow_down: |\n| webapp/graphite/render/functions.py | 96.13% <85.71%> (-0.09%) | :arrow_down: |\n| webapp/graphite/settings.py | 77.61% <0%> (\u00f8) | :arrow_up: |\n| webapp/graphite/composer/views.py | 55.55% <0%> (+30.86%) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 8820eb8...e012af4. Read the comment docs.\n. # Codecov Report\nMerging #2402 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2402   +/-\n=======================================\n  Coverage   80.11%   80.11%         \n=======================================\n  Files          85       85         \n  Lines        8910     8910         \n  Branches     1906     1906         \n=======================================\n  Hits         7138     7138         \n  Misses       1504     1504         \n  Partials      268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update b6d6d93...2d2de08. Read the comment docs.\n. # Codecov Report\nMerging #2403 into 1.1.x will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\n1.1.x    #2403   +/-\n=======================================\n  Coverage   79.79%   79.79%         \n=======================================\n  Files          85       85         \n  Lines        8910     8910         \n  Branches     1906     1906         \n=======================================\n  Hits         7110     7110         \n  Misses       1537     1537         \n  Partials      263      263\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 54515b9...5594294. Read the comment docs.\n. # Codecov Report\nMerging #2407 into master will not change coverage.\nThe diff coverage is n/a.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2407   +/-\n=======================================\n  Coverage   80.11%   80.11%         \n=======================================\n  Files          85       85         \n  Lines        8910     8910         \n  Branches     1906     1906         \n=======================================\n  Hits         7138     7138         \n  Misses       1504     1504         \n  Partials      268      268\n```\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update c3b06cc...22e8348. Read the comment docs.\n. # Codecov Report\nMerging #2409 into master will decrease coverage by 0.04%.\nThe diff coverage is 78.94%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2409      +/-\n==========================================\n- Coverage   80.11%   80.07%   -0.05%   \n==========================================\n  Files          85       85            \n  Lines        8910     8922      +12   \n  Branches     1906     1907       +1   \n==========================================\n+ Hits         7138     7144       +6   \n  Misses       1504     1504            \n- Partials      268      274       +6\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/settings.py | 76.38% <100%> (-0.24%) | :arrow_down: |\n| webapp/graphite/logger.py | 78.46% <75%> (-1.94%) | :arrow_down: |\n| webapp/graphite/dashboard/views.py | 98.51% <0%> (-0.75%) | :arrow_down: |\n| webapp/graphite/wsgi.py | 40.62% <0%> (\u00f8) | :arrow_up: |\n| webapp/graphite/app_settings.py | 77.41% <0%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 928affa...caed07b. Read the comment docs.\n. # Codecov Report\nMerging #2411 into master will not change coverage.\nThe diff coverage is 100%.\n\n\n```diff\n@@           Coverage Diff           @@\nmaster    #2411   +/-\n=======================================\n  Coverage   80.11%   80.11%         \n=======================================\n  Files          85       85         \n  Lines        8910     8910         \n  Branches     1906     1906         \n=======================================\n  Hits         7138     7138         \n  Misses       1504     1504         \n  Partials      268      268\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/render/views.py | 69.95% <100%> (\u00f8) | :arrow_up: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update bddaed2...c56d546. Read the comment docs.\n. # Codecov Report\nMerging #2426 into master will decrease coverage by 0.05%.\nThe diff coverage is 52.94%.\n\n\n```diff\n@@            Coverage Diff             @@\nmaster    #2426      +/-\n==========================================\n- Coverage   80.11%   80.05%   -0.06%   \n==========================================\n  Files          85       85            \n  Lines        8910     8926      +16   \n  Branches     1906     1912       +6   \n==========================================\n+ Hits         7138     7146       +8   \n- Misses       1504     1508       +4   \n- Partials      268      272       +4\n```\n| Impacted Files | Coverage \u0394 | |\n|---|---|---|\n| webapp/graphite/tags/utils.py | 79.41% <52.94%> (-9.05%) | :arrow_down: |\n\nContinue to review full report at Codecov.\n\nLegend - Click here to learn more\n\u0394 = absolute <relative> (impact), \u00f8 = not affected, ? = missing data\nPowered by Codecov. Last update 7ce7893...7149908. Read the comment docs.\n. \n",
    "marianogg9": "@cbowman0 hi there. Is there a file where I can get a template example? like the one you described \"linux server\". I need to apply that template to my existing graphite installation.\nThanks!!\n. Hi Jeff. Thanks for your answer. I'm sorry, but the last log line must be \"varnish_server.domain.varnish.backend_conn\" (dots instead of commas), was just a typing error. Again, my question is more like \"if dots are delimeters, then shouldn't be a folder named varnish??\"\nThanks!\n. Jason\nHere're few examples of metric_varnish.rb plugin's output:\nvarnish_server.varnish.hcb_nolock\nvarnish_server.varnish.LCK.sms.locks\nvarnish_server.varnish.SMA.s0.c_req\nWhere \"varnish_server\" is a name I've replaced instead of my real varnish server name (just privacy matter). Again, there's no folder \"varnish\" shown in graphite, but LCK and SMA are shown normally. So, why LCK and SMA are, but \"varnish\" isn't? \nDigging up a little into metric_varnish.rb, I found that:\noption :scheme,\n    :description => \"Metric naming scheme, text to prepend to metric\",\n    :short => \"-s SCHEME\",\n    :long => \"--scheme SCHEME\",\n    :default => \"#{Socket.gethostname}.varnish\"\ndef run\n    begin\n      varnishstat = varnishstat -x\n      stats = Crack::XML.parse(varnishstat)\n      stats['varnishstat']['stat'].each do |stat|\n        path = \"#{config[:scheme]}\"\n        path += \".\" + graphite_path_sanitize(stat['type'])    if stat['type']\n        path += \".\" + graphite_path_sanitize(stat['ident'])   if stat['ident']\n        path += \".\" + graphite_path_sanitize(stat['name'])\n        output path, stat['value']\n      end\nSo, shouldn't be a folder named \"varnish\" shown on graphite's folder tree? If it shouldn't, then I don't get which characters are delimeters.\nThanks.\n. Yes I did. So..this varnish_metric.rb cannot be ever graphed correctly.\nLet's take this example instead: metric_disk.rb (https://github.com/sensu/sensu-community-plugins/blob/master/plugins/system/disk-metrics.rb). These are three of its output lines:\nvarnish_server.disk.xvda1.reads ..\nvarnish_server.disk.xvdb.writes .. \nvarnish_server.disk.xvdb1.writeTime ..\nIn this example, folders xvda1, xvdb and xvdb1 are correctly shown while \"disk\" isn't present. This is the same issue as before and there's no \"varnish\" folder involved.\nThanks.\n. Thanks Jason. I'm sorry if I didn't get it. I finally understood it with the bug.\n. Thanks @pcn , that was a really complete explanation.\nSo, once I edit the parameters to 60s:180d, the whisper files will have to be edited too. Can be this done by deleting the folders in ../graphite/storage/whisper/stats/ ? or where do I get this resize script?\nThanks!\n. ",
    "frejsoya": "What version of whisper is used?\n. No, This is in graphite Remote_storage.py not whisper.\nOn Sunday, March 9, 2014, Bruno Reni\u00e9 notifications@github.com wrote:\n\nDuplicate of #597https://github.com/graphite-project/graphite-web/issues/597\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/issues/657#issuecomment-37138234\n.\n. Sorry for the 'me2' post, but just release something. Either there are  a lot out there just running master or all the new contributions are not being used. \n. \n",
    "murtazavf": "Upgrading the whisper solved the problem. Thanks a lot for giving hints.\nRegards\nMurtaza\nOn Thu, Sep 12, 2013 at 6:15 PM, Frej Soya notifications@github.com wrote:\n\nWhat version of whisper is used?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/issues/426#issuecomment-24317590\n.\n. \n",
    "odedpriva": "I have the latest one ( whisper-0.9.10 ) but I'm still getting the same error\nTraceback (most recent call last):\n  File \"/opt/graphite/bin/carbon-cache.py\", line 32, in \n    run_twistd_plugin(file)\n  File \"/opt/graphite/lib/carbon/util.py\", line 94, in run_twistd_plugin\n    config.parseOptions(twistd_options)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/application/app.py\", line 614, in parseOptions\n    usage.Options.parseOptions(self, options)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/python/usage.py\", line 266, in parseOptions\n    self.subOptions.parseOptions(rest)\n  File \"/usr/local/lib/python2.7/dist-packages/twisted/python/usage.py\", line 276, in parseOptions\n    self.postOptions()\n  File \"/opt/graphite/lib/carbon/conf.py\", line 221, in postOptions\n    if whisper.CAN_FALLOCATE:\nAttributeError: 'module' object has no attribute 'CAN_FALLOCATE'\nAny idea ? \n. ",
    "mcortinas": "my currently whysper version is 0.9.12 and i'm with the same error, how can I resolve it? by the moment i had to modify the line WHISPER_FALLOCATE_CREATE = False\n. ",
    "susampath": "@frejsoya could you please tell me how to update whispher. Thnk you very much\nOn 26 Jul 2017 12:58 pm, \"Denis Zhdanov\" notifications@github.com wrote:\nIt really depends on how it was installed, @susampath\nhttps://github.com/susampath\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/426#issuecomment-317972055,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AdDAurEG6eB1MbXkjyQwyHvPKXHvyq6Aks5sRuqDgaJpZM4A-iIi\n.\n. ",
    "thepaul": "As far as I can tell, perSecond(x) is equivalent to scaleToSeconds(nonNegativeDerivative(x),1), so there's a workaround. But I agree, it's lots cleaner and more convenient just to have perSecond() available. And since 0.10 might be a looong way out, can we have it back in 0.9.x?\n. ",
    "natebc": "It appears to my amateur eye to be a problem with the grammar parser leading to the **kwargs argument to evaluateTokens (evaluator.py:31) being empty. FWIW if i pass in an argument of the form \"something=somethingelse\" I do not get this 500 error but instead end up with a \"No Data\" graph.\nI tried my best to understand whats happening in the grammar parser but unfortunately I'm out of my depth in there.\n. @denniskong that worked for me too. I'm not familiar with the grammar parser at all but I'll poke around in there and see if I can find where the problem is.\n. ",
    "dglloyd": "I get the same thing while running a version cloned from head, with any function that expects more than a single argument.\nTypeError at /render\nscaleToSeconds() takes exactly 3 arguments (1 given)\nRequest Method:     GET\nRequest URL:    http://localhost:10001/render?target=scaleToSeconds(servers.d01.postgres.database.trac.size,60)\nDjango Version:     1.5.4\nException Type:     TypeError\nException Value:    \nscaleToSeconds() takes exactly 3 arguments (1 given)\nException Location:     /opt/graphite/webapp/graphite/render/evaluator.py in evaluateTokens, line 31\nPython Executable:  /usr/bin/python\nPython Version:     2.7.3\nPython Path:    \n['/opt/graphite/webapp',\n '/opt/graphite/webapp/graphite',\n '/usr/bin',\n '/usr/local/lib/python2.7/dist-packages/pystatsd-0.1.10-py2.7.egg',\n '/usr/lib/python2.7',\n '/usr/lib/python2.7/plat-linux2',\n '/usr/lib/python2.7/lib-tk',\n '/usr/lib/python2.7/lib-old',\n '/usr/lib/python2.7/lib-dynload',\n '/usr/local/lib/python2.7/dist-packages',\n '/usr/lib/python2.7/dist-packages',\n '/usr/lib/python2.7/dist-packages/gtk-2.0',\n '/usr/lib/pymodules/python2.7',\n '/opt/graphite/webapp',\n '/opt/graphite/webapp/graphite']\nServer time:    Mon, 7 Oct 2013 16:22:14 -0500\nEnvironment:\nRequest Method: GET\nRequest URL: http://localhost:10001/render?target=scaleToSeconds(servers.d01.postgres.database.trac.size,60)\nDjango Version: 1.5.4\nPython Version: 2.7.3\nInstalled Applications:\n('graphite.metrics',\n 'graphite.render',\n 'graphite.cli',\n 'graphite.browser',\n 'graphite.composer',\n 'graphite.account',\n 'graphite.dashboard',\n 'graphite.whitelist',\n 'graphite.events',\n 'django.contrib.auth',\n 'django.contrib.sessions',\n 'django.contrib.admin',\n 'django.contrib.contenttypes',\n 'tagging')\nInstalled Middleware:\n('django.middleware.common.CommonMiddleware',\n 'django.middleware.gzip.GZipMiddleware',\n 'django.contrib.sessions.middleware.SessionMiddleware',\n 'django.contrib.auth.middleware.AuthenticationMiddleware',\n 'django.contrib.messages.middleware.MessageMiddleware',\n 'django.contrib.messages.middleware.MessageMiddleware')\nTraceback:\nFile \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\" in get_response\n1.                         response = callback(request, _callback_args, _callback_kwargs)\n   File \"/opt/graphite/webapp/graphite/render/views.py\" in renderView\n2.         seriesList = evaluateTarget(requestContext, target)\n   File \"/opt/graphite/webapp/graphite/render/evaluator.py\" in evaluateTarget\n3.   result = evaluateTokens(requestContext, tokens)\n   File \"/opt/graphite/webapp/graphite/render/evaluator.py\" in evaluateTokens\n4.     return evaluateTokens(requestContext, tokens.expression)\n   File \"/opt/graphite/webapp/graphite/render/evaluator.py\" in evaluateTokens\n5.     return func(requestContext, _args, _kwargs)\nException Type: TypeError at /render\nException Value: scaleToSeconds() takes exactly 3 arguments (1 given)\n. ",
    "denniskong": "+1\n. My solution for now: I reverted \nhttps://github.com/graphite-project/graphite-web/issues/429#issuecomment-28316477\nwith git revert b776c6d42177c09e29fb191a2803300a79922608\nthen python setup.py install \n. ",
    "miikka": "~~I was experiencing this bug with 0c210d3cb9268d026a035eadf8f1f8cceefe28c1, but upgraded to the latest master (8072890e0b7364f5cf38bc497a971804b8017d93) and now it's gone. Great.~~\nEdit: this turned out to be false and the real reason was updating pyparsing; see my comments below\n. I think that the problem lies in pyparsing. I have a setup that is similar to @mbogosian's failing setup (Ubuntu 12.04, Python 2.7.3, gunicorn). If I use pip-installed pyparsing (version 1.5.7), everything works. If I use the Ubuntu-provided python-pyparsing 1.5.2-2ubuntu1, this problem happens.\n. Okay, now I noticed that @mbogosian is using indentical packages in virtualenv, so this might not be the same thing.\nAnyway, here's why this now fails on pyparsing-1.5.2 but previously it worked. pyparsing 1.5.6 (changelog) includes the following change.\n\n\nEnhanced form of using the \"expr('name')\" style of results naming,\n  in lieu of calling setResultsName.  If name ends with an '*', then\n  this is equivalent to expr.setResultsName('name',listAllMatches=True).\n\n\nb776c6d42177c09e29fb191a2803300a79922608 makes use of this feature and changes args to args*. On earlier pyparsing versions the expression name will just include the asterisk and that's why token.call.args is empty on evaluator.py#L28 (token.call['args*'] seems to work).\n. ",
    "posita": "[EDIT: @mbogosian is now @posita.]\nI have been able to reproduce this problem on only one of two very similar setups with HEAD (8072890e0b as of this writing). The working architecture is OS X running python 2.7.6 from MacPorts. The nonworking architecture is Ubuntu 12.04 (running in Parallels on the same machine) running that OS's packaged python (2.7.3-0ubuntu2.2). Both are running from gunicorn installed into fresh virtualenvs (created and populated this morning) using the same package versions. Here is the test URL I'm using which works on one, but not the other:\nhttp://[ADDR:PORT]/render?from=-5min&target=stdev%28stats.statsd.processing_time,10%29\nFYI, the URL in the first comment (i.e., .../render?target=constantLine(123.456)) results in a ZeroDivisionError for me in environments that don't exhibit the TypeError behavior associated with this issue.\n. I can also confirm that git revert b776c6d42177c09e29fb191a2803300a79922608 (i.e., reverting commit b776c6d that allows kwargs) fixes the issue on my Ubuntu installation. I have not yet investigated further.\n. They have identical storage-schema.conf files, and I'm (currently) not pumping anything to them (I'm just going of of whatever is produced by carbon and an \"idle\" statsd). I have just confirmed the following URL also works on 2.7.6, but fails on 2.7.3 with identical (fresh) configurations:\nhttp://[ADDR:PORT]/render?from=-5min&target=stdev%28carbon.agents.*.avgUpdateTime,10%29\nIf I back out commit b776c6d on 2.7.3, it works again.[1]\n[1] By the way, I'm now doing this with git revert b776c6d, which requires a commit to the local repository. I'm not sure git diff b776c6d\\^ b776c6d | patch -R -p1 (or similar) will work anymore, since the file has been hacked on a bit since that commit.\n. @miikka, nice find!\nI _thought_ I was installing pyparsing (as a dependency) via virtualenv and pip, but you're right. On the nonworking version, it's picking up the Ubuntu version (1.5.2). On the working version, I have:\n% [path_to_virtualenv]/bin/python -c 'import pyparsing ; print(pyparsing.__version__)'\n2.0.1\nBut on the non-working version, I have:\n% [path_to_virtualenv]/bin/python -c 'import pyparsing ; print(pyparsing.__version__)'\n1.5.2\n% [path_to_virtualenv]/bin/python -c 'import pyparsing ; print(pyparsing.__file__)'\n/usr/lib/python2.7/dist-packages/pyparsing.pyc\n_Whoops!_\nSo I guess this has become a dependency specification issue?\n. I don't think there's much to be done. The requirements.txt file already specifies pyparsing==1.5.7. The only other thing I can think of is the following patch to setup.cfg:\n```\ndiff --git a/setup.cfg b/setup.cfg\nindex e8976a4..b17c8b6 100644\n--- a/setup.cfg\n+++ b/setup.cfg\n@@ -12,7 +12,7 @@ requires = Django => 1.4\n            python-sqlite2\n            python-hashlib\n            pytz\n-           pyparsing\n+           pyparsing => 1.5.7\npost-install = distro/redhat/misc/postinstall\n```\n. ",
    "pabigot": "FWIW, I worked around this on Ubuntu 12.04 with:\nsudo apt-get remove python-pyparsing\npip install pyparsing\nwhich installed pyparsing 2.0.2 which apparently works fine with Python 2.7 and graphite-web HEAD.  (python-matplotlib got removed along with py-parsing, and its pip version requires additional upgrades, but that's a battle for another day.)\nIt'd help if check-dependencies had noticed I was using pyparsing 1.5.2.\n. @obfuscurity sorry, not without more effort than it's worth to me.  The main point is, if somebody wants to really fix #552, don't assume the file system encoding is ascii.\n. @brutasse @obfuscurity Rebased and requested change applied.\n. @obfuscurity no, I don't use that version.  Should be easy enough to cherry-pick though.\n. For what it's worth, that didn't replicate the problem, which would have resulted in a traceback failing in an rrdtool call as shown in #552.  What you've got just shows a different graphite unicode issue.\nIf you want to fix those, I suggest using six to support both python2 and python3.  It has features that make managing the distinction between text and data much easier.\n. @deniszh If you can't reproduce it, either it's not present in 0.9.x or you're not successfully executing a code path that includes a call to rrdtool.  #552 is not an Unicode error: it's an error where a filename that happens to be stored in a unicode string is passed to a function that doesn't accept unicode.  Could be that in 0.9.x self.fs_path is an str instead of unicode, at least for some code paths.\n. @deniszh If you can't reproduce it, either it's not present in 0.9.x or you're not successfully executing a code path that includes a call to rrdtool.  #552 is not an Unicode error: it's an error where a filename that happens to be stored in a unicode string is passed to a function that doesn't accept unicode.  Could be that in 0.9.x self.fs_path is an str instead of unicode, at least for some code paths.\n. @deniszh Did you install the Python rrdtool bindings in a location where graphite finds them, and verify that rrdtool support is enabled at runtime?  Beyond that I don't know what the problem might be; I did rewrite most of the code that detects which RRD databases and datasets are visible, but that was after it worked in the base installation.\n. @obfuscurity no, you have not reproduced the rrdtool problem.  Per my previous comment you've simply discovered an unrelated unicode problem.  The problem in #552 does not require that non-ASCII characters appear in the filename, merely that the filename be a unicode rather than a str object.\n. ",
    "davcamer": "@drawks I was surprised too that python's array wasn't faster. To be clear I'm talking about this: http://docs.python.org/2/library/array.html\nLooking around afterwards to try to understand the difference, I found people describing array as more memory efficient, but less processor efficient than lists. For many cases. \nhttp://stackoverflow.com/questions/176011/python-list-vs-array-when-to-use\nAlso, because array doesn't handle \"None\", the implementation was more complex. \nUnfortunately I didn't save the timing. It was much worse. I don't think I even saved the code. :(\nI keep imagining I'll try an implementation with NumPy, but haven't done that yet.\n. I saw this lately too. Worse, it seems to happen for keys that do exist if the first datapoint is None. \n. ",
    "zstyblik": "Could somebody give me any hints how possibly to fix this? We've hit the same problem with tip of master and it's rather annoying bug, to be honest.\nMetric in question is set with aggregation method last and how this did happen is there is a hole with no data at all. Graph looked just fine until new data were written. Then average function has been applied to part of data on the left side of the hole and it seems like correct function is being applied to the data on the right side of the hole.\nThanks for any hints!\n. Here's broken graph.\n\n. local_settings.py ~ TIME_ZONE = 'Etc/UTC' ?\n. With my current knowledge, either one works for me. Should having manage.py or any equivalent mean code duplication, I vote for the former - document django-admin.py.\n. ",
    "bvoss": "+1\n. +1\n. +1 really useful in our HA scenario\n. +1\n. ",
    "ollihoo": "+1\n. +1\n. +1 I'd also like to get this feature. It would help to fix some problems...\n. +1\n. +1\n. +1 great. Exactly what I need\n. ",
    "daccle": "@obfuscurity Hi Jason! Do you had the time to have a look on this one? \n. @obfuscurity  Hi Jason! Who is familiar with this code? \n. @obfuscurity thx @drawks Hi Dave! What do you think about my changes?\n. @drawks @obfuscurity any comments?\n. +1 for that\nAllthough it can be just configured in local_settings.py to avoid the problem, this should be changed in the upstream.\n. well, maybe there is potential, but from my perspective graphite-web needs to be stabilized before one should add extra complexity\n. +1 missed that when working on the spec file\n. ",
    "abarre": "I find it inconsistent as well. Is there a way to transform the otherKey.that.does.not.exist to a null series?\n. ",
    "tereska": "I have the same thing when metric1.?.cnt.avg and metric2.?.cnt200.avg does not exists\ndivideSeries(summarize(sumSeries(metric1.?.cnt.avg), \"1h\", \"sum\"),summarize(sumSeries(metric2.?.cnt200.avg), \"1h\", \"sum\"))\nTraceback (most recent call last):\n  File \"/local/lib/python2.7/site-packages/django/core/handlers/base.py\", line 111, in get_response\n    response = callback(request, _callback_args, _callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 106, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 10, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 21, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 29, in evaluateTokens\n    return func(requestContext, args)\n  File \"/opt/graphite/webapp/graphite/render/functions.py\", line 416, in divideSeries\n    raise ValueError(\"divideSeries second argument must reference exactly 1 series\")\nValueError: divideSeries second argument must reference exactly 1 series\n. ",
    "nharkins": "I'd also prefer the \"No Data\" img to the stacktrace (html)\nin this case.\nIn Firefox OSX v28.0, the browser tab spins infinitely\nwhen an html table's cell's BACKGROUND attribute\nis a graphite render url that returns the html stacktrace.\n. anything more i can do to help get this merged?\n. it is a timestamp, e.g. any value valid to the &from= arg\nas per http://graphite.readthedocs.org/en/latest/render_api.html\n\"ABSOLUTE_TIME is in the format HH:MM_YYMMDD, YYYYMMDD, MM/DD/YY, or any other at(1)-compatible time format.\"\nit seems that now an underscore is often used: HH:MI_YYYYMODD\nbut i seem to recall that was without it in the docs a while back, and\nthe old style still seems to work in my instance (backwards compat?)\ni found that on the docs somewhere. looks like \nHH::MI_YYYYMODD is preferred now.\n. thanks! :)\n. ",
    "jpemberthy": "did you guys find out how to avoid this issue?\n. ",
    "chundongwang": "If you'd like a work around, here's mine.\nI added a series full of zero to the possibly non-data series before sending it to divideSeries(). But constantLine() doesn't work as its frequency is too low. So I make two identity() of \"The.time.series\" and diffSeries() them to get a constant zero line with a lot of data points. And the perf of generating it should be fast as it doesn't have to pull anything from data source.\nSo at the end, it'd look like,\ndivideSeries(<your.data.series>, sumSeries(<your.nullable.data.series>, diffSeries(identity(\"The.time.series\"),identity(\"The.time.series\"))))\nI know it's long but if you're generating it dynamically this wouldn't bother too much.\n. ",
    "deepakkamesh": "+1 this would be a good feature.\n. ",
    "vasyas": "Any ideas to workaround the issue?. ",
    "ypomortsev": "Any updates on this? Should I resubmit this pull request as a patch against the master branch?\n. ",
    "grosskur": "Just wanted to say this looks awesome. In my setup I want all logging to go to stderr, and with this change it's now possible by defining LOGGING appropriately.\n. ",
    "fdChasm": "I didn't find the tools page until after I made my first comments on IRC. Splitting the tools and APIs into two seperate pages sounds good. I'll probably be able to submit an updated pull request in a day or so.\n. @SEJeff Updated.\n. I take it that having the revert and merge from parent in the history made it cleaner to do a manual merge?\n. I'll do that next time. Thanks.\n. I'm not very familiar with RST either. I was just looking at how it worked other places in the page and copying them. It does appear to compile to html as expected though;\nhtml\n<p>There are also  <a href=\"tools.html\" class=\"reference internal\"><em>client daemons and APIs</em></a> which can be used to send data to Carbon.</p>\n. ",
    "bcoughlan": "Is there another place to find these docs? The file was created in 2011, how have people set up Graphite without them?\n. ",
    "dangarthwaite": "Metric browing (auto completion) wasn't working for me on a new install.  I fixed it with this change.\n. ",
    "jssjr": "Great patch! It works, but causes undesirable side-effects with other pattern matching queries. For example:\ncollectd.metrics01*_domain_*.cpu-*.cpu-{user,system,softirq,steal,interrupt,nice,wait,idle}.value\nwill return the set reorder'ed instead of as specified in the match list. I also believe this affects the sortByMinima, sortByMaxima functions, but don't have hard evidence yet.\n. @drawks Sounds reasonable.\n. This probably won't work in a clustered environment where the order of series returned can vary.\n. I'll take the work to get carbon 0.9.13 shipped. I'm going to focus on the existing issues in https://github.com/graphite-project/carbon/milestones/0.9.13 in order to cut an RC release. Once we have an RC I want to make sure we have repeatable and testable packaging for redhat/debian based distros. If time permits, I'd also like to start doing some more in-depth profiling on carbon to find and fix performance issues in hot code paths.\n. :+1: to merging this.\n. The function tests need to be backported to 0.9.x before we can add in a test for this. I also feel like its unnecessary considering that we're just calling sort/cmp.\n. :+1:\n@obfuscurity - How do you feel about adding this to 0.9.13?\n. @penpen - Can you look at the whisper file that is returning the incorrect data and verify the aggregationMethod is correctly set to average? Also, please read the docs for scaleToSeconds function.\nThis patch, as is, does not make sense to me either and drastically changes performance and behavior. If I'm misunderstanding something, can you provide a test case demonstrating the problem?\n. > 0.1 as avg value for 10s interval is 10, but for 60s interval it should be 0.0166.., not 0.1).\nThis is the part that doesn't make sense. If you're pushing a per-second instantaneous gauge value into carbon every 10 seconds, then the unit of the datapoint recorded every 10 seconds is per-second, not per 10 seconds. And after 6 of these are collected, the 60 second aggregation using the average aggregation strategy is also per-second and shouldn't require any manipulation.\nTo continue with the example you provided, if you push '0.1 things/second' to graphite every 10 seconds, the datapoint at the 10s intervals is 0.1. If you average these values over 60 seconds (0.1+0.1+0.1+0.1+0.1+0.1)/6 = 0.1 you will store '0.1 things/second' as the datapoint in your 60s archive also.\nThis is the expected (and correct) behavior.\n. Will someone please explain why memcached caching should be disabled on clustered graphite with replication factor >= 2?\n. :+1: \nLike @tmm1 mentioned, this PR is useful to our organization. We store graph target strings externally and render them with graphite-web. This allows us to store common graph targets for things like disk utilization and substitute host names or device names back into the parameter string before rendering.\nI suspect there are other use cases as well. Would other organizations find this change valuable too?\n. :+1: \n. :+1: \n. > statsd simply does not work for percentiles or counts because you can't aggregate percentiles and I mostly care about latency distribution for my whole system.\nThis statement is not correct. A statsd-compatible aggregator can generate percentiles exactly as intended. The time-period (usually 10s) is defined ahead of time and since the daemon observes all data points submitted in that range it is capable of reporting percentile measurements back to graphite.\nWhisper files with multiple retention periods aren't well suited for storing percentile data, because averaging measurements taken over the less granular timespan will result in a loss of accuracy. However a single retention period (e.g. 10s for 30d) in a whisper file with no aggregation does not have this problem.\nThe fact that we cannot aggregate percentiles in a way that is mathematically sound is a major reason why statsd aggregators should be deployed per-site and not per-host. Modern computers are really fast and statsd traffic is minimal. Send all this data to a central statsd daemon (brubeck is designed for exactly this scenario) and you will get proper percentile data.\nIf your deployment cannot properly handle percentile measurements, then it's due to design flaws, not problems with Graphite, whisper, or statsd.\nHowever...\nI have yet to see a time series deployment where the mathematical accuracy issues with aggregating percentile data matters in practice. You're looking at data across 10's of seconds, or minutes, or longer... If you require exact data, then a round-robin time series database is the wrong choice. The trend data after roll-up is still meaningful for all intents and purposes. Even with percentiles.\n. > > The fact that we cannot aggregate percentiles in a way that is mathematically sound is a major reason why statsd aggregators should be deployed per-site and not per-host.\n\nTo me this sounds like you're trying add some pretty serious design limitations.\nDoing any form of mathematical aggregation on a percentile and calling it a percentile in the end does not work. Period.\n\nI think we're in agreement. The math limits the design. Centralizing where your statistics aggregation happens is a solution.\nLike @Dieterbe suggested already, the other solution is teaching graphite a storage mechanism that is capable of storing histogram data effectively.\nI'm :-1: on the approach presented here. This still requires the user to specify bucket sizes on the data collection side, so I'm unclear what the benefit is for calculating the additional series in graphite-web as opposed to placing all of that logic at the same place where you collect the data. (Could be statsd, collectd, some-custom-agent-d, etc.) A PR to extend the data reported by statsd's histogram binning seems more appropriate. \n. @bluecmd @iksaif - We're pushing right at 3 million metrics per second through our aggregators, so I think this does scale for 99+% of this project's users. I'm concerned about adding complexity to graphite for such a small subset of people when (imo) more elegant solutions exist.\nAre you on IRC to discuss this? I'd love to understand the use-case better and brainstorm alternate ways of achieving the result's you're looking for. I'm jssjr on #freenode and my email should be easy to locate if you want to use that medium instead.\n. There must be a more python-y way to do this...\n. Since we're determining the local node's host info in the Store() init, we need to import the STORE object here. Is this OK, or should I be determining the host info earlier. (When parsing settings?)\n. Yep! This needs tests to verify behavior. Thanks for the reminder.\n. Yep. That is much cleaner.\n. ",
    "qz267": "+1\n. ",
    "offlinehacker": "Some ideas how this should be properly implemented would be welcome. I am\nbeginner in R, but i see huge potential with usage of different models for\nprediction.\nOn Mon, Oct 28, 2013 at 4:51 PM, lreed notifications@github.com wrote:\n\n++1 .\nI know a few folks have been looking at\nhttp://rpy.sourceforge.net/rpy2_documentation.html.\ncould be very interesting to have those hooks!\nI've been wanting to start using R with Graphite but have not made much\nprogress yet myself.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/issues/478#issuecomment-27223945\n.\n. \n",
    "paulcgt": "+1, could be some potential there. \n. +1\nI confirmed this is the behavior, even after saving the edited JSON.\n. That's interesting Fernando. What do you storage schema retention periods look like?\n. I've noticed interesting things myself - but each time there's been a good explanation. \nMost often the JSON output is difficult for my brain to parse, so I use format=csv and then view that in Excel... it helps to view datestamps and values from the two targets side-by-side in one worksheet.\n. @tbenk, when you changed your retention intervals you remembered to reset the whisper file resolution using whisper-resize.py, right? Just checking. ;)\n. ",
    "tsabirgaliev": "@brutasse, I updated the PR with your suggestions, tested on my system -- works as expected.\n. ",
    "pschultz": "Could you rebase this please?\n. ",
    "prfalken": "I'm closing this one to reopen it from another branch (was on master)\n. Actually I also hit this bug, which is why I submit this patch.\nIt currently works on my graphite platform, with the ip_nonlocal_bind flag set to 1.\nAnother good part is that it removes the loop over all ports to check if the IP address is local or not.\n. Sure, is it OK that way ?\n. Hello, I hope this test is enough. I also rebased it against master.\n. Is there any chance you will make separate cactiStyle() be aware of each other ? \nThird party dashboards allow users to create separate targets, which make them impossible to tabulate correctly unless you add a single big function in one field (i.e. Gdash, Leonardo ...)\n. Wouldn't it be a nice to have ? For instance when a globbed metric is expanded, there is no way to know which color was picked by graphite on each metric. So I don't see any way for third parties to follow them.\n. OK at first I thought it would include more properties than just colors, so this may not be a real issue.\nThe color problem is still there, as you need to maintain two sets of themes if you want to support both graphite PNG and third parties. But I agree it's not a big deal so I will close this.\n. ",
    "pyr": ":metal:\n. I'm +1 on a standard layout, as long as it preserves the ability for old graphite users to still keep things where they are used to seeing them (the install-option trick looks like a nice way to do that).\n. :+1:\nOn Jul 30, 2014 12:01 PM, \"Bruno Reni\u00e9\" notifications@github.com wrote:\n\n\nYou can merge this Pull Request by running\ngit pull https://github.com/brutasse/graphite-web fix/remove-cli\nOr view, comment on, or merge it at:\nhttps://github.com/graphite-project/graphite-web/pull/800\nCommit Summary\n- Fix #796 -- remove the cli app completely\nFile Changes\n- M docs/releases/0_10_0.rst\n  https://github.com/graphite-project/graphite-web/pull/800/files#diff-0\n  (2)\n- M setup.py\n  https://github.com/graphite-project/graphite-web/pull/800/files#diff-1\n  (1)\n- M webapp/content/js/browser.js\n  https://github.com/graphite-project/graphite-web/pull/800/files#diff-2\n  (72)\n- M webapp/graphite/app_settings.py\n  https://github.com/graphite-project/graphite-web/pull/800/files#diff-3\n  (1)\n- D webapp/graphite/cli/init.py\n  https://github.com/graphite-project/graphite-web/pull/800/files#diff-4\n  (0)\n- D webapp/graphite/cli/commands.py\n  https://github.com/graphite-project/graphite-web/pull/800/files#diff-5\n  (507)\n- D webapp/graphite/cli/completer.py\n  https://github.com/graphite-project/graphite-web/pull/800/files#diff-6\n  (51)\n- D webapp/graphite/cli/parser.py\n  https://github.com/graphite-project/graphite-web/pull/800/files#diff-7\n  (140)\n- D webapp/graphite/cli/urls.py\n  https://github.com/graphite-project/graphite-web/pull/800/files#diff-8\n  (21)\n- D webapp/graphite/cli/views.py\n  https://github.com/graphite-project/graphite-web/pull/800/files#diff-9\n  (92)\n- D webapp/graphite/templates/cli.html\n  https://github.com/graphite-project/graphite-web/pull/800/files#diff-10\n  (143)\n- M webapp/graphite/urls.py\n  https://github.com/graphite-project/graphite-web/pull/800/files#diff-11\n  (1)\nPatch Links:\n- https://github.com/graphite-project/graphite-web/pull/800.patch\n- https://github.com/graphite-project/graphite-web/pull/800.diff\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/800.\n. \n",
    "softwareklinic": "&target=verticalLine(\"12:3420131108\",\"event\",\"blue\")\nHow did you create this number - and what does it represent   \"12:3420131108\"\n. Thank you.\nRegards,\nKeyur\nOn Oct 20, 2015, at 5:39 PM, Neil Harkins notifications@github.com<mailto:notifications@github.com> wrote:\nit is a timestamp, e.g. any value valid to the &from= arg\nas per http://graphite.readthedocs.org/en/latest/render_api.html\n\"ABSOLUTE_TIME is in the format HH:MM_YYMMDD, YYYYMMDD, MM/DD/YY, or any other at(1)-compatible time format.\"\nit seems that now an underscore is often used: HH:MI_YYYYMODD\nbut i seem to recall that was without it in the docs a while back, and\nthe old style still seems to work in my instance (backwards compat?)\ni found that on the docs somewhere. looks like\nHH::MI_YYYYMODD is preferred now.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/493#issuecomment-149722765.\n. ",
    "lakshmi-kannan": "Sorry. Didn't intend on a PR to graphite project.\n. @esc: Sorry about the regression! I see the problem now. Not too happy with the inconsistencies :/\n. ",
    "justino": "I should note that this patch was built against the 0.9.x branch\n. Don't do anything with this PR.\nI want to combine this with #542 for an \"aggregateLine\" function for min, max, avg\n. Screenshot\n\n. I have no idea what the Master is for, so I don't touch it. I also don't have a working installation of Master so, at least right now, I wouldn't be able to test it properly\n. Ok, figured out what I needed to do to test against Master. PR #498 is against Master\n. This is for the master branch. The 0.9.x branch has it's own PR #503 \n. \n. \n. This is correct. Wildcard metric are not taken into consideration.\nI put this in so that I could order individual (as the composer window shows) for stacked graphs, which was the only time I needed this ability.\nIt got frustrating having to always build a graph in a specific order in order to have the stacks look reasonable.\n. Yes, it simply moves them within the legend as a block, the block itself may or may not be rearranged (non-deterministic), See the attached images:\n\n\n. This is for the 0.9.x branch. The master branch has it's own PR #502 \n. I want to combine this with #496 , turn it into an \"aggregateLine\"  function for min, max, avg.\nRight now they don't quite work the same, but that's the fault of #496 and is easily rectifiable \n. :+1:\n. ",
    "gorsuch": "This is excite.\n. Amazing!\n. ",
    "duclet": "So I see this is merged into master all the way from 2013 but it was not merged in the 0.9.x branch which seems like the releases are tagged from. As such, this is still not working on the current version.\n. If you are not going to release it anymore, then it is fine. I've been trying to setup a graph that needs this feature and was surprised that a fix was merged into master in 2013 and with the current latest version didn't have the fix (don't see any other version besides the 0.9.x). If there will be a new version based on master soon, then I can wait. Thanks.\n. ",
    "jkandula": "Just want to find ouf if this has this been part of any release? Thanks\n. ",
    "tamale": "Fix and pull request submitted:  https://github.com/graphite-project/graphite-web/pull/514\n. ",
    "zeha": "Would be great if this could become part of a release sooner than later :)\n. Also seeing this.\n. ",
    "petegallagher": "My first attempt used the same timestamp technique, but I felt it would be better to have the option to return a base64 encoded version so no trick is required.\n. ",
    "jcsp": "It looks like they're not quite equivalent -- #349 is using get_script_prefix() as the root, which works when using apache to put graphite somewhere other than root, but not when exposing graphite views from a custom url prefix inside another django project (my use case).  That said, it probably makes sense for me to rebase this PR onto #349 as it's really a special case of that.\n. @brutasse thanks, fixed\n. ",
    "kamaradclimber": "we have tested it only against 0.9.x branch, but you can also merge it against master if you want.\n. @obfuscurity  the master branch is so different from 0.9.x that I don't feel confident to offer a patch for master. We heavily use 0.9.x however so this patch targets it first.\n. noCache would help but the dashboard using realtime will be consulted simultaneously by several users, having a cache will be a great benefit.\n. I have added another fix regarding cache.\n. I have not taken the time to write tests for this PR, I am sorry.\nI close this PR, feel free to reopen it if you want to.\n. I'll split the PR in half no pb\n. done as two different commits in this PR. Both have been tested\n. @obfuscurity done\n@drawks I've removed the commit with the test since more evolved tests have been introduced in the meantime.\n. thanks!\n. ",
    "jburnham": "Should this not also be applied to the same class in carbon.util as the docs mention? Thanks for the fix, I just discovered this issue when rolling out a new version to our systems.\n. ",
    "elementalvoid": "Additionally in 0.9.12 you must import the sys module otherwise util.py fails at line 101:\npython\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 231, in fetchData\n    cachedResults = CarbonLink.query(dbFile.real_metric)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 140, in query\n    results = self.send_request(request)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 166, in send_request\n    result = self.recv_response(conn)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 181, in recv_response\n    return unpickle.loads(body)\n  File \"/opt/graphite/webapp/graphite/util.py\", line 110, in loads\n    return pickle_obj.load()\n  File \"/opt/graphite/webapp/graphite/util.py\", line 101, in find_class\n    mod = sys.modules[module]\nNameError: global name 'sys' is not defined\n. ",
    "edasque": "Is it really planned for summer of 2015?\n. ",
    "steveakers": "I've messed around with this all day and think I have it as fast as I can given I'm no python expert. It's likely usable in most cases. However, if I could use a cython implementation it would be lightening fast even for days worth of 10sec data. Is this approach something you would consider if I submitted a PR?\n. BTW, I have the cython implementation and it works very well. Just let me know how you'd like me to proceed.\n. Yeah, I had to add it to the dependencies to make it work. I'll remove it and go back to my original version. The only other route would be a patch users could apply after installing graphite.\n. I like that idea! That\u2019s what I\u2019ll do.\nOn December 18, 2013 at 9:08:13 AM, Jeff Schroeder (notifications@github.com) wrote:\nOr make it optional and use cython if available, otherwise fallback to the pure python version.\nText by Jeff, typos by iPhone\n\nOn Dec 18, 2013, at 7:06, Steve Akers notifications@github.com wrote:\nYeah, I had to add it to the dependencies to make it work. I'll remove it and go back to my original version. The only other route would be a patch users could apply after installing graphite.\n\u2014\nReply to this email directly or view it on GitHub.\n\u2014\nReply to this email directly or view it on GitHub.\n. Just pushed a change to address all concerns.\n1) made cylowess optional\n2) check for cylowess during lowess execution\n    -- if found run the cython version which is super fast\n    -- if NOT found run the pure python which is fast enough at normal data set sizes\n3) made improvements to pure python version to speed things up\n    -- modified the way the delta is calculated\n    -- skip data for larger data sets (the larger the data set the bigger the skips)\n\nThis results in the following for 7200 data points:\n-- pure python with no skips was 9+ seconds (from memory and didn't verify)\n-- improved pure python was 4.85 seconds (most users won't smooth that much - if they do 4.58 is reasonable)\n-- cython was 0.25 seconds\nAs you can see cython is the way to go for this particular function, but by making it option and improving pure python it's very usable for users who don't care to install cylowess.\n. Just pushed a change to address all concerns.\n1) made cylowess optional\n2) check for cylowess during lowess execution\n-- if found run the cython version which is super fast\n-- if NOT found run the pure python which is fast enough at normal data set sizes\n3) made improvements to pure python version to speed things up\n-- modified the way the delta is calculated\n-- skip data for larger data sets (the larger the data set the bigger the skips)\nThis results in the following for 7200 data points:\n-- pure python with no skips was 9+ seconds (from memory and didn't verify)\n-- improved pure python was 4.85 seconds (most users won't smooth that much - if they do 4.58 is reasonable)\n-- cython was 0.25 seconds\nAs you can see cython is the way to go for this particular function, but by making it option and improving pure python it's very usable for users who don't care to install cylowess.\n. Doh! I was just thinking that this morning. I definitely want to be fancy. :) I\u2019ll try to get those in today.\nOn December 19, 2013 at 11:53:48 PM, Jeff Schroeder (notifications@github.com) wrote:\nAwesome stuff! If you want to be super fancy, could you add tests for this to:\nhttps://github.com/graphite-project/graphite-web/tree/master/webapp/tests somewhere?\n\u2014\nReply to this email directly or view it on GitHub.\n. I understand. I'm looking forward to the pluggable functions feature!\n. We are currently using it to create control charts for seasonal data such as number of logins or number of payments processed. Basically, we subtract today's actuals from last weeks smoothed data to create a delta that in theory should hover around zero as week over week growth should be quite small. By smoothing the previous week we also eliminate its variance so that the resulting delta only contains today's variance. This is then plotted against limits calculated as +/- 3 standard deviations from the mean. It could be in these cases that linear regression would produce a similar result, but in general locally weighted scatterplot smoothing is designed to address nonlinear relationships where linear methods do not perform well.\n. ",
    "avnivamsikrishna": "@esc  I fixed it by changing the render views function in the graphite webapp\n. @esc  Feel Free to correct me, \ndiff -r webapp/graphite/render/views.py webapp2/graphite/render/views.py\ntimestamps = range(series.start, series.end, series.step)\nchange: \ntimestamps = range(int(series.start), int(series.end), int(series.step))\ndiff -r webapp/graphite/render/functions.py webapp2/graphite/render/functions.py\nstart = timestamp( requestContext['startTime'] )\n   end = timestamp( requestContext['endTime'] )\n   step = (end - start) / 1.0\n   series = TimeSeries(str(value), start, end, step, [value, value])\nchange:\nstart = int(timestamp( requestContext[\"startTime\"] )/10)*10\n   end = int(timestamp( requestContext[\"endTime\"])/10)*10\n   step = (end -start -1)/1.0\n   step = 10\n   val_tmp = [value] * int((end-start)/10)\n   series = TimeSeries(str(value), start, end, step,val_tmp)\nAfter this change; graphite returns a series of points and not just one. This is useful when we use third party graphing options such as grafana or giraffe or Highcharts.\n. ",
    "nickryand": "I hadn't thought about that actually but I will make it so..\n. I actually thought about this while implementing this function. I am going to re-factor slightly to allow for passing a function instead of having it hard coded to provide the max value. I'll open a new pull request with that functionality.\n. See https://github.com/graphite-project/graphite-web/pull/549\n. @justino how does this look?\n. Hows this look?\n. ",
    "richadlr": "Hi, is there a chance for this to be released in an office 0.9.x version, and by when?\n. ",
    "legionus": "For example we have following metrics:\ntesting.example1.eth\ntesting.example2.eth\nservices.foobar.xxx\nNow we query them:\nQuery http://example.com/metrics/expand?query=testing.** will produce {\"results\": [\"testing\", \"testing.example1\", \"testing.example2\" \"testing.example1.eth\", \"testing.example2.eth\"]}\nAnd http://example.com/metrics/expand?query=** will produce a list of metrics from all cluster nodes.\nFor now it works only with ceres.\n. You change it, but you don't change the function get_request_locks() itself. Following patch will solve the problem:\n@@ -246,7 +237,7 @@ class RemoteReader(object):\n     self.cache_lock.acquire()\n     try:\n       if url not in self.request_locks:\n-        self.request_locks[url] = (Lock(), Lock(), Event())\n+        self.request_locks[url] = (Lock(), Event())\n         self.request_times[url] = time.time()\n       return self.request_locks[url]\n     finally:\n. ",
    "jregovic": "Just fixed this on ours today.\nI'll work on a patch, but you need to fix readers.py and re-encode the fs_path.\nBelow is an excerpt for the RRDReader class\nclass RRDReader:\n  supported = bool(rrdtool)\ndef init(self, fs_path, datasource_name):\nadd this\nif isinstance(fs_path,unicode) :\n    fs_path = fs_path.decode('utf8').encode('ascii')\n### DONE\n....\n  def get_datasources(fs_path):\nadd this\nif isinstance(fs_path,unicode) :\n    fs_path = fs_path.decode('utf8').encode('ascii')\n### DONE\ninfo = rrdtool.info(fs_path)\n...\n  def get_retention(fs_path):\nadd this\nif isinstance(fs_path,unicode) :\n    fs_path = fs_path.decode('utf8').encode('ascii')\n### DONE\ninfo = rrdtool.info(fs_path)\n\n. ",
    "mrmanc": "This deserves a :+1: \n. ",
    "slaupster": "Is there any plan to pull this back to .9x ?  We are happily using events at the moment but patch this into our 0.9.x to get them to work so would be grateful if this could be done!\n. thanks very much for the speedy turn around!\n. +1 on this please.\n. ",
    "justenwalker": "Updates to groupByNodeMetric\nAfter thinking about the results of groupByMultiNode, I'm not happy about how\nthe resulting series is named - it loses a bunch of information that\ncould be used in subsequent function calls.\nPreviously, the resulting series list was akin to:\nalias(asPercent(servers.server1.value1,servers.server1.total),'server1')\nalias(asPercent(servers.server1.value1,servers.server2.total),'server2')\n...\nalias(asPercent(servers.serverN.value1,servers.serverN.total),'serverN')\nResulting in a seriesList of:\nserver1, server2,...serverN\nWith this change, the results are now:\nservers.server1.asPercent.value1.total,\nservers.server2.asPercent.value1.total\n...\nservers.serverN.asPercent.value1.total\nSummary of Changes:\n\nRename to groupByMultiNode\nAlias the resulting series so that it is easier to submit to other\n  functions\nAdd tests for series aliasing\n. @obfuscurity I'm not sure how to write an accurate test for this function - as a lot of information is lost in the resulting series.  I can't test that each series is indeed wrapped by the callback; the best I can do is check for the naming convention - unless you can suggest a different approach?\n. @obfuscurity rebased on master\n. @drawks are you using a mocking framework in this project? I did a git grep mock to see if I could find an example but nothing comes up.\n. @obfuscurity I had an idea to make this method more generic and potentially make this just a map reduce. \n\nWe could split it into two methods:\n```\nmap(seriesList,groupNode)\n  => seriesLists (ie: [seriesList1,seriesList2,seriesList3])\nreduce(seriesLists,function,reduceNode,reduceMatchers*)\n   => callback(seriesList1),callback(seriesList2),callback(seriesList3)\n```\nExample usage:\nreduce(map(servers.production.*.df.*,2),\"asPercent\",4,\"used\",\"total\")\nThoughts?\n. Yeah i can do that.\n. Closed in favor of #578\n. Sorry about that - example given uses the wrong order of parameters. I've updated the example to use the correct order of parameters.\n@multilinear is correct in that map takes a list of series and just breaks them up into a list of seriesLists which can be operated on by reduce.  Also reduce is an alias for reduceSeries, same as map is an alias for mapSeries and sum is an alias for sumSeries.\nExample\nAssume we have metrics in the form:\n- servers.SERVERNAME.disk.bytes_used\n- servers.SERVERNAME.disk.total_bytes\nSERVERNAME is node 1 (one dot precedes it)\nbytes_used and total_bytes is node 3  (three dots precede it)\nmapSeries Example (not very useful on its own):\nmap(servers.*.disk.*,1) =>\n      [\n        servers.server1.disk.*,\n        servers.server2.disk.*,\n        ...\n        servers.serverN.disk.*\n      ]\nreduceSeries with mapSeries Example:\n```\n  reduceSeries(mapSeries(servers..disk.,1),\"asPercent\",3,\"bytes_used\",\"total_bytes\") =>\n    alias(asPercent(servers.server1.disk.bytes_used,servers.server1.disk.total_bytes),\"servers.server1.disk.reduce.asPercent\"),\n    alias(asPercent(servers.server2.disk.bytes_used,servers.server2.disk.total_bytes),\"servers.server2.disk.reduce.asPercent\"),\n    alias(asPercent(servers.server3.disk.bytes_used,servers.server3.disk.total_bytes),\"servers.server3.disk.reduce.asPercent\"),\n    ...\n    alias(asPercent(servers.serverN.disk.bytes_used,servers.serverN.disk.total_bytes),\"servers.serverN.disk.reduce.asPercent\")\n```\n. Implying rename metricsNode to metricNode? I can do this and modify the doc - seems like an improvement.\n. yeah, i forgot the colon when i tested it.\n. ",
    "pkonar": "This is on a Virtual machine running Centos 6.4. \n. ",
    "jippi": ":+1: ditto\n. I experience the same, my box is also idle without any disk io or activity worth anything. It's running on RAID 10 SSD with 16G of memory and 8 core Xeon 5\n\nIn my experience this happens randomly, and a simple refresh of the same request (with same headers etc) yield the correct result\nThe png response always says 'no data' while the json response have plenty of data - so wonder if it's a read / exception issue somewhere \nThey error usually happens at once in grafana, it's very rarely just one request that fails, but all of them at the same time. Could it be something with request/response workers or uwsgi throttle timeout that causes some default response to be sent to the client?\n. ~~I use gunicorn too :)~~\nI actually use uwsgi on the graphite server\n```\n-> uwsgi --version\n1.2.3-debian\n-> python --version\nPython 2.7.3\n-> nginx -v\nnginx version: nginx/1.4.4\n-> lsb_release -a\nNo LSB modules are available.\nDistributor ID: Debian\nDescription:    Debian GNU/Linux 7.2 (wheezy)\nRelease:    7.2\nCodename:   wheezy\n-> cat /etc/nginx/sites-enabled/graphite.conf\nserver {\n    listen 80 default_server;\n    server_name _;\n    rewrite ^ https://$host$request_uri? permanent;\n}\nHTTPS server\nserver {\n    listen 443 ssl spdy;\n    server_name _;\nroot /opt/graphite/webapp;\n\n# ssl\nssl on;\nssl_certificate /etc/nginx/ssl/STAR_bownty_net.crt;\nssl_certificate_key /etc/nginx/ssl/STAR_bownty_net.key;\nssl_session_timeout 5m;\nssl_protocols SSLv3 TLSv1;\nssl_ciphers ALL:!ADH:!EXPORT56:RC4+RSA:+HIGH:+MEDIUM:+LOW:+SSLv3:+EXP;\nssl_prefer_server_ciphers on;\n\n# Django admin media.\nlocation /media/admin/ {\n        alias /usr/share/pyshared/django/contrib/admin/static/admin/;\n    }\n\n    # Your project's static media.\n    location /content/ {\n\n    }\n\n    # Finally, send all non-media requests to the Django server.\nlocation  / {\n    add_header Access-Control-Allow-Origin \"*\";\n    add_header Access-Control-Allow-Methods \"GET, OPTIONS, POST, HEAD, PUT, DELETE\";\n    add_header Access-Control-Allow-Headers \"X-Requested-With, X-Requested-By, Origin, Authorization, Accept, Content-Type, Pragma\";\n    add_header Access-Control-Allow-Credentials \"true\";\n\n    include uwsgi_params;\n    uwsgi_pass unix:/run/uwsgi/app/graphite/socket;\n}\n\n}\n-> cat /etc/uwsgi/apps-enabled/graphite.ini\n[uwsgi]\nplugins = python\nchdir = /opt/graphite/conf\nmodule = wsgi:application\nuid = root\ngid = root\nchmod-socket = 777\n```\n. :+1: \n. ",
    "abhinav-upadhyay": "Is there any progress on this bug? For me this behavior doesn't seem to be arbitrary, it is happening for all the metrics, all the time.\n. The cpu idle is between 60-80% and iowait is mostly 0-5%.\n. ",
    "rodriguezsergio": "I'm also running graphite on Apache and the addition to local_settings.py seems to have done the trick for me.\n. Closing this for now. Switched from runit to init and now I can't reproduce this.\n. ",
    "rmca": "I'm an employee of Hosted Graphite. Some of our customers were affected by this issue, so we spent some time fixing it. We now have this fix in production for our customers. Additionally we have a pull request open with our fix.\n@brutasse was correct, the hashRequest function was always choosing to hash the GET parameters, which means that requests of other method types would hash to the same cache key.\n. For those that are interested here's some code to reproduce this reliably: https://gist.github.com/rmca/5fc7feb596d7b4e579ff\n. Good call. Updated.\n. Steve,\nI added a test in 7b20aef1. Let me know if it will do the trick. Also, it seems to me that the stripControlCharacters function could be removed, along with its usage. For instance in https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/hashing.py#L28-30 character stripping is performed, and the reason given in the comment is that memcached doesn't like them, but then the string gets md5 hashed anyway, so memcached will never see the control characters. Am I being mislead by that comment and there's another reason for this, or could that just be removed?\n. In 0b8188ba I removed stripControlChars. I also sorted the targets in hashData so that the same targets would hash to the same value regardless of order, for the same start and end time. Finally, I added a (small) test for hashData.\n. Ah cool. Sorry about that.\n. ",
    "sprugman": "Sorry about that. Thanks for the info.\n. ",
    "foozlevazquez": "Running graphite-web off the current master branch, on Ubuntu14.04, with Django 1.6.1, I am unable to run any management collectstatic command.\nWhat version of Django is expected?  Is an additional installation step missing?\nThanks!\n```\nUsage: django-admin subcommand [options] [args]\nOptions:\n  -v VERBOSITY, --verbosity=VERBOSITY\n                        Verbosity level; 0=minimal output, 1=normal output,\n                        2=verbose output, 3=very verbose output\n  --settings=SETTINGS   The Python path to a settings module, e.g.\n                        \"myproject.settings.main\". If this isn't provided, the\n                        DJANGO_SETTINGS_MODULE environment variable will be\n                        used.\n  --pythonpath=PYTHONPATH\n                        A directory to add to the Python path, e.g.\n                        \"/home/djangoprojects/myproject\".\n  --traceback           Raise on exception\n  --version             show program's version number and exit\n  -h, --help            show this help message and exit\nType 'django-admin help ' for help on a specific subcommand.\nAvailable subcommands:\n[django]\n    check\n    cleanup\n    compilemessages\n    createcachetable\n    dbshell\n    diffsettings\n    dumpdata\n    flush\n    inspectdb\n    loaddata\n    makemessages\n    runfcgi\n    runserver\n    shell\n    sql\n    sqlall\n    sqlclear\n    sqlcustom\n    sqldropindexes\n    sqlflush\n    sqlindexes\n    sqlinitialdata\n    sqlsequencereset\n    startapp\n    startproject\n    syncdb\n    test\n    testserver\n    validate\nroot@vagrant:~# django-admin --version\n1.6.1\n```\n. After some more knocking around, and seeing this: http://stackoverflow.com/questions/17804743/django-admin-py-unknown-command-collectstatic\nI realized I hadn't been running the collectstatic with the settings specified - silly me.\nSuccess followed:\n```\nroot@vagrant:~#   django-admin  collectstatic --settings=graphite.settings\nYou have requested to collect static files at the destination\nlocation as specified in your settings.\nThis will overwrite existing files!\nAre you sure you want to do this?\nType 'yes' to continue, or 'no' to cancel: yes\nCopying '/opt/graphite/webapp/content/img/calBt.gif'\n[...]\nCopying '/usr/lib/python2.7/dist-packages/django/contrib/admin/static/admin/css/changelists.css'\n445 static files copied.\nroot@vagrant:~# \n```\n. I was getting an error here that the cached_datapoints was a dict, so this code addressed the issue.  Perhaps this was due to some misconfiguration (of mine) upstream?\n. ",
    "jordant": "Same issue here with master.\n. ",
    "ramiro": "LGTM.\n. I see @brutasse has already commented on the named URLs topic. Sorry for the noise.\n. @brutasse  Thanks for the suggestion. PR updated.\n. Refs PR #580 that modernizes assets handling and unifies the URl they are published under.\n. Superfluous space inserted.\n. Superfluous space inserted.\n. (part 2 of 2)\nI'd suggest to use the named URL  feature of Django instead of specifying the path to the actual view in the url template tag.\nThe former is possibly more future-proof (there is a chance the latter gets deprecated at some point).\nAs graphite-web is still compatible with Django 1.4 this means {% load url from future %} needs to be used at the top of every template that uses such tag (https://docs.djangoproject.com/en/1.4/ref/templates/builtins/#url)\nThe modifications proposed in this PR work so perhaps the above suggestion can be implemented in a different commit once it lands.\n. (part 1 of 2)\nI'd suggest to use the named URL  feature of Django instead of specifying the path to the actual view in the url template tag.\nThe former is possibly more future-proof (there is a chance the latter gets deprecated at some point).\nAs graphite-web is still compatible with Django 1.4 this means {% load url from future %} needs to be used at the top of every template that uses such tag (https://docs.djangoproject.com/en/1.4/ref/templates/builtins/#url)\nThe modifications proposed in this PR work so perhaps the above suggestion can be implemented in a different commit once it lands.\n. Oops. I've just opened PR #648 to remove these Location blocks.\nPerhaps it's better to add these changes here?\n. ",
    "johnseekins": "Some updates:\n1) Permissions on the data structure are all correct.\n2) Index has been generated \n. An example of a local fetch on the box:\n```\n[root@graph05 cache-a]# curl -k \"https://localhost/render/?target=Metric1.*&format=raw\"\n\n\n\nGraphite encountered an unexpected error while handling your request.\nPlease contact your site administrator if the problem persists.\n\n\n\n\n\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.6/site-packages/django/core/handlers/base.py\", line 111, in get_response\n    response = callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 109, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 10, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 21, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 24, in evaluateTokens\n    return fetchData(requestContext, tokens.pathExpression)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 99, in fetchData\n    fetches = [(node, node.fetch(startTime, endTime)) for node in matching_nodes if node.is_leaf]\n  File \"/opt/graphite/webapp/graphite/storage.py\", line 54, in find\n    for node in request.get_results():\n  File \"/opt/graphite/webapp/graphite/remote_storage.py\", line 114, in get_results\n    node = BranchNode(node_info['path'])\nKeyError: 'path'\n\n\n\n\n```\n. Ah. Backends cannot communicate between current master and master from last September...Closing...\n. I don't see any issue with changing the except. I was just in a hurry at the time and built it that way so it would stop failing.\nI just can't remember what the actual exception is...\n. You need to be on django 1.4 (based on the release notes).\n. These aren't OpenTSDB-connected, but rather we connect directly to HBase with the python thrift libraries. Currently, we are handling ~7 million+ metrics per minute going in and it seems pretty stable.\nhttps://github.com/johnseekins/graphite-web\nhttps://github.com/johnseekins/carbon\nWe based this on JBooth's awesome work with https://github.com/jbooth/graphite-data. He laid the groundwork. We just cleaned things up a bit, added a few functions, moved things around. added network disconnect handling, etc.\nOf note, you do have to manually run the propagate function. But, since you're sending this to an HBase cluster, making a map/reduce job to propagate seems logical (and works quite well).\n. Example issues:\nhttps://github.com/graphite-project/graphite-web/issues/1541\nhttps://github.com/graphite-project/graphite-web/issues/1514\nhttps://github.com/graphite-project/graphite-web/issues/1509\n. So...obviously we just throw Django out for something like Bottle or Flask, right? \n:trollface: \n. Awesome solution. Thanks!\nI guess I'll just get started on that flask migration...\n. https://github.com/graphite-project/graphite-web/pull/1560 does this cleaner. Closing.\n. :+1: \n. ",
    "pythiannunez": "I've tested it and it's pretty cool! neat!\nI think I've stumbled upon some bugs, though. I'll open issues. But apart from that, way to go!!\n. ",
    "torkelo": "Thanks! \nYes, please open an issue for any problem you encounter. The project is still pretty early so bugs should be expected. But I think it has an good foundatio that can grow into a solid dashboard & graph composer. It will be interesting to evolve the target parser and function/metric editing. \n. Anyone have any feedback on this or idea why fz is not affecting the data shown but only the time labels? \n. @brutasse nice. will test it. \n. @obfuscurity seems to be working. This is not an issue for Grafana any more, as Grafana switched to using epochs in the from and until parmas\n. @brutasse nice! will take a look. \n. I would like to help any way I can. Mostly I would like to create a new home page for graphite on a domain like graphite.org (or similar), with updated install guides, links to read the docs, download links, links to graphite-api etc. And usage screenshots & diagrams to show of what graphite can do. \n. yay!\n. I think this feature sounds very useful, but feel that it belong to the /metric/find api as it is just another way of finding metric paths / nodes.\n. @obfuscurity its not something I can patch as I use the native json parsers built into browsers. \n. Grafana does not use any jsonp. Not sure about this, I think there are still some scenarios where cors might not be possible or feasible. So jsonp support could be good to have unless it adds a lot of complexity to graphite-webb. It would be great to know why graphitus is using jsonp\n. @obfuscurity Well without using CORS then there is only putting them on the same domain or jsonp.\nGrafana requires that graphite is setup with CORS headers or on the same domain. If you have CORS setup there is no need for jsonp (unless you need to support old browser like IE7 which has no CORS support)\n. @obfuscurity It looks like aliasByMetric is not extracting the metric name correctly. \nQuery: aliasByMetric(scaleToSeconds(statsd.fakesite.timers.ads_timer.upper_25,10))\nreturned target in json:\n[{\"target\": \"upper_25,10)\", \"datapoints\": [] }]\n. nice! \n. @mikhailov\nYou are mistaken, the performance in Grafana even over very large timespans can be as good over even better than Graphite server side rendered PNG. This was a critical thing that I made sure was solved even before I started working on Grafana last December. \nThe reason client side rendering for graphite has been severely crippled with slow performance and browser crashes for large timespans was solved last summer, via this issue https://github.com/graphite-project/graphite-web/pull/170   . The reason client side rendering was slow was not because client sider rendering was slow but because the graphite json api returned every datapoint to the browser (which could be many many megs of datapoints when viewing large time spans). The maxDataPoints feature make sure that the same consolidation that happens for the PNG rendering happen for the json rendering, that is that only points that are drawn in the graph are returned. \nI have mentioned this multiple times, in grafana docs, (and old grafana github wiki), and in issues. http://grafana.org/docs/performance/ \nGrafana can render 10-15 graphs with \"the last year\" of data in about the same time as the PNG :) \n. Strange, grafana should send the same query every time when auto refresh. The query is built only when you use the query editor. The error looks strange, given the query. It complains about the aliasByNode parameters must be integers, and they are, it is the summarize parameters that are strings. It almost seems like a threading or caching issue. \naliasByNode(summarize(myproject.prod...timers.app.view.myproject.api.views.MyProjectQuery.POST.mean_99, \"10min\", \"max\"), 2,3)\n. Well from my point of view the really important thing is to return series \"legend values\", like avg, sum/total ,min, max etc. The important one is \"Total\" (or sum) which cannot be calculated client side as the maxDataPoints parameter does point consolidation that averages points so the total calculated client side is wrong, there are other cases as well there Total cannot be calculated client side (depending on graphite function used). So that is the most important thing for me & Grafana. \nThe new return format would be nice, as it will eliminate the need to transform it into a format charting libs can handle but I do not think there will be any noticeable performance impact from this change. \n. @toni-moreno I am not sure we are on the same page. \nI do not want Ymin/Ymax for every consolidated point that would be a waste (I think). \nI am more after something like this: \njavascript\n[ \n  {\n    target: \"my.series.count\",\n    datapoints: [[1,2],...],\n    legendValues: {\n           max: 123,\n           min 10,\n           total: 21312312312,\n           avg: 50,\n           current: 20\n    }\n  }\n]\nThat is exactly as it is today but with legend Values calculated on the server and returned in as part of the json object. \nI am not sure why you want yMin & yMax for every point in the datapoints array.\n. Grafana always fetches new data ok zomm/date range changes. \nI see you point, but your idea of return format is only applicable for summarize function and would not be useful in other scenarios.\n. @obfuscurity I agree, I just named it legendValues so it would be clear what the values would mean, as they would be basically the same as those generated by the legendValues function. \n. Congrats, reaching #1000 !\n. Congrats, reaching #1000 !\n. Not sure either,  probably the node index out of bounds\n. interesting, not sure I understand exactly what it returns. Just summery data or time series? Example response?\n. :+1: \n. Interesting, what http api changes would this introduce? Also thought the plan was to remove the Django dependency post 1.0\n. No,  I think this is an improvement, if I recall correctly the Infinite value returned by graphite today cause invalid json errors . 9x slower json encoding sounds like a major issue. Was this due to the fix of non standard JSON Infinity values? That fix is not worth a 9x json encoding perf hit . Functions are grouped in the default graphite Web UI as well, which is where the grafana groups comes from :) . The function model looks solid to me, should be enough to do what we want in the query builder UI in Grafana. ",
    "yinyangxu": "Thanks.\n. ",
    "contentfree": "If it helps someone in the future: I had mod_python26 installed instead of mod_python27 and therefore the stuff I'd installed with pip (for 2.7) wasn't working (though I apparently had enough vestiges of previous attempts that graphite got to the point captured in this issue).\n. ",
    "josephmc5": "Thanks for the response. Definitely makes sense. I'm hoping we can upgrade to CentOS 6 soon. At the time the INSTALL said 2.4 minimum but it's since been updated to 2.5. I'm glad to see that.\n. ",
    "abooitt": "Indeed looks easy. Looking forward to the fix.\n. ",
    "kaathewise": "Maybe this regex is just obsolete, as long as this function is called from evaluateTokens, where tokenization has already taken place so why series.name should require additional processing?\n. ",
    "nzlosh": "Has there been any progress on this issue?\nWe encountered this bug when attempting to select specific hosts in the target.  Specifically host-?? or host-[0-9]* failed to be parsed correctly by aliasByNode.  We normally use host-* but sometimes this selection is too broad.\nWould a patch which includes - ? [ ]  characters in the regex pattern be accepted as an interim solution?\n. ",
    "dutchiechris": "nzlosh, I have the issue with : separator (same as kaathewise mentioned in the original posting) so I'd prefer a proper fix :-)\n. ",
    "jimbydamonk": "I just hit this today. Everything works fine when I don't use aliasbynode with graphite-web-0.9.12-5.el6.noarch. As soon as I try the alias it gets truncated. \n. I just hit this today. Everything works fine when I don't use aliasbynode with graphite-web-0.9.12-5.el6.noarch. As soon as I try the alias it gets truncated. \n. ",
    "w4tt": "Same here with an @ character in the metric name.\n. I've never developed in Python but I've checked it out. I'm wondering what's the use here for the regex ? The aliasByMetric function doesn't use a regex at all (it splits on \".\").\nWhen I do the same in aliasByNode, it fixes my problem (and probably the other's problem too).\nDo you think it's a correct solution?\n. ",
    "phillijw": "aliasByNode(movingAverage(sumSeries(stats_counts.lp.prod.msp0wlnpp00{6,7,9}.pf1.request),25), 3) //This returns msp0wlnpp00 instead of msp0wlnpp006 or msp0wlnpp007 or msp0wlnpp009\naliasByNode(movingAverage(sumSeries(stats_counts.lp.prod.msp0wlnpp00{6,7,9}.pf1.request),25), 4) //This crashed instead of returning pf1\nBoth of these seem incorrect behavior\n. ",
    "szibis": "Even with this patch i have very similar issue with remote rendering enabled:\nFri Feb 14 19:35:54 2014 :: Exception in graphite.render.views.rawrender\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 332, in renderLocalView\n    options = unpickle.loads(optionsPickle)\n  File \"/opt/graphite/webapp/graphite/util.py\", line 111, in loads\n    return pickle_obj.load()\n  File \"/opt/graphite/webapp/graphite/util.py\", line 100, in find_class\n    raise pickle.UnpicklingError('Attempting to unpickle unsafe module %s' % module)\nI have tested this with graphite-web 0.9.12 from mainstream and with evernote graphite-web - same issue.\n. Latest branch have many others issues for me. I need to revert https://github.com/graphite-project/graphite-web/commit/fc3f018544c19b90cc63797d18970a4cc27ef2ad#diff-a4d0ec3654c07a600b09d46705b38d9f\ni have now problems with rendering any graph with no exception in webapp log and only 500 code from django. Indexes and dshboard loading, but no graphs - tested on django 1.3, 1.5 and 1.6. \n. No still nothing on 1.4 with remote rendering.\nSat Feb 15 00:15:03 2014 :: Exception while attempting remote rendering request on 10.1.2.160\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 344, in delegateRendering\n    assert response.status == 200, \"Bad response code %d from %s\" % (response.status,server)\nAssertionError: Bad response code 500 from 10.1.2.160\nSat Feb 15 00:15:03 2014 :: Exception while attempting remote rendering request on 10.1.6.197\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 344, in delegateRendering\n    assert response.status == 200, \"Bad response code %d from %s\" % (response.status,server)\nAssertionError: Bad response code 500 from 10.1.6.197\nWithout rendering everything is working now.\n. I am out of computer until Monday, but i will confirm this as soon as i am able.\n\nOn 10 sie 2014, at 21:57, Denis Zhdanov notifications@github.com wrote:\nMaybe @szibis can confirm?\n\u2014\nReply to this email directly or view it on GitHub.\n. I have same problem with latest 0.9.x branch and changes from whisper solve problem (like this patch) with this trace like above.\n. \n",
    "ffMathy": "Two years later :smile:  Did you ever get to look at this @gwaldo?\n. ",
    "justanyone": "abandoned this branch, created a smaller pull request for much more limited scope.\n. According to a co-worker of mine, ExtJS 3.x to 4.x migrations are somewhat tricky due to a bunch of API changes.  There is a migration tool to help, but he guestimated it might be as much as a man-week of work to do this.  It would yield a much faster tree, as well as (possibly) other better/faster/stronger features.  I'm not sure what features we're using from ExtJS now, so I can't say how many calls, or how many lines of javascript code, we'd need to change.  Seemingly, it's just the composer page, the advanced user page, and the graphlot page (are there other pages?).\n. ",
    "wingZero21": "@pcn + @stevebanik thanks for that!!\n. ",
    "gsaray101": "in order for this to work, python client memecahced needs to be installed, right?\n. hi,\nI installed memcached and python-memchace module. Inserted these lines to my local_settings.py file:\nCACHE_MIDDLEWARE_SECONDS = 60\n    CACHES = {\n        'default': {\n            'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',\n            'LOCATION': [\n                '127.0.0.1:11211'\n            ]\n        }\n}\nWhen I click on a big node on graphite, it is timing out, not able to exapand the node. Any idease what might cause that, how could I increase the timeout value? I get this error from the apache log:\nRuntimeError: Unable to save data of type  to cache\n. I really like to see if graphite can use openTSDB as backend. This really would be awesome. Any progress on this?\n. I am using graphite web-app, have the default configs. Is there a conf file somewhere that over rides the data roll up? If the data frame is lower than 30 days, I see hourly data. If the data frame >= 30 days, I see aggreation happening in 2 hours. Any ideas?\n. @brutasse, I am seeing the same problem on 0.10.0-alpha. How do you get rid of these errors?\n. @brutasse, yes. I have installed whitenoise-1.0.6 and I also put this in wsgi.py file:\nfrom django.core.wsgi import get_wsgi_application\nfrom whitenoise.django import DjangoWhiteNoise\napplication = get_wsgi_application()\napplication = DjangoWhiteNoise(application)\nI keep getting this error also in apache log, my secret key is already set:\n[Thu May 28 09:41:24 2015] [error] /opt/graphite/webapp/graphite/settings.py:233: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security\n. @brutasse,\nI compilted mod_wsgi my wsgi.conf file under apache:/etc/apache2/conf.d looks like this:\nLoadModule wsgi_module /usr/lib64/apache2/mod_wsgi.so\nWSGISocketPrefix /var/run/wsg\nI do see the module when I do /usr/sbin/http2 -M\n wsgi_module (shared)\nBut I dont see this directory:\n/usr/local/lib64/python2.6/site-packages/mod_wsgi-4.3.1-py2.6-linux-x86_64.egg/\nWSGIPythonPath /usr/local/lib64/python2.6/site-packages does not seem to exists. Do I need this?\nDo you think this is the problem\n. I've installed your wsgi.py, no change. I still get static errors on browser.\n. @brutasse , I really need help with this. I really dont know what is going on at this point. I do see this message in apache error: [Thu May 28 10:42:34 2015] [notice] Apache/2.2.12 (Linux/SUSE) mod_wsgi/4.3.0 Python/2.6.9 mod_ssl/2.2.12 OpenSSL/0.9.8j-fips configured -- resuming normal operations\nWhich means that mod_wsgi should be ok. Any ideas what I may be missing?\n. Hi Jason,\nWhen are you going release the latest version of graphite web?\n. I installed graphite web app version 0.10-alpha. Which branch should I use?\n\nOn May 26, 2015, at 2:56 AM, Bruno Reni\u00e9 notifications@github.com wrote:\nYou're probably using graphite 0.9.x, in which case the example wsgi file can be found in that branch: https://github.com/graphite-project/graphite-web/blob/0.9.x/conf/graphite.wsgi.example\ngraphite.wsgi is new in 0.10.\n\u2014\nReply to this email directly or view it on GitHub.\n. @brutasse, I need to download 0.10 to use opentsdb as backend storage. Which version/link I should download? I downloaded this: graphite-web-master.zip, is this right?\n. there is https://github.com/mikebryant/graphite-opentsdb-finder.\n\nI'cannot get the mod_wsgi working for graphite. I keep getting these errors:\n[Tue May 26 13:55:01 2015] [error] [client 29.0.122.25] Traceback (most recent call last):\n[Tue May 26 13:55:01 2015] [error] [client 29.0.122.25]   File \"/opt/graphite/conf/graphite.wsgi\", line 4, in \n[Tue May 26 13:55:01 2015] [error] [client 29.0.122.25]     from graphite.wsgi import application\n[Tue May 26 13:55:01 2015] [error] [client 29.0.122.25] ImportError: No module named wsgi\n[Tue May 26 13:55:02 2015] [error] [client 29.0.122.25] mod_wsgi (pid=51856): Target WSGI script '/opt/graphite/conf/graphite.wsgi' cannot be loaded as Python module.\n[Tue May 26 13:55:02 2015] [error] [client 29.0.122.25] mod_wsgi (pid=51856): Exception occurred processing WSGI script '/opt/graphite/conf/graphite.wsgi'.\n[Tue May 26 13:55:02 2015] [error] [client 29.0.122.25] Traceback (most recent call last):\n[Tue May 26 13:55:02 2015] [error] [client 29.0.122.25]   File \"/opt/graphite/conf/graphite.wsgi\", line 4, in \n[Tue May 26 13:55:02 2015] [error] [client 29.0.122.25]     from graphite.wsgi import application\n[Tue May 26 13:55:02 2015] [error] [client 29.0.122.25] ImportError: No module named wsgi\n. these are the errors from apache:\n[Wed May 27 09:15:01 2015] [error] Could not import graphite.local_settings, using defaults!\n[Wed May 27 09:15:01 2015] [error] /usr/local/lib64/python2.6/site-packages/graphite_web-0.9.13-py2.6.egg/graphite/          settings.py:244: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for b          etter security\n[Wed May 27 09:15:01 2015] [error]   warn('SECRET_KEY is set to an unsafe default. This should be set in local_sett          ings.py for better security')\n[Wed May 27 09:15:02 2015] [error] Could not import graphite.local_settings, using defaults!\n[Wed May 27 09:15:02 2015] [error] /usr/local/lib64/python2.6/site-packages/graphite_web-0.9.13-py2.6.egg/graphite/          settings.py:244: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for b          etter security\n[Wed May 27 09:15:02 2015] [error]   warn('SECRET_KEY is set to an unsafe default. This should be set in local_sett          ings.py for better security')\n[Wed May 27 09:20:05 2015] [error] Could not import graphite.local_settings, using defaults!\n[Wed May 27 09:20:05 2015] [error] /usr/local/lib64/python2.6/site-packages/graphite_web-0.9.13-py2.6.egg/graphite/settings.py:244: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security\n[Wed May 27 09:20:05 2015] [error]   warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\n[Wed May 27 09:20:06 2015] [error] Could not import graphite.local_settings, using defaults!\n[Wed May 27 09:20:06 2015] [error] /usr/local/lib64/python2.6/site-packages/graphite_web-0.9.13-py2.6.egg/graphite/settings.py:244: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security\n[Wed May 27 09:20:06 2015] [error]   warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\n. this file says:\n vi /usr/local/lib64/python2.6/site-packages/graphite_web-0.9.13-py2.6.egg/graphite/settings.py\nWEBAPP_VERSION = '0.9.12'\nbut this file /opt/graphite/webapp/graphite/settings.py\nWEBAPP_VERSION = '0.10.0-alpha'\nI am confused. I only installed graphite once.\n. can I just remove the .egg file under site-packages? How do I make sure that it uses version 0.10.0?\n. also when I do python setup.py install on graphite-web-master, it it is doing the install as this:\npython setup.py install\nrunning install\nrunning build\nrunning build_py\nrunning build_scripts\nrunning install_lib\nbyte-compiling /opt/graphite/webapp/graphite/app_settings.py to app_settings.pyc\nrunning install_scripts\nchanging mode of /opt/graphite/bin/build-index to 755\nchanging mode of /opt/graphite/bin/build-index.sh to 755\nchanging mode of /opt/graphite/bin/run-graphite-devel-server.py to 755\nrunning install_data\ncopying conf/dashboard.conf.example -> /opt/graphite/conf\ncopying conf/graphite.wsgi.example -> /opt/graphite/conf\nrunning install_egg_info\nRemoving /opt/graphite/webapp/graphite_web-0.10.0_alpha-py2.6.egg-info\nWriting /opt/graphite/webapp/graphite_web-0.10.0_alpha-py2.6.egg-info\nI am not sure where is this come from\n/usr/local/lib64/python2.6/site-packages/graphite_web-0.9.13-py2.6.egg/graphite/settings.py\n. I installed from downloading the master zip file from the gihub. Can you please guide me through removing the all install and and installing fresh 0.10.0-alpha. Something is wrong and cannot figure it out.\n. @dentrfonov, I removed the .egg directoryies under site-packages. I need to install 0.10.0-alpha. Can  you please provide me the tar file?\n. I have suse 11.3 and I need to install from source.\n. I installed all of the dependencies, including django, my graphite web app location is /opt/graphite. I compiled and installed mod_wsgi and configured apache. What else I need to make this work?\n. @eentrfonov, I checked the dependencies, all met. I installed djaggo, webapp in /opt/graphite. I installed this in our POC long time back and was able to do it. It has so many pieces, that I am missing something. I hoped there was an SOP to go through each component and configure it. Again, I only need webapp since I will be using opentsdb as the backend storage.\n. I've got the opentsdb, it is a simple plugin. I just need to get the graphite installed and working, which I am not able to at this time. This is really painful!\n. @deniszh, There is nothing I can do about the OS, it is company wide. I was able to install this on a different machine with the same OS. Is this a good version of graphite 0.10.0-alpha to install: https://github.com/graphite-project/graphite-web, or can somebody supply me a tar file?\n. All, really for all this post. I am doing a fresh install, downloaded all of the files and installed it. Now I am going through the configuration listed on this page: http://graphite.wikidot.com/installation\nI am at Intitial Database Creation, when I execute this command: \ncd /opt/graphite/webapp/graphite\nsudo python manage.py syncdb\nI get these errors:\nTraceback (most recent call last):\n  File \"manage.py\", line 11, in \n    execute_from_command_line(sys.argv)\n  File \"/usr/local/lib64/python2.6/site-packages/Django-1.6.11-py2.6.egg/django/core/management/init.py\", line 399, in execute_from_command_line\n    utility.execute()\n  File \"/usr/local/lib64/python2.6/site-packages/Django-1.6.11-py2.6.egg/django/core/management/init.py\", line 392, in execute\n    self.fetch_command(subcommand).run_from_argv(self.argv)\n  File \"/usr/local/lib64/python2.6/site-packages/Django-1.6.11-py2.6.egg/django/core/management/init.py\", line 261, in fetch_command\n    commands = get_commands()\n  File \"/usr/local/lib64/python2.6/site-packages/Django-1.6.11-py2.6.egg/django/core/management/init.py\", line 107, in get_commands\n    apps = settings.INSTALLED_APPS\n  File \"/usr/local/lib64/python2.6/site-packages/Django-1.6.11-py2.6.egg/django/conf/init.py\", line 54, in getattr\n    self._setup(name)\n  File \"/usr/local/lib64/python2.6/site-packages/Django-1.6.11-py2.6.egg/django/conf/init.py\", line 49, in _setup\n    self._wrapped = Settings(settings_module)\n  File \"/usr/local/lib64/python2.6/site-packages/Django-1.6.11-py2.6.egg/django/conf/init.py\", line 132, in init\n    % (self.SETTINGS_MODULE, e)\nImportError: Could not import settings 'graphite.settings' (Is it on sys.path? Is there an import error in the settings file?): No module named graphite.settings\ncan anybody tell, what I am missing?\n. I was able to figure out what was the problem. I am making some progress, I was able to bring up the graphite web but browser window is showing these errors:\nGET http://51.19.236.10:8090/static/js/ext/resources/css/ext-all.css \ncomposer:25 GET http://51.19.236.10:8090/static/js/ext/ext-all-debug.js \ncomposer:24 GET http://51.19.236.10:8090/static/js/ext/adapter/ext/ext-base-debug.js \ncomposer:30 Uncaught ReferenceError: Ext is not defined(anonymous function) @ composer:30\ncomposer:72 GET http://51.19.236.10:8090/static/js/browser.js \ncomposer:73 GET http://51.19.236.10:8090/static/js/composer_widgets.js \ncomposer:74 GET http://51.19.236.10:8090/static/js/composer.js \nbrowserheader:1 GET http://51.19.236.10:8090/static/img/carbon-fiber.png 404 (NOT FOUND)\ncomposer:75 GET http://51.19.236.10:8090/static/js/completer.js \ncomposer:92 Uncaught ReferenceError: Ext is not defined(anonymous function) @ composer:92\ncomposer:102 Uncaught ReferenceError: GraphiteBrowser is not defined\n1. All the file under content directory has read and execute riights.\n2. Also, I see static in the url, I dont see any static directory in the graphite install. I only see graphite and content under /opt/graphite/webapp,\nAny ideas about these erors?\n. I have apache server\nSent from my iPhone\n\nOn May 27, 2015, at 10:43 PM, Denis Trifonov notifications@github.com wrote:\nYou need setup web server to serve content folder by /static/* requests. My example for Nginx:\n...\nlocation /static {\n  alias /opt/graphite/webapp/content/;\n}\n...\n\u2014\nReply to this email directly or view it on GitHub.\n. I tried that, I still get these errors on chrome\n\nSent from my iPhone\n\nOn May 27, 2015, at 11:06 PM, Denis Trifonov notifications@github.com wrote:\nI don't use Apache Server. Google say this about Apache config:\nAlias /static /opt/graphite/webapp/content/\n\u2014\nReply to this email directly or view it on GitHub.\n. Looks like I've made some progress. I don't see the static errors anymore. I have two more issues:\n\n1.looks like my graphite istall cannot read json:\nRequest Method: GET\nRequest URL:    http://51.19.236.10:8090/metrics/find/?_dc=1432826124809&query=*&format=treejson&path=&node=GraphiteTree\nDjango Version: 1.6.8\nException Type: ValueError\nException Value:\nNo JSON object could be decoded\nException Location: /usr/lib64/python2.6/json/decoder.py in raw_decode, line 338\n1. I still see these errors in the apache log:\n   /opt/graphite/webapp/graphite/settings.py:233: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security\nany ideas?\n. it is located in /opt/graphite/webapp/graphite\nls /opt/graphite/webapp/graphite\ninit.py       carbonlink.pyc  intervals.py               manage.py       readers.pyc         storage.pyc    version\ninit.pyc      compat.py       intervals.pyc              metrics         remote_storage.py   templates      views.py\naccount           compat.pyc      local_settings.py          middleware.py   remote_storage.pyc  url_shortener  views.pyc\napp_settings.py   composer        local_settings.py.example  middleware.pyc  render              urls.py        whitelist\napp_settings.pyc  dashboard       local_settings.pyc         node.py         settings.py         urls.pyc       wsgi.py\nbrowser           events          logger.py                  node.pyc        settings.pyc        util.py        wsgi.py.backup\ncarbonlink.py     finders         logger.pyc                 readers.py      storage.py          util.pyc       wsgi.pyc\n. graphite apache conf file:\nListen 8090\nLoadModule wsgi_module /usr/lib64/apache2/mod_wsgi.so\nAlias /static/ \"/opt/graphite/webapp/content/\"\nWSGISocketPrefix /etc/apache2/wsgi\n\n    Options All\n    AllowOverride All\n    Order deny,allow\n    Allow from all\n\nDocumentRoot \"/opt/graphite/webapp\"\n\nServerName 51.19.236.10\nHeader set Access-Control-Allow-Origin \"*\"\nDocumentRoot \"/opt/graphite/webapp\"\nWSGIDaemonProcess graphite processes=5 threads=5 display-name='%{GROUP}' inactivity-timeout=120\nWSGIProcessGroup graphite\nWSGIScriptAlias / /opt/graphite/conf/graphite.wsgi\nAlias /content/ /opt/graphite/webapp/content/\n\nSetHandler None\n\n\nAllow from all\n\nErrorLog /var/log/apache2/graphite_error\n\n. what do you think about graphite not able to convert json object?\nRequest Method: GET\nRequest URL:    http://51.19.236.10:8090/metrics/find/?_dc=1432828098585&query=*&format=treejson&path=&node=GraphiteTree\nDjango Version: 1.6.8\nException Type: ValueError\nException Value:\nNo JSON object could be decoded\nException Location: /usr/lib64/python2.6/json/decoder.py in raw_decode, line 338\nPython Executable:  /usr/bin/python\nPython Version: 2.6.9\nPython Path:\n['/usr/local/lib64/python2.6/site-packages/setuptools-16.0-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/python_memcached-1.54-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/six-1.9.0-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/gitversion-2.1.3-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/celery-3.1.18-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/kombu-3.0.26-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/billiard-3.3.0.20-py2.6-linux-x86_64.egg',\n '/usr/local/lib64/python2.6/site-packages/ordereddict-1.1-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/importlib-1.0.3-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/amqp-1.4.6-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/anyjson-0.3.3-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/requests-2.7.0-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/txAMQP-0.6.2-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/django_cacheback-0.8-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/django_celery-3.1.16-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/pip-7.0.1-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages',\n '/app/data.1/files/graphite-opentsdb-0.4.1',\n '/usr/local/lib64/python2.6/site-packages/whitenoise-1.0.6-py2.6.egg',\n '/usr/lib/python26.zip',\n '/usr/lib64/python2.6',\n '/usr/lib64/python2.6/plat-linux2',\n '/usr/lib64/python2.6/lib-tk',\n '/usr/lib64/python2.6/lib-old',\n '/usr/lib64/python2.6/lib-dynload',\n '/usr/lib64/python2.6/site-packages',\n '/usr/lib64/python2.6/site-packages/Numeric',\n '/usr/lib64/python2.6/site-packages/gtk-2.0',\n '/opt/graphite/webapp']\nServer time:    Thu, 28 May 2015 11:48:59 -0400\n. How can I send you the file?\n. does this help:\nEnvironment:\nRequest Method: GET\nRequest URL: http://51.19.236.10:8090/metrics/find/?_dc=1432826124809&query=*&format=treejson&path=&node=GraphiteTree\nDjango Version: 1.6.8\nPython Version: 2.6.9\nInstalled Applications:\n('graphite.metrics',\n 'graphite.render',\n 'graphite.browser',\n 'graphite.composer',\n 'graphite.account',\n 'graphite.dashboard',\n 'graphite.whitelist',\n 'graphite.events',\n 'graphite.url_shortener',\n 'django.contrib.auth',\n 'django.contrib.sessions',\n 'django.contrib.admin',\n 'django.contrib.contenttypes',\n 'django.contrib.staticfiles',\n 'tagging',\n 'graphite_opentsdb',\n 'cacheback')\nInstalled Middleware:\n('graphite.middleware.LogExceptionsMiddleware',\n 'django.middleware.common.CommonMiddleware',\n 'django.middleware.gzip.GZipMiddleware',\n 'django.contrib.sessions.middleware.SessionMiddleware',\n 'django.contrib.auth.middleware.AuthenticationMiddleware',\n 'django.contrib.messages.middleware.MessageMiddleware',\n 'django.middleware.cache.UpdateCacheMiddleware',\n 'django.middleware.cache.FetchFromCacheMiddleware')\nTraceback:\nFile \"/usr/local/lib64/python2.6/site-packages/django/core/handlers/base.py\" in get_response\n1.                     response = wrapped_callback(request, _callback_args, _callback_kwargs)\n   File \"/opt/graphite/webapp/graphite/metrics/views.py\" in find_view\n2.     matches = list( STORE.find(query, fromTime, untilTime, local=local_only) )\n   File \"/opt/graphite/webapp/graphite/storage.py\" in find\n3.       for node in finder.find_nodes(query):\n   File \"/app/data.1/files/graphite-opentsdb-0.4.1/graphite_opentsdb/finder.py\" in find_nodes\n4.         for node in find_nodes_from_pattern(self.opentsdb_uri, self.opentsdb_tree, query.pattern):\n   File \"/app/data.1/files/graphite-opentsdb-0.4.1/graphite_opentsdb/finder.py\" in find_nodes_from_pattern\n5.     nodes = list(find_opentsdb_nodes(opentsdb_uri, query_parts, \"%04X\" % opentsdb_tree, shared_reader=shared_reader))\n   File \"/app/data.1/files/graphite-opentsdb-0.4.1/graphite_opentsdb/finder.py\" in find_opentsdb_nodes\n6.     for node, node_data in get_branch_nodes(opentsdb_uri, current_branch, shared_reader, path):\n   File \"/app/data.1/files/graphite-opentsdb-0.4.1/graphite_opentsdb/finder.py\" in get_branch_nodes\n7.     results = get_opentsdb_url(opentsdb_uri, \"tree/branch?branch=%s\" % current_branch)\n   File \"build/bdist.linux-x86_64/egg/cacheback/decorators.py\" in __wrapper\n8.             return job.get(fn, _args, _kwargs)\n   File \"build/bdist.linux-x86_64/egg/cacheback/base.py\" in get\n9.                 return self.refresh(_args, _kwargs)\n   File \"build/bdist.linux-x86_64/egg/cacheback/base.py\" in refresh\n10.         result = self.fetch(_args, _kwargs)\n    File \"build/bdist.linux-x86_64/egg/cacheback/function.py\" in fetch\n11.         return fn(_args, _kwargs)\n    File \"/app/data.1/files/graphite-opentsdb-0.4.1/graphite_opentsdb/finder.py\" in get_opentsdb_url\n12.     return requests.get(full_url).json()\n    File \"/usr/local/lib64/python2.6/site-packages/requests-2.7.0-py2.6.egg/requests/models.py\" in json\n13.         return complexjson.loads(self.text, kwargs)\n    File \"/usr/lib64/python2.6/json/init*.py\" in loads\n14.         return _default_decoder.decode(s)\n    File \"/usr/lib64/python2.6/json/decoder.py\" in decode\n15.         obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n    File \"/usr/lib64/python2.6/json/decoder.py\" in raw_decode\n16.             raise ValueError(\"No JSON object could be decoded\")\nException Type: ValueError at /metrics/find/\nException Value: No JSON object could be decoded\n. I am using this plugin to connect to opentsdb for metrics: https://github.com/mikebryant/graphite-opentsdb-finder\nmy configuration to connect to opentsdb set in local_settings.py file like this:\nSTORAGE_FINDERS = (\n    'graphite_opentsdb.finder.OpenTSDBFinder',\n)\nOPENTSDB_URI = 'http://192.168.191.1:4242/api/v1/'\nOPENTSDB_TREE = 1\nSince my graphite install is not seeing the local_settings.py, not able to retrieve the opentsdb tree. Can anybody tell me why my graphite install not able to read settings.py or local_settings.py file located in /opt/graphite/webapp/graphite directory? I change the perm on those files to 777 to make sure, no perm problem.\nI keep getting these errors in apache logs:\n[Thu May 28 13:32:25 2015] [error] /opt/graphite/webapp/graphite/settings.py:233: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security\n[Thu May 28 13:32:25 2015] [error]   warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\nI included my apache graphite.conf file above. Any one think of anything that I may be missing.\n. OMG, I got it working. I was missing these in my apache conf file:\nWSGIApplicationGroup %{GLOBAL}\n        WSGIImportScript /opt/graphite/conf/graphite.wsgi process-group=graphite application-group=%{GLOBAL}\nOne other question, when I click on tree nodes on graphite, it takes a bit of time to come back. Is there a way to speed this up by increasing threads etc? Any ideas?\n. @brutasse, I've graphite working but I still see these errors in the apache logs:\n[Mon Jun 01 10:02:11 2015] [error]   warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\nany ideas?\n. @brutasse, I've graphite working but I still see these errors in the apache logs:\n[Mon Jun 01 10:02:11 2015] [error]   warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\nany ideas?\n. Yes, it's set to something else and I still get that error .\nSent from my iPhone\n\nOn Jun 2, 2015, at 8:55 PM, Phil Dufault notifications@github.com wrote:\n@gsaray101 Check your local_settings.py for your SECRET_KEY setting, it's set to the default which isn't a good idea. local_settings.py is in /opt/graphite/webapp/graphite/local_settings.py for me.\n\u2014\nReply to this email directly or view it on GitHub.\n. I installed python-memcached it seems to be semi-working now. If one of the tree has many nodes, when I attempt to expand the tree, I think it is timing out and I get internal server error. My current settings are:\n\nMEMCACHE_HOSTS = ['localhost:11211']\nFIND_CACHE_DURATION = 1500\nFIND_TOLERANCE = 2 * FIND_CACHE_DURATION\nDEFAULT_CACHE_DURATION = 1800 \nIs there a variable to set the timeout value?\n. I keep getting this error:\nRuntimeError at /metrics/find/\nUnable to save data of type  to cache\nI started memcached as wwwrun (apache). This only happens if the branch has many nodes like greater than 100. I need help, any ideas?\n. I think /metric/find is timing out. Is there a variable in local_settings.py file to increase this value?\n. @brutasse, is there a method to increase the /metric/find timeout on graphite-web?\n. when I comment out this line: MEMCACHE_HOSTS = ['localhost:11211'], I dont get this error but the purpose of why I want to use memcache is to improve the performance. \n1. is there a timeout to /metric/find so that I can increase the value?\n2. Or is there a limitation to memcache where I can only write a certain size  to it?\n. these are the errors from apache, anybody out there any ideas? I am stuck.\n[Thu Jun 25 16:08:40 2015] [error]   File \"build/bdist.linux-x86_64/egg/graphite_opentsdb/finder.py\", line 82, in find_opentsdb_nodes\n[Thu Jun 25 16:08:40 2015] [error]     node.path,\n[Thu Jun 25 16:08:40 2015] [error]   File \"build/bdist.linux-x86_64/egg/graphite_opentsdb/finder.py\", line 82, in find_opentsdb_nodes\n[Thu Jun 25 16:08:40 2015] [error]     node.path,\n[Thu Jun 25 16:08:40 2015] [error]   File \"build/bdist.linux-x86_64/egg/graphite_opentsdb/finder.py\", line 82, in find_opentsdb_nodes\n[Thu Jun 25 16:08:40 2015] [error]     node.path,\n[Thu Jun 25 16:08:40 2015] [error]   File \"build/bdist.linux-x86_64/egg/graphite_opentsdb/finder.py\", line 82, in find_opentsdb_nodes\n[Thu Jun 25 16:08:40 2015] [error]     node.path,\n[Thu Jun 25 16:08:40 2015] [error]   File \"build/bdist.linux-x86_64/egg/graphite_opentsdb/finder.py\", line 59, in find_opentsdb_nodes\n[Thu Jun 25 16:08:40 2015] [error]     for node, node_data in get_branch_nodes(opentsdb_uri, current_branch, shared_reader, path):\n[Thu Jun 25 16:08:40 2015] [error]   File \"build/bdist.linux-x86_64/egg/graphite_opentsdb/finder.py\", line 97, in get_branch_nodes\n[Thu Jun 25 16:08:40 2015] [error]     results = get_opentsdb_url(opentsdb_uri, \"tree/branch?branch=%s\" % current_branch)\n[Thu Jun 25 16:08:40 2015] [error]   File \"build/bdist.linux-x86_64/egg/cacheback/decorators.py\", line 28, in __wrapper\n[Thu Jun 25 16:08:40 2015] [error]     return job.get(fn, _args, _kwargs)\n[Thu Jun 25 16:08:40 2015] [error]   File \"build/bdist.linux-x86_64/egg/cacheback/base.py\", line 81, in get\n[Thu Jun 25 16:08:40 2015] [error]     return self.refresh(_args, _kwargs)\n[Thu Jun 25 16:08:40 2015] [error]   File \"build/bdist.linux-x86_64/egg/cacheback/base.py\", line 187, in refresh\n[Thu Jun 25 16:08:40 2015] [error]     result)\n[Thu Jun 25 16:08:40 2015] [error]   File \"build/bdist.linux-x86_64/egg/cacheback/base.py\", line 178, in cache_set\n[Thu Jun 25 16:08:40 2015] [error]     type(data)))\n[Thu Jun 25 16:08:40 2015] [error] RuntimeError: Unable to save data of type  to cache\n. This is the graphite debug post back:\nRuntimeError at /metrics/find/\nUnable to save data of type  to cache\nRequest Method: GET\nRequest URL:    http://10.5.151.236:8090/metrics/find/?_dc=1435264498916&query=vSphereGuest.dc1.Guest.Resource_Utilization.*&format=treejson&path=vSphereGuest.dc1.Guest.Resource_Utilization&node=vSphereGuest.dc1.Guest.Resource_Utilization\nDjango Version: 1.6.8\nException Type: RuntimeError\nException Value:\nUnable to save data of type  to cache\nException Location: build/bdist.linux-x86_64/egg/cacheback/base.py in cache_set, line 178\nPython Executable:  /usr/bin/python\nPython Version: 2.6.9\nPython Path:\n['/usr/local/lib64/python2.6/site-packages/mod_wsgi-4.3.1-py2.6-linux-x86_64.egg',\n '/usr/local/lib64/python2.6/site-packages/Twisted-14.0.2-py2.6-linux-x86_64.egg',\n '/usr/local/lib64/python2.6/site-packages/zope.interface-4.1.1-py2.6-linux-x86_64.egg',\n '/usr/local/lib64/python2.6/site-packages/daemonize-2.3.1-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/pyOpenSSL-0.14-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/six-1.7.3-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/cffi-0.8.2-py2.6-linux-x86_64.egg',\n '/usr/local/lib64/python2.6/site-packages/pytz-2014.9-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/requests-2.4.3-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/django_cacheback-0.8-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/celery-3.1.16-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/kombu-3.0.23-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/amqp-1.4.6-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/anyjson-0.3.3-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/billiard-3.3.0.18-py2.6-linux-x86_64.egg',\n '/usr/local/lib64/python2.6/site-packages/django_celery-3.1.16-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/pip-1.5.6-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/pysftp-0.2.8-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/paramiko-1.15.1-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/pyodbc-3.0.7-py2.6-linux-x86_64.egg',\n '/usr/local/lib64/python2.6/site-packages/suds-0.4-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/graphite_opentsdb-0.4.1-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages/python_memcached-1.54-py2.6.egg',\n '/usr/local/lib64/python2.6/site-packages',\n '/usr/lib/python26.zip',\n '/usr/lib64/python2.6',\n '/usr/lib64/python2.6/plat-linux2',\n '/usr/lib64/python2.6/lib-tk',\n '/usr/lib64/python2.6/lib-old',\n '/usr/lib64/python2.6/lib-dynload',\n '/usr/lib64/python2.6/site-packages',\n '/usr/lib64/python2.6/site-packages/Numeric',\n '/usr/lib64/python2.6/site-packages/gtk-2.0',\n '/opt/graphite/webapp']\nServer time:    Thu, 25 Jun 2015 16:40:42 -0400\nTraceback Switch to copy-and-paste view\n/usr/local/lib64/python2.6/site-packages/django/core/handlers/base.py in get_response\n                    response = wrapped_callback(request, _callback_args, _callback_kwargs) ...\n\u25b6 Local vars\n/opt/graphite/webapp/graphite/metrics/views.py in find_view\n    matches = list( STORE.find(query, fromTime, untilTime, local=local_only) ) ...\n\u25b6 Local vars\n/opt/graphite/webapp/graphite/storage.py in find\n      for node in finder.find_nodes(query): ...\n\u25b6 Local vars\nbuild/bdist.linux-x86_64/egg/graphite_opentsdb/finder.py in find_nodes\n        for node in find_nodes_from_pattern(self.opentsdb_uri, self.opentsdb_tree, query.pattern): ...\n\u25b6 Local vars\nbuild/bdist.linux-x86_64/egg/graphite_opentsdb/finder.py in find_nodes_from_pattern\n    nodes = list(find_opentsdb_nodes(opentsdb_uri, query_parts, \"%04X\" % opentsdb_tree, shared_reader=shared_reader)) ...\n\u25b6 Local vars\nbuild/bdist.linux-x86_64/egg/graphite_opentsdb/finder.py in find_opentsdb_nodes\n                        node.path, ...\n\u25b6 Local vars\nbuild/bdist.linux-x86_64/egg/graphite_opentsdb/finder.py in find_opentsdb_nodes\n                        node.path, ...\n\u25b6 Local vars\nbuild/bdist.linux-x86_64/egg/graphite_opentsdb/finder.py in find_opentsdb_nodes\n                        node.path, ...\n\u25b6 Local vars\nbuild/bdist.linux-x86_64/egg/graphite_opentsdb/finder.py in find_opentsdb_nodes\n                        node.path, ...\n\u25b6 Local vars\nbuild/bdist.linux-x86_64/egg/graphite_opentsdb/finder.py in find_opentsdb_nodes\n    for node, node_data in get_branch_nodes(opentsdb_uri, current_branch, shared_reader, path): ...\n\u25b6 Local vars\nbuild/bdist.linux-x86_64/egg/graphite_opentsdb/finder.py in get_branch_nodes\n    results = get_opentsdb_url(opentsdb_uri, \"tree/branch?branch=%s\" % current_branch) ...\n\u25b6 Local vars\nbuild/bdist.linux-x86_64/egg/cacheback/decorators.py in __wrapper\n            return job.get(fn, _args, _kwargs) ...\n\u25b6 Local vars\nbuild/bdist.linux-x86_64/egg/cacheback/base.py in get\n                return self.refresh(_args, *_kwargs) ...\n\u25b6 Local vars\nbuild/bdist.linux-x86_64/egg/cacheback/base.py in refresh\n                       result) ...\n\u25b6 Local vars\nbuild/bdist.linux-x86_64/egg/cacheback/base.py in cache_set\n                        type(data))) ...\n\u25b6 Local vars\nRequest information\nGET\nVariable    Value\nnode\nu'vSphereGuest.dc1.Guest.Resource_Utilization'\nquery\nu'vSphereGuest.dc1.Guest.Resource_Utilization.'\npath\nu'vSphereGuest.dc1.Guest.Resource_Utilization'\n_dc \nu'1435264498916'\nformat\nu'treejson'\nPOST\nNo POST data\nFILES\nNo FILES data\nCOOKIES\nNo cookie data\nMETA\nVariable    Value\nmod_wsgi.listener_port\n'8090'\nmod_wsgi.listener_host\n''\nSERVER_SOFTWARE \n'Apache/2.2.12 (Linux/SUSE)'\nSCRIPT_NAME \nu''\nmod_wsgi.enable_sendfile\n'0'\nmod_wsgi.handler_script \n''\nSERVER_SIGNATURE\n'Apache/2.2.12 (Linux/SUSE) Server at 10.5.151.236 Port 8090\\n'\nREQUEST_METHOD\n'GET'\nPATH_INFO\nu'/metrics/find/'\nSERVER_PROTOCOL \n'HTTP/1.1'\nQUERY_STRING\n'_dc=1435264498916&query=vSphereGuest.dc1.Guest.Resource_Utilization.&format=treejson&path=vSphereGuest.dc1.Guest.Resource_Utilization&node=vSphereGuest.dc1.Guest.Resource_Utilization'\nHTTP_USER_AGENT \n'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/43.0.2357.130 Safari/537.36'\nHTTP_CONNECTION \n'keep-alive'\nSERVER_NAME \n'10.5.151.236'\nREMOTE_ADDR \n'29.0.33.7'\nmod_wsgi.queue_start\n'1435264819028975'\nmod_wsgi.request_handler\n'wsgi-script'\napache.version\n(2, 2, 12)\nwsgi.url_scheme \n'http'\nPATH_TRANSLATED \n'/opt/graphite/conf/graphite.wsgi/metrics/find/'\nSERVER_PORT \n'8090'\nwsgi.multiprocess\nTrue\nmod_wsgi.input_chunked\n'0'\nSERVER_ADDR \n'10.5.151.236'\nDOCUMENT_ROOT\n'/opt/graphite/webapp'\nmod_wsgi.process_group\n'graphite'\nmod_wsgi.daemon_connects\n'1'\nSCRIPT_FILENAME \n'/opt/graphite/conf/graphite.wsgi'\nSERVER_ADMIN\n'[no address given]'\nwsgi.input\n\nHTTP_HOST\n'10.5.151.236:8090'\nmod_wsgi.daemon_start\n'1435264819030153'\nwsgi.multithread\nTrue\nmod_wsgi.callable_object\n'application'\nmod_wsgi.daemon_restarts\n'0'\nREQUEST_URI \n'/metrics/find/?dc=1435264498916&query=vSphereGuest.dc1.Guest.Resource_Utilization.&format=treejson&path=vSphereGuest.dc1.Guest.Resource_Utilization&node=vSphereGuest.dc1.Guest.Resource_Utilization'\nHTTP_ACCEPT \n'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,/;q=0.8'\nwsgi.file_wrapper\n''\nwsgi.version\n(1, 0)\nGATEWAY_INTERFACE\n'CGI/1.1'\nwsgi.run_once\nFalse\nwsgi.errors \n\nREMOTE_PORT \n'57845'\nHTTP_ACCEPT_LANGUAGE\n'en-US,en;q=0.8'\nmod_wsgi.version\n(4, 3, 0)\nmod_wsgi.script_start\n'1435264819598568'\nmod_wsgi.application_group\n'10.5.151.236:8090|'\nmod_wsgi.script_reloading\n'1'\nmod_wsgi.request_start\n'1435264819027502'\nHTTP_ACCEPT_ENCODING\n'gzip, deflate, sdch'\nSettings\nUsing settings module graphite.settings\nSetting Value\nREMOTE_RENDERING\nFalse\nREMOTE_FETCH_TIMEOUT\n900\nEMAIL_USE_TLS\nFalse\nTIME_ZONE\n'America/New_York'\nDOCUMENTATION_URL\n'http://graphite.readthedocs.org/'\nCSRF_COOKIE_SECURE\nFalse\nLDAP_URI\nNone\nLANGUAGE_CODE\n'en-us'\nROOT_URLCONF\n'graphite.urls'\nMANAGERS\n()\nCARBONLINK_TIMEOUT\n1.0\nGRAPHITE_ROOT\n'/opt/graphite'\nCLUSTER_SERVERS \n[]\nDEFAULT_CHARSET \n'utf-8'\nWEBAPP_DIR\n'/opt/graphite/webapp'\nSESSION_SERIALIZER\n'django.contrib.sessions.serializers.JSONSerializer'\nSTATIC_ROOT \n'/opt/graphite/static'\nCARBONLINK_RETRY_DELAY\n15\nLOG_ROTATE\nTrue\nALLOWED_HOSTS\n['']\nREMOTE_RENDER_CONNECT_TIMEOUT\n60.0\nMESSAGE_STORAGE \n'django.contrib.messages.storage.fallback.FallbackStorage'\nWSGI_APPLICATION\nNone\nEMAIL_SUBJECT_PREFIX\n'[Django] '\nSEND_BROKEN_LINK_EMAILS \nFalse\nCONF_DIR\n'/opt/graphite/conf'\nSESSION_CACHE_ALIAS \n'default'\nSESSION_COOKIE_DOMAIN\nNone\nSESSION_COOKIE_NAME \n'sessionid'\nLDAP_BASE_USER\n''\nREMOTE_READER_CACHE_SIZE_LIMIT\n10000\nADMIN_FOR\n()\nTIME_INPUT_FORMATS\n('%H:%M:%S', '%H:%M:%S.%f', '%H:%M')\nREPLICATION_FACTOR\n1\nDATABASES\n{'default': {'ATOMIC_REQUESTS': False,\n             'AUTOCOMMIT': True,\n             'CONN_MAX_AGE': 0,\n             'ENGINE': 'django.db.backends.sqlite3',\n             'HOST': '',\n             'NAME': '/opt/graphite/storage/graphite.db',\n             'OPTIONS': {},\n             'PASSWORD': u'***_',\n             'PORT': '',\n             'TEST_CHARSET': None,\n             'TEST_COLLATION': None,\n             'TEST_MIRROR': None,\n             'TEST_NAME': None,\n             'TIME_ZONE': 'UTC',\n             'USER': ''}}\nLDAP_USE_TLS\nFalse\nFILE_UPLOAD_PERMISSIONS \nNone\nFILE_UPLOAD_HANDLERS\n('django.core.files.uploadhandler.MemoryFileUploadHandler',\n 'django.core.files.uploadhandler.TemporaryFileUploadHandler')\nLOG_CACHE_PERFORMANCE\nFalse\nDEFAULT_CONTENT_TYPE\n'text/html'\nTEST_RUNNER \n'django.test.runner.DiscoverRunner'\nAPPEND_SLASH\nFalse\nFIRST_DAY_OF_WEEK\n0\nDATABASE_ROUTERS\n[]\nREMOTE_USER_BACKEND \n''\nCARBONLINK_HOSTS\n['127.0.0.1:7002']\nMAX_FETCH_RETRIES\n5\nYEAR_MONTH_FORMAT\n'F Y'\nSTATICFILES_STORAGE \n'django.contrib.staticfiles.storage.StaticFilesStorage'\nINDEX_FILE\n'/opt/graphite/storage/index'\nSTORAGE_DIR \n'/opt/graphite/storage'\nLEGEND_MAX_ITEMS\n10\nSERVER_EMAIL\n'root@localhost'\nSESSION_COOKIE_PATH \n'/'\nFIND_TOLERANCE\n3000\nMIDDLEWARE_CLASSES\n('graphite.middleware.LogExceptionsMiddleware',\n 'django.middleware.common.CommonMiddleware',\n 'django.middleware.gzip.GZipMiddleware',\n 'django.contrib.sessions.middleware.SessionMiddleware',\n 'django.contrib.auth.middleware.AuthenticationMiddleware',\n 'django.contrib.messages.middleware.MessageMiddleware',\n 'django.middleware.cache.UpdateCacheMiddleware',\n 'django.middleware.cache.FetchFromCacheMiddleware')\nUSE_I18N\nTrue\nLDAP_USER_QUERY \n''\nSECRET_KEY\nu'**_'\nLANGUAGE_COOKIE_NAME\n'django_language'\nUSE_REMOTE_USER_AUTHENTICATION\nFalse\nFILE_UPLOAD_TEMP_DIR\nNone\nOPENTSDB_URI\n'http://10.5.151.237:4242/api/v1/'\nLDAP_SEARCH_BASE\n''\nTRANSACTIONS_MANAGED\nFalse\nLOGGING_CONFIG\n'django.utils.log.dictConfig'\nTEMPLATE_LOADERS\n('django.template.loaders.filesystem.Loader',\n 'django.template.loaders.app_directories.Loader')\nLOG_DIR \n'/opt/graphite/storage/log/webapp'\nTEMPLATE_DEBUG\nFalse\nX_FRAME_OPTIONS \n'SAMEORIGIN'\nCSRF_COOKIE_NAME\n'csrftoken'\nFORCE_SCRIPT_NAME\nNone\nUSE_X_FORWARDED_HOST\nFalse\nDASHBOARD_REQUIRE_AUTHENTICATION\nFalse\nLDAP_PORT\n389\nSIGNING_BACKEND \n'django.core.signing.TimestampSigner'\nSESSION_COOKIE_SECURE\nFalse\nLDAP_BASE_PASS\nu'**_'\nJAVASCRIPT_DEBUG\nFalse\nCSRF_COOKIE_DOMAIN\nNone\nFILE_CHARSET\n'utf-8'\nDEBUG\nTrue\nCERES_DIR\n'/opt/graphite/storage/ceres/'\nSESSION_FILE_PATH\nNone\nDEFAULT_FILE_STORAGE\n'django.core.files.storage.FileSystemStorage'\nINSTALLED_APPS\n('graphite.metrics',\n 'graphite.render',\n 'graphite.browser',\n 'graphite.composer',\n 'graphite.account',\n 'graphite.dashboard',\n 'graphite.whitelist',\n 'graphite.events',\n 'graphite.url_shortener',\n 'django.contrib.auth',\n 'django.contrib.sessions',\n 'django.contrib.admin',\n 'django.contrib.contenttypes',\n 'django.contrib.staticfiles',\n 'tagging',\n 'graphite_opentsdb',\n 'cacheback')\nLANGUAGES\n(('af', 'Afrikaans'),\n ('ar', 'Arabic'),\n ('az', 'Azerbaijani'),\n ('bg', 'Bulgarian'),\n ('be', 'Belarusian'),\n ('bn', 'Bengali'),\n ('br', 'Breton'),\n ('bs', 'Bosnian'),\n ('ca', 'Catalan'),\n ('cs', 'Czech'),\n ('cy', 'Welsh'),\n ('da', 'Danish'),\n ('de', 'German'),\n ('el', 'Greek'),\n ('en', 'English'),\n ('en-gb', 'British English'),\n ('eo', 'Esperanto'),\n ('es', 'Spanish'),\n ('es-ar', 'Argentinian Spanish'),\n ('es-mx', 'Mexican Spanish'),\n ('es-ni', 'Nicaraguan Spanish'),\n ('es-ve', 'Venezuelan Spanish'),\n ('et', 'Estonian'),\n ('eu', 'Basque'),\n ('fa', 'Persian'),\n ('fi', 'Finnish'),\n ('fr', 'French'),\n ('fy-nl', 'Frisian'),\n ('ga', 'Irish'),\n ('gl', 'Galician'),\n ('he', 'Hebrew'),\n ('hi', 'Hindi'),\n ('hr', 'Croatian'),\n ('hu', 'Hungarian'),\n ('ia', 'Interlingua'),\n ('id', 'Indonesian'),\n ('is', 'Icelandic'),\n ('it', 'Italian'),\n ('ja', 'Japanese'),\n ('ka', 'Georgian'),\n ('kk', 'Kazakh'),\n ('km', 'Khmer'),\n ('kn', 'Kannada'),\n ('ko', 'Korean'),\n ('lb', 'Luxembourgish'),\n ('lt', 'Lithuanian'),\n ('lv', 'Latvian'),\n ('mk', 'Macedonian'),\n ('ml', 'Malayalam'),\n ('mn', 'Mongolian'),\n ('my', 'Burmese'),\n ('nb', 'Norwegian Bokmal'),\n ('ne', 'Nepali'),\n ('nl', 'Dutch'),\n ('nn', 'Norwegian Nynorsk'),\n ('os', 'Ossetic'),\n ('pa', 'Punjabi'),\n ('pl', 'Polish'),\n ('pt', 'Portuguese'),\n ('pt-br', 'Brazilian Portuguese'),\n ('ro', 'Romanian'),\n ('ru', 'Russian'),\n ('sk', 'Slovak'),\n ('sl', 'Slovenian'),\n ('sq', 'Albanian'),\n ('sr', 'Serbian'),\n ('sr-latn', 'Serbian Latin'),\n ('sv', 'Swedish'),\n ('sw', 'Swahili'),\n ('ta', 'Tamil'),\n ('te', 'Telugu'),\n ('th', 'Thai'),\n ('tr', 'Turkish'),\n ('tt', 'Tatar'),\n ('udm', 'Udmurt'),\n ('uk', 'Ukrainian'),\n ('ur', 'Urdu'),\n ('vi', 'Vietnamese'),\n ('zh-cn', 'Simplified Chinese'),\n ('zh-tw', 'Traditional Chinese'))\nUSE_L10N\nFalse\nCOMMENTS_ALLOW_PROFANITIES\nFalse\nSTATICFILES_DIRS\n('/opt/graphite/webapp/content',)\nPREPEND_WWW \nFalse\nSECURE_PROXY_SSL_HEADER \nNone\nSESSION_COOKIE_HTTPONLY \nTrue\nDEBUG_PROPAGATE_EXCEPTIONS\nFalse\nCACHES\n{'default': {'BACKEND': 'django.core.cache.backends.memcached.MemcachedCache',\n             'KEY_PREFIX': u'**',\n             'LOCATION': ['51.16.48.94:11211'],\n             'TIMEOUT': 1200}}\nURL_PREFIX\n''\nMONTH_DAY_FORMAT\n'F j'\nSTORAGE_FINDERS \n('graphite_opentsdb.finder.OpenTSDBFinder',)\nLOGIN_URL\n\nSESSION_EXPIRE_AT_BROWSER_CLOSE \nFalse\nSTANDARD_DIRS\n['/opt/graphite/storage/whisper/', '/opt/graphite/storage/rrd/']\nLDAP_SERVER \n''\nTIME_FORMAT \n'P'\nMEMCACHE_HOSTS\n['51.16.48.94:11211']\nAUTH_USER_MODEL \n'auth.User'\nDATE_INPUT_FORMATS\n('%Y-%m-%d',\n '%m/%d/%Y',\n '%m/%d/%y',\n '%b %d %Y',\n '%b %d, %Y',\n '%d %b %Y',\n '%d %b, %Y',\n '%B %d %Y',\n '%B %d, %Y',\n '%d %B %Y',\n '%d %B, %Y')\nGRAPHITE_WEB_APP_SETTINGS_LOADED\nTrue\nAUTHENTICATION_BACKENDS \n['django.contrib.auth.backends.ModelBackend']\nEMAIL_HOST_PASSWORD \nu'**_'\nCARBONLINK_HASHING_KEYFUNC\nu'**_'\nPASSWORD_RESET_TIMEOUT_DAYS \nu'**_'\nCACHE_MIDDLEWARE_ALIAS\n'default'\nSESSION_SAVE_EVERY_REQUEST\nFalse\nNUMBER_GROUPING \n0\nSESSION_ENGINE\n'django.contrib.sessions.backends.db'\nCSRF_FAILURE_VIEW\n'django.views.csrf.csrf_failure'\nCSRF_COOKIE_PATH\n'/'\nLOGIN_REDIRECT_URL\n'/accounts/profile/'\nTEMPLATE_STRING_IF_INVALID\n''\nOPENTSDB_TREE\n1\nFLUSHRRDCACHED\n''\nDECIMAL_SEPARATOR\n'.'\nFIND_CACHE_DURATION \n1500\nIGNORABLE_404_URLS\n()\nLOCALE_PATHS\n()\nWHITELIST_FILE\n'/opt/graphite/storage/lists/whitelist'\nOPENTSDB_METRIC_QUERY_LIMIT \n4000\nLOGOUT_URL\n'/accounts/logout/'\nDASHBOARD_CONF\n'/opt/graphite/conf/dashboard.conf'\nTEMPLATE_DIRS\n('/opt/graphite/webapp/graphite/templates',)\nRRD_DIR \n'/opt/graphite/storage/rrd/'\nFIXTURE_DIRS\n()\nEMAIL_HOST\n'localhost'\nDATE_FORMAT \n'N j, Y'\nMEDIA_ROOT\n''\nCARBON_METRIC_PREFIX\n'carbon'\nDEFAULT_EXCEPTION_REPORTER_FILTER\n'django.views.debug.SafeExceptionReporterFilter'\nADMINS\n()\nLOG_METRIC_ACCESS\nFalse\nFORMAT_MODULE_PATH\nNone\nDEFAULT_FROM_EMAIL\n'webmaster@localhost'\nTHOUSAND_SEPARATOR\n','\nWEB_DIR \n'/opt/graphite/webapp/graphite'\nMEDIA_URL\n''\nDATETIME_FORMAT \n'N j, Y, P'\nMEMCACHE_KEY_PREFIX \nu'**_'\nREMOTE_FIND_TIMEOUT \n900\nDISALLOWED_USER_AGENTS\n()\nALLOWED_INCLUDE_ROOTS\n()\nUSE_THOUSAND_SEPARATOR\nFalse\nUSE_LDAP_AUTH\nFalse\nLOGGING \n{'disable_existing_loggers': False,\n 'filters': {'require_debug_false': {'()': 'django.utils.log.RequireDebugFalse'}},\n 'handlers': {'mail_admins': {'class': 'django.utils.log.AdminEmailHandler',\n                              'filters': ['require_debug_false'],\n                              'level': 'ERROR'}},\n 'loggers': {'django.request': {'handlers': ['mail_admins'],\n                                'level': 'ERROR',\n                                'propagate': True}},\n 'version': 1}\nSHORT_DATE_FORMAT\n'm/d/Y'\nRRD_CF\n'AVERAGE'\nWEBAPP_VERSION\n'0.10.0-alpha'\nSTATICFILES_FINDERS \n('django.contrib.staticfiles.finders.FileSystemFinder',\n 'django.contrib.staticfiles.finders.AppDirectoriesFinder')\nCACHE_MIDDLEWARE_KEY_PREFIX \nu'**_'\nSMTP_SERVER \n'localhost'\nFILE_UPLOAD_MAX_MEMORY_SIZE \n2621440\nWHISPER_DIR \n'/opt/graphite/storage/whisper/'\nEMAIL_BACKEND\n'django.core.mail.backends.smtp.EmailBackend'\nCSRF_COOKIE_HTTPONLY\nFalse\nDEFAULT_TABLESPACE\n''\nTEMPLATE_CONTEXT_PROCESSORS \n('django.contrib.auth.context_processors.auth',\n 'django.core.context_processors.debug',\n 'django.core.context_processors.i18n',\n 'django.core.context_processors.media',\n 'django.core.context_processors.static',\n 'django.core.context_processors.tz',\n 'django.contrib.messages.context_processors.messages')\nRENDERING_HOSTS \n[]\nALLOW_ANONYMOUS_CLI \nTrue\nSESSION_COOKIE_AGE\n1209600\nSETTINGS_MODULE \n'graphite.settings'\nUSE_ETAGS\nFalse\nDEFAULT_CACHE_DURATION\n1200\nLANGUAGES_BIDI\n('he', 'ar', 'fa', 'ur')\nDEFAULT_INDEX_TABLESPACE\n''\nINTERNAL_IPS\n()\nSTATIC_URL\n'/static/'\nEMAIL_PORT\n25\nLOG_RENDERING_PERFORMANCE\nFalse\nDASHBOARD_REQUIRE_EDIT_GROUP\nNone\nUSE_TZ\nTrue\nSHORT_DATETIME_FORMAT\n'm/d/Y P'\nPASSWORD_HASHERS\nu'**_'\nREMOTE_RETRY_DELAY\n300\nABSOLUTE_URL_OVERRIDES\n{}\nCACHE_MIDDLEWARE_SECONDS\n60\nDASHBOARD_REQUIRE_PERMISSIONS\nFalse\nDATETIME_INPUT_FORMATS\n('%Y-%m-%d %H:%M:%S',\n '%Y-%m-%d %H:%M:%S.%f',\n '%Y-%m-%d %H:%M',\n '%Y-%m-%d',\n '%m/%d/%Y %H:%M:%S',\n '%m/%d/%Y %H:%M:%S.%f',\n '%m/%d/%Y %H:%M',\n '%m/%d/%Y',\n '%m/%d/%y %H:%M:%S',\n '%m/%d/%y %H:%M:%S.%f',\n '%m/%d/%y %H:%M',\n '%m/%d/%y')\nGRAPHTEMPLATES_CONF \n'/opt/graphite/conf/graphTemplates.conf'\nEMAIL_HOST_USER \n''\nPROFANITIES_LIST\nu'*****'\nYou're seeing this error because you have DEBUG = True in your Django settings file. Change that to False, and Django will display a standard 500 page.\n. any body?\n. ",
    "bartschelfhout": "Hi,\nHave you manage to solve this issue? I am encountering the same problem with my data. I have the following retention schema\nretentions = 3s:5m, 10s:1h, 1m:7d, 15m:30d, 1h:3y\nand data shows fine when selecting the last 2 minutes to view. once I select the last minute, I get the following\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/django/core/handlers/base.py\", line 111, in get_response\n    response = callback(request, _callback_args, *_callback_kwargs)\n  File \"/usr/lib/python2.6/site-packages/graphite/render/views.py\", line 171, in renderView\n    image = doImageRender(requestOptions['graphClass'], graphOptions)\n  File \"/usr/lib/python2.6/site-packages/graphite/render/views.py\", line 377, in doImageRender\n    img = graphClass(graphOptions)\n  File \"/usr/lib/python2.6/site-packages/graphite/render/glyph.py\", line 185, in init\n    self.drawGraph(params)\n  File \"/usr/lib/python2.6/site-packages/graphite/render/glyph.py\", line 653, in drawGraph\n    self.consolidateDataPoints()\n  File \"/usr/lib/python2.6/site-packages/graphite/render/glyph.py\", line 996, in consolidateDataPoints\n    bestXStep = numberOfPixels / divisor\nZeroDivisionError: float division\nany help would be greatly appreciated.\n. ",
    "iksaif": "Looks good to me, could you rebase it ?. I think I would be fine with that as long as it's rebased, tested, and you provide a graph showing the improvements.. Is this still an issue with the latest HEAD ?. Note: this pull request isn't final but I'd like an initial feedback pn the concept  before writing the tests and double-check the maths.\nAdditional idea: plugins for functions would be great !\n. You would be surprised, without advertising this we already have 3 or 4 teams who came up with such ways to store histograms in Graphite but have hard time displaying and interpreting them.\nI'll go ahead and write tests and double-check the math and update this PR once ready.\n. Thanks for this post, it also shows that there is improvement to do on the Graph side. I'm sure that we could some up with a nice Grafana that could show that with a friendly UX.\n. For a given histogram over time (= one series per bucket) it returns the multiple series representing the summary of this histogram. In this summary you'll series representing the instant min, max, mean, and percentiles.\nIf don't want instant values but would like a window of time instead for this summary you can just apply a movingAverage on the input (I think).\nThe typical use case is SLOs and SLAs: we look at the distribution when debugging and we use the 99pctl as an overview but we don't care about percentiles for a single host so we need to compute it in Graphite.\n. To put some context: this is mostly useful to look at distributions across a large distributed system and is irrelevant if you have only a few hosts without much traffic, in this case you can do most of this work client side.\nWhile something like HdrHistogram would be interesting, you'll always have to bucketise client side for anything with serious traffic (let's say I have 100 memcached servers with 100kqps, I certainly don't want to send raw measurement to Graphite).\nIt's also certainly something worth looking at medium to long term, but short term we probably won't be able to do much more than using the existing protocol and storage layer.\nRelated to choosing buckets, I'm not sure why it would be hard ? We mostly know the distribution latency of most of our applications and the healthy and unhealthy ranges. Even then, with this proposal I believe that re-bucketing would mostly work (and those histograms are mostly useful for short-term data anyway).\nstatsd simply does not work for percentiles or counts because you can't aggregate percentiles and I mostly care about latency distribution for my whole system.\n(https://vimeo.com/173610069 was good at explaining this).\n. Sure it would work with a per-service statsd, in fact we already use prometheus for exactly that because it's way more powerful than statsd, but we found out that it introduced an additional layer of complexity that made it harder to provide a self-service monitoring system. Which is why having that done in Graphite without custom rules per applications is pretty appealing.\nRelated to retention policies, these are high level service stats used for SLAs, alerts and alike, which are usually never downsampled, so the fact that it's round robin database doesn't really matter. We get a bunch of counters from which can be extracted useful percentiles, and that's fine :) (That's how borgmon,  and prometheus are approaching the problem, and I've seen in working pretty well with a lot of people and systems :) ). \nEven with multiple retention policies this would work because these are monotonic counters so you can simply take the max when downsampling and apply the derivative when reading.\n. @Dieterbe yep I've seen the plugins and I hope to be able to adapt them soon. Shouldn't be terribly hard.\n. Even if you do that in a centralized place you'll need to either send a sample of your data. This sample can be either a fixed sized list of samples of bucketed histogram. In any case you need additional client side code and you loose precision, and that's fine, because you really can't send back the complete data set when you have 100kqps and thousands of machines. Do we agree on that ?\nThe main advantage of this approach is that people are already sending that kind of data today to Graphite but have a hard time using it. It also makes the interface simpler for users because they don't need to care about an additional statsd/prometheus to do the aggregation.\n. We deployed that in preprod recently, and it looks like there was a huge increase in latency since we merge this.\nI confirm that reverting this patch reduces the number of QPS on the leafs by 30 for a setup with 2 frontends and 3 leafs and a replication factor of 2. Enabling this option I would only expect traffic to double.\n. With 3 nodes and a replication factor of 2, even if it fetches from all replicas, it should only double the traffic, true ? In my case I've seen a 30x increase.\nMy query is simply carbon.{agents,aggregator}.*.metricsReceived.\nIt also looks like (but I might be wrong) that the RemoteFinder will return one leaf per path, but all of them will have the same query pattern, which would mean that you're effectively fetching this data multiple times (and I can't really find what would prevent that).\nThere seem to be a concept of \"bulk_query\" but since we create one node per metric and fetch all of them I'm not sure how it can work, even less when there is a 'extract_my_results' that explicitely filters the metric after running the query.\nAlso I don't believe readers are able to read multiple metrics at once (which they probably should). In the meantime we should probably change self.query = bulk_query or node_info['path']\n. Responding to my own comment: this is deduplicated by get_inflight_requests because the url is the same.\n. I'll try to debug the issue next week. I suspect this happens we you hit a graphite-web on a cache with a global query.\n. Ok, that's because the bulk request finishes while we send the request to the second replica, and thus is removed from the inflight requests. Because of that we do the bulk_request multiple times. I'll try to come up with a patch.\nRemoving del self.inflight_requests[url] fixes the issue so this confirms my theory\n. No, it isn't good as a permanent fix because it will leak memory. My patch simple use the cache with a small TTL. I'll hopefully be able to validate that it works tomorrow and send a merge request.\n. https://github.com/iksaif/graphite-web/commit/c98f246c4ec466c07de661216bcbab1af6ce15b4 seems to work for us. I'll leave it running this afternoon and send a PR tomorrow.\n. I don't think the cache is harmful and it also helps cases were the sub-queries are not done in the right order (which might happen even without REMOTE_STORE_MERGE_RESULTS)\n. I don't think it's worth it, and this cache is probably useful even when this setting is disabled.\n. Make a must better and simpler fix in https://github.com/graphite-project/graphite-web/pull/1666\n. Yes because I need this patch to be able to run the tests (https://github.com/graphite-project/graphite-web/pull/1666/commits/69b8da1679f0946691353920f5e07a272aa874d1). I can send if separately if you want.\n. \nHere are the \"benchmarks\" (log scale)\nI'll remove the setup.py changes right away.\n. Yes it is still relevant, the MutliReader should never make potentially AsynchronousReaders synchronous.\n. I'm not running Graphite with Python 3, but my tox command seems to use python3 to run setup.py\n. To add more context: this change is needed to simply be able to run \"tox\" on modern systems (such as my ubuntu box where python3 is default). Running tox in a virtualenv works, but is kind of counter-intuitive because tox will then install more virtualenv to run the tests.\n. With my fix I still had a ~30% penalty, which was better than the massive slowdown that I got without.\nCould you dump the infos logs ? This should tell us more about what's happening. (With my fix I was definitely doing only on request per replica instead of one per metric thanks to the bulk_query batching)\nAnything with a wildcard will be affected. A good way to debug is checking how many queries are generated by carbon.agents.*.metricsReceived (should be one per replica).\n. Could you send us your local_settings.py ? Most important settings are CARBONLINK* and REMOTE* and CLUSTER* variables.\n. I don't think this PR actually works and does what we want. The usually case is having 'holes' in the middle, not at the end. And when with happens you can't trust the intervals  (because they only have start end end, and do not report 'holes'). The previous did what we want, and I believe it's ok to have a small performance penalty, as long as this setting is not enabled by default.\n. The performance penalty here is totally fine, it's only a few milliseconds and that's expected if you set REMOTE_STORE_MERGE_RESULTS because you explicitly want to fill gaps by querying multiple replicas (= a least a remote one). While initially this setting would create an additional query per metric, with my fix it only does one query per glob (usually one).\nThe most common usecase for this is when you have REPLICATION_FACTOR>1 and a node goes down and come back up with holes. These holes will take hours to be fixed by carbonate, and in the meantime you want to make sure that you're service the correct date. The old implementation of REMOTE_STORE_MERGE_RESULTS + my fix did exactly that with < 10% performance penalty on my setup.\nMaybe we should have an additional setting (REMOTE_STORE_FETCH_REPLICAS = x) to state that we one to fetch multiple replicas at any time ? I can write a patch for that, it would be set to 1 by default.\n. Forget everything I said, I just spotted the ' if node.local' line 106.\nThanks for fixing this !\n. Looking at my graph I can't see any significant performance change (which is why I expected because I do not do local queries). Seems to work !\n. any news ?. This could potentially help us add support for X-Forwarded-For, LGTM\n. My issue with this is that not all backends use HTTP behind the scene (https://github.com/criteo/biggraphite for example calls Cassandra instead), and changing the API to add headers is kind of weird.\nWould there be a better / more generic way to pass the current request context ?\nOr maybe there are enough HTTP based finders/readers that we don't care ... You might also be interested in https://github.com/criteo-forks/graphite-web/commit/479958f2826c286e15f6d9182466b72aec75b7fd that I haven't had the time to send here. . LGTM. LGTM. The idea of the random.shuffle() is that it changes the first node that get contacted, this help spreading the load, on our setup we found out that one node would timeout way more than the others, and it was always the first node in the list.\nApparently this is breaking a ceres test, I'll fix that early tomorrow.. looks like it calls listdir in ceres less now because it doesn't try to get the intervals even when you don't need them, which is good.\nTest fixed now !. @obfuscurity test are fixed, do I get a final LGTM ? :). Done, also re-wrote the thing using grammar.. could somebody with complex aliasByNode queries validate that it works before I merge it ? :). let's merge and see. I guess you can mock  drawVTitle to validate that its being called with the correct argument. How much faster is the new version compared to the old one ?. As the function isn't trivial, the commit log and doctstring could eventually contain a rationale for the use of such function and/or some links.. The function itself :). Test failure fixed by https://github.com/graphite-project/graphite-web/pull/1797. Test failure fixed by https://github.com/graphite-project/graphite-web/pull/1797. Any suggestion of an alternative name for user_util ?. Created test_user_util.py. Somehow the file got lost, even on my filesystem. I'll push it again on monday. Sorry for that. Everything should be good now.. LGTM. I use 2.1.10 and it works just fine. Could you try in an isolated venv ?. @ybizeul ah, I don't use threads.. Would be easy to write a benchmark for grammer, but I strongly suspect that grammar parsing is blazing fast compared to the I/O made when rendering the target.. As said earlier, I don't think we should worry about performances here, most of the time is spent doing I/O anyway.\n. @ybizeul would you have time to write a small benchmark for that ?. @ybizeul I guess simply try to call grammar.parseString(target) ~10000 times with and without the patch and record the time :). @DanCech do you have an approximate ETA for this ? Just checking if we should start tagging an RC without it or not.. looks good ! added a few comments, nothing big :). I'm fine with the current trunk of graphite-web/carbon/whisper, it's what\nhas been running in production for almost a year. The external projects\n(carbonate, biggraphite, etc..) will quickly adapt after the release.\nWhile doing the release, could you create a wiki page describing the steps\n? That would help for the next one :).\nI'd just like to upstream\nhttps://github.com/criteo-forks/carbon/commit/0bed7c78c35e7b9777cb2a2f7c18a11e225e828b\nfor carbon first, which I'll do next week.\nAlso, do we have a list of breaking changes and upgrade instructions ? That\nwould be nice to advertise them somewhere.\nOn Sun, Feb 12, 2017 at 12:07 AM, Jason Dixon notifications@github.com\nwrote:\n\n/cc @graphite-project/committers\nhttps://github.com/orgs/graphite-project/teams/committers\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1823#issuecomment-279182463,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA_DAzXWyLcOAAuGgk4_5PflDEQVpsswks5rbj8VgaJpZM4L-VCD\n.\n\n\n-- \nCorentin Chary\nhttp://xf.iksaif.net\n. By the way, since the changes are massive for this release, would it make\nsense to tag a RC for each projects, advertise it, and accept only fixes\nfor the next few weeks on these branches ?\nOn Sun, Feb 12, 2017 at 7:46 PM, Dan Cech notifications@github.com wrote:\n\n@obfuscurity https://github.com/obfuscurity yes, the optimizations\ndiscussed in #1063\nhttps://github.com/graphite-project/graphite-web/issues/1063 are\nincluded in #1818\nhttps://github.com/graphite-project/graphite-web/pull/1818\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1823#issuecomment-279238852,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA_DA-joFpuqCdubdusYLvp3Gw6oJvM1ks5rb1OOgaJpZM4L-VCD\n.\n\n\n-- \nCorentin Chary\nhttp://xf.iksaif.net\n. To go forward, I propose:\n- create the 1.0.x branch on monday for each sub-project and tag a rc1\n- publish a pre-release (at least a github) and advertise it on the mailing\nlist / irc for people to test\n- wait two weeks until we decide if we need a rc2 or if we can go forward\nwith a full release\nOn Fri, Feb 17, 2017 at 10:54 AM, Denis Zhdanov notifications@github.com\nwrote:\n\nI think we should do that after merging that massive changes, indeed.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1823#issuecomment-280606523,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA_DA7tpvDXegPZsoUoGrXtlYEU9xzylks5rdW5PgaJpZM4L-VCD\n.\n\n\n-- \nCorentin Chary\nhttp://xf.iksaif.net\n. Good point, let's move this plan by one week then :).\nOn Thu, Feb 23, 2017 at 2:34 PM, Dan Cech notifications@github.com wrote:\n\nI'd be very hesitant to call it an rc without #1818\nhttps://github.com/graphite-project/graphite-web/pull/1818 or similar.\nI'm on vacation this week but will be back in action Monday and can push\nthrough any remaining issues with #1818\nhttps://github.com/graphite-project/graphite-web/pull/1818.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1823#issuecomment-281992443,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA_DAy4shpd47KCeR29WLbNFzVfj2fqdks5rfYrigaJpZM4L-VCD\n.\n\n\n-- \nCorentin Chary\nhttp://xf.iksaif.net\n. Let's tag whisper and carbon to 1.0-rc1 on monday, and tag graphite-web as\nsoon as https://github.com/graphite-project/graphite-web/pull/1818 is\nmerged.\nDoes that work for you ?\nOn Thu, Mar 16, 2017 at 9:36 PM, Denis Zhdanov notifications@github.com\nwrote:\n\nAccording to milestone\nhttps://github.com/graphite-project/graphite-web/milestone/2 major\nblocker is #1690\nhttps://github.com/graphite-project/graphite-web/issues/1690 - but we\nhave good news - fix is in #1818\nhttps://github.com/graphite-project/graphite-web/pull/1818 and under\nreview now.\n@kkdk5535 https://github.com/kkdk5535 - you can try to test master\nbranch with #1818\nhttps://github.com/graphite-project/graphite-web/pull/1818 applied and\nshare your results - it will make 1.0 release much faster. \ud83d\ude38\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1823#issuecomment-287183353,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA_DA7_O-SjTjPKQ24KamrRlF7RV0PoHks5rmZ1OgaJpZM4L-VCD\n.\n\n\n-- \nCorentin Chary\nhttp://xf.iksaif.net\n. If nobody disagrees today I'll branch 1.0.0 for carbon/whisper/graphite-web tomorrow and create a rc1 tag. Doing that now. I'll tag 1.0.0-pre1 to match the convention used by previous tags.. https://github.com/graphite-project/whisper/releases/tag/1.0.0-pre1\nhttps://github.com/graphite-project/carbon/releases/tag/1.0.0-pre1\nhttps://github.com/graphite-project/graphite-web/releases/tag/1.0.0-pre1\nLet's wait until April 1st before we see if we do an rc2 or not.\nProbably worth tweeting to encourage people to test, I don't think there is a good way to advertise pre-releases on pypi.. Can influencial twitter people encourage users to test 1.0.0-pre1 ? :)\nIt would be great to do a release next week if there isn't any blocker.. @deniszh there is https://github.com/graphite-project/graphite-web/releases I guess: pip install https://github.com/graphite-project/graphite-web/archive/1.0.0-pre1.zip. https://github.com/graphite-project/carbon/releases works too .. But it's true that there isn't one single page to do that.\nAnother thing: there are a few commits on master that we may want on 1.0.x (https://github.com/graphite-project/graphite-web/compare/1.0.x...master). I'll let you choose which ones.. Related to https://github.com/graphite-project/graphite-web/compare/1.0.x...master, what is our current merge path ?\nMerge PR to 1.0.x if they qualify, then merge back to master ?\nMerge everything to master, and cherry-pick to 1.0.x ?\n. Do you think it's realistic to target wednesday ?. LGTM\nLe 11 avr. 2017 22:13, \"Denis Zhdanov\" notifications@github.com a \u00e9crit :\nOK, I tagged releases already, prepared PKG_INFO and going to upload\npackages to PyPI. A little bit out of schedule, but...\nIt's the last chance to fix something, though.\nAny objections, corrections, etc? @obfuscurity\nhttps://github.com/obfuscurity @iksaif https://github.com/iksaif\n@DanCech https://github.com/DanCech\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1823#issuecomment-293386358,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA_DA0g-5Eyf__ACbd5NgvenGdTu6yVAks5ru97LgaJpZM4L-VCD\n.\n. Woohoo ! thanks for the hard word this finish to wrap up evertyhing. I posted https://news.ycombinator.com/item?id=14095347 (but it isn't exactly neutral :P)\n. Would it work to only use setuptools and to patch PYTHONPATH at the beggining of the file to contain /opt/graphite/ (when GRAPHITE_NO_PREFIX is not set) ?. Would it work to only use setuptools and to patch PYTHONPATH at the beggining of the file to contain /opt/graphite/ (when GRAPHITE_NO_PREFIX is not set) ?. Could you add to the commit message an example of data before and after ?. I think is is done. 1. Yes we need to tag carbonate can you do that ?\n2. I would exclude Ceres. I would include Grafana and statsd or collectd.. People have been asking me for a BigGraphite image, I'll be happy to base it on top of a Graphite image.. Cool, thanks for working on it. > Let's pick a date for release, fix date here, cherry-pick this to 1.0.0 and master and finally make a release!\nI guess early next week would work if we can finishing everything in the 1.0.0 milestone.. Any news from somebody with accessto graphiteapp.org? . REMOTE_PREFETCH_DATA = True seems to mitigate the issue. I'm trying to debug the issue on my setup because I think it will be quite hard to reproduce. I have it with and without USE_WORKER_POOL and with 10 hosts and a replication factor of 2. Ok, it looks like it's caused by the slaves values from other slaves. This is likely caused by 'remove noCache from backend requests'. I expect this to be enough:\ndiff\ndiff --git a/webapp/graphite/remote_storage.py b/webapp/graphite/remote_storage.py\nindex e50098b..6e3dbfe 100755\n--- a/webapp/graphite/remote_storage.py\n+++ b/webapp/graphite/remote_storage.py\n@@ -198,6 +198,7 @@ class RemoteReader(object):\n       ('target', self.query),\n       ('format', 'pickle'),\n       ('local', '1'),\n+      ('noCache', '1'),\n       ('from', str( int(startTime) )),\n       ('until', str( int(endTime) ))\n     ]. See\n https://github.com/criteo-forks/graphite-web/commit/667e6d5d89e523b51e4d66ac50ec38519a4ddb56\n https://github.com/criteo-forks/graphite-web/commit/bc3385aa515e23df9a393d606a8bfa093bfc530b\nWill send a PR tomorrow. https://github.com/graphite-project/graphite-web/pull/1884. Can you open a dedicated issue for that with the exact version that you are using ? \n@deniszh this patch was wrongly sent to 1.0.0 only. I'll spend some time on Monday to cherry-pick whatever we need on 1.0.0 and merge back everything that didn't make it to master.. With 9136 being the pid of one of the processes:\n$ sudo netstat -p | grep 9136 | wc -l\n408\n$ sudo netstat -p | grep 9136 | wc -l\n410\n$ sudo netstat -p | grep 9136 | wc -l\n411\n$ sudo netstat -p | grep 9136 | wc -l\n417. Current theory is that the memcached driver will create a set of connection per thread.\n- 4 connections per thread\n- 10 * 10 + 10 threads per uwsgi worker (=110)\n- 440 connections per uwsgi worker\n- 70 workers -> bazillions of connections.\nI'm pretty sure that the default number of worker is too big since in most situation Graphite will be run with uwsgi or similar which will probably have one process per request.. Drasticaly reducing the number of workers is a valid workaround. I think we should have more sensible defaults.. @DanCech  would you aggree to decrease the number of threads that are set by default to 1 per server and 1 base thread ?. Yes, I have REMOTE_PREFETCH_DATA=False (which is the default I believe).\nIn any case, since in most cases people will use graphite with uwsgi or similar, I don't think we should have more than one thread by backend by default (it also uses a lot more memory than before).. I think that's fine, you end up with between 2 and N threads (with N-1 being the number of backends). That's usually way enough since you're not going to process requests in parallel most of the time.\nRemember that the total number of threads is POOL_WORKERS_PER_BACKEND * len(CLUSTER_SERVERS) + POOL_WORKERS.. Updated :). Also see https://github.com/criteo/biggraphite/pull/259. (wait before reviews, some things apparently don't work yet). ok good now :). @deniszh : I'll probably use the same singleton.py in carbon too as there are a few singleton that could benefit from it. woops. Using tons of wildcard in Graphite is an anti-pattern IMHO.\nThat being said, did you try changing  fnmatch._MAXCACHE in local_settings.py ?\nI also wonder if https://github.com/criteo/biggraphite/blob/master/biggraphite/glob_utils.py is affect or not.. Could you trying measuring the effects of raising _MAXCACHE ? If it doesn't change the memory consumption much and improve performances, that's probably good enough.\nRegarding the way of storing things, when you end up aggregating a lot of metrics, you should some time consider sotring also the pre-aggregated value to make read queries lighter. Not sure it applies to your usecase.. IMOH it's a bug fix, even if the Graphite syntax makes it hard to do that kind of thing.\n\ngroupByNode() returns iowait\nasPercent() returns asPercent(iowait, bla..)\naliasByNode() should work on the first argument of asPercent, which it does by working on iowait. And iowait happens to contain a single node.. I'm not sure I understand what you mean, what would you expect the behavior of aliasByNode(.., 4) to be in your case ?. The thing is that given asPercent(iowait,sumSeries(sumSeries(servers.node0*_graphite.system.cpu.{user,iowait,system,irq,softirq,steal}))), there is no reason to use the second argument of asPercent instead as node of the first one. I think use the first one makes much more sense.\n\n. This thing is that aliasByNode() has been working like this in the 1.0.x for quite somet time already, so changing the behavior again (to something that doesn't really make sense) wouldn't be great. LGTM. We could start by making the UI optional, and by making part of Graphite more easilly reusable by other tools. Ideally Graphite UI should not have to re-implement the functions or the grammar, but would just import the relevant files.. See #1943 . The format changes are \"caused\" by running autopep8 on the code that I modified. Some bits can be manually tweaked later. I'll take care of the lint errors during the day.. Starting to work on that in https://github.com/iksaif/graphite-web/tree/pr-clean-finders. @DanCech : yes, graphite events translate to annotations. Are you sure that Grafana can store them ?\n@deniszh : the API for events is quite CRUD. I would have a default event plugin using Django's database, and custom ones using whatever people want. If there was an API for that, we would implement it biggraphite.. I guess that if Grafana is going to get that feature, I'll be happy to deprecate it from Graphite.. Can you try with https://github.com/graphite-project/graphite-web/pull/1946 ?. Cool, next step is 2to3 + 3to2 :)\nCan we wait for https://github.com/graphite-project/graphite-web/pull/1958 and rebase this one ?. I think that is safe and won't really affect performances. Maybe add a comment before the lock to explain why this is needed. I don't with it's for the width of the graph. Isn't this controlled by from/until ? For me it was just to avoid drowing / tranmitting points that would not be useful.\nThe current code doesn't seem to work if there is an empty timeseries. Maybe valuesToLose = min(int(nudge/series.step), len(series) is better ?. Closing for now as I lost my patch by mistake. I also fix the finder that was returning an empty timeseries so I guess we will see next time it happens.. See https://github.com/graphite-project/graphite-web/issues/1943\nSorry for the super large PR, but it was super tricky to isolate the changes.\n. Ok, how does it look like now ?\nThis is still some potential for cleanups but I'd like to get that in first. Some ideas for future cleanups:\n\nUse concurrent.futures for ThreadPool and futures (this would give us better abstraction for futures).\nDouble check all the usages of FetchInProgress\n. Just sent a bunch of fixes and added a few unit tests.. So, what do you all think ? I'd merge that to master, and then merge all the potential regressions before the next release.. ok, there is only one more failing test, but it really doesn't seem to be related. What do you think ?. I'll give a chance to @DanCech  to look at this :). Nop, I would not include DYNAMIC_ROUTER in 1.0.2 :). :+1: . LGTM. LGTM. Cool, I'll review that early next week and think about how this can integrate with BigGraphite. High level comment: this should be abstracted in the existing finder/reader API (even if there is a new API) to make sure others can re-implement that using other backends.. What I mean is: why do we need a separate TAGDB while this could be just be done inside the finders (adding a tag argument or something like that).. I guess that is one way of seeing it. IMHO this could also be a lib mixin that would be used by a finder named WhisperAndRedis for example. But sure. I'll review the actual code tomorrow :). Hey,\nI think I commented on both requests. I'm currently in the Himalayas for\nthe next 10 days but I'll double check when I'm back.\nSorry if I forgot to submit the form\n\nLe 9 ao\u00fbt 2017 6:01 AM, \"Dan Cech\" notifications@github.com a \u00e9crit :\n\n@iksaif https://github.com/iksaif did you have a chance to take a look\nat this yet?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/2002#issuecomment-321118860,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA_DAz87ignm4isjGcDkbEUIIEC8JD3Vks5sWP3xgaJpZM4OehUP\n.\n. Thanks for your work. Others: please review :). LGTM. LGTM. cc: @geobeau who is trying to do that with zipkin. would be great to add an associated unit test to clearly show that this fixes.. That would certainly make it more readable.. Would be great to advertise 1.1.0-pre1 a little bit to make sure that people try it before the release.\nI'll try to release a version of BigGraphite that works with this (without custom TagDB for now) when we release 1.1.0 .. On the filesystem:\n\n\u2514\u2500\u2500 _tagged\n        \u251c\u2500\u2500 200\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 97e\n        \u2502\u00a0\u00a0     \u2514\u2500\u2500 tags-diceroll2;foo=bar5;label=value.wsp\n        \u251c\u2500\u2500 6d8\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 5fe\n        \u2502\u00a0\u00a0     \u2514\u2500\u2500 tags-diceroll2;foo=bar2;label=value.wsp\n        \u251c\u2500\u2500 800\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 a22\n        \u2502\u00a0\u00a0     \u2514\u2500\u2500 tags-diceroll;foo=bar;label=value.wsp\n        \u2514\u2500\u2500 d16\n            \u2514\u2500\u2500 0fa\n                \u2514\u2500\u2500 tags-diceroll2;foo=bar;label=value.wsp\nIn the database:\ntags_series:\n50|800a22102fd965a88dc9588446cc17838ac77d795f9b845b836dcc2476b0324e|tags.diceroll;foo=bar;label=value\n51|d160fae1a6a882cc459d0a456c71958614cebc801b39ece02754ee24daa02ab2|tags.diceroll2;foo=bar;label=value\n52|6d85fe9b3066405a43f869d085b914fdabd145ccdb5aff28e04f7c4c8470cd06|tags.diceroll2;foo=bar2;label=value\n53|20097e6e32bbddca626bbbfe325e1399367128a05c8764333df08e4362767d54|tags.diceroll2;foo=bar5;label=value\nI tried seriesByTag('label=value') and get empty results. I added a print in the whisper finder and it seems to try to find these:\n('find nodes', <FindQuery: tags.diceroll2;foo=bar2;label=value from Wed Oct  4 13:59:39 2017 until Thu Oct  5 13:59:39 2017>)\n('find nodes', <FindQuery: tags.diceroll2;foo=bar5;label=value from Wed Oct  4 13:59:39 2017 until Thu Oct  5 13:59:39 2017>)\n('find nodes', <FindQuery: tags.diceroll2;foo=bar;label=value from Wed Oct  4 13:59:39 2017 until Thu Oct  5 13:59:39 2017>)\n('find nodes', <FindQuery: tags.diceroll;foo=bar;label=value from Wed Oct  4 13:59:39 2017 until Thu Oct  5 13:59:39 2017>). Ok .. my graphite web was pointing to the wrong directory (/tmp/whisper/whisper ..).\nSorry for the noise !. I'm honestly not sure what would be the implications of that. But it's worth trying, it can be reverted in a minor version if it causes too much trouble.. Isn't this likely to break quite a lot of metrics that currently return points ?. Forget my comment, I though we were using the xfilesfactor from the underlying whisper files.. I'm out until next week. Will try to look on Monday.\nLe 3 nov. 2017 19:05, \"Dan Cech\" notifications@github.com a \u00e9crit :\n\n@iksaif https://github.com/iksaif I'd love to get your feedback on this\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/2093#issuecomment-341766563,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA_DAwVgBH-0o6MH2xsKtrgLJtoNYwzgks5sy0fbgaJpZM4QJtJh\n.\n. This looks good. I'm mostly afraid about plugins compatibility here as it will be quite hard to support 1.0 and >1.1 with all these changes in the finder/reader API.\n\nDo you think there would be a way to keep some of the symbols just for backward compatibility ? (Thinking about FetchInProgress and alike).. Ok, makes sense.. I'll make biggraphite compatible during the week. Shouldn't we keep POOL_WORKERS to cap the number of threads ?. Yes (as one remote host = one finder)\nLe 11 nov. 2017 10:19, \"Dan Cech\" notifications@github.com a \u00e9crit :\nYou mean to use it as a maximum if someone has a really large number of\nfinders?\n\u2014\nYou are receiving this because your review was requested.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/2103#issuecomment-343651762,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA_DA_UnVFOSQ0Mpzi1fhKT6UY6Go11tks5s1Wa4gaJpZM4QThRH\n.\n. \ud83d\udc4d . Anything left to do there ?. Do you think wecan close this now ?. :+1: . Why is our requirements.txt not compatible with python 3 ?. Do they really need to be pinned ? Isn't pip smart enough to see that some versions don't support Python2 ?. \ud83d\udc4d  great !\nTime to make biggraphite Py3 compatible now :P . setuptools is fine imho. :+1:. Let's go for a branch, it master doesn't diverge too much it will be just a matter of merging so it won't be a big issue.. When you configure a Django app in Sentry they explain you what to put in your local_settings.py file, so the documentation is there already :)\nHere I'm just making it so people don't need to import the settings.py file.. need to fix the tests. tests are happy on my machine, let's see if travis agrees. . Will rebase on monday, would like to get it in the next release as this really makes the error messages more useful, in particular in case of timeouts.. Rebased. I changed the messages a little bit following @DanCech feedback. better now. I'll try to implement it in my finder to see how much more code it is :). Here it is https://github.com/criteo/biggraphite/pull/317. Let 's close it for now. Wait for https://github.com/graphite-project/carbon/pull/727#issuecomment-355980527 to be merged, I'll include these changes too. Arf .. you're right, the order do matters because the replication factor is checked after. Please ignore this one for now, I'll send a fix for carbon.\nI'll make it return a tuple like before, this is the real performance fix.. pushed tentative fix, let's see if travis is happy on both sides. Note that we plan to add tags support to https://github.com/criteo/graphite-remote-adapter very soon, at least for writes. That would work nicely with this. \nOur graphite-remote-adapter should hopefully be more up to date than the one in prometheus/documentation/example. I'll try to write more doc for it and advertise it a little bit more.. Exactly, also when you have multiple Prometheus but a single Graphite cluster you want to be able to control the prefix (as a way to namespace the metrics and not pollute every prometheus instance). I set tagdb to None because the few places using tagdb outsife of explicit tag code would check that. I'll thing about something better tomorrow if you are really against this one :). better :). Arg .. I was sure I checked the return types but I must have been interrupted just when I was about to do it .. sorry for that.. I tested it locally plugged to carbon, it errors as expected. Last time we did something like that I think it broke aliasByNode for some people, do you see anything that would break doing this for all functions ?. Ok, the code you suggested looks good\n. Exactly, the signature didn't change, but now it works correctly when you give it a pattern :). Two time series, but still one single seriesList :). > but I'm much less keen on coupling ever more tightly with django,\nPer view/method/service stats really need to be done at the framework level. If it ever moves to another framework the metrics will change, but that's fine. Using django-prometheus gives us most of the metrics we want for free\n\nfind, render and processing pipeline\n\nSure, I'll add something that doesn't depend directly on the view. I'll probably instrument find(), fetch(), and the associated \"multi\" versions. Probably some tags methods too. Likely using the database name as a label/node.\nFYI, here is how it currently looks like: metrics.txt\n. The django related metrics already have Django in their names. Would not\nmake much sense for process_fds for example :)\nLe 22 f\u00e9vr. 2018 14:24, \"Denis Zhdanov\" notifications@github.com a \u00e9crit :\n\nI also not really like more Django, but it's better to have some metrics\nthen no metrics, right?\nMaybe default prefiux should be switched to graphite.django. ?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/2240#issuecomment-367677439,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA_DA0zHf1radSf91jie9-L7S4Y_GRe1ks5tXWjGgaJpZM4SO49s\n.\n. Here it is with a few basic metrics on top of what django provides for http endpoints. Maybe.. I'll experiment with django-prometheus insternally then, it's doable to integrate it using local_settings.py only. I agree with making REMOTE_BUFFER_SIZE optional. I'm pretty sure this will have performance regressions. Doing it in two loops (first getting all the generators, then expanding them) would maybe work.. FYI: It's not only RRD, it's also a bunch for third-party readers (mostly biggraphite). I won't have a stable  internet connection in the next three weeks so I can't validate your patch, but it looks like it would work (maybe add some code comments to explain why it is done this way).\n\nThanks. I think it's still missing comments to explain why it is done this way. Else it's likely that the code will be changed again at some point.. Please add a comment in the code explaining why you don 't use a generator, else in ~6 month somebody will try to switch back to a generator without knowing why it wasn't one in the first place :). \ud83d\udc4d. > * Target small to medium installations (big installs can be covered by Metrictank, Biggraphite, and Clickhouse).\nI think it would be nice to have something that works easilly for smalls installs but still can be extended for big installs. Ideally with the same components.\n\n\nOfficially deprecate python carbon daemon and replace it with go-carbon. Please note, that go-carbon is a separate project with own maintainers, not sure should we use it as is and contribute to its development or fork it.\n\n\nWhich is why I'm a bit afraid about that... Plus it makes it harder to share code between graphite and carbon. But considering the workforce that we have I do not have better suggestions.\n\n\nGet rid of Django - it was (and still) a constant source of incompatibilities and installation issues. (OTOH - will we have the same issues with e.g. Flask then? Or maybe we just strictly should use LTS versions?)\nGet rid of state in graphite-web - or make it at optional, at least (i.e. separate rendering and dashboards/tree-view, as was done in graphite-api).\n\n\nI'm all in to simplify graphite-web. It's nice to keep a minimialistic console view to debug it though but I don't think dashboards are super useful these days.\n. Protocols and clients are now \"pluginized\", which will allow custom ones to be writtens.. I'm not sure why we need \n+    del scandir\n+    del os\n+    import os as scandir\nMaybe add a comment ?\nThe canonical way to import scandir seems to be:\n```\nUse the built-in version of scandir/walk if possible, otherwise\nuse the scandir module version\ntry:\n    from os import scandir, walk\nexcept ImportError:\n    from scandir import scandir, walk\n``. uppercase variables are usually for constants. Simplyema` here would be fine.. There is quite a bit of duplicated code in these tests ? Could you try refactoring that ?\nHaving a helper function called in the various tests and putting some of the values (like the dates) into global constants might help. Looks good to me :). can you add a comment explaining why we need this version ?. maybe time to read requirements.txt content here. Add a pip install -r requirements.txt (or pip install -e .) instead ?. It's unclear to me why this is needed since we already had asynchronous communication here. Is it mostly to parse the response ?\nAlso, this is mostly a setting for the the remote storage finder, maybe prefix it with REMOTE_ ?. General comment: this is adding a lot of code that should really be isolated in remote.py (for me, all this remote finder thing is just \"one\" of the finders that we can have).\nMaybe it's a good time to start moving more code to remote.py ?. Put each branch in its own private function for clarity ?. For another review: good use case for a decorator . is this missing a \"return\" or append ?. Make it a set at the begining ?. Why not from multiprocessing.pool import ThreadPool ?. If there is no need to pin it I guess I would leave it without version. https://github.com/graphite-project/graphite-web/issues/1843. https://github.com/graphite-project/graphite-web/issues/1843. The API is basically multiprocessing.Pool. It's not documented because they want people to use concurrent.futures.ThreadPoolExecutor (Python 3 only). It's exposed so it's supposed to be stable. indentation seems off here. A lot of code seems common between this, movingMin, and probably other moving* functions. Do you think you could try to refactor it ?. Could you add a small unittest for this one ? (There is already a quite similar function in functions.py with some unit tests). I'm not sure why it's 0.01 here ?. nit: you could have named it just \"TestCase\" (if django's TestCase was imported differently). Can't find this link, is it up to date ?. right, I'll find a solution to avoid that. good catch. Ok, totally forgot that. Fixed.\nIn the current situation that also means that all 'remote' finders must support prefetch() or have an assert on settings.REMOTE_PREFETCH_DATA, so I'm not totally happy with it, I think there should be a way to do it another way, but I don't want to change the behavior too much. So I'll keep it this way for now.. Right, I assumed this didn't really matter because we rarely mix local and remote together, but I'll make sure we do that as late as we can. New patch coming on monday :). I agree that calling super() on object isn't super useful, I'll remove that. done. Oh right, fixed.. I think it'd like to keep it that way for now. It's not trivial to change.. wrong indentation, fixed. We only set remote_done to False if we don't query with the same start/end/now.. changed, more readable. Are you use you're looking at the right else ?. right, this was going to break. Ah right...\nI feel like the mix of FetchInProgress + Threads + generators will need to be cleaned to use a single way to doing async stuff.. What do you think about putting the redis implementation in a separate repository ?. I'm not sure what this does here, can you add a few comments ?. this seems to be a copy of the previous code, maybe make it a function ?. What are the implcations of this change ? Will it break people using = in their metric names ?. Is there a specific reason why you don't use the django orm ?. does that work with pgsql / mysql / sqlite ?. Please add more documentation to the methods of this class (because people will want to implement plugins). Is performance really important when using the local database ? And the ORM isn't that bad.. That's reasonable for the first version, I could later try to port it to the ORM to see how it looks. Do you think it would make sense to have graphite-common now, shared between carbon and graphite for that kind of things ?. if path[-2:] == '\"}' and '{' in path:\n  return self.parse_open_metric(cls, path)\nelse:\n  return self.parse_metric(cls, path)\nWill be cleaner / more reusable. Maybe add disabled = False in BaseFinder ?. Maybe add a link to https://docs.djangoproject.com/en/1.11/topics/logging/#examples ?. Yes, it's a logging/django thing, it always work :). maybe this could be a continue here to avoid the else going to much to the right ?. many levels of indentation here, any luck this could go in a function ?. part of this code seems duplicated between tags and value. Can you add an enum for that ? (to avoid typos). Can you add an enum for that ? (to avoid typos). maybe add some function validations here ? (types are known, etc..). name was a [], which obviously doesn't work well with re.match(). You can reproduce that with time(time) (instead of time(\"time\")). fixed. prometheus-metrics/metrics isn't nice. Maybe having prometheus/metrics, prometheus/write, prometheus/read isn't really an issue ? I could just create a prometheus apps that will later be extended to add /read and redirect its /metrics to django-prometheus. ",
    "dongweiming": "@iksaif time long past. this repository had deleted. i have submitted a new pull request https://github.com/graphite-project/graphite-web/pull/1763 . ",
    "MasterScrat": "The axis of the graph wouldn't be time... you would still be manipulating and graphing time series data.\n. ",
    "imkin": "Has fix been merged? \n. ",
    "wolfedale": "This is how I'm sending metrics.\n@time = Time.now.to_i\nserver.send(node, value, @time)\n. Yes, that was the case. I'm closing it.\n. ",
    "ocervell": "Any updates on this ? I am running into the same problem.\n. Issue is 3 years old. Any update ? \nMy company moving out of Graphite because of this.\nI was still hoping that this issue would get resolved since I'm using Graphite in my own projects.. ",
    "Exocomp": "\nYou have a large Graphite install and/or are just stuck with slow disks, such that data enters carbon-cache and then doesn't reach disk for a number of minutes.\n\nI'm hitting the above scenario. I have to lower MAX_UPDATES_PER_SECOND because it kills the disk's throughput but that causes the cache to increase. Then queries start returning null for a few mintues because the cache is not being checked.  \u00af\\_(\u30c4)_/\u00af. ",
    "AsenZahariev": "@deniszh , i saw that you included this issue into milestones 1.1.0 and 1.2.0, can you confirm in which version it will be definitely included ? Seems that we are struggling with the same problem.\nThank you!. First thank you for the quick answer. Just curious, because we have a large cluster and very heavy queries.\nLet me describe in short what we have and what we done so far so you can get the picture of my curiosity.\nWe have 4 graphite clusters running on 0.9.15( i will lie you about the commit version) combine these servers are receiving around 4Mil metrics per minute. On these nodes are running all carbon components (relays(5) and caches(10) per node) also with graphite webapp using memcached for 5 minutes(600seconds). In front of all there is LoadBalancer. Relays are using const hashing for distributing the metrics and each relay can sent metrics to all caches (10caches x node , 40 caches combine).  No aggregation. All are running on pypy. Which btw helped a lot with minimizing the utilization of CPU and RAM for carbon`s relay and cache. Each carbonlink_host is directed to their local caches and instances.\nWhat we have done to increase the reading speed for Grafana and overall stability is that we build a separate box only with graphite webapp latest version from the master. \nConfiguration of the graphite webapp box we put a local  memcached with configured policy (e.g. 0,60 ; 7200, 120; 21600,180; and so on). \nOn cluster_servers directive we put the IP addresses of the 4 nodes with the respective port.\nThe first issue we encounter is that the queries started to timeout so we put a big values and increase the retries.\nWe started to use POST request ( REMOTE_STORE_USE_POST = True) \nUsing the new option (REMOTE_BUFFER_SIZE = 1024 * 1024), and yes it helped a lot! \nOn the graphite nodes we only enable REMOTE_PREFETCH_DATA.\nI will be forever grateful if this threat can become a guide for building sustainable graphite cluster, because we search and experiment  a lot before even thinking to start threat here.\nP.S.\n1) This is one of cluster , the other one is even more ...heavy \n2) We have two instances of Grafana ( 2.5 and 4.4.6) both with 700 dashboards and around 5k-6k of graphs , with crazy queries using multiple wildcards (*) \n3) Seyren v1.5 with nearly 400 checks. \n4) We can't update the graphite nodes to newer version. \nKind regards,\nAsen Z.. Sorry, IIRC ? I'm little bit new :) \nAnyway, do you have any other recommendation/best practices for such setup like mine ? . Hey Denis,\nWe switched back to POST since the post is having some limitation. In general is perfect for small queries, but for something with multiple wildcards not so much.\nAbout \"REMOTE_PREFETCH_DATA\" yes we saw that change and we only are using it on our graphite nodes (version 0.9.15). Do you think there is some limitation? We use 0.9.15(back-end graphite webapp nodes) and 1.1.0(front-end graphite web app) ? . @deniszh Thank you for your feedback! I can confirm that with the above settings the getting graphite webapp in front of your graphite cluster with the settings  we have(of course, once again, it depends on your environment/infrastructure), the reading is better. I have a couple of more questions ,but there are going to be in a separate thread. Cheers!. ",
    "mwtzzz": "I just wanted to add my two cents. We also are running a large graphite installation. This problem has been apparent for a while, but was tolerable because we were using nvme-backed storage. But I recently converted to iops-provisioned EBS volumes (because we were losing historical data every time an nvme drive bombed out) and the problem has been exasperated. \nIt's problematic for us because a lot our monitoring and auto-scaling stuff relies on being able to retrieve timely metrics from the graphite front-end. \nI should note that for us, some metrics  current values are retrievable from the cache, while others aren't. I don't know if this is because of a hashing problem or because of the sheer number of metrics in the cache at any given time. But in either case, the fact that it retrieves some of them means it shouldn't be a big deal to update the code to retrieve any of them, right? In our case, we're not using wildcards, if that makes a difference.. @piotr1212 Ok, if this issue #629 is about metrics for which a .wsp file does not exist yet, then my problem is different. Sorry to bother. . Since the odd behavior seems to occur beyond a day, maybe it has something to do with my storage schema\n[hosts]\npattern = .sum.[^.]+$\nretentions = 1m:1d,15m:10d\n. @DanCech nvm, I figured it out. it's due to a mismatch in the retentions for \"sum\" (given above), compared to \"sum_all\" which has defaults to a different retention schedule.  Making them the same and resizing will fix this. .... Also if we display metrics with scaleToSeconds the two different metrics will match. . ",
    "fhossain": "@SEJeff - sorry I dropped the ball on this as notifications were going to a different email. Added the unit test as requested.\n. @obfuscurity - it would be great if you can get this into a patch.\n. @obfuscurity this change is accompanied by by unit tests, self contained and it's got a clean health from CI. Isn't that good enough :-) \n. Just wanted to give a thumbs up on this change. I checked this into our own 0.9.x branch and released to our production instance. This solves a long standing complaint of a node selected from wildcard metrics get different colors in different graphs. With sortByName() all the graphs have consistent coloring now for a given node. Awesome! I will check back in a week and contribute tests if that helps to merge to master.\n. @paitel I submitted this pull request https://github.com/graphite-project/graphite-web/pull/1055. Seems like it is too naive and needs to be more involved. Can you comment?\n@SEJeff - FYI\n. ",
    "tuxinaut": ":+1: \n. @obfuscurity great :smile: \n. ",
    "justdaver": "Hey Jarrett,\nDid you ever find a solution for this problem? I'm experiencing the same problem, have one master graphite-web server configured with CLUSTER_SERVERS set to 2 backend carbon-cache servers. Queries directly to a single carbon-cache servers are super fast, while querying via the main graphite-web server is extremely slow.\nI have 2 carbon-cache daemons running on each carbon-cache server and have the following in local_settings.py:\nCARBONLINK_HOSTS = [\"127.0.0.1:7012:1\", \"127.0.0.1:7022:2\"]\nOn the graphite-web server I have no carbon-cache daemons running and have the following in local_settings.py:\nCLUSTER_SERVERS = [\"192.168.0.11\", \"192.168.0.12\"]\nHas any one else out there run into this problem before? Am also not sure if I'm doing something completely wrong but from what I've read online this is the method a separate graphite-web server would hook into multiple carbon-cache servers, but performance is almost unusable.\nI'm using graphite-web 0.9.12\nThanks!\nDave\n. ",
    "Comradin": "Any chance, this will be fixed in 0.9.13?\n. Hmm, but why is this issue then still open if it is fixed in master?\nAny news on when 0.9.13 will finally arrive?\n. ",
    "pauled23": "Can you try the same curl but with \"&from=-20min\"?\nOn Feb 28, 2014 2:28 AM, \"pythiannunez\" notifications@github.com wrote:\n\nHi! I'm the original poster. Thanks for your help!\nI've applied it to my graphite installation (0.9.12), and it's still doing\nthe same\n$ curl \"http://localhost/render/?target=constantLine(21)&format=json\"\nGraphite encountered an unexpected error while handling your request.\nPlease contact your site administrator if the problem persists.\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 111, in get_response\n    response = callback(request, _callback_args, *_callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 132, in renderView\n    timestamps = range(series.start, series.end, series.step)\nTypeError: range() integer end argument expected, got float.\nIt's a standard 0.9.12 installation. Maybe the fix just works in 0.9.x?\nI guess I applied it correctly...\n$ grep \"value, value\" /opt/graphite/webapp/graphite/render/functions.py -B\n3 -A 1\nstart = timestamp( requestContext['startTime'] )\nend = timestamp( requestContext['endTime'] )\nstep = (end - start) / 2.0\nseries = TimeSeries(str(value), start, end, step, [value, value, value])\nreturn [series]\n\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/pull/641#issuecomment-36330460\n.\n. \n",
    "ctavan": "+1 Works great for me. Would be great if this could be merged!\n. ",
    "kevwilde": "+1 works, would love to see this in the next release.\n. ",
    "jeblair": "This change recently merged into 0.9.x which lists Django==1.3 as a dependency in requirements.txt\n. Additionally 0.9.x has the following problem with Django 1.3:\nFile \"/usr/local/lib/python2.7/dist-packages/graphite/account/urls.py\", line 20, in \n    urlpatterns = patterns('graphite.account.views',\nNameError: name 'patterns' is not defined\n. ",
    "Dunedan": "What I didn't noticed when opening this PR is that a function (sortByName()) got included recently (#419 and #522) which solves the same problem, but making it optional to use the consistent ordering. #224 contains some background about the rationale of making it a function instead of making it the default behavior.\n. ",
    "toni-moreno": "Sorry \nI've found the error:\nOn the original installation I configured the finder parameter \"STORAGE_FINDERS\" on the settings.py instead of  do it on local_settings.py and the reinstall process rewrited the config to the \ngraphite.finders.standard.StandardFinder.\nSo I fix by add in local_settings.py the STORAGE_FINDER \nSTORAGE_FINDERS = (\n        'graphite.finders.ceres.CeresFinder',\n)\nAnd restart the apache process.\nThx!!\n. I've investigated on how to migrate data to sqlite to mysql \nhttps://gist.github.com/toni-moreno/9779979\nAfter that I notice that exista a table \"dashboard_dashboard\" with json formated dashboards and \nmysql> describe dashboard_dashboard;\n+-------+--------------+------+-----+---------+-------+\n| Field | Type         | Null | Key | Default | Extra |\n+-------+--------------+------+-----+---------+-------+\n| name  | varchar(128) | NO   | PRI | NULL    |       |\n| state | longtext     | NO   |     | NULL    |       |\n+-------+--------------+------+-----+---------+-------+\nand another with graph URL's in account_mygraph\n```\nmysql> describe account_mygraph;\n+------------+-------------+------+-----+---------+----------------+\n| Field      | Type        | Null | Key | Default | Extra          |\n+------------+-------------+------+-----+---------+----------------+\n| id         | int(11)     | NO   | PRI | NULL    | auto_increment |\n| profile_id | int(11)     | NO   | MUL | NULL    |                |\n| name       | varchar(64) | NO   |     | NULL    |                |\n| url        | longtext    | NO   |     | NULL    |                |\n+------------+-------------+------+-----+---------+----------------+\n4 rows in set (0.00 sec)\n```\nI've created a script to help import/export from mysql Graphite DB.\nhttps://gist.github.com/toni-moreno/9807776\n. I've tested import/export between 0.9.10 and 0.10.0-alpha and all seems ok .!!\n. Hi !!!\nI've discovered a web interface  embedded in  graphite  URL/admin/: but  it seems like some static files weren't served. \nHow can I fix it?\n\n. I've configured in my /etc/apache2/sites-enabled/graphite-vhost.conf , the following DJANGO_ROOT with both, \"media\" and \"static\" path... but no fixes  the problem.\n```\n        # XXX In order for the django admin site media to work you\n        # must change @DJANGO_ROOT@ to be the path to your django\n        # installation, which is probably something like:\n        # /usr/lib/python2.6/site-packages/django\nAlias /media/ \"/usr/local/lib/python2.7/dist-packages/django/contrib/admin/media/\"\n    Alias /media/ \"/usr/local/lib/python2.7/dist-packages/django/contrib/admin/static/\"\n    Alias /static/ \"/usr/local/lib/python2.7/dist-packages/django/contrib/admin/static/\"\n\n```\n. I've finally found somebody who had the same trouble it seems to be in the django versions upper than 1.4  and I haveDjango (1.6.2) \nhttp://obfuscurity.com/2014/01/Graphite-Tip-Django-Admin-Workaround\nI've applied this apache rules and now seems to work fine\nRedirectMatch ^/admin(.*)admin/([^/]+)/([^/]+)$ /media/$2/$3\n        Alias /media/ \"/usr/local/lib/python2.7/dist-packages/django/contrib/admin/static/admin/\"\n. OK !!! it runs OK .. Thanks !!!\n. I've imported successfully a  0.9.10 over 0.10.0-alpha version with this tool !!!\nWhat I can not understand is relationship between the complete json data imported and \"template\" that version 0.10.0-alpha is showing us.\n. Thanks... I've tested in a test machine with 0.10.0-alpha  version and works OK, but not in my production 0.9.10 version...\nI should to upgrade it.\n2 more questions:\na) Can I configure Legend to put a suffix on the M/K/G prefixes ( something like...)\ncactiStyle( \"metric\",\"si\", \"bps\" )  and show \"Gbps\" better than Gi ?\"\nb) What about column alignement.\nThe code says ( NOTE: column alignment only works with monospace fonts such as terminus.) \nbut courier doesn't align !!\nHow can I align ?\n\n. I'm working on master branch ( 0.10-alpha) .\n\nAnd no way to align.. can you send me a picture of  a cactiStyle() function aligned ?\nPerhaps I need an extra python - font related package ?\n. I'have the same problem in a 0.9.10 version but also in 0.10.0-alpha. ( next screeshont is from 0.10)\n\nwith this graph definition:\n\nIf I force metric Alias with a fixed legth it also build picture unaligned\n\nAnd the result.\n\nThis last version with  Python 2.7.3 and very closed pip package versions than you uses ( you can see at the pip freeze).\nhow have you defined your graph?\n```\npip freeze\nDjango==1.6.2\nGnuPGInterface==0.3.2\nMySQL-python==1.2.5\nPIL==1.1.7\nPyYAML==3.11\nTwisted==11.1.0\napt-xapian-index==0.44\napturl==0.5.1ubuntu3\nargparse==1.2.1\nceres==0.10.0\nchardet==2.0.1\ncommand-not-found==0.2.44\ndjango-tagging==0.3.1\ngunicorn==18.0\nhttplib2==0.7.2\niotop==0.4.4\njockey==0.9.7\nkeyring==0.9.2\nlanguage-selector==0.1\nlaunchpadlib==1.9.12\nlazr.restfulclient==0.12.0\nlazr.uri==1.0.3\nmock==1.0.1\nnose==1.2.1\nnvidia-common==0.0.0\noauth==1.0.1\npexpect==2.3\npycairo==1.8.10\npycrypto==2.4.1\npycups==1.9.61\npycurl==7.19.0\npyparsing==1.5.7\npysmbc==1.0.13\npython-apt==0.8.3ubuntu7.1\npython-debian==0.1.21ubuntu1\npython-graphite-dashgen==0.1.1\npython-memcached==1.47\npytz==2013.9\npyudev==0.13\npyxdg==0.19\nreportlab==2.5\nsimplejson==2.1.6\nsupybot==0.83.4.1\nsynaptiks==0.8.1\ntornado==3.1.1\ntxAMQP==0.4\nufw==0.31.1-1\nunattended-upgrades==0.1\nusb-creator==0.2.23\nvirtualenv==1.7.1.2\nwadllib==1.3.0\nwhisper==0.9.10\nwhisperctl==0.2.0\nwsgiref==0.1.2\nxkit==0.0.0\nzope.interface==3.6.1\n```\n. Yes !!! @brutasse now it its working , with and without group().\n\nWhat perhaps will improve a lot the look&Feel with a reasonable little refactoring (as I commented some days ago) is to add a \"unit\" parameter to the cactiStile() function to aboid a \"M/G/K\" or \"Mi/Gi/Ki\" that has no sense without units.\n\nby example:\ncactiStyle( \"metric\",\"system\", \"units\" )\nThat will show : \nG<units> if units != null \nor G / Gi if units == null `\nSo: \n- cactiStyle( \"metric\",\"si\", \"bps\" )  will show   XXX.XX Gbps /Mbps/..\n- cactiStyle( \"metric\",\"bynari\", \"bytes\" )  will show  XXX.XXX Gbytes /Mbytes/..\n- cactiStyle( \"metric\",\"bynari\", \"b\" )  will show  XXX.XXX Gb/Mb/..\nWhat do you think about ?\n. I 've added a  PR  to close this issue !\n:)\n. Hi Guys.\nfirst I would like to say thanks to @obfuscurity @esc @brutasse @timbunce @steve-dave @SEJeff @torkelo @drawks @timbunce  and all the other collaborators who created and are maintaining this great tool. \nI'm new in the project ,and I would like to help you as much as possible (I will dedicate as time as my son my wife and my job let to me) .\nI think there was in this thread  very good ideas on how to manage the github repositori ,so I prefer ( by now) not enter in this discussion.\nWe are  planning to build a big graphite platform and I'm installing from the current master repository  for graphite-web, whisper and,carbon. But we need some new features and we are coding these.\nWe  have  some proposed PR's  to add a few interesting new features on the 0.10 milestone.  (  I would like contribute from these release) \nhttps://github.com/graphite-project/graphite-web/pulls/toni-moreno\nI have experience on proprietary tools like  ( CA-Wily Introscope ) , that gave me some good ideas to improve Graphite-web, and I would  like to contribute with these new features.( https://github.com/graphite-project/graphite-web/issues/721 )\nI'm waiting now on your feedback and perhaps a new \"Roadmap\"  discussion and instructions for what to do to help  all you.\n. Hi @esc  I would like to help you with 0.9.13 , but we are already working with current master branch 0-10-beta and with our PR from 0.10.\nIt is too late for us revert to previous version and remake the new features we need to  work. And as I said previously I have not  time enough to spent on this release. \nWhat I will do sure (mandatory on my job)  is a complete testing of the current master releases for graphite-web, carbon and whisper ( on a big production environment) and I will report you ( and fix myself when I can and I have enough acknowledgement) all whatever we  can see.\nAt the same time while  you are  working on both 0.10 and  0.9.13 I will try to follow all your discussions and take part on them waiting to resume the 0.10 development. \nI hope you can let me take part on the  discussion and development for define the 0.10-beta milestone and roadmap for the following releases.\n. @esc OK! I will wait !! Thanks a lot !! \n. I'm not logged and I can select dashboards and see them\n\n\nThe only authorization rule applied is you can not SAVE templates if user doesn't belongs to \"dashboard-editors-group\", even being super user\n\n. I've read again and the local_settings.py comment  is showing us that  this authentication is only for modify dashboards.\n```\nIf set to True, users must be logged in to save or delete dashboards. Defaults to False\nDASHBOARD_REQUIRE_AUTHENTICATION = True\n```\nthere is some way to anable authentication also for view dashboards  and or  composer?\n. I've added code to enable a full authenticated graphite if needed, only tested with  django user database backend.\nhttps://github.com/graphite-project/graphite-web/pull/686\n. Hi, guys.\nI've been working with this improvements from April and all seems ok.\nWhat should I do to merge into the master branch?\nThanks !!\n. I'm Sorry by this ( perhaps too late) comment.\nI'm trying to choose among all the available  dashboards  for graphite. But I like a lot the  composer and dashboard editor embedded in graphite I thing is  faster and easier work on composer  than export data to other external dashbord.\nAnd IMHO perhaps could be a good idea to add a dashboard viewer into graphite which handle metadata from server sending it to a http/js templates while rendering is done by the client with just a Rickshaw or highcharts ( better for me ).\nIn this approach you will have all metadata available and no need to break existing json format now.\nIf finally is done:\n-Server will have less render CPU load.\n-Users will have more accurated data ( the exact values can be shown with tooltips as shown in the picture.)\n\n- Users will be able to hide/show some of metrics by graph /( by only click over the legend)\n- Users will be able to do Zoom on a subset of data.\n- autorefresh will be able to  fetch only one point  per graph in the dashboard  ( this will save  a lot of CPU cicles \n- User will be able to navigate across dashboards by linking them i\nWhat do you think about ?\n. Hi ! \nThe new login form is working ok  at least on: IE11, firefox 28 and Chrome 34\n. Somebody has tested with LDAP Authentication backend ?\nAnyway this PR could be  used with django user administration only.\nAny feedback on this?\n. Somebody has tested with LDAP Authentication backend ?\nAnyway this PR could be  used with django user administration only.\nAny feedback on this?\n. Hi, guys.\nI've been working with this improvements from 1 moth ago and all seems ok. \nWhat should I do to merge into the master branch?\nThanks !!\n. Yes !!! It would be great !!!!\n. ;) hi to all.\nwhat about this PR?  I'm still waiting any collaborator who can  merge it.\nCould be good for us to have this new filter system to more flexible searching way.\nThank you very much!\n. I will rebase it ( and rewrite or fix it if needed)  in next few days.\nWe need this function to add selector combos to a dashboard front-end which uses dashboard templates.  \nThis new function  should be like \"/metric/find\"  but with powerful filtering options.\nIs a bit different what does /metric/find because \"find\" ( as I know)  always returns all values on the the last node in the query. ( just for a tree construction) \nWhy is important in templated dashboarding?\nIf you have a distribution tree metric like that.\n<environment>.<state>.<hostname>.<product>.<metric_of_the_products>\nby  example:\nwebservers.production.webserver001.apache.*\nwebservers.production.webserver001.exim.*.\nSuppose we have 500 webservers under webservers.production,  500 have installed apache and 5 of them has  also installed a mail system \"exim\".\nWe have a dashboard front-end based in templates ( by example a exim template )  and we should only choose the hostname to render the exim  dashboard, and we need to query what hostnames user could select for this dashboard.\nwhen you make a query with metric/find to get all host with exim you can do:\nhttp://<graphite>/metrics/find/?query=webservers.production.*&format=treejson\nIn this case we will get all 500 webserver names, when indeed what we only need is the 5 hostnames which have installed exim.\nWith this one:\nhttp://<graphite>/metrics/get-node/?query=webservers.production.*.exim.*&node=2\nwe will have only the 5 webserver names which has exim metrics.\nWe can also add to our dashboard front-end combos for \"environment\" and another one for \"state\", with this new function we will show only environments and states which have exim products  installed.\nTo make this new function I took the \"metric/find\" as model. So it  should support also clustering as metric/find does if you think it won't be able to support remote querying I should fix it.\nAny advice would be appreciated\n. Hi @obfuscurity , I've finally rebased and fixed to support search metrics in both local and non local only ( by default) as find_view or expand_view does.\nI've also justified you why  this new function can help a lot to everybody who are developing dashboards front-ends to simplify programming dashboard templates when you need node selectors with predefined metrics.\nDo you need I do something more to this PR in order to be merged ?\nThank you very much!\n. Indeed at first I thought do it as you say but after review the code I saw that find_view was complex enough and seems more oriented to  build  treeviews. \nI also could see that exist another related method expand_view outside find_view. So I decided to separate also the get-node method.\nPerhaps what could be good idea If you decide maintain this method it is rename this function to something more suitable according current naming scheme.\ndo you agree?\n. Hi  guys. \nAny Plans to merge this PR to current master , please???\nI need it to get node info to our dashboards. And will be usefull also for any other dasboard frontend.\nThank you very much !!\n. Hi @obfuscurity @brutasse .\nDo you prefer if I can modify  the \"expand\" API to get this data?\nsomething like:\nhttp://graphite/metrics/expand?query=webservers.production.*.exim.*&groupByNode=2\nIs there any other API where better to place this new functionality?\n. I've added new features so I will put new screenshots.\nGraphite Login\n\nSynchronized croshhair\n\nSynchronized zoom\n\nLinks to other (related) dashboards\n\n. Hi @obfuscurity @SEJeff I'm open to move to another graphing javascript API . Like http://www.jqplot.com/  or any other you choose instead of highcharts.\nAbout maintainability: I've made the viewer by only copy and modify the current dashboard.js  code it is almost the same code , so not much difficult to maintain. \nIMHO The viewer is only a way to only see data a complement to the composer that should be maintained also because of its powerful and easy way to made  dashboards with a few clicks  thanks to extjs window based UI  .\nIMHO The viewer and a embedded user control (perhaps like django)  is the only piece of the puzzle which graphite web needs to become the powerful way to build and maintain dashboards for a lot of different people.\nAnyway we can test and contribute to any other graphite dashboard project if you can recommend us one with these needed features.\n- separated viewer and composer.\n- Viewer based on JS to get more accurate data point values\n- login / user control.\n- authorization control for dashboads.\n- templating dashboards.\n- real time graphite tree filtering \n- auto refresh querying only new values.\n- shared zoom\n- shared cross-hair\n- links between  dashboards.\nThanks a lot!\n. I agree, and also day .. hour...\nIt can be also interesting define time window to avoid data from outside this window time.\nBy example summarize only \"workdays\" ( avoiding local holidays ) , \"workhours\" ( to avoid nigths.. ) by  example...\nIt can help to compute SLA's better.\n. Hi !\nI would like to add a few interesting new  features on the 0.10 milestone \nhttps://github.com/graphite-project/graphite-web/pulls/toni-moreno\nhow can I add them ?\nI'm also working on a new \"dashboard viewer\" but I'm waiting to some collaborator feedback to complete the work. \nhttps://github.com/graphite-project/graphite-web/issues/721\n. Hi @obfuscurity this error is still happening on current master branch . where did you merge this fix?\nThank you very much\n. I \nThis commit doesn't fix a .\naliasByMetric(averageSeries(some.series.*.metric))\nthe result is still \"metric)\". \n. Hi @torkelo \nWhen  consolidation is done  ( whatever the request was -- maxDataPoints , summarize()) if subformat were \"XmsYC\"\nyou should get N datapoints in this format:\n[ X ( timestamp in ms) , Y  ( averaged) , C ( # values when averaged) ]\nIn this case you could compute the exact Total value in the client side  as: \nTotal = y[0]*C[0] + y[1]*C[1]+.....y[N-1]*C[N-1]\nIf you need too the exact max/min/avg values even after any consolidation/summarization process we could use:\nsummarizeExtendedJson(any series)\nIn this case you will get N datapoints in this format.\n[ X , Yavg , Ymax, Ymin , C( Number of time points from to get computed Y's) ]\nAnd you will be able to compute from client side from any arbitrary window time the exact value for:\nSum/Total =Yavg[0]*C[0] + Yavg[1]*C[1]+.....Yavg[N-1]*C[N-1] \nAverage=(Yavg[0]*C[0] + Yavg[1]*C[1]+.....Yavg[N-1]*C[N-1]  ) / ( C[0] + C[1] + .... + C[N-1] )\nMax = Max ( Ymax[0],Ymax[1],....Ymax[N-1])\nMin = Min ( Ymin[0],Ymin[1],...Ymin[N-1])\nwith summarizeExtendedJson() you will get all data needed to compute what you need and we can show users in each point in the dashboard how many original points where there and the max/min values of each interval time when consolidation done.\nWhat do you think about?\n. Hi @cdavis , @brutasse  , @SEJeff , @obfuscurity,  @pyr , @esc \nWe would like to share this proposal with all you. If you let me I can begin to work in the most useful and easy task.\n- add XmsY subformat.\nAdding more information into the json returned data-points  I suppose it  could be a little more complex, is because of that I prefer you guys were all in agreement with that. \nAnd Of course as milestone for next 0.10.X or 1.0.0 or next stable release, Because of It should require some months to finish this work.\nWhat do you think about?\n. Hi @torkelo , I understood you  but I think this proposal is more generic and  can give us solution for me and also for you.\nIn one hand, why statistics per datapoint ?\nThis is the first question I did myself first time I begin work with  CA APM ( http://www.ca.com/us/opscenter/ca-application-performance-management.aspx)  but after 5 years work in with  I discovered that there are metrics in average with a stable behaviour but when you inspect max values per Interval you discover some times strange things like ie peaks.\nThanks to this behaviour in the APM we are discovered a lot of timeout issues in our systems.\nIn the other hand, if you get data as you say. you will have max, min, total,average statistics for all points in the interval you choose.\nIf you do a zoom in the client side , (As I know ) chart libraries redraw itself the data without querying  from the source again because client side has all needed data. \nIs so, how do you will recompute the statistics for the zoomed window time if you don't have max, min,count on each data-point ?\n. Hi  @obfuscurity @torkelo  I've done a little PR to improve performance on the client side with the new proposed subformat parameter that modifies format per datapoint (dp)\nPerhaps could be a better suited name for this parameter jsondpformat.\nif  you agree I will add a commit  the the PR with the new parameter name.\nMetadata should be for each series so I suggest a new parameter jsonseriemd , and depending on the data you need you can add more or lest metadata for serie.\njsonseriemd=\"stats,XXX,YYYY,ZZZZ\" \nstats could be a good name for what @torkelo needs [max,min,total,avg,current]\nAre you agree?\n. Hi @cdavis  , @brutasse , @SEJeff , @obfuscurity, @pyr , @esc @torkelo \nAs I told you I've renamed subformat to jsondpfmt \nhttps://github.com/graphite-project/graphite-web/pull/980\nIn next days I have plans to add a new json subparameter jsonseriemd to add any metadata we need  to each serie.\nI propose to name \"basicstats\" what you need @torkelo , so in the future we can add extendedstats with other kind of statistics data, ( percentiles, stdev) etc.\nSo this would be the returned data to this query\nhttp://<graphite>/render?format=json&jsondpfmt=XmsY&jsonseriemd=basicstats&from=-6h&until=now&target=XXXX\n[ \n  {\n    target: \"XXXXXX\",\n    datapoints: [[1,2],...],\n    dpfmt=\"XmsY\",\n    metadata: {\n           basicstats { \n               max: 123,\n               min 10,\n               total: 21312312312,\n               avg: 50,\n               current: 20\n           }\n    }\n]\nAre you agree?\n. Hi @torkelo, @cdavis , @brutasse , @SEJeff , @obfuscurity, @pyr , @esc\nI've just ready tu push a commit  with a new jsonseriemd json subformat \nhttp://<grafite>/render?format=json&jsondpfmt=XmsY&jsonseriemd=basicstats,dummy,notexist&from=-6h&until=now&target=XXX\nWith this request you will get for each serie a metadata array with data taken for each one of the list \n- jsonseriemd=basicstats,dummy,notexist\n\nAs metadata \"notexist\" is not an metadata registered function a log will appear in the rendering.log\nError no metadata type notexist\nPreviously to doing the commit , would you like to discuss on the  name of the metadata functions \"basicstats\" , what content to do and the returned format ? \n. Hi @torkelo I've finally pushed the change.\nAs soon as graphite-web developers accept and merge it , I could begin to build a  new PR to grafana to add support to this 2 new parameters if you agree.\n. Hi @obfuscurity \nAs I said in https://github.com/graphite-project/graphite-web/issues/976 , and yourself proposed \nI've added a expandable way to add multiple metadata per serie and also I've added two metadata functions.\n- basicstats : to compute basic statistics per serie before any consolidation is done when maxDataPoints are requested. ( avoiding lost of statistical information as @torkelo asked for)\n- dummy : only for testing purposes. \nYou will get metadata in this way.\n\nDo you need I do something more to this PR in order to be merged ?\nThank you very much!\n. Feel Free to choose better suitable names for these parameters, is because of that I've open first an Issue to discuss about these questions.\nhttps://github.com/graphite-project/graphite-web/issues/976.\nAnyway I will show you why I chose this naming scheme and any suggestion will be appreciated.\nIt could be weird. but I've tried  to design expandable and easy to understand, parameters. (also backwards compatible).\nabout  Naming:\njsondpformat\nchosen with:\n- \"json\" prefix to easily suggest it has only effect with format=json,\n- \"dpformat\" as suffix  to indicate only format of the datapoints.\ndatapoint now has 2 possible values  YXs (the default and current format) and new XmsY (IMHO anybody who are developing with charting libraries , could understand easily the difference).\n```\nperhaps we can change X by t ( of time) and Y by v ( of value) so :\nYXs would be \"VTs\" \nXmsY would be \"TmsV\"\n```\nIn this case format should be chosen ( one or other not both ) \njsonseriesmd\nchosen with:\n- \"json\" prefix to easily suggest it has only effect with format=json\n- \"seriesmd\" as suffix because we can compute things in the server side related to series.\nIve coded \"basicstats\" because if   @torkelo  and I  need to compute statistics to improve the grafana dashboard.\nbut anybody could need another kind of computed data in addition to basicstats, is because of that I've chosen to add metadata types to in csv format to anybody could select as they need.\nI hope you can understand better my design but I'm open to change if needed .\nThank you very much.\n@brutasse perhaps could be good continue this thread on the https://github.com/graphite-project/graphite-web/issues/976. Are you Agree?\n.  Hi @cdavis , @brutasse , @SEJeff , @obfuscurity, @pyr , @esc, related to this issue I've finally coded it  with this PR, and I hope that could be merged.\nI of course understand that this change is not priority on your plans but , could be useful for all dashboard front-ends developers. \nCould you give me some feedback in order to do changes in code if needed , to adapt as much as I can this great project ?\n. Hi @brutasse , I understand you are agree with functionality but not the format, if all team members agree  I will begin to do change\njsonseriemd=x,y,z\nby\nmedatata=x&metadata=y&metadata=z\nin the meanwhile  could you give me the replacement parameter name  for \"jsondpformat\"  and the options  YXs , XmsY ?\nThaks!\n. Hi @brutasse I've already changed the name an query mode for metadata as you suggested.\nAny idea on the  \"jsondpformat\" replacement?\nI've maintained the output format and the 'basicstats' metadata generator.\nhttp://<graphite_url>/render?format=json&metadata=basicstats&metadata=dummy&metadata=z&jsondpfmt=XmsY&from=-6h&until=now&target=XXXX\nAnd the output is :\n\n. @brutasse , OK ! I will do (again) ASAP .\nIn order to avoid rewrite code twice , can you give me your replacement for jsondpformat and specifications on how it should work ?\nThx!\n. Thank you @brutasse  to let me add this new features.\nCorrect me if I'm wrong\nI should  make :\nOne PR adding parameter jsondpformat=\"XmsY\"\nhttp://<graphite>/render?format=json&jsondpformat=XmsY\nAnd  another separate PR with new parameter generic metadata=xxxxx\nhttp://<graphite>/render?format=json&metadata={true,cur,avg,max,min,sum}\nIsn't it?\n. Hi @brutasse I've finally leave this PR for the  jsondpformat parameter. I've also rebased it to have only one valid commit.\nI will be enormously grateful if you could merge it.  ( I need  urgently to fix issues importing data with IE8 and  highcharts, ... that my customer has a requirement).\nIn next days I will do a new PR with parameter metadata as you suggested some weeks.\nThank you very much.\n. Hi @brutasse Of course I hope you will merge it because  you think is a good improvement also for other dashboard projects and not for my customers.:wink:\nOnce merged,  I have plans to build a PR for this new parameter for grafana ( open source project at which I've already contributed).\n@obfuscurity , I had focused , metadata only for json  based format, but we can open  a new discussion about  metadata if you wish in a new PR. I would like to contribute writing this code in the way you think is better for graphite, so will be grateful if you say me how do you like to query metadata and how do you expect to get response from graphite\n. Hi @obfuscurity  , I'm happy to see your comment. feel free to simplify/refactors as your own PR.\nThank you very much\n. Hi @leoleovich I've seen this PR  and  I've been looking for something like this, but I would like to know if you can add metric permissions as regexp ?\n@brutasse there is any plans to merge this PR or any other which give us  at least metric filtering per user with regexp ?\nWe are discussing how to add security to grafana\nhttps://github.com/grafana/grafana/issues/1313       \nbut any approach seems not to be good enough in the frontend.\nI'm evaluating  cost of developing a per user metric filtering solution for grafana + graphite  but i would like support from graphite/grafana project owners/main contributors.\nWhat do you think about ?\n. Hi @obfuscurity. I'm working with current master  branch.\nAre these two parches medged on the current master branch ?\n. Hi @bmhatfield I'll be happy to test it once merged to master.\n. Hi.\nI would like to remember , that I did a PR some months ago (https://github.com/graphite-project/graphite-web/pull/980) and also opened a discussion (https://github.com/graphite-project/graphite-web/issues/976)  on how to add more datapoint formats to the render side to help  external developers to get data easier to different javascript libraries . And also extended metadata not available at client side.\nWhile we are working with highcharts and flot these new parameters jsondpfmt,jsonseriemd could also be used by you @chicagobuss .\nI invite you to resume the discussion and I'll be happy to help you.\n. @brutasse I agree that  ?format=json_dygraph is more explicit than ?format=json&jsondpfmt=dygraphs  or ?format=json&jsondpfmt=XmsY but with this second way client side frontend like grafana can make the same query across different graphite versions even if not supporting the new format and check  if supported by only check the existence of a datapoint  as I explained (https://github.com/graphite-project/graphite-web/pull/980#issue-44665207)\nif(serie[i].dpfmt && serie[i].dpfmt='dygraph' ) \u00b4{\n // dygraph format supported\n// do something with returned data\n} else {\n//old version of graphite doesn't support dygraph\nconvert_format()\n// do something with returned data\n}\nIn other way as format=json_dygraphs is not known you wil get de default format this is a png image.\nAs a workarround the dashboard fronted developers should maintain a registry with supported formats on each graphite version and check graphite version explicitely.\nIMHO the proposed way by adding a new jsondpfmt=xxxxx is backward-compatible and also more scalable and more mantenible for both frontend and backends developers\n. Yes !!! \nSorry @steve-dave I didn't see before. \nI've changed and now it is logging in the correct place.\n. Hi @brutasse.\nIt seems like no effect for the settings.TIME_ZONE.\n```\nDJANGO_SETTINGS_MODULE=graphite.settings django-admin.py shell                                                                                                                                 Python 2.6.6 (r266:84292, May 27 2013, 05:35:12)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-3)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n(InteractiveConsole)\n\n\n\nfrom django.utils import timezone\nprint(timezone.now())\n2015-04-20 06:48:17.605693+00:00\nfrom django.conf import settings\nprint(settings.TIME_ZONE)\nEurope/Madrid\n```\n\n\n\ndate from \"date command, is ok ! but not timezone.now() from graphite.\n```\ndate\nMon Apr 20 08:48:47 CEST 2015\n```\n. I'm working with 0.10 (commit 8f13d44d446acfaecefb438f1f4b8f56e5164092) . \nI've tested forcing \"tz\" in the query but I got  the same result\nhttp://graphite/render?format=json&from=4/01/2015&until=-&target=summarize(my.system.cpu.percent-active,'1day','avg')&tz='Europe/Madrid'\n[{\"target\": \"summarize(my.system.cpu.percent-active, \\\"1day\\\", \\\"avg\\\")\", \"datapoints\": [[12.180165230020865, 1427846400], [9.8991865472222234, 1427932800], [8.4890914388888916, 1428019200], [8.545845312499992, 1428105600], [8.5893371986111049, 1428192000], [11.584696202777787, 1428278400], [14.573881442361127, 1428364800], [11.976672966666678, 1428451200], [11.967281797222219, 1428537600], [18.55253356875, 1428624000], [7.5416060506944405, 1428710400], [7.184546814583328, 1428796800], [10.414414825694418, 1428883200], [11.440549245138893, 1428969600], [10.391664684027786, 1429056000], [6.3419650625000017, 1429142400], [9.4203433758765573, 1429228800], [6.7883954076388857, 1429315200], [7.427704547916667, 1429401600], [9.0221278432989589, 1429488000]]}]\n. Yes .. I have pytz\n```\npip freeze |grep pytz\npytz==2014.10\n```\n. Any workaround ? perhaps force LANG environment in http.conf with SetEnv ??\n. Um.. in these two lines the from/until values are already in seconds as timestamp , isn't it.\nSo , the : [ local date ]  to [ timestamp ] conversion seems to be done here.\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/views.py#L300-L308\nIn my timezone I expect to have from value  for \"4/01/2015 00:00\" = 1427839200  (http://www.epochconverter.com/)\n\nbut I got 1427846400 ( +120 minutes after)\n. Hi @brutasse \nI've tested the parseATTime() method and it is not returning a timestamp.\n```\n\n\n\nfrom graphite.render.attime import parseATTime\nimport pytz\ntzinfo = pytz.timezone(settings.TIME_ZONE)\nfromTime = parseATTime('4/01/2015', tzinfo)\nprint(fromTime)\n2015-04-01 00:00:00.554059+02:00\n```\n\n\n\nIt seems to return a Date with correct TimeZone (+02:00) set.\nThe timestamp conversion mistake should be place after that... I will continue investigating... \nThank you !\n. I've added two log lines at \nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/functions.py#L2984\nto get more detailed info on time conversions.\ntimestamps = range( int(series.start), int(series.end), int(series.step) )\n    datapoints = zip(timestamps, series)\n    log.info(\"REQUEST_CONTEXT TIME: (%s , %s)\" % (requestContext['startTime'],requestContext['endTime']) )\n    log.info(\"SERIES TIMESTAMPS: results for: %s (%s ,%s, %s)\" % (series.name,series.start,series.end,series.step))\n1) Query with MM/DD/YYYY.\nFor this query:\nhttp://graphite/render/?format=json&from=04/1/2015&target=summarize(my.system.cpu.percent-user,%271day%27,%27avg%27)\nwe got this logs:\nTue Apr 21 10:39:37 2015 :: REQUEST_CONTEXT TIME: (2015-04-01 00:00:00.849499+02:00 , 2015-04-21 10:39:37.849448+02:00)\nTue Apr 21 10:39:37 2015 :: SERIES TIMESTAMPS: results for: dwily.pro.c1.s1.wastest.system.cpu.percent-user (1427839260 ,1429605600, 60)\nAnd this output:\n\"datapoints\": [[null, 1427760000]....\ntimestamp conversion is not an exact value (00:00:00.849499)..  when converted to timestamp it got 1427839260  instead of 1427839260, but result is not correct alignement\n2) Query with timestamp\nwhen this query is done:\nhttp://graphite/render/?format=json&from=1427839200&target=summarize(my.system.cpu.percent-user,'1day','avg')\nWe got this log: \nTue Apr 21 10:46:33 2015 :: REQUEST_CONTEXT TIME: (2015-04-01 00:00:00+02:00 , 2015-04-21 10:46:33.341698+02:00)\nTue Apr 21 10:46:33 2015 :: SERIES TIMESTAMPS: results for: dwily.pro.c1.s1.wastest.system.cpu.percent-user (1427839260 ,1429606020, 60)\nAnd this output: \n\"datapoints\": [[null, 1427760000]...\nIn this case also we got 1427839260 instead of 1427839200 , but final alignment is not done well.\n3) Query with timestamp. (- 1 second and AlignedToFrom=true)\nwhen this query is done:\nhttp://graphite/render/?format=json&from=1427839199&target=summarize(my.system.cpu.percent-user,'1day','avg')\nwe got this log\nTue Apr 21 10:51:48 2015 :: REQUEST_CONTEXT TIME: (2015-03-31 23:59:59+02:00 , 2015-04-21 10:51:48.839702+02:00)\nTue Apr 21 10:51:48 2015 :: SERIES TIMESTAMPS: results for: dwily.pro.c1.s1.wastest.system.cpu.percent-user (1427839200 ,1429606320, 60)\nAnd this output:\n\"datapoints\": [[null, 1427839200]...\nNow it seems to be working fine..\n. Hi guys.\nThere is any plans to merge this PR in master ?\n. After switch to Django 1.7\n. .root:graphite-web > pip freeze\n....\nDjango==1.7\ndjango-tagging==0.3.6\n....\ngraphite-web==0.10.0a0\n....\nwe got this new error:\nroot:graphite-web > django-admin.py syncdb --settings=graphite.settings\nTraceback (most recent call last):\n  File \"/usr/bin/django-admin.py\", line 2, in <module>\n    from django.core import management\n  File \"/usr/lib/python2.6/site-packages/django/core/management/__init__.py\", line 68\n    commands = {name: 'django.core' for name in find_commands(__path__[0])}\n                                      ^\nSyntaxError: invalid syntax\n. After switch to 1.6 syncdb worked fine. \nBut I've finished all my new config and I got and Apache error when I connect to the http://graphite_url/\nThis is the apache error.log \n[Fri Jun 19 09:25:45 2015] [error] mod_wsgi (pid=20146): Target WSGI script '/opt/graphite/conf/graphite.wsgi' cannot be loaded as Python module.\n[Fri Jun 19 09:25:45 2015] [error] mod_wsgi (pid=20146): Exception occurred processing WSGI script '/opt/graphite/conf/graphite.wsgi'.\n[Fri Jun 19 09:25:45 2015] [error] Traceback (most recent call last):\n[Fri Jun 19 09:25:45 2015] [error]   File \"/opt/graphite/conf/graphite.wsgi\", line 4, in <module>\n[Fri Jun 19 09:25:45 2015] [error]     from graphite.wsgi import application\n[Fri Jun 19 09:25:45 2015] [error]   File \"/opt/graphite/webapp/graphite/wsgi.py\", line 21, in <module>\n[Fri Jun 19 09:25:45 2015] [error]     application = DjangoWhiteNoise(application)\n[Fri Jun 19 09:25:45 2015] [error]   File \"/usr/lib/python2.6/site-packages/whitenoise/django.py\", line 31, in __init__\n[Fri Jun 19 09:25:45 2015] [error]     settings_key = 'WHITENOISE_{}'.format(attr.upper())\n[Fri Jun 19 09:25:45 2015] [error] ValueError: zero length field name in format\n. I 'm on a RHEL6.4 with a closed corportive yum repo. So I'm not able to upgrade python.\nThere is any way to disable only whitenoise ? what commit was the last before the whitenoise? \n. @deniszh , are you referring to the /opt/graphite/webapp/graphite/wsgi.py  file?\n. @deniszh It seems to be working fine now !!! Lots of thanks!!\nwhen are you planning to update master branch with this patch ?\n. Hi @deniszh @brutasse \nAfter reviewing again I've noticed that  I can not see metric names and Y labels on the png's .\n\nIs like no text could be rendered , but any excepcion / error found in files\n```\nFri Jun 19 14:53:52 2015 :: Retrieval of carbon.agents.*.cpuUsage took 0.028361\nFri Jun 19 14:53:52 2015 :: Rendered PNG in 0.065290 seconds\nFri Jun 19 14:53:52 2015 :: Total rendering time 0.115250 seconds\n```\nAny idea ?\nthese are my python modules.\n.root:graphite > pip freeze\nargparse==1.3.0\ncairocffi==0.7\ncffi==1.1.2\nDjango==1.6\ndjango-tagging==0.3.6\nethtool==0.6\ngunicorn==19.3.0\niniparse==0.3.1\nisodate==0.5.0\niwlib==1.0\nM2Crypto==0.20.2\nmocker==1.1.1\npulp-agent==2.4.0\npulp-common==2.4.0\npulp-rpm-common==2.4.0\npulp-rpm-handlers==2.4.0\npycparser==2.14\npycurl==7.19.0\npygpgme==0.1\npyOpenSSL==0.10\nPyPAM==0.5.0\npyparsing==1.5.7\npython-dateutil==1.4.1\npython-dmidecode==3.10.13\npython-memcached==1.47\npytz==2015.4\nqpid-python==0.22\nrhnlib==2.5.22\nrhsm==1.9.7\nsimplejson==2.1.6\nTwisted==15.2.1\ntxAMQP==0.4\nurlgrabber==3.9.1\nwheel==0.24.0\nwhisper==0.9.10\nwhitenoise==1.0.6\nyum-metadata-parser==1.1.2\nzope.interface==4.1.2\n. forced aliasByNode() doen't work \n\nif forced json format\nhttp://graphite_host/render/?format=json&target=aliasByNode(carbon.agents.*.creates%2C2)&from=-5min\nI got the correct target as you can see.\n```\n[{\"target\": \"host-wr03c\", \"datapoints\": [[0.0, 1434718800], [0.0, 1434718860], [0.0, 1434718920], [0.0, 1434718980], [0.0, 1434719040]]}, {\"target\": \"host-wr03b\", \"datapoints\": [[0.0, 1434718800], [0.0, 1434718860], [0.0, 1434718920], [0.0, 1434718980], [0.0, 1434719040]]}, {\"target\": \"host-wr03a\", \"datapoints\": [[0.0, 1434718800], [0.0, 1434718860], [0.0, 1434718920], [0.0, 1434718980], [0.0, 1434719040]]}, {\"target\": \"host-wr03d\", \"datapoints\": [[0.0, 1434718800], [0.0, 1434718860], [0.0, 1434718920], [0.0, 1434718980], [0.0, 1434719040]]}]\n```\n. I can now see text after forcing display font-face and font-style ? Whats the matter?\n\n. Hi @brutasse I have installed \ncairo-1.8.8-3.1.el6.x86_64.\nName        : cairo\nArch        : x86_64\nVersion     : 1.8.8\nRelease     : 3.1.el6\nSize        : 779 k\nRepo        : installed\nFrom repo   : anaconda-RedHatEnterpriseLinux-201301301459.x86_64\nSummary     : A 2D graphics library\nURL         : http://cairographics.org\nLicense     : LGPLv2 or MPLv1.1\nDescription : Cairo is a 2D graphics library designed to provide high-quality display\n            : and print output. Currently supported output targets include the X Window\n            : System, OpenGL (via glitz), in-memory image buffers, and image files (PDF,\n            : PostScript, and SVG).\n            :\n            : Cairo is designed to produce consistent output on all output media while\n            : taking advantage of display hardware acceleration when available (e.g.\n            : through the X Render Extension or OpenGL).\nAs a workarround I  have created the graphTemplates.conf and changed on the default section the fontItalic to true: \nfontSize = 12\nfontBold = False\nfontItalic = True\n\nAnd now we can see text , but I'm afraid on how to fix this error and how can impact on production servers. \nThere is any way to check cairo logs? or related debug info?\n. @brutasse I've checked version In another graphite installation that have the same cairo version, and is working ok.\nThis other platform has :\ngraphite-web  from master (commit : 8f13d44d446acfaecefb438f1f4b8f56e5164092)\ncairo-1.8.8-3.1.el6.x86_64\nroot:/root > pip freeze\nargparse==1.2.2\ncairocffi==0.6\ncarbon==0.9.10\ncarbonate==0.2.2\ncffi==0.8.6\nDjango==1.4\ndjango-tagging==0.3.1\nethtool==0.6\ngraphite-web==0.10.0a0\ngunicorn==19.1.1\niniparse==0.3.1\niotop==0.3.2\nisodate==0.5.0\niwlib==1.0\nM2Crypto==0.20.2\nmocker==1.1.1\npulp-agent==2.4.0\npulp-common==2.4.0\npulp-rpm-common==2.4.0\npulp-rpm-handlers==2.4.0\npycparser==2.10\npycurl==7.19.0\npygpgme==0.1\npyOpenSSL==0.10\nPyPAM==0.5.0\npyparsing==1.5.7\npython-dateutil==1.4.1\npython-dmidecode==3.10.13\npython-memcached==1.47\npytz==2014.10\nqpid-python==0.22\nrhnlib==2.5.22\nrhsm==1.12.5\nsimplejson==2.1.6\nTwisted==11.1.0\ntxAMQP==0.4\nurlgrabber==3.9.1\nwhisper==0.9.10\nyum-metadata-parser==1.1.2\nzope.interface==4.1.1\nAnd working ok\n\nmaybe something related to the graphite or cairocffi releases?\n. Hi @brutasse  , I've finally found the fix for this problem. \nAs in this link somebody says  (http://serverfault.com/questions/579850/new-graphite-installation-doesnt-render-fonts-in-graphs-except-courier) Centos and Redhat misses needed fonts packages.\nafter installed them I can now see rendered graphs correctly\nyum install dejavu-sans-fonts.noarch dejavu-serif-fonts.noarch\n. Thanks for the PR (https://github.com/graphite-project/graphite-web/pull/1256) after reinstall it seems to be working ok!\n. Hi @brutasse I saw this pull request some days ago. But I think we have not exactly the same goals.\nIn our PoC, changes are much lighter and only affect on metric visibility by user. ( nothing to do with dashboards ).\nWe will  prepare  a new git branch with our changes over a graphite-web fork to show you how we apply filters\n. Hi @brutasse . \nYou can see code here.\nhttps://github.com/graphite-project/graphite-web/compare/master...cosm0s:metric_filter\nAs you can see  , we have only created a model in a new folder (metric_filter) but we can relocate this code if needed, and intercepted the returned nodes/metrics previosly filtered.( only applied if ALLOW_METRICFILTER_BY_USER is set to true).\nUse Case (3 users)\nSupose we have a metric scheme like that\n<group>.<datacenter>.<server_type>.<hostname>.<product>.<metric_name>\nby example 2 groups with 2 hosts running two products ( linux/oracle).\n- marketing.datacenter1.bbdd.mktdb01.linux.xxxxx\n- marketing.datacenter1.bbdd.mktdb01.oracle.xxxxx\n- sales.datacenter1.bbdd.salesdb01.linux.xxxxx\n- sales.datacenter1.bbdd.salesdb01.oracle.xxxxx\nWe can create 3 types of users with diferent metric visibility by appling a simple  RegExp Filter.\nmarkeging users (mktusr)\nuser:  mktusr\nfilterRegex:  \"^marketing.*\"\nYou will see only : \nmarketing.datacenter1.bbdd.mktdb01.linux.xxxxx\n           marketing.datacenter1.bbdd.mktdb01.oracle.xxxxx\nsales users ( salesusr)\nuser: salesusr\nfilterRegex: \"^sales.*\"\nYou will see only \nsales.datacenter1.bbdd.salesdb01.linux.xxxxx\n           sales.datacenter1.bbdd.salesdb01.oracle.xxxxx\ndb administrators (dba)\nuser: dba \nfilterRegex: \"\\w+.\\w+.\\w+.\\w+.oracle..*\"\nYou will see only \nmarketing.datacenter1.bbdd.mktdb01.oracle.xxxxx\n             sales.datacenter1.bbdd.salesdb01.oracle.xxxxx\nAt this point we can choose to do more than one filter level but we prefer make it simple and only add one level.\nWe have plans to add more than one allowed filter by user in that case the result should be the concatenation (AND) of each individual filter , and nothing more complicated now.\nOf course this code needs for extensive testing,  best documentation  and examples to help people to apply these filters.\nWhat do you think about ? If you like we can do a PR with suggested changes if needed and extended documentation.\n. OK @brutasse \nWe will begin to work in a new PR from this PoC  , we will change Per-user permissions to per-group permissions. \nWe will be gratefully if you can also show us those couple of things in the code that we need to fix and also things you think could cause issues.\nThank you very much.\n. Hi  @deniszh , we are working with  response time in apache log as the data origin (%D in microseconds)\nLogFormat \"%h %l %u %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\" \\\"%{User-Agent}i\\\" %D\" combined\n       CustomLog /var/log/graphite/webapp/access.log combined\n. @deniszh graphiteXX nodes doesn't have CLUSTER_SERVER configured.\nWe are only rendering with grafana (flot) not direct png from graphite.\n. @deniszh : I think 50 times slower than the sum of all other times , are too much . \nAnyway I would like to test the \"parallel fech\" patch in master . There is any plans to merge in master branch also?\nNote (off topic): I was pretty sure than 0.9.13 had been the last version previous to 0.10 ( from master) , Is a bad news to me and this make me difficult manage updates from a stable release. :(\n. sorry @deniszh. I meant that the best improvements done in last months seems have been built in the 0.9.X branch before than the master branch. ( https://github.com/graphite-project/graphite-web/pull/1026#issuecomment-66033224) \nAnyway thank you for the fast response:\nI will be waiting anxiously this merge\nIn the meanwhile I would like to tune as fine as I can to mitigate this strange behavior.\n- There is any way to limit response time if REMOTE_READER_CACHE_SIZE_LIMIT is set to 0 ?\n- There is any way to change order to query first CLUSTER_SERVERS and after local CARBONLINK_HOSTS ?\n- what kind of tunning could be done to improve the response time?\n. One comment about my configuration.\nWe are working without \"memcached\". \nIf I review the cache.log I'm getting always \"misses\" in all Request-Cache .\n```\ngrep Request-Cache  /logs/apps/graphite/webapp/cache.log\nMon Nov 02 11:43:26 2015 :: Request-Cache miss [dbcf92eb3567276a1c882021682b3b70]\nMon Nov 02 11:43:31 2015 :: Request-Cache miss [dc7b91f44f013f6eea6a3e2ece4b443f]\nMon Nov 02 11:48:42 2015 :: Request-Cache miss [d9421f2fac1316f78e4aa63fc766f32c]\nMon Nov 02 11:50:01 2015 :: Request-Cache miss [7f1b87e41006e91214f50b84a69fa4c3]\nMon Nov 02 11:50:02 2015 :: Request-Cache miss [7f1b87e41006e91214f50b84a69fa4c3]\nMon Nov 02 11:50:04 2015 :: Request-Cache miss [3d3d7d5fa8f52cdf3623dd95524543d1]\nMon Nov 02 11:53:59 2015 :: Request-Cache miss [c554c4c79de920f91fa1dec089da1794]\nMon Nov 02 11:56:02 2015 :: Request-Cache miss [7f1b87e41006e91214f50b84a69fa4c3]\nMon Nov 02 11:56:02 2015 :: Request-Cache miss [7f1b87e41006e91214f50b84a69fa4c3]\nMon Nov 02 11:56:04 2015 :: Request-Cache miss [b9ac89ae7a2273e9625dd35848b581f1]\nMon Nov 02 11:59:39 2015 :: Request-Cache miss [bc75e3b8d145017523ba243ac26d1004]\nMon Nov 02 12:02:02 2015 :: Request-Cache miss [7f1b87e41006e91214f50b84a69fa4c3]\nMon Nov 02 12:02:02 2015 :: Request-Cache miss [7f1b87e41006e91214f50b84a69fa4c3]\nMon Nov 02 12:02:04 2015 :: Request-Cache miss [fcb75f99b0ddff5319f7f170f673c289]\nMon Nov 02 12:04:50 2015 :: Request-Cache miss [4add2cccf7dba335a84d3df2f2f2ff75]\nMon Nov 02 12:08:02 2015 :: Request-Cache miss [7f1b87e41006e91214f50b84a69fa4c3]\nMon Nov 02 12:08:02 2015 :: Request-Cache miss [7f1b87e41006e91214f50b84a69fa4c3]\nMon Nov 02 12:08:04 2015 :: Request-Cache miss [39e88011a0338760ceb40c835387da08]\nMon Nov 02 12:09:59 2015 :: Request-Cache miss [e877230b2bf22a8dda334782abd1f64a]\nMon Nov 02 12:13:23 2015 :: Request-Cache miss [7f1b87e41006e91214f50b84a69fa4c3]\nMon Nov 02 12:13:23 2015 :: Request-Cache miss [7f1b87e41006e91214f50b84a69fa4c3]\nMon Nov 02 12:13:26 2015 :: Request-Cache miss [c710a67da9f49ebad3466eeb472ca07f]\nMon Nov 02 12:15:38 2015 :: Request-Cache miss [3dadc7f96a00cdb9057cceb9e2b80147]\nMon Nov 02 12:20:02 2015 :: Request-Cache miss [7f1b87e41006e91214f50b84a69fa4c3]\nMon Nov 02 12:20:02 2015 :: Request-Cache miss [7f1b87e41006e91214f50b84a69fa4c3]\nMon Nov 02 12:20:04 2015 :: Request-Cache miss [5dc82d786fd84ccb2ab088a1816aba6f]\nMon Nov 02 12:20:38 2015 :: Request-Cache miss [70c854b10d686d8e8f094b837ff18984]\n```\nWatching this log I'm suspecting there is not any cache engine working by default in graphite-web application. ( I have supposed that django had its own cache system).\nHow can I check that my graphite is working fine with the cache system? If none by default , there is any way to disable \"Request-Cache\" and CacheKeyWarnings?\n. Hi @deniszh I'm working with master 8710ff76f0b4571481d295d997a83e4c895dd092 ( from June) the fix you show me was merged on February. \nthis error happens mainly querying carbon metrics when carbon processes are heavily loaded , you can see https://github.com/graphite-project/carbon/issues/479 \n```\ngrep \"Exception encountered in\"  /logs/apps/graphite/webapp/exception.log\nMon Dec 07 15:39:43 2015 :: Exception encountered in \nMon Dec 07 15:42:26 2015 :: Exception encountered in \nMon Dec 07 15:42:26 2015 :: Exception encountered in \nMon Dec 07 15:42:27 2015 :: Exception encountered in \nMon Dec 07 15:42:27 2015 :: Exception encountered in \nMon Dec 07 15:49:01 2015 :: Exception encountered in \nMon Dec 07 15:49:02 2015 :: Exception encountered in \nMon Dec 07 15:49:03 2015 :: Exception encountered in \nMon Dec 07 15:49:03 2015 :: Exception encountered in \nMon Dec 07 15:49:03 2015 :: Exception encountered in \nMon Dec 07 15:49:03 2015 :: Exception encountered in \nMon Dec 07 15:51:46 2015 :: Exception encountered in \nMon Dec 07 16:23:40 2015 :: Exception encountered in \nMon Dec 07 16:23:41 2015 :: Exception encountered in \nMon Dec 07 16:23:42 2015 :: Exception encountered in \nMon Dec 07 16:25:26 2015 :: Exception encountered in \nMon Dec 07 16:25:27 2015 :: Exception encountered in \nMon Dec 07 16:25:27 2015 :: Exception encountered in \nMon Dec 07 16:25:28 2015 :: Exception encountered in \nMon Dec 07 16:27:00 2015 :: Exception encountered in \nMon Dec 07 16:27:00 2015 :: Exception encountered in \nMon Dec 07 16:27:00 2015 :: Exception encountered in \nMon Dec 07 16:27:00 2015 :: Exception encountered in \nMon Dec 07 16:34:36 2015 :: Exception encountered in \nMon Dec 07 16:34:37 2015 :: Exception encountered in \nMon Dec 07 16:34:38 2015 :: Exception encountered in \nMon Dec 07 16:34:38 2015 :: Exception encountered in \nMon Dec 07 16:37:12 2015 :: Exception encountered in \nMon Dec 07 16:49:47 2015 :: Exception encountered in \nMon Dec 07 16:49:47 2015 :: Exception encountered in \nMon Dec 07 16:49:48 2015 :: Exception encountered in \nMon Dec 07 16:49:48 2015 :: Exception encountered in \nMon Dec 07 17:06:01 2015 :: Exception encountered in \nMon Dec 07 17:07:06 2015 :: Exception encountered in \nMon Dec 07 17:07:06 2015 :: Exception encountered in \nMon Dec 07 17:17:31 2015 :: Exception encountered in \nMon Dec 07 17:17:31 2015 :: Exception encountered in \nMon Dec 07 17:17:31 2015 :: Exception encountered in \nMon Dec 07 17:17:32 2015 :: Exception encountered in \nMon Dec 07 17:17:32 2015 :: Exception encountered in \nMon Dec 07 17:33:57 2015 :: Exception encountered in \n```\n. hi @deniszh , what I can do to debug this error code on the graphite-web or carbon code? . It seems like self made metrics \"carbon...*\" sometimes (randomly) are not in correct format.\n```\nawk '{ if ($9 == 500) print $0 }' /logs/apps/graphite/webapp/access.log`\n..\n..\n10.16.172.147 - - [10/Dec/2015:11:47:18 +0100] \"GET /render/?target=carbon.%2A.graphitehost01%2A.cache.size&format=pickle&local=1&noCache=1&from=1449740838&until=1449744438 HTTP/1.1\" 500 1637 \"-\" \"-\" 86419\n10.16.172.147 - - [10/Dec/2015:11:49:38 +0100] \"GET /render/?target=carbon.relays.graphitehost01-dist07.destinations.%2A.relayMaxQueueLength&format=pickle&local=1&noCache=1&from=1449740978&until=1449744578 HTTP/1.1\" 500 1637 \"-\" \"-\" 180795\n10.16.172.147 - - [10/Dec/2015:11:51:08 +0100] \"GET /render/?target=carbon.agents.graphitehost01%2A.cpuUsage&format=pickle&local=1&noCache=1&from=1449741068&until=1449744668 HTTP/1.1\" 500 1637 \"-\" \"-\" 79747\n10.16.172.147 - - [10/Dec/2015:11:51:09 +0100] \"GET /render/?target=carbon.%2A.graphitehost01%2A.cache.size&format=pickle&local=1&noCache=1&from=1449741069&until=1449744669 HTTP/1.1\" 500 1637 \"-\" \"-\" 92099\n10.16.172.147 - - [10/Dec/2015:11:51:09 +0100] \"GET /render/?target=carbon.agents.graphitehost01%2A.cache.queries&format=pickle&local=1&noCache=1&from=1449741069&until=1449744669 HTTP/1.1\" 500 1637 \"-\" \"-\" 77749\n10.16.172.147 - - [10/Dec/2015:11:51:20 +0100] \"GET /render/?target=carbon.agents.graphitehost01%2A.cpuUsage&format=pickle&local=1&noCache=1&from=1449741080&until=1449744680 HTTP/1.1\" 500 1637 \"-\" \"-\" 68884\n10.16.172.147 - - [10/Dec/2015:11:52:05 +0100] \"GET /render/?target=carbon.agents.graphitehost01%2A.cpuUsage&format=pickle&local=1&noCache=1&from=1449741125&until=1449744725 HTTP/1.1\" 500 1637 \"-\" \"-\" 34663\n10.16.172.147 - - [10/Dec/2015:11:52:09 +0100] \"GET /render/?target=carbon.agents.graphitehost01%2A.cpuUsage&format=pickle&local=1&noCache=1&from=1449741129&until=1449744729 HTTP/1.1\" 500 1637 \"-\" \"-\" 38939\n10.16.172.147 - - [10/Dec/2015:11:52:10 +0100] \"GET /render/?target=carbon.%2A.graphitehost01%2A.creates&format=pickle&local=1&noCache=1&from=1449741130&until=1449744730 HTTP/1.1\" 500 1637 \"-\" \"-\" 42131\n10.16.172.147 - - [10/Dec/2015:11:53:15 +0100] \"GET /render/?target=carbon.agents.graphitehost01%2A.cpuUsage&format=pickle&local=1&noCache=1&from=1449741195&until=1449744795 HTTP/1.1\" 500 1637 \"-\" \"-\" 86375\n10.16.172.147 - - [10/Dec/2015:11:53:15 +0100] \"GET /render/?target=carbon.agents.graphitehost01%2A.memUsage&format=pickle&local=1&noCache=1&from=1449741195&until=1449744795 HTTP/1.1\" 500 1637 \"-\" \"-\" 69825\n10.16.172.147 - - [10/Dec/2015:11:53:15 +0100] \"GET /render/?target=carbon.agents.graphitehost01%2A.metricsReceived&format=pickle&local=1&noCache=1&from=1449741195&until=1449744795 HTTP/1.1\" 500 1637 \"-\" \"-\" 42753\n10.16.172.147 - - [10/Dec/2015:11:53:16 +0100] \"GET /render/?target=carbon.relays.graphitehost01-dist07.destinations.%2A.relayMaxQueueLength&format=pickle&local=1&noCache=1&from=1449741196&until=1449744796 HTTP/1.1\" 500 1637 \"-\" \"-\" 73782\n..\n..\n```\nI've tried to force with different render format  ( png/json) but the error is not dependent on the format.\nAnd always  with this stacktrace.\nThu Dec 10 12:00:55 2015 :: Exception encountered in <GET http://graphite/render/?width=586&height=308&_salt=1449745256.475&target=carbon.agents.graphitehost01-wr07b.cpuUsage&target=carbon.agents.graphitehost01-wr07a.cpuUsage&target=carbon.agents.graphitehost01-wr07c.cpuUsage&target=carbon.agents.graphitehost01-wr07d.cpuUsage&target=carbon.agents.graphitehost01-wr07e.cpuUsage>\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/django/core/handlers/base.py\", line 114, in get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 112, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 8, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 29, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression, replacements)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 45, in evaluateTokens\n    return fetchData(requestContext, expression)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 141, in fetchData\n    raise Exception(\"Failed after %i retry! See: %s\" % (settings.MAX_FETCH_RETRIES, e))\nException: Failed after 2 retry! See: 'float' object is not iterable\nIt seems like no data is returned from carbon in any of the queried series.\nThere is any debut log in the  that I can put in datalib.py _fetchData() to get more information on the error origin.\n. Thank you very much @mleinart  for this patch I will apply ASAP to get more information that can help us to locate the origin of this exception.\n. Hi @mleinart , after apply the patch and reproducte the error, this is the exception stacktrace. ( it seems like something wrong in the path isn't it?)\nMon Dec 14 09:30:09 2015 :: Exception encountered in <GET http://graphitehost01/render/?target=carbon.%2A.graphitehost01-wr%2A.metricsReceived&format=pickle&local=1&noCache=1&from=1450060209&until=1450081689>\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/django/core/handlers/base.py\", line 114, in get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 112, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 8, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 29, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression, replacements)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 45, in evaluateTokens\n    return fetchData(requestContext, expression)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 147, in fetchData\n    (retries, settings.MAX_FETCH_RETRIES), format_exc())\nTypeError: not enough arguments for format string\n. Hi @mleinart after change this line\n(retries, settings.MAX_FETCH_RETRIES), format_exc())\nby this other:\n(retries, settings.MAX_FETCH_RETRIES, format_exc()))\nI could see the complete exception and this is the output\nWed Dec 16 15:05:06 2015 :: Got an exception when fetching data! Try: 1 of 2. Root cause:\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 138, in fetchData\n    seriesList = _fetchData(pathExpr,startTime, endTime, requestContext, seriesList)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 101, in _fetchData\n    fetches = [(node, node.fetch(startTime, endTime)) for node in matching_nodes if node.is_leaf]\n  File \"/opt/graphite/webapp/graphite/node.py\", line 30, in fetch\n    return self.reader.fetch(startTime, endTime)\n  File \"/opt/graphite/webapp/graphite/readers.py\", line 177, in fetch\n    for (timestamp, value) in cached_datapoints:\nTypeError: 'float' object is not iterable\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 138, in fetchData\n    seriesList = _fetchData(pathExpr,startTime, endTime, requestContext, seriesList)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 101, in _fetchData\n    fetches = [(node, node.fetch(startTime, endTime)) for node in matching_nodes if node.is_leaf]\n  File \"/opt/graphite/webapp/graphite/node.py\", line 30, in fetch\n    return self.reader.fetch(startTime, endTime)\n  File \"/opt/graphite/webapp/graphite/readers.py\", line 177, in fetch\n    for (timestamp, value) in cached_datapoints:\nTypeError: 'float' object is not iterable\nWed Dec 16 15:05:06 2015 :: Failed after 2 retry! Root cause:\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 138, in fetchData\n    seriesList = _fetchData(pathExpr,startTime, endTime, requestContext, seriesList)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 101, in _fetchData\n    fetches = [(node, node.fetch(startTime, endTime)) for node in matching_nodes if node.is_leaf]\n  File \"/opt/graphite/webapp/graphite/node.py\", line 30, in fetch\n    return self.reader.fetch(startTime, endTime)\n  File \"/opt/graphite/webapp/graphite/readers.py\", line 177, in fetch\n    for (timestamp, value) in cached_datapoints:\nTypeError: 'float' object is not iterable\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 138, in fetchData\n    seriesList = _fetchData(pathExpr,startTime, endTime, requestContext, seriesList)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 101, in _fetchData\n    fetches = [(node, node.fetch(startTime, endTime)) for node in matching_nodes if node.is_leaf]\n  File \"/opt/graphite/webapp/graphite/node.py\", line 30, in fetch\n    return self.reader.fetch(startTime, endTime)\n  File \"/opt/graphite/webapp/graphite/readers.py\", line 177, in fetch\n    for (timestamp, value) in cached_datapoints:\nTypeError: 'float' object is not iterable\nWed Dec 16 15:05:06 2015 :: Exception encountered in <GET http://10.16.172.192/render/?target=carbon.%2A.graphitehost01-wr%2A.cache.overflow&format=pickle&local=1&n                                                                             oCache=1&from=1450101905&until=1450274645>\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/django/core/handlers/base.py\", line 114, in get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 112, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 8, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 29, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression, replacements)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 45, in evaluateTokens\n    return fetchData(requestContext, expression)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 144, in fetchData\n    raise e\nTypeError: 'float' object is not iterable\nThere is enough information ? \n. Hi @mleinart , as you can see in the image , the patch is applied in the file /opt/graphite/webapp/graphite/readers.py on the affected server.\n\nThe exception ONLY affect to carbon metrics but not always sometimes happens and sometimes not. The exception seems happens usually when  carbon-caches has I/O heavy load and number of cached points are growing, it happens 20 or 30 times every day.\n. Hi @mleinart I'm still getting errors only from carbon metrics (as you can see in the next screenshot) And only when there is heavy IO in the system (you can say I/O wait has grown until 50% last 2 hours) , all the other stored metrics ( system/apache/etc) are got  from the same carbon instances and all ok.\nIt's something annoying when trying to check graphite/carbon health in our systems.\n\nThere is any other way to debug this error?\nMon Jan 18 11:46:31 2016 :: Exception encountered in <GET http://graphite/render?from=-15min&format=raw&target=carbon.agents.graphitehost-wr05a.cpuUsage>\nTraceback (most recent call last):\n  File \"/usr/lib/python2.6/site-packages/django/core/handlers/base.py\", line 114, in get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 112, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 8, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 29, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression, replacements)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 45, in evaluateTokens\n    return fetchData(requestContext, expression)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 141, in fetchData\n    raise Exception(\"Failed after %i retry! See: %s\" % (settings.MAX_FETCH_RETRIES, e))\nException: Failed after 2 retry! See: 'float' object is not iterable\nMon Jan 18 11:51:31 2016 :: Got an exception when fetching data! See: 'float' object is not iterable Will do it again! Run: 1 of 2\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 136, in fetchData\n    seriesList = _fetchData(pathExpr,startTime, endTime, requestContext, seriesList)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 99, in _fetchData\n    fetches = [(node, node.fetch(startTime, endTime)) for node in matching_nodes if node.is_leaf]\n  File \"/opt/graphite/webapp/graphite/node.py\", line 30, in fetch\n    return self.reader.fetch(startTime, endTime)\n  File \"/opt/graphite/webapp/graphite/readers.py\", line 177, in fetch\n    for (timestamp, value) in cached_datapoints:\nTypeError: 'float' object is not iterable\nMon Jan 18 11:51:31 2016 :: Failed after 2 retry! See: 'float' object is not iterable\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 136, in fetchData\n    seriesList = _fetchData(pathExpr,startTime, endTime, requestContext, seriesList)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 99, in _fetchData\n    fetches = [(node, node.fetch(startTime, endTime)) for node in matching_nodes if node.is_leaf]\n  File \"/opt/graphite/webapp/graphite/node.py\", line 30, in fetch\n    return self.reader.fetch(startTime, endTime)\n  File \"/opt/graphite/webapp/graphite/readers.py\", line 177, in fetch\n    for (timestamp, value) in cached_datapoints:\nTypeError: 'float' object is not iterable\nThank you very much!\n. As you can see @mleinart , after heavy load finishes carbon instances begin to report carbon metrics ok.\n\n. Hi @mleinart  I have more information about when this problem happens.\nPerhaps this issue should be placed better to a carbon issue, but in the meanwhile I would like to give you more feedback.\nI've notice than query over carbon.agent.*. are failing over 2 cache  instances in the same moment than they begin to queue( both at the same time)..\n\nI don't know why but queue size on both instances seems to reach over 40.000 metrics.\nThis is my configuration\n```\n[cache]\nLOG_DIR=/logs/apps/graphite\nSTORAGE_DIR    = /tsdb/metrics/\nUSER=carbon\nLOCAL_DATA_DIR = %(STORAGE_DIR)s/\nENABLE_LOGROTATE = False\nMAX_CACHE_SIZE = inf\nMAX_UPDATES_PER_SECOND = 50000\nMAX_CREATES_PER_MINUTE = 500\nLINE_RECEIVER_INTERFACE = 127.0.0.1\nENABLE_UDP_LISTENER = False\nUDP_RECEIVER_INTERFACE = 127.0.0.1\nUDP_RECEIVER_PORT = 2003\nPICKLE_RECEIVER_INTERFACE = 127.0.0.1\nUSE_INSECURE_UNPICKLER = False\nCACHE_QUERY_INTERFACE = 127.0.0.1\nUSE_FLOW_CONTROL = True\nLOG_UPDATES = False\nLOG_CACHE_HITS = False\nWHISPER_AUTOFLUSH = False\nLOG_LISTENER_CONN_SUCCESS = False\n...\n[cache:wr05x]\nLINE_RECEIVER_PORT=2X13\nPICKLE_RECEIVER_PORT = 2X14\nCACHE_QUERY_PORT = 7X02\n...\n```\nI will be grateful if you can help us on this issue. Thank you very much.\n. Hi @brutasse .\nWe have been working for last weeks in this PR from (https://github.com/graphite-project/graphite-web/issues/1285)\nOf course this new feature is optional and disabled by default (for backwards compatibility), to enable you should change ALLOW_METRICFILTERING_BY_USER= True in local_settings.\nAfter this change has been done all metrics in STORE.find() will pass a filter list , this filter list is made from user filter first and group filters last.\nIf filtering enabled nobody will be able to see any metrics until added some filters.\nYou can add filter to user and groups with the django admin in the \"Metric Filter Administration\" section.\n\n1.- Create  metric filters\nYou should first create all metric name filters \nThe filter table has 3 fields.\n1.- filter regex field\n2.- Description\n3.- Type ( include / Exclude) .\n\n2.- Asign filters to user / groups.\nAdd filters to user  / group in the \"User Metric filter  / Group Metric Filter \" table , by choosing the previously configured filters\n\nAt this point you will see only selected metrics in the tree \n. Hi I'm experiencing the same behavior, while carbon cache instances are queuing data graphite doesn't return recent metrics , we have 6x carbon-cache instances (200k metrics/minute aprox)\nAs you can see in this queue graph , we have this error frequently.\n\nthe problem persists until data queues disappear, usually 10/15 minutes  ( sometimes 20/30 minutes)\n. @DazWorrall @deniszh , we have noticed exactly the same behavior but in our case we are working with git master versions \n-  graphite-web:   ( commit 8710ff76f0b4571481d295d997a83e4c895dd092) from june 2015.  Which has not implemented CARBONLINK_QUERY_BULK option yet.\n- carbon: ( commit https://github.com/graphite-project/carbon/commit/b80ce915a5e420b46e6972512801491e536db1b6 ) from April 2015.\nWe have only last values when carbon instances can flush all data to disk.\n. sorry @deniszh I have not implemented DIVERSE_REPLICAS option. :(\nIf there is any way to \"force\"/\"simulate\"  data queue growing  , we could update to the last master version in a testing box and debug anything you need to fix this error.\nHow can simulate it?\n. @deniszh . I'm not sure about that. \nI Think there is some error when queue grows , as a example we have already opened an issue (https://github.com/graphite-project/graphite-web/issues/1399) ( carbon  metrics query crashes), this happens only when queue grows and only with carbon metrics, all other metrics queries are working but only until last flush to disk.\n. Hi @deniszh , @DazWorrall we have increased in 6Gb RAM and 2 cpu's our resources in a graphite node ( a VMware VM), and restarted the machine after that at 6:00 ( aprox). \nNOTE: This machine has 210K metrics /minute.\n\nAs you can see in our queue graph all all cache instances wr07x have been caching for 3 hours , while this happens  graphite-web query's response contains only data until 6:00 am.graphite-web response time is really good here.\nOnce all data is cached on memory  ( at 9:30 aprox) it seems like carbon-caches begin to flush all data to disk.. cpu wait time disappears and graphite-web query's responses contains all previous data. ( no data lost seems to happen).\nIn all this process the only problem is he have been working 3 hours without recent data. If graphite-web is running ok, it have enough cpu why carbon-cache are not sending queued data? and only  disk data?\n. ",
    "hwang3419": "I have rebased this PR.\n. ",
    "JeanFred": "@g76r \n\nI tried aptitude install python-tox on debian 8, thinking \"that's probably a great tool\".\nFound that it crashes because of python27/python3 lacks of compatibility (I won't troll about that).\n\nArf :-/ Aptitude is usually the best way to ensure compatibility with your system though :-/ I\u2019m not a Debian user (running Ubuntu here) though so unsure how much I can help. Maybe installing tox via pip?\n\nI tried tox-quickstart, then encountered other errors and gave away\n\ntox-quickstart purpose is to setup the tox layout on a new project \u2212 it\u2019s definitely not what you want.\n\nthinking that pushing several times into the PR branch was not that bad...\n\nOh, yes, definitely \u2212 It\u2019s Travis job after all :)\n. > I'm clearly very intersted in the consistency of my system, therefore I won't mix package managers on my runtime used computer.\nYes, I totally understand :)\n\nCollecting cairocffi\n  Using cached cairocffi-0.7.2.tar.gz\n    Complete output from command python setup.py egg_info:\n    c/_cffi_backend.c:2:20: fatal error: Python.h: No such file or directory\n     #include \n\nNow this looks better! Looks like building cffi necessites the Python header files. Can you try installing (via aptitude of course ) the python-dev package? That should do the trick.\n. > thanks, it works (when installing libffi-dev in addition to python-dev).\nYay! :-)\n. Can\u2019t think of anything @obfuscurity :)\n. We now have https://codecov.io/github/graphite-project/graphite-web so I\u2019m closing this.\nWe may want to add more bells & whistles (like posting on PRs etc.) but that can come later.\n. @bitprophet The rationale sounds sensible to me, and the code looks fine \u2212 but I\u2019m not familiar enough with the codebase in general for even an emoji on this :-)\n. Wait, looks like #1480 is the same as this? (Good news is that @deniszh +1 it ;-)\n. @SEJeff Thanks!\nOkay, so this is failing on the Python 2.6 builds, as I am using some unittest features only introduced in 2.7\n- assertRaisesRegexp: I used because the exceptions raised felt too generic to me (Exception, ValueError), but this is not super useful anyway ;\n- skip in order to skip the 'noon+yesterday' test ; but what I really wanted to do was expectedFailure\nI have updated the pull request to not use 2.7 only constructs. Let\u2019s see if Travis is happy with it. :)\n. Oh, so expectedFailure is also a 2.7 construct apparently (I thought from the docs it was not). Shall I just comment out this test @SEJeff ? Or add unittest2 package to the dependencies ?\n. Sorry for the lag @SEJeff. I updated the pull-request to have unittest2 intstalled in a py26 environment. All tests are now passing :)\n. Deal :)\n. (I caught this by flake8-ing this file. We should enable flake8 for that file too (although it contains quite a lot of legitimate F401))\n. Having it all consolidated under one service is certainly the best plan. As I said, I\u2019m happy to change that to Coveralls.\nThat being said, CodeCov appears more polished & feature-complete \u2212 subjectively nicer reports ; but especially branch coverage (which is a pretty big miss from coveralls, see lemurheavy/coveralls-public#31).\n. Gently upping this. :)\n(I just noticed that Sentry also uses CodeCov \u2212 see eg https://github.com/getsentry/sentry/pull/2345#issuecomment-158218706)\n. @obfuscurity Definitely! I\u2019ll submit PRs to Carbon and Whisper in the coming weeks.\n@SEJeff Just for clarification (we can take that further on the Whisper issue), do you want me to remove Coveralls in Whisper and replace it by CodeCov, or you\u2019d rather not have any such tool on Whisper ?\n. 0.9.14 sounds like a plan :)\n. That was fast \u2212 thanks @brutasse!\n(By the way, I did all my recent few PRs against master \u2212 should I backport all or part to 0.9.x ?)\n. My pleasure :) If if have some time later today I will also port #1276 back to 0.9.x so it can make it in 0.9.14 release ^__^\n. This is beautiful. Great job @deniszh\n. Yay!\n. @gmlexx The Travis build failed:\ntools.rst:219: WARNING: Definition list ends without a blank line; unexpected unindent.\nCan you fix?\n. Happy to merge this @deniszh\u00a0\u2212 and glad you approved it, this is my very first merge in graphite after all :)\n@gmlexx This is nitpicking, but can you squash your changes?\n. @gmlexx And merged!\n. Would you be able to provide a test for that new method?\n. Thanks for writing this test @mechairoi!\nThis looks fine to me, but as a recent maintainer I would much prefer someone else has a look to it (poke @obfuscurity ?)\n. @Songmu Looks good to me ; but still too junior to press the merge button :)\n. All right, let\u2019s do this then :) @mechairoi, would you be able to squash this?\n. Aaaaaaand done!\n. Oh my, I spent quite some time wrestling with map/reduce trying to such things \u2212 I\u2019d use this for sure :)\n(Thanks for writing a test too!)\n. Looks good to me @Roguelazer. Would you mind squashing these?\n. Perfect @Roguelazer, thanks :)\nUnless there is opposition (thoughts @brutasse, @obfuscurity?) I\u2019d be happy to go ahead and merge this by the end of the week (to let some time for others to chime in).\n. And merged!\n. And merged!\n. Thanks @teamurko! Would you be able to provide a test for this new utc_to_local method?\n. Thanks @teamurko! Would you be able to provide a test for this new utc_to_local method?\n. > Can you elaborate on what you've done in the past day or so? It looks like you added \"GitHub releases\" to the 0.9.14 and 0.9.15 tags from last year. Just want to clarify that you didn't push anything on top of the 0.9.x releases by accident.\nSure thing. I did not commit nor pushed anything. It is my understanding that \u00ab Github releases \u00bb are merely a UI thing around tags. I simply found it weird that someone navigating to https://github.com/graphite-project/graphite-web/releases would be prominently invited to download the pre-release 0.9.13 (which was not labelled as such either) while we had two releases since then.\nI thought this uncontroversial (since there was one for 0.9.13 ; but it seems you deleted that as well) and just went bold on this. I was evidently mistaken and I apologize if I caused any turmoil.\n\nDeleted those releases. FWIW we don't typically do \"GitHub informational releases\" there, just tags. We use the formal documentation to provide release notes.\n\nNoted.\n\nNote: I temporarily revoked your committer status until we grok what has changed.\n\nNoted.\n. > I'm totally :+1: on taking charge and being forward-thinking, but just keep in mind that operating \"rogue\", especially when you haven't done much administratively with the project, may cause some raised eyebrows. Best to open an issue first (as you've done here) or discuss with folks in IRC before making any non-code type changes (as a committing team member).\nNoted, will keep that in mind \u2212 will come to IRC first next time I\u2019m being too enthusiastic. :)\nI appreciate the time taken to explain things and the renewed trust with the committer status.\n. Looks good to me as well.\n. Same as others \u2212 very much appreciate all this work! Will try to have a look later :)\n. Looks good to me!\n. This  is great @cbowman0 ! Love it :)\n. It looks like an upgrade to flake8/pyflakes\nIn this build the requirements installed are:\nflake8==2.6.0,graphite-web==0.10.0a0,mccabe==0.5.0,pycodestyle==2.0.0,pyflakes==1.2.3\nIn one of your previous PRs it was\nflake8==2.5.5,graphite-web==0.10.0a0,mccabe==0.4.0,pep8==1.7.0,pyflakes==1.0.0\n. If these prove annoying to fix we can also pin flake8 in tox.ini\n. Fine for me!\n. This renderView method is quite a beast. What would you think of refactoring it by extracting methods for each render format \u2212 at least for the new ones introduced here? (Ultimately it could be nice to have one class per format and use polymorphism?).\n. Thanks for this \u2212 not a fun patch to make! :)\nI\u2019d suggest going through this in two passes\u00a0\u2212 one ignoring E111, and one only fixing E111. Otherwise the diff makes it insane to spot issues (although autopep8 should be fairly safe to use).\nP.S. My slightly preferred way to control flake8 ignores is  via a dedicated entry in tox.ini\u00a0\u2212 not that it matters much :)\n```\n[flake8]\nexclude = .venv,.tox\nignore = E111,E231\nE111 indentation is not a multiple of four\nE231 missing whitespace after ','\n```\n. That is a good point\u00a0\u2212 did not think of that.\nGiven how @obfuscurity is on a spree to tackle open pull-requests, might be a good plan to wait for the backlog to further slim down.\n. I\u2019m using Graphite behind nginx \u2212 happy to help if needed.\n. > @JeanFred : Can you give me a step by step starting from installation\n\nincluding all configuration and file creations.\n\nI can share you my Ansible play later today - it's completely reproducible\n:-)\n. @anirbanroydas I quickly dumped relevant bits of my Ansible role at https://gist.github.com/JeanFred/f9357bb485fd9428d52895951e26875d \u2212 let me know if that helps.\nDo note that everything is pinned to 0.9.13.\n. >  is there any documentation on how to get this version of Graphite to run behind uWSGI (in case of an nginx setup) ?\n@Serphentas In case this helps, here is the Ansible play I use to run Graphite with UWSGI+Nginx. I have not tested it yet against 1.x series but it should mostly work there as well.\nEDIT: https://gist.github.com/JeanFred/f9357bb485fd9428d52895951e26875d\n  . @Serphentas Arf, forgot the link. Here it is : https://gist.github.com/JeanFred/f9357bb485fd9428d52895951e26875d :). @MFAnderson This should not be necessary, is it? mock is installed by tox for the tests, and is not necessary to run the application.\n. \u00ab uses n SQLite \u00bb \u2192 \u00ab uses a SQLite \u00bb ?\n. Trigges \u2192\u00a0Triggers\n. Looks like you are opening a double-quote at divideSeries that does not get closed?\n. This makes Travis lint check fail:\ngraphite-web/webapp/graphite/render/functions.py:2331:1: W293 blank line contains whitespace\n. You probably want to use self.assertTrue and self.assertFalse.\n. Generally speaking, I think it is better practice to keep test methods short, with explicit names on what they do. There are 50 assertions in this one test method! This implies that laters assertions might be dependant on the state left by previous test assertions\u00a0\u2212 if that is the case, it should be done as part of a setUp method.\nThe comments left actually make good basis for methods like test_append_dashboard_name_to_url_returns_200()\n. ",
    "arpat": "Hi, I've just set up graphite, and have the same django admin page you show, it looks OK to me.\nWhat static files were you expecting?\nCan you successfully set up django users and groups for graphite?\n. ",
    "genuss": "You could use cactiStyle(metric, system), where system may be \"si\" for multipliers of 1000 or \"binary\" for multipliers of 1024.\n. 1. That's not possible afaik. But it's pretty easy to make by yourself.\n2. Actually courier works for me, but I use master branch.\n   I suppose this feature came out later than 0.9.10. Maybe check out 0.9.12 ?\n. If you'd like, here it is.\n\nI don't think you need any additional package. I use python 2.6.6 and here is my pip freeze\nDjango==1.6.1\nTwisted==11.1.0\nceres==0.10.0\ncffi==0.8.1\ndjango-tagging==0.3.1\ngunicorn==18.0\nmocker==1.1.1\nnumpy==1.8.0\npycairo==1.8.10\npycparser==2.10\npyparsing==1.5.7\npython-memcached==1.47\npython-rrdtool==1.4.7\npytz==2013.8\nsimplejson==2.1.6\nsix==1.4.1\nsqlparse==0.1.10\ntxAMQP==0.4\nwhisper==0.9.10\nwsgiref==0.1.2\nzope.interface==4.0.5\n. Or use group()\ncactyStyle(group(aliasByMetric(casa.test.nono.system.memory.memory-buffered),aliasByMetric(casa.test.nono.system.memory.memory-free), ... ))\n. O you could also set up retention rates in carbon.\n. This could happen if the number of points on graph is less than number of points stored in whisper.\nTry to decrease observed time range (for example 10 minutes).\n. If my assumptions are correct, you could try using hitcount function to work around this behavior.\n. Yes, it is. hitcount assumes you have 1 sec intervals. You could use scale or use the rate metric, where statsd always stores values per second.\n. Checkout #431.\n. ",
    "davinmien": "There's no data smoothing per se, but you can try using a function like summarize to aggregate the data points so that each graph point represents something like 5 minutes.  Example:\n  summarize(metric_here, \"5min\", \"sum\")\nOr you could use movingAverage to get the average of N previous datapoints. \n  movingAverage(meric_here,5)\n. ",
    "timbunce": "I strongly agree that getting into a regular release cycle is more important that worrying too much about combing the backlog.\nSome suggestions that I've seen work elsewhere:\n- Work out your branching strategy (e.g. separate development and production branches) and corresponding version numbering strategy\n- Get someone who's done it before (ideally) to make a release and document all the steps carefully.\n- Plan to make one development release a month, plus production bug fix releases if needed.\n- Ask for a list of volunteers to act as a 'release manager' for just one of the upcoming development releases. They'd just need to follow the script. There's less pressure since they're only development releases and their only commitment is for a single release.\nWhat I've seen happen before is that each volunteer will polish and automate the process a little.\nSoon you'll have a release process that takes very little effort plus a pool of people with experience making releases.\nThe key point is that working on smoothing out the release process is an investment that will be more than paid back. Once momentum is restored to the project more people will be motivated to contribute and the backlog will be unblocked.\n. Only two issues left open on the 0.9.13 milestone, and one of those is this one!\nThe other one, #1081, hasn't been updated in two months.\nIs it really a blocker?\nIs it the only one?\n(I see carbon has only two issues as well, and those seem a little more active.) I wish I could be helpful rather than just pestering from the peanut gallery.\n. Only two issues left open on the 0.9.13 milestone, and one of those is this one!\nThe other one, #1081, hasn't been updated in two months.\nIs it really a blocker?\nIs it the only one?\n(I see carbon has only two issues as well, and those seem a little more active.) I wish I could be helpful rather than just pestering from the peanut gallery.\n. This case was opened on Apr 3, 2014. I deeply appreciate the desire to create a good release but it seems that momentum keeps getting lost.\nPlease, just ship something! Then ship something a little better in a month or two. Then ship something a little better a month or two after that. Version numbers are not a finite resource and momentum breeds contributions.\nOr perhaps it's time to admit defeat and, echoing @obfuscurity's comment in June 2014, tell people that \"projects like graphite-api and cyanite are the future of the Graphite community\" and that this project is near-dead.\n. ",
    "hggh": "I'm also interested in a new release of graphite-web. (I'm the debian maintainer) The current release 0.9.12 does not work any more with new Django 1.7 from debian experimental and newer Python libraries from Debian unstable.\nso .. :+1:\n. @obfuscurity I think the release should named 0.9.14, because there is already a 0.9.13 release on PyPi:\nhttps://pypi.python.org/pypi/graphite-web/0.9.13\n. I don't know what pypi will do if you have already a 0.9.13 installed and you replace the version. Better play safe and bump the version, it's only a number :)\n. don't worry @obfuscurity! thanks your work on graphite. it's just a number.\n. don't worry @obfuscurity! thanks your work on graphite. it's just a number.\n. thanks for your bug report. Yes it is debian specific, but since Graphlot will be removed from Graphite, I will upload soon a new package to Debian that has no support for Graplot enabled.\n. @pakal graphite-web 0.9.12+debian-6 was uploaded to unstable. will hit Jessie in ~5 days. thanks for your report and @obfuscurity  thanks for removing that old piece in Graphite.\n. having a lock at https://github.com/graphite-project/graphite-web/tree/master/webapp/content/js I see more very outdated javascript libraries. \"scriptaculous\" and \"ace\" I don't know if we could update that libraries or are they in use? scriptaculous was not updated since 6 years... \n. @obfuscurity and the \"content/js/window/\" is also only used by cli\n. There already debian packages available:\nhttps://packages.debian.org/search?keywords=graphite-carbon\nhttps://packages.debian.org/search?keywords=graphite-web\nThey will be shipped with Jessie - if there is demand, I can create backports and upload it to backports.org\n. Graphite is already packaged in Debian:\nhttps://packages.debian.org/search?keywords=graphite-web\nhttps://packages.debian.org/search?keywords=graphite-carbon\n. @obfuscurity you can remove the check for pytz, because pytz is shipped with graphite itself in https://github.com/graphite-project/graphite-web/tree/0.9.x/webapp/graphite/thirdparty\n. ",
    "jraby": "@obfuscurity, I plan on upgrading our clusters to 0.9.13 (+ our local patches) this week, I'll report back if I encounter any weirdness.\n. For the record, we've been running 0.9.x + https://github.com/graphite-project/carbon/pull/351 + https://github.com/graphite-project/whisper/pull/105 without issue for a few days.\n. Congrats!\nI'll test this out next week.\nOn Nov 7, 2015 1:19 AM, \"Jason Dixon\" notifications@github.com wrote:\n\nP.P.P.S. I've also hidden the 0.9.13-pre1 packages on PyPi.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/677#issuecomment-154635201\n.\n. I haven't written a unit test for this, but it is pretty easy to reproduce:\n- Set django tz to America/Montreal\n- hit /render with an absolute from (from=6:30_20140729)\n- observe that the data shown starts one hour later than the requested from \n\nThe behaviour does not trigger when using relative times.\nWith the patch, both relative and absolute times work.\n(we've been running with that patch for a month now without noticing any issue)\n. I don't have a test env for master at the moment, but I seem to recall that remote fetches worked when I tested it a few weeks ago.\nHere's the stack trace of a remote fetch using 0.9.x without the patch above:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 112, in get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"./graphite/render/views.py\", line 113, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"./graphite/render/evaluator.py\", line 10, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"./graphite/render/evaluator.py\", line 21, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression)\n  File \"./graphite/render/evaluator.py\", line 24, in evaluateTokens\n    return fetchData(requestContext, tokens.pathExpression)\n  File \"./graphite/render/datalib.py\", line 230, in fetchData\n    dbResults = dbFile.fetch( timestamp(startTime), timestamp(endTime), timestamp(now))\nTypeError: fetch() takes exactly 3 arguments (4 given)\n. I'd say feature too since the current code works. it is slow but it works.\n. I'll see what I can do, I don't have a setup based on master at the moment.\n. @steve-dave 0.9.x with local modifications.\n. Thanks Steve!\n. Thanks Steve!\n. This patch solves the problem I reported in #756.\n. @steve-dave I just tested this, there's a slight error, query_params is a list, not a dict.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 112, in get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"./graphite/render/views.py\", line 113, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"./graphite/render/evaluator.py\", line 10, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"./graphite/render/evaluator.py\", line 21, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression)\n  File \"./graphite/render/evaluator.py\", line 24, in evaluateTokens\n    return fetchData(requestContext, tokens.pathExpression)\n  File \"./graphite/render/datalib.py\", line 230, in fetchData\n    dbResults = dbFile.fetch( timestamp(startTime), timestamp(endTime), timestamp(now))\n  File \"./graphite/remote_storage.py\", line 117, in fetch\n    query_params['now'] = str( int(now) )\nTypeError: list indices must be integers, not str\nThe following diff fixes the issue for me and I can see graphs with data from remote hosts (https://github.com/datacratic/graphite-web/commit/e6affc1b3b53e2f81ea8551902fff03a55586493):\n```\ndiff --git a/webapp/graphite/remote_storage.py b/webapp/graphite/remote_storage.py\nindex 5d47b2e..7122217 100644\n--- a/webapp/graphite/remote_storage.py\n+++ b/webapp/graphite/remote_storage.py\n@@ -114,7 +114,7 @@ class RemoteNode:\n       ('until', str( int(endTime) ))\n     ]\n     if now is not None:\n-      query_params['now'] = str( int(now) )\n+      query_params.append(('now', str( int(now) )))\n     query_string = urlencode(query_params)\n connection = HTTPConnectionWithTimeout(self.store.host)\n\n```\nI don't really like the style of that, it would be much easier on the eye if query_params was a dict, but well, better stick to the existing style.\n. No problem, like I said above, this is not really a request for inclusion, more like a request for comments.\n. Yeah, we use 0.9.x at the moment, that's why all my patches are against this branch.\nI'll try to setup a test environment running master and forward port our patches there.\nThis might not be the right place to ask, but is there a plan to roll out a 1.0/master release in the foreseeable future?  I'd be much more inclined to put our effort in master and try to deploy it here if I knew there would be an official release soon(ish).\n. simply fyi,  issue  https://github.com/graphite-project/graphite-web/issues/795 had a different approach, using multiprocessing.pool.ThreadPool to do the fetches.\n(I'm currently forward porting/testing this patch to the new bulk fetch code)\n. Testing cluster setups on a single host?\nor having client visible instances on multiple ports (one 'private' instance per client) on a single ip and having a cluster frontend for 'internal use' aka not client visible that would query all the client instance?\nI'm not saying it is a good idea, but I could imagine having to build something like this.\n. fwiw, I still think using a threadpool makes the code easier to read:\n- https://github.com/datacratic/graphite-web/commit/14aee682581504ec8871e73a72c9bcb3797c87be\n- https://github.com/datacratic/graphite-web/commit/887c0b81f12cf56f5637200c30a7f26d82147a61\nThese would do mostly the same as the current PR, but with much less boiler plate code. (if we remove all the if USE_THREAD stuff)\n. As I've said in another thread a few month ago, we don't currently have an environment to test master, but we do have multiple 0.9.x clusters... So for the short term, I'm not sure I'll be able to help with forward porting this kind of patches to master.\nAs for the performance, with a 10 node cluster where 4 nodes are 80ms away from the cluster head,  the original code takes ~400ms to return results for a /metrics/find query, whereas the new code takes ~170ms.\nRegarding the commit cutoff to 0.9.x, I'm currently updating our fork to the latest code (with threads), and I planned to cleanup my prefetch cache patch from issue #1045 since, in our case, using that patch makes queries with multiple targets more than twice as fast.\nWould that be of interest to the project, or is the focus shifting on master from now on? (or at least after 0.9.13 is released)\n. Hi @deniszh,\nThis builds on #1010. #1010 does the bulk fetching for a single target only.\nFor example:  /render?target=*.*.*.a&target=*.*.*.b&target=*.*.*.c would yield 3 http requests.\nWith that patch all the targets are fetched at once.\nFor local environment where the backends are close to the frontend, it does not make much of a difference (although in my test in was a few ms faster). In environments where you have backends more than a few ms (>50ms) away from the frontend, the performance gain can be massive. (especially if your graphs have more than 4-5 targets)\n. Sure, but if it works well enough, maybe there's no point in having the option to turn it off. I use that setting to A/B test the performance, but other than that why would we want to have a 'slow' knob?\n. I totally agree with you @obfuscurity, I was thinking long term and didn't think there was a chance to get this into 0.9.13.\n. @obfuscurity I work at Datacratic and we have a few clusters based on aws machines and physical boxes distributed between east and west coast (and europe recently).\nThe biggest cluster has 10 backends and each backend receives between 100k and 150k metrics per minute depending on what is happening (most metrics come in every 10s).  The input rate is not that massive, it is just that each machine records its metrics locally. We then generate graphs on a single 'frontend', which is 10ms away from some machines and 80ms away from others.\n. Did you read the description in #1045  and https://github.com/datacratic/graphite-web/pull/2 ?\nI'm not sure it is that complicated, but it is certainly bolting something in front of the original design which may not be straight forward to understand...\nLet me know if you want to discuss parts of the patch, I'd be happy to explain the reasoning behind it.\n. Something like this?\n```\nPrefetch cache\nset to True to fetch all metrics using a single http request per remote server\ninstead of one http request per target, per remote server.\nEspecially useful when generating graphs with more than 4-5 targets or if\nthere's significant latency between this server and the backends.\n```\n. ",
    "biancalana": "There are 4 issues on  https://github.com/graphite-project/graphite-web/milestones/0.9.13, three of then already have pull request assigned (https://github.com/graphite-project/graphite-web/pull/1212, https://github.com/graphite-project/graphite-web/pull/1208 and https://github.com/graphite-project/graphite-web/pull/1157) the other one (https://github.com/graphite-project/graphite-web/issues/1081) looks solved waiting some more agreements from @SEJeff  @obfuscurity @dstufft\nAnything that needs additional help ?\n. What about https://github.com/graphite-project/carbon/issues/366 ?\nI'm facing this on everything after 0.9.12 of carbon...\n. +1, is there anybody actually using those functions ?\n. @obfuscurity this issue is still happening with current master\n. Does this commit fix metric containing '@' ?. ",
    "dstufft": "Well, you can't replace the version. PyPI will reject a re-upload of the same filename if it's ever been uploaded to PyPI. Version numbers are free, use another one ;)\nAlso if you want to get the RC out, you can use an RC version number, like 0.9.13rc1.\n. It might be that pip won't install pre-releases by default, because generally people want opt-in to prereleases.\n. It might be that pip won't install pre-releases by default, because generally people want opt-in to prereleases.\n. I wonder if this is related to Wheels. If I remember correctly Ubuntu 14.04 has pip 1.5.x which will install Wheels by default and if I look at pytz on PyPI it has a wheel file. Does it still install into different locations if you do pip install -r requirements.txt --no-use-wheel?\n. My guess is this code https://github.com/pypa/pip/blob/develop/pip/locations.py#L211-L259 in particular the use of the Distutils command is probably checking the current directory for setup.cfg. When not installing from a Wheel we cd into the build directory and out of the current directory.\n. File a pip bug please. \n\nOn Jan 10, 2015, at 2:02 PM, Jeff Schroeder notifications@github.com wrote:\nNope, just put the pip version in the requirements and I believe it will do the right thing. For several internal webapps at $real_job we deploy via wheels and have to do this.\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "munhitsu": "It's 2015 - will there ever be a new graphite release?\n. ",
    "greg-barnett": "An alternative/addition would to expand the error message, something along the lines of \"invalid pickle, running under Python {version} which supports Pickle Protocol {protocol}\".\n. ",
    "changbl": "I am seeing the same issue: Graphite-web dashboard authentication simply does NOT work. Every user which can access the dashboard via a web browser can modify/save dashboard without the need to login.\nOS: Ubuntu 14.04\nGraphite-web: 0.9.12\n. No one bother to review this commit and merge it?\n. ",
    "galdreiman": "Is there any solution for this issue?\nI really like Graphite and want to use it, but I cannot allow access to my systems data without authentication.\n. ",
    "jhwio": "Thanks for the quick reply. I'll see if we are going to port this patch.\n. @brutasse We didn't apply any patches yet. @rtoma: Did that line fix the issure for you?\n. ",
    "rtoma": "I was wondering the same. Is there a 0.9.12 fork somewhere with the patch applied?\n. After some digging I found the current master branch (@2014-04-28) already has a fix.\nBad code (in 0.9.12):\nreturn datetime.strptime(s,'%H:%M%Y%m%d').replace(tzinfo=tzinfo)\nFixed code (in master):\nreturn tzinfo.localize(datetime.strptime(s,'%H:%M%Y%m%d'), daylight)\n. @jhwbz: yes it did.\nOutput from my isolated tests:\n```\ntzinfo = \n[912] 21:00 20140101 -> datetime.datetime(2014, 1, 1, 21, 0, tzinfo=) (epoch=1388606400)\n[FIX] 21:00 20140101 -> datetime.datetime(2014, 1, 1, 21, 0, tzinfo=) (epoch=1388606400)\n^---- epochs equal, because bug only hits in summertime\n[912] 21:00 20140428 -> datetime.datetime(2014, 4, 28, 21, 0, tzinfo=) (epoch=1398715200)\n[FIX] 21:00 20140428 -> datetime.datetime(2014, 4, 28, 21, 0, tzinfo=) (epoch=1398711600)\n^---- epochs differ because of bug.\n```\nMy tester:\n```\n!/usr/bin/env python\nimport os\nimport sys\nfrom datetime import datetime\nfrom time import daylight\nsys.path.insert(0, \"/opt/graphite/webapp\")\nos.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"graphite.settings\")\nfrom graphite.thirdparty import pytz\nfrom django.conf import settings\ntzinfo = pytz.timezone(settings.TIME_ZONE)\nprint 'tzinfo = %r' % tzinfo\ndef test_912(s):\n    dt = datetime.strptime(s, '%H:%M %Y%m%d')\n    dt = dt.replace(tzinfo=tzinfo)\n    print '[912] %s -> %r (epoch=%s)' % (s, dt, dt.strftime('%s'))\ndef test_fix(s):\n    dt = datetime.strptime(s, '%H:%M %Y%m%d')\n    print '[FIX] %s -> %r (epoch=%s)' % (s, tzinfo.localize(dt, daylight), dt.strftime('%s'))\nif name == 'main':\n    tests = ['21:00 20140101', '21:00 20140428']\n    for t in tests:\n        test_912(t)\n        test_fix(t)\n```\n. ",
    "tony-dillon": "Hey Toni-Moreno,\nDo you think you could fix the merge conflicts?  I would love to get this merged in!\nThank you for your efforts.\n. ",
    "adriaanthomas": "Chris is er natuurlijk niet - ander iemand van CMI?\nAdriaan.\nOn 25 Aug 2014, at 16:54, \"Jason Dixon\" notifications@github.com<mailto:notifications@github.com> wrote:\nFWIW I was finally able to repro thanks to help from dlloyd in freenode. I hadn't realized you were looking at the dashboard with the side navigation panel.\n\nReply to this email directly or view it on GitHubhttps://github.com/graphite-project/graphite-web/issues/689#issuecomment-53274155.\n. ",
    "kds119": "Yes we are planning to open source it . We wanted to first measure the interest that the community has in this initiative . \n. @brutasse\n1. In the graphite-api docs itself, http://graphite-api.readthedocs.org/en/latest/ its mentioned that, Graphite-api is not for the use of graphite composer. It is only for third party graphing tools like grafana, etc.\n1. Integrating TSDB with graphite has two advantages:\n   a.  Existing graphite users can use TSDB\n   b.  Third party tools running on top of graphite need not use graphite-api, instead directly have a connection with TSDB through graphite\n   It means, we have a inline integration with TSDB, like RRD and whisper\n2. We have also solved the various integration problems like, tag value problems, step interval problems, etc. \n3. This would also be an easier setup rather than installing graphite api and other dependencies. \n. @manolama \nWe would love to publish our work . We are just wrapping up on the internal review process and will publish the code soon . \n. ",
    "tungstencarbide": "I do want to get my changes up, but have to take care of a couple things in the code before I do. Unfortunately, I'm involved in other things now, so getting the time to deal with it is problematic.\nI added a \"tags\" panel to the interface. It's kind of clunky but it works. I have not adapted graphlot to use tags, though, so that's not working. \n. ",
    "grinapo": "Pity. I would have tried Apache Kudu which does have an OpenTSDB compatible interface but it seems this project has been missing in action. There is no good-looking opensource distributed backends around (and InfluxDB has been split to undistributed and paid), apart from the quite overcomplex hadoop based ones, and kudu looked like a pretty simple one. Unfortunately its interface isn't popular -- yet.\n. ",
    "fguillen": "I'm asking to my appreciated dev-op.. but maybe I don't get answer until friday.\n. The issue is solved, was a problem with the resolution and the aggregation formule. I was sending information once per hour, as the most of the datapoints were null the aggregation formule was setting null on the change of resolution.\n. ",
    "jiwanrai": "Hey Fernando, \nFor the app related datapoints we are using the following retention period:\nretentions = 1m:30d,1h:5y\nLet me know if there are any other details you need.\n. ",
    "bjornpost": "I'm seeing this as well. @gtaylor: have you found a solution to this?\n. ",
    "gtaylor": "Seems to drastically inflate my numbers. Looks like hitcount assumes that your data is in 1-second intervals, whereas my statsd setup is using the suggested flush interval of 10 seconds. Graphite is set up to aggregate over 10 seconds, as well (as the docs suggest).\n. ",
    "ihaworth": "I've been seeing similar issues.  I've tracked it down to being dependent upon the size of the time range over which the data is being viewed.  For example, the following is the same data set shown over 3 different size time intervals:\n2 Months:\n\n1 Week:\n\n1 Day:\n\nThe left-hand graphs are the monotonic count, and the right-hand graphs are the derivative.  As you can see, each change should be a count of 1, so all the derivatives should be 1.  However, they're around 0.04 for the 2 month graph, 0.33 for the 1 week graph, and (correctly) 1 for the 1 day graph.\nThis illustrates that the underlying data is sound (as confirmed with the format=raw parameter) but the rendering of it gets more inaccurate as the time interval increases.\nThis isn't a critical issue for me, but it would be nice to see a solution.\n. ",
    "sravanireddyl": "I am new to graphite graphs, I am also facing the same problem with y axis value. \nAny one got sollution?\n. I have a counter metric which increases by 1 at a time,\nthe first graph renders when I select time range of 1 day, and 2nd graph rendered when I select time range of 1 week.\nThe y-axis value changing according to time but i need the fixed Y-axis value without fractional values(exact count of metric).\n\n\n. @deniszh \nThank you for your reply.\nI have applied functions \"scaletoseconds\" and \"nonNegativeDerivative\", still am getting the graph with fractional values as below.\n\nHow can i identify number of logins per minute with that graph.?\n(This question seems to be silly to you but I am very new to all these graphs and dashboards.)\n. But the value changing when I change the date range.\n. ",
    "bilal-fazlani": "Is this issue resolved ? I am facing the same issue\n. @deniszh \nstorage-aggregation.conf\n```\n[min]\npattern = .lower$\nxFilesFactor = 0.1\naggregationMethod = min\n[max]\npattern = .upper(_\\d+)?$\nxFilesFactor = 0.1\naggregationMethod = max\n[sum]\npattern = .sum$\nxFilesFactor = 0\naggregationMethod = sum\n[count]\npattern = .count$\nxFilesFactor = 0\naggregationMethod = sum\n[count_legacy]\npattern = ^stats_counts.*\nxFilesFactor = 0\naggregationMethod = sum\n[default_average]\npattern = .*\nxFilesFactor = 0.3\naggregationMethod = average\n```\nstorage-schemas.conf\n```\n[default]\npattern = .*\nretentions = 10s:6h,1min:6d,10min:365d\n[catch_em_all]\npattern = .*\nretentions = 10s:6h,1min:6d,10min:365d\n```\n. ",
    "tterrace": "This confused me for a while as well. I sent a few test counts via nc and the Y values on the graph are incorrect for a 60 minute time range, but correct for 59 minutes or below. \nHere's at a 60 time range:\n\nThen at 59 minutes:\n\nYou can see the Y values in the second screen shot are correct, but the first screen shot shows values that are half of what they should be.. ",
    "loisaidasam": "For anyone still struggling with this, it seems that the fine folks over at Etsy have gone into a bit of detail writing about it and how to fix it:\nhttps://github.com/etsy/statsd/blob/master/docs/graphite.md\nTL;DR:\nThe default aggregationMethod for storage is average (example config: storage-aggregation.conf.example), where as for counters it should be sum.\nTo fix:\nEither\n1. Counter end with .count\nMake your counter key end with .count, for example my.data.point.count. This will hit the [sum] config and use aggregationMethod = sum\nOr\n2. [statsd] in storage_aggregation.conf\nAdd the following section to /opt/graphite/conf/storage-aggregation.conf before the [default_average] section:\n[statsd]\npattern = ^stats_counts.*\nxFilesFactor = 0\naggregationMethod = sum\n. My pleasure. I'm kinda an amateur when it comes to sys-admin'ing, and I spent wayyyyy too much time struggling to setup apache 2.4 w/ mod_wsgi with the previous example vhost, so hopefully these comments and edits will help others going forward.\n. Similar issue: https://github.com/brutasse/graphite-api/issues/51\n. ",
    "SalahAdDin": "I have the same error with python 3.4 and Django 1.6.5\n. ",
    "H0L0GH05t": "I'm also getting that error with python 2.7 and Django 1.6.5 (using postgresql as my db)\n. ",
    "bruce-lyft": "mangled merge, please ignore\n. Thanks!\n. :+1:  for this...   There's no good workaround for this right now.\n. ",
    "dpkp": "Unfortunately request.REQUEST was deprecated in django 1.7...\nhttps://docs.djangoproject.com/en/dev/ref/request-response/#django.http.HttpRequest.REQUEST\nprobably better to itertools.chain(request.GET.lists(), request.POST.lists())\n. +1\n. ok, though you realize that the \"fix\" you are referencing isn't related to this bug.  admittedly brutasse fixed this bug back in march in a separate patch, unrelated to timezone issues: https://github.com/brutasse/graphite-api/commit/86e55af2f5f20fd01e0380865890f4abd98b2745\nare you planning to merge with his fork entirely?  even if yes, what about the 0.9.x branch which really should get this bug fix too?\n. his tree has the fix included.  if you merge the entire tree you're good.  if you just merge the timezone commit (i.e. cherry-pick it), it will depend on your merge strategy.  but please do fix identity() in the 0.9.x branch.  the fix here is low-hanging fruit.  would be a shame to leave it broken for too long.\n. awesome -- thanks\n. awesome -- thanks\n. ",
    "GLundh": "Really cool stuff. Looking forward to see this stuff (hopefully) merged.\n. ",
    "tbenk": "The retentions:\nretentions = 15s:1h,30s:2h,60s:4h,6m:24h,1d:1y\nI do not have multi-node graphite cluster but i do have 10 carbon-relay instances with 10 carbon-cache instances per carbon-relay. So time synchronisation should not be a problem in general.\nAs 4h is quite near the next interval of 24h, i have changed my retentions now to:\nretentions = 15s:61m,30s:121min,60s:361min,6m:1441min,1d:525601m\nI will keep collecting some data for the next hours and I will check if the problem stays the same.\nFYI: despite having a lot of carbon-cache and carbon-relay instances on the same host, cpu usage is not a problem for now. The cpus are still idling most of the time.\n. Yes, i completely removed the whisper files created before.\nAnd it seems that now that the retention intervals are changed the displayed resolution does not change anymore.\n. Yes., the problem is solved for me by choosing the correct retention times.\n. ",
    "markolson": "1220 took care of this\n. This isn't strictly germane to this issue, but is splitting functions.py up into functions/*.py, along with corresponding tests and some callback/hook into SeriesFunctions feasible for 0.10? I think that it may make things more maintainable, understandable, and testable going forward, especially as the number of functions grows.\n. With this, you can do horrible things like /render?yMin=0&yMax=100&target=scale(randomWalkFunction(%22sample%20data%22),12)&target=alpha(color(alias(areaBetween(group(constantLine(70),constantLine(90))),%22%22),%22yellow%22),0.5)&target=alpha(color(alias(areaBetween(group(constantLine(90),constantLine(100))),%22%22),%22red%22),0.5)&width=950 now!\n\n. @brutasse fwiw, adding w=1 to the params here switches it to show non-whitespace only changes. https://github.com/graphite-project/graphite-web/pull/936/files?diff=split&w=1\n. For why the test is failing: I think that that the first list of expected values should be [0,1,1,1,0,1,0,0,1,1], and the 2nd should be either [0,0,0,0,0,0,0,0,0,1] or [0,1,0,0,1,0,0,1,0,1], depending on transformNull/drawNullAsZero.\n. Derp. Reading the code of changed, it should be [0,0,0,0,0,0,0,0,0,1] for the 2nd.\n. ",
    "shakefu": "Having looked at the code for smartSummarize a bit, I'd be interested in writing and submitting an upstream patch to allow smartSummarize (or perhaps a parallel function, which would drop the deprecated alignToFrom param) to align to weeks, month, quarters or years.\nPerhaps a signature like:\nalignSummarize(requestContext, seriesList, intervalString, align=None)\nWhere align could be one of ('minute', 'hour', 'day', 'monday' ... 'sunday', 'month', 'quarter', 'year'), or use the current behavior of aligning to minute, hour or day if align is None.  Day of the week would start/end buckets on that day for the week, while month, quarter, or year would start/end buckets on the appropriate first of the month/quarter/year.\nIs this something that you guys would be interested in accepting upstream?\n. ",
    "marccardinal": "Our biggest installation so far collects and serves around 300 000 metrics\ncollected at second granularity. This represents a stream of roughly 10 TB\nper day that is sent to 8 backend carbon processes. I actually don't know\nif this constitutes a large installation but we are having some challenges\ngrowing it further... specially on the reporting side (graphite-web).\nCheers,\nMarc Vieira-Cardinal\nHead of Operations and Infrastructure\nDatacratic Inc. (formely Recoset Inc.)\nwww.datacratic.com\nOn Sat, Jun 21, 2014 at 3:48 AM, steve-dave notifications@github.com\nwrote:\n\nOut of interest, what constitutes a large scale Graphite system? Do you\nknow much about how the bigger users are using it?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/744#issuecomment-46747280\n.\n. \n",
    "axos88": "+1\n. @mathias-baumann-sociomantic, can you tell us what you got out of it? I can't get my head around it either\n. ",
    "heliodor": "To be specific, this is for urls of the form: /render?target=target.path.goes.here&format=raw\n. I chose to use the raw interface over the json one in a project on which I'm working, and I ran into this str/repr issue at some point when verifying my data. I made this fix and was able to confirm that my data was coming out the raw interface with full precision and the correct values.\nThat's in Python 2.7. I just realized we need to consider Python 3 too, so I just tried this in Python 3.3 and repr() works the same as in Python 2.7. But interestingly, str() in Python 3 prints floats with full precision, unlike in Python 2, so in Python 3 this issue we're discussing for the raw interface does not exist. But that also means the raw interface right now produces different results depending on whether it's running with Python 2 or Python 3!\nHere's how things look like at the interactive prompt.\nPython 2.7.5:\n    >>> str(0.1234567890123456789012)\n    '0.123456789012'\n    >>> repr(0.1234567890123456789012)\n    '0.12345678901234568'\n    >>> float(str(0.1234567890123456789012)) == 0.1234567890123456789012\n    False\n    >>> float(repr(0.1234567890123456789012)) == 0.1234567890123456789012\n    True\nPython 3.3.2:\n    >>> str(0.1234567890123456789012)\n    '0.12345678901234568'\n    >>> repr(0.1234567890123456789012)\n    '0.12345678901234568'\n    >>> float(str(0.1234567890123456789012)) == 0.1234567890123456789012\n    True\n    >>> float(repr(0.1234567890123456789012)) == 0.1234567890123456789012\n    True\nI don't think this change should cause a problem for third parties. The numbers printed in the raw interface are of variable length ('0.12', '0.123456', '100.12', etc) so the only reasonable way to parse the data from the raw interface is to split the string on the comma character that separates the numbers. Then, I would think it's safe to assume that in most, if not all languages, the float parser can handle an excessive number of digits.\n. Uhh, that should not be there. I'm a bit of a newbie to github, so I guess the sys.path change snuck into this pull request. Good catch! Let me create another pull request without it.\n. Ok, I removed the sys.path commit. I didn't know that subsequent commits on the branch get added to the pull request!\n. ",
    "mscuthbert": "My guess is that's what the OP is suggesting (well, probably a hybrid, .six-based version).  After supervisor, carbon/graphite is now the second most used Python package not to support Py3 [and two of the next three either have a perfect Py2/3 drop-in replacement (PyMYSQL for MySQL-Python) or is a backport of a Py3 library].  It would be a wonderful thing to be able to use in Py3 if any of the developers are interested.\nSource:\nhttps://python3wos.appspot.com\n. ",
    "takluyver": "If anyone is looking to work on this, modernize and future are two tools that can automate a lot of the porting work - like 2to3, but they aim to keep compatibility with Python 2 as well.\nModernize is generally the more conservative of the two, and it will insert use of the well-known six compatibility library. Future's futurize tool makes the code more like idiomatic Python 3, using its companion future library to make it work on Python 2.. Yup, the tools can help with the straightforward syntax changes, like changing print x to print(x), but figuring out the changes to the data model still needs human work. I'm all too familiar with those issues ;-). But the job is much more tractable with the tools to help than doing it all manually.. Attempted in #2139 .. With a few tweaks to the redis interface, Travis now agrees with me that the only remaining failure is the _listdir_counter one I posted above.. I think ceres is compatible. The test is making an assumption about the inner workings of ceres - specifically, how it looks at directories - that doesn't appear to hold on Py3. It should be easy to fix the test, but I don't understand why it's checking that, so I've left it for now.. Pip has recently gained the ability to look at the Requires-Python metadata to check compatibility with the Python version it's running on. E.g. if you pip install ipython on Python 2, you should get a compatible 5.x version: it will skip over the 6.x releases that require Python 3.\nThis was quite a recent shift, and only a few packages are taking advantage of it so far. But most packages still support Python 2, so the latest versions will probably work.. I've updated the pinned requirements to the latest available versions. It looks like they support Python 3 except for txamqp - the work has been done for that one, but it's not yet in a release.. Here's a better link for the txamqp work: https://github.com/txamqp/txamqp/pull/1. The only code that I can see importing txamqp is the check-dependencies script, so I assume it's only there as an indirect dependency via carbon.. Yup, I agree. Do you want that removed as part of this PR?. I've cleaned up simplejson and StringIO as suggested. I'm leaving the pickle fallback, because the import works a bit differently on Python 3: you always import pickle, and it automatically uses the compiled version if it's available, or the pure Python fallback if not.. I think I've responded to all the comments - let me know if you think I've missed some.. Thanks everyone :-). txAMQP 0.8.0 was just released with Python 3 support, if someone wants to bump the pinned version again.. It does, but io.StringIO is strictly unicode only, whereas StringIO.StringIO attempts to handle bytes and unicode. I'm not sure if Python 2's ConfigParser handles unicode properly, so this is a convenient fudge.. The future import isn't needed to make this work - explicit relative imports work in Py2 as well.\nThe future import makes it a bit harder to add back implicit relative imports (i.e. what I changed this from). I can remove it if you prefer.. Ah, I misunderstood how slots work. Fixing.. As far as I can work out, overriding the Python 3 pickle interface works this way even when it's using the compiled implementation, so we do need both branches. I'll make it clearer what's going on.. Automatic tool being overenthusiastic. I'll fix it.. Yes-ish. I think it will work, but:\n\nIt may be surprising that constructing the object accepts either str or bytes, but its .write() method only accepts bytes.\nThe naming is not ideal from a Py3 perspective - I expect a StringIO object to be accepting and returning str data, not bytes. This is my fault from the porting.\nIn the longer term, I'd see it as a stopgap solution. It's best for application code to be clear about where it's handling unicode and where it's handling bytes, and minimise the code paths that can be either.. \n",
    "cclauss": "Is this issue closed?. This issue can be closed?. Is this issue closed?. Is this issue closed?. ",
    "sonnysideup": "Nice catch guys.\n. ",
    "mbrochh": "Yea I think I will have to setup an authorization layer in front of graphite. My app will send the GET requests to my auth view, that view will check if that user would have access to the requested graphite graph, then forward that request to graphite, take the response from graphite and return that response to the user.\nOh dear... I'm sure this is not going to play well with grafana...\n. ",
    "Silox": ":+1: \n. ",
    "cschneid": "It's what google returned to me when I searched up the api. I'm fairly new to the project, so wasn't familiar with version numbers as much. \nSounds like 1.0 shouldn't even be on RTD? \n. ",
    "heshiyou": "@brutasse django.conf.urls.defaults has been removed in Django 1.6.\n. ",
    "macolu": "@SEJeff  Unfortunately I can't test with master branch on my installation, for various reasons.\nMoreover, this part of code is rather different between master and 0.9, so my fix can't be applied on master (function no longer exists).\n. ",
    "powo": "I replaced the above by the following and got my 1-week graph displayed in 1.6s instead of 49s.\nI would appreciate a review:\npython\n  __consolidation_functions = dict(\n    sum = lambda usable: sum(usable),\n    average = lambda usable: float(sum(usable) / len(usable)),\n    max = lambda usable: max(usable),\n    min = lambda usable: min(usable),\n  )\n  def __consolidatingGenerator(self, gen):\n    try:\n      cf = self.__consolidation_functions[self.consolidationFunc]\n    except KeyError:\n      raise Exception, \"Invalid consolidation function!\"\n    buf = []\n    valcnt = 0\n    for x in gen:\n      valcnt += 1\n      if x is not None:\n        buf.append(x)\n      if valcnt == self.valuesPerPoint:\n        valcnt = 0\n        if buf:\n          yield cf(buf)\n          buf = []\n        else:\n          yield None\n    if buf:\n      yield cf(buf)\n    else:\n      yield None\n    raise StopIteration\n. ",
    "BErelentless": "hello\nhave you been able to finally speedu you rendering graph?\n. hello\nhave you been able to finally speedu you rendering graph?\n. ",
    "mbethke": "Whow, this is such a massive improvement I'm surprised it hasn't made it in a release in over three years! We have a large amount of metrics that would be a headache to configure manually WRT resolution so we'd rather waste a bit of cheap disk space and use a fine resolution for everything if it didn't result in several minutes of CPU time for a handful of graphs.\nI'd be happy to contribute any unit tests that might be missing if that would help get this into v1.0.3, alas I can't figure out in what way the existing ones aren't adequate. Any ideas, @esc and @drawks?\n@steve-dave You're right, the first line of the __consolidate function is redundant. When I ran into this problem I thought about dropping the whole buf logic and using only this list comprehension but in the end @powo's optimization is probably more efficient as it does less copying.. @deniszh It's such a big speedup that I believed it to be a fluke at first. One of our Whisper files takes just over a minute to condense down to 10 JSON data points on reasonably beefy hardware (2.4.GHz E5-2630, 128 GB), most of which is spent pointlessly manipulating lists of None in __consolidatingGenerator (as I think @powo has pointed out already, the remove operation is O(n) over the length of the list, and with most entries being None this is basically done n times for an effective O(n\u00b2)).\nWith the patch, this time is down to a little over two seconds!. As I couldn't rebase this PR myself, I worked on my own repo and just opened  #2051.\nI didn't see any issues with extra values but the 'average' function didn't work due to a cast in the wrong order and the new-since-2014 functions 'first' and 'last' were missing. Feel free to cherry-pick and merge/reject as is easiest :). ",
    "bbrown": "Sorry, here's another PR for the Dashboard documentation page. I didn't see the other TODO in the page until today. In the future, I'll bundle all my changes in a single PR.\n. ",
    "teroxik": "Ok, I will update documentation. I had problem with parsing and it took me a while to find out why only the stdev function doesn't work.\n. ",
    "cornelf": "\ud83d\udc4d \n. ",
    "1mentat": "I am seeing this as well.\n. ",
    "darkstar": "I have this too. All graphs are empty boxes.\nEdit: Simple fix:\n1) in /usr/share/graphite-web/static/js, link to jquery.flot.time.js:\n   ln -s  ../../../javascript/jquery-flot/jquery.flot.time.js\n2) edit /usr/lib/python2.7/dist-packages/graphite/templates/graphlot.html and insert this line in the beginning (you'll see where):\n <script type=\"text/javascript\" src=\"../content/js/jquery.flot.time.js\"></script>\n. Nothing is trying to load jquery.flot.time.js. You only get the error the OP posted in the Chrome JS console, saying that \"to use time mode you need the flot.time plugin\". Probably a newer/incompatible version of flot in Ubuntu?\nI'm not in the office right now but I can check next week and copy/paste the exact error message(s)\n. It might be a bug in the Ubuntu graphite package... \nThere definitely are some files in /usr/share/graphite-web/static/jswhich link to ../../../javascript/jquery-flot/, which resolves to /usr/share/javascript/jquery-flot/\nThe Ubuntu packager(s) might have split graphite from flot and introduced a bug...\n. ",
    "pavel-odintsov": "@darkstar thanks so much! Your advice helped me :) \n. There is illustration about this issue. I've graphed this both graphics at 22/34.\nThat's not summarized data. We have all data points up to 22/34.\n\nThat's data with applied summarize function. As you can see we have data only up to 22/30:\n\n. Graphite version: 0.9.12+debian-3\n. Hello, again!\nLooks like I've found issue. It's related with function \"summarize\" from this file: https://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/render/functions.py\nI've traced this code and got following output:\nbash\nSat Apr 16 23:44:22 2016 :: Current timestamp 1460839440\nSat Apr 16 23:44:22 2016 :: Bucket interval is: 1460839200\nAs you can see we calculated bucket interval is lesser than current timestamp. That's because Graphite rounding down timestamp value when creating \"bucket\".\nActually data for last interval calculated correctly but we are using rounding down and store all this data on rounded down bucket (which lesser than current time for up to 599 seconds with 600 seconds interval).\nFor example for current timestamp 1460839440, interval 600 we could store 1460839200. So we store more recent data in bucket which older than this value in 240 seconds (in worst case this value lower than 599 seconds).\nSo I've changed folllwing code to (I've just replaced rounding down to rouding up):\nbash\nbucketInterval = timestamp_ + (timestamp_ % interval)\nnewEnd = series.end + (series.end % interval) + interval\nbucketInterval = timestamp_ + (timestamp_ % interval)\nIn this case we are using rounding up and will store data for latest 599 seconds in \"future\" value. \nThat's not perfect solution. As perfect solution we could align start and end of interval by interval value (in my case 600 seconds).\nBut it produces more complicated code and I could not implement proof of concept for it.\n. ",
    "Gibby": "Working for me, only had to add the following the Apache, you can ignore https part if you don't need/require it.\nRewriteEngine On\nRewriteCond %{HTTPS} !=on\nRewriteRule ^/?graphite/(.*) https://%{SERVER_NAME}/graphite/$1 [R,L]\nProxyPass /graphite http://127.0.0.1:8002/graphite\nProxyPassReverse /graphite http://127.0.0.1:8002/graphite\n. ",
    "kareblak": "Someone hit that button!\n. ",
    "kg4zow": "Any word on when this will be merge into the 0.9.x branch, and maybe even an 0.9.13 release?\n. ",
    "angelatlarge": "Tried to test this feature (from master) under gunicorn / nginx: it worked partially: some paths were parsed correctly but some were not. Seems like there is an issue with parsing static paths?\n\n. Did that, and now I am getting a plain old 404, so that change helped :)  I am not 100% confident that my static files are were they should be, though I did manage to run the collectstatic command.  Is the \"static\" prefix now erroneosely stripped now?\nRequest URL:    http://my_server_here/graphite/static/js/ext/resources/css/ext-all.css\n'js/ext/resources/css/ext-all.css' could not be found\n. Yup, that did it. Thanks!\n. BTW, do we expect the path without the trailing slash not to work?\nUsing the URLconf defined in graphite.urls, Django tried these URL patterns, in this order:\n^graphite/\n^graphite\\/static\\/(?P<path>.*)$\nThe current URL, graphite, didn't match any of these.\n. ",
    "dipthegeezer": "Will this be merged in to the 0.9.x branch, and maybe even an 0.9.13 release? It would help us greatly.\n. ",
    "Jessicalx": "@brutasse Which version includes this fix? I am using 0.9.13-prel\n. ",
    "vlebedev": "\narch:~:% pip2 show pyparsing\n---\nName: pyparsing\nVersion: 1.5.7\nLocation: /usr/lib/python2.7/site-packages\nRequires: \n\n. Installed pyparsing==2.0.2, now have the following trouble:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/django/core/handlers/base.py\", line 112, in get_response\n    response = wrapped_callback(request, _callback_args, *_callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/dashboard/views.py\", line 130, in dashboard\n    'schemes_json' : json.dumps(config.schemes),\nNameError: global name 'json' is not defined\n. just in case:\n\narch:pixel:% sudo pip2 freeze                                                                                 \nBeaker==1.6.4\nDjango==1.6.5\nJinja2==2.7.3\nMako==1.0.0\nMarkupSafe==0.23\nPyYAML==3.11\nTerminator==0.97\nTwisted==11.1.0\nansible==1.6.6\narandr==0.1.7.1\nbatti==0.3.8\nboto==2.31.1\nceres==0.10.0\ndjango-tagging==0.3.1\necdsa==0.11\ngunicorn==19.1.0\nmeld3==1.0.0\nmercurial==3.0.2\nmocker==1.1.1\nobkey==1.0\nparamiko==1.14.0\npsutil==2.1.1\npycairo==1.8.10\npycrypto==2.6.1\npyparsing==2.0.2\npython-memcached==1.47\npython-musicbrainz==2.1.5\npytz==2014.4\npyxdg==0.25\nsimplejson==3.6.0\nsupervisor==3.0\ntxAMQP==0.4\nvirtualenv==1.11.6\nwhisper==0.9.10\nwsgiref==0.1.2\nxlwt==0.7.5\nzope.interface==4.1.1\n\n. Well, those errors are gone now, but here is the output from ./run-graphite-devel-server.py:\n\n[30/Jul/2014 14:11:32] \"GET /dashboard/ HTTP/1.1\" 200 1067\n[30/Jul/2014 14:11:32] \"GET /static/css/dashboard.css HTTP/1.1\" 404 98\n[30/Jul/2014 14:11:32] \"GET /static/js/ext/resources/css/ext-all.css HTTP/1.1\" 404 113\n[30/Jul/2014 14:11:32] \"GET /static/js/ext/ext-all.js HTTP/1.1\" 404 98\n[30/Jul/2014 14:11:32] \"GET /static/css/dashboard-default.css HTTP/1.1\" 404 106\n[30/Jul/2014 14:11:32] \"GET /static/js/ace/ace.js HTTP/1.1\" 404 94\n[30/Jul/2014 14:11:32] \"GET /static/js/ext/ux/DataViewTransition.js HTTP/1.1\" 404 112\n[30/Jul/2014 14:11:32] \"GET /static/js/ext/adapter/ext/ext-base.js HTTP/1.1\" 404 111\n[30/Jul/2014 14:11:32] \"GET /static/js/composer_widgets.js HTTP/1.1\" 404 103\n[30/Jul/2014 14:11:32] \"GET /static/js/dashboard.js HTTP/1.1\" 404 96\n[30/Jul/2014 14:11:32] \"GET /static/js/ace/theme-textmate.js HTTP/1.1\" 404 105\n[30/Jul/2014 14:11:32] \"GET /static/js/ace/mode-json.js HTTP/1.1\" 404 100\n[30/Jul/2014 14:11:32] \"GET /static/css/dashboard-default.css HTTP/1.1\" 404 106\n[30/Jul/2014 14:11:32] \"GET /static/js/ext/adapter/ext/ext-base.js HTTP/1.1\" 404 111\n[30/Jul/2014 14:11:33] \"GET /static/js/ext/ext-all.js HTTP/1.1\" 404 98\n[30/Jul/2014 14:11:33] \"GET /static/js/ext/ux/DataViewTransition.js HTTP/1.1\" 404 112\n[30/Jul/2014 14:11:33] \"GET /static/js/composer_widgets.js HTTP/1.1\" 404 103\n[30/Jul/2014 14:11:33] \"GET /static/js/dashboard.js HTTP/1.1\" 404 96\n[30/Jul/2014 14:11:34] \"GET /static/js/ace/ace.js HTTP/1.1\" 404 94\n[30/Jul/2014 14:11:34] \"GET /static/js/ace/mode-json.js HTTP/1.1\" 404 100\n[30/Jul/2014 14:11:34] \"GET /static/js/ace/theme-textmate.js HTTP/1.1\" 404 105\n\nLooks like that something is broken with static assets now.\n. ",
    "astrostl": "Repeating drawks comment - requirements.txt isn't updated.\n. ",
    "aronatkins": "I'm using the offset function now, but the reason for the failure was non-obvious.\n. No, because it loses the granularity that you would have with a true moving sum.. No, because it loses the granularity that you would have with a true moving sum.. ",
    "jnovack": "Are we the only two people that have this issue?\n. ```\npip install graphite-web carbon\nDownloading/unpacking graphite-web\n  Downloading graphite-web-0.9.13.tar.gz (2.2MB): 2.2MB downloaded\n  ... boring warnings suppressed ...\nSuccessfully installed graphite-web\nCleaning up...\npip install carbon\nDownloading/unpacking carbon\n  Downloading carbon-0.9.13.tar.gz (47kB): 47kB downloaded\n  ... boring warnings suppressed ...\nSuccessfully installed carbon\nCleaning up...\n```\nPerforming the same test (multiple times from multiple servers) as my initial post says, I am NO LONGER seeing this issue and CAN NOT replicate it.  I will consider this closed until I can replicate.\nThank you all for working on such an obscure issue.  I, and I hope many others, appreciate the hard work and dedication to the open source cause.\n. Let's be clear, thank you for DOING. I did the easy part. :)\n. ",
    "alexdean": "Causes a JSON parse error in Grafana. ref https://github.com/grafana/grafana/issues/651\n. Not sure I understand. You'd like me to try setting allow_nan = False? Sounds like that will just result in a ValueError when I try to render the data.\n. The assertion that this behavior is consistent with most JavaScript based encoders and decoders seems incorrect. Infinity is a parse error in both Chrome & Firefox. \nChrome:\n\nFirefox:\n\n. Encoding as null would be fine for my immediate case. I'm not sure what other implications that might have.\n. Re: JSON parsing of Infinity... My previous examples were mistaken, though the result is the same.\nFirefox:\n\nChrome:\n\n. I completely understand maintaining backwards-compatibility.\nThat said, I'm not sure that it's correct to say this only affects some minority of JSON parsers. So far Python is the only language I've tried where Infinity actually decodes to the expected value.\n```\n$ irb\n\n\nRUBY_VERSION\n=> \"2.1.2\"\nrequire 'json'\n=> true\nJSON.parse 'Infinity'\nJSON::ParserError: 757: unexpected token at 'Infinity'\n    from /Users/alex/.rvm/rubies/ruby-2.1.2/lib/ruby/2.1.0/json/common.rb:155:in parse'\n    from /Users/alex/.rvm/rubies/ruby-2.1.2/lib/ruby/2.1.0/json/common.rb:155:inparse'\n    from (irb):3\n    from /Users/alex/.rvm/rubies/ruby-2.1.2/bin/irb:11:in `'\n```\n\n\n```\n$ php -a\nInteractive shell\nphp > echo phpversion().\"\\n\";\n5.4.24\nphp > var_dump(json_decode('Infinity'));\nNULL\n```\n. ",
    "zdenekpizl": "Yeah, Already have asked on Librelist without any reply till now :-/ Going to cross post to Launchpad also.\nThank you, .zp.\n. That's great, could anybody from elders check this PR proposal and elaborate in it? Thank you, regards, .zp.\n. I noticed if there is huge in-memory cache and the physical file is not update often (let's say modify time of wsp is more than 2 hours ago) such file is not taken into account when returning data. It is probably configurable by some option. \nCould it be this case?. Hi Dan,\ndo you mean to add i.e. two items into CLUSTER_SERVERS of local_settings.py? I am in doubts, because documentation says:\n\"\"\"\nCLUSTER_SERVERS\nDefault: []\nThe list of IP addresses and ports of remote Graphite webapps in a cluster. Each of these servers\nshould have local access to metric data to serve. The first server to return a match for a query will be\nused to serve that data. Ex: [\u201c10.0.2.2:80\u201d, \u201c10.0.2.3:80\u201d]\n\"\"\"\nThat means, as far as I understand it, there will be data served from ONE of these servers but not both in union. Or am I mistaken?\nMy expectation is to get data of requested time-range from both servers, because each of them does have part of that time-range.. Awesome, I will try it. This option is not covered in latest documentation at graphite.readthedocs.io/en/latest/config-local-settings.html.\nThank you, with regards, .zp.. ",
    "ryneeverett": "I ran into the same issue and solution a month ago. Arch's python-cairo also works fine.\n. ",
    "kali-hernandez": "If you have a graphite instance running in a different host, the metric queries won't work because of the cross-domain issue. Json output format for graphs is already able to return jsonp objects, but not metric queries. It changes no behaviour on the current functionality but only extends it to allow metric query on cross-domain setups.\nI might agree, however, to the also on other paths that use the json_response_for. I just needed it for metric querying so far.\n. I'm using graphitus, a simple dashboard generator. It calls the api for searching metrics, using ajax, which won't work without jsonp when using a different domain. I can guess other scenarios where you might want to search for metrics using ajax and there's no apparent reason to have jsonp option on data request with format json but not on the metric search.\n. @obfuscurity yes, grafana seems to use CORS indeed for this goal.\nOne could probably argue though that in some hosting scenarios you won't be able to set CORS in your web server.\nAlso since the jsonp response is enabled for graph data when requested as json format, and the change to enable it on metric search as well is kind of minimal (see commit), I don't think it adds any complexity at all as @torkelo says.\nIMHO it seems just a matter of unifying criteria. If there is no reason to enable jsonp support on metric search, there should be no reason to have it for data request, and vice-versa.\n. I could do that, however I believe the line:\npython\ncontent = \"%s(%)\" % (jsonp, content)\nIs anyway a bug since it's missing the string formatting after the %, and it seems to be inherited from the older:\npython\nreturn HttpResponse(\"%s(%s)\" % (jsonp, json.dumps(matches)), mimetype='text/javascript')\nand would besides trigger an error\nValueError: unsupported format character ')' (0x29) at index 4\nI checked master branch for that that missing s, and it happens as well.\n. Cool, thank you!\n. ",
    "jjshoe": "So, you don't believe the config should make it more clear that changing these values in the config isn't a value add? \n. I'm telling you right now that changing the value in carbon.conf has no affect on where data is stored on disk. The only way the directory of storage on disk changed was when I set the environment variable. \n. Unless I'm misunderstanding https://github.com/graphite-project/carbon/blob/master/lib/carbon/conf.py#L512 I see no way it's looking at the config file.\n. ",
    "Kenterfie": "summarize with 1h parameter combine all same hours of all days. not only from one day. you get a summarize of all metrics of the given interval and i want a separation by day. i want to combine only metrics from single hour per day, that i can see for example: monday between 1:00 and 2:00 are 45 visitors and on tuesday between 1:00 and 2:00 are 32. the only way i can do this currently to call summarize function for every day, what it makes very inefficient.\n. [xxx]\npattern = ^stats.xxx.\nretentions = 1m:7d,15m:30d\nhttp://take.ms/7Z21m\nrender?width=800&height=400&target=summarize(stats.XXXX.ms475.*.memoryUsage,\"1h\",\"avg\")&lineMode=staircase&from=-8days&format=png\n. You can see only Saturday and Sunday on the graph, but you can see also from=-8days at render parameters\n. 0.9.12 ubuntu package\nsorry, i forgot one digit\n. I have everything updated 3 hours ago, so yes. The latest commit is 2aee2bc, but the problem was with 0.9.15 the same.\n. In my case not necessary. I don't use replication between the two graphite hosts. Both hosts holding different data. I use the render node to have one api endpoint.\nAnd yes, im very sure i use the latest version. I have compared my webapp folder with the latest git master.\n. On render host\nThu Sep 08 17:27:57 2016 :: Retrieval of sortByMaxima(summarize(..)) took 0.900179\nThu Sep 08 17:27:57 2016 :: Retrieval of sortByMaxima(summarize(..)) took 0.893999\nThu Sep 08 17:27:58 2016 :: Retrieval of sortByMaxima(summarize(..)) took 1.089880\nThu Sep 08 17:27:58 2016 :: Retrieval of sortByMaxima(summarize(..)) took 1.084184\nThu Sep 08 17:27:59 2016 :: Retrieval of sortByMaxima(summarize(..)) took 0.905862\nThu Sep 08 17:27:59 2016 :: Retrieval of sortByMaxima(summarize(..)) took 0.900468\nThu Sep 08 17:28:00 2016 :: Retrieval of sortByMaxima(summarize(..)) took 0.901901\nThu Sep 08 17:28:00 2016 :: Retrieval of sortByMaxima(summarize(..)) took 0.900201\nThu Sep 08 17:28:01 2016 :: Retrieval of sortByMaxima(summarize(..)) took 1.088676\nThu Sep 08 17:28:01 2016 :: Retrieval of sortByMaxima(summarize(..)) took 1.084956\nThu Sep 08 17:28:03 2016 :: Retrieval of sortByMaxima(summarize(..)) took 1.081588\nThu Sep 08 17:28:03 2016 :: Retrieval of sortByMaxima(summarize(..)) took 1.357989\nThu Sep 08 17:28:04 2016 :: Retrieval of sortByMaxima(summarize(..)) took 0.908793\nThu Sep 08 17:28:04 2016 :: Retrieval of sortByMaxima(summarize(..)) took 0.897626\nThu Sep 08 17:28:05 2016 :: Retrieval of sortByMaxima(summarize(..)) took 1.374054\nThu Sep 08 17:28:06 2016 :: Retrieval of sortByMaxima(summarize(..)) took 2.076580\nThu Sep 08 17:28:06 2016 :: Retrieval of sortByMaxima(summarize(..)) took 1.084878\nThu Sep 08 17:28:07 2016 :: Retrieval of sortByMaxima(summarize(..)) took 1.082674\nThu Sep 08 17:28:07 2016 :: Retrieval of sortByMaxima(summarize(..)) took 0.910757\nThu Sep 08 17:28:08 2016 :: Retrieval of sortByMaxima(summarize(..)) took 0.899178\nThu Sep 08 17:28:09 2016 :: Retrieval of sortByMaxima(summarize(..)) took 1.102875\nThu Sep 08 17:28:09 2016 :: Retrieval of sortByMaxima(summarize(..)) took 1.083390\nThu Sep 08 17:28:10 2016 :: Retrieval of sortByMaxima(summarize(..)) took 1.099460\nThu Sep 08 17:28:10 2016 :: Retrieval of sortByMaxima(summarize(..)) took 1.080182\nAnd on the storage node the forwarded request from render host\nThu Sep 08 17:30:07 2016 :: Retrieval of .. took 0.013766\nThu Sep 08 17:30:08 2016 :: Retrieval of .. took 0.015371\nThu Sep 08 17:30:08 2016 :: Retrieval of .. took 0.015578\nThu Sep 08 17:30:11 2016 :: Retrieval of .. took 0.015855\nThu Sep 08 17:30:11 2016 :: Retrieval of .. took 0.008226\nThu Sep 08 17:30:12 2016 :: Retrieval of .. took 0.014218\nThu Sep 08 17:30:12 2016 :: Retrieval of .. took 0.011763\nThu Sep 08 17:30:13 2016 :: Retrieval of .. took 0.013829\nThu Sep 08 17:30:13 2016 :: Retrieval of .. took 0.017468\nThu Sep 08 17:30:14 2016 :: Retrieval of .. took 0.014424\nThu Sep 08 17:30:14 2016 :: Retrieval of .. took 0.012316\nThu Sep 08 17:30:15 2016 :: Retrieval of .. took 0.010825\nThu Sep 08 17:30:15 2016 :: Retrieval of .. took 0.015276\nThu Sep 08 17:30:17 2016 :: Retrieval of .. took 0.014506\nThu Sep 08 17:30:18 2016 :: Retrieval of .. took 0.013076\nThu Sep 08 17:30:18 2016 :: Retrieval of .. took 0.010940\nThu Sep 08 17:30:19 2016 :: Retrieval of .. took 0.010102\nThu Sep 08 17:30:21 2016 :: Retrieval of .. took 0.009397\nThu Sep 08 17:30:21 2016 :: Retrieval of .. took 0.010097\nThu Sep 08 17:30:22 2016 :: Retrieval of .. took 0.012833\nThu Sep 08 17:30:22 2016 :: Retrieval of .. took 0.014324\nwhen i run the same query like above with wget from render host against storage host\ntime wget \"example.de:1444/render=sortByMaxima(..)\"\ni get this timing.\nreal    0m0.045s\nuser    0m0.007s\nsys 0m0.000s\nsomething strange happens on the render host, what i dont understand. both storage hosts response under 100ms.\n. other ideas what cause the hugh delay between data gathering and data delivery?\n. Here are a exact query\nThu Sep 08 20:15:33 2016 :: Retrieval of sortByMaxima(summarize(statsite.machines.{xy462}.gz1029059_1.{memoryUsage,cpuUsage},'1d','avg')) took 1.083491\n. Ok i think i found the issue\nRetrieval of summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg') took 0.194305\nRetrieval of summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg') took 0.010299\nRetrieval of summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg') took 0.010212\nRetrieval of summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg') took 0.016453\nRetrieval of summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg') took 0.192053\nRetrieval of summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg') took 0.011103\nRetrieval of summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg') took 0.016286\nRetrieval of sortByMaxima(summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg')) took 0.903806\nRetrieval of sortByMaxima(summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg')) took 1.907195\nRetrieval of sortByMaxima(summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg')) took 1.086445\nRetrieval of sortByMaxima(summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg')) took 0.903029\nRetrieval of sortByMaxima(summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg')) took 1.091595\nRetrieval of sortByMaxima(summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg')) took 0.924130\nRetrieval of sortByMaxima(summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg')) took 0.902272\nsortByMaxima is 10 times slower, without a reason in my opinion. the returned data is very small. should not take 1 second\n. Ok, the above code was distorted by caching. The problem still consist \nRetrieval of summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg') took 0.942750\nRetrieval of summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg') took 0.898640\nRetrieval of summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg') took 0.903292\nRetrieval of summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg') took 2.085402\nRetrieval of summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg') took 0.900449\nRetrieval of summarize(statsite.nodeXY.{memoryUsage,cpuUsage},'1d','avg') took 1.898584\n. I had tried to use 0.9.15, but it has to many bugs which are only fixed in later commits and dont like to merge every necessary commit himself to get a \"stable\" version. The regression is visible on all kinds of requests. I have tried now different ones and all very slow compared to http request to storage host directly. I have written now a small script to bypass this behaviour in this specific subject, but i hope later a performance improved version is released and someone found the reason for it. When i had more time i would do it himself.\nSmall note\nI dont know, why no stable version is released since month to fix the most annoying bugs in 0.9.15\n. Thanks @obfuscurity for your tests. This shows me the exact same behavior like i noticed. The regression is very low, but the problem itself exist. The time differences between backend and frontend are in my opinion to high. In my case mostly factor 10. I dont know if you want to fix this in the next version, but currently is it impossible to use the graphite cluster like intended. I have to use the backends directly to bypass the performance issue.\n. Sorry was busy. I have applied the patch above and get these outputs.\nThu Nov 17 17:45:02 2016 :: DEBUG: started all fetches at 1479401102.64\nThu Nov 17 17:45:02 2016 :: DEBUG: started all fetches at 1479401102.64\nThu Nov 17 17:45:02 2016 :: DEBUG: completed fetch in  0.092004 at 1479401102.7\nThu Nov 17 17:45:02 2016 :: DEBUG: started fetch at 1479401102.7\nThu Nov 17 17:45:02 2016 :: DEBUG: completed fetch in  0.000179 at 1479401102.7\nThu Nov 17 17:45:02 2016 :: DEBUG: started fetch at 1479401102.7\nThu Nov 17 17:45:02 2016 :: DEBUG: completed fetch in  0.000188 at 1479401102.7\nThu Nov 17 17:45:02 2016 :: DEBUG: started fetch at 1479401102.7\nThu Nov 17 17:45:02 2016 :: DEBUG: completed fetch in  0.000193 at 1479401102.7\nThu Nov 17 17:45:02 2016 :: DEBUG: started fetch at 1479401102.7\nThu Nov 17 17:45:02 2016 :: DEBUG: completed fetch in  0.000211 at 1479401102.7\nThu Nov 17 17:45:02 2016 :: DEBUG: completed all fetches in  1.048134 at 1479401102.7\nThu Nov 17 17:45:02 2016 :: Retrieval of aliasByNode(keepLastValue(statsite.xxxx.yyyy.tt.),4) took 1.062054\nThu Nov 17 17:45:02 2016 :: DEBUG: started all fetches at 1479401102.72\nThu Nov 17 17:45:02 2016 :: DEBUG: started fetch at 1479401102.97\nThu Nov 17 17:45:03 2016 :: DEBUG: completed fetch in  0.091993 at 1479401103.07\nThu Nov 17 17:45:03 2016 :: DEBUG: started fetch at 1479401103.07\nThu Nov 17 17:45:03 2016 :: DEBUG: completed fetch in  0.000132 at 1479401103.07\nThu Nov 17 17:45:03 2016 :: DEBUG: started fetch at 1479401103.07\nThu Nov 17 17:45:03 2016 :: DEBUG: completed fetch in  0.000252 at 1479401103.07\nThu Nov 17 17:45:03 2016 :: DEBUG: started fetch at 1479401103.07\nThu Nov 17 17:45:03 2016 :: DEBUG: completed fetch in  0.000168 at 1479401103.07\nThu Nov 17 17:45:03 2016 :: DEBUG: started fetch at 1479401103.07\nThu Nov 17 17:45:03 2016 :: DEBUG: completed fetch in  0.000159 at 1479401103.07\nThu Nov 17 17:45:03 2016 :: DEBUG: started fetch at 1479401103.07\nThu Nov 17 17:45:03 2016 :: DEBUG: completed fetch in  0.000157 at 1479401103.07\nThu Nov 17 17:45:03 2016 :: DEBUG: completed all fetches in  1.045338 at 1479401103.07\nThu Nov 17 17:45:03 2016 :: Retrieval of aliasByNode(keepLastValue(statsite.xxxx.yyyy.tt2.),4) took 1.054909\nThu Nov 17 17:45:03 2016 :: DEBUG: started all fetches at 1479401103.08\nThu Nov 17 17:45:03 2016 :: DEBUG: started fetch at 1479401103.33\nThu Nov 17 17:45:03 2016 :: DEBUG: started fetch at 1479401103.33\nThu Nov 17 17:45:03 2016 :: DEBUG: started fetch at 1479401103.39\nThu Nov 17 17:45:03 2016 :: DEBUG: completed fetch in  0.088522 at 1479401103.42\nThu Nov 17 17:45:03 2016 :: DEBUG: started fetch at 1479401103.42\nThu Nov 17 17:45:03 2016 :: DEBUG: completed fetch in  0.000198 at 1479401103.42\nThu Nov 17 17:45:03 2016 :: DEBUG: started fetch at 1479401103.42\n@obfuscurity REMOTE_PREFETCH_DATA is disabled in my case\n. Thu Nov 17 17:58:17 2016 :: DEBUG: started all fetches at 1479401897.07\nThu Nov 17 17:58:17 2016 :: DEBUG: started fetch at 1479401897.14\nThu Nov 17 17:58:17 2016 :: DEBUG: started fetch at 1479401897.15\nThu Nov 17 17:58:17 2016 :: DEBUG: completed fetch in  0.091928 at 1479401897.24\nThu Nov 17 17:58:17 2016 :: DEBUG: started fetch at 1479401897.24\nThu Nov 17 17:58:17 2016 :: DEBUG: completed fetch in  0.000256 at 1479401897.24\nThu Nov 17 17:58:17 2016 :: DEBUG: started fetch at 1479401897.24\nThu Nov 17 17:58:17 2016 :: DEBUG: completed fetch in  0.000179 at 1479401897.24\nThu Nov 17 17:58:17 2016 :: DEBUG: started fetch at 1479401897.24\nThu Nov 17 17:58:17 2016 :: DEBUG: completed fetch in  0.000206 at 1479401897.24\nThu Nov 17 17:58:17 2016 :: DEBUG: started fetch at 1479401897.24\nThu Nov 17 17:58:17 2016 :: DEBUG: completed fetch in  0.000199 at 1479401897.24\nThu Nov 17 17:58:17 2016 :: DEBUG: completed all fetches in  1.041527 at 1479401897.24\nWhy are so many fetches for one query? This makes no sense to me, what cause the hugh delay of >1 second. Only two web api endpoints are defined, so only two fetches needed.\n. I have tried the branch above, but without noticable performance improvement in my case. The timings are nearly the same. I would help more, when i had the time to write a own implementation currently, but thanks for your effort to fix this bottleneck. In my opinion the http fetch himself is not the reason of the hugh render times on frontend side.\nLess than 10ms on backend (rendering) + ~60ms transmit of data + magic on webapp frontend (800ms). Tue Nov 29 11:22:17 2016 :: Retrieval of aliasByNode(keepLastValue(statsite.YXZ.),4) took 1.055472\nTue Nov 29 11:22:17 2016 :: Total json rendering time 1.059517\nTue Nov 29 11:22:20 2016 :: Retrieval of aliasByNode(keepLastValue(statsite.YXG.),4) took 1.035930\nTue Nov 29 11:22:20 2016 :: Total json rendering time 1.039196\nTue Nov 29 11:22:22 2016 :: Retrieval of aliasByNode(keepLastValue(statsite.YRG.),4) took 1.079519\nTue Nov 29 11:22:22 2016 :: Total json rendering time 1.083786\nTue Nov 29 11:22:23 2016 :: Retrieval of aliasByNode(keepLastValue(statsite.EFG.),4) took 1.045477\nTue Nov 29 11:22:23 2016 :: Total json rendering time 1.053923\nTue Nov 29 11:22:28 2016 :: Retrieval of aliasByNode(keepLastValue(statsite.EEG.),4) took 1.054044\nTue Nov 29 11:22:28 2016 :: Total json rendering time 1.058418\nTue Nov 29 11:22:30 2016 :: Retrieval of aliasByNode(keepLastValue(statsite.EED.),4) took 1.051837\nTue Nov 29 11:22:30 2016 :: Total json rendering time 1.058412\nTue Nov 29 11:22:32 2016 :: Retrieval of aliasByNode(keepLastValue(statsite.EDS.),4) took 1.053786\nTue Nov 29 11:22:32 2016 :: Total json rendering time 1.058767\nTue Nov 29 11:22:34 2016 :: Retrieval of aliasByNode(keepLastValue(statsite.XDF.),4) took 1.065419\nTue Nov 29 11:22:34 2016 :: Total json rendering time 1.069655\nTue Nov 29 11:22:35 2016 :: Retrieval of aliasByNode(keepLastValue(statsite.RGH.),4) took 1.050324\nTue Nov 29 11:22:35 2016 :: Total json rendering time 1.054232\nTue Nov 29 11:22:39 2016 :: Retrieval of aliasByNode(keepLastValue(statsite.NBG.),4) took 0.880803\nTue Nov 29 11:22:39 2016 :: Total json rendering time 0.884234\nall renderings around the 1 second on frontend server\nTue Nov 29 10:51:33 2016 :: Retrieval of statsite.NBG.* took 0.010941\nTue Nov 29 10:51:33 2016 :: Total pickle rendering time 0.012926\n~10ms on backend\nother logs later, but you can see, they dont very different from logs before. sure, i do this later after my main work. https://gist.github.com/Kenterfie/32b3041bc29bea87aa94d0c826b149e0 here are anonymised log with your changes @DanCech . is your latest code and i have updated now the log from above. Sorry was my mistake. Was on your master branch. Now i have your clustering branch. Here you can find log.\nhttps://gist.github.com/Kenterfie/52c2f8caacdcbb9a06c3f86348a2c15f. @deniszh i have tested this pull request without any noticable speed improvements. all request are slower than before with master.\nMy test case needs currently 4 seconds. With this parallel_fetch commit the request time is increased to 7 seconds. Overall i would say this changed nothing on the current performance issue, perhaps in some special cases, but in my setup it has no impact.\n. Very strange. I noticed it since 1.1.3 and havn't changed anything else. Possible a config default change to force it?. Names of metricts like \"root.yxz.count\". Before i changed it i could write \"root.yxz.Count\" and also \"root.yxz.count\" both with the same result.. ",
    "ptbrowne": "I'll add the tests for movingAverage later. Thanks for the quick response :-)\n. ",
    "jiapanlong": "Thanks. Got resolved, This is because the mod_wsgi.so version issue, need upgrade the python version with compile the latest mod_wsgi module\n. ",
    "robbkidd": "I'm using 0.9.12 on Ubuntu 12.04 by judicious application of hw-cookbooks/graphite. Would it be crazy to go down the path of an omnibus package install of graphite and all its dependencies? I think most of what the Heavy Water graphite cookbook installs by way of python packages could be done in an embedded python in an omnibus install.\n. /me discards omnibus idea\n/me starts reading up on debhelper and dh-python\n(Update: dh_python deprecated in 12.04+, apparently. Directs to using dh_python2)\n. ",
    "thorrsson": "Hey folks,\nmy $0.02 for the comments from today:\n- I have boxes initialized for testing on OEL6.5, Centos (6.5, 7),\n  Fedora 20 and RHEL6.5 (I can probably initialize a EL 5 box fairly quickly\n  too)\n- I don't think that we need to get into the omnibus game, the existing\n  cookbooks are solid and fairly easy to wrap (as are the puppet modules)\n- mod_python should not be an option, there was a noticeable performance\n  difference compared to mod_wsgi, I would stick to wsgi\n- I wouldn't mind including optional nginx/uwsgi configs as bonus\n  points, though I am not sure if we should simply express a dependency on\n  the webserver and provide example configs or actually manage the site\n  configs and the webserver. Whenever I deployed graphite at $previous_jobs,\n  The webserver was always managed by my Configuration Management As well,\n  If we handle the webserver and the config we may need to account for shared\n  server situations where folks don't run graphite exclusively on the box. I\n  have also seen a ton of different existing apache configs (sites-* vs /conf\n  vs hacking the httpd.conf etc) that we may not want to try and work around\nHi @mckern! This is where I am at as of last night:\nI am building on Centos 6.5 for the time being, though spent a good part of\nyesterday evening getting an environment that works (thanks to just\nreplacing my laptop :) )\nI have started with Carbon, based off of master as well as ensuring I can\nport back to 0.9.x\nI put in a PR last night that Jason merged to fix the install-lib location\nthat was broken (it dumped carbon libs into ./webapp), and we will need to\nmake a decision on https://github.com/graphite-project/carbon/issues/148\nregarding managing the installation of the init scripts or not (I hacked\nthis into my local fork in order to build on Centos, and determines if we\nneed to worry about init script installation vs just creating the scripts)\nI can produce an RPM that is targeted at python 2.7 and manages Carbon and\nthe init scripts using bdist_rpm at this point. I also generated the .spec\nwhile I was at it and found that we needed to add the long_description\n(which was also merged today). I also tried out building with FPM, which\nhad some quirks that I didn't bother working around. The artifact installs\ncleanly, with a dependency on whisper (which I built out really quickly\njust to get carbon installed) The services all start as expected as well.\nOn Sat, Aug 23, 2014 at 5:24 PM, Ryan McKern notifications@github.com\nwrote:\n\nHTTPD + mod_wsgi wounds like a plan. @thorrsson\nhttps://github.com/thorrsson, how's things on your end?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/864#issuecomment-53171866\n.\n. I'll try to clean up what I have and push it up to my fork today\n\nOn Sunday, August 24, 2014, Ryan McKern notifications@github.com wrote:\n\n@thorrsson https://github.com/thorrsson Have you got this work in a\nbranch somewhere? And have you got the .spec files you've created available\nanywhere?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/864#issuecomment-53211622\n.\n. I pushed the .spec for carbon to:\nhttps://github.com/thorrsson/carbon/tree/rpm_spec\nIt is working at this point based on master of my fork (which is right now\nthe same as master of the upstream)\n\nthis spec was generated with :\npython2.7 setup.py bdist_rpm --packager \"Tim Hunter tim@thunter.ca\"\n--provides graphite --obsoletes 0.9.11 --spec-only\nand the RPM can be built from the same command without the --spec-only :)\nThis was an effort to make sure it could build 'as-is' so that I knew the\nstarting point was sane\nOn Sun, Aug 24, 2014 at 3:56 PM, Tim Hunter tim@thunter.ca wrote:\n\nI'll try to clean up what I have and push it up to my fork today\nOn Sunday, August 24, 2014, Ryan McKern notifications@github.com wrote:\n\n@thorrsson https://github.com/thorrsson Have you got this work in a\nbranch somewhere? And have you got the .spec files you've created available\nanywhere?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/864#issuecomment-53211622\n.\n. Hey folks,\nI have had some personal life happen and am probably not going to have any\nspare cycles to work on this. Sorry to bail out, feel free to keep me on\nthe thread and I can provide input and possibly review if time permits.\nCheers\nTim\n\n\nOn Tue, Aug 26, 2014 at 10:06 AM, Ryan McKern notifications@github.com\nwrote:\n\nHere's something that Fedora is cooking up:\nhttp://pkgs.fedoraproject.org/cgit/graphite-web.git/tree/\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/864#issuecomment-53454293\n.\n. \n",
    "vincentbernat": "Current distro packaging for Debian/Ubuntu can be found here:\n http://sources.debian.net/src/graphite-web/latest/debian/\n http://sources.debian.net/src/graphite-carbon/latest/debian/\n. ",
    "ArturGajowy": "Any chance to have 1.0.2 RPMs for centos anytime soon?. Does this mean \"just forget it, the python world uses pip\"?. graphite-web, carbon, whisper\nI tried installing from pip, but got this:\n\n. ",
    "wavded": "I have the latest graphite installed from pip.  It's quite simple to illustrate:\nSay I have saved graph I've loaded and I want to make the background black (it is currently white):\n\nI make my change and then save the graph:\n\nWhen I click to open the saved graph, the background goes back to white:\n\nThis happens with any of the graph customization options I've tried.  It only seems to persist the graph data.\n. Latest from pip... graphite-web-0.9.12.tar.gz.\n. Hmmm... I'll take a peak, I'm running Ubuntu 12.04.  Everything has been installed through pip (w/ sudo).  Other than that I'm not sure.  Perhaps a reinstall?  I'll check the logs in a bit.\n. @obfuscurity I think something got hosed, I'm going to try to remove and reinstall.  Do you know what files I need to keep in order to not loose my data? (couldn't find that in the docs)\n. alright, got it back up, thx for your help, saves seem to be working now\n. ",
    "Haacked": "An author owns the copyright for the life of the author + 70 years before it enters the public domain. Clearly we're not in that situation here. :smile:\nUnless Chris Davis has explicitly assigned copyright over to some organization, you can't just remove his name. He still owns the copyright to his contributions. Likewise, unless other contributors explicitly assign copyright over to the project (typically via some CLA), they all own their individual contributions.\nPerhaps the simplest approach is:\n\nCopyright 2014, Chris Davis and The Graphite Developers\n\nAgain, IANAL.\n. Copyright law doesn't require any notice at all. A copyright notice is mostly to communicate others that the owner intends to claim the copyright, so don't mess with us.\nSo it's really up to you how you want to communicate that. What you showed seems fine to me.\n. > limited to his years of contribution?\nWhoops, forgot to answer this part. I would limit it to the years of his contributions.\n. ",
    "arnaudlimbourg": "\ud83d\udc4d\n. ",
    "tulanian": "Whisper.\n\nhttps://www.vizify.com/es/501a727aba8a2200020003b3\nhttp://www.gunternation.com\nhttp://www.slowfooddublin.com\n\"I think we delight to praise what we enjoy because the praise not merely\nexpresses but completes the enjoyment; it is its appointed consummation.\"\nReflections on the Psalms, C.S. Lewis\nOn Tue, Sep 2, 2014 at 3:27 PM, Jason Dixon notifications@github.com\nwrote:\n\nWhich storage backend are you using (Whisper, Ceres, etc)?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/925#issuecomment-54159316\n.\n. \n",
    "andrewbaxter": "Thank you for taking the time to comment!\n\nJust log the exception and make sure you return None from process_exception.\n\nWhile I'd be glad to make the change, this would log all exceptions from all entry points (rather than just the render ones) right?  Is that the preferred behavior?\n. PR for #933 (some code changes)\n. Thank you for the feedback, it was very helpful.  I did not consider local_settings - now I believe I've fixed that as you suggested.\n. Thanks for the comments, again!  I'm sorry, I should have spotted some of that myself.\nHere's what the output looks like now:\nWed Sep 10 21:18:54 2014 :: Exception encountered in <GET http://127.0.0.1:8000/render/?width=586&height=308&_salt=1410401934.282&target=q(1)>\nTraceback (most recent call last):\n  File \"/.../graphite-web/env/lib/python2.7/site-packages/django/core/handlers/base.py\", line 113, in get_response\n    response = callback(request, *callback_args, **callback_kwargs)\n  File \"/.../graphite-web/webapp/graphite/render/views.py\", line 110, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/.../graphite-web/webapp/graphite/render/evaluator.py\", line 7, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/.../graphite-web/webapp/graphite/render/evaluator.py\", line 18, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression)\n  File \"/.../graphite-web/webapp/graphite/render/evaluator.py\", line 24, in evaluateTokens\n    func = SeriesFunctions[tokens.call.funcname]\nKeyError: u'q'\n. Thanks for the comments, again!  I'm sorry, I should have spotted some of that myself.\nHere's what the output looks like now:\nWed Sep 10 21:18:54 2014 :: Exception encountered in <GET http://127.0.0.1:8000/render/?width=586&height=308&_salt=1410401934.282&target=q(1)>\nTraceback (most recent call last):\n  File \"/.../graphite-web/env/lib/python2.7/site-packages/django/core/handlers/base.py\", line 113, in get_response\n    response = callback(request, *callback_args, **callback_kwargs)\n  File \"/.../graphite-web/webapp/graphite/render/views.py\", line 110, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/.../graphite-web/webapp/graphite/render/evaluator.py\", line 7, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/.../graphite-web/webapp/graphite/render/evaluator.py\", line 18, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression)\n  File \"/.../graphite-web/webapp/graphite/render/evaluator.py\", line 24, in evaluateTokens\n    func = SeriesFunctions[tokens.call.funcname]\nKeyError: u'q'\n. I've added the UI function menu entry and tested it out locally.\n. The goal was really to be able to reimplement derivative on the fly, so that's probably correct for the same reason this method was used in derivative itself.  There may have been other uses for which that property was important as well, but it's been a while.\n. No relation :horse: \n. Yeah, I'm fairly certain we're not related.\n. It may be a good way to participate in future github discussions though.\n. ",
    "miloszszymczak": "As shown on the picture, there are some null values, but usually everything's fine even with them, error shows up at random while Grafana auto-update.\n[null, 1402905000], [null, 1402905600], [null, 1402906200], [null, 1402906800], [null, 1402907400], [null, 1402908000], [null, 1402908600], [null, 1402909200], [null, 1402909800], [null, 1402910400], [null, 1402911000], [null, 1402911600], [null, 1402912200], [null, 1402912800], [2.1949026000000003, 1402913400], [2.1984015666666665, 1402914000], [2.2147782333333335, 1402914600], [2.1966841999999995, 1402915200], [2.1873180000000003, 1402915800], [2.224719733333333, 1402916400],\n. As I wrote above - because of some glitch aliasByNode receives (\"10min\", \"max\") tuple instead of (2, 3) as arguments - I logged it right from the function. That's why it complains about non-integer indices.\n. Do you have any ideas on how I can debug it? I have access to the code and since the error appears several times a day, I can log any information you need to find out what's happening.\n. ",
    "mlowicki": "```\nTraceback:\nFile \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\" in get_response\n  111.                         response = callback(request, callback_args, callback_kwargs)\nFile \"/opt/graphite/webapp/graphite/render/views.py\" in renderView\n  113.         seriesList = evaluateTarget(requestContext, target)\nFile \"/opt/graphite/webapp/graphite/render/evaluator.py\" in evaluateTarget\n  10.   result = evaluateTokens(requestContext, tokens)\nFile \"/opt/graphite/webapp/graphite/render/evaluator.py\" in evaluateTokens\n  21.     return evaluateTokens(requestContext, tokens.expression)\nFile \"/opt/graphite/webapp/graphite/render/evaluator.py\" in evaluateTokens\n  29.     return func(requestContext, args)\nFile \"/opt/graphite/webapp/graphite/render/functions.py\" in aliasByNode\n  1022.     series.name = '.'.join(metric_pieces[n] for n in nodes)\nFile \"/opt/graphite/webapp/graphite/render/functions.py\" in \n  1022.     series.name = '.'.join(metric_pieces[n] for n in nodes)\nException Type: TypeError at /render\nException Value: list indices must be integers, not str\n```\nPOST\nformat u'json'\nfrom    u'-24h'\ntarget u\"aliasByNode(summarize(share.prod.{ams,os}.{db1,db2,faviconer1,faviconer2,front1,front2}.gauges.memory.MemFree,'10min','max'),2,3)\"\nmaxDataPoints u'424'\nuntil   u'now'\n. ",
    "SITZ": "@obfuscurity I tried various combinations but, it only picks up first value in * position and does not perform all the calculations.\n. @obfuscurity Does integral sum / subtract current data point from last available (non-null) data with no data sent between them? In that case, It should work just fine.\nI hope am wrong but, I'm under impression that integral only accumulates from neighbouring time. Is it?\n. I will give it a shot once more and update back here.\n. ",
    "andreparodi": "I'm facing this problem as well. \nLooking at the code i can see a simple fix would be to:\n- check dividend and divisor are same length\n- loop from 0 to length\n- calculate dividend[i]/divisor[i]\nI am thinking of creating a new function for this and submit it. Would that make sens? What would be a good name? divideSeriesMulti?\n. ",
    "AndreevDm": "Hi. Any progress here?. ",
    "linar-jether": "+1 for this issue, when querying many nodes just the latency of many fetch() calls can be very expensive (when using custom finders).\n. Solved using apache's request-timeout option\n. ",
    "linkslice": "if I actually pass 'when' I was receiving an error 500.  With DEBUG = true I see the traceback as: \nFile \"/usr/lib/python2.6/site-packages/django/core/handlers/base.py\" in get_response\n1.                         response = callback(request, _callback_args, *_callback_kwargs)\n   File \"/opt/graphite/webapp/graphite/events/views.py\" in view_events\n2.         return post_event(request)\n   File \"/opt/graphite/webapp/graphite/events/views.py\" in post_event\n3.             event.get(\"when\", time.time()))\nHow I'm calling it.\ncurl -X POST http://graphite.url/events/ -d'{\"what\": \"test\", \"tags\": \"test\", \"when\": \"1411142400.0\"}'\nWith the patch above both methods appear to work (with and without 'when').\n. Nevermind, I see the error now.  I was quoting the values, so it was being interpreted as a string.\n. Honestly this was my mistake. I've done the exact same thing before. My one critique might be to change the example to include an optional when clause showing the correct way. I'll send a pull request for that, and you can do with it as you like. :)\n. ",
    "aroben": "Note that we don't set Pragma: no-cache anymore, but that's apparently only needed for HTTP/1.0 clients. I figured we could just go with whatever Django thinks best.\n. Thanks for the quick turnaround!\n. :wave: \n. Anything I can do to help get this merged?\n. Rebased! (It was just the test file that conflicted.)\n. ",
    "ramuta": "Ok, link removed\n. ",
    "leoleovich": "I want to add protection for dashboards and metrics. I have already did this for my company.\nSorry, I if I push not a last version of graphite.\nBut the question is - do you need restriction in central repository? \n. Also added commit 27e87de3fca9dd07c5dceebbd3c2cb59afc88864\nHope you will find it too (Present in Commits)\n. @obfuscurity thanks, I noticed :) I will fix it.\n. I will write when I am fully sure it is fixed :)\n. Now I have OverflowError: math range error sometimes.\n. @obfuscurity so:\nI had problems with overfloating:\nfor integers\n```\nmath.pow(200, 134)\n\nOverflowError                             Traceback (most recent call last)\n in ()\n----> 1 math.pow(200, 134)\nOverflowError: math range error\n```\nfor floats\n```\nmath.pow(200.1, 134.1)\n\nOverflowError                             Traceback (most recent call last)\n in ()\n----> 1 math.pow(200.1, 134.1)\nOverflowError: math range error\n```\nwith ** is it kinda better\nIn [29]: 200 ** 134\nOut[29]: 217780714829400616616559748756331655331840000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000L\nBut only for integers. Floats broken the same way\n```\n200.1 ** 134.1\n\nOverflowError                             Traceback (most recent call last)\n in ()\n----> 1 200.1 ** 134.1\nOverflowError: (34, 'Numerical result out of range')\n```\nBut these numbers are very small and having broken graph is not the best option.\nSo far I put None if we catch exception in safePow.\n. I am checking tests. Ok, I think I see what is happening:\n Call of groupByNode returns already \"aliased\" version: [{\"target\": \"iowait\", \"datapoints\": [[5145058.0, 1494763200]...\n asPercent around it transforms it to the \"full\" path again:\n[{\"target\": \"asPercent(iowait,sumSeries(sumSeries(servers.node0*_graphite.system.cpu.{user,iowait,system,irq,softirq,steal})))\", \"datapoints\": [[0.39007737399918074, 1494763320]...\n* And then magic starts:\n  * Old version of aliasByNode would work fine with aliasByNode(<previous_output>,4)\n  * New version of aliasByNode requires 0 to be the node in this case. Which kicks out the possibility to use alias by another node, than was returned by groupByNode.\nIs this a bug or a feature?. @iksaif then aliasByNode is getting useless by 70% :(. @iksaif \n aliasByNode is used quite often as the last function. For example would be nice to operate with multiple parts of the origin metric, like aliasByNode(1, 4) to see \"node01_graphite.iowait\" and so on\n It is bit confusing, when you want to call your metric by 4th part, but you have to use 0\n* It breaks all existing graphs. @iksaif with this part I agree. It was just very convenient to use aliasByNode at the end to fix metric's name.. Ok, then, probably, we should get used to the new behaviour. Thanks.. Would be very nice if you can preserve this part.\n. Other option would be something like\nqqq = Decimal(323.4500000000001) ** Decimal(372.85)\nprint(qqq)\n6.047806945385690914856988084E+935\nprint(float(qqq))\ninf\n. Fixed. ",
    "leizha": "I don't have anything written in public for now. I work for Uber :D\n. I don't have anything written in public for now. I work for Uber :D\n. ",
    "fizyk": "True, I think it's more related to etrsy/statsd we use as intermediate layer.... https://github.com/etsy/statsd/issues/467\nHmmm... does Graphite implements anything like the Raw metrics at all?\n. ",
    "romankoblov": "yep, this is disables carbon link for any resolution other than high precision (lesser pointsPerSecond).\nBug in that for any other resolution this data will be incorrect (so better slow data than wrong):\nfor example:\n1) we have average value metric (like bandwidth or requests/per second)\n2) data sent to carbon with 10s interval, highest precision -- 10s.\n3) no problem with carbon link as 10s interval, it will work fine\n4) if we request 60s interval, we got from cache same value as for 10s, which is completely incorrect.\nfor averages its should be downscaled (i don't have enough experience with graphite to do this, so all I can suggest this small patch). and value will be replaced even if there already any.\n5) 0.1 as avg value for 10s interval is 10, but for 60s interval it should be 0.0166.., not 0.1).\n6) for now, on avg metrics on 60s interval we get value which is in 6x more than should be.\n. There is hitcount function, which aggregates per-second values into hourly/daily total. If you push per-second (not per 10 sec) average value each 10 seconds, you cannot restore hourly/daily values. We pre-aggregate on our side per-second values to per 10 seconds values, and then push this to graphite. \nAnd take a look at this code, for new cache value its replaces old one 60s value, instead of re-aggregation.\n. We don't use memcached, and its not related, since values comes from carbonlink (carbon_cache), and there is no memcache support as I know, so probably bug with memcache is different one.\nBTW, same issue will happens with hourly/daily calculation on summarize too, and with any other way to restore daily/hourly sum from average, since it will be related to step.\nI checked other ways to fix that, and most of them will kill performance more (like aggregate on graphite-web/carbon-cache). Aggregation on metric receiving can help, but will cause a lot of problem if file changed between aggregation and write.\nBTW, this patch just disable cache link on non-highest precision retention, and I cannot find a way how this raw data can be useful and correct for non-highest precision, can you provide example, so maybe I can find out way to fix that in more proper way?\n. We don't use memcached, and its not related, since values comes from carbonlink (carbon_cache), and there is no memcache support as I know, so probably bug with memcache is different one.\nBTW, same issue will happens with hourly/daily calculation on summarize too, and with any other way to restore daily/hourly sum from average, since it will be related to step.\nI checked other ways to fix that, and most of them will kill performance more (like aggregate on graphite-web/carbon-cache). Aggregation on metric receiving can help, but will cause a lot of problem if file changed between aggregation and write.\nBTW, this patch just disable cache link on non-highest precision retention, and I cannot find a way how this raw data can be useful and correct for non-highest precision, can you provide example, so maybe I can find out way to fix that in more proper way?\n. Hm, if wrong data from cache for non-highest precision retention is ok and needed by most of users, I have simple solution:\nadd to hitcount/summarize ignoreCache=False by default, which will disable using cache for these queries, and users who wants realtime can flush metrics partially using my other pull-request (as we do for now). In that way its doesn't impact performance and help in cases like our. What would you think?\n. ",
    "LukaszStrzelecki": "Hi,\nI am an autor of #832 it indeed this one looks like the exact same problem we have.\nWe have worked around it in similar way (probably less elegant ;)).\nLet me try to explain the problem one more time:\nAssumptions:\n- metric: \"my.metric.sum\" \n- aggregated with \"sum\" function (so wider bucket will be a sum of smaller buckets)\n- an aggregation scheme of : 10s for 2min, 1min for 1h  (for simplicity)\n- imagine that you emit my.metric.sum each 10 second with value of \"100\" (and you are doing that for long time).\nThen if you query for last 2 minutes of data you would get [100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100] (12 times)\nBUT if you query for last 10 minutes (so we ended up using the 1min:1h aggregates) you will get [600, 600, 600, 600, 600, 100]\nThe last value in response is invalid because it will be taken from cache (which will override the value already stored for most recent minute (which depending when within a minute you query, could be 100, 200, 300, 400 or 500))\nI hope it sheds more light on this problem.\nThanks,\nLukasz\n. Hi,\nI am not sure what is the correct fix here either.\nWhat we did say that is it better to show slightly stale data than invalid one.\nI don't see a point in providing an invalid value (it can only cause confusion) \nI think that I will apply that patch locally to our instance. It still gives you the most up to date data if you query for short enough time range, and it does not lie if you look at long time trends. \n. That is what I did as well :).\nThe problem I have with such approach is that it one of the 4cache black box lives on the same physical box as the UBER webapp.\nI had to disable change from https://github.com/graphite-project/graphite-web/issues/377 to actually let is connect to different web-app living on the same host.\nFor some reasons uber-webapp tries to use that tiny additional carbon-cache to read meta data about metrics queried from web-app that live on the same box.\nLets say that my box is called BOX1.\nIt has relay -> [4x cache] <- web-app running.\nIt has uber web-app running that queries BOX1, BOX2, BOX3, BOX4 via CLUSTER_SERVERS and its local fake small carbon cache (for the sake of not erroring constantly).\nThe gw.service.BOX1.limiting.VDC2.WDC1.poll.max lives on BOX1 in one of the 4 carbon caches. It reaches my uber web-app by CLUSTER_SERVERS. Then uber-webapp tries to read some metadata about that metric using that small carbon cache (and its storege dir) \nAny hints about that?\nTue Feb 24 18:02:47 2015 :: Exception Caught\nTraceback (most recent call last):\n  File \"/mnt/disk/home/graphiteweb/graphite-web-1.2.2/graphite/webapp/graphite/render/datalib.py\", line 232, in fetchData\n    minStep = CarbonLink.get_metadata(dbFile.real_metric, 'minStep')\n  File \"/mnt/disk/home/graphiteweb/graphite-web-1.2.2/graphite/webapp/graphite/render/datalib.py\", line 146, in get_metadata\n    results = self.send_request(request)\n  File \"/mnt/disk/home/graphiteweb/graphite-web-1.2.2/graphite/webapp/graphite/render/datalib.py\", line 173, in send_request\n    raise CarbonLinkRequestError(result['error'])\nCarbonLinkRequestError: Traceback (most recent call last):\n  File \"/home/graphiteweb/current/graphite/lib/carbon/management.py\", line 15, in getMetadata\n    archives = whisper.info(wsp_path)['archives']\n  File \"/home/graphiteweb/current/graphite/lib/whisper.py\", line 697, in info\n    fh = open(path,'rb')\nIOError: [Errno 2] No such file or directory: '/home/graphiteweb/storage/whisper/gw/service/BOX1/limiting/VDC2/WDC1/poll/max.wsp'\n. Hi,\nActually I found the problem last night.\nIt was due to fix we did long time ago to workaround issue described here: https://github.com/graphite-project/graphite-web/issues/832\nThe initial workaround was: (We have not published it because it is probably not the way it should be fixed)\n``` python\n--- a/src/main/graphite/webapp/graphite/render/datalib.py\n+++ b/src/main/graphite/webapp/graphite/render/datalib.py\n@@ -225,13 +225,21 @@ def fetchData(requestContext, pathExpr):\n     store = STORE\nfor dbFile in store.find(pathExpr):\n     log.metric_access(dbFile.metric_path)\n     dbResults = dbFile.fetch( timestamp(startTime), timestamp(endTime) )\n+\n     try:\n-      cachedResults = CarbonLink.query(dbFile.real_metric)\n-      results = mergeResults(dbResults, cachedResults)\n+      minStep = CarbonLink.get_metadata(dbFile.real_metric, 'minStep')\n+      step = dbResults[0][2]\n+      if step > minStep:\n+        log.cache(\"Step %s is larger than min step %s, not using cache\" % (step, minStep))\n+        results = dbResults    \n+      else:\n+        log.cache(\"Step %s is equal to min step %s, using cache\" % (step, minStep))\n+        cachedResults = CarbonLink.query(dbFile.real_metric)\n+        results = mergeResults(dbResults, cachedResults)\n     except:\n       log.exception()\n       results = dbResults\n if not results:\n\n```\nIn summary, our fix did not work in distributed environment.\nSo we can close this issue. The conclustion is that for now you always have to have a local cache running, even if it is completely dummy one that one writes to.\nThanks!\nLukasz\n. ",
    "klen": "I didn't know about it. Have sorted.\n. No problem.\n. Thank you!\n. ",
    "fmenabe": "I'm using the version 0.9.12. From your previous comment JSON.parse is already done in the dashboard template in master branch (so this issue is already solved). If you accept the behaviour I propose, I could submit a pull request based on the master branch?\n. ",
    "lamont": "No. Graphite has no provisions for pulling data, data must be sent to it. The tools to do so are very easy to install and use, I prefer collectd with the write_graphite plugin, but use whatever you like.\n. Oh clever, I see what you did there. Thanks, I'll try that tonight and let you know.\n. Thank you, that was exactly my problem and your fix solved it.\n. Oh sorry, I should have mentioned, that was the latest version of the 0.9.x branch.\n. Thanks!  I found the performance to be out of this world awesome for our large wildcard queries, so I'm really driven to help you debug this. The divideSeries error was the only one I could get a specific error message out of, the other issue I saw in the 40 minutes this was live was with rawData=true returning an empty body yet an http status code of 200 and I'm not sure how to debug it.  (I saw that on the production cluster but could not replicate it on the development cluster)\n. Thanks for looking at this.  Here's some details about my implementation:\n3 frontend (graphite-web, carbon-relay CH, replication 2)\n10 backend (also graphite-web, carbon-relay, carbon-cache, no replication)\nThe frontends only know about the backend graphite-web, nothing about their carbon caches. it's more or less the setup recommended by http://grey-boundary.com/the-architecture-of-clustering-graphite/\nboxes are all redhat 6.4\nPython 2.6.6, \n$ pip freeze\nCheetah==2.4.1\nDjango==1.4.15\nM2Crypto==0.20.2\nMarkdown==2.0.1\nMySQL-python==1.2.3c1\nPyYAML==3.10\nPygments==1.1.1\nSSSDConfig==1.9.2\nTwisted==13.1.0\nargparse==1.2.1\nbackports.ssl-match-hostname==3.4.0.2\nboto==2.27.0\nchardet==2.0.1\ncloud-init==0.7.2\nconfigobj==4.6.0\ndjango-tagging==0.3.2\nethtool==0.6\niniparse==0.3.1\nordereddict==1.1\npolicycoreutils-default-encoding==0.1\nprettytable==0.7.2\npy-rrdtool==0.2.1\npyOpenSSL==0.10\npycurl==7.19.0\npygpgme==0.1\npyparsing==2.0.2\npython-dateutil==1.4.1\npython-dmidecode==3.10.13\npython-memcached==1.53\npytz==2014.4\nrequests==1.1.0\nrhnlib==2.5.22\nrhsm==1.1.8\nsetools==1.0\nsimplejson==2.0.9\nsix==1.6.1\ntxAMQP==0.6.2\nuWSGI==2.0.6\nurlgrabber==3.9.1\nurllib3==1.5\nvirtualenv==1.11.6\nwhisper==0.9.12\nyum-metadata-parser==1.1.2\nzope.interface==4.1.1\nI'm using the latest 0.9.x of whisper, carbon and graphite-web, specifically:\nwhisper-0.9.x-4404f173c0\ncarbon-0.9.x-36177586ff\ngraphite-web-0.9.x-ce239313b\n. @bmhatfield I can try without that commit, thanks for pointing that out.\n. I just confirmed that the frontend machines (A, B, C) only list the backend boxes in CLUSTER_SERVERS. The order of listed backend boxes varies per frontend box but there is no duplication in each list. Also, the frontend boxes have no local metrics that match the pattern of metrics on the backend.  (backend has collectd.* in whisper, frontend has some nfs mounted RRD files under cacti.*)\nNone of the backend nodes have CLUSTER_SERVERS set as they're meant to only answer questions about their local metrics.\n. I'm rolling out again with 0.9.x HEAD minus the fe6cda0\n. Ok, the problem is at least repeatable.  \nhttp://varnish-graphite/render/?target=divideSeries(sumSeries(grouper.APP.FOO1.*.bar-baz?TcSrv-TotalDuration),sumSeries(grouper.APP.FOO1.*.baz?TcSrv-TotalHits))&from=-30mins&rawData=true\nThat errors with the divideSeries error:\nFile \"/opt/graphite/webapp/graphite/render/functions.py\", line 501, in divideSeries\n    raise ValueError(\"divideSeries second argument must reference exactly 1 series\")\nValueError: divideSeries second argument must reference exactly 1 series\n. Ok, that's helpful.\n  if len(divisorSeries) != 1:\n    print \"saw series too many\", len(divisorSeries)\n    raise ValueError(\"divideSeries second argument must reference exactly 1 series\")\n2014-11-06_01:33:16.24137 saw series too many 0\nso now the question is why the pre 1010 came up with some divisor series and post 1010 has none.\n. pre-1010, it does not error and has a series present. Some of the data in question is spotty (ie, it is possible that there are holes in the data for a given timerange) but a spot check showed that both copies of the data are consistent. The exact query I mailed you has a numerator of 3 unique series (6 total including duplicates) and a divisor of 3 unique series (again, 6 total copies).  \nAll series in question are coming from a remote node, the backend nodes.  It's possible timeouts are in play, but the load on the backend seems very steady and under the 1010 code the \"busyness\" of the uwsgi is less than a quarter of normal so I'm not inclined to believe that.\nI just walked the cluster to verify that there are no metrics which are out of place, ie, no local files on the frontend boxes and only 2 copies of each on the backend boxes.\nI've been rolling the code out to both the frontend and backend boxes, but if it was possible to just change the code on the frontend boxes I could take one frontend node out of rotation and compare things head to head forever.  Does the frontend 1010 need something matching on the backend?\n. Ok, let me try that and get some numbers.  Just a count of various series against both servers should do.\n. No, thank YOU for digging into it. This feature would be fantastic and I'd love to help get it working.\nSo, great success with narrowing down the problem. I setup app001 with the 1010 code and app002 with the \"stable\" 0.9.x branch from nov 1. They are both querying the same set of 10 backend hosts (running the same stable nov 1 0.9.x)  I had to remember to disable memcached as well.\n$ curl 'http://app001:8080/render?target=countSeries(collectd.px*.STUFF.counter-TotalDuration)&rawData=true'\n1\n$ curl 'http://app002:8080/render?target=countSeries(collectd.px*.STUFF.counter-TotalDuration)&rawData=true'\n3\nActually walking the filesystems did show me the 3 unique values.  I'll try and add some debugging statements to see what the frontend collected then possibly discarded. Python is not a language I'm any good with so if you have specific debug suggestions that would help a great deal.\n. I added some debug statements and think I know what is happening. I added:\nfor series in results:\n        print \"considering \", series['name']\nand\nif candidate_nones >= known_nones:\n              # If we already have this series in the seriesList, and\n              # it is 'worse' than the other series, we don't need to\n              # compare anything else. Save ourselves some work here.\n              print \"break cause candidate_nones >= known_nones\"\n              break\nand\nif not series_handled:\n          print \"appending\"\n          seriesList.append(ts)\nAnd saw the following:\n2014-11-06_04:56:37.71547 considering  collectd.app142.STUFF.counter-TotalDuration\n2014-11-06_04:56:37.71552 appending\n2014-11-06_04:56:37.72104 considering  collectd.app142.STUFF.counter-TotalDuration\n2014-11-06_04:56:37.72107 break cause candidate_nones >= known_nones\n2014-11-06_04:56:37.72662 considering  collectd.app142.STUFF.counter-TotalDuration\n2014-11-06_04:56:37.72670 break cause candidate_nones >= known_nones\n2014-11-06_04:56:37.73190 considering  collectd.app142.STUFF.counter-TotalDuration\n2014-11-06_04:56:37.73214 break cause candidate_nones >= known_nones\n2014-11-06_04:56:37.73887 considering  collectd.app142.STUFF.counter-TotalDuration\n2014-11-06_04:56:37.73920 break cause candidate_nones >= known_nones\n2014-11-06_04:56:37.74432 considering  collectd.app142.STUFF.counter-TotalDuration\n2014-11-06_04:56:37.74462 break cause candidate_nones >= known_nones\n2014-11-06_04:56:37.74963 considering  collectd.app142.STUFF.counter-TotalDuration\n2014-11-06_04:56:37.74996 break cause candidate_nones >= known_nones\n2014-11-06_04:56:37.75516 considering  collectd.app142.STUFF.counter-TotalDuration\n2014-11-06_04:56:37.75548 break cause candidate_nones >= known_nones\n2014-11-06_04:56:37.76087 considering  collectd.app142.STUFF.counter-TotalDuration\n2014-11-06_04:56:37.76119 break cause candidate_nones >= known_nones\n2014-11-06_04:56:37.76653 considering  collectd.app142.STUFF.counter-TotalDuration\n2014-11-06_04:56:37.76699 break cause candidate_nones >= known_nones\nThis is interesting for a few reasons.  app142 came up 6 times whereas it should have come up twice (the metric and its replica). But there are 6 copies of the matching metrics, just the other 4 should be the 2 copies of apps 141 and 143.  \nI'll keep poking at it (my python is terrible). I see you just posted a debug branch, so let me try that and get it back to you.\n. Also, the app142 machine's metric is on the first node in the CLUSTER_SERVERs list.\n. So yeah, that was totally my fault. Disabling memcache on the backend seems to have fixed the issue for me. I'll do a fuller test tomorrow and let you know.\nMy apologies for muddying the water, your stuff rocks.\n. @deniszh I'm doing a full acceptance test (with humans) at noon today and I'll close the issue after that.\n. @bmhatfield 0 or 1 per box, for a total of 3.  When I disable memcached on the backend boxes I also get the correct results. \n. Ok, rolled the 1010 software to all frontend boxes and it is now \"live\" to the in-use cluster which gets hammered by queries.  So far, I'm seeing all queries returning with proper counts. I am seeing each of the backend queries take 2.5-5 seconds, which when serialized across 10 backends results in a total query delay of about 30s, but that's better than before.\n@bmhatfield I'll take one of the machines out and roll the old software back to it and hack in the remote_storage logging to show you the results.\n. Well, it depends on how your cluster is setup. Mine was two tier with  front-end webui nodes that list a bunch of backend storage nodes in the cluster, but the backend nodes are unaware of each other. Originally I had every machine in the cluster sharing a global memcache cluster, and everything was fine. \nPatch 1010 changed from fetching individual metrics from each backend (after a parallel \"what metrics does this backend have that satisfy this wildcard query\") to asking for the raw series results to the original wildcard query against each backend. That caused results returned from backend node A to be cached and visible to backend node B. So when all backend nodes shared the same memcache cluster, you'd start getting the same results from each node in the backend. I doubt the replication factor matters, it was just that the designer of patch 1010 didn't anticipate a global memcache pool, and there was no good reason for me to have one. \nI changed it so that each backend node used only a local memcache store and everything was fine. You want the local memcache store so that the expensive /metrics/find requests are cached for a few minutes between all the processes serving web requests on the backend. If the proposal is to namespace the request cache with the local node address, that seems like a good solution as well, both for the problem I had and any future ones.\n. ",
    "clohtd": "If the remote server is linux, you can send it with bash scripts and netcat.  Netcat is a standalone binary that you can copy over to the remote server.  Then just pipe your data to netcat.  Here's an example:\necho \"1000\" | gawk '{print \"test.test1 \" $1 \" \" systime()}' | /usr/local/bin/netcat 192.168.1.1 2003\n. Did you resolve this?  It looks like your metric has a \".\" following it so that instead of it being stored as a metric, it is stored as a subfolder.\n. ",
    "ssgelm": "One such URL is https://GRAPHITE_URL/content/js/ext/examples/shared/icons/fam/cog_edit.png\n. 0.9.12\n. Oh, strange, looks like those icons should be there in 0.9.12.\n. Ah.  It turns out debian removes the icons when packaging graphite-web.  I apologize!  Having said that it looks like this is a problem in the current git HEAD.\n. @obfuscurity Closing this seems totally reasonable.  It's been a while but at this point my best guess is that I was just mistaken.  If I manage to figure anything out I'll reopen.\n. ",
    "bobemoe": "Bingo! Working fine now I've installed pytz.  Thanks :)\n. ",
    "billowqiu": "I have the same problem when use uwsgi with graphite.wsgi.example.\nBut start the app with run-graphite-devel-server.py is ok, even don't install pytz.\n. @stttt2003pk install pytz?. ",
    "stttt2003pk": "@deniszh @billowqiu but anybody know how to fix this on Centos7\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/django/core/handlers/base.py\", line 113, in get_response\n    response = callback(request, callback_args, *callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 56, in renderView\n    (graphOptions, requestOptions) = parseOptions(request)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 318, in parseOptions\n    fromTime = parseATTime('-1d', tzinfo, now)\n  File \"/opt/graphite/webapp/graphite/render/attime.py\", line 48, in parseATTime\n    return tzinfo.normalize(parseTimeReference(ref).astimezone(tzinfo) + parseTimeOffset(offset))\nAttributeError: 'LocalTimezone' object has no attribute 'normalize'\nif i try to get the render\ni wanna update this process i ve fix this problem\ni ctrl + shift +i to see which is wrong and i found the Traceback excception;\nand i grep -rwn 'normailize' * in the dir which contains the dependency,it all ok\nso i think its about the permission problem,i think the wsgi file cannot access out depandency lib,so i try\n\n                #Order deny,allow\n                #Allow from all\n                Require all granted\n        \napactl -v 2.4.*\nso it can rend the data out.\nso it s because the mod wsgi file cannot access our depandency,problem solved. ",
    "vlad-vintila-hs": "In case someone gets here and wants to subtract a metric from a constant, something which offset doesn't support directly:\noffset(scale(<metric>,-1),<constant>)\n. ",
    "pkittenis": "There is only one, single node, the one running graphite-web. All versions of graphite-web, whisper and carbon are from current master.\n. There is only one, single node, the one running graphite-web. All versions of graphite-web, whisper and carbon are from current master.\n. Closing, was problem talking to incompatible version of whisper\n. ",
    "jeromegit": "Thanks for the quick response, obfuscurity!\nThe issue is not happening on the DST change boundaries.\nHere's what it looks like for today's data where the max/avg are shifted to 6:10 while today's data (in blue) starts at 7:10 as it should.\n\nvs. the data for last Friday (before the time change) where everything starts at the same time (7:10).\n\n. Thanks for your feedback deniszh.\nI understand that it would complicate things but the behavior doesn't seem right to me..\nLet's take a simpler example. If I'm in the habit of comparing how my system behave from one week to another, seeing a chart like the one below doesn't look right to me:\n\nTo me timeShift() is really like datetime arithmetic and typically DST is taken into consideration.\nSince I had a mysql prompt handy, i tried this:\nmysql> SELECT ADDDATE('2014-11-07 07:10:10', '-7');\n+--------------------------------------+\n| ADDDATE('2014-11-07 07:10:10', '-7') |\n+--------------------------------------+\n| 2014-10-31 07:10:10                  |\n+--------------------------------------+\n and it works as I would expect.\nThis is just my $.02.\nThanks.\n. Using UTC epoch across all sites generating the data did the trick for me.. Using UTC epoch across all sites generating the data did the trick for me.. ",
    "pparth": "+1\n. ",
    "philbooth": "@deniszh:\n```\n[carbon]\npattern = ^carbon.\nretentions = 60:90d\n[default]\npattern = .*\nxFilesFactor = 0.0\nretentions = 10s:12h,1min:7d,10min:5y\n```\n. @deniszh:\n```\n[min]\npattern = .lower$\nxFilesFactor = 0.1\naggregationMethod = min\n[max]\npattern = .upper$\nxFilesFactor = 0.1\naggregationMethod = max\n[sum]\npattern = .sum$\nxFilesFactor = 0\naggregationMethod = sum\n[count]\npattern = .count$\nxFilesFactor = 0\naggregationMethod = sum\n[count_legacy]\npattern = ^stats_counts.*\nxFilesFactor = 0\naggregationMethod = sum\n[default_average]\npattern = .*\nxFilesFactor = 0.3\naggregationMethod = average\n```\n. No, your explanation is great, thanks. I didn't understand how aggregation works at all before, now it makes sense.\nI'll speak to my colleagues about either fixing our graphs to certain time windows or using average for aggregation if we really need to cross the boundary between archives.\n. ",
    "Leobaillard": "As mentioned in the last referenced issue from grafana, I'm still experiencing this problem in 0.9.13. Any ideas? My timezone is set as \"Europe/Paris\".\n. Okay, my bad. I'll wait :smile: \n. ",
    "drax68": "+1 Hit this issue too. Any chance that this will be merged soon?\n. ",
    "bsnape": "Thanks! Keep up the great work :+1: \n. ",
    "MartinNowak": "Wouldn't a movingAverage of summarize fit your needs?. Wouldn't a movingAverage of summarize fit your needs?. ",
    "kevin-lin": "+1. I've also been looking for a way to have a moving version of the Summarize function so I can graph a movingMin.\nIf you are only looking for movingSum, you can use movingAverage(timeInterval) * (number of data points in timeInterval) to achieve that functionality. See the scale function.. ",
    "fessyfoo": "thank you @deniszh \nwe didn't disable memcached,  we rolled back before we got that idea.    there are 8 servers in our cluster.  don't know if that makes a difference. \nI guess we need to try to to reproduce it off the main cluster.   For now we back ported the fix we needed to 0.9.12\nMEMCACHE_HOSTS = ['host1:11211','host2:11211','host3:11211','host4:11211','host5:11211','host6:11211','host7:11211','host8:11211']\nCLUSTER_SERVERS =['host1:80','host2:80','host3:80','host4:80','host5:80','host6:80','host7:80','host8:80']\n. I've since learned how to probe graphite better and see what's coming from where.  In particular local=1 is required for me as localOnly=true didn't seem to work, though it's in the version of the code we're using. \nthere is some possibility the issue was with our carbon-relays not replicating all metrics properly. (we've found that if carbon-cache is restarted carbon-relays start throwing exceptions connecting to them. File \"/usr/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 451, in _startRunCallbacks\n    raise AlreadyCalledError  and need to be restarted. ) \nWe'll try to replicate this issue and upgrade again.  (especially now that 0.9.13 was released) \nclosing this issue for now. \n. thank you.\n. ",
    "fote": "Oh, i forgot. There is no error messages in log files, and when i open a metric folder (cpu_usage for example) i got 200OK http from /metrics/find/\n. Yes, i resolved it by edit STORAGE_FINDERS in settings.py:\nSTORAGE_FINDERS = (\n    'graphite.finders.ceres.CeresFinder',\n)\n. ",
    "jkur": "Could you have another look? i cleaned up as you suggested.\n. ",
    "chicagobuss": "any new on this?  still pretty annoying :(\n. Yeah, I just really want to get these into the next official release so we can stop patching them into our deploy every time we make another graphite instance :)\n. FWIW we tried doing this client side for a long time, but doing it in\ngraphite was up to 10x faster for complex multi-target graphs.  Our\ndashboard app can draw hundreds of dygraphs per page and without this\ngraphite addition we crush it.\nI just wanted it added \"for real\" so we wouldn't have to keep patching it\nin ourselves.  :)\nJosh\nOn Jan 24, 2015 8:19 AM, \"gingerlime\" notifications@github.com wrote:\n\nThanks for the update / link. And you did a really good job with your\nresponse @SEJeff https://github.com/SEJeff. Very reassuring.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1119#issuecomment-71318920\n.\n. Cool. I'll try to get that added soon.\nOn Jan 24, 2015 9:44 AM, \"Jason Dixon\" notifications@github.com wrote:\n@chicagobuss https://github.com/chicagobuss We're in favor of this\nchange as long as there's documentation and tests.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1119#issuecomment-71323113\n.\n. \n",
    "aww-yiss": "As a troubleshooting measure, we did rename the metrics in a testing tree and sure enough the chart rendered without issue. This is purely the result of misusing the pie chart. Closing issue, don't expect a fix.\n. ",
    "RyPeck": "@obfuscurity will you still take down the wikidot site?\n. ",
    "JarleB": "Thanks for the reply @deniszh. My wording was perhaps not correct, but yes I'm aware that the issue becomes evident because of the number of metrics returned by the glob and not the size of each metric.  Seems #1010 is addressing this in particular. Will check it out. \n. For the record,  I can can confirm that 0.9.13-pre1 (includes  #1010 + #1026 ) completely eliminates this described slow behavior.\n. ",
    "jjneely": "1293 has a small improvement for bulk-fetch and also merges duplicate time series together which I'd like to port to master.  But being this work was based on the bulk-fetch code, I'm blocked until that is also ported to master.\n. The Travis checks don't seem related to this patch....\n. This is probably crack...but, why not?\n. Updated to replace the seriesList list in the bulk-fetch code with a dict.  Using the dict for large queries provides a significant improvement in performance.\nThis makes PR #1304 obsolete.\n. Porting to master:  This patch builds on top of the performance improvements and code from the bulk-fetch patches, but those don't seem to be ported to 0.10.x.  \nIs there an Issue/Comment to track getting that ported to 0.10.x?\nI can probably still port the code to master, but that feels like the cart in front of the horse in this case.\n. That makes the most sense.  \nAs far as unit tests:  Merging assumes there are duplicate metrics which can only be the case if there are remote metrics.  I don't see anything that serves as a guide for how I might setup unit tests for cluster functionality and I don't want to mock half of graphite.\nIf we can figure out the framework here, I'll be glad to write the tests.\n. or I guess this can me merged in lieu of #1293\n. #1293 is now updated.  Thanks!\n. With the timeout's on the joins I was silently loosing data.  The HTTPConnectionWithTimeout wasn't timing out at all for me so long queries ended up hitting the worst case performance there where the timeout was effectively multiplied by the number of servers.\nUsing the timeout with Python 2.6's code the timeout would work if the server didn't return any data before the timeout was reached.  However, if the client was actively downloading pickle data from the remote server the timeout was not triggered.\nFor the last case, I decided that the thread was working appropriately and the timeout on the join was causing harm.  We want to timeout the HTTP connection...not the unpicking or other operations that we know are safe to complete.\n. But yeah, I can tell we are on the same thought track with #1208\n. This looks great to me.  Thanks!\n. Nice catch!\n. Any comments / approvals here?\n. These look equivalent.  I tend to use try/except sparingly is the only real difference.  Let's get 492ef8f into this branch as well as its already in master.  That's what I lean toward.\n. ",
    "avi-ram": "The simplest fix is to change functions.py, line 2754:\nmetaSeries[key] = SeriesFunctions[reduceFunction](requestContext,metaSeries[key])[0]\nto\nmetaSeries[key] = SeriesFunctions[reduceFunction](requestContext,metaSeries[key][0],metaSeries[key][1])[0]\n. ",
    "warsaw": "Just a quick note until I get a chance to try to reproduce this.  The Debian wiki briefly describes why dist-packages instead of site-packages on Debuntu systems:\n\"dist-packages instead of site-packages. Third party Python software installed from Debian packages goes into dist-packages, not site-packages. This is to reduce conflict between the system Python, and any from-source Python build you might install manually.\"\nhttps://wiki.debian.org/Python\nIt's completely expected that apt-get installed third party packages end up in /usr/lib/pythonX.Y/dist-packages and manually installed packages end up in /usr/local/lib/pythonX.Y/dist-packages.  AFAIK, none of the standard Debuntu Python tools touch /opt.\n. It's not at all surprising that carbon ends up installing in /opt, since the setup.cfg is explicitly telling it to do that.  Having [install]prefix point to /opt/graphite and having install-lib variable are exactly equivalent to calling \"python setup.py install --prefix=/opt/graphite --install-lib=/opt/graphite/lib\"\nNote that if you enable setuptools by doing \"USE_SETUPTOOLS=1 python setup.py install\" on Ubuntu 15.04 you'll get a nice warning from setuptools.  stdlib distutils doesn't include this warning.\nI tested the recipe above in Ubuntu 15.05 and Debian sid (unstable).  Same behavior in both (i.e. installs to /opt/graphite).\nSeems like everything's working as intended (where \"intended\" means \"what the setup.cfg\" is saying to do).  :)\n. Right, sorry I missed that.  I still think it's the setup.cfg that's messing you up.  If I comment out the [install] section in carbon's setup.cfg, the pip install -r requirements.txt doesn't go into /opt.  (Tested in Debian sid just to take Ubuntu out of the picture).\nSid's version of pip is 1.5.6 so my guess is that it applies setup.cfg to all requirements installed via -r.\nOr IOW, I think it's pip's problem. :)\n. @dstufft Well that's interesting.  Again, in Debian sid (which has pip 1.5.6), adding --no-use-wheel seems to avoid installing into /opt.  Do you know why that would be?\n. ",
    "arthru": "@obfuscurity : done there : https://github.com/graphite-project/graphite-web/pull/1085\n. ",
    "magicrobotmonkey": "On further inspection, it seems there's no way this can work, as the first call just spits out the nodename rather than the absolute path to the metric. \n. ",
    "jimpriest": "FYI - I get this with movingAverage inside groupByNode as well.\n. ",
    "AndrewStickler": "I get this with aliasByNode(keepLastValue(,100)). . ",
    "arma26": "The issue was resolved when I disabled memcache across all storage nodes and spun up a 5th graphite-web instance with memcache enabled.\n. ",
    "senkrad76": "Sorry for the confusion... let me see if I can rephrase without the 'Grafana-isms' and maybe provide some clarity.\nI'll start with the naming conventions:\nZenoss.collector[0-3].devices.[0-5]server[0-9].memAvail.value\n&\nZenoss.collector[0-3].devices.[0-5]server[0-9].memTotal.value\nWhat I'm trying to do is invert the memTotal's and multiply them by the corresponding memAvail's\nThere are two wildcards in the server node: one designates its silo, one designate the iteration.\nWe also have different collectors for Zenoss. (all the servers would most likely be on the same collector, but the wildcard is there to catch ones that aren't)\nWhat would be the correct syntax to achieve:\ntarget=multiplySeries(Zenoss.collector2.devices.1server1.memAvail.value,invert(Zenoss.collector2.devices.1server1.memTotal.value))&\ntarget=multiplySeries(Zenoss.collector2.devices.1server2.memAvail.value,invert(Zenoss.collector2.devices.1server2.memTotal.value))&\ntarget=multiplySeries(Zenoss.collector2.devices.2server1.memAvail.value,invert(Zenoss.collector2.devices.2server1.memTotal.value))& \ntarget=multiplySeries(Zenoss.collector3.devices.3server1.memAvail.value,invert(Zenoss.collector3.devices.2server1.memTotal.value))& etc\nIs a problem occurring because of the two wildcards in the same node?\n. ",
    "tomekit": "I am facing the similar problem. I would like to multiple a list of series by another series.\nAdded the Grafana issue: https://github.com/grafana/grafana/issues/7106 \nObviously it's been closed, as it's really a Graphite problem.. ",
    "plockaby": "Well, we collect our metrics by router interface and our engineers care about metrics by router interface. Our customers care about metrics by site and their sites might be served by several different interfaces over the course of time depending on infrastructure upgrades or network topology changes. This pull request also imitates a feature that we've used heavily with RRD files.\n. I'll get the tests in real soon now.\n. Sorry about all the commits. I'm not generally a python programmer so it took me a short bit to figure out how to test it. This should be ready.\n. Squashed it down to five commits.\n. Easy fix. That var wasn't even being used anyway.\n. ",
    "ankit1987": "I dont know what value I should put over there and how does it work.\nIf you could give an insight abt it would be very helpful\nOn 18 Jan 2015 21:03, \"Jason Dixon\" notifications@github.com wrote:\n\nWhy are you setting maxValue to 0?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1111#issuecomment-70412443\n.\n. I am using graphana for my dashboard. it takes 0 as default value.\n\nif you could give any insight about this value, it would be very helpful. I\nlooked at documentation of this function but couldn't figure out exactly\nwat is the significance of this max value.\nOn Sun, Jan 18, 2015 at 9:37 PM, Jason Dixon notifications@github.com\nwrote:\n\nYou don't need to set it at all, it's optional.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1111#issuecomment-70413773\n.\n. \n",
    "Sineos": "Many thanks for this first reply. But I could not get plausible values from the derivative nor the summarize functions or a combination here of.\nMaybe an example of the mathematical model helps:\nLets assume an electrical device that 2000 Watt power.\nYou run the device for 2 hours.\nNow you have consumed 4 kWh.\nDuring this 2 hours my metric would increase from its [current value] to [current value + 4] in small steps.\nThe graph I intend to get should read 2000 during this, which is the wattage of the electrical device.\n. I capture the data currently every 20 seconds. Lets assume that my meter has currently a value of 100 kWh. Given the above example, this total would increase by 0.005555556 kWh every 20 seconds and would reach 104 after 7200 seconds (= 2h).\n. Yes, 20 seconds matches the whisper file\nWhen analysing it, the \"derivative(home.power.ehz.in2)\" is at least close to the expected result with respect to the shape of the graph. I cannot give it more precisely but the result is about a factor of 180000 too low. This might somehow be related to the 180 measuring points and the difference between me expecting Watts and the meter counting kilo Watts hours. --> 180 * 1000 = 180000??\nscale(home.power.ehz.in2, 180) is way off\nbut ....\nscale(derivative(home.power.ehz.in2), 180000) does the trick and with pretty high accuracy but some small aberrations that remain a mystery\n\nThis image is from a test meter that actually delivers both the value (Power in Watts) I'm searching and the total consumption in kWh.\nMany thanks for pointing me into the right direction :+1: \n. Thanks a lot for this awesome tool and the support.\nThe features and possibilities are still quite overwhelming to me and honestly the above results is based on trial and error and not on deeper understanding of the tool but nonetheless I'm totally enthusiastic with it.\n. My endeavors on electrical measurement continue. Currently I'm using a test meter that displays more values than later my actual setup will to verify the results and test for plausibility of the values.\nintegral(scale(home.power.ehz.power, 0.000005555555555))\nThis function gives me the opposite way of what I have done before with the derivative function. It takes the current power usage in Watt and provides the total consumption over time. The 5.555e-6 value in this case is the reciprocal of the 180,000 used above. So far so good, this calculation works and gives me exact results.\nNow I wanted to generate a daily consumption as total. So lets assume at the start of the day the graphs last value was 100, at the end of the day it was 119. This means I used 19 kWh and the value I want to get from the graphite functions would exactly be this 19. I used:\nsummarize(derivative(integral(scale(home.power.ehz.power, 0.000005555555555))), '24h', 'sum', false)\nor for better readability:\nsummarize(derivative(calculation_above), '24h', 'sum', false)\nI expected to get 19 as result, but it turned out to be 16. Anyone an idea where this big deviation is coming from?\n. Works, thnx for the pointer\n. ",
    "landlord11": "You aren't alone; I was seeing this too.  This resulted in graphite-web failing any time carbon returned data from its cache.\n. On Mon, 9 Feb 2015, Denis Zhdanov wrote:\n\nSorry, but why? IMO graphite is quite slow by itself :)\nLooks like you trying to solve some other problem, which one?\n\nI'm not sure what sense it makes to catch an exception and retry when \nthe retries happen so fast that the underlying condition probably \ncouldn't have resolved itself?  What sort of problems is the existing \nexception handling attempting to catch that would resolve themselves so \nquickly?  Maybe it would be better to remove the exception handling \ncompletely?\nYou are correct, though, that there was another issue.  Carbonlink is \nreturning a dict and not a list so the loop over the data returned by \ncarbonlink fails.  Someone else has submitted a pull request (#1122) to \nfix this.  For this problem as long as carbon-cache has some data in \ncache the query fails.  Since graphite fired off the retry requests so \nquickly it was highly unlikely that a call that failed the first time \nwould ever succeed.  By slowing down the retries it made it more likely \nthat it could catch the carbon daemon at a time when it had flushed its \ncache to disk.\nAnd probably a better retry logic than what I submitted could make \nsense; some sort of backoff logic so the initial attempts are faster \nthan my suggestion, for example.\nI do agree, it of course is better to fix the underlying condition that \ncauses the exception in the first place!\nBest regards,\nBrian\n\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/pull/1138#issuecomment-73476448\n. \n",
    "afilipchik": "Here is another way of doing it:\npython\nif hasattr(cached_datapoints, 'iteritems'):\n   cached_datapoints = cached_datapoints.iteritems()\nCould someone please merge it, cause graphite-web is kind of broken now?\n. I see same error with the latest build.\n. the hacky fix is to add:\npython\n if hasattr(cached_datapoints, 'iteritems'):\n   cached_datapoints = cached_datapoints.iteritems()\nto\n``` python\n    try:\n      cached_datapoints = CarbonLink.query(self.real_metric_path)\n    except:\n      log.exception(\"Failed CarbonLink query '%s'\" % self.real_metric_path)\n      cached_datapoints = {}\nif hasattr(cached_datapoints, 'iteritems'):\n  cached_datapoints = cached_datapoints.iteritems()\n\nfor (timestamp, value) in cached_datapoints\n  interval = timestamp - (timestamp % data.timeStep)\n\n  try:\n    i = int(interval - data.startTime) / data.timeStep\n    values[i] = value\n  except:\n    pass\n\n```\n. the hacky fix is to add to graphite.readers.py :\npython\n if hasattr(cached_datapoints, 'iteritems'):\n   cached_datapoints = cached_datapoints.iteritems()\nto\n``` python\n    try:\n      cached_datapoints = CarbonLink.query(self.real_metric_path)\n    except:\n      log.exception(\"Failed CarbonLink query '%s'\" % self.real_metric_path)\n      cached_datapoints = {}\nif hasattr(cached_datapoints, 'iteritems'):\n  cached_datapoints = cached_datapoints.iteritems()\n\nfor (timestamp, value) in cached_datapoints\n  interval = timestamp - (timestamp % data.timeStep)\n\n  try:\n    i = int(interval - data.startTime) / data.timeStep\n    values[i] = value\n  except:\n    pass\n\n```\n. Did you change it in both places (WhisperReader and CeresReader)? It looks like it should be fixed by:\nhttps://github.com/graphite-project/graphite-web/pull/1122 but only for WhisperReader :(\n. ",
    "mathyourlife": "whisper-dump would seem to to indicate the stored values are not rounded.\n$ pip freeze\nargparse==1.2.1\nwhisper==0.9.12\nwsgiref==0.1.2\n$ whisper-dump.py cpuUsage.wsp | grep -F -A 10 -B 4 \"1422343380\"\n65894: 0,          0\n65895: 0,          0\n65896: 0,          0\n65897: 0,          0\n65898: 1422343380, -0.2259152934616708574555588029397768\n65899: 1422343440, 0.014113819984270315963104103218483942\n65900: 1422343500, 0.014294421680595659554513510158813006\n65901: 1422343560, 0.011067390164903840449328598083411634\n65902: 1422343620, 0.008860985979784796739977537072263658\n65903: 1422343680, 0.012301582784986258428672201148401655\n65904: 1422343740, 0.01002157782340001336562895772885895\n65905: 1422343800, 0.0070676991639336602529786190984850691\n65906: 1422343860, 0.007873066854831694758032334391373297\n65907: 1422343920, 0.008597852064861463000644548060336092\n65908: 1422343980, 0.0073200056138081805862105611026890983\n$ epoch 1422343380\n2015-01-27 07:23:00 1422343380.0\n. ",
    "jaingaurav": "Would it be possible to merge this? grafana has already been updated to use mapSeries/reduceSeries, but the UI is broken without this change.\n. Seems pull request #1125 already addressed this\n. ",
    "evverx": "Done: #1130 \n. Sorry:)\n. ",
    "rbroemeling": "@obfuscurity I thought about that, but it doesn't make any sense (I believe that you are referring to the number of rows returned by \"production deploy\", correct?)  Specifically the rows that match both \"production\" AND \"deploy\" will be strictly smaller than the number of rows that match \"production\" OR \"deploy\".  As such, if \"production\" alone works (which it does), then so should \"production\" AND \"deploy\".  It doesn't make any sense for the larger set (\"production\" alone) to work and then the more specific set (\"production\" AND \"deploy\") to fail.\n. ",
    "logikal": "I don't think this was your issue, since you could replicate with a regular query to your graphite server, but searching for my issue led me here. This is for those internet friends:\nWe ran into this issue, but it turned out someone had enabled the \"Proxy\" setting for the graphite datasource in our grafana settings, unintentionally. We had not configured graphite to allow requests like that, which resulted in the 500s we saw. Setting it back to \"direct\" resolved the issue for us.\n. ",
    "lilydjwg": "I've found in my case, django-tagging is generating SQLs like this:\nSELECT \"events_event\".\"id\", \"events_event\".\"when\", \"events_event\".\"what\", \"events_event\".\"data\", \"events_event\".\"tags\" FROM \"events_event\" WHERE (\"events_event\".\"id\" IN (%s, ....\nThere are a lot of %s there, no wonder it hits SQLite's limit. It seems SQLite doesn't support the where xx in ? syntax.\n. ",
    "lomik": "In fact natsort uses regex: https://github.com/SethMMorton/natsort/blob/master/natsort/utils.py#L128 \n\"split\" do same, but more simple that my solution (I didn't know \"split\" method):\n```\nnatsort\n\n\n\nre.compile(r\"(\\d+)\").split(\"aaa3cc234xx\")\n['aaa', '3', 'cc', '234', 'xx']\n\n\n\nmy\n\n\n\nre.compile(r\"([^0-9]+|[0-9]+)\").findall(\"aaa3cc234xx\")\n['aaa', '3', 'cc', '234', 'xx']\n```\n. > @lomik I'd love to hear your thoughts/suggestions/reaction to the idea of adopting go-carbon as the official daemon\n\n\n\nI don't mind. It is hard for me to support it since I already migrated all my own installations to the clickhouse stack.\nBut you need to do something with built in carbonserver component:\n It is incompatible with graphite-web\n IMHO, this is a bad idea to merge store and fetch layers in single daemon. I like the separation in classic graphite stack.\n. > There's no official goal exist on go-carbon page. But I can assume it has the same goal as \"go-graphite\" project - reimplement Graphite in Go\nReimplement in go is not a goal, but a means to achieve it. This is why I did not transfer go-carbon to go-graphite project.\nMain goals was:\n maximum compatibility with graphite-web and graphite-carbon. But for some features (such as relay and blacklist) I always used external aggregators and never planned to implement it\n performance\n easy installation (single binary instead of large number of libraries)\n easy maintenance (single process instead of relay and multi process sharding)\nAt the moment I think that all these goals have been achieved. And I have no new goals.. ",
    "loopping": "Hello, more info on my errors:\nWhen using a carbonlink and with this http query the graph is ok:\nhttp://myurl:8080/render/?target=nmon.os1.cpu_all.Idle_perc\nIf I try this http query I get the error mentionned above:\nhttp://myurl:8080/render/?target=nmon.os1.cpu_all.Idle_perc&target=nmon.os1.Sys_perc\nRegards\n. Hello some more news about carbonlink and cache query:\nI have set some log debug in the file /opt/graphite/webapp/graphite/carbonlink.py in the function.\nIn the class  CarbonLinkPool and in the function send_request (line 106), I printed request and result:\nHere is what I got:\nsend_request:request:{'metric': 'nmon.os1.cpu_all.Idle_perc', 'type': 'cache-query'}\nsend_request:result:{'datapoints': {1423491741.0: 2.2000000000000002, 1423491681.0: 3.1000000000000001, 1423491621.0: 2.2000000000000002, 1423491801.0: 2.1000000000000001}}\n\nI was wondering if the error I have \"float' object is not iterable\" might come from the epoch time has a \".0\" in it.\n. Some more logs on this subject:\nI logged the content of datapoints,result in function stringReceived in file lib/carbon/protocols.py.\nI logged the content of request,results in function query in file webapp/graphite/carbonlink.py\nI logged the content of cached_datapoints in function fetch in file webapp/graphite/readers.py\nI have edited the carbon/examples/example-client.py, changed DELAY from 60 to 0.\nI ran example-client.py for a few seconds and asked graphite to render carbon.agents.vmlinux-a.metricsReceived and refreshed every 5 sec or so.\nHere are the contents of datapoints,request and results when I have the error mentionned above:\n\nSun Feb 22 09:34:48 2015 :: carbonlink:query:request:{'metric': 'carbon.agents.vmlinux-a.metricsReceived', 'type': 'cache-query'}\nNone\n22/02/2015 10:34:48 :: protocols:stringReceived:datapoints for carbon.agents.vmlinux-a.metricsReceived:{1424597616.247383: 822, 1424597676.270806: 848, 1424597556.274894: 852}:\n22/02/2015 10:34:48 :: protocols:stringReceived:result for carbon.agents.vmlinux-a.metricsReceived:{'datapoints': {1424597616.247383: 822, 1424597676.270806: 848, 1424597556.274894: 852}}\nSun Feb 22 09:34:48 2015 :: carbonlink:query:results:{'datapoints': [1424597616.247383, 1424597556.274894, 1424597676.270806]}\nNone\nSun Feb 22 09:34:48 2015 :: readers:fetch:cached_datapoints:[1424597616.247383, 1424597556.274894, 1424597676.270806]\nNone\nSun Feb 22 09:34:48 2015 :: Got an exception when fetching data! See: 'float' object is not iterable Will do it again! Run: 1 of 2\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 136, in fetchData\n    seriesList = _fetchData(pathExpr,startTime, endTime, requestContext, seriesList)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 99, in _fetchData\n    fetches = [(node, node.fetch(startTime, endTime)) for node in matching_nodes if node.is_leaf]\n  File \"/opt/graphite/webapp/graphite/node.py\", line 30, in fetch\n    return self.reader.fetch(startTime, endTime)\n  File \"/opt/graphite/webapp/graphite/readers.py\", line 177, in fetch\n    for (timestamp, value) in cached_datapoints:\nTypeError: 'float' object is not iterable\n\nIn some conditions I can't find, values are not fetched correctly from the cache\n. I tried your hacky fix with no luck:\nwebapp/cache.log\nSat Feb 14 14:53:27 2015 :: CarbonLink sending request for carbon.agents.vmlinux-a.cache.size to ('127.0.0.1', 'a')\nSat Feb 14 14:53:27 2015 :: CarbonLink finished receiving carbon.agents.vmlinux-a.cache.size from ('127.0.0.1', 'a')\nSat Feb 14 14:53:27 2015 :: CarbonLink sending request for carbon.agents.vmlinux-a.cache.size to ('127.0.0.1', 'b')\nSat Feb 14 14:53:27 2015 :: CarbonLink finished receiving carbon.agents.vmlinux-a.cache.size from ('127.0.0.1', 'b')\nSat Feb 14 14:53:27 2015 :: CarbonLink cache-query request for carbon.agents.vmlinux-a.cache.size returned 2 datapoints\n\nwebapp/exception.log\nSat Feb 14 14:53:27 2015 :: Failed after 2 retry! See: 'float' object is not iterable\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 136, in fetchData\n    seriesList = _fetchData(pathExpr,startTime, endTime, requestContext, seriesList)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 99, in _fetchData\n    fetches = [(node, node.fetch(startTime, endTime)) for node in matching_nodes if node.is_leaf]\n  File \"/opt/graphite/webapp/graphite/node.py\", line 30, in fetch\n    return self.reader.fetch(startTime, endTime)\n  File \"/opt/graphite/webapp/graphite/readers.py\", line 177, in fetch\n    for (timestamp, value) in cached_datapoints:\nTypeError: 'float' object is not iterable\nSat Feb 14 14:53:27 2015 :: Exception encountered in \nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 111, in get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 111, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 7, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 18, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 21, in evaluateTokens\n    return fetchData(requestContext, tokens.pathExpression)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 141, in fetchData\n    raise Exception(\"Failed after %i retry! See: %s\" % (settings.MAX_FETCH_RETRIES, e))\nException: Failed after 2 retry! See: 'float' object is not iterable\n. ",
    "checkmypi": "+1 \n. ",
    "MFAnderson": "Yeah, I had come to more or less the same conclusion. My one concern regarding passing timezone through to parseTimeReference is that I think we'll run into issues around DST again once the timedelta from parseOffset gets added. The solution I was thinking of was to keep parseTimeReference dealing with naive datetimes and only localize after the offset has been included. From what I can gather reading the pytz docs this should avoid the weird DST issues that create the need for normalize. I've got an attempt at a fix for this on my fork MFAnderson/graphite-web@86999666073569a0ed0568654a3fdffca92f202f but I want to add some test coverage around the DST specific issues before submitting a pull request. Can you take a look at that code and see if it makes sense to you?\n. I've been playing around with stuff on the DST boundary and realized there's a couple of weird things  that don't necessarily follow intuitive assumptions. Namely, the way localize works means that for, say, \"America/New_York\" \npython\ntz.localize(datetime(2015, 3, 8, 2, 0)) == tz.localize(datetime(2015, 3, 8, 3, 0))\n=> True\nbecause it treats the first datetime as being in EST and the second in EDT.\nas it pertains to attime, this results in \npython\nparseATTime(\"02:00_20150308\", tz) == parseATTime(\"03:00_20150308\", tz)\n=> True\nand similarly with the fix I tried out\n``` python\ngiven the current day is the day DST begins\nparseATTime(\"midnight+2h\", tz) == parseATTime(\"midnight+3h\", tz)\n=> True\n```\nOther than this bit of weirdness, I've confirmed that the fix at least ensures that absolute and relative times behave consistently with regard to timezone.\n. Yeah, absolutely.\n. @obfuscurity #1157 is the pull request for the fix ported to 0.9.x\n. @obfuscurity No worries. Rebase done.\n. ",
    "urtzurd": "Hi,\nWe're in the same situation trying to bump the version to start using some of the new functionalities. We have been testing the latest development version and we have noticed some very strange behaviours with relative time ranges. I'm not sure if they might be directly related to the latests changes introduced by this merge.\nOur scenario is as following: we have our graphite server configured to use UTC timezone and so is configured graphite-web too. We set the timezone when creating graphs/dashboards. Using the latest version we have experienced that when we have a graph to show the past 24h and the timezone is Europe/London, is shows 24h of data, but not from the current time, but with 1h difference. So if it's 9:00AM London time, it will show 24h data until 8:00AM London time. I have tried with Europe/Madrid and it has a similar problem (it shows until 9:00AM when it should show until 11:00AM).\nMay I be missing some new configuration variable or is there a new bug?\nBest regards.\n. Yes, we are testing master. I guess that master will be the branch that will become 0.10 in the future and I should stick to the 0.9.x in the meantime. Will give it a try just now. Thanks a lot for the very quick response!\n. I've been testing 0.9.x and definitely there's still some issue with the timezones. It seems that it's dashboard related only. I've plenty of graphs saved with Europe/London timezone and they display properly when loaded. The problem is when those graphs are included into a dashboard, sometimes they work, sometimes not. The behaviour is very erratic and sometimes the data appears correctly with all the data until now, then sometimes after a refresh it goes wrong and moves back not displaying the latest data.\nI tried creating a dashboard from scratch and it showed the same issues. First I thought that it could be related to format changes in the dashboard JSON, so I tried cleaning up them and removed repeated parameters like from and until inside each graph, but that didn't make any difference.\nAs a workaround we have changed all our dashboards to be UTC and that seems to work.\n. ",
    "adubkov": "@obfuscurity well, I was thing about name, maybe it now the best, but I try to follow pip convention, other   tools\\modules called graphite-something. graphite-dashboard-cli will be too long :)\n. Renamed to graphite-dashboardcli\n. ",
    "MorrisJobke": "What is the state of this?\n. What is the state of this?\n. ",
    "AlexAkulov": "I get the same error.\nThis error appears if i use the option CARBONLINK_HOSTS, and it doesn't appear if i use only the option WHISPER_DIR in the file local_settings.py.\nAlso this error appears very often when Disk Utization reaches 100% (it doesn't with WHISPER_DIR and hight Disk Utilization)\nMy configuration is the following\n4 processes carbon-cache\n1 gunicorn for graphite-wsgi\nI`m ready to give any additional information. \nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/django/core/handlers/base.py\", line 115, in get_response\n    response = callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite-storage/webapp/graphite/render/views.py\", line 111, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/opt/graphite-storage/webapp/graphite/render/evaluator.py\", line 7, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/opt/graphite-storage/webapp/graphite/render/evaluator.py\", line 18, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression)\n  File \"/opt/graphite-storage/webapp/graphite/render/evaluator.py\", line 21, in evaluateTokens\n    return fetchData(requestContext, tokens.pathExpression)\n  File \"/opt/graphite-storage/webapp/graphite/render/datalib.py\", line 141, in fetchData\n    raise Exception(\"Failed after %i retry! See: %s\" % (settings.MAX_FETCH_RETRIES, e))\nException: Failed after 2 retry! See: 'float' object is not iterable\n. ",
    "orestes": "Just came by to say I encountered this bug as well. Looking forward to the merge.\n. ",
    "mattus": "Just noticed that the query to fetch profiles wasn't doing any sorting, fixed now.\n. ",
    "melan": "My change passed all test at least once w/o any further changes in the testable area - Travis CI fails some random tests usually because of timeout while downloading packages for the testing env.\n. ",
    "rbuckland": "You're welcome.\n;-) it had me puzzled for a day. \n. ",
    "gboily": "Yes, it would be possible to merge some code, but due to the callback parameter being not at the same place, we must either : \n- keep the function groupByNodes and modify the function groupByNode to call groupByNodes\n```\ndef groupByNode(requestContext, seriesList, nodeNum, callback)\n  groupByNodes(requestContext, seriesList, callback, nodeNum)\ndef groupByNodes(requestContext, seriesList, callback, *nodes)\n  ...\n```\n- or we can keep only the groupByNode function and accept parameters in several orders by detecting their types, etc (a bit ugly IMO)\ndef groupByNode(requestContext, seriesList, *args)\n  if isinstance(args[0], int)\n    ...\nAny preference/other  idea ?\n. @steve-dave I've updated groupByNode and added tests.\nI was not able to use \"sumSeries\" in \"groupByNodes\" tests (nor other *Series I guess) so I used \"keepLastValue\" instead.\nSo, tests only test keys manipulation and not really calculation results as keepLastValue does not change the values in that case.\nHere is the error when using \"sumSeries\" : \n```\nERROR: test_groupByNodes (tests.test_functions.FunctionsTest)\n\nTraceback (most recent call last):\n  File \"/usr/local/src/graphite-web2/webapp/tests/test_functions.py\", line 291, in test_groupByNodes\n    verify_groupByNodes(expectedResult, 1)\n  File \"/usr/local/src/graphite-web2/webapp/tests/test_functions.py\", line 281, in verify_groupByNodes\n    results = functions.groupByNodes({}, copy.deepcopy(seriesList), \"sumSeries\", *nodes)\n  File \"/usr/local/src/graphite-web2/webapp/graphite/render/functions.py\", line 2855, in groupByNodes\n    metaSeries[key])[0]\n  File \"/usr/local/src/graphite-web2/webapp/graphite/render/functions.py\", line 191, in sumSeries\n    name = \"sumSeries(%s)\" % formatPathExpressions(seriesList)\n  File \"/usr/local/src/graphite-web2/webapp/graphite/render/functions.py\", line 157, in formatPathExpressions\n    [pathExpressions.append(s.pathExpression) for s in seriesList if not pathExpressions.count(s.pathExpression)]\nAttributeError: 'TimeSeries' object has no attribute 'pathExpression'\n```\nI also got a timezone problem and temporaly disabled test_render.test.\nIt may be due to running tests in a docker container (I tried changing my timezone to GMT and GMT+1 without success) :  \n```\ntox -e py27-django14 -- /usr/local/src/graphite-web2/webapp/tests/test_functions.py:FunctionsTest\nGLOB sdist-make: /usr/local/src/graphite-web2/setup.py\npy27-django14 inst-nodeps: /usr/local/src/graphite-web2/.tox/dist/graphite-web-0.10.0-alpha.zip\npy27-django14 runtests: PYTHONHASHSEED='1309592725'\npy27-django14 runtests: commands[0] | django-admin.py test\n/usr/local/src/graphite-web2/webapp/graphite/settings.py:233: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security\n  warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\nCreating test database for alias 'default'...\n....................................F.../usr/local/src/graphite-web2/.tox/py27-django14/local/lib/python2.7/site-packages/django/db/models/fields/init.py:808: RuntimeWarning: DateTimeField received a naive datetime (1970-01-01 01:00:00) while time zone support is active.\n  RuntimeWarning)\n.............\n======================================================================\nFAIL: test_correct_timezone (tests.test_render.RenderTest)\n\nTraceback (most recent call last):\n  File \"/usr/local/src/graphite-web2/webapp/tests/test_render.py\", line 122, in test_correct_timezone\n    self.assertEqual(data, expected)\nAssertionError: Lists differ: [[12, 1393394460], [12, 139339... != [[12, 1393398060], [12, 139340...\nFirst differing element 0:\n[12, 1393394460]\n[12, 1393398060]\n\n[[12, 1393394460], [12, 1393398060]]\n[[12, 1393398060], [12, 1393401660]]\n\n\nRan 53 tests in 0.251s\nFAILED (failures=1)\n```\n. ",
    "robertclancy": "This would be very useful. What needs to happen to make this mergeable?\n. ",
    "ctrochalakis": "I am also willing to help if @gboily and @Dieterbe are busy.\n. @bmhatfield I started implementing it and never checked for other PRs, sorry for that :)\n. @bmhatfield I hijacked #1171, rebased to master and changed the documentation a bit (the as defined by multiple nodes part). I have kept the original author's name.\nI pushed it here since I couldn't update #1171.\n. ",
    "sglwlb": "Anyone can help me? Very strange problem.\n. @fferner Yeah, you are right, i also downgrading the Django14 to fix this problem. But now EPEL has updated the Django14 to the 1.4.18-1.el6, and the graphite-web on the EPEL became incompatible of the  Django, that's not good. Hope graphite-web will update also.\n. @fferner Ah, cool, look forward some one will fix it, -:)\n. @azikic Cool! Just upgrade the Django and try your fix, it works, thanks very much!\n. ",
    "fferner": "@sglwlb which version of Django have you got installed?  I've run into the same problem after upgrading the Django14 rpm from version 1.4.14-1.el6 to 1.4.18-1.el6. Downgrading again fixed it for me.\nSo far I've not check what happened and which package might be at fault...\n. @sglwlb FWIW, I've reported this issue as a bug to fedora-epel: https://bugzilla.redhat.com/show_bug.cgi?id=1199126\n. ",
    "mark-5": "It looks like Django decided it will not serve static files, unless in debug mode or insecure mode. See --insecure documention and stackoverflow for more information.\nMore specifically, the problem is that staticfiles_urlpatterns in graphite.urls is not returning the appropriate regex, unless either of these modes are set.\n. As far as I can tell, it looks like this has always been the way Django behaved. I'm not actually sure why downgrading Django would have fixed this for the above users, though. When I manually install older Django releases(outside of a package manager), I still see this behavior.\nIt also might be worth noting that graphite didn't always use django.contrib.staticfiles, which is what causes this behavior. That seems to have been started in 1e1de1.\n. According to the docs, os.walk returns a list of tuples like (dirpath, dirnames, filenames), so I figured that first item we are getting is always a dir, and doesn't need an isdir check.\n. ",
    "jeremycrussell": "@obfuscurity I'm running with Apache 2.2.15-39/WSGI 3.2-7 and Django 1.4.18 and am experiencing the same issue.\n. @obfuscurity I'm running with Apache 2.2.15-39/WSGI 3.2-7 and Django 1.4.18 and am experiencing the same issue.\n. Additionally, my installation is requesting the js and css from /content (i.e. /content/js/composer.js) and it's getting \"200\" responses, the content is just empty.  However, the files do exist at /opt/graphite/webapp/content and permissions all appear to be fine.  BTW,  running graphite 0.9.12 installed via PIP.\n. Additionally, my installation is requesting the js and css from /content (i.e. /content/js/composer.js) and it's getting \"200\" responses, the content is just empty.  However, the files do exist at /opt/graphite/webapp/content and permissions all appear to be fine.  BTW,  running graphite 0.9.12 installed via PIP.\n. ",
    "azikic": "@sglwlb I had the same problem. The apache site configuration was missing an alias for the content directory.\nI added Alias /content/ \"/usr/share/graphite/webapp/content/\" to /etc/httpd/conf.d/graphite-web.conf to make it work.\n. @sglwlb I had the same problem. The apache site configuration was missing an alias for the content directory.\nI added Alias /content/ \"/usr/share/graphite/webapp/content/\" to /etc/httpd/conf.d/graphite-web.conf to make it work.\n. @yoavp77 I am using centos 6.5 with the graphite-web package from EPEL. This is the default configuration file with the added alias configuration:\n```\nGraphite Web Basic mod_wsgi vhost\n\nServerName graphite-web\nDocumentRoot \"/usr/share/graphite/webapp\"\nErrorLog /var/log/httpd/graphite-web-error.log\nCustomLog /var/log/httpd/graphite-web-access.log common\nAlias /content/ \"/usr/share/graphite/webapp/content/\"\nAlias /media/ \"/usr/lib/python2.6/site-packages/django/contrib/admin/media/\"\n\nWSGIScriptAlias / /usr/share/graphite/graphite-web.wsgi\nWSGIImportScript /usr/share/graphite/graphite-web.wsgi process-group=%{GLOBAL} application-group=%{GLOBAL}\n\n<Location \"/content/\">\n    SetHandler None\n</Location>\n\n<Location \"/media/\">\n    SetHandler None\n</Location>\n\n\n```\n. ",
    "yoavp77": "@azikic can you share your entire graphite-web.conf vhost file?\n. @azikic awesome! that's all I was missing to get it to work, also with graphite-web installed from EPEL (centos 6.2)\n. ",
    "mhitza": "@azikic thanks. Your fix couldn't have been more appropriately timed, since I've just encountered the same issue.\n. ",
    "ccoenen": "i can confirm that adding that line to /etc/httpd/conf.d/graphite-web.conf on CentOS fixed the issue.\nAlias /content/ \"/usr/share/graphite/webapp/content/\"\n. All of the inofficial docker images are out of date (0.9.x), but I can report that I'm using the hopsoft image succesfully. Basically, I want exactly that but with the current stable ;-)\nIt may be worth noting that there's a WIP branch to upgrade to 1.0.1. ",
    "SamuelMarks": "Oh, I found it with a release data of 31st of December 2014 (on pypi).\n. ",
    "kokje": "I think something similar also needs to be done on the 0.9.x branch, because we're currently on that and are facing this issue\n. Yup, I noticed it when I got this error while inspecting code from the UI\nAttributeError: 'LocalTimezone' object has no attribute 'localize'. \nBut since we're on 0.9.x which is pretty different I tried out these lines \ntzinfo = timezone.get_current_timezone()\ntzinfo.localize(datetime.strptime(s,'%H:%M%Y%m%d'), daylight)\nI was able to fix it by explicitly using pytz. I saw that's what views.py does and passes in the object so I thought I would change it here as well\n. ",
    "crazzy": "Found the issue after a lot of \"insert log.info here, restart, request graph, look at log, repeat\". If there exists a local metrics file, fetchData() in render/datalib.py will never iterate to the next available storage location (which in my case is checking my other node using cluster_servers feature), even if all array members of the results array has the value of \"None\".\nThe cause of this is me migrating from an old single-node graphite installation using carbon-sync from https://github.com/jssjr/carbonate which apparently was not a good idea. I solved it in my case by whiping all the data on both nodes.\nAnother solution I came to think about but decided to not go forward with would be to just before \"if not results\" line in render/datalib.py iterate trough results and if all members are of value \"None\", set the whole results to null or whatever evaluates to false in python. That would however have had the downside of not working as expected if one were to request data from a really long time ago (for which there is data in the synced metrics, but not new data on the same node) which would not give all the data expected. A more clever solution would probably be to iterate through results, and do remote requests for the few metrics being of \"None\" value. That would require some serious thinking though..\n. ",
    "vikas22": "Hi, \n  As you said  I'm new to the system, and the person who configured it is also not available right now, \n I sent some 40000000 requests, in this the data seems to be fine but if i send some 1000 requests the same problem what i mentioned above is happening. So can you please let me know what all the configuration files required for you to cross check. (I think its expecting continuous data )\n. ",
    "clhynfield": "Exact same symptoms here, on OS X 10.10.2, having followed nearly the same installation and configuration procedures. I didn't specify a version for Django, taking the default 1.7.7 version. I also had to brew install cairo py2cairo as documented in gist relaxdiego/graphite.md, as graphs weren't rendering initially. \n\nChanging the font size to 1 pt: \n\n. ",
    "tonyghita": "I have the same issue, with the same environment. Any updates @atroschinetz-rmn @clhynfield ?\n. FWIW that's also what I did @calebmadrigal \n. ",
    "calebmadrigal": "Same problem here. I followed these setup directions: https://gist.github.com/relaxdiego/7539911\n. ",
    "PanzaSancho": "same problem\n. ",
    "ryanricard": "A solution in the discussion thread at the bottom of the gist (https://gist.github.com/relaxdiego/7539911) resolved this issue for me. Apparently the most recent version of Cairo has a bug.\nIf you are using Homebrew, unlink or uninstall Cairo. Then install an older version. Cairo 1.12.16 is working for me on Yosemite. \n``` shell\n// unlink current installation of Cairo\n$ brew unlink cairo\n// install Cairo 1.12.16\n$ brew install https://raw.githubusercontent.com/Homebrew/homebrew/26d5775494b3535820c48442c23af44f72974880/Library/Formula/cairo.rb\n```\n. ",
    "wiml": "I had the same issue (using the cairocffi bindings on OSX 10.9.5) and ryanricard's fix worked perfectly for me. But, does anyone have a link to the bug that this is working around? I'd like to keep an eye on that so I can upgrade cairo once it's fixed.\n. ",
    "derekleverenz": "this can be changed to still maintain that ordered list of keys and then output using the comprehension at the end, and the setdefault optimization will still be good (instead of iterating over the keys every time)\n. ",
    "jstultz": "Updated to not use OrderedDict; this is now a pathetically small pull request that just removes the unnecessary and inefficient call to .keys() for membership checks. Removed in two other places as well.\n. ",
    "JackLeo": "Ah yes. My Bad. Too much time on this had been spend so everything mixed up together at some point for me. Ignore and continue on.\n. ",
    "qijianbo": "http://graphite.readthedocs.org/en/latest/you can have a try?\n\u539f\u59cb\u90ae\u4ef6\n\u53d1\u4ef6\u4eba:Bruno Reni\u00e9notifications@github.com\n\u6536\u4ef6\u4eba:graphite-project/graphite-webgraphite-web@noreply.github.com\n\u53d1\u9001\u65f6\u95f4:2015\u5e743\u670824\u65e5(\u5468\u4e8c)\u200722:30\n\u4e3b\u9898:Re: [graphite-web] holtWintersAberration: list index out of range(#1198)\n@resnostyle can you provide the exact URL for this query?\n\u2014\nReply to this email directly or view it on GitHub.\n. http://graphite.readthedocs.org/en/latest/\n. ",
    "resnostyle": "@brutasse curl 'http://localhost/render/?from=-7days&until=-1minutes&uniq=1426001228492&format=json&target=holtWintersAberration(statsd.processing_time)'\n. ",
    "mkrb": "Think graphite needs full filled and aligned time series for this. Modifying cyanite.py (graphite-cyanite) as follows could be helpful (fetch_multi is another beast):\n```\n def fetch(self, start_time, end_time):\n    data = requests.get(urls.metrics, params={'path': self.path,\n                                              'from': start_time,\n                                              'to': end_time}).json()\nif len(data['series']) == 0 or 'error' in data:\n    data['from'] = end_time / data['step'] * data['step']\n    data['to'] = data['from']\n    data['series'][self.path] = []\n\nwhile (data['from'] - data['step']) >= start_time :\n    data['from'] = data['from'] - data['step']\n    data['series'].get(self.path, []).insert(0, None)\n\nwhile (data['to'] + data['step']) <= end_time :\n    data['to'] = data['to'] + data['step']\n    data['series'].get(self.path, []).append(None)\n\ntime_info = data['from'], data['to'], data['step']\nreturn time_info, data['series'].get(self.path, [])\n\n```\n. ",
    "0x63lv": "Maybe these will help you if it's still relevant:\nhttp://www.slideshare.net/AnatolijDobrosynets/graphite-cluster-setup-blueprint\nhttps://answers.launchpad.net/graphite/+question/228472\n. ",
    "coredump": "Yes this still a problem. I will try some more fiddling this week.\n. ",
    "manivannan-g": "@coredump do you have any sample for the cluster configurations of carbon.conf, relay-rules.conf, local-settings.py. I've tried creating a two node graphite cluster but not successful. Here's my config looks like:\nI have two machines dedicated for graphite 10.0.1.52 and 10.0.1.27. In this, I wanted to use 52 as the main host.\nlocal_settings.py\nCLUSTER_SERVERS = [\"10.0.1.52:80\", \"10.0.1.27:80\"]\nCARBONLINK_HOSTS = [\"10.0.1.52:7002:a\", \"10.0.1.27:7102:b\"]\ncarbon.conf\n```\n[cache]\nGRAPHITE_STORAGE_DIR = /opt/graphite/storage/\nLOCAL_DATA_DIR = /opt/graphite/storage/whisper/\nLOG_DIR        = /var/log/carbon/\nENABLE_LOGROTATION = True\nMAX_CACHE_SIZE = inf\nMAX_UPDATES_PER_SECOND = 1000\nMAX_CREATES_PER_MINUTE = 5000\nLINE_RECEIVER_INTERFACE = 0.0.0.0\nLINE_RECEIVER_PORT = 2003\nENABLE_UDP_LISTENER = False\nUDP_RECEIVER_INTERFACE = 0.0.0.0\nUDP_RECEIVER_PORT = 2003\nPICKLE_RECEIVER_INTERFACE = 0.0.0.0\nPICKLE_RECEIVER_PORT = 2004\nUSE_INSECURE_UNPICKLER = False\nCACHE_QUERY_INTERFACE = 0.0.0.0\nCACHE_QUERY_PORT = 7002\nUSE_FLOW_CONTROL = True\nLOG_UPDATES = False\nLOG_CACHE_HITS = False\nCACHE_WRITE_STRATEGY = sorted\nWHISPER_AUTOFLUSH = False\n[relay]\nLINE_RECEIVER_INTERFACE = 0.0.0.0\nLINE_RECEIVER_PORT = 2013\nPICKLE_RECEIVER_INTERFACE = 0.0.0.0\nPICKLE_RECEIVER_PORT = 2014\nRELAY_METHOD = consistent-hashing\nREPLICATION_FACTOR = 2\nDESTINATIONS = 10.0.1.52:2004:a, 10.0.1.27:2004:b\nMAX_QUEUE_SIZE = 10000\nMAX_DATAPOINTS_PER_MESSAGE = 1000\nQUEUE_LOW_WATERMARK_PCT = 0.8\nTIME_TO_DEFER_SENDING = 0.0001\nUSE_FLOW_CONTROL = True\nUSE_RATIO_RESET=False\nMIN_RESET_STAT_FLOW=1000\nMIN_RESET_RATIO=0.9\nMIN_RESET_INTERVAL=121\n[aggregator]\nLINE_RECEIVER_INTERFACE = 0.0.0.0\nLINE_RECEIVER_PORT = 2023\nPICKLE_RECEIVER_INTERFACE = 0.0.0.0\nPICKLE_RECEIVER_PORT = 2024\nDESTINATIONS = 10.0.1.52:2004:a, 10.0.1.27:2004:b\nREPLICATION_FACTOR = 1\nMAX_QUEUE_SIZE = 10000\nUSE_FLOW_CONTROL = True\nMAX_DATAPOINTS_PER_MESSAGE = 5000\nMAX_AGGREGATION_INTERVALS = 5\n```\nrelay-rules.conf\n```\n[default]\ndefault = true\ndestinations = 10.0.1.52:2004:a, 10.0.1.27:2004\n```\nNow when I try to test by creating a metric in 10.0.1.52, the same is not replicating in 10.0.1.27.\nThis is how I create new metric echo \"test.metric.a.count 25 1465480800\" | nc 10.0.1.52 2003\nAs a newbie tried a lot, but couldn't get through, requesting you to help me :-)\n. @ashokrajar thanks for rightly pointing :-) echo \"test.metric.a.count 25 1465480800\" | nc 10.0.1.52 2013 is working very well !\n. ",
    "ashokrajar": "From your config relay is running on port 2013. But you are sending the metrics to port 2003 directly to the carbon cache process which will not replicate and store locally.\nAlso since you are running just one carbon-cache per node you don't have to specify :a or :b\n. Quick thing to try is to add one/two more relay servers behind the VIP.\nWhat is the load/IO/mem stats on relay & carbon-cache servers ?\nIf I understand correctly you are running one carbon-cache daemon per 24 core server. That means you are under utilizing the hardware. Carbon can't take advantage of the multi-cores though you have 24 cores but carbon will still utilize only one core. You should be running as many as carbon-daemons possible based on your disk IOPS.\nThumb rule is no. carbon-cache daemons = no. cores. But in you case if you run 24 carbon-cache daemons then you may hit the disk IOPS limitations and all the process will suffer from IO wait. So you need to carefully choose the no. of carbon-cache daemons w.r.t your disk IO. Multiple small SSD disks with RAID-10(eg: 12 X 128GB SSD with RAID10) instead of one beefy disk can be a good option.\nThat said then your replication factor also has to be set to a higher value for such a complicated setup. And number of relay servers also has to be increased. If your F5 VIP can do port level load balancing then you can run multiple relay-daemons on different ports on the same servers similar to cache. Unlike carbon-cache relay doesn't require any storage so I would say a pool of tiny servers for relay(even a single/dual core is enough per server).\n. Quick thing to try is to add one/two more relay servers behind the VIP.\nWhat is the load/IO/mem stats on relay & carbon-cache servers ?\nIf I understand correctly you are running one carbon-cache daemon per 24 core server. That means you are under utilizing the hardware. Carbon can't take advantage of the multi-cores though you have 24 cores but carbon will still utilize only one core. You should be running as many as carbon-daemons possible based on your disk IOPS.\nThumb rule is no. carbon-cache daemons = no. cores. But in you case if you run 24 carbon-cache daemons then you may hit the disk IOPS limitations and all the process will suffer from IO wait. So you need to carefully choose the no. of carbon-cache daemons w.r.t your disk IO. Multiple small SSD disks with RAID-10(eg: 12 X 128GB SSD with RAID10) instead of one beefy disk can be a good option.\nThat said then your replication factor also has to be set to a higher value for such a complicated setup. And number of relay servers also has to be increased. If your F5 VIP can do port level load balancing then you can run multiple relay-daemons on different ports on the same servers similar to cache. Unlike carbon-cache relay doesn't require any storage so I would say a pool of tiny servers for relay(even a single/dual core is enough per server).\n. You will not hit the IO/Mem unless you are running as many carbon-cache servers per cpu core and they are stressed by millions & millions of metrics.\nTake a look at this blog which explains how to do clustering with graphite. I would strongly propose such setup  for you. I also have a similar setup in production but I have replaced the relay component with carbon-c-relay for performance and data loss issues. When carbon-relay/carbon-cache is under stress and when it hit the cpu limit(GIL) it stops accepting new metrics.\nThis is not a issue. It's more of a knowledge sharing & best practices in setting up a graphite cluster. So please close this issue and move this discussion to the mailing-list/IRC.\n. You will not hit the IO/Mem unless you are running as many carbon-cache servers per cpu core and they are stressed by millions & millions of metrics.\nTake a look at this blog which explains how to do clustering with graphite. I would strongly propose such setup  for you. I also have a similar setup in production but I have replaced the relay component with carbon-c-relay for performance and data loss issues. When carbon-relay/carbon-cache is under stress and when it hit the cpu limit(GIL) it stops accepting new metrics.\nThis is not a issue. It's more of a knowledge sharing & best practices in setting up a graphite cluster. So please close this issue and move this discussion to the mailing-list/IRC.\n. Sorry about the link. Here is it https://grey-boundary.io/the-architecture-of-clustering-graphite/\n. Sorry about the link. Here is it https://grey-boundary.io/the-architecture-of-clustering-graphite/\n. Answer to your question is in the error itself. Look at your last line.\n3932:tid 140310950643456] [remote 172.24.23.6:512] OperationalError: attempt to write a readonly database\nMake sure graphite installation folder and the sqlitedb file is writable by apache process.\n. Answer to your question is in the error itself. Look at your last line.\n3932:tid 140310950643456] [remote 172.24.23.6:512] OperationalError: attempt to write a readonly database\nMake sure graphite installation folder and the sqlitedb file is writable by apache process.\n. ",
    "zerthimon": "I'm having the same issue, but in my case I don't want to use alignToFrom (cause I want to specify a relative time, like from=-10d, instead of having to specify absolute time), yet I'd like to have Summarize to allign the date correctly to 00:00 and honor the time zone. I currently get +3 hours shift (as my timezone UTC+3) and summarize is aligned to 03:00.\nAny workaround ?\n. ",
    "tbillet": "I'm in the same situation as @zerthimon. I'd like to summarize by day from 00:00 to 00:00 without using alignToFrom.\nCould this issue be re-opened ?\n. ",
    "alecxvs": "Probably a similar issue to https://github.com/hw-cookbooks/graphite/issues/227\nA newer django-tagging is only compatible with django 1.7+, it needs to be locked down for older djangos.\n. ",
    "scosist": "This sounds like the same problem when using existing and non-existing tags with the events function as described here: https://github.com/grafana/grafana/issues/1474#issuecomment-136417967\nI am currently experiencing this unexpected behavior with the events function. Using graphite-web debian package 0.9.12+debian-6. I would also expect to match on all tags or return none.\n. Perhaps we could take a page from odoo domain filters for added flexibility in tag matching events?\n. @deniszh I'm using postgresql as a backend here.\n@obfuscurity I wouldn't say this is a duplicate of #1210. I don't think it's even related. This is a problem of failing to retrieve an event given a tag, due to the character length of the tag it seems. #1210 is a problem where events are returned when only some (IOW not all) of the tags match.\n. ",
    "elsmorian": "How can you add the set argument within graphite-web dashboards? Have tried adding drawAsInfinite(events('tag1','2',set='union')) and other combination but I'm afraid my understanding of the target functions etc is a bit limited!. @deniszh It was pinned to 1.9 before this change- looks like it was changed in commit fd53fad51e1b4807d1da1c2c4cd23e7726a10397\n. ",
    "klynch": "Are there any issues with merging this feature in?\n. This policy is applied only in the case when an explicit cacheTimeout is\nnot requested in the query (and a policy is defined). At the time #L239 is\nexecuted, we have not yet calculated the requested time range. Once we do\nthis, we can then update the cacheTimeout if applicable. Since we update\nthe cacheTimeout variable in parseOptions(), we don't need to modify\nmuch else.\nrenderView() is a little hard to follow, but I believe line format\nqueries is cached by\nhttps://github.com/Squarespace/graphite-web/blob/cache-policy/webapp/graphite/render/views.py#L116\nand pie, raw, and svg format queries are cached by\nhttps://github.com/Squarespace/graphite-web/blob/cache-policy/webapp/graphite/render/views.py#L215\nJSON queries were not being cached before but should have been.\nraw, csv, and pickle formats were not being cached before so I did\nnot modify that since I am unfamiliar with the uses of those formats.\nThough in retrospect, I suppose it makes sense to cache at least csv. Do\nyou have any guidance regarding which of these should actually be cached?\nAm I missing any code paths?\nOn Fri, May 1, 2015 at 12:38 AM, Jason Dixon notifications@github.com\nwrote:\n\nWhy should this only apply to JSON? Also, the change to parseOptions()\nonly affects line graphs, not JSON afaict. Don't you want to change\nhttps://github.com/Squarespace/graphite-web/blob/cache-policy/webapp/graphite/render/views.py#L239\nas well?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1213#issuecomment-98042220\n.\n. Am I wrong about this, or are there still any outstanding issues? I believe this is a great way to cut down on load to the rendering services.\n. Am I wrong about this, or are there still any outstanding issues? I believe this is a great way to cut down on load to the rendering services.\n. \n",
    "EdstromP": "I agree. I have a graph displaying accumulated gas consumtion since a couple of years ago and would like to add a table showing the value of the end of each month. I've tried summarize(foo.bar.h2, \"1month\", \"last\") but depending on the currently chosen time range, I get different results.\nIs there any workaround to achieve this?. ",
    "dnotivol": "Thanks a lot for your assistance. I already have it working. I was tweaking some Apache configuration to leave it simpler and easier to debug. \nBut at last, I think the error was the silliest on earth... I forgot the trailing slash in the URI when posting the event. When using http://localhost:8000/events/ ending in a slash, everything works fine.\nSorry to disturb you with such a silly thing (I don't know how many times I checked everything before posting, and I didn't see it...).\n. ",
    "aaronrl": "I had installed Graphite from instructions on Digital Ocean's Community Tutorials (https://www.digitalocean.com/community/tutorials/how-to-install-and-use-graphite-on-an-ubuntu-14-04-server) for Ubuntu 14.04 and ran into this issue also.  The graphite-web package that it advises you to install also installs Django 1.6.1.  I decided not to try to yank out that package and install Django with pip as I didn't feel comfortable creating a potentially hellish dependency nightmare for myself.\nInstead, I opted to edit the file that the traceback mentions (\"/usr/lib/python2.7/dist-packages/graphite/events/views.py\", line 45) to the following (changed 'raw_post_data' to 'body'):\n``` python\ndef post_event(request):\n    if request.method == 'POST':\n        event = json.loads(request.body)\n        assert isinstance(event, dict)\n    values = {}\n    values[\"what\"] = event[\"what\"]\n    values[\"tags\"] = event.get(\"tags\", None)\n    values[\"when\"] = datetime.datetime.fromtimestamp(\n        event.get(\"when\", time.time()))\n    if \"data\" in event:\n        values[\"data\"] = event[\"data\"]\n\n    e = models.Event(**values)\n    e.save()\n\n    return HttpResponse(status=200)\nelse:\n    return HttpResponse(status=405)\n\n```\nI deleted the views.pyc file and restarted Apache and everything was good to go!  Hopefully that helps as a workaround.\n. ",
    "makefu": "i stumbled upon the same issue, installing the most recent version of django-tagging worked.\n. ",
    "jazzzz": "Could we have this merged on 0.9.x?\n. A cherry pick worked fine.\n. ",
    "tomasdubec": "This happens only for metrics starting with CARBON_METRIC_PREFIX, because they are handled by send_request_to_all instead of send_request and send_request_to_all is not updated for new interface between graphite-web and carbon (dict instead of list). I will create PR momentarily..\n. This happens only for metrics starting with CARBON_METRIC_PREFIX, because they are handled by send_request_to_all instead of send_request and send_request_to_all is not updated for new interface between graphite-web and carbon (dict instead of list). I will create PR momentarily..\n. https://github.com/graphite-project/graphite-web/pull/1224\n. ",
    "ttarczynski": "I have the same issue with the latest versions of graphite-web, carbon and whisper from master:\nFri Aug 14 08:15:38 2015 :: Got an exception when fetching data! See: 'float' object is not iterable Will do it again! Run: 1 of 2\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 136, in fetchData\n    seriesList = _fetchData(pathExpr,startTime, endTime, requestContext, seriesList)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 99, in _fetchData\n    fetches = [(node, node.fetch(startTime, endTime)) for node in matching_nodes if node.is_leaf]\n  File \"/opt/graphite/webapp/graphite/node.py\", line 30, in fetch\n    return self.reader.fetch(startTime, endTime)\n  File \"/opt/graphite/webapp/graphite/readers.py\", line 177, in fetch\n    for (timestamp, value) in cached_datapoints:\nTypeError: 'float' object is not iterable\nFri Aug 14 08:15:38 2015 :: Failed after 2 retry! See: 'float' object is not iterable\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 136, in fetchData\n    seriesList = _fetchData(pathExpr,startTime, endTime, requestContext, seriesList)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 99, in _fetchData\n    fetches = [(node, node.fetch(startTime, endTime)) for node in matching_nodes if node.is_leaf]\n  File \"/opt/graphite/webapp/graphite/node.py\", line 30, in fetch\n    return self.reader.fetch(startTime, endTime)\n  File \"/opt/graphite/webapp/graphite/readers.py\", line 177, in fetch\n    for (timestamp, value) in cached_datapoints:\nTypeError: 'float' object is not iterable\nFri Aug 14 08:15:38 2015 :: Exception encountered in <GET http://graph1/render/?width=586&height=308&_salt=1439540137.906&target=carbon.agents.graph1-a.droppedCreates>\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/django/core/handlers/base.py\", line 112, in get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 112, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 8, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 29, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression, replacements)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 45, in evaluateTokens\n    return fetchData(requestContext, expression)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 141, in fetchData\n    raise Exception(\"Failed after %i retry! See: %s\" % (settings.MAX_FETCH_RETRIES, e))\nException: Failed after 2 retry! See: 'float' object is not iterable\nAnd it also happens only for metrics within CARBON_METRIC_PREFIX.\nI've tried applying #1224 patch and it has fixed the problem for me.\n. I've just applied this patch on my graphite-web and it fixes the #1223 issue for me.\n. ",
    "aniketbhatnagar": "+1\nI ran into this issue as well\n. ",
    "deejay1": "Same here :/\n. @deniszh Thanks for merging it! Should I backport it for 1.0.x branch? A cherry pick works fine at our installation here.. This is a \"backport\" of https://github.com/graphite-project/graphite-web/pull/1906. It sort of \"breaks\" (just an ugly warning) a lot of dashboards in Grafana where there was  selected, that's why I noticed it. Maybe a HTTP 400 like for the missing parameter would be in order? I only wonder how to properly unit test it :(. Ok, I think we can close this issue now, I'm ok with a proper error message.. @cbowman0 oh, so sorry about that I missed that in my testing :(. ",
    "fmargelidon": "I had check my file /opt/graphite/webapp/graphite/render/functions.py and I have already this line. This is why I don't understand the reason of my issue.\n. ",
    "dentrifonov": "I use this graphite.wsgi config:\n```\n/opt/graphite/conf/graphite.wsgi\nimport os, sys\nsys.path.append('/opt/graphite/webapp')\nos.environ['DJANGO_SETTINGS_MODULE'] = 'graphite.settings'\nimport django.core.handlers.wsgi\napplication = django.core.handlers.wsgi.WSGIHandler()\n```\nand this config for uWSGI:\n```\n/etc/uwsgi/apps-available/graphite.ini\n[uwsgi]\nprocesses = 2\ngid = www-data\nuid = www-data\nchmod-socket = 660\nchdir = /opt/graphite/conf\nwsgi-file = graphite.wsgi\nsocket = /tmp/graphite.sock\npidfile2 = /tmp/graphite.pid\nmaster = true\nvacuum = true\nplugins = python\n```\nGraphite Web from master branch, Carbon from megacarbon branch. May be it will be work for you.\n. I also have WEBAPP_VERSION = '0.10.0-alpha', installed from Git. Are you install from PyPi or Git?\n. I have Ubuntu 14.04.1 LTS (GNU/Linux 2.6.32-32-pve x86_64) and Python 2.7.6.\n- Install from apt:\n  - python-cairo\n  - python-django\n  - python-pip\n  - python-pyparsing\n  - python-django-tagging\n  - python-memcache\n- Install from pip:\n  - pytz\n- Checkout source code to any directory. I work with 15ccba8c83910507ff0a8f4205ec039aecce9711 commit hash.\n- python setup.py install from checkouted directory.\n- Make /opt/graphite/webapp/graphite/local_settings.py. My version:\n```\nSECRET_KEY = \"generate_your_django_secret_key\"             \nTIME_ZONE = \"UTC\"                                                                                                                               \nSTORAGE_FINDERS = (\n'graphite.finders.ceres.CeresFinder',\n'graphite.finders.standart.StandartFinder', if using Whisper\n)\nCLUSTER_SERVERS = [\"\"]\nCARBONLINK_HOSTS = [\"127.0.0.1:7102:cache1\", \"127.0.0.1:7202:cache2\"]\nREPLICATION_FACTOR = 1\n``\n- Make/opt/graphite/conf/graphite.wsgi`. My version:\n```\nimport os, sys\nsys.path.append('/opt/graphite/webapp')\nos.environ['DJANGO_SETTINGS_MODULE'] = 'graphite.settings'\nimport django.core.handlers.wsgi\napplication = django.core.handlers.wsgi.WSGIHandler()\n```\n- Create DB:\n$ export PYTHONPATH=\"/opt/graphite/webapp\"\n$ django-admin syncdb --settings=graphite.settings --noinput\n- Install from apt:\n  - uwsgi\n  - uwsgi-plugin-python\n- Create uwsgi config /etc/uwsgi/apps-available/graphite.ini. My version:\n[uwsgi]\nprocesses = 2\ngid = www-data\nuid = www-data\nchmod-socket = 660\nchdir = /opt/graphite/conf\nwsgi-file = graphite.wsgi\nsocket = /tmp/graphite.sock\npidfile2 = /tmp/graphite.pid\nmaster = true\nvacuum = true\nplugins = python\n- Create symlink to config /etc/uwsgi/apps-enabled/graphite.ini\n- Start uwsgi sudo service uwsgi start graphite\nAlso need install Apache or Nginx for proxing requests to uwsgi, and correct mode and chown rights for storage and logs directories, and DB file (I use www-data user).\n. I remembered, this article very helpful for full installing Graphite (with Ceres storage):\nhttp://anatolijd.blogspot.ru/2013/06/graphitemegacarbonceres-multi-node.html\n. Sorry, I didn't used SuSe. In article Graphite installed from source excluding dependencies.\n. In general you need install Ceres or Whisper for storing metrics, and Carbon for receiving and quering metrics from Graphite Web application.\n. I don't use OpenTSDB and InfluxDB as the backend storage for Graphite. It seems I won't be able to help more. :(\n. You need setup web server to serve content folder by /static/* requests. My example for Nginx:\n...\nlocation /static {\n  alias /opt/graphite/webapp/content/;\n}\n...\n. I don't use Apache Server. Google say this about Apache config:\nAlias /static /opt/graphite/webapp/content/\n. @GoozeyX: Cool! I'm glad that it was helpful at least someone. :)\n. ",
    "pdufault": "@gsaray101 Check your local_settings.py for your SECRET_KEY setting, it's set to the default which isn't a good idea.  local_settings.py is in /opt/graphite/webapp/graphite/local_settings.py for me.\n. ",
    "GoozeyX": "@dentrifonov : Just came here to say thank you for posting the wsgi.py ... :+1: \n. Is there any specific reason why this was not merged into the master branch?\n. ",
    "olliewalsh": "Appears to be fixed already on 0.9.13-pre1, but not on master: https://github.com/graphite-project/graphite-web/blob/0.9.13-pre1/webapp/graphite/render/datalib.py#L372\n. ",
    "joshmyers": "Bump. There is a recent Django vuln here. \nGraphite-web doesn't look to currently support django 1.8.7. It would be nice if it did.\n. ",
    "theromis": "I can work on it for graphite, do you interested in it?\n. ",
    "yuriyzubarev": "Solved by creating a dedicate daily pattern in storage-schemas.conf\n. ",
    "evansd": "Just to let you know that as of v2.0.1 WhiteNoise supports Python 2.6 (and is now tested with that) so you can now remove the version check if you want to.\n. Ah I see: you're not in control of the version of WhiteNoise installed. That does make things more complicated.\n. I've now added a __version__ attribute to WhiteNoise and created a pull request which checks this:\nhttps://github.com/graphite-project/graphite-web/pull/1296\n. Shall I close this?\n. ",
    "jstangroome": "CI needs to be re-run following merge of PR #1273\n. CI needs to be re-run following merge of PR #1273\n. @deniszh I couldn't find the mentioned documentation file in the 0.9.x branch so I used docs/config-local-settings.rst instead. I hope that's ok.\nI will investigate what is required to port this feature to the master branch but I would appreciate if it can be merged into 0.9.x until then.. @DanCech Good points.\nContinuing my example above, an end-user request for ?target=customer1.some.metric would propagate to the back-end unaltered but upon return would be presented as just some.metric.\nSimilarly if there was a back-end metric named customer1.customer1.foo then the end-user would need to request ?target=customer1.customer1.foo to access it and upon return it would be presented as just customer1.foo.\nThis behaviour is certainly suboptimal but should still mean that all metrics returned from the back-end are constrained to the subset defined by the prefix configuration even if the returned names don't match the requested names.\nTo be honest, the original commit for this PR was written so long ago that I don't recall if it was necessary for the prefix to be conditionally prepended and truncated or if it was just because I was uncertain about the how the original code would call my code.\nI'll take a stricter approach when I implement this for master. Do you need a stricter approach before merging this PR for 0.9.x?. CI needs to be re-run following merge of PR #1273\n. @deniszh How can consolidation always be using 'avg'?\n\nMy test queries target=consolidateBy(metric, 'sum')\nThe consolidateBy function sets the series.consolidationFunc: https://github.com/graphite-project/graphite-web/blob/0.9.15/webapp/graphite/render/functions.py#L794\nWhen format == 'json' and maxDataPoints < numberOfDataPoints then series.consolidate(valuesPerPoint) is called: https://github.com/graphite-project/graphite-web/blob/0.9.15/webapp/graphite/render/views.py#L163 \nThe series iterator checks valuesPerPoint and consolidationFunc to apply the chosen sum aggregation to produce the returned data: https://github.com/graphite-project/graphite-web/blob/0.9.15/webapp/graphite/render/datalib.py#L77. Also, numberOfDataPoints == 100 and the consolidation works correctly for all values of maxDataPoints from 50 to 101 inclusive. Only when maxDataPoints <= 49 do incorrect results occur.\n\nI've just repeated the test with a seed Whisper file containing only 10 data points and I'm fairly sure the problem is due to the \"nudge\" deleting the first data points: https://github.com/graphite-project/graphite-web/blob/0.9.15/webapp/graphite/render/views.py#L156-L162\nThe original commit introducing the nudge logic doesn't seem to add much insight beyond the comment \"removing 'jitter' seen when refreshing\" which in my mind would mostly be  applicable when the requested time range is relative to the wall clock and time is passing between queries.\nI wonder whether the ability to disable the nudge logic would be a useful render query parameter, especially for JSON queries where the results are often used for purposes other than drawing line charts.\nDetails of test output with only 10 real data points: output10.txt\n. ",
    "Thermatix": "So it is version '0.9.13', just that the version number hasn't been bumped?\n. ",
    "krajesh112": "@brutasse  thanx for your reply..                                                                                                                 no file is available in manage.py in //opt/graphite/webapp/graphite\n. ",
    "llicour": "I've just update the PR to include a new API that allow to change some carbon cache settings online.\nThe following settings are supported :\n- CACHE_WRITE_STRATEGY\n- CACHE_WRITE_TUNED_STRATEGY_LARGEST\n- CACHE_WRITE_TUNED_STRATEGY_RANDOM\n- CACHE_WRITE_TUNED_STRATEGY_OLDEST\n- CACHE_PERSIST_INTERVAL\n- MAX_UPDATES_PER_SECOND\n  Sample :\n  curl localhost/metrics/set-param -X POST -d '{\"key\":\"CACHE_WRITE_STRATEGY\",\"value\":\"tuned\"}'\n  curl localhost/metrics/set-param -X POST -d '{\"key\":\"MAX_UPDATES_PER_SECOND\",\"value\":\"1\"}'\nNote: changes are not persistant. If process is restarted, settings from configuration file will be restored\n. ",
    "apg-pk": "Yes, shared memcached, but unfortunately not harmless. The nodes without the metric are able to insert an empty [] into the cache. The empty list is considered a cache hit  and sent in the response even though the cache log reports a miss. In the code below, the empty list is considered false for the conditional but also not None for the actual request context:\n``` python\n    if useCache:\n      targets = requestOptions['targets']\n      startTime = requestOptions['startTime']\n      endTime = requestOptions['endTime']\n      dataKey = hashData(targets, startTime, endTime)\n      cachedData = cache.get(dataKey)\n      if cachedData:\n        log.cache(\"Data-Cache hit [%s]\" % dataKey)\n      else:\n        log.cache(\"Data-Cache miss [%s]\" % dataKey)\n    else:\n      cachedData = None\nif cachedData is not None:\n  requestContext['data'] = data = cachedData\nelse: # Have to actually retrieve the data now\n\n```\nhttps://github.com/graphite-project/graphite-web/blob/0.9.13-pre1/webapp/graphite/render/views.py#L98-L113\n. Hrm... we are using REPLICATION_FACTOR=1 and getting empty metrics, so perhaps this is a slightly different issue from getting redundant metric time series from clustered backends w/ REPLICATION_FACTOR>=2?  PR #1279 just prevents empty results from getting into shared memcached and locking out the actual results.\n. ",
    "zhangela": "Just had the same issue. Would love to have this feature as well! . ",
    "nnuss": "This may be a potential alternative in this specific case of counters over time:\nnonNegativeDerivative(movingSum(system.neonisp[0-9].isp.neon_updater_http_fetch_fail,\"10min\"))\nIf you're not running latest graphite this still works well with wildcards (++) but lowers resolution (--): nonNegativeDerivative(summarize(system.neonisp[0-9].isp.neon_updater_http_fetch_fail,\"10min\",\"last\")). Some minor(?) points:\n That table was last substantially updated in July. I just skimmed the changes in functions.py and added delay().\n The table lists the first release occurance of a function as it appears now and this might differ from @OrangeDog's original request. Notably it lists cactiStyle() as \"latest\" because of b347316e95830a392684367419c82309b730ff11 but it was first introduced long ago in 7328a8d2a700b38e6a3b80dd31f8ab3c9b567766 (0.9.9) and the change to specify the units in a52ef41e66a124ecfb7bc51bd72d0c6fccc20817 would have coincided with 0.9.11 had it been in the 0.9.x branch.. // A little more concrete set - replacing pagenumber with acutal numbers\nwebsite.API.version.1.response\nwebsite.API.version.2.response\n...\nwebsite.API.version.99.response\nWhere the upper bound of pagenumber is not known (but in this example 99):\nYou want:  website.API.version.99.response\nIs that what you would like to achieve?. The closest I can conjure up is:\nlimit(sortByName(website.API.version.*.response),1) // requires 0.9.14+\nbut that gives the pagenumber which is first in sort order (lowest -or- with highest leading digits ;  depending on how you use sortByName).\nYou seem to need at least one of:\n- sortByName(seriesList, natural=False, descending=False) : support descending order\n- reverse(seriesList) : new function to reverse ordering\n- limit(seriesList, n) : support negative indexing for 'n' (from the end of the list)\n- first(seriesList, n) / last(seriesList,n) : alias first() and limit() / make new last() function\nHowever. If pagenumber is not the first variable element in the names even those above may not be enough.. sortByMaxima() examines series data points to perform the sort.\nIn fact limit(sortByMaxima(),n) gives the same behavior as highestMax().\nWhat we described above (locating highest pagenumber node) needs sorting of the series name to select the desired metric.\nThese are functions I spot that operate over the name alone:\n- alias & aliasByMetric & aliasByNode & aliasSub\n- exclude & grep\n- sortByName\n- substr. > >  Is there a way to indicate carbon to keep one value per month ?\n\nYes, you can configure the storage schema accordingly ...\n\nNot exactly no. The size of a [whipser] datapoint must be a fixed multiple of a second (true months are not).\nOne point per day is a good way to capture this however; in combination with @piotr1212's suggestion to output it [daily]. (Which amounts to just over 4KB per year)\nBe aware that in graphite 1 month == 30 days ; 1 year == 365 days (see graphite/render/attime.py). > Looks good, not sure how up to date those carbon-api benchmarks are.\nThey are almost certainly dated. When we made those comparisons the reference was 0.9.12 as I recall.. ",
    "dooblem": "Hello,\nI have the problem too.\nSome news on that ?\nThanks in advance,\nMarc\n. I just patched my 0.9.12 with the following to make it work:\n```\n--- views.py.bak    2015-09-15 16:23:13.000000000 +0200\n+++ views.py    2015-09-15 16:25:55.000000000 +0200\n@@ -7,7 +7,7 @@\n from django.shortcuts import render_to_response, get_object_or_404\n from pytz import timezone\n-from graphite.util import json\n+from graphite.util import json, epoch\n from graphite.events import models\n from graphite.render.attime import parseATTime\n@@ -19,7 +19,7 @@\n class EventEncoder(json.JSONEncoder):\n     def default(self, obj):\n         if isinstance(obj, datetime.datetime):\n-            return to_timestamp(obj)\n+            return epoch(obj)\n         return json.JSONEncoder.default(self, obj)\n```\n. ",
    "jeffday1113": "Don't know if I am missing something but in the post above describing use cases, in what file/configuration section would you specify the user and associated filterRegex?\n. ",
    "franklupo": "Hi,\nthere's news about that?\nthank you\n. ",
    "netmarkjp": "+1\nI have same problem with ceres==0.10.0\n. I tried directory find in django shell, but failed...\n```\n$ PYTHONPATH=/opt/graphite/webapp python /opt/graphite/webapp/graphite/manage.py shell\n\n\n\nfrom graphite.storage import STORE\n[o for o in STORE.find(\"ME.web0*.linux.cpu.usage.cpu.user_cpu\")]\n[)>, )>]\n[o for o in STORE.find(\"ME.web0[12].linux.cpu.usage.cpu.user_cpu\")]\n[)>, )>]\n[o for o in STORE.find(\"ME.web0{1,2}.linux.cpu.usage.cpu.user_cpu\")]\n[]\n[o for o in STORE.find(\"ME.{web01,web02}.linux.cpu.usage.cpu.user_cpu\")]\n[]\n```\n. \n\n\n",
    "ibuclaw": "Previous attempts at adding this functionality to graphite. #483 #550\n. I think this should use glob(pattern) to support {foo*,bar*}baz also.\n. Oh, I didn't see that glob was use in the finder instead.  I had a for fs_path in extract_variants pattern in mind.\n. ",
    "LeonidVasilyev": "@obfuscurity strange that it works fine in Graph Composer. Is it documented somewhere which characters are valid and which are not? \nI use Metrics.NET library which use this notation by default\n. ",
    "pvanassen": "PR merged\n. Brought up to date with master\n. ping\n. ",
    "jkeifer": "I also have a similar problem, and had to go digging into the source to figure out why.\nAnd that reason why is because reduce supplies the \"reduce function\" with the metrics that match the \"reduce matchers\" as a series list. That is, your example calls divideSeries([used_bytes, total_bytes]) when you really want to call divideSeries(used_bytes, total_bytes). Notice this second version pass in two arguments, whereas the first only provides one argument (you may notice from the exception that divideSeries takes a third argument internally, which is the HTTP request context, but you don't need to worry about that)\nIn fact, because of this behavior, the example used in the docs for reduceSeries (which you are effectively trying to replicate here) does not seem like it would result in the correct values. As you probably realized, your example\nreduce(map(SERVER008.diskinfo.*.*,2),\"asPercent\", 3, \"used_bytes\", \"total_bytes\")\ngives you 100 * used_bytes / (used_bytes + total_bytes), not 100 * used_bytes / (total_bytes).\nI believe one option to fix reduce so it actually works would be to accept lists of \"reduce matchers\" for each argument to be passed into the reduce function. Thus, in your case, you would call\nreduce(map(SERVER008.diskinfo.*.*,2),\"divideSeries\", 3, [\"used_bytes\"], [\"total_bytes\"])\nHowever, I am not part of the graphite team and cannot make such a decision. I will likely put in a pull request for a new reduce function that would work in such a manner; whether that pull request gets included is up to someone else.\n. Note that I spoke too soon. It appears that others have already noticed this problem and have solved it. We apparently both need to update. See issues #1066 and #1265.\n. ",
    "Civil": "@tmm1 maybe you'll know why it was done like this?\nThis function calls 'save', do some stuff and then restore context. So it seems to be useless.\n. Also I even can't find any use of this for svg. Because you save context, do something, restore context. And restoring context erase all the changes. At least from what I saw and what I understand.\n. Just some proofs that it's not needed:\nhttp://cairographics.org/documentation/pycairo/2/reference/context.html (http://www.cairographics.org/manual/cairo-cairo-t.html#cairo-save - original one from cairo itself)\nContexts can be pushed to a stack via Context.save(). They may then safely be changed, without loosing the current state. Use Context.restore() to restore to the saved state.\nSo actually whatever you do between save-resotre will be lost. So this function is useless, doing unnecessary mem copies a lot of times, then it draws some text outside of the canvas and then safely restores context to previous saved state. Whatever was the reason behind adding this function - it is WRONG and doing a lot of useless work.\n@brutasse @obfuscurity can you please look at this closely? From my point of view you should remove encodeHeaders function or at least don't call it for png images if you are afraid of removing it (even though cairo docs says it shouldn't do anything meaningful). \nI've also sent an email to Aman Gupta, and he said following things:\n```\nIt is meant for svgs, basically puts some metadata in the generated svg xml that is not inside the frame. This can be used to manipulate the svg later on with framework like d3.\nYou should be able to compare the generated svg before and after that change to see the results.\n```\nSo to be sure that it's useless you can just diff svgs with and without it. Also it proofs that even originally it's useless for png.\n. @deniszh this is still happens in recent 1.1 :). same thing is also true for green <-> darkgreen, blue <-> darkblue, . Just a note - FNV1a != Jump FNV1a, so the issue is not fixed.\n. The thing is that the \"Fixed\" colors are very hard to read on graphs. I think it'll be much better to redefined \"dark\" versions of colors and leave \"normal\" as-is.\n\n\n. > what are the downsides to carbonserver compared to our current cluster method? do we lose any functionality or desirable characteristics?\ncarbonserver appeared as a attempt to implement a subset of graphite-web that's enough to server as a CLUSTER_SERVER for graphite-web. With time it evolved and also have a binary protobuf-based protocol to return those metrics. However it lacks TagDB support, however there were some attempts to implement it here: https://github.com/go-graphite/go-carbon/tree/carbonserver-tags and here https://github.com/go-graphite/go-carbon/pull/1 (maybe it's worth to revive and finish them).\n\nCarbonlink-like GRPC api (aka \"carbonserver\")\n\nThere is a Protobuf + GRPC support for CLUSTER_SERVER-style communication inside carbonserver (but it doesn't support tags as of now).\nThere is also a true carbonlink-style API with the same purpose:\nhttps://github.com/lomik/go-carbon/blob/master/helper/carbonpb/carbon.proto. https://github.com/graphite-project/graphite-web/blob/b6d6d93665a4fd11c347a8e966b51fec6520fc33/webapp/graphite/render/functions.py#L3510-L3513\nThis is the place where the 'value' is used. For a reference point.\nPlus here's the quote from the description, where it's also clear that it should be \"integer\":\nhttps://github.com/graphite-project/graphite-web/blob/b6d6d93665a4fd11c347a8e966b51fec6520fc33/webapp/graphite/render/functions.py#L3500-L3502. Well, that's true, \"float\" makes more sense for me as well, however in the examples you've always used \"integer\" which is a bit confusing.. ",
    "rlueckl": "Hi @deniszh,\nthanks for the fast answer. Unfortunately I don't really understand what you write, because that's the exact opposite. If the cached wouldn't cache the 'carbon' metrics, then all would be good. The latest metrics would always be available on the disk and graphite would show them \"up-to-date\".\nBut the 'carbon' metrics are cached (in the cache). The problem is that graphite ask the wrong cache (because it doesn't know that 'carbon' metrics are \"localized\" to the cache).\nAn example:\ncarbon-relay, DESTINATIONS = ['127.0.0.1:2004:a','127.0.0.1:2104:b','127.0.0.1:2204:c']\ngraphite-web, CARBONLINK_HOSTS = ['127.0.0.1:7004:a','127.0.0.1:7104:b','127.0.0.1:7204:c']\ncarbon-cache:a writes metric 'carbon.agents.server1-a.memUsage' to his own LOCAL_DATA_DIR.\ngraphite-web wants to query 'carbon.agents.server1-a.memUsage', therefore graphite-web hashes the metric and discovers that he has to ask carbon-cache:c for the metric. But carbon-cache:c doesn't have the metric, because it's generated \"locally\" in carbon-cache:a.\nAm I making sense? :)\n. Hi @deniszh,\nthanks, 48bbfbe was exactly what I was looking for. I see that the change was merged on Feb 28, 2014, but I don't see it in the Dec 31, 2014 release of 0.9.13?\nDo you know when we can expect this \"feature\"? Will it be in the next release and if yes, when is the next release coming?\n. Thanks for all the information! My company is currently staying with carbon 0.9.13, so we will have to live with this \"issue\". :smile: \n. ",
    "dforste": "Graphite 0.9.13 is bata but it is the version you get if you do a pip install. I hit issues when I was using it. I have since reverted to 0.9.12 in production but this brings other issues. \n. ",
    "ravibhooshan": "Thanks for all reply and sorry for very late in reply.\nI am having issue in rendered graph. When I render a graphite graph in our application for last 20 minutes, it doesn't show any data but when we change the time in rendered link 1hour behind it starts showing correct data.\nFor example - If my rendered link is showing start time - 10:15 end time 10:35. Graph will not show any data (assuming i am running it at 10:35) but when I will change start time to and end time to 9:15 and 9:35 respectively it will start showing data from 10:15 to 10:35.\nI have not noticed this i n 0.9.13.\n. Thanks Ashok.\nMy IO/Mem stats are average i means it is not stressed out.\nI have 3 carbon-caches on 2 of four carbon cache server. But in multi-cache setup relay-rules.conf is not forwarding data as per regex.\nDo you suggest modifying any of carbon.conf setting which can improve.\nMy main concern is that why relay is dropping 50% of data.\n. Thanks Ashok.\nMy IO/Mem stats are average i means it is not stressed out.\nI have 3 carbon-caches on 2 of four carbon cache server. But in multi-cache setup relay-rules.conf is not forwarding data as per regex.\nDo you suggest modifying any of carbon.conf setting which can improve.\nMy main concern is that why relay is dropping 50% of data.\n. Thanks again Ashok. I was planning to move on to relay-c.\nI think you missed to add blog url.Please send me blog link.\n. Thanks again Ashok. I was planning to move on to relay-c.\nI think you missed to add blog url.Please send me blog link.\n. # USE_WHITELIST = False\nUSE_WHITELIST = True\n[cache:a]\nLINE_RECEIVER_PORT = 2023\nPICKLE_RECEIVER_PORT = 2024\nCACHE_QUERY_PORT = 7022\n[cache:b]\nLINE_RECEIVER_PORT = 2033\nPICKLE_RECEIVER_PORT = 2034\nCACHE_QUERY_PORT = 7032\n. Yes i have copied that section of carbon conf. Do you want me to pasre whole carbon conf.\nSent via the Samsung Galaxy Alpha\u2122, an AT&T 4G LTE smartphone\n-------- Original message --------\nFrom: Jason Dixon notifications@github.com\nDate: 09/29/2015  10:19 AM  (GMT-05:00)\nTo: graphite-project/graphite-web graphite-web@noreply.github.com\nCc: ravibhooshan ravibhooshan@hotmail.com\nSubject: Re: [graphite-web] Carbon whitelist (#1333)\nCan you paste the relevant section of your carbon.conf?\n\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/issues/1333#issuecomment-144074918\n. I have similar setup and achieved unified web console using Grafana.\n. 'HOST': '192.168.*.', - is it correct.\nI am not sure about statsd config , are you running carbon-relay.\nAlso please check whetehr you have whisper directory created or not. carbon user should own this directory.\n. Belos is result from whisper-dump. See the value for 3rd datapoint.\n230: 1450440000, 15.047333333333332561210227140691131\n231: 1450443600, 13.88779166666666675666874652961269\n232: 1450447200, -1.0000000000000000438458430450761974e+47\n233: 1450450800, 17.735624999999998863131622783839703\n. ",
    "halysan": "Hi !\nThks for your answer,\nYes I had observed this error.\nBut my folder \"/var/lib/graphite\" is \"_graphite:_graphite\"..\nAnd my file \"graphite.db\" is \"_graphite:_graphite\"...\nI don't understand very well...\nI just made a normal installation with \"aptitude\"...\n. OK.... I changed permission and handed the default\nand it works..\nStrange...\nThanks for your help\n. ",
    "Firm": "Well, they are not related closely. This one can be investigated separately as it has no relation with requests to carbon-cache instances.\nP.S. I found similiar issue with carbon-relay - https://github.com/grobian/carbon-c-relay/issues/8\n. I understand that in most cases this means 'no data yet, try again'.\n. Hi,\ntcpdump looks insteresting:\n```\n12:32:23.498448 IP ip6-localhost.43123 > ip6-localhost.8090: Flags [S], seq 1301429992, win 32768, options [mss 16396,sackOK,TS val 2962778175 ecr 2962778013,nop,wscale 9], length 0\nE..V.@.@............s..M...........0....@....\n..h?..g....\n12:32:23.498474 IP ip6-localhost.8090 > ip6-localhost.43123: Flags [S.], seq 3003916123, ack 1301429993, win 32768, options [mss 16396,sackOK,TS val 2962778175 ecr 2962778175,nop,wscale 9], length 0\nE....@.@.<............s...[M.......0....@....\n..h?..h?...\n12:32:23.498496 IP ip6-localhost.43123 > ip6-localhost.8090: Flags [.], ack 1, win 64, options [nop,nop,TS val 2962778175 ecr 2962778175], length 0\nE..4V.@.@............s..M.>.......@.(.....\n..h?..h?\n12:32:23.498539 IP ip6-localhost.43123 > ip6-localhost.8090: Flags [P.], seq 1:241, ack 1, win 64, options [nop,nop,TS val 2962778175 ecr 2962778175], length 240\nE..$V.@.@............s..M.>.......@.......\n..h?..h?GET /dashboard/find/ HTTP/1.0\nHost: gweb.XXX.XX\nX-Real-IP: X.X.X.X\nConnection: close\nUser-Agent: curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.16.2.3 Basic ECC zlib/1.2.3 libidn/1.18 libssh2/1.4.2\nAccept: /\n12:32:23.498552 IP ip6-localhost.8090 > ip6-localhost.43123: Flags [.], ack 241, win 64, options [nop,nop,TS val 2962778175 ecr 2962778175], length 0\nE..4wc@.@..^...........s...\\M.?....@.(.....\n..h?..h?\n```\nAnd there's no evidence in strace output that this request arrived into graphite-web app code.\n. Further investigation found that graphite app gets stuck after we added more metrics (5 mln->7 mln) while using a lot of aggregating functions for groups of hosts/metrics. If I increase memcached cache time from 1 min to 10 then things get better. But I need more granular (even 5-10 seconds max caching time). Even increasing number of graphite apps from 16 to 32 don't help much.\n. ",
    "pu239ppy": "Any chance this might get merged one day?  I am currently deploying graphite from my own repo\n. Great I will drop out the extra code and tests and update the PR\n. Rebased on upstream master.  All tests are passing\n. Anything else I need to do in order to get this in?\n. I think I've gotten everything in, any chance this will get merged?\n. Hi any ETA on this PR?  Am I missing something that prevents it form being merged?\n. Is support for python < 2.6 still required for HTTP? I would happily remove the connector_class_selector along HTTPSConnectionWithTimeout and HTTPConnectionWithTimeout\n. ",
    "schancel": "Sure, thank you.  The attached file is a sample of what would be there -- I don't have the particular wsp file that produced the image.  This is the series that would produce such an image though.\nhttps://s3-us-west-1.amazonaws.com/shammah/disk_used.wsp\n. @obfuscurity Sorry for the delay.  Looks like I'll be able to test 0.9.15 soon.   I'll post here once I find out if that resolved the issue.  Thanks for looking into it.\n. ",
    "atnak": "Let's say I toss a six-sided dice.  The result of the toss will be a number between 1 to 6.\nNow, the dice is actually weighted, and the chance of throwing a one is much higher than the chance of throwing any other number.  After 100 tosses, the results are 66 ones, no twos, 24 threes, 10 fours and no fives or sixes.\nI'm going to put the result into graphite:\nface.one.count 66 t=1\nface.three.count 24 t=1\nface.four.count 10 t=1\nNotice how I didn't include two, five or six?  I'll get back to why I did that in a minute, but it should be implicit that two, five, and six are zeroes at t=1.  The rule follows that, at any t=T, the existence of any face.X.count N implies there also exists face.Y.count 0 for all possible faces that aren't explicitly represented at time T.\nGetting back to the dice, I actually have thousands them.  They each have their own weights, and make few hundred tosses a second (summed minutely).  Moreover, they're all 10 sided.\ndice.1.face.one.count 66 t=1\ndice.1.face.three.count 24 t=1\ndice.1.face.four.count 10 t=1\ndice.2.face.two.count 78 t=2\ndice.2.face.four.count 90 t=2\n...\nThe reason number 1 of why I don't store explicit zeroes at every time T is because, if I filled zeroes for every face for every time I took a summary, the order of magnitude increase in data will swamp carbon and cripple my system by the IO involved. (I tried a few times and ultimately gave up.)\nI need the zeroes there however, whether it be from transformNulls(), because they're fed into funky things like derivative().  Also, graphs with connected lines plot better with proper zeros.\nThe reason number 2 of why I don't (or rather can't) store explicit zeros is because, the total number of dies and faces increases dynamically over time.  It'll be extremely tedious to go back and explicitly add historical zeroes every time I add a new face or die, just so that I can retrospectively treat their intervals as zeroes .\n. > @atnak dare I ask, are you using Graphite to study DNA strands?\nNo, but the example was probably too abstract.  Here's another attempt.\nLet's turn the analogy into web servers, HTTP requests, and HTTP status codes.\nLet's say I have a server that accepts thousands of requests per minute, and I'm going to feed graphite the count of responses, summarized per status code, to plot minutely.\nserver.1.status.200 1595 t=1\nserver.1.status.202 37 t=1\nserver.1.status.503 12 t=1\nserver.1.status.301 407  t=1\n...\nHere's a tabular representation. (t=1 is 9:20)\n| server 1 | 9:20 | 9:21 | 9:22 | 9:23 |\n| --- | --- | --- | --- | --- |\n| 200 | 1595 | 1634 | 1612 |  |\n| 202 | 37 | 41 | 10 |  |\n| 503 | 12 | 16 | 17 |  |\n| 301 | 407 | 398 | 393 |  |\n| 500 |  | 2 | 1 |  |\n| 404 |  |  | 1 |  |\n- Notice there're no values at 9:23.  That's the no data zone.  It's the time interval for which server logs haven't been summarized and fed into graphite yet.  They shouldn't be plot as their values are unknown.  (These zones can also be in the past, due to missing logs.)\n- Notice also how there're no values for 500 and 404 at 9:20 and 9:21.  Those are the should be treated as zero zones.  We know they're zeros because there exists other values at those times.\nObviously, it's infeasible to feed graphite explicit zeros for every other conceivable HTTP status code at every single time.\nHere's example usage:\ntransformNulls(server.1.status.*, 0, server.1.status.*)\nderivative(transformNulls(server.*.status.*, 0, server.*.status.*))\n. Thanks for the feedback! I'll look them over as soon as I get back home at the end of the week.\nI'm a bit on the fence about the term keySeries.. Maybe referenceSeries might be more self-documenting.\nAlso, more I look at the second example, more I wonder if there should be some consideration to grouping (like sumSeriesWithWildcard et al.)..  Though I suppose this could be done by someone else if they feel the itch.. (Maybe its possible w/ groupByNode())\n. Need to test the series.name part when I get into work, but fixes aside from that.\n. #### With regards to series.name...\nMade it:\n- when referenceSeries is not specified:\n  transformNulls(server.1.status.202,0)\n- when referenceSeries is specified:\n  transformNulls(server.1.status.202,0,referenceSeries)\ni.e. The word \"referenceSeries\" is printed.\nThis could be changed to something like:\npython\nseries.name = \"transformNull(%s,%g,[%s])\" % (series.name, default, ','.join(x.name for x in keySeries))\nwhich would output:\ntransformNulls(server.1.status.202,0,[server.1.status.200,server.1.status.202,server.1.status.503,server.1.status.301])\nBut it's pretty verbose and I don't know how useful this is.. What do you think? @obfuscurity\n. Here you go:\nTraceback (most recent call last):\n  File \"/opt/graphite/lib/python2.7/site-packages/django/core/handlers/base.py\", line 112, in get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 113, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 8, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 29, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression, replacements)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 59, in evaluateTokens\n    return func(requestContext, *args, **kwargs)\n  File \"/opt/graphite/webapp/graphite/render/functions.py\", line 1536, in aliasByNode\n    series.name = '.'.join(metric_pieces[n] for n in nodes)\n  File \"/opt/graphite/webapp/graphite/render/functions.py\", line 1536, in <genexpr>\n    series.name = '.'.join(metric_pieces[n] for n in nodes)\nIndexError: list index out of range\nI have couple of graphs that do this and I think this one is one of them:\naliasByNode(sortByTotal(maximumAbove(transformNull(sumSeriesWithWildcards(foo.error_rate.*.error_id.{1,3,9}.point, 4), 0, aggre.error_rate.*.zero), 25)), 2)\n. That seems to be yes. \n(My code base comes from pip install git+https://github.com/graphite-project/graphite-web.git that I did earlier in the day, but looking at webapp/graphite/readers.py and webapp/graphite/readers.py shows both changes applied.)\n. I did a pip install git+https://github.com/graphite-project/graphite-web.git --upgrade just in case and retested, and I can definitely say it's reproducible with the current master at 1f56f5156aa688c705f0cb60f420a6beddb6cdab.\n. This graph is affected:\naliasByNode(sortByTotal(maximumAbove(transformNull(sumSeriesWithWildcards(foo.error_rate.*.error_id.{1,3,9}.point, 4), 0, aggre.error_rate.*.zero), 25)), 2)\nMetrics are:\nfoo.error_rate.OP.error_id.ERROR_IDS.point\nfoo.error_rate.OP.zero\nWhere OP is one of 2248 identifiers, and ERROR_IDS is 6.5 identifiers on averages (max 11).\n. I have a feeling a picture would be just as unhelpful but here goes:\n\n. > Where OP is one of 2248 identifiers, and ERROR_IDS is 6.5 identifiers on averages (max 11).\nThis works out to 14612 metrics times data points of (10m:30d) over 8 hours per render.  (But it's actually another 14612 more because I'm doing something inefficient with foo.error_rate.OP.zero.  (It's inefficient because I don't actually need a wildcard here I just realized).\n. > Could you dump the infos logs ? This should tell us more about what's happening. (With my fix I was definitely doing only on request per replica instead of one per metric thanks to the bulk_query batching)\n@iksaif I'm sorry I don't have a sandbox to get this.  I'm at work and the server I'm working on gets hit by requests by as soon as I restart uwsgi, making it impossible to separate out lines.\n\nWith my fix I still had a ~30% penalty, which was better than the massive slowdown that I got without.\n\nThis 30% hit is likely what I'm experiencing.\n. > similar to what @iksaif did in the comment I linked earlier\nRight, this looks doable looking at uwsgi.log..\n. Okay, I thought about either familiarizing myself with collectd or grokking cache.log manually but I wanted to get some sleep so here's one I took with cat storage/log/webapp/uwsgi.log | cut -d' ' -f16,25 and plotted in gasp Excel.\n\n- At 1:30, REMOTE_STORE_MERGE_RESULTS is False\n- At 1:40, REMOTE_STORE_MERGE_RESULTS is commented out and uwsgi is restarted\n- At 1:50, REMOTE_STORE_MERGE_RESULTS is set back to False and uwsgi is restarted\nI didn't intentionally make any requests to Graphite during this time.  The plotted render times are purely the result of number of Grafana dashboards open and refreshing automatically every 30 seconds or so.\n. For whatever it's worth:\n- @iksaif's fix appears to be about increasing parallelism, and I'm not sure how this can help for local fetches.\n  - On the other hand, could it possibly decrease performance for local fetches?  (I'm mainly concerned about the comment in the commit message about not benefiting from reusing queries anymore.)\n- Is this condition missing a if not local the way it's done here and here?\n. @iksaif \n``` diff\n--- webapp/graphite/local_settings.py.example   2016-09-02 22:46:10.000000000 +0900\n+++ webapp/graphite/local_settings.py   2016-09-03 01:50:57.070934105 +0900\n@@ -11,7 +11,7 @@\n # install. This key is used for salting of hashes used in auth tokens,\n # CRSF middleware, cookie storage, etc. This should be set identically among\n # instances if used behind a load balancer.\n-#SECRET_KEY = 'UNSAFE_DEFAULT'\n+SECRET_KEY = ...\n# In Django 1.5+ set this to the list of hosts your graphite instances is\n # accessible as. See:\n@@ -21,7 +21,7 @@\n # Set your local timezone (Django's default is America/Chicago)\n # If your graphs appear to be offset by a couple hours then this probably\n # needs to be explicitly set to your local timezone.\n-#TIME_ZONE = 'America/Los_Angeles'\n+TIME_ZONE = 'Japan'\n# Set the default short date format. See strftime(3) for supported sequences.\n #DATE_FORMAT = '%m/%d'\n@@ -264,6 +264,9 @@\n # that choosing the \"most complete\" one (pre-0.9.14 behaviour).\n #REMOTE_STORE_MERGE_RESULTS = True\n+# 2016-09-02: this destroys performance\n+REMOTE_STORE_MERGE_RESULTS = False\n+\n ## Remote rendering settings\n # Set to True to enable rendering of Graphs on a remote webapp\n #REMOTE_RENDERING = True\n```\negrep 'CARBONLINK|REMOTE|CLUSTER' webapp/graphite/local_settings.py:\n```\nREMOTE_USER authentication. See: https://docs.djangoproject.com/en/dev/howto/auth-remote-user/\nUSE_REMOTE_USER_AUTHENTICATION = True\nCLUSTER_SERVERS = [\"10.0.2.2:80\", \"10.0.2.3:80\"]\nINTRACLUSTER_HTTPS = False\nREMOTE_FIND_TIMEOUT = 3.0           # Timeout for metric find requests\nREMOTE_FETCH_TIMEOUT = 3.0          # Timeout to fetch series data\nREMOTE_RETRY_DELAY = 60.0           # Time before retrying a failed remote webapp\nREMOTE_EXCLUDE_LOCAL = False\nREMOTE_STORE_MERGE_RESULTS = True\nREMOTE_STORE_MERGE_RESULTS = False\nREMOTE_RENDERING = True\naccess to metric data or should have CLUSTER_SERVERS configured\nREMOTE_RENDER_CONNECT_TIMEOUT = 1.0\nCARBONLINK_HOSTS = [\"127.0.0.1:7002:a\", \"127.0.0.1:7102:b\", \"127.0.0.1:7202:c\"]\nCARBONLINK_TIMEOUT = 1.0\nCARBONLINK_RETRY_DELAY = 15 # Seconds to blacklist a failed remote server\nCARBONLINK_HASHING_KEYFUNC = \"/opt/graphite/bin/keyfuncs.py:my_keyfunc\"\n```\n. > - On the other hand, could it possibly decrease performance for local fetches?  (I'm mainly concerned about the comment in the commit message about not benefiting from reusing queries anymore.)\nRegarding this, I tested it and found that it doesn't make the slightest difference.  (i.e. It's a good thing.)\nThe test I carried out is similar to before except runs where compared with changes to readers.py applied and not applied.  REMOTE_STORE_MERGE_RESULTS = False in both cases.\nHere's the uneventful graph:  (Changes in a87035629b3d1fb5a95e655e42265fbcf1dd3f56 to readers.py reverted and uwsgi restarted at 20:05)\n\n. > I'm suspecting that fix should be simple as deniszh@7f3c01b, testing it now.\nJust a question, what happens if you're using clusters but also using local?  (Or isn't this a possible setup?)  Wouldn't \"we need all nodes for merging results\" apply in that case?\n. > @atnak - could you please test my fix in PR #1674 ? I tested it only on synthesize - unfortunately I had no production installation on master yet.\n@deniszh I'm afraid it wasn't fixed.  It looks like local is coming here False.\n\n- At 01:11: Patch applied + REMOTE_STORE_MERGE_RESULTS = False commented out [1]\n- At 01:19: Patch removed + REMOTE_STORE_MERGE_RESULTS = False commented out [2]\n- At 01:22: Patch removed + REMOTE_STORE_MERGE_RESULTS = False reapplied\n- At 01:31: Patch applied + REMOTE_STORE_MERGE_RESULTS = True commented out [1]\n- At 01:40: Patch removed + REMOTE_STORE_MERGE_RESULTS = True commented out [2]\n- At 01:53: Patch removed + REMOTE_STORE_MERGE_RESULTS = False reapplied\n[1]\n``` diff\ndiff --git a/webapp/graphite/local_settings.py b/webapp/graphite/local_settings.py\n--- a/webapp/graphite/local_settings.py\n+++ b/webapp/graphite/local_settings.py\n@@ -265,7 +265,7 @@ TIME_ZONE = 'Japan'\n #REMOTE_STORE_MERGE_RESULTS = True\n# 2016-09-02: this destroys performance\n-REMOTE_STORE_MERGE_RESULTS = False\n+#REMOTE_STORE_MERGE_RESULTS = False\n## Remote rendering settings\n # Set to True to enable rendering of Graphs on a remote webapp\ndiff --git a/webapp/graphite/storage.py b/webapp/graphite/storage.py\n--- a/webapp/graphite/storage.py\n+++ b/webapp/graphite/storage.py\n@@ -80,7 +80,7 @@ class Store:\n       if not leaf_nodes:\n         continue\n\nif settings.REMOTE_STORE_MERGE_RESULTS:\nif settings.REMOTE_STORE_MERGE_RESULTS and not local:\n         # we need all nodes for merging results\n         minimal_node_set = leaf_nodes\n       else:\n```\n\n[2]\n``` diff\ndiff --git a/webapp/graphite/local_settings.py b/webapp/graphite/local_settings.py\n--- a/webapp/graphite/local_settings.py\n+++ b/webapp/graphite/local_settings.py\n@@ -265,7 +265,7 @@ TIME_ZONE = 'Japan'\n #REMOTE_STORE_MERGE_RESULTS = True\n# 2016-09-02: this destroys performance\n-REMOTE_STORE_MERGE_RESULTS = False\n+#REMOTE_STORE_MERGE_RESULTS = False\n## Remote rendering settings\n # Set to True to enable rendering of Graphs on a remote webapp\n``\n. Ahh, thatlocalparameter [is actually [1]](https://github.com/graphite-project/graphite-web/blob/6063737c0946db420f580101d8000842770646fa/webapp/graphite/render/datalib.py#L115) this [localOnly` [2]](https://github.com/graphite-project/graphite-web/blob/c8623f73d46702a81fc3524c0ef50110f2c928ba/docs/render_api.rst#localonly) value and hence doesn't represent whether nodes are going to be local or remote.  Confusing naming I guess.\n. +1 I think the latest code is good for local queries.\n\nPlease check the latest code - if it works I will squash and merge, mentioning you as an author.\nOr you can create own PR and I'll ditch this one out.\n\n@deniszh I don't know enough about what REMOTE_STORE_MERGE_RESULTS is doing to follow this through so I prefer it if you authored it.\n. Although I think this PR is good as it is, I'm wondering if there might be a follow up problem that can be addressed in a subsequent PR...\nSince we found that local nodes have redundant intervals, could remote nodes be the same?\nIn that case, it might be required to reduce nodes to the best measure_of_added_coverage(node, False) per path per remote host.\n. Had me worried for a moment there.  Thanks @deniszh for merging and @iksaif for looking over our fix!\n. - [x] wip: pending testing for verboseness\n- [x] wip: does this break aliasByNode?\n. I should point out you're doing this inside the for path, nodes loop started here but these remote_nodes don't look to be filtered by path.\nThere's something I want to check up on...  If we're assuming this branching is the cause of slow down, then we're also assuming there are overlaps in coverage with purely local nodes and that this was working to reduce that overlaps.\nHowever, if there aren't overlaps with purely local nodes, then the whole assumption of cause was incorrect and here and here in datalib.py were most likely the real culprits.\nThis is easy enough to test, as soon as I finish writing this commit.\nMight I suggest for storage.py:\nSomething like this slotted into line 114 instead:\npython\nif settings.REMOTE_STORE_MERGE_RESULTS:\n    remote_nodes = [n for n in nodes_remaining if not n.local]\n    for node in remote_nodes:\n        nodes_remaining.remove(node)\n        minimal_node_set.add(node)\n        covered_intervals = covered_intervals.union(node.intervals)\nIf however the \"there are no overlaps in coverage with purely local nodes\" holds true, then your original method actually looks sound and probably didn't need to be changed.  It could even be one-upped to if not remote_requests or settings.REMOTE_STORE_MERGE_RESULTS.\n. Okay, the results are in:\n- My comment about datalib.py can be ignored.  storage.py is the culprit alright.\n- There are apparently overlaps with purely local nodes and this code is helping to reduce that.\n- I've included the tests for a possible fix as the last item.\nNo slow down:\n``` diff\ndiff --git a/webapp/graphite/local_settings.py b/webapp/graphite/local_settings.py\n--- a/webapp/graphite/local_settings.py\n+++ b/webapp/graphite/local_settings.py\n@@ -265,7 +265,7 @@ TIME_ZONE = 'Japan'\n #REMOTE_STORE_MERGE_RESULTS = True\n# 2016-09-02: this destroys performance\n-REMOTE_STORE_MERGE_RESULTS = False\n+REMOTE_STORE_MERGE_RESULTS = True\ndiff --git a/webapp/graphite/storage.py b/webapp/graphite/storage.py\n--- a/webapp/graphite/storage.py\n+++ b/webapp/graphite/storage.py\n@@ -80,7 +80,7 @@ class Store:\n       if not leaf_nodes:\n         continue\n\nif settings.REMOTE_STORE_MERGE_RESULTS:\nif False:\n         # we need all nodes for merging results\n         minimal_node_set = leaf_nodes\n```\n\nSlowed down:\n``` diff\ndiff --git a/webapp/graphite/local_settings.py b/webapp/graphite/local_settings.py\n--- a/webapp/graphite/local_settings.py\n+++ b/webapp/graphite/local_settings.py\n@@ -265,7 +265,7 @@ TIME_ZONE = 'Japan'\n #REMOTE_STORE_MERGE_RESULTS = True\n# 2016-09-02: this destroys performance\n-REMOTE_STORE_MERGE_RESULTS = False\n+REMOTE_STORE_MERGE_RESULTS = True\ndiff --git a/webapp/graphite/storage.py b/webapp/graphite/storage.py\n--- a/webapp/graphite/storage.py\n+++ b/webapp/graphite/storage.py\n@@ -80,7 +80,7 @@ class Store:\n       if not leaf_nodes:\n         continue\n\nif settings.REMOTE_STORE_MERGE_RESULTS:\nif True:\n         # we need all nodes for merging results\n         minimal_node_set = leaf_nodes\n```\n\nSlowed down:\n``` diff\ndiff --git a/webapp/graphite/local_settings.py b/webapp/graphite/local_settings.py\n--- a/webapp/graphite/local_settings.py\n+++ b/webapp/graphite/local_settings.py\n@@ -266,5 +266,5 @@ TIME_ZONE = 'Japan'\n# 2016-09-02: this destroys performance\n-REMOTE_STORE_MERGE_RESULTS = False\n+REMOTE_STORE_MERGE_RESULTS = True\n## Remote rendering settings\ndiff --git a/webapp/graphite/storage.py b/webapp/graphite/storage.py\n--- a/webapp/graphite/storage.py\n+++ b/webapp/graphite/storage.py\n@@ -81,7 +81,7 @@ class Store:\n         continue\n\nif settings.REMOTE_STORE_MERGE_RESULTS:\nif True:\n         # we need all nodes for merging results\nminimal_node_set = leaf_nodes\nminimal_node_set = set(leaf_nodes)\n       else:\n         # Calculate best minimal node set\n```\n\nNo slow down: yay!\n``` diff\ndiff --git a/webapp/graphite/local_settings.py b/webapp/graphite/local_settings.py\n--- a/webapp/graphite/local_settings.py\n+++ b/webapp/graphite/local_settings.py\n@@ -265,7 +265,7 @@ TIME_ZONE = 'Japan'\n #REMOTE_STORE_MERGE_RESULTS = True\n# 2016-09-02: this destroys performance\n-REMOTE_STORE_MERGE_RESULTS = False\n+REMOTE_STORE_MERGE_RESULTS = True\n## Remote rendering settings\n # Set to True to enable rendering of Graphs on a remote webapp\ndiff --git a/webapp/graphite/storage.py b/webapp/graphite/storage.py\n--- a/webapp/graphite/storage.py\n+++ b/webapp/graphite/storage.py\n@@ -80,7 +80,7 @@ class Store:\n       if not leaf_nodes:\n         continue\n\nif settings.REMOTE_STORE_MERGE_RESULTS:\n\nif False:\n         # we need all nodes for merging results\n         minimal_node_set = leaf_nodes\n       else:\n@@ -112,16 +112,23 @@ class Store:\n             minimal_node_set.add(node)\n             covered_intervals = covered_intervals.union(node.intervals)\n\n\nwhile nodes_remaining:\n\nnode_coverages = [ (measure_of_added_coverage(n), n) for n in nodes_remaining ]\n\nbest_coverage, best_node = max(node_coverages)\n\nif best_coverage == 0:\n\nbreak\n\nnodes_remaining.remove(best_node)\nminimal_node_set.add(best_node)\ncovered_intervals = covered_intervals.union(best_node.intervals)\nif settings.REMOTE_STORE_MERGE_RESULTS:\nremote_nodes = [n for n in nodes_remaining if not n.local]\nfor node in remote_nodes:\nnodes_remaining.remove(node)\nminimal_node_set.add(node)\ncovered_intervals = covered_intervals.union(node.intervals)\nelse:\nwhile nodes_remaining:\nnode_coverages = [ (measure_of_added_coverage(n), n) for n in nodes_remaining ]\nbest_coverage, best_node = max(node_coverages)\n+\nif best_coverage == 0:\nbreak\n+\nnodes_remaining.remove(best_node)\nminimal_node_set.add(best_node)\ncovered_intervals = covered_intervals.union(best_node.intervals) # Sometimes the requested interval falls within the caching window.\n # We include the most likely node if the gap is within tolerance.\n\n```\n. \n\n",
    "HeMan": "I've updated the documentation somewhat. Should I add a short note in local_setting.py.example as well?\n. Like that?\n. There!\n. ",
    "sylr": "It seems that it's the \"nodes.sort()\" at the ends of the func that messes things up.\n. @obfuscurity If you think it would be more coherent I'm totally fine with it.\nI however am not very familiar with graphite code so I don't really know what needs to be done to extend sortByName().\n. OK thanks I'll provide another patch soon.\n. Any comments ?\n. Fixed.\n. It seems that braceexpand does that:\nhttps://pypi.python.org/pypi/braceexpand/0.1.1\n. It should be \"Natural\" instead of \"Normal\".\n. ",
    "breml": "Today I was missing this function as well. It would be nice to have a rounding function for timeseries.\n. ",
    "hau21um": "Would help to have round and trim functions..\n. ",
    "cout": "I believe I have a solution.  Inside wait_for_results(), I think it is expected that the first thread to acquire request_lock is also the first thread to acquire wait_lock.\nChanging request_lock to be an RLock instead of a Lock allows wait_for_results() to use request_lock instead of wait_lock (and wait_lock can be removed).  This ensures that the thread which initiated the http request is also the one which checks for the response; the remaining thread wait on the completion event.\n. ",
    "redbaron": "@cout , @wooparadog thanks for researching and providing a patch. I tested it with my current setup and seems to be working fine now.\n. new feedback: it solved exceptions mentioned in the report, but installing patch on just 1 server crippled whole cluster  when memcache TTL start to expire. \nwait_for_results started to fail with socket timeouts massively on the server with patch (rest were left unpatched):\nFile \"/opt/graphite-web/webapp/graphite/remote_storage.py\", line 185, in wait_for_results\n    response = self.connection.getresponse()\n  File \"/opt/rh/python27/root/usr/lib64/python2.7/httplib.py\", line 1073, in getresponse\n    response.begin()\n  File \"/opt/rh/python27/root/usr/lib64/python2.7/httplib.py\", line 415, in begin\n    version, status, reason = self._read_status()\n  File \"/opt/rh/python27/root/usr/lib64/python2.7/httplib.py\", line 371, in _read_status\n    line = self.fp.readline(_MAXLINE + 1)\n  File \"/opt/rh/python27/root/usr/lib64/python2.7/socket.py\", line 476, in readline\n    data = self._sock.recv(self._rbufsize)\n. I still dont like how RemoteReader.request_cache is used, it can potentially return really stale results should server have enough workers and settings.REMOTE_READER_CACHE_SIZE_LIMIT is not reached for a long time\n. Run it overnight, even though it fixed problem reported in this issue,there are still occasinional failures of passive writers without corresponding failure in the main fetcher:\nThu Jun 30 08:50:32 2016 :: Failed to complete subfetch\nTraceback (most recent call last):\n  File \"/opt/graphite-web/webapp/graphite/readers.py\", line 52, in fetch\n    results[i] = result.waitForResults()\n  File \"/opt/graphite-web/webapp/graphite/readers.py\", line 29, in waitForResults\n    return self.wait_callback()\n  File \"/opt/graphite-web/webapp/graphite/remote_storage.py\", line 219, in extract_my_results\n    for series in wait_for_results():\n  File \"/opt/graphite-web/webapp/graphite/remote_storage.py\", line 216, in wait_for_results\n    raise Exception(\"Passive remote fetch failed to find cached results\")\nException: Passive remote fetch failed to find cached results\nI run single threaded processes (mod_wsgi threads=1) so it is not a race between threads. Only way I see it to be happening is that  within same thread RemoteReader.fetch are called not in the same order as corresponding FetchInProgress.waitForResult , this would cause passive checks to fail with no error from  RemoteReader which initiated a request.\nLooking at the  code I am puzzled how it can happen:\n``` python\nclass MultiReader:\n....\n  def fetch(self, startTime, endTime):\n    # Start the fetch on each node\n    results = [ n.fetch(startTime, endTime) for n in self.nodes ]\n# Wait for any asynchronous operations to complete\nfor i, result in enumerate(results):\n  if isinstance(result, FetchInProgress):\n    try:\n      results[i] = result.waitForResults()\n    except:\n      log.exception(\"Failed to complete subfetch\")\n      results[i] = None\n\n...\n```\nany ideas?\n. This is a total mess. Single threaded graphite keeps calling wait_for_result on passive readers first, which obviously never going to succeed, for whole that RemoteReader thing to work, order of calls to RemoteReader.fetch must match order of calls to wait_for_result, at least first call in both sequences must be the first one, if it isn't it all deadlocks for FETCH_TIMEOUT.\nI have a suspicion active call might never be called at all in some cases. \nNeeds to be reworked ,so that it doesn't depend on order of calls\n. It has all the right to be so :) Fixed missing url var and unused Event import\n. There is slight regression,  but I'd say it because instead of failing on a buggy code it actually fetches and processes data now.\nUnpatched code, cache disabled, histogram for 3 minutes:\n```\ncat /var/log/httpd24/graphite-web-access.log | grep '/render' |grep -v 'local' | awk '/26\\/Jun\\/2016:10:4[5-7]/ {print $11}'  |histogram.py  -m 1.0 -x 5000 -b 10 -p\nNumSamples = 288; Min = 1.00; Max = 5000.00\nMean = 89.565972; Variance = 112033.259537; SD = 334.713698; Median 3.000000\neach \u220e represents a count of 3\n1.0000 -   500.9000 [   276]: \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e (95.83%)\n\n500.9000 -  1000.8000 [     7]: \u220e\u220e (2.43%)\n 1000.8000 -  1500.7000 [     2]:  (0.69%)\n 1500.7000 -  2000.6000 [     0]:  (0.00%)\n 2000.6000 -  2500.5000 [     1]:  (0.35%)\n 2500.5000 -  3000.4000 [     1]:  (0.35%)\n 3000.4000 -  3500.3000 [     1]:  (0.35%)\n 3500.3000 -  4000.2000 [     0]:  (0.00%)\n 4000.2000 -  4500.1000 [     0]:  (0.00%)\n 4500.1000 -  5000.0000 [     0]:  (0.00%)\n```\nWith this PR applied, cache disabled:\n```\ncat /var/log/httpd24/graphite-web-access.log | grep '/render' |grep -v 'local' | awk '/26\\/Jun\\/2016:10:1[3-5]/ {print $11}'  |histogram.py  -m 1.0 -x 5000 -b 10 -p\nNumSamples = 365; Min = 1.00; Max = 5000.00\n1 value outside of min/max\nMean = 233.926027; Variance = 422460.643843; SD = 649.969725; Median 7.000000\neach \u220e represents a count of 4\n1.0000 -   500.9000 [   320]: \u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e\u220e (87.67%)\n\n500.9000 -  1000.8000 [    22]: \u220e\u220e\u220e\u220e\u220e (6.03%)\n 1000.8000 -  1500.7000 [    11]: \u220e\u220e (3.01%)\n 1500.7000 -  2000.6000 [     5]: \u220e (1.37%)\n 2000.6000 -  2500.5000 [     1]:  (0.27%)\n 2500.5000 -  3000.4000 [     2]:  (0.55%)\n 3000.4000 -  3500.3000 [     0]:  (0.00%)\n 3500.3000 -  4000.2000 [     1]:  (0.27%)\n 4000.2000 -  4500.1000 [     1]:  (0.27%)\n 4500.1000 -  5000.0000 [     1]:  (0.27%\n```\n. > I assume you want to close this one in favor of #1680?\nNo, they should be both in, but they fix different things, hence 2 PRs\n. Thats fine, whichever is merged first, I rebase another\n. done\n. How do you run graphite? You have to make sure, that pool of workers graphite talks to is different from pool of workers browsers/end clients talk to. Otherwise extensive wait times and deadlocks possible\n. unfortunately it is mandatory. Simple example:\n2 nodes 1 worker thread each, each is configured to talk to same port when making graphite -> graphite requests.\n2 browsers requests come in, each Graphite-web tries to contact another graphite and these requests are waiting while thread becomes free and ready to serve request. Everything just deadlocks until timeouts start to kick in and when they do both requests return error.\nIt doesn't really matter how many threads you run, as long as there is enough activity from browsers/clients and given that each request there causes 2*(N-1) requests to other graphite servers (first to find metric and next to fetch it), it's not long until deadlocks like described above start happening.\n. we had deadlocks all over, whole cluster was coming down to a halt until I separated pools\n. My setup:\n8  nodes,  12M metrics/minute received with replication factor of 2, so 24M /minute persisted, graphite is run with apache + mod_wsgi, but I can't see how twisted will be different here.\nthere are not that many requests coming from browsers: 1200-1500 / minute, but graphs are quite big, with json response sizes being as high as 500KB (10% are >225KB).\nI  configure apache with 2 WSGI process groups on ports 8000 and 8010, both pointing to the same graphite install with same with CLUSTER_SERVERS in local_settings.py pointing to port 8010. Browsers/clients go to port 8000 only.\nThis way workers on port 8000 always waiting for workers on ports 8010 and never other way aroud. It eliminated lockups and improved responsiveness (although I didn't measure response time effect, so can't prove it with numbers)\n. yes, thats a tuple of 1 element, but still iterable\n. ",
    "rajatwy": "I have upgraded my cluster to  0.9.x and the issue has disappeared.\nThanks @obfuscurity!\n. Facing this issue again. I have seen two false alerts (got empty data from graphite for both).\nLater when I queried for the same time period, it gives me non-empty data.\n. ",
    "ddlatham": "I'm sorry I didn't explain that well as it was misunderstood.  That was purely an example.  Currently it appears any values that appear to begin with the same letters or prefixes as the supported time units.  This could cause confusion, accidentally or maliciously.  \nfrom=-2whales&until=-1deadlydragons\n. ",
    "avneradania": "Thanks for quick response, obfuscurity.\nI found the problem. In 6.5 there was no issues, but after upgrading to 6.6, there is fonts related RPM missing, dejavu-sans-fonts.\nWhen i changed font to courier, i saw legends and values.\nI installed dejavu-sans-fonts and graphs look great with sans(font).\nThanks,\nAvner\n. ",
    "yzehani": "I was able to get through this by changing STATICFILES_DIRS in settings.py\nReplaced:\nSTATICFILES_DIRS = (\n   join(WEBAPP_DIR, 'content'),\n)\nby:\nSTATICFILES_DIRS = (\n        join(\"/opt/data/graphite/webapp\", 'content'),\n)\nThe problem comes apparently from hard coding the value of WEBAPP_DIR in:\n```\nFilesystem layout\nWEB_DIR = dirname( abspath(file) )\nWEBAPP_DIR = dirname(WEB_DIR)\nGRAPHITE_ROOT = dirname(WEBAPP_DIR)\n```\nAn easy workable solution could be to be able to set WEBAPP_DIR in local_settings.py\nsettings.py should change to pickup that setting.\n. Yes, I'm using master.\nlocal_settings.py doesn't have any reference to WEBAPP_DIR\nand even if so, it would be clobbered by settings.by\nhttps://github.com/graphite-project/graphite-web/blob/master/webapp/graphite/local_settings.py.example\n. Ok, then I suggest adding a commented line defining WEBAPP_DIR right under #STATIC_ROOT\nin local_settings.py.example to address that, since it's not intuitive and one would assume that the default will be derived from the default storage directory.\nI think we can close the issue if this change is added.\nThanks for your support.\n. ",
    "addshore": "I would love this to magically make its way into 9.15!!!\nIt would be a great new years present for me :D\n. \nMany thanks!\n. ",
    "miguelxpn": "Turns out I had actually missed something, I changed the code and added support for bracket notation so that the function works with ipv6 addresses with a port. Hopefully that's it now.\n. Not really, if host is already a valid ipv6 address then we don't need to manipulate it further, if it's not then socket.error will be raised and we'll split the port out of it and then everything will proceed as it did before, should work unless there's something I'm missing.\n. ",
    "floppy84": "Hi thank you for your answer, i got the same issue if i set timeshift in minute ::\n\n. I don't need to mgrate data, i will set my server in UTC , and next time change in summer i will not have same issue.\n Thanks for your help\n. ",
    "peterstory": "Would it be possible to add another argument to timeShift, toggling whether DST-compensation is applied? It seems like a common use case: using timeShift(-1w) to compare metric values at 7am this week to values at 7am last week... not values at 7am to values at 6am. \nI've written an application which analyzes trends in metrics, week-over-week (using an arbitrary number of historic weeks). I expect to write some gnarly work-around code for this, but it would be nice if timeShift could save others from the issue! \n. I'm thinking it would be a shortcut for +/- 1 hour to the time shift. It would only take affect if the from and until times were both in DST, and the timeShift-ed time was outside DST (and vice-versa). \nExample: a graph showing a metric from 8am-8pm, on November 1st. Currently, if you add a 1 day/week timeShift-ed version of the metric, it will be actually be showing 9am-7pm. \nmy.great.metric.count\ntimeShift(my.great.metric.count,\"-1w\")\nBut with the DST-compensation, both graphs would show from 8am-8pm. \nmy.great.metric.count\ntimeShift(my.great.metric.count,\"-1w\",dst=true)\nequivalent to \ntimeShift(my.great.metric.count,\"-169h\")   # 168 hours in a week\n. I'm working on a patch for myself, though if you're interested in integrating it into the main project, that would be great, too. Unfortunately, my changes aren't having the desired effect: the if-conditional is always evaluating to true. Any thoughts? \n``` diff\ndiff --git a/webapp/graphite/render/functions.py b/webapp/graphite/render/functions.py\nindex 1908273..86dad6f 100644\n--- a/webapp/graphite/render/functions.py\n+++ b/webapp/graphite/render/functions.py\n@@ -2465,7 +2465,7 @@ def timeStack(requestContext, seriesList, timeShiftUnit, timeShiftStart, timeShi\nreturn results\n-def timeShift(requestContext, seriesList, timeShift, resetEnd=True):\n+def timeShift(requestContext, seriesList, timeShift, resetEnd=True, dstTweak=True):\n   \"\"\"\n   Takes one metric or a wildcard seriesList, followed by a quoted string with the\n   length of time (See from / until in the render_api_ for examples of time formats).\n@@ -2499,6 +2499,20 @@ def timeShift(requestContext, seriesList, timeShift, resetEnd=True):\n   myContext = requestContext.copy()\n   myContext['startTime'] = requestContext['startTime'] + delta\n   myContext['endTime'] = requestContext['endTime'] + delta\n+\n+  dstOffset = timedelta(hours=0)\n+  if dstTweak:\n+    # If the requestContext is entirely in DST, and the timeshifted range is\n+    # entirely NOT in DST\n+    if ((requestContext['startTime'].dst() and requestContext['endTime'].dst()) and (not myContext['startTime'].dst() and not myContext['endTime'].dst())):\n+        dstOffset = timedelta(hours=-1)\n+    # Or if the requestContext is entirely NOT in DST, and the timeshifted\n+    # range is entirely in DST\n+    elif ((not requestContext['startTime'].dst() and not requestContext['endTime'].dst()) and (myContext['startTime'].dst() and myContext['endTime'].dst())):\n+        dstOffset = timedelta(hours=1)\n+    myContext['startTime'] += dstOffset\n+    myContext['endTime']   += dstOffset\n+\n   results = []\n   if len(seriesList) > 0:\n     # if len(seriesList) > 1, they will all have the same pathExpression, which is all we care about.\n``\n. Looks like the.dst()` method wasn't doing what I thought. Instead, I need to use something like this: \npython\nnow = requestContext['startTime']\ntime.localtime(time.mktime(now.timetuple())).tm_isdst\nHowever, that's not very readable. Is there a better (and probably safer) way to check whether DST was in effect at a given datetime?\n. I did some digging, and I guess the hack described above is the only way to do this, since it basically is a hack (since we're applying our local time zone info to something that originally didn't have time zone info)! \nAnyway, here is a final, working version: \n``` diff\ndiff --git a/webapp/graphite/render/functions.py b/webapp/graphite/render/functions.py\nindex 1908273..c3891ff 100644\n--- a/webapp/graphite/render/functions.py\n+++ b/webapp/graphite/render/functions.py\n@@ -2465,7 +2465,7 @@ def timeStack(requestContext, seriesList, timeShiftUnit, timeShiftStart, timeShi\nreturn results\n-def timeShift(requestContext, seriesList, timeShift, resetEnd=True):\n+def timeShift(requestContext, seriesList, timeShift, resetEnd=True, dstTweak=True):\n   \"\"\"\n   Takes one metric or a wildcard seriesList, followed by a quoted string with the\n   length of time (See from / until in the render_api_ for examples of time formats).\n@@ -2499,6 +2499,27 @@ def timeShift(requestContext, seriesList, timeShift, resetEnd=True):\n   myContext = requestContext.copy()\n   myContext['startTime'] = requestContext['startTime'] + delta\n   myContext['endTime'] = requestContext['endTime'] + delta\n+\n+  def localDST(dt):\n+    return time.localtime(time.mktime(dt.timetuple())).tm_isdst\n+\n+  reqStartDST = localDST(requestContext['startTime'])\n+  reqEndDST   = localDST(requestContext['endTime'])\n+  myStartDST  = localDST(myContext['startTime'])\n+  myEndDST    = localDST(myContext['endTime'])\n+\n+  dstOffset = timedelta(hours=0)\n+  if dstTweak:\n+    # If the requestContext is entirely in DST, and we are entirely NOT in DST\n+    if ((reqStartDST and reqEndDST) and (not myStartDST and not myEndDST)):\n+        dstOffset = timedelta(hours=1)\n+    # Or if the requestContext is entirely NOT in DST, and we are entirely in DST\n+    elif ((not reqStartDST and not reqEndDST) and (myStartDST and myEndDST)):\n+        dstOffset = timedelta(hours=-1)\n+    # Otherwise, we don't do anything, because it would be visually confusing\n+    myContext['startTime'] += dstOffset\n+    myContext['endTime'] += dstOffset\n+\n   results = []\n   if len(seriesList) > 0:\n     # if len(seriesList) > 1, they will all have the same pathExpression, which is all we care about.\n```\n. Good idea! Yes, it is funky, potentially unexpected behavior, so it does make sense to leave it disabled by default. If you'd like to integrate this into the project, just let me know, since I made my change in a local Git repo. \nA revised diff:\n``` diff\ndiff --git a/webapp/graphite/render/functions.py b/webapp/graphite/render/functions.py\nindex 1908273..d66ad72 100644\n--- a/webapp/graphite/render/functions.py\n+++ b/webapp/graphite/render/functions.py\n@@ -2465,7 +2465,7 @@ def timeStack(requestContext, seriesList, timeShiftUnit, timeShiftStart, timeShi\nreturn results\n-def timeShift(requestContext, seriesList, timeShift, resetEnd=True):\n+def timeShift(requestContext, seriesList, timeShift, resetEnd=True, alignDST=False):\n   \"\"\"\n   Takes one metric or a wildcard seriesList, followed by a quoted string with the\n   length of time (See from / until in the render_api_ for examples of time formats).\n@@ -2480,6 +2480,10 @@ def timeShift(requestContext, seriesList, timeShift, resetEnd=True):\n   ending at the current time. If resetEnd is False, will instead draw full range including\n   future time.\n\nBecause time is shifted by a fixed number of seconds, comparing a time period with DST to\na time period without DST, and vice-versa, will result in an apparent misalignment. For\nexample, 8am might be overlaid with 7am. To compensate for this, use the alignDST option.\n+\n   Useful for comparing a metric against itself at a past periods or correcting data\n   stored at an offset.\n\n@@ -2499,6 +2503,27 @@ def timeShift(requestContext, seriesList, timeShift, resetEnd=True):\n   myContext = requestContext.copy()\n   myContext['startTime'] = requestContext['startTime'] + delta\n   myContext['endTime'] = requestContext['endTime'] + delta\n+\n+  if alignDST:\n+    def localDST(dt):\n+      return time.localtime(time.mktime(dt.timetuple())).tm_isdst\n+\n+    reqStartDST = localDST(requestContext['startTime'])\n+    reqEndDST   = localDST(requestContext['endTime'])\n+    myStartDST  = localDST(myContext['startTime'])\n+    myEndDST    = localDST(myContext['endTime'])\n+\n+    dstOffset = timedelta(hours=0)\n+    # If the requestContext is entirely in DST, and we are entirely NOT in DST\n+    if ((reqStartDST and reqEndDST) and (not myStartDST and not myEndDST)):\n+        dstOffset = timedelta(hours=1)\n+    # Or if the requestContext is entirely NOT in DST, and we are entirely in DST\n+    elif ((not reqStartDST and not reqEndDST) and (myStartDST and myEndDST)):\n+        dstOffset = timedelta(hours=-1)\n+    # Otherwise, we don't do anything, because it would be visually confusing\n+    myContext['startTime'] += dstOffset\n+    myContext['endTime'] += dstOffset\n+\n   results = []\n   if len(seriesList) > 0:\n     # if len(seriesList) > 1, they will all have the same pathExpression, which is all we care about.\n```\n. ",
    "danielladd": "Both the graphite web server and grafana are set to EST, local_settings.py has the following attribute : TIME_ZONE = 'America/New_York'\n. The metrics data within graphite that is being returned to grafana has the correct timestamp, I can verify this by viewing the raw data. It seems to only be for the calls to retrieve events. I also verified that all of the cluster machines have the same idea of what time it is. \n. It looks like this may have been taken care of in a commit last year : 72099b17490a772043d0974d823ec4956759f427 @brutasse \n. Great! Is there a timeline for the pre-release of 0.9.14 that anyone knows of?\n. ",
    "movermeyer": "Just to be clear, I'm using the 0.9.x branch\nAdding in the line:\nsys.path.append('/opt/graphite/webapp')\nallows it to work.\nI still seem to have other issues, so there might be some confounding issue with my environment.\nHowever, it's not obvious to me how it's expected to find the source in /opt/graphite/webapp without that line.\nHere are my installed pip modules\n. ",
    "roeezab": "I got the same error, and adding the line \"sys.path.append('/opt/graphite/webapp')\" also helped in my case.\n. I'll check it out. Thanks!\n. In case someone will need it, I wrote this piece of code that goes into webapp/content/js/browser.js:\n``` javascript\nfunction openTree (currentPlaceInTree, path) {\n    if (path.indexOf(\".\") != -1) {\n      var nextPlaceInTree = currentPlaceInTree.findChild('text',path.substr(0,path.indexOf(\".\")));\n      nextPlaceInTree.ensureVisible();\n      nextPlaceInTree.expand(false, true, function(){openTree(nextPlaceInTree, path.substr(path.indexOf(\".\") + 1));});\n    } else {\n      var nextPlaceInTree = currentPlaceInTree.findChild('text',path);\n      nextPlaceInTree.ensureVisible();\n      nextPlaceInTree.expand();\n    }\n}\nfunction gotoPath (path) {\n  Browser.panel.activate(0);\n  Browser.trees.graphite.expand(false, true, function(){openTree(Browser.trees.graphite, path);});\n}\n```\nIn order to open the tree, the gotoPath function should be used like that:\njavascript\ngotoPath('dc1.all_servers.serverX');\nIn my case, I replaced the search in Graphite to go to the tree, instead of showing the metric in the composer. So in the handleSearchResponse function I replaced the following line:\njavascript\nli.innerHTML = \"<a href=\\\"javascript: Composer.toggleTarget('\" + item + \"');\\\">\" + item + \"</a>\";\nwith this one:\njavascript\nli.innerHTML = \"<a href=\\\"javascript:void(0)\\\" onclick=\\\"gotoPath('\" + item + \"');\\\">\" + item + \"</a>\";\n. Thanks!\n. The patch has solved it after a little change for python 2.6:\nhttps://github.com/graphite-project/graphite-web/pull/1588\n. No. I tried adjusting my environment to test it but having trouble with it :/\n. ok thanks, I'll keep working on it.\n. ",
    "gretel": "just today i got this issue, but on the master branch. seems like no dependency gets loaded at all.\n. just today i got this issue, but on the master branch. seems like no dependency gets loaded at all.\n. yeah! :golf: \n. yeah! :golf: \n. ",
    "joafri": "I found the Issue - I belive.\nI sent new data to carbon once per 60 seconds.\nWhen I checked the log in chrome in developerview (F12) then I saw that I got two null and one line with data for every minute.\nI changed the rate of date to carbon to once per 10 seconds and voila - the graphs started to work fine.\nThis feels like a workaround thou and I belive that the graphing should be able to handle null by ignoring them maybe?\n. ",
    "Minnozz": "We are currently running 0.9.12; I will check if the problem still exists as soon as we upgrade to 0.9.14.\n. I did not realise we were running an older version of graphite-web (0.9.12). That version indeed has an incorrect log statement: https://github.com/graphite-project/graphite-web/blob/0.9.12/webapp/graphite/render/datalib.py#L141\nBut this seems to be fixed in HEAD.\n. ",
    "toote": "From what I can see, this will also happen with the removeBelowPercentile function\n. Patch to solve the issue:\n``` patch\n1466c1466,1469\n<     percentile = nPercentile(requestContext, [s], n)[0][0]\n\n\ntry:\n  percentile = nPercentile(requestContext, [s], n)[0][0]\nexcept IndexError:\n  continue\n\n1495c1498,1501\n<     percentile = nPercentile(requestContext, [s], n)[0][0]\n\n\n\ntry:\n  percentile = nPercentile(requestContext, [s], n)[0][0]\nexcept IndexError:\n  continue\n\n``\n. As far as I know the 95-percentile of an empty set of data is an empty set of data. Thecontinueactually implements that behaviour. I would completely agree with the current behaviour (and with your assessment of thecontinue` not helping) if there were a way to remove empty series from a seriesList so as to be able to just weed out empty series if one actually wanted that.\n. @obfuscurity and I really appreciate how quickly you replied :)\n. \n",
    "mhagger": "I forgot to mention one thing I'm unsure about..._LogAxisTics sets self.step to None because that parameter is ignored and unneeded. That value later gets copied to LineGraph.yStep, which eventually can get incorporated into the output of Graph.output here. Does anybody know whether None is permissible there, or whether we maybe have to record some sensible fake value as the step for logarithmic axes, or whether maybe Graph.output should get smarter about omitting that key/value pair altogether?\n. I just pushed some more commits that have to do with how the data range and user options are combined, especially in the case that they are inconsistent with each other. This fixes a bug that the \"rounding up\" behavior for maxValue sometimes made it exceed axisLimit.\n. I've done quite a bit of manual testing of the AxisTics classes in isolation, fixed some edge cases, and added more input validation. I think these classes are pretty solid now.\nI still haven't done any integration testing because I'm not set up for it. So this still could be terribly broken, though hopefully only in trivial ways (plus possibly the point mentioned above).\nI'm going to stop pushing now to give people a chance to look over the changes.\n. @cbowman0, thanks for the testing and the feedback.\nI suspect it comes down to a one-character difference that I noticed between two versions of makeLabel in the code before my changes: here vs. here. The discrepancy originated in 53bfcd04389054b439503be21ae6eea9d11fee25. I took it to be a mistake in that commit that the two functions were not adjusted in the same way. I added a space to one version of the function in e1dde2d7815d9ff82c594654a645c04c16585f33 because it looked like the space was forgotten from that version. After that the two versions of the function were identical, so I unified them.\nIt's easy to take that space back out here, and I expect that would fix your problem. What I'm not sure of is whether the other version of makeLabel really needed the space. @nleskiw, you wrote 53bfcd04389054b439503be21ae6eea9d11fee25; can you shed some light on the situation?\n. @cbowman0: thanks for the testing! (I don't have a graphite-web setup that I can use for testing, so I've been testing the input/output as text.)\nRegarding graph (a): I take it that the inputs are all exactly zero? If so, I personally don't see a strong reason to prefer [-1..1] or [0..1]. If the latter is preferred, let me know and I will implement it (the place is _AxisTics.reconcileLimits() here).\nRegarding graph (b): there is also an aesthetic difference in the choice of axis labels. I personally think that the new version looks better.\nRegarding graph (c): here, to me, the new choice of y axis endpoints and tic marks is clearly better than the old. In fact it was examples like this that led me to start work on this PR.\nThanks for your proposed change to makeLabel. I'd be happy to make this change in my name or to pull the change from your fork, whichever you prefer.\nHowever, taking a step back, I have to question why makeLabel has to add whitespace at the right side of label strings in the first place. Wouldn't it make more sense for the labels to be generated without padding, and for the rendering part of the code to position the labels more intelligently? And when there is no unit string, surely the extra space (which would otherwise separate the number from the label) should be omitted, too? I don't know that part of the code, though, so take this suggestion with a grain of salt.\nIf it were up to me, I would also omit the .0 on label values when none of the labels' numbers have fractional parts. For example, instead of\n\"0\", \"5.0 k\", \"10.0 k\", \"15.0 k\"\n, why not\n\"0\", \"5 k\", \"10 k\", \"15 k\"\n? I'd be happy to implement that, too (though preferably as part of a separate PR).\nThanks again for the feedback, and let me know how you would like to proceed.\n. I think that changing where padding is implemented and whether to omit trailing zeros from label values should be addressed in separate PRs.\nI just pushed a commit that should fix the spacing problem pointed out by @cbowman0, using more or less his suggested change.\nIf there's anything else that people would like to see changed in this PR, please let me know. Otherwise, as far as I know, I've addressed all points that have been brought up.\n. Sorry about the test failure. I just fixed it.\n. Sorry about the test failure. I just fixed it.\n. > @mhagger Do you have feedback on these unit tests of some of the changes you recently made to render/glyph.ph?\nI made some comments here, and I also just submitted a PR against this PR. I'm afraid that's all that I will have time for in the near future.\n. @cbowman0: your suggested patch didn't include this last change. I think this was an oversight, so I removed the space here, too.\n. Tests involving floating-point numbers are often subject to floating-point rounding errors, because assertEqual() insists that the numbers are identical, whereas they might, say, differ in the 13th decimal place due to rounding. assertAlmostEqual() is more robust in situations like this.\n. range(-10, -101) is [] (or the iterator equivalent in Python 3+). If that's what you want, it's probably clearer to write [] here. Alternatively, maybe you meant to add a third parameter?: range(-10, -101, -1).\n. The AxisTics classes were meant to be used by constructing an object and then calling the instance's applySettings(), chooseStep(), and chooseLimits() methods in sequence. Why? Because that was the smallest change from the previous code.\nreconcileLimits() is really meant to be called only by applySettings(), and only in the case that not (self.minValue < self.maxValue). So I think it is inappropriate for the tests in this section to be calling reconcileLimits() directly.\nOTOH I think it would make sense to move the not (self.minValue < self.maxValue) check into reconcileLimits() to make it a little bit more self-contained. We could even make the method private, though when I was coding this I think I checked and didn't find many private methods, so I thought they were disfavored in this codebase.\nI'm working on some patches to clear up minor points like this.\n. ",
    "nicholaskuechler": "A bit behind the reasoning: I have a number of regions, some of which are quite large in terms of the number of nodes in the region, and we're collecting a lot of metrics, which is the reason for multiple regional graphite clusters. We have a lot of tooling that dumps metrics in or pulls metrics out, such as diamond, nagios plugins, custom tools, etc. The number of nodes and the amount of metrics, as well as not having a lot of data traverse between regions, has necessitated creating graphite clusters in each region.\nWe have some dashboards we use for a global view and some other tools that operate on global metrics. The \"easy mode\" would be to set up a graphite-web node that's able to present the unified view of metrics from all regions.\nAs a work-around, I can add the regional level carbon-cache nodes to my \"global\" graphite-web, and that works, but I'm not sure it's an ideal solution. For example, config management becomes more difficult when I add / remove carbon-cache nodes, it's a little bit more difficult to take advantage of the memcached the regional graphite-web nodes are using, I lose the advantages of the load balancer, etc.\nIt might be nice if local=1 was a configurable setting. In my case, my regional graphite clusters will never have a different region configured in CLUSTER_SERVERS, so the recursive requests would be avoided.\n. ",
    "xneo64": "For what it's worth, we needed this same requirement, and got around it by using this config in the apache graphite configuration:\n#Rewrite any query local=1 to local=0 and resend to url matching engine ([PT])\n  RewriteEngine on\n  RewriteCond %{QUERY_STRING} ^(.*)local=1(.*)$\n  RewriteRule ^/(.*) /$1?%1local=0%2 [PT]\n@obfuscurity certainly support implementing this as a configurable parameter.\n. @DanCech has this config flag for letting local=1 or local=0 been implemented in 1.0.x? Couldn't find it anywhere in the sample config.. \nIs there any plan to implement this in the near future?. I got the same issue here with pyparsing 2.1.10 and graphite master. After reading this thread I tried downgrading pyparsing to 1.5.7 and now not getting random errors anymore. \nGive me a shout if anybody wants me to test other versions.. Same here.\nI think the culprit is https://github.com/graphite-project/graphite-web/blob/622830db42f392ce2f1f7fb4a1ddf0dbfd16185c/webapp/graphite/logger.py#L39\nI usually patch my graphite installations with the above set to False.\nAs @Kenterfie said, this used to quickly fill up docker containers on busy systems, even with log rotation set to 1 day. Would be great to have the possibility to disable.. ",
    "kalebwalton": "Any chance of this being implemented? Although it gives central, unified access to data - Grafana does not allow for federated querying across 'regions'. . ",
    "xyntrix": "For what it's worth, I have a hack for Graphite 0.9.16 (and 0.9.15) that turns a graphite-web installation into a federated render node, allowing for a multi-region  / mutilple-tier cluster approach.\nThis enables one to have a setup where federated1 will fetch concurrently data from datacenter1 and datacenter2, without additional changes to datacenter1/datacenter2.\nThe patch:\nhttps://gist.github.com/xyntrix/93d536089d0e992fddf8a9ec9233a074\nI have not explored this feature for Graphite 1.0 yet.. and there's several design improvements that could be made (such as a configurable enabled/disable flag).\nWe use this today by dropping a graphite-web + memcached install along-side each Grafana server, that allows for a universal Grafana configuration to federate n-many graphite clusters.  \nOne huge benefit of this approach is migrating graphite metrics storage from one cluster to another, without having to modify caller queries or dashboards (as long as the metric paths do not change).\n. Probably doesn't matter for 1.x, but in 0.9.x, I couldn't pass local=0 and have it be pickleformat with some of the endpoints -- can't remember if this was /find or /expand or which..  but not all the endpoints supported pickle + local=0, so where jsontree + local=0 did work, I used that and aligned with some of the jsontree key names.. ",
    "ChrisHeerschap": "Seriously, if you find yourself in the Philly area, lemme know. Offer stands.\n. Saw the same thing this morning. I live on this page, constantly referring to it, and shared it with coworkers today only to discover the functions weren't there. I know they were on Friday!\n. Well, I was working on perfecting my Graphite graphs until about 4pm EST on Friday (12/4/15), if that helps narrow it down more. I noticed it missing around 9-10am EST this morning.\n. Thank you!\n. The current documentation on value lists infers the old usage, as well, listing {user,system,iowait} which would now be returned as \"iowait system user\".\n. Having the ability to sort is good, having the ability to control the sorting is wonderful, and being able to pass an arbitrary order (as happens with brace expansion in the shell) and have it be preserved would be divine. Especially when dealing with stacked graphs, I doubt any type of sorting would work for all instances, so having the ability to preserve an arbitrary ordering (the functionality in 0.9.12) is kinda key.. > If you configure the graph using an explicit order, whether clustered or not, it is already \"sorted\" - you did it manually yourself rather than relying on a computer to do it - so further sorting shouldn't be needed.\nIf I configured the graph with a bunch of individual metrics, sure... but this defeats the entire point of using wildcards and brace expansion in targets.\nAs it stands in 0.9.12, I can create a stacked graph in the order that I want using just one target and the braces. In the case of a stacked CPU graph on a 16 core box, that's still just one target. To do it the way you're saying and make an explicit target for each metric would require 128 individual targets, unless there's some other way I haven't figured out. I've spent a whole bunch of time reading through the 0.9.15 docs to figure out how to create the graphs I want in the order I want without resorting to an individual graph for each metric and so far have been unsuccessful.\nThis is one target in 0.9.12 and impossible in 0.9.15 without 192 individual targets.\n\nIf there's something I'm missing that would make that possible in 0.9.15, I'd love to know.\n. ",
    "wooparadog": "Yeah, I realized that this implementation can't have lookup support. I'm working on carbon-cache to add a new DrainStrategy so that new metrics can get written faster.\n. Closing this idae now...\n. ",
    "imbellish": "I've got a fork with some changes that allow Django 1.9 to migrate, but there seems to be some major breaking changes with regards to django-tagging which is perhaps related. See: https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=807454\n. ",
    "wawrzek": "https://bugs.launchpad.net/ubuntu/+source/graphite-web/+bug/1555621 . I found that \n    PYTHONPATH=/opt/graphite/webapp django-admin.py migrate --settings=graphite.settings --run-syncdb\nworks for me\n. Did you install all require dependences? (E.g. django)\n. Have you checked that '/usr/local/django/contrib/admin/media/' exist?\nI installed django via pip and had to set media alias as follow:\n        Alias /media/ \"/usr/local/lib/python2.7/dist-packages/django/contrib/admin/static/admin/\"\n. @kennedyoliveira - You use WhiteNoise, don't you? I tried, but it didn't work so I turned it off. After sorting out /media alias I was fine.\nIt might be useful if you tell exactly what version of graphite you are installing. BTW. I noticed that Django 1.9.x is another problem. \n. ",
    "vpereira": "@wawrzek  thank you :+1: . ",
    "bergquist": "I miss this wonderful page! Full of knowledge and inspiration! \n. @DanCech @RichiH My guess would be that the ingestion would be responsible for receiving open metrics. Its just a communication protocol, not for queries and storage. Unless I misunderstand something :). ",
    "cosm0s": "Thanks for the answer, the advices and the useful guide you provide us to keep on with this new feature.\nThis PR is a litte sample to show what we want to do. With this good feedback, I'm going to spend my time and my efforts to complete the stylistic and implement all the necessary test to complete the asked feature.\n. I intend to continue with this pr, but right now I don't have enough time.\n. ",
    "anitakrueger": "Is there any update on this by any chance? Or another workaround on how to restrict metric access in graphite/grafana? Thanks!!. ",
    "iain-buclaw-sociomantic": "Sure, I'll look into it.\n. Hmm, would the desired behaviour be to ignore aggregated None values here?  Maybe not...\n. OK, done.\n. Wait! I seem to have missed negative floats.\n. Done\n. Thanks.\n. @obfuscurity - Was reverting #1305 intentional?\n. Hmm, I didn't check to see that there are already a test for all None series.  I guess I should tweak the behaviour then.\n@deniszh - Running graphite-api, but the same problem is present here also.\n. Of course, and if anything is not clear, put a big red flag on it.  I wrote many parts very quickly.\n. @obfuscurity - Hi, OK, no problem.\nI've just managed to get round to looking at this again, and got graphite-web set-up with some test ceres data.\n\nConfigure carbon / graphite to write to and read from ceres. I can do a short section on this later.\n\nOnly thing I had to change on the configuration side in local_settings.py was:\nSTORAGE_FINDERS = (\n    'graphite.finders.ceres.CeresFinder',\n)\nThat's pretty much about it on the user side!  This configuration is already documented from the alternative storage finders page, so can check that off the list.. Looks OK to me.\nI was curious to see what the linked bug report was about, I found the following revision change that this should probably be attributed to.\nhttps://hg.python.org/cpython/rev/dafca4714298\nI was surprised to see how similar this now looks to it after my suggestions. :-)\n. OK, lets go for #2168 then.. > Looking at hashing.py in carbon I see that actually there are quite a lot of differences besides the fnv1a_ch hashing that we should really resolve to be sure that it's doing the same thing as graphite-web.\nIts almost as if you need a common library between the two projects. ;-). Could say something about filesystem compression level (ZFS, Btrfs, Zram, ...).  Then again, maybe not.\nJust thinking aloud. :-)\n. Shouldn't this re.compile be a global? I would have thought it expensive to run it every time expand_braces is called.\n. I don't think this copy is needed, .replace doesn't change the contents inplace afaics. (Just a couple of notes for optimization :-)\n. Doesn't open_brace, close_brace = m.span(1) do the same thing?\n. Alternatively, if \",\" in sub:. Whichever reads better I suppose. :-)\n. Well, you have the following below:\nfor pat in sub.strip('{}').split(','):\n        res.extend(expand_braces(s[:open_brace] + pat + s[close_brace + 1:]))\n    else:\n        res.extend(expand_braces(s[:open_brace] + sub.replace('}', '\\\\}') + s[close_brace + 1:]))\nWith m.span(1) you can drop the + 1 bit.\nfor pat in sub.strip('{}').split(','):\n        res.extend(expand_braces(s[:open_brace] + pat + s[close_brace:]))\n    else:\n        res.extend(expand_braces(s[:open_brace] + sub.replace('}', '\\\\}') + s[close_brace:]))\n:-)\n. ",
    "gmlexx": "I'm shocked, so much attention :)\n. @JeanFred done!\n. ",
    "fuqqer": "Well then it shouldn't be called a \"forecast\"?\nMaybe this should be a feature request to display the predicted (at the current time) over/under of the forecast given a window of past data?\n. ",
    "anayrat": ":+1:\n. ",
    "yuuki": ":+1: \n. ",
    "shoichimasuhara1982": ":+1: \n. ",
    "mechairoi": "@JeanFred Thank you for the advice. I have written a test.\n. These arguments  are copied from the timeSlice function, but it sounds great.\nDoes it also make sense to default endSourceAt to the until as follows?\n``` diff\n--- a/webapp/graphite/render/functions.py\n+++ b/webapp/graphite/render/functions.py\n@@ -2389,7 +2389,7 @@ def linearRegressionAnalysis(series):\n     offset = (sumII * sumV - sumIV * sumI) /denominator - factor * series.start\n     return factor, offset\n-def linearRegression(requestContext, seriesList, startSourceAt, endSourceAt=\"now\"):\n+def linearRegression(requestContext, seriesList, startSourceAt=None, endSourceAt=None):\n   \"\"\"\n   Graphs the liner regression function by least squares method.\n@@ -2407,8 +2407,8 @@ def linearRegression(requestContext, seriesList, startSourceAt, endSourceAt=\"now\n   \"\"\"\n   results = []\n   sourceContext = requestContext.copy()\n-  sourceContext['startTime'] = parseATTime(startSourceAt)\n-  sourceContext['endTime'] = parseATTime(endSourceAt)\n+  if startSourceAt is not None: sourceContext['startTime'] = parseATTime(startSourceAt)\n+  if endSourceAt is not None: sourceContext['endTime'] = parseATTime(endSourceAt)\n``\n. @obfuscurity Thank you for the review. I fixed defaultstartSourceAtandendSourceAt. Please let me know if you have any concerns.\n. @JeanFred  squashed :)\n. You're probably looking for theharakiri` option, if using uWSGI.\n. @bmhatfield Thanks for your reviews! I fixed them and added some tests. But latest Sphinx breaks the CI build.\n. ",
    "Songmu": "@JeanFred Thank you for the review.\nWe heavily use graphite in our monitoring SaaS Mackerel for two years. @mechairoi he is in our team.\nThis feature is very useful for us (and all users, of cource).\nWe hope this pull request to be accepted.\n. @obfuscurity @JeanFred \nHow about it?\n. :smile:\n. :+1:\n. Mackerel base the product on Graphite for displaying monitoring graphs on users dashboard. All of our time series data are stored as whisper files.\n. Thank you!\n. ",
    "puddean": "thanks!\ni've figured that out last week.\nThe problem is that i forgot to edit /etc/graphite-web/local_settings.py. Under #Filesystem Paths# part the Data directory was not right.\nthanks for noticing and answering my question~^^\n. ",
    "pra85": "Updated the pull request with the changes you suggested\n. ",
    "mrh666": "No it doesn't looks similar!\npip output is:\n[root@graphitecarbon-172 graphite]# pip list | grep graphite-web\nDEPRECATION: Python 2.6 is no longer supported by the Python core team, please upgrade your Python. A future version of pip will drop support for Python 2.6\ngraphite-web (0.9.15)\nBut file webapp/graphite/util.py is different:\n```\n  try:\n    defaultProfile = Profile.objects.get(user=defaultUser)\n  except Profile.DoesNotExist:\n    log.info(\"Default profile does not exist, creating it...\")\n    defaultProfile = Profile(user=defaultUser)\n    defaultProfile.save()\nThis whole song & dance is due to pickle being insecure\nThe SafeUnpickler classes were largely derived from\nhttp://nadiana.com/python-pickle-insecure\nThis code also lives in carbon.util\nif USING_CPICKLE:\n  class SafeUnpickler(object):\n    PICKLE_SAFE = {\n      'copy_reg': set(['_reconstructor']),\n      'builtin': set(['object', 'list']),\n      'graphite.intervals': set(['Interval', 'IntervalSet']),\n      'graphite.render.datalib': set(['TimeSeries']),\n      'collections': set(['deque']),\n    }\n@classmethod\ndef find_class(cls, module, name):\n  if not module in cls.PICKLE_SAFE:\n    raise pickle.UnpicklingError('Attempting to unpickle unsafe module %s' % module)\n  __import__(module)\n  mod = sys.modules[module]\n  if not name in cls.PICKLE_SAFE[module]:\n    raise pickle.UnpicklingError('Attempting to unpickle unsafe class %s' % name)\n  return getattr(mod, name)\n\n```\nIs there is any chance I still have old files during the upgrade (pip install upgrade graphite-web)?\nHow can I properly upgrade or fully remove it and install it again?\n. https://gist.github.com/mrh666/0ea1eb1f195d94bac2ad\n. deniszh, thank you!\n\"pip uninstall graphite-web\" <- sure doesn't delete files under  /opt/graphite/webapp, so I removed them manually. \nThen \"pip install graphite-web --install-option=\"--prefix=/opt/graphite\" --install-option=\"--install-lib=/opt/graphite/webapp\" did the clean install. All those exceptions are gone after \"service httpd restart\".\n. ",
    "lildude": "We encountered this same issue when upgrading from 0.9.12 to 0.9.15.\nToday I did a lot of digging into this and found what I believe to be the cause. \nThings were all tickety-boo in 0.9.x until https://github.com/graphite-project/graphite-web/pull/1026 which made its way into 0.9.13-pre1.  The pertinent change here is https://github.com/graphite-project/graphite-web/commit/5519e5903c3e148d5d86b129dfd668613011cd87 which sorts the final data passed to fetchData().\nThings were then refactored and changes made to handle multiple TimeSeries data after a cluster partition in https://github.com/graphite-project/graphite-web/pull/1352 which involved changing the seriesList from a list (which retains order) to a dictionary (which does not retain order) in https://github.com/graphite-project/graphite-web/commit/3a7e1a88cae0a7af5e850bbf4153d237428cd8cb, which is also sorted on return as it was in 0.9.13-pre1.\nRemoving the sorting on the return value at https://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/render/datalib.py#L478 isn't sufficient to solve the issue as dictionaries don't preserve order.\nIn order to preserve order, whilst still retaining the benefits of using the dictionary, the sorted at https://github.com/graphite-project/graphite-web/blob/0.9.x/webapp/graphite/render/datalib.py#L478 needs to be removed and then rely on something like collections.OrderedDict() to retain the order of elements as they're added.\nThe simple solution I found which does the trick for us is:\n```\n--- a/webapp/graphite/render/datalib.py\n+++ b/webapp/graphite/render/datalib.py\n@@ -17,6 +17,7 @@ import struct\n import time\n import threading\n import Queue\n+import collections\n from django.conf import settings\n from graphite.logger import log\n from graphite.storage import STORE, LOCAL_STORE\n@@ -365,7 +366,7 @@ def fetchRemoteData(requestContext, pathExpr, usePrefetchCache=settings.REMOTE_P\n# Data retrieval API\n def fetchData(requestContext, pathExpr):\n-  seriesList = {}\n+  seriesList = collections.OrderedDict()\n   (startTime, endTime, now) = _timebounds(requestContext)\ndbFiles = [dbFile for dbFile in LOCAL_STORE.find(pathExpr)]\n@@ -475,7 +476,7 @@ def fetchData(requestContext, pathExpr):\n# Stabilize the order of the results by ordering the resulting series by name.\n   # This returns the result ordering to the behavior observed pre PR#1010.\n-  return [ seriesList[k] for k in sorted(seriesList) ]\n+  return [ seriesList[k] for k in seriesList ]\ndef mergeResults(dbResults, cacheResults, lowest_step):\n```\nThis takes us from the \"inverted\" graphs we see with 0.9.15:\n\n... to the correct graphs we see with 0.9.12 ...\n\nMy 2c: if sorting is desired, I think it should probably be at the user's request using the sortByName function rather than being implemented somewhere that they can't override. \nI've not opened a PR for this yet as I may be missing something obvious. Let me know if I'm not and if not, if you want a PR for this.. From what I can see, there wasn't any sorting before https://github.com/graphite-project/graphite-web/pull/1010 either, though I've not dug that far back. The function itself was using a list at the time so any order provided would have been honoured.\n\nAs a comment above says sort is needed because 'Stabilize the order of the results by ordering the resulting series by name. This returns the result ordering to the behavior observed pre PR#1010'.\n\nWithout any context about why that comment was added and what the behaviour was before, this sounds like a perfect candidate for sortByName() \ud83d\ude09\n. I hear you @deniszh and it makes good sense.\nWith your comments in mind, do you have a suggestion on how people would go about getting the old desired appearance from deliberately ordered keys when creating stacked graphs that results in an intuitive graph as seen in 0.9.12 with 0.9.15 and later?\nI think if we can come up with a good method then nothing really needs to change from a graphite perspective just how the user \"configures\" the graph.\nI also think it's worth documenting that graphite doesn't guarantee an order, or that it does now order regardless of whether you use the sortByName() function or not, else we'll be back here in the future if it's decided sorting isn't the best approach.. @deniszh Thinking about this further...\n\nAs a comment above says sort is needed because 'Stabilize the order of the results by ordering the resulting series by name. This returns the result ordering to the behavior observed pre PR#1010'.\nWithout sort, results will be inconsistent during refreshes.\n\nIf you configure the graph using an explicit order, whether clustered or not, it is already \"sorted\" - you did it manually yourself rather than relying on a computer to do it - so further sorting shouldn't be needed.\nWouldn't switching to using an ordered dictionary so order in == order out negate the need for additional sorting and thus things would work consistently with parallel cluster fetches too? (I don't have a cluster setup so can't test this myself).. > If I configured the graph with a bunch of individual metrics, sure... but this defeats the entire point of using wildcards and brace expansion in targets.\nOooo, and there's something I didn't consider: mixing both a wildcard and an explicit braces list. That said, I'd be interested to know if the graphs display properly if you apply my patch to 0.9.15.\n\nIf there's something I'm missing that would make that possible in 0.9.15, I'd love to know.\n\nDitto.. ",
    "mrcabbage972": "Duplicate of https://github.com/graphite-project/graphite-web/issues/604\n. ",
    "DazWorrall": "Should also say the carbon and graphite-web version is 0.9.15 on python 2.7.3 on Ubuntu 12.04\n. Thanks for the suggestion @deniszh but no joy I'm afraid - I can see the queries come in for individual metrics so the change took affect but we're still seeing gaps :(\n. ",
    "FractalizeR": "We experience the same issue with our setup with new metrics.\nIf we send a metric which already had some value in the past, it appears almost instantly. But if we send a new metric, it does not appear for about 20-30 min.. @deniszh I see, thanks!. ",
    "utkarshcmu": "+1\n. ",
    "daks": "I have the same problem: I installed with pip (either from master or from released versions) and I got stuck at the 'Webapp Database Setup' step. I thought Django will be installed as a requirement to graphite-web but it's not the case.\nI wonder if installing Django is sufficient or if other deps will be missing? And which Django version is needed?\n. ",
    "kennedyoliveira": "Hi @deniszh, i used the example one, but did not changed any configuration, except for the secret key,\nanyway, check below:\n```\nGraphite local_settings.py\nEdit this file to customize the default Graphite webapp settings\n\nAdditional customizations to Django settings can be added to this file as well\n\nGeneral Configuration\n\nSet this to a long, random unique string to use as a secret key for this\ninstall. This key is used for salting of hashes used in auth tokens,\nCRSF middleware, cookie storage, etc. This should be set identically among\ninstances if used behind a load balancer.\nSECRET_KEY = 'TESTING'\nIn Django 1.5+ set this to the list of hosts your graphite instances is\naccessible as. See:\nhttps://docs.djangoproject.com/en/dev/ref/settings/#std:setting-ALLOWED_HOSTS\nALLOWED_HOSTS = [ '*' ]\nSet your local timezone (Django's default is America/Chicago)\nIf your graphs appear to be offset by a couple hours then this probably\nneeds to be explicitly set to your local timezone.\nTIME_ZONE = 'America/Los_Angeles'\nOverride this to provide documentation specific to your Graphite deployment\nDOCUMENTATION_URL = \"http://graphite.readthedocs.org/\"\nMetric data and graphs are cached for one minute by default\nDEFAULT_CACHE_DURATION = 60\nLogging\nLOG_ROTATION = True\nLOG_ROTATION_COUNT = 1\nLOG_RENDERING_PERFORMANCE = True\nLOG_CACHE_PERFORMANCE = True\nEnable full debug page display on exceptions (Internal Server Error pages)\nDEBUG = True\nIf using RRD files and rrdcached, set to the address or socket of the daemon\nFLUSHRRDCACHED = 'unix:/var/run/rrdcached.sock'\nThis lists the memcached servers that will be used by this webapp.\nIf you have a cluster of webapps you should ensure all of them\nhave the exact same value for this setting. That will maximize cache\nefficiency. Setting MEMCACHE_HOSTS to be empty will turn off use of\nmemcached entirely.\n\nYou should not use the loopback address (127.0.0.1) here if using clustering\nas every webapp in the cluster should use the exact same values to prevent\nunneeded cache misses. Set to [] to disable caching of images and fetched data\nMEMCACHE_HOSTS = ['10.10.10.10:11211', '10.10.10.11:11211', '10.10.10.12:11211']\nMetric data and graphs are cached for one minute by default. If defined,\nDEFAULT_CACHE_POLICY is a list of tuples of minimum query time ranges mapped\nto the cache duration for the results. This allows for larger queries to be\ncached for longer periods of times. All times are in seconds. If the policy is\nempty or undefined, all results will be cached for DEFAULT_CACHE_DURATION.\nDEFAULT_CACHE_DURATION = 60 # Cache images and data for 1 minute\nDEFAULT_CACHE_POLICY = [(0, 60), # default is 60 seconds\n(7200, 120), # >= 2 hour queries are cached 2 minutes\n(21600, 180)] # >= 6 hour queries are cached 3 minutes\nMEMCACHE_KEY_PREFIX = 'graphite'\nSet URL_PREFIX when deploying graphite-web to a non-root location\nURL_PREFIX = '/graphite'\n\nFilesystem Paths\n\nChange only GRAPHITE_ROOT if your install is merely shifted from /opt/graphite\nto somewhere else\nGRAPHITE_ROOT = '/opt/graphite'\nMost installs done outside of a separate tree such as /opt/graphite will only\nneed to change these three settings. Note that the default settings for each\nof these is relative to GRAPHITE_ROOT\nCONF_DIR = '/opt/graphite/conf'\nSTORAGE_DIR = '/opt/graphite/storage'\nSTATIC_ROOT = '/opt/graphite/static'\nTo further or fully customize the paths, modify the following. Note that the\ndefault settings for each of these are relative to CONF_DIR and STORAGE_DIR\n\nWebapp config files\nDASHBOARD_CONF = '/opt/graphite/conf/dashboard.conf'\nGRAPHTEMPLATES_CONF = '/opt/graphite/conf/graphTemplates.conf'\nData directories\nNOTE: If any directory is unreadable in STANDARD_DIRS it will break metric browsing\nCERES_DIR = '/opt/graphite/storage/ceres'\nWHISPER_DIR = '/opt/graphite/storage/whisper'\nRRD_DIR = '/opt/graphite/storage/rrd'\nData directories using the \"Standard\" finder (i.e. not Ceres)\nSTANDARD_DIRS = [WHISPER_DIR, RRD_DIR] # Default: set from the above variables\nLOG_DIR = '/opt/graphite/storage/log/webapp'\nINDEX_FILE = '/opt/graphite/storage/index'  # Search index file\n\nEmail Configuration\n\nThis is used for emailing rendered Graphs\nDefault backend is SMTP\nEMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'\nEMAIL_HOST = 'localhost'\nEMAIL_PORT = 25\nEMAIL_HOST_USER = ''\nEMAIL_HOST_PASSWORD = ''\nEMAIL_USE_TLS = False\nTo drop emails on the floor, enable the Dummy backend:\nEMAIL_BACKEND = 'django.core.mail.backends.dummy.EmailBackend'\n\nAuthentication Configuration\n\nLDAP / ActiveDirectory authentication setup\nUSE_LDAP_AUTH = True\nLDAP_SERVER = \"ldap.mycompany.com\"\nLDAP_PORT = 389\nLDAP_USE_TLS = False\nOR\nLDAP_URI = \"ldaps://ldap.mycompany.com:636\"\nLDAP_SEARCH_BASE = \"OU=users,DC=mycompany,DC=com\"\nLDAP_BASE_USER = \"CN=some_readonly_account,DC=mycompany,DC=com\"\nLDAP_BASE_PASS = \"readonly_account_password\"\nLDAP_USER_QUERY = \"(username=%s)\"  #For Active Directory use \"(sAMAccountName=%s)\"\n\nUser DN template to use for binding (and authentication) against\nthe LDAP server. %(username) is replaced with the username supplied at\ngraphite login.\nLDAP_USER_DN_TEMPLATE = \"CN=%(username)s,OU=users,DC=mycompany,DC=com\"\n\nIf you want to further customize the ldap connection options you should\ndirectly use ldap.set_option to set the ldap module's global options.\nFor example:\n\nimport ldap\nldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, ldap.OPT_X_TLS_ALLOW) # Use ldap.OPT_X_TLS_DEMAND to force TLS\nldap.set_option(ldap.OPT_REFERRALS, 0) # Enable for Active Directory\nldap.set_option(ldap.OPT_X_TLS_CACERTDIR, \"/etc/ssl/ca\")\nldap.set_option(ldap.OPT_X_TLS_CERTFILE, \"/etc/ssl/mycert.pem\")\nldap.set_option(ldap.OPT_X_TLS_KEYFILE, \"/etc/ssl/mykey.pem\")\nldap.set_option(ldap.OPT_DEBUG_LEVEL, 65535) # To enable verbose debugging\nSee http://www.python-ldap.org/ for further details on these options.\nREMOTE_USER authentication. See: https://docs.djangoproject.com/en/dev/howto/auth-remote-user/\nUSE_REMOTE_USER_AUTHENTICATION = True\nOverride the URL for the login link (e.g. for django_openid_auth)\nLOGIN_URL = '/account/login'\n\nAuthorization for Dashboard\n\nBy default, there is no security on dashboards - any user can add, change or delete them.\nThis section provides 3 different authorization models, of varying strictness.\nIf set to True, users must be logged in to save or delete dashboards. Defaults to False\nDASHBOARD_REQUIRE_AUTHENTICATION = True\nIf set to the name of a user group, dashboards can be saved and deleted by any user in this\ngroup.  Groups can be set in the Django Admin app, or in LDAP.  Defaults to None.\nNOTE: Ignored if DASHBOARD_REQUIRE_AUTHENTICATION is not set\nDASHBOARD_REQUIRE_EDIT_GROUP = 'dashboard-editors-group'\nIf set to True, dashboards can be saved or deleted by any user having the appropriate\n(change or delete) permission (as set in the Django Admin app).  Defaults to False\nNOTE: Ignored if DASHBOARD_REQUIRE_AUTHENTICATION is not set\nDASHBOARD_REQUIRE_PERMISSIONS = True\n\nDatabase Configuration\n\nBy default sqlite is used. If you cluster multiple webapps you will need\nto setup an external database (such as MySQL) and configure all of the webapp\ninstances to use the same database. Note that this database is only used to store\nDjango models such as saved graphs, dashboards, user preferences, etc.\nMetric data is not stored here.\n\nDO NOT FORGET TO RUN 'django-admin.py syncdb' AFTER SETTING UP A NEW DATABASE\n\nThe following built-in database engines are available:\ndjango.db.backends.postgresql_psycopg2\ndjango.db.backends.mysql\ndjango.db.backends.sqlite3\ndjango.db.backends.oracle\n\nThe default is 'django.db.backends.sqlite3' with file 'graphite.db'\nlocated in STORAGE_DIR\n\nDATABASES = {\n'default': {\n'NAME': '/opt/graphite/storage/graphite.db',\n'ENGINE': 'django.db.backends.sqlite3',\n'USER': '',\n'PASSWORD': '',\n'HOST': '',\n'PORT': ''\n}\n}\n\n\nCluster Configuration\n\n(To avoid excessive DNS lookups you want to stick to using IP addresses only in this entire section)\n\nThis should list the IP address (and optionally port) of the webapp on each\nremote server in the cluster. These servers must each have local access to\nmetric data. Note that the first server to return a match for a query will be\nused.\nCLUSTER_SERVERS = [\"10.0.2.2:80\", \"10.0.2.3:80\"]\nThese are timeout values (in seconds) for requests to remote webapps\nREMOTE_FIND_TIMEOUT = 3.0             # Timeout for metric find requests\nREMOTE_FETCH_TIMEOUT = 6.0            # Timeout to fetch series data\nREMOTE_RETRY_DELAY = 60.0             # Time before retrying a failed remote webapp\nREMOTE_EXCLUDE_LOCAL = False          # Try to detect when a cluster server is localhost and don't forward queries\nREMOTE_READER_CACHE_SIZE_LIMIT = 1000 # Maximum number of remote URL queries to cache\nFIND_CACHE_DURATION = 300             # Time to cache remote metric find results\nIf the query doesn't fall entirely within the FIND_TOLERANCE window\nwe disregard the window. This prevents unnecessary remote fetches\ncaused when carbon's cache skews node.intervals, giving the appearance\nremote systems have data we don't have locally, which we probably do.\nFIND_TOLERANCE = 2 * FIND_CACHE_DURATION\nRemote rendering settings\nSet to True to enable rendering of Graphs on a remote webapp\nREMOTE_RENDERING = True\nList of IP (and optionally port) of the webapp on each remote server that\nwill be used for rendering. Note that each rendering host should have local\naccess to metric data or should have CLUSTER_SERVERS configured\nRENDERING_HOSTS = []\nREMOTE_RENDER_CONNECT_TIMEOUT = 1.0\nIf you are running multiple carbon-caches on this machine (typically behind a relay using\nconsistent hashing), you'll need to list the ip address, cache query port, and instance name of each carbon-cache\ninstance on the local machine (NOT every carbon-cache in the entire cluster). The default cache query port is 7002\nand a common scheme is to use 7102 for instance b, 7202 for instance c, etc.\n\nYou should use 127.0.0.1 here in most cases\nCARBONLINK_HOSTS = [\"127.0.0.1:7002:a\", \"127.0.0.1:7102:b\", \"127.0.0.1:7202:c\"]\nCARBONLINK_TIMEOUT = 1.0\nCARBONLINK_RETRY_DELAY = 15 # Seconds to blacklist a failed remote server\nA \"keyfunc\" is a user-defined python function that is given a metric name\nand returns a string that should be used when hashing the metric name.\nThis is important when your hashing has to respect certain metric groupings.\nCARBONLINK_HASHING_KEYFUNC = \"/opt/graphite/bin/keyfuncs.py:my_keyfunc\"\nPrefix set in carbon for the carbon specific metrics.  Default in carbon is 'carbon'\nCARBON_METRIC_PREFIX='carbon'\nThe replication factor to use with consistent hashing\nThis should usually match the value configured in Carbon\nREPLICATION_FACTOR = 1\nHow often should render.datalib.fetch() retry to get remote data\nMAX_FETCH_RETRIES = 2\n\nAdditional Django Settings\n\nUncomment the following line for direct access to Django settings such as\nMIDDLEWARE_CLASSES or APPS\nfrom graphite.app_settings import *\n```\n. Hey @deniszh, any news on that? It's driving me crazy, i can't start testing the monitoring module for my application in production due to this :cry:\nMaybe i'm doing something wrong? I need to change any of this configuration to work or the defaults is ok?\n. Hi @deniszh, thanks again for you help!\nI already ran synthesize with vagrant and it worked flawlessly, i checked the versions and they all match, but i noticed a difference in the django installation, in mine the files are in the dir site-packages instead of dist-packages like the template from synthesize, maybe it's helping to cause some sort of problem?\nYour wsgi2.py works, the project starts but the it doesn't serve the static files, i managed to make it works removing the SetHandler None from /media and /content then it worked but django admin isn't working, it gives and error saying a admin template wasn't found.\nEven managing it to work like that, sounds like a lot of workaround, doesn't look like a easy fresh installation and probably will cause problems in the future when i forgot that to install need to do all that stuff or maybe cause some sort of problem because it's not working as expected.\nI'm a java developer and i don't have apache and django knowledge, maybe i'm doing something wrong? \nWhat is this whitenoise? I have any significative benefits from it or can i forget it and continue using your wsgi2.py?\nAbout the django admin, what i'll miss if i let it not working? sorry for the newbie question, like a said i have no knowledge about django stuff, if you have some patience to help me.\nThank you again.\n. @deniszh , i don't know why whitenoise didn't work, i noticed that synthesize also isn't using it.\nI used a ubuntu 14.04 machine like the one from synthesize and installed everything from 0, i noticed the following diferences:\n-  When i install python manually and after install django it will be installed in site-packages, and if i point apache to there for admin stuff it doesn't work. Is there any difference when installed in site-packages?\n-  Whitenoise never works with me, no matter what i try, it doesn't work even with synthesize. Currently it use without whitenoise, and if i put the original wsgi with withenoise or the one that you sent it doesn't work too. synthesize wsgi\n- If i install python from ubuntu repository (apt-get install) and remove whitenoise stuff from wsgi, like using your wsgi2.py and configure apache correctly to handle the static content, it works fine.\nThanks for the nginx suggestion, i'll try to configure white that, but since the problem is the whitenoise, i guess i'll need to do the same thing as with apache, configuring to nginx serve the static files then it'll work.\nJust to be sure, for whitenoise work i need to do something extra? Because i saw some people saying you need to use a collectstatic django admin command so it can process, but it's not available with django 1.4, that used in graphite 0.9.x\n. @deniszh , thank you for all the support, i will try Nginx + gunicorn and will post the results here after\n. hey @wawrzek, yeah i checked for \"/usr/local/lib/python2.7/dist-packages/django/contrib/admin/static/admin/\" and it doesn't existed, so i found that mine was installed in '/usr/local/django/contrib/admin/media/', thats why i pointed to there\n. Hey @wawrzek - Yeah, i'm using WhiteNoise, i had to turn it off to work too. I'm installing 0.9.12.\nAbout Django, what version do you recommend?\n. @deniszh, @wawrzek thanks for your help, i solved the problem using the NGinx + GUnicorn, and installing django 1.4 and removing white noise, that was the most \"easily\" way i found to get the setup working fine.\nSorry for the delay, i got really busy with some health issues and work, anyway, i appreciate the help you guys gave me.\n. ",
    "squarebracket": "I can submit a PR for this, but I don't know if this is a problem due to my old versions or what. \nHere's a fix which would handle any case where the number of closing tags is less than or equal to the number of opening tags:\n```\n...\nopen_g = svgData.count('\nfileObj.write(svgData)\n...\n```\n. Oh, that sucks. Even for bug fixes? SVG rendering is broken in my environment and I was going to submit another PR for that. \nJust took a look at the master branch code. Looks like it will take me longer to set up a test rig for the master branch than porting the code. I suppose I have to open a new PR since it's merging to a different branch, right?\n. Is there anything holding this back from getting merged?\n. ...hello?\n. No problem, I just wanted to make sure I wasn't missing anything!\n. ",
    "toopa2002": "Are you setting 1 week for retention period?  Check this -> https://github.com/graphite-project/whisper/issues/166#issuecomment-243713438\n. Strange,  the render api cannot query test.count. Is there any know issue or setting I should consider? \n. I will re-check the log file. \nIs this issue fixed? - > https://github.com/graphite-project/graphite-web/issues/193#thread-subscription-status\n. The problem that I found is on render api . It is not consistent some time I can retrieve \"xxx.count\", some time i can't.   I have checked whisper file and they are not corrupted.\nThe graphite-web version is 0.9.12.\n- call api -> xxx.count\ncurl 'http://localhost/render/?target=env.dc.bd.eth.eth_ws.veth-ppws04.eth-ws.CreateEbd.req.count&format=json'\n[]\n- call api -> xxx.count.max\ncurl 'http://localhost/render/?target=env.dc.bd.eth.eth_wveth-ppws04.eth-ws.CreateEbd.req.count.max&format=json'\n100 27470  100 27470 [{\"target\": \"env.dc.bd.eth.eth_ws.veth-ppws04.eth-ws.CreateEbd.req.count.max\", \"datapoints\": [[0.0, 1473144180], [0.0, 1473144240], [0.0, 1473144300], [0.0, 1473144360\n   0     0  6], [0.0, 1473144420], [0.0, 1473144480], [0.0, 1473144540], [0.0, 1473144600], [0.0, 1473144660], [0.0, 1473144720], [0.0, 1473144780], [0.0, 1473144840], [0.0, 1473144900], [0.0, 1\n*Whisper file are store on 2 machines\n- machine A\nls -l /opt/graphite/whisper/env/dc/bd/eth/eth_ws/veth-ppws02/eth-ws/CreateEbd/req\ntotal 340\n-rw-r--r-- 1 carbon carbon 344812 Sep  7 06:45 count.wsp\n- machine B\n```\nls -l /opt/graphite/whisper/env/dc/bd/eth/eth_ws/veth-ppws02/eth-ws/CreateEbd/req/\ntotal 4\ndrwxr-xr-x 2 carbon carbon 4096 Aug 15 15:34 count\nls -l /eikonmonnas/graphite/whisper/env/dc/bd/eth/eth_ws/veth-ppws02/eth-ws/CreateEbd/req/count/\ntotal 340\n-rw-r--r-- 1 carbon carbon 344812 Sep  7 06:48 max.wsp\n```\n. ",
    "teamurko": "Thanks for feedback @JeanFred . Actually, it looks like it might be sufficient to use timezone aware util.epoch function because django db returns datetime objects with timezone specified. Tested on graphite-web=0.9.15. \n. ",
    "virtualdxs": "I haven't used Graphite in awhile as I've moved away from doing monitoring stuff. I'll go ahead and close this issue.\n. ",
    "sraupach": "Sorry for that late reply. I reinstalled it from scratch and it's working right now.\n. ",
    "srhopkins": "Do the metrics exist on both nodes in the cluster? I encountered this on upgrade to 9.15 where we also added 2 more nodes to the hash ring taking advantage of the new merge feature. Most of my errors were actually from storage retention sizes being different where the metrics were living but after fixing those I still get this error on my scaleToSeconds metrics. Not sure what it is yet I'm guessing scale is an issue somehow.\n. I checked today and the ones with issues after the resizing did have different storage-aggregation settings. In my case, inspecting a few metrics with whisper-info I found one would have a 10 secondsPerPoint archive that was missing in the other which had 60 secondsPerPoint. Without officially tracing anything I would guess the one node was returning 6x in its values list/array causing an IndexError. Not sure about your situation unless maybe you were sending less frequently around the create timeframe and missing the percent necessary to perform the aggregation but it seems even in that case the arrays would be the same length and just contain more Nones.\n. ",
    "angadsingh": "Our storage retention size were the same. Just noticed that the storage-aggregation settings were different for these metrics. One was sum with xFilesFactor = 0.1 and the other was average with xFilesFactor = 0.5. But why would it fail the merge (that too for recent data only)?\n. ",
    "HackRanger": "Hi,\nGraphite package version in 0.9.12+debian-3.\n$graphite-manage --version\n1.6.1\nThe java client was written by me to populate some data.\n. Thanks for your suggestion. It was problem with the string formatting. After digging into carbon logs, it was telling me about invalid line. I had to type cast time stamp to long. I could solve the problem without updating graphite. But anyhow I did update after fixing the problem.\n. ",
    "meldavis": "It looks like it is a debian-releated error.   A lot of path and permission errors. \n. Just as an FYI: I do have graphite 0.9.15 working with django 1.9.5 on debian.\nAfter installing, install databases: graphite-manage migrate --run-syncdb\nDebian specific:\n- change ownership of /var/log/graphite to www-data:www-data\n- change ownership of /var/lib/graphite to www-data:www-data\n- Debian did not install graphite/build-index.sh.  Copy graphite/build-index.sh from github to /usr/share/graphite-web/bin/\nGenerally:\nThe python-tagging for django on pypi, and the one installed by debian, does not support django 1.9.  The latest python-tagging on github, supports only django 1.9.  So, I modified /usr/lib/python2.7/dist-packages/tagging/templatetags/tagging_tags.py to support either either version of django. \n. ",
    "ckolbeck": "@obfuscurity I don't believe it's the same, though I could be wrong. For instance consider something like countSeries(currentAbove(<query>, <threshold>)) Since currentAbove here could return an empty list the issue isn't replacing nulls, but dealing with the absence of any series.\n. @bitprophet Between the version we're running and HEAD the behavior when normalize()ing an empty series list went from throwing to returning an empty series list. that seems totally reasonable in almost every case, but countSeries() seems like an exception.\n. @bitprophet As it stands the empty series list will propagate out to the requester and you'll get no data. That makes perfect sense for most functions, so I wouldn't call it a bug in normalize() or even in countSeries(). Just that countSeries() is semi-unique in that there's a sensible alternative to return.\nIt's also worth noting that it's possible to get this same behavior by combining other functions, for instance: fallbackSeries(countSeries(server*.requests_per_second), constantLine(0)) (a small modification to an example in the fallbackSeries() docs), but this doesn't play all that nicely with some dashboard systems such as Grafana.\n. @obfuscurity Sorry to prod, but has the above discussion changed your mind at all?\n. @obfuscurity @1337newbee I'm just getting back from vacation and this is on my list. I'm pretty backlogged, but I should have something this week.\n. :heart: \n. ",
    "1337newbee": "I'm glad to see this one approved. Which version of Graphite version that contains this change?\n. @ckolbeck let me know if there's any testing I can help with. We are currently blocking by this issue, so the seyren alert won't be triggered downstream. \nAlso, I'm wondering if there's any equivalent function I can use for now. I tried fallbackSeries(countSeries(server*.requests_per_second), constantLine(0)), it threw out some exceptions as well. Not sure if that's because it requested the latest version of Graphite.\nThanks\n. my series is the basic one with uptime\ncountSeries(currentAbove(container_name.uptime.uptime,0))\n. when collectd is down, it will give out null value, I was able to use currentAbove and transformNull to get it return to  [ ]. But I'm wondering if there's any formula can convert [ ] to -1 or other metrics so seyren server can process it. @obfuscurity  any idea? Thanks\n. @obfuscurity,\n Any idea on how I can use the latest code to implement it on my scenario? Is that in master or Graphite official website?\n. I followed the documentation to do a step by step installation, and it was using 0.9.x.\nAny idea on how to deploy it using the master? Like documentations for that?\nI checked the documentation, but it's not up-to-date. \n. I followed the link above and did pip install, after installation, I went to the directory and see this\ndrwxr-xr-x   7  staff   238 Jun 30 11:39 graphite_web-0.9.15-py2.7.egg-info\ndoes that mean I was using 0.9.15? Or there's another place I can check the version of my Graphite?\n. This is the link\nhttp://graphite.readthedocs.io/en/latest/install-pip.html\nand here are the commands I issue. I was using mac os\npip install https://github.com/graphite-project/ceres/tarball/master\npip install whisper\npip install carbon\npip install graphite-web\nIs Synthesize 3.0.0RC  Ubuntu only?\n. This is the link\nhttp://graphite.readthedocs.io/en/latest/install-pip.html\nand here are the commands I issue. I was using mac os\npip install https://github.com/graphite-project/ceres/tarball/master\npip install whisper\npip install carbon\npip install graphite-web\nIs Synthesize 3.0.0RC  Ubuntu only?\n. I finally got the above one working. Validating this \nhttps://9.37.*./render?target=countSeries(currentBelow(collectd.graphite.processes.ps_state-paging,1)&format=json\nIt's still return [] rather than give me the desired 0. \nBut I can use fallbackSeries() to get it return to 0.\n. I finally got the above one working. Validating this \nhttps://9.37.*./render?target=countSeries(currentBelow(collectd.graphite.processes.ps_state-paging,1)&format=json\nIt's still return [] rather than give me the desired 0. \nBut I can use fallbackSeries() to get it return to 0.\n. ",
    "Aldian-fr": "I just encountered a similar problem trying to install graphite with pip and solved it by downgrading pip the following way, then retrying pip install pip==1.2.1\n. ",
    "bigunyak": "Have just encountered the same problem.\nLooks like the problem has been known for at least 4 months now but the installation documentation under http://graphite.readthedocs.io/en/latest/install-pip.html still hasn't been fixed.\nI use pip-8.1.2 and it doesn't install graphite under /opt/graphite, using /usr/local/lib/python2.7/dist-packages/opt/graphite/ instead.\nWhat are my options here? Would using --install-option pip option solve this problem?\n. ",
    "Ajedi32": "It seems that installing from GitHub behaves as expected regardless of pip version even not on the master branch.\nFor example:\npip install https://github.com/graphite-project/graphite-web/tarball/0.9.15\nworks as expected, but \npip install graphite-web==0.9.15\ndoes not.\nSo it seems like this problem has not been fixed on master; the only reason it currently seems to work is that you're not installing the master branch from PyPi. Even once the next version of graphite-web is released, the problem will continue for anyone installing from PyPi. IMO this issue should be reopened.\nAnyone have any idea why pip behaves so differently based where it fetches the package from?\nEdit: Link to relevant question on Launchpad: https://answers.launchpad.net/graphite/+question/401544\n. It seems the problem isn't just related to where you got the tarball from (as in, the install mechanism used). The tarball on GitHub is actually different from the one hosted on PyPI! Compare the contents of https://github.com/graphite-project/graphite-web/tarball/0.9.15 with that of https://pypi.python.org/packages/b6/f3/7e4bae02f1a21cc29e9e9205bbc01aa29cdc6c696a996d41c1143e8935e3/graphite-web-0.9.15.tar.gz#md5=f4e80ba810fa83f57a62a2b8dd4e3545\n~~Even if both packages are moved to a different web server and installed with pip install <url>, the one from GitHub gets installed to /opt/graphite/webapp/graphite_web-0.9.15-py2.7.egg-info/ while the one from PyPi ends up in /usr/local/lib/python2.7/dist-packages/graphite_web-0.9.15.dist-info/.~~ (Edit: This is not the case, see my later comment below.)\nIt's still unclear as to what specific difference between the two files is causing this.. Actually, it looks like the contents don't really matter after all. While those two packages do have slightly different contents, whether they get installed to  /opt/graphite/webapp/graphite_web-0.9.15-py2.7.egg-info/ or /opt/graphite/webapp/graphite_web-0.9.15-py2.7.egg-info/ seems to depend entirely on the filename of the package when it gets installed by pip! :scream:\nI conducted 4 tests, each with a freshly-built, identically configured VM, using pip 8.1.1 from the official Ubuntu Xenial apt repository. In both cases where I named the file graphite-web-0.9.15.tar.gz and installed with pip install ./graphite-web-0.9.15.tar.gz (regardless of whether I used the file from GitHub or the file from PyPI), Graphite was installed to /usr/local/lib/python2.7/dist-packages/graphite_web-0.9.15.dist-info/ and I got this output:\nProcessing ./graphite-web-0.9.15.tar.gz\nBuilding wheels for collected packages: graphite-web\n  Running setup.py bdist_wheel for graphite-web ... done\n  Stored in directory: /root/.cache/pip/wheels/4c/f2/34/33e2da2971584766ab6c6f9d7cb2b7167b00d3ce9d9ef85104\nSuccessfully built graphite-web\nInstalling collected packages: graphite-web\nSuccessfully installed graphite-web-0.9.15\nYou are using pip version 8.1.1, however version 9.0.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nIn both cases where I named the file test.tar.gz and installed with pip install ./test.tar.gz (regardless of whether I used the file from GitHub or the file from PyPI), Graphite was installed to /opt/graphite/webapp/graphite_web-0.9.15-py2.7.egg-info/ and I got the following output:\nInstalling collected packages: graphite-web\n  Running setup.py install for graphite-web ... done\nSuccessfully installed graphite-web\nYou are using pip version 8.1.1, however version 9.0.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nSo... yeah. What the heck, pip?!?! :frowning: . After running a few more experiments, it looks like it always installs under /user/local/lib/ if I include the characters g, r, a, p, h, i, t, e, -, w, and b anywhere in the name of the file, in any order, otherwise it installs under /opt (seriously, what the heck pip?):\npip install $x | Installs to | Other notes\n------------ | ------------- | ---------\n./graphite-web.tar.gz | /user/local/lib/...\n./graphite-web | N/A | Fails with: \"Invalid requirement: './graphite-web'\\nIt looks like a path. Does it exist ?\" (Even though the file does, in fact, exist at that location. Maybe it was expecting a directory?)\n./graphite_web.tar.gz | /user/local/lib/...\n./graphite\\ web-0.9.15.tar.gz | /user/local/lib/...\n./graphitexweb.tar.gz | /opt/...\n./graphite.asdf | N/A | Fails with: \"Invalid requirement: './graphite.asdf'\\nIt looks like a path[...]\"\n./graphite.gz | N/A | Fails with: \"Invalid requirement[...]\"\n./graphite.tar | /opt/...\nhttp://server/0.9.15 | /opt/...\n./graphite-namehack-web-0.9.15.tar.gz | /user/local/lib/...\n./namehackgraphite-web-0.9.15.tar.gz | /user/local/lib/...\n./graphitename-hackweb-0.9.15.tar.gz | /user/local/lib/...\n./graphitexweb-0.9.15.tar.gz | /user/local/lib/...\n./x-webgraphite.tar.gz | /user/local/lib/...\n./x\\ webgraphite.tar.gz | /opt/...\n./web-hitegrap.tar.gz | /user/local/lib/...\n./grap-hitewb.tar.gz | /user/local/lib/...\npip 9.0.1 also appears to behave similarly.. @deniszh I'm dealing with some rather unique circumstances which make using the --no-binary flag especially difficult (see https://github.com/echocat/puppet-graphite/pull/309#issuecomment-261961088), so I'm exploring alternative options.. ",
    "jvanasco": "I'm deploying a new Ubuntu image next week -- I'll put this on my task list!\n. I think this seems to be working.\nglad to see pycairo replaced.  fwiw, these were my old build notes to patch pycairo to get around errors:\n```\ncd ~/build\nwget http://cairographics.org/releases/py2cairo-1.10.0.tar.bz2\nbzip2 -d py2cairo-1.10.0.tar.bz2\ntar -xf py2cairo-1.10.0.tar\ncd py2cairo-1.10.0\nwget http://cvsweb.netbsd.org/bsdweb.cgi/~checkout~/pkgsrc/graphics/py-cairo3/patches/patch-.waf3-1.6.4-e3c1e08604b18a10567cfcd2d02eb6e6_waflib_Tools_python.py?rev=1.1&content-type=text/plain&only_with_tag=MAIN\npatch -p3 .waf-1.6.3-3c3129a3ec8fb4a5bbc7ba3161463b22/waflib/Tools/python.py < patch-.waf3-1.6.4-e3c1e08604b18a10567cfcd2d02eb6e6_waflib_Tools_python.py\\?rev\\=1.1\n./waf configure --prefix=/opt/graphite/\n./waf build\n./waf install\n```\n. Sorry, I just realized where the issue happened as I got deeper into the (re)install.\nThat command needs scandir to run, which doesn't get installed via dependencies somehow.\ninstalling that, then running it, works -- and runs a handful of tasks that my method didn't pick up .  (and you've addressed that in #1871)\n. ",
    "dene14": "Any hint on this? Experiencing exactly the same issue on ubuntu 14.04.\nGetting this on the latest master for map/reduce functions:\nSun Dec 18 18:30:58 2016 :: Exception encountered in <POST https://graphite.bitposter.co/render>\nTraceback (most recent call last):\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/core/handlers/base.py\", line 147, in get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 115, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 8, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 29, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression, replacements)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 59, in evaluateTokens\n    return func(requestContext, *args, **kwargs)\n  File \"/opt/graphite/webapp/graphite/render/functions.py\", line 3181, in reduceSeries\n    metaSeries[key] = SeriesFunctions[reduceFunction](requestContext,*[[l] for l in metaSeries[key]])[0]\n  File \"/opt/graphite/webapp/graphite/render/functions.py\", line 581, in asPercent\n    normalize([seriesList])\n  File \"/opt/graphite/webapp/graphite/render/functions.py\", line 141, in normalize\n    step = reduce(lcm,[s.step for s in seriesList])\nAttributeError: 'NoneType' object has no attribute 'step'. Haven't tried this, however statement that now is failing was definitely working before... The reason it's started to fail - most probably kind of upgrade (graphite's itself or some of its dependencies). Unfortunately it's totally unclear from the error traceback what's wrong in the system... All other functions do work properly (I have quite complex constructions)...\nWhat I can say additionally according to uwsgi behaviour, request lasts in infinite loop causing 100% CPU usage per worker.\nI've tried to install HEAD of 0.9.x branch, functions don't work either, however I haven't noticed infinite loop problem. Traceback also different:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 115, in get_response\n    response = callback(request, *callback_args, **callback_kwargs)\n  File \"./graphite/render/views.py\", line 126, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"./graphite/render/evaluator.py\", line 10, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"./graphite/render/evaluator.py\", line 21, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression)\n  File \"./graphite/render/evaluator.py\", line 28, in evaluateTokens\n    args = [evaluateTokens(requestContext, arg) for arg in tokens.call.args]\n  File \"./graphite/render/evaluator.py\", line 21, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression)\n  File \"./graphite/render/evaluator.py\", line 27, in evaluateTokens\n    func = SeriesFunctions[tokens.call.func]\nKeyError: u'reduceSeries'. ",
    "jdbranham": "@obfuscurity \nI have the same issue; some of my metrics have only half of the required metric pair.  \nUsing the example -\nreduceSeries(mapSeries(servers..disk.,1),\"asPercent\",3,\"bytes_used\",\"total_bytes\")  \nIn my dataset, there may be a series with only \"total_bytes\".  \nWould there be a way to use the \"grep\" or another function to validate the existence of both sides, before passing to reduce?\nThanks!. ",
    "khoi-thinh": "@deniszh Thank you, i'll try it!\n. Ah, i tried to use 0.9.15  and saw this.\n[Optional] Unable to import the 'python-rrdtool' module, this is required for reading RRD.\n[Optional] Unable to import the 'whitenoise' module. This is useful for serving static file/\nAnd the same error \"ldap-python\"  and txamqp. Should i process? \n. ",
    "skilledmonster": "I also made sure to install all the required dependencies - \n```\n[root@server graphite]# ./check_dependency.py\n[OPTIONAL] Unable to import the 'python-rrdtool' module, this is required for reading RRD.\n[OPTIONAL] Unable to import the 'whitenoise' module. This is useful for serving static files.\n2 optional dependencies not met. Please consider the optional items before proceeding.\nAll necessary dependencies are met.\n```\n. ",
    "rlewkowicz": "Unable to import the 'python-rrdtool' module, this is required for reading RRD.\nThis is required for the rendering of rrd files. pip install python-rrd. You need a slew of deps related to cario. It's quite the trek to get them all. \n. For whatever reason, I had to modify the local settings. I been throwing a lot at the wall, so I don't quite know exactly what stuck. \nFirst I blew out the old db and redid my migration. Then I did this:\nSTATIC_ROOT = ''   <------ this is ' '\nSTATICFILES_DIRS = '/opt/graphite/static' <--- this is from my tom foolery with 0.10.0 \nRRD_DIR = '/opt/graphite/storage/rrd' (but I never set #DATA_DIRS = [WHISPER_DIR, RRD_DIR]) so that may have had no effect.\nin my local settings. \nI also reset my uWSGI to 2.1 flat \nWho knows. More reading indicates that perhaps django collects static files differently via the dev server. But there is no \"collectstatic\" under the manage.py for the 9.15 project there is it for 10.0. Ultimately I'm trying to use grafana with this platform so I don't care too much about the interfaces. \nI got it working, but I wouldn't mind just a hair of input to further clarify perhaps what I did to make it work or why it works that way. \n. ",
    "eddawley": "I ran into a similar issue installing graphite-web into a virtualenv on centos6.  Downgrading to django==1.8.15 fixes it for me.\n```\n[root@localhost vagrant]# /opt/graphite/bin/pip freeze\ncairocffi==0.7.2\ncarbon==0.9.15\ncffi==1.8.3\nDjango==1.9.10\ndjango-tagging==0.4.3\ngraphite-web==0.9.15\npycparser==2.14\npytz==2016.7\nTwisted==16.4.1\ntxAMQP==0.6.2\nuWSGI==2.0.14\nwhisper==0.9.15\nzope.interface==4.3.2\n[root@localhost vagrant]# curl \"http://127.0.0.1:80/render/\"\n\n\n\nGraphite encountered an unexpected error while handling your request.\nPlease contact your site administrator if the problem persists.\n\n\n\n\n\n\nTraceback (most recent call last):\n  File \"/opt/graphite/lib/python2.7/site-packages/django/core/handlers/base.py\", line 149, in get_response\n    response = self.process_exception_by_middleware(e, request)\n  File \"/opt/graphite/lib/python2.7/site-packages/django/core/handlers/base.py\", line 147, in get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 56, in renderView\n    (graphOptions, requestOptions) = parseOptions(request)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 236, in parseOptions\n    queryParams = request.REQUEST\nAttributeError: 'WSGIRequest' object has no attribute 'REQUEST'\n\n\n\n\n```\n. ",
    "howdoicomputer": "I'm getting this problem as well.. @obfuscurity Works fine with 1.4. Chef cookbook I was using was setting the Django version at 1.9.. Oh, shoot. This is the web project. Uh, should I file this in carbon?. Created issue here: https://github.com/graphite-project/carbon/issues/618. ",
    "charlesdunbar": "@cbowman0 - Thanks for the quick response!  I'll look into applying that patch.\n@deniszh - I think it's only carbon.agent.* metrics - happy to rename the issue for clarity.\n. Follow up question - is there any place to track when/if master gets released to a version?  Just noticed how long ago that patch was committed.  Is master what 0.10 is going to be, or always just bleeding edge?\n. ",
    "arielnh56": "or maybe pre-process the query to look for the need for a bootstrap and pull the correct data the first time?\n. this seems to apply whenever movingAverage is applies to a series that already has a function applied\nfor instance \nmovingAverage(services.real-time-bidder.pnc.*.Heap.heap.percent,11)\ngenerates two retrievals\nThu May 19 15:40:51 2016 :: Retrieval of services.real-time-bidder.pnc..Heap.heap.percent took 0.182778\nThu May 19 15:40:52 2016 :: Retrieval of services.real-time-bidder.pnc..Heap.heap.percent took 0.130155\nwhile \nmovingAverage(nonNegativeDerivative(services.real-time-bidder.pnc.*.Heap.heap.percent),11)\ngets the initial wildcard, then a hit for each node, and takes an age. As most of our interesting metrics are counters that reset to zero at daemon restart, not taking nonNegativeDerivative() on them first would generate silly numbers in a movingAverage\n. I dug deep and I think I get it now - and I think I see the fix - and I have tested it narrowly.\nIt appears to be an inconsistent way that TimeSeries.pathExpression is handled.\nthe field is initially set in fetchData as follows:\nseries.pathExpression = pathExpr #hack to pass expressions through to render functions\nsome functions (all of the combination functions) pass this uphill nicely.\nMany of the functions set it to be the same as 'name', after putting their own wrapper on name\nI tweaked one of these - nonNegativeDerivative() as follows and got the problem to go away. I still get the double-tap on the storage nodes, but only double, not 300X.\nas follows:\n```\nseries.name = \"nonNegativeDerivative(%s)\" % series.name\nseries.pathExpression = \"nonNegativeDerivative(%s)\" % series.pathExpression\nseries.pathExpression = series.name\n```\n. The more I dig into this, the more work it seems to be to fix. It appears that the intent of pathExpression is to be able to re-pull the data with different parameters (typically different time periods). Therefor it should be consistently updated in every function but it is not. \nit is initiated in fetchData, and then is rolled up through the stack of  functions.\nSome function e.g. sumSeries(), derive it from the pathExpression values in the received seriesList - those seem to work correctly.\nSome functions derive it from the name values (many just set it equal to name) in the received seriesList. As name can be populated with arbitrary strings, that's going to generate odd behavior in many cases and result in a pathExpression that cannot be used to re-pull the data.\nSome functions e.g. maxSeries() don't update it at all so the resulting pathExpression will no generate the original result if used.\nI see two approaches to fixing this:\n1) Fix all the functions to consistently update pathExpression as it walks back up the tree. This is a lot of code, and would take quite a bit of maintenance going forward..\n2) Top down approach. When a seriesList is passed to a function, pass the pathExpression that was used to generate it along with it. I'm not sure how to achieve this best (weak python foo), maybe a  tuple, or make seriesList a class? This would eliminate the need for every function to syntactically correctly update the pathExpression in each series. If it is a function that needs to re-piull the data, it can just pull the pathExpression out of its arguments.\nAm I completely off-base with this? It's entirely possible I am missing some design history on this one.\n. The more I dig into this, the more work it seems to be to fix. It appears that the intent of pathExpression is to be able to re-pull the data with different parameters (typically different time periods). Therefor it should be consistently updated in every function but it is not. \nit is initiated in fetchData, and then is rolled up through the stack of  functions.\nSome function e.g. sumSeries(), derive it from the pathExpression values in the received seriesList - those seem to work correctly.\nSome functions derive it from the name values (many just set it equal to name) in the received seriesList. As name can be populated with arbitrary strings, that's going to generate odd behavior in many cases and result in a pathExpression that cannot be used to re-pull the data.\nSome functions e.g. maxSeries() don't update it at all so the resulting pathExpression will no generate the original result if used.\nI see two approaches to fixing this:\n1) Fix all the functions to consistently update pathExpression as it walks back up the tree. This is a lot of code, and would take quite a bit of maintenance going forward..\n2) Top down approach. When a seriesList is passed to a function, pass the pathExpression that was used to generate it along with it. I'm not sure how to achieve this best (weak python foo), maybe a  tuple, or make seriesList a class? This would eliminate the need for every function to syntactically correctly update the pathExpression in each series. If it is a function that needs to re-piull the data, it can just pull the pathExpression out of its arguments.\nAm I completely off-base with this? It's entirely possible I am missing some design history on this one.\n. Argh. It occurs to me that the current behaviour has some advantages.\ne.g. maxSeries() does not need to add itself, if the series it is filtering are already split out with expanded pathExpressions to individual datapoints. If you time-shift you might get a different set of series from the maxSeries filter if you just used the original wildcard. Which would make that graph invalid.\nDang this is hard.\n. had fun digging and coding.\nfixed. filed pull request\nhttps://github.com/graphite-project/graphite-web/pull/1523\nThis is against 0.9.x as we are still on 0.9.15.\n. I would port to 10, but we're still on 0.9.x on our clusters, so I don't have a basis for testing. This is tested and running on our production clusters, found while figuring out why things were so slow.\nThis is also my first contrib, so I am unclear on the process for adding/updating tests. I ran these in parallel locally with a 'New\" suffix on the name, and eyeballed the output for comparison, and dug into some of the tabular data to be sure.\n. I figured the discontinuity could not be right, but frankly I don't know\nwhat holtWintersAberration output is supposed to be like.\nWhat I changed was the method of re-retrieving the data, which is required\nas the time period for the function starts prior to the output window. In\nthis case the pre-time is fixed at 7 days.\nIn our environment we have default\nretentions = 10s:2d,1m:7d,10m:2y\nSo at 7 days (168 hours) the data retrieved switches from 1m to 10m buckets.\nWhat I think was happening is that in the old method\n(_fetchWithBootstrap) for times under 7d is would pull an additional 7d\nfrom the 1m bucket and glop that on the beginning of the 1m data, resulting\nin a data series that was based on mixed bucket sizes. fetchWithBootstrap\ndid try to compensate for this by mapping the bootsrap data in some way\nthat probably worked for things like movingAverage but gave a radically\ndifferent result for holtWintersAberration\nSo in the case where the preview period extends across the retention\nboundary, the new method uses the data from the retention corresponding to\nthe older low-res bucket, while the old one tried to map the data from the\nlow-res bucket onto the hi-res bucket granularity.\nthere was this:\nif bootstrap.step != original.step:\n      ratio = bootstrap.step / original.step\n      for value in bootstrap:\n         #XXX For series with aggregationMethod = sum this should also\n         # divide by the ratio to bring counts to the same time unit\n         # ...but we have no way of knowing whether that's the case\n         newValues.extend([ value ] * ratio)\n     else:\n       newValues.extend(bootstrap)\nAnyhoo when I was going through the results of my underlying change I\nnoticed this change in the hWA results and when I dug in I saw this radical\nchange in the old output when I changed the time from -167h to -169h. One\nshould see an offset graph, not a different graph.\nSo if you are wanting holtWintersAberration at any given granularity, I\nthink if you increase your retention for the buckets to at least 7 days\nlonger than you want to view, you should get the results that are what you\nneed. holtWintersAberration is, apparently, sensitive to bucket size (or\naggregation method?)\nProbably the discrepancy I saw was because I was looking at random series\ndata which was set up with the default (average) aggregation method. There\nmay be a better aggregation method for data that wants to be looked at with\nhWA.\nOn Fri, Jun 9, 2017 at 5:22 AM, Markus Uckelmann notifications@github.com\nwrote:\n\nAhh, this explains the difference with 0.9.15. That leaves us currently\nwith either partially wrong data (0.9.15) or a timerange/graph that is too\nbig (0.9.16). I'll meditate on this for a while... ;)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1957#issuecomment-307374350,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AFGOQfyplVNhJJGeJJEVCh1ix5ziBwL7ks5sCTkWgaJpZM4N1Frc\n.\n. edit\n\nWhat I think was happening is that in the old method\n(_fetchWithBootstrap) for times under 7d is would pull an additional 7d\nfrom the 10m bucket and glop that on the beginning of the 1m data\nOn Fri, Jun 9, 2017 at 10:20 AM, Alastair Young alastair@redhunter.com\nwrote:\n\nI figured the discontinuity could not be right, but frankly I don't know\nwhat holtWintersAberration output is supposed to be like.\nWhat I changed was the method of re-retrieving the data, which is required\nas the time period for the function starts prior to the output window. In\nthis case the pre-time is fixed at 7 days.\nIn our environment we have default\nretentions = 10s:2d,1m:7d,10m:2y\nSo at 7 days (168 hours) the data retrieved switches from 1m to 10m\nbuckets.\nWhat I think was happening is that in the old method\n(_fetchWithBootstrap) for times under 7d is would pull an additional 7d\nfrom the 1m bucket and glop that on the beginning of the 1m data, resulting\nin a data series that was based on mixed bucket sizes. fetchWithBootstrap\ndid try to compensate for this by mapping the bootsrap data in some way\nthat probably worked for things like movingAverage but gave a radically\ndifferent result for holtWintersAberration\nSo in the case where the preview period extends across the retention\nboundary, the new method uses the data from the retention corresponding to\nthe older low-res bucket, while the old one tried to map the data from the\nlow-res bucket onto the hi-res bucket granularity.\nthere was this:\nif bootstrap.step != original.step:\n      ratio = bootstrap.step / original.step\n      for value in bootstrap:\n         #XXX For series with aggregationMethod = sum this should also\n         # divide by the ratio to bring counts to the same time unit\n         # ...but we have no way of knowing whether that's the case\n         newValues.extend([ value ] * ratio)\n     else:\n       newValues.extend(bootstrap)\nAnyhoo when I was going through the results of my underlying change I\nnoticed this change in the hWA results and when I dug in I saw this radical\nchange in the old output when I changed the time from -167h to -169h. One\nshould see an offset graph, not a different graph.\nSo if you are wanting holtWintersAberration at any given granularity, I\nthink if you increase your retention for the buckets to at least 7 days\nlonger than you want to view, you should get the results that are what you\nneed. holtWintersAberration is, apparently, sensitive to bucket size (or\naggregation method?)\nProbably the discrepancy I saw was because I was looking at random series\ndata which was set up with the default (average) aggregation method. There\nmay be a better aggregation method for data that wants to be looked at with\nhWA.\nOn Fri, Jun 9, 2017 at 5:22 AM, Markus Uckelmann <notifications@github.com\n\nwrote:\nAhh, this explains the difference with 0.9.15. That leaves us currently\nwith either partially wrong data (0.9.15) or a timerange/graph that is too\nbig (0.9.16). I'll meditate on this for a while... ;)\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1957#issuecomment-307374350,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AFGOQfyplVNhJJGeJJEVCh1ix5ziBwL7ks5sCTkWgaJpZM4N1Frc\n.\n\n\n. The difference between the old code and the new code is how the\npre-time-period data is retrieved. Some functions need data before the\nrequested period in order to calculate the first datapoints. e.g. if you\nhave a daily moving average, you need the data for one day prior to time\nperiod in order to calculate the\n\nThe old code used a routine called _fetchWithBootstrap to retrieve the\npre-time data. The problem I was fixing was that in our federated cluster\nthis occurs after the wildcard requests to the storage nodes are expanded.\nWhile the original data request could have been a single call to the back\nend, this \"go back for more\" was generating hundreds of calls on large\naggregate graphs leading to performance issues. Sometimes the graphs would\nnot render at all. My need came from movingAverage but I saw the same\ninefficient code was being used by the holt-winters funtions.\nThe new code passes the wildcard meta-data to to the function and it uses\nthe wildcard to retrieve the data once more with the earlier start point.\nIf the start point is now in a lower resolution time buck, all the data\nretrieved will be from the lower resolution bucket.\nThe holt-winters problem arises as follows:\n\nIt has a fixed pre-fetch of 7 days.\nFor recent-period graphs, this forces the pre-fetch to be pulled from\n   a non-recent bucket\n_fetchWithBootstrap tries to map the older data to the new by\n   reversing the aggregation process, assuming that sum() was the aggregation.\n   This is clunky and at best inaccurate.\nThis generated the discontinuity in behavior I observed at the bucket\n   boundaries.\n\nI think the fixed 7 day pre-fetch on the HWA is the outstanding problem\nhere. It implies that holt-winters needs 7 days prior data to build any\ngraph. I am not a statistician, and google doesn't immediately show up such\na link, so I'm thinking that that was an assumption/convenience for the\nauthor of these functions that does not apply to every case. How much prior\ndata does you 6-12 hour graph need? Ideally the function should set that as\nthe pre-fetch and the bucket period should extend to include the ask time\nand the pre-fetch. Your bucket period is 28h so that's fine. It's the 7\ndays that is forcing you into the low-res bucket\nI transcribed the 7-day prefetch to the new calls in\ndef holtWintersForecast(requestContext, seriesList):\npreviewSeconds = 7 * 86400 # 7 days\ndef holtWintersConfidenceBands(requestContext, seriesList, delta=3):\npreviewSeconds = 7 * 86400 # 7 days\nI think these values should not be constants. They should be calculated\nto match the request period.\nAs a quick hack for your problem, I would try dropping previewSeconds to 15\nhours (54000), this should allow your 12 hour requests with a 15 hour\npre-fetch all to come from your 28h:10s bucket.\nOn Mon, Jun 12, 2017 at 1:16 AM, Markus Uckelmann notifications@github.com\nwrote:\n\n@arielnh56 https://github.com/arielnh56 Thanks for your answer. But I'm\nstill a little bit confused. What is the behavior of your new code? We have\nconfigured a retention of 10s:28h, 1m:8d, 10m:750d. As I wrote earlier I\ncan get valid data only with -9 days. What I'd like is to view the last 6\nor 12 hours.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1957#issuecomment-307720498,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AFGOQT5YCvjZ2q6Y9Lt-miGP-iE_ZQiIks5sDPPAgaJpZM4N1Frc\n.\n. \n",
    "wonmo01": "Yes, I work at Pandora, and we are using graphite.\n. ",
    "fred777": "Hi! Seems I have something to add here....\nI'm running latest docker containers of hopsoft/graphite-statsd and grafana/grafana and collecting metrics via codahale graphite reporter which pushes data every 10s.\nGraphite seems to be at v0.9.15.\nNow, I'm having issues with different counter metrics, e.g. a \"Jobs Running\" counter which gets incremented when a batch job is scheduled and decremented when it's done.\nThis is my query: consolidateBy(forecastservice.service.batch.runningjobs.ConsumptionAveragerBatchJob.testforecast.count,\"max\")\nThe number of running jobs never exceeds 1, but as soon as the visualized time span gets too big, the displayed maximum is much higher.\n\n\nI've played around with consolidateBy min, max, avr. sum, but it doesn't make a difference, nor does leaving out consolidateBy.\nI bet that this has to do with the fact that I'm displaying a counter which somewhat get's summed up - the number of running jobs is a 1 for 60 seconds - hence there are 6 data points with counter-value 1.\nLooks like consolidateBy always sums a counter....\nIs there any workaround? (Of course, I could implement my own counter and report it as a gauge, but this would be my last resort...)\nOr has this been fixed in a later version of graphite? (Didn't see something in change logs). ",
    "dnayyl": "Hi bmhatfield, that is a very good point. Here is what I found in carbon.conf: The log_updates is false.\n```\nBy default, carbon-cache will log every whisper update. This can be excessive and\ndegrade performance if logging on the same volume as the whisper data is stored.\nLOG_UPDATES = False\nOn some systems it is desirable for whisper to write synchronously.\nSet this option to True if you'd like to try this. Basically it will\nshift the onus of buffering writes from the kernel into carbon's cache.\nWHISPER_AUTOFLUSH = False\nBy default new Whisper files are created pre-allocated with the data region\nfilled with zeros to prevent fragmentation and speed up contiguous reads and\nwrites (which are common). Enabling this option will cause Whisper to create\nthe file sparsely instead. Enabling this option may allow a large increase of\nMAX_CREATES_PER_MINUTE but may have longer term performance implications\ndepending on the underlying storage configuration.\nWHISPER_SPARSE_CREATE = False\nEnabling this option will cause Whisper to lock each Whisper file it writes\nto with an exclusive lock (LOCK_EX, see: man 2 flock). This is useful when\nmultiple carbon-cache daemons are writing to the same files\nWHISPER_LOCK_WRITES = False\nSet this to True to enable whitelisting and blacklisting of metrics in\nCONF_DIR/whitelist and CONF_DIR/blacklist. If the whitelist is missing or\nempty, all metrics will pass through\nUSE_WHITELIST = False\nBy default, carbon itself will log statistics (such as a count,\nmetricsReceived) with the top level prefix of 'carbon' at an interval of 60\nseconds. Set CARBON_METRIC_INTERVAL to 0 to disable instrumentation\nCARBON_METRIC_PREFIX = carbon\nCARBON_METRIC_INTERVAL = 60\n```\n. @obfuscurity Thanks for your advice, the problem is actually happening to be the weather station with 30min interval has its interval set to be 1800, as I do \"sudo wee_device --set-interval=300\" on the remote weather station, it changes interval to 5min interval.\n. ",
    "svenmueller": "Thx for the answer!\n. ",
    "suhussai": "@deniszh I deleted the old .pyc files and restarted the server, but I ended up getting the same error when I navigated to the server.\n. Thanks @deniszh! Updating django-tagging to 0.4.3 seems to have worked. I ran across a different error but I've gotten that resolved. I think the issue was resolved by installing Django version 1.8 instead of using version 1.9.7.\n. This issue is resolved.\nTicket is related to #1538 \n. ",
    "dpk1228": "thanks @suhussai , one solution for all !!! thanks.... ",
    "jbergler": "I'm not sure if this is the right way to port changes between branches in this project - if you'd prefer I can cherry-pick individual commits rather than the merge commit.\n. ",
    "vinhlh": "@deniszh so should we add this to the document?\ndifferent carbon caches should share single whisper storage directory\n. ",
    "genericgithubuser": "Some clarification would be helpful.  I've seen a few different setups which utilize a single server with separate caches going to separate directories.  The Standard_dirs seems to support this setup, though it does not appear to work (or work the way I understood it to).\nIn our case, we have servers able to run the multiple cache instances, but we can better spread the load across different disk groups if we have them hitting separate arrays, so ideally like the single node writing to different cache locations.  Of course, the display is important to us for user experience, so we will do the single write location if that's the only way to get the view to be \"clean\".\n. ",
    "brunorey": "For the record, this is how my actual use case is shown: (metrics name deleted)\n\nMy workaround was to just use the same storage for both metrics\n. ",
    "DaxDupont": "...Seriously?\n. ",
    "jpscaletti": "The term \"blacklist\" is not racial at all but come from book bounded in black used by Henry VIII to listing monasteries to be dissolved. \n. ",
    "bluecmd": "\nThe fact that we cannot aggregate percentiles in a way that is mathematically sound is a major reason why statsd aggregators should be deployed per-site and not per-host. \n\nTo me this sounds like you're trying add some pretty serious design limitations. It might be OK for some companies to have a central aggregation that is able to calculate these percentiles in a way that makes sense (it is after all much simpler conceptually), but as soon as you start to be multi-datacenter or want any other sort of distributed calculation it just falls apart.\nDoing any form of mathematical aggregation on a percentile and calling it a percentile in the end does not work. Period. Consider simply calculating the tail latencies for a service that is distributed among different datacenters. How would you do that? With bucketed distributions you can at least get the full picture and a bounded error (\"I know my 95th is inside this bucket\") and an hopefully acceptable approximation by something like linear interpolation.\nDisclaimer: Jetlagged, but hopefully this makes sense.\n. As @iksaif is saying \"Centralizing where your statistics aggregation happens\" is not a solution for all cases.\nIt may be a solution for the simple case, but not if you're doing a non trivial amount of metric updates (many or often). We already agreed that math is not allowing the centralized solution to be distributed, and hence does not scale. Distributions are more complex, but they do scale with known characteristics.\n. ",
    "stefanosala": "Oh, interesting, thanks a lot!\n. ",
    "nathanbower": "Thanks @obfuscurity ! Glad to hear there isn't a fixed limit.  Running the query again is now working...a bit perplexed because we don't know what changed.\nI've confirmed when misspelling some target names within an existing successful query that the api does seem robust to respond with known target values despite errant target names submitted.  Probably something we're missing that had broken our query.  I'll respond back if we identify the cause.\n. ",
    "Azef1": "I resolved myself , i modifed my query color(stacked(keepLastValue(a.b.available)), \"red\")\n. ",
    "klaffke": "Ahh, thanks for the information! It confuses me that when I follow the link to the documentation\nhttp://graphite.readthedocs.io/\nthat per default the documentation of an unreleased version is displayed. Usually such a link redirects to the latest stable release version instead.\nAnother idea would be to note the version number next to the descriptions of the funktions under /functions.html (e.g. \"since 0.10.0 \u2013 UNDER DEVELOPMENT\", \"since 0.9.15\")\n. ",
    "wjaspers": "as was reported to me this morning, the spike was the result of memcached not evicting keys as expected (or possibly caching something forever). someone did mention that an upgrade of graphite-web may have occurred and memcached was not restarted. the resulting changes to our request URI just so happened to produce a new image.\nsince this isn't necessarily a graphite-web issue, ill close this.\n. ## What you attempted\nFor both tests, lets assume now is 1:00 PM CDT.\nWithout a timezone parameter\nTrying another example URI .\nhttp://graphite/render?from=noon&until=now&target=\nThe rendered X axis 'noon' time is displayed as 7:00 AM CDT, and the now is displayed as 13:00. The latter I am expecting. The former, I am not.\nWith the UTC timezone parameter\nNow trying this with tz=utc\n(and lets assume \"now\" is 1:00 PM CDT).\nhttp://graphite/render?from=noon&until=now&tz=utc&target=\nThe rendered X axis 'noon' time is displayed as 12:00, and the now is displayed as 18:00.. ",
    "bemeyert": "We really could use this feature. We use the fnv1a_ch from Carbon-C-Relay to make our cluster easy to scale out, since we are not dependent on hostnames and ports.\n. @deniszh All other queries using that series work fine with from set to less than 9 days. E.g.\ngroupByNode(scaleToSeconds(series 1), 7, 'sum')\nholtWintersForecast(groupByNode(scaleToSeconds(series, 1), 7, 'sum'))\nholtWintersConfidenceBands(groupByNode(scaleToSeconds(series, 1), 7, 'sum'), 10)\nOnly Aberration is making problems.. I tried that. First the diff:\n```\ndiff /usr/lib/python2.7/site-packages/graphite/render/functions.py /usr/lib/python2.7/site-packages/graphite/render/functions.py.orig\n2078,2079c2078\n<       #elif upperBand[i] is not None and series[i] > upperBand[i]:\n<       elif series[i] > upperBand[i] :\n\n\n  elif upperBand[i] is not None and series[i] > upperBand[i]:\n\n2081,2082c2080\n<       #elif lowerBand[i] is not None and series[i] < lowerBand[i]:\n<       elif series[i] < lowerBand[i] :\n\n\n\n  elif lowerBand[i] is not None and series[i] < lowerBand[i]:\n\n``\nAnd the the request:http://metrics/render?&rawData=true&format=json&from=-2d&until=now&target=holtWintersAberration(groupByNode(scaleToSeconds(series,%201),%207,%20%27sum%27),%2010)Except the error message nothing has changed.. When looking withgit difftool 0.9.15 0.9.16 webapp/graphite/render/functions.pyI can see a lot of changes in theholtWintersConfidenceBandsfunction. Maybe the order or the structure of the returned data differs between the versions?. Ahh, this explains the difference with 0.9.15. That leaves us currently with either partially wrong data (0.9.15) or a timerange/graph that is too big (0.9.16). I'll meditate on this for a while... ;). @arielnh56 Thanks for your answer. But I'm still a little bit confused. What is the behavior of your new code? We have configured a retention of10s:28h, 1m:8d, 10m:750d. As I wrote earlier I can get valid data only with -9 days. What I'd like is to view the last 6 or 12 hours.. @arielnh56 Thanks for your very good explanation. I'm slowly grasping what's happening here.\n@deniszh It might be a good idea to makepreviewSeconds` variable. If it should be configurable I can't tell, since my statistic-foo is not good enough. I first want to dig through the article you posted before I decide where I go from here. I'll post here what came out of it ;)\nI want to thank you all for your time and effort this far.\n\nPeace ;). @deniszh I'll get back to you as soon as I can. We have a math guy in our company and I try to get him involved ;). @deniszh It would be great if we could set previewSeconds dynamically. We then could play around with ConfidenceBands and see what works best for our case. We want to see if there are drastic changes in a metric in the last minute.. Just one thing is still bugging me. I can use ConfidenceBands in any timeframe (-6h, -2d ...) and it works. But I get the error when I use Aberration. So, would it really fix our problem if previewSeconds is configurable?. @deniszh I think that seconds as unit would be better. I.e. we have our first aggregation at 28 hours and I don't want to calculate the fractional days for that. It's much too error prone.. @deniszh I think that seconds as unit would be better. I.e. we have our first aggregation at 28 hours and I don't want to calculate the fractional days for that. It's much too error prone.. @deniszh hours or seconds. I guess both would work. Although I would prefer seconds.. @deniszh Even better since one can choose. Thanks a lot! It should be easy to patch our current installation until a 1.x release is available for RHEL-based distributions.\nI will try out the patch in the course of next week and update this issue.. @deniszh It works now. While looking at the last hour we use the query holtWintersAberration(groupByNode(scaleToSeconds(metrics, 1), 7, 'sum'), 10, '26h'). Works like a charm. If there are enough data for Holt-Winters only time will tell ;)\nIn which releases will this patch appear?. So there will be no 0.9.x release anymore? Just asking. I'd like to use 1.0.x. So the sooner it hits EPEL the better ;). The backport is not necessary. The next RPM will be a 1.0.x release. So long the manual patch stays on the system. A big thank you for your help and excellent work!. ",
    "56quarters": "@deniszh Thanks for the feedback!\nI'll try to get some more numbers around latency vs overall time and CPU usage and then draft a PR!\n. Well, profiling consistently indicates that disabling the cache reduces the time spend parsing the expression, but benchmarks are really all over the place. I think your intuition about less CPU but not much effect on latency is on point.\nI guess I opened this issue prematurely. I'm going to try running  a branch of Graphite in production and get some more solid numbers. I'll open another issue if the testing turns up anything. Thanks for your help @deniszh!\n. ",
    "anirbanroydas": "Actually, this wsgi.py is what came with the source code. So I haven't done anything in that file. It's the same file. \nApart from that, I could see there is a python issue, where it is unable to find it, also can you put some light on why is this happening and what about the line where it says thread support is disabled?\n. ```\nThis file takes a single regular expression per line\nIf USE_WHITELIST is set to True in carbon.conf, only metrics received which\nmatch one of these expressions will be persisted. If this file is empty or\nmissing, all metrics will pass through.\nThis file is reloaded automatically when changes are made\n.*\n```\nThe above is the wsgi.py\nI am using python 2.7.6\ndjango 1.6.11\ngraphite i am doing a pip install as mentioned in the docs.\nLet me know if you need more info.\n. @JeanFred  : Can you give me a step by step starting from installation including all configuration and file creations.\nNOTE:  Give me the steps with the pip installation method.\nSystem: Ubuntu 14.04\nGraphite : 0.9.15\n. Also, what is this setting?\nPYTHONPATH=$GRAPHITE_ROOT/webapp django-admin.py syncdb --settings=graphite.settings\nAnd whenever I try this I get this following error?\nUnknown command: 'syncdb'\nType 'django-admin.py help' for usage.\nNOTE: I already made the changes regarding DATABASE in the local_settings.py before running the above command. I am trying to use postgres.\n. @obfuscurity  Yes my mistake with the wsgi.py. So, do I have to cp the graphite.wsgi.example as wsgi.py and put in /opt/graphite/conf directory or is it not required for setting up graphite with nginx?\n. @obfuscurity  I used this one http://graphite.readthedocs.io/en/0.9.15/install-pip.html \nAnd I made a wsgi.py file in the conf directory as a copy of graphite.wsgi.example\nAlso, @JeanFred  This is my current configs for the setup.\nMy wsgi.py file in the /opt/graphite/conf directory \n```\nimport os\n import sys\n sys.path.append('/opt/graphite/webapp')\ntry:\n     from importlib import import_module\n except ImportError:\n     from django.utils.importlib import import_module\nos.environ.setdefault('DJANGO_SETTINGS_MODULE', 'graphite.settings')  # noqa\nfrom django.conf import settings\n from django.core.wsgi import get_wsgi_application\n # from graphite.logger import log\napplication = get_wsgi_application()\ntry:\n     import whitenoise\n except ImportError:\n     whitenoise = False\n else:\n     # WhiteNoise < 2.0.1 does not support Python 2.6\n     if sys.version_info[:2] < (2, 7):\n         whitenoise_version = tuple(map(\n                 int, getattr(whitenoise, 'version', '0').split('.')))\n         if whitenoise_version < (2, 0, 1):\n             whitenoise = False\nif whitenoise:\n     from whitenoise.django import DjangoWhiteNoise\n     application = DjangoWhiteNoise(application)\n     prefix = \"/\".join((settings.URL_PREFIX.strip('/'), 'static'))\n     for directory in settings.STATICFILES_DIRS:\n         application.add_files(directory, prefix=prefix)\n     for app_path in settings.INSTALLED_APPS:\n         module = import_module(app_path)\n         directory = os.path.join(os.path.dirname(module.file), 'static')\n         if os.path.isdir(directory):\n             application.add_files(directory, prefix=prefix)\n# Initializing the search index can be very expensive. The import below\n # ensures the index is preloaded before any requests are handed to the\n # process.\n # log.info(\"graphite.wsgi - pid %d - reloading search index\" % os.getpid())\n import graphite.metrics.search  # noqa\n```\nmy uwsgi config, graphite.ini \n[uwsgi]\n vacuum = true\n master = true\n processes = 2\n socket = 127.0.0.1:3031\n gid = www-data\n uid = www-data\n # virtualenv = /opt/graphite\n chdir = /opt/graphite/conf\n module = wsgi:application\nmy nginx conf\n```\nserver {\n     listen 80 default_server;\n     listen [::]:80 default_server ipv6only=on;\n root /usr/share/nginx/html;\n index index.html index.htm;\n\n # Make site accessible from http://localhost/\n server_name localhost;\n\n  charset     utf-8;\n     # Django admin media.\n     location /media/admin/ {\n\n         alias /usr/lib/python2.7/site-packages/django/contrib/admin/media/;\n     }\n\n     # Your project's static media.\n     location /content/ {\n\n         alias /opt/graphite/webapp/content/;\n     }\n\n\n location /graphite {\n\n         uwsgi_pass  127.0.0.1:3031;\n         include     uwsgi_params;\n }\n\n\n\n location /grafana/ {\n     proxy_pass  http://localhost:3000/;\n     proxy_set_header Host $host;\n }\n\n}\n```\n. @obfuscurity @JeanFred \nAlso adding the uwsgi log file located at /var/log/uwsgi/apps/graphite.log\nFri Aug 26 18:37:56 2016 - SIGINT/SIGQUIT received...killing workers...\n Fri Aug 26 18:37:57 2016 - worker 1 buried after 1 seconds\n Fri Aug 26 18:37:57 2016 - worker 2 buried after 1 seconds\n Fri Aug 26 18:37:57 2016 - goodbye to uWSGI.\n Fri Aug 26 18:37:57 2016 - VACUUM: unix socket /run/uwsgi/app/graphite/socket removed.\n Fri Aug 26 13:07:57 2016 - *** Starting uWSGI 1.9.17.1-debian (64bit) on [Fri Aug 26 13:07:57 2016] ***\n Fri Aug 26 13:07:57 2016 - compiled with version: 4.8.2 on 23 March 2014 17:15:32\n Fri Aug 26 13:07:57 2016 - os: Linux-3.19.0-65-generic #73~14.04.1-Ubuntu SMP Wed Jun 29 21:05:22 UTC 2016\n Fri Aug 26 13:07:57 2016 - nodename: heineken\n Fri Aug 26 13:07:57 2016 - machine: x86_64\n Fri Aug 26 13:07:57 2016 - clock source: unix\n Fri Aug 26 13:07:57 2016 - pcre jit disabled\n Fri Aug 26 13:07:57 2016 - detected number of CPU cores: 4\n Fri Aug 26 13:07:57 2016 - current working directory: /\n Fri Aug 26 13:07:57 2016 - writing pidfile to /run/uwsgi/app/graphite/pid\n Fri Aug 26 13:07:57 2016 - detected binary path: /usr/bin/uwsgi-core\n Fri Aug 26 13:07:57 2016 - setgid() to 33\n Fri Aug 26 13:07:57 2016 - setuid() to 33\n Fri Aug 26 13:07:57 2016 - your processes number limit is 112504\n Fri Aug 26 13:07:57 2016 - your memory page size is 4096 bytes\n Fri Aug 26 13:07:57 2016 - detected max file descriptor number: 1024\n Fri Aug 26 13:07:57 2016 - lock engine: pthread robust mutexes\n Fri Aug 26 13:07:57 2016 - thunder lock: disabled (you can enable it with --thunder-lock)\n Fri Aug 26 13:07:57 2016 - uwsgi socket 0 bound to UNIX address /run/uwsgi/app/graphite/socket fd 3\n Fri Aug 26 13:07:57 2016 - uwsgi socket 1 bound to TCP address 127.0.0.1:3031 fd 5\n Fri Aug 26 13:07:57 2016 - Python version: 2.7.6 (default, Jun 22 2015, 18:01:27)  [GCC 4.8.2]\n Fri Aug 26 13:07:57 2016 - *** Python threads support is disabled. You can enable it with --enable-threads ***\n Fri Aug 26 13:07:57 2016 - Python main interpreter initialized at 0x1100a70\n Fri Aug 26 13:07:57 2016 - your server socket listen backlog is limited to 100 connections\n Fri Aug 26 13:07:57 2016 - your mercy for graceful operations on workers is 60 seconds\n Fri Aug 26 13:07:57 2016 - mapped 218376 bytes (213 KB) for 2 cores\n Fri Aug 26 13:07:57 2016 - *** Operational MODE: preforking ***\nTraceback (most recent call last):\n   File \"./wsgi.py\", line 16, in <module>\n     application = get_wsgi_application()\n   File \"/usr/local/lib/python2.7/dist-packages/django/core/wsgi.py\", line 13, in get_wsgi_application\n     django.setup()\n   File \"/usr/local/lib/python2.7/dist-packages/django/__init__.py\", line 18, in setup\n     apps.populate(settings.INSTALLED_APPS)\n   File \"/usr/local/lib/python2.7/dist-packages/django/apps/registry.py\", line 108, in populate\n     app_config.import_models(all_models)\n   File \"/usr/local/lib/python2.7/dist-packages/django/apps/config.py\", line 202, in import_models\n     self.models_module = import_module(models_module_name)\n   File \"/usr/lib/python2.7/importlib/__init__.py\", line 37, in import_module\n     __import__(name)\n   File \"/usr/lib/python2.7/dist-packages/graphite/events/models.py\", line 6, in <module>\n     from tagging.managers import ModelTaggedItemManager\n   File \"/usr/lib/python2.7/dist-packages/tagging/managers.py\", line 8, in <module>\n     from tagging.models import Tag, TaggedItem\n   File \"/usr/lib/python2.7/dist-packages/tagging/models.py\", line 10, in <module>\n     from django.contrib.contenttypes import generic\n ImportError: cannot import name generic\n Fri Aug 26 18:37:57 2016 - unable to load app 0 (mountpoint='') (callable not found or import error)\n Fri Aug 26 18:37:57 2016 - *** no app loaded. going in full dynamic mode ***\n Fri Aug 26 18:37:57 2016 - *** uWSGI is running in multiple interpreter mode ***\n Fri Aug 26 18:37:57 2016 - spawned uWSGI master process (pid: 9888)\n Fri Aug 26 18:37:57 2016 - spawned uWSGI worker 1 (pid: 9900, cores: 1)\n Fri Aug 26 18:37:57 2016 - spawned uWSGI worker 2 (pid: 9901, cores: 1)\n Fri Aug 26 18:38:15 2016 - --- no python application found, check your startup logs for errors ---\n [pid: 9901|app: -1|req: -1/1] 106.51.64.7 () {40 vars in 690 bytes} [Fri Aug 26 18:38:15 2016] GET /graphite => generated 21 bytes in 0 msecs (HTTP/1.1 500) 1 headers in 57 bytes   (0 switches on core 0)\n. @JeanFred. Okay. That would be great.\n@deniszh , yes, its 1.9, but do you thing the generic import issue is because of 1.9, I think it may appear it 1.8 too. \n. @deniszh What is the compatible version then? Also, I followed the 0.9.15 docs for installation, so it  led me to this.\nWhere else can I reach you guys? Need solutions quickly. IRC?\n. @deniszh  @JeanFred @obfuscurity \nTill now I was doing the pip installation, but now I tried the apt-get install.\nI followed this link http://arpitbhayani.me/techie/graphite.html almost except the nginx config.\nMy nginx config is : \n```\nserver {\n     listen 80 default_server;\n     listen [::]:80 default_server ipv6only=on;\n root /usr/share/nginx/html;\n index index.html index.htm;\n\nserver_name localhost;\n\nlocation / {\n     try_files $uri $uri/ =404;\n\n     auth_basic \"Restricted Content\";\n     auth_basic_user_file /etc/nginx/.htpasswd;\n }\n\n\n location /content {\n     alias /usr/share/graphite-web/static;\n     expires max;\n }\n\n\n location /graphite {\n         alias /usr/share/graphite-web;\n         proxy_set_header Host $http_host;\n         uwsgi_pass  unix:/var/run/graphite.sock;\n         include     uwsgi_params;\n }\n\n}\n```\nNow although nginx, uwsgi, carbon-cache all seem to start properly, but I am now getting the following error, Since I made my Debug=True.\nImportError at /graphite\ncannot import name Col\nRequest Method: GET\nRequest URL:    http://<my_server_domain>/graphite\nDjango Version: 1.6.11\nException Type: ImportError\nException Value:    \ncannot import name Col\nException Location: /usr/local/lib/python2.7/dist-packages/django/utils/importlib.py in import_module, line 40\nPython Executable:  /usr/local/bin/uwsgi\nPython Version: 2.7.6\nPython Path:    \n['/usr/lib/python2.7/dist-packages',\n '/usr/share/graphite-web/',\n '.',\n '',\n '/usr/share/graphite-web',\n '/usr/lib/python2.7',\n '/usr/lib/python2.7/plat-x86_64-linux-gnu',\n '/usr/lib/python2.7/lib-tk',\n '/usr/lib/python2.7/lib-old',\n '/usr/lib/python2.7/lib-dynload',\n '/usr/local/lib/python2.7/dist-packages',\n '/usr/lib/python2.7/dist-packages',\n '/etc/graphite',\n '/usr/lib/python2.7/dist-packages/graphite/thirdparty']\nServer time:    Sun, 28 Aug 2016 02:24:12 +0530\nYou can check my python and django version from the above error itself.\nOther info:\nGraphite : 0.9.12\nSystem : Ubuntu 14.04\nI installaed django 1.8, then there were errors, I changed to 1.7,  old errors gone, but new errors came, I changed to 1.6 and finally 1.6.11.\nWhat is going on here? I don't want to invest too much time in Django. Please tell me.\n. @deniszh  I ran the version mentioned in the link I provided earlier since I installed it via apt-get.\nthe command I ran is : \nsudo graphite-manage syncdb\nPlease have a look at the installation from the link. I followed each and every step except the nginx configuration.\n. @deniszh @JeanFred @obfuscurity \nNow I uninstalled cleanly, and reinstalled Django 1.6. Now its not even showing the Django Debug page unlike earlier when it was showing, now its showing 404 error.\nI have followed the installation from the same link again - https://github.com/graphite-project/graphite-web/issues/url\nI uninstalled many elements cleanly and reinstalled them.\ngraphite = 0.9.12 ( this installed graphite-web, graphite-carbon, whisper all version = 0.9.12)\nDjango = 1.6\nTwisted = 13.2.0\ndjango-tagging = 0.3.6\ncarbonate = 0.2.2\nceres = 0.10.0\nuwsgi = 2.0.13.1\npostgres = 9.3.13\nMy nginx config is still the same as above:\n```\nserver {\n     listen 80 default_server;\n     listen [::]:80 default_server ipv6only=on;\n root /usr/share/nginx/html;\n index index.html index.htm;\n\nserver_name localhost;\n\nlocation / {\n     try_files $uri $uri/ =404;\n\n     auth_basic \"Restricted Content\";\n     auth_basic_user_file /etc/nginx/.htpasswd;\n }\n\n\n location /content {\n     alias /usr/share/graphite-web/static;\n     expires max;\n }\n\n\n location /graphite {\n         alias /usr/share/graphite-web;\n         proxy_set_header Host $http_host;\n         uwsgi_pass  unix:/var/run/graphite.sock;\n         include     uwsgi_params;\n }\n\n}\n```\nMy custom uwsgi init script is same as the one mentioned in the link I mentioned.\nMy nginx access.log (generalizing few confidential info)\n<ip-address> - - [28/Aug/2016:12:38:33 +0000] \"GET / HTTP/1.1\" 401 605 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/source_ip-address  Safari/537.36\"\n <ip-address> - <my_user_name> [28/Aug/2016:12:38:42 +0000] \"GET / HTTP/1.1\" 200 419 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko)    Chrome/source_ip-address Safari/537.36\"\n <ip-address> - <my_user_name> [28/Aug/2016:12:38:48 +0000] \"GET /graphite/ HTTP/1.1\" 200 542 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like  Gecko) Chrome/source_ip-address Safari/537.36\"\n <ip-address> - <my_user_name> [28/Aug/2016:12:38:48 +0000] \"GET /browser/header/ HTTP/1.1\" 404 208 \"http://my_public_domain/graphite/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X       10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/source_ip-address Safari/537.36\"\n <ip-address> - <my_user_name> [28/Aug/2016:12:38:48 +0000] \"GET /composer/? HTTP/1.1\" 404 208 \"http://my_public_domain/graphite/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4)   AppleWebKit/537.36 (KHTML, like Gecko) Chrome/source_ip-address Safari/537.36\"\n <ip-address> - <my_user_name> [28/Aug/2016:12:38:51 +0000] \"GET /graphite HTTP/1.1\" 200 542 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like   Gecko) Chrome/source_ip-address Safari/537.36\"\n <ip-address> - <my_user_name> [28/Aug/2016:12:38:51 +0000] \"GET /browser/header/ HTTP/1.1\" 404 208 \"http://my_public_domain/graphite\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X        10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/source_ip-address Safari/537.36\"\n <ip-address> - <my_user_name> [28/Aug/2016:12:38:51 +0000] \"GET /composer/? HTTP/1.1\" 404 208 \"http://my_public_domain/graphite\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4)    AppleWebKit/537.36 (KHTML, like Gecko) Chrome/source_ip-address Safari/537.36\"\nMy Nginx error.log has no info regarding this, so not updating that.\nAnd this is my graphite.log file in /var/log/graphite.log (its configured as the uwsgi log file, if you follow the installation link then you will understand).\n*** Starting uWSGI 2.0.13.1 (64bit) on [Sun Aug 28 12:36:23 2016] ***\n compiled with version: 4.8.4 on 27 August 2016 19:25:41\n os: Linux-3.19.0-65-generic #73~14.04.1-Ubuntu SMP Wed Jun 29 21:05:22 UTC 2016\n nodename: heineken\n machine: x86_64\n clock source: unix\n detected number of CPU cores: 4\n current working directory: /\n detected binary path: /usr/local/bin/uwsgi\n !!! no internal routing support, rebuild with pcre support !!!\n uWSGI running as root, you can use --uid/--gid/--chroot options\n *** WARNING: you are running uWSGI as root !!! (use the --uid flag) ***\n your processes number limit is 112504\n your memory page size is 4096 bytes\n detected max file descriptor number: 1024\n lock engine: pthread robust mutexes\n thunder lock: disabled (you can enable it with --thunder-lock)\n uwsgi socket 0 bound to UNIX address /var/run/graphite.sock fd 3\n Python version: 2.7.6 (default, Jun 22 2015, 18:01:27)  [GCC 4.8.2]\n *** Python threads support is disabled. You can enable it with --enable-threads ***\n Python main interpreter initialized at 0x1f71af0\n your server socket listen backlog is limited to 100 connections\n your mercy for graceful operations on workers is 60 seconds\n mapped 507200 bytes (495 KB) for 4 cores\n *** Operational MODE: preforking ***\n added /usr/share/graphite-web/ to pythonpath.\n WSGI app 0 (mountpoint='') ready in 1 seconds on interpreter 0x1f71af0 pid: 45501 (default app)\n *** uWSGI is running in multiple interpreter mode ***\n spawned uWSGI master process (pid: 45501)\n spawned uWSGI worker 1 (pid: 45509, cores: 1)\n spawned uWSGI worker 2 (pid: 45510, cores: 1)\n spawned uWSGI worker 3 (pid: 45511, cores: 1)\n spawned uWSGI worker 4 (pid: 45512, cores: 1)\n [pid: 45510|app: 0|req: 1/1] 122.167.241.239 () {42 vars in 762 bytes} [Sun Aug 28 18:08:48 2016] GET /graphite/ => generated 542 bytes in 29 msecs (HTTP/1.1 200) 4 headers in 135  bytes (1 switches on core 0)\n [pid: 45509|app: 0|req: 1/2] 122.167.241.239 () {42 vars in 760 bytes} [Sun Aug 28 18:08:51 2016] GET /graphite => generated 542 bytes in 17 msecs (HTTP/1.1 200) 4 headers in 135   bytes (1 switches on core 0)\nNOTE: please have a look at the installation link, so that you can be in sync with what I have tried so far.\nNow tell me where is the problem here? Is is with my nginx config, or anything else going on here? Or is it some python or django or any dependency issue?\nReally need to get this over with? Need graphite big time. But unfortunately stuck since 3-4 days.\n. This issue got solved, I made two changes.\nOne in the custom graphite_wsgi.py in /usr/share/graphite-web/ , I commented the import graphite.metrics.search  line. And in my nginx config, I did a direct proxy pass like \n```\nlocation / {\n       // proxy pass   directive\n   } \ninstead of indirect\nlocation /graphite {\n       // proxy pass directives\n}\n```\nThis is the only way I could resolve this. I am closing the issue now.\n. ",
    "hostingwalk": "CARBONLINK_HOSTS = [\"127.0.0.1:7002:a\"]\nCARBONLINK_TIMEOUT = 1.0\ni have set it like this, or can this maybe the problem:\n[carbon]\npattern = ^carbon\\.\nretentions = 1s:1d,1m:7d,10m:1y\nHere a screenshot from Graphite:\n\n. - I dont not have a  [cache]  you have a example for that ?\n[servers]\npattern = ^servers.*\nretentions = 1s:1d,1m:7d,10m:1y,5m:1d\n-  i cannot find anything for lineMode=connected  local_settings.py  it is not here.\n. Here a pastebin of  /etc/carbon/carbon.conf   http://pastebin.com/N96aFUQh\n. The problem was the sending clients(servers) yeah, In Diamond.conf:\n # Default Poll Interval (seconds)\ni# nterval = 300\ni uncommented # and change it to 10 or 30 and now it works yeah, thank you!\n. ",
    "iliapolo": "This issue is also present in the master branch.\n. Cool, ill do that shortly.\n. @obfuscurity thanks for the quick merge! :)\n@deniszh regarding the PR against master, should i create a corresponding issue? or just edit #1671 and reference it?\n. @deniszh can you maybe explain why the codecov/patch check failed? what should i do?\n. Fair enough, ignored :)\n. @deniszh why do you say its legacy? isn't 0.9.15 the latest stable version?\n. :+1: \n. ",
    "dantate": "yeah, the graphite-web.conf is the apache conf.d file.  strange it's not reporting an error, it seems like thatd be something obvious to log.\nhere's a snippit.  all i did was change 80 to 81.\n(angle brackets removed due to render error) \nVirtualHost :81\n    ServerName graphite-web\n    DocumentRoot \"/usr/share/graphite/webapp\"\n    ErrorLog /var/log/httpd/graphite-web-error.log\n    CustomLog /var/log/httpd/graphite-web-access.log common\n. No such file on CentOS.  going to give up on it; too many problems.   It keeps misidentifying my system as an old DNS name anyway, that I can find no trace of.\n. thanks though\n. this is actually part of httpd.conf via the include conf.d directive; so it's all the same.  Unfortunately I have other apps that run on the server, so having it monoplize :80 is unacceptable.\n. Yeah, but the problem is graphite fails to load (without an error message) if the port is changed to anything but 80.  That makes it oddly suspicious.\n. Just found this:\n[debug] util_ldap.c(2089): LDAP merging Shared Cache conf: shm=0x7f9fc1b460b8 rmm=0x7f9fc1b46110 for VHOST: monitor.pallino.com\nwhich is odd, since I don't use LDAP\n. ",
    "jonkerj": "Ok, somehow I managed to miss these functions. A big thanks, these fix my problem. On the risk of being pendant: I still think the name of this parameter is misleading, as it only adjust pure counter wraps, and does not enforce a maximum value. But this may be a matter of taste/opinion :-)\n. ",
    "ellipses": "Ha, oops looks like I messed up the rebase, but yea sure. Think I'll just create a new pr :). ",
    "garep": "Thanks!\n. @obfuscurity we have the following setting:\nMAX_CACHE_SIZE = inf \nStill seeing this issue.\n. Got it, here is our entire config\n```\ndefault[\"ges-graphite\"][\"carbonrelay_role\"] = \"carbonrelay\"\ndefault[\"ges-graphite\"][\"datanode_role\"] = \"carboncache\"\ndefault[\"ges-graphite\"][\"webapi_role\"] = \"graphiteweb\"\ndefault[\"graphite\"][\"install_type\"] = \"package\"\ndefault[\"graphite\"][\"version\"] = \"0.9.15\"\ndefault[\"graphite\"][\"twisted_version\"] = \"11.1\"\ndefault[\"graphite\"][\"user\"]         = \"graphite\"\ndefault[\"graphite\"][\"group\"]        = \"www-data\"\ndefault[\"graphite\"][\"user_account\"] = \"graphite\"\ndefault[\"graphite\"][\"group_account\"] = \"www-data\"\ndefault[\"graphite\"][\"base_dir\"]     = \"/opt/graphite\"\ndefault[\"graphite\"][\"doc_dir\"]      = \"/opt/graphite/webapp\"\ndefault[\"graphite\"][\"storage_dir\"]  = \"/opt/graphite/storage\"\ndefault[\"graphite\"][\"listen_address\"]  = \"0.0.0.0\"\ndefault[\"graphite\"][\"listen_port\"]  = 80\ndefault[\"graphite\"][\"ssl\"][\"enabled\"] = false\ndefault[\"graphite\"][\"uwsgi\"][\"service_type\"] = 'runit'\ndefault[\"graphite\"][\"uwsgi\"][\"socket_permissions\"] = 777\ndefault[\"graphite\"][\"uwsgi\"][\"port\"] = 8080\ndefault[\"graphite\"][\"web\"][\"memcached_hosts\"] = []\ndefault[\"graphite\"][\"carbon_cache\"][\"default\"][\"config\"][\"pickle_receiver_port\"] = 2004\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"enable_logrotation\"] = true\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"max_cache_size\"] = \"inf\"\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"max_updates_per_second\"] = 500\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"max_creates_per_minute\"] = 50\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"line_receiver_interface\"] = \"127.0.0.1\"\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"line_receiver_port\"] = 2023\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"udp_receiver_interface\"] = \"127.0.0.1\"\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"udp_receiver_port\"] = 2023\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"pickle_receiver_interface\"] = \"127.0.0.1\"\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"pickle_receiver_port\"] = 2024\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"enable_udp_listener\"] = true\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"cache_query_port\"] = 7022\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"cache_write_strategy\"] = \"sorted\"\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"use_flow_control\"] = true\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"log_updates\"] = false\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"log_cache_hits\"] = false\ndefault[\"graphite\"][\"carbon_cache\"][\"a\"][\"config\"][\"whisper_autoflush\"] = false\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"enable_logrotation\"] = true\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"max_cache_size\"] = \"inf\"\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"max_updates_per_second\"] = 500\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"max_creates_per_minute\"] = 50\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"line_receiver_interface\"] = \"127.0.0.1\"\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"line_receiver_port\"] = 2033\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"udp_receiver_interface\"] = \"127.0.0.1\"\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"udp_receiver_port\"] = 2033\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"pickle_receiver_interface\"] = \"127.0.0.1\"\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"pickle_receiver_port\"] = 2034\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"enable_udp_listener\"] = true\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"cache_query_port\"] = 7032\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"cache_write_strategy\"] = \"sorted\"\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"use_flow_control\"] = true\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"log_updates\"] = false\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"log_cache_hits\"] = false\ndefault[\"graphite\"][\"carbon_cache\"][\"b\"][\"config\"][\"whisper_autoflush\"] = false\ndefault[\"graphite\"][\"carbon_relay\"][\"config\"][\"line_receiver_interface\"] = \"0.0.0.0\"\ndefault[\"graphite\"][\"carbon_relay\"][\"config\"][\"line_receiver_port\"] = 2003\ndefault[\"graphite\"][\"carbon_relay\"][\"config\"][\"pickle_receiver_interface\"] = \"0.0.0.0\"\ndefault[\"graphite\"][\"carbon_relay\"][\"config\"][\"pickle_receiver_port\"] = 2004\ndefault[\"graphite\"][\"carbon_relay\"][\"config\"][\"relay_method\"] = \"consistent-hashing\"\ndefault[\"graphite\"][\"carbon_relay\"][\"config\"][\"replication_factor\"] = 3\ndefault[\"graphite\"][\"carbon_relay\"][\"config\"][\"pattern\"] = \".*\"\ndefault[\"graphite\"][\"carbon_relay\"][\"config\"][\"max_queue_size\"] = 3000000\ndefault[\"graphite\"][\"carbon_servers\"] = []\ndefault[\"graphite\"][\"cluster_servers\"] = []\ndefault[\"graphite\"][\"carbonlink_hosts\"] = []\ndefault[\"graphite\"][\"databases\"] = {\n    'default' => {\n        'NAME' => '/opt/graphite/storage/graphite.db',\n        'ENGINE' => 'django.db.backends.sqlite3',\n        'USER' => '',\n        'PASSWORD' => '',\n        'HOST' => '',\n        'PORT' => ''\n    }\n}\n```\nEverything ok here?\n. @obfuscurity thanks, will take a look at those params. Appreciate the quick response!\n. ",
    "strajansebastian": "Added a link to the URL for more info, as suggested. The graphite feature can be seen as an embedded Graphios script into the icinga2 software that automatically feeds performance data to the carbon daemon.\n. ",
    "Cka3o4Huk": "I've faced issue with sortByMaxima() too, server went down due to lack of RAM. \n. ",
    "reyjrar": "Can confirm this with the upgrade to master, I pulled yesterday to ed04b23.  The only difference being 0.9.15 to master.  Render times are noticeably through the roof.  I didn't change the config and had CLUSTER_SERVERS enabled in the past to merge results.  I'll have a graph of the performance degradation soon, but it's with all queries, not just sortByMaxima.  I can produce the results with a single metric with no transforms.\nActually, I lied, I didn't have request timing in Apache enabled, nor did I have render timing enabled in graphite web. \nHere's a gist with request timing on master\n. @deniszh REMOTE_STORE_MERGE_RESULTS = False had zero effect\nHere's render times with CLUSTER_SERVERS commented out:\n\nThu Sep 15 14:22:48 2016 :: Total rendering time 0.161674 seconds\nThu Sep 15 14:22:50 2016 :: Total rendering time 0.169428 seconds\n\nAdding it back in with REMOTE_STORE_MERGE_RESULTS = False\n\nThu Sep 15 14:23:39 2016 :: Total rendering time 4.806623 seconds\nThu Sep 15 14:23:48 2016 :: Total rendering time 4.845406 seconds\n\nAlso, for posterity, timing when REMOTE_STORE_MERGE_RESULTS = True\n\nThu Sep 15 14:27:02 2016 :: Total rendering time 4.854576 seconds\nThu Sep 15 14:27:15 2016 :: Total rendering time 4.806077 seconds\n. FWIW, I just git reset --hard 6e6c77abd2fad247d97b6a0adce8b595d2f7daba to rule out @redbaron 's patch.  The bug exists prior to that commit.\n. My setup: 8 carbon-cache instances per node, 4 nodes.  carbon-c-relays in front doing aggregation and sending to all 8 carbon-caches.  Graphite-web is configured to talk to the same 8 backend carbon-caches.\n\nRe-reading this thread, it sounds like I need to spin up extra cache instances for graphite-web?\nWhat's weird is, if the problem existed on 0.9.15, it was not as pronounced.  IE, I updated earlier this week and we instantly began to see timeouts on grafana dashaboards communicating to the graphite-web servers.  This is why I chirped up here. There may be existing deadlocks, but they were rare in my 0.9.15 setup.  They are now common place building off master.\n. Sure thing, I'll test tomorrow morning.\n. Further testing based on slack call with @obfuscurity : \nhttps://gist.github.com/reyjrar/4c0ad075a528d4d68e14b8b1427a99be\nSetup is 2 local nodes in each DC, 2 DC's.\n. Testing the effect of CLUSTER_SERVERS recursively\n```\nFront with CLUSTER_SERVERS, Backend w/o\nThu Oct 06 14:39:07 2016 :: Retrieval of carbon.agents.*.updateOperations took 0.116185\nThu Oct 06 14:39:07 2016 :: Rendered PNG in 0.275612 seconds\nThu Oct 06 14:39:07 2016 :: Total rendering time 0.393449 seconds\n```\n```\nFront and Back ends with CLUSTER_SERVERS set to eachother's IPs\nThu Oct 06 14:37:25 2016 :: Retrieval of carbon.agents.*.updateOperations took 0.059602\nThu Oct 06 14:37:26 2016 :: Rendered PNG in 0.266713 seconds\nThu Oct 06 14:37:26 2016 :: Total rendering time 0.327662 seconds\n```\nThese results weren't statistically significant.\n. I'm wondering if there's an optimization to be done, ie, keep-alive or connection pooling?  I can only venture a guess that the whole TCP setup/teardown to the CLUSTER_SERVERS is happening before the results are returned and/or merged?\n. So, I noticed this performance hit is real in /metrics/find as well.  So I diverted my resources there.  I think, and I might be wrong, that the caching in remote_storage.py is broken.  I am never seeing a cached result.\nI don't have MEMCACHED_HOSTS set, so I noticed that my CACHES dict is empty.  I am attempting to override that local_settings.py with:\nCACHES = {\n    'default': {\n        'BACKEND': 'django.core.cache.backends.db.DatabaseCache',\n        'LOCATION': 'caching',\n    }\n}\nThen I ran:\ndjango-admin.py --pythopath=<WEBAPP> --settings=graphite.settings createcachetable caching\nThis created the caching table in my database.  But after several minutes of making requests, I'm not seeing anything in the caching table of my database.\nUPDATE: None issue, please ignore, caching is irrelevant and I did not import the django core settings.  Once I enabled that cache setting, caching worked, but again this isn't the heart of the issue.\n. Would also like to confirm, I'm updated to 1.1.2 and recently moved to using clustering over HTTP and have the same issue.  1.x and onward is dramatic performance degradation.  I even tried adding memcached caches in to relieve some of the slowness, but that only works for queries in the caches.\nI'll post logs soon from the render on the graphite-web nodes talking directly to carbon-cache's over carbonlink and of the \"front end\" nodes I have communicating to the backend servers via CLUSTER_SERVERS.. Side note: Apache is serving the static content, why bother with this check at all?\n. Solved:\nFrom 0.9.15 -> 0.10.0 the WEBAPP_DIR setting needs to be set for this to work, the automatic lookup doesn't work.\n. I'm building for RedHat, and package the webapp bits into /usr/share/graphite.  Previously, that meant setting CONTENT_DIR=/usr/share/graphite/webapp/content.\nThere's some logic in settings.py to figure out where WEBAPP_DIR is, and that is used to determine where to look for static content (which again, I don't understand why that's even necessary).\nWhen using the standard RH model, settings.py is installed in %{pythong_sitelib}/graphite/settings.py, but the web app lands in the /usr/share/graphite path.  So, it's necessary for users packaging according to this setup: https://github.com/dcarley/graphite-rpms  (fwiw, the first hit for \"graphite rpm\" on teh Googles) to manually set WEBAPP_DIR to /usr/share/graphite/webapp.\nI think it's a documentation issue, which, btw, when I have some time, I'm going to submit a patch for because the upgrade process for me has been \"Read the source code to figure out which switches to try flipping\" :)\n. This may help dramatically with #1690.  I'm going to build this week and report my findings.. Was looking for a place to converse, but I'm seeing another major performance degradation on queries with large numbers of metrics.  In older 0.9.15 versions of graphite, I could get a dataset of 10,000 metrics to return in under 20 seconds.. Now in 1.0.2, that same query is taking upwards of 90 seconds.  Group, filtering, and combining functions also fail because the fetch for the data takes too long.\nI'm stuck on pre-1.x.x because most of queries select data from thousands of metrics and display the most interesting ones.  I think this is related to the OP's bug.. I was experiencing a similar issue and upgraded to 1.1.1 and the issue is resolved now.. OK, will rebuild with 1.1.x branch. Do I need both carbon and graphite-web from 1.1.x? I just pulled 1.1.x, rebuilt, and restarted the web server and am still seeing what looks like cache misses, e.g., the graphs look the same (caching is disabled during the test). So I just stopped the carbon 1.1.1 daemon, and it exited immediately.. all the in-memory statistics were lost, is that what you mean by \"issues\" ?. Have half the cluster running on the new 1.1.x branch.. it's a slow process because I need to stop the carbon-c-relay in front of the caches since they flush correctly and then wait for the carbon 1.1.1 daemons to flush their metrics to disk before I restart the daemons.\nIt's encouraging so far and I'll have a report later when I have the whole cluster upgraded.. ",
    "OrangeDog": "If #1768 is merged, this is fixed now, right?. > involve tons of research\nBetter that one person does it and puts it in the docs, rather than every user has to do it themselves.\nIf your git history is already clean enough, it could be done automagically with git blame and tags?. ",
    "stan-sack": "@iksaif @deniszh this is still definitely an issue. I just spent around 3 weeks digging through render logs and trying different version/cluster configurations after upgrading a 0.9.15 installation to 1.1.2, and finding a relatively large expression with 2 wildcards (approx 1000 metrics) began to time out. \nI set up 3 different graphite-web instances fetching from the same 3 node carbonlink cluster over http. The expression would render in around 50 seconds on 0.9.15, 220 seconds on 1.0.2, and take over 600 (timeout here) seconds on 1.1.2. This was on cold start with no cache and version 0.9.15 running on the carbonlink cluster nodes.. ",
    "ksaihtam": "I mean this option is only for the graph and not for the x axis labels. It doesn't change anything except the graph.\n. ",
    "yarons": "Have you found the solution?. ",
    "shammy12": "For anyone else running into this, try running it without the PYTHONPATH variable to make sure it isn't an issue with the PATH. Find the settings file location and run from there e.g. cd /opt/graphite/webapp/graphite\nthen\ndjango-admin.py migrate --settings=graphite.settings --run-syncdb\n. ",
    "makarczyk": "good call shammy12.. ",
    "saifromeo": "I still get the following error\nubuntu@ip-172-31-27-43:/opt/graphite/webapp/graphite$ django-admin.py migrate --settings=graphic.settings --run-syncdb\nTraceback (most recent call last):\n  File \"/usr/local/bin/django-admin.py\", line 5, in \n    management.execute_from_command_line()\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/init.py\", line 353, in execute_from_command_line\n    utility.execute()\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/init.py\", line 302, in execute\n    settings.INSTALLED_APPS\n  File \"/usr/local/lib/python2.7/dist-packages/django/conf/init.py\", line 55, in getattr\n    self._setup(name)\n  File \"/usr/local/lib/python2.7/dist-packages/django/conf/init.py\", line 43, in _setup\n    self._wrapped = Settings(settings_module)\n  File \"/usr/local/lib/python2.7/dist-packages/django/conf/init.py\", line 99, in init\n    mod = importlib.import_module(self.SETTINGS_MODULE)\n  File \"/usr/lib/python2.7/importlib/init.py\", line 37, in import_module\n    import(name)\nImportError: No module named graphic.settings\n. ",
    "cixelsyd": "This is hitting me also.  All the graphite releases since 1.0.0 are giving me this error.. A bit more information: I'm installing on Centos7 into a python2.7 virtualenv.  it doesnt matter which of the 1.x.y series releases I install from github, I always receive the \"ImportError: No module named graphite.settings\" that OP reports.  It doesnt matter if i activate my virtualenv first, and it doesnt matter if I prefix my command with \"PYTHONPATH=/opt/graphite/webapp\", or if I cd into my /opt/graphite/webapp/graphite/ directory and run from there.  I always receive this:\nbash\nTraceback (most recent call last):\n  File \"/opt/graphite/bin/django-admin.py\", line 5, in <module>\n    management.execute_from_command_line()\n  File \"/opt/graphite/lib/python2.7/site-packages/django/core/management/__init__.py\", line 353, in execute_from_command_line\n    utility.execute()\n  File \"/opt/graphite/lib/python2.7/site-packages/django/core/management/__init__.py\", line 302, in execute\n    settings.INSTALLED_APPS\n  File \"/opt/graphite/lib/python2.7/site-packages/django/conf/__init__.py\", line 55, in __getattr__\n    self._setup(name)\n  File \"/opt/graphite/lib/python2.7/site-packages/django/conf/__init__.py\", line 43, in _setup\n    self._wrapped = Settings(settings_module)\n  File \"/opt/graphite/lib/python2.7/site-packages/django/conf/__init__.py\", line 99, in __init__\n    mod = importlib.import_module(self.SETTINGS_MODULE)\n  File \"/usr/lib64/python2.7/importlib/__init__.py\", line 37, in import_module\n    __import__(name)\nImportError: No module named graphite.settings\nThe command I am attempting is:\n```bash\nPYTHONPATH=/opt/graphite/webapp /opt/graphite/bin/django-admin.py migrate auth --settings=graphite.settings --noinput\nPYTHONPATH=/opt/graphite/webapp /opt/graphite/bin/django-admin.py migrate --settings=graphite.settings --noinput --run-syncdb\n```. I tried both 1.8 and 1.11 and they both exhibited the same behavior.. thank you very much for your efforts, i will verify today or this weekend and post my results.\nmobile\n\nOn Apr 20, 2018, at 4:56 AM, Piotr Popieluch notifications@github.com wrote:\nJust tried it on a quite clean centos7 and it just works for me. Are you sure you cd into the correct directory?\nyum install python-virtualenv libffi-devel python-devel gcc\nvirtualenv /opt/venv\nsource /opt/venv/bin/activate\npip install -U setuptools\npip install -U carbon\npip install -U graphite-web\ncd /opt/graphite/webapp\nPYTHONPATH=/opt/graphite/webapp/ django-admin migrate --settings=graphite.settings --run-syncdb\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \n",
    "mknornschild": "Seems the same problem at Ubuntu 18.04\nI installed graphite with this tutorial https://www.christian-klemm.de/icingaweb2-graphite-modul/\nAt the point \"sudo graphite-manage syncdb \" I get the following errors:\nroot@icinga2:/# sudo graphite-manage syncdb\nTraceback (most recent call last):\n  File \"/usr/bin/graphite-manage\", line 15, in \n    execute_from_command_line(sys.argv)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/init.py\", line 443, in execute_from_command_line\n    utility.execute()\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/init.py\", line 382, in execute\n    self.fetch_command(subcommand).run_from_argv(self.argv)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/base.py\", line 196, in run_from_argv\n    self.execute(args, *options.dict)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/base.py\", line 231, in execute\n    self.validate()\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/base.py\", line 266, in validate\n    num_errors = get_validation_errors(s, app)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/validation.py\", line 30, in get_validation_errors\n    for (app_name, error) in get_app_errors().items():\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/models/loading.py\", line 158, in get_app_errors\n    self._populate()\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/models/loading.py\", line 64, in _populate\n    self.load_app(app_name, True)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/models/loading.py\", line 88, in load_app\n    models = import_module('.models', app_name)\n  File \"/usr/local/lib/python2.7/dist-packages/django/utils/importlib.py\", line 35, in import_module\n    import(name)\n  File \"/usr/lib/python2.7/dist-packages/graphite/dashboard/models.py\", line 3, in \n    from graphite.util import json\n  File \"/usr/lib/python2.7/dist-packages/graphite/util.py\", line 38, in \n    from graphite.logger import log\n  File \"/usr/lib/python2.7/dist-packages/graphite/logger.py\", line 87, in \n    log = GraphiteLogger() # import-shared logger instance\n  File \"/usr/lib/python2.7/dist-packages/graphite/logger.py\", line 40, in init\n    level = logging.INFO,\n  File \"/usr/lib/python2.7/dist-packages/graphite/logger.py\", line 65, in _config_logger\n    handler = Rotater(log_file, when=when, backupCount=backupCount)\n  File \"/usr/lib/python2.7/logging/handlers.py\", line 171, in init\n    BaseRotatingHandler.init(self, filename, 'a', encoding, delay)\n  File \"/usr/lib/python2.7/logging/handlers.py\", line 64, in init\n    logging.FileHandler.init(self, filename, mode, encoding, delay)\n  File \"/usr/lib/python2.7/logging/init.py\", line 920, in init\n    StreamHandler.init(self, self._open())\n  File \"/usr/lib/python2.7/logging/init.py\", line 950, in _open\n    stream = open(self.baseFilename, self.mode)\nIOError: [Errno 2] No such file or directory: '/usr/lib/python2.7/storage/log/webapp/info.log'. ",
    "cosmingodeanu": "I can't get this to work on solaris 11, and I've installed a lot of libraries with both pip and pkgutil. ",
    "andChow": "I have the same question\uff0cbut I had installed cairocffi\u3002\n[root@kafka-3-2016-7 Twisted-14.0.0]# pip list\ncairocffi (0.8.0)\nceres (1.1.0)\ncffi (1.10.0)\ndistribute (0.6.14)\nDjango (1.9.13)\ndjango-tagging (0.4.3)\npip (1.5.5)\npycparser (2.17)\npyparsing (2.2.0)\npytz (2017.2)\nscandir (1.5)\nsetuptools (7.0)\nTwisted (14.0.0)\ntxAMQP (0.7.0)\nurllib3 (1.21.1)\nWeasyPrint (0.36)\nwhisper (1.0.1)\nwsgiref (0.1.2)\nzope.interface (4.4.1)\n@obfuscurity . tks, you are right, In our system\uff0clack of libcairo.so.2\u3002\n[root@kafka-3-2016-7 ~]# find / -name 'libcairo.so.2'\n[root@kafka-3-2016-7 ~]# cat /etc/redhat-release \nCentOS release 6.5 (Final)\n[root@kafka-3-2016-7 ~]#  cat /proc/version\nLinux version 2.6.32-431.el6.x86_64 (mockbuild@c6b8.bsys.dev.centos.org) (gcc version 4.4.7 20120313 (Red Hat 4.4.7-4) (GCC) ) #1 SMP Fri Nov 22 03:15:09 UTC 2013\n* Solution\nyum install cairo-devel.i686. ",
    "rehevkor5": "Disclaimer: I am not really a Python developer, so hopefully I didn't do anything terribly thickheaded.\n. @obfuscurity regarding your question about whether this might cause problems for Grafana: I created this PR in order to resolve problems with Grafana, actually! I have seen graphs refuse to render if Infinity is in the JSON returned by Graphite.. @obfuscurity My apologies for the unintended side effect. My only comment is about the replace. It's not syntax-aware, so obviously you'd want to be reasonably sure that there wouldn't be something like {title: \"To Infinity and Beyond\"} in the JSON, otherwise it's going to show up as {title: \"To 1e9999 and Beyond\"}. I'm sure you all know this component well enough to be sure whether that's an actual problem or not. I don't.\nIt's possible there's something bad about the way I extended JSONEncoder. For example, does overriding default instead of iterencode make any difference?\nI'm also wondering whether using https://simplejson.readthedocs.io/en/latest/ might enable customization without such a big impact on performance? (it gets None right but still does +-inf and NaN the same way). Btw ujson is not an option if infinity or NaN is present, \"OverflowError: Invalid Inf value when encoding double\" and \"Invalid Nan value when encoding double\".\nPerhaps a better option would be to fix the data itself before it gets to the JSON encoder. You'd have to figure out how to iterate over all the values (though maybe there are already places in the code where it's iterating over them). That approach would be unambiguous compared to string replacement, and would enable use of any naive json encoder (including ujson).\nRegardless of what happens, I hope we will be able to retain this fix eventually. The JSON spec from ECMA explicitly states \"Numeric values that cannot be represented as sequences of digits (such as Infinity and NaN) are not permitted.\" In that regard, both built-in Python json and simplejson are setting poor examples.. @DanCech if you can give me guidance on how to test this with your large dataset, I could do some experimentation to help out.. Please take a look at https://github.com/rehevkor5/graphite-web/tree/fix_json_serialization_perf_regression\nIf we are willing to accept a slight change in the behavior of JSON serialization, we can fix the performance reversion while still getting reliable JSON. Specifically, instead of serializing inf and -inf as +-1e9999, I replace the values with Python's sys.float_info.max and sys.float_info.min. Similarly, I replace NaN with None in the data before it gets to the serializer. This allows the JSON serializer to serialize them normally.\nIf you think it's a good approach, I can do some minor cleanup & make a PR. Currently, the functionality is activated by including a useNewEncoder query param. And again, sorry for any style problems: I don't have much Python experience.. @obfuscurity yes I will remove it as cleanup before I submit a PR.\n@DanCech It will output null, not None. I'm not sure if there's a reasonable fix for the values seeming random though. Any choice other than sys.float_info.max seems arbitrary.. Created https://github.com/graphite-project/graphite-web/pull/1785 for your consideration. ",
    "sul4bh": "I had to read through 'https://github.com/graphite-project/graphite-web/issues/207' to figure out the proper behavior of total parameter.\n. ",
    "KarnG": "@deniszh .. I have python 2.7.10 \n. While I am starting the httpd service it throws this error on the log file\n. I,m using python 2.7.10 and django 1.7 ..though I installed python 2.7 into /opt on centos 6.5 and exported the path for new installed python and it's running good. I,ll downgrade the django and will see then\n. After downgrading still the same issue.. It Does not looks to be version issue..\n```\n[Tue Oct 11 18:22:25 2016] [error]     from django.utils.importlib import import_module\n[Tue Oct 11 18:22:25 2016] [error] ImportError: No module named django.utils.importlib\n[Tue Oct 11 18:22:25 2016] [error] mod_wsgi (pid=22631): Target WSGI script '/opt/graphite/conf/graphite.wsgi' cannot be loaded as Python module.\n[Tue Oct 11 18:22:25 2016] [error] mod_wsgi (pid=22631): Exception occurred processing WSGI script '/opt/graphite/conf/graphite.wsgi'.\n[Tue Oct 11 18:22:25 2016] [error] Traceback (most recent call last):\n[Tue Oct 11 18:22:25 2016] [error]   File \"/opt/graphite/conf/graphite.wsgi\", line 4, in \n[Tue Oct 11 18:22:25 2016] [error]     from graphite.wsgi import application\n[Tue Oct 11 18:22:25 2016] [error]   File \"/opt/graphite/webapp/graphite/wsgi.py\", line 8, in \n[Tue Oct 11 18:22:25 2016] [error]     from django.utils.importlib import import_module\n[Tue Oct 11 18:22:25 2016] [error] ImportEror: No module named django.utils.importlib\n```\n. @brutasse .. I have downgraded the django as per @deniszh to 1.6 but didnt get it resolved so i did upgraded it to the 1.8 now. I using \"pip install\" for this installation..\nBelow are the Installed pks via pip..\n```\npip freeze\ncairocffi==0.7.2\ncffi==1.8.3\nDjango==1.7\ndjango-socketio==0.3.9\ndjango-tagging==0.4.3\nmod-wsgi-httpd==2.4.23.1\npycparser==2.14\npyparsing==2.1.10\nPython-fontconfig==0.5.1\npython-ldap==2.4.27\npython-memcached==1.58\npytz==2016.7\nsix==1.10.0\nTwisted==16.4.1\ntxAMQP==0.6.2\nwhisper==0.10.0rc1\nzope.interface==4.3.2\nWhen i check manual import , it works good..\n```\n[root@noi-dcgrafana ~]# python\nPython 2.7.10 (default, Oct 10 2016, 16:23:05)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport sys\nprint(sys.version_info)\nsys.version_info(major=2, minor=7, micro=10, releaselevel='final', serial=0)\nprint(sys.prefix)\n/opt/python\nimport django\nprint django.file\n/opt/python/lib/python2.7/site-packages/django/init.pyc\n\n\n\n\n. Just for attention : \n1) I have default python is 2.6 \n2) I installed python 2.7.10 Under \"/opt/python\"\n3) So, everything installed under /opt/ including graphite Installation\nThis is all Running Under OS CenyOs Version 6.5.\n. @deniszh .. Nt Sure if thats the case here..\n. Can anyone tell me the Standard Set of pkgs for graphite install  on RedHat 6.5 with custom install python 2.7 as RHEL 6 comes default with 2.6 python\n. ",
    "str4d": "Possible hint: I have an import issue with my Twisted plugin, where it works fine with Twisted 16.3 but fails with 16.4: https://travis-ci.org/str4d/txi2p/builds/169930132\n. ",
    "jack17529": "I too had this error , is there any work around , i am not able to do so.\n. ",
    "devent": "You can also just leave out python-twisted.\npython-twisted pulls in python-openssl, which will break pyOpenSSL.\nMy Docker file was based on an outdated Graphite docker file from someone else.\n. Here is my Dockerfile for reference. Works 100%. :)\nhttps://github.com/devent/graphite_carbon_docker/blob/master/Dockerfile\n. Thank you. But I still have problems and I can't find anything in Google.\n```\nRUN set -x \\\n    # supervisor\n  && pip install -U  pip setuptools \\\n  && easy_install supervisor \\\n # python dependencies\n&& pip install django==1.9.13\\\n python-memcached==1.58\\\n txAMQP==0.4\\\n simplejson==3.11.1\\\n django-tagging==0.4.3\\\n twisted==11.1.0\\\n pytz==2017.2\\\n pyparsing==2.2.0\\\n cairocffi==0.8.0\\\n git+https://github.com/graphite-project/whisper.git#egg=whisper\\\n git+https://github.com/graphite-project/ceres.git#egg=ceres\\\n scandir==1.5\nStep 20/48 : RUN PYTHONPATH=\"/opt/graphite/webapp/\" django-admin.py collectstatic --noinput --settings=graphite.settings\n ---> Running in e2f482ae6a94\nTraceback (most recent call last):\n  File \"/usr/local/bin/django-admin.py\", line 5, in \n    management.execute_from_command_line()\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/init.py\", line 353, in execute_from_command_line\n    utility.execute()\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/init.py\", line 327, in execute\n    django.setup()\n  File \"/usr/local/lib/python2.7/dist-packages/django/init.py\", line 18, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File \"/usr/local/lib/python2.7/dist-packages/django/apps/registry.py\", line 85, in populate\n    app_config = AppConfig.create(entry)\n  File \"/usr/local/lib/python2.7/dist-packages/django/apps/config.py\", line 123, in create\n    import_module(entry)\n  File \"/usr/lib/python2.7/importlib/init.py\", line 37, in import_module\n    import(name)\nImportError: No module named cli\nThe command '/bin/sh -c echo $PYTHONPATH;ls -l /opt/graphite/webapp/;PYTHONPATH=\"/opt/graphite/lib/:/opt/graphite/webapp/\" django-admin.py collectstatic --noinput --settings=graphite.settings' returned a non-zero code: 1\n../utils/Makefile.image:18: recipe for target '_build' failed\nmake: *** [_build] Error 1\n```. ",
    "edlitmus": "It might: in the relevant rrdtool code it uses realpath(3) which should resolve symlinks, but we found that adding this helped us with an issue where RRDs were not getting flushed properly before graphite queried them.\n. ",
    "spektre1": "Thanks for the suggestion. Yah, I saw that with someone else's issue thread, so I already tried changing versions a bunch of times to see if any work. I haven't had much luck so far. Here's what happens on your rec'd version:\n[root@monitor graphite]# pip install \"Django>=1.8,<1.9\"\nCollecting Django<1.9,>=1.8\n  Downloading Django-1.8.15-py2.py3-none-any.whl (6.2MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.2MB 149kB/s \nInstalling collected packages: Django\n  Found existing installation: Django 1.10.2\n    Uninstalling Django-1.10.2:\n      Successfully uninstalled Django-1.10.2\nSuccessfully installed Django-1.8.15\n[root@monitor graphite]# PYTHONPATH=$GRAPHITE_ROOT/webapp django-admin.py migrate --settings=graphite.settings --run-syncdb\n/usr/lib/python2.7/site-packages/tagging/models.py:10: RemovedInDjango19Warning: django.contrib.contenttypes.generic is deprecated and will be removed in Django 1.9. Its contents have been moved to the fields, forms, and admin submodules of django.contrib.contenttypes.\n  from django.contrib.contenttypes import generic\nTraceback (most recent call last):\n  File \"/bin/django-admin.py\", line 5, in \n    management.execute_from_command_line()\n  File \"/usr/lib/python2.7/site-packages/django/core/management/init.py\", line 354, in execute_from_command_line\n    utility.execute()\n  File \"/usr/lib/python2.7/site-packages/django/core/management/init.py\", line 328, in execute\n    django.setup()\n  File \"/usr/lib/python2.7/site-packages/django/init.py\", line 18, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File \"/usr/lib/python2.7/site-packages/django/apps/registry.py\", line 115, in populate\n    app_config.ready()\n  File \"/usr/lib/python2.7/site-packages/django/contrib/admin/apps.py\", line 22, in ready\n    self.module.autodiscover()\n  File \"/usr/lib/python2.7/site-packages/django/contrib/admin/init.py\", line 24, in autodiscover\n    autodiscover_modules('admin', register_to=site)\n  File \"/usr/lib/python2.7/site-packages/django/utils/module_loading.py\", line 74, in autodiscover_modules\n    import_module('%s.%s' % (app_config.name, module_to_search))\n  File \"/usr/lib64/python2.7/importlib/init.py\", line 37, in import_module\n    import(name)\n  File \"/usr/lib/python2.7/site-packages/tagging/admin.py\", line 3, in \n    from tagging.forms import TagAdminForm\n  File \"/usr/lib/python2.7/site-packages/tagging/forms.py\", line 11, in \n    class TagAdminForm(forms.ModelForm):\n  File \"/usr/lib/python2.7/site-packages/django/forms/models.py\", line 274, in new\n    \"needs updating.\" % name\ndjango.core.exceptions.ImproperlyConfigured: Creating a ModelForm without either the 'fields' attribute or the 'exclude' attribute is prohibited; form TagAdminForm needs updating.\n. Bump? I'm kind of lost here.The docs aren't holding up with the pip install method. Is there a recommended platform and build versions? \nI'm going to try from source today and see where I can get. Thanks in advance for any help.\n. For the record, I got this debugged. I had permissions errors on the log files that graphite was trying to write to, so it was failing to execute the wsgi. I had to insert a try/catch into the code to actually get an accurate enough error response to resolve this. This suggests that better error handling may be a good idea here.\nThe actual root cause was SELinux contexts were not setup correctly on those log directories; once I disabled SELinux entirely, it works. I should've realized that, I've run into that problem dozens of times with CentOS which has it enabled by default.\nApologies for wasting time, I'm closing this. Perhaps if I have some time I'll submit and error handling patch.\n. Done, does that work?\n. ",
    "shahmaulikn": "This file controls the state of SELinux on the system.\nSELINUX= can take one of these three values:\nenforcing - SELinux security policy is enforced.\npermissive - SELinux prints warnings instead of enforcing.\ndisabled - No SELinux policy is loaded.\nSELINUX=disabled\nSELINUXTYPE= can take one of these two values:\ntargeted - Targeted processes are protected,\nminimum - Modification of targeted policy. Only selected processes are protected.\nmls - Multi Level Security protection.\nSELINUXTYPE=targeted \nit is disable then also observing same issue, please do needful . [root@localhost b]# pip show django whipser carbon graphite-web\nMetadata-Version: 2.0\nName: Django\nVersion: 1.9.13\nSummary: A high-level Python Web framework that encourages rapid development and clean, pragmatic design.\nHome-page: http://www.djangoproject.com/\nAuthor: Django Software Foundation\nAuthor-email: foundation@djangoproject.com\nInstaller: pip\nLicense: BSD\nLocation: /usr/lib/python2.7/site-packages\nRequires: \nClassifiers:\n  Development Status :: 5 - Production/Stable\n  Environment :: Web Environment\n  Framework :: Django\n  Intended Audience :: Developers\n  License :: OSI Approved :: BSD License\n  Operating System :: OS Independent\n  Programming Language :: Python\n  Programming Language :: Python :: 2\n  Programming Language :: Python :: 2.7\n  Programming Language :: Python :: 3\n  Programming Language :: Python :: 3.4\n  Programming Language :: Python :: 3.5\n  Topic :: Internet :: WWW/HTTP\n  Topic :: Internet :: WWW/HTTP :: Dynamic Content\n  Topic :: Internet :: WWW/HTTP :: WSGI\n  Topic :: Software Development :: Libraries :: Application Frameworks\n  Topic :: Software Development :: Libraries :: Python Modules\nEntry-points:\n  [console_scripts]\n  django-admin = django.core.management:execute_from_command_line\nThis is what I am getting for pip show django whipser carbon graphite-web\n. ",
    "ybizeul": "Yes, I'm no expert in how this can be implemented, but certainly defining in a global location that files are UTF-8 encoded would make sense\n. Ok, the problem was actually introduced on 61f62ee by special characters in :\n```\nType of metric hashing function.\nThe default carbon_ch is Graphite's traditional consistent-hashing implementation.\nAlternatively, you can use fnv1a_ch, which supports the Fowler\u2013Noll\u2013Vo hash\nfunction (FNV-1a) hash implementation offered by the carbon-c-relay project\nhttps://github.com/grobian/carbon-c-relay\n```\nAround Fowler\u2013Noll\u2013Vo, I think these are typographic dashes, maybe we can just fix that.\n. I'm having lots of random issues recently with that kind of errors in the logs, could that be related?\nI think it started happening with a recent Graphite update. I don't see any quote in the query at least in Grafana\nKeyError: ({Suppress:(\"\\\") W:((){}...)}, u'netapp.perf.VYN.S18SAN.node.S18SAN-02.vol_summary.total_ops', 6, True, True)\nKeyError: (\"}\", u'aliasByNode(highestAverage(netapp.perf.{CEL,HAN,MRI,NOR,ODH,PRI,SHA,TOK,VYN}.{S10SAN,S11SAN,S13SAN,S16SAN,S18SAN,S25SAN,S32SAN,S34SAN,S37SAN}.node.*.vol_summary.total_ops, 4), 5)', 75, True, True)\nKeyError: ({{Suppress:(\"\\\") W:((){}...)} | W:(!#%$...)}, u'netapp.perf.VYN.S18SAN.node.S18SAN-02.vol_summary.total_ops', 12, False, True)\nKeyError: ({{{Suppress:(\"\\\") W:((){}...)} | W:(!#%$...)}}..., u'netapp.perf.HAN.S37SAN.node.S37SAN-02.vol_summary.total_data', 15, False, True)\nKeyError: (\"\\\", u'aliasByNode(highestAverage(netapp.perf.{CEL,HAN,MRI,NOR,ODH,PRI,SHA,TOK,VYN}.{S10SAN,S11SAN,S13SAN,S16SAN,S18SAN,S25SAN,S32SAN,S34SAN,S37SAN}.node.*.vol_summary.avg_latency, 5), 5)', 33, False, True). Same query performed again :\ntarget=aliasByNode(highestAverage(netapp.perf.{MBP}.{clus1}.node.*.vol_summary.total_data, 5), 5)&from=-6h&until=now&format=json&maxDataPoints=437\nHTTP/1.1 200 OK\\r\\n. I was actually looking at https://github.com/graphite-project/graphite-web/commit/587a48cd6fbb07a4c79dcf7ce573e02c6b6580d8. Certainly looks like a pyparsing issue.\nThe parser chokes on this for example :\naliasByNode(highestAverage(netapp.perf.{MBP}.{clus1}.node.*.processor.avg_processor_busy, 5), 5)\nWith that said, before that, I see it parsing things like that :\nParse:[\\0xc0-\\0xd6\\0xd8-\\0xf6\\0xf8-\\0xff]\nDone: ['[', [[u'\\xc0', u'\\xd6'], [u'\\xd8', u'\\xf6'], [u'\\xf8', u'\\xff']], ']']\nParse:[\\0xa1-\\0xbf\\0xd7\\0xf7]\nDone: ['[', [[u'\\xa1', u'\\xbf'], u'\\xd7', u'\\xf7'], ']']\nI don't know where that comes from yet. Apparently pyparsing 2.1 is broken with Graphite.\nDebian Jessiec omew with 2.0 preinstaller, but installing setuptools (I assume) bring 2.1 as part of the dependencies.\nJust uninstalling with pip uninstall pyparsing did the job.\nBefore :\n```\nroot@adva:~# pip show pyparsing\nName: pyparsing\nVersion: 2.1.10\nSummary: Python parsing module\nHome-page: http://pyparsing.wikispaces.com/\nAuthor: Paul McGuire\nAuthor-email: ptmcg@users.sourceforge.net\nLicense: MIT License\nLocation: /usr/local/lib/python2.7/dist-packages\nRequires: \nroot@adva:~# dpkg -l python-pyparsing\nDesired=Unknown/Install/Remove/Purge/Hold\n| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\n|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\n||/ Name           Version      Architecture Description\n+++-==============-============-============-=================================\nii  python-pyparsi 2.0.3+dfsg1- all          Python parsing module\n```. 1.5.7 i son longer pinned indeed. 2.0 is just fine. Are you sure graphite is using it? Make a dashboard with around 10 panels on it and refresh until it breaks, it doesn't take more than 3 or 4 refresh here before some panels show errors. Has been confirmed with fresh installs as well. Pyparsing developer came back to me and was very helpful explaining the major difference with multi threading with 2.1 that didn't exists in 2.0, it seems pretty clear < 2.1 needs to be pinned. \n[Edit] Fixed wrong versions in my explanation. @deniszh Yes, sorry for the confusion!\n@iksaif It's actually unclear what the issue is, not sure what needs to be done to actually use 2.1, it looks like we end up with some garbage pulled from the dictionary when concurrent requests are done. There you go!. Please see #1828, this becomes a real issue as pinning doesn't work in some circumstances.. @DanCech That I don't know. What I have is just words from pyparsing author that performance shouldn't be impacted. Is that your question?. @DanCech Yes in my original post, last quote. Look, I'm trying to pretend I know what we're talking about here. Obviously you got me :-D\nI assumed \"packrat\" was the other chain parsing method, which I dropped in favor of regular expression. That's not what it is right?. @iksaif Not sure how to do that?. Ok, both runs with 1000 iterations.\nLegacy :\nnetapp.perf.MBP.clus1.node.clus1-01.vol_summary.total_data 2s\nnetapp.perf.MBP.clus1.node.clus1-01.aggr.disk_busy 3s\nnetapp.perf.MBP.clus1.node.clus1-01.vol_summary.read_latency 4s\nnetapp.perf.MBP.clus1.node.clus1-01.vol_summary.write_latency 4s\nnetapp.perf.MBP.clus1.node.clus1-01.processor.avg_processor_busy 6s\nsumSeries(netapp.perf.MBP.clus1.node.clus1-01.vol_summary.total_ops) 7s\nPatched:\nnetapp.perf.MBP.clus1.node.clus1-01.vol_summary.read_latency 1s\nnetapp.perf.MBP.clus1.node.clus1-01.vol_summary.total_data 1s\nnetapp.perf.MBP.clus1.node.clus1-01.vol_summary.write_latency 2s\nnetapp.perf.MBP.clus1.node.clus1-01.processor.avg_processor_busy 2s\nsumSeries(netapp.perf.MBP.clus1.node.clus1-01.vol_summary.total_ops) 2s\nnetapp.perf.MBP.clus1.node.clus1-01.aggr.disk_busy 1s\nNot only there no negative impact but that's a 200% increase in performance :-). I didn't know how to isolate it, so I don't really have a script.\nWhat I did is this in graphite/render/evaluator.py (and I reinstalled since then but you'll get the idea) :\n```\ndef evaluateTarget(requestContext, target):\nsec_ = calendar.timegm(time.gmtime())\nfor x in xrange(1, 1000):                                  # Loop\n    tokens = grammar.parseString(target)\n  result = evaluateTokens(requestContext, tokens)\nlog target + \" \" + calendar.timegm(time.gmtime()) - sec_ # pseudo code\n  if type(result) is TimeSeries:\n    return [result] #we have to return a list of TimeSeries objects\nelse:\n    return result\n```\nThen just loaded a dashboard page. Not sure I did correctly what you're saying.\nBasically I set PYTHONPATH to /opt/graphite and exported it. I don't think GRAPHITE_NO_PREFIX is set, and I get the same error, it shows PYTHONPATH is set accordingly :\n```\nChecking .pth file support in /opt/graphite/webapp/\n/usr/bin/python -E -c pass\nTEST FAILED: /opt/graphite/webapp/ does NOT support .pth files\nerror: bad install directory or PYTHONPATH\nYou are attempting to install a package to a directory that is not\non PYTHONPATH and which Python does not read \".pth\" files from.  The\ninstallation directory you specified (via --install-dir, --prefix, or\nthe distutils default setting) was:\n/opt/graphite/webapp/\n\nand your PYTHONPATH environment variable currently contains:\n'/opt/graphite'\n\n```. Not sure I did correctly what you're saying.\nBasically I set PYTHONPATH to /opt/graphite and exported it. I don't think GRAPHITE_NO_PREFIX is set, and I get the same error, it shows PYTHONPATH is set accordingly :\n```\nChecking .pth file support in /opt/graphite/webapp/\n/usr/bin/python -E -c pass\nTEST FAILED: /opt/graphite/webapp/ does NOT support .pth files\nerror: bad install directory or PYTHONPATH\nYou are attempting to install a package to a directory that is not\non PYTHONPATH and which Python does not read \".pth\" files from.  The\ninstallation directory you specified (via --install-dir, --prefix, or\nthe distutils default setting) was:\n/opt/graphite/webapp/\n\nand your PYTHONPATH environment variable currently contains:\n'/opt/graphite'\n\n```. That's Python 2.7. That's Python 2.7. Synthesize is using requirements.txt file. Installing \"pyparsing\" so not honoring pinning requirements if I understand correctly.\nNote that you would find much of an issue, the only problem I have so far is if you end up using pyparsing 2.10, then randomly panel load will error when reloading full pages.\nBottomline, I don't think Synthesize would give me pyparsing according to install_require defined in setup.py, would it?. Synthesize is using requirements.txt file. Installing \"pyparsing\" so not honoring pinning requirements if I understand correctly.\nNote that you would find much of an issue, the only problem I have so far is if you end up using pyparsing 2.10, then randomly panel load will error when reloading full pages.\nBottomline, I don't think Synthesize would give me pyparsing according to install_require defined in setup.py, would it?. Same here, I don't know much about it. What I know is that pyparsing 2.10 is broken, unless we finally merge #1810, and I don't think using requirements.txt will fix this.\nIn the end, it depends on what version of pyparsing is found. If the distribution, debina 8.7 for example, provides pyparsing 2.0, you're all set. But if you install setuptools (for example) and it comes with 2.10 as a dependency, you break graphite, silently.. Got it. Everything is on track to merge #1810 I suppose.\nI should look into virtualenv then for my installs. But it's still confusing that a bare environment and simply running setup.py leads into dependencies not being honored.. That would be in storage-schema.conf for carbon, explained here .\nThe sample one provided with the standard installation, that you can see here defines the following :\n```\n[carbon]\npattern = ^carbon.\nretentions = 60:90d\n[default_1min_for_1day]\npattern = .*\nretentions = 60s:1d\n```\nI'm actually surprised it's not more than that. How is yours configured?\n. Awesome, many thanks Dan, I was also able to use the integral to trace kWh for the different sensors!\nClosing the issue!. Thanks, looks good now, I think it was regression from my work trying to troubleshoot #667. ",
    "benburry": "@obfuscurity I agree. I was struggling with how to access the request (and so, have access to the header values) anywhere outside of the view\nIs there an easy django/graphite-web way to do this that I missed?\n. Thanks @obfuscurity. I'll get working on porting this to master\n. I'm generally a little less comfortable with this on master because of the addition of the local param to STORE.find - having both local and headers parameters feels odd, as local=True will mean headers is unused.\nRegardless, this is a bare port of #1733 to the master branch\n. I hear your concerns, and had overlooked that master now has custom storage backend support.\nWithout adjusting the API, making the request context available is achievable either by using a threadlocal (which has enough negative/conflicting opinions to make it an undesirable approach) or ... I'll have a think. Thanks for running with this @DanCech . ",
    "bodgit": "Those tickets seemed more towards changing the line receiver/pickle protocol used  for metric ingestion/relaying. I was talking about the interaction between clusters/hierarchies of frontend apps.\nCurrently I can use Graphite API which doesn't know how to talk to other instances of itself or Graphite web, but it's currently impossible to mix them as neither can agree on a compatible format.\nI've worked around it by just running Graphite web everywhere but it's probably overkill on the machines that don't need it.\n. > I just don't think it's appropriate for this use case.\nIt's how every other (non-Python) dashboard/frontend talks to Graphite :wink: Graphite web serves JSON content fine, it just won't ever request it from a peer.\nI'll see what I can do.\n. ",
    "pfctgeorge": "@eagleliang \nyou can simply do these 2 things to resolve:\n\n\nimport smart_str\nfrom django.utils.encoding import smart_str\n\n\nedit this line from hash.update(string) to hash.update(smart_str(string)). \n\n",
    "eagleliang": "@deniszh the change mentioned by @pfctgeorge is OK to pull request? . Yes\uff0c it's out of date. It's resolved now. Sorry about that. Just close this issue.. ",
    "gardenmwm": "You need to add the manage.py from the .9x branch and change the settings to graphite.settings. I'll add a pull request. ",
    "nyerup": "All those questions make good sense. I'll get back with answers as soon as I can.\nMy problem is that I haven't found a way to reliably reproduce this problem in my test setup \u2013 only in my production setup, which has >50 cluster backends, and in which outages disturb my colleagues.\n. All right, makes good sense. Sorry for not getting around to clearing out these questions sooner.\nI'll look through #1818, and open a new merge request if I find anything meaningful to contribute.. ",
    "artkrz": "Same error, get rid of the bloody Django crap! No one cares about it. We have Grafana :). ",
    "pat1": "try:\nhttp://graphite.readthedocs.io/en/latest/config-local-settings.html\nRemember, setting up a new database requires running PYTHONPATH=$GRAPHITE_ROOT/webapp django-admin.py migrate --settings=graphite.settings --run-syncdb to create the initial schema.. Seems the data is taken as valid for a period of time (step) after the timestamps; so the steps will be equal to the numer of data. \nwith http://rmapv.rmap.cc/graphite/render/?from=13%3A30_20180101&height=308&width=586&salt=1515158842.781&until=14%3A30_20180101&showTarget=report_fixed.digiteco.1104278_4413417..254_0_0.103_2000_-_-.B12101&target=report_fixed.digiteco.1104278_4413417..254_0_0.103_2000-_-.B12101&lineMode=staircase&areaMode=first\nBut the graph have a very strange behavior with slope line:\nhttps://github.com/r-map/rmap/issues/265\nseems that in some cases the data is taken as a value in a period (step) and in other no.\nThe first data produce a line for 15 minutes but is only a point and after there is a line connecting two points.\nFor me is a strange behaviour.. ",
    "shaik1990": "I changed db_table variable in Meta class to verbose_name_plural and so i`m getting the error..! Did you solve this issue..?. ",
    "gogich77": "Same issue at Ubuntu by manual and by source installation. \nIt looks the project is dropped.\n. Fix in case it is installed by the package manager\ngraphite-manage  migrate --settings=graphite.settings --run-syncdb\n. ",
    "tsvsj": "\nFix in case it is installed by the package manager\ngraphite-manage migrate --settings=graphite.settings --run-syncdb\n\n@gogich77 Thank you very much! That hint saved my day!. ",
    "NotSqrt": "So many functions evolved between 0.9.15 and the latest doc !\nDocumenting those changes would be very useful !. Here is an example in django's documentation : https://github.com/django/django/blob/1.8.16/docs/topics/http/shortcuts.txt#L49\nSo this stuff can have its life in a separate documentation ... I think the most frustrating thing is that the doc on readthedocs shows the master state by default, not the latest release.\nFor instance, asPercent changed to allow a list of series as second argument.\nI was so happy to finally upgrade to graphite-web==0.9.15, just for this, but no, it's just in the master, not any released version despite being developed 6 months ago.. Thanks !\nAny visibility on the next release ?. Sphinx has the required tools to add the version info in the function docstrings : http://www.sphinx-doc.org/en/stable/markup/para.html : versionadded, versionchanged, deprecated. ",
    "dgryski": "That was mostly scraped together by @nnuss.  It's not complete and of course focused on things carbonapi does or doesn't support, as determined by people at $WORK complaining that they're graphs don't work.. I have plans to move carbonapi over to using https://github.com/dgryski/go-lttb as the default aggregation method.. ",
    "ctaintor": "Thanks for the info - I suspected as much but couldn't find the release notes (although I admit I didn't try that hard...).\nI updated the PR. good point. In my case, I was talking about the output of get_data changing. I agree it's confusing so added what you suggested and clarified what I wrote.. Sorry - I had meant to remove that sentence. I'll remove it. (the \"When matching...\")\nAbout the output, I only have it there because it changed - in 0.9, the output doesn't match the example. It seemed worth noting. If you feel that has no value, I will remove it as well.. ",
    "Dan33l": "The command graphite-manage is a wrapper :\n```\n! /usr/bin/python\nimport os\nimport sys\ntry:\n    import graphite.settings # Assumed to be in the same directory.\nexcept ImportError:\n    sys.stderr.write(\"Error: Can't find the file 'settings.py' in the directory containing %r. It appears you've customized things.\\nYou'll have to run django-admin.py, passing it your settings module.\\n(If the file settings.py does indeed exist, it's causing an ImportError somehow.)\\n\" % file)\n    sys.exit(1)\nif name == \"main\":\n    os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"graphite.settings\")\n    from django.core.management import execute_from_command_line\n    execute_from_command_line(sys.argv)\n/usr/bin/graphite-manage (END)\n```. ",
    "Marek77": "I can't say I know what is that failed check about..?. You will probably need to download the images to see the bottom border getting bigger in the first three images, and their fixed status (no extra border) in the last three. The issue becomes really visible with lots of metrics (I saw the bug with 'hideLegend=false').\nBorder:\n\n\n\nPatch applied:\n\n\n\n. OK, noise added ;-). As the fix changes the generated image, I admit I have hard time seeing what to test and how.. ",
    "nickstenning": "Thank you for the link to consolidateBy -- that looks like exactly what I was looking for.\nAnd btw, sorry if my tweets annoyed you. I definitely wasn't trying to point the finger here -- I was just confused by what graphite's solution was to a real and general problem with plotting lots of points efficiently, and I wasn't able to find consolidateBy in the documentation. FWIW I'm pretty sure it is exactly the problem discussed by Heinrich in the post linked -- the percentile aggregation in Circonus seems to solve the same problem as consolidateBy.. ",
    "deltaskelta": "yeah I just did a quick PR #1773 \nI think the pypi version was out of date. I changed it to install from github on a custom install to get the most up to date version. ",
    "zacharya19": "Look like the problem was with the time zone.\n1482 fixed it..",
    "dmoll1974": "Cool thanks!. ",
    "vbichov": "+1. ",
    "Roguelazer": "The particular metric I'm concerned with this is \"time since queue X was empty\". I guess I could divide the series by itself to normalize all non-zeros to one and then integrate that? Presuming 0/0 equals 0, which I don't think is strictly speaking true.\n-- \nJames Brown,\ncurrently mobile\n\nOn Mar 18, 2017, at 09:18, Denis Zhdanov notifications@github.com wrote:\n@Roguelazer - I'm with @obfuscurity here. Do you have some usecase where his proposal will not work?\nI'm generally \ud83d\udc4d for adding more functions to Graphite but let's not bloat it over the top.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Somewhat unrelatedlay, it would be neat if there were a way to add functions to graphite without patching the package; if you could, say, list other python packages in a config file and they'd be imported and searched via some standard protocol for functions.\n\nThen I'd probably not buy y'all with things like this.\n-- \nJames Brown,\ncurrently mobile\n\nOn Mar 18, 2017, at 11:10, Denis Zhdanov notifications@github.com wrote:\nHm. Looks legit. What do you think, @obfuscurity ?\nOn Sat, 18 Mar 2017 at 17:58, James Brown notifications@github.com wrote:\n\nThe particular metric I'm concerned with this is \"time since queue X was\nempty\". I guess I could divide the series by itself to normalize all\nnon-zeros to one and then integrate that? Presuming 0/0 equals 0, which I\ndon't think is strictly speaking true.\n--\nJames Brown,\ncurrently mobile\n\nOn Mar 18, 2017, at 09:18, Denis Zhdanov notifications@github.com\nwrote:\n@Roguelazer - I'm with @obfuscurity here. Do you have some usecase where\nhis proposal will not work?\nI'm generally \ud83d\udc4d for adding more functions to Graphite but let's not\nbloat it over the top.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/1789#issuecomment-287559224,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABK51tqrAsM0jJIUITsa2g-c8sa__Klkks5rnA0dgaJpZM4LjU4q\n.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n. Absolutely -- but this was faster to fix, and other folks like us may need it unbroken now. If you have code knowledge to identify and fix the underlying issue, I'd be glad to contribute a code review.\n\n\nJames Brown,\ncurrently mobile\n\nOn Oct 30, 2014, at 01:15, Bruno Reni\u00e9 notifications@github.com wrote:\nIn webapp/content/js/dashboard.js:\n\n@@ -2536,6 +2536,13 @@ function getState() {\n function applyState(state) {\n   setDashboardName(state.name);\n-  // sometimes the server calls json.dumps once too many times.\n-  // fixing that seems hard.\n  Hmm, that's what needs to be fixed instead.\n\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "Pita123": "Any idea why next release [1.0] will be published?\n. Thank you, we will test it soon. ",
    "cmaurer": "I added those entries while testing.  I can remove them and update the pull request.  . @iksaif I am not sure I understood your comment.  Is it related to .gitignore, or the EMA function itself?. @deniszh, @DanCech I commented out the lines checking ts - 60, and committed.  I don't know if the changes will be picked up after a merge or not.. No worries.  I can do that.\nChris\nSent from my iPhone\n\nOn Feb 21, 2017, at 8:23 AM, Denis Zhdanov notifications@github.com wrote:\nI think you should create new PR, @cmaurer :(\nI see latest commit 15 hours ago. Sorry for that.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. yep.  Sorry about that.  Doing it now.. @deniszh removed pyparsing version binding.. Cool.  I will change it. . Cool. Let me take a look at that this afternoon.  . @iksaif I did some work on your second request yesterday, I would like to get your feedback to make sure it is in-line with what you are requesting before I refactor everything, and commit. \n\nThanks,\nChris\n\nextracted gen_seriesList to global helper function\ncreated global function to create request contexts\n\n```python\n    def test_exponentialMovingAverage_evaluateTokens_returns_half_none(self):\n        seriesList = self._gen_series_list_with_data(start=10)\n        expectedResults = self._gen_series_list_with_data(\n            key='exponentialMovingAverage(collectd.test-db0.load.value,10)',\n            start=20,\n            end=30,\n            data=[0, 0.0, 0.182, 0.512, 0.965, 1.517, 2.15, 2.85, 3.604, 4.404, 5.239]\n        )\n    def mock_evaluateTokens(reqCtx, tokens, replacements=None):\n        return self._gen_series_list_with_data(key='collectd.test-db0.load.value',start=10, end=30, data=([None] * 10 + range(0, 10)))\n\n    with patch('graphite.render.functions.evaluateTokens', mock_evaluateTokens):\n        result = functions.exponentialMovingAverage(self._build_request_context(endTime=datetime(1970, 1, 1, 0, 9, 0, 0, pytz.timezone(settings.TIME_ZONE))), seriesList, 10)\n\n    self.assertEqual(result, expectedResults)\n\ndef _build_request_context(self, startTime=datetime(1970, 1, 1, 0, 0, 0, 0, pytz.timezone(settings.TIME_ZONE)), endTime=datetime(1970, 1, 1, 0, 59, 0, 0, pytz.timezone(settings.TIME_ZONE)),data=[]):\n    return {\n        'template': {},\n        'args': ({}, {}),\n        'startTime': startTime,\n        'endTime': endTime,\n        'localOnly': False,\n        'data': data\n    }\n\ndef _gen_series_list_with_data(self, key='collectd.test-db0.load.value', start=0, end=59, step=1, data=[]):\n    seriesList = [\n        TimeSeries(key, start, end, step, data)\n    ]\n    for series in seriesList:\n        series.pathExpression = series.name\n\n    return seriesList\n\n```. cool.  I will refactor and commit.  It might take me until tonight (GMT-5), as I am at work now, etc.\nThanks,\nChris. @iksaif ok, checked in.  Please let me know what you think.\nThanks,\nChris. ",
    "dothebart": "well, the final error you will get is logged the same way as I suggested:\nif SECRET_KEY == 'UNSAFE_DEFAULT':\n  warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\nImho in most cases where you can't load the local_settings.py you won't be able to do anything  usefull in first place, and will get all sorts of permission and path errors.\nSo, if not change the way the message is printed above, attach it to all other errors or warnings below.. ",
    "idnorton": "Fixed this by removing the following from conf/wsgi.py:\nlog.info(\"graphite.wsgi - pid %d - reloading search index\" % os.getpid())\nimport graphite.metrics.search\nMight be helpful to someone else, thanks for the pointer in the right direction @deniszh :). ",
    "thecityofguanyu": "@obfuscurity thanks for the info! I'll give that blog post a read and probably modify that ratio for the collectd data. I'll close the issue if that ends up resolving it.. @obfuscurity Looks like this was indeed my problem. Guess I need to RTFM a little more carefully next time. Thank you!. ",
    "dsongc": "We, scianos and I, made it working after removing escape in the code.\ngraphUrlParams[param[0]].append(escape(param[1]))\nI think the issue is related to #1662. ",
    "scianos": "It definitely looks like the escaping of the quote to an HTML entity is related to the issue at hand. The question is how we implement a fix which both solves the XSS and allows existing graph URLs containing quotes to work (the internal patch we're using will re-open the XSS problem, so this isn't a solution we can/would want to commit back yet without further work).. ",
    "mcostacano": "I have been thinking about this bug and I have arrived to the conclusion the xss protection from  #1662 is broken. urlencode works because you do not want your users submiting '\"<> and other garbage. But in graphite we need the \" or '. And there is no way to protect from XSS if you have \" in a url, eg: target=\" onload=\"nasty()\".\nThe threat model in #1470 does not apply to me, because we know personally everybody who has account and can save a graph. We use drawAsInfinite(events('deploy')) very much, so whe have undone the changes from #1662 and live is good again. But I understand graphite as a project can't ignore a XSS. I am also able to fix this if my patches have any posibility to get commited. I think there are various solutions\nAdding a configuration variable XSS_PROTECTION that can have 3 values NONE, NAIVE and FULL. NONE is obvious, everything  in, everything out. FULL is as now, with urlencode. NAIVE would be our own custom filter, allowing ' but removing garbage in literals (naive because I am pretty sure it will have some hole). Graphite-web ships with default FULL and document this.\nImplementing literal binding? target=drawAsInfinite(events(\\1))&literal=deploy and urlencode. Very complex, and I am not sure even if it is viable.\nAs is, but urldecoding in evaluator.evaluateTokens and fixing the client side to unquote when editing. As it is now, the browser when editing the target shows drawAsInfinite(events(&#39;deploy&#39;)). ",
    "louwrentius": "I did a clean install from source on a Raspberry Pi and I just hit this issue too. Am I missing something?\nIt seems to me that Graphite is fundamentally broken due to #1662 because a graph is broken the moment you save it. \nHow do people deal with this other than removing #1662 ? \n. @deniszh I guess so, but still a bit sad because that 3rd party dashboard is an additional requirement I would not need. I don't really see a solution either, I'm not that into web programming but I understand maybe just enough that I think that - because of how the GUI works - it may not be fixable.. ",
    "rewolf": "I just saw issue #1782 \nSo I realize that #1690 is the blocker for 0.10.0.  Is 0.10.0 the same as 1.0.0 ?. ",
    "ajcs3": "Thanks for the reply. \nCan we apply the functions on the second last datapoint for example on 'p3' i.e. getting highestMax of 'p3' , because whats happening is it again pulls me latest of all the metrics 'metric1' for all the different 'p3' varients available. What I need is to pull highestMax('p3') and pull related corresponding metric 'metric1' for that 'p3'. . @deniszh Thanks for the help.\nSuppose I have the following sample hierarchy in Graphite.\nwebsite.API.version.pagenumber.response\nIf I want to extract the response time for the highest page number, how do I do it? It sounded simple but I can't seem to get it. I try to use the maxSeries function but it's instead giving me the maximum response time out of all the pagenumbers rather than the response time from highest page number. I'm not able to apply the maxSeries function on the second last column.. @nnuss Thanks for understanding in detail.\nYes that's what I like to achieve. Could you please provide a way to achieve it.. @nnuss Thanks for the suggestions. Sortbyname is not working in my graphite version. But I tried with sortByMaxima and limit together and it almost solved my issue. But I am getting the second highest pagenumber by this approach. Any suggestions what can be done to get the first highest one instead of second highest one. ",
    "mcoolive": "You're right. I am going to update local_settings.py with an example and some explanation.. ",
    "kviktor": "Nop, I tested it on 1.9 only.. ",
    "zhkmxx9302013": "I've solved this issue now. It caused by the wrong version of the Python. The tag 0.9.15 is not support the python 2.6, it use some new feature of the python 2.7, for example \u201cDictionary comprehensions\u201d.  \nSo, by using the tag 0.9.14 with the python 2.6, can solve this problem.. ",
    "mocomoc": "+1. ",
    "blysik": "Aha!  so it looks like I could remove the following lines, and limit graphite-web to only render functionality?\n('^admin/', include(admin.site.urls)),\n  ('^composer/?', include('graphite.composer.urls')),\n  ('^browser/?', include('graphite.browser.urls')),\n  ('^account/?', include('graphite.account.urls')),\n  ('^dashboard/?', include('graphite.dashboard.urls')),\n  ('^content/(?P<path>.*)$', 'django.views.static.serve', {'document_root' : settings.CONTENT_DIR})\n  ('', 'graphite.browser.views.browser'),\nLooks like the CLI urls can be removed as well.\n. ",
    "AlexZzz": "Hi! This is an annoying bug. I see the same errors.\nIn my case request looks like: target=groupByNodes(collectd.host*_{region}*.some_*.metric-opWLat,'average',1)&from=-1hour&until=now&format=json\nWill it be fixed in near future or maybe it's not so bad to add if series: before del series[0]?. Hi! Faced with similar issue:\nTraceback (most recent call last):\n  File \"/opt/graphite/local/lib/python2.7/site-packages/whitenoise/base.py\", line 66, in __call__\n    return self.application(environ, start_response)\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/core/handlers/wsgi.py\", line 157, in __call__\n    response = self.get_response(request)\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/core/handlers/base.py\", line 124, in get_response\n    response = self._middleware_chain(request)\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/core/handlers/exception.py\", line 43, in inner\n    response = response_for_exception(request, exc)\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/core/handlers/exception.py\", line 93, in response_for_exception\n    response = handle_uncaught_exception(request, get_resolver(get_urlconf()), sys.exc_info())\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/core/handlers/exception.py\", line 135, in handle_uncaught_exception\n    extra={'status_code': 500, 'request': request},\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 1178, in error\n    self._log(ERROR, msg, args, **kwargs)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 1271, in _log\n    self.handle(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 1281, in handle\n    self.callHandlers(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 1321, in callHandlers\n    hdlr.handle(record)\n  File \"/usr/lib/python2.7/logging/__init__.py\", line 749, in handle\n    self.emit(record)\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/utils/log.py\", line 119, in emit\n    message = \"%s\\n\\n%s\" % (self.format(no_exc_record), reporter.get_traceback_text())\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/views/debug.py\", line 334, in get_traceback_text\n    return t.render(c)\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/template/base.py\", line 207, in render\n    return self._render(context)\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/template/base.py\", line 199, in _render\n    return self.nodelist.render(context)\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/template/base.py\", line 990, in render\n    bit = node.render_annotated(context)\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/template/base.py\", line 957, in render_annotated\n    return self.render(context)\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/template/defaulttags.py\", line 216, in render\n    nodelist.append(node.render_annotated(context))\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/template/base.py\", line 957, in render_annotated\n    return self.render(context)\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/template/base.py\", line 1040, in render\n    output = self.filter_expression.resolve(context)\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/template/base.py\", line 736, in resolve\n    new_obj = func(obj, *arg_vals)\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/template/defaultfilters.py\", line 255, in stringformat\n    return (\"%\" + six.text_type(arg)) % value\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/utils/functional.py\", line 79, in __repr__\n    return repr(self.__cast())\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/utils/functional.py\", line 131, in __cast\n    return self.__text_cast()\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/utils/functional.py\", line 119, in __text_cast\n    return func(*self.__args, **self.__kw)\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/urls/base.py\", line 91, in reverse\n    return force_text(iri_to_uri(resolver._reverse_with_prefix(view, prefix, *args, **kwargs)))\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/urls/resolvers.py\", line 438, in _reverse_with_prefix\n    possibilities = self.reverse_dict.getlist(lookup_view)\n  File \"/opt/graphite/local/lib/python2.7/site-packages/django/urls/resolvers.py\", line 335, in reverse_dict\n    return self._reverse_dict[language_code]\nKeyError: 'en-us'\nDjango version 1.11.11, graphite-web 1.1.2, installed in virtualenv, with uwsgi 1.9.17.1-debian, ubuntu 14.04.. Found solution: python 'requests' package was not installed.. @piotr1212 , I've checked this fix, it works for me.\nAnyway, i'm not sure how to fix this in a normal way. We don't use a lot of aggregation functions, so probably we can't test it well.. Does the latest change break something?\nIf it is not, might these changes be merged?. ",
    "rickrian": "Hi. I have the same error. if series before del series[0] works, but if i use functions like sumSeriesWithWildcards, only one datapoint on graph. My request is sumSeriesWithWildcards(aliasByNode(somename.somename.somename.somename.*.*.*.count, 4), 5, 6)\nI build graphite-web from yandex dockerfile https://github.com/yandex/graphouse/tree/master/docker/graphite-web. ",
    "jacobweber": "Installing the older version of django fixed this:\npip uninstall Django\npip install \"Django>=1.9,<1.9.99\nI also had to remove and re-install all the other python modules, so the right versions were installed.. ",
    "q2dg": "Ei, I have the same problem!\nInstalling old Django versions is not a solution!\nPlease reopen this issue. Thanks a lot @deniszh . It's strange because I'm using 1.11 version of Django (it's the one is package in artful/bionic Ubuntu distros: https://packages.ubuntu.com/artful/python-django). Thanks. Well..I'm using packages provided by official Ubuntu repositories, which, now I see, are below 1.0 version except in bionic. What a pity.Thanks. ",
    "ainsey11": "Installing old Django worked for me \ud83d\udc4d . ",
    "ponikrf": "The installation in ubuntu 17.10 from the packages leads to the same error, tell it will be fixed in the future?. ",
    "olevchyk": "that would be awesome.  I already see that imports are not optimal and probably I would need to change the tests to use Mock >> call_count. https://github.com/graphite-project/carbon/blob/master/lib/carbon/tests/data/conf-directory/storage-aggregation.conf#L24-L27\nsum has more sense for counters. \nThx for confirming about Whisper, \n this problem anyways happens with aggregationMethod=sum. raised the PR https://github.com/graphite-project/graphite-web/pull/2081\nPlease close if doesn't make sense. . In my case this idea comes from default conf example storage-aggregation.conf\nhttps://github.com/graphite-project/carbon/blob/master/conf/storage-aggregation.conf.example#L24-L27\n. in our case metric has \"count\" ending, which made  it aggregationMethod=sum on creation. :man_facepalming: \nBut can you please still check the PR, as it can improve performance for the data we get from cache , in case when reporting interval is smaller than default_retention. . In our company  we mostly use 2 client libraries: \ngo-metrics-graphite/go-metrics for golang and dropwizard for java -  both generate *.count.wsp extension for increasing counters, \nwhich can be also the part of Meters, and Histograms reporting. \nAnd with using default config  we now have all .count.wsp files with increasing counters with aggregationMethod=sum. \nFor now I'm going to change our storage-aggregation.conf to be like this: \n```\n[last]\npattern = .count$\nxFilesFactor = 0\naggregationMethod = last\n[sum]\npattern = .counter$\nxFilesFactor = 0\naggregationMethod = sum\n```\nto fix at least new metrics. \nThere seems to be a confusion between the clients: \nhttp://metrics.dropwizard.io/3.1.0/getting-started/#counters\nhttp://statsd.readthedocs.io/en/v0.5.0/types.html\nhttps://collectd.org/wiki/index.php/Data_source\nbut at the same time it looks like most of them send both rate and counter as separate metrics. \n. PR merged, Thank you @DanCech, \nClosing this for now, I will re-open if happens again after update.  . Closing in favor of https://github.com/graphite-project/graphite-web/pull/2082. Yeah this looks much better.  Thanks a lot. \nreferencing https://github.com/graphite-project/graphite-web/issues/2080. Thank you. modified as suggested. . no, removed , thank you . ",
    "cherweg": "Any news, here?\nLooking forward to 1.0 :-)\nregards \nchristian. ",
    "gdv-deepakk": "Any latest news around Graphite 1.0 release date? Cannot wait to get my hands on it.. ",
    "chaturvedia": "Looking forward to it. Will give it a spin. Oh yeah\nOn Tue, Apr 11, 2017 at 2:58 PM, Denis Zhdanov notifications@github.com\nwrote:\n\nClosed #1823\nhttps://github.com/graphite-project/graphite-web/issues/1823.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/1823#event-1039183081,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ALCw-HsigfJnZdm8YnnyyBhTpTPVxtNgks5ru_d0gaJpZM4L-VCD\n.\n. \n",
    "guyisra": "Ah so I think I understand what is happening\nI'm trying to get series that are non-zero, but some series are Null/None in the last recorded value, so currentAbove grabs the last recorded non None value of the series in the time period. \nIs there alternative way to filter out series based on the actual current value? (or to map None to 0 in graphite? its possible to do it in grafana but its too late for the query). ",
    "Neurobion": "It is completely fresh vagrant box with only few commands mentioned above in history order. There is no custom settings or whatever. . apt-remove python-pip\nwget https://bootstrap.pypa.io/get-pip.py\npython get-pip.py\npip install https://github.com/graphite-project/graphite-web/tarball/master. Or alternatively install older Twisted 16.0.0.\npip install Twisted==16.0.0\nThank you. Missing environment variable $GRAPHITE_ROOT. But help is not showing mentioned option.. ",
    "mayurmahajan": "Hi Deniszh, \nSorry, for late response and thanks for following up. This PR is to fix this issue: https://github.com/graphite-project/graphite-web/issues/248.  The response \"target\" field sometimes does not show what was sent as in target field in the URL. This field is not making change in the response target field (to maintain backwards compatibility), however adding a new field input_target that reflects what's been sent as an input target.  I will add another commit to get the unit test checks pass.  . ",
    "hexian55": "Traceback (most recent call last):\n  File \"/usr/lib64/python2.7/site-packages/django/core/handlers/base.py\", line 134, in get_response\n    resolver_match = resolver.resolve(request.path_info)\n  File \"/usr/lib64/python2.7/site-packages/django/core/urlresolvers.py\", line 376, in resolve\n    sub_match = pattern.resolve(new_path)\n  File \"/usr/lib64/python2.7/site-packages/django/core/urlresolvers.py\", line 248, in resolve\n    return ResolverMatch(self.callback, args, kwargs, self.name)\n  File \"/usr/lib64/python2.7/site-packages/django/core/urlresolvers.py\", line 255, in callback\n    self._callback = get_callable(self._callback_str)\n  File \"/usr/lib64/python2.7/site-packages/django/utils/lru_cache.py\", line 100, in wrapper\n    result = user_function(*args, kwds)\n  File \"/usr/lib64/python2.7/site-packages/django/core/urlresolvers.py\", line 115, in get_callable\n    mod = import_module(mod_name)\n  File \"/usr/lib64/python2.7/importlib/init.py\", line 37, in import_module\n    import(name)\n  File \"/opt/graphite/webapp/graphite/browser/views.py\", line 22, in \n    from graphite.util import getProfile, getProfileByUsername, defaultUser, json\n  File \"/opt/graphite/webapp/graphite/util.py\", line 89, in \n    defaultProfile = Profile.objects.get(user=defaultUser)\n  File \"/usr/lib64/python2.7/site-packages/django/db/models/manager.py\", line 122, in manager_method\n    return getattr(self.get_queryset(), name)(*args, kwargs)\n  File \"/usr/lib64/python2.7/site-packages/django/db/models/query.py\", line 381, in get\n    num = len(clone)\n  File \"/usr/lib64/python2.7/site-packages/django/db/models/query.py\", line 240, in len\n    self._fetch_all()\n  File \"/usr/lib64/python2.7/site-packages/django/db/models/query.py\", line 1074, in _fetch_all\n    self._result_cache = list(self.iterator())\n  File \"/usr/lib64/python2.7/site-packages/django/db/models/query.py\", line 52, in iter\n    results = compiler.execute_sql()\n  File \"/usr/lib64/python2.7/site-packages/django/db/models/sql/compiler.py\", line 852, in execute_sql\n    cursor.execute(sql, params)\n  File \"/usr/lib64/python2.7/site-packages/django/db/backends/utils.py\", line 64, in execute\n    return self.cursor.execute(sql, params)\n  File \"/usr/lib64/python2.7/site-packages/django/db/utils.py\", line 95, in exit\n    six.reraise(dj_exc_type, dj_exc_value, traceback)\n  File \"/usr/lib64/python2.7/site-packages/django/db/backends/utils.py\", line 64, in execute\n    return self.cursor.execute(sql, params)\n  File \"/usr/lib64/python2.7/site-packages/django/db/backends/sqlite3/base.py\", line 323, in execute\n    return Database.Cursor.execute(self, query, params)\nOperationalError: no such table: account_profile. python manage.py migrate\nOperations to perform:\n  Apply all migrations: admin, contenttypes, tagging, auth, sessions\nRunning migrations:\n  No migrations to apply.\nI have been executed ,but I do not think it's the problem\n@SEJeff . but graphite-web don't work,how to do?. When I visit through the browser, I will report the above error message. \nOperationalError: no such table: account_profile. Django (1.9)\ndjango-tagging (0.4.5)\nthe Issues can reopen?. Django must be 1.8.15!!!   it's ok. ",
    "davidhusselmann": "I just figured I'd make a quick change to make it easier for me to use (my graphite server is quite slow so the images take a few seconds to load up) - figured I'd PR it in case the community can also use it.. Thanks @deniszh.  I might do another enhancement to this soon as it isn't too obvious when images have finished loading when you do a full screen reload (not automatic refresh).. ",
    "raju-divakaran": "Hello, \nI am trying to find this. Both grafana and graphite web is slow. It was actually slow before the upgrade and the was the reason I tried the upgrade. But after upgrade it became very slow.\nYes I tried the graphite db upgrade.\nFor me it looks the carbon cache is slow in responding to graphite webapp requests.. I am having these from webapp error log\n[Mon Mar 06 17:14:48.178399 2017] [:error] [pid 47261] [remote 10.99.25.22:10944] mod_wsgi (pid=47261): Exception occurred processing WSGI script '/opt/graphite/conf/graphite.wsgi'.\n[Mon Mar 06 17:14:48.178452 2017] [:error] [pid 47261] [remote 10.99.25.22:10944] IOError: failed to write data\n[Mon Mar 06 17:14:48.985024 2017] [:error] [pid 47265] [remote 10.99.25.22:180] mod_wsgi (pid=47265): Exception occurred processing WSGI script '/opt/graphite/conf/graphite.wsgi'.\n[Mon Mar 06 17:14:48.985671 2017] [:error] [pid 47265] [remote 10.99.25.22:180] IOError: failed to write data\n[Mon Mar 06 17:14:50.262234 2017] [:error] [pid 3333] [remote X.X.X.X:4646] mod_wsgi (pid=47260): Exception occurred processing WSGI script '/opt/graphite/conf/graphite.wsgi'.\n[Mon Mar 06 17:14:50.262554 2017] [:error] [pid 3333] [remote X.X.X.X:4646] IOError: failed to write data\n[Mon Mar 06 17:14:50.598324 2017] [:error] [pid 3333] [remote X.X.X.X:4646] mod_wsgi (pid=47262): Exception occurred processing WSGI script '/opt/graphite/conf/graphite.wsgi'.\n[Mon Mar 06 17:14:50.598446 2017] [:error] [pid 3333] [remote X.X.X.X:6363] IOError: failed to write data\nI have masked the IP address and ports in above logs.\nThere are near to 3000 GET or POST requests against the grahite webapp.\nThanks. ",
    "pasanm": "Its cool. I got the package build for stretch :).  @jcharaoui  I got the python-django-tagging built for debian stretch and I also needed scandir to get the site back in working state.\npython-django-tagging - Generic tagging application for Django projects (Python 2)\npython-scandir - Backport of the \"scandir\" stdlib module (Python 2)\n. ",
    "jcharaoui": "Could you explain in more detail how you solved it?. ",
    "Himanshu-pupneja": "Hi @deniszh ,\nI tried that a long back but didn't work well.\ni will get it another try and see how it goes.\nThanks for your time and suggestion.\n-Himanshu. ",
    "ElsaHuang": "same issue here...install Graphite also failed.\npip install Graphite\nCollecting Graphite\n  Downloading graphite-0.71.tar.gz (56kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 61kB 221kB/s \n    Complete output from command python setup.py egg_info:\n    Traceback (most recent call last):\n      File \"\", line 1, in \n      File \"/private/var/folders/mn/fyndsss93cvcj14gjgc81g5w0000gn/T/pip-build-n0rhcr3p/Graphite/setup.py\", line 23\n        print \"EZ_setup\"\n                       ^\n    SyntaxError: Missing parentheses in call to 'print'\n----------------------------------------\n\nCommand \"python setup.py egg_info\" failed with error code 1 in /private/var/folders/mn/fyndsss93cvcj14gjgc81g5w0000gn/T/pip-build-n0rhcr3p/Graphite/. ",
    "JZO": "still no python3 support?. ",
    "Quentin-M": "Apps that forcefully log to disk are a major pain when using containers.. Thanks, yeah I have written and tested #2244 specifically to fix this.\nAre you mentioning the following variable? I am having trouble finding the location where it is being used and where the logic you are describing is. I haven't gotten time to look very closely yet though. So I apologize if I missed it blindly.\nhttps://github.com/graphite-project/graphite-web/blob/8ad9ad4060845ed51bf283d37b62bc59bf6b97e6/webapp/graphite/finders/remote.py#L52\nThat sounds great thought and thought it\u2019d be the default behavior. Before coming up with the fix, I added a single Redis instance for all 4 nodes, hoping that this is what graphite-web would do by default. To make sure I understand, you are saying that if all 3 remote instances have that Redis server, and are defined in CLUSTER_SERVERS with ?format=msgpack&noTags=1&local=1, then the receiving instance should properly query Redis for the tags, and pass the resolved lists of tags alongside the metrics query / return immediately in the case of autoComplete?. Thanks for your prompt and exhaustive answer. This parameter is not documented anywhere, though very useful - well, at least there is this referencable issue now!  \nFor completeness, I actually just found the branch from the initial PR, that does that.  \nI\u2019ll try that on my cluster as soon as I get to the office!  \nOn February 26, 2018 at 09:14:26, Dan Cech (notifications@github.com(mailto:notifications@github.com)) wrote:\n\nYup, that's it exactly. For remote servers defined with noTags=1 a graphite-web server will resolve the seriesByTag(...) calls to a list of tagged series and pass that resolved list to the remote servers rather than simply passing through the seriesByTag(...) request and letting the remote host resolve the call itself. Direct tag API calls like autocomplete will not be forwarded to remote hosts that have noTags=1 specified.\nIn your scenario with a single shared TagDB you'll likely see better performance that way since you'll cut the number of requests to the TagDB and avoid the clustering HTTP requests for tag API calls.\nIf you had the TagDB sharded with each graphite-web host having a separate DB that deals with only the series stored on that host then you would not want to use the noTags=1 option and instead allow each graphite-web to query its own TagDB.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub(https://github.com/graphite-project/graphite-web/issues/2243#issuecomment-368576420), or mute the thread(https://github.com/notifications/unsubscribe-auth/ABRUQbWz2bm9L8670HESoKWqHwKkCcktks5tYuZtgaJpZM4SSzz6).\n\n. ```\nFAIL: test_WhisperReader_get_intervals (tests.test_readers_whisper.WhisperReadersTests)\n\nTraceback (most recent call last):\n  File \"/home/travis/build/graphite-project/graphite-web/webapp/tests/test_readers_whisper.py\", line 129, in test_WhisperReader_get_intervals\n    self.assertEqual(int(interval.start),ts - 60)\nAssertionError: 1519637574 != 1519637573\n```\n^ Don't think my PR has anything with it.. This PR appears to break metrics discovery: https://github.com/graphite-project/graphite-web/issues/2352.. Great call @DanCech, thank you so much for your prompt answer. It did seem it was bound per worker, while all the factory code was static. I missed the Disabled test in get_finders, now it makes sense!\nI did notice in exception.info that quite a few requests would fail with the following, which is probably what is marking the backends as failed:\n```\nDuring handling of the above exception, another exception occurred:\nTraceback (most recent call last):\nFile \"/opt/graphite/webapp/graphite/worker_pool/pool.py\", line 43, in run\n    self.result = self.func(self.args, *self.kwargs)\nFile \"/opt/graphite/webapp/graphite/finders/remote.py\", line 153, in fetch\n    return reader.fetch_multi(start_time, end_time, now, requestContext)\nFile \"/opt/graphite/webapp/graphite/readers/remote.py\", line 96, in fetch_multi\n    data = self.deserialize(result)\nFile \"/opt/graphite/webapp/graphite/readers/remote.py\", line 141, in deserialize\n    raise Exception(\"Error decoding render response from %s: %s\" % (result.url_full, err))\nException: Error decoding render response from http://graphite-cache-0.graphite-cache.app-testnet.svc.cluster.local:8080/render/?[...]: Ran out of input\n```\nNo idea what causes this issue though.. ",
    "shivagopalan": "@obfuscurity Thanks a lot for your response. \n1. I do not understand how I can use two URL's for the same page. Maybe I am misunderstanding your point, could you show me an example of how this works?\n2. Thanks, this worked. As a follow up, I would like to assign a separate color to each of the events returned. I tried doing this - \"target=drawAsInfinite(events('*'))&colorList=red,yellow,green,purple\" , but the colors are not getting assigned to each of the individual events. Instead, all events are getting marked by the same color. What is the right way to do this?\nThank you once again. Appreciate your help. @deniszh Thank you for your response. \n1. I believe I may have to use Grafana eventually for my work, but for now the option @obfuscurity suggested should work just fine. Thank you for that, makes my life simpler :)\n2. I am trying to do something like this - \"&target=drawAsInfinite(color(events(%27*%27),[\"red\",\"green\"]))&from=18:10_20170317&until=18:30_20170317&format=svg\" but not successful so far. @obfuscurity thanks for sharing that link. I went through it but could not find how to assign a colorList to a number of events. Please advise if this functionality is supported. . @obfuscurity Sure. Thanks for your help. Please let me know if you are aware of this functionality being available. @deniszh Thank you very much. I will continue to look at this usecase. \n. ",
    "leochen4891": "I ran into this issue as well with graphite-web 1.0.2 and django 1.7.11.\nAdding TEMPLATE_DIRS solves the problem.. I have tried django 1.8.18 and it didn't work because of some \"Table auth_user does not exist\" issue.\nMaybe I can try django 1.9 ---- still have the \"Table auth_user does not exist\" issue. I'm hitting this issue as well. Have made a PR to fix it.. Use @zasca's example\n```\n\n\n\nabsolute_path='/var/lib/graphite/whisper/Env/HTTP/NumConnections.wsp'\nmetric_path='Env.HTTP.NumConnections'\nreal_absolute_path='/mnt/ssd/graphite/db/Env/HTTP/NumConnections.wsp'\nrelative_fs_path = metric_path.replace('.', os.sep)\nprint relative_fs_path\nEnv/HTTP/NumConnections\nabsolute_path_no_ext, _ext = os.path.splitext(absolute_path)\nprint absolute_path_no_ext\n/var/lib/graphite/whisper/Env/HTTP/NumConnections\nbase_fs_path = os.path.dirname(absolute_path_no_ext[:-len(relative_fs_path)])\nprint base_fs_path\n/var/lib/graphite/whisper\nreal_base_fs_path=\"/mnt/ssd/graphite/db/\"\nreal_relative_fs_path = real_absolute_path[len(real_base_fs_path):].lstrip(os.sep)\nprint real_relative_fs_path\nEnv/HTTP/NumConnections.wsp\nfs_to_metric(real_relative_fs_path)\n'Env.HTTP.NumConnections'\n``\n. Thanks for the review.\nunit test added.\nwithout the fix, the test will fail withAssertionError: 'HTTP.NumConnections' != 'Env.HTTP.NumConnections'`. @cbowman0 \nGood point to cover the case that the symbolic link is inside the metric path.\nTest case added.\n\n\n\n. No, I'm installing graphite on a VM. \nStrangely, I don't find manage.py anywhere on the machine.. PYTHONPATH=$GRAPHITE_ROOT/webapp django-admin.py migrate --settings=graphite.settings --run-syncdb works for me\nThanks @DanCech . ",
    "worrathep": "@deniszh \nI try run syncdb again but not complete\n[root@localhost ~]# PYTHONPATH=/opt/graphite/webapp django-admin.py migrate --settings=graphite.settings --run-syncdb\nTraceback (most recent call last):\n  File \"/usr/bin/django-admin.py\", line 5, in \n    management.execute_from_command_line()\n  File \"/usr/lib64/python2.7/site-packages/django/core/management/init.py\", line 353, in execute_from_command_line\n    utility.execute()\n  File \"/usr/lib64/python2.7/site-packages/django/core/management/init.py\", line 345, in execute\n    self.fetch_command(subcommand).run_from_argv(self.argv)\n  File \"/usr/lib64/python2.7/site-packages/django/core/management/base.py\", line 348, in run_from_argv\n    self.execute(args, *cmd_options)\n  File \"/usr/lib64/python2.7/site-packages/django/core/management/base.py\", line 398, in execute\n    self.check()\n  File \"/usr/lib64/python2.7/site-packages/django/core/management/base.py\", line 426, in check\n    include_deployment_checks=include_deployment_checks,\n  File \"/usr/lib64/python2.7/site-packages/django/core/checks/registry.py\", line 75, in run_checks\n    new_errors = check(app_configs=app_configs)\n  File \"/usr/lib64/python2.7/site-packages/django/core/checks/urls.py\", line 13, in check_url_config\n    return check_resolver(resolver)\n  File \"/usr/lib64/python2.7/site-packages/django/core/checks/urls.py\", line 23, in check_resolver\n    for pattern in resolver.url_patterns:\n  File \"/usr/lib64/python2.7/site-packages/django/utils/functional.py\", line 33, in get\n    res = instance.dict[self.name] = self.func(instance)\n  File \"/usr/lib64/python2.7/site-packages/django/core/urlresolvers.py\", line 417, in url_patterns\n    patterns = getattr(self.urlconf_module, \"urlpatterns\", self.urlconf_module)\n  File \"/usr/lib64/python2.7/site-packages/django/utils/functional.py\", line 33, in get\n    res = instance.dict[self.name] = self.func(instance)\n  File \"/usr/lib64/python2.7/site-packages/django/core/urlresolvers.py\", line 410, in urlconf_module\n    return import_module(self.urlconf_name)\n  File \"/usr/lib64/python2.7/importlib/init.py\", line 37, in import_module\n    import(name)\n  File \"/opt/graphite/webapp/graphite/urls.py\", line 22, in \n    url('^render/?', include('graphite.render.urls')),\n  File \"/usr/lib64/python2.7/site-packages/django/conf/urls/init.py\", line 52, in include\n    urlconf_module = import_module(urlconf_module)\n  File \"/usr/lib64/python2.7/importlib/init.py\", line 37, in import_module\n    import(name)\n  File \"/opt/graphite/webapp/graphite/render/urls.py\", line 16, in \n    from . import views\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 34, in \n    from graphite.remote_storage import extractForwardHeaders, prefetchRemoteData\n  File \"/opt/graphite/webapp/graphite/remote_storage.py\", line 10, in \n    from graphite.readers import FetchInProgress\n  File \"/opt/graphite/webapp/graphite/readers.py\", line 9, in \n    from scandir import scandir, stat # noqa # pylint: disable=unused-import\nImportError: No module named scandir. Hello @deniszh \ni install scandir and runsyncdb again it ok \nthank you. ",
    "replay": "thx for the report @iksaif. could you please describe your env some more?\nhow many cluster nodes do you have, is USE_WORKER_POOL true?. oh, great debugging work, i must have missed that else in https://github.com/criteo-forks/graphite-web/commit/667e6d5d89e523b51e4d66ac50ec38519a4ddb56\n. @iksaif is it possible that in your settings you had USE_WORKER_POOL=True and REMOTE_PREFETCH_DATA=False? \nIn that case I think I can see why that's happening. \nIf it's not too much trouble, could you try setting REMOTE_PREFETCH_DATA to True just to see if this fixes it?\nThere seem to be only two locations where the cache is used, and only one of them might be called from a pool worker. Setting REMOTE_PREFETCH_DATA to True should disable that piece of code.. Yes I agree that one thread per backend should be a sensible default setting and should be sufficient for most people.\nSo i think the reason for the large number of connections is that To provide thread-safety, a different instance of the cache backend will be returned for each thread.:\nhttps://docs.djangoproject.com/en/1.10/topics/cache/#accessing-the-cache\nIt should be relatively easy to wrap the cache with locks and only instantiate it once for each of our thread pools, but if the default setting is to have only one worker per pool then this is probably not necessary.. This looks really good, still going through it in detail. Yeah, appears that this was the fix :). Added a basic validator:\n```\n(graphite-web) mst@mst-nb1:~/documents/code/go/src/github.com/graphite-project/graphite-web/webapp$ python \nPython 2.7.15rc1 (default, Nov 12 2018, 14:31:15) \n[GCC 7.3.0] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nfrom graphite.tags import utils\nutils.TaggedSeries.validateCharacters('abc', 'cba~')\nFalse\nutils.TaggedSeries.validateCharacters('abc', 'c;ba')\nFalse\nutils.TaggedSeries.validateCharacters('ab!c', 'cba')\nFalse\nutils.TaggedSeries.validateCharacters('a^bc', 'cba')\nFalse\nutils.TaggedSeries.validateCharacters('a=bc', 'cba')\nFalse\nutils.TaggedSeries.validateCharacters('abc', 'cb=a')\nTrue\nutils.TaggedSeries.validateCharacters('abc', 'cba')\nTrue\n```. Just some more testing, by actually running graphite and calling the API:\n\n\n\nmst@mst-nb1:~$ curl -XPOST 'localhost:8000/tags/tagSeries' --data-urlencode 'path=disk.used;rack=a1'\n\"disk.used;rack=a1\"\nmst@mst-nb1:~$ curl -XPOST 'localhost:8000/tags/tagSeries' --data-urlencode 'path=disk.used;rack=a1~'\n{\"error\": \"Tag/Value contains invalid characters: rack/a1~\"}\nmst@mst-nb1:~$ curl -XPOST 'localhost:8000/tags/tagSeries' --data-urlencode 'path=disk.used;rac!k=a1'\n{\"error\": \"Tag/Value contains invalid characters: rac!k/a1\"} \nmst@mst-nb1:~$ curl -XPOST 'localhost:8000/tags/tagSeries' --data-urlencode 'path=disk.used;rack^=a1'\n{\"error\": \"Tag/Value contains invalid characters: rack^/a1\"} \nmst@mst-nb1:~$ curl -XPOST 'localhost:8000/tags/tagSeries' --data-urlencode 'path=disk.used;rack====a1'\n\"disk.used;rack====a1\"\nmst@mst-nb1:~$ curl -XPOST 'localhost:8000/tags/tagSeries' --data-urlencode 'path=disk.used;rack=~a1'\n{\"error\": \"Tag/Value contains invalid characters: rack/~a1\"}. @DanCech i'm trying to see where that's used, but i can't find it. Do we need that or can we drop it for the sake of reducing the size of the PR?. @DanCech Can I put that into a separate feature PR?. @DanCech I'm wondering why they've had this as a class property. This might be a bug, because it will mark all remote storages of this type as failed if one failed, or maybe that's the expected behavior but i don't understand why. I think I should just remove this property because it's not used anymore.. Good call, although that library seems to be pretty much undocumented except for http://lucasb.eyer.be/snips/python-thread-pool.html\nDo you have experience with it, is it reliable? I can give it a try. That multiprocessing.Pool is nice, thx for the suggestion. I replaced the self-made pool with it, the performance seems to be pretty much the same (i benchmarked both with three local backends), and it's way more simple.. i think that's just supposed to loop over the list, willfix. Fixed that. Good idea: https://github.com/graphite-project/graphite-web/pull/1818/commits/0007cd6c80b081ab4a1de4c4d74ec0d11be160db. That's to make that while loop because on each iteration it's checking again if the deadline has been reached or not. \nOr is your question more about why exactly 0.01 and not any other number? I think it just needed to be some small number to re-check the deadline often.. I guess it would be possible to calculate the remaining time until the deadline and then just pass it into get(). is this what you had in mind? https://github.com/graphite-project/graphite-web/pull/1818/commits/6ede6a97eb9dde2935923d20702071ec9814910d. yup, done: https://github.com/graphite-project/graphite-web/pull/1818/commits/412e37cd7b198208cebe6ef5965c6bc35ecead53. added that: https://github.com/graphite-project/graphite-web/pull/1818/commits/412e37cd7b198208cebe6ef5965c6bc35ecead53. Sorry, I rebased after commenting: https://github.com/graphite-project/graphite-web/pull/1818/commits/c3e7f0c528ef915ef761b8438b0e48bec8e67a02. sorry, this one https://github.com/graphite-project/graphite-web/pull/1818/commits/dd09fbf9c647dc48f9016f5b00929cdd56e8a3ef. https://github.com/graphite-project/graphite-web/pull/1818/commits/c3e7f0c528ef915ef761b8438b0e48bec8e67a02\n. is this used anywhere?. evaluate. I'm not sure if i just don't get what's happening here, but doesn't that just always call __init__() on the object it's inheriting from? what does that do?. This is a tiny unimportant detail: \nComparing two strings from the beginning is probably faster than search through a string for a character, because if query is not starting with _ it can return directly with False, so I'd reverse the order of these two checks.. Then the result could be kept for line 61, so it doesn't have to scan through the string on each iteration of the following for. I think I might be misunderstanding this regex. But shouldn't it also match uppercase chars in function names, like for example movingAverage(bla). F.e. at the  webapp/graphite/render/functions.py:1183 name might be set to such a value and then passed into TimeSeries(). This regex is defined twice, in webapp/graphite/tags/localdatabase.py as well. Might make sense to move them into one definition, just to make sure that future changes get applied to every location where they are needed.. That's nice. I was trying to find a race condition when there are different values/operators, but can't find one. I didn't check, but do you know what self.r.sscan_iter() (at :126) returns on a non-existent key? If it fails gracefully and only returns an empty [] then the self.r.sismember() would be an unnecessary round trip. would be nice if the cache key generation would be more configurable so additional differentiating factors can be added via configs. shouldn't that filter also be surrounded by () to prevent something like ^a|b. here the filter is not anchored at the beginning. that's fine if it's done before calling list_tags(), but it's kind of inconsistent with the local_database tagdb then. that's just a detail:\nbut wouldn't it be faster to first sort the list of tag strings and then build a list of {'tag': tag} dicts based on it, instead of having to build a lambda function and then call it many times inside the sort function to extract that tag back out again and sort by it?. furthermore, then the limit could be applied before building the list of dicts so the number of dicts that need to be instantiated would be limited to the limit. kind of similar like above, why not apply the limit earlier and save a lot of dicts and calls to self.r.scard()?. this regex only needs to be compiled once instead of in each iteration. same here, regex only needs to compile once. ",
    "yunstanford": "I have a graphite cluster that has 2 shards (data are different on these two nodes based on consistent hashing). And we also have Master Graphite-Web talk with these two shards and merge the results. But..... there are some chance (very low, about 1/60) that Master graphite-web just return data from one shard. but i can't find any exception from log around this. Does this fix the issue ? any idea ?. we have collectd, statsite, monitoring system connected to the graphite cluster.\neach shard includes a graphite-web, a carbon-relay-ng, 8 carbon-cache.\na master graphite-web fetch remote data from these two shards and merge them. (there is no data on master graphite-web host)\na carbon-relay-ng does consistent hashing in front of these two shards. (for writing)\nwriting part works fine based on consistent hashing. the thing is that our Master graphite-web sometimes(very unlikely, but i do observe) just return data from one shard, although most time it works fine. (basically it doesn't get data from the other shard). . ",
    "wdauchy": ":+1: . ",
    "UKNC": "I have graphite and carbon instances running on CentOS6. Graphite runs on 8080.\n[root@NQPC002 graphite]# netstat -tlpn | grep python\ntcp        0      0 0.0.0.0:2003                0.0.0.0:*                   LISTEN      30151/python        \ntcp        0      0 0.0.0.0:2004                0.0.0.0:*                   LISTEN      30151/python        \ntcp        0      0 0.0.0.0:7002                0.0.0.0:*                   LISTEN      30151/python\n```\n[root@NQPC002 graphite]# ps aux | grep python\nroot     25823  0.0  0.0 103316   864 pts/1    S+   12:04   0:00 grep python\ncarbon   30151 40.6  0.0 303724 17380 ?        Rl   Apr05 591:49 /usr/bin/python /usr/bin/carbon-cache start --config=/etc/carbon/carbon.conf --pidfile=/var/run/carbon-cache.pid --logdir=/var/log/carbon\n```\nHere is graphical description of the problem:\nhttp://pix.toile-libre.org/upload/original/1491473848.png. Great thanks for the link which mentioned whisper-dump.py. After looking at time stamps  it turned out  server's hwclock and date from which data was sent to carbon were incorrect. \nThanks again!. ",
    "tonychoe": "I am working at Oracle ;) \n\nOn Apr 6, 2017, at 2:15 AM, Denis Zhdanov notifications@github.com wrote:\nHi @tonychoe https://github.com/tonychoe,\nSorry for asking, but how do you know that Oracle using Graphite?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/graphite-project/graphite-web/pull/1894#issuecomment-292115980, or mute the thread https://github.com/notifications/unsubscribe-auth/AL7ZftnNvfjmVY15UmPcquCVC3Oem3A4ks5rtK1NgaJpZM4M0sIw.\n\n\n. ",
    "chriwill": "1893 is merged... closing this one....",
    "zivillian": "The example target may be misleading - there is an additional wildcard before the value list:\nprefix.*.{2,4,6,...,198,200}.component.*.value\nThis wildcard is matching some hundred folders, so the regular expressions are recompiled for each match for the first star. Without this first wildcard, the performance impact may be negligible.. I agree, that many wildcards may seem like an anti pattern. I'll try to illustrate my usecase - maybe there is a more elegant way to query the result.\nWe are using graphite to store usage metrics from our customers over time. The metrics are grouped by metric type - not by customer. The reference to our customer is stored in a different database. This way we can dynamically change the customer reference for any metric or exclude a single metric without deleting it from graphite.\nThe metrics are mostly stored using immutable guids, so my the structure in this scenario is something like this:\n\nusage\niaas\nb057671c-90d4-4081-ada4-31eb346edc72\ncpu\nram\ndisks\n06a58854-4a28-4de8-94a8-36b38ea2fd44\ntotal\nused\n\n\n375a06f6-b5e5-4b21-803f-efa969b11300\n...\n... (1500 more)\n\n\n\nthe graphite query then looks like this:\nalias(sumSeries(usage.iaas.*.disks.{06a58854-4a28-4de8-94a8-36b38ea2fd44,...}.used), 'Disk Size')\nI am able to remove the first wildcard and send a value list containing all the guids, but this will not help, since the value list is also matched via fnmatch. Another approach may be to specify multiple targets for sumSeries without any value list or wildcard - this is what I meant by \"workaround in client\".\nDisclaimer: I'm not a python developer\nI tried your suggestion to set the _MAXCACHE in local_settings.py and this solved the performance issue. My concern is, that this may introduce a memory constraint for large queries and I guess that I need to set this to a very high value like 3000.\nAs far as I can tell, the biggraphite implementation will not work out of the box, since it creates a single regex for the whole target, which will not work with the recursive directory traversal currently implemented.\n. I measured 5 to 10 requests for the above request with 243 values in the value list and 1543 matches for the first wildcard.\n_MAXCACHE = 3000\n\nResponse Time: 5,55s avg (may be limited by IO)\n\n_MAXCACHE = 300\n\nResponse Time: 7,08s avg (may be limited by IO)\n\n_MAXCACHE = 100\n\nResponse Time: 280s avg\n\nI tried to also measure the memory usage, but was unable to find any memory metric which shows reasonable difference. Also the wsgi processes seem to be recycled regularly after some time, so the largest difference I saw in ps aux was 850MB vs. 580 MB (VSZ).\nI will set _MAXCACHE to 3000 for now and look wether any memory problems arise.\n. We've tested the workaround over the last few weeks and had no problems until now. I guess this can be closed for now.. ",
    "blafrisch": "That's why I raised the question because the documentation around CARBONLINK_HOSTS says \"If multiple carbon-caches are running on this machine, each should be listed here so that the Graphite webapp may query the caches for data that has not yet been persisted. Remote carbon-cache instances in a multi-host clustered setup should not be listed here.\" If the documentation is to be believed then CARBONLINK_HOSTS should not contain 10.0.0.2 and 10.0.0.3 in 10.0.0.1's local_settings.. That's why I raised the question because the documentation around CARBONLINK_HOSTS says \"If multiple carbon-caches are running on this machine, each should be listed here so that the Graphite webapp may query the caches for data that has not yet been persisted. Remote carbon-cache instances in a multi-host clustered setup should not be listed here.\" If the documentation is to be believed then CARBONLINK_HOSTS should not contain 10.0.0.2 and 10.0.0.3 in 10.0.0.1's local_settings.. Okay, so the exception on line 40 of carbonlink.py is erroneous. This is what I was thinking. There's no other way that CARBONLINK_HOSTS can be local and have the situation as described above: 3 hosts, RF of 3, 2 relays/host.\n. 3x hosts: 10.0.0.1, 10.0.0.2, and 10.0.0.3 for simplicity.\n2x carbon caches per host: instance 'a' bound to port 2003 for LINE_RECEIVER_PORT and port 7002 for CACHE_QUERY_PORT; instance 'b' bound to port 2103 for LINE_RECEIVER_PORT and port 7102 for CACHE_QUERY_PORT\nCarbon relay configured with the following \nDESTINATIONS = ['10.0.0.1:2003:a', '10.0.0.1:2103:b', '10.0.0.2:2003:a', '10.0.0.2:2103:b', '10.0.0.3:2003:a', '10.0.0.3:2103:b']\nREPLICATION_FACTOR = 3\nDISTINCT_REPLICAS = True\nExample graphite-web config on 10.0.0.1:\nCLUSTER_SERVERS = ['10.0.0.2:80', '10.0.0.3:80']\nCARBONLINK_HOSTS = ['10.0.0.1:7002:a', '10.0.0.1:7102:b']\nREPLICATION_FACTOR = 3\nA graphite-web server cannot start with this configuration due to len(CARBONLINK_HOSTS) < REPLICATION_FACTOR.\nSo in my situation what should REPLICATION_FACTOR on graphite-web be?. Thank you for helping me understand the system a bit better. I was trying to avoid the 2-tier relay setup because I was having trouble grasping how I'd rebalance the cluster in that situation using tools like carbonate. It looks like if I did want to do something like that then I'd have to write new tooling to rebalance.. Does that statement remain true if the 2x instances of carbon-cache on each host are writing to unique data directories? i.e. instance 'a' is writing to LOCAL_DATA_DIR=/data1 and instance 'b' is writing to LOCAL_DATA_DIR=/data2?. I was more concerned about having each carbon-relay on a given host having its own dedicated drives for performance since I've read that carbon-relay is I/O bound rather than CPU bound. Would it be preferable to RAID 0 the disks and have the carbon caches write to the same LOCAL_DATA_DIR?. Yep, multiple hosts for redundancy hence the REPLICATION_FACTOR > 1 and the proposed relay setup. I have a lot to look into now including carbonzipper and all of those related components but this has certainly helped get my mind set straight about how to architect the layout.. ",
    "usmanC9": "\nOr you can use reference implementation:\n\nTwo tier relay - one relay (tier0) send data to 3 hosts, with e.g. RF=2. Second relay (tier1) just taking metrics from tier0 relay and spreading them across 2-4 local caches per host (for scalability).\nGraphite-web installed on each host, having in CARBONLINK_HOSTS only local caches (from tier1) and 2 other graphite-web in  CLUSTER_SERVERS.\n\n\nAccording to the reference architecture , tier0 relay may cause single point of failure. Actually I am trying to setup an HA architecture. For that I have added another relay on tier0 and add a LB on top of it. Does it makes sense ? or is there any alternate approach to achieve HA on tier0 ?. ",
    "pipamc": "@DanCech Can I have your test if you feel Okey? In my compute, I can't get so high improvement.. ",
    "dugajean": "Netdata sends data at least every 10 seconds, but it might take more from time to time.. @deniszh I tried increasing the retention policy to 40s and then lower it to 10s. I still keep getting those gaps for some reason. Isn't there something that can be done now, without extending transformNull in order to fix this?. ",
    "pengyusong": "some details as follows:\n\u2002\u2002\u2002\u2002apache + graphite-web(fnv1a_ch hash)\n\u2002\u2002\u2002\u2002carbon cache with 3 instances\n\u2002\u2002\u2002\u2002carbon-c-relay with fnv1a_ch cluster\nNotice:\n\u2002\u2002\u2002\u2002graphite version is 1.0.0-pre. i think i have fix this issue, the problem is not about graphite, it about collectd.\nwhen i set collectd read thread num larger than configured read plugins num, it will be easily reproduct the problem.\nSo, i change the num equal to the num of read plugins, then, the problem disappeared. this issue same as #1952. @deniszh  please help me solve this issue, thank you\nIt seems like pyparsing library's bug. In multi-thread environment, grammar.parseString  will sometimes failed. please check this. yes, i put one lock in evaluateTarget func, around \"grammar.parseString(target)\", it seems work, but i think simply add a lock may be influence performance, so have some better solutions?\nversions: \n- graphite  v1.0.1\n- pyparsing  v2.2.0. pyparsing not throw an exception, just lose some data, i found this:\n\nYou can use pyparsing from any thread, and you can create multiple parsers each running in a separate thread, but you cannot concurrently use one parser from two different threads. Some users work around this by instantiating a separate parser per thread using pickle to quickly construct the parser at thread start time.\n. OK, i will try it.. @deniszh  i write a script to test it, this script use graphite's grammar file. but i run it with two thread, no lock. whatever version is, the result still parse fail: lose some parse data. show as follows:\nversion: 1.5.7\nthread2 -- [[u'alias']]\nthread1 -- [[[u'alias', [[[u'asPercent', [[u'windows.system.memory.available']], [[u'windows.system.memory.total']]]]], [u\"'XXX'\"]]]]\nbut when i use lock it run normally.\n\nscript code as follows:\n```\n-- coding: utf8 --\nfrom grammar import grammar\nimport time\nimport threading\nlock = threading.Lock()\nclass MyThread(threading.Thread):\n    def init(self, name):\n        threading.Thread.init(self)\n        self.name = name\ndef run(self):\n    self.parse()\n\ndef parse(self):\n    i = 1\n    targets = [\n        u\"alias(asPercent(windows.system.memory.available, windows.SACA_SERVER_2.system.memory.total), \"\n        u\"'XXX')\"\n    ]\n\n    while i <= 1000:\n        i += 1\n\n        tmp_results = [grammar.parseString(target) for target in targets]\n\n        count = 0\n        for result in tmp_results:\n            print self.name, '--', result\n            # print str(result).find(\"SERVER\")\n            # try:\n            #     if str(result).find(\"SERVER\") == -1:\n            #         print result\n            # except:\n            #     import traceback\n            #     traceback.print_exc()\n            count += 1\n\n        time.sleep(10)\n\nif name == \"main\":\n    import threading\nthread1 = MyThread('thread1')\nthread2 = MyThread('thread2')\nthread3 = MyThread('thread3')\n\nthreads = []\n\nthread1.start()\nthread2.start()\n\nthreads.append(thread1)\nthreads.append(thread2)\n\nfor t in threads:\n    t.join()\nprint \"Exiting Main Thread\"\n\n```\n. Obviously, in multi-thread env, pyparsing work not very well. @deniszh i try the version 2.1.0, this problem still exist. I cannot  install version 1.5.7, so, sorry.. Thank you very much! \nI just test lock around will be useful, but i doubt its performance of this method or else bugs. \nIt's up to you, Thanks. I'm a program beginner.\n. ",
    "drnarahari": "Can you explain clearly what do you mean by read thread num and read thread num? What is the value you set to each of them?\n. ",
    "mailRavi": "runniing the  below command solved thhe issue. Thanks \nmanage.py migrate --run-syncdb. ",
    "admin-pro": "Hello, i have the same issue, i was working with 0.9.13 version and i have performed an upgrade to 1.1.0(edited not 1.1.1) but the bug still present.\n. @deniszh i have updated my comment it is 1.1.0 not 1.1.1 sorry. Yes is that. Oh ok, sorry.. Hello all,\nSomeone can help me about the process to upgrade from graphite source installed version 0.9.13 to the latest please ?. Hello @deniszh  thank you for your return. To be honest. I have install from source the graphite after git clone : \ngit clone https://github.com/graphite-project/graphite-web.git && git clone https://github.com/graphite-project/carbon.git && git clone https://github.\ncom/graphite-project/whisper.git\ncd whisper; git checkout 0.9.13-pre1; python setup.py install\ncd ../carbon; git checkout 0.9.13-pre1; python setup.py install\ncd ../graphite-web; python check-dependencies.py; git checkout 0.9.13-pre1; python setup.py\nBy following a guide on the web... But i saw too late that the last version is 1.1.1 of graphite. So now, i want to know the right process to upgrade from my current version to the latest.\nActually my server are in production and received many metrics.\nMy OS is CentOS 7.4\nThanks a lot\nEdited: My graphite work with grafana 4.6\n  . @deniszh \nHere my Django version : \n\nSo i can't upgrade Graphite ?\nBut if i upgrade Django first from current version to 1.8 or newer , do you think it is possible ?\nThx a lots again dude\n. Thank you @deniszh \nYes, is not easy.\nI have installed django from \"pip install\" not from \"yum install\" as you can see bellow (from my history commands) : \ncurl -O https://bootstrap.pypa.io/get-pip.py\npython get-pip.py\npip install pyparsing\npip install 'Twisted<12.0'\npip install 'django<1.5'\npip install 'django-tagging==0.3.6'\nI'm thinking about restarting from scratch on a new server by a \"simple\" yum install graphite or install from source with the latest version... Then migrate all my wishpers to the new server.\nThank you a lot @deniszh for your help.. @deniszh Yes thank you man.\n@deniszh Do you know how i can verify exactly the version of graphite installed and used for my metrics please ?. Yes ! Great ! Thank you @deniszh \nDo you know if i will have a specific impact on my server and metrics received if i upgrade my django and my graphite version from source with following commands please ? \npip install https://github.com/graphite-project/whisper/tarball/master\npip install https://github.com/graphite-project/carbon/tarball/master\npip install https://github.com/graphite-project/graphite-web/tarball/master\nI have tried this on the clone of my production server and i have to installed new packages for install properly the latest version(he uninstall auto the older :+1:  ) but i'm not sure if all it's ok : \npip freeze list on the clone server after pip install new release : \ncarbon==1.2.0\nDjango==1.11.9\ndjango-tagging==0.3.1\ngraphite-web==1.2.0\nwhisper==1.2.0\nHere my packages version on the current production server : \ni have only django and whisper installed :rofl:  : \nwhisper==0.9.13\ndjango-tagging==0.3.6\nDjango==1.4.22\nConclusion, on the clone server i have performed all the following commands : \nyum install python-django python-pip python-cffi python-django-tagging python-simplejson python-rrdtool cairo-devel\npip install --upgrade django\nexport PYTHONPATH=\"/opt/graphite/lib/:/opt/graphite/webapp/\"\npip install https://github.com/graphite-project/whisper/tarball/master\npip install https://github.com/graphite-project/carbon/tarball/master\npip install https://github.com/graphite-project/graphite-web/tarball/master\nThanks a lot @deniszh \n. ",
    "mageo": "Hello @deniszh,\nI will have a look at the examples and write a test function.. @DanCech yes, your approach would give way more flexibility !\nDo you think something like this would be enough ?\nseries.name = newName.format(current)\n. Sorry, series is similar in french and singular is serie. I will change that. I will also update the test function to test the failure conditions.. ",
    "kexinrong": "Thanks for your interests! Just want to add that ASAP can also be implemented without FFT. FFT was originally introduced as an optimization to make ASAP run faster. Details for this optimization can be found in Section 4.3 of our paper. \nHere is an example implementation that does not involve FFT. . ",
    "hapx101": "@deniszh - Yes, I am using the master. Trying with 1.0.x branch.\nEdit: 1.0.x branch works.. ",
    "yesoreyeram": "I am also getting the same error. My installation script is available in https://raw.githubusercontent.com/yesoreyeram/graphite-setup/master/graphite-install.sh . yes @deniszh . Now it is working fine. . ",
    "jermorse": "Thanks I had never known about aliasSub().\nCheers!. ",
    "Haelium": "Getting what looks like a similar issue using django 1.11.10, graphite-web-1.1.2, and uWSGI (2.0.17)  on CentOS 7. Am using pypy 5.0.1, all packages installed via pip.\nFrom cffi callback <function uwsgi_pypy_wsgi_handler at 0x00007f418124f268>:\nTraceback (most recent call last):\n  File \"c callback\", line 472, in uwsgi_pypy_wsgi_handler\n  File \"/usr/lib64/pypy-5.0.1/site-packages/whitenoise/base.py\", line 66, in __call__\n    return self.application(environ, start_response)\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/core/handlers/wsgi.py\", line 157, in __call__\n    response = self.get_response(request)\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/core/handlers/base.py\", line 124, in get_response\n    response = self._middleware_chain(request)\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/core/handlers/exception.py\", line 43, in inner\n    response = response_for_exception(request, exc)\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/core/handlers/exception.py\", line 93, in response_for_exception\n    response = handle_uncaught_exception(request, get_resolver(get_urlconf()), sys.exc_info())\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/core/handlers/exception.py\", line 135, in handle_uncaught_exception\n    extra={'status_code': 500, 'request': request},\n  File \"/usr/lib64/pypy-5.0.1/lib-python/2.7/logging/__init__.py\", line 1204, in error\n    self._log(ERROR, msg, args, **kwargs)\n  File \"/usr/lib64/pypy-5.0.1/lib-python/2.7/logging/__init__.py\", line 1297, in _log\n    self.handle(record)\n  File \"/usr/lib64/pypy-5.0.1/lib-python/2.7/logging/__init__.py\", line 1307, in handle\n    self.callHandlers(record)\n  File \"/usr/lib64/pypy-5.0.1/lib-python/2.7/logging/__init__.py\", line 1347, in callHandlers\n    hdlr.handle(record)\n  File \"/usr/lib64/pypy-5.0.1/lib-python/2.7/logging/__init__.py\", line 770, in handle\n    self.emit(record)\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/utils/log.py\", line 119, in emit\n    message = \"%s\\n\\n%s\" % (self.format(no_exc_record), reporter.get_traceback_text())\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/views/debug.py\", line 334, in get_traceback_text\n    return t.render(c)\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/template/base.py\", line 207, in render\n    return self._render(context)\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/template/base.py\", line 199, in _render\n    return self.nodelist.render(context)\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/template/base.py\", line 990, in render\n    bit = node.render_annotated(context)\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/template/base.py\", line 957, in render_annotated\n    return self.render(context)\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/template/defaulttags.py\", line 216, in render\n    nodelist.append(node.render_annotated(context))\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/template/base.py\", line 957, in render_annotated\n    return self.render(context)\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/template/base.py\", line 1040, in render\n    output = self.filter_expression.resolve(context)\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/template/base.py\", line 736, in resolve\n    new_obj = func(obj, *arg_vals)\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/template/defaultfilters.py\", line 255, in stringformat\n    return (\"%\" + six.text_type(arg)) % value\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/utils/functional.py\", line 79, in __repr__\n    return repr(self.__cast())\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/utils/functional.py\", line 131, in __cast\n    return self.__text_cast()\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/utils/functional.py\", line 119, in __text_cast\n    return func(*self.__args, **self.__kw)\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/urls/base.py\", line 91, in reverse\n    return force_text(iri_to_uri(resolver._reverse_with_prefix(view, prefix, *args, **kwargs)))\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/urls/resolvers.py\", line 438, in _reverse_with_prefix\n    possibilities = self.reverse_dict.getlist(lookup_view)\n  File \"/usr/lib64/pypy-5.0.1/site-packages/django/urls/resolvers.py\", line 335, in reverse_dict\n    return self._reverse_dict[language_code]\nKeyError: 'en-us'. ",
    "mizhdi": "@deniszh no \nI try to install from source. ",
    "hehnope": "@deniszh I've ran into this issue installing from pip version 1.1.3 of all 3 components: graphite-web, carbon, and whisper.\nIf I install with --prefix=/opt/graphite then it seems to partially work. But, this is weird because it wasn't doing this earlier. Any idea what causes this? It also doesn't seem to work 100% of the time.. ",
    "fidergo-stephane-gourichon": "Turns out giving a title to the graph with an accent appeared to cause the problem.\nManually deleting the entry in carbon graph database allowed to see all previously saved graphs.\n. To see where is the problematic entry:\nsqlite3 /var/lib/graphite/graphite.db\nselect * from account_mygraph ;\nTo delete it:\ndelete from account_mygraph where ID=1\n. We now know the actual cause and an efficient workaround, yet the issue is still fully present.  Also, when given a title with an accent, another consequence is that the graph rendering feature is immediately broken, the graph image is replaced with a small broken icon in top left.. Bias? ;o) My PR has cleaner codecov-report ;-). (Just kidding, I guess PR #1980 has already reached the best it could do.) Thanks a lot for your effort!. ",
    "scraly": "Good question. I use graphite with Grafana so do you know where I can find this version?\nThw.. In my graphite repository I've got webapp/graphite-web-0.9.13-py2.7.egg-info repository.\nI don't know if it's the version used by last version of Grafana.. So, the graphite version we use is: 0.9.12 . Hi,\nI search for whisper files smaller than 676k (the size of correct files) and I found 0 files ... :-(\nWe don't have full disk space issue but our sevrer have been rebooted by our hoster, maybe it's the root cause but how to fix the problem?\nHave you some ideas ?\nThanks.\n. ",
    "hamshiva": "Ok I just upgraded to 1.0.1 and everything is working as expected. My version was quite old (I think it was 0.9.10) :). ",
    "rajats105": "I am providing you with the incorrect response that was received for the requests.\nFor those same requests now when I query I am getting correct results.\nRequest/response under Queried now are correct responses, whereas the other one is incorrect response.\nPlease note that the Queried now response are being requested with absolute from/until params, whereas the Queried on 'date' is taken from our logs which are using relative from/until params.\nCase 1\nQueried on 2017-07-05 15:28:43\nRequest :: curl -X GET 'http://webapp.graphite.xyz.com/render?from=-20min&until=-5min&target=scale(divideSeries(collectd.production.ec2.ip-w-x-y-z.processes-uvwx.ps_rss,sumSeries(collectd.production.ec2.ip-w-x-y-z.memory.*)),100)&format=json'\nResponse :: json[{\"target\": \"scale(divideSeries(collectd.production.ec2.ip-w-x-y-z.processes-uvwx.ps_rss,sumSeries(collectd.production.ec2.ip-w-x-y-z.memory.*)),100)\", \"datapoints\": [[970.7317073170732, 1499267400], [491.35802469135797, 1499267700], [723.6363636363636, 1499268000]]}]\nQueried now\nRequest :: curl -X GET 'http://webapp.graphite.xyz.com/render?from=15:08_20170705&until=15:23_20170705&target=scale(divideSeries(collectd.production.ec2.ip-w-x-y-z.processes-uvwx.ps_rss,sumSeries(collectd.production.ec2.ip-w-x-y-z.memory.*)),100)&format=json'\nResponse :: json[{\"target\": \"scale(divideSeries(collectd.production.ec2.ip-w-x-y-z.processes-uvwx.ps_rss,sumSeries(collectd.production.ec2.ip-w-x-y-z.memory.*)),100)\", \"datapoints\": [[0.04125754660651496, 1499267400], [0.04125754660651496, 1499267700], [0.04125754660651496, 1499268000]]}]\nCase 2\nQueried on 2017-07-05 15:30:40\nRequest :: curl -X GET 'http://webapp.graphite.xyz.com/render?from=-20min&until=-5min&target=scale(divideSeries(collectd.production.ec2.ip-w-x-y-z.processes-pqrs.ps_rss,sumSeries(collectd.production.ec2.ip-w-x-y-z.memory.*)),100)&format=json'\nResponse :: json[{\"target\": \"scale(divideSeries(collectd.production.ec2.ip-w-x-y-z.processes-pqrs.ps_rss,sumSeries(collectd.production.ec2.ip-w-x-y-z.memory.*)),100)\", \"datapoints\": [[302.6092710494313, 1499267700], [304.3672651834895, 1499268000], [320.3823592531655, 1499268300]]}]\nQueried now\nRequest :: curl -X GET 'http://webapp.graphite.xyz.com/render?from=15:10_20170705&until=15:25_20170705&target=scale(divideSeries(collectd.production.ec2.ip-w-x-y-z.processes-pqrs.ps_rss,sumSeries(collectd.production.ec2.ip-w-x-y-z.memory.*)),100)&format=json'\nResponse :: json[{\"target\": \"scale(divideSeries(collectd.production.ec2.ip-w-x-y-z.processes-pqrs.ps_rss,sumSeries(collectd.production.ec2.ip-w-x-y-z.memory.*)),100)\", \"datapoints\": [[8.274735275663945, 1499267700], [8.32280695577161, 1499268000], [8.760733604158323, 1499268300]]}]\nCase 3\nQueried on 2017-07-05 15:21:35\nRequest :: curl -X GET 'http://webapp.graphite.xyz.com/render?from=-35min&until=-5min&target=absolute(offset(sumSeriesWithWildcards(collectd.production.ec2.ip-w-x-y-z.cpu-*.cpu-{wait,idle},6),-100))&format=json'\nResponse :: json[{\"target\": \"absolute(offset(collectd.production.ec2.ip-w-x-y-z.cpu-0,-100))\", \"datapoints\": [[99.956667, 1499266200], [99.973333, 1499266500], [99.966667, 1499266800], [99.966667, 1499267100], [99.963333, 1499267400], [99.966667, 1499267700]]}, {\"target\": \"absolute(offset(collectd.production.ec2.ip-w-x-y-z.cpu-1,-100))\", \"datapoints\": [[1.1833349999999996, 1499266200], [1.1299819999999983, 1499266500], [1.356650000000002, 1499266800], [1.3066809999999975, 1499267100], [1.259990000000002, 1499267400], [1.2466689999999971, 1499267700]]}]\nQueried now\nRequest :: curl -X GET 'http://webapp.graphite.xyz.com/render?from=14:46_20170705&until=15:16_20170705&target=absolute(offset(sumSeriesWithWildcards(collectd.production.ec2.ip-w-x-y-z.cpu-*.cpu-{wait,idle},6),-100))&format=json'\nResponse :: json[{\"target\": \"absolute(offset(collectd.production.ec2.ip-w-x-y-z.cpu-0,-100))\", \"datapoints\": [[0.4200000000000017, 1499266200], [0.4133119999999906, 1499266500], [0.486653000000004, 1499266800], [0.4566810000000032, 1499267100], [0.453322, 1499267400], [0.44000400000000184, 1499267700]]}, {\"target\": \"absolute(offset(collectd.production.ec2.ip-w-x-y-z.cpu-1,-100))\", \"datapoints\": [[1.1833349999999996, 1499266200], [1.1299819999999983, 1499266500], [1.356650000000002, 1499266800], [1.3066809999999975, 1499267100], [1.259990000000002, 1499267400], [1.2466689999999971, 1499267700]]}]\nCase 4\nQueried on 2017-07-05 15:01:08\nRequest :: curl -X GET 'http://webapp.graphite.xyz.com/render?from=-35min&until=-5min&target=scale(divideSeries(collectd.production.ec2.ip-w-x-y-z.memory.memory-used,sumSeries(collectd.production.ec2.ip-w-x-y-z.memory.*)),100)&format=json'\nResponse :: json[{\"target\": \"scale(divideSeries(collectd.production.ec2.ip-w-x-y-z.memory.memory-used,sumSeries(collectd.production.ec2.ip-w-x-y-z.memory.*)),100)\", \"datapoints\": [[98.86561823052527, 1499265000], [98.89608393872908, 1499265300], [98.93428705945156, 1499265600], [99.17636048082642, 1499265900], [99.13713056913697, 1499266200], [99.13348292778778, 1499266500]]}]\nQueried now\nRequest :: curl -X GET 'http://webapp.graphite.xyz.com/render?from=14:26_20170705&until=14:56_20170705&target=scale(divideSeries(collectd.production.ec2.ip-w-x-y-z.memory.memory-used,sumSeries(collectd.production.ec2.ip-w-x-y-z.memory.*)),100)&format=json'\nResponse :: json[{\"target\": \"scale(divideSeries(collectd.production.ec2.ip-w-x-y-z.memory.memory-used,sumSeries(collectd.production.ec2.ip-w-x-y-z.memory.*)),100)\", \"datapoints\": [[48.10253867037586, 1499265000], [49.88782937740536, 1499265300], [52.209201267892404, 1499265600], [53.74422946094032, 1499265900], [51.904210641121004, 1499266200], [52.3104898483997, 1499266500]]}]\nCase 5\nQueried on 2017-07-05 14:57:44\nRequest :: curl -X GET 'http://webapp.graphite.xyz.com/render?from=-20min&until=-5min&target=scale(divideSeries(collectd.production.ec2.ip-w-x-y-z.processes-abcd.ps_rss,sumSeries(collectd.production.ec2.ip-w-x-y-z.memory.*)),100)&format=json'\nResponse :: json[{\"target\": \"scale(divideSeries(collectd.production.ec2.ip-w-x-y-z.processes-abcd.ps_rss,sumSeries(collectd.production.ec2.ip-w-x-y-z.memory.*)),100)\", \"datapoints\": [[18368.421052631576, 1499265600], [18209.090909090908, 1499265900], [40085.71428571428, 1499266200]]}]\nQueried now\nRequest :: curl -X GET 'http://webapp.graphite.xyz.com/render?from=14:37_20170705&until=14:52_20170705&target=scale(divideSeries(collectd.production.ec2.ip-w-x-y-z.processes-abcd.ps_rss,sumSeries(collectd.production.ec2.ip-w-x-y-z.memory.*)),100)&format=json'\nResponse :: json[{\"target\": \"scale(divideSeries(collectd.production.ec2.ip-w-x-y-z.processes-abcd.ps_rss,sumSeries(collectd.production.ec2.ip-w-x-y-z.memory.*)),100)\", \"datapoints\": [[1.4471239965501228, 1499265600], [1.4534473893717244, 1499265900], [1.454380348968354, 1499266200]]}]\nCase 6\nQueried on 2017-07-05 14:56:40\nRequest :: curl -X GET 'http://webapp.graphite.xyz.com/render?from=-35min&until=-5min&target=absolute(offset(sumSeriesWithWildcards(collectd.production.ec2.ip-w-x-y-z.cpu-*.cpu-{wait,idle},6),-100))&format=json'\nResponse :: json[{\"target\": \"absolute(offset(collectd.production.ec2.ip-w-x-y-z.cpu-0,-100))\", \"datapoints\": [[99.956667, 1499264700], [99.956667, 1499265000], [99.96, 1499265300], [99.963333, 1499265600], [99.953333, 1499265900], [99.96, 1499266200]]}]\nQueried now\nRequest :: curl -X GET 'http://webapp.graphite.xyz.com/render?from=14:21_20170705&until=14:51_20170705&target=absolute(offset(sumSeriesWithWildcards(collectd.production.ec2.ip-w-x-y-z.cpu-*.cpu-{wait,idle},6),-100))&format=json'\nResponse :: json[{\"target\": \"absolute(offset(collectd.production.ec2.ip-w-x-y-z.cpu-0,-100))\", \"datapoints\": [[1.5966729999999956, 1499264700], [1.6099940000000004, 1499265000], [1.6366639999999961, 1499265300], [1.6333430000000106, 1499265600], [1.7233209999999985, 1499265900], [1.680025999999998, 1499266200]]}]\n. ### storage-schemas.conf\n```\nSchema definitions for Whisper files. Entries are scanned in order,\nand first match wins. This file is scanned for changes every 60 seconds.\n\n[name]\npattern = regex\nretentions = timePerPoint:timeToStore, timePerPoint:timeToStore, ...\n\nCarbon's internal metrics. This entry should match what is specified in\nCARBON_METRIC_PREFIX and CARBON_METRIC_INTERVAL settings\n[collectd]\npriority = 120\npattern = ^(collectd).\nretentions = 300s:60d\n[com]\npriority = 110\npattern = .*\nretentions = 300s:30d\n[carbon]\npattern = ^carbon.\nretentions = 300s:60d\n[default_1min_for_1day]\npattern = .*\nretentions = 60s:1d\n```\nstorage-aggregation.conf\n```\nAggregation methods for whisper files. Entries are scanned in order,\nand first match wins. This file is scanned for changes every 60 seconds\n\n[name]\npattern = \nxFilesFactor = \naggregationMethod = \n\nname: Arbitrary unique name for the rule\npattern: Regex pattern to match against the metric name\nxFilesFactor: Ratio of valid data points required for aggregation to the next retention to occur\naggregationMethod: function to apply to data points for aggregation\n\n[counters_fall_here]\npattern = ^(counters).(production)\nxFilesFactor = 0.0\naggregationMethod = sum\n[timers_fall_here]\npattern = .*\nxFilesFactor= 0.0\naggregationMethod = average\n. Actually cpu-0 is not a metric in itself.\n.\n\u251c\u2500\u2500 cpu-0\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cpu-idle.wsp\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cpu-interrupt.wsp\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cpu-softirq.wsp\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cpu-steal.wsp\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 cpu-user.wsp\n\u251c\u2500\u2500 df-dev\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 df_complex-reserved.wsp\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 df_complex-used.wsp\n```\nRunning whisper-info in all of the metrics mentioned above gives same output.\nwhisper-info.py /opt/graphite/storage/whisper/collectd/production/ec2/ip-x-x-x-x/cpu-0/cpu-{xyz}.wsp\n```maxRetention: 5184000\nxFilesFactor: 0.0\naggregationMethod: average\nfileSize: 207388\nArchive 0\nretention: 5184000\nsecondsPerPoint: 300\npoints: 17280\nsize: 207360\noffset: 28\n```\n. ",
    "rpk113": "tried django 1.9\n[Fri Jul 07 18:47:52.718850 2017] [wsgi:error] [pid 28770:tid 140199944910592]   File \"/python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/conf/init.py\", line 137, in init\n[Fri Jul 07 18:47:52.718876 2017] [wsgi:error] [pid 28770:tid 140199944910592]     os.path.exists(os.path.join(zoneinfo_root, *(self.TIME_ZONE.split('/'))))):\n[Fri Jul 07 18:47:52.718905 2017] [wsgi:error] [pid 28770:tid 140199944910592] AttributeError: 'America/New_York' object has no attribute 'split'\nsame issue. import sys, os\nsys.path.append('mdsinfra/mds_software/collectd/common/scripts')\nsys.path.append('python/tzlocal/1.2/common/tzlocal-1.2-py2.7.egg')\nimport cmf_werks, tzlocal\nlogdir = cmf_werks.getLogDir()\nTIME_ZONE = America/New_York\nGRAPHITE_ROOT = '{}graphite'.format(logdir)\nCONF_DIR = '{}graphite/conf'.format(logdir)\nSTORAGE_DIR = '{}carbon/storage'.format(logdir)\nSTATIC_ROOT = '{}graphite/static'.format(logdir)\nLOG_DIR = logdir\nDASHBOARD_CONF = '{}/dashboard.conf'.format(CONF_DIR)\nGRAPHTEMPLATES_CONF = '{}/conf/graphTemplates.conf'.format(CONF_DIR)\nWHISPER_DIR = '{}/whisper'.format(STORAGE_DIR). Does that include when incorporating it into an httpd server?\nI changed to this TIME_ZONE = 'America/New_York'\nbut the local.setttings.py doesn't seem to recompile.\nSo I still get my error\n. Maybe I'm missing something, here is full error..\n[Mon Jul 10 10:51:22.008803 2017] [wsgi:error] [pid 23880:tid 140562645305088] mod_wsgi (pid=23880): Target WSGI script '/data/mktlocal/logs/mktdata/graphite/conf/graphite.wsgi' cannot be loaded as Python module.\n[Mon Jul 10 10:51:22.008869 2017] [wsgi:error] [pid 23880:tid 140562645305088] mod_wsgi (pid=23880): Exception occurred processing WSGI script '/data/mktlocal/logs/mktdata/graphite/conf/graphite.wsgi'.\n[Mon Jul 10 10:51:22.008912 2017] [wsgi:error] [pid 23880:tid 140562645305088] Traceback (most recent call last):\n[Mon Jul 10 10:51:22.008951 2017] [wsgi:error] [pid 23880:tid 140562645305088]   File \"/data/mktlocal/logs/mktdata/graphite/conf/graphite.wsgi\", line 5, in \n[Mon Jul 10 10:51:22.009023 2017] [wsgi:error] [pid 23880:tid 140562645305088]     from graphite.wsgi import application\n[Mon Jul 10 10:51:22.009056 2017] [wsgi:error] [pid 23880:tid 140562645305088]   File \"mdsinfra/amrs/graphite/common/webapp/graphite/wsgi.py\", line 16, in \n[Mon Jul 10 10:51:22.009149 2017] [wsgi:error] [pid 23880:tid 140562645305088]     application = get_wsgi_application()\n[Mon Jul 10 10:51:22.009185 2017] [wsgi:error] [pid 23880:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/core/wsgi.py\", line 13, in get_wsgi_application\n[Mon Jul 10 10:51:22.009267 2017] [wsgi:error] [pid 23880:tid 140562645305088]     django.setup()\n[Mon Jul 10 10:51:22.009382 2017] [wsgi:error] [pid 23880:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/init.py\", line 17, in setup\n[Mon Jul 10 10:51:22.009472 2017] [wsgi:error] [pid 23880:tid 140562645305088]     configure_logging(settings.LOGGING_CONFIG, settings.LOGGING)\n[Mon Jul 10 10:51:22.009507 2017] [wsgi:error] [pid 23880:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/conf/init.py\", line 55, in getattr\n[Mon Jul 10 10:51:22.009634 2017] [wsgi:error] [pid 23880:tid 140562645305088]     self._setup(name)\n[Mon Jul 10 10:51:22.009668 2017] [wsgi:error] [pid 23880:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/conf/init.py\", line 43, in _setup\n[Mon Jul 10 10:51:22.009721 2017] [wsgi:error] [pid 23880:tid 140562645305088]     self._wrapped = Settings(settings_module)\n[Mon Jul 10 10:51:22.009751 2017] [wsgi:error] [pid 23880:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/conf/init.py\", line 137, in init\n[Mon Jul 10 10:51:22.009791 2017] [wsgi:error] [pid 23880:tid 140562645305088]     os.path.exists(os.path.join(zoneinfo_root, (self.TIME_ZONE.split('/'))))):\n[Mon Jul 10 10:51:22.009832 2017] [wsgi:error] [pid 23880:tid 140562645305088] AttributeError: 'America/New_York' object has no attribute 'split'\n[Mon Jul 10 10:51:22.015776 2017] [wsgi:error] [pid 23882:tid 140562645305088] mod_wsgi (pid=23882): Target WSGI script '/data/mktlocal/logs/mktdata/graphite/conf/graphite.wsgi' cannot be loaded as Python module.\n[Mon Jul 10 10:51:22.015805 2017] [wsgi:error] [pid 23882:tid 140562645305088] mod_wsgi (pid=23882): Exception occurred processing WSGI script '/data/mktlocal/logs/mktdata/graphite/conf/graphite.wsgi'.\n[Mon Jul 10 10:51:22.015835 2017] [wsgi:error] [pid 23882:tid 140562645305088] Traceback (most recent call last):\n[Mon Jul 10 10:51:22.015860 2017] [wsgi:error] [pid 23882:tid 140562645305088]   File \"/data/mktlocal/logs/mktdata/graphite/conf/graphite.wsgi\", line 5, in \n[Mon Jul 10 10:51:22.015919 2017] [wsgi:error] [pid 23882:tid 140562645305088]     from graphite.wsgi import application\n[Mon Jul 10 10:51:22.015939 2017] [wsgi:error] [pid 23882:tid 140562645305088]   File \"mdsinfra/amrs/graphite/common/webapp/graphite/wsgi.py\", line 16, in \n[Mon Jul 10 10:51:22.016017 2017] [wsgi:error] [pid 23882:tid 140562645305088]     application = get_wsgi_application()\n[Mon Jul 10 10:51:22.016044 2017] [wsgi:error] [pid 23882:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/core/wsgi.py\", line 13, in get_wsgi_application\n[Mon Jul 10 10:51:22.016112 2017] [wsgi:error] [pid 23882:tid 140562645305088]     django.setup()\n[Mon Jul 10 10:51:22.016134 2017] [wsgi:error] [pid 23882:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/init.py\", line 17, in setup\n[Mon Jul 10 10:51:22.016199 2017] [wsgi:error] [pid 23882:tid 140562645305088]     configure_logging(settings.LOGGING_CONFIG, settings.LOGGING)\n[Mon Jul 10 10:51:22.016221 2017] [wsgi:error] [pid 23882:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/conf/init.py\", line 55, in getattr\n[Mon Jul 10 10:51:22.016333 2017] [wsgi:error] [pid 23882:tid 140562645305088]     self._setup(name)\n[Mon Jul 10 10:51:22.016355 2017] [wsgi:error] [pid 23882:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/conf/init.py\", line 43, in _setup\n[Mon Jul 10 10:51:22.016387 2017] [wsgi:error] [pid 23882:tid 140562645305088]     self._wrapped = Settings(settings_module)\n[Mon Jul 10 10:51:22.016404 2017] [wsgi:error] [pid 23882:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/conf/init.py\", line 137, in init\n[Mon Jul 10 10:51:22.016431 2017] [wsgi:error] [pid 23882:tid 140562645305088]     os.path.exists(os.path.join(zoneinfo_root, (self.TIME_ZONE.split('/'))))):\n[Mon Jul 10 10:51:22.016461 2017] [wsgi:error] [pid 23882:tid 140562645305088] AttributeError: 'America/New_York' object has no attribute 'split'\n[Mon Jul 10 10:51:22.028042 2017] [wsgi:error] [pid 23884:tid 140562645305088] mod_wsgi (pid=23884): Target WSGI script '/data/mktlocal/logs/mktdata/graphite/conf/graphite.wsgi' cannot be loaded as Python module.\n[Mon Jul 10 10:51:22.028089 2017] [wsgi:error] [pid 23884:tid 140562645305088] mod_wsgi (pid=23884): Exception occurred processing WSGI script '/data/mktlocal/logs/mktdata/graphite/conf/graphite.wsgi'.\n[Mon Jul 10 10:51:22.028130 2017] [wsgi:error] [pid 23884:tid 140562645305088] Traceback (most recent call last):\n[Mon Jul 10 10:51:22.028174 2017] [wsgi:error] [pid 23884:tid 140562645305088]   File \"/data/mktlocal/logs/mktdata/graphite/conf/graphite.wsgi\", line 5, in \n[Mon Jul 10 10:51:22.028244 2017] [wsgi:error] [pid 23884:tid 140562645305088]     from graphite.wsgi import application\n[Mon Jul 10 10:51:22.028276 2017] [wsgi:error] [pid 23884:tid 140562645305088]   File \"mdsinfra/amrs/graphite/common/webapp/graphite/wsgi.py\", line 16, in \n[Mon Jul 10 10:51:22.028664 2017] [wsgi:error] [pid 23884:tid 140562645305088]     application = get_wsgi_application()\n[Mon Jul 10 10:51:22.028704 2017] [wsgi:error] [pid 23884:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/core/wsgi.py\", line 13, in get_wsgi_application\n[Mon Jul 10 10:51:22.028786 2017] [wsgi:error] [pid 23884:tid 140562645305088]     django.setup()\n[Mon Jul 10 10:51:22.028818 2017] [wsgi:error] [pid 23884:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/init.py\", line 17, in setup\n[Mon Jul 10 10:51:22.028898 2017] [wsgi:error] [pid 23884:tid 140562645305088]     configure_logging(settings.LOGGING_CONFIG, settings.LOGGING)\n[Mon Jul 10 10:51:22.028932 2017] [wsgi:error] [pid 23884:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/conf/init.py\", line 55, in getattr\n[Mon Jul 10 10:51:22.029061 2017] [wsgi:error] [pid 23884:tid 140562645305088]     self._setup(name)\n[Mon Jul 10 10:51:22.029094 2017] [wsgi:error] [pid 23884:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/conf/init.py\", line 43, in _setup\n[Mon Jul 10 10:51:22.029139 2017] [wsgi:error] [pid 23884:tid 140562645305088]     self._wrapped = Settings(settings_module)\n[Mon Jul 10 10:51:22.029168 2017] [wsgi:error] [pid 23884:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/conf/init.py\", line 137, in init\n[Mon Jul 10 10:51:22.029209 2017] [wsgi:error] [pid 23884:tid 140562645305088]     os.path.exists(os.path.join(zoneinfo_root, (self.TIME_ZONE.split('/'))))):\n[Mon Jul 10 10:51:22.029250 2017] [wsgi:error] [pid 23884:tid 140562645305088] AttributeError: 'America/New_York' object has no attribute 'split'\n[Mon Jul 10 10:51:22.035874 2017] [wsgi:error] [pid 23883:tid 140562645305088] mod_wsgi (pid=23883): Target WSGI script '/data/mktlocal/logs/mktdata/graphite/conf/graphite.wsgi' cannot be loaded as Python module.\n[Mon Jul 10 10:51:22.035907 2017] [wsgi:error] [pid 23883:tid 140562645305088] mod_wsgi (pid=23883): Exception occurred processing WSGI script '/data/mktlocal/logs/mktdata/graphite/conf/graphite.wsgi'.\n[Mon Jul 10 10:51:22.035936 2017] [wsgi:error] [pid 23883:tid 140562645305088] Traceback (most recent call last):\n[Mon Jul 10 10:51:22.035962 2017] [wsgi:error] [pid 23883:tid 140562645305088]   File \"/data/mktlocal/logs/mktdata/graphite/conf/graphite.wsgi\", line 5, in \n[Mon Jul 10 10:51:22.036023 2017] [wsgi:error] [pid 23883:tid 140562645305088]     from graphite.wsgi import application\n[Mon Jul 10 10:51:22.036047 2017] [wsgi:error] [pid 23883:tid 140562645305088]   File \"mdsinfra/amrs/graphite/common/webapp/graphite/wsgi.py\", line 16, in \n[Mon Jul 10 10:51:22.036128 2017] [wsgi:error] [pid 23883:tid 140562645305088]     application = get_wsgi_application()\n[Mon Jul 10 10:51:22.036152 2017] [wsgi:error] [pid 23883:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/core/wsgi.py\", line 13, in get_wsgi_application\n[Mon Jul 10 10:51:22.036221 2017] [wsgi:error] [pid 23883:tid 140562645305088]     django.setup()\n[Mon Jul 10 10:51:22.036243 2017] [wsgi:error] [pid 23883:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/init.py\", line 17, in setup\n[Mon Jul 10 10:51:22.036309 2017] [wsgi:error] [pid 23883:tid 140562645305088]     configure_logging(settings.LOGGING_CONFIG, settings.LOGGING)\n[Mon Jul 10 10:51:22.036380 2017] [wsgi:error] [pid 23883:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/conf/init.py\", line 55, in getattr\n[Mon Jul 10 10:51:22.036511 2017] [wsgi:error] [pid 23883:tid 140562645305088]     self._setup(name)\n[Mon Jul 10 10:51:22.036545 2017] [wsgi:error] [pid 23883:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/conf/init.py\", line 43, in _setup\n[Mon Jul 10 10:51:22.036589 2017] [wsgi:error] [pid 23883:tid 140562645305088]     self._wrapped = Settings(settings_module)\n[Mon Jul 10 10:51:22.036620 2017] [wsgi:error] [pid 23883:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/conf/init.py\", line 137, in init\n[Mon Jul 10 10:51:22.036659 2017] [wsgi:error] [pid 23883:tid 140562645305088]     os.path.exists(os.path.join(zoneinfo_root, (self.TIME_ZONE.split('/'))))):\n[Mon Jul 10 10:51:22.036701 2017] [wsgi:error] [pid 23883:tid 140562645305088] AttributeError: 'America/New_York' object has no attribute 'split'\n[Mon Jul 10 10:51:22.039782 2017] [wsgi:error] [pid 23881:tid 140562645305088] mod_wsgi (pid=23881): Target WSGI script '/data/mktlocal/logs/mktdata/graphite/conf/graphite.wsgi' cannot be loaded as Python module.\n[Mon Jul 10 10:51:22.039807 2017] [wsgi:error] [pid 23881:tid 140562645305088] mod_wsgi (pid=23881): Exception occurred processing WSGI script '/data/mktlocal/logs/mktdata/graphite/conf/graphite.wsgi'.\n[Mon Jul 10 10:51:22.039835 2017] [wsgi:error] [pid 23881:tid 140562645305088] Traceback (most recent call last):\n[Mon Jul 10 10:51:22.039859 2017] [wsgi:error] [pid 23881:tid 140562645305088]   File \"/data/mktlocal/logs/mktdata/graphite/conf/graphite.wsgi\", line 5, in \n[Mon Jul 10 10:51:22.039917 2017] [wsgi:error] [pid 23881:tid 140562645305088]     from graphite.wsgi import application\n[Mon Jul 10 10:51:22.039936 2017] [wsgi:error] [pid 23881:tid 140562645305088]   File \"mdsinfra/amrs/graphite/common/webapp/graphite/wsgi.py\", line 16, in \n[Mon Jul 10 10:51:22.040014 2017] [wsgi:error] [pid 23881:tid 140562645305088]     application = get_wsgi_application()\n[Mon Jul 10 10:51:22.040038 2017] [wsgi:error] [pid 23881:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/core/wsgi.py\", line 13, in get_wsgi_application\n[Mon Jul 10 10:51:22.040105 2017] [wsgi:error] [pid 23881:tid 140562645305088]     django.setup()\n[Mon Jul 10 10:51:22.040126 2017] [wsgi:error] [pid 23881:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/init.py\", line 17, in setup\n[Mon Jul 10 10:51:22.040192 2017] [wsgi:error] [pid 23881:tid 140562645305088]     configure_logging(settings.LOGGING_CONFIG, settings.LOGGING)\n[Mon Jul 10 10:51:22.040214 2017] [wsgi:error] [pid 23881:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/conf/init.py\", line 55, in getattr\n[Mon Jul 10 10:51:22.040338 2017] [wsgi:error] [pid 23881:tid 140562645305088]     self._setup(name)\n[Mon Jul 10 10:51:22.040360 2017] [wsgi:error] [pid 23881:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/conf/init.py\", line 43, in _setup\n[Mon Jul 10 10:51:22.040391 2017] [wsgi:error] [pid 23881:tid 140562645305088]     self._wrapped = Settings(settings_module)\n[Mon Jul 10 10:51:22.040409 2017] [wsgi:error] [pid 23881:tid 140562645305088]   File \"python/Django/1.9.13/common/2.7/lib/python/Django-1.9.13-py2.7.egg/django/conf/init.py\", line 137, in init\n[Mon Jul 10 10:51:22.040434 2017] [wsgi:error] [pid 23881:tid 140562645305088]     os.path.exists(os.path.join(zoneinfo_root, *(self.TIME_ZONE.split('/'))))):\n[Mon Jul 10 10:51:22.040463 2017] [wsgi:error] [pid 23881:tid 140562645305088] AttributeError: 'America/New_York' object has no attribute 'split'\n. yeap..\nls -lrt |grep New\n-rw-r--r--   3 root root 3519 Mar 20 23:45 New_York\npwd\n/usr/share/zoneinfo/America. sorry, type, it is local_settings.py\njust for giggles I ran the devel web server to force local_settings.py to recompile with what I believe is the proper settings.  Then started up the httpd server with the same error.  Is there any other place that looks for/sets the TIME_ZONE?. So I changed TIME_ZONE = 'America/Chicago', recompiled with dev server, then ran httpd server\nSame error as before:\n[Mon Jul 10 12:01:57.978835 2017] [wsgi:error] [pid 23490:tid 140693908838144]     os.path.exists(os.path.join(zoneinfo_root, *(self.TIME_ZONE.split('/'))))):\n[Mon Jul 10 12:01:57.978876 2017] [wsgi:error] [pid 23490:tid 140693908838144] AttributeError: 'America/New_York' object has no attribute 'split'\n. when I start the httpd server local_settings.py doesn't recompile to local_settings.pyc.  so I do it via the devel server.. ran strace against the httpd process.  I didn't see any logging that opened local_settings or complained about it missing. I think I fixed it.  there was a second file and the time zone was not a string object.  Now if I can just get the website to show. ",
    "zyzwizki": "Thanks,\nyou were right abou the version. I used pip to install 1.1\nWhen running \ndjango-admin.py  migrate --settings=graphite.settings --run-syncdb\nI get the following error: django-admin.py migrate: error: unrecognized arguments: --run-syncdb\nWhen leaving out --run-syncdb\nI get:\nOperations to perform:\n  Synchronize unmigrated apps: render, staticfiles, whitelist, metrics, composer, browser\n  Apply all migrations: account, sessions, admin, auth, url_shortener, contenttypes, dashboard, events, tagging\nSynchronizing apps without migrations:\n  Creating tables...\n    Running deferred SQL...\n  Installing custom SQL...\nRunning migrations:\n  Rendering model states... DONE\n  Applying contenttypes.0001_initial...Traceback (most recent call last):\n  File \"/usr/local/bin/django-admin.py\", line 5, in <module>\n    management.execute_from_command_line()\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/__init__.py\", line 338, in execute_from_command_line\n    utility.execute()\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/__init__.py\", line 330, in execute\n    self.fetch_command(subcommand).run_from_argv(self.argv)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/base.py\", line 390, in run_from_argv\n    self.execute(*args, **cmd_options)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/base.py\", line 441, in execute\n    output = self.handle(*args, **options)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/management/commands/migrate.py\", line 221, in handle\n    executor.migrate(targets, plan, fake=fake, fake_initial=fake_initial)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/migrations/executor.py\", line 110, in migrate\n    self.apply_migration(states[migration], migration, fake=fake, fake_initial=fake_initial)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/migrations/executor.py\", line 147, in apply_migration\n    state = migration.apply(state, schema_editor)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/migrations/migration.py\", line 115, in apply\n    operation.database_forwards(self.app_label, schema_editor, old_state, project_state)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/migrations/operations/models.py\", line 59, in database_forwards\n    schema_editor.create_model(model)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/backends/base/schema.py\", line 286, in create_model\n    self.execute(sql, params or None)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/backends/base/schema.py\", line 111, in execute\n    cursor.execute(sql, params)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/backends/utils.py\", line 79, in execute\n    return super(CursorDebugWrapper, self).execute(sql, params)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/backends/utils.py\", line 64, in execute\n    return self.cursor.execute(sql, params)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/utils.py\", line 97, in __exit__\n    six.reraise(dj_exc_type, dj_exc_value, traceback)\n  File \"/usr/local/lib/python2.7/dist-packages/django/db/backends/utils.py\", line 62, in execute\n    return self.cursor.execute(sql)\ndjango.db.utils.ProgrammingError: relation \"django_content_type\" already exists\nAs I am neither familiar with python, nor with django I dont really know what to make of this...\n. ",
    "williamleuschner": "I'm having a similar issue when running django-admin.py migrate --settings=graphite.settings --run-syncdb.\nHere is the traceback on my end:\nOperations to perform:\n  Synchronize unmigrated apps: account, cli, render, whitelist, metrics, url_shortener, dashboard, composer, events, browser\n  Apply all migrations: admin, contenttypes, tagging, auth, sessions\nSynchronizing apps without migrations:\n  Creating tables...\n    Running deferred SQL...\nRunning migrations:\n  Rendering model states... DONE\n  Applying contenttypes.0001_initial...Traceback (most recent call last):\n  File \"/bin/django-admin.py\", line 5, in <module>\n    management.execute_from_command_line()\n  File \"/usr/lib64/python2.7/site-packages/django/core/management/__init__.py\", line 353, in execute_from_command_line\n    utility.execute()\n  File \"/usr/lib64/python2.7/site-packages/django/core/management/__init__.py\", line 345, in execute\n    self.fetch_command(subcommand).run_from_argv(self.argv)\n  File \"/usr/lib64/python2.7/site-packages/django/core/management/base.py\", line 348, in run_from_argv\n    self.execute(*args, **cmd_options)\n  File \"/usr/lib64/python2.7/site-packages/django/core/management/base.py\", line 399, in execute\n    output = self.handle(*args, **options)\n  File \"/usr/lib64/python2.7/site-packages/django/core/management/commands/migrate.py\", line 200, in handle\n    executor.migrate(targets, plan, fake=fake, fake_initial=fake_initial)\n  File \"/usr/lib64/python2.7/site-packages/django/db/migrations/executor.py\", line 92, in migrate\n    self._migrate_all_forwards(plan, full_plan, fake=fake, fake_initial=fake_initial)\n  File \"/usr/lib64/python2.7/site-packages/django/db/migrations/executor.py\", line 121, in _migrate_all_forwards\n    state = self.apply_migration(state, migration, fake=fake, fake_initial=fake_initial)\n  File \"/usr/lib64/python2.7/site-packages/django/db/migrations/executor.py\", line 198, in apply_migration\n    state = migration.apply(state, schema_editor)\n  File \"/usr/lib64/python2.7/site-packages/django/db/migrations/migration.py\", line 123, in apply\n    operation.database_forwards(self.app_label, schema_editor, old_state, project_state)\n  File \"/usr/lib64/python2.7/site-packages/django/db/migrations/operations/models.py\", line 59, in database_forwards\n    schema_editor.create_model(model)\n  File \"/usr/lib64/python2.7/site-packages/django/db/backends/base/schema.py\", line 284, in create_model\n    self.execute(sql, params or None)\n  File \"/usr/lib64/python2.7/site-packages/django/db/backends/base/schema.py\", line 110, in execute\n    cursor.execute(sql, params)\n  File \"/usr/lib64/python2.7/site-packages/django/db/backends/utils.py\", line 64, in execute\n    return self.cursor.execute(sql, params)\n  File \"/usr/lib64/python2.7/site-packages/django/db/utils.py\", line 95, in __exit__\n    six.reraise(dj_exc_type, dj_exc_value, traceback)\n  File \"/usr/lib64/python2.7/site-packages/django/db/backends/utils.py\", line 62, in execute\n    return self.cursor.execute(sql)\n  File \"/usr/lib64/python2.7/site-packages/django/db/backends/sqlite3/base.py\", line 321, in execute\n    return Database.Cursor.execute(self, query)\ndjango.db.utils.OperationalError: table \"django_content_type\" already exists. ",
    "GowthamShanmugam": "even I am facing the same issue while migration, what is the solution for this?.  i have modified CARBONLINK_HASHING_TYPE = 'fnv1a_ch'  in \"vi /usr/lib/python2.7/site-packages/graphite/local_settings.py\"\ndo i want to run migrate command again? and without carbon-c-relay is it work?\ni am using graphite-web-1.1.4-1, python2-django-1.11.15-1\nApplying contenttypes.0001_initial...Traceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/django/bin/django-admin.py\", line 5, in \n    management.execute_from_command_line()\n  File \"/usr/lib/python2.7/site-packages/django/core/management/init.py\", line 364, in execute_from_command_line\n    utility.execute()\n  File \"/usr/lib/python2.7/site-packages/django/core/management/init.py\", line 356, in execute\n    self.fetch_command(subcommand).run_from_argv(self.argv)\n  File \"/usr/lib/python2.7/site-packages/django/core/management/base.py\", line 283, in run_from_argv\n    self.execute(args, cmd_options)\n  File \"/usr/lib/python2.7/site-packages/django/core/management/base.py\", line 330, in execute\n    output = self.handle(*args, options)\n  File \"/usr/lib/python2.7/site-packages/django/core/management/commands/migrate.py\", line 204, in handle\n    fake_initial=fake_initial,\n  File \"/usr/lib/python2.7/site-packages/django/db/migrations/executor.py\", line 115, in migrate\n    state = self._migrate_all_forwards(state, plan, full_plan, fake=fake, fake_initial=fake_initial)\n  File \"/usr/lib/python2.7/site-packages/django/db/migrations/executor.py\", line 145, in _migrate_all_forwards\n    state = self.apply_migration(state, migration, fake=fake, fake_initial=fake_initial)\n  File \"/usr/lib/python2.7/site-packages/django/db/migrations/executor.py\", line 244, in apply_migration\n    state = migration.apply(state, schema_editor)\n  File \"/usr/lib/python2.7/site-packages/django/db/migrations/migration.py\", line 129, in apply\n    operation.database_forwards(self.app_label, schema_editor, old_state, project_state)\n  File \"/usr/lib/python2.7/site-packages/django/db/migrations/operations/models.py\", line 536, in database_forwards\n    getattr(new_model._meta, self.option_name, set()),\n  File \"/usr/lib/python2.7/site-packages/django/db/backends/base/schema.py\", line 369, in alter_unique_together\n    self.execute(self._create_unique_sql(model, columns))\n  File \"/usr/lib/python2.7/site-packages/django/db/backends/base/schema.py\", line 989, in _create_unique_sql\n    \"name\": self.quote_name(self._create_index_name(model, columns, suffix=\"_uniq\")),\n  File \"/usr/lib/python2.7/site-packages/django/db/backends/base/schema.py\", line 879, in _create_index_name\n    hash_suffix_part = '%s%s' % (self._digest(hash_data), suffix)\n  File \"/usr/lib/python2.7/site-packages/django/db/backends/base/schema.py\", line 147, in _digest\n    h = hashlib.md5()\nValueError: error:060800A3:digital envelope routines:EVP_DigestInit_ex:disabled for fip. I just modified CARBONLINK_HASHING_TYPE = 'fnv1a_ch' in \"vi /usr/lib/python2.7/site-packages/graphite/local_settings.py\" and restarted carbon-cache, and grphite gives 500 internel error.\ncan you please explain step by step what I have to do to make it work in FIPS mode. ok, can you please have a look above traceback, is it Django not working with FIPS ? or if we switch to carbon-c-relay will solve this above issue which is displayed in traceback?\nI am seeing this error when I try to run initialization on freshmachine:\nPYTHONPATH=$GRAPHITE_ROOT/webapp/ /usr/lib/python2.7/site-packages/django/bin/django-admin.py migrate --settings=graphite.settings --run-syncdb\nso switching to carbon-c-relay will solve this? is there any document on how to configure carbon-relay\nMy RPMs are:\ngraphite-web-1.1.4-1\npython2-django-1.11.15-1\npython-cachetools-1.0.3-1\npython-carbon-1.1.4-1\npython-whisper-1.1.4-1\n. what is the migration command actually suggested by graphite-web document ?. > Exact migration command really depends on source and destination Django and Graphite version, I'm afraid, it will be quite hard to create a really generic solution.\n\nBut still open for any suggestions or even PRs.\n\ncan you please tell me an example? in our database migration is not happening if we use a fake command, old schema remains the same.. I saw this issue lot of times, why I can't use the soft link there if I install web-1.1.4-1 in the fresh machine then I create soft-link then it works fine. but why it is not working in above case. i am seeing this issue in  graphite-web-1.1.4-1. i can see fixed code in my setup but still i am faceing this issue, i removed all packages \nyum remove graphite-web pythoon-carbon python-whisper python-django \nand then installed again but still i am seeing this issue.. version which i am using is:\n python2-django-1.11.15-1\n  graphite-web-1.1.4-1\n python-carbon-1.1.4-1\n  python-whisper-1.1.4-1. First i installed \n  graphite-web-0.9.15\n  python-carbon-0.9.15-2\n  python-whisper-0.9.15\nafter that i ran graphite command to intialize:\n  /usr/lib/python2.7/site-packages/graphite/manage.py syncdb --noinput\nAfter intialized i just updated with new graphite-web package which i pasted in above command and then i intialized new version of graphite using a command:\nPYTHONPATH=$GRAPHITE_ROOT/webapp/ django-admin migrate --settings=graphite.settings --run-syncdb\nIt executed successfully, then i start carbon-cache service:\n   service carbon-cache start\nThen i created softlink inside /var/lib/carbon/whisper/{app_name}/{soft_link}\n(e.g)\n[root@tendrl-server graphite_dependency-]# ll /var/lib/carbon/whisper/tendrl/names/e43fcdf8-45b9-4146-909b-fb26b9078e26\nlrwxrwxrwx. 1 root root 77 Dec  7 12:49 /var/lib/carbon/whisper/tendrl/names/e43fcdf8-45b9-4146-909b-fb26b9078e26 -> /var/lib/carbon/whisper/tendrl/clusters/e43fcdf8-45b9-4146-909b-fb26b9078e26\nthe softlink is not displayed in graphite-web, but i can see all directory using softlink in terminal.\n. If i install new version of graphite-web in a fresh machine  then it works fine, i am faceing issue when i try to update to new version from old version.. I am constantly reproducing this issue. for RHEL we built it manually, during graphite-web build we figured out dependencies one by one and we build all, then issue which I am facing is once I removed graphite package and reinstalled higher version or same version then a soft link is not working.\nI am working in redhat as SE, for our product we are using graphite as time series DB in RHEL\nhttps://github.com/Tendrl/monitoring-integration/blob/master/tendrl-monitoring-integration.spec#L13. One thing is not clear, i removed all graphite related packages and then reinstalled same version then soft-link is not working.. i found the issue it is actually issued with my selinux :) when i do  setenforce 0 then immediately all I can see all values are reflected.. Thanks for the quick replys :) :+1: . other metrics are works fine\n\n. if I give \" \\|mnt\\|data1\\|1\"  like this then it gives values in a graph, but using this solution I cant use template variables in grafana.  template variables are populated at runtime, so I can't mention it as a hardcoded value with the slash.\n\n. We used grafana 0.9.x with same metrics for a long time, and recently we moved to grafana 1.1.X with same grafana version, it looks like some problem with pipe symbol in a new version. . Escape in the sense does i have to give \\| instead of |. But we can give this when we have static query what in the case of a dynamic query which is formed using templates?\nHow to solve that case?. For example\n\nActually, I want to know a solution for this scenario, Here brick_path is template variable, so it is populated from graphite already based on that below metrics will full-fill. There I can't escape the pipe symbol. because it is dynamic. \nGraphite unable to respond for this query, So i don't know how to solve this problem. is it not possible to use graphite metrics in this way?. or graphite have any other function to replace special characters in original metrics and make a query with new metrics. Those are only for display purpose I think, it won't do any impact on actual metrics. What are all the support symbols in graphite series name?. > All Graphite functions are for display purpose, but some functions operate on data values, some - on metric names. aliasSub() operates on metric names, so, theoretically, you can use aliasSub() when providing variable in Grafana, to replace all \"|\" with \"/|\" in metric name - but for some reason, it's not working on Grafana side (it works on Graphite).\ncan you please paste the aliasSub() metric which you tried?. What are all the other special character are allowed? any other symbol which won't cause issue? . ",
    "RichiH": "Neat!\nAs the underlying scheme is compatible with Prometheus format you might be interested in https://github.com/RichiH/OpenMetrics which will spin out the format into a distinct project.. OpenMetrics is a wire format to exchange metrics, correct.. ",
    "CCoruble": "Hello,\nThank you for answering so fast ! \nI did \"sudo apt-get install libffi-dev\" and it worked !\nLog of the installation :\nCollecting https://github.com/graphite-project/graphite-web/tarball/master\n  Downloading https://github.com/graphite-project/graphite-web/tarball/master (1.4MB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.4MB 386kB/s\nRequirement already satisfied (use --upgrade to upgrade): Django<1.11.99,>=1.8 in /usr/lib/python2.7/dist-packages (from graphite-web===1.1.0)\nCollecting django-tagging==0.4.3 (from graphite-web===1.1.0)\n  Using cached django_tagging-0.4.3-py2.py3-none-any.whl\nRequirement already satisfied (use --upgrade to upgrade): pytz in /usr/lib/python2.7/dist-packages (from graphite-web===1.1.0)\nCollecting pyparsing (from graphite-web===1.1.0)\n  Using cached pyparsing-2.2.0-py2.py3-none-any.whl\nCollecting cairocffi (from graphite-web===1.1.0)\n  Using cached cairocffi-0.8.0.tar.gz\nCollecting urllib3 (from graphite-web===1.1.0)\n  Downloading urllib3-1.22-py2.py3-none-any.whl (132kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 1.5MB/s\nCollecting scandir (from graphite-web===1.1.0)\n  Downloading scandir-1.5.tar.gz\nCollecting cffi>=1.1.0 (from cairocffi->graphite-web===1.1.0)\n  Downloading cffi-1.10.0-cp27-cp27mu-manylinux1_x86_64.whl (392kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 399kB 951kB/s\nCollecting pycparser (from cffi>=1.1.0->cairocffi->graphite-web===1.1.0)\n  Downloading pycparser-2.18.tar.gz (245kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 256kB 1.6MB/s\nBuilding wheels for collected packages: cairocffi, scandir, pycparser\n  Running setup.py bdist_wheel for cairocffi ... done\n  Stored in directory: /root/.cache/pip/wheels/ed/67/1b/c2e45826aef8e76b0942c2c1e62120da72259a5c1e46a3d02d\n  Running setup.py bdist_wheel for scandir ... done\n  Stored in directory: /root/.cache/pip/wheels/47/af/a2/eb591a17e9709a17d8b53696f6ad89914a05eaf9c091f36e18\n  Running setup.py bdist_wheel for pycparser ... done\n  Stored in directory: /root/.cache/pip/wheels/95/14/9a/5e7b9024459d2a6600aaa64e0ba485325aff7a9ac7489db1b6\nSuccessfully built cairocffi scandir pycparser\nInstalling collected packages: django-tagging, pyparsing, pycparser, cffi, cairocffi, urllib3, scandir, graphite-web\n  Found existing installation: django-tagging 0.4\n    Not uninstalling django-tagging at /usr/lib/python2.7/dist-packages, outside environment /usr\n  Running setup.py install for graphite-web ... done\nSuccessfully installed cairocffi-0.8.0 cffi-1.10.0 django-tagging-0.4.3 graphite-web pycparser-2.18 pyparsing-2.2.0 scandir-1.5 urllib3-1.22\nYou are using pip version 8.1.1, however version 9.0.1 is available.\nYou should consider upgrading via the 'pip install --upgrade pip' command.\nI'm far from being an ubuntu pro so I might post issues again if I see problems.\nThank you again for solving this one !\nRegards.. Finally found a way to make it works with this tutorial.\nI hope you will updates you docs, I know this is a lot of works but couldn't manage to make graphite works with official doc...\nRegards. ",
    "msk610": "@deniszh thank you for the review, I am not sure why I removed part of the events function, I will add that back and fix my test function later today.. ",
    "zasca": "I found the issue in the code.\nAn absolute path to whisper file is truncated to the length of the full metric name. This is not correct because absolute_path ends up with a file extension (.wsp), but relative_fs_path is not.\n```\n\n\n\nabsolute_path='/var/lib/graphite/whisper/Env/HTTP/NumConnections.wsp'\nreal_fs_path='/mnt/ssd/graphite/db/Env/HTTP/NumConnections.wsp'\nmetric_path='Env.HTTP.NumConnections'\nrelative_fs_path = metric_path.replace('.', os.sep)\nrelative_fs_path\n'Env/HTTP/NumConnections'\nabsolute_path[:-len(relative_fs_path)]\n'/var/lib/graphite/whisper/Env/'\nbase_fs_path = os.path.dirname(absolute_path[:-len(relative_fs_path)])\nbase_fs_path\n'/var/lib/graphite/whisper/Env'\nreal_base_fs_path = os.path.realpath(base_fs_path)\nreal_base_fs_path\n'/mnt/ssd/graphite/db/Env'\nrelative_real_fs_path = real_fs_path[len(real_base_fs_path):].lstrip('/')\nrelative_real_fs_path\n'HTTP/NumConnections.wsp'\nfs_to_metric(relative_real_fs_path)\n'HTTP.NumConnections'\nIn addition, it is not at all clear why the metric name is newly calculated, because it is originally contained in the variable `metric_path`. Or am I missing something?. All working as expect with this patch for me:\n--- init.py 2017-08-08 09:00:16.000000000 +0300\n+++ init-new.py 2017-08-08 09:44:21.000000000 +0300\n@@ -6,15 +6,6 @@\n EXPAND_BRACES_RE = re.compile(r'.({.?[^\\]?})')\n\n\n\ndef get_real_metric_path(absolute_path, metric_path):\n-  # Support symbolic links (real_metric_path ensures proper cache queries)\n-  real_fs_path = os.path.realpath(absolute_path)\n-  if absolute_path != real_fs_path:\n-    relative_fs_path = metric_path.replace('.', os.sep)\n-    base_fs_path = os.path.dirname(absolute_path[:-len(relative_fs_path)])\n-    real_base_fs_path = os.path.realpath(base_fs_path)\n-    relative_real_fs_path = real_fs_path[len(real_base_fs_path):].lstrip('/')\n-    return fs_to_metric(relative_real_fs_path)\n-\n   return metric_path\n```. @DanCech yes, this should work too. \nBut is not clear what get_real_metric_path() purpose? In my case with a symbolic link, all work fine without this function. Based on the code, this function extracts the metric path by truncating the real path and the resulting value is exactly the same as the metric_path.. patch\n```\n--- standard.py_orig    2017-08-31 15:09:46.000000000 +0300\n+++ standard.py 2017-09-05 11:28:14.000000000 +0300\n@@ -90,7 +90,7 @@\n     if using_globstar:\n         matching_subdirs = map(operator.itemgetter(0), walk(current_dir))\n     else:\n-        subdirs = [entry for entry in entries if isdir(join(current_dir, entry))]\n+        subdirs = [entry for entry in entries if isdir(join(current_dir, str(entry)))]\n         matching_subdirs = match_entries(subdirs, pattern)\n # if this is a terminal globstar, add a pattern for all files in subdirs\n\n@@ -113,15 +113,15 @@\n     if patterns: #we've still got more directories to traverse\n       for subdir in matching_subdirs:\n\nabsolute_path = join(current_dir, subdir)\n\nabsolute_path = join(current_dir, str(subdir))\n         for match in self._find_paths(absolute_path, patterns):\n           yield match\nelse: #we've got the last pattern\n   if not has_wildcard:\n     entries = [ pattern + '.wsp', pattern + '.wsp.gz', pattern + '.rrd' ]\n-      files = [entry for entry in entries if isfile(join(current_dir, entry))]\n+      files = [entry for entry in entries if isfile(join(current_dir, str(entry)))]\n   matching_files = match_entries(files, pattern + '.*')\nfor base_name in matching_files + matching_subdirs:\n-        yield join(current_dir, base_name)\n+        yield join(current_dir, str(base_name))\n```\n. No, I'm not. I've specially launched a new environment in the docker container now. Base image ubuntu:trusty.\n\n\n```\npython --version\nPython 2.7.6\npip list\nargparse (1.2.1)\nattrs (17.2.0)\nAutomat (0.6.0)\ncairocffi (0.8.0)\ncffi (1.10.0)\nchardet (2.0.1)\ncolorama (0.2.5)\nconstantly (15.1.0)\nDjango (1.9.13)\ndjango-tagging (0.4.3)\nhtml5lib (0.999)\nhyperlink (17.3.1)\nincremental (17.5.0)\npip (1.5.4)\npycparser (2.18)\npyOpenSSL (17.2.0)\npyparsing (2.2.0)\npython-apt (0.9.3.5ubuntu2)\npytz (2017.2)\nrequests (2.2.1)\nscandir (1.5)\nsetuptools (3.3)\nsix (1.10.0)\nssh-import-id (3.21)\nTwisted (17.5.0)\ntxAMQP (0.7.0)\nurllib3 (1.7.1)\nwheel (0.24.0)\nwhisper (1.0.2)\nwsgiref (0.1.2)\nzope.interface (4.4.2)\n```\nlocal_settings.py\nLOG_RENDERING_PERFORMANCE = True\nLOG_CACHE_PERFORMANCE = True\nLOG_METRIC_ACCESS = True\nGRAPHITE_ROOT = '/opt/graphite'\nSTORAGE_DIR = '/var/lib/graphite/whisper'\nWHISPER_DIR = '/var/lib/graphite/whisper'\nLOG_DIR = '/var/log/graphite'\nDATABASES = {\n    'default': {\n        'NAME': '/var/lib/graphite/graphite.db',\n        'ENGINE': 'django.db.backends.sqlite3',\n        'USER': '',\n        'PASSWORD': '',\n        'HOST': '',\n        'PORT': ''\n    }\n}\nCARBONLINK_HOSTS = ['127.0.0.1:7002:a']\nCARBONLINK_TIMEOUT = 1.0\ndirname in hex:\n```\nls /var/lib/graphite/whisper/app | xxd -pu\nd0bfd180d0b8d0bcd0b5d1805fd180d1840a\n```\n\nI do get carbonlink exceptions though\n\nThis exception returned on the http request? . Yes, I use apache. But setting locale not fix the issue in my environment...\nAlso, I've tested this case at docker-graphite-statsd container and all work as expecting!\nIt looks like there really is a problem in the locale. I will continue to investigate.\nThanks, @piotr1212!. >Yes I too think it is something with the locale.\nMy issue was that /usr/lib/locale/locale-archive was missing (this is default state at ubuntu 14.04 docker image, but does not at phusion/baseimage wich is used by official Graphite & Statsd docker image). \nSteps to fix:\n\nCreate /etc/locale.gen and add locales and charsets\nRun locale-gen\nEnsure locales was added  to /etc/apache2/envvars, as said piotr1212 earlier\n\n\n\nBut remember, even if you get this fixed you will run into other UTF-8 issues.\n\n@piotr1212, you are right.\n127.0.0.1:8080/render/?from=-1y&target=%D0%BF.connections-accepted&format=json is failing:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 149, in get_response\n    response = self.process_exception_by_middleware(e, request)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 147, in get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 130, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 7, in evaluateTarget\n    tokens = grammar.parseString(target)\n  File \"/usr/local/lib/python2.7/dist-packages/pyparsing.py\", line 1632, in parseString\n    raise exc\nParseException: Expected {Group:({\"template\" Suppress:(\"(\") {Group:({W:(ABCD...,ABCD...) ...\nLooks like the issue is in the render/grammar.py. Because, looks like, pyparsing works well with unicode. I tried to fix it, but with no success.\n\nMy advise would be to just use ASCII unless you are willing to spend a lot of time fixing issues.\n\nIn some cases it is more convenient to work with Unicode.. As said earlier the issue was in the incorrect locale config.\n\n127.0.0.1:8080/render/?from=-1y&target=%D0%BF.connections-accepted&format=json is failing\n\nAnd this issue looks related more to #2024.\nTherefore, I close this.. looks like the issue is here. Thank you, Denis, for that info.. My bad, all works as expected without this changes, but with well configured locales only.. This is due to changes in the pip versions. By default virtualenv installs the latest version pip.\nWorkarounds:\n1. pip install --no-binary=:all: <carbon/graphite-web>\n2. pip install pip==6.1.1\n\nsince 7.0.0 introduced: build wheels prior to installing from sdist\nAffects to python package sources: pip install <carbon/graphite-web>\nsince 10.0.0b1 introduced:\npackages which specify one or more build dependencies this way will be built into wheels\nAffects to VCS sources:\npip install https://github.com/graphite-project/<carbon/graphite-web>/tarball/<tag/commit>. @DanCech, your explanation sounds good. Thank you.\nBut it seems this cannot be get around without an upgrade of graphite 1.0.2.. > See also this one: #1124 (don't know if related).\nNot related\n\nIf you want to avoid that you can add |xFilesFactor(1) to your query and you'll only get values for intervals that have enough points, so instead you'd get 2,2,2,null\n\nConfirm, it work since 1.1.1 stable version.. Fixed #2358. @deniszh, yeah, I was going to do it, but I ran into regression - setting up a new database and creating the initial schema does not happening with this setting model:\n```\nGRAPHITE_SETTINGS_MODULE=graphite.settings.root PYTHONPATH=/opt/graphite/webapp django-admin.py migrate --settings=graphite.settings.root --run-syncdb\nSTORAGE_DIR: \n2 join(STORAGE_DIR, 'graphite.db'): /opt/graphite/webapp/storage/graphite.db\nDATABASES.default: {'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': '/var/lib/graphite/graphite.db', 'HOST': '', 'USER': '', 'PASSWORD': '', 'PORT': ''}}\n4 STORAGE_DIR: /opt/graphite/webapp/storage\n3 STORAGE_DIR: /opt/graphite/webapp/storage\n4 STORAGE_DIR: /opt/graphite/webapp/storage\nLAST STORAGE_DIR: /opt/graphite/webapp/storage\nOperations to perform:\n  Apply all migrations: (none)\nRunning migrations:\n  No migrations to apply.\n\nfind / -name graphite.db | wc -l\n0\n```\nAny ideas how to fix this?\nOr is this issue not a hindrance for the new release?. > I ran into regression - setting up a new database and creating the initial schema does not happening with this setting mode\nMy bad. graphite.settings module was should pass instead:\n```\nGRAPHITE_SETTINGS_MODULE=graphite.settings.root \\\nPYTHONPATH=/opt/graphite/webapp django-admin.py \\\nmigrate --settings=graphite.settings --run-syncdb\n``. >I'm wondering if movingsettings.pytosettings/init.py` affect all django admin commands.\n@deniszh I've done some tests with debugging and done code review of django tools.\nAnd all looks and working good with suggested settings code.\nYes, potential \"out of scopes\" errors may occurs, but only when two conditions are met:\n1. The level of settings file passed direct to django-admin is NOT a root\n2. At the parent modules there are defined variables required for django-admin\n```\n$ tree settings/\n\u251c\u2500\u2500 init.py -> ../orig_settings.py\n\u251c\u2500\u2500 app_settings.py -> ../app_settings.py\n\u2514\u2500\u2500 root.py -> ../local_settings.py\n$ cat settings/root.py\nSECRET_KEY = 'UNSAFE_DEFAULT'\nDATABASES = {'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': '/var/lib/graphite/graphite.db', 'HOST': '', 'USER': '', 'PASSWORD': '', 'PORT': ''}}\n$ django-admin migrate --settings graphite.settings.root 2>&1 | grep -v STORAGE\nDATABASES.default: {'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': '/var/lib/graphite/graphite.db', 'HOST': '', 'USER': '', 'PASSWORD': '', 'PORT': ''}}\nOperations to perform:\n  Apply all migrations: (none)\nRunning migrations:\n  No migrations to apply.\n$ #########################################################\n$ echo \"INSTALLED_APPS = ( 'graphite.account', 'graphite.functions' )\" >> settings/root.py\n$ django-admin migrate --settings graphite.settings.root 2>&1 | grep -v STORAGE\nDATABASES.default: {'default': {'ENGINE': 'django.db.backends.sqlite3', 'NAME': '/var/lib/graphite/graphite.db', 'HOST': '', 'USER': '', 'PASSWORD': '', 'PORT': ''}}\nTraceback (most recent call last):\n  File \"/usr/local/bin/django-admin.py\", line 5, in \n    management.execute_from_command_line()\n  ...\n  File \"/usr/local/lib/python2.7/dist-packages/django/conf/init.py\", line 63, in getattr\n    val = getattr(self._wrapped, name)\nAttributeError: 'Settings' object has no attribute 'LOG_ROTATION_COUNT'\n```. In my opinion, the location of all settings modules in one package is optimal.\nAnd after movingsettings.pytosettings/init.pywill be possible to put all other settings modules likeapp_settings.py` there.. Thanks, Denis.\nFixed.. ",
    "pauldavidgilligan": "Thank you great job could come in very handy.\n\u2063Sent from Blue \u200b\nOn 3 Sep 2017, 18:24, at 18:24, Denis Zhdanov notifications@github.com wrote:\n\nYes, according to https://github.com/grafana/grafana/issues/1588\nGrafana supporting that now.\n-- \nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub:\nhttps://github.com/graphite-project/graphite-web/issues/2016#issuecomment-326814867\n. \n",
    "6d6178": "I have the same problem and the file '/opt/graphite/webapp/graphite/wsgi.py' does exist.\nSame OS, same installation process.. The issue seems to be realted to #1232 \nI hadn't removed graphite-web, therefore the module graphite 0.9.6 was still in python and caused the error.. The last time this happened to me (yesterday) I have resolved this error with\nsystemctl stop httpd\nsystemctl start httpd\nWhen I used \nsystemctl restart httpd\nit didn't work.\nI can imagine it being some parts of mod_wsgi/httpd that aren't fully closed with restart, but that's just a guess. If someone stumbles upon this comment I would greatly appreciate feedback if this resolved the error.\n. You might need to use the absolute path to django-admin.py.\ndjango-admin.py is most likely located at\n/usr/lib/python2.7/site-packages/django/bin/django-admin.py\nmaking the full commad\nPYTHONPATH=$GRAPHITE_ROOT/webapp/ /usr/lib/python2.7/site-packages/django/bin/django-admin.py migrate --settings=graphite.settings --run-syncdb. You might need to use the absolute path to django-admin.py.\ndjango-admin.py is most likely located at\n/usr/lib/python2.7/site-packages/django/bin/django-admin.py\nmaking the full commad\nPYTHONPATH=$GRAPHITE_ROOT/webapp/ /usr/lib/python2.7/site-packages/django/bin/django-admin.py migrate --settings=graphite.settings --run-syncdb. > Warning: django.contrib.contenttypes.generic is deprecated and will be removed in Django 1.9. Its contents have been moved to the fields, forms, and admin submodules of django.contrib.contenttypes.\nfrom django.contrib.contenttypes import generic\n@Abid1986 This is just a guess, but judging by the warning you are using Django < 1.9\nYou could update to the latest version and see if it resolves it.\npip install \"django<2\". @deniszh This one is resolved too.. Have you created the static content? Alternatively use Python Whitenoise (see examples/example-graphite-vhost.conf:41)\nPYTHONPATH=/opt/graphite/webapp/ /usr/lib/python2.7/site-packages/django/bin/django-admin.py collectstatic --noinput --settings=graphite.settings\nThe folder /opt/graphite/static has to be accessible by apache.. Just out of interest, is PYTHONPATH=/opt/graphite/webapp django-admin.py collectstatic --noinput --settings=graphite.settings?\nThe command I gave to you specified the location of django to where it would be, if installed with pip. As you used the django-admin.py in your question succesfully, it probably will work without a location.. This is not possible as Grafana is requesting from graphite-web.\nThe link @deniszh provided in his first response shows you how to add authentication to graphite-web which should work with Grafana if you provide the Credentials in the \"Data Source\". (Disclaimer: Haven't tried it myself)\nYou can look at https://github.com/graphite-project/graphite-web/blob/master/README.md for a diagram of the whole system. . @deniszh @JeKa0111 There is a way to use authentication to some extend. If you set the storage_dir to a subdir of the real storage_dir, you only have access to the whisper files in the subdir. This would require keeping the whisper files in a logical system, that puts all files for a user in one subdir. Or use several carbon-cache with different storage dir.\nUser-Access can be controlled by Apache utilities like .htaccess as graphite is a vhost.\nI considered doing something like this but decided to keep a single carbon-cache (or rather three). After all it's like running serveral stacks on one server. And Grafana Viewers can't see anything they aren't supposed to.. Please give us more informations about your setup. Which webserver and which version of graphite and webserver are you using? And which distro do you use?. How did you install Graphite? And are you following an installtion guide? (If so, please add a link)\nThe error given is pretty clear in that there is no such file and the directory probably doesn't exist either. The installtion you made probably requires a different command.\nOtherwise, you can follow the official installation guide here: https://graphite.readthedocs.io/en/latest/install.html. Just want to give my opinion as a happy-ish user of the graphite/carbon stack. I started using Graphite about 1.5 years ago and it is one of the first projects that I got indepth expirience with.\nWhat bothers me the most about the current situation is the installation, as most people seem to agree on. Not being able to install graphite without errors on the 6th go is \"unacceptable\". (My install script sits at 30 lines for installation from plain CentOS/pip and it still failed last time because of Django)\nWhich brings me to Django. Honestly I expect most people to use Grafana in front of Graphite. Maintaining the Dashboard is probably not worth a lot of time and dependency problems. So finding the easiest way to open the API should probably be the focus. (Personal opinion, may be wrong)\nAs for the components, I am mostly happy with performance. Haven't had issues with the performance of the relay. Whisper can be slow, but it is okay most of the time.\nWhisper seems to me like a sane way to store metrics. In my expirience, if you size the whisper-storages properly, they are also not as big. And I like the fact that the databases are files and therefore easily accessible and editable. But again, maybe a personal thing.\nI do like the idea to have the whole stack in one language. Graphite will probably have applications on every processor architecture. But I would also accept (not sure about prefer) that a different solution is better and therfore ditch a selfmade solution. Then again, for example, should it be carbon-relay-ng or c-carbon-relay (or maybe keep carbon-relay)?\nOverall I really like the project and what it spawned with different implementations in different languages.\nIf I was a better programmer, I probably would have done some work on the project. But I am really not that great. I'll keep an eye on documentation and finding bugs though.. ",
    "hammondr": "Thanks, Piotr.  Are you saying there is a policy I can disable in graphite / carbon?  Or that I should disable the OS's FIPS policy?  (I can't do the latter because many government systems require the OS to operate in FIPS mode, crippling openssl's md5 features.). ",
    "luigiberrettini": "I did not change the test_render_view failing in Travis CI and it seems not to depend on smartSummarize\nIt is failing after changing another test: this is a nonsense. Thanks for the info!\nI cloned it from scratch but erroneously removed only the old webapp directory within the installation directory: it was taking my old conf file, sorry \ud83d\ude1e . The Graphite suite is installed on a CentOS Linux 7 (Core) VM with Kernel Linux 3.10.0-514.16.1.el7.x86_64\nI tried viewing graphs from the latest version of Chrome, Firefox and Internet Explorer on Windows 8.1 and the result is the same.\nGraphs rendered by already installed older versions are displayed correctly.\nTo complete installation I had to run:\nshell\ndjango-admin.py migrate  --pythonpath $INSTALL_DIR/webapp --settings graphite.settings --run-syncdb\ndjango-admin.py makemigrations  --pythonpath $INSTALL_DIR/webapp --settings graphite.settings\ndjango-admin.py collectstatic --noinput --pythonpath $INSTALL_DIR/webapp --settings graphite.settings. Can it depend from old config files or from file/folder permissions?. Perfect thank you: it works now!. ",
    "bartekgb": "Depends when. \nThe initial layout is:\nroot@c939778cccf3:/opt/graphite/storage/whisper/test# find\n.\n./1\n./1/test.wsp\nand at this stage issue doesn't emerge. \nAfterwards i populate /opt/graphite/storage/whisper/test with dirs with stated above \"for dirNr in seq 2 5000;do mkdir $dirNr;done\" command so that \"test/x\" dirs are created:\n```\nroot@c939778cccf3:/opt/graphite/storage/whisper/test# ls -l \ntotal 20000\ndrwxr-xr-x 2 root root 4096 Sep  5 08:51 1\ndrwxr-xr-x 2 root root 4096 Sep  5 09:12 10\ndrwxr-xr-x 2 root root 4096 Sep  5 09:12 100\ndrwxr-xr-x 2 root root 4096 Sep  5 09:12 1000\ndrwxr-xr-x 2 root root 4096 Sep  5 09:12 1001\ndrwxr-xr-x 2 root root 4096 Sep  5 09:12 1002\ndrwxr-xr-x 2 root root 4096 Sep  5 09:12 1003\ndrwxr-xr-x 2 root root 4096 Sep  5 09:12 1004\ndrwxr-xr-x 2 root root 4096 Sep  5 09:12 1005\n(...)\nroot@c939778cccf3:/opt/graphite/storage/whisper/test# find -maxdepth 1 -type d | wc -l\n5001\n```\nand then the issue emerges.. That's correct, all dirs except \"1\" are empty. \nPutting wsp files in them doesn't change things a bit.. ",
    "melnikk": "sorry, my fault, it should be a reverse merge.. ",
    "brobeckero": "Hi,\nThx for your answers.\nI can'to do that because my \"customer\" want to have a montly vision.\nHe wants to know how much customer used the service in january for exemple.\nI will propose to compute continuousely one sliding month but it won't\nanswer to his need.\nBR.\n2017-09-08 20:15 GMT+04:00 Piotr Popieluch notifications@github.com:\n\nIs there a way to indicate carbon to keep one value per month ?\nYes, you can configure the storage schema accordingly, see:\nstorage-schemas.conf\nhttps://graphite.readthedocs.io/en/latest/config-carbon.html#storage-schemas-conf\nThis kpi is calculated on another system and cannot be computed using\ndaily values ( nb of distinct user using the service: one customer can use\nthe services 2 diff\u00e9rents days and must be show as 1 customer ).\nNot sure if I understand this, why can't you compute that value more\noften? eg (pseudo code), select distinct count(username) from logins\nwhere login_date > day(-31) and schedule that every minute.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/issues/2031#issuecomment-328148098,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AclTLc7ZgAcuYPNAdvwkH9Uu5r6F1zk0ks5sgWgfgaJpZM4PQ7XY\n.\n. \n",
    "YouriAndropov": "Update : graphite-web seems to work when I use the following URL : curl http://localhost:80/graphite/\nA trailing slash seems to be needed, no error log is generated this way.... I'm using the standard apache setup. What should I do to fix this issue ?. Fine ! Now a curl http://localhost:80/graphite gets the following response (302) :\n```\n<!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n\n302 Found\n\nFound\nThe document has moved here.\n\n```\nThanks.. I changed the regexp because I had the same issue with /graphite/version etc :\nRedirectMatch ^(/graphite|/graphite/[^/]+)$ $1/. Hello, I'm still having a serious issue with graphite webapp. When I access http://server/graphite, I get the following apache log :\n[Thu Oct 26 16:47:54.953750 2017] [wsgi:error] [pid 4780:tid 140031000672000] [remote 172.27.164.1:51517] WARNING: Couldn't write lextab module 'pycparser.lextab'. [Errno 13] Permission denied: 'lextab.py'\n[Thu Oct 26 16:47:54.958945 2017] [wsgi:error] [pid 4780:tid 140031000672000] [remote 172.27.164.1:51517] WARNING: yacc table file version is out of date\n[Thu Oct 26 16:47:55.546186 2017] [wsgi:error] [pid 4780:tid 140031000672000] [remote 172.27.164.1:51517] WARNING: Couldn't create 'pycparser.yacctab'. [Errno 13] Permission denied: 'yacctab.py'\n[Thu Oct 26 16:47:56.071998 2017] [wsgi:error] [pid 4779:tid 140031025850112] [remote 172.27.164.1:51517] WARNING: Couldn't write lextab module 'pycparser.lextab'. [Errno 13] Permission denied: 'lextab.py'\n[Thu Oct 26 16:47:56.088285 2017] [wsgi:error] [pid 4779:tid 140031025850112] [remote 172.27.164.1:51517] WARNING: yacc table file version is out of date\n[Thu Oct 26 16:47:56.126466 2017] [wsgi:error] [pid 4777:tid 140031017457408] [remote 172.27.164.1:51518] WARNING: Couldn't write lextab module 'pycparser.lextab'. [Errno 13] Permission denied: 'lextab.py'\n[Thu Oct 26 16:47:56.142810 2017] [wsgi:error] [pid 4777:tid 140031017457408] [remote 172.27.164.1:51518] WARNING: yacc table file version is out of date\n[Thu Oct 26 16:47:57.232541 2017] [wsgi:error] [pid 4779:tid 140031025850112] [remote 172.27.164.1:51517] WARNING: Couldn't create 'pycparser.yacctab'. [Errno 13] Permission denied: 'yacctab.py'\n[Thu Oct 26 16:47:57.290915 2017] [wsgi:error] [pid 4777:tid 140031017457408] [remote 172.27.164.1:51518] WARNING: Couldn't create 'pycparser.yacctab'. [Errno 13] Permission denied: 'yacctab.py'\nAny idea ?\nI tried to add to the httpd.conf :\n<Directory /usr/lib/python2.7/site-packages/>\n            Require all granted\n        </Directory>\nBut it didn't work.. Ok I found the solution. Latest pycparser includes following warning in its package :\n\nKnown problems\n\nSome users who've installed a new version of pycparser over an existing\n  version ran into a problem using the newly installed library. This has to do\n  with parse tables staying around as .pyc files from the older version. If\n  you see unexplained errors from pycparser after an upgrade, remove it (by\n  deleting the pycparser directory in your Python's site-packages, or\n  wherever you installed it) and install again.\n\n\nSo I just removed pycparser module, then deleted its directory, then reinstalled it. It seems to work now.\nNB: this is absolutely unrelated to my trailing slash issue, which is still unresolved.. I'm wondering if this trailing slash issue isn't messing my grafana :\n::1 - - [30/Oct/2017:16:25:47 +0100] \"POST /graphite/render HTTP/1.1\" 302 217\nGrafana sends a HTTP POST to graphite on /graphite/render, and is redirected, due to the RedirectMatch directive. The result is : no data is retrieved by grafana.. graphite-web doesn't support non-ASCII characters, but carbon-cache does.\nThis is quite inconsistent.. ",
    "earonesty": "These issues occur because pycparser requires one-time write access to the source directory where the python scripts are installed.   Running it as root, once, solves this.. ",
    "tetianakh": "Thank you for quick reply, @deniszh. Do you have an idea on how to get a list of metrics created in the last X minutes? . @deniszh thank you!. ",
    "chrbayer84": "Awesome, thanks!. ",
    "UnitedMarsupials": "Using ctime was just an example. Yes, a careless copying of the Whisper file-tree or restoring from backup could break things -- though fixing them is easy too.\nAnd the method may not work with other storage-methods. But some way of querying these timestamps ought to be available.... > Also not sure if ctime/mtime is reliable enough.\nWhy? Of course, someone may mess them up either out of malice or incompetence, but the same can be said about all of the Graphite data -- there are no checksums, much less cryptographic signatures of the databases, are there?\nBut using ctime/mtime was just a suggestion -- a possible implementation of the feature. Whether they are used or not, the functionality is needed.\n\nIf you just need to get this info for analytical purposes [...]\n\nI didn't think, I need to explain my use-case, but here goes... I'm developing a program to feed some already timestamped values (such as web-server logs) into Graphite. The values available may go back for weeks and months, but the program does not know, when its previous instance ran, and so it can not know -- not reliably -- which values have already been submitted and which ones are new.\nIt would be very useful, if it were possible for the program to query the last timestamp of the value(s) from Graphite to avoid duplication.. > and walk the result to find the last non-null value.\nYes, thank you. This is exactly what I meant, when I wrote -- in this very ticket -- the following: to reliably determine, when a particular metric was last updated (or first created), a remote needs to make multiple requests.. ",
    "Serphentas": "Not sure if this is the right place to ask, but is there any documentation on how to get this version of Graphite to run behind uWSGI (in case of an nginx setup) ? So far, I've only found bits of information on how to use Apache and nothing else.. Thanks @deniszh, I got the stack running now.\nAbout that documentation you say, I'm making my own now so I wouldn't mind contributing to that if it's possible.. @JeanFred I can't see any file or link. Did you actually include it ?. Just so you know, the latest CI build fails because it seems Python 3.4 couldn't be downloaded.. @deniszh Thanks for merging, and you're welcome.\nI will also try to complete other parts of the doc when I have some time.. ",
    "hzy001": "@DanCech \nFirst, I don't think it will break parsing for specifications like 8:50 when now is specified because the s.replace(':','').isdigit() will be False.\nSecond, I jusk try a set of examples and the parsing fails because the length of the string isn't 13, for example 8:5020171013 etc.\nI just loose the restrict and I do think we need more advanced checks.. ",
    "ceven": "+1. Hi Denis, I was just showing interest in the absence of a 'vote for this issue' button. I would gladly add documentation, however I am also trying to understand what maxValue does, hence my need for documentation! \nAFAIS this doc is equivalent to https://graphite.readthedocs.io/en/latest/functions.html#graphite.render.functions.nonNegativeDerivative and does not contain information for maxValue\nOther than that, I think the existing documentation is pretty good and I'm very thankful for it!. ",
    "innomation": "A 'yum -y update' installed the newest version of python-cffi, which brought it up to v1.6, and resolved the issue. CentOS can't use PIP to fulfill the cairocffi version requirement. I hope this helps someone in the future. . I've tried pinning it manually and running, but had no luck. However, it wouldn't hurt to replicate the tests to confirm. If confirmed, my guess is older version installed and managed by yum package manager. \nRan a few tests, a 'yum update -y' will also update the packages required for scripts to pass. I jumped the gun too quickly and ran install on fresh 7.2 ISO out of the box in a dev environment.. ",
    "davidpsv17": "Hi @DanCech! Thanks for your reply\nYou mean the reply of the request?\n[{\"target\": \"sumSeries(newapi.rc..request..sqlite..count)\", \"datapoints\": [[1.0, X], [null, X], [null, X], [3.0, X], [null, X], [null, X], [3.0, X], [1.0, X], [1.0, X], [2.0, X], [2.0, X], [1.0, X], [null, X], [null, X], [null, X], [null, X], [1.0, X], [null, X], [1.0, X], [null, X], [3.0, X], [1.0, X], [2.0, X], [3.0, X], [2.0, X], [1.0, X], [1.0, X], [2.0, X], [1.0, X], [2.0, X], [2.0, X], [1.0, X], [2.0, X], [null, X], [null, X], [null, X], [3.0, X], [2.0, X], [1.0, X], [2.0, X], [3.0, X], [2.0, X], [1.0, X], [3.0, X], [2.0, X], [4.0, X], [2.0, X], [1.0, X], [1.0, X], [2.0, X], [1.0, X], [2.0, X], [1.0, X], [5.0, X], [3.0, X], [3.0, X], [3.0, X], [2.0, X], [1.0, X], [3.0, X], [2.0, X], [3.0, X], [1.0, X], [1.0, X], [3.0, X], [null, X], [4.0, X], [1.0, X], [1.0, X], [1.0, X], [2.0, X], [6.0, X], [2.0, X], [4.0, X], [6.0, X], [4.0, X], [4.0, X], [2.0, X], [2.0, X], [3.0, X], [11.0, X], [1.0, X], [3.0, X], [1.0, X], [1.0, X], [2.0, X], [1.0, X], [1.0, X], [1.0, X], [1.0, X], [7.0, X], [2.0, X], [5.0, X], [1.0, X], [7.0, X], [1.0, X], [2.0, X], [4.0, X], [1.0, X], [3.0, X], [5.0, X], [2.0, X], [2.0, X Connection #0 to host 127.0.0.1 left intact\n], [2.0, X], [2.0, X], [1.0, X], [3.0, X], [1.0, X], [2.0, X], [3.0, X], [5.0, X], [2.0, X], [3.0, X], [4.0, X], [9.0, X], [2.0, X], [5.0, X], [1.0, X], [3.0, X], [1.0, X]]}]\nI tried another request like:\ncurl -v '127.0.0.1:8888/render/?from=-2hours&target=averageSeries(newapi.rc..request..test.*.time)&format=json'\nAnd this was the reply:\nHTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\"\n\n504 Gateway Timeout\n\nGateway Timeout\nThe gateway did not receive a timely response\nfrom the upstream server or application.\n\nApache/2.4.18 (Ubuntu) Server at 127.0.0.1 Port 8888\n\nThanks!. Sorry @DanCech \nIt has 200 series and in the log you can see something like:\n2017-11-08,09:57:43.369 :: Retrieval of highestAverage(groupByNodes(newapi.newapi.test...*.query.time, 'avg', 4, 5), 10) took 412.889228\n2017-11-08,09:57:43.373 :: JSON rendering time 0.001519\n2017-11-08,09:57:43.373 :: Total request processing time 412.894376\nThis is happening with all the queries..\nThanks!. ",
    "wangzaiqqhop": "hello all:\n     when i use sumSeries(API..QOSController..hx_category_Q19_count) to request the data from graphite\uff0cit goes very slow\uff1bbut if i change it to sumSeries(api.QOSController.A.category_Q19_count,api..QOSController.B.category_Q19_count,api.*.QOSController.C.category_Q19_count),the request goes very fast . \nthe amount of wsp file under the QOSController folder is about  30 thousand\uff0cbut i only want to use asterisk to match the A\u3001B\u3001C folder \uff0ci don\u2018t understand why use the asterisk in the query could be so slowly\n      could you help me,thank you!. \n. @deniszh thank you for answer my question so soon,\ni use just a single server\uff0cone carbon-cache use to collect the metrics from about 10 online-mechine\uff0creceive around 35 thousand datapoint per minute\uff0c\nthe graphite-carbon version is 1.0\uff0c\nwhen i use exact query or the node of asterisk has less amount of wsp file\uff0cthe render request goes very fast\uff1bbut once the query contines asterisk and the wsp files under the asterisk node is very large, the query becomes very slow\uff0cit just like the asterisk has slow down the speed of query the location of the target file\uff0ccould you tell me the principle of graphite use to matching documents while the asterisk exists or not ?\njust like the query sumSeries(api..QOSController..hx_subCategory_Q19_daily_info1_count),the second asterisk is exactly only need to match A\u3001B\u3001C three categories,and the 'hx_subCategory_Q19_daily_info1_count' is just exists in the A/B/C folder \uff0cbut under the node of QOSController there exist other 30 thousand wsp file\uff0ci thought whether is because of the large amount of wsp file exist under the QOSController node\uff0cso use asterisk to match the A/B/C folder become very slowly\uff0cbut i exactly don\u2019t understand the principle.\n. just as the picture show the 4 query above is very slow but the 4 below is very fast\n\n. @DanCech thank you for your answer!\nbut i still do not quite understand the principle, except the A/B/C node is folder\uff0cthe other file under the QOSController is all leaf node\uff0cso while the finder match the leaf node it will never continue to operate, i can't understand how could the finder matching the need node so slowly?\nadd i try to use the query  sumSeries(api..QOSController.{A,B,C}.hx_subCategory_Q19_daily_info1_count) it seems has no difference with the sumSeries(api..QOSController.*.hx_subCategory_Q19_daily_info1_count) , also is not so fast.\nwhether graphite has a index list of the wsp files and when it's exact query \uff0cgraphite will use hash to position the file directly\uff1fand while use asterisk it will match the file under the node one by one \uff1f\nthank you!\n. @DanCech oh\uff0cthank you\uff01. @DanCech hello\uff0cDanCech\uff1a\ncould you give me a brief description why the exact query is very fast\uff0cmy python is not so good \uff0ci have just go the check the finder code but haven't checkout the reason\uff1bis there a index file tree in the graphite\uff1fi have checked the  path '/opt/graphite/storage/ceres',  nothing is in it\uff0cor is there some other reason \uff0cwhy exact query has no need to scan the entire directory and could position the target file so quickly?\nthank you!. @DanCech ok,thank you for your answer!. @DanCech hello\uff0cDanCech\uff1a\n      i have changed the  finder part of graphite to your new commit code\uff0cand the query  efficiency has improved about 60 percent\uff0cthat is great\uff1b\n      according to your last commit \uff0cmay i think that you have changed the logic to avoid some leaf node execution the method of join\uff1f\n\n      i tested the finder code today and found that it seems like the main time cost on the fuzzy query now\uff0c is because of the folder scan \uff0ci am thinking is it possible to cache the filename list of each folder in a container such as a hash Set for some time\uff0cto avoid the folder scan of the subsequent queries?\n      thank you!\n. ",
    "innerx": "@piotr1212 \ngreat, it works, thanks very much!. ",
    "xtus": "Just to clarify, my guess it that normalize should output a (start, end, step) range that guarantee same amount of timestamps and datapoints between the series involved.\nAnd I guess instead that in version 0.9.12 there was a bug with the __consolidatingGenerator in TimeSeries, that was returning an extra None.\nI think, probably not correctly, that normalize should actually produce in output series that are aligned in terms of timestamp; if can somebody provide some documentation or a comment about the expected behaviour for normalize would be great.. @DanCech yep, my quick fix was indeed the numPoints = min(len(timestamps), len(datapoints)) in _summarizeValues.\nPR #2114 fix the issue.\nThanks. ",
    "wolfzhaoshuai": "For question 2, I think carbon-c-relay can help you. Carbon-c-relay block, transfer, and aggregate metrics based on pre-defined patterns.. ",
    "kira510": "Hi @deniszh @wolfzhaoshuai  ,\nThanks for the suggestions. You may close this now.\nRegards,. ",
    "wridgers": "\nThanks, but already fixed in #2111\n\nAh. Nothing showed up when I search for it.\nOh well, glad it's fixed!. Increasing the REMOTE_FETCH_TIMEOUT stopped most of these, but afaiu graphite-web isn't doing any remote fetching here.... CLUSTER_SERVER and USE_WORKER_POOL are commented out, so I'd guess they're whatever the default is?\nFrom local_settings.py:\n```\nCLUSTER_SERVERS = [\"10.0.2.2:80\", \"10.0.2.3:80\"]\nUSE_WORKER_POOL = True\n```\nShould we try setting CLUSTER_SERVER = [] and USE_WORKER_POOL=False?. We have four caches, four aggregators, and two relays all running on the same machine as graphite-web (nginx + gunicorn). Relays are behind haproxy.. I will tentatively say this seems to have worked. I'll confirm later today and make a PR fixing the issue.\nThanks @deniszh . root@graphite2:~# cat /etc/lsb-release \nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=16.04\nDISTRIB_CODENAME=xenial\nDISTRIB_DESCRIPTION=\"Ubuntu 16.04.3 LTS\"\nroot@graphite2:~# python --version\nPython 2.7.12\nroot@graphite2:~# pip --version\npip 9.0.1 from /usr/local/lib/python2.7/dist-packages (python 2.7)\nroot@graphite2:~#. This can probably be closed. :)\nUpgraded to 1.1.2 today and it seems much smoother! \nThanks guys :). Hey @deniszh, if this was merged then https://github.com/graphite-project/carbon/pull/696 should probably be merged too. :). My PR concerns carbon-aggregator aggregation methods, which I believe to be distinct from whisper storage aggregation methods that are defined here.\nLooks like the docs don't include avg_zero, absmax, or absmin.\nI don't think it would make sense to add percentile aggregation to whisper file storage aggregation, what do you think?. Ah yes, you are right. I understand now. Thinking back, I'm not sure why I decided 107 was a good place to put the warning! :)\nFixed and amended the offending commit!. ",
    "shanson7": "For something like seriesByTag('name=~node.*.node3') would _getFirstPathExpression return None? If so, this looks good to me.. ",
    "AlejandroRivera": "When i tried it myself without auth (and without --run-syncdb) it seemed to have created all the tables (including auth relations), since when i tried --run-syncdb right after, there were no issues.\nAt this point, i'm not sure what's the difference between:\nPYTHONPATH=$GRAPHITE_ROOT/webapp django-admin.py migrate auth --settings=graphite.settings\nand\nPYTHONPATH=$GRAPHITE_ROOT/webapp django-admin.py migrate --settings=graphite.settings\nbut if you say that's the right way of creating the DB for the first time, i can make the change in the PR. I'll await confirmation.. > Looks like migrate auth and migrate behaves little better.\nSorry, i'm not sure i understand what you mean. Can you elaborate? . My version is 1.9.13. @deniszh, do you have any ideas of who to tag so the discussion can take place? I do not :(. ",
    "chrim5": "Hi everybody,\nBuild seems to work again:\nStep 28 : RUN PYTHONPATH=/opt/graphite/webapp django-admin.py migrate --settings=graphite.settings --run-syncdb\n ---> Running in f95acd8c278d\nSystem check identified some issues:\nWARNINGS:\n?: (urls.W002) Your URL pattern '^/(.+)$' [name='functionDetails'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/(.+)$' [name='tagDetails'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/(?P\\d+)/?$' [name='events_detail'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/(?P[^/]+)$' [name='dashboard'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/(?P[^/]+)/(?P[^/]+)$' [name='dashboard_template'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/?$' [name='browser'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/?$' [name='composer'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/?$' [name='dashboard'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/?$' [name='events'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/?$' [name='functionList'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/?$' [name='metrics'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/?$' [name='render'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/?$' [name='tagList'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/?$' [name='version_index'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/?$' [name='whitelist_show'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/add$' [name='whitelist_add'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/autoComplete/tags$' [name='tagAutoCompleteTags'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/autoComplete/values$' [name='tagAutoCompleteValues'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/create-temporary/?$' [name='dashboard_create_temporary'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/delSeries$' [name='delSeries'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/delete/(?P[^/]+)$' [name='dashboard_delete'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/delete_template/(?P[^/]+)$' [name='dashboard_delete_template'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/edit/?$' [name='account_edit'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/email$' [name='dashboard_email'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/expand/?$' [name='metrics_expand'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/find/?$' [name='dashboard_find'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/find/?$' [name='metrics_find'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/findSeries$' [name='findSeries'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/find_template/?$' [name='dashboard_find_template'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/get-metadata/?$' [name='metrics_get_metadata'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/get_data?$' [name='events_get_data'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/header/?$' [name='browser_header'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/help/?$' [name='dashboard_help'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/index.json$' [name='metrics_index'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/load/(?P[^/]+)$' [name='dashboard_load'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/load/(?P[^/]+)/(?P[^/]+)$' [name='dashboard_load_template'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/load_template/(?P[^/]+)/(?P[^/]+)$' [name='dashboard_load_template'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/local/?$' [name='render_local'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/login/?$' [name='account_login'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/login/?$' [name='dashboard_login'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/logout/?$' [name='account_logout'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/logout/?$' [name='dashboard_logout'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/mygraph' [name='composer_mygraph'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/mygraph/?$' [name='browser_my_graph'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/remove$' [name='whitelist_remove'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/save/(?P[^/]+)$' [name='dashboard_save'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/save_template/(?P[^/]+)/(?P[^/]+)$' [name='dashboard_save_template'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/search/?$' [name='browser_search'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/send_email' [name='composer_send_email'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/set-metadata/?$' [name='metrics_set_metadata'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/tagMultiSeries$' [name='tagMultiSeries'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/tagSeries$' [name='tagSeries'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/update/?$' [name='account_update'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/usergraph/?$' [name='browser_usergraph'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n?: (urls.W002) Your URL pattern '^/~(?P[^/]+)/(?P[^/]+)/?$' [name='render_my_graph'] has a regex beginning with a '/'. Remove this slash as it is unnecessary.\n\u001b[0mOperations to perform:\n  Synchronize unmigrated apps: render, staticfiles, whitelist, metrics, composer, browser\n  Apply all migrations: account, sessions, admin, tags, auth, url_shortener, contenttypes, dashboard, events, tagging\nSynchronizing apps without migrations:\n  Creating tables...\n    Running deferred SQL...\nRunning migrations:\n  Rendering model states... DONE\n  Applying contenttypes.0001_initial... OK\n  Applying auth.0001_initial... OK\n  Applying account.0001_initial... OK\n  Applying admin.0001_initial... OK\n  Applying admin.0002_logentry_remove_auto_add... OK\n  Applying contenttypes.0002_remove_content_type_name... OK\n  Applying auth.0002_alter_permission_name_max_length... OK\n  Applying auth.0003_alter_user_email_max_length... OK\n  Applying auth.0004_alter_user_username_opts... OK\n  Applying auth.0005_alter_user_last_login_null... OK\n  Applying auth.0006_require_contenttypes_0002... OK\n  Applying auth.0007_alter_validators_add_error_messages... OK\n  Applying dashboard.0001_initial... OK\n  Applying events.0001_initial... OK\n  Applying sessions.0001_initial... OK\n  Applying tagging.0001_initial... OK\n  Applying tagging.0002_on_delete... OK\n  Applying tags.0001_initial... OK\n  Applying url_shortener.0001_initial... OK. ",
    "mlausch": "@deniszh: The Fix works for me.. ",
    "vilalobus": "It does not work using this nonNegativeDerivative, shows nothing on the Graph, even if I do not use the MaxValue, it keeps Blank. Is there any other way ? . \n. Without this nonNegativeDerivative:\n\n. Thanks a Lot Dan. Worked !. ",
    "redhotpenguin": "I saw other commercial alternative storage backends here as was mentioned, so it seemed to fit well to list IRONdb here. IRONdb is not a replacement for Graphite; it only acts as the storage component (hence the Whisper compatible API). IRONdb is closed core like some of the alternatives, but has open source components (like other alternate storage backends listed).. ",
    "ahoffpauir": "Yeah, the space separated list works! I can query them fine that way too. It is just when putting them in the array that this happens, and it doesn't allow me to query them either.. ",
    "redbaron4": "So I ran into this problem again (with both carbon and graphite-web 1.1.2 using pip 9.0.1) and here is what I can summarize:\n\npip install finds an existing older version (mostly 1.x) and uninstalls it.\nDuring uninstall it removes all the files that it remembers installing. (This list can be seen in /opt/graphite/lib/carbon-1.1.1-py2.7.egg-info/installed-files.txt).\nUnfortunately storage/ is in the file and hence is removed. Here is the trace when using debug option with pip\n\nInstalling collected packages: graphite-web\n  Found existing installation: graphite-web 1.1.1\n    Uninstalling graphite-web-1.1.1:\n      Removing file or directory /opt/graphite/bin/build-index\n      Removing file or directory /opt/graphite/bin/build-index.sh\n      Removing file or directory /opt/graphite/bin/run-graphite-devel-server.py\n      Removing file or directory /opt/graphite/conf/dashboard.conf.example\n      Removing file or directory /opt/graphite/conf/graphTemplates.conf.example\n      Removing file or directory /opt/graphite/conf/graphite.wsgi.example\n      Removing file or directory /opt/graphite/examples/example-client.py\n      Removing file or directory /opt/graphite/examples/example-graphite-vhost.conf\n      Removing file or directory /opt/graphite/storage/ceres\n      Removing file or directory /opt/graphite/storage/log\n      Removing file or directory /opt/graphite/storage/rrd\n      Removing file or directory /opt/graphite/storage/whisper\n      Removing file or directory /opt/graphite/webapp/content/css/darkX.css\n      Removing file or directory /opt/graphite/webapp/content/css/darkX/button-close-focused.png\n      Removing file or directory /opt/graphite/webapp/content/css/darkX/button-maximize-focused.png\n      Removing file or directory /opt/graphite/webapp/content/css/darkX/button-minimize-focused.png\n\nNext the install happens where these are recreated\n\ncopying webapp/content/js/ext/resources/images/default/shared/calendar.gif -> /opt/graphite/webapp/content/js/ext/resources/images/default/shared\n    creating /opt/graphite/storage/whisper\n    creating /opt/graphite/storage/ceres\n    creating /opt/graphite/storage/rrd\n    creating /opt/graphite/storage/log\n    creating /opt/graphite/storage/log/webapp\n    copying conf/dashboard.conf.example -> /opt/graphite/conf\n    copying conf/graphite.wsgi.example -> /opt/graphite/conf\n    copying conf/graphTemplates.conf.example -> /opt/graphite/conf\n    copying examples/example-graphite-vhost.conf -> /opt/graphite/examples\n    copying examples/example-client.py -> /opt/graphite/examples\nI can understand the motivation of making all storage/ paths as data files (so that users have minimal work to do). Unfortunately, pip protocol is somewhat different than what a more mature tools like RPM would provide and so it removes the directories even during an update. It may be possible to raise this issue with them but that will be long drawn process (if they do decide to fix it).. ",
    "surendarchandra": "We are rendering metric where the matching metrics might be a1-metric and b1-metric (and so there would be different number of data points for each metric). Would that cause this issue? Thanks @deniszh . Thanks for the fix!! Is this possible if Graphite was in a clustered setting, failed to detect localhost and so sent a query to itself and then try to merge those two results?\n. ",
    "codexiao": "@DanCech  Awesome! Thanks a lot!. ",
    "yuribit": "@deniszh, thank you so much! I was missing the MIDDLEWARE and AUTHENTICATION_BACKENDS settings.\nClosing this.\n. ",
    "mcbsdEU": "@DanCech it is also valid point. Well it was already fixed once...\nMaybe configuration parameter could be applied so setting that into \"True\" could continue with other node when previous one failed.\nAlso, it should log error in any case, the question is if it is reason to do exception and exit.. In our use-case, we are sending same metrics in two parallel streams (into first node and into second node of cluster). So it behaves as active-active at the end.\nWhen one node is down, etc. during maintenance, only second node receives metrics. When node which was down again is up and running and it receives query for metrics which for certain time came just into other node, it will \"merge\" results and provide complete response to requester.\nI think the key point here is, that we send same metrics to both.. So, will you include respective configuration option?. Hi,\nis it still for discussion?\nThx. @clusterfudge Based on description @2303 I'd also say it seems to be opposite...\nI'd like to write PR but I'm not really an expert in this.... Hello @clusterfudge \nIf it is true regarding #2303, then it seems all ok - it is configurable.\nHowever, it is strange, we are using graphite-web-1.1.1-1.fc28.noarch on RHEL, and there is nothing like STORE_FAIL_ON_ERROR or REMOTE_STORE_HARD_FAILURES. Could you clarify this one?\nThanks. Ah I see....Ok, let's wait for @deniszh reply.. @funtiik please check reply from @clusterfudge : \"#2303 hasn't made it into a release yet.\". ",
    "toerkl": "Hi. I would be happy to be able to set this parameter. I have an ELB with two graphite frontends that look into two backends with replicated data for high availability purposes. I would still want to return the result if one backend goes down. Or is there a better way to achieve high availability?. ",
    "clusterfudge": "@DanCech I've created #2303 in an attempt to address this issue (it works in my test cluster as expected). Care to take a look?. So, there's a windy story here. \n1) In this original issue, mcbsdEU's complaint is that in 1.0.x, if any of CLUSTER_SERVERS is unavailable (or a request fails), all requests fail. \n2) mcbsdEU also brings to light that behavior is inconsistent between 1.0.x and 1.1.x, and the 1.1.x behavior is preferred.\n3) In Dan's comment, he states an opinion that mcbsdEU's complaint about 1.0.x is the correct behavior, and the behavior in 1.1.x is a regression.\n4) In #2303 we opine that while the 1.1.x behavior may be a regression, it is now the expected behavior and a fix would have to preserve the existing behavior. As such, the fail-hard behavior is configurable and disabled by default.\n5) #2303 is merged, and you can decide what is the appropriate behavior for you.\n. #2303 hasn't made it into a release yet. @deniszh anything keeping it from getting into 1.1.4?. @DanCech FYI. Related to conversation in https://github.com/graphite-project/graphite-web/issues/2218#issuecomment-362283812. I actually intended False to be the default to preserve existing behavior (late night coding, always a winner). I'll update that, and rewrite the conditional to be more straightforward. Thanks for the feedback!. @deniszh I believe the default in the current iteration is that in settings, we'll default to False (preserving existing behavior), and I've update the local_settings.py.example with a value of true, but commented out (so the action to change behavior is uncommenting the line). Is that not the correct approach here?. Ok, I'll update. Working on the unit test as we speak.. I've continued digging on another edge case that's bugging my team, and I think there's another issue I want to capture this (we currently only capture failures, not timeouts). Placing on hold.. @DanCech @deniszh This is now ready for review again, thanks for your patience. \nI understand based on our conversation (and the comments in #2218 ) that there's a desire to yield a subset of metrics when not all backends are available. I'm of the mindset (and it seems that as of a few months ago @DanCech agreed ) that this is incorrect, and is more akin to your database silently lying to you. While users manually browsing metrics may find this behavior acceptable, it makes it near impossible to build tools on top of the graphite api without consistency guarantees. \n~~My recommendation would be to continue a full audit of the codebase and abandon anywhere we silently swallow errors and return partial results. I'm hoping this is the only place, but I'm honestly not sure. I'm still very new to the source (even though I've been loving graphite since 2011, so I guess egg on my face :)). ~~\nEdit I feel kind of dickish recommending a full audit in retrospect, so I'll walk that back a bit. I think this is a bug, and a kind of scary one. I'm totally on board with Dan's sentiment from Feb that Graphite should fail in scenarios like this, as it's much more predictable behavior. Understanding that fixing this bug would likely render some clusters broken (where broken is failing queries vs. wrong data), I'd still recommend this become the default behavior and holding to a strongly-consistent response philosophy when encountering more issues like this going forward.. @DanCech @deniszh anything blocking us here?. @JosephFY I was about to open an issue in the same vein, but I've dug a little bit more into where the time is being spent. I checked apache logs on my backend instances and they were responding well within the timeout, but the graphite-web frontend instance was struggling to handle the data within the timeout. \n@deniszh and @DanCech you may be interested in hearing this: Between graphite 1.0.x and 1.1.x, remote reads switched deserialization using the file-like input APIs. In the past, the full response was read into memory, and then deserialized with pickle.loads. I'm prepping an example gist+data, but my anecdotal experience is that with both pickle and the version of msgpack that graphite uses, the file-like deserialization mechanism is ~500x slower than pickle.loads for large payloads (> 5mb). \nThe file-like reader usage appears to have been introduced in 1.1.0-pre1, and optimized with BuffereHTTPReader impl. Based on the BufferedHTTPReader PR, it sounds like the streaming reads were introduced to keep memory consumption under control, and the buffered reader as a performance optimization. Neither of these appear to have accounted for the inherent performance regression of the original change.\nI've got a change that I'm not thrilled with that will attempt to read the first BUFFER_SIZE bytes and avoid the file-read deserialization if possible. With a large BUFFER_SIZE (mine is set to 25mb), this works pretty well. The downside is that anything over 25mb (by even a byte) is practically guaranteed to fail. We're talking 4s to read, 40ms to deserialize vs > 100s to read+deserialize using the file-read deserialization. \nI'd love to hear some thoughts in this space, and I'll share some data as soon as I can package it nicely.\ncheers!\nNOTE I make copious use of \"sounds like\" and \"appears\" in my  spiel because I wasn't there, and you should feel free to correct me if I'm wrong :). NOTE: NOT FINAL, PLEASE DO NOT MERGE\nUPDATE: IT'S COOL NOW PEOPLE. @deniszh yeah, I like the idea of 0 to disable (vs using an absurdly high number to avoid). I'll refresh this PR to reflect.. Hey @deniszh , I spent a bit of time polishing yesterday and should be able to get it out tonight. What's the target cutoff for 1.1.4?. @deniszh That was an artifact of the prior implementation and is no longer relevant. In this implementation, we use the streaming deserializers if we have REMOTE_BUFFER_SIZE > 0, otherwise we use the inline deserialization. This does not change the behavior of existing users, but allows people to opt-out of streaming serialization to increase performance on large payloads.\nTL/DR; that comment is no longer relevant.. Looks like I have some character encoding issues to fix in py3.x. Will tackle tomorrow.. @DanCech , based on some analysis that I ran recently and shared in this gist, json should be more performant on all metrics. Since the names of metrics aren't schema'd, there's really few savings in either space or compute (at least where I was testing on 2.7). If we wanted to take on an additional dependency, ujson is an amazing drop in replacement json library (the perf is bananas). \nAgain, the above tests are local, somewhat contrived, and on python2.7, but the differences were pretty stark. The implementation of umsgpack embedded in graphite does not appear to be particularly performant, probably because it's a pure-python implementation (as opposed to something with native bindings).. I used the message pack module that's embedded in graphite-web. Is that a fall back if the native implementation isn't installed?. I find that very plausible. What's the path forward here? Is there a api/schema definition somewhere that I can reference to bring the json api in line with the other response formats? Otherwise, I feel like I'll be reverse engineering this and likely miss some edge cases. . I'm not; with the native python-msgpack library is installed, performance is perfectly acceptable.. I agree @deniszh , but I don't feel like enough of an owner here yet to start a new module. I like @DanCech 's approach of reuse better than my own, but I explicitly didn't bring over deserialize because of what I thought were incompatibilities in the data was transformed from raw response. Maybe I'm wrong :man_shrugging: \nWe could at the very least combine the two so that there's good unit test coverage for both find and read when REMOTE_BUFFER_SIZE <= 0\n. @deniszh can you speak to this at all?. Thanks, Dan. In lieu of that (and my availability), I'm going to put this on hold and not attempt to get it into the 1.1.4 release this weekend. I'll reach out if I run into any issues, but off the top of my head I can honestly say I don't understand the purpose of pathExpression vs name. Can you clarify?. ",
    "funtiik": "Hi guys,\nAre there any plans/ideas on how to resolve this issue?\nThanks.. Hi @clusterfudge \nRegarding  #2303 I've tried setting the option STORE_FAIL_ON_ERROR = True and when one of the backend nodes is unavailable it still raises an exception and not returning the data.. @mcbsdEU  #2303 is merged and I have used master branch.. ",
    "t4ko": "Well that was fast and it works! Thank you.\nI quickly made this docker image using a Graphite installation made from the new branch including this feature for anyone interested : https://hub.docker.com/r/t4ko/docker-graphite-tags-hash/. ",
    "twiedenbein": "The merging functionality seems to work for me.. ",
    "raghavan20": "Sorry, I do not understand your comment completely as I managed to read data points in the whisper file using whisper-dump utility.\n```\nSearching for interested data points based on timestamp\nwhisper-dump.py direction.wsp  | grep 150019\n```\nwhy do you say that i cannot visualize data older than 14 days?\n@DanCech are there other tools to visualize historical metrics if it is not possible with graphite-web?. thanks @deniszh  for the answer. our original intention was to create a backup for a  historic event and visualize later in time. i was thinking if i just copied the whisper file immediately soon after the interested event occurred and assuming that no new data went in ( hence no aggregation happens ), i should be able to visualize historic data. i guess we should have set retention like 1s:2y if we wanted to visualize data at 1s granularity in the next two years. i originally did not think about it considering that all data except the interested event duration is waste of file space.\nbut from what you have explained graphite-web would not work like that. the amount of data that can be visualized from whisper files is based on archives and points. now given that i have the whisper file which holds 14 days of historical data, is it possible to run through some tools so that i can visualize from Graphite apart from the now query parameter supported with 1.1+ version of graphite-web application?\n. Hi, I tried to run the latest version of graphite-web so that i can use the now query parameter. \nI believe I am using 1.1+ of graphite-web based on the below output. I am running a Docker container from graphiteapp/graphite-statsd.\n```\nls /opt/graphite/lib/* | grep py2\nlib/carbon-1.1.1-py2.7.egg-info\n```\nException while viewing the direction.wsp file from graphite/exception.log\n```\n 2018-02-05,15:54:52.884 :: Exception encountered in \nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 132, in get_response\n    response = wrapped_callback(request, callback_args, *callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 117, in renderView\n    data.extend(evaluateTarget(requestContext, targets))\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 15, in evaluateTarget\n    prefetchData(requestContext, pathExpressions)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 255, in prefetchData\n    for result in STORE.fetch(pathExpressions, startTime, endTime, now, requestContext):\n  File \"/opt/graphite/webapp/graphite/storage.py\", line 137, in fetch\n    raise Exception(\"Fetch for %s failed: %s\" % (str(patterns), str(job.exception)))\nException: Fetch for [u'DEVICE_3375.direction'] failed: 'NoneType' object is not iterable\n```\n. ",
    "LTMXcitrus": "Sorry about that, I'm a python newbie and I must have missed this...\nTests are ok now :+1: . Thanks for your help, it's done as well :+1: . ",
    "DineshGuptaa": "Hello @deniszh\nThanks for quick reply :-)\nWhile running said command then getting below error:\ndjango-admin.py: command not found\nI have Python version : 2.7.14 and DJango version (1, 11, 10, u'final, 0)\nPlease let me know what I need to do.\nRegards,\nDinesh\n. ",
    "Abid1986": "I am getting below error \n[root@localhost graphite]# PYTHONPATH=/opt/graphite/webapp /usr/lib/python2.7/site-packages/django/bin/django-admin.py  migrate --settings=graphite.settings --run-syncdb\n/usr/lib/python2.7/site-packages/tagging/models.py:10: RemovedInDjango19Warning: django.contrib.contenttypes.generic is deprecated and will be removed in Django 1.9. Its contents have been moved to the fields, forms, and admin submodules of django.contrib.contenttypes.\n  from django.contrib.contenttypes import generic\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/django/bin/django-admin.py\", line 5, in \n    management.execute_from_command_line()\n  File \"/usr/lib64/python2.7/site-packages/django/core/management/init.py\", line 338, in execute_from_command_line\n    utility.execute()\n  File \"/usr/lib64/python2.7/site-packages/django/core/management/init.py\", line 312, in execute\n    django.setup()\n  File \"/usr/lib64/python2.7/site-packages/django/init.py\", line 18, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File \"/usr/lib64/python2.7/site-packages/django/apps/registry.py\", line 115, in populate\n    app_config.ready()\n  File \"/usr/lib64/python2.7/site-packages/django/contrib/admin/apps.py\", line 22, in ready\n    self.module.autodiscover()\n  File \"/usr/lib64/python2.7/site-packages/django/contrib/admin/init.py\", line 24, in autodiscover\n    autodiscover_modules('admin', register_to=site)\n  File \"/usr/lib64/python2.7/site-packages/django/utils/module_loading.py\", line 74, in autodiscover_modules\n    import_module('%s.%s' % (app_config.name, module_to_search))\n  File \"/usr/lib64/python2.7/importlib/init.py\", line 37, in import_module\n    import(name)\n  File \"/usr/lib/python2.7/site-packages/tagging/admin.py\", line 3, in \n    from tagging.forms import TagAdminForm\n  File \"/usr/lib/python2.7/site-packages/tagging/forms.py\", line 11, in \n    class TagAdminForm(forms.ModelForm):\n  File \"/usr/lib64/python2.7/site-packages/django/forms/models.py\", line 274, in new\n    \"needs updating.\" % name\ndjango.core.exceptions.ImproperlyConfigured: Creating a ModelForm without either the 'fields' attribute or the 'exclude' attribute is prohibited; form TagAdminForm needs updating.\n. Currently i am using Django 1.11  and still getting the below error.\n[root@localhost graphite]# django-admin.py version\n1.11\n[root@localhost graphite]# PYTHONPATH=/opt/graphite/webapp /usr/lib/python2.7/site-packages/django/bin/django-admin.py migrate --settings=graphite.settings --run-syncdb\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/django/bin/django-admin.py\", line 5, in \n    management.execute_from_command_line()\n  File \"/usr/lib64/python2.7/site-packages/django/core/management/init.py\", line 363, in execute_from_command_line\n    utility.execute()\n  File \"/usr/lib64/python2.7/site-packages/django/core/management/init.py\", line 337, in execute\n    django.setup()\n  File \"/usr/lib64/python2.7/site-packages/django/init.py\", line 27, in setup\n    apps.populate(settings.INSTALLED_APPS)\n  File \"/usr/lib64/python2.7/site-packages/django/apps/registry.py\", line 108, in populate\n    app_config.import_models()\n  File \"/usr/lib64/python2.7/site-packages/django/apps/config.py\", line 202, in import_models\n    self.models_module = import_module(models_module_name)\n  File \"/usr/lib64/python2.7/importlib/init.py\", line 37, in import_module\n    import(name)\n  File \"/opt/graphite/webapp/graphite/events/models.py\", line 4, in \n    from tagging.models import Tag\n  File \"/usr/lib/python2.7/site-packages/tagging/models.py\", line 10, in \n    from django.contrib.contenttypes import generic\nImportError: cannot import name generic\n. I reinstalled django-tagging again with latest version.It worked.\nMany thanks :). Hi,I am running it in chrome browser.\nIn  /opt/graphite/storage/log/webapp error.log i can find below error as no module named wsgi.\n[Sun Mar 25 02:24:02.226890 2018] [:error] [pid 2647] Traceback (most recent call last):\n[Sun Mar 25 02:24:02.226910 2018] [:error] [pid 2647]   File \"/opt/graphite/conf/graphite.wsgi\", line 4, in \n[Sun Mar 25 02:24:02.226957 2018] [:error] [pid 2647]     from graphite.wsgi import application\n[Sun Mar 25 02:24:02.226973 2018] [:error] [pid 2647] ImportError: No module named wsgi\n. ",
    "pinkynrg": "@deniszh: It takes quite a while to process and then it doesn't show any points. \n@DanCech: do you have an idea on how i can solve this issue? Is there a way to either:\n- get some single value average so i can use the scale() function?\n- get an actual series (with more than 3 points) so I can use the divideSeries() function?. That would be an awesome improvement if it will not slow things down terribly. I love it, this patch would help me out so much, hopefully it will not break anything else!. Nice man, I'll wait for the merge into master! :) thanks again, that was fast support indeed.. I see the merge into master :) I'll update to the new version of graphite, do some testing, and give you feedback. \nThanks. worked like a charm. closing. Boom! closing :). looks like python manage.py is equivalent to django-admin.py --settings=graphite.settings. > I can be wrong, but I suspecting that template can't contain '.'\nDo you mean that template[prefix] can't have dots inside the passed string portal.ovomec.santa_maria_di_sala.1308756.ovomec_smart_one.food_distributed_today?\nIf that's what you meant I have used the template functionality by passing string with dots without having issues. \nOnly this particular query, used with templating, gives me hard time. \nI'll try as well to pin point the issue so I can give more details.. for instance the below query works as expected:\nhttp://graphite.net/render?target=template(averageSeries($ctrl.temperature_probe.0))&template[ctrl]=this.is.one.of.my.controllers&from=00:00_20180101&until=01:00_20180101&format=json. working query:\nhttp://graphite.faccousa.net/render?format=json&from=20180215&until=20180301&target=template(timeStack(summarize(portal.ovomec.santa_maria_di_sala.1308756.ovomec_smart_one.food_distributed_today.0,'10minutes','max'),'24h',0,10))\nresult of working query:\n[{\"datapoints\": [[596.0, 1518674400], [596.0, 1518675000], [596.0, 1518675600], [596.0, 1518676200], [596.0, 1518676800], [596.0, 1518677400], [596.0, 1518678000], [596.0, 1518678600], [596.0, 1518679200], [596.0, 1518679800], [596.0, 1518680400], [596.0, 1518681000], [596.0, 1518681600], [596.0, 1518682200], [596.0, 1518682800], [596.0, 1518683400], [596.0, 1518684000], [596.0, 1518684600], [596.0, 1518685200], [596.0, 1518685800], [596.0, 1518686400], [596.0, 1518687000], [596.0, 1518687600], [596.0, 1518688200], [596.0, 1518688800], [596.0, 1518689400], [596.0, 1518690000], [596.0, 1518690600], [596.0, 1518691200], [596.0, 1518691800], [596.0, 1518692400], [596.0, 1518693000], [596.0, 1518693600], [596.0, 1518694200], [596.0, 1518694800], [596.0, 1518695400], [596.0, 1518696000], [596.0, 1518696600], [596.0, 1518697200], [596.0, 1518697800], [596.0, 1518698400], [596.0, 1518699000], [596.0, 1518699600], [596.0, 1518700200], [596.0, 1518700800], [596.0, 1518701400], [596.0, 1518702000], [596.0, 1518702600], [596.0, 1518703200], [596.0, 1518703800], [596.0, 1518704400], [596.0, 1518705000], [596.0, 1518705600], [596.0, 1518706200], [596.0, 1518706800], [596.0, 1518707400], [596.0, 1518708000], [596.0, 1518708600], [596.0, 1518709200], [596.0, 1518709800], [596.0, 1518710400], [596.0, 1518711000], [596.0, 1518711600], [596.0, 1518712200], [596.0, 1518712800], [1664.0, 1518713400], [1664.0, 1518714000], [1664.0, 1518714600], [1664.0, 1518715200], [1664.0, 1518715800], [1664.0, 1518716400], [3275.0, 1518717000], [3275.0, 1518717600], [3275.0, 1518718200], [3275.0, 1518718800], [3275.0, 1518719400], [3275.0, 1518720000], [3275.0, 1518720600], [3275.0, 1518721200], [3275.0, 1518721800], [3275.0, 1518722400], [3275.0, 1518723000], [3275.0, 1518723600], [3275.0, 1518724200], [3275.0, 1518724800], [3275.0, 1518725400], [3275.0, 1518726000], [3275.0, 1518726600], [3275.0, 1518727200], [3275.0, 1518727800], [3275.0, 1518728400], [3275.0, 1518729000], [3275.0, 1518729600], [3275.0, 1518730200], [3275.0, 1518730800], [3275.0, 1518731400], [3275.0, 1518732000], [3275.0, 1518732600], [3275.0, 1518733200], [3275.0, 1518733800], [3275.0, 1518734400], [3275.0, 1518735000], [0.0, 1518735600], [0.0, 1518736200], [0.0, 1518736800], [0.0, 1518737400], [0.0, 1518738000], [0.0, 1518738600], [0.0, 1518739200], [0.0, 1518739800], [0.0, 1518740400], [0.0, 1518741000], ...\nbroken query:\nhttp://graphite.faccousa.net/render?format=json&from=20180215&until=20180301&target=template(timeStack(summarize($ctrl.food_distributed_today.0,'10minutes','max'),'24h',0,10))&template[ctrl]=portal.ovomec.santa_maria_di_sala.1308756.ovomec_smart_one\nresult of broken query:\n[]. The problem seems to be related with the timeStack function.\nthis works: \nhttp://graphite.il.faccousa.net/render?format=json&from=20180215&until=20180301&target=template($ctrl.food_distributed_today.0)&template[ctrl]=this.is.my.controller\nthis does not work:\nhttp://graphite.il.faccousa.net/render?format=json&from=20180215&until=20180301&target=template(timeStack($ctrl.food_distributed_today.0,'24h',0,10))&template[ctrl]=this.is.my.controller\n. Yes it works perfectly. I'm going to try that but that would kind of break the pattern I was trying to follow in my application. Is it known why it behaves that way? \nBy the way, I'm definitely committing myself to use Graphite for quite a bit of time in the future so I would love to eventually be more active in its development. . I wished that the template function worked by str.replacing of $ prefixed string with passed parameter in the url.\nWhat I have is a list of formulas with a $ctrl and an $index placeholder.\nSince my average user doesn't know how to write a query I want him to select a pre-built query and associated it to a controller and an index: this way I can have him select the curve I pre-built for him associated though to the resource he wants. . If there is no way that the template function could be improved to handle this particular case I believe we can close the issue.. I kept thinking about this and I'm thinking that the way I'm using this function is the most intuitive way, the rule is one: \"wrap your function with it, that's it\". \nI would really like to see it function the way I'm describing above unless, of course, there are side effects I'm not taking into consideration. \nI have not clue how the template function works, but wouldn't be kind of easy to just replace the $placeholders with the parameter placeholder? Isn't it just a matter of \"string replacing\" the placeholders with actual values passed in the url?. Fantastic I'll try my best to find the best way to do it. Any suggestions before I start? \nthe core of the template logic resides in render/evaluator.py, correct? . and also in the documentation roundFunction should be round. If I use the roundFunction i get the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/exception.py\", line 41, in inner\n    response = get_response(request)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 187, in _get_response\n    response = self.process_exception_by_middleware(e, request)\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 185, in _get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 117, in renderView\n    data.extend(evaluateTarget(requestContext, targets))\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 28, in evaluateTarget\n    result = evaluateTokens(requestContext, target)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 57, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression, replacements)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 84, in evaluateTokens\n    func = SeriesFunction(tokens.call.funcname)\n  File \"/opt/graphite/webapp/graphite/functions/__init__.py\", line 77, in SeriesFunction\n    raise KeyError('Function \"%s\" not found' % name)\nKeyError: u'Function \"roundFunction\" not found'\nI thought I was using the correct version of graphiteWeb considering that I have of the latest change done on this repository due one of my last month requests. . for some reason I cannot get the same result as a few days ago, I'm investigating the history of my past queries. As soon as I'll have them I'll post. Anyway, even though i was wrapping my original query (without the rounding) with round(...,2), the result set was returning as nothing was done. . I found out what the problem is, it seems like everything works as expected but when I add the maxDataPoints constrain. \nThe following works:\nhttp://blabla.net/render?target=round(averageSeries(ctrl.temperature_probe.{0,1,2}),2)&from=00:00_20180101&until=01:00_20180101&format=json\nresult: \n[{\"datapoints\": [[22.03, 1514786700], [22.53, 1514787000], [22.33, 1514787300], [22.27, 1514787600], [22.53, 1514787900], [22.03, 1514788200], [22.53, 1514788500], [22.23, 1514788800], [22.23, 1514789100], [22.4, 1514789400], [22.1, 1514789700], [22.57, 1514790000]], \"target\": \"averageSeries(ctrl.temperature_probe.{0,1,2})\", \"tags\": {\"aggregatedBy\": \"average\", \"name\": \"averageSeries(ctrl.temperature_probe.{0,1,2})\"}}]\nThis doesn't:\nhttp://blabla.net/render?target=round(averageSeries(ctrl.temperature_probe.{0,1,2}),2)&from=00:00_20180101&until=01:00_20180101&format=json&maxDataPoints=500\nresult:\n[{\"datapoints\": [[22.03333333333333, 1514786700], [22.53333333333333, 1514787000], [22.333333333333332, 1514787300], [22.266666666666666, 1514787600], [22.53333333333333, 1514787900], [22.033333333333335, 1514788200], [22.53333333333333, 1514788500], [22.233333333333334, 1514788800], [22.233333333333334, 1514789100], [22.400000000000002, 1514789400], [22.099999999999998, 1514789700], [22.566666666666663, 1514790000]], \"target\": \"averageSeries(ctrl.temperature_probe.{0,1,2})\", \"tags\": {\"aggregatedBy\": \"average\", \"name\": \"averageSeries(ctrl.temperature_probe.{0,1,2})\"}}]. How could I user consolidateBy to get the same effect of maxDataPoints so that I can get my maxDataPoints function execute first and then have the round function execute last?\n. would you be so nice to write a pseudo-query in order to avoid misunderstandings?\n. yeah I understood that, was just trying to understand what you meant. I can definitely check the code, all I'm asking is if there is a valid and known reason for this to be designed this way.. thanks, that's exactly what I was thinking of doing. I really appreciated. \nI'll post the solution once finished.. This is my custom function:\n```python\nfrom graphite.functions.params import Param, ParamTypes\nfrom graphite.util import epoch\nfrom graphite.render.attime import getUnitString, parseTimeOffset, parseATTime, SECONDS_STRING, MINUTES_STRING, HOURS_STRING, DAYS_STRING, WEEKS_STRING, MONTHS_STRING, YEARS_STRING\nfrom graphite.render.datalib import TimeSeries\nimport json\ndef generateFromJson(requestContext, startDateTime, jsonString, step=604800, name='generatedTimeSeries'):\n  \"\"\"\n  Short Alias: generateFromJson\n\"\"\"\n  values = json.loads(jsonString)\nstartTimeStamp = int(epoch(parseATTime(startDateTime)))\n  endTimeStamp = startTimeStamp + (len(values) * step)\nreturn [TimeSeries(name,\n            int(startTimeStamp),\n            int(endTimeStamp),\n            step, values, xFilesFactor=requestContext.get('xFilesFactor'))]\ngenerateFromJson.group = 'Custom'\ngenerateFromJson.params = [\n  Param('startDateTime', ParamTypes.date, required=True),\n  Param('jsonString', ParamTypes.string, required=True),\n  Param('step', ParamTypes.integer, required=False),\n  Param('name', ParamTypes.string, required=False),\n]\nSeriesFunctions = {\n  'generateFromJson': generateFromJson,\n}\n```\nclosing . ",
    "Farfaday": "Hi,\nOkay, thanks for your reply!\nHaving the possibility to escape it would be great, as this char was allowed in the metric names before.\nMany Thanks!. ",
    "rwyse": "@deniszh, yes it is based off of that example.  I am including it attached now.\ngraphite-vhost.txt\n. @deniszh it is exactly that.  As I said, I tried to leave things as close to what is documented in the wiki to ensure that this would function as expected.  I am attaching the file though.\ngraphite-wsgi.txt\nFor what it is worth, I can run a python shell and run the commands just fine as well.  Also, I can run the syncdb commands with Django-admin as well.  Let me know if there is any other information or files I can provide and thanks for looking into this.. @deniszh I am installing graphite by grabbing the tarball, extracting it into a temp location, running a pip install on requirements.txt and then once those are done running a python setup.py install.  It is being done with chef and all I did was grab the new tar balls from the git repo and then using them in the cookbooks that have been running and working without issue for a long time now.  The output requested is below:\nls -al /opt/graphite/webapp/graphite/render/\ntotal 524\ndrwxr-xr-x  2 apache apache   4096 Feb 16 16:23 .\ndrwxr-xr-x 18 apache apache   4096 Feb 16 16:06 ..\n-rw-r--r--  1 apache apache   5530 Jan  9 07:34 attime.py\n-rw-r--r--  1 apache apache   5967 Feb  5 23:30 attime.pyc\n-rw-------  1 root   root     9936 Feb 13 16:39 datalib.py\n-rw-------  1 root   root     8620 Feb 16 15:01 datalib.pyc\n-rw-r--r--  1 apache apache   5893 Jan  9 07:34 evaluator.py\n-rw-r--r--  1 apache apache   4615 Feb  5 23:30 evaluator.pyc\n-rw-r--r--  1 apache apache 125506 Feb 13 17:00 functions.py\n-rw-r--r--  1 apache apache 124346 Feb 13 17:01 functions.pyc\n-rw-r--r--  1 apache apache  72586 Jan  9 07:34 glyph.py\n-rw-r--r--  1 apache apache  59462 Feb  5 23:30 glyph.pyc\n-rw-r--r--  1 apache apache   3547 Jan  9 07:34 grammar.py\n-rw-r--r--  1 apache apache   3753 Feb  5 23:30 grammar.pyc\n-rw-r--r--  1 apache apache   4722 Jan  9 07:34 hashing.py\n-rw-r--r--  1 apache apache   6073 Feb  5 23:30 hashing.pyc\n-rw-r--r--  1 apache apache      0 Jan  9 07:34 init.py\n-rw-r--r--  1 apache apache    135 Feb  5 23:30 init.pyc\n-rw-r--r--  1 apache apache    871 Jan  9 07:34 urls.py\n-rw-r--r--  1 apache apache   1101 Feb  5 23:30 urls.pyc\n-rw-r--r--  1 apache apache  21430 Jan  9 07:34 views.py\n-rw-r--r--  1 apache apache  18569 Feb  5 23:30 views.pyc\nThanks again and looking forward to trying to figure out what the heck is going on with this.\nRyan. Alright so after seeing that I chown'd the web app dir correctly and that got the data lib error resolved although I am not entirely certain how the permissions on those files would have changed anyhow considering chef sets them to owned by apache and I have done it many times.  Regardless here is the new output:\nls -al /opt/graphite/webapp/graphite/render/\ntotal 524\ndrwxr-xr-x  2 apache apache   4096 Feb 16 16:23 .\ndrwxr-xr-x 18 apache apache   4096 Feb 16 16:06 ..\n-rw-r--r--  1 apache apache   5530 Jan  9 07:34 attime.py\n-rw-r--r--  1 apache apache   5967 Feb  5 23:30 attime.pyc\n-rw-------  1 apache apache   9936 Feb 13 16:39 datalib.py\n-rw-------  1 apache apache   8620 Feb 16 15:01 datalib.pyc\n-rw-r--r--  1 apache apache   5893 Jan  9 07:34 evaluator.py\n-rw-r--r--  1 apache apache   4615 Feb  5 23:30 evaluator.pyc\n-rw-r--r--  1 apache apache 125506 Feb 13 17:00 functions.py\n-rw-r--r--  1 apache apache 124346 Feb 13 17:01 functions.pyc\n-rw-r--r--  1 apache apache  72586 Jan  9 07:34 glyph.py\n-rw-r--r--  1 apache apache  59462 Feb  5 23:30 glyph.pyc\n-rw-r--r--  1 apache apache   3547 Jan  9 07:34 grammar.py\n-rw-r--r--  1 apache apache   3753 Feb  5 23:30 grammar.pyc\n-rw-r--r--  1 apache apache   4722 Jan  9 07:34 hashing.py\n-rw-r--r--  1 apache apache   6073 Feb  5 23:30 hashing.pyc\n-rw-r--r--  1 apache apache      0 Jan  9 07:34 init.py\n-rw-r--r--  1 apache apache    135 Feb  5 23:30 init.pyc\n-rw-r--r--  1 apache apache    871 Jan  9 07:34 urls.py\n-rw-r--r--  1 apache apache   1101 Feb  5 23:30 urls.pyc\n-rw-r--r--  1 apache apache  21430 Jan  9 07:34 views.py\n-rw-r--r--  1 apache apache  18569 Feb  5 23:30 views.pyc\nWeb interface is working properly however I am still not able to expand metrics and see any thing in the tree.  Thanks.\nRyan. @deniszh \nSo the apache logs are looking a little better and the previous error is gone; however in the carbon cache logs I am getting the following repeatedly:\n18/02/2018 17:39:33 :: Unhandled error in Deferred:\n18/02/2018 17:39:33 :: Unhandled Error\nTraceback (most recent call last):\nFailure: twisted.internet.error.TimeoutError: User timeout caused connection failure.\n18/02/2018 17:45:33 :: Unhandled error in Deferred:\n18/02/2018 17:45:33 :: Unhandled Error\nTraceback (most recent call last):\nFailure: twisted.internet.error.TimeoutError: User timeout caused connection failure.\nAlong with that I am seeing that the relays have all stopped running at this point as well.  The only thing that I am seeing in the logs for the relays is the following (I am leaving out a lot due to letting you assume it is the same for any connection from relay to cache):\n17/02/2018 11:42:25 :: CarbonClientFactory(127.0.0.1:2214:k)::startedConnecting (127.0.0.1:2214)\n17/02/2018 11:42:25 :: CarbonClientFactory(127.0.0.1:2504:e)::startedConnecting (127.0.0.1:2504)\n17/02/2018 11:42:27 :: CarbonClientFactory(127.0.0.1:2204:b)::startedConnecting (127.0.0.1:2204)\n17/02/2018 11:42:27 :: CarbonClientFactory(127.0.0.1:2804:h)::startedConnecting (127.0.0.1:2804)\n17/02/2018 11:42:29 :: CarbonClientFactory(127.0.0.1:2214:k)::clientConnectionFailed (127.0.0.1:2214) Connection was refused by other side: 111: Connection refused.\n17/02/2018 11:42:29 :: Destination is down: 127.0.0.1:2214:k (103/5)\n17/02/2018 11:42:29 :: CarbonClientFactory(127.0.0.1:2504:e)::clientConnectionFailed (127.0.0.1:2504) Connection was refused by other side: 111: Connection refused.\n17/02/2018 11:42:29 :: Destination is down: 127.0.0.1:2504:e (188/5)\n17/02/2018 11:42:29 :: CarbonClientFactory(127.0.0.1:2204:b)::clientConnectionFailed (127.0.0.1:2204) Connection was refused by other side: 111: Connection refused.\n17/02/2018 11:42:29 :: Destination is down: 127.0.0.1:2204:b (282/5)\n17/02/2018 11:42:29 :: CarbonClientFactory(127.0.0.1:2804:h)::clientConnectionFailed (127.0.0.1:2804) Connection was refused by other side: 111: Connection refused.\n17/02/2018 11:42:29 :: Destination is down: 127.0.0.1:2804:h (680/5)\n17/02/2018 11:42:34 :: CarbonClientFactory(127.0.0.1:2214:k)::startedConnecting (127.0.0.1:2214)\nHowever if I stop carbon and start it up, I get connections establishing just fine....:\n16/02/2018 16:53:36 :: CarbonClientProtocol(127.0.0.1:2224:l)::connectionMade\n16/02/2018 16:53:36 :: CarbonClientFactory(127.0.0.1:2224:l)::connectionMade (CarbonClientProtocol(127.0.0.1:2224:l))\n16/02/2018 16:53:36 :: Destination is up: 127.0.0.1:2224:l\n16/02/2018 16:53:36 :: CarbonClientProtocol(127.0.0.1:2404:d)::connectionMade\n16/02/2018 16:53:36 :: CarbonClientFactory(127.0.0.1:2404:d)::connectionMade (CarbonClientProtocol(127.0.0.1:2404:d))\n16/02/2018 16:53:36 :: Destination is up: 127.0.0.1:2404:d\nThe only thing that is throwing errors in the apache logs is in the webapp/exception.log I am seeing the following, but I suspect it is unrelated to the issue:\n2018-02-18,17:57:24.018 :: Exception encountered in \nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/django/core/handlers/base.py\", line 147, in get_response\n    response = wrapped_callback(request, callback_args, callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 117, in renderView\n    data.extend(evaluateTarget(requestContext, targets))\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 15, in evaluateTarget\n    prefetchData(requestContext, pathExpressions)\n  File \"/opt/graphite/webapp/graphite/render/datalib.py\", line 253, in prefetchData\n    for result in STORE.fetch(pathExpressions, startTime, endTime, now, requestContext):\n  File \"/opt/graphite/webapp/graphite/storage.py\", line 179, in fetch\n    'fetch for %s' % str(patterns))\n  File \"/opt/graphite/webapp/graphite/storage.py\", line 130, in wait_jobs\n    '\\n'.join(traceback.format_exception(job.exception_info))\nAttributeError: 'Job' object has no attribute 'exception_info'\nThis is also mirrored in the apache error log itself, but the web UI is up and I am able to interact with it.  I am not sure how those permissions would have changed considering that at the very least chef should ensure that the owner and mode are set correctly as it has done for the past few years we have run this....Regardless, I still cannot expand the metrics tree in the browser and see any data....\nThanks again for the assist and let me know what the next steps are...If it helps, I have rebuilt from scratch and reproduce the issue every time so I suspect either there is a config that is incorrect that I am not understanding properly or something entirely different that I am missing altogether.\n-Ryan. @deniszh the permissions look to be set properly; however I will double check on those tomorrow when I am back to my workstation.  As for the installation, I have reinstalled this many times now with the exact same results.  I suppose it is possible that I grabbed tar balls that are problematic and I need to grab new ones to attempt the installation, but as it stands, I get this same result any time that I install the application on any new instance or current.  Repeatable every time.  If its an issue with the source that I am using then do you have known good or stable tar balls that you could point me to so that I can attempt it with them?\nAgain, I really appreciate the help on it.  Redoing things from scratch yields the same results I currently have so I am thinking at this point it has to be something code related.....\nThanks and let me know what you think or what you think I should use,\n-Ryan. Yes, here is the output requested:\nls -al /opt/graphite/webapp/graphite\ntotal 348\ndrwxr-xr-x 18 apache apache  4096 Feb 16 16:06 .\ndrwxr-xr-x  4 apache apache    78 Feb  5 23:30 ..\ndrwxr-xr-x  3 apache apache   240 Feb  5 23:30 account\n-rw-r--r--  1 apache apache  3336 Jan  9 07:34 app_settings.py\n-rw-r--r--  1 apache apache  3764 Feb  5 23:30 app_settings.pyc\ndrwxr-xr-x  2 apache apache   109 Feb  5 23:30 browser\n-rw-r--r--  1 apache apache  7059 Jan  9 07:34 carbonlink.py\n-rw-r--r--  1 apache apache  8880 Feb  5 23:30 carbonlink.pyc\n-rw-r--r--  1 apache apache   539 Jan  9 07:34 compat.py\n-rw-r--r--  1 apache apache  1156 Feb  5 23:30 compat.pyc\ndrwxr-xr-x  2 apache apache   109 Feb  5 23:30 composer\ndrwxr-xr-x  3 apache apache   238 Feb  5 23:30 dashboard\n-rw-r--r--  1 apache apache    94 Jan  9 07:34 errors.py\n-rw-r--r--  1 apache apache   340 Feb  5 23:30 errors.pyc\ndrwxr-xr-x  3 apache apache   230 Feb  5 23:30 events\ndrwxr-xr-x  2 apache apache   185 Feb  5 23:30 finders\ndrwxr-xr-x  3 apache apache   158 Feb  5 23:30 functions\n-rw-r--r--  1 apache apache    95 Jan  9 07:34 http_pool.py\n-rw-r--r--  1 apache apache   290 Feb  5 23:30 http_pool.pyc\n-rw-r--r--  1 apache apache    53 Jan  9 07:34 __init__.py\n-rw-r--r--  1 apache apache   128 Feb  5 23:30 __init__.pyc\n-rw-r--r--  1 apache apache  4223 Jan  9 07:34 intervals.py\n-rw-r--r--  1 apache apache  7251 Feb  5 23:30 intervals.pyc\n-rw-r--r--  1 apache apache 15351 Feb 16 16:06 local_settings.py\n-rw-r--r--  1 apache apache  2131 Feb 16 16:06 local_settings.pyc\n-rw-r--r--  1 apache apache 15574 Jan  9 07:34 local_settings.py.example\n-rw-r--r--  1 apache apache  3644 Jan  9 07:34 logger.py\n-rw-r--r--  1 apache apache  4438 Feb  5 23:30 logger.pyc\ndrwxr-xr-x  2 apache apache   109 Feb  5 23:30 metrics\n-rw-r--r--  1 apache apache   396 Jan  9 07:34 middleware.py\n-rw-r--r--  1 apache apache   854 Feb  5 23:30 middleware.pyc\n-rw-r--r--  1 apache apache   971 Jan  9 07:34 node.py\n-rw-r--r--  1 apache apache  2179 Feb  5 23:30 node.pyc\ndrwxr-xr-x  2 apache apache   245 Feb  5 23:30 readers\ndrwxr-xr-x  2 apache apache  4096 Feb 16 16:23 render\n-rw-r--r--  1 apache apache 10186 Feb  6 11:47 settings.py\n-rw-------  1 apache apache  7892 Feb  6 11:51 settings.pyc\n-rw-r--r--  1 apache apache  2703 Jan  9 07:34 singleton.py\n-rw-r--r--  1 apache apache  4089 Feb  5 23:30 singleton.pyc\n-rw-r--r--  1 apache apache 18295 Feb 13 09:44 storage.py\n-rw-r--r--  1 apache apache 14269 Feb 13 09:45 storage.pyc\ndrwxr-xr-x  3 apache apache  4096 Feb  5 23:30 tags\ndrwxr-xr-x  2 apache apache   236 Feb  5 23:30 templates\n-rw-r--r--  1 apache apache 35513 Jan  9 07:34 umsgpack.py\n-rw-r--r--  1 apache apache 33021 Feb  5 23:30 umsgpack.pyc\ndrwxr-xr-x  3 apache apache   170 Feb  5 23:30 url_shortener\n-rw-r--r--  1 apache apache  1826 Jan  9 07:34 urls.py\n-rw-r--r--  1 apache apache  2137 Feb  5 23:30 urls.pyc\n-rw-r--r--  1 apache apache  1653 Jan  9 07:34 user_util.py\n-rw-r--r--  1 apache apache  2034 Feb  5 23:30 user_util.pyc\n-rw-r--r--  1 apache apache 10075 Jan  9 07:34 util.py\n-rw-r--r--  1 apache apache 13839 Feb  5 23:30 util.pyc\ndrwxr-xr-x  2 apache apache   109 Feb  5 23:30 version\n-rw-r--r--  1 apache apache   314 Jan  9 07:34 views.py\n-rw-r--r--  1 apache apache   623 Feb  5 23:30 views.pyc\ndrwxr-xr-x  2 apache apache   109 Feb  5 23:30 whitelist\ndrwxr-xr-x  2 apache apache    76 Feb  5 23:30 worker_pool\n-rw-r--r--  1 apache apache  1241 Jan  9 07:34 wsgi.py\n-rw-r--r--  1 apache apache  1410 Feb  5 23:30 wsgi.pyc\nls -al /opt/graphite/webapp/graphite/worker_pool\ntotal 16\ndrwxr-xr-x  2 apache apache   76 Feb  5 23:30 .\ndrwxr-xr-x 18 apache apache 4096 Feb 16 16:06 ..\n-rw-r--r--  1 apache apache    0 Jan  9 07:34 __init__.py\n-rw-r--r--  1 apache apache  140 Feb  5 23:30 __init__.pyc\n-rw-r--r--  1 apache apache 2686 Jan  9 07:34 pool.py\n-rw-r--r--  1 apache apache 4078 Feb  5 23:30 pool.pyc\nThanks.  I am currently testing what I think to be a more stable download that I grabbed out of the git repo upstream as well to see if that resolves the issue, but the install manual that I have been using is the one on the graphite read the docs wiki:\nhttp://graphite.readthedocs.io/en/stable/releases/1_1_2.html\nRyan. Yes, here is the output requested:\nls -al /opt/graphite/webapp/graphite\ntotal 348\ndrwxr-xr-x 18 apache apache  4096 Feb 16 16:06 .\ndrwxr-xr-x  4 apache apache    78 Feb  5 23:30 ..\ndrwxr-xr-x  3 apache apache   240 Feb  5 23:30 account\n-rw-r--r--  1 apache apache  3336 Jan  9 07:34 app_settings.py\n-rw-r--r--  1 apache apache  3764 Feb  5 23:30 app_settings.pyc\ndrwxr-xr-x  2 apache apache   109 Feb  5 23:30 browser\n-rw-r--r--  1 apache apache  7059 Jan  9 07:34 carbonlink.py\n-rw-r--r--  1 apache apache  8880 Feb  5 23:30 carbonlink.pyc\n-rw-r--r--  1 apache apache   539 Jan  9 07:34 compat.py\n-rw-r--r--  1 apache apache  1156 Feb  5 23:30 compat.pyc\ndrwxr-xr-x  2 apache apache   109 Feb  5 23:30 composer\ndrwxr-xr-x  3 apache apache   238 Feb  5 23:30 dashboard\n-rw-r--r--  1 apache apache    94 Jan  9 07:34 errors.py\n-rw-r--r--  1 apache apache   340 Feb  5 23:30 errors.pyc\ndrwxr-xr-x  3 apache apache   230 Feb  5 23:30 events\ndrwxr-xr-x  2 apache apache   185 Feb  5 23:30 finders\ndrwxr-xr-x  3 apache apache   158 Feb  5 23:30 functions\n-rw-r--r--  1 apache apache    95 Jan  9 07:34 http_pool.py\n-rw-r--r--  1 apache apache   290 Feb  5 23:30 http_pool.pyc\n-rw-r--r--  1 apache apache    53 Jan  9 07:34 __init__.py\n-rw-r--r--  1 apache apache   128 Feb  5 23:30 __init__.pyc\n-rw-r--r--  1 apache apache  4223 Jan  9 07:34 intervals.py\n-rw-r--r--  1 apache apache  7251 Feb  5 23:30 intervals.pyc\n-rw-r--r--  1 apache apache 15351 Feb 16 16:06 local_settings.py\n-rw-r--r--  1 apache apache  2131 Feb 16 16:06 local_settings.pyc\n-rw-r--r--  1 apache apache 15574 Jan  9 07:34 local_settings.py.example\n-rw-r--r--  1 apache apache  3644 Jan  9 07:34 logger.py\n-rw-r--r--  1 apache apache  4438 Feb  5 23:30 logger.pyc\ndrwxr-xr-x  2 apache apache   109 Feb  5 23:30 metrics\n-rw-r--r--  1 apache apache   396 Jan  9 07:34 middleware.py\n-rw-r--r--  1 apache apache   854 Feb  5 23:30 middleware.pyc\n-rw-r--r--  1 apache apache   971 Jan  9 07:34 node.py\n-rw-r--r--  1 apache apache  2179 Feb  5 23:30 node.pyc\ndrwxr-xr-x  2 apache apache   245 Feb  5 23:30 readers\ndrwxr-xr-x  2 apache apache  4096 Feb 16 16:23 render\n-rw-r--r--  1 apache apache 10186 Feb  6 11:47 settings.py\n-rw-------  1 apache apache  7892 Feb  6 11:51 settings.pyc\n-rw-r--r--  1 apache apache  2703 Jan  9 07:34 singleton.py\n-rw-r--r--  1 apache apache  4089 Feb  5 23:30 singleton.pyc\n-rw-r--r--  1 apache apache 18295 Feb 13 09:44 storage.py\n-rw-r--r--  1 apache apache 14269 Feb 13 09:45 storage.pyc\ndrwxr-xr-x  3 apache apache  4096 Feb  5 23:30 tags\ndrwxr-xr-x  2 apache apache   236 Feb  5 23:30 templates\n-rw-r--r--  1 apache apache 35513 Jan  9 07:34 umsgpack.py\n-rw-r--r--  1 apache apache 33021 Feb  5 23:30 umsgpack.pyc\ndrwxr-xr-x  3 apache apache   170 Feb  5 23:30 url_shortener\n-rw-r--r--  1 apache apache  1826 Jan  9 07:34 urls.py\n-rw-r--r--  1 apache apache  2137 Feb  5 23:30 urls.pyc\n-rw-r--r--  1 apache apache  1653 Jan  9 07:34 user_util.py\n-rw-r--r--  1 apache apache  2034 Feb  5 23:30 user_util.pyc\n-rw-r--r--  1 apache apache 10075 Jan  9 07:34 util.py\n-rw-r--r--  1 apache apache 13839 Feb  5 23:30 util.pyc\ndrwxr-xr-x  2 apache apache   109 Feb  5 23:30 version\n-rw-r--r--  1 apache apache   314 Jan  9 07:34 views.py\n-rw-r--r--  1 apache apache   623 Feb  5 23:30 views.pyc\ndrwxr-xr-x  2 apache apache   109 Feb  5 23:30 whitelist\ndrwxr-xr-x  2 apache apache    76 Feb  5 23:30 worker_pool\n-rw-r--r--  1 apache apache  1241 Jan  9 07:34 wsgi.py\n-rw-r--r--  1 apache apache  1410 Feb  5 23:30 wsgi.pyc\nls -al /opt/graphite/webapp/graphite/worker_pool\ntotal 16\ndrwxr-xr-x  2 apache apache   76 Feb  5 23:30 .\ndrwxr-xr-x 18 apache apache 4096 Feb 16 16:06 ..\n-rw-r--r--  1 apache apache    0 Jan  9 07:34 __init__.py\n-rw-r--r--  1 apache apache  140 Feb  5 23:30 __init__.pyc\n-rw-r--r--  1 apache apache 2686 Jan  9 07:34 pool.py\n-rw-r--r--  1 apache apache 4078 Feb  5 23:30 pool.pyc\nThanks.  I am currently testing what I think to be a more stable download that I grabbed out of the git repo upstream as well to see if that resolves the issue, but the install manual that I have been using is the one on the graphite read the docs wiki:\nhttp://graphite.readthedocs.io/en/stable/releases/1_1_2.html\nRyan. @deniszh \n  I downloaded new tar files and loaded them into what I was doing yesterday.  That seems to have corrected the issue.  I am not sure why the original files I downloaded were problematic, but the new ones I have running in a test environment now and am looking to put it into stage shortly.  If there is anything else that you can tell me on this I would be curious else I would think I can close this issue since I have it resolved with the new installation files.\nThanks,\n-Ryan. ",
    "hanks": "@DanCech Thank you for your advice, I also noticed that The percentage of values that are 1 in a series of 1 and 0 will be the average value of the series, so I use averageSeries(isNonNull(local.random.diceroll) to solve this problem. ",
    "jdblack": "I am also seeing the following errors on rds:\n2018-02-28T23:55:05.182387Z 12272 [Note] Aborted connection 12272 to db: 'graphite_dev' user: 'graphite' host: '10.10.1.76' (Got an error reading communication packets)\n2018-02-28T23:55:09.185732Z 12282 [Note] Aborted connection 12282 to db: 'graphite_dev' user: 'graphite' host: '10.10.2.86' (Got an error reading communication packets)\n2018-02-28T23:55:13.017837Z 12284 [Note] Aborted connection 12284 to db: 'graphite_dev' user: 'graphite' host: '10.10.1.76' (Got an error reading communication packets)\n2018-02-28T23:55:17.964528Z 12300 [Note] Aborted connection 12300 to db: 'graphite_dev' user: 'graphite' host: '10.10.2.86' (Got an error reading communication packets)\n2018-02-28T23:55:21.054052Z 12302 [Note] Aborted connection 12302 to db: 'graphite_dev' user: 'graphite' host: '10.10.2.90' (Got an error reading communication packets)\n2018-02-28T23:55:28.138228Z 12301 [Note] Aborted connection 12301 to db: 'graphite_dev' user: 'graphite' host: '10.10.1.76' (Got an error reading communication packets)\n2018-02-28T23:55:34.630480Z 12318 [Note] Aborted connection 12318 to db: 'graphite_dev' user: 'graphite' host: '10.10.2.90' (Got an error reading communication packets)\n2018-02-28T23:55:34.715368Z 12316 [Note] Aborted connection 12316 to db: 'graphite_dev' user: 'graphite' host: '10.10.2.86' (Got an error reading communication packets)\n2018-02-28T23:55:41.230670Z 12317 [Note] Aborted connection 12317 to db: 'graphite_dev' user: 'graphite' host: '10.10.1.76' (Got an error reading communication packets)\n. I do not think django was upgraded at the time we upgraded graphite 1.1.2,  though we may have upgraded during the process to figure out what's wrong..  I tried upgrading django from 1.9.13 to 1.11.0 with no success.\nHowever, I did find that graphite-web seems to work if I firewall off posts to  /tags/tagMultiSeries:\niptables -A OUTPUT -p tcp -m string --string \"POST /tags/tagMultiSeries\" --algo kmp -j REJECT\ngraphite-web and grafana seem to work reliably with this firewall rule to prevent posts to tag,\n. We really, really want to use  tags, but haven't quite put it into place yet.   Switching to using TAGDB=None   has worked for us! (Thanks! configuration via firewall makes my skin crawl).\nI'll double check it started working under uswgi too (though I'm sure it will as with TAGDB=None, it works great in gunicorn).\nWe are in a clustered mode. I'll pull in the configs from work on monday\n. This is going to sound a little crazy, but we have 4 separate graphite-dev clusters in dev, becuase we're emulating a much larger buildout in prod. We have 3 layers in our graphite-setup;   a relay layer, a cache layer, and a  presentation layer. \n\n\nThe relay layer is just  carbon-c-relay. \n\n\nThe presentation layer contains grafana and graphite-web\n\ngrafana is configured to talk to graphite-web on localhost\nthe graphite-web is configured to cluster with all of the other  graphite-webs in our stack and local_settings.py includes the CLUSTER_SERVERS directive that includes all of the graphite-webs from our cache layer\n   CLUSTER_SERVERS = [\"x.x.x.x:8080\", \"x.x.x.x:8080\", \"x.x.x.x:8080\", \"x.x.x.x:8080\"]\n\n\n\nThe  cache layer  contains carbon-c-relay,  carbon,  and graphite-web. \n\nWe have 4 carbon_ch clusters in dev, which I will call 1a-old,  1c-old,   1a-new and 1c-new.\nEach cluster in dev has a single system in it (again, emulating a much larger stack in prod)\n1a-old and 1c-old are meant to be independent redundant clusters\n1a-new and 1c-new are meant to be eventual replacements for 1a-old and 1c-old \ngraphite-web on these systems do not have a CLUSTER_SERVERS directive\n\n\ngraphite-web on these systems does have a \n          CARBONLINK_HOSTS = [\"127.0.0.1:7012:1\", \"127.0.0.1:7022:2\", \"127.0.0.1:7032:3\"]\n\n. @deniszh\nIn our setting,   we have a  graphite-web on two layers;  a   UI layer (for grafana and graphite-web rendering requests) and a cache layer.  On the  UI layer,  graphite-web  has a CLUSTER_SERVERS that includes all of the  graphite-webs on the cache layer.   The  graphite-webs on the cache layer  do not themselves have a  CLUSTERS_SERVERS  configuration\nRight now graphite-web on both layers have TAGDB=None.   My understanding is that I should remove that from both layers. Then,  on the UI layer,  I should update  graphite-web on  CLUSTER_SERVERS config to add ?noTags=1. Is this correct?\n. We've set up graphite-web to use the same mysql db across all of the same hosts, so I suppose it makes the most sense for us, then, to tell the UI layer to  ?noTags=1  since it can look up  for itself.. ",
    "lrhazi": "Thank you so much!. ",
    "JelleZijlstra": "The codacy failure is due to an issue that was already present in the previous code. I'm happy to fix it of a maintainer requests it, but wanted to limit the scope of this PR.. Done, thanks for the quick review.. It does, because previously safeMax() might return None, and None > 0 is a TypeError in Python 3.. ",
    "scrwr": "Its not removing it:\n```\ncurl 'https://grafana/api/datasources/proxy/1/render' \n-H 'Cookie: grafana_user=...; grafana_sess=...' \n--data 'target=...&from=-5min&until=-1min&format=json&maxDataPoints=1&noNullPoints=True'\n[{\"target\": \"e1\", \"datapoints\": [[null, 1520508480]]}, {\"target\": \"e2\", \"datapoints\": [[null, 1520508480]]}, {\"target\": \"e3\", \"datapoints\": [[null, 1520508480]]},...\n```\nbut:\n```\ncurl 'https://grafana/api/datasources/proxy/1/render' \n-H 'Cookie: grafana_user=...; grafana_sess=...' \n--data 'target=...&from=-5min&until=-1min&format=json&noNullPoints=True' \n[{\"target\": \"e456\", \"datapoints\": [[1.0, 1520508720]]}, {\"target\": \"e765\", \"datapoints\": [[1.0, 1520508600]]}, {\"target\": \"e987\", \"datapoints\": [[1.0, 1520508600]]}, {\"target\": \"e1082\", \"datapoints\": [[1.0, 1520508660]]}]\n```. ",
    "bitfur": "Great!. ",
    "ssinhaRepo": "Hi DanCech,\n    First thanks for your comment. I can't hardcode the nodes (placements here), I will be using this target function for an alert so wouldn't know in advance which placement counts will meet the filter criteria. Also, new placements can be added in future, so this has to be dynamic.\nThanks.. Is there anyway to do it using mapSeries()/reduceSeries() and filterSeries()?. @DanCech, as I mentioned above we can't hardcode the nodes since the new nodes can be added in future. However, I do know the list of nodes/placements which I want to exclude, is there any blacklisting functionality in graphite like !{node1, node2...}. ",
    "Tushar1983": "@deniszh Thanks! That resolved the issue for me.. ",
    "KenpachiRules": "It is incredibly hard and pain staking to get graphite-web , I have been banging my head still it does not work , not at all intuitive , other than \"GET / HTTP/1.1\" 404 74 in my access.log file there is no information . Attaching contents of local-settings.py and graphite-vhost.conf .\nlocal-settings.py:\nSECRET_KEY = '#123Bankai'\nALLOWED_HOSTS = [ '*' ]\nDEFAULT_XFILES_FACTOR = 0\nURL_PREFIX = '/graphite'\nFUNCTION_PLUGINS = []\ngraphite-vhost.conf \n\n    LoadModule wsgi_module modules/mod_wsgi.so\n\nWSGISocketPrefix run/wsgi\n\n        ServerName graphite\n        DocumentRoot \"/opt/graphite/webapp\"\n        ErrorLog /opt/graphite/storage/log/webapp/error.log\n        CustomLog /opt/graphite/storage/log/webapp/access.log common\n    WSGIDaemonProcess graphite processes=5 threads=5 display-name='%{GROUP}' inactivity-timeout=120\n    WSGIProcessGroup graphite\n    WSGIApplicationGroup %{GLOBAL}\n    WSGIImportScript /opt/graphite/conf/graphite.wsgi process-group=graphite application-group=%{GLOBAL}\n\n    WSGIScriptAlias / /opt/graphite/conf/graphite.wsgi\n\n\n    Alias /static/ /opt/graphite/static/\n\n    Alias /media/ \"@DJANGO_ROOT@/contrib/admin/media/\"\n\n    <Directory /opt/graphite/conf/>\n               Require all granted\n    </Directory>\n\n\nIt would be great if you could explain a greater detail on what all the settings I do need to take care if I have missed out something\n. ",
    "rgeniesse": "Thank you! I attempted the command, but it didn't work. Ensured permissions were proper on /opt/graphite/static for apache to read it. Suspect a config option somewhere isn't set properly to tell Apache to look there.\nI ran pip install whitenoise based on a comment in example-graphite-vhost.conf and now the webpage loads as expected. Appreciate it!. Yes, PYTHONPATH=/opt/graphite/webapp django-admin.py collectstatic --noinput --settings=graphite.settings works just the same as the one you provided. Creates and populates a folder called static in /opt/graphite.. ",
    "MatthewFin": "Is this setting at all configurable? It seems to be the case that existing metric queries includes the until param but excludes the from param. \nI think that both ends of the from/until query should be inclusive, or at least configurable.. ",
    "ivnilv": "The thing is, if I empty the graphite index file, and restart the app, it works for a week or so... until the graphite index file becomes too big I guess, and times out while being queried from the web app. Does that make sense ? If yes, is there something I can do about the index growing too much ?. I'll try that, thanks !\nAny other suggestions what might be causing the metrics browser to hang when expanding some folders down the metrics tree ?\nBtw, also when I refresh the page and try a couple of times more, I might get the folder expanded and could dig further in the structure.\nHere are some other logs I captured while the app was failing with the same symptoms:\n```\nApril 12th 2018, 15:52:22.539 | 172.16.97.0 - - [12/Apr/2018:14:52:22 +0000] \"GET /metrics/find/?_dc=1523544622520&query=*&format=treejson&contexts=1&path=&node=GraphiteTree HTTP/1.1\" 499 0 \"https://graphite-test.colo.elex.be/composer/?\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\"\n-- | --\n| April 12th 2018, 15:52:15.934 | 172.16.99.0 - guest [12/Apr/2018:14:52:15 +0000] \"POST /events/ HTTP/1.1\" 200 5 \"-\" \"Apache CXF 3.0.0-milestone1\"\n| April 12th 2018, 15:52:05.635 | -p destination: Stale file handle\n| April 12th 2018, 15:51:53.463 | [Thu Apr 12 14:51:53 UTC 2018]  building index...\n| April 12th 2018, 15:51:52.361 | 172.16.15.0 - - [12/Apr/2018:14:51:52 +0000] \"GET /metrics/find/?_dc=1523544712318&query=k8s.*&format=treejson&contexts=1&path=k8s&node=k8s HTTP/1.1\" 200 110 \"https://graphite-test.colo.elex.be/composer/?\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\"\n| April 12th 2018, 15:51:48.923 | 172.16.52.0 - - [12/Apr/2018:14:51:48 +0000] \"GET /metrics/find/?_dc=1523544708731&query=*&format=treejson&contexts=1&path=&node=GraphiteTree HTTP/1.1\" 200 210 \"https://graphite-test.colo.elex.be/composer/?\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\"\n| April 12th 2018, 15:51:16.055 | 172.16.8.0 - guest [12/Apr/2018:14:51:16 +0000] \"POST /events/ HTTP/1.1\" 200 5 \"-\" \"Apache CXF 3.0.0-milestone1\"\n| April 12th 2018, 15:51:04.644 | File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 134, in maybeDeferred\n| April 12th 2018, 15:51:04.642 | ---  ---\n| April 12th 2018, 15:51:04.631 | 12/04/2018 14:51:04 :: [console] Unhandled error in Deferred:\n| April 12th 2018, 15:50:58.313 | -\n| April 12th 2018, 15:50:58.312 | record('allocatedBuffers', len(carbon.aggregator.buffers.BufferManager))\n| April 12th 2018, 15:50:58.312 | exceptions.NameError: global name 'carbon' is not defined\n| April 12th 2018, 15:50:58.311 | File \"/opt/graphite/lib/carbon/instrumentation.py\", line 119, in recordMetrics\n| April 12th 2018, 15:50:58.309 | result = f(args, *kw)\n| April 12th 2018, 15:50:58.306 | File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 134, in maybeDeferred\n| April 12th 2018, 15:50:58.305 | ---  ---\n| April 12th 2018, 15:50:58.305 | d = defer.maybeDeferred(self.f, self.a, *self.kw)\n| April 12th 2018, 15:50:58.302 | File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/task.py\", line 208, in call\n| April 12th 2018, 15:50:58.301 | call.func(call.args, *call.kw)\n| April 12th 2018, 15:50:58.299 | File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py\", line 800, in runUntilCurrent\n| April 12th 2018, 15:50:58.298 | self.runUntilCurrent()\n| April 12th 2018, 15:50:58.296 | File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py\", line 1178, in mainLoop\n| April 12th 2018, 15:50:58.295 | self.mainLoop()\n| April 12th 2018, 15:50:58.293 | File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py\", line 1169, in run\n| April 12th 2018, 15:50:58.292 | Traceback (most recent call last):\n| April 12th 2018, 15:50:58.290 | 12/04/2018 14:50:58 :: [console] Unhandled Error\n| April 12th 2018, 15:50:58.282 | 12/04/2018 14:50:58 :: [console] Unhandled error in Deferred:\n| April 12th 2018, 15:50:57.334 | 172.16.99.1 - - [12/Apr/2018:14:50:57 +0000] \"GET /render/?width=588&height=311&_salt=1523544657.184 HTTP/1.1\" 200 868 \"https://graphite-test.colo.elex.be/composer/?\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\"\n| April 12th 2018, 15:50:57.253 | 172.16.57.0 - - [12/Apr/2018:14:50:57 +0000] \"GET /favicon.ico HTTP/1.1\" 200 542 \"https://graphite-test.colo.elex.be/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\"\n| April 12th 2018, 15:50:57.240 | No handlers could be found for logger \"cache\"\n| April 12th 2018, 15:50:56.713 | 172.16.15.1 - guest [12/Apr/2018:14:50:56 +0000] \"POST /events/ HTTP/1.1\" 200 5 \"-\" \"Apache CXF 3.0.0-milestone1\"\n| April 12th 2018, 15:50:56.492 | 172.16.8.0 - - [12/Apr/2018:14:50:56 +0000] \"GET /browser/header/ HTTP/1.1\" 200 1018 \"https://graphite-test.colo.elex.be/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\"\n| April 12th 2018, 15:50:56.374 | -\n| April 12th 2018, 15:50:56.374 | exceptions.NameError: global name 'carbon' is not defined\n| April 12th 2018, 15:50:56.372 | record('allocatedBuffers', len(carbon.aggregator.buffers.BufferManager))\n| April 12th 2018, 15:50:56.372 | File \"/opt/graphite/lib/carbon/instrumentation.py\", line 119, in recordMetrics\n| April 12th 2018, 15:50:56.371 | File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/defer.py\", line 134, in maybeDeferred\n| April 12th 2018, 15:50:56.371 | d = defer.maybeDeferred(self.f, self.a, *self.kw)\n| April 12th 2018, 15:50:56.371 | result = f(args, *kw)\n| April 12th 2018, 15:50:56.371 | ---  ---\n| April 12th 2018, 15:50:56.369 | call.func(call.args, *call.kw)\n| April 12th 2018, 15:50:56.369 | File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/task.py\", line 208, in call\n| April 12th 2018, 15:50:56.368 | self.runUntilCurrent()\n| April 12th 2018, 15:50:56.368 | File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py\", line 1178, in mainLoop\n| April 12th 2018, 15:50:56.368 | self.mainLoop()\n| April 12th 2018, 15:50:56.368 | File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py\", line 800, in runUntilCurrent\n| April 12th 2018, 15:50:56.357 | File \"/usr/local/lib/python2.7/dist-packages/twisted/internet/base.py\", line 1169, in run\n| April 12th 2018, 15:50:56.306 | Traceback (most recent call last):\n| April 12th 2018, 15:50:56.305 | 12/04/2018 14:50:56 :: [console] Unhandled Error\n| April 12th 2018, 15:50:35.848 | -p destination: Stale file handle\n| April 12th 2018, 15:50:22.589 | [Thu Apr 12 14:50:22 UTC 2018]  building index...\n| April 12th 2018, 15:50:19.984 | 172.16.52.0 - - [12/Apr/2018:14:50:19 +0000] \"GET /favicon.ico HTTP/1.1\" 200 542 \"https://graphite-test.colo.elex.be/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\"\n| April 12th 2018, 15:50:19.980 | No handlers could be found for logger \"cache\"\n| April 12th 2018, 15:50:19.246 | 172.16.8.1 - - [12/Apr/2018:14:50:19 +0000] \"GET /browser/header/ HTTP/1.1\" 200 1018 \"https://graphite-test.colo.elex.be/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\"\n```\nThank you for your support !. Twisted version is 11.1.0:\n```\nroot@graphite-3316971257-44grr:/usr/local/src/carbon# pip show twisted\n\nName: Twisted\nVersion: 11.1.0\nLocation: /usr/local/lib/python2.7/dist-packages\nRequires: zope.interface\nroot@graphite-3316971257-44grr:/usr/local/src/carbon# \n```\nGraphite is latest one.. That was the cause actually... I updated Twisted python package to v17+ , along with pip 10 and Incrementals module v17+ and all started working as expected. Closing - thank you for support !. ",
    "TaeWoo21": "Thank you for your reply @deniszh\nNow I understand why the results were returned.\nI really appreciate it! :-)\n. ",
    "mdsalman729": "Turns out it was an issue due to an older version of mod_wsqi.  A simple update of mod_wsgi fixed it.. ",
    "keyboardfann": "Hi @piotr1212 ,\n    Thank you very much and it works. It's wierd that I install graphite-web before and don't install dejavu-sans-fonts dejavu-serif-fonts still works normal. Is it because new version library use it?\nFann. Hi @piotr1212 ,\n    Thank you very much and it works. It's wierd that I install graphite-web before and don't install dejavu-sans-fonts dejavu-serif-fonts still works normal. Is it because new version library use it?\nFann. HI @piotr1212 ,\n    Maybe, It's fine. After installing the package and resolve the issue. Thank you again.\nFann. ",
    "marcelfischer": "One Minute. I tried different options for the summarize. With and without keepLastValue.\nIm also using grafana where this call is functioning.. Hmm sorry but still the same error. I tried both:\ncurl -u user:pass -i \"http://graphite-server/render?target=summarize(icinga2.icingaclient.host.icmp.perfdata.rta.value,\"3min\",'avg',true)&from=-1h&until=now&format=csv\"\ncurl -u user:pass -i \"http://graphite-server/render?target=summarize(icinga2.icingaclient.host.icmp.perfdata.rta.value,\"180\",'avg',true)&from=-1h&until=now&format=csv\"\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/site-packages/django/core/handlers/base.py\", line 112, in get_response\n    response = wrapped_callback(request, callback_args, callback_kwargs)\n  File \"/usr/lib/python2.7/site-packages/graphite/render/views.py\", line 123, in renderView\n    seriesList = evaluateTarget(requestContext, target)\n  File \"/usr/lib/python2.7/site-packages/graphite/render/evaluator.py\", line 10, in evaluateTarget\n    result = evaluateTokens(requestContext, tokens)\n  File \"/usr/lib/python2.7/site-packages/graphite/render/evaluator.py\", line 21, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression)\n  File \"/usr/lib/python2.7/site-packages/graphite/render/evaluator.py\", line 31, in evaluateTokens\n    return func(requestContext, args)\n  File \"/usr/lib/python2.7/site-packages/graphite/render/functions.py\", line 2556, in summarize\n    bucketInterval = int((timestamp - series.start) / interval)\nZeroDivisionError: integer division or modulo by zero. Hmm the query in Grafana is the same. In the meantime I think it has something to do with escaping or so. Because I changed to wget and it works.... You're right, thanks! I will close this one.. ",
    "gms1234": "After some fiddling around, I'm now able to access the Graphite Composer.\nI think what fixed my problem was pip install pysqlite.\nBut my initial problem persists. My Icinga2 instance still shows No graphs found and the Graphite Composer shows No Data.\nPYTHONPATH=$GRAPHITE_ROOT/webapp django-admin.py migrate --settings=graphite.settings --run-syncdb  still gives me the error message ImportError: No module named graphite.settings. I got the graphite.db sync to work via\n$ export PYTHONPATH=/opt/graphite/webapp/\n$ django-admin.py migrate --settings=graphite.settings\nBut still, my Icinga2 instance shows No graphs found.. ",
    "suratovvlad": "@deniszh \nThank you so much for the quick response and for the advices about disabling this feature!. ",
    "surajbora007": "Thanks @deniszh for quick reply, one more query regarding authentication to graphite-web url.\nWhen i am trying to login graphite-web for creating dashboard, entered credential are passed in plain text format, I can see this in developer tool in web browser, any way we can send it in encrypted format. Is it possible to stop front end graphite-web if i have already configure grafana for viewing data collected in whisper.. Thanks all for inputs.. ",
    "JeKa0111": "Is there any way that this will be supported in the next update/release?. ",
    "ptonelli": "You were right, there was only a process runner for sshd but not the sshd daemon itself and nothing was listening on port 22. \nI found what I was searching for:\nhttps://github.com/Ennexa/docker-graphite/blob/master/docker-compose.yml\nthank you for your answer. ",
    "danutvasc": "Adding -1 as timestamp worked. Somehow I missed that part of documentation, but thank you.. ",
    "andysworkshop": "For performance reasons we use redis for the tags having very quickly hit the limits of the SQL option. redis solved the performance problem but because redis is in-memory it cannot be allowed to grow forever.\nI take it from your response that there's nothing in the .wsp file itself that links directly to the entries in tagdb that need to be deleted?\nIf our delete job on the one hand remembers all the .wsp filenames that it's deleted, and on the other hand can query for all keys and values in redis, is there a way to then correlate the two sets and issue delete commands to redis?. OK, looks like I can code a usable workaround from that. Do you have a pointer to the code for calculating the hashed filename given the key in redis? Is it a 1:many mapping from a whisper file to keys in redis or 1:1?\nDo you recommend using the http api for the delete (slower) or can I just go ahead and delete the key from redis directly (faster).\n. We have now got cleanup jobs working with redis based on the above information.. ",
    "vinaykumarrao": "Using RHEL 7.4 . Installed graphite using cephmetrics-ansible playbook.. ",
    "JosephFY": "@deniszh changing that FETCH_TIMEOUT to 10 resolved the issue. I started by setting USE_WORKER_POOL to False based on #2192 and that solved initially it as well. Had it back to True based on @DanCech recommendations in #2192.\nAppreciate the help guys. gonna go ahead and close this one. Issue has been resolved.. Hello guys,\nFirst, big thanks to @DanCech for the nice webinar yesterday, was really helpful. I was the guy asking questions here and there :)\nSo couple of observations on this. I was mistaken initially of the version. Had done a test today and noticed the following \nHave 2 separate graphite instances, they are stand alone so they are not part of clustering at all but I push data 2 both at the same time, so both have same metrics as far as this is concerned.\nGrafana 5.2.3\n\n0.9.13  The Singlstat throws  no value and it shows a Server Internal Error.\n1.1.3 The Singlstat throw no value BUT it doesn't show the Internal Server Error. But of course putting this in a an actual graph returns no data points\n\nWe don't care about the 0.9.13 as we are migrating from it. We also trying to get historical data of those counts, I usually do them in a combination of scale(0) and offest(1) sumSeries() functions.\nI will be testing the fallbackSeries in the next few minutes and get back to you with results.\n. @deniszh so couple of notes about the fallback.\nThe fallbackSeries is working perfectly on a singlestat (assuming \n the sparkle are not shown) fallbackSeries(countSeries(currentAbove(servers.*.status, 1.1)),constantLine(0))\nWith the fallback, I still cant seem to apply scale(0), offest(1) and sumSeries() i.e. to be able to get the count of series historically .\nI really think that a countSeries or a new/similar function should be available to count series across selected period of time, I guess anything besides that will be just non efficient workarounds.\n\nThat is a good question, I have a trivial patch to make countSeries \"work\" for empty result sets, but a drawback is that it doesn't have an easy way to tell what the output alias should be, due to the internal implementation of series lists in graphite functions not keeping track of the pathExpression for a given list. It should be possible to fix that problem by implementing a custom SeriesList type based on list (similar to the existing TimeSeries type) so that it could carry pathExpression and thus support proper aliasing when empty.\n\nAssuming this can be done as a custom function, I will start trying (with my modest programming skills) to work on a custom function to do this, I haven't done custom functions before but it seems straight forward. I am OK with a function that wont do Alias functions if it cal do the simple alias('') to rename the series. Can test it out if you have the code ready and provide feedback.\nwe have have plans to do many of count series on the fly with our data which seems so difficult to do so. Aggregating metrics before and is not an option for us as well for many of those. I guess what you need is smartSummarize\nThis function will apply different calculations per the time interval you specify . I am actually doing some testing on this box before going to prod. So let me explained a little bit.\nI am intentially lowering MAX_UPDATES_PER_SECOND = 50 so I can get the data from CarbonLink only thus from memory,  I wanted the results to get back to me as fast as possible.  I am sending around 2.2Mil metrics/Minute to this box and been steady with no issues for about 2 weeks.\nI have 8 carbons [a-h], each has a cache size of ~12 Mil and  ~ 100 points per update.\nThis is the graph:\n\nQuery inspector on Grafana:\n\nResults on cache.log\n2019-02-04,13:09:10.619 :: CarbonLink cache-query request for xxxxxxxx returned 15 datapoints\n2019-02-04,13:09:49.201 :: CarbonLink cache-query request for xxxxxxxx returned 16 datapoints\n2019-02-04,13:10:11.215 :: CarbonLink cache-query request for xxxxxxxx returned 16 datapoints\n2019-02-04,13:10:15.786 :: CarbonLink cache-query request for xxxxxxxx returned 16 datapoints\n2019-02-04,13:10:23.640 :: CarbonLink cache-query request for xxxxxxxx returned 16 datapoints\n2019-02-04,13:12:13.741 :: CarbonLink cache-query request for xxxxxxxx returned 18 datapoints\n2019-02-04,13:12:31.016 :: CarbonLink cache-query request for xxxxxxxx returned 18 datapoints\n2019-02-04,13:12:39.613 :: CarbonLink cache-query request for xxxxxxxx returned 19 datapoints\n2019-02-04,13:12:48.084 :: CarbonLink cache-query request for xxxxxxxx returned 19 datapoints\n2019-02-04,13:12:54.276 :: CarbonLink cache-query request for xxxxxxxx returned 19 datapoints\n2019-02-04,13:13:18.224 :: CarbonLink cache-query request for xxxxxxxx returned 19 datapoints\n2019-02-04,13:13:28.463 :: CarbonLink cache-query request for xxxxxxxx returned 19 datapoints\n2019-02-04,13:18:28.642 :: CarbonLink cache-query request for xxxxxxxx returned 24 datapoints\n2019-02-04,13:20:51.593 :: CarbonLink cache-query request for xxxxxxxx returned 27 datapoints\n2019-02-04,13:29:19.423 :: CarbonLink cache-query request for xxxxxxxx returned 35 datapoints\n2019-02-04,13:29:27.626 :: CarbonLink cache-query request for xxxxxxxx returned 35 datapoints\n2019-02-04,13:29:39.134 :: CarbonLink cache-query request for xxxxxxxx returned 36 datapoints\n2019-02-04,14:03:58.352 :: CarbonLink cache-query request for xxxxxxxx returned 70 datapoints\n2019-02-04,14:04:08.129 :: CarbonLink cache-query request for xxxxxxxx returned 70 datapoints\n2019-02-04,14:05:19.516 :: CarbonLink cache-query request for xxxxxxxx returned 71 datapoints\nMultiple duplicate values (like returned 70 datapoints) occured when I refresh multiple times within the minute and no new metrics came yet.\n. yea but my concern is, why showing 71 data points where the graph has only 5. I see, Thanks @DanCech  for clarifying, I was assuming that the number should match the exact number of datapoints I'm querying for, looks like it's returning all the values for series X in the cache as you described. It's clear now.\nI know this should be in saperate question but the reason I dug through the cach.log is to enhance the queries time on our metrics. So we have ~10K servers and our metrics look like\nservers.serverX.status  and it returns either 0 or 1 depending on servers availability.\nOften we will need to apply different functions on those metrics (and other) if we do \nservers.serverX.status|currentbelow(0.9)|countSeries() the query is taking 10-15 seconds to finish and I'm almost sure all the data is actually getting queried from carbonLink thus from memory\nWe have a 16 cores and 32 Gb memory with SSD 5K IO vm\nusing WSGIDaemonProcess graphite processes=8 threads=16 display-name='%{GROUP}' inactivity-timeout=120 on graphite-vhost.conf\nI know mialge may vary but what would you suggest to speed up these type of queries ?  especially that we need to list those currentBelow() results in a table in grafana or applying aggregate('count') for historical graphs. I turned on memcached and that helped when someone asking for the exact same query, but it's not helping when the query has to refresh for the first time on a Grafana dashboard.\nAny advice?. I have 8 carbons [a-h], each has a cache size of ~12 Mil and ~ 100 points per update.\n. I will try doubling and get back with results, so you think the bottleneck is on the carbon side not the graphite-web side ?\nYOu think adding more process/workers for WSGIDaemonProcess would help ?\nIs go-carbon ready for production environment? I take it it will only replace the carbon-cache not graphite-web and whisper?. ",
    "xcycharles": "Appreciated Deniszh! We are looking into how to make Carbon more scalable.. ",
    "TimWhalen": "I packaged and tested with graphite-web 1.1.3 with the same result so I updated the version in this issue.. The cause of this issue is that prefetched data in webapp/graphite/render/datalib.py is in a generator, converting it to a list in the cache resolves my issue: https://github.com/graphite-project/graphite-web/pull/2322. I moved the generator expansion into a separate loop. I've added a brief comment explaining the change.. You are correct, that was a mistake introduced when I moved the check into its own loop to address an earlier change request.  Sorry about that.  I've updated the pr again.. ",
    "yadsirhc": "Any ideas on why Travis CI is flagging it as \"Abuse detected\"?. I clicked the Travis CI image link on the project readme, then Pull\nRequests, then the menu in the upper right, then Requests and saw four red\nbuilds with that text to the right.\nOn Wed, Jul 25, 2018, 6:20 PM Denis Zhdanov notifications@github.com\nwrote:\n\nWhere did you see that, @yadsirhc https://github.com/yadsirhc ?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/graphite-project/graphite-web/pull/2321#issuecomment-407914991,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AHL6VOfFMfB-_OPr8r9Gw71xHYVDM1D8ks5uKO8egaJpZM4VbvwI\n.\n. @deniszh Should I try closing this PR and opening a new one?  My gut tells me that the initial PR for the change followed later by adding the unit test may be why it was rejected by Travis CI.. @deniszh  No dice.  From everything else I'm reading it looks like someone from the graphite project will need to engage Travis CI support.. @deniszh It finally went through successfully on Travis CI after this most recent PR attempt.. Fix for #2320 . \n",
    "Diablo2050": "I think you mean:\n/opt/graphite/conf/graphite.wsgi.example\njust copy it in the same directory and remove .example.. ",
    "rejkube": "there are values which are present [pls see the ones above]. The key itself is not getting detected in graphite-web. expected  hostname->iops->[disks]->key\nin the dasboard selection all the other 3 i get  but key :(\n. yes there are values in the  key.wsp ;that file is inside [disks] directory as expected. ",
    "kowalskey": "Hello @deniszh \nLooks like  I found what was the problem.\nI listened to your advice and installed latest graphite from source. I migrated old metrics and found out that queries that used alias() still did not work. That made me think that it's not issue in graphite, but instead I started to play with queries.\nWe had query like this:\nalias(sets.transactions.count.,'Transactions')\nthat was working correctly with graphite 0.9, but stopped displaying any data under graphite >= 1.0\nNow here's interesting part:\nalias(sets.transactions.count,'Transactions') works with 1.x (note missing trailing dot in metric name)\nsets.transactions.count and sets.transactions.count. works under 1.x too\nUnder 0.9 query works only with trailing dot in metric name (either aliased or not), and query without trailing dot does not work\nIs this expected behaviour? I failed to notice that in documentation\n. I think this also may be specific to Debian/Ubuntu because of how whisper files are organized\nFor my example above file with metric would have following path under graphite 0.9 installation\n/var/lib/graphite/whisper/sets/transactions/count/.wsp\nand during migration I was moving it to something like /var/lib/graphite/whisper/sets/transactions/count.wsp\nbecause graphite wouldn't display metrics from files named .wsp in browser\nIt would display tree like this:\n+ sets\n  + transactions\n      + count\n         (empty) <my metric is missing>\nand after I moved files it would display\n+ sets\n   + transactions\n       [count] <my metric is visible here and I can add it to graph and display correctly>\nSo maybe that metric trailing dot was previously required before because of extra directory + .wsp file and it stopped working for me because both  graphite from newer .debs and source save files as <metric_name>.wsp instead of <metric_name>/.wsp \n. After some more digging I ruled out debian/ubuntu changes to graphite/whisper. \n(More digging was required because new stats in files in \"old\" locations mysteriously started to pop up when I pointed statsd to new graphite instance)\nIt was problem with our statsd installation, setting globalPrefix to '' is actually very different from leaving it undefined in statsd's config.js, because it caused single dot to be added at end of each metric name when submitting them to graphite. :man_facepalming: \nHere's relevant fragment of code: https://github.com/etsy/statsd/commit/c87b0a1c3eaadc2c5002ddc3fffb82cf69fbc606#diff-f2021c24d4a3c65f0999d3286c2f60bbR200\nSo I guess this issue can be safely closed, because it has nothing to do with graphite components behaviour.. ",
    "m2cci-KGY": "I Make this commande : \n]# PYTHONPATH=/usr/share/graphite/webapp manage.py syncdb --settings=graphite.settings\n-bash: manage.py: command not found. ]# PYTHONPATH=/usr/share/graphite/webapp manage.py syncdb --settings=graphite.settings\n-bash: manage.py: command not found. PYTHONPATH=/usr/share/graphite/webapp manage.py syncdb --settings=graphite.settings\n-bash: manage.py: command not found\n. ",
    "viper539": "manage.py didn't work for me also. I tried django-admin (without .py) but then it dies with \"ImportError: cannot import name generic\" which stackoverflow answer with \"The django.contrib.contenttypes module has been reorganized in Django 1.7\"\nIn my case: it seems i have django-tagging 0.3.1 and min is 0.4.6. ",
    "eachirei": "Thanks! Do you have an ETA for the fix?. This is my attempt at fixing the issue:  https://github.com/graphite-project/graphite-web/pull/2338. Done! Not sure if I should add a test here as well: https://github.com/graphite-project/graphite-web/blob/824800a4218c8cb9ab56ca67277b0f7774a7ff1d/webapp/tests/test_readers_util.py#L179-L210. I also added tests for the test_readers_util suite. ",
    "fengyehong": "I was testing the STORE_FAIL_ON_ERROR = False flag, I have 4 storage nodes(with graphite-web and carbon running), one relay node, and a separate master graphite-web node.\nIf i shutdown one of the storage node, and query the master graphite-web /metrics/find?query=*, I still get HTTP 500 returned, which body contains the detail: connection refused to the storage ndoe.\nDepending on the REMOTE_RETRY_DELAY and FIND_CACHE_DURATION parameters, refresh may return successfully, but after the retry dealy or cache duration, I get HTTP 500 again.\nI check the code and find that as RemoteFinder.find_nodes is a generator, exceptions in this function can not be found by Store.wait_jobs\nBWT, I am not very sure about the effect of not using generator here. AFAK it only costs more memory.. comment added. ",
    "dreddynarahari": "@deniszh \nHi! I have populated the directory as you suggested. The UI still does not show up as expected.. @deniszh I missed to add \n<Directory /opt/graphite/static/>\n         Require all granted\n  </Directory> and it now worked. Thank you.. This is solved now. Closing out.. @DanCech What is the command for Django 1.8. Currently I am running \ndjango-admin.py migrate --noinput --settings=graphite.settings. @deniszh Installed  Django (1.8), django-tagging (0.4.3)\nThis was installed using pip install command. . @deniszh We have only one version of Python installed .ie. Python=2.7. @piotr1212 2018-09-27,12:52:36.980 :: RemoteFinder[0.0.0.0:8888] Error response 500 from http://0.0.0.0:8888/render/?format=pickle&local=1&noCache=1&from=1538069856&until=1538070756&target=stg.mon.UP.rg.mon01.services.kafka.kafka_server.total_lag&now=1538070756\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/readers/remote.py\", line 64, in fetch_multi\n    timeout=settings.FETCH_TIMEOUT,\n  File \"/opt/graphite/webapp/graphite/finders/remote.py\", line 273, in request\n    raise Exception(\"Error response %d from %s\" % (result.status, url_full))\nException: Error response 500 from http://0.0.0.0:8888/render/?format=pickle&local=1&noCache=1&from=1538069856&until=1538070756&target=stg.mon.UP.rg.mon01.services.kafka.kafka_server.total_lag&now=1538070756\n2018-09-27,12:52:36.980 :: Failed after 5 attempts! Root cause:\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/readers/remote.py\", line 64, in fetch_multi\n    timeout=settings.FETCH_TIMEOUT,\n  File \"/opt/graphite/webapp/graphite/finders/remote.py\", line 273, in request\n    raise Exception(\"Error response %d from %s\" % (result.status, url_full))\nException: Error response 500 from http://0.0.0.0:8888/render/?format=pickle&local=1&noCache=1&from=1538069856&until=1538070756&target=stg.mon.UP.rg.mon01.services.kafka.kafka_server.total_lag&now=1538070756\nTraceback (most recent call last):\n  File \"/opt/graphite/webapp/graphite/readers/remote.py\", line 64, in fetch_multi\n    timeout=settings.FETCH_TIMEOUT,\n  File \"/opt/graphite/webapp/graphite/finders/remote.py\", line 273, in request\n    raise Exception(\"Error response %d from %s\" % (result.status, url_full))\nException: Error response 500 from http://0.0.0.0:8888/render/?format=pickle&local=1&noCache=1&from=1538069856&until=1538070756&stg.mon.UP.rg.mon01.services.kafka.kafka_server.total_lag&now=1538070756\nI see the above logs. The data is present in whisper when I try to use whisper-fetch.. 2018-09-27,12:52:33.904 :: Exception encountered in <GET http://10.141.16.117/render?format=json&from=-15min&until=now&target=asPercent(keepLastValue(stg.mon.UU.rg.mon01.servers.osscmrstg1.df-root.df_complex-used),sumSeries(keepLastValue(stg.mon.UU.rg.mon01.servers.osscmrstg1.df-root.df_complex-%7Bused,free,reserved%7D)))>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 132, in get_response\n    response = wrapped_callback(request, *callback_args, **callback_kwargs)\n  File \"/opt/graphite/webapp/graphite/render/views.py\", line 117, in renderView\n    data.extend(evaluateTarget(requestContext, targets))\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 28, in evaluateTarget\n    result = evaluateTokens(requestContext, target)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 57, in evaluateTokens\n    return evaluateTokens(requestContext, tokens.expression, replacements)\n  File \"/opt/graphite/webapp/graphite/render/evaluator.py\", line 93, in evaluateTokens\n    return func(requestContext, *args, **kwargs)\n  File \"/opt/graphite/webapp/graphite/render/functions.py\", line 913, in asPercent\n    raise ValueError(\"asPercent second argument must be missing, a single digit, reference exactly 1 series or reference the same number of series as the first argument\")\nValueError: asPercent second argument must be missing, a single digit, reference exactly 1 series or reference the same number of series as the first argument. Any ideas on how to fix this?. ",
    "erez-rabih": "I agree there is a workaround to get the behaviour I am looking for it just seems very inconsistent that maximumAbove uses > and minimumBelow uses <=\nOf course it might impact existing queries and there are other considerations to take into account I just wanted to raise the flag since it took me two hours to understand why my query returns unexpected series\nI would at least add a comment on the documentation to make users know that although it says below it is implemented as below or equal to. ",
    "rahulr311295": "Ok, so the latest version has not been reflected in the apt repositories so instead of V 1.x it's downloading 0.9.x so can you close this issue. ",
    "caldavid": "Oh God, i spent half day just because of this underscore!\nThanks a lot!\nDavid. Thanks for the tips!\nDavid. Hi Deniszh,\nThanks for your reply.\nThat said, I could use CLUSTER_SERVER config to let two graphite-web servers talk to each other? Like this way?\nCLUSTER_SERVERS=['127.0.0.1:80', 'xxx.xxx.xxx.xxx:80']\nBest,\nDavid. So the CLUSTER_SERVERS and CARBONLINK can be used together? I tried that, but didn't seem worked to me somehow.\n. gotcha! Thanks a lot!  \ud83d\udc4d . ",
    "andreasferber": "As suspected, the fix was pretty simple indeed, PR coming up shortly.. I don't really understand why the TOXENV=lint check failed, none of the files mentioned in the build log were even touched by me. Looks to me like an issue with the check itself, although other PRs seem to pass it.... ",
    "uwemaurer": "Thanks for the tip.  This should be mentioned much more prominently in the documentation, since it doesn't work without running this.\nhttps://graphite.readthedocs.io/en/latest/config-local-settings.html?highlight=collectstatic#filesystem-paths\nIt worked with these commands:\npip  install django-admin\nPYTHONPATH=/opt/graphite/webapp django-admin.py collectstatic --noinput --settings=graphite.settings. ",
    "mbarbon": "\nCan you add documentation for minValue and maxValue to the docblock?\n\nAdded. I tried to be concise; let me know if you would prefer a longer explanation.. ",
    "dengshaochun": "@deniszh ok\uff0cthanks\u3002but i think you should update requirements.txt of 0.9.16 releases. ",
    "ploxiln": "looks like #2354 was back-ported to the 1.1.x branch as ed795939b329f70d70ec438147c3ab9c55eb7bdf and released in graphite-web 1.1.5 in December. I think a rebase went a bit wrong, you ended up with a copy of the commit \"fix dashboard graph metric list icon paths with URL_PREFIX\" from the master branch\nin master: 0a037db4b2d864734e14dd6302bc71194f53e8d3\nin this branch: 1ba4da55c08035cccfcdaae2220f8d384dbd1929. All looks good to me.\nI had another thought about storage dirs ... I think the original idea behind using /opt/graphite is those storage and log dirs, which would be awkward in the python site-packages directory. Maybe the thing to do is divorce them from the graphite application root, and default to /opt/graphite/storage and /opt/graphite/log regardless of the install prefix? And not mention them in setup.py at all? I suppose the downside is they would not be created by install. Just an idea - I suppose I always customize these dirs anyway.. I also found this way of getting the static root:\nhttps://github.com/graphite-project/graphite-web/blob/1.1.5/webapp/content/js/composer_widgets.js#L154. one test matrix entry failing: TOXENV=py27-django111-pyparsing2-mysql\ndue to:\n...\n  File \".../site-packages/django/db/backends/base/base.py\", line 189, in connect\n    self.connection = self.get_new_connection(conn_params)\n  File \".../site-packages/django/db/backends/mysql/base.py\", line 276, in get_new_connection\n    conn.encoders[SafeBytes] = conn.encoders[bytes]\nKeyError: <type 'str'>. I think this is pretty good. I'll squash down to one commit if I get the go-ahead :). Thanks. squashed. --run-syncdb was needed before graphite-web 1.1, but with 1.1+ it should not be needed, even for databases originally created with graphite-web 0.9.x and older django. While figuring out #2412 also I tested making a new database from nothing, without --run-syncdb, and all tables were created (by initial migrations which are now present).\n(Now --fake-initial may be needed only because --run-syncdb was used before, when it was needed.). possibly worth mentioning: --fake-initial was added in django 1.8 and --run-syncdb was added in django 1.9\n(according to https://docs.djangoproject.com/en/1.9/ref/django-admin/#migrate). looks like, in django 1.8 and earlier, the behavior of --run-syncdb was unconditionally provided:\n\nNo arguments: All migrated apps have all of their migrations run, and all unmigrated apps are synchronized with the database. OK, I'll try adding a bit more to the docs about this. Let me know how that looks/sounds.\n\nI'll squash commits before merge.. Good idea - I think I figured this out, but I'm no django expert :). I don't think there would be any harm in always using --fake-initial.\nIt should be needed at most once, when migrating from < 1.1 to >= 1.1, and the two places that that I did not include --fake-initial in the example were about \"setting up a new database\" - so I guess I did not want to encourage \"just type this even though it does not make sense\".. \"servers\" -> \"serves\". since you're editing this, might as well clean it up a bit:\npython\nfor subdir in ('whisper', 'ceres', 'rrd', 'log', 'log/webapp'):\n    storage_dirs.append(('storage/%s/dummy.txt' % subdir, []))\nalso not sure if bot \"log\" and \"log/webapp\" are needed. it also might be nice to use \".keep\" instead of \"dummy.txt\". could simplify to:\npython\nexcept OSError as exc:\n    if not (exc.errno == errno.EEXIST and os.path.isdir(path)):\n        raise. very minor - this \"kA\" looks unintentional. ",
    "balasenthil-d": "Yes I am using tags. I mean I use tags-as-prefix property in spring boot. I use them to differentiate the services that push the metrics. The spring boot applications push metrics to graphite server. Tags is how I identify them. . I see the CPU is around 3-5% after doing the above change. Thank you @DanCech . I have another question. Is it a good practice to go with sqllite db for production monitoring considering there is no graphite clustering ? Or should I use a RDMS for storing metrics ?. Ok thank you @deniszh I will look at migrating to a RDBMS.. Actually I am quite happy with the performance ever since changing TAG_UPDATE_INTERVAL. The CPU has been around 3-10% with occasional CPU spikes. I am not planning to use RDBMS unless its really needed.. ",
    "gksinghjsr": "Just to be clear, when you do diffSeries(A,B), you expect the result to be A-B. However, when A and B contain a few nulls each, the result is:-\na) at times just the value at A, if B is null.\nb) when A is null, it returns the value at B, which is not what you expect. This leads to a scenario where if the first metric is entirely null, it will just return the second metric. This when extended to n metrics gives the second scenario you guys mentioned.\nThe problems arise because we are not consistent in calculating the differences across timestamps. The approach wouldn't matter in case of sumSeries, or similar other functions, but in case of diff (and maybe others), that simply isn't true.\nhttps://github.com/graphite-project/graphite-web/blob/1134385b0bd34f00a7661a72c8a0276ff0264eee/webapp/graphite/render/functions.py#L70\n. ",
    "idling11": "@deniszh thanks for your answer\u3002Since this method is inefficient, is there any other efficient way for me to search on the page?  Or is there any plug-in that can do this?  Because I have too many metrics. @deniszh Ok , I will have a try.  thank you very much. ",
    "njchandu": "I updated django-tagging to 0.4.5. I'm still getting the same error.. ```\nFor the ImportError: No module named fields. http://www.reemachugani.com/blog/2015/feb/14/no-module-named-fields-importing-GenericForeignKey/\nIn the official django documentation for ContentTypes, the import statement used for GenericForeignKey is given as - from django.contrib.contenttypes.fields import GenericForeignKey\nThis will raise an ImportError. Replace it with the following - from django.contrib.contenttypes.generic import GenericForeignKey\ncd /usr/local/lib/python2.7/site-packages\nfind . -type f -exec sed -i 's/django.contrib.contenttypes.fields/django.contrib.contenttypes.generic/' {} +\n``\nI found the culprit, this was replacing thefieldswithgeneric`.\nThe error is gone, but graphite doesn't start up\nI'm following this doc: https://www.digitalocean.com/community/tutorials/how-to-keep-effective-historical-logs-with-graphite-carbon-and-collectd-on-centos-7 to see what am I doing wrong.. As in the dashboard UI doesn't load.\n```\ndjango-admin.py runserver --pythonpath /opt/graphite/webapp --settings graphite.settings 0.0.0.0:8080\n/opt/graphite/webapp/graphite/settings.py:332: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security\n  warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\n/opt/graphite/webapp/graphite/settings.py:332: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security\n  warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\nPerforming system checks...\nSystem check identified no issues (0 silenced).\nYou have 21 unapplied migration(s). Your project may not work properly until you apply the migrations for app(s): account, admin, auth, contenttypes, dashboard, events, sessions, tagging, tags, url_shortener.\nRun 'python manage.py migrate' to apply them.\nDecember 09, 2018 - 18:35:56\nDjango version 1.11, using settings 'graphite.settings'\nStarting development server at http://0.0.0.0:8080/\nQuit the server with CONTROL-C.\n```\nThis is what I see in UI.\nThis page isn\u2019t working dashboard-stress-internal.devaws.net didn\u2019t send any data.\nERR_EMPTY_RESPONSE. @DanCech I'll try that.. @DanCech @piotr1212 \nI tried a few combination of versions for a couple of days. I was not able to solve it, here are the details.\nVersions:\ndjango: 1.11.17\ntwisted: 18.7.0\ngraphite-web: 1.1.4\nwhisper: 1.1.4\ncarbon: 1.1.4\ngo-carbon: 0.13.0\nhttps://github.com/lomik/go-carbon/releases/download/v0.13.0/go-carbon-0.13.0-1.x86_64.rpm\nPYTHONPATH=\"/opt/graphite/lib/:/opt/graphite/webapp/\"\nThis is the error I'm getting now.\n[ec2-user@ip ~]$ ./dashboard.sh\nstarting carbon\n2018/12/11 18:30:40 [WARNING] `common.log-level` and `common.logfile` is DEPRICATED. Use `logging` config section\nstarting graphite-web\nstarting dashboard\nnohup: redirecting stderr to stdout\nStarting dxedge-dashboard at port 9000...                             ERROR [Returned: 7]\nERROR [no response from server, timeout]\ndashboard.sh\n```\n!/bin/bash\nsource /home/ec2-user/.bashrc\nexport PATH=$PATH:/usr/local/bin\necho \"starting carbon\"\ngo-carbon -config /usr/local/etc/go-carbon.conf -daemon\necho \"starting graphite-web\"\nnohup /opt/graphite/bin/run-graphite-devel-server.py /opt/graphite > graphite.out &\ncd {{ dashboard_home }}\necho \"starting dashboard\"\nmkdir logs\nbin/service.sh start {{ dashboard_port }}\n```\nThe same set up works fine for below versions.\nVersions:\ndjango: 1.6.7\ntwisted: 13.1.0\ngraphite-web: 0.9.15\nwhisper: 0.9.15\ncarbon: 0.9.15\ngo-carbon: 0.9.1\nhttps://github.com/lomik/go-carbon/releases/download/v0.9.1/go-carbon-0.9.1-1.el6.x86_64.rpm\nI'm using ansible for automation and I have removed this bit of code and there are no script changes apart from the version changes.\n```\nFix problems with Python imports.\nBecause defaults is deprecated. See http://stackoverflow.com/questions/21058177/no-module-named-defaults-django-simple-friends\ncd /opt/graphite\nfind . -type f -exec sed -i 's/django.conf.urls.defaults/django.conf.urls/' {} +\nFor the ImportError: No module named fields. http://www.reemachugani.com/blog/2015/feb/14/no-module-named-fields-importing-GenericForeignKey/\nIn the official django documentation for ContentTypes, the import statement used for GenericForeignKey is given as - from django.contrib.contenttypes.fields import GenericForeignKey\nThis will raise an ImportError. Replace it with the following - from django.contrib.contenttypes.generic import GenericForeignKey\ncd /usr/local/lib/python2.7/site-packages\nfind . -type f -exec sed -i 's/django.contrib.contenttypes.fields/django.contrib.contenttypes.generic/' {} +\n```\nOn execution of django-admin.py runserver --pythonpath /opt/graphite/webapp --settings graphite.settings 0.0.0.0:8080\n```\nPerforming system checks...\nSystem check identified no issues (0 silenced).\nYou have 20 unapplied migration(s). Your project may not work properly until you apply the migrations for app(s): account, admin, auth, contenttypes, dashboard, events, sessions, tagging, tags, url_shortener.\nRun 'python manage.py migrate' to apply them.\nDecember 11, 2018 - 18:46:04\nDjango version 1.11.17, using settings 'graphite.settings'\nStarting development server at http://0.0.0.0:8080/\nQuit the server with CONTROL-C.\n``\nAnd I don't seemanage.pyunder/opt/graphite/webapp/graphite`\nWould appreciate any help on this.. @deniszh I will re write the scripts and update it here.. @deniszh \nI believe it is breaking here\n+ echo 'Moving graphite storage folders'\n+ mkdir /mnt/data/graphite\n+ mv /opt/graphite/storage /mnt/data/graphite/storage\n+ ln -s /mnt/data/graphite/storage /opt/graphite/storage\n+ chown -R ec2-user:ec2-user /mnt/data/\n+ cd /opt/graphite/webapp/graphite/\n+ python ./manage.py syncdb --noinput\npython: can't open file './manage.py': [Errno 2] No such file or directory\nand the script is\n```\nfunction configure_graphite(){\n  echo \"Graphite/Carbon configuration\"\n  # Update links\n  GRAPHITE_DATA=${DATA_MOUNT}/graphite\n  if [ ! -d \"${GRAPHITE_DATA}\" ]; then\n    echo \"Moving graphite storage folders\"\n    mkdir ${GRAPHITE_DATA}\n    mv /opt/graphite/storage ${GRAPHITE_DATA}/storage\n  else\n    echo \"Graphite storage folders exist - removing\"\n    rm -r /opt/graphite/storage\n  fi\n  ln -s ${DATA_MOUNT}/graphite/storage /opt/graphite/storage\n  chown -R ec2-user:ec2-user ${DATA_MOUNT}/\ncd /opt/graphite/webapp/graphite/\n  python ./manage.py syncdb --noinput\n  chown -R ec2-user:ec2-user ${DATA_MOUNT}/graphite/storage\n  chown -R ec2-user:ec2-user /opt/graphite/storage\n}\n```\nThis is what I see on the dashboard UI (http://dashboard-stress-internal.devaws.net:8080/)\ndashboard-stress-internal.dxedge.devaws.dataxu.net didn\u2019t send any data.\n. @deniszh \nI tried the following commands with no success\ndjango-admin migrate --settings=graphite.settings --run-syncdb\n/var/lib/cloud/instance/scripts/part-001: line 200: django-admin.py: command not found\npython django-admin.py migrate --settings=graphite.settings --run-syncdb\npython: can't open file 'django-admin.py': [Errno 2] No such file or directory\nln -fs /usr/local/lib64/python2.7/site-packages/django/bin/django-admin.py /usr/local/bin/django-admin\nPYTHONPATH=/opt/graphite/webapp django-admin.py migrate --settings=graphite.settings --run-syncdb\n/var/lib/cloud/instance/scripts/part-001: line 205: django-admin.py: command not found\n. @deniszh \nApologies that happened because the set was happening from a root user. It works fine now. But, it doesn't load the data on the dashboard. The left navigation bar doesn't load. This is what I notice in the logs.\n[16/Dec/2018 13:56:40] \"GET / HTTP/1.1\" 200 598\n[16/Dec/2018 13:56:41] \"GET /browser/header HTTP/1.1\" 200 1167\n[16/Dec/2018 13:56:41] \"GET /composer? HTTP/1.1\" 200 1643\n[16/Dec/2018 13:56:41] \"GET /static/js/browser.js HTTP/1.1\" 404 94\n[16/Dec/2018 13:56:41] \"GET /static/js/ext/resources/css/ext-all.css HTTP/1.1\" 404 113\n[16/Dec/2018 13:56:41] \"GET /static/js/ext/adapter/ext/ext-base.js HTTP/1.1\" 404 111\n[16/Dec/2018 13:56:41] \"GET /static/js/ext/ext-all.js HTTP/1.1\" 404 98\n[16/Dec/2018 13:56:41] \"GET /static/js/composer_widgets.js HTTP/1.1\" 404 103\n[16/Dec/2018 13:56:41] \"GET /static/js/composer.js HTTP/1.1\" 404 95\n[16/Dec/2018 13:56:41] \"GET /static/js/completer.js HTTP/1.1\" 404 96\n[16/Dec/2018 13:56:41] \"GET /static/img/carbon-fiber.png HTTP/1.1\" 404 101\n[16/Dec/2018 13:56:41] \"GET /static/img/graphite-logo.png HTTP/1.1\" 404 102\n[16/Dec/2018 13:56:41] \"GET /static/js/browser.js HTTP/1.1\" 404 94\n[16/Dec/2018 13:56:42] \"GET /static/js/composer_widgets.js HTTP/1.1\" 404 103\n[16/Dec/2018 13:56:42] \"GET /static/js/composer.js HTTP/1.1\" 404 95\n[16/Dec/2018 13:56:42] \"GET /static/js/completer.js HTTP/1.1\" 404 96. @deniszh \n```\n[ec2-user@ip-172-17-68-79 ~]$ PYTHONPATH=/opt/graphite/webapp django-admin.py collectstatic --settings=graphite.settings\n/opt/graphite/webapp/graphite/settings.py:332: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security\n  warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\nCopying '/opt/graphite/webapp/content/js/dashboard.js'\nCopying '/opt/graphite/webapp/content/js/composer.js'\nCopying '/opt/graphite/webapp/content/js/completer.js'\nCopying '/opt/graphite/webapp/content/js/browser.js'\nCopying '/opt/graphite/webapp/content/js/composer_widgets.js'\nCopying '/opt/graphite/webapp/content/js/ace/mode-csharp.js'\nCopying '/opt/graphite/webapp/content/js/ace/mode-clojure.js'\nCopying '/opt/graphite/webapp/content/js/ace/mode-json.js'\nCopying '/opt/graphite/webapp/content/js/ace/mode-css.js'\nCopying '/opt/graphite/webapp/content/js/ace/mode-java.js'\nCopying '/opt/graphite/webapp/content/js/ace/worker-javascript.js'\nCopying '/opt/graphite/webapp/content/js/ace/mode-c_cpp.js'\nCopying '/opt/graphite/webapp/content/js/ace/keybinding-vim.js'\nCopying '/opt/graphite/webapp/content/js/ace/theme-textmate.js'\nCopying '/opt/graphite/webapp/content/js/ace/ace.js'\nCopying '/opt/graphite/webapp/content/js/ace/mode-groovy.js'\nCopying '/opt/graphite/webapp/content/js/ace/mode-coffee.js'\nCopying '/opt/graphite/webapp/content/js/ace/mode-javascript.js'\nCopying '/opt/graphite/webapp/content/js/ace/mode-html.js'\nCopying '/opt/graphite/webapp/content/js/ext/ext-all.js'\nCopying '/opt/graphite/webapp/content/js/ext/ext-all-debug.js'\nCopying '/opt/graphite/webapp/content/js/ext/adapter/ext/ext-base-debug.js'\nCopying '/opt/graphite/webapp/content/js/ext/adapter/ext/ext-base.js'\nCopying '/opt/graphite/webapp/content/js/ext/resources/css/ext-all.css'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/gradient-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/shadow.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/shadow-lr.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/s.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/shadow-c.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/panel/tools-sprites-trans.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/panel/left-right.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/panel/white-left-right.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/panel/tool-sprite-tpl.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/panel/tool-sprites.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/panel/top-bottom.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/panel/white-corners-sprite.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/panel/light-hd.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/panel/white-top-bottom.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/panel/corners-sprite.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/panel/top-bottom.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/form/text-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/form/trigger-square.psd'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/form/radio.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/form/trigger-tpl.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/form/clear-trigger.psd'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/form/date-trigger.psd'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/form/exclamation.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/form/search-trigger.psd'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/form/search-trigger.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/form/error-tip-corners.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/form/clear-trigger.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/form/checkbox.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/form/trigger.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/form/trigger.psd'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/form/date-trigger.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/form/trigger-square.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/dd/drop-add.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/dd/drop-no.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/dd/drop-yes.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/shared/warning.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/shared/calendar.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/shared/large-loading.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/shared/blue-loading.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/shared/hd-sprite.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/shared/loading-balls.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/shared/right-btn.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/shared/glass-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/shared/left-btn.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/window/icon-error.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/window/left-corners.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/window/icon-question.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/window/right-corners.psd'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/window/right-corners.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/window/icon-warning.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/window/top-bottom.psd'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/window/left-corners.psd'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/window/left-right.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/window/icon-info.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/window/top-bottom.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/window/left-right.psd'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/box/r-blue.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/box/corners-blue.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/box/tb-blue.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/box/r.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/box/corners.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/box/l.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/box/tb.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/box/l-blue.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/elbow-end-plus.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/drop-add.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/folder-open.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/leaf.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/elbow-end-minus.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/elbow.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/loading.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/folder.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/elbow-end-minus-nl.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/drop-between.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/elbow-plus-nl.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/elbow-end.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/drop-no.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/elbow-line.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/drop-yes.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/arrows.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/drop-over.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/elbow-plus.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/elbow-end-plus-nl.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/s.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/elbow-minus.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/elbow-minus-nl.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tree/drop-under.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/wait.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/hmenu-lock.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/group-expand-sprite.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/grid3-hrow-over.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/hmenu-desc.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/row-over.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/col-move-top.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/dirty.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/page-next.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/page-first-disabled.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/sort_asc.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/sort-hd.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/done.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/grid-split.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/loading.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/grid-blue-split.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/page-first.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/grid3-rowheader.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/columns.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/hd-pop.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/footer-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/page-last.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/sort_desc.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/page-last-disabled.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/hmenu-unlock.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/group-expand.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/refresh.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/hmenu-asc.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/invalid_line.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/grid3-hrow.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/pick-button.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/grid-loading.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/drop-no.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/hmenu-unlock.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/drop-yes.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/row-expand-sprite.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/grid-blue-hd.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/col-move-bottom.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/group-collapse.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/row-sel.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/row-check-sprite.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/mso-hd.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/page-prev-disabled.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/grid-hrow.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/page-prev.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/page-next-disabled.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/hmenu-lock.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/arrow-right-white.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/grid3-special-col-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/grid3-hd-btn.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/grid-vista-hd.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/refresh-disabled.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/group-by.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/nowait.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/arrow-left-white.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/grid/grid3-special-col-sel-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/sizer/ne-handle.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/sizer/se-handle-dark.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/sizer/e-handle.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/sizer/e-handle-dark.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/sizer/sw-handle-dark.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/sizer/s-handle.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/sizer/se-handle.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/sizer/ne-handle-dark.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/sizer/nw-handle.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/sizer/nw-handle-dark.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/sizer/sw-handle.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/sizer/square.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/sizer/s-handle-dark.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/button/s-arrow-b-noline.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/button/arrow.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/button/group-tb.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/button/s-arrow-b.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/button/s-arrow.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/button/btn.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/button/group-cs.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/button/s-arrow-o.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/button/s-arrow-bo.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/button/group-lr.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/button/s-arrow-noline.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/layout/stick.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/layout/stuck.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/layout/ns-collapse.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/layout/mini-top.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/layout/panel-title-light-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/layout/expand.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/layout/mini-bottom.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/layout/panel-title-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/layout/ns-expand.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/layout/panel-close.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/layout/tab-close.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/layout/gradient-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/layout/mini-right.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/layout/mini-left.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/layout/tab-close-on.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/layout/collapse.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tabs/tab-btm-right-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tabs/tab-btm-over-right-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tabs/tab-btm-inactive-right-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tabs/tab-strip-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tabs/tab-btm-inactive-left-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tabs/tab-close.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tabs/tabs-sprite.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tabs/tab-btm-left-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tabs/tab-strip-btm-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tabs/scroll-left.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tabs/tab-strip-bg.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tabs/scroll-right.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tabs/tab-btm-over-left-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/tabs/scroller-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/editor/tb-sprite.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/progress/progress-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/menu/unchecked.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/menu/item-over.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/menu/menu.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/menu/menu-parent.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/menu/checked.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/menu/group-checked.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/qtip/bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/qtip/tip-anchor-sprite.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/qtip/close.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/qtip/tip-sprite.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/toolbar/gray-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/toolbar/tb-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/toolbar/tb-xl-sep.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/toolbar/btn-arrow.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/toolbar/bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/toolbar/tb-xl-btn-sprite.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/toolbar/more.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/toolbar/tb-btn-sprite.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/toolbar/btn-arrow-light.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/toolbar/btn-over-bg.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/slider/slider-v-thumb.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/slider/slider-thumb.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/slider/slider-v-bg.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/images/default/slider/slider-bg.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user_add.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/folder_wrench.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user_comment.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user_suit.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user_green.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/information.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/folder_go.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user_female.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user_add.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/book.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user_edit.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/plugin.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/error.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user_delete.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user_red.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/grid.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/connect.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/connect.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/control_rewind.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user_gray.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user_orange.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/cog_edit.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/feed_error.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/rss_go.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/feed_delete.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user_delete.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/add.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/plugin_add.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/application_view_list.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/add.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user_female.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/SILK.txt'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/table_refresh.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/feed_add.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user_green.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/image_add.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/accept.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/cross.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/user_suit.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/delete.gif'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/application_go.png'\nCopying '/opt/graphite/webapp/content/js/ext/resources/icons/fam/cog.png'\nCopying '/opt/graphite/webapp/content/js/ext/ux/DataViewTransition.js'\nCopying '/opt/graphite/webapp/content/css/dashboard-white.css'\nCopying '/opt/graphite/webapp/content/css/table.css'\nCopying '/opt/graphite/webapp/content/css/dashboard-default.css'\nCopying '/opt/graphite/webapp/content/css/darkX.css'\nCopying '/opt/graphite/webapp/content/css/default.css'\nCopying '/opt/graphite/webapp/content/css/dashboard.css'\nCopying '/opt/graphite/webapp/content/css/default/maximize.gif'\nCopying '/opt/graphite/webapp/content/css/default/minimize.gif'\nCopying '/opt/graphite/webapp/content/css/default/top_left.gif'\nCopying '/opt/graphite/webapp/content/css/default/center_right.gif'\nCopying '/opt/graphite/webapp/content/css/default/overlay.png'\nCopying '/opt/graphite/webapp/content/css/default/resize.gif'\nCopying '/opt/graphite/webapp/content/css/default/bottom_mid.gif'\nCopying '/opt/graphite/webapp/content/css/default/top_mid.gif'\nCopying '/opt/graphite/webapp/content/css/default/top_right.gif'\nCopying '/opt/graphite/webapp/content/css/default/bottom_right.gif'\nCopying '/opt/graphite/webapp/content/css/default/sizer.gif'\nCopying '/opt/graphite/webapp/content/css/default/inspect.gif'\nCopying '/opt/graphite/webapp/content/css/default/clear.gif'\nCopying '/opt/graphite/webapp/content/css/default/close.gif'\nCopying '/opt/graphite/webapp/content/css/default/bottom_right_resize.gif'\nCopying '/opt/graphite/webapp/content/css/default/bottom_left.gif'\nCopying '/opt/graphite/webapp/content/css/default/center_left.gif'\nCopying '/opt/graphite/webapp/content/css/darkX/button-close-focused.png'\nCopying '/opt/graphite/webapp/content/css/darkX/frame-bottom-right-focused.png'\nCopying '/opt/graphite/webapp/content/css/darkX/frame-bottom-mid-focused.png'\nCopying '/opt/graphite/webapp/content/css/darkX/titlebar-right-focused.png'\nCopying '/opt/graphite/webapp/content/css/darkX/frame-left-focused.png'\nCopying '/opt/graphite/webapp/content/css/darkX/button-maximize-focused.png'\nCopying '/opt/graphite/webapp/content/css/darkX/frame-bottom-left-focused.png'\nCopying '/opt/graphite/webapp/content/css/darkX/titlebar-left-focused.png'\nCopying '/opt/graphite/webapp/content/css/darkX/frame-right-focused.png'\nCopying '/opt/graphite/webapp/content/css/darkX/titlebar-mid-focused.png'\nCopying '/opt/graphite/webapp/content/css/darkX/button-minimize-focused.png'\nCopying '/opt/graphite/webapp/content/img/graphite.png'\nCopying '/opt/graphite/webapp/content/img/refresh.png'\nCopying '/opt/graphite/webapp/content/img/trash.png'\nCopying '/opt/graphite/webapp/content/img/leaf.gif'\nCopying '/opt/graphite/webapp/content/img/move_up.png'\nCopying '/opt/graphite/webapp/content/img/upload.png'\nCopying '/opt/graphite/webapp/content/img/mini-bottom2.gif'\nCopying '/opt/graphite/webapp/content/img/calendar.png'\nCopying '/opt/graphite/webapp/content/img/share.png'\nCopying '/opt/graphite/webapp/content/img/overview.png'\nCopying '/opt/graphite/webapp/content/img/blank.gif'\nCopying '/opt/graphite/webapp/content/img/favicon.ico'\nCopying '/opt/graphite/webapp/content/img/graphite_short.png'\nCopying '/opt/graphite/webapp/content/img/mini-top2.gif'\nCopying '/opt/graphite/webapp/content/img/clock_16.png'\nCopying '/opt/graphite/webapp/content/img/move_down.png'\nCopying '/opt/graphite/webapp/content/img/carbon-fiber.png'\nCopying '/opt/graphite/webapp/content/img/graphite-logo.png'\nCopying '/opt/graphite/webapp/content/img/save.png'\nCopying '/opt/graphite/webapp/content/img/clock.png'\nCopying '/opt/graphite/webapp/content/html/searchHelp.html'\nCopying '/opt/graphite/webapp/content/html/completerHelp.html'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/urlify.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/collapse.min.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/SelectFilter2.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/prepopulate.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/popup_response.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/timeparse.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/actions.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/actions.min.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/inlines.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/SelectBox.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/jquery.init.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/calendar.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/change_form.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/cancel.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/prepopulate.min.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/core.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/collapse.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/prepopulate_init.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/inlines.min.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/vendor/jquery/jquery.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/vendor/jquery/LICENSE-JQUERY.txt'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/vendor/jquery/jquery.min.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/vendor/xregexp/xregexp.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/vendor/xregexp/xregexp.min.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/vendor/xregexp/LICENSE-XREGEXP.txt'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/admin/DateTimeShortcuts.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/js/admin/RelatedObjectLookups.js'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/css/widgets.css'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/css/base.css'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/css/forms.css'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/css/fonts.css'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/css/login.css'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/css/rtl.css'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/css/dashboard.css'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/css/changelists.css'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/icon-calendar.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/search.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/icon-no.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/calendar-icons.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/icon-addlink.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/icon-yes.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/inline-delete.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/LICENSE'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/icon-clock.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/icon-deletelink.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/tooltag-add.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/icon-alert.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/icon-unknown-alt.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/icon-changelink.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/README.txt'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/selector-icons.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/icon-unknown.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/tooltag-arrowright.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/sorting-icons.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/gis/move_vertex_on.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/img/gis/move_vertex_off.svg'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/fonts/Roboto-Regular-webfont.woff'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/fonts/LICENSE.txt'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/fonts/Roboto-Light-webfont.woff'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/fonts/README.txt'\nCopying '/usr/local/lib64/python2.7/site-packages/django/contrib/admin/static/admin/fonts/Roboto-Bold-webfont.woff'\n409 static files copied to '/opt/graphite/static'.\n[ec2-user@ip-172-17-68-79 ~]$ PYTHONPATH=/opt/graphite/webapp django-admin.py migrate --settings=graphite.settings --run-syncdb\n/opt/graphite/webapp/graphite/settings.py:332: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security\n  warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\nOperations to perform:\n  Synchronize unmigrated apps: browser, composer, functions, metrics, render, staticfiles, whitelist\n  Apply all migrations: account, admin, auth, contenttypes, dashboard, events, sessions, tagging, tags, url_shortener\nSynchronizing apps without migrations:\n  Creating tables...\n    Running deferred SQL...\nRunning migrations:\n  No migrations to apply.\n[ec2-user@ip-172-17-68-79 ~]$ /usr/local/bin/go-carbon -config /usr/local/etc/go-carbon.conf -daemon\n[ec2-user@ip-172-17-68-79 ~]$ nohup /opt/graphite/bin/run-graphite-devel-server.py /opt/graphite > graphite1.out &\n[1] 18902\nI tried to hit the URL on the browser, I got 404.\n[ec2-user@ip-172-17-68-79 ~]$ tail -f graphite.out\n/opt/graphite/webapp/graphite/settings.py:332: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security\n  warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\n/opt/graphite/webapp/graphite/settings.py:332: UserWarning: SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security\n  warn('SECRET_KEY is set to an unsafe default. This should be set in local_settings.py for better security')\n[16/Dec/2018 16:22:18] \"GET / HTTP/1.1\" 200 598\n[16/Dec/2018 16:22:18] \"GET /favicon.ico HTTP/1.1\" 404 85\n[16/Dec/2018 16:22:18] \"GET /browser/header HTTP/1.1\" 200 1167\n[16/Dec/2018 16:22:18] \"GET /composer? HTTP/1.1\" 200 1643\n[16/Dec/2018 16:22:18] \"GET /static/js/ext/resources/css/ext-all.css HTTP/1.1\" 404 113\n[16/Dec/2018 16:22:18] \"GET /static/img/carbon-fiber.png HTTP/1.1\" 404 101\n[16/Dec/2018 16:22:18] \"GET /static/img/graphite-logo.png HTTP/1.1\" 404 102\n[16/Dec/2018 16:22:18] \"GET /static/js/ext/adapter/ext/ext-base.js HTTP/1.1\" 404 111\n[16/Dec/2018 16:22:18] \"GET /static/js/browser.js HTTP/1.1\" 404 94\n[16/Dec/2018 16:22:18] \"GET /static/js/ext/ext-all.js HTTP/1.1\" 404 98\n[16/Dec/2018 16:22:18] \"GET /static/js/composer_widgets.js HTTP/1.1\" 404 103\n[16/Dec/2018 16:22:18] \"GET /static/js/composer.js HTTP/1.1\" 404 95\n[16/Dec/2018 16:22:18] \"GET /static/js/completer.js HTTP/1.1\" 404 96\n[16/Dec/2018 16:22:18] \"GET /static/js/browser.js HTTP/1.1\" 404 94\n[16/Dec/2018 16:22:19] \"GET /static/js/composer_widgets.js HTTP/1.1\" 404 103\n[16/Dec/2018 16:22:19] \"GET /static/js/composer.js HTTP/1.1\" 404 95\n[16/Dec/2018 16:22:19] \"GET /static/js/completer.js HTTP/1.1\" 404 96\n``\n. Hi @piotr1212 , I'm getting the same error when starting the django server withdjango-admin.py runserver --pythonpath /opt/graphite/webapp --settings graphite.settings 0.0.0.0:8080`. @deniszh \nI restarted nginx.\n[ec2-user@ip-172-17-68-151 nginx]$ sudo service nginx stop\nStopping nginx:                                            [  OK  ]\n[ec2-user@ip-172-17-68-151 nginx]$ sudo service nginx start\nStarting nginx:                                            [  OK  ]\nLogs\n[ec2-user@ip-172-17-68-151 nginx]$ ls -l\ntotal 8\n-rw-r--r-- 1 root     root     1309 Jan 25 02:14 access.log\n-rw-r--r-- 1 root     root      258 Jan 25 02:10 error.log\n-rw-r----- 1 ec2-user ec2-user    0 Jan 24 06:28 graphite.access.log\n-rw-r----- 1 ec2-user ec2-user    0 Jan 24 06:28 graphite.error.log\n[ec2-user@ip-172-17-68-151 nginx]$ cat access.log\n172.31.1.238 - - [25/Jan/2019:02:10:10 +0000] \"GET / HTTP/1.1\" 200 3770 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\"\n172.31.1.238 - - [25/Jan/2019:02:10:10 +0000] \"GET /nginx-logo.png HTTP/1.1\" 200 368 \"http://172.17.68.151/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\"\n172.31.1.238 - - [25/Jan/2019:02:10:11 +0000] \"GET /poweredby.png HTTP/1.1\" 200 3412 \"http://172.17.68.151/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\"\n172.31.1.238 - - [25/Jan/2019:02:10:11 +0000] \"GET /favicon.ico HTTP/1.1\" 404 3696 \"http://172.17.68.151/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\"\n172.31.1.238 - - [25/Jan/2019:02:14:16 +0000] \"GET / HTTP/1.1\" 304 0 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\"\n172.31.1.238 - - [25/Jan/2019:02:14:17 +0000] \"GET / HTTP/1.1\" 304 0 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36\" \"-\"\n[ec2-user@ip-172-17-68-151 nginx]$ ls\naccess.log  error.log  graphite.access.log  graphite.error.log\n[ec2-user@ip-172-17-68-151 nginx]$ cat error.log\n2019/01/25 02:10:11 [error] 49725#0: *2 open() \"/usr/share/nginx/html/favicon.ico\" failed (2: No such file or directory), client: 172.31.1.238, server: localhost, request: \"GET /favicon.ico HTTP/1.1\", host: \"172.17.68.151\", referrer: \"http://172.17.68.151/\"\nnginx config\n```\n[ec2-user@ip-172-17-68-151 nginx]$ cat /etc/nginx/sites-available/graphite\nupstream graphite {\n    server 127.0.0.1:8080 fail_timeout=0;\n}\nserver {\n    listen 80 default_server;\n172.17.68.151 HOSTNAME;\n\nroot /opt/graphite/webapp;\n\naccess_log /var/log/nginx/graphite.access.log;\nerror_log  /var/log/nginx/graphite.error.log;\n\nlocation = /favicon.ico {\n    return 204;\n}\n\n# serve static content from the \"content\" directory\nlocation /static {\n    alias /opt/graphite/webapp/content;\n    expires max;\n}\n\nlocation / {\n    try_files $uri @graphite;\n}\n\nlocation @graphite {\n    proxy_pass_header Server;\n    proxy_set_header Host $http_host;\n    proxy_redirect off;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Scheme $scheme;\n    proxy_connect_timeout 10;\n    proxy_read_timeout 10;\n    proxy_pass http://graphite;\n}\n\n}\n```\nAnd I see this on the UI now.\n\n. @deniszh Sure, let me check. Thanks. ",
    "TinajaLabs": "BTW, I've also researched other references to this problem.\nI've tried chown variations.\nI've set different users and group combinations of root, graphite, and www-data.\nI ran this:\n```\nPYTHONPATH=/opt/graphite/webapp django-admin.py migrate --settings=graphite.settings --run-syncdb\nOperations to perform:                                                                                                                                                                                                               \n  Synchronize unmigrated apps: browser, composer, functions, metrics, render, staticfiles, whitelist                                                                                                                                 \n  Apply all migrations: account, admin, auth, contenttypes, dashboard, events, sessions, tagging, tags, url_shortener                                                                                                                \nSynchronizing apps without migrations:\n  Creating tables...\n    Running deferred SQL...\nRunning migrations:\n  No migrations to apply.\n```\nStill no joy.... hmm - it looks like this as shown above:\n-rwxrwxr-x 1 www-data www-data 311296 Dec  9 21:34 graphite.db\nwww-data is the apache user on Raspbian (Debian for Raspberry Pi).  Does the directory need these settings also?\nThank you,\nChris.\n. -rwxrwxr-x 1 graphite www-data 311296 Dec  9 21:34 /opt/graphite/storage/graphite.db\n-rwxrwxr-x 1 www-data graphite 311296 Dec  9 21:34 /opt/graphite/storage/graphite.db\nran systemctl restart apache2 after each change (just in case)  but same issue...\n. Oh! Oh!  Almost there...\nthe storage dir:\ndrwxrwxr-x 7 graphite www-data 4096 Dec  9 23:27 storage\nThe data file:\n-rwxrwxr-x 1 graphite www-data 311296 Dec 10 01:45 /opt/graphite/storage/graphite.db\nNow I see the header of the graphite web page, but not the rest.  I think this is the most tenacious install I've ever tried... ;)\nAny thoughts on the problem of seeing only the header but not the list of whisper databases and the default chart?\n. Looking at the graphite log, I see a dozen or so entries using this:\n# tail -f /opt/graphite/storage/log/webapp/error.log\nclient denied by server configuration: /opt/graphite/static, referer: http://tinaja-gr:8080/composer?\nI'm not sure about the reference to composer; I'm using the URL: http://tinaja-gr:8080\n. OK.  Excellent.  I see the section on WSGIScriptAlias in the example I have is all commented out.\n```\nWSGIScriptAlias /graphite /srv/graphite-web/conf/graphite.wsgi/graphite\nAlias /graphite/static /opt/graphite/webapp/content\n\nSetHandler None\n\n```\nI'll rework that and see how it goes.\nThank you,\nChris. FINALLY!\nThe static attributes required that I run this:\nPYTHONPATH=/opt/graphite/webapp django-admin.py collectstatic --noinput --settings=graphite.settings\nMy current vhost file:\n```\n/etc/httpd/conf.d/graphite-vhost.conf\nLoadModule wsgi_module \"/usr/lib/apache2/modules/mod_wsgi-py27.so\"                                                                                                                                                                                \nWSGISocketPrefix /var/run/wsgi\nListen 8080\n\nServerName graphite\nDocumentRoot \"/opt/graphite/webapp\"\nErrorLog /opt/graphite/storage/log/webapp/error.log\nCustomLog /opt/graphite/storage/log/webapp/access.log common\n\nWSGIDaemonProcess graphite-web processes=5 threads=5 display-name='%{GROUP}' inactivity-timeout=120\nWSGIProcessGroup graphite-web\nWSGIApplicationGroup %{GLOBAL}\nWSGIImportScript /opt/graphite/conf/graphite.wsgi process-group=graphite-web application-group=%{GLOBAL}\n\nWSGIScriptAlias / /opt/graphite/conf/graphite.wsgi\n\nAlias /static/ /opt/graphite/static/\n\n<Directory /opt/graphite/static/>\n        <IfVersion < 2.4>\n                Order deny,allow\n                Allow from all\n        </IfVersion>\n        <IfVersion >= 2.4>\n                Require all granted\n        </IfVersion>\n</Directory>\n\n<Directory /opt/graphite/conf/>\n        <IfVersion < 2.4>\n                Order deny,allow\n                Allow from all\n        </IfVersion>\n        <IfVersion >= 2.4>\n                Require all granted\n        </IfVersion>\n</Directory>\n\n\n```\nThank you, @deniszh , for helping me over the finish line.... ",
    "pumbaacave": "Thank you! Very happy to hear that! . ",
    "Siddhu1096": "Fixed. You can close the issue. ",
    "blesaffre": "I am talking about timeFunction.\nI already asked on Grafana repo, and was told \"Probably a graphite issue so closing for now\"\nThe issue is that when I use the mentionned above function on any metric, I get an Internal Server Error, and the graph stops displaying.\nWhen I look for more information about this error in the network tab, I can see \nTypeError | \u00a0\n-- | --\ntimeFunction() takes at most 3 arguments (4 given)\nwhile I only pass 1 and, according to the documentation, it takes a \"name\" parameter, and an optionnal \"step\".\n. Yes I know that, but that is the problem, because I am calling the function like that :\ntimeFunction(\"The.time.series\")\nand not as it is displayed in the error.\nIn the error, the first parameter you see is the metric I apply the function to, not the parameter I give to it.. Awesome I didn't know about that.\nIt seems to work however. Not exactly what I want but I need to change a few things.\nWhat I am trying to achieve is to get the time at which 2 metrics are received to compare delay between them.\nSo right now what I get, using your example \"alias(time(stats.sets.stats.kamailio.marek.customer.registration.200.count),'blah')\" and replacing \"blah\" with what I want to be displayed, is the timestamp of the metric given to time().. ",
    "moozhub": "Thanks for the reply. Didn't see the requirements there. Appreciate it.. ",
    "aussiearef": "Thanks @deniszh \nNo offense. I have been a Windows user for 20 years hence struggling with Ubuntu. I would like to install Graphite myself rather than using Docker for some reasons so please bear with me.\nI will try your solution, but I am pretty sure I granted permissions to /var as well. \nOne question. Is it the Apache user that runs carbon-cache or is it the user who started the service?. Thanks @deniszh \nSo what I have done is:\n1- Launch a fresh Ubuntu 18.04 on AWS\n2- Update, upgrade and reboot\n3- sudo apt-get install graphite-carbon graphite-web\n4- sudo apt-get install sqlite3\n5- sudo nano /etc/graphite/local_settings.pt   ,  then change the secret key, and time zone (not db)\n6- sudo graphite-manage migrate auth (just in case models need sync)\n7- sudo nano /etc/default-graphite-carbon  (and enable carbon cache)\n8- sudo systemctl start carbon-cache\n9- sudo systemctl enable carbon-cache\n10- sudo apt-get install apache2\n11-sudo apt-get install libapache2-mod-wsgi\n12-sudo cp /usr/share/graphite-web/apache2-graphite.conf  /etc/apache2/sites-available\n13- sudo a2dissite 000-default\n14- sudo a2ensite apache2-graphite\n15- sudo systemctl stop apache2    +  sudo systemctl start apache2\nAfter this if I navigate to http:// I will get HTTP 500\nLooking at /var/log/apache2/graphite-web_error.log I always see a permission denied error on /var/log/graphite/info.log.\nUsing chown and chomod then rebooting the whole machine I will get rid of the permission denied error. But then after that I always have this:\nFile \"/usr/share/graphite-web/graphite.wsgi\", line 14, in \n     application = get_wsgi_application()\n File \"/usr/lib/python2.7/dist-packages/django/init.py\", line 27, in setup\napps.populate(settings.INSTALLED_APPS)\nI have tried this a good 7 or 8 times, with both sqlite3 and mysql!\nI used to install graphite with no problem at all but since the recent updates it seems that something has changed! Do you have any clue as to what is not wright?\nLooking at the logs python2.7 is being used.\nCheers\n. @piotr1212 \nAlso :\nI executed \n\"ps -ef | grep carbon\"\nWhich reported:\n\nI have a user called \"ubuntu\" which is the default user in AWS Ubuntu . But there is no user called \"_graphite\". Is this user created by Graphite??  Do I have to create it?!!\n. @deniszh \nSo I have made some progress:\n1- The problem was that the user \"_graphite\" had to have access to info.log and exception.log and now www-data user. So I granted access and then I could see some proper errors in the logs.\n2- There is an exception raised by Django saying \"context must be dict not Context\". It is because \"Context is deprecated but it is used in views.py file. I edited this file and removed the Context and so this error went away too. (I see that this is already fixed in master\n3- Now I get this error although I have run \"sudo graphite-manage migrate auth\" command.\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/django/core/handlers/exception.py\", line 41, in inner\n    response = get_response(request)\n  File \"/usr/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 249, in _legacy_get_response\n    response = self._get_response(request)\n  File \"/usr/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 187, in _get_response\n    response = self.process_exception_by_middleware(e, request)\n  File \"/usr/lib/python2.7/dist-packages/django/core/handlers/base.py\", line 185, in _get_response\n    response = wrapped_callback(request, callback_args, callback_kwargs)\n  File \"/usr/lib/python2.7/dist-packages/graphite/composer/views.py\", line 35, in composer\n    profile = getProfile(request)\n  File \"/usr/lib/python2.7/dist-packages/graphite/user_util.py\", line 25, in getProfile\n    return default_profile()\n  File \"/usr/lib/python2.7/dist-packages/graphite/user_util.py\", line 44, in default_profile\n    profile, created = Profile.objects.get_or_create(user=user)\n  File \"/usr/lib/python2.7/dist-packages/django/db/models/manager.py\", line 85, in manager_method\n    return getattr(self.get_queryset(), name)(*args, kwargs)\n  File \"/usr/lib/python2.7/dist-packages/django/db/models/query.py\", line 464, in get_or_create\n    return self.get(*lookup), False\n  File \"/usr/lib/python2.7/dist-packages/django/db/models/query.py\", line 374, in get\n    num = len(clone)\n  File \"/usr/lib/python2.7/dist-packages/django/db/models/query.py\", line 232, in len\n    self._fetch_all()\n  File \"/usr/lib/python2.7/dist-packages/django/db/models/query.py\", line 1118, in _fetch_all\n    self._result_cache = list(self._iterable_class(self))\n  File \"/usr/lib/python2.7/dist-packages/django/db/models/query.py\", line 53, in iter\n    results = compiler.execute_sql(chunked_fetch=self.chunked_fetch)\n  File \"/usr/lib/python2.7/dist-packages/django/db/models/sql/compiler.py\", line 899, in execute_sql\n    raise original_exception\nProgrammingError: (1146, u\"Table 'graphite.account_profile' doesn't exist\")\n. Never mind I fixed it\nJust so if anyone else faces this problem, the main issue is that for a new db setup  you need to run:\nPYTHONPATH=$GRAPHITE_ROOT/webapp\ncd /usr/bin \nsudo django-admin migrate --settings=graphite.settings --run-syncdb\nThis is explained HERE\nI did not run syncdb and only run migrate because almost everywhere in the Internet it is said that since the deprecation of syncdb, migrate will do the database migration!. ",
    "pszafer": "Don't know what was wrong then. I rewrote my role and it is working now.\nIf you want you can copy it to your docs, maybe it will help somebody to have graphite working in a few minutes.\nRepo with ansible role:\nhttps://github.com/pszafer/graphite-web-ansible. ",
    "fuzzball5000": "Thanks @deniszh, although I tried 1.8 with similar results, syncdb works when forced to use a PYTHONPATH which has an older Django installed. I think we've narrowed the issue down to graphite picking up a newer Django from somewhere in the path - will attempt to run it up in a virtualenv. . ",
    "azhiltsov": "\nwhat are the downsides to carbonserver compared to our current cluster method? do we lose any functionality or desirable characteristics?\n\nCarbonserver has nothing to do with clusters. It just returning metrics based on a query.\nClusterization is responsibility of carbonzipper on render side and carbon-c-relay on ingest side.. Most of statsd implementations can't provide you a redundant distributed aggregations, the only one I know which give a such promise is https://github.com/avito-tech/bioyino \nAnother pain point is a lack of delivery acknowledgement in relay protocol. I would rather replace it with GRPC/HTTP/whatever and provide a backward compatibility on a client-facing side.. ",
    "sbancal": "@deniszh \nYou're right, running collectstatic creates that missing folder. I was running all side of the doc ... but didn't see that one ...\nI was talking of the lines 79 and 257 in that same file.. ",
    "shirishkale": "Let me further expand on my issue.\nI have a POC machine on which I have a 4 node causal cluster, 3 core + 1 read replica. I have followed instructions to install Carbon followed by whisper as well as graphite-web. Once the neo4j causal cluster is started and carbon startup. I see 4 directories populated under \"/data/graphite/storage/whisper\" named as core1,core2,core3 and readrep1 specifying 4 node metrics captured by prefix which I have added in the neo4j.conf for the respective nodes of my causal cluster.\nThis was my proof that Carbon and whisper seems to be working? Is there any reason that should not be the case? \nSimilarly I have installed graphite web, and as per the URL specified when I go there I see this blank graphite website.\n\nAs per the installation instructions this should have displayed me my metrics but it does not.. No. But when I try to run, it complains there is no module like graphite.settings.\ndjango-admin.py collectstatic --noinput --settings=graphite.settings\n. > Then your Pythonpath is broken\nOnce I fixed the PYTHONPATH and ran collectstatic. Are there any other steps. I restarted graphite-web after collectstatic and still there is no metrics displayed in graphite-web\n. Thanks Guys. After installing whitenoise, I am able to see the graphs. Thanks for your help\n. I just uninstalled everything and reinstalled it with the same steps but now in spite of having whitenoise I can not see any data in the graph. In fact the metric is empty even when I can see the files in /data/graphite/storage/whisper.\nHow to fix this?. I am also getting \"WARNING: whisper module could not be loaded, whisper support disabled\"\nin webapp process.log which was not coming before but now it has started.. This is related to issue #2427 . Installing Django again versio 1.11.20 ran the graphite server but the error log file still has\nWARNING: whisper module could not be loaded, whisper support disabled\nand no metrics is getting displayed in graphite-web.. Yes. No use. Still the same error message\n. All components of graphite were installed 3 times.\nFirst Round\n\nAll Carbon, Whisper and Graphite were installed using the downloaded zip files version 1.1.5.\nUsing python setup.py install --prefix=  --install-lib=\nIt installed fine except django-admin.py was not under /data/graphite directory where the install were done but in /usr/lib/python2.7/site-packages/Django-1.11.16-py2.7.egg/django/bin/\nNeo4j metrics was captured and graphs were correctly displayed.\n\nI uninstalled everything and followed the same procedure to have a repeatable process\nSecond Round\n\nInstall went fine. All components loaded correctly\nThe error for Not able load Whisper started and no metrics or graphs were displayed\nI went ahead and uninstalled anything\n\nTo fix the issue of not having django-admin.py under PYTHONPATH all the three components were reinstalled using a different method.\nRound Three \n\nInstall Carbon, Whisper and Graphite-web using \"pip install https://github.com/graphite-project/graphite-web/tarball/master\" --prefix=  --install-lib=\nInstall went ok but ran into issues with Django\nREinstalled Django. Noe django -admin.py is under \"/data/graphite\" where graphite is installed\nStill the same error\n\nHence I am not sure why it worked the first time and will not work the last two times.\nAll this while my python binaries are under \"/usr/bin/python\".\nHence can Graphite be installed under any custom directory or has to be under \"/usr/bin/python\"\npath somewhere?\nIf the install worked the first time then what could create issues for Python suddenly?\n. The following line complains about not able to load whisper module\ndjango-admin.py runserver --pythonpath /data/graphite/webapp --settings graphite.settings 0.0.0.0:80\nWhat could be wrong with pythonpath when this command explicitly mentions the PYTHONPATH?\n. No. Whisper module is installed in \"/data/graphite\" libraries in \"/data/graphite/lib\".\nGraphite web is installed in \"/data/graphite\" libraries in \"/data/graphite/webapp\"\nWhen I try to start the graphite-web, then the runserver commands of graphite web complains that it can not load whisper module.. Only whisper and carbon are installed in /data/graphite/lib. Graphite-Web is in webapp? Should I change the install location for graphite-web also to \"/data/graphite/lib\"?. Does that mean graphite-web will not work correctly if I install it into a non-default location? Because even if I install everything in /data/graphite even then graphite-web complains that Whisper module can not be loaded. Which means there is no option but to install it in /opt/graphite. Is that so?. Any Path I use its the same message can not load whisper module? Where exactly it is looking for Whisper module?\n. ",
    "dumbdonkey": "hi @deniszh , I've re-edited my question, hope that is clear now. . @deniszh  most of the time,  yesterday series has the same length as current ones. what if we lost some ages in yesterday series, but they do exist in current ones.  then they have different length, divideSeriesLists will give a wrong answer.\neg:\nyesteday:  age=0  [10], age=30 [50], age=50 [60]\ncurrent:     age=0  [10], age=10 [20], age=30 [50]\nsince we lost \"age=10\" in yesterday, divideSeriesLists will try to divide \"age=10\" by \"age=30\", which is not expected.\n. ",
    "JustinVenus": "this line should be \n      \"'NAME': '',\n. your change isn't python2.4 compatible consider this as an alternative\ndict(seconds=100,   minorGridUnit=HOUR, minorGridStep=2,  majorGridUnit=HOUR, majorGridStep=4,  labelUnit=HOUR, labelStep=4,  format=percent_l_supported and \"%a %l%p\" or \"%a %I%p\", maxInterval=6*DAY)\n. Same issue as my previous comment.\n. Fair warning, the way that exception block is written won't work in python3\nPython 3.2.3 (default, Jun  8 2012, 05:36:09) \n[GCC 4.7.0 20120507 (Red Hat 4.7.0-5)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\ntry:\n...    raise ValueError(\"foo\")\n... except ValueError,e:\n  File \"\", line 3\n    except ValueError, e:\n                     ^\nSyntaxError: invalid syntax\n. \n\n\n",
    "zr40": "Might want to remove this print :)\n. ",
    "ifdattic": "Makes sense. Should I change it to it?\n. ",
    "tarnfeld": "This function urlopen is not imported in the file.\n. ",
    "malkowski": "Man, that sticks out like a sore thumb now that you point it out. I normally go the other direction with everything in double quotes, so I know the importance of small details like this. Consistency in code is under-valued IMO.\n. ",
    "Simage": "Probably a bit, But since we're talking relative times on the y axis it \nshould be good enough. I guess it could use weeks rather than months, \nunless you have an alternate suggestion..\nOn 15-07-08 11:10 PM, Jeff Schroeder wrote:\n\nIn webapp/graphite/render/glyph.py \nhttps://github.com/graphite-project/graphite-web/pull/1220#discussion_r34224084:\n\n@@ -126,6 +126,19 @@\n     ('G', 1000.03),\n     ('M', 1000.02),\n     ('K', 1000.0   )),\n-  'sec': (\n-    ('Y', 60_60_24365),\n-    ('M', 60_60_2430),\n-    ('D', 60_60_24),\n-    ('H', 60*60),\n-    ('m', 60)),\n-  'msec': (\n-    ('Y', 60_60_24_365_1000),\n-    ('M', 60_60_24_30_1000),\n\nIsn't that kind of cheating? Not all months are 30 days.\n\u2014\nReply to this email directly or view it on GitHub \nhttps://github.com/graphite-project/graphite-web/pull/1220/files#r34224084.\n. \n",
    "dnathe4th": "nitpick: bytes_used is what the example uses, not bytes_free.\n. ",
    "yati-sagade": "Can be a set (seenNames = set()) and then seenNames.add(series.name) :)\nEDIT: I'm just looking around in this project, and happened to see this PR.. ",
    "ngash": "According to [0] and [1], a blank line indicates the directive's content. I'm thinking blank line also includes lines with whitespace only.\n[0] http://www.sphinx-doc.org/en/stable/rest.html#directives\n[1] http://docutils.sourceforge.net/docs/ref/rst/restructuredtext.html#directives. ",
    "micolous": "I think this means that data can still be accessed, just the standard web interface is unavailable.  Is there a way to protect these endpoints too?. "
}
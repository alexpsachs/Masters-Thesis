{
    "gnawux": "Hi, bketelsen\nCould you stop the daemon and re-install hyper. Sorry for the inconvenience, there is one broken installation pack in the past hour, a qboot bios with a wrong filename in it.\nAnd we will provide the tarball directly on the homepage.\nThank you!\n. The direct download link has been put on readme: https://hyper-install.s3.amazonaws.com/hyper-latest.tgz\nCheers\n. Thank you philips\n. Hi @achanda ,\nyou need put them into a pod description json, there is some examples in the examples dir, such as this: https://github.com/hyperhq/hyper/blob/master/examples/multi-container.pod\n. Document updated, sorry for the inconsistency. @achanda \n. Hi @achanda, Good idea, but looks you committed your whole local workspace\n. Yes, will do soon, and we also need more test cases. Thanks @achanda \n. #85 added travis CI\n. hi @charliedrage , the hyper0 could configure in the config file,  /etc/hyper/config by default, it supports the following config items:\nBridge = hyper0\nBridgeIP = 192.168.123.1/24\ncurrently, only bridge is supported, more networking features in the future.\n. looks no further problems, close this issue\n. Hi @hustcat , looks there is a bug in fail processing routine. \nAs you are under CentOS 7, could you just try which qemu-system-x86_64. if it is not listed in your ${PATH}, add the qemu bin PATH (such as /usr/local/bin) into PATH.\nSorry for the panic.\n. no further progress in this issue, will close this.\n@hustcat hyper has many new commits and release since this issue was create, if you find any more problems, you can reopen this or create a new one.\n. hi @hustcat , sorry to say I am not sure what happened. \nBased on the log and code, it seems like the qemu process did not start in about 1 second. If you did not do this in a slow virtual machine, the suspicion should not right.\nCould you start the daemon with -v=3 flag to show a bit more information.\nThank you\n. hmm.... the latter case is not identical with the previous one.\nfor the previous one, looks the vm in vm is too slow, we will extends the wait time for unix socket, I think this could fix the problem.\nfor the latter one, it shows the kernel did not boot correctly, I will ask our init maintainer to check the kernel, and did you ever tried enable the Bios and Cbfs option in config file? the qboot should be even faster.\n. @PRADEEPKJ \nFor CentOS 7.x, could you remove all the hyper binaries under /usr/local/bin, and install the following prebuilt RPMs:\nx86_64 binary packages:\n\n\nhyper-0.4-2.el7.centos.x86_64.rpm\nhyperstart-0.4-2.el7.centos.x86_64.rpm\nqemu-hyper-2.4.1-2.el7.centos.x86_64.rpm\n. Thank you @oilbeater , this issue will be closed after eliminate all docker dependencies.\n. #84 fixed the instant run images\n. did you start hyperd service\nif it was started, find the log in /var/log/hyper/\nif not, start it first.\n\n\nAnd in your pod file, the containers.$.volumes field is a reference to the top level volume description. If you want to use volume, reference this example:\nhttps://github.com/hyperhq/hyper/blob/master/examples/with-volume.pod\nThank you\n. @qcbupt did you ever try edit /etc/resolv.conf to set a nameserver? such as\nnameserver 114.114.114.114\n. Hi @holidayworking \nWe have located the root cause, it is a filesystem issue, and we will soon push a work around.\n. @holidayworking we have pushed a bug fix version to solve this issue, you can found it here: https://docs.hyper.sh/get_started/install.html\nThe root cause is the database init script script used multi-line cat (<<), and it is broken by the virtfs. After investigating for a weekend, we found a clean and simple solution. It looks good in our tests. \n. @ayufan I think in most of the cases, you can directly run docker image with hyper. And I'd like to learn your case, why you want to run docker in hyper.\nAnd actually, you can use your own kernel in the config file, and reference our kernel configuration . PS: if you want use a customized kernel, you need disable the Cbfs config item, or build a new cbfs rom (reference our make cbfs code), and I will add a document page on build your own kernel/initrd.\n. @ayufan the volumes work in current version, and if you meet any problems, you can file an issue.\nI do not think it will take too much time to boot with a few more options :), just curious about the scenario. If it is a common case for many users, the features should be add to the release package.\n. True. Thanks for your case @ayufan . \nWe will do further investigation, and maybe have all these options in next release.\n. @junoyoon I think you should update hyperhq/hyperstart to the latest master.\nCould you confirm this, @gao-feng ?. Hi @erictune ,\nYes, there is some resource limit, quota, and QoS issue to solve, and cgroup is one of the frequently used methods. We are preparing to integrate Hyper with kubernetes and other DCOS, and will introduce those resource restriction methods then.\nThank you.\n. Hi @halacs \nThe when use libvirt as a hypervisor driver, we could use the functionalities of it. However, we have not integrated such codes in the current code.. @resouer done\n. hi @shimiaofeng ,\nAs you did not provide any further info, I can not judge what's the matter.\nIf you do not have ubuntu image in your host, it may be downloading the image from docker hub. In the next release, hyper will provide friendly message about on going jobs. If you think you met other problems, you'd better paste logs here.\nThank you.\n. hi @shimiaofeng \nI read the log message, looks like the qemu in your machine did not start properly, could you give the version of your qemu. And if you built qemu by yourself, could you give your qemu configuration?\nThank you!\n. hi @shimiaofeng ,\nNo, default qemu should works well, but I suggest you check whether the virtfs is enabled (--enable-virtfs). Actually we had tested hyper with self built qemu on CentOS, and did not specify any special flags.\nAnd you can have more detailed daemon log with hyperd -v=1 \n. @shimiaofeng , yes, you can find the configuration in doc  and this FAQ page. \nThank you\n. use \nhyper list container\nyou can get all the container IDs inside pod. then you can use  docker cp with the container. to avoid conflict with docker. hyper share the on disk container fs with docker.\n. @puresoul did your xen compiled with virtfs support \n./configure --with-extra-qemuu-configure-args=\"--enable-virtfs\"\n\nnote the double u, it should be double u for  xen 4.5\n\nreference this doc: https://docs.hyper.sh/trouble_shooting/xen.html\n. @puresoul panic is program fault definitely, but I can't tell if this is because of new behavior of xen 4.6, and will investigate it.\nfor the i686 and x86_64 problem, xen always use qemu-system-i386 by default. As xen does not rely on qemu for CPU emulation, this should not be caused by the qemu build target. I wonder whether this means you build xen itself for i686 target. PS: I just guess the reason, and I am not sure about xen itself.\n. The git repo is ahead of the binary. And we will look into the network issue\n. We will test with the new code and publish a new release later....\n. @puresoul \nas you are using a new version of hyper, you can use ./hyper run -t ubuntu /bin/bash, the -t flag for tty prompt. This may not relate with the problem, just a hint\n. @puresoul have you ever tried other images, such as busybox, nginx, etc.? I am afraid it is not pure network issue, might be low memory or something else.\n. close this issue, virtualbox support has been removed from the latest release. hi @a93ushakov \nWe have website at https://hyper.sh , and there are some comparisons in the FAQs page: https://hyper.sh/faq.html .\nHope these could give you some information.\n. @feiskyer does this problem still present in the HEAD of master?\n. Thank you @feiskyer \n@tuxknight This issue has been fixed in master branch, and we will prepare a new release including this and other fixes soon.\n. @tuxknight \nYour issue seems to be caused by centOS kernel issue ref https://github.com/docker/docker/issues/10294\nit is caused by CentOS 7.x + xfs backend fs + overlayfs storage engine, to work around this, use devicemapper instead. writing this line in configure file\nStorageDriver=devicemapper\n. @huikang \nfor centOS user, could you try the binary in this rpm\nhttps://s3.amazonaws.com/hyper-install/hyper-0.4-1.el7.centos.x86_64.rpm\nI was working in build hyper native packages in hyper VMs (PR: https://github.com/hyperhq/hyper/pull/135 ), this is the first distro package to build, and I need some tester to try it.\nPS: you need remove hyper and hyperd binary in /usr/local/bin, because rpm will put binary in /usr/bin\nThank you.\n. and the SRPM is here https://s3.amazonaws.com/hyper-install/hyper-0.4-1.el7.centos.src.rpm\n. pull request #105 should fix this\n. +1\nlooks good\n. I have tested the following cases:\n- install package in clean environment\n- update install\n- overwrite install\n- purge existing installation\n. @xiaods I agree with your point. @carmark will take the responsibility of the command line compatibility.\nThank you\n. @xiaods It has already been implemented in master branch. Uou will be able to use hyper pull next week, or build from source of master branch. \nThank you.\n. @carmark os is linux, and hypervisor is qemu/kvm, check the slack discussion :)\n. For images, hyper support devicemapper(as raw device), aufs and overlayfs (though virtfs), for volumes, qcow2, raw(as raw device), and vfs(though virtfs) are supported.\n@carmark please check the fs problem\n. @carmark we'd better make it an configurable parameter\n. #92 fixed this\n. but what's the problem in the example? the slash?\nI's the normal case a image name having a slash to separate the repo name and the image name. should we do a special process on slash?\n. @carmark \nI don't think users care about it either. However, I think we should do things consistently. If we use image name as the container name prefix, then we should use NNNNN as name prefix of image RRRR/NNNNN. Otherwise, we should never use image name as the prefix for all cases.\n. looks good\n. pr #72 replaced this one\n. is this ready to merge? I am not at home or office, can not do the test. @carmark @laijs \n. ref https://issues.apache.org/jira/browse/MESOS-3435\n. @xiaods I watched the issue. Feel free to file issue or submit pr to us.\none more thing, @feiskyer is working on implement a hyper driver for kubernetes in repo hyperhq/kubernetes, there might be something similar to mesos integration though they are written in different programming languages.\n. @laijs @carmark pleas confirm whether it is a bug of the code or my eyes. \n. when you compiled with 1.4 firstly, then switch to 1.5, this issue will caused a build error. And the patch is comes from docker.\n. can not build under go 1.5.1/ubuntu without this patch, could you review and test this patch? @carmark @laijs \n. this PR won't build success before https://github.com/hyperhq/runv/pull/33 got merged.\n. I have updated the patch to version 2, here is the explanation of this PR and https://github.com/hyperhq/runv/pull/33.\nBackground\nStart State Transition of VM in runV\nThere are three states are related with this PR:\n- For a VM, it is start when we call vm.Launch(), and in 'Init' state.\n- When vm.StartPod() is called, it get into Starting state, and it associated with a Pod in this state.\n- When hyperstart confirm the Pod is running, it is in Running state.\nIn the previous version, vm.Attach() can only be called after vm is associate with a Pod, i.e. it can not be invoked during Init state. The result is---\nWe can only call vm.Attach() after having called vm.StartPod(). which means the container command does not connected with a tty when it is starting, and if the command is a one-shot command such as ps, we can not get the output.\nAbout this patch\nIn runV\nWe enabled attach in Init state by add a buffer, which cache the tty connections before Pod is existing, and then plumb them when the Pod is initiating before the Pod starting.\nIn Hyper Daemon\nWhen start pod, if has TtyIO connected, it will call vm.Attach() before vm.StartPod()\nIn Hyper CLI\nHyper CLI will initial hijack stream for pod/start request if there need setup a tty connection.\n. @carmark ok, will update this\n. @carmark formatted \n. I think the hyper run will show the shell prompt in the ubuntu container.\n. @carmark the volume mapping should allow mapping both file and dir, please fix it\n. +1\n. the feature was implemented by #112 \n. Thank you @danielbodart , we will evaluate it first :)\n. @danielbodart yes, the original code does not return non-zero value for the error of all commands execution. Sorry for the inconvenient. \n. this issue is caused by \"got new command to init\" before getting ACK of last command from init. And it is introduced by the NEXT command. Before NEXT command was introduced, daemon won't send next command to init until it gets the ACK or ERROR. \nThis is a bug of runv, and I am finding a clean way to solve this.\n. this patch works well and convenient, I merge it\n. only a rename, I will merge it once the travis-ci passed\n. virtualbox 5.0.6 is out, need check the compatibility of it.\n. close this issue, virtualbox support has been removed from the latest release. o... it is failed as we merged hyperhq/runv#49 , more dependency are included\n. it can only pass the test if hyperhq/runv#49 is merged\n. updated by #112 \n. it can pass the test only if hyperhq/runv#49 is merged\n. and will push the godep update later, because the godep commit will make this github pr page unreadable.\n. will merge after ci pass\n. There is output chain listed here, and I guess this error might be caused by SELinux. (guess only)\n. @CMGS \nfor centOS user, could you try the binary in this rpm\nhttps://s3.amazonaws.com/hyper-install/hyper-0.4-1.el7.centos.x86_64.rpm\nI was working in build hyper native packages in hyper VMs (PR: https://github.com/hyperhq/hyper/pull/135 ), this is the first distro package to build, and I need some tester to try it.\nPS: you need remove hyper and hyperd binary in /usr/local/bin, because rpm will put binary in /usr/bin\nThank you.\n. and the SRPM is here https://s3.amazonaws.com/hyper-install/hyper-0.4-1.el7.centos.src.rpm , if you are interesting\n. @CMGS \nBased on our tests and user reports, the following hyper and qemu RPMs will fix hyper on CentOS, could you remove all the hyper binaries under /usr/local/bin, and install the following prebuilt RPMs:\nx86_64 binary packages:\n\n\nhyper-0.4-2.el7.centos.x86_64.rpm\nhyperstart-0.4-2.el7.centos.x86_64.rpm\nqemu-hyper-2.4.1-2.el7.centos.x86_64.rpm\n. @CMGS thank you for your reports :)\n. tested with the following PODs:\nexamples/service-discovery.pod: vol created by image, vol pass through virtfs\nexamples/file-mapping.pod: file mapping through virtfs\nexamples/insert-file.pod: insert file contents\nexamples/with-volume.pod: multiple vols mounted by multiple containers.\n. I think it's also need to close logger if startpod failed.\n. should change line 706 err := to err = in order to guarantee the defer behavior.\n. Listed in current API Doc. @zhengxiaochuan-3 , sorry for the inconvenient.\n\n\nwhat's the hyper version (build from source or downloaded binary)? And could you paste the log file in /var/log/hyper/  .\nThank you!\n. @zhengxiaochuan-3 weird, is there anything not normal in the configuration of this machine? I think your operating system and image are both tested. And can you see any error messages when start the qemu command line manually?\nqemu -machine pc-i440fx-2.0,accel=kvm,usb=off -global kvm-pit.lost_tick_policy=discard -cpu host -drive if=pflash,file=/var/lib/hyper/bios-qboot.bin,readonly=on -drive if=pflash,file=/var/lib/hyper/cbfs-qboot.rom,readonly=on -realtime mlock=off -no-user-config -nodefaults -no-hpet -rtc base=utc,driftfix=slew -no-reboot -display none -boot strict=on -m 128 -smp 1 -qmp unix:/var/run/hyper/vm-NunbJiqFZu/qmp.sock,server,nowait -serial unix:/var/run/hyper/vm-NunbJiqFZu/console.sock,server,nowait -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x2 -device virtio-scsi-pci,id=scsi0,bus=pci.0,addr=0x3 -chardev socket,id=charch0,path=/var/run/hyper/vm-NunbJiqFZu/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0 -chardev socket,id=charch1,path=/var/run/hyper/vm-NunbJiqFZu/tty.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1 -fsdev local,id=virtio9p,path=/var/run/hyper/vm-NunbJiqFZu/share_dir,security_model=none -device virtio-9p-pci,fsdev=virtio9p,mount_tag=share_dir\n. @zhengxiaochuan-3 \nThese are the information after the failure, could you paste the INFO messages before the message? And you can enable verbose log buy add the -v flag to the daemon (https://docs.hyper.sh/trouble_shooting/index.html)\nAnd looks there is some device operation failure, did you compile qemu from source?\n. @zhengxiaochuan-3 looks the qemu didn't initialized in 10 seconds, this is not a normal case. I think reboot or restart might help...\nAnd the current master branch improved the vm start process, you may have a try to Build from Source. And we are also planning to provide new version of binary soon.\n. @luluprat sorry for the inconvenient. it looks the daemon tried to work with overlayfs driver but did not success. Could you try to add\nStorageDriver=aufs\nor\nStorageDriver=devicemapper\nin config file /etc/hyper/config\n. @luluprat we will check it in detail. by the way: did you installed the binary from the project homepage? If so, we will update the binary packages soon.\n. @luluprat If it does not work under 15.04, there should be a bug. We will update the binary package in hours.\n. ok, and we will still update the package soon, as It fixed several bugs and has some new features.\n. to specify memory for a Pod, use hyper run -m 256 elasticsearch. the document for run is here. And you can configure the pod with a Pod file, and run it with hyper run -p podfile.\n. @carmark no, the async flag should not wait the vm started if it's works well, and if there is some problem in async creating vm, I will fix it. I don't want to keep the pod/run api, which is confused\n. @carmark I tested this PR, and the /pod/run, and the current master in one box, can not find significant performance difference, will test more cases before merge\n. tested with\n- master\n- this pr\n- previous one shot run api\nthe result in short ---\n- for overlay, almost the same boot time for the three methods.\n- for devicemapper, this pr faster about 1/4 than the other two.\nwill do clean up and merge this pr.\n. @carmark could you give me a config file with all configurable fields (and most of them are commented out) when you have time?\n. initial rpm for centOS 7 has been built and uploaded to S3\nhttps://s3.amazonaws.com/hyper-install/hyper-0.4-1.el7.centos.src.rpm\nhttps://s3.amazonaws.com/hyper-install/hyper-0.4-1.el7.centos.x86_64.rpm\n. update: \n- add systemd service config file into rpm package\n- add hyperstart rpm and set it as dependency of hyper\nBuild result for testing:\nbinary rpm:\n- https://s3.amazonaws.com/hyper-install/hyper-0.4-2.el7.centos.x86_64.rpm\n- https://s3.amazonaws.com/hyper-install/hyperstart-0.4-1.el7.centos.x86_64.rpm\nsrc rpm:\n- https://s3.amazonaws.com/hyper-install/hyper-0.4-2.el7.centos.src.rpm\n- https://s3.amazonaws.com/hyper-install/hyperstart-0.4-1.el7.centos.src.rpm\n. The docker image to generate the rpm has been pushed to official Docker hub: https://hub.docker.com/r/hyperhq/centos_builder/\n. for the latest hyper, use \nhyper run -t ubuntu /bin/bash\ndocs: https://docs.hyper.sh/reference/run.html\nThis change is introduced by the hyper log support.\n. This should be a known issue related with rh kernel, could you pls run the following steps\n1. install the RPM here: https://github.com/hyperhq/hyper/pull/135#issuecomment-159552340\n2. in /etc/hyper/config, set StorageDriver=devicemapper\n3. remove hyperd and hyper under /usr/local/bin, and restart the service\nand try again?\nWe are building and testing native packages for the distros, if you meet any problem, comments appreciated.\nThank you\n. And updated RPMs for test: https://github.com/hyperhq/hyper/pull/135#issuecomment-161171532\n. @ptptptptptpt \nfor the systemd issue: the old systemd use /usr/local/bin/hyperd instead of /usr/bin/hyperd, and the new rpm shipped with an updated systemd config. If you met the problem with the new rpm (hyper-0.4-2.el7.centos.src.rpm), let me know and I will check it again.\nfor the qemu problem, did you compiled qemu with virtfs enabled?\nThank you\n. @ptptptptptpt you can modify the  ExecStart line to\nExecStart=/usr/bin/hyperd --nondaemon\nor for verbose log \nExecStart=/usr/bin/hyperd --nondaemon -v=1\nand we will check the remain issues.\n. @ptptptptptpt \nThank you for your logs, It shows the qemu process itself didn't start in 10 seconds, which is weird. We will try to setup a similar environment to re-produce the problem. This may take some time. Sorry for the inconvenience. \n. Just tried with \n- CentOS 7.1.1503\n- hyper rpms\n- qemu 2.3.0 and 2.4.1 (with ./configure --enable-virtfs --target-list=x86_64-softmmu)\nhyper run works\n[root@cns-2 bin]# hyper run -t ubuntu /bin/bash\nPOD id is pod-zmnccvoLlc\nroot@ubuntu-6826423242:/#\nI suspect there are some problem in environment state, and wonder if a reboot could help. And I am considering build qemu rpm package together with hyper RPMs for centOS.\n. could you try to remove /var/lib/hyper and /var/run/hyper; re-install hyperstart ; and restart the service. then try again?\n. I will try to package qemu for centos as well and send it to you for testing.\n. ip_forware was not enabled. \nNow we enable it by default (https://github.com/hyperhq/runv/pull/92)\n. hi @jzarzuela , what's your OS X version? The virtualbox on 10.11 works different from the previous version, we are working on fix it. If you are on 10.11, please wait some time. Sorry for the inconvenience.\n. close this issue, virtualbox support has been removed from the latest release. could you contribute a test case for this? or describe how to test in text.\n. Now it is a small change and has passed the tests, let's merge it.\n. will it compile fail if no libvirt present? If so, we need some code to disable libvirt related code if libvirt develop files are not available.\n. @DylanSale so far, the only open source hypervisor supports Hypervisor.framwork is xhyve, and it lacks some feature we need. We are also trying do some work on that, but currently it is still under development.\n. I wrote test cases for this patch, and it shows this pr works correctly for both run and exec\n. looks #132 was reverted. Why not just \"hide\" prefetchVm in createVm?\n. @linfan could you try with this pre-build qemu rpm:\n- https://s3.amazonaws.com/hyper-install/qemu-hyper-2.4.1-1.el7.centos.x86_64.rpm\nit is still a testing build, check whether it helps\n. @linfan for self-built qemu, you need make sure you have --enable-virtfs\n. and there are pre-build centos rpm package for testing\n- hyper: https://s3.amazonaws.com/hyper-install/hyper-0.4-2.el7.centos.x86_64.rpm\n- hyperstart: https://s3.amazonaws.com/hyper-install/hyperstart-0.4-1.el7.centos.x86_64.rpm\n. after having installed the rpms, did you remove or rename those binaries in /usr/local/bin?\n. my test has passed with my runv patch for json:\n\n{\n        \"id\": \"test-container-volume\",\n        \"containers\" : [{\n            \"name\": \"c1\",\n            \"image\": \"busybox\",\n            \"command\": [\"/bin/sh\", \"-c\", \"grep -q 'hello, world' /mnt/with-volume-test-1 && df | grep -q '/var/log' && echo 'container 1 OK'\"],\n            \"volumes\": [{\n                \"volume\": \"log\",\n                \"path\": \"/var/log\",\n                \"readOnly\": false\n             },{\n                \"volume\": \"tmp\",\n                \"path\": \"/mnt\",\n                \"readOnly\": false\n             }]\n        },\n        {\n            \"name\": \"c2\",\n            \"image\": \"busybox\",\n            \"workdir\": \"/\",\n            \"command\": [\"/bin/sh\", \"-c\", \"echo 'container-2 OK' > /mnt/with-volume-test-2\"],\n            \"volumes\": [{\n                \"volume\": \"log\",\n                \"path\": \"/var/log\",\n                \"readOnly\": false\n             },{\n                \"volume\": \"tmp\",\n                \"path\": \"/mnt\",\n                \"readOnly\": false\n             }]\n        }],\n        \"resource\": {\n            \"vcpu\": 1,\n            \"memory\": 512\n        },\n        \"files\": [],\n        \"volumes\": [{\n            \"name\": \"log\",\n            \"source\": \"\",\n            \"driver\": \"\"\n        },{\n            \"name\": \"tmp\",\n            \"source\": \"TMPDIR\",\n            \"driver\": \"vfs\"\n        }],\n        \"tty\": true\n}\n\n. sorry for that, 10.11 changes much and virtualbox doesn't work well with it. we are working on the xhyve based version for the future releases.\n. should not modify the code under Godep directly. code here will be overwritten when we update the dependencies.\n. For CentOS 7.x, could you remove all the hyper binaries under /usr/local/bin, and install the following prebuilt RPMs:\nx86_64 binary packages:\n\n\nhyper-0.4-2.el7.centos.x86_64.rpm\nhyperstart-0.4-2.el7.centos.x86_64.rpm\nqemu-hyper-2.4.1-2.el7.centos.x86_64.rpm\n. close as the RPMs works\n. :+1: this fixes the regression introduced by  https://github.com/hyperhq/hyper/commit/faa589abca13bad48c7ded5b35b8201d80b0b93f \n. did you start the pod with tty enabled?\n. should be fixed when we switched to ipvs service implementation. restarted tests, and passed\n. Hi @justin8 ,\n\n\nYou may set the destination with ./configure --prefix=/tmp/foo to install to /tmp/foo/bin, or ./configure --bindir=/tmp/foo to install binaries to /tmp/foo\n. looks get exit code failed\n. LGTM\n. wait, don't merge, another commit is comming\n. Good\n. hi @zenny ,\ncould you try hyper run -t alpine /bin/sh?\nThe current master branch improved docker 1.10 compatibility and had many fix on tty. Once it had been well tested, it will be released as a new version.\n. @zenny \nFor reboot. I suspect you only need enable the service. As packager, we do not set our service start during boot. the decision should be done by user explicitly with command systemctl enable hyperd.service. I notice your command line has this\n\nLoaded: loaded (/lib/systemd/system/hyperd.service; disabled)\n\nFor list/attach, if a pod in state succeed, that means it has successfully exited, and you cannot attach to it any more. And if you 'exit' from a container, it should exit. That's why you cannot attach to it. For the /bin/sh: hy: not found, I have no idea about it, it looks like hy was input to /bin/sh. We did not experience this in our tests before, but this should not be alpine specific behavior. If possible, could you try this with another image, such as ubuntu or busybox?\nmany thanks for your report.\n. ```\n[root@h8s-single hyperd]# ./hyperctl run -t -d busybox\nPOD id is pod-xesMisdpYn\nTime to run a POD is 414 ms\n[root@h8s-single hyperd]# ./hyperctl attach pod-xesMisdpYn\n/ #\n```\nTwo tips: \n1. run with -t, then you can get shell run on tty (in interactive mode, otherwise you can not get the PS1 prompt)\n2. press Enter key after attach, you will get the prompt of next line.\n. I agree with you, Thank you\n. depends on https://github.com/hyperhq/runv/pull/166\n. ## Note\nChanged behavior\nThis patch only modified a small set of behavior\n- support send signal to container with kill command\n- deal with log from tty on non-tty container (together with #262 )\nAnd for most of the command line, it behave as before. I do not want to change too much in one PR (it is already big enough).\nRefactor\nmove API related actions to client/api package, and leave the following in client package\n- command line flags process\n- output format\n- retry and some plumbing jobs\n- raw terminal related\n. @laijs reviews are welcome\n. LGTM. Thanks @allencloud \nI will merge this after #263 is well tested and merged, as it changes much on the client side.\n. @allencloud could you help to do a rebase? We move many codes in the client package. The info file is still here https://github.com/hyperhq/hyper/blob/master/client/info.go#L53\n. Thank you very much @allencloud \n. hi @zenny , \nThe current https://docs.hyper.sh is for new launched https://hyper.sh service, which is based on this project.\nWe are moving the docs and installer of the open source project to a proper url, it will ready soon.\nThank you\n. @zenny for the containerization part, the public service and the open source project share the same code base. And the public container host service integrates the hyper, hypernetes project and some other functionalities. \nDuring the development and serving of the public service, we also find bugs and improve the quality of open source project. You can find many fix and improvement in the recent PRs. Most of them comes from the service operating, and serve for both open source community and the public service.\n. @zenny these subcommands are not features of the container engine, they are features of hypernetes project. In the public service, we provide a docker-compatible cmdline interface to users, then users can use a command line tools to manage their containers and other resources. If like, you can also check the hyperhq/hypernetes project.\n. This repo was point to hyper.sh, and we have a new domain name (hypercontainer.io) for it, dedicate for the open source project. And the source of docs are still here https://github.com/hyperhq/book .\n. As I have said, the new site for the project is not ready yet.\n. I suspect @zenny didn't update hyperstart to new version.\n. @bruceherve looks the daemon is shutdown before you run hyperctl info\nI0624 20:58:37.811799 18247 daemon.go:357] The daemon will be shutdown\nAnd you may open new issue if you find any issues.. @jdc0589 \nHyper client itself is a docker client with API signature, thus it is affected by some docker environment. With docker private ca configured, I've tried this\n\u279c  docker git:(master) \u2717 hyper images\nAn error occurred trying to connect: Get https://us-west-1.hyper.sh:443/v1.23/images/json: x509: certificate signed by unknown authority\n\u279c  docker git:(master) \u2717 DOCKER_TLS_VERIFY=\"\" hyper images\nREPOSITORY                        TAG                 IMAGE ID            CREATED             SIZE\nubuntu                            latest              a1e4ed2ac65b        46 hours ago        187.9 MB\nalpine                            latest              d7a513a663c1        5 days ago          4.794 MB\nnginx                             latest              af4b3d7d5401        4 weeks ago         190.5 MB\nWe will update a new client to eliminate this problem, and you can use this (DOCKER_TLS_VERIFY=\"\" hyper) as a work around.\nPS: This is a bit confusing. The hyperhq/hyper rep is the backend container engine of  hyper.sh, while the https://github.com/hyperhq/hypercli is the repo for command line tool. \nAnd the code in the command line repo has already fixed the problem.\n. @jdc0589 It's our problem. Sorry for the confusing. Anyway, the same team replies the Issues.\n. @zenny \nI think we could discuss same question in a same thread\n. @feiskyer Do we still need this issue for 0.8 and later release?. gRPC is welcome \n. Hi @zenny , \nThe command line has only a few options (and we will add more later), but you can specified more in a json file (the --podfile option). \nYou may find some examples in the examples directory(examples/) and test directory (hack/pods/). Or ref the documents here (http://docs.hypercontainer.io/reference/podfile.html).\nIf you want to add more command line options, you might modify here: https://github.com/hyperhq/hyper/blob/master/client/run.go#L200 . PRs are always welcome.\nThank you\n. LGTM, the #289 is not introduced by this PR\n. done in the refactor. done in the refactor. but, \nIIRC, runc do not care about mounting filesystems, right?\n. Do you mean got it from info API?\n. @zenny Currently, we don't change the resource of a Pod once it is created. This make the resource allocation and scheduling predictable. \nHowever, kvm/qemu allow us to hot plug cpu/memory to them. I think it is possible in the future. \n. @zenny \nUnfortunately, it is a bug... we have already move all resource specification to the creating step, but forgot to remove the command line parameters, just like docker. \nI know the parameter looks useful, and it is reasonable to decide the resources when starting a pod, this should be add back in the future.\nFor now, could you try create another pod with bigger memory?\n. Whether do thin provisioning is the decision by scheduler, for a pod, you run it with 8GB memory, you have the ability to access them.\n. @zenny do you mean the memory parameter?\n[root@h8s-single hyperd]# ./hyperctl run --memory 768 -t busybox\n/ # free\n             total       used       free     shared    buffers     cached\nMem:        775980      23752     752228       3656          0       4000\n-/+ buffers/cache:      19752     756228\nSwap:            0          0          0\n/ #\n. close this issue due to not be active in a long period.. to pass the test, we need to update all the import in project, should we merge #282 before this? However, the #282 won't pass too if I do not modify the imports\n. Inject file is still useful, and insert /etc/resolv.conf is the default behavior. The hyperd process is:\n- if dns is configured in pod spec, do not inject\n- if user has inject or map /etc/resolv.conf, then do not inject\nThe solution should be: write /etc/resolv.conf if and only if the dns is configured\n. @zenny \nhypercli repo is the client for Hyper_ cloud, it is based on docker client and provide some hyper specified functions, such as floating IP associate, volume snapshot, etc. \nWhile the hyperctl is the control tool directly interact with hyperd, just like docker's containerd and its control tool.\nI am very sorry for making you feel disappointed, however, I had to do in order to avoid further confusing. For hyperd + runv repo/project, our direction is to make it more compatible to docker behavior. And in the client side, we are working on providing a stabler and more efficient API interface based on gRPC.\nAnd could you share your case with us? Although the project has not reach 1.0 yet (we want to make it reach 1.0 in this summer), we do not want to break your works when we do some changes.\n. Thank you @zenny \nI know the binary package has not been updated. I think we should not modify a published package, and we will release new version after fix the current critical issues.\n. PR #282 is part of the fix for hyper build, before #282, the build is broken by previous commits (move to docker 1.10 new image id mechanism compatible).\nAnd there are still problem to fix as I written in the comments. Before #289 close, the build won't work.\n. Hi @linfan , \nMany thanks for your report.\nYes,  qemu-hyper provide qemu-system-x86_64, I checked centOS official repo, there is not a qemu-system-x86-2 in centOS 7.\nThere is a qemu-system-x86 in epel, I think both  qemu-hyper and qemu-system-x86 could work with our hyperd. However we suggest user use qemu-hyper.\n. ok, this patch should work well. And let's talk about merge SetContainerStatus and setPodContainerStatus later, which is an independent topic.\n. This should not exist on latest versions. @gao-feng, I agree with you, set p.vm to nil only in podStopped. will update this pr in couple of minutes.\n. @gao-feng did you produced the problem? I think the hyperHandlePodEvent itself is a callback function, and should be called whenever the vm loop has new event.\n. will update, don't merge now\n. move to #333 \n. Thank you @allencloud , and this will be included in 0.6.0 release\n. Will support this soon. Thank you @haosdent \n. supported in latest version. LGTM\n. LGTM\n. LGTM, thank you @Crazykev \n. IIRC, this is an expected behavior. A run command will lead to pre-start vm before the pod was created, which will save the time of the start procedure if everything goes well. And the VM will be reclaim if something failed.\nWill ask @laijs for confirming this.\n. Could we close this issue? @laijs . Thank you @daehyeok \n. Broken by the site update, Thank you @daehyeok \n. let's merge and test it.\n. @daehyeok Thank you for the report, will check it.\nPS: what's the version of xen in Ubuntu 16.04?\n. I think we may close this PR, remove container from pod is supported in #445 and #469 . @resouer had you done this one?. this does not change any result, close it.\n. with gflag.PassAfterNonOption, the flags after first non-option arg will be pass as arguments. It has been included in commit 7481c54f0ebed4d3019adf5c3679d4512803e27d\n. add it to exec in #561 . already fixed in the latest release. closed by #652 . no, just volumes for the new containers.\n. @feiskyer could we close this issue now?. could we close this, @gao-feng ?. not present in current release. close this issue due to long time inactive, reopen this if anybody has similar problem and more information.. Hi @marcochiappero , thanks for reporting, and sorry for the inconvenience.\nPlease disable it, the cbfs is inherited from the previous versions and will be removed in the future release.\n. I think this has already included in the latest master. woo... Sorry for introduce conflict for you.... retest this please, @hykins. Agree. I have the same idea when I refactor the hyperd,  all determined name might be collision. \n. I suggest put debian/  in the package/ubuntu/ dir --\nIn the hyperd repo, ubuntu packaging file should be placed in package/ubuntu/ and the scripts and other support stuff for building pkg. On the other hand, there is a debian/ dir for the control files for deb package.\n. How do you generate a deb package? What's the extra work should be done before generating it with dpkg-buildpackage command?\n. looks good.\n@xlgao-zju maybe you should add install golang in the install script.\n. LGTM\n. @xlgao-zju \nCould you build a package with xen support? Include libxl related library and let the result of ./confugure has \"xen support: yes\".\nWe may merge this PR, and open another one for the deb package with xen support.\nThank you\n. I love this.\n. LGTM\n. LGTM\nThis is a minor fix and blocks k8s 1.3 integration. Merge it.\n. looks this is a vmware vm configuration issue, close it. please reopen this if there need further discussion.. The address was move to https://s3-us-west-1.amazonaws.com/hypercontainer-install/hyper-latest.tgz  \nBut the static build is out of date. We have RPMs for redhat and fedora, and we will provide deb packages in days.\nCould you tell us where do you get the link? We will update it.\n. @sameo Thank you!\nWe will try, however, it's not easy to build a static version because we have more dependencies in recent version. (devicemapper, libvirt, etc.)  It is possible if we build a static binary without supporting libvirt some others.\nWhich distro are you working on?\n. LGTM\n. LGTM\n. LGTM\n. @ptptptptptpt could you update the PR to add the --logtostderr flag to .travis.yaml just as @laijs  said?\n. The download urls in document has been updated to 0.7.0 (https://github.com/hyperhq/docs.hypercontainer.io/pull/19), together with deb packages (https://github.com/hyperhq/docs.hypercontainer.io/pull/20).\nThanks for report.\n. Looks good for my debian pine64.\n- self built official qemu 2.6.2 \n- go 1.6.2\n- kernel 3.10\n- storage driver: devicemapper\nWith kvm enabled (no vm cache):\n```\nroot@pine64:/home/debian# time hyperctl run -t aarch64/busybox sh -c \"echo hello, arm64\"\nhello, arm64\nreal    0m4.381s\nuser    0m0.130s\nsys 0m0.080s\n```\n. Still have something to fix, including\n- ~~info~~\n- ~~service update~~\n- ~~build~~\n- ~~vbox graph driver (should we disable it now?)~~\nand will rebase to current master after the fix\n. All the major parts have been done. now\n- ~~fix build~~\n- ~~rebase to current master~~\n- test\n. rebased to master ( #447 ), however, still lot of things to be fixed\n. fixed the build, but still need to correct the behaviors.\n. OOPS: 9 passed, 1 skipped, 8 FAILED\n--- FAIL: Test (74.55s)\nno panic at least, better than 0 passed.\ngo on\n. OOPS: 11 passed, 1 skipped, 6 FAILED\ntwo more passed. OOPS: 13 passed, 1 skipped, 4 FAILED\ntwo more again.\n\n[x] PASS: hyper_test.go:256: TestSuite.TestAddListDeleteService     24.350s\n[x] PASS: hyper_test.go:125: TestSuite.TestCreateAndStartPod        17.858s\n[ ] FAIL: hyper_test.go:168: TestSuite.TestCreateContainer\n[x] PASS: hyper_test.go:56: TestSuite.TestGetContainerInfo  0.003s\n[x] PASS: hyper_test.go:44: TestSuite.TestGetContainerList  0.001s\n[x] PASS: hyper_test.go:70: TestSuite.TestGetContainerLogs  0.001s\n[x] PASS: hyper_test.go:50: TestSuite.TestGetImageList      0.002s\n[x] PASS: hyper_test.go:111: TestSuite.TestGetPodInfo       0.001s\n[x] PASS: hyper_test.go:32: TestSuite.TestGetPodList        0.000s\n[x] PASS: hyper_test.go:38: TestSuite.TestGetVMList 0.000s\n[x] PASS: hyper_test.go:417: TestSuite.TestPauseAndUnpausePod       7.729s\n[x] PASS: hyper_test.go:491: TestSuite.TestPing     0.000s\n[x] PASS: hyper_test.go:84: TestSuite.TestPostAttach        11.977s\n[x] PASS: hyper_test.go:222: TestSuite.TestPullImage        45.522s\n[ ] FAIL: hyper_test.go:191: TestSuite.TestRenameContainer\n[ ] FAIL: hyper_test.go:389: TestSuite.TestSetPodLabels\n\n[ ] FAIL: hyper_test.go:357: TestSuite.TestStartAndStopPod\n. OOPS: 15 passed, 1 skipped, 2 FAILED\n\n\n[x] PASS: hyper_test.go:256: TestSuite.TestAddListDeleteService     24.350s\n\n[x] PASS: hyper_test.go:125: TestSuite.TestCreateAndStartPod        17.858s\n[x] PASS: hyper_test.go:168: TestSuite.TestCreateContainer  14.071s\n[x] PASS: hyper_test.go:56: TestSuite.TestGetContainerInfo  0.003s\n[x] PASS: hyper_test.go:44: TestSuite.TestGetContainerList  0.001s\n[x] PASS: hyper_test.go:70: TestSuite.TestGetContainerLogs  0.001s\n[x] PASS: hyper_test.go:50: TestSuite.TestGetImageList      0.002s\n[x] PASS: hyper_test.go:111: TestSuite.TestGetPodInfo       0.001s\n[x] PASS: hyper_test.go:32: TestSuite.TestGetPodList        0.000s\n[x] PASS: hyper_test.go:38: TestSuite.TestGetVMList 0.000s\n[x] PASS: hyper_test.go:417: TestSuite.TestPauseAndUnpausePod       7.729s\n[x] PASS: hyper_test.go:491: TestSuite.TestPing     0.000s\n[x] PASS: hyper_test.go:84: TestSuite.TestPostAttach        11.977s\n[x] PASS: hyper_test.go:222: TestSuite.TestPullImage        45.522s\n[x] FAIL: hyper_test.go:191: TestSuite.TestRenameContainer\n[x] PASS: hyper_test.go:389: TestSuite.TestSetPodLabels     7.335s\n\n[x] FAIL: hyper_test.go:357: TestSuite.TestStartAndStopPod\n. rename container is fixed\n\n\n[x] PASS: hyper_test.go:256: TestSuite.TestAddListDeleteService     24.350s\n\n[x] PASS: hyper_test.go:125: TestSuite.TestCreateAndStartPod        17.858s\n[x] PASS: hyper_test.go:168: TestSuite.TestCreateContainer  14.071s\n[x] PASS: hyper_test.go:56: TestSuite.TestGetContainerInfo  0.003s\n[x] PASS: hyper_test.go:44: TestSuite.TestGetContainerList  0.001s\n[x] PASS: hyper_test.go:70: TestSuite.TestGetContainerLogs  0.001s\n[x] PASS: hyper_test.go:50: TestSuite.TestGetImageList      0.002s\n[x] PASS: hyper_test.go:111: TestSuite.TestGetPodInfo       0.001s\n[x] PASS: hyper_test.go:32: TestSuite.TestGetPodList        0.000s\n[x] PASS: hyper_test.go:38: TestSuite.TestGetVMList 0.000s\n[x] PASS: hyper_test.go:417: TestSuite.TestPauseAndUnpausePod       7.729s\n[x] PASS: hyper_test.go:491: TestSuite.TestPing     0.000s\n[x] PASS: hyper_test.go:84: TestSuite.TestPostAttach        11.977s\n[x] PASS: hyper_test.go:222: TestSuite.TestPullImage        45.522s\n[x] PASS: hyper_test.go:191: TestSuite.TestRenameContainer  14.601s\n[x] PASS: hyper_test.go:389: TestSuite.TestSetPodLabels     7.335s\n[x] FAIL: hyper_test.go:357: TestSuite.TestStartAndStopPod\n. OK: 17 passed, 1 skipped\n--- PASS: Test (116.71s)\nPASS\nok      github.com/hyperhq/hyperd/integration   116.720s\n\nThere are still lots of issues to fix, however, at least the integration cases are passed. Cheers and go on.. ```\nThe log length has exceeded the limit of 4 MB\nThe job has been terminated\n```\n.... +++ [1122 23:39:27] TEST PASSED\n+++ [1122 23:39:27] Clean up complete\nPassed one round, still need fix the save/re-load issues. WOW! It is the first time this branch pass the test in the past 5 months. \nThere are still lot of known issues to be solved, however, it has never been so close to be completed.. Added save/load to/from leveldb. Squashed the commits. Same contents with the previous commits. And the old commits are kept here.. I listed remain tasks in #462 , and we may merge this and keep working on fix all issues.. @wahmedswl It is on the roadmap, but not scheduled.. Good, Thank you @xlgao-zju . I used the following patch for 0.7.0 deb build, sorry for not posting in time, and could you just update your pr based on it?\n```\ndiff --git a/package/dist/lib/systemd/system/hyperd.service b/package/dist/lib/systemd/system/hyperd.service                                                                                            [32/649]\nindex 3aac5ea..59f6410 100644\n--- a/package/dist/lib/systemd/system/hyperd.service\n+++ b/package/dist/lib/systemd/system/hyperd.service\n@@ -1,6 +1,6 @@\n [Unit]\n Description=hyperd\n-Documentation=http://docs.hyper.sh\n+Documentation=http://docs.hypercontainer.io\n After=network.target\n Requires=\ndiff --git a/package/ubuntu/hypercontainer/debian/changelog b/package/ubuntu/hypercontainer/debian/changelog\nindex 70a0cdf..5a3f697 100644\n--- a/package/ubuntu/hypercontainer/debian/changelog\n+++ b/package/ubuntu/hypercontainer/debian/changelog\n@@ -1,3 +1,13 @@\n+hypercontainer (0.7.0-2) xenial; urgency=low\n+\n+  * add service and config file to package\n+\n+ -- Hyper Dev Team dev@hyper.sh  Sat, 30 Oct 2016 16:21:08 +0800\n+hypercontainer (0.7.0-1) xenial; urgency=low\n+\n+  * Release 0.7.0\n+\n+ -- Hyper Dev Team dev@hyper.sh  Fri, 29 Oct 2016 22:30:08 +0800\n hypercontainer (0.6.2-1) xenial; urgency=low\n\nInitial release\ndiff --git a/package/ubuntu/hypercontainer/debian/rules b/package/ubuntu/hypercontainer/debian/rules\nindex 97006e6..b4aed2f 100755\n--- a/package/ubuntu/hypercontainer/debian/rules\n+++ b/package/ubuntu/hypercontainer/debian/rules\n@@ -25,6 +25,7 @@ override_dh_auto_install:\n        mkdir -p debian/hypercontainer/usr/bin\n        cp -aT hyperd debian/hypercontainer/usr/bin/hyperd\n        cp -aT hyperctl debian/hypercontainer/usr/bin/hyperctl\ncp -r package/dist/* debian/hypercontainer/\n\noverride_dh_auto_clean:\n        rm -rf go hyperd hyperctl\ndiff --git a/package/ubuntu/hypercontainer/make-hypercontainer-deb.sh b/package/ubuntu/hypercontainer/make-hypercontainer-deb.sh\nindex 1215748..1d855d6 100755\n--- a/package/ubuntu/hypercontainer/make-hypercontainer-deb.sh\n+++ b/package/ubuntu/hypercontainer/make-hypercontainer-deb.sh\n@@ -9,7 +9,7 @@ if [ $# -gt 0 ] ; then\n fi\n# install addtional pkgs in order to build deb pkg\n-sudo apt-get install -y autoconf automake pkg-config libdevmapper-dev libsqlite3-dev libvirt-dev libxen-dev uuid-dev golang xen-hypervisor-4.6-amd64 -qq\n+sudo apt-get install -y autoconf automake pkg-config libdevmapper-dev libsqlite3-dev libvirt-dev libxen-dev uuid-dev golang xen-system-amd64 -qq\n# get hyperd tar ball\n cd $PROJECT\ndiff --git a/package/ubuntu/hyperstart/debian/changelog b/package/ubuntu/hyperstart/debian/changelog\nindex 2de9466..6adc846 100644\n--- a/package/ubuntu/hyperstart/debian/changelog\n+++ b/package/ubuntu/hyperstart/debian/changelog\n@@ -1,3 +1,8 @@\n+hyperstart (0.7.0-1) xenial; urgency=low\n+\n+  * Release 0.7.0\n+\n+ -- Hyper Dev Team dev@hyper.sh  Fri, 29 Oct 2016 22:30:08 +0800\n hyperstart (0.6.2-1) xenial; urgency=low\n\nInitial release\ndiff --git a/package/ubuntu/make-deb.sh b/package/ubuntu/make-deb.sh\nindex 37253fe..d42d7ea 100755\n--- a/package/ubuntu/make-deb.sh\n+++ b/package/ubuntu/make-deb.sh\n@@ -3,6 +3,6 @@\n PROJECT=$(readlink -f $(dirname $0)/../..)\n UBUNTU_DIR=${PROJECT}/package/ubuntu\n\n-${UBUNTU_DIR}/hypercontainer/make-hypercontainer-deb.sh\n-${UBUNTU_DIR}/hyperstart/make-hyperstart-deb.sh\n+${UBUNTU_DIR}/hypercontainer/make-hypercontainer-deb.sh \"$@\"\n+${UBUNTU_DIR}/hyperstart/make-hyperstart-deb.sh \"$@\"\n``\n. @xlgao-zju thank you very much\n. What's your configuration file (/etc/hyper/config`) ?\n. Unfortunately, we didn't leave the backdoor.\nAt the very beginning, we had a debug mode to exec in the initramfs. But it was removed later.\n. #533 add this feature again as it is requested by many debugging users.. looks failed on unix socket\nhyperctl ERROR: An error occurred trying to connect: Get http:///var/run/hyper.sock/info: http: no Host in request URL\n!!! Error in hack/test-cmd.sh:135\n  'sudo env PATH=$PATH hyperctl info' exited with status 255. looks good. I checked the server code, and do not find anything wrong with the url path. @vitan could you show me the error again? I am managing to eliminate the error.. Can not reproduce this issue, and looks the patch may not fix. @marcosnils Could you try install libvirt0 first? If it works, we will add it to the pre-install list in the install script.\nThank you. @xiaods Will update install script, and close this once the script updated.. @jgillich It is a dropped access method, the create does not use any url parameter. And the remove parameter is dropped as well.. The undocumented container create API is not fully supported yet, and the implementation will be changed soon.. @jgillich The variable is defined in build command line: https://github.com/hyperhq/hyperd/blob/master/Makefile.am#L19. Edit: add #464 to the list. Edit: add hyperhq/runv#392 to the list\nwith hyperhq/runv#392 and #464 , the dm will pass the tests. Edit: add #469. Edit: add #471 #468 into the list. Edit: update #471 , add #476 . Edit: add #482. Update, most of the work items are done. Move the rest work items to next release milestone. Yes, that's incorrect, and will be fixed after the refactor is finally done.. The pod created time will be persist and reload in the latest release.. ~~I suspect there are some other issue with dm, and will push other commits here.~~\n~~Dont't merge now~~\nEdit: all fixed. Now the dm issues found in the tests are all fixed. I restart the test, I don't think this patch could cause any error.. @jgillich We notice the docker changes as well. \nCurrently we are busying in fixing the issues ( #462  ) introduced in the recent refactor, and will move Godeps to vendor/ and fix the  godep issues.\nCould you add the dep with some manually work, or wait for a while?\nThanks very much for your contribution.. Thanks @jgillich , I think @laijs may like to do this because he has done the same thing on hyperhq/runv weeks ago.. @jgillich The godeps has been moved to vendor/, update is welcome.. open too long time. re-open it if there is an update. duplicated with #595, and #595 has more info. has this been fixed @feiskyer ?. gnawux@h8s-single:~/Workspace/src/github.com/hyperhq/hyperd$ sudo ./hyperctl run -d nginx\n[sudo] password for gnawux:\nPOD id is nginx-3997343532\nTime to run a POD is 2939 ms\ngnawux@h8s-single:~/Workspace/src/github.com/hyperhq/hyperd$ sudo ./hyperctl list\nPOD ID              POD Name            VM name             Status\nnginx-3997343532    nginx-3997343532    vm-MKgkSlpPnC       running\n1 gnawux@h8s-single:~/Workspace/src/github.com/hyperhq/hyperd$ sudo ./hyperctl list container\nContainer ID                                                       Name                POD ID              Status\n45bde7faac604712b16cdf9b77de9b54d582ab32a2d7123da248402ba8e48f5e   nginx-3997343532    nginx-3997343532    running\ngnawux@h8s-single:~/Workspace/src/github.com/hyperhq/hyperd$ sudo ./hyperctl stop -c nginx-3997343532\ngnawux@h8s-single:~/Workspace/src/github.com/hyperhq/hyperd$ sudo ./hyperctl list container\nContainer ID                                                       Name                POD ID              Status\n45bde7faac604712b16cdf9b77de9b54d582ab32a2d7123da248402ba8e48f5e   nginx-3997343532    nginx-3997343532    succeeded\ngnawux@h8s-single:~/Workspace/src/github.com/hyperhq/hyperd$ sudo ./hyperctl list pod\nPOD ID              POD Name            VM name             Status\nnginx-3997343532    nginx-3997343532    vm-MKgkSlpPnC       running\ngnawux@h8s-single:~/Workspace/src/github.com/hyperhq/hyperd$ sudo ./hyperctl rm -c nginx-3997343532\ncontainer nginx-3997343532 is successfully deleted!\ngnawux@h8s-single:~/Workspace/src/github.com/hyperhq/hyperd$ sudo ./hyperctl list pod\nPOD ID              POD Name            VM name             Status\nnginx-3997343532    nginx-3997343532    vm-MKgkSlpPnC       running\ngnawux@h8s-single:~/Workspace/src/github.com/hyperhq/hyperd$ sudo ./hyperctl list container\nContainer ID        Name                POD ID              Status\ncontainer remove works. \u279c sudo ./hyperctl run -d nginx\nPOD id is nginx-6924339727\nTime to run a POD is 2939 ms\n\u279c sudo ./hyperctl list container\nContainer ID                                                       Name                POD ID              Status\n6be159032b84b712a481182ec7e31647e3228c644aa219296206bc43ae5e801f   nginx-6924339727    nginx-6924339727    running\n\u279c sudo ./hyperctl create -c -t -d nginx-6924339727 busybox\nContainer ID is 237e923d66d07a6405581b02f429017aa436d95b1347947099c50a490c053329\n\u279c sudo ./hyperctl list container\nContainer ID                                                       Name                 POD ID              Status\n237e923d66d07a6405581b02f429017aa436d95b1347947099c50a490c053329   busybox-8257013083   nginx-6924339727    pending\n6be159032b84b712a481182ec7e31647e3228c644aa219296206bc43ae5e801f   nginx-6924339727     nginx-6924339727    running\n\u279c sudo ./hyperctl start nginx-6924339727 busybox-8257013083\nSuccessfully started container busybox-8257013083 in pod nginx-6924339727\n\u279c sudo ./hyperctl list container\nContainer ID                                                       Name                 POD ID              Status\n6be159032b84b712a481182ec7e31647e3228c644aa219296206bc43ae5e801f   nginx-6924339727     nginx-6924339727    running\n237e923d66d07a6405581b02f429017aa436d95b1347947099c50a490c053329   busybox-8257013083   nginx-6924339727    running\n\u279c sudo ./hyperctl stop -c nginx-6924339727\n\u279c sudo ./hyperctl stop -c busybox-8257013083\nfail to stop container busybox-8257013083: Error from daemon's response: timeout while waiting containers: map[string]bool{\"237e923d66d07a6405581b02f429017aa436d95b1347947099c50a490c053329\":true} of [[237e923d66d07a6405581b02f429017aa436d95b1347947099c50a490c053329]]\n\u279c sudo ./hyperctl list container\nContainer ID                                                       Name                 POD ID              Status\n6be159032b84b712a481182ec7e31647e3228c644aa219296206bc43ae5e801f   nginx-6924339727     nginx-6924339727    succeeded\n237e923d66d07a6405581b02f429017aa436d95b1347947099c50a490c053329   busybox-8257013083   nginx-6924339727    running\n\u279c sudo ./hyperctl kill --signal=9 busybox-8257013083\n\u279c sudo ./hyperctl list container\nContainer ID                                                       Name                 POD ID              Status\n237e923d66d07a6405581b02f429017aa436d95b1347947099c50a490c053329   busybox-8257013083   nginx-6924339727    succeeded\n6be159032b84b712a481182ec7e31647e3228c644aa219296206bc43ae5e801f   nginx-6924339727     nginx-6924339727    succeeded\n\u279c sudo ./hyperctl rm -c busybox-8257013083  nginx-6924339727\ncontainer busybox-8257013083 is successfully deleted!\ncontainer nginx-6924339727 is successfully deleted!\n\u279c sudo ./hyperctl rm  nginx-6924339727\nPod(nginx-6924339727) is successfully deleted!\nIn the above op list, there is an failure of stop. It is not a bug, the busybox with -t option will ignore its StopSignal SIGTERM, and could be killed with SIGKILL.. With carefully operation, you may add, start, stop, kill, remove a container inside a pod now @feiskyer . > attach/rm should be set to false by default.\nHmm... you are right. @feiskyer updated\n\u279c sudo ./hyperctl run -d nginx\n[sudo] password for gnawux:\nPOD id is nginx-8530075287\nTime to run a POD is 2884 ms\n\u279c sudo ./hyperctl list container\nContainer ID                                                       Name                POD ID              Status\n6e8c08420389a165682586380fa35432f5922287b9aec34059ed1ff68b3f1623   nginx-8530075287    nginx-8530075287    running\n\u279c sudo ./hyperctl create -c -d nginx-8530075287 busybox\nContainer ID is a8fd34f686e6587979936df880b9e9144fefd895d19560493d2028aa82c47f0c\n\u279c sudo ./hyperctl list container\nContainer ID                                                       Name                 POD ID              Status\n6e8c08420389a165682586380fa35432f5922287b9aec34059ed1ff68b3f1623   nginx-8530075287     nginx-8530075287    running\na8fd34f686e6587979936df880b9e9144fefd895d19560493d2028aa82c47f0c   busybox-2602968228   nginx-8530075287    pending\n\u279c sudo ./hyperctl start -c busybox-2602968228\nSuccessfully started container busybox-2602968228\n\u279c sudo ./hyperctl list container\nContainer ID                                                       Name                 POD ID              Status\na8fd34f686e6587979936df880b9e9144fefd895d19560493d2028aa82c47f0c   busybox-2602968228   nginx-8530075287    running\n6e8c08420389a165682586380fa35432f5922287b9aec34059ed1ff68b3f1623   nginx-8530075287     nginx-8530075287    running\n\u279c sudo ./hyperctl stop -c busybox-2602968228 nginx-8530075287\n\u279c sudo ./hyperctl list container\nContainer ID                                                       Name                 POD ID              Status\n6e8c08420389a165682586380fa35432f5922287b9aec34059ed1ff68b3f1623   nginx-8530075287     nginx-8530075287    succeeded\na8fd34f686e6587979936df880b9e9144fefd895d19560493d2028aa82c47f0c   busybox-2602968228   nginx-8530075287    succeeded\n\u279c sudo ./hyperctl rm -c busybox-2602968228 nginx-8530075287\ncontainer busybox-2602968228 is successfully deleted!\ncontainer nginx-8530075287 is successfully deleted!\n\u279c sudo ./hyperctl list container\nContainer ID        Name                POD ID              Status\n\u279c sudo ./hyperctl list\nPOD ID              POD Name            VM name             Status\nnginx-8530075287    nginx-8530075287    vm-pqEnBvQVdS       running\n\u279c sudo ./hyperctl rm nginx-8530075287\nPod(nginx-8530075287) is successfully deleted!\n\u279c sudo ./hyperctl list\nPOD ID              POD Name            VM name             Status\n\u279c. @feiskyer I pushed a new commit with more verbose logs, the successful log should be like this\nI1215 04:12:47.374276  110567 server.go:152] Calling POST /v0.7.0/container/create\nI1215 04:12:47.374338  110567 container_routes.go:123] Create container {\"name\":\"busybox-1897372744\",\"image\":\"busybox\",\"restartPolicy\":\"never\"} in pod nginx-5903557292\nDEBU[0060] container mounted via layerStore: /var/lib/hyper/overlay/3f0f58765e5f569b821dbbe72caf4d1e079d94782e3b3b8aeeec1ca5e376153b/merged\nI1215 04:12:47.379800  110567 container.go:488] Pod[nginx-5903557292] Con[(busybox-1897372744)] create container aaf2a98749108503bdee8f502931447cb1f2cef088aeee487af1df272a056a72 (w/: [])\nI1215 04:12:47.380009  110567 container.go:505] Pod[nginx-5903557292] Con[aaf2a9874910(busybox-1897372744)] container info config &container.Config{Hostname:\"aaf2a9874910\", Domainname:\"\", User:\"\", AttachStdin:false, AttachStdout:false, At\ntachStderr:false, ExposedPorts:map[nat.Port]struct {}(nil), PublishService:\"\", Tty:false, OpenStdin:false, StdinOnce:false, Env:[]string{\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"}, Cmd:(*strslice.StrSlice)(0xc820\n7fd400), ArgsEscaped:false, Image:\"busybox\", Volumes:map[string]struct {}(nil), WorkingDir:\"\", Entrypoint:(*strslice.StrSlice)(nil), NetworkDisabled:true, MacAddress:\"\", OnBuild:[]string(nil), Labels:map[string]string{}, StopSignal:\"\"}, C\nmd [sh], Args []\nI1215 04:12:47.380020  110567 container.go:510] Pod[nginx-5903557292] Con[aaf2a9874910(busybox-1897372744)] describe container\nI1215 04:12:47.380039  110567 container.go:518] Pod[nginx-5903557292] Con[aaf2a9874910(busybox-1897372744)] mount id: 3f0f58765e5f569b821dbbe72caf4d1e079d94782e3b3b8aeeec1ca5e376153b\nI1215 04:12:47.380071  110567 container.go:581] Pod[nginx-5903557292] Con[aaf2a9874910(busybox-1897372744)] Container Info is\n&api.ContainerDescription{Id:\"aaf2a98749108503bdee8f502931447cb1f2cef088aeee487af1df272a056a72\", Name:\"/busybox-1897372744\", Image:\"sha256:e02e811dd08fd49e7f6032625495118e63f597eb150403d02e3238af1df240ba\", Labels:map[string]string(nil), T\nty:false, StopSignal:\"TERM\", RootVolume:(*api.VolumeDescription)(0xc8207c9d60), MountId:\"3f0f58765e5f569b821dbbe72caf4d1e079d94782e3b3b8aeeec1ca5e376153b\", RootPath:\"rootfs\", UGI:(*api.UserGroupInfo)(nil), Envs:map[string]string{\"PATH\":\"/\nusr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"}, Workdir:\"/\", Path:\"sh\", Args:[]string{}, Rlimits:[]*api.Rlimit{}, Sysctl:map[string]string(nil), Volumes:map[string]*api.VolumeReference(nil), Initialize:false}\nI1215 04:12:47.380082  110567 container.go:728] Pod[nginx-5903557292] Con[aaf2a9874910(busybox-1897372744)] configure dns\nI1215 04:12:47.380091  110567 container.go:786] Pod[nginx-5903557292] Con[aaf2a9874910(busybox-1897372744)] inject file /etc/resolv.conf\nI1215 04:12:47.380549  110567 provision.go:146] Pod[nginx-5903557292] Con[aaf2a9874910(busybox-1897372744)] volume etchosts-volume has already been included, don't need to be inserted again\nI1215 04:12:47.380557  110567 provision.go:152] Pod[nginx-5903557292] Con[aaf2a9874910(busybox-1897372744)] volumes to be added: []\nI1215 04:12:47.380584  110567 container.go:845] Pod[nginx-5903557292] Con[aaf2a9874910(busybox-1897372744)] begin add to sandbox\nI1215 04:12:47.380592  110567 volume.go:180] Pod[nginx-5903557292] Vol[etchosts-volume] subcribe volume insert\nI1215 04:12:47.380601  110567 volume.go:184] Pod[nginx-5903557292] Vol[etchosts-volume] the subscribed volume has been inserted, need nothing.\nI1215 04:12:47.380814  110567 container.go:867] Pod[nginx-5903557292] Con[aaf2a9874910(busybox-1897372744)] finished container prepare, wait for volumes\nI1215 04:12:47.380822  110567 container.go:876] Pod[nginx-5903557292] Con[aaf2a9874910(busybox-1897372744)] resources ready, insert container to sandbox\nI1215 04:12:47.380880  110567 context.go:261] SB[vm-PvYdiuMhuV] add container &api.ContainerDescription{Id:\"aaf2a98749108503bdee8f502931447cb1f2cef088aeee487af1df272a056a72\", Name:\"/busybox-1897372744\", Image:\"sha256:e02e811dd08fd49e7f603\n2625495118e63f597eb150403d02e3238af1df240ba\", Labels:map[string]string(nil), Tty:false, StopSignal:\"TERM\", RootVolume:(*api.VolumeDescription)(0xc8207c9f40), MountId:\"3f0f58765e5f569b821dbbe72caf4d1e079d94782e3b3b8aeeec1ca5e376153b\", Root\nPath:\"rootfs\", UGI:(*api.UserGroupInfo)(nil), Envs:map[string]string{\"PATH\":\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"}, Workdir:\"/\", Path:\"sh\", Args:[]string{}, Rlimits:[]*api.Rlimit{}, Sysctl:map[string]string(nil),\nVolumes:map[string]*api.VolumeReference{}, Initialize:true}\nIn which shows, the /etc/hosts has been inserted by the first container, and the second one does not need to insert it again... Could you help reproduce the error again and put the -v=4 logs here? Thank you very much.. You may merge it by yourself after it pass the tests. @feiskyer . In both Rest and gRPC APIs, the attach and tty info are removed from the StartPod API, the client calls Attach API before StartPod in case it is called with attach flag.\nPlease take a look @laijs @feiskyer . LGTM. based the message here, I close this issue. If there are still anything to do in hyperd, please reopen this @resouer .. When talking processes, do you mean exec + container, @laijs ?. I will have a look soon.. This patch set is really appreciated, and I have only one question -- Should we have a protocol field in serviceKey? @Crazykev @feiskyer . I merged this PR, thank you @Crazykev . retest this please, @hykins. retest this please, @hykins. @bergwolf any update?. retest this please, @hykins. @laijs need rebase here.. I am moving Godeps/ to vendor/ in #502 , please update this after it got merged. retest this please. retest this please. looks like a display issue, only run once, but 2 items show here.. Passed, cool!. a  commit for debug jenkins, do not merge it. retest this please. OK, merge this if build success. retest this please. retest this please. PASS\nThis PR only affect the test environment, merge it.. retest this please, @hykins. Will we have other assistant container, such as those for the pod initialization? Or will they be treat as normal container?. @feiskyer @resouer could you confirm that we (hyperd) don't need any type of assistant containers in recent months?. At least the etcd client has code import \"github.com/coreos/etcd/Godeps/_workspace/src/golang.org/x/net/context\" in its source code.. have you done this @laijs ?. This is a feature, this is not a bug.\n. Let me check again. @Crazykev What did you do exactly?\nFor me ,\n-> sudo ./hyperctl run -t busybox /bin/sh\n/ # ls\nbin   dev   etc   home  lib   proc  root  sys   tmp   usr   var\n/ # exit\n->\nThe only thing not like before is (this is what I said \"feature\"):\n-> sudo ./hyperctl list\nPOD ID               POD Name             VM name             Status\nbusybox-8525184801   busybox-8525184801   vm-hLwLLokFZp       running\n->\ni.e. Pod is still running although the container has already exited.. LGTM. solved by #525 . Any update on this, @gao-feng ?. @gao-feng need update runV dep?. fixed by #568 . travis build failed, looks like download lvm source failed.. @laijs  we need change the lvm download link to: ftp://sources.redhat.com/pub/lvm2/LVM2.2.02.131.tgz \nor newer version: \nftp://sources.redhat.com/pub/lvm2/LVM2.2.02.168.tgz\nThe fedora site has retired.. How did this change apply to jenkins?. All the travis/aufs tests passed, merge it, and followed PR will deal with other testing options.. looks vendor/github.com/docker/docker/ was changed, but not updated in Godeps/Godeps.json. cc @laijs . looks like RemoveVolume is called after VM was set to nil. fixed by #542 and #545 . @halacs A pod of hypercontainer is run in a VM, and the resource constraints are applied to a pod (vm).\nThe memory and cpu are hotplug-able, however, we don't scale the resources now.\nAnd for the network constraint, when use libvirt driver, we could limit some of the network parameters.. This issue is included in the Q&A page of HyperContainer Docs.\nThanks for your question @halacs . Is this not fixed yet? @Crazykev . Thank you @Crazykev , I just confirm its status.. confirm this bug, running but attach exit at once. \ndaemon logs here:\nI0403 05:22:18.641427  125105 server.go:152] Calling GET /v0.8.0/container/info\nI0403 05:22:18.641459  125105 info.go:50] GetContainerInfo of busybox-3606458946\nI0403 05:22:18.641531  125105 container.go:225] Pod[busybox-3606458946] Con[76e29e6ae62c(busybox-3606458946)] retrive info &types.ContainerStatus{Name:\"busybox-3606458946\", ContainerID:\"76e29e6ae62c176b8d2e678692c67c4914af74d5a7b6644953b696e7d8b76904\", Phase:\"running\", Waiting:(*types.WaitingStatus)(0xc42103eea0), Ru\nnning:(*types.RunningStatus)(0xc42103eeb0), Terminated:(*types.TermStatus)(0xc420052370)} from status &pod.ContainerStatus{State:3, CreatedAt:time.Time{sec:63626808057, nsec:270934219, loc:(*time.Location)(0x1f5fd40)}, StartedAt:time.Time{sec:63626808118, nsec:889764424, loc:(*time.Location)(0x1f5fd40)}, FinishedAt:t\nime.Time{sec:63626808101, nsec:617087491, loc:(*time.Location)(0x1f144e0)}, ExitCode:0, Killed:false, RWMutex:sync.RWMutex{w:sync.Mutex{state:0, sema:0x0}, writerSem:0x0, readerSem:0x0, readerCount:1, readerWait:0}, stateChanged:(*sync.Cond)(0xc4210f7680)}\nI0403 05:22:18.642486  125105 server.go:152] Calling POST /v1.17/attach\nI0403 05:22:18.642573  125105 container.go:1287] Pod[busybox-3606458946] Con[76e29e6ae62c(busybox-3606458946)] attach: stdin: begin\nI0403 05:22:18.642608  125105 container.go:1322] Pod[busybox-3606458946] Con[76e29e6ae62c(busybox-3606458946)] attach: stdout: begin\nI0403 05:22:19.814805  125105 container.go:1313] Pod[busybox-3606458946] Con[76e29e6ae62c(busybox-3606458946)] attach: stdin: end\nI0403 05:22:19.814849  125105 container.go:1336] Pod[busybox-3606458946] Con[76e29e6ae62c(busybox-3606458946)] attach: stdout: end\nI0403 05:22:19.814862  125105 attach.go:30] Defer function for attach!\nI0403 05:22:19.815054  125105 server.go:152] Calling GET /v0.8.0/container/info\nI0403 05:22:19.815081  125105 info.go:50] GetContainerInfo of 76e29e6ae62c176b8d2e678692c67c4914af74d5a7b6644953b696e7d8b76904\nI0403 05:22:19.815139  125105 container.go:225] Pod[busybox-3606458946] Con[76e29e6ae62c(busybox-3606458946)] retrive info &types.ContainerStatus{Name:\"busybox-3606458946\", ContainerID:\"76e29e6ae62c176b8d2e678692c67c4914af74d5a7b6644953b696e7d8b76904\", Phase:\"running\", Waiting:(*types.WaitingStatus)(0xc42103f570), Ru\nnning:(*types.RunningStatus)(0xc42103f580), Terminated:(*types.TermStatus)(0xc420052550)} from status &pod.ContainerStatus{State:3, CreatedAt:time.Time{sec:63626808057, nsec:270934219, loc:(*time.Location)(0x1f5fd40)}, StartedAt:time.Time{sec:63626808118, nsec:889764424, loc:(*time.Location)(0x1f5fd40)}, FinishedAt:t\nime.Time{sec:63626808101, nsec:617087491, loc:(*time.Location)(0x1f144e0)}, ExitCode:0, Killed:false, RWMutex:sync.RWMutex{w:sync.Mutex{state:0, sema:0x0}, writerSem:0x0, readerSem:0x0, readerCount:1, readerWait:0}, stateChanged:(*sync.Cond)(0xc4210f7680)}. Every std stream related issue is not simple, and this one is even more complicated than common cases. There are several issues in this case:\n\ncontainer restart request will failed in hyperstart, which prevent restart an existing container ( hyperhq/hyperstart#287 )\ncontainer start failure is ignored ( #584 )\ncontainer restart won't re-connect std streams ( #585 )\nrunv will reject streams reconnection because it did not cleanup streams status well ( hyperhq/runv#469 )\n\n. finally it is fixed after hyperhq/hyperstart#306 closed. dup with #537 . LGTM. Thanks @Crazykev , I don't have any more comment on the code. \nIf not too complicated, could you help adding the case in #537 to the test cases of hyperd? . I think this is an issue of clean up and should fix there. What's you guys' opinion, @laijs @gao-feng @bergwolf ?. I think this is a hyperstart issue, could @bergwolf take a look?. I checked this image again, which specified \"user\": \"nobody:nobody\". is this correct? If so, we need handle this.... I really hate a image specified like this.\n{\n        \"Id\": \"sha256:fc5e302d8309a8deb3315452b428969aed4aa931189b10a3231ce48fc0687224\",\n        \"RepoTags\": [\n            \"gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1\"\n        ],\n        \"RepoDigests\": [],\n        \"Parent\": \"\",\n        \"Comment\": \"\",\n        \"Created\": \"2017-02-27T18:56:29.617416621Z\",\n        \"Container\": \"ec1d14a8bbb5f23b3062e513c5f5403b6525bed842db4a680388e65c40ff2b52\",\n        \"ContainerConfig\": {\n            \"Hostname\": \"11fbdc1f630f\",\n            \"Domainname\": \"\",\n            \"User\": \"nobody:nobody\",\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": [\n                \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n            ],\n            \"Cmd\": [\n                \"/bin/sh\",\n                \"-c\",\n                \"#(nop) \",\n                \"ENTRYPOINT [\\\"/sidecar\\\"]\"\n            ],\n            \"Image\": \"sha256:2de46c332e8f9574a87bc5a5c6dfebbc0c88829e8ab8ba1757efdd4b00592c74\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"\",\n            \"Entrypoint\": [\n                \"/sidecar\"\n            ],\n            \"OnBuild\": [],\n            \"Labels\": {}\n        },\n        \"DockerVersion\": \"1.12.6\",\n        \"Author\": \"Bowei Du \\u003cbowei@google.com\\u003e\",\n        \"Config\": {\n            \"Hostname\": \"11fbdc1f630f\",\n            \"Domainname\": \"\",\n            \"User\": \"nobody:nobody\",\n            \"AttachStdin\": false,\n            \"AttachStdout\": false,\n            \"AttachStderr\": false,\n            \"Tty\": false,\n            \"OpenStdin\": false,\n            \"StdinOnce\": false,\n            \"Env\": [\n                \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n            ],\n            \"Cmd\": null,\n            \"Image\": \"sha256:2de46c332e8f9574a87bc5a5c6dfebbc0c88829e8ab8ba1757efdd4b00592c74\",\n            \"Volumes\": null,\n            \"WorkingDir\": \"\",\n            \"Entrypoint\": [\n                \"/sidecar\"\n            ],\n            \"OnBuild\": [],\n            \"Labels\": {}\n        },\n        \"Architecture\": \"amd64\",\n        \"Os\": \"linux\",\n        \"Size\": 44517061,\n        \"VirtualSize\": 44517061,\n        \"GraphDriver\": {\n            \"Name\": \"rbd\",\n            \"Data\": {\n                \"DeviceId\": \"0\",\n                \"DeviceName\": \"docker_image_c299af88270190387755d7ea0ad39a704dfa72bcbeec30989fb7bead560695ab\",\n                \"DeviceSize\": \"10737418240\"\n            }\n        },\n        \"RootFS\": {\n            \"Type\": \"\"\n        }\n    }. Is there anyone willing to fix this? If nobody self-assigned tonight, I will try to make a PR.. need regression test for this issue. Thank you @bergwolf . fixed by #550 . @Crazykev Thank you. I think it's better to add two cases for this, one is \"nobody:nobody\", and the other is \"nobody\". @bergwolf I think this fix is enough for #544 , you could add more fix for the pod spec in a separated PR.. @kadogo Thanks for the report.\nConfirm it. Will update them in the coming release (around this weekend).. New deb packages are built from debian jessie, and work for both debian jessie and ubuntu xenial. fixed by #550 . how about try without -t flag, or kill with signal 9. @Crazykev Let me explain, the stop container kill container with its StopSignal.\nIn the busybox image, the StopSignal is SIGTERM, which works well in non-tty mode. However, if the sh of busybox run in tty mode, SIGTERM will be ignored.  Instead, only SIGKILL works. More or less, I think this is an busybox issue.\nA general solution is kill with SIGKILL if stop not takes into effect in several seconds.. @Crazykev Yes, I agree. Are you willing to add this mechanism? Or I will check if any other have time to make this fix now.. @Crazykev Thank you. And if you met any problem, you could ping me here.. found a comment:\n// configEtcHosts should be called later than parse volumes, guarantee the file is not described in container\n        c.configEtcHosts()\nThen, who moved parseVolumes(). @gao-feng in #490 moved the parseVolume(), therefore, I suggest you ( @bergwolf  ) check the case fixed by #490 . dup with #557 . Is this stable repeated or randomly occurred?. got a similar log: log-vm-exec-failed.txt\n. I0403 02:45:17.335797   23544 vm.go:194] SB[vm-QBHwitcBNN] got shut down msg, acked here\nI0403 02:45:17.335819   23544 vm.go:498] hyperstart-exec \"hyperstart-exec-SXlePMVCXt\" terminated at 2017-04-03 02:45:17.335755185 +0000 UTC with code 0\nI0403 02:45:17.335887   23544 json.go:401] tty: read 4 bytes for stream 3\nI0403 02:45:17.335944   23544 vm_states.go:106] Close tty \nI0403 02:45:17.336256   23544 vm_console.go:46] SB[vm-QBHwitcBNN] [CNL] hyper_handle_exec_exit exec exit pid 334, seq 3, container hyperstart\nI0403 02:45:17.336266   23544 json.go:401] tty: read 0 bytes for stream 3\nI0403 02:45:17.336273   23544 json.go:414] session 3 closed by peer, close pty\nI0403 02:45:17.336303   23544 json.go:504] session 3 send eof to hyperstart\nlooks stream bytes have been received by hyperd, but not output to client.. The issue is comes from here: https://github.com/hyperhq/runv/blame/master/hypervisor/vm_states.go#L129\nThe tty.Close() is called earlier than the out stream end.. The exec vm is very simple:\nfunc (p *XPod) ExecVM(cmd string, stdin io.ReadCloser, stdout, stderr io.WriteCloser) (int, error) {\n    return p.sandbox.HyperstartExec(cmd, &hypervisor.TtyIO{\n        Stdin:  stdin,\n        Stdout: stdout,\n        Stderr: stderr,\n    })\n}\nright after the the process is end, it will return and this may interrupt the hijack stream even there are still data on their way.\n. looks the failure has vanished in recent builds, close it. . but the message said it is used by container 0b3b567ec59c. could you try reproduce with other image with digest? If cannot, that should be an issue of container clean up.. @feiskyer @xlgao-zju  It might be issue of container cleanup as well... because too many tests use busybox image.\nJust do a little more test with another image with digest (never pulled before), it will help to locate the issue.. @xlgao-zju not sure whether you should close it or change it to a issue for clean up.... @xlgao-zju I suggest you do another test:\n\npull image without digest (only)\npull the image with digest (same image, but ref with digest)\nrun container with image:tag pair\nremove the container\nremove image without digest (only)\nremove image with digest\n\nTo see if there are cleanup issue with digest image\n. can not reproduce, will check and report if found similar issue. This is confirmed that not related with this issue\nI0317 16:49:45.829586   23145 json.go:521] get hyperstart API version error: send ctl channel error, the hyperstart might have closed. I0317 10:34:55.297563   23218 decommission.go:544] Pod[busybox] got vm exit event\nI0317 10:34:55.297575   23218 decommission.go:592] Pod[busybox] umount all containers and volumes, release IP addresses\nI0317 10:34:55.297588   23218 etchosts.go:97] cleanupHosts /var/lib/hyper/hosts/busybox, /var/lib/hyper/hosts/busybox/hosts\nI0317 10:34:55.297616   23218 etchosts.go:101] cannot find /var/lib/hyper/hosts/busybox/hosts\nI0317 10:34:55.297708   23218 decommission.go:572] Pod[busybox] sandbox info removed from db\nI0317 10:34:55.297719   23218 decommission.go:577] Pod[busybox] tag pod as stopped\nI0317 10:34:55.297726   23218 decommission.go:584] Pod[busybox] pod stopped\nI0317 10:34:58.021414   23218 json.go:521] get hyperstart API version error: send ctl channel error, the hyperstart might have closed\nE0317 10:35:19.764693   23218 hypervisor.go:49] SB[vm-QeEPbljMVQ] watch hyperstart timeout\nanother case, on TestSuite.TestCreateAndStartPod\nlog-toolong-1.txt\n. another similar one on TestSuite.TestRenameContainer\nlog-toolong-2.txt\n. ~~I think it's more or less related with the hyperstart protocol handling, at least, it should not stall here if the protocol state machine transits to an illegal state. @laijs could you check this one?~~\nnow it will fail fast, however, the problem has not been solved. looks the serial port disconnected while the kernel init the serial device...\nI0316 16:48:30.441463   23090 vm_console.go:46] SB[vm-CRDiGlKuWp] [CNL] virtio-pci 0000:00:02.0: virtio_pci: leaving for legacy driver\nI0316 16:48:30.447921   23090 vm_console.go:46] SB[vm-CRDiGlKuWp] [CNL] ACPI: PCI Interrupt Link [LNKC] enabled at IRQ 11\nI0316 16:48:30.448010   23090 vm_console.go:46] SB[vm-CRDiGlKuWp] [CNL] virtio-pci 0000:00:03.0: virtio_pci: leaving for legacy driver\nI0316 16:48:30.453452   23090 vm_console.go:46] SB[vm-CRDiGlKuWp] [CNL] ACPI: PCI Interrupt Link [LNKD] enabled at IRQ 11\nI0316 16:48:30.453663   23090 vm_console.go:46] SB[vm-CRDiGlKuWp] [CNL] virtio-pci 0000:00:04.0: virtio_pci: leaving for legacy driver\nI0316 16:48:30.458682   23090 vm_console.go:46] SB[vm-CRDiGlKuWp] [CNL] ACPI: PCI Interrupt Link [LNKA] enabled at IRQ 10\nI0316 16:48:30.459030   23090 vm_console.go:46] SB[vm-CRDiGlKuWp] [CNL] virtio-pci 0000:00:05.0: virtio_pci: leaving for legacy driver\nBased on the console message, Interrupt here, do you have any ideas @bergwolf @gao-feng ?\nI0316 16:48:30.465716   23090 vm_console.go:46] SB[vm-CRDiGlKuWp] [CNL] Serial: 8250/16550 driver, 4 ports, IRQ sharing enabled\nI0316 16:48:30.489577   23090 vm_console.go:46] SB[vm-CRDiGlKuWp] [CNL] 00:04: ttyS0 at I/O 0x3f8 (irq = 4, base_baud = 115200) is a 16550A. the #554 is the same problem.\nlog-test-stopped.txt\n. @gao-feng go 1.8 only?. looks this commit is included in both 1.7 and 1.8.. The qemu in travis testing is 2.0.0. The qemu in jenkins belongs to ubuntu 16.04, and version is 2.5. This issue spread in both 2.0 and 2.5, and should be a guest kernel issue?. some cases from Jenkins\n\nhttp://ci.hypercontainer.io:8080/job/hyperd-auto/304/console\nhttp://ci.hypercontainer.io:8080/job/hyperd-auto/302/console\n\nI checked about 20 recent build failures, all of this cases were happened with libvirt driver, and no qemu driver meet the init interrupt problem.\n. didn't happen in recent Hykins, but still in travis, such as: \nlog-serial.txt\n. five commits and nine changed files, it's far beyond my imagination... Does docker implement ps -q in this way?. @hzmangel Very sorry to ask you change so much code. And may I have a last request --- could you squash the changes to a new commit? I will merge it as soon as possible (I hope it could ship with the 0.8 release targeting on Monday. And I am working on updating the docs for 0.8 as well.) . Thank you @hzmangel . ok to test, hykins. retest this please, hykins. @laijs could you review this patch? Its size is far beyond what I imagined before, but looks good except for the grace==0 case.. change glog.Fatalf to glog.Errorf? . @gao-feng sorry, I merged #597 and introduced conflict with this PR, could you update it again?. /home/travis/gopath/src/github.com/hyperhq/hyperd/hack/lib/test.sh: line 33: test: too many arguments\n!!! Error in /home/travis/gopath/src/github.com/hyperhq/hyperd/hack/lib/test.sh:33\n  'test $res -eq 17' exited with status 2. need an image without PATH env to test the patch. gnawux@shutup:~/Workspace/src/github.com/hyperhq/hyperd$ sudo ./hyperctl run -t gcr.io/google_containers/busybox:1.24 sh -c 'echo hello'\nhello\ngnawux@shutup:~/Workspace/src/github.com/hyperhq/hyperd$\nit works. done by hyperhq/hyperstart#281. @Crazykev to apply runv patch on hyperd deps, I think you may simply \ngodep update github.com/hyperd/runv/.... I think it was master of that repo at some time. cc @laijs . close-open to trigger re-merge/re-test as #576 has been merged. need update protobuf generated code\n```\nGenerated types from proto updated.\n--- hack/../_tmp/types.pb.go    2017-04-02 02:44:17.024529037 +0000\n+++ hack/../types/types.pb.go   2017-04-02 02:44:23.569360767 +0000\n@@ -997,7 +997,7 @@ func (m *PodStats) GetContainersStats()\ntype CpuStats struct {\n    Usage       *CpuUsage protobuf:\"bytes,1,opt,name=usage\" json:\"usage,omitempty\"\n-   LoadAverage int32     protobuf:\"varint,2,opt,name=LoadAverage,json=loadAverage,proto3\" json:\"LoadAverage,omitempty\"\n+   LoadAverage int32     protobuf:\"varint,2,opt,name=LoadAverage,proto3\" json:\"LoadAverage,omitempty\"\n }\nfunc (m CpuStats) Reset()                    { m = CpuStats{} }\n@@ -2272,9 +2272,9 @@ func (m *UserUser) GetAdditionalGroups()\n }\ntype Ulimit struct {\n-   Name string protobuf:\"bytes,1,opt,name=Name,json=name,proto3\" json:\"Name,omitempty\"\n-   Hard uint64 protobuf:\"varint,2,opt,name=Hard,json=hard,proto3\" json:\"Hard,omitempty\"\n-   Soft uint64 protobuf:\"varint,3,opt,name=Soft,json=soft,proto3\" json:\"Soft,omitempty\"\n+   Name string protobuf:\"bytes,1,opt,name=Name,proto3\" json:\"Name,omitempty\"\n+   Hard uint64 protobuf:\"varint,2,opt,name=Hard,proto3\" json:\"Hard,omitempty\"\n+   Soft uint64 protobuf:\"varint,3,opt,name=Soft,proto3\" json:\"Soft,omitempty\"\n }\nfunc (m Ulimit) Reset()                    { m = Ulimit{} }\n@@ -2319,7 +2319,7 @@ type UserContainer struct {\n    User          UserUser              protobuf:\"bytes,13,opt,name=user\" json:\"user,omitempty\"\n    Labels        map[string]string      protobuf:\"bytes,14,rep,name=labels\" json:\"labels,omitempty\" protobuf_key:\"bytes,1,opt,name=key,proto3\" protobuf_val:\"bytes,2,opt,name=value,proto3\"\n    Id            string                 protobuf:\"bytes,15,opt,name=id,proto3\" json:\"id,omitempty\"\n-   StopSignal    string                 protobuf:\"bytes,17,opt,name=StopSignal,json=stopSignal,proto3\" json:\"StopSignal,omitempty\"\n+   StopSignal    string                 protobuf:\"bytes,17,opt,name=StopSignal,proto3\" json:\"StopSignal,omitempty\"\n    Ulimits       []Ulimit              protobuf:\"bytes,18,rep,name=ulimits\" json:\"ulimits,omitempty\"\n    LogPath       string                 protobuf:\"bytes,19,opt,name=logPath,proto3\" json:\"logPath,omitempty\"\n }\n@@ -3149,7 +3149,7 @@ func (InfoRequest) ProtoMessage()\n func (InfoRequest) Descriptor() ([]byte, []int) { return fileDescriptorTypes, []int{70} }\ntype InfoResponse struct {\n-   ID                 string          protobuf:\"bytes,1,opt,name=ID,json=iD,proto3\" json:\"ID,omitempty\"\n+   ID                 string          protobuf:\"bytes,1,opt,name=ID,proto3\" json:\"ID,omitempty\"\n    Containers         int32           protobuf:\"varint,2,opt,name=containers,proto3\" json:\"containers,omitempty\"\n    Images             int32           protobuf:\"varint,3,opt,name=images,proto3\" json:\"images,omitempty\"\n    Driver             string          protobuf:\"bytes,4,opt,name=driver,proto3\" json:\"driver,omitempty\"\nGenerated types from proto is out of date. Please run hack/update-generated-proto.sh\nThe command \"hack/verify-generated-proto.sh\" failed and exited with 1 during .\n``. This works for #575 / #570 as well.\"github.com/opencontainers/image-spec\"is updated inGodeps.json, however, no related file is updated. Should it be removed inGodeps.json`?. @Crazykev looks like the container process quit before killed...\nEdited:\noh, no\nget hyperstart API version error: send ctl channel error, the hyperstart might have closed. looks like an runV issue, APIVersion Command result error before it is sent to hyperstart. Will file an issue in runV.. ~~The issue is come from the test case itself. It should wait before issuing a kill signal, otherwise, it may wait forever because the container has already exit.~~. ~~#578 is aiming to fix the test case~~. Update, the issue should be an hyperstart issue ( hyperhq/hyperstart#285 ) if my further investigation is correct. could have a try with ; instead of &&? with &&, if any of the cat return non-zero, the rest command won't be executed.. @feiskyer all the non zero size files are symlink, and can't tell if they are empty at that time.. any update on this issue?. oops, the problem should be a deadlock. fixed by #582. only for logs in deps, merge it. updated this PR as #605 has been merged and runv vendor has been updated.. Test result:\n```\n\u279c sudo ./hyperctl list container\nContainer ID                                                       Name                 POD ID               Status\n2c8abf13b126d71e8fdfdf21d117633178f6f2635f4d884f3179c3e7e306a54c   busybox-9326359365   busybox-9326359365   succeeded\n\u279c sudo ./hyperctl start -c busybox-9326359365\nSuccessfully started container busybox-9326359365\n\u279c sudo ./hyperctl attach busybox-9326359365\n/ # ls\nbin   dev   etc   home  lib   proc  root  sys   tmp   usr   var\n/ # exit\n\u279c\n```. @laijs & @Crazykev please take a look, this should fix #539 . @Crazykev woo.... my test was not on the latest hyperstart. @Crazykev confirm this issue, hyperhq/hyperstart#290 looks doesn't work, while  hyperhq/hyperstart#287 works. further test shows this could not solve the issue yet.. \nThe issue is -- container stop will lead to streams breaking at once, and message will be lost if they have not been sent to the client.. @laijs  do you have any suggestion?. @laijs Agree. This PR does a correct job, but does not solved the issue. I will file a new one and discuss with you.. Just like the error messages, the daemon project has been rename from hyper to hyperd. And #590 has already updated the instruction in README.\nThank you for reporting.. The packages in http://download.hypercontainer.io/ have been updated. The release page has been updated. @matti All the packages were built from the same sources.\nThe deb package problems had been found right after the release when we did the kubernetes tests with it. However, only the download site has been updated with new packages, and the github release page was left. \nI have updated the github release page, and centralized all the packages to our download site.. You are welcome.. For binaries, please refer to the packages: http://download.hypercontainer.io/ . . And will update the README.md. There is a build issue with latest xen, and it will fix with future release.. Do you mean you want use Xen. You may try with with xen 4.5. fixed by hyperhq/runv#484. No plan on openvz yet, and openvz is not a hypervisor.... close this issue. reopen it if need more discussion on this topic. It's a code dependency of hyperd, and it is included in the vendor of the newest release of hyperd, i.e. don't need to clone it for build hyperd now.. Yes, the steps should be updated.. the docs has been updated. Tried but give up -- It is too hard to operate low level storage functionalities inside docker container.. We should release an official image for hyperd with the v0.9.0 release. The Mac version had been disabled, and the link to this page had been removed form the TOC.. Mac OS X build is not available in current version, and I closed this issue. However, it's always welcome if you could work on it and submit PR for it if you like to make a client for Mac.. Jenkins failed, init container before the device was found by the guest kernel:\n07:53:44 I0407 07:53:43.489775    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] hyper_handle_event event EPOLLOUT, he 0x61d648, fd 3, 0x61d4c0\n07:53:44 I0407 07:53:43.489781    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] hyper_modify_event modify event fd 3, 0x61d648, event 8193\n07:53:44 I0407 07:53:43.491644    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] hyper_handle_event event EPOLLIN, he 0x61d648, fd 3, 0x61d4c0\n07:53:44 I0407 07:53:43.491655    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] hyper_ctlfd_read: get length 581\n07:53:44 I0407 07:53:43.491660    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] hyper ctl append type 14, len 4\n07:53:44 I0407 07:53:43.492387    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] hyper_modify_event modify event fd 3, 0x61d648, event 8197\n07:53:44 I0407 07:53:43.492396    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] hyper_ctlmsg_handle, type 17, len 581\n07:53:44 I0407 07:53:43.493592    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] call hyper_new_container, json {\"id\":\"f8f4069306dbaf7c522004f3f4c90c64573ce636d62bae0b6098d0a321171038\",\"rootfs\":\"rootfs\",\"fstype\":\"xfs\",\"image\":\"sda\",\"fsmap\":[{\"source\":\"VpQiSGtlmV\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"id\":\"init\",\"user\":\"nobody\",\"terminal\":false,\"stdio\":1,\"stderr\":2,\"args\":[\"sh\"],\"envs\":[{\"env\":\"LANG\",\"value\":\"C.UTF-8\"},{\"env\":\"IRSSI_VERSION\",\"value\":\"1.0.2\"},{\"env\":\"PATH\",\"value\":\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"},{\"env\":\"HOME\",\"value\":\"/home/user\"}],\"workdir\":\"/home/user\"},\"restartPolicy\":\"never\",\"initialize\":true}, len 573\n07:53:44 I0407 07:53:43.497166    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] create directory /tmp/hyper/f8f4069306dbaf7c522004f3f4c90c64573ce636d62bae0b6098d0a321171038\n07:53:44 I0407 07:53:43.497759    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] create directory /tmp/hyper/f8f4069306dbaf7c522004f3f4c90c64573ce636d62bae0b6098d0a321171038/devpts/\n07:53:44 I0407 07:53:43.502866    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] hyper send type 8\n07:53:44 I0407 07:53:43.506686    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] create child process pid=336 in the sandbox\n07:53:44 I0407 07:53:43.509856    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] path /sys/class/scsi_host/host0/scan\n07:53:44 I0407 07:53:43.730078    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] finish scan scsi\n07:53:44 I0407 07:53:43.735710    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] create directory /tmp/hyper/f8f4069306dbaf7c522004f3f4c90c64573ce636d62bae0b6098d0a321171038/root/\n07:53:44 I0407 07:53:43.736054    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] device /dev/sda\n07:53:44 I0407 07:53:43.740184    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] mount device failed: No such file or directory\n07:53:44 I0407 07:53:43.740195    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] hyper send type 10\n07:53:44 I0407 07:53:43.740621    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] wait for setup container rootfs failed\n07:53:44 I0407 07:53:43.747791    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] create child process pid=593 in the sandbox\n07:53:44 I0407 07:53:43.752330    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] fail to enter container ns: Bad file descriptor\n07:53:44 I0407 07:53:43.752340    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] hyper send type -1\n07:53:44 I0407 07:53:43.755289    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] hyper ctl append type 10, len 0\n07:53:44 I0407 07:53:43.755298    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] hyper_handle_event event EPOLLOUT, he 0x61d648, fd 3, 0x61d4c0\n07:53:44 I0407 07:53:43.755304    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] hyper_modify_event modify event fd 3, 0x61d648, event 8193\n07:53:44 I0407 07:53:43.759540    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] pid 335 exit normally, status 0\n07:53:44 I0407 07:53:43.759551    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] pid 336 exit normally, status 125\n07:53:44 I0407 07:53:43.759829    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] pid 592 exit normally, status 0\n07:53:44 I0407 07:53:43.761117    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] pid 593 exit normally, status 255\n07:53:44 I0407 07:53:43.764750    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] sd 0:0:0:0: [sda] Write Protect is off\n07:53:44 I0407 07:53:43.765127    7347 vm_console.go:46] SB[vm-aPcOrtjWvO] [CNL] sd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA\ndrivers: rawblock, qemu\ndetails: http://ci.hypercontainer.io:8080/job/hyperd-auto/321/console\n. 15:38:46 I0408 15:38:45.695372    6300 vm_console.go:46] SB[vm-OOveuPMxos] [CNL] INFO: rcu_sched detected stalls on CPUs/tasks:\ndetail: http://ci.hypercontainer.io:8080/job/hyperd-auto/322/console\nretest this please, hykins. close as 600 has been merged. And this should be part of #462. retest this please, hykins. The CI result shows, even failed by #557, this patch help preventing hyperd from the panic ( #598 ).. safe quit from #557 init error:\nE0409 12:04:48.721138   23772 json.go:363] read tty data failed\nE0409 12:04:48.721174   23772 json.go:420] SB[vm-VwoaqdiWbA] tty socket closed, quit the reading goroutine: EOF\nI0409 12:04:48.721182   23772 json.go:88] SB[vm-VwoaqdiWbA] close jsonBasedHyperstart\nE0409 12:04:48.721211   23772 json.go:545] SB[vm-VwoaqdiWbA] get hyperstart API version error: hyperstart closed\nW0409 12:04:48.721220   23772 hypervisor.go:59] SB[vm-VwoaqdiWbA] keep-alive test end with error: hyperstart closed\nI0409 12:04:48.721265   23772 hypervisor.go:23] SB[vm-VwoaqdiWbA] main event loop got message 14(ERROR_INIT_FAIL)\nE0409 12:04:48.721277   23772 vm_states.go:237] SB[vm-VwoaqdiWbA] hyperstart failed: hyperstart closed\nE0409 12:04:48.721286   23772 vm_states.go:195] SB[vm-VwoaqdiWbA] Shutting down because of an exception: %!(EXTRA string=connection to vm broken)\nI0409 12:04:48.721293   23772 vm_states.go:198] SB[vm-VwoaqdiWbA] poweroff vm based on command: connection to vm broken\nE0409 12:04:48.721424   23772 hypervisor.go:42] SB[vm-VwoaqdiWbA] hyperstart stopped\nI0409 12:04:48.721444   23772 json.go:388] SB[vm-VwoaqdiWbA] tty chan closed, quit sent goroutine\nE0409 12:04:48.721478   23772 vm_states.go:176] SB[vm-VwoaqdiWbA] Start POD failed\nI0409 12:04:48.721641   23772 provision.go:307] Pod[pod-xMGXDCItWf] sandbox init result: &api.ResultBase{Id:\"vm-VwoaqdiWbA\", Success:false, ResultMessage:\"got failed event when wait init message\"}\nE0409 12:04:48.724827   23772 json.go:139] read init data failed\nI0409 12:04:48.724934   23772 vm_console.go:63] Input byte chan closed, close the output string chan\nI0409 12:04:48.724962   23772 vm_console.go:48] SB[vm-VwoaqdiWbA] console output end\nE0409 12:04:48.724998   23772 json.go:173] SB[vm-VwoaqdiWbA] error when readVmMessage() for ready message: EOF\n\u001b[37mDEBU\u001b[0m[0258] container mounted via layerStore: /var/lib/hyper/rawblock/mnt/c4530c04010a5cf91dcb2f3cd9d1172f748edc423fb4991560f59912820cc882/rootfs \nI0409 12:04:48.922956   23772 context.go:257] SB[vm-VwoaqdiWbA] VmContext Close()\nI0409 12:04:48.923026   23772 hypervisor.go:31] SB[vm-VwoaqdiWbA] main event loop exiting\nI0409 12:04:48.923091   23772 decommission.go:526] Pod[pod-xMGXDCItWf] got vm exit event\nI0409 12:04:48.923129   23772 decommission.go:574] Pod[pod-xMGXDCItWf] umount all containers and volumes, release IP addresses\nI0409 12:04:48.923146   23772 etchosts.go:97] cleanupHosts /var/lib/hyper/hosts/pod-xMGXDCItWf, /var/lib/hyper/hosts/pod-xMGXDCItWf/hosts\nI0409 12:04:48.923186   23772 etchosts.go:101] cannot find /var/lib/hyper/hosts/pod-xMGXDCItWf/hosts\nI0409 12:04:48.923340   23772 decommission.go:554] Pod[pod-xMGXDCItWf] sandbox info removed from db\nI0409 12:04:48.923363   23772 decommission.go:559] Pod[pod-xMGXDCItWf] tag pod as stopped\nI0409 12:04:48.923374   23772 decommission.go:566] Pod[pod-xMGXDCItWf] pod stopped\nI0409 12:04:48.923955   23772 container.go:498] Pod[pod-xMGXDCItWf] Con[(pod-xMGXDCItWf-irssi-0)] create container c16a3c1481821215c77eb0a84e114112bbb904d07600997345c5c6af03dc777b (w/: [])\nI0409 12:04:48.924041   23772 container.go:515] Pod[pod-xMGXDCItWf] Con[c16a3c148182(pod-xMGXDCItWf-irssi-0)] container info config &container.Config{Hostname:\"c16a3c148182\", Domainname:\"\", User:\"user\", AttachStdin:false, AttachStdout:false, AttachStderr:false, ExposedPorts:map[nat.Port]struct {}(nil), PublishService:\"\", Tty:false, OpenStdin:false, StdinOnce:false, Env:[]string{\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", \"HOME=/home/user\", \"LANG=C.UTF-8\", \"IRSSI_VERSION=1.0.2\"}, Cmd:(*strslice.StrSlice)(0xc420de0fc0), ArgsEscaped:false, Image:\"irssi:1\", Volumes:map[string]struct {}(nil), WorkingDir:\"/home/user\", Entrypoint:(*strslice.StrSlice)(nil), NetworkDisabled:true, MacAddress:\"\", OnBuild:[]string(nil), Labels:map[string]string{}, StopSignal:\"\"}, Cmd [sh], Args []\nI0409 12:04:48.924055   23772 container.go:520] Pod[pod-xMGXDCItWf] Con[c16a3c148182(pod-xMGXDCItWf-irssi-0)] describe container\nI0409 12:04:48.924267   23772 container.go:528] Pod[pod-xMGXDCItWf] Con[c16a3c148182(pod-xMGXDCItWf-irssi-0)] mount id: c4530c04010a5cf91dcb2f3cd9d1172f748edc423fb4991560f59912820cc882\nI0409 12:04:48.924334   23772 container.go:608] Pod[pod-xMGXDCItWf] Con[c16a3c148182(pod-xMGXDCItWf-irssi-0)] Container Info is \n&api.ContainerDescription{Id:\"c16a3c1481821215c77eb0a84e114112bbb904d07600997345c5c6af03dc777b\", Name:\"/pod-xMGXDCItWf-irssi-0\", Image:\"sha256:c9b9931a3c0a33319eb291eb546a063b10ea1de6a1e9104ac8cbaf566a659c61\", Labels:map[string]string(nil), Tty:false, StopSignal:\"TERM\", RootVolume:(*api.VolumeDescription)(0xc42107c640), MountId:\"c4530c04010a5cf91dcb2f3cd9d1172f748edc423fb4991560f59912820cc882\", RootPath:\"rootfs\", UGI:(*api.UserGroupInfo)(0xc4214f7680), Envs:map[string]string{\"IRSSI_VERSION\":\"1.0.2\", \"PATH\":\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", \"HOME\":\"/home/user\", \"LANG\":\"C.UTF-8\"}, Workdir:\"/home/user\", Path:\"sh\", Args:[]string{}, Rlimits:[]*api.Rlimit{}, Sysctl:map[string]string(nil), Volumes:map[string]*api.VolumeReference(nil), Initialize:false}\nI0409 12:04:48.924367   23772 container.go:749] Pod[pod-xMGXDCItWf] Con[c16a3c148182(pod-xMGXDCItWf-irssi-0)] configure dns\nI0409 12:04:48.924391   23772 container.go:807] Pod[pod-xMGXDCItWf] Con[c16a3c148182(pod-xMGXDCItWf-irssi-0)] inject file /etc/resolv.conf\nE0409 12:04:49.006063   23772 provision.go:406] Pod[pod-xMGXDCItWf] pod is not alive, can not prepare resources\nE0409 12:04:49.006101   23772 run.go:38] pod-xMGXDCItWf: failed to add pod: hyper pod not alive: cannot complete the operation, because the pod pod-xMGXDCItWf is not alive\nE0409 12:04:49.006236   23772 server.go:170] Handler for POST /v0.8.0/pod/create returned error: cannot complete the operation, because the pod pod-xMGXDCItWf is not alive. wait, let me check if I could use Cond/Wait instead of a chan, which will help understanding.. @Crazykev do you mean just return success if pod is running?. updated. @laijs would you like look at this test result: http://ci.hypercontainer.io:8080/job/hyperd-auto/338/console . looks runv can't get hyperstart version. I move the CI failure to #606 , and now could this PR be merged? @laijs @Crazykev  . @Crazykev which error message do you mean? the already running? I have add a commit for it: 9b4e4388959fc54cf39608d3757169f73faa9dd5 , does this work?. .... one line need two fix.... updated again. Just do it. I do not have a perfect idea to handle this case, should we just let it fail if this is a rarely seen case.. Another case: http://ci.hypercontainer.io:8080/job/hyperd-auto/325/console\nsome of the log, looks the kernel had taken some long time to init the device, right? @bergwolf \n20:05:45  [CNL] virtio-pci 0000:00:05.0: enabling device (0000 -> 0003)\n20:05:45  [CNL] ACPI: PCI Interrupt Link [LNKA] enabled at IRQ 10\n20:05:45  [CNL] virtio-pci 0000:00:05.0: virtio_pci: leaving for legacy driver\n20:05:45  [CNL] sd 0:0:0:0: [sda] 20971520 512-byte logical blocks: (10.7 GB/10.0 GiB)\n20:05:45  [CNL] clocksource: timekeeping watchdog on CPU0: Marking clocksource 'tsc' as unstable because the skew is too large:\n20:05:45  [CNL] clocksource:                       'acpi_pm' wd_now: 444985 wd_last: 2f4401 mask: ffffff\n20:05:45  [CNL] clocksource:                       'tsc' cs_now: 2ff0510b020 cs_last: 23fd3dd4d28 mask: ffffffffffffffff\n20:05:45  [CNL] sd 0:0:0:0: Attached scsi generic sg0 type 0\n20:05:45  [CNL] clocksource: Switched to clocksource acpi_pm\n20:05:45  [CNL] finish rescan\n20:05:46  [CNL] call hyper_new_container, json {\"id\":\"890439e2afad9c4fafd434a4c7ba466b770168f1c47a2135aec2c4bc5167ce6a\",\"rootfs\":\"rootfs\",\"fstype\":\"xfs\",\"image\":\"sda\",\"fsmap\":[{\"source\":\"aynVetpvpD\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"id\":\"init\",\"terminal\":false,\"stdio\":1,\"stderr\":2,\"args\":[\"/bin/sh\"],\"envs\":[{\"env\":\"PATH\",\"value\":\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"}],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":true}, len 444\n20:05:46  [CNL] device /dev/sda\n20:05:46  [CNL] mount device failed: No such file or directory\n20:05:46  [CNL] sd 0:0:0:0: [sda] Write Protect is off\n20:05:46  [CNL] sd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA\n20:05:46  [CNL] sd 0:0:0:0: [sda] Attached SCSI disk\n. Looks all of the failures happened in hykins rather than travis.. is there any event we could wait for?. In recent tests, all of this exception happened in Test TestSendContainerSignal :\n\nhttp://ci.hypercontainer.io:8080/job/hyperd-auto/384/consoleFull\nhttp://ci.hypercontainer.io:8080/job/hyperd-auto/385/consoleFull\nhttp://ci.hypercontainer.io:8080/job/hyperd-auto/386/consoleFull\nhttp://ci.hypercontainer.io:8080/job/hyperd-auto/393/consoleFull\n\nAny thoughts? @bergwolf . It always need some manual process, which tastes bad.. @laijs @gao-feng @bergwolf what's your opinion?. Yep, you have to update Godep.json to remove the nest vendor entries. another case: http://ci.hypercontainer.io:8080/job/hyperd-auto/328/console\na third case: http://ci.hypercontainer.io:8080/job/hyperstart-auto/118/console\na forth case: http://ci.hypercontainer.io:8080/job/hyperd-auto/344/console\na fifth case: http://ci.hypercontainer.io:8080/job/hyperd-auto/345/console. Does all this error happened on hykins (no travis)?. checked recent ci build, looks fixed. I repeated more than 100 tests and it did not interrupt output any more. let's see if it affects other cases.\nBefore the patch, there are about 4-5% failure rate, and now no failure for about 150 tests now. travis passed\nhykins failed because of #601. hykins failed again because of #601, retest this please . @Crazykev I didn't found this and will try to reproduce it.. I extends the timeout from 3 seconds to 10 seconds and confirm what you report. Will check and fix it.. The reason is stdout and stderr of TtyIO are not have to be a Closer, esp. the stdout/stderr of StreamConfig and stdcopy streams are not Closer. . tested with run and exec, both work. We meet this about twice or three times, all of them were met in hykins. didn't reproduce in recent tests. LGTM. build failed. and  hyperhq/runv#491 conflict..... retest this please, hykins. LGTM. We should protect it.\nAnd I wonder what's the test context. the p.waitVMStop() is called quite close to where p.sandbox is assigned, and p.decommissionResources() is a subsequent step of p.waitVmStop(). \np.waitVmStop() --> p.cleanup() --> p.decommissionResources(), i.e.  p.sandbox = nil in p.decommissionResources() should not cause the panic in p.waitVmStop().\nIs this possible: the sb, err = startSandbox() returns nil, nil, i.e. no error,  no vm either. @laijs . could we close this issue @laijs ?. The panic is fixed by #615. If you think we need further action on it, please file a new issue. This is an issue derived from docker 1.10.3\n\u279c docker --version\nDocker version 1.10.3, build 20f81dd\n\u279c docker pull busybox\nUsing default tag: latest\nlatest: Pulling from library/busybox\n7520415ce762: Pull complete\nDigest: sha256:32f093055929dbc23dec4d03e09dfe971f5973a9ca5cf059cbfb644c206aa83f\nStatus: Downloaded newer image for busybox:latest\n\u279c docker run -itd busybox@sha256:32f093055929dbc23dec4d03e09dfe971f5973a9ca5cf059cbfb644c206aa83f\nUnable to find image 'busybox@sha256:32f093055929dbc23dec4d03e09dfe971f5973a9ca5cf059cbfb644c206aa83f' locally\nsha256:32f093055929dbc23dec4d03e09dfe971f5973a9ca5cf059cbfb644c206aa83f: Pulling from library/busybox\nDigest: sha256:32f093055929dbc23dec4d03e09dfe971f5973a9ca5cf059cbfb644c206aa83f\nStatus: Downloaded newer image for busybox@sha256:32f093055929dbc23dec4d03e09dfe971f5973a9ca5cf059cbfb644c206aa83f\n35a4538cf675a93980801db880cba0697ce6a4cc56da172b4c4a65ba6116eb47\n\u279c. @Crazykev not an easy job, I am working on a simpler work around. dup with #601 ?. no..... the container may be restarted, and before restart, it could be attached. LGTM. hyperhq/hyperstart#302 has fixed via add the wait code in guest. LGTM. retest this please, hykins. on the result of hykins -- http://ci.hypercontainer.io:8080/job/hyperd-auto/404/console\nI am afraid that this PR will affect exec output in some case. We need make it clear before merge it.. Read again, looks this PR has no effect on exec.... I think the failure of http://ci.hypercontainer.io:8080/job/hyperd-auto/404/console is because the exec exit before the tty stream is finished. And it does not related with this PR.. test exceed 40 minutes and is killed by hykins. retest this please, hykins. @bergwolf I think the writeMultiCloser is too complicated. I think just like exec does is enough --- \n\nuse io.Writer as signature instead of io.WriteCloser\nat the end, try cast to io.Closer, and Close() only if it is a Closer.\npass stdout as writeCloser, and stderr as pure Writer\n. LGTM again. confirm, it is a known issue of 0.8.x. It is listed in the milestone of 0.9.0.. @nickdoikov not yet. However, this is not a feature hard to be implement, just was temporarily removed during the refactor of 0.8.x developing.\n\n. @nickdoikov back to 0.7, it should work. But 0.8.x fixed many bugs of previous version, and the port mapping could be done in external systems, such as iptables, kubernetes service, and so on.. @SergeyOvsienko In all the previous, the port mapping is done with iptables . @SergeyOvsienko \nIn the previous versions, we define port mapping in the spec of each container. While in the new version, the port mapping will become a pod level configuration, and could be update during runtime. The related work item is #382 . @nickdoikov it's just an API/model change, not about the backend implementation. The past is iptables, and the future is iptables too.. Yes, in the 0.9, both REST/cli and GRPC will be supported.\nNote: The life cycle has changed since 0.8.x. (ref: 0.8 release note)\n\n\nEnhancement: simplify the model and state machine -- one Pod is one VM.\nEnhancement: allow add/remove containers to/from a Pod/sandbox.\nEnhancement: do not stop sandbox without an explicit stop command even if the last container is stopped (To be compatible to Kubernetes CRI).. iptables and cli will all exist in future release. But the interface may be changed.. For the REST API, \n\n\n\nthe port mapping spec will move from containers to top level of pod\nwill have new port mapping add/remove API\nwill support the port range better\n\nFor command line, I think we will try to keep it unchanged.\n. It should be done in 1 or 2 months. And if we could not make a major release, we will release these not-so-big functions in a 0.8.2 release.. Great, if you met any other issues, feel free to file an issue.. LGTM. closed by #642 . LGTM. Did the daemon shutdown by yourself or the OS? Try increase the hyperd log level with -v=4. Then check what's the problem with OS. Based on the current logs, It's hard to judge what's the matter.. Build from source won't break anything and FC25 is supported as well.. As I mentioned in the first comment, try increase the log level, maybe we could found why it quit.. Good to know it works. And the default hypervisor would be qemu if no one is specified, as shown in the log\nI0624 20:57:55.170355 18247 daemon.go:215] The hypervisor's driver is qemu\n. Enjoy it \ud83d\udc4d . @punitagrawal what's the Go version ?. Thank you for the response @punitagrawal . Almost done, will do test/fix and add more unit test cases. Functional test finished, usage:\nPort Mapping configuration in Pod Level\njson\n{\n  \"portmappings\": [{\n    \"containerport\": \"80\",\n    \"hostport\": \"3000\",\n    \"protocol\": \"tcp\",\n  },{\n    \"containerport\": \"8000-8080\",\n    \"hostport\": \"3000-3080\",\n    \"protocol\": \"tcp\",\n  }]\n}\nPublish port from command line\nhyperd@[Go:(Workspace)]\u279c sudo ./hyperctl run -t --rm --publish 3000:1300 busybox:latest /bin/sh\n/ # nc -l -p 1300\naaa\n/ # exit\ncorrespond request from outside\n```\nnc 192.168.142.136 3000 <<END\naaa\nEND\n```\nPublish port range\nhyperd@[Go:(Workspace)]\u279c time sudo ./hyperctl run -t --rm --publish 3000-3100:1300-1400 busybox:latest /bin/sh\n/ # nc -l -p 1300\naaa\n/ # exit\nDynamically change portmapping with hyperctl ports subcommands\nhyperd@[Go:(Workspace)]\u279c  sudo ./hyperctl ports add -p 3000:1300  busybox-latest-4488498752\nhyperd@[Go:(Workspace)]\u279c  sudo ./hyperctl ports ls  busybox-latest-4488498752\nProtocol            Host Ports          Container Ports\ntcp                 3000                1300\nhyperd@[Go:(Workspace)]\u279c  sudo ./hyperctl ports delete -p 3000:1300  busybox-latest-4488498752\nhyperd@[Go:(Workspace)]\u279c  sudo ./hyperctl ports ls  busybox-latest-4488498752\nProtocol            Host Ports          Container Ports\nhyperd@[Go:(Workspace)]\u279c. added an example pod for podmapping:\nexamples/port-mapping.pod:\njson\n{\n        \"id\": \"port-mapping-test\",\n        \"containers\" : [{\n            \"name\": \"pmtest\",\n            \"image\": \"busybox:latest\",\n            \"command\": [\"/bin/nc\", \"-l\", \"-p\", \"1300\"]\n        }],\n        \"resource\": {\n            \"vcpu\": 1,\n            \"memory\": 128\n        },\n        \"portmappings\": [{\n            \"containerport\": \"1300-1310\",\n            \"hostport\": \"3000-3010\",\n            \"protocol\": \"tcp\"\n        }]\n}\n. ready for review @bergwolf . the new commit re-enable the in sandbox portmapping for hypernetes, controlled by (same as before):\n\ndisableIptables configuration entry in config file\nportmappingWhiteLists fields in pod spec\n\nAn example pod proviede in hyperd/examples/port-mapping-in-sandbox.pod. \nNote: the \"in sandbox portmapping\" doesn't do NAT in hyperd, it should be completed by other components. i.e. during test of this feature, you should access the ip address of the container instead of the host's.. stop it with SIGHUP instead of SIGTERM. @haoyixin stop hyperd with kill -HUP, it won't stop the qemu processes.. ok, I will try my best to update my PR today. The two qemu case on travis are always failed even if I close/re-open the PR, however, the failed point looks nothing related with qemu (often be pulling image failed). And the hykins test could complete successfully. @haoyixin you can't enable qboot with default kernel/initrd because of the rom size restriction of qboot. The size of kernel/initrd is a trade off between footprint and functionality. However, if you enable the template/cache configuration in the configuration file, it could start in a few hundred milliseconds with the bigger kernel/initrd.. @minz1027 the DEFAULT_DM_VOL_SIZE is the default value in case:\n\ndevicemapper is chosen as the storage driver\nneed create a volume by the daemon\n\nAnd in most cases, the volumes are created by an external dedicated volume management system, and passed to the daemon. This is why it is not an configurable item now.\nHowever, it is welcome to submit a PR for adding an size field for the pod spec.. @minz1027 currently, only the block devices support a \"size\" property. Once you have a size field, you may apply it to all storage drivers. For block devices, such as devicemapper and rawblock, they may create a volume with specified size. And for directories, the might be implemented based on quota, which depends on the feature of the underlying filesystem.. I think you may change the interface of create volume, \nhttps://github.com/hyperhq/hyperd/blob/bb3ac60ffd5b7c5dfe03f1da56b77b24a5683db6/daemon/storage.go#L204\nhere we create the volume with default size, and you may change it to employ the size specified in pod spec.. @minz1027 thank you, and it would be released in this week when #664 get merged . - depends on #657 \n- docs updates and release notes in hyperhq/docs.hypercontainer.io#27. @minz1027 @antoineherzog sorry folks, I have had to stay in hospital for a week accidentally in the past 7 days, and now, let's do it.. @laijs the reason I didn't move protoc-gen-gogo to hack/ is -- I found in kubernetes's repo, I found some auxiliary tools were placed in cmd/ dir as well...  . @laijs updated, removed the binary files checked in by mistake, and update .gitignore to avoid checking in binary files.. hyperd is intended to be deployed on physical machine by default, and it employs hypervisor to provide isolated containers. \nFor running in VMs, you could try to use xenpv driver with xen 4.9 environment, ref: https://blog.hyper.sh/performance_runv_gce.html . . Looks the qemu didn't start correctly. Could you run hyperd with -v=4 and see what's in the logs.\nI am sorry I couldn't reproduce it because I don't have a s390x machine . looks like your qemu did not build with VirtFs enabled? . could you find the same message\nqemu-system-s390x: -device virtio-9p-ccw,fsdev=virtio9p,mount_tag=share_dir: 'virtio-9p-ccw' is not a valid device model name\n. could you find the qemu command line and run it directly? I think there are still some qemu issue, but could not get any useful info here. It's too hard to debug without an environment to reproduce it.... thank you @pmorjan \nnice to see you again here.. @xuchenhao001 do you have this parameter -device sclpconsole,chardev=charconsole0 provided by @pmorjan . Hi @m-barthelemy , could you show the GOPATH environment?\nActually the official rpm and deb binary packages are built with the same source as well, and the build was done inside a clean docker image (ref: https://github.com/hyperhq/hyperd/tree/master/package). I think the FTBFS should be related with environments.. I am not sure whether it is the root cause, but xen 4.4 is not supported.\n\nFor xen full virtualization, 4.5 at least;\nFor xen paravirtualization, 4.9 at least;. @Ddnirvana sorry, I mistake read the messages... Will take a further look soon.. @Ddnirvana checked and this is caused by golang cgo update. Will update the code for the compatible with latest go compiler.. This is a bug introduced by hyperhq/runv#597 , and will be fixed soon.\n\nIt also affected #690 . @Ddnirvana xen build has been fixed on master, and this issue is auto closed by github. If you still suffer from this issue, you may reopen it.. @m-barthelemy looks like a bug, will check it.. @m-barthelemy I confirmed this is introduced here by hyperhq/runv#592 .\nIt will be fixed soon and add additional tests for it.. #692 fixed the issue\n@m-barthelemy if it is not fixed in your side, please reopen this issue.. @gogolxdong Is your QEMU built with virtfs enabled?. qemu started failed, what's qemu version and OS distro are you working on @yangwanli2017 ?\nAnd looks you are working with runV instead of hyperd.. ok to test, @hykins. LGTM. @nanzhong sorry for the very late reply. \nYes, what you mention is correct, and the change you mentioned is explained in the following comment\nhttps://github.com/hyperhq/hyperd/pull/704#issuecomment-364733623\nFor backward compatibility, we had to make the ifname be optional, and as you said this made us lose a primary key for interface.. We adopted the fix in #704, which is a more completed solution for the issue. However, the analysis in this PR is really helpful. Thank you very much. looks like a container name as c1 had not been cleaned up well... Is this the first time of failure? I guess there were different fail log for the first time.. @zero-88 I checked the server code and suspect it is related with some client code. Which language SDK are you working with?\nPS: this repo is not the hyper.sh server code, and it is the open source local runtime daemon repo. And I suggest you submit a topic in the hyper.sh forum , or slack ; or file issue in the API repo if you are using the official golang SDK.. need confirm if this is fixed in frakti\ncc @resouer . confirm fixed after update to updated xen package. This PR will fix #679 . duplicated with #690 . ok to test, hykins. retest this please, hykins. lgtm. when you run it with a pod file, you could specify the dns field in the pod spec, and it is a slice of string ([]string), you may specify a list of nameservers. no, you can't map a socket as volume.. ok to test, hykins. Read the related code again, and in short, this has not solved the issue (it's solved it in one case occasionally). I will write why and how the issue occured. . The issue comes from the define of interface structure. Let's come back to the struct before merging the mtu PR,\nmessage UserInterface {\n  string bridge = 1;\n  string ip     = 2;\n  string ifname = 3;\n  string mac    = 4;\n  string gateway  = 5;\n  string tap    = 6;//discarded\n}\nThe original definition is:\n\nifname: the id for the device\ntap: device name in the host\n\n, where the name in host could be empty and auto assigned.\nHowever, this definition break the backward compatibility for hypernetes and kubernetes/frakti, which use ifname for name in the host, and is optional.\nAfter we discard the tap field and use the ifname as the name of host device, we lose the primary key of interface data struct. \nEven with this PR, if there are more than one interfaces in the spec, there will be problem when initResources()\np.interfaces[nspec.Ifname] = inf\nThe proper solution should be\n\nadd back a Id field in types.proto\nstring id  = 100;\nand use the id in  initResources() , check empty and assign random or indexed name for it.\n\nassined the id to description in prepare(), here and here. and assign the spec.id to descript.Id \n\n\nhttps://github.com/hyperhq/hyperd/blob/master/daemon/pod/networks.go#L48\n\nhttps://github.com/hyperhq/hyperd/blob/master/daemon/pod/networks.go#L60. lgtm. LGTM. what's the relation with https://github.com/hyperhq/runv/pull/673\n\nhyperhq/runv#673 addresses the double close as well.. discussed with @teawater , confirmed the case\nwhen sandbox unexpected quits right after stop command received but before stop operation beginning:\n\nhere https://github.com/hyperhq/hyperd/blob/master/daemon/pod/decommission.go#L370 , we set the stopping status, and do not have the resourceLock\nhere https://github.com/hyperhq/hyperd/blob/master/daemon/pod/decommission.go#L543 , cleanup() holds the lock before doStopPod() ( https://github.com/hyperhq/hyperd/blob/master/daemon/pod/decommission.go#L374 )\ncleanup() will clean the sandbox resource and status, then set p.sandbox to nil through decommissionResources() at https://github.com/hyperhq/hyperd/blob/master/daemon/pod/decommission.go#L615\nwhen doStopPod() holds resourceLock finally, it may access to the nil p.sandbox.. looks like some bugs in rollback procedures.\n\nWhat is the a7c41708ef58?. could you try hyperctl run with another name? \nand could you try list all pod and containers? Looks the message shows there is no pod named as hello, but there has already been a container named with /hello. @joelmcdonald which version of hyperd are you working on? master branch or a binary package?. After the release, there are quite a few fixes landed in master branch, and we are going to release a new package in a week (some of the PRs have not been merged), before that, would you mind try built with the latest master?. could you try these RPMs:\nhttps://s3-us-west-1.amazonaws.com/hypercontainer-build/upload/1.1-rc1/hyper-container-1.0.0-1.el7.src.rpm\nhttps://s3-us-west-1.amazonaws.com/hypercontainer-build/upload/1.1-rc1/hyperstart-1.0.0-1.el7.x86_64.rpm\nI have just built them for release test, and have not update the package meta (still show version 1.0.0-1).. could you purge your old pods and try again with the new binaries?\npurge the /var/lib/hyper/ stuff, but leave /var/lib/hyper/{kernel,hyper-initrd.img} not delete. Weird, as it report name conflict, the old leveldb file should still be there.. it should under /var/lib/hyper/lib/. retest this please, hykins. ok to test, hykins. Travis report a build error\n\n../../daemon/pod/persist.go:362: unknown field 'ContConfig' in struct literal of type \"github.com/hyperhq/hyperd/types\".PersistContainer\n. LGTM\n\nlooks reasonable. why didn't we do this before trying resource lock on info and the snap?. CI passed\nCheers. @spearl Thanks very much for reporting this. This bug is left here because the IP addresses were managed by external network controllers in production systems and there are few cases to cover these leakages.\nTo solve this, I prefer\n\nadd pod interface cleanup to the synchronous shutdown code path when the interface is deleted from leveldb\n\nWould you like to contribute a fix for this bug? Otherwise we will fix it.\n. Thank you @spearl \nwill push it forward asap. will check it asap, thanks for reporting @ukd1 . Fixed by @Jimmy-Xu \nThe cert expiration is caused by an out-of-date certbot, and we enabled auto-upgrade of it now.. we should have #685 merged and #733 fixed before the release. And the docker images for package new binaries are ready here: hyperhq/hypercontainer-packager. Both merged, will begin to make the new packages for release.. We are confirming and fixing some new recent issues, and will file the release PR in two days.. ok to test, hykins. @teawater could we use the docker/distribution to parse the image references?\nIf it changes too much, I am fine with the current patch to help the 1.1 release sooner.. alright, merge it firstly. @teawater any ideas on this?. @Chen8132 thanks for reporting, will check it.\nIt would be appreciated if you could paste related logs which located in /var/log/hyper/ by default. . OK, will take a look. Thank you. @Chen8132 I checked the code of these two flags, and both designed for OCI Image format\ngolang\n        Name  string   `short:\"n\" long:\"name\" value-name:\"\\\"\\\"\" description:\"Name to use when loading OCI image layout tar archive\"`\n        Refs  []string `short:\"r\" long:\"references\" value-name:\"\\\"\\\"\" description:\"References to use when loading an OCI image layout tar archive\"`\nI don't think it could work proper with docker images. These two options are not for the images saved by docker. Did you try using the -t option?. try hit the enter key after attach, if your container is launched with -t. Sometimes it simply no output after you attached. And could you got output if you exec a bash -c 'echo \"hello, world\"' if you have bash in the container.. Specific image or no output with any images? And could you get the output with hyperctl logs?. any explanation? such as which patch in fsnotify fixed this issue. It's hard for the reviewer to digg from the big change set. thanks @teawater \nLGTM now. merge this after 1.1 release. Why just ContainerCreate? From my point of view, I think the PodCreate may suffer from the parallel requesting issue more because it leads to a sandbox creation operation.. @sjkeerthi do you mean the host or guest platform?. There had been some effort on windows container support but did not complete because some windows networking issue.. you may find the version at the beginning of the logs (by default /var/log/hyper/hyperd.INFO):\nI0924 10:30:44.809559   20376 hyperd.go:189] Hyper daemon: 1.1.0\n. And checked with the code\n```\nhyperctl version\nThe hyperctl version is 1.1.0\n```\nIt's supported. One more thing, you may query the server with API\n```\necho -e \"GET /version HTTP/1.0\\r\\n\" | nc -U /var/run/hyper.sock\nHTTP/1.0 200 OK\nContent-Type: application/json\nServer: Docker/library-import (linux)\nDate: Mon, 24 Sep 2018 14:40:27 GMT\nContent-Length: 31\n{\"ID\":20376,\"Version\":\"1.1.0\"}\n``. Both the client side and server side. I think it should be documented. What's the return value ofreadBodyin case the image does not exist? What's the difference between the 2 cases?\n. this debug print should be removed\n. v is not defined, scope problem\n. I think this patch changed the semantics of theCreateVolume`. In the previous code, the daemon checks the user spec, and only creates volume when the user doesn't specify the source of the volume.\n. https://github.com/hyperhq/runv/blob/master/hypervisor/pod.go#L152\nwithin SetPodContainerStatus, the mypod.status and mypod.FinishedAt are both set. This is why I said here is a bug\n. it is called when pod stopped.\n. https://github.com/hyperhq/hyperd/pull/301/files#diff-cfb48529ee9117af9bd2e69c80e66291R25\nevery time pod stopped, this method will be called. No matter whether remove the pod.\n. ok, this is just copied from the original code.  \nwill update this, and are there any other comments.\n. sorry, failed to clean up, will update\n. @laijs updated\n. err... reversed order.....  should be\nvar outStream io.Writer = wf\n    errStream := outStream\n    if !container.Config.Tty {\n        errStream = stdcopy.NewStdWriter(outStream, stdcopy.Stderr)\n        outStream = stdcopy.NewStdWriter(outStream, stdcopy.Stdout)\n    }\n. The definition is in runV project, will submit the PR to remove it after this is merged\n. use HyperContainer Dev Team <dev@hyper.sh> here.\n. use HyperContainer Dev Team dev@hyper.sh here.\n. not quite sure.\nI don't know if there could be a github url. For safety, we can remove the two lines.\n. currently, it only build for amd64, and might have more in the future.\n. hypercontainer\n. hypercontainer\n. hypercontainer\n. replace with a one line description, reference the rpm\n. 2016 HyperHQ\n. http://hypercontainer.io\n. replace with Apache 2.0, which could be found in the LICENSE file in the repo\n. I am not quite sure. If you want to build for ubuntu, maybe you should use the latest ubuntu distro code name instead of unstable. IIRC unstable is for new/updated regular debian packages.\n. I think you could list these packages in Build Dependencies of control meta file:\nlibdevmapper-dev libsqlite3-dev libvirt-dev\nAnd I think this two file is not required for build procedure: qemu libvirt-bin\n. Normally, we use a flag -rfakeroot to build package with non-root user. But if your install procedure do not need this, omit this comment.\n. IIRC, the distribution field should be a single word, like this (soruce)\n```\nhello (2.8-0ubuntu1) trusty; urgency=low\n\nNew upstream release with lots of bug fixes and feature improvements.\n\n-- Jane Doe packager@example.com  Thu, 21 Oct 2013 11:12:00 -0400\n```\n. \bHere you spot an error, but won't interrupt the stream. Is this what you want exactly?\n. I think you could remove this line simply. \nAnd if you set it to false, could we open it through the command line parameter?\n. How about change if err != nil { to if copt == nil {. Will update this. Actually this is copied from the original one, I don't know if you are depends on this behavior.. Is this ok using %s for err?. Then every container will listen the tty stream even if one does not have log at all, right?\nIf yes, I like it.. I think you should just pass this volume if v.Detail is not nil. The legacy pod specs do not carry this field, such as those for travis test.. how about make an if...else with the following lines. Should we remove the volume file here? Or is it created in the Pod dir?. The hyperd inject resolv.conf file with InjectFile by default. And we have inject file cases in CI scripts. I think we need this to be implemented finally.. This commit supports btrfs only, right?. Dose this work? or just use exec for debug?. Won't this failed if it is not on btrfs?. Got it. Thank you. should be simple-volume.pod instead of single-volume.pod, or you should change the file name. I think you don't need to list the container, and could remove the container by name.. you should remove the pod before the last test $res -eq 0 to pass jenkins test.. could you just hyperctl rm -c container-with-volume, then you don't need to get the container id via a list.. I suspect here may lead to panic if config.user is something like \"root\" (no group at all). IMHO, if graceful == 0, we should not wait, and fire force kill directly.. Then, do we need wait 5 seconds and kill again?. If graceful == 0, does this mean the user don't want to wait anything, just return at once?. do we still need this as we implemented it in the command line side?. build failed..... looks a tab in the inserted line, which looks different in different view.... In case there are several op wait pod running, only one could get notification from the chan, and send back to let others get the event.. I don't think so. But I am thinking whether I should change to Cond/Wait here..... hmm.... I didn't change this line, but looks you are right..... let's leave this status in later issue/pr. \nIn principle, if a pod failed and could be restarted, it should be set back to stopped status. Only those meet fatal exception and could not start any more should stay in ERROR status. \nHowever, sometimes the daemon can not judge the situation accurately. And I think it might be better if we could allow a user to set back status from ERROR to STOPPED manually.. yes, never. Just got a canceller, and don't want to waste it....... sth. similar here\nhttps://github.com/hyperhq/hyperd/pull/605/files#diff-3a27bed0599dd83f8d78dded991998deR81. will this cause double Close() of stdout? And does this hurt?. add Mtu in line 53 as well.. what's the magic \ud83d\udcaf . why is the id different with the Id field in the data structure? should we correct the data structure instead of pass a new id here?. not sure if this is what you want: https://play.golang.org/p/L6-rWGbuHbn. put path and json to the above official packages group . do we still need this field?. remove the line commented out, we have git to find it back :). ",
    "bketelsen": "That solved the problem, thank you.\n. ",
    "carmark": "Hi Brian,\nReally thanks for your help.\nBR\n. Hi Pekka,\nYou need to move the hyper repository into your $GOPATH/src/ and re-compile it.\nBR\n. @penberg \nYour GOPATH is /home/penberg/go/, so you need move the hyper directory into /home/penberg/go/src/.\nSorry for the inconvenience, it may be modified later.\n. Hello @achanda,\nSince there is also a third-party library in the code, I will not merge your pull request.  However, your idea is really great to build this dependency.  I did the work based on your idea to use godep to make this work.\nThanks again.\nRegards,\n-Simon\n. Hello Henry,\nThanks for trying hyper.  You can use hyper list container command to check the running containers if you want.\nMore details regarding with list sub-command are following:\n```\nhyper list --help\nUsage:\n  hyper list [pod|container]\nlist all pods or container information\nHelp Options:\n  -h, --help  Show this help message\n```\n. @hustcat \n\n[HYPER ERROR 0601 17:53:39 03555 hyperd.go] The hyperd create failed, An error encountered returned from Docker daemon, client and server don't have same version (client : 1.17, server: 1.15)\n\nThis error is caused by the different version between docker daemon and our docker rest api, you may have to update the docker.\nBy default, the bios.bin and cbfs.rom should be in /var/lib/hyper, you can verify it and put their paths in the config file if you want it.\n. fixed by https://github.com/hyperhq/hyper/commit/b569471193d5a730deb5e509ec2fb1fd22c659d4\n. @darkheaven1983  Thanks for you reported this issue.  Do you encounter this issue if you specific a command for the hyper run, such as hyper run centos:7 /bin/sh?\nRegards,\n. @darkheaven1983 \nCould you please remove that centos:7 image and pull it again?  I did the test locally, but it works well.  I am not sure whether the image of centos:7 has issue, so you can remove and try it again?\nAlso could you please try hyper run ubuntu if it won't work.\nThanks,\n. Hello @hjianhao ,\nThanks for you to try the hyper. The hyper uses Qemu+KVM as the virtual machine, I think you installed the KVM and Qemu.  However, the vbox can not run while the kvm kernel module is loaded.\nIf you want to run the vbox, you may remove the kvm module and run it again.\nFor the Hyper uninstall, you can stop the Hyper service and remove the files from /var/lib/hyper and the binaries.\nThanks,\n-Lei\n. Hi @holidayworking \nThanks for reporting this issue, I did reproduce this, and I will figure out.\nFor my local test, it works well to run a pod by sudo hyper run -p mysql.pod, you may try that as a workaround.\nThanks,\n-Lei\n. Hello @chiefy ,\nReally thanks for your trying hyper on Mac, I did reproduce this issue locally.  I think this should be a bug, since there is no default command in Docker official dockerfile.\nFROM scratch\nADD rootfs.tar.gz /\nI will try to fix this ASAP and provide a new binary.\nThanks again.\nBR\n. Hi @chiefy ,\nI fixed this issue, and did some unit test, it works well, you may check the following output:\nlei@leis-MacBook-Pro:[~]$ hyper run --attach alpine /bin/ash\nPOD id is pod-BGPgUpQsjX\n/ # ls\nbin      dev      etc      home     lib      linuxrc  media    mnt      proc     root     run      sbin     sys      tmp      usr      var\n/ # uname -a\nLinux alpine-4805162867 4.0.4-hyper #81 SMP Tue Jul 28 19:10:58 CST 2015 x86_64 Linux\n/ # echo $0\n/bin/ash\n/ #\nYou can download the latest binary from here\nPlease let me know if you encounter any other problem with this new package.  I will do more test on this package and replace the old one later.\nThanks,\n. Hello @chiefy ,\nThanks for your report, I also think it is a VirtualBox issue, could you please restart your system and run it again?  I did not encounter that before.\nRegards,\n. @chiefy \nI searched this error message and found it should be an issue in VirtualBox forum and checked the code of VirtualBox, I suppose it should init the state based on previous task.  I can not reproduce this issue, but you may try to run the following commands:\n```\nlaunchctl unload /Library/LaunchDaemons/sh.hyper.hyper.plist\nvboxmanage unregistervm hyper-mac-pull-vm --delete\nrm -fr /var/lib/hyper/\n```\nRe-install the hyper package and try it again.\nRegards,\n. Hi @jefby ,\nYeah, hyper can wok with KVM.  You can configure your kernel and initrd in the config file.\nRegards,\n. @jefby \nYou can compile the hyperd without xen support via the following command:\n./autogen.sh\n ./configure --without-xen\n make\nAnd then you can specify that config file with --config=xxx option, you may refer this link if you want more details.\nRegards, \n. @Termina1 \nNow, we have no plan to implement the hyper with xhyve, but any contribution is welcome.\n. Hello @chiefy ,\nIt will be great that you can create a Homebrew formula, thanks.  Now, I am doing refactor on the source code on Mac, and open it later, so I did not add tagging/releasing.  The latest hyper version on Mac is 0.3, but the previous one(0.2) is for Linux(xen support).  I am not familiar with the process of Homebrew, can I use the 0.3 as the initial version for hyper in Homebrew?\nRegards,\n. Hello @YaoZengzeng ,\nCurrently, you can not do live migration with virtfs.\nRegards,\n. Hi @dhargitai ,\nCould you please provide more details, such as\n- hyper info \n- Which image you want to build\n- Dockerfile\nThanks,\n. Thanks @dhargitai , I will try to reproduce and investigate this issue.\nI will update once I have any finding.\n. @darylteo Could you please provide the build.sh script?\n. @darylteo  @dhargitai \nThanks for your support.\nFrom your dockerfile, I doubt that this build fail may be caused by network issue, since both of you tried to install the additional package via apt-get.  I will do more test and investigation.\nRegards,\n. I have investigated this issue for a while, but got nothing.  With the latest source code from master branch, I did encounter other issues.  It looks like the hyper on Mac OSX with virtualbox has a long way to be product.  I will continue to fix the issues on Mac with VirtualBox.\nThanks,\n. Hi @darylteo \nReally thanks to your suggestion, we will consider this enhancement.\nRegards,\n. Hi @YaoZengzeng \nThanks for your work, it will be interesting if you have any update.\nRegards,\n. @gao-feng fixed by https://github.com/carmark/hyper/commit/d61ef01fd3456873da685b27d8aaaa250094b025\n. @gao-feng  They are same.\n. LGTM\n. @Jimmy-Xu Could you please check this?\n. hello @a93ushakov \nFor the current version, you need to copy your own resolv.conf file before you want to download some files.  We will fix this DNS issues in next release, others works well.\n. @gao-feng Yes, I noticed that there is no protocol configured by docker.\n-p, --publish=[]           Publish a container's port to the host\n                               format: ip:hostPort:containerPort | ip::containerPort | hostPort:containerPort | containerPort\n                               (use 'docker port' to see the actual mapping)\n. @feiskyer @jstoja \nThis issue should be encountered while creating a new pod, but the image is not exist.   The patch https://github.com/hyperhq/hyper/pull/53 fix this.\n. @huikang Could you please provide your log?\n. Tested by @feiskyer .  Ready to merge.\n. LGTM\n. @laijs Yeah, it should be. I will modify it.\n. Could you please review it again with the updated patch? @laijs \nAnd the test result is:\nWithout fix:\nlei@leis-MacBook-Pro:[hyper] (master)$ ./hyper images\nREPOSITORY     TAG                 IMAGE ID                           CREATED             VIRTUAL SIZE\npuller         latest              100000000000                2015-07-18 06:01:13                 0 B\nbusybox        latest              8c2e06607696                2015-04-18 06:01:13              2.3 MB\nlei@leis-MacBook-Pro:[hyper] (autopull)$ ./hyper create examples/ubuntu.pod\n./hyper ERROR: could not find image: no such id: 8251da35e7a79dca688682f6da6148a06d358c6f094020844468a782842c2172\nWith fix:\nlei@leis-MacBook-Pro:[hyper] (autopull)$ ./hyper images\nREPOSITORY     TAG                 IMAGE ID                           CREATED             VIRTUAL SIZE\npuller         latest              100000000000                2015-07-18 06:01:13                 0 B\nbusybox        latest              8c2e06607696                2015-04-18 06:01:13              2.3 MB\nlei@leis-MacBook-Pro:[hyper] (autopull)$ ./hyper create examples/ubuntu.pod\nlatest: Pulling from ubuntu\nd3a1f33e8a5a: Pull complete\nc22013c84729: Pull complete\nd74508fb6632: Pull complete\n91e54dfb1179: Already exists\nubuntu:latest: The image you are pulling has been verified. Important: image verification is a tech preview feature and should not be relied on to provide security.\nDigest: sha256:fde8a8814702c18bb1f39b3bd91a2f82a8e428b1b4e39d1963c5d14418da8fba\nStatus: Downloaded newer image for ubuntu:latest\nPod ID is pod-UFRYUuCCec\nlei@leis-MacBook-Pro:[hyper] (autopull)$ ./hyper images\nREPOSITORY     TAG                 IMAGE ID                           CREATED             VIRTUAL SIZE\nubuntu         latest              91e54dfb1179                2015-08-21 04:21:15            179.6 MB\npuller         latest              100000000000                2015-07-18 06:01:13                 0 B\nbusybox        latest              8c2e06607696                2015-04-18 06:01:13              2.3 MB\nlei@leis-MacBook-Pro:[hyper] (autopull)$ ./hyper list\n         POD ID                      POD Name             VM name    Status\n pod-TULrekKtvx   test-container-create-ubunt                       pending\n pod-bAnFGVNYMn            busybox-6252283306                       pending\n pod-UFRYUuCCec   test-container-create-ubunt                       pending\nlei@leis-MacBook-Pro:[hyper] (autopull)$\n. @jstoja This is a known issue of golang on Redhat, you may check that above bug link.\n. LGTM\n. fixed by https://github.com/hyperhq/hyper/pull/57\n. LGTM\n. This have been fixed by https://github.com/hyperhq/hyper/commit/d157ab756960f3e032896757c9822e7ca0bdba44, would @xiaods like to try that commit or wait the latest version(hyper#0.4.0)?\n. @danielbodart \ncould you please provide more information? such as hyper info and host os information.\n. @danielbodart  Thanks, I will reproduce this on my local environment and fix it ASAP.\n. Hello @danielbodart ,\nI did reproduce this issue, and this bug should be caused by DNS issue.  Since there is no DNS setting by the current release(next release will add default DNS setting), and it will be failed when you want to clone the code from github.\nI have a simple workaround, you can create a pod file, like this one:\nroot@ubuntu:/home/lei/src/github.com/hyperhq/hyper# cat examples/crazyfast.pod\n{\n    \"id\": \"test-container-create-crazyfast\",\n    \"containers\" : [{\n        \"name\": \"crazyfast-sh\",\n        \"image\": \"danielbodart/crazyfast.build:latest\",\n        \"files\":  [{\n            \"path\": \"/etc/\",\n            \"filename\": \"resolv.conf\",\n            \"perm\": \"0755\"\n        }]\n    }],\n    \"resource\": {\n        \"vcpu\": 1,\n        \"memory\": 512\n    },\n    \"files\": [{\n        \"name\": \"resolv.conf\",\n        \"encoding\": \"raw\",\n        \"content\": \"nameserver 8.8.8.8\"\n    }],\n    \"volumes\": [],\n    \"tty\": true\n}\nThat container's name must be unique, or you can not create the pod successfully.  I used this pod file and attach that created pod, you will find that the cmd clone the code from github.  However, there are some errors, I am not sure what it is, such as:\n```\nroot@ubuntu:/home/lei/src/github.com/hyperhq/hyper# ./hyper attach pod-VQMBfJsrpM\nclone code from github\nerror: unable to open object pack directory: /crazyfast.build/totallylazy/.git/objects/pack: Invalid argument\n.\n.\n.\nerror: unable to open object pack directory: /crazyfast.build/totallylazy/.git/objects/pack: Invalid argument\nerror: unable to open object pack directory: /crazyfast.build/totallylazy/.git/objects/pack: Invalid argument\nerror: unable to open object pack directory: /crazyfast.build/totallylazy/.git/objects/pack: Invalid argument\nerror: unable to open object pack directory: /crazyfast.build/totallylazy/.git/objects/pack: Invalid argument\nResolving deltas: 100% (14856/14856), done.\nfatal: Unable to create temporary file '/crazyfast.build/totallylazy/.git/objects/pack/tmp_idx_XXXXXX': Invalid argument\nfatal: index-pack failed\nls: cannot access */: No such file or directory\n./run: line 8: ./jcompilo.sh: No such file or directory\n```\nI will check your dockerfile and try to find out it.\nThanks,\n. Thanks for your report, I will check it again.\nRegards,\n-Lei\n. @danielbodart \nFor now, the priority is defined in lib/docker/daemon/graphdriver, and it is \nPriority = []string {\n    \"aufs\",\n    \"overlay\",\n    \"devicemapper\",\n    \"vbox\",\n    \"vfs\",\n}\nYou may need to modify that above code and rebuild hyper if you want to use overlay as the storage backend.\n. @gnawux It is a good choice to make it configurable, I will do that fix.\n. @gnawux \nYeah, the slash is invalid character in a container's name.\nI also did think of doing a special process for that slash, but the user may not care about that container's name if container's name is not specified.   So a valid and random container's name should be all right.\n. sounds reasonable, I will update it.\n. @gnawux \nupdated, could you please review it again?\n. @xiaods \nYeah, that's great.\nYou can contact me if you need any help on implementing this feature. Both Slack and Wechat are all right.\n. Yes, it is definitely a regression bug caused by pr#70.\nCode LGTM\n. @gnawux \nI re-checked the code and removed the unnecessary code. \nTest cases(run command and create command) passed.\n. Test cases passed, code LGTM\n``\nlei@ubuntu:~/src/github.com/hyperhq/hyper$ make\nmake[1]: Entering directory/home/lei/src/github.com/hyperhq/hyper'\ngo build -tags \"static_build libdm_no_deferred_remove\" -ldflags \"-X github.com/hyperhq/hyper/utils.VERSION 0.4.0\" hyperd.go\ncommand-line-arguments\nlink: warning: option -X github.com/hyperhq/hyper/utils.VERSION 0.4.0 may not work in future releases; use -X github.com/hyperhq/hyper/utils.VERSION=0.4.0\ngo build -ldflags \"-X github.com/hyperhq/hyper/utils.VERSION 0.4.0\" hyper.go\ncommand-line-arguments\nlink: warning: option -X github.com/hyperhq/hyper/utils.VERSION 0.4.0 may not work in future releases; use -X github.com/hyperhq/hyper/utils.VERSION=0.4.0\nmake[1]: Leaving directory `/home/lei/src/github.com/hyperhq/hyper'\nlei@ubuntu:~/src/github.com/hyperhq/hyper$ go version\ngo version go1.5beta1 linux/amd64\nlei@ubuntu:~/src/github.com/hyperhq/hyper$ git status\nOn branch rmi\nChanges not staged for commit:\n  (use \"git add ...\" to update what will be committed)\n  (use \"git checkout -- ...\" to discard changes in working directory)\nmodified:   lib/docker/pkg/devicemapper/devmapper_wrapper.go\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nlei@ubuntu:~/src/github.com/hyperhq/hyper$\n``\n. LGTM\n. You may need dogo fmton the changes' files.\n. LGTM\n. LGTM\n. code LGTM\n. code LGTM\n. code LGTM\n. Two issues are found after my manual test with vbox 5.0.8.\n-runcommand can not attach to the container\n-pull` command does not work\n. @CMGS \nHow do you get the hyperd binary, build it from source code?\n. this issue will be fixed by https://github.com/hyperhq/hyper/pull/121\n. No more issues with the master source code.\n. Would you please run the hyperd with --v 4 and paste it again?  It will print debug messages with that option.\nAnd could you please show the result of hyper images to show the images list?\n. Thanks. Do you build the latest hyperstart guest os from master source code?\n. Hello @zhengxiaochuan-3 \nThe option --insecure-registry is not supported now, we will consider to add it.  Could you please try a workaround like the warning said?  \n\nsimply place the CA certificate at /etc/docker/certs.d/192.168.212.23:5060/ca.crt\n. LGTM\n. Yes, some options(such as --registry-mirror and listed options in https://github.com/hyperhq/hyper/issues/59) are not supported now.  We have to use VPN to access the docker hub, the daocloud mirror seems not a valid registry.  You can specify the private registry if you want to set it up on your network.\n\nThanks for your trying and interesting.\n. fixed by https://github.com/hyperhq/hyper/pull/149\n. long time no update, so close\n. +   var (\n+       spec  pod.UserPod\n+       async = true\n+   )\n+   json.Unmarshal([]byte(podJson), &spec)\n+\n+   vmId, err := cli.CreateVm(spec.Resource.Vcpu, spec.Resource.Memory, async)\n+\n    podId, err := cli.CreatePod(podJson, opts.Remove)\nThis process still starts pod one by one, creating vm and pod, then start it.  How about using API interface of /pod/run?  It will create pod during vm's creating and starting, and then associate the pod into the vm.\n. OK, fine for me.  I think it is good to remove those code(/pod/run) with this pr.\n. ```\nKernel=/var/lib/hyper/kernel                   # Boot kernel for KVM/XEN\nInitrd=/var/lib/hyper/hyper-initrd.img         # Boot initrd for KVM/XEN\nVbox=xxxx                                      # Boot image for vbox\nBios=/var/lib/hyper/bios-qboot.bin          \nCbfs=/var/lib/hyper/cbfs-qboot.rom\nBridge=\nBridgeIP=\nHost=                                          # Host address, same as the '--host' option\nStorageDriver=overlay                          # Hyper storage backend driver\nRoot=xxxx                                      # Root directory for hyper\nHypervisor=kvm                                 # Hypervisor which is used by hyper\nDisableIptables=false                          # Disable the iptables\nLogger=xxxx                                    # Not sure???\n```\n. The travis-ci should pass, but get failure:\n``\n$ make\nmake[1]: Entering directory/home/travis/gopath/src/github.com/hyperhq/hyper'\ngo build -tags \"static_build libdm_no_deferred_remove\" -ldflags \"-X github.com/hyperhq/hyper/utils.VERSION 0.4.0\" hyperd.go\ngo build -ldflags \"-X github.com/hyperhq/hyper/utils.VERSION 0.4.0\" hyper.go\ngithub.com/hyperhq/hyper/client\nclient/hijack.go:76: undefined: \"github.com/hyperhq/runv/lib/term\".TtySplice\nmake[1]: *** [build-hyper] Error 2\nmake[1]: Leaving directory /home/travis/gopath/src/github.com/hyperhq/hyper'\nmake: *** [all-recursive] Error 1\n```\n. Locally, I blocked the docker registry(index.docker.io) via adding it into/etc/hosts, then it won't pull any images.  It works well if I add the--registry_mirror=http://883cbacd.m.daocloud.io` option.\nBefore\nlei@ubuntu:~/src/github.com/hyperhq/hyper$ ./hyper pull busybox\nlatest: Pulling from library/busybox\n583635769552: Verifying Checksum\nb175bcb79023: Verifying Checksum\nPulling repository docker.io/library/busybox\nlei@ubuntu:~/src/github.com/hyperhq/hyper$ ./hyper images\nREPOSITORY          TAG                 IMAGE ID            CREATED               VIRTUAL SIZE\nhaproxy             latest              95e32ba2d22b        2015-12-30 08:55:19   132.6 MB\nhaproxy             1.4                 5c151997083d        2015-12-22 02:49:12   126.5 MB\nnginx               latest              5328fdfe9b8e        2015-12-17 01:29:18   127.7 MB\nAfter\nlei@ubuntu:~/src/github.com/hyperhq/hyper$ ./hyper pull busybox\nlatest: Pulling from library/busybox\n583635769552: Pull complete\nb175bcb79023: Pull complete\nDigest: sha256:c1bc9b4bffe665bf014a305cc6cf3bca0e6effeb69d681d7a208ce741dad58e0\nStatus: Downloaded newer image for busybox:latest\nlei@ubuntu:~/src/github.com/hyperhq/hyper$ ./hyper images\nREPOSITORY          TAG                 IMAGE ID            CREATED               VIRTUAL SIZE\nbusybox             latest              b175bcb79023        2016-01-16 02:06:41   1.1 MB\nhaproxy             latest              95e32ba2d22b        2015-12-30 08:55:19   132.6 MB\nhaproxy             1.4                 5c151997083d        2015-12-22 02:49:12   126.5 MB\nnginx               latest              5328fdfe9b8e        2015-12-17 01:29:18   127.7 MB\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Would you please remove that line in Makefile.am file? It looks unrelated with this commit.\n. LGTM\n. LGTM\n. LGTM\n. This failed is caused by the finish of default command.  It will be running when you setup the command to /bin/sh as follow:\n```\n{\n    \"id\": \"myweb\",\n    \"tty\": true,\n\"resource\": {\n    \"vcpu\": 1,\n    \"memory\": 128\n},\n\n\"containers\" : [{\n    \"image\": \"nginx\",\n    \"command\": [\"/bin/sh\"],\n    \"volumes\": [{\n        \"volume\": \"prod_log\",\n        \"path\": \"/var/log\",\n        \"readOnly\": false\n     }]\n}],\n\n\"volumes\": [{\n        \"name\": \"prod_log\",\n        \"source\": \"/mnt\",\n        \"driver\": \"vfs\"\n}]\n\n}\n``\n. LGTM\n. LGTM\n. Could you please move the\"github.com/hyperhq/hyper/engine\"` into the second part of imported packages?\n. Hello @robrotheram \nThanks for trying hyper, this issue should be a configuration error.  Could you please try to modify the user/group of /etc/libvirt/qemu.conf to root/root and try it again?\n. I am not sure, this pod is managed by k8s, I will check that.  However, shall we allow user' input if the pod is not created with tty enabled?\n@gnawux \n. LGTM\n. LGTM\n. LGTM\n. +1, need recover this and log the stack.\n. Hello @apobbati ,\nYou may download the binary on mac from here, you can find more information on doc. \nPS. The brew install is waiting approved by homebrew, so the current version does not work.\nThanks,\n. Yes, I created a new pr for runv repo, [https://github.com/hyperhq/runv/pull/5].  These two commits should be integrated together.\n. - err = nil seems useless\n- how about moving id=\"\" into var ()?\n. I think we should not support the label format which does not contain \"=\".  And it will be better to delete the space if it has via strings.TrimSpace(v).\n. how about adding qemu and libvirt?\n. cdroom -> CDROM?\nvbox -> VirtualBox?\n. VirtualBox should work on linux, it does support all platforms.  The hyper may use that when virtualbox is available on any system.\n. storage -> Storage?\n. We may fix this format, this is an old format error.\nif -> If\n. fine for me.\n. should we check the args number to avoid the array out of range?\n. same as the above\n. look like this stdoutBuf is useless, should we delete it?\n. ",
    "Jimmy-Xu": "Thanks for your report, will fix it soon.\n. The install script was updated, support jessie now,  please retry.\n. @a93ushakov \n- \"Can not get docker version!\"\n  The reason is docker version output format changed. \n$ sudo docker version\n  Client version: 1.5.0\n  Client API version: 1.17\n  Go version (client): go1.4.1\n  Git commit (client): a8a31ef\n  OS/Arch (client): linux/amd64\n  Server version: 1.5.0  <<----------------\n  Server API version: 1.17\n  Go version (server): go1.4.1\n  Git commit (server): a8a31ef\n$ sudo docker version\n  Client:\n   Version:      1.8.1\n   API version:  1.20\n   Go version:   go1.4.2\n   Git commit:   d12ea79\n   Built:        Thu Aug 13 02:35:49 UTC 2015\n   OS/Arch:      linux/amd64\nServer:\n   Version:      1.8.1   <<----------------\n   API version:  1.20\n   Go version:   go1.4.2\n   Git commit:   d12ea79\n   Built:        Thu Aug 13 02:35:49 UTC 2015\n   OS/Arch:      linux/amd64\nI had updated the install script(merged  some of your code), it support linux mint(17.x) and docker 1.8+ now, \n  just run curl -sSL https://hyper.sh/install | bash to retry.\n- installer code is not in this repo, right?\n  The install script is here https://github.com/hyperhq/hyper-installer\nThanks for report.\n. picutre updated. added some details for demo script.\n. Hyper support debian jessie.\nThe error message comes from here: https://github.com/hyperhq/hyper-installer/blob/master/hyper-bootstrap.sh#L353\nCan you check this file /tmp/hyper-bootstrap-root/hyper-pkg/service/systemd/hyperd.service, after you run curl -sSL https://hyper.sh/install | bash\n\nFYI, here is my env:\nroot@osboxes:/home/osboxes# cat /etc/debian_version \n8.2\nroot@osboxes:/home/osboxes# cat /etc/os-release \nPRETTY_NAME=\"Debian GNU/Linux 8 (jessie)\"\nNAME=\"Debian GNU/Linux\"\nVERSION_ID=\"8\"\nVERSION=\"8 (jessie)\"\nID=debian\nHOME_URL=\"http://www.debian.org/\"\nSUPPORT_URL=\"http://www.debian.org/support/\"\nBUG_REPORT_URL=\"https://bugs.debian.org/\"\nroot@osboxes:/home/osboxes# qemu-system-x86_64 --version\nQEMU emulator version 2.1.2 (Debian 1:2.1+dfsg-12+deb8u5a), Copyright (c) 2003-2008 Fabrice Bellard\nroot@osboxes:/home/osboxes# ls -l /tmp/hyper-bootstrap-root/hyper-pkg/service/systemd/hyperd.service \n-rw-r--r-- 1 osboxes osboxes 286 Feb  6 06:53 /tmp/hyper-bootstrap-root/hyper-pkg/service/systemd/hyperd.service\n```\nroot@osboxes:/home/osboxes# systemctl status hyperd\n\u25cf hyperd.service - hyperd\n   Loaded: loaded (/lib/systemd/system/hyperd.service; disabled)\n   Active: active (running) since Sat 2016-03-26 16:23:37 GMT; 6min ago\n     Docs: http://docs.hyper.sh\n Main PID: 13607 (hyperd)\n   CGroup: /system.slice/hyperd.service\n           \u2514\u250013607 /usr/local/bin/hyperd --nondaemon --log_dir=/var/log/hyper\nMar 26 16:23:37 osboxes hyperd[13607]: I0326 16:23:37.518877   13607 server.go:1245] Registering POST, /image/build\nMar 26 16:23:37 osboxes hyperd[13607]: I0326 16:23:37.518984   13607 server.go:1245] Registering POST, /pod/unpause\nMar 26 16:23:37 osboxes hyperd[13607]: I0326 16:23:37.519054   13607 server.go:1245] Registering DELETE, /image\nMar 26 16:23:37 osboxes hyperd[13607]: I0326 16:23:37.519181   13607 server.go:1245] Registering DELETE, /pod\nMar 26 16:23:37 osboxes hyperd[13607]: I0326 16:23:37.519289   13607 server.go:1245] Registering DELETE, /service\nMar 26 16:23:37 osboxes hyperd[13607]: I0326 16:23:37.519348   13607 server.go:1245] Registering DELETE, /vm\nMar 26 16:23:37 osboxes hyperd[13607]: I0326 16:23:37.519432   13607 server.go:1245] Registering OPTIONS,\nMar 26 16:28:38 osboxes hyperd[13607]: I0326 16:28:38.709068   13607 server.go:1128] Calling GET /images/get\nMar 26 16:28:38 osboxes hyperd[13607]: I0326 16:28:38.709178   13607 job.go:78] +job images(no)\nMar 26 16:28:38 osboxes hyperd[13607]: I0326 16:28:38.709249   13607 job.go:84] -job images(no) OK\nroot@osboxes:/home/osboxes# \n``\n. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. retest this please. retest this please. retest this please. @jgillich \"retest this please\" is a statement for Jenkins. Please ignore.. retest this please. retest this please. retest this please. retest this please, hykins. retest this please @hykins. @Crazykev The error in hykins and travis are same now.FAIL: hyper_test.go:533: TestSuite.TestSendExecSignal`. Can one of the admins verify this patch?. test this please @hykins. retest this please @hykins. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. @hykins retest this please. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. Can one of the admins verify this patch?. ",
    "henrysher": "fixed, thanks\n. @achanda the param \"-p\" indicates the file path of your podfile. (detail for podfile: https://docs.hyper.sh/reference/podfile.html)\n```\nroot@debian:~# hyper run --help\nUsage:\n  hyper run [OPTIONS] IMAGE [COMMAND] [ARG...]\ncreate a pod, and launch a new VM to run the pod\nApplication Options:\n  -p, --podfile=\"\"       Create and Run a pod based on the pod file\n  -k, --kubernetes=\"\"    Create and Run a pod based on the kubernetes pod file\n  -y, --yaml             Create a pod based on Yaml file\n      --name=\"\"          Assign a name to the container\n      --attach           Attach the stdin, stdout and stderr to the container\n      --workdir=\"\"       Working directory inside the container\n      --tty              Allocate a pseudo-TTY\n      --cpu=1            CPU number for the VM\n      --memory=128       Memory size (MB) for the VM\n      --env=[]           Set environment variables\n      --entrypoint=\"\"    Overwrite the default ENTRYPOINT of the image\n      --restart=\"\"       Restart policy to apply when a container exits (never, onFailure, always)\nHelp Options:\n  -h, --help             Show this help message\n```\n. +1, better to prepare a docker image (with all dependencies) to build, test and deliver the binaries, hyper & hyperd .\n. @carmark thanks, i have got this container via hyper list container. One more problem: the running container did not run as a web server, expected in pod file. I have gone through your document, it seems no kind of tips for the troubleshooting. Besides, i have used \"hyper exec\" and tried to execute some command in my container, but the outuput was nothing. Could you please help me on this problem?\n. @gao-feng i got the error message, but as the document said, if no tty configured, by default turned on, details can be found here: https://gist.github.com/f46d3d6830e711ed086f\n[HYPER INFO  0531 20:54:^@55 26892 qemu_process.go] [:53] [console] setup pty device /dev/null for exec\n[HYPER INFO  0531 20:54:^@55 26892 qemu_process.go] [:53] [console] ioctl pty device for execcmd failed: Inappropriate ioctl for device\n[HYPER INFO  0531 20:54:^@55 26892 qemu_process.go] [:53] [console] setup tty failed\n[HYPER INFO  0531 20:54:^@55 26892 qemu_process.go] [:53] [console] hyper send type 10, len 0\n[HYPER INFO  0531 20:54:^@55 26892 qemu_process.go] [:53] [console] wait for container started failed\n[HYPER INFO  0531 20:54:^@55 26892 qemu_process.go] [:53] [console] container 2e60d916bb16655746830d89e1503c7067ef13560883a6ba62a77f11c44cadc8 init exit code -1\nbtw, as no futher explannation about ContainerPort, HostPort, ServicePort, could you please give me more examples? is there any command like docker inspect?\n. ",
    "achanda": "Thanks all. I was going by the documentation in https://hyper.sh/how-it-works.html\nThe example says [root@user ~:]# hyper run -p nginx rails logstash cronjob\n. This worked fine for me a min back\n. @gnawux I think that's how it works. Godep saves a snapshot of the deps in the Godep folder. This can be committed in git and distributed. Thus, even if the deps change and it breaks the app, we always have a local working copy in source control.\n. ",
    "penberg": "@carmark AFAICT, it is already in the correct path, no?\n[penberg@nero hyper]$ echo $GOPATH\n/home/penberg/go\n[penberg@nero hyper]$ pwd\n/home/penberg/go/src/github.com/hyperhq/hyper\n. @carmark Aah, right, I guess I'm unable to read the documentation properly. :wink: It probably should be fixed, though, as it's inconsistent with pretty much every other Go application out there. Anyway, thanks for help!\n. ",
    "cdrage": "@gnawux awesome. looking forward to the new network features, hopefully there's the ability to specify a network interface / ip address in the future.\n. ",
    "gao-feng": "@henrysher Hi, try to run hyperd manually with  parameter -v=3, this will give you full debug message.\n. @henrysher Thanks for your helpful information, this is a bug in hyperStart. you can clone hyperStart from https://github.com/hyperhq/hyperstart.git and recompile it, change the Initrd in /etc/hyper/config to the path of hyperStart/build/hyper-initrd.img. \nNOTE: you should use '#' to comment the Cbfs in hyper config too, since the cbfs-qboot.rom doesn't contain new hyper-initrd.img. if you want to use cbfs-qboot.rom, please use cbfstool to generate new cbfs-qboot.rom\ndd if=/dev/zero of=boot.bin bs=4096 count=1\ncbfstool cbfs-qboot.rom create -s 4096k -B boot.bin -m x86  0x1000\ncbfstool cbfs-qboot.rom add -f /prefix/hyperStart/build/kernel -n vmlinuz -t raw\ncbfstool cbfs-qboot.rom add -f /prefix/hyperStart/build/hyper-initrd.img -n initrd -t raw\necho \"console=ttyS0,panic=1\" > cmdline\ncbfstool cbfs-qboot.rom add -f cmdline -n cmdline -t raw\n. @henrysher the port-map is on the way, will come soon. Thanks!\n. @henrysher the install script updated too, you can re-install it by curl -sSL https://hyper.sh/install | bash now. thanks!\n. @hustcat boot.bin comes from qboot (https://github.com/bonzini/qboot.git), the README in qboot repo tells you how to generate cbfs.rom.\nAnd the cbfstool comes from coreboot (https://github.com/coreboot/coreboot.git)\ncbfs.rom is a different way to pass kernel&initrd to qemu. if hyperd find Bios and Cbfs from hyper config file, Kernel&Initrd will loss effect.\n. @hustcat \nHi, can you manually run this command on your host?\nKernel and hyperStart will print debug information on console. let's check it.\nmkdir /var/run/hyper/vm-WgtIMOTrEn/\nqemu-kvm  -machine pc-i440fx-2.0,accel=kvm,usb=off -global kvm-pit.lost_tick_policy=discard -cpu host -kernel /var/lib/hyper/kernel -initrd /var/lib/hyper/hyper-initrd.img -append \"console=ttyS0 panic=1\" -realtime mlock=off -no-user-config -nodefaults -no-hpet -rtc base=utc,driftfix=slew -no-reboot -display none -boot strict=on -m 128 -smp 1 -qmp unix:/var/run/hyper/vm-WgtIMOTrEn/qmp.sock,server,nowait -serial stdio -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x2 -device virtio-scsi-pci,id=scsi0,bus=pci.0,addr=0x3 -chardev socket,id=charch0,path=/var/run/hyper/vm-WgtIMOTrEn/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0 -chardev socket,id=charch1,path=/var/run/hyper/vm-WgtIMOTrEn/tty.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1\n. Hi, right now hyper only support linux birdge, the default ip subnet is 192.168.123.1/24, the default bridge is hyper0 and its ip address is 192.168.123.1.  You can change the hyper bridge and ip subnet\nby change the hyper config, then restart hyperd.\nIf you want to access the pod, you can add the \"ports\":[{\"containerPort\": 80,\"hostPort\": 8088}] under the containers section. then access the hyper0-ip:hostPort.\n. @junoyoon Did you update hyper-initrd.img & kernel to the latest version? https://github.com/hyperhq/hyperstart\n. @shimiaofeng hi, the man page of docker cp says docker-cp - Copy files or folders from a container's PATH to a HOSTDIR or to STDOUT.. \n. @puresoul Hi, are  you using the hyper/hyper-kernel/hyper-initrd.image installed from hyper.sh or compile it yourself?\n. @puresoul sorry for late response, qemu: terminating on signal 1 from pid 5693  Can you check who is the pid 5693, is it hyperd? And you can run hyperd with -v=3, this will print more information. Thanks!\n. @puresoul please reset hyperstart to this commit https://github.com/hyperhq/hyperstart/commit/46ffda72bdd85009af7380fc81812d8b745a7ae6, and have a try. We will update hyper to support NEXT message asap.\n. @puresoul Sorry for late response, Can you check if you replace the old initrd under /var/lib/hyper/ with the compiled new initrd? it seems like hyperstart are busy handling pts data and had no chance to handle control data. but this bug is fixed by hyperstart, so it's weired. \n. @puresoul The successful case is running qemu without hardware virtualization(xen or kvm)?\nStill got hyper runing in dom0 and no as domU what's this means? the error case is running hyper in domU?\n. @puresoul What's the memory size of pod? could you try again with bigger memory?\nhttps://docs.hyper.sh/reference/podfile.html\n. container has name in pod files, what's the difference between name in pod and the name --name specified in run command line?\n. @a93ushakov  HI, Can you give more informations? such as the log? you can run hyperd with -v=3 to get  detail log.\n. LGTM\n. only for tcp protocol? \n. LGTM\n. LGTM\n. test-by: @feiskyer , merged.\n. LGTM\n. golang bug on fedora\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1135152\n. LGTM\n. @phillipp Thanks for your report, we add it to our todo list, will support configure public ip asap. :)\n. @phillipp Look forward to your pr\uff0cI prefer a) too.\n. code lgtm\n. lgtm\n. LGTM\n. https://github.com/hyperhq/runv/pull/41\n. LGTM\n. LGTM\n. @CMGS Can you show me the output of iptables -t nat -nvL |grep Chain, and check if xt_addrtype kernel module is available on your centos? Thanks!\n. unavailable addrtype  match  will cause this error too\n. updated\n. LGTM\n. --without-libvirt, same with xen\n. LGTM\n. LGMT\n. LGTM\n. LGTM\n. close,we decide to disable the features running pods on same vm, it makes things complicated \n. close,we decide to disable the features running pods on same vm, it makes things complicated \n. LGTM\n. LGTM\n. LGTM\n. this behavior  is consist with docker. you can specify the tag in /image/create API.\n. @zenny  Is /mnt/tank/HYPER/hyper-initrd.img  the new version of hyperstart initrd image?\n. Can you show your hyperd configuration(/etc/hyper/config)?  these vms looks like created by cache\n. seems like StopPod(WithinLock) holds the lock of pod and then set the p.vm to nil first, then the PodStopped will not cleanup resource, should we stop setting p.vm to nil in StopPod, leave this job to PodStopped.\n. LGTM\n. LGTM\n. LGTM\n. Please try the latest hyperd, this should be fixed\n. LGTN\n. @keyolk Hello, can you show the  /var/log/hyper/hyperd.INFO?\n. @Ddnirvana Hello, Do you have logs of hyperd? you can use hyperd -v=3 --nondaemon to start hyperd with full logs.\n. @Ddnirvana Seams like another hyperd is running background, you should kill it firstly \n. @Ddnirvana Hello, \nLooks like hyperstart is mismatched with hyperd, fails to parse json.\nCan you update the hyperstart and try again?  https://github.com/hyperhq/hyperstart\n```\nI0721 09:04:00.274872   10996 init_comm.go:68] [console] in maps incorrect dockerVolume\nI0721 09:04:00.275244   10996 init_comm.go:68] [console] parse pod json failed\n``\n. @Ddnirvana HI, Do you change theInitrdin /etc/hyper/config to the path of new built hyperstart?\n. @Ddnirvana Do you configure theCbfsin /etc/hyper/config? if yes, please remove it, hyperd uses cbfs prior. \n. @Ddnirvana Thanks for your report! We will remove the cbfs in next release. Sorry to cause you trouble.\n. LGTM, thanks!\n. LGTM, thanks!\n. Done. Please check if your qemu supports virtfs, and comment theBiosandCbfs`, we don't maintain this two files now.\nThanks\n. @GrantLajs please execute this command line to check if there is any error output\nqemu-system-x86_64 -fsdev local,id=virtio9p,path=/tmp,security_model=none -device virtio-9p-pci,fsdev=virtio9p,mount_tag=hello\n. @GrantLajs your qemu doesn't support virtfs, please consider compile qemu with --enable-virtfs\n. Fixed. If you want to install some linux drivers(modules), you should compile the drivers and put them into the container, it's fine to install modules in container by insmod, you don't need to access the VM, we already support some drivers(modules) in hyper kernel, please check https://github.com/hyperhq/hyperstart/blob/master/build/kernel_config to get the modules we support.\nAnd hypervisor is different with docker, you cannot see the devices of hosts in hypervisor VM.. please show me your /etc/hyper/config, thanks!. @marcosnils I guess your hyper-initrd.img is incorrect, can you remove and recompile it?\nI can reproduce this problem by speficy an incorrect file as Initrd.\nI1130 11:53:13.915896   13370 init_comm.go:68] [console] 9pnet: Installing 9P2000 support\nI1130 11:53:13.916582   13370 init_comm.go:68] [console] registered taskstats version 1\nI1130 11:53:13.917462   13370 init_comm.go:68] [console] rtc_cmos 00:00: setting system clock to 2016-11-30 03:53:13 UTC (1480477993)\nI1130 11:53:13.918208   13370 init_comm.go:68] [console] md: Waiting for all devices to be available before autodetect\nI1130 11:53:13.918639   13370 init_comm.go:68] [console] md: If you don't use raid, use raid=noautodetect\nI1130 11:53:13.919014   13370 init_comm.go:68] [console] md: Autodetecting RAID arrays.\nI1130 11:53:13.919335   13370 init_comm.go:68] [console] md: Scanned 0 and added 0 devices.\nI1130 11:53:13.919496   13370 init_comm.go:68] [console] md: autorun ...\nI1130 11:53:13.919643   13370 init_comm.go:68] [console] md: ... autorun DONE.\nI1130 11:53:13.920163   13370 init_comm.go:68] [console] RAMDISK: Couldn't find valid RAM disk image starting at 0.\nI1130 11:53:13.920610   13370 init_comm.go:68] [console] VFS: Cannot open root device \"(null)\" or unknown-block(0,0): error -6\nI1130 11:53:13.921149   13370 init_comm.go:68] [console] Please append a correct \"root=\" boot option; here are the available partitions:\nI1130 11:53:13.921404   13370 init_comm.go:68] [console] 0100           16384 ram0  (driver?)\nI1130 11:53:13.921735   13370 init_comm.go:68] [console] 0101           16384 ram1  (driver?)\nI1130 11:53:13.922065   13370 init_comm.go:68] [console] 0102           16384 ram2  (driver?)\nI1130 11:53:13.922403   13370 init_comm.go:68] [console] 0103           16384 ram3  (driver?)\nI1130 11:53:13.922755   13370 init_comm.go:68] [console] 0104           16384 ram4  (driver?)\nI1130 11:53:13.923105   13370 init_comm.go:68] [console] 0105           16384 ram5  (driver?)\nI1130 11:53:13.923428   13370 init_comm.go:68] [console] 0106           16384 ram6  (driver?)\nI1130 11:53:13.923779   13370 init_comm.go:68] [console] 0107           16384 ram7  (driver?)\nI1130 11:53:13.924118   13370 init_comm.go:68] [console] 0108           16384 ram8  (driver?)\nI1130 11:53:13.924393   13370 init_comm.go:68] [console] 0109           16384 ram9  (driver?)\nI1130 11:53:13.924677   13370 init_comm.go:68] [console] 010a           16384 ram10  (driver?)\nI1130 11:53:13.925019   13370 init_comm.go:68] [console] 010b           16384 ram11  (driver?)\nI1130 11:53:13.925376   13370 init_comm.go:68] [console] 010c           16384 ram12  (driver?)\nI1130 11:53:13.925717   13370 init_comm.go:68] [console] 010d           16384 ram13  (driver?)\nI1130 11:53:13.925996   13370 init_comm.go:68] [console] 010e           16384 ram14  (driver?)\nI1130 11:53:13.926345   13370 init_comm.go:68] [console] 010f           16384 ram15  (driver?)\nI1130 11:53:13.927011   13370 init_comm.go:68] [console] Kernel panic - not syncing: VFS: Unable to mount root fs on unknown-block(0,0)\nI1130 11:53:13.927501   13370 init_comm.go:68] [console] CPU: 0 PID: 1 Comm: swapper/0 Not tainted 4.4.28-hyper #1\nI1130 11:53:13.928187   13370 init_comm.go:68] [console] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Bochs 01/01/2011\nI1130 11:53:13.928823   13370 init_comm.go:68] [console]  0000000000000000 ffffffff8127272c ffffffff81590ac8 ffff880006c9fe60\nI1130 11:53:13.929282   13370 init_comm.go:68] [console]  ffffffff810cdbda ffff880000000010 ffff880006c9fe70 ffff880006c9fe10\nI1130 11:53:13.929726   13370 init_comm.go:68] [console]  ffff0035316d6172 ffff880006c9fe78 0000000000000001 ffff880006580870\nI1130 11:53:13.929839   13370 init_comm.go:68] [console] Call Trace:\nI1130 11:53:13.930238   13370 init_comm.go:68] [console]  [<ffffffff8127272c>] ? dump_stack+0x5c/0x80\nI1130 11:53:13.930595   13370 init_comm.go:68] [console]  [<ffffffff810cdbda>] ? panic+0xc3/0x1e0\nI1130 11:53:13.931050   13370 init_comm.go:68] [console]  [<ffffffff816d753b>] ? mount_block_root+0x218/0x2a4\nI1130 11:53:13.931519   13370 init_comm.go:68] [console]  [<ffffffff816d78b2>] ? prepare_namespace+0x168/0x19e\nI1130 11:53:13.931878   13370 init_comm.go:68] [console]  [<ffffffff816d727a>] ? kernel_init_freeable+0x1f1/0x1fe\nI1130 11:53:13.932210   13370 init_comm.go:68] [console]  [<ffffffff816d68e3>] ? initcall_blacklist+0xa1/0xa1\nI1130 11:53:13.932483   13370 init_comm.go:68] [console]  [<ffffffff814a05f0>] ? rest_init+0x80/0x80\nI1130 11:53:13.932875   13370 init_comm.go:68] [console]  [<ffffffff814a05f5>] ? kernel_init+0x5/0xd0\nI1130 11:53:13.933288   13370 init_comm.go:68] [console]  [<ffffffff814aa7cf>] ? ret_from_fork+0x3f/0x70\nI1130 11:53:13.933670   13370 init_comm.go:68] [console]  [<ffffffff814a05f0>] ? rest_init+0x80/0x80\nI1130 11:53:13.933989   13370 init_comm.go:68] [console] Kernel Offset: disabled\nE1130 11:53:14.973398   13370 init_comm.go:99] read init data failed. @marcosnils Hmm, Do you copy the new built hyper-initrd.img to /var/lib/hyper/hyper-initrd.img?\n. @marcosnils Can you try this hyper-initrd.img ?. @marcosnils Had better to show your compile step & output.. LGTM. LGTM. Fixed by https://github.com/hyperhq/hyperd/issues/490. I0113 12:07:16.935281   23599 vm_console.go:46] SB[vm-FaqvuiPhrh] [CNL] hyper_modify_event modify event fd 4, 0x61a588, event 1\nE0113 12:07:21.756979   23599 exec.go:170] Pod[busybox] Con[dc8df5016929] Exec[exec-jRZoYAsQAf] wait exec exit code timeout\nE0113 12:07:21.757011   23599 exec.go:97] Wait error: wait exec exit code timeout\nhyper_test.go:564:\n    c.Assert(err, IsNil)\n... value grpc.rpcError = grpc.rpcError{code:0x2, desc:\"wait exec exit code timeout\"} (\"rpc error: code = 2 desc = \\\"wait exec exit code timeout\\\"\")\n. Seems like ExecStart api doesn't wait the result of AddProcess from server, it only return the stream. the Execstart request may haven't been delivered to server or handled by server when the next ExecSignal request arrived.. https://github.com/hyperhq/hyperstart/pull/257 fix the missing of process finished event.. LGTM. LGTM. seems like fixed by https://github.com/hyperhq/runv/pull/457. @gnawux https://github.com/hyperhq/hyperd/pull/568. @gnawux The changes are added by myself, we need to access dockerDaemon.layerstore imagestore in hyperd daemon, first way is maintaining the changed docker-daemon package in hyperd, second way is changing docker-daemon directly in vendor. Since we will reuse docker's codes when pr is merged, I chose the second way to resolve this problem it is simplest.  . retest this please @hykins\n. It's more reasonable to fix missing cleaning up container problem, and this is not an impossible mission.\nPlease report the cleanup bugs when you meet them next time.\nThanks.. not quite sure if this bug is fixed by https://github.com/golang/go/commit/5bcdd639331cd7f8d844fd38a674c4751423f938. closed by https://github.com/hyperhq/hyperd/pull/567. do you mean hyperctl run --read-only IMAGE ?. retest this please, hykins. hykins failed when mounting nfs?\n```\nmount nfs share 192.168.123.22:/export to /export, tmp path /tmp//export\nexecuting cmd mount.nfs4 -n 192.168.123.22:/export /tmp//export\nRPC: Registered named UNIX socket transport module.\nRPC: Registered udp transport module.\nRPC: Registered tcp transport module.\nRPC: Registered tcp NFSv4.1 backchannel transport module.\nKey type dns_resolver registered\nNFS: Registering the id_resolver key type\nKey type id_resolver registered\nKey type id_legacy registered\nINFO: rcu_sched detected stalls on CPUs/tasks:\n(detected by 0, t=218673 jiffies, g=531, c=530, q=6)\n\nAll QSes seen, last rcu_sched kthread activity 218673 (4294890128-4294671455), jiffies_till_next_fqs=3, root ->qsmask 0x0\nkworker/0:0H    R  running task        0     5      2 0x00000008\nWorkqueue: xprtiod xs_tcp_setup_socket [sunrpc]\nffff88000f603ec0 ffffffff81075bb2 ffff88000f618200 0000000000000000\nffffffff8183e180 ffffffff8109be67 0000000000000000 0000000000000000\n0000000000000006 ffffffff8183e180 00745d1e000ccccd ffff88000e8f4c40\nCall Trace:\n  [] ? sched_show_task+0xd2/0x140\n[] ? rcu_check_callbacks+0x727/0x730\n[] ? update_process_times+0x23/0x50\n[] ? tick_sched_timer+0x33/0x60\n[] ? __hrtimer_run_queues+0x92/0x100\n[] ? hrtimer_interrupt+0x94/0x170\n[] ? smp_apic_timer_interrupt+0x34/0x50\n[] ? apic_timer_interrupt+0x87/0x90\n  [] ? __dev_queue_xmit+0x27c/0x5f0\n[] ? neigh_probe+0x3b/0x50\n[] ? neigh_probe+0x3b/0x50\n[] ? __neigh_event_send+0x98/0x220\n[] ? neigh_resolve_output+0x117/0x1a0\n[] ? ip_finish_output2+0x1a4/0x300\n[] ? ip_finish_output+0x83/0x1d0\n[] ? ip_output+0xb1/0xc0\n[] ? __sk_dst_check+0x30/0x70\n[] ? ip_local_out+0x12/0x40\n[] ? tcp_transmit_skb+0x479/0x880\n[] ? tcp_connect+0x5dd/0x810\n[] ? secure_tcp_sequence_number+0x71/0xb0\n[] ? tcp_v4_connect+0x32a/0x460\n[] ? __inet_stream_connect+0x8b/0x290\n[] ? release_sock+0x3b/0x90\n[] ? do_tcp_setsockopt.isra.43+0xfb/0x800\n[] ? inet_stream_connect+0x2e/0x50\n[] ? xs_tcp_setup_socket+0x70/0x3a0 [sunrpc]\n[] ? process_one_work+0x12f/0x310\n[] ? worker_thread+0x11d/0x490\n[] ? process_one_work+0x310/0x310\n[] ? kthread+0xb9/0xd0\n[] ? kthread_park+0x50/0x50\n[] ? ret_from_fork+0x25/0x30\nrcu_sched kthread starved for 218673 jiffies! g531 c530 f0x2 RCU_GP_WAIT_FQS(3) ->state=0x0\nrcu_sched       R  running task        0     7      2 0x00000000\nffff88000dbfa800 0000000000000000 ffff88000e8ce040 ffff88000f617500\nffff88000e928c80 ffffc900000a3dc8 ffffffff814e2782 00000000fffb7c5e\nffff88000e928c80 00000000fffb7c62 ffffc900000a3e00 ffff88000f60f780\nCall Trace:\n[] ? __schedule+0x192/0x530\n[] ? schedule+0x31/0x80\n[] ? schedule_timeout+0x121/0x240\n[] ? del_timer_sync+0x50/0x50\n[] ? rcu_gp_kthread+0x344/0x800\n[] ? __schedule+0x19a/0x530\n[] ? force_qs_rnp+0x180/0x180\n[] ? kthread+0xb9/0xd0\n[] ? kthread_park+0x50/0x50\n[] ? ret_from_fork+0x25/0x30\nclocksource: timekeeping watchdog on CPU0: Marking clocksource 'tsc' as unstable because the skew is too large:\nclocksource:                       'hpet' wd_now: 1a573bb1 wd_last: 18994f65 mask: ffffffff\nclocksource:                       'tsc' cs_now: 153182c2744 cs_last: e2f03dbc17 mask: ffffffffffffffff\nclocksource: Switched to clocksource hpet\nmount.nfs4: Connection timed out\nmount.nfs4 -n 192.168.123.22:/export /tmp//export cmd exit normally, status 32\ncmd mount.nfs4 -n 192.168.123.22:/export /tmp//export exit unexpectedly, status 8192\ncontainer sets up voulme failed\nhyper send container inited event: error\nwait for setup container rootfs failed\ncreate child process pid=345 in the sandbox\nfail to enter container ns: Bad file descriptor\nhyper send enter container ns event: error\nhyper ctl append type 10, len 0\n```. had better post hyperd logs, seems we should regard \"user\": \"1001\" as a valid configuration even hyperstart cannot find it in /etc/passwd. @laijs . https://github.com/hyperhq/hyperstart/pull/283. https://github.com/hyperhq/runv/pull/463\nhttps://github.com/hyperhq/hyperstart/pull/284.  Is /dev/loop6 and /dev/loop7 used in your host? hyperd devicemapper storage driver uses these two devices by default.\nhttps://github.com/hyperhq/hyperd/blob/master/daemon/storage.go#L108\nhttps://github.com/hyperhq/hyperd/blob/master/storage/defaults.go#L6. Is there some way to fix this?. need hyperhq/runv#491. it requires your comment,\nthere is ContainerDescription in the runv/api/, should this hypervisor.Process be moved to runv/api/ ? CC @gnawux for whom created runv/api/. retest this please, @hykins. retest this please, @hykins. seems still has the serial port broken problem in this pr?(hykins faild http://ci.hypercontainer.io:8080/job/hyperd-auto/371/console)\n01:25:42 I0422 01:25:41.793208    8892 vm_console.go:96] SB[vm-iYjTNjuyPE] [CNL] ACPI: PCI Interrupt Link [LNKB] enabled at IRQ 10\n01:25:42 I0422 01:25:41.797414    8892 vm_console.go:96] SB[vm-iYjTNjuyPE] [CNL] virtio-pci 0000:00:02.0: virtio_pci: leaving for legacy driver\n01:25:42 I0422 01:25:42.172757    8892 vm_console.go:96] SB[vm-iYjTNjuyPE] [CNL] ACPI: PCI Interrupt Link [LNKC] enabled at IRQ 11\n01:25:42 I0422 01:25:42.172929    8892 vm_console.go:96] SB[vm-iYjTNjuyPE] [CNL] virtio-pci 0000:00:03.0: virtio_pci: leaving for legacy driver\n01:25:43 I0422 01:25:42.522286    8892 vm_console.go:96] SB[vm-iYjTNjuyPE] [CNL] ACPI: PCI Interrupt Link [LNKD] enabled at IRQ 11\n01:25:43 I0422 01:25:42.526319    8892 vm_console.go:96] SB[vm-iYjTNjuyPE] [CNL] virtio-pci 0000:00:04.0: virtio_pci: leaving for legacy driver\n01:25:43 I0422 01:25:42.888697    8892 vm_console.go:96] SB[vm-iYjTNjuyPE] [CNL] ACPI: PCI Interrupt Link [LNKA] enabled at IRQ 10\n01:25:43 I0422 01:25:42.893060    8892 vm_console.go:96] SB[vm-iYjTNjuyPE] [CNL] virtio-pci 0000:00:05.0: virtio_pci: leaving for legacy driver\n01:25:43 E0422 01:25:42.907218    8892 json.go:370] read tty data failed\n01:25:43 E0422 01:25:42.907241    8892 json.go:427] SB[vm-iYjTNjuyPE] tty socket closed, quit the reading goroutine: EOF\n01:25:43 I0422 01:25:42.907247    8892 json.go:89] SB[vm-iYjTNjuyPE] close jsonBasedHyperstart\n01:25:43 E0422 01:25:42.907271    8892 json.go:570] SB[vm-iYjTNjuyPE] get hyperstart API version error: hyperstart closed\n01:25:43 W0422 01:25:42.907280    8892 hypervisor.go:47] SB[vm-iYjTNjuyPE] keep-alive test end with error: hyperstart closed\n01:25:43 I0422 01:25:42.907309    8892 hypervisor.go:23] SB[vm-iYjTNjuyPE] main event loop got message 14(ERROR_INIT_FAIL)\n01:25:43 E0422 01:25:42.907317    8892 vm_states.go:285] SB[vm-iYjTNjuyPE] hyperstart failed: hyperstart closed\n01:25:43 E0422 01:25:42.907326    8892 vm_states.go:243] SB[vm-iYjTNjuyPE] Shutting down because of an exception: %!(EXTRA string=connection to vm broken)\n01:25:43 I0422 01:25:42.907331    8892 vm_states.go:246] SB[vm-iYjTNjuyPE] poweroff vm based on command: connection to vm broken\n01:25:43 I0422 01:25:42.907405    8892 json.go:395] SB[vm-iYjTNjuyPE] tty chan closed, quit sent goroutine\n01:25:43 E0422 01:25:42.907435    8892 vm_states.go:223] SB[vm-iYjTNjuyPE] Start POD failed: hyperstart closed\n01:25:43 E0422 01:25:42.907468    8892 run.go:38] test-remove-container-with-volume: failed to add pod: hyperstart closed\n01:25:43 E0422 01:25:42.907495    8892 server.go:170] Handler for POST /v0.8.0/pod/create returned error: hyperstart closed. Can you update this pr with commit of runv https://github.com/hyperhq/runv/pull/491? Otherwise I need to update vendor again after this pr is merged. thanks!. oh shit, I need to review the issues, close this one. retest this please @hykins. LGTM\nand this fix one part of panic causing by p.sandbox is nil, pod.waitVMStop may have the same problem.\nI0818 13:08:27.618807 7901 vm_states.go:278] SB[vm-NGIAvyvfYa] VM has exit, or not started at all (12)\nI0818 13:08:27.618852 7901 context.go:199] SB[vm-NGIAvyvfYa] VmContext Close()\nI0818 13:08:27.618896 7901 json.go:93] SB[vm-NGIAvyvfYa] close jsonBasedHyperstart\nI0818 13:08:27.619004 7901 hypervisor.go:31] SB[vm-NGIAvyvfYa] main event loop exiting\nW0818 13:08:27.619041 7901 report.go:58] SB[vm-NGIAvyvfYa] panic during send vm fault message to channel\nE0818 13:08:27.619073 7901 vm_states.go:226] SB[vm-NGIAvyvfYa] Start POD failed: hyperstart closed\nE0818 13:08:27.619123 7901 provision.go:219] Pod[ubuntu-4717473828] failed to create sandbox for the stopped pod: hyperstart closed\nI0818 13:08:27.619142 7901 run.go:55] failed to start pod ubuntu-4717473828: hyperstart closed\nE0818 13:08:27.619163 7901 server.go:170] Handler for POST /v0.8.1/pod/start returned error: hyperstart closed\nI0818 13:08:27.619365 7901 decommission.go:526] Pod[ubuntu-4717473828] got vm exit event\nI0818 13:08:27.619413 7901 decommission.go:574] Pod[ubuntu-4717473828] umount all containers and volumes, release IP addresses\nW0818 13:08:27.619517 7901 dm.go:259] device to be umounted (/dev/mapper/docker-252:1-2364430-8dbd82c69ffda157296c4ce80afc70cdd452d4a1fa2928b1e033624e3530a469) does not exist\nI0818 13:08:27.619536 7901 container.go:1142] Pod[ubuntu-4717473828] Con[84b68bb319eb(ubuntu-4717473828)] umounted root volume\nW0818 13:08:27.619579 7901 volumes.go:70] Cannot umount volume /var/run/hyper/vm-NGIAvyvfYa/share_dir/jNpXCqgmco: no such file or directory\nW0818 13:08:27.619597 7901 volumes.go:73] Cannot lazy umount volume /var/run/hyper/vm-NGIAvyvfYa/share_dir/jNpXCqgmco: no such file or directory\nE0818 13:08:27.619620 7901 decommission.go:588] Pod[ubuntu-4717473828] Vol[etchosts-volume] no such file or directory\nI0818 13:08:27.619655 7901 networks.go:95] Pod[ubuntu-4717473828] Nic[eth-default] release IP address: 192.168.222.2\nI0818 13:08:27.619683 7901 etchosts.go:97] cleanupHosts /var/lib/hyper/hosts/ubuntu-4717473828, /var/lib/hyper/hosts/ubuntu-4717473828/hosts\nI0818 13:08:27.620128 7901 hypervisor.go:41] SB[vm-NGIAvyvfYa] watch hyperstart\nI0818 13:08:27.620148 7901 hypervisor.go:44] SB[vm-NGIAvyvfYa] issue VERSION request for keep-alive test\nE0818 13:08:27.620194 7901 json.go:604] SB[vm-NGIAvyvfYa] get hyperstart API version error: send ctl channel error, the hyperstart might have closed\nW0818 13:08:27.620219 7901 hypervisor.go:47] SB[vm-NGIAvyvfYa] keep-alive test end with error: send ctl channel error, the hyperstart might have closed\nI0818 13:08:27.623357 7901 vm_console.go:82] SB[vm-NGIAvyvfYa] connected console as telnet mode.\nI0818 13:08:27.629393 7901 xenpv.go:177] xl console finished\nI0818 13:08:27.629449 7901 vm_console.go:120] Input byte chan closed, close the output string chan\nI0818 13:08:27.629474 7901 vm_console.go:93] SB[vm-NGIAvyvfYa] console output end\ngot SIGCHLD, send msg to libxl\ngot child pid: -1\npanic: runtime error: invalid memory address or nil pointer dereference\n[signal SIGSEGV: segmentation violation code=0x1 addr=0x40 pc=0x55f066685b16]\ngoroutine 64 [running]:\npanic(0x55f067332840, 0xc4200140a0)\n /usr/lib/go-1.7/src/runtime/panic.go:500 +0x1a1\ngithub.com/hyperhq/hyperd/vendor/github.com/hyperhq/runv/hypervisor.(Vm).GetResponseChan(0x0, 0x1, 0xc4208923c0, 0x0)\n /home/goter/go/src/github.com/hyperhq/hyperd/vendor/github.com/hyperhq/runv/hypervisor/vm.go:49 +0x26\ngithub.com/hyperhq/hyperd/vendor/github.com/hyperhq/runv/hypervisor.(Vm).WaitResponse(0x0, 0x55f0675d70b0, 0xffffffffffffffff, 0x0)\n /home/goter/go/src/github.com/hyperhq/hyperd/vendor/github.com/hyperhq/runv/hypervisor/vm.go:117 +0x7a\ngithub.com/hyperhq/hyperd/vendor/github.com/hyperhq/runv/hypervisor.(Vm).WaitInit(0x0, 0x0, 0x0)\n /home/goter/go/src/github.com/hyperhq/hyperd/vendor/github.com/hyperhq/runv/hypervisor/vm.go:243 +0x47\ngithub.com/hyperhq/hyperd/daemon/pod.(XPod).waitVMInit(0xc42070efd0)\n /home/goter/go/src/github.com/hyperhq/hyperd/daemon/pod/provision.go:304 +0x42\ncreated by github.com/hyperhq/hyperd/daemon/pod.(*XPod).createSandbox\n /home/goter/go/src/github.com/hyperhq/hyperd/daemon/pod/provision.go:270 +0x278\n. @bergwolf \nHykins failed due to mount nfs failed, could you please check this? Thanks!\nI0713 07:29:48.689520   10258 vm_console.go:96] SB[vm-gjFVAQMaPR] [CNL] hyper_modify_event modify event fd 3, 0x61e688, event 8193\nI0713 07:29:49.259477   10258 vm_console.go:96] SB[vm-lOdpSnxhru] [CNL] mount.nfs4: rpc.statd is not running but is required for remote locking.\nI0713 07:29:49.262624   10258 vm_console.go:96] SB[vm-lOdpSnxhru] [CNL] mount.nfs4: Either use '-o nolock' to keep locks local, or start statd.\nI0713 07:29:49.277648   10258 vm_console.go:96] SB[vm-lOdpSnxhru] [CNL] mount.nfs4 -n 192.168.123.22:/export /tmp//export cmd exit normally, status 32\nI0713 07:29:49.281505   10258 vm_console.go:96] SB[vm-lOdpSnxhru] [CNL] cmd mount.nfs4 -n 192.168.123.22:/export /tmp//export exit unexpectedly, status 8192\nI0713 07:29:49.285639   10258 vm_console.go:96] SB[vm-lOdpSnxhru] [CNL] container sets up voulme failed\nI0713 07:29:49.287730   10258 vm_console.go:96] SB[vm-lOdpSnxhru] [CNL] hyper send container inited event: error\nI0713 07:29:49.289419   10258 vm_console.go:96] SB[vm-lOdpSnxhru] [CNL] wait for setup container rootfs failed\nI0713 07:29:49.303818   10258 vm_console.go:96] SB[vm-lOdpSnxhru] [CNL] create child process pid=609 in the sandbox\nI0713 07:29:49.314948   10258 vm_console.go:96] SB[vm-lOdpSnxhru] [CNL] fail to enter container ns: Bad file descriptor\nI0713 07:29:49.315820   10258 vm_console.go:96] SB[vm-lOdpSnxhru] [CNL] hyper send enter container ns event: error. need to wait iptables works assigned to @gnawux, otherwise there is no nat support for hyperd, container can not reach outside.. @bergwolf  @gnawux  any update? kvmtool and xenpv support are included in this pr.. LGTM. @xuchenhao001 \nhyper uses two unix sockets to communicate with hyperstart in vm, for x86/arm64, these two unix socket on host are represented as virtio-port in vm. the path are /var/run/hyper/vm-oOMYrhTfNz/hyper.sock and /var/run/hyper/vm-oOMYrhTfNz/tty.sock on host.\nso you should launch qemu with virtserialport device.\nhttps://github.com/hyperhq/runv/blob/master/hypervisor/qemu/qemu_amd64.go#L87\n. Is this better?\nhypervisor.HDriver.SupportLazy() && vmId == \"\"\n. the hypervisor driver \"qemu-kvm\" and \"qemu\" are not supported yet, at lease driverload doesn't support them.\n. If there is no libvirt-dev or the version is incorrect, you should reconfigure hyper/runv with --without-libvirt, please check https://github.com/hyperhq/runv/pull/96.\nWe want xen&libvirt driver being available by default, so we outthrow warning if there is no xen or libvirt available,and user can use --without- option to reconfigure hyper/runv.\nAnd it's weird to have both --with & --without options, qemu doesn't have, libvirt doesn't have.\n. Ok. I saw  the runv part \n. driverloader will decide the hypervisor. In the trivals environment, only qemu is available, so actually we test both driverloader and qemu driver.\n. removed\n. Already make sure args array at least has two args. Doesn't line 23 did this job? or we need another check here?\n. @laijs pause api introduced here is for docker commit container too, commit need to pause container. we provide this api to consist with docker api. the internal implements can be change when we can really implement pause container.\n. I saw  the pr https://github.com/hyperhq/hyper/pull/201 checks the entrypoint before assign it to config.Entrypoint\n-               if len(c.Entrypoint) != 0 {\n-                       config.Entrypoint = strslice.New(c.Entrypoint...)\n              }\n. I saw GetByName GetByContainerId have the same problem.\n. return E_OK here\n. apitypes.UserPod and runv.UserPod(shoud be hyperd.UserPod) are both user faced API structure. One for grpc/protobuf, One for json. they will be converted to runv.PodInfo(doesn't exist yet), they are two types of structure services for two types of API. \nSince the runv.UserPod represents the pod spec right now(in json), If user uses grpc api and pod spec to create pod,  the extra conversion from json to protobuf is needed.\n. I mean If hyprectl use grpc someday, the command hyperctl run -p pod.spec  will need to do conversion\n. you should check if p.vm is nil (pod is not started yet)\n. Should stop pod by default.\n. return or print out err returned by ContainerRm\n. return fmt.Errorf(...) directly.\n. no need to put pod twice.\n. why change mountid to containerid?\n. podlist.Put updates the containers array in podlist while daemon.WritePodAndContainers uses podstatus.Containers to updates the database, the podstatus.Containers was already updated.\n. pod.TransitionLock(\"rmContainer\") \n. Are the packages below necessary?. seems like this pod file is useless?. daemon/exec.go:51: id declared and not used\n. this option is used as the name of loaded image. and this option is consist with https://github.com/docker/docker/pull/26369. https://github.com/gao-feng/hyper-public/blob/310ebcb653f2688efcde5f9e5f44adb73ca19ad3/types/types.proto#L492 looks good. seems this defer function will never be called?. ",
    "hustcat": "@carmark Thanks.\n. @gnawux Thanks for your reply.\nThese are logs:\n[HYPER INFO  0603 19:49:50 05325 server.go] Calling POST /pod/run\n[HYPER INFO  0603 19:49:50 05325 job.go] +job podRun({\"id\":\"ubuntu:14.04-7518475523\",\"containers\":[{\"name\":\"ubuntu:14.04-7518475523\",\"image\":\"ubuntu:14.04\",\"command\":[\"/bin/sh\"],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\"})\n[HYPER INFO  0603 19:49:50 05325 pod.go] {\"id\":\"ubuntu:14.04-7518475523\",\"containers\":[{\"name\":\"ubuntu:14.04-7518475523\",\"image\":\"ubuntu:14.04\",\"command\":[\"/bin/sh\"],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\"}\n[HYPER INFO  0603 19:49:50 05325 pod.go] The config: kernel=/var/lib/hyper/kernel, initrd=/var/lib/hyper/hyper-initrd.img\n[HYPER INFO  0603 19:49:50 05325 pod.go] leveldb: not found\n[HYPER INFO  0603 19:49:50 05325 pod.go] Process the Containers section in POD SPEC\n[HYPER INFO  0603 19:49:50 05325 create.go] The Repository is ubuntu, and the tag is 14.04\n[HYPER INFO  0603 19:49:50 05325 context.go] kvm not exist change to no kvm mode\n[HYPER INFO  0603 19:49:50 05325 qemu_process.go] cmdline arguments: -machine pc-i440fx-2.0,usb=off -cpu core2duo -kernel /var/lib/hyper/kernel -initrd /var/lib/hyper/hyper-initrd.img -append \"console=ttyS0 panic=1\" -realtime mlock=off -no-user-config -nodefaults -no-hpet -rtc base=utc,driftfix=slew -no-reboot -display none -boot strict=on -m 128 -smp 1 -qmp unix:/var/run/hyper/vm-KWioHNrTgl/qmp.sock,server,nowait -serial unix:/var/run/hyper/vm-KWioHNrTgl/console.sock,server,nowait -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x2 -device virtio-scsi-pci,id=scsi0,bus=pci.0,addr=0x3 -chardev socket,id=charch0,path=/var/run/hyper/vm-KWioHNrTgl/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0 -chardev socket,id=charch1,path=/var/run/hyper/vm-KWioHNrTgl/tty.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1 -fsdev local,id=virtio9p,path=/var/run/hyper/vm-KWioHNrTgl/share_dir,security_model=none -device virtio-9p-pci,fsdev=virtio9p,mount_tag=share_dir\n[HYPER INFO  0603 19:49:50 05325 qemu_process.go] qemu daemon pid 5424.\n[HYPER INFO  0603 19:49:50 05325 qemu_process.go] starting daemon with pid: 5424\n[HYPER INFO  0603 19:49:50 05325 create.go] The returned status code is 201!\n[HYPER INFO  0603 19:49:50 05325 container.go] ready to get the container(e96a948eca9cdc9907ce4b1777064b56ca81f09606cab4e3bc4be05993d38c2d) info\n[HYPER INFO  0603 19:49:50 05325 pod.go] Parsing envs for container 0: 0 Evs\n[HYPER INFO  0603 19:49:50 05325 pod.go] The fs type is xfs\n[HYPER INFO  0603 19:49:50 05325 pod.go] WorkingDir is \n[HYPER INFO  0603 19:49:50 05325 pod.go] Image is /dev/mapper/docker-253:1-101030670-e96a948eca9cdc9907ce4b1777064b56ca81f09606cab4e3bc4be05993d38c2d\n[HYPER INFO  0603 19:49:50 05325 pod.go] Container Info is \n&{e96a948eca9cdc9907ce4b1777064b56ca81f09606cab4e3bc4be05993d38c2d /rootfs /dev/mapper/docker-253:1-101030670-e96a948eca9cdc9907ce4b1777064b56ca81f09606cab4e3bc4be05993d38c2d xfs  [] [/bin/bash] map[]}\n[HYPER INFO  0603 19:49:50 05325 pod.go] container 0 created e96a948eca9cdc9907ce4b1777064b56ca81f09606cab4e3bc4be05993d38c2d, workdir , env: map[]\nPOD id is pod-rnKThaapsH\n[HYPER INFO  0603 19:49:50 05325 qemu.go] main event loop got message 20(COMMAND_RUN_POD)\n[HYPER INFO  0603 19:49:50 05325 vm_states.go] got spec, prepare devices\n[HYPER INFO  0603 19:49:50 05325 context.go] VM vm-KWioHNrTgl: state change from  to 'STARTING'\n[HYPER INFO  0603 19:49:50 05325 qmp_handler.go] got new session during initializing\n[HYPER INFO  0603 19:49:50 05325 qemu.go] main event loop got message 12(EVENT_INTERFACE_ADD)\n[HYPER INFO  0603 19:49:50 05325 qmp_wrapper.go] send net to qemu at 14\n[HYPER INFO  0603 19:49:50 05325 qmp_handler.go] got new session during initializing\n[HYPER ERROR 0603 19:49:51 05325 init_comm.go] Cannot connect to hyper socket dial unix /var/run/hyper/vm-KWioHNrTgl/hyper.sock: connection refused\n[HYPER ERROR 0603 19:49:51 05325 tty.go] Cannot connect to tty socket dial unix /var/run/hyper/vm-KWioHNrTgl/tty.sock: connection refused\n[HYPER ERROR 0603 19:49:51 05325 qmp_handler.go] failed to connected to /var/run/hyper/vm-KWioHNrTgl/qmp.sock dial unix /var/run/hyper/vm-KWioHNrTgl/qmp.sock: connection refused\n[HYPER ERROR 0603 19:49:51 05325 qemu_process.go] failed to connected to /var/run/hyper/vm-KWioHNrTgl/console.sock dial unix /var/run/hyper/vm-KWioHNrTgl/console.sock: connection refused\n[HYPER INFO  0603 19:49:51 05325 qemu.go] main event loop got message 30(ERROR_INIT_FAIL)\n[HYPER ERROR 0603 19:49:51 05325 vm_states.go] Cannot connect to hyper socket dial unix /var/run/hyper/vm-KWioHNrTgl/hyper.sock: connection refused\n[HYPER ERROR 0603 19:49:51 05325 vm_states.go] Shutting down because of an exception: Fail during init pod running environment\n[HYPER INFO  0603 19:49:51 05325 context.go] VM vm-KWioHNrTgl: state change from STARTING to 'TERMINATING'\n[HYPER INFO  0603 19:49:51 05325 qemu.go] main event loop got message 30(ERROR_INIT_FAIL)\n[HYPER INFO  0603 19:49:51 05325 vm_states.go] got event during terminating\n[HYPER ERROR 0603 19:49:51 05325 qmp_handler.go] QMP initialize failed\n[HYPER INFO  0603 19:49:51 05325 qemu.go] main event loop got message 30(ERROR_INIT_FAIL)\n[HYPER INFO  0603 19:49:51 05325 vm_states.go] got event during terminating\n[HYPER INFO  0603 19:49:51 05325 pod.go] Get the response from QEMU, VM id is vm-KWioHNrTgl!\n[HYPER INFO  0603 19:49:51 05325 qemu.go] main event loop got message 23(COMMAND_SHUTDOWN)\n[HYPER INFO  0603 19:49:51 05325 vm_states.go] got event during terminating\n[HYPER INFO  0603 19:50:01 05325 qemu.go] main event loop got message 2(EVENT_QEMU_TIMEOUT)\n[HYPER WARN  0603 19:50:01 05325 vm_states.go] Qemu did not exit in time, try to stop it\n[HYPER ERROR 0603 19:50:01 05325 vm_states.go] Shutting down because of an exception: vm terminating timeout\n[HYPER INFO  0603 19:50:01 05325 vm.go] Got response: 7: vm terminating timeout\n[HYPER INFO  0603 19:50:11 05325 qemu_process.go] kill Qemu... 5424\n[HYPER INFO  0603 19:50:11 05325 qemu.go] main event loop got message 1(EVENT_QEMU_KILL)\n[HYPER INFO  0603 19:50:11 05325 vm_states.go] Got Qemu force killed message, go to cleaning up\n[HYPER INFO  0603 19:50:11 05325 vm_states.go] qemu has exit...\n[HYPER INFO  0603 19:50:11 05325 devicemap.go] need remove dm file/dev/mapper/docker-253:1-101030670-e96a948eca9cdc9907ce4b1777064b56ca81f09606cab4e3bc4be05993d38c2d\n[HYPER INFO  0603 19:50:11 05325 devicemap.go] remove network card 0: 192.168.123.2\n[HYPER INFO  0603 19:50:11 05325 context.go] VM vm-KWioHNrTgl: state change from TERMINATING to 'DESTROYING'\n[HYPER INFO  0603 19:50:11 05325 vm.go] Got response: 2: qemu shut down\n[HYPER ERROR 0603 19:50:11 05325 pod.go] QEMU response data is nil\n[HYPER INFO  0603 19:50:11 05325 job.go] -job podRun({\"id\":\"ubuntu:14.04-7518475523\",\"containers\":[{\"name\":\"ubuntu:14.04-7518475523\",\"image\":\"ubuntu:14.04\",\"command\":[\"/bin/sh\"],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\"}) ERR: QEMU response data is nil\n[HYPER ERROR 0603 19:50:11 05325 server.go] Handler for POST /pod/run returned error: QEMU response data is nil\n[HYPER ERROR 0603 19:50:11 05325 server.go] HTTP Error: statusCode=500 QEMU response data is nil\n[HYPER INFO  0603 19:50:11 05325 qemu.go] main event loop got message 13(EVENT_INTERFACE_DELETE)\n[HYPER INFO  0603 19:50:11 05325 devicemap.go] interface 0 released\n[HYPER INFO  0603 19:50:11 05325 vm_states.go] Unplug interface return with true\n[HYPER INFO  0603 19:50:11 05325 qemu.go] main event loop got message 9(EVENT_VOLUME_DELETE)\n[HYPER INFO  0603 19:50:11 05325 devicemap.go] blockdev /dev/mapper/docker-253:1-101030670-e96a948eca9cdc9907ce4b1777064b56ca81f09606cab4e3bc4be05993d38c2d deleted\n[HYPER INFO  0603 19:50:11 05325 vm_states.go] release volume return with true\nIt seem that /dev/kvm is not exist.\n. Still failed.\n[HYPER INFO  0603 20:04:06 03059 server.go] Calling POST /pod/run\n[HYPER INFO  0603 20:04:06 03059 job.go] +job podRun({\"id\":\"ubuntu:14.04-3539368516\",\"containers\":[{\"name\":\"ubuntu:14.04-3539368516\",\"image\":\"ubuntu:14.04\",\"command\":[\"/bin/sh\"],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\"})\n[HYPER INFO  0603 20:04:06 03059 pod.go] {\"id\":\"ubuntu:14.04-3539368516\",\"containers\":[{\"name\":\"ubuntu:14.04-3539368516\",\"image\":\"ubuntu:14.04\",\"command\":[\"/bin/sh\"],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\"}\n[HYPER INFO  0603 20:04:06 03059 pod.go] The config: kernel=/var/lib/hyper/kernel, initrd=/var/lib/hyper/hyper-initrd.img\n[HYPER INFO  0603 20:04:06 03059 pod.go] leveldb: not found\n[HYPER INFO  0603 20:04:06 03059 pod.go] Process the Containers section in POD SPEC\n[HYPER INFO  0603 20:04:06 03059 create.go] The Repository is ubuntu, and the tag is 14.04\n[HYPER INFO  0603 20:04:06 03059 qemu_process.go] cmdline arguments: -machine pc-i440fx-2.0,accel=kvm,usb=off -global kvm-pit.lost_tick_policy=discard -cpu host -kernel /var/lib/hyper/kernel -initrd /var/lib/hyper/hyper-initrd.img -append \"console=ttyS0 panic=1\" -realtime mlock=off -no-user-config -nodefaults -no-hpet -rtc base=utc,driftfix=slew -no-reboot -display none -boot strict=on -m 128 -smp 1 -qmp unix:/var/run/hyper/vm-WgtIMOTrEn/qmp.sock,server,nowait -serial unix:/var/run/hyper/vm-WgtIMOTrEn/console.sock,server,nowait -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x2 -device virtio-scsi-pci,id=scsi0,bus=pci.0,addr=0x3 -chardev socket,id=charch0,path=/var/run/hyper/vm-WgtIMOTrEn/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0 -chardev socket,id=charch1,path=/var/run/hyper/vm-WgtIMOTrEn/tty.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1 -fsdev local,id=virtio9p,path=/var/run/hyper/vm-WgtIMOTrEn/share_dir,security_model=none -device virtio-9p-pci,fsdev=virtio9p,mount_tag=share_dir\n[HYPER INFO  0603 20:04:06 03059 qemu_process.go] qemu daemon pid 3205.\n[HYPER INFO  0603 20:04:06 03059 qemu_process.go] starting daemon with pid: 3205\n[HYPER INFO  0603 20:04:06 03059 init_comm.go] Wating for init messages...\n[HYPER INFO  0603 20:04:06 03059 init_comm.go] trying to read 8 bytes\n[HYPER INFO  0603 20:04:06 03059 tty.go] tty socket connected\n[HYPER INFO  0603 20:04:06 03059 tty.go] tty: trying to read 12 bytes\n[HYPER INFO  0603 20:04:06 03059 qmp_handler.go] connected to /var/run/hyper/vm-WgtIMOTrEn/qmp.sock\n[HYPER INFO  0603 20:04:06 03059 qmp_handler.go] begin qmp init...\n[HYPER INFO  0603 20:04:06 03059 qemu_process.go] connected to /var/run/hyper/vm-WgtIMOTrEn/console.sock\n[HYPER INFO  0603 20:04:06 03059 qemu_process.go] connected /var/run/hyper/vm-WgtIMOTrEn/console.sock as telnet mode.\n[HYPER INFO  0603 20:04:06 03059 tty.go] Input byte chan closed, close the output string chan\n[HYPER INFO  0603 20:04:06 03059 qemu_process.go] console output end\n[HYPER ERROR 0603 20:04:06 03059 tty.go] read tty data failed\n[HYPER INFO  0603 20:04:06 03059 tty.go] tty socket closed, quit the reading goroutine read unix /var/run/hyper/vm-WgtIMOTrEn/tty.sock: connection reset by peer\n[HYPER ERROR 0603 20:04:06 03059 init_comm.go] read init data failed\n[HYPER ERROR 0603 20:04:06 03059 init_comm.go] read init message failed... read unix /var/run/hyper/vm-WgtIMOTrEn/hyper.sock: connection reset by peer\n[HYPER ERROR 0603 20:04:06 03059 qmp_handler.go] get qmp welcome failed: read unix /var/run/hyper/vm-WgtIMOTrEn/qmp.sock: connection reset by peer\n[HYPER INFO  0603 20:04:06 03059 qemu.go] main event loop got message 32(ERROR_INTERRUPTED)\n[HYPER INFO  0603 20:04:06 03059 vm_states.go] Connection interrupted, quit...\n[HYPER ERROR 0603 20:04:06 03059 vm_states.go] Shutting down because of an exception: connection to VM broken\n[HYPER INFO  0603 20:04:06 03059 context.go] VM vm-WgtIMOTrEn: state change from  to 'DESTROYING'\n. @gnawux @gao-feng Thanks very much.\nI used Bios and Cbfs from here, and it seems OK now.\n. ",
    "PRADEEPKJ": "I have the same issue on centos7.  Below are the logs. Please help to resolve. I am using the recent version of hyper\n[HYPER ERROR 0126 09:55:52 20510 pod.go] [:101] VM response data is nil\n[HYPER ERROR 0126 09:55:52 20510 server.go] [:1078] Handler for POST /pod/start returned error: VM response data is nil\n[HYPER ERROR 0126 09:55:52 20510 server.go] [:132] HTTP Error: statusCode=500 VM response data is nil\n[HYPER ERROR 0126 09:55:55 20510 qmp_handler.go] [:180] get qmp welcome failed: read unix /var/run/hyper/vm-kVotisctic/qmp.sock: connection reset by peer\n[HYPER ERROR 0126 09:55:55 20510 qmp_handler.go] [:364] QMP initialize failed\n[HYPER ERROR 0126 09:55:55 20510 vm_states.go] [:400] read unix /var/run/hyper/vm-kVotisctic/qmp.sock: connection reset by peer\n[HYPER ERROR 0126 09:55:55 20510 vm_states.go] [:289] Shutting down because of an exception: Fail during init pod running environment\n[HYPER ERROR 0126 09:55:55 20510 tty.go] [:77] read tty data failed\n[HYPER ERROR 0126 09:55:55 20510 init_comm.go] [:78] read init data failed\n[HYPER ERROR 0126 09:55:55 20510 init_comm.go] [:116] read init message failed... read unix /var/run/hyper/vm-kVotisctic/hyper.sock: connection reset by peer\n[HYPER ERROR 0126 09:56:05 20510 vm_states.go] [:298] Shutting down because of an exception: vm terminating timeout\n[HYPER ERROR 0126 09:56:15 20510 pod.go] [:101] VM response data is nil\n[HYPER ERROR 0126 09:56:15 20510 server.go] [:1078] Handler for POST /pod/start returned error: VM response data is nil\n[HYPER ERROR 0126 09:56:15 20510 server.go] [:132] HTTP Error: statusCode=500 VM response data is nil\n. Thanks..it works fine now.\n-Pradeep\nOn 29 January 2016 at 15:29, Xu Wang notifications@github.com wrote:\n\n@PRADEEPKJ https://github.com/PRADEEPKJ\nFor CentOS 7.x, could you remove all the hyper binaries under\n/usr/local/bin, and install the following prebuilt RPMs:\nx86_64 binary packages:\n- hyper-0.4-2.el7.centos.x86_64.rpm\n  https://s3.amazonaws.com/hyper-install/hyper-0.4-2.el7.centos.x86_64.rpm\n- hyperstart-0.4-2.el7.centos.x86_64.rpm\n  https://s3.amazonaws.com/hyper-install/hyperstart-0.4-2.el7.centos.x86_64.rpm\n- qemu-hyper-2.4.1-2.el7.centos.x86_64.rpm\n  https://s3.amazonaws.com/hyper-install/qemu-hyper-2.4.1-2.el7.centos.x86_64.rpm\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/hyperhq/hyper/issues/14#issuecomment-176781387.\n. I tried to read the status of the VM which is inside the hyper container by running the below cli. I mean using the qmp socket. But its not responding.\n\n./qmp --path=\"/var/run/hyper/vm-VDzrArkOzk/qmp.sock\" query-status\nCan you please help me to fix this?\n. Hi Wang,\nThanks, I will try those.\nRegards,\nPradeep\nOn 29 January 2016 at 15:25, Xu Wang notifications@github.com wrote:\n\nFor CentOS 7.x, could you remove all the hyper binaries under\n/usr/local/bin, and install the following prebuilt RPMs:\nx86_64 binary packages:\n- hyper-0.4-2.el7.centos.x86_64.rpm\n  https://s3.amazonaws.com/hyper-install/hyper-0.4-2.el7.centos.x86_64.rpm\n- hyperstart-0.4-2.el7.centos.x86_64.rpm\n  https://s3.amazonaws.com/hyper-install/hyperstart-0.4-2.el7.centos.x86_64.rpm\n- qemu-hyper-2.4.1-2.el7.centos.x86_64.rpm\n  https://s3.amazonaws.com/hyper-install/qemu-hyper-2.4.1-2.el7.centos.x86_64.rpm\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/hyperhq/hyper/issues/186#issuecomment-176780570.\n. Hi,\n\nI was trying to add a -fsdev device in runv\nlike below\n\"-fsdev\", fmt.Sprintf(\"local,id=virtio9pEX,path=%s,security_model=none\",\nctx.ExDir),\n\"-device\", fmt.Sprintf(\"virtio-9p-pci,fsdev=virtio9pEX,mount_tag=%s\",\nhypervisor.ExDirTag),\nBut when I start the container, it hangs and get Timeout message as below\ncontainer run error: Run container timeout!.\nAny idea. How can I add a external device?\nThanks,\nPradeep\nOn 30 January 2016 at 04:37, Xu Wang notifications@github.com wrote:\n\nclose as the RPMs works\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/hyperhq/hyper/issues/186#issuecomment-177058369.\n. \n",
    "darkheaven1983": "@carmark Removing the image and downloading it again works. Closing this issue....\n. ",
    "hjianhao": "Thanks, I use the rmmod kvm and rmmod kvm_intel to remove the kvm modules, then I can start the virutalbox VM.\nBut I add the kvm modules by modprobe again, virtualbox VM can still start successfully. A little confused. :)\n. I can not find any clue from your documentation. :)\n. ",
    "tbronchain": "Hi,\nWe are currently working on a how-to tutorial. I will keep you updated when it will be online.\nThibault.\n. ",
    "bahador": "tyvm!\n. ",
    "qcbupt": "Thanks for reply.\nIssue is resolved now.\nThanks for support.\n. ",
    "holidayworking": "@carmark \nThank you for the reply.\nI tried sudo hyper run -p mysql.pod, but did't resolve.\n. @gnawux I understand. I'm looking forward to that this problem is fixed.\n. @gnawux Thanks for resolving this issue. I launch MySQL.\n```\nvagrant@vagrant-ubuntu-trusty-64:/vagrant/hyper-rails-example$ sudo hyper run -p mysql.pod\nPOD id is pod-GUMNkDosbs\nTime to run a POD is 2053 ms\nvagrant@vagrant-ubuntu-trusty-64:/vagrant/hyper-rails-example$ sudo hyper exec pod-GUMNkDosbs mysql -uroot\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 2\nServer version: 5.6.25 MySQL Community Server (GPL)\nCopyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\nmysql> show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| mysql              |\n| performance_schema |\n+--------------------+\n3 rows in set (0.01 sec)\n```\n. ",
    "ayufan": "@gnawux I did it yesterday (recompiled kernel and rebuilt cbfs) and it works great.\nAbout the use case:\nCurrently Docker-in-Docker is possible, but insecure (requires --privileged). With using full-virtualization docker-in-hyper can work without any security concerns. It's especially useful for CI that for time of build spinups new VM and requires access to Docker in order to build, test and deploy container.\nI specifically were thinking about implementing it in https://gitlab.com/gitlab-org/gitlab-ci-multi-runner/ as additional method to run builds.\nI guess that your main concern is not inclusion of features, but boot time that can be longer with them enabled. Am I right?\n@gnawux One quest: do volumes work in 0.2.1?\n. Is it common? hard to guess. It would be nice to have, because there's no secure option to run dind :) You could sell it that this is supported in secure manner. There are companies that are fighting with creating selinux, apparmor policies for LXC to have it working and isolated, but still this is not \"that\" secure as full VM.\n. @gnawux I'll be happy to prepare PR with required kernel features enabled. Just let me know :)\n. ",
    "Gnep": "I think it makes sense to support cgroup and other namespaces in pod. So when I create a pod, I have the option to isolate multiple containers in a pod, or share namespace.\nThe multi-container feature should do the job for @ayufan , though it doesn't require docker daemon in the VM instance.\n. Thanks!\nOne question: are you building a multi-tenant CaaS with Hyper?\n. @danielbodart \nJust FYI, we are working on HyperStack, which is scheduled to release next month. Guess that might be interesting to you.\n. This is very interesting, and we'd love to see the PR. If there is a working beta in the next two weeks, we could throw a demo at the coming DockerCon at Barcelona!\n. @zenny \nAs you may have noticed, we released our public container service a couple weeks ago. We had quite some discussion of naming, and decided to use Hyper_ and hyper.sh for the public service. \nTo avoid more chaos in the future, we decided to rename Hyper to HyperContainer and the repo to hyperd. I fully understand your feeling. What we want is to avoid greater confusing in the future with the tradeoff of confusion to the current community (including you).\nWith that said, we will fix the typo and docs to get thing consistent soon.\n. @zenny \nThe commit is a quick fix, not a complete one. Moving forward, what you will see is:\n- HyperContainer (hyperd, hyperctl)\n- Hypernetes=Kuberentes+HyperContainer+OpenStack Cinder/Neutron\n- Hyper_: the public service, CLI hypercli \n- Hypernova: HyperContainer driver for OpenStack Nova\n- HyperStack=Nova+Hypernova+Cinder+Neutron\n- runV: hypervisor-based equivalent to runC\nI hope this could help to clarify the confusion. We'd love to see your contribution on the project governance. Could you send an email to peng@hyper.sh to chat? I also want to invite you to our new public service.\nThanks!\n. @zenny plz sign up our beta at https://hyper.sh and let me know the email address you used. I'll approve ASAP.\n. ",
    "geku": "Any news on this? I have exactly the same use case and would like to use Hyper pods running a Docker daemon to build Docker images in a secure, isolated way.\n. ",
    "junoyoon": "Any progress on this? I have same issues as well. \n. @feiskyer  thanks for reply.\nI've tried the following steps to test it.\n1. download and run cgroupfs-mount in centos7 PM.\n2. install hyperd in centos7 PM.\n3. run hyperd container using hyperctl run -t centos:latest bash \n4. install docker in the hyperd container using \n   - https://docs.docker.com/engine/installation/linux/centos/#/install-with-yum\n5. start docker daemon in the hyperd container.\nFinally I got the following errors.\n[root@centos-latest-6240306100 /]# systemctl start docker\nRunning in chroot, ignoring request.\n[root@centos-latest-6240306100 /]#\nDo I need special steps more for docker daemon?\n. @feiskyer Thanks... I could run successfully  docker daemon by just running dockerd\nThen I tried to run docker command docker run -it centos:latest\nAnd I saw the following result.\nERRO[0029] containerd: start container                   error=oci runtime error: container_linux.go:247: starting container process caused \"process_linux.go:359: container init caused \\\"could not create session key: function not implemented\\\"\"\n                                     id=2dec4e56ab4579541bc35a509cfbbe0b6b70c67c6c0a2a46d629e7ddbb13bf67\n                                                                                                        ERRO[0030] Create container failed with error: invalid header field value \"oci runtime error: container_linux.go:247: starting container process caused \\\"process_linux.go:359: container init caused \\\\\\\"could not create session key: function not implemented\\\\\\\"\\\"\\n\"\n                                                                                                                                                                  ERRO[0030] Handler for POST /v1.24/containers/2dec4e56ab4579541bc35a509cfbbe0b6b70c67c6c0a2a46d629e7ddbb13bf67/start returned error: invalid header field value \"oci runtime error: container_linux.go:247: starting container process caused \\\"process_linux.go:359: container init caused \\\\\\\"could not create session key: function not implemented\\\\\\\"\\\"\\n\"\ndocker: Error response from daemon: invalid header field value \"oci runtime error: container_linux.go:247: starting container process caused \\\"process_linux.go:359: container init caused \\\\\\\"could not create session key: function not implemented\\\\\\\"\\\"\\n\".\nIs hyperd container not compatible with docker 1.12?\n[root@centos-latest-6240306100 /]# docker --version\nDocker version 1.12.3, build 6b644ec\n. @gao-feng  \nI've installed the hyperd yesterday following its instruction. https://docs.hypercontainer.io/get_started/install/linux.html\nIs the binary distribution outdated?\nDo I need to build it from source?\n. ",
    "feiskyer": "@junoyoon Latest docker can be run smoothly inside Hyper pods, just don't forget to mount cgroups before starting dockerd, refer here.\n. @junoyoon Systemd requires much more configurations in containers, try to start dockerd without systemd, e.g. just run dockerd\n. Another commit has fixed this problem, so this one should be closed.\n. @gnawux No. I haven't encountered this problem since I updated hyper.\n. @carmark  What is AuthConfig? \nPlease provide a sample usage of image pulling with credentials.\n. It works.\n. Has been fixed\n. ```\nhyper run centos:latest\nPOD id is pod-lxpCHxfANR\nbash-4.2#\n``\n. TODO: \n- [ ] self-made haproxy image\n- [ ] pull image\n- [ ] fake service\n- [ ] exec/attach avoid proxy\n. Fixed by https://github.com/hyperhq/runv/pull/41\n. lgtm\n. lgtm\n. LGTM\n. LGTM\n. Merged as #156 \n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Hmm, it is caused by log volume. Can't find all files of host/mntin container's/var/logdir.\n. Fixed by #179\n. Here is indeed a dependency update. We can't wait for libvirt-go upstream to accept it. The modified version of libvirt-go is maintained at https://github.com/feiskyer/libvirt-go\n. sure\n. @gnawux Travis recovered\n. Implemented at https://github.com/hyperhq/runv/pull/117\n. LGTM\n. LGTM\n. Fixed by https://github.com/hyperhq/runv/pull/136\n. Fixed by #215\n. LGTM\n. Y, there are some problems onexecandattach. @laijs will fix this.\n. LGTM\n. LGTM\n. OK\n. LGTM\n. LGTM.\n. Close to recheck deps.\n. There are still some interfaces missing, e.g. image build, container commit and so on. And current http api is still not respect of restful.. Y, in/pod/info` api.\n. Implemented in #295.\n. Thanks for advising, the grpc server is still on working and we may change the log lib in the future.\nBy the way, what's your usecase of using hyperd as a library?\n. LGTM. Thanks.\n. @YaoZengzeng To pass travis, need run hack/update-generated-proto.sh.\n. LGTM. Thanks.\n. Please run hack/update-generated-proto.sh, do not edit types.pb.go manually.\ngo get -u github.com/golang/protobuf/protoc-gen-go\nhack/update-generated-proto.sh\n. @laijs Ready for review.\n. @laijs Rebased and added apis.\n. I will add user in #335 \n. LGTM\n. lgtm\n. @YaoZengzeng Pls rebase and complete the test as @resouer figured out.\n. LGTM\n. LGTM\n. LGTM\n. Closed by #368 \n. Attach failed in integration test:\nI0608 08:32:44.732454   25396 attach.go:17] Attach with request containerID:\"e7ae3942fd3b9e3db52f7205d198b848e2ee78025f82811ec4de304543b88b72\" tag:\"abcdefgh\" \nhyper_test.go:108:\n    c.Assert(err, IsNil)\n... value grpc.rpcError = grpc.rpcError{code:0x2, desc:\"Can find VM whose Id is !\"} (\"rpc error: code = 2 desc = Can find VM whose Id is !\")\nI0608 08:32:44.735090   25396 pod.go:89] PodRemove with request podID:\"pod-lpxrCToTwN\" \nI0608 08:32:44.735282   25396 pod.go:309] lock pod pod-lpxrCToTwN for operation rm\nI0608 08:32:44.735661   25396 pod.go:315] failed to lock pod pod-lpxrCToTwN for operation rm\nE0608 08:32:44.735832   25396 rm.go:27] Pod pod-lpxrCToTwN is under other operation\nE0608 08:32:44.735911   25396 pod.go:97] CleanPod pod-lpxrCToTwN failed: Pod pod-lpxrCToTwN is under other operation\nFAIL: hyper_test.go:84: TestSuite.TestPostAttach\n. Fixed attach and pod start problem #364 in latest commits. \nCC @resouer \n. lgtm\n. Close and reopen to trigger test\n. LGTM. Thanks.\n. Please also add serviceIP and  servicePort according to https://github.com/hyperhq/hyperd/pull/369/files#diff-f325a92d451852ea8923aaa3a50731beR322\n. LGTM. Thanks.\n. @gnawux Yep, already done.. lgtm\n. lgtm\n. lgtm\n. @resouer This will introduce problems on https://github.com/hyperhq/hyperd/blob/master/daemon/list.go#L107, maybe change filterPodContainers logic with suffix instead of name?\n. lgtm\n. @laijs CI failed, is this related with https://github.com/hyperhq/runv/pull/322?\n. @xlgao-zju Do we need addtional packages to build those deb files, e.g. https://github.com/hyperhq/hyperd/blob/master/.travis.yml#L12\n. LGTM. @gnawux Would you like to take another look?\n. Cool. LGTM.. LGTM. Thanks.\n. @gnawux Yep, already fixed.. ```\nhyperctl create -c nginx-5665882047 nginx\nhyperctl ERROR: \"create\" does not support attach and rm parameter\n```\nattach/rm should be set to false by default.. @gnawux Still same errors on creating a new container, blocks on wait for volumes:\nI1214 02:20:34.275461   21026 server.go:152] Calling POST /v0.7.0/container/create\nI1214 02:20:34.275539   21026 container_routes.go:123] Create container {\"name\":\"nginx-4959865160\",\"image\":\"nginx\",\"restartPolicy\":\"never\"} in pod nginx-0861304337\nI1214 02:20:34.280807   21026 container.go:488] Pod[nginx-0861304337] Con[(nginx-4959865160)] create container d39bc20cb9c2d54499e2135fc624cf78e29558fda4ab754eb49d2a325645a269 (w/: [])\nI1214 02:20:34.280903   21026 container.go:505] Pod[nginx-0861304337] Con[d39bc20cb9c2(nginx-4959865160)] container info config &container.Config{Hostname:\"d39bc20cb9c2\", Domainname:\"\", User:\"\", AttachStdin:false, AttachStdout:false, AttachStderr:false, ExposedPorts:map[nat.Port]struct {}{\"443/tcp\":struct {}{}, \"80/tcp\":struct {}{}}, PublishService:\"\", Tty:false, OpenStdin:false, StdinOnce:false, Env:[]string{\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", \"NGINX_VERSION=1.11.6-1~jessie\"}, Cmd:(*strslice.StrSlice)(0xc420f81360), ArgsEscaped:false, Image:\"nginx\", Volumes:map[string]struct {}(nil), WorkingDir:\"\", Entrypoint:(*strslice.StrSlice)(nil), NetworkDisabled:true, MacAddress:\"\", OnBuild:[]string(nil), Labels:map[string]string{}, StopSignal:\"\"}, Cmd [nginx -g daemon off;], Args [-g daemon off;]\nI1214 02:20:34.281489   21026 container.go:510] Pod[nginx-0861304337] Con[d39bc20cb9c2(nginx-4959865160)] describe container\nI1214 02:20:34.281547   21026 container.go:518] Pod[nginx-0861304337] Con[d39bc20cb9c2(nginx-4959865160)] mount id: 436515018fc23f22495e0b69973eec62a9c0500cc01030f4778c7709765b28a3\nI1214 02:20:34.281584   21026 container.go:581] Pod[nginx-0861304337] Con[d39bc20cb9c2(nginx-4959865160)] Container Info is\n&api.ContainerDescription{Id:\"d39bc20cb9c2d54499e2135fc624cf78e29558fda4ab754eb49d2a325645a269\", Name:\"/nginx-4959865160\", Image:\"sha256:abf312888d132e461c61484457ee9fd0125d666672e22f972f3b8c9a0ed3f0a1\", Labels:map[string]string(nil), Tty:false, StopSignal:\"TERM\", RootVolume:(*api.VolumeDescription)(0xc42088d3b0), MountId:\"436515018fc23f22495e0b69973eec62a9c0500cc01030f4778c7709765b28a3\", RootPath:\"rootfs\", UGI:(*api.UserGroupInfo)(nil), Envs:map[string]string{\"PATH\":\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\", \"NGINX_VERSION\":\"1.11.6-1~jessie\"}, Workdir:\"/\", Path:\"nginx\", Args:[]string{\"-g\", \"daemon off;\"}, Rlimits:[]*api.Rlimit{}, Sysctl:map[string]string(nil), Volumes:map[string]*api.VolumeReference(nil), Initialize:false}\nI1214 02:20:34.281595   21026 container.go:728] Pod[nginx-0861304337] Con[d39bc20cb9c2(nginx-4959865160)] configure dns\nI1214 02:20:34.281609   21026 container.go:786] Pod[nginx-0861304337] Con[d39bc20cb9c2(nginx-4959865160)] inject file /etc/resolv.conf\nI1214 02:20:34.282103   21026 container.go:845] Pod[nginx-0861304337] Con[d39bc20cb9c2(nginx-4959865160)] begin add to sandbox\nI1214 02:20:34.287811   21026 container.go:867] Pod[nginx-0861304337] Con[d39bc20cb9c2(nginx-4959865160)] finished container prepare, wait for volumes. LGTM. LGTM. There are generally three problems for service operations:\n\nWe should operations VM first (if running), and update podspec later. or else, podspec needs to rollback if vm operations failed.\nif podspec updated, it should also be saved to db so we don't lose it if pod restarted\n\nhow is ipvs rules being determined? could them support configurations? e.g. algorithms, mode (DR/NAT/FULLNAT)\n. lgtm. cc/ @gnawux would you like to take another look?. @gnawux It could, but I don't think there are such use cases.. I think volume validation is still required because\n\n\nPod spec didn't changed, multiple containers may reference same volume.\n\nA new container could reference an existing volume of the Pod, e.g. the same volume already being mounted on an existing container.\nIf the referenced volume doesn't exist, we should add it to the volume list instead of dropping it.\n\ncc @gnawux WDYT. @gnawux Yes, we don't need it any more.. LGTM. LGTM. Thanks.. @Crazykev  Conflicts, please re-generate the protobuf.. LGTM. Thanks.. @gnawux @laijs CI passed. PTAL.. LGTM. \nEncoutered hyperd crash on removing containers/pods:\nMar 14 07:42:21 ubuntu-0 hyperd[24575]: E0314 07:42:21.798543   24575 persist.go:102] Pod[k8s_POD.1_kube-dns-1178038448-z0v4p_kube-system_55c4bc75-\nMar 14 07:42:21 ubuntu-0 hyperd[24575]: E0314 07:42:21.798704   24575 persist.go:102] Pod[k8s_POD.1_kube-dns-847575035-p8fcw_kube-system_55c8a2d0-0\nMar 14 07:42:21 ubuntu-0 hyperd[24575]: E0314 07:42:21.798866   24575 persist.go:102] Pod[k8s_POD.2_kube-dns-514368535-kj73m_kube-system_f04cf2da-0\nMar 14 07:42:21 ubuntu-0 hyperd[24575]: E0314 07:42:21.799097   24575 persist.go:102] Pod[k8s_POD.2_kube-dns-806549836-1pjqc_kube-system_d96d7910-0\nMar 14 07:42:21 ubuntu-0 hyperd[24575]: E0314 07:42:21.799343   24575 persist.go:102] Pod[k8s_POD.2_kube-dns-806549836-8w1b9_kube-system_5f5cbb0a-0\nMar 14 07:42:21 ubuntu-0 hyperd[24575]: E0314 07:42:21.799548   24575 persist.go:102] Pod[k8s_POD.3_kube-dns-806549836-gxjx1_kube-system_117c4a2e-0\nMar 14 07:42:21 ubuntu-0 hyperd[24575]: E0314 07:42:21.799716   24575 persist.go:102] Pod[k8s_POD.4_kube-dns-2233971047-8b83r_kube-system_bc555f07-\nMar 14 07:43:10 ubuntu-0 hyperd[24575]: panic: runtime error: invalid memory address or nil pointer dereference\nMar 14 07:43:10 ubuntu-0 hyperd[24575]: [signal SIGSEGV: segmentation violation code=0x1 addr=0x10 pc=0x698f79]\nMar 14 07:43:10 ubuntu-0 hyperd[24575]: goroutine 369 [running]:\nMar 14 07:43:10 ubuntu-0 hyperd[24575]: panic(0x11d5a80, 0xc420010040)\nMar 14 07:43:10 ubuntu-0 hyperd[24575]:         /usr/local/go/src/runtime/panic.go:500 +0x1a1\nMar 14 07:43:10 ubuntu-0 hyperd[24575]: github.com/hyperhq/hyperd/vendor/github.com/hyperhq/runv/hypervisor.(*Vm).RemoveVolume(0x0, 0xc421057460, 0\nMar 14 07:43:10 ubuntu-0 hyperd[24575]:         /go/src/github.com/hyperhq/hyperd/vendor/github.com/hyperhq/runv/hypervisor/vm.go:571 +0x49\nMar 14 07:43:10 ubuntu-0 hyperd[24575]: github.com/hyperhq/hyperd/daemon/pod.(*Volume).tryRemoveFromSandbox(0xc4202a1f90, 0xc4206a9950, 0xc4210579a\nMar 14 07:43:10 ubuntu-0 hyperd[24575]:         /go/src/github.com/hyperhq/hyperd/daemon/pod/volume.go:138 +0x69\nMar 14 07:43:10 ubuntu-0 hyperd[24575]: github.com/hyperhq/hyperd/daemon/pod.(*XPod).RemoveContainer(0xc4201cf1e0, 0xc420a7e480, 0x40, 0x0, 0x0)\nMar 14 07:43:10 ubuntu-0 hyperd[24575]:         /go/src/github.com/hyperhq/hyperd/daemon/pod/decommission.go:286 +0x3ed\nMar 14 07:43:10 ubuntu-0 hyperd[24575]: github.com/hyperhq/hyperd/daemon.(*Daemon).RemoveContainer(0xc4206a40e0, 0xc420a7e480, 0x40, 0x2, 0xc420a6d\nMar 14 07:43:10 ubuntu-0 hyperd[24575]:         /go/src/github.com/hyperhq/hyperd/daemon/rm.go:47 +0x7e\nMar 14 07:43:10 ubuntu-0 hyperd[24575]: github.com/hyperhq/hyperd/serverrpc.(*ServerRPC).ContainerRemove(0xc420695300, 0x7f5f24a99b28, 0xc420a717a0\nMar 14 07:43:10 ubuntu-0 hyperd[24575]:         /go/src/github.com/hyperhq/hyperd/serverrpc/container.go:62 +0x173\nMar 14 07:43:10 ubuntu-0 hyperd[24575]: github.com/hyperhq/hyperd/types._PublicAPI_ContainerRemove_Handler(0x1354780, 0xc420695300, 0x7f5f24a99b28,\nMar 14 07:43:10 ubuntu-0 hyperd[24575]:         /go/src/github.com/hyperhq/hyperd/types/types.pb.go:3146 +0xdd\nMar 14 07:43:10 ubuntu-0 hyperd[24575]: github.com/hyperhq/hyperd/vendor/google.golang.org/grpc.(*Server).processUnaryRPC(0xc4206e9800, 0x1c99460,\nMar 14 07:43:10 ubuntu-0 hyperd[24575]:         /go/src/github.com/hyperhq/hyperd/vendor/google.golang.org/grpc/server.go:425 +0x9ca\nMar 14 07:43:10 ubuntu-0 hyperd[24575]: github.com/hyperhq/hyperd/vendor/google.golang.org/grpc.(*Server).handleStream(0xc4206e9800, 0x1c99460, 0xc\nMar 14 07:43:10 ubuntu-0 hyperd[24575]:         /go/src/github.com/hyperhq/hyperd/vendor/google.golang.org/grpc/server.go:574 +0x6ad\nMar 14 07:43:10 ubuntu-0 hyperd[24575]: github.com/hyperhq/hyperd/vendor/google.golang.org/grpc.(*Server).serveStreams.func1.1(0xc4207912f0, 0xc420\nMar 14 07:43:10 ubuntu-0 hyperd[24575]:         /go/src/github.com/hyperhq/hyperd/vendor/google.golang.org/grpc/server.go:296 +0xab\nMar 14 07:43:10 ubuntu-0 hyperd[24575]: created by github.com/hyperhq/hyperd/vendor/google.golang.org/grpc.(*Server).serveStreams.func1\nMar 14 07:43:10 ubuntu-0 hyperd[24575]:         /go/src/github.com/hyperhq/hyperd/vendor/google.golang.org/grpc/server.go:297 +0xa3\nMar 14 07:43:10 ubuntu-0 systemd[1]: hyperd.service: Main process exited, code=exited, status=2/INVALIDARGUMENT\nMar 14 07:43:10 ubuntu-0 systemd[1]: hyperd.service: Unit entered failed state.\n@YaoZengzeng @laijs Any updates on this. \n. LGTM. LGTM. Let's merge this first and check whether there are other potential problems by adding loads to hyperd (e.g. via kubernetes tests).. Yep, also found these logs, but nobody exists in /etc/passwd:\nhyperctl exec 9e635efb753f476a2bb3fa4e3d5d9b41e24c059f8e4d1b43ee78a23472882448 cat /etc/passwd | grep nobody\nnobody:x:65534:65534:nobody:/:/sbin/nologin\n. Cool!. @gnawux: @xlgao-zju said it reports different container IDs when trying to remove the image.. LGTM. Fixed by #575.. @gnawux @gao-feng Any ideas on this?\n. Fixed by hyperhq/hyperstart#283.. cc @laijs @gao-feng . LGTM. Please update the runv vendor.. @gnawux The command executes success, so there is no non-zero return code.. @laijs @gao-feng Could you help to fix the problem?. ```\nhyperctl logs dce0c7c9bd61fc7b53fa0936a0442e0b4c8b93e5000dfd06dd73014380db1938\ntotal 16\ndrwxrwxrwt    3 root     root           140 Apr 13 03:31 .\ndrwxr-xr-x    3 root     root          4096 Apr 13 03:31 ..\ndrwxr-xr-x    2 root     root           100 Apr 13 03:31 ..4984_13_04_03_31_10.967674860\ndrwxr-xr-x    2 root     root           100 Apr 13 03:31 ..data\n-rw-r--r--    1 root     root            17 Apr 13 03:31 configmap-data\n-rw-r--r--    1 root     root             4 Apr 13 03:31 podname\n-rw-r--r--    1 root     root             5 Apr 13 03:31 secret-data\n``. If the pod is started withsh` and exec the same command after it has been started, the output is (which is expected above):\ntotal 16\ndrwxrwxrwt    3 root     root           140 Apr 13 04:10 .\ndrwxr-xr-x    3 root     root          4096 Apr 13 04:10 ..\ndrwxr-xr-x    2 root     root           100 Apr 13 04:10 ..4984_13_04_04_10_49.294962174\ndrwxr-xr-x    2 root     root           100 Apr 13 04:10 ..data\n-rw-r--r--    1 root     root            17 Apr 13 04:10 configmap-data\n-rw-r--r--    1 root     root             5 Apr 13 04:10 podname\n-rw-r--r--    1 root     root             5 Apr 13 04:10 secret-data\ntest2adminconfigmap-value-1. @bergwolf Could we add a fix for this?. Fixed by https://github.com/hyperhq/hyperd/pull/624. LGTM. Also cc/ @resouer. Fixed by https://github.com/hyperhq/hyperstart/pull/295. Also cc/ @resouer . Dup of #613. Closing this one.. LGTM. cc @gnawux . @bergwolf Any updates on this?. The code LGTM. . LGTM. Updated. And shell will delete the trailing space so TrimSpace is not needed.\n. Log error msg here?\n. Same meaning with entrypoint and commands\n. why repeated string option? A struct is more clear than repeated string.\n. Thanks. Another ref https://github.com/opencontainers/runc/blob/master/libcontainer/specconv/spec_linux.go#L583-L608\n. image is actually the name of image\n. Where is processinfo defined?\n. Y, thanks.\n. Is pod.cleanupEtcHosts() needed here?\n. Y, thanks.\n. Y, should be &types.ContainerListResponse{ContainerList: nil}, nil\n. Updated.\n. Updated.\n. If req is indeed nil, then req.Cpu == 0 || req.Memory == 0 will panic.\n. The client may pass in a nil req. \n. This PR adds UserPod in hyperd, so the dedicated pod.UserPod could be called RunvPodSpec and pod.UserPod should be removed in another runv's PR. \nThere is a todo to remove this convertion: // TODO: remove convertToRunvPodSpec after pod.UserPod is deleted from runv\n. Not really. Apitypes.UserPod is the new interface in hyperd to replace runv's UserPod.\n. No,apitypes.UserPodis for all hyperd APIs and clients. Users can see it directly:\n- For gRPC api, users must specifyapitypes.UserPodwhile creating Pods\n- For original restful api, thepod.jsonis unmarshaled toapitypes.UserPod`\nAfter this PR, hyperd APIs will no longer depend on UserPod in runv. \n\nI don't think apitypes.UserPod can replace pod.UserPod. \n\nWhy can't?\n. > If user uses grpc api and pod spec to create pod, the extra conversion from json to protobuf is needed.\nNo, there is no json to protobuf conversion in gRPC api. Users could pass apitypes.UserPod struct directly in create API.\n. Yes, if hyperctl switches to gRPC, then the conversion should be done on hyperctl client.\n. Should use bytes here to avoid conversions.\n. No need for repeated, bytes is mapping to []byte in Go,\n. What will happen if there is no \\n in logs? Will it block for ever?\n. Remove new lines here\n. Yes, that is what we should do.\n. @Crazykev we can make a workaround for now: Rename ExitCode to Wait, and rename tag to id. \n. command should be []string here.\n. Command should be json marshaled:\ngo\ncmd, err := json.Marshal(command)\n    if err != nil {\n        return err\n    }\nAnd then set Command: string(cmd)\n. Remove key as we don't really need it.\n. rename this field to containerID\n. @laijs  Any updates on pod/container state machine?\n. ContainerStop stops the specified container\n. /container/stop restful api and client are also required. \n. Could you update the status of container and keep pod running even container is stopped?\n. Is it possible to replace data with UserService?\n. nit: modifying those fields will also change restful apis.\n. Please update those docs as ServiceList gets a list of services\n. Please indent those field by =\n. Update doc as ContainerRemove deletes the specified container\n. Rename RmContainer to full name? e.g. RemoveContainer\n. Follow docs above.\n. It's better to keep {} in one line same as other parts.\n. New line is not needed here.\n. GetPodStats only works for libvirt, but the tests all cover qemu hypervisor. Could you make this test only runs for libvirt? \nThe hypervisor type could be got by calling Info api:\nhyperctl info | grep Execu\nExecution Driver: libvirt\n. s/GEt/Get/\n. s/GetPodStats/PodStats/, for keeping doc consistent with real interface name.\n. My mistake. Fixed.\n. Could you define -1 and -2 as const?\n. nit: s/start/starts. Check whether copt is nil before this.. For help subcommand, both copt and err are nil.. nit: also logs the container id for clear.. nit: also logs the pod id for clear.. podId is not required when starting a created container, add a new -c option for containers?. nit: remove podId from params.. nit: remove podId. dup with else block, it's better move the code after else block.. log as warning instead of info?. glog.Warningf. Those are removed accidently or intended?. >  remove container shouldn't remove related volume from pod\nWhy? I think if a volume is not referenced by any containers, it should be removed.. nit: indent with another space. It's better to use %q for strings in logs.. ",
    "halacs": "Hi @gnawux ,\nAre there any progress on this resource isolation issue? I am gathering info about the resource management of HyperContainer.\nYou mentioned Kubernetes as well. Does Kubernetes already have these capabilities? hyperctl seems don't have similar command line argument than the above mentioned.\nThanks for your answer!. ",
    "shimiaofeng": "Logs : \n[HYPER INFO  0725 07:09:23 16679 xen.go] Xen Driver Load failed: failed to initialize xen context\n[HYPER INFO  0725 07:09:23 16679 vm.go] Qemu Driver Loaded\n[HYPER INFO  0725 07:09:23 16679 hyperd.go] The config file is \n[HYPER INFO  0725 07:09:23 16679 daemon.go] The config: kernel=/var/lib/hyper/kernel, initrd=/var/lib/hyper/hyper-initrd.img\n[HYPER INFO  0725 07:09:23 16679 daemon.go] The config: bridge=, ip=\n[HYPER INFO  0725 07:09:23 16679 daemon.go] The config: bios=/var/lib/hyper/bios-qboot.bin, cbfs=/var/lib/hyper/cbfs-qboot.rom\n[HYPER INFO  0725 07:09:23 16679 network.go] bridge exist\n[HYPER INFO  0725 07:09:23 16679 network.go] modprobe br_netfilter failed modprobe br_netfilter failed\n[HYPER INFO  0725 07:09:23 16679 hyperd.go] Hyper daemon: 0.2.1 0\n[HYPER INFO  0725 07:09:23 16679 job.go] +job acceptconnections()\n[HYPER INFO  0725 07:09:23 16679 job.go] -job acceptconnections() OK\n[HYPER INFO  0725 07:09:23 16679 hyperd.go] Daemon has completed initialization\n[HYPER INFO  0725 07:09:23 16679 daemon.go] Get the pod item, pod is pod-pod-bjYnwuYLoe!\n[HYPER INFO  0725 07:09:23 16679 daemon.go] Get the pod item, pod is pod-pod-jtXeHNBYRj!\n[HYPER INFO  0725 07:09:23 16679 pod.go] leveldb: not found\n[HYPER INFO  0725 07:09:23 16679 pod.go] Process the Containers section in POD SPEC\n[HYPER INFO  0725 07:09:23 16679 create.go] The Repository is ubuntu, and the tag is latest\n[HYPER INFO  0725 07:09:23 16679 job.go] +job serveapi(unix:///var/run/hyper.sock)\n[HYPER INFO  0725 07:09:23 16679 server.go] Listening for HTTP on unix (/var/run/hyper.sock)\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering GET, /info\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering GET, /pod/info\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering GET, /version\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering GET, /list\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering POST, /pod/stop\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering POST, /vm/create\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering POST, /attach\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering POST, /container/create\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering POST, /image/create\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering POST, /pod/create\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering POST, /pod/start\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering POST, /pod/remove\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering POST, /tty/resize\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering POST, /pod/run\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering POST, /vm/kill\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering POST, /exec\n[HYPER INFO  0725 07:09:23 16679 server.go] Registering OPTIONS, \n[HYPER INFO  0725 07:09:24 16679 create.go] The returned status code is 201!\n[HYPER INFO  0725 07:09:24 16679 daemon.go] leveldb: not found for pod-bjYnwuYLoe\n[HYPER INFO  0725 07:09:24 16679 pod.go] leveldb: not found\n[HYPER INFO  0725 07:09:24 16679 pod.go] Process the Containers section in POD SPEC\n[HYPER INFO  0725 07:09:24 16679 create.go] The Repository is ubuntu, and the tag is latest\n[HYPER INFO  0725 07:09:24 16679 create.go] The returned status code is 201!\n[HYPER INFO  0725 07:09:24 16679 daemon.go] leveldb: not found for pod-jtXeHNBYRj\n[HYPER INFO  0725 07:09:54 16679 server.go] Calling POST /pod/run\n[HYPER INFO  0725 07:09:54 16679 job.go] +job podRun({\"id\":\"ubuntu-latest-2931021243\",\"containers\":[{\"name\":\"ubuntu-latest-2931021243\",\"image\":\"ubuntu:latest\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\"})\n[HYPER INFO  0725 07:09:54 16679 pod.go] {\"id\":\"ubuntu-latest-2931021243\",\"containers\":[{\"name\":\"ubuntu-latest-2931021243\",\"image\":\"ubuntu:latest\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\"}\n[HYPER INFO  0725 07:09:54 16679 pod.go] The config: kernel=/var/lib/hyper/kernel, initrd=/var/lib/hyper/hyper-initrd.img\n[HYPER INFO  0725 07:09:54 16679 pod.go] leveldb: not found\n[HYPER INFO  0725 07:09:54 16679 pod.go] Process the Containers section in POD SPEC\n[HYPER INFO  0725 07:09:54 16679 create.go] The Repository is ubuntu, and the tag is latest\n[HYPER INFO  0725 07:09:54 16679 qemu_process.go] cmdline arguments: -machine pc-i440fx-2.0,accel=kvm,usb=off -global kvm-pit.lost_tick_policy=discard -cpu host -drive if=pflash,file=/var/lib/hyper/bios-qboot.bin,readonly=on -drive if=pflash,file=/var/lib/hyper/cbfs-qboot.rom,readonly=on -realtime mlock=off -no-user-config -nodefaults -no-hpet -rtc base=utc,driftfix=slew -no-reboot -display none -boot strict=on -m 128 -smp 1 -qmp unix:/var/run/hyper/vm-hvTPypPyGc/qmp.sock,server,nowait -serial unix:/var/run/hyper/vm-hvTPypPyGc/console.sock,server,nowait -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x2 -device virtio-scsi-pci,id=scsi0,bus=pci.0,addr=0x3 -chardev socket,id=charch0,path=/var/run/hyper/vm-hvTPypPyGc/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0 -chardev socket,id=charch1,path=/var/run/hyper/vm-hvTPypPyGc/tty.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1 -fsdev local,id=virtio9p,path=/var/run/hyper/vm-hvTPypPyGc/share_dir,security_model=none -device virtio-9p-pci,fsdev=virtio9p,mount_tag=share_dir\n[HYPER INFO  0725 07:09:54 16679 qemu_process.go] qemu daemon pid 16792.\n[HYPER INFO  0725 07:09:54 16679 qemu_process.go] starting daemon with pid: 16792\n[HYPER INFO  0725 07:09:54 16679 init_comm.go] Wating for init messages...\n[HYPER INFO  0725 07:09:54 16679 init_comm.go] trying to read 8 bytes\n[HYPER INFO  0725 07:09:54 16679 tty.go] tty socket connected\n[HYPER INFO  0725 07:09:54 16679 tty.go] tty: trying to read 12 bytes\n[HYPER INFO  0725 07:09:54 16679 init_comm.go] connected to /var/run/hyper/vm-hvTPypPyGc/console.sock\n[HYPER INFO  0725 07:09:54 16679 init_comm.go] connected /var/run/hyper/vm-hvTPypPyGc/console.sock as telnet mode.\n[HYPER INFO  0725 07:09:54 16679 qmp_handler.go] connected to /var/run/hyper/vm-hvTPypPyGc/qmp.sock\n[HYPER INFO  0725 07:09:54 16679 qmp_handler.go] begin qmp init...\n[HYPER ERROR 0725 07:09:54 16679 init_comm.go] read init data failed\n[HYPER ERROR 0725 07:09:54 16679 init_comm.go] read init message failed... read unix /var/run/hyper/vm-hvTPypPyGc/hyper.sock: connection reset by peer\n[HYPER ERROR 0725 07:09:54 16679 qmp_handler.go] get qmp welcome failed: read unix /var/run/hyper/vm-hvTPypPyGc/qmp.sock: connection reset by peer\n[HYPER INFO  0725 07:09:54 16679 hypervisor.go] main event loop got message 30(ERROR_INIT_FAIL)\n[HYPER ERROR 0725 07:09:54 16679 vm_states.go] read init message failed... read unix /var/run/hyper/vm-hvTPypPyGc/hyper.sock: connection reset by peer\n[HYPER ERROR 0725 07:09:54 16679 vm_states.go] Shutting down because of an exception: Fail during init environment\n[HYPER INFO  0725 07:09:54 16679 context.go] VM vm-hvTPypPyGc: state change from  to 'DESTROYING'\n[HYPER ERROR 0725 07:09:54 16679 qmp_handler.go] QMP initialize failed\n[HYPER INFO  0725 07:09:54 16679 hypervisor.go] main event loop got message 30(ERROR_INIT_FAIL)\n[HYPER WARN  0725 07:09:54 16679 vm_states.go] got event during vm cleaning up\n[HYPER INFO  0725 07:09:54 16679 tty.go] Input byte chan closed, close the output string chan\n[HYPER INFO  0725 07:09:54 16679 init_comm.go] console output end\n[HYPER ERROR 0725 07:09:54 16679 tty.go] read tty data failed\n[HYPER INFO  0725 07:09:54 16679 tty.go] tty socket closed, quit the reading goroutine read unix /var/run/hyper/vm-hvTPypPyGc/tty.sock: connection reset by peer\n[HYPER INFO  0725 07:09:54 16679 hypervisor.go] main event loop got message 32(ERROR_INTERRUPTED)\n[HYPER INFO  0725 07:09:54 16679 vm_states.go] Connection interrupted while destroying\n[HYPER INFO  0725 07:09:54 16679 tty.go] tty chan closed, quit sent goroutine\n[HYPER INFO  0725 07:09:54 16679 create.go] The returned status code is 201!\n[HYPER INFO  0725 07:09:54 16679 container.go] ready to get the container(30e342681b5842bc790f8cb6b1f38562c576d9e074cc7da79f97d869959d78bf) info\n[HYPER INFO  0725 07:09:54 16679 pod.go] Parsing envs for container 0: 0 Evs\n[HYPER INFO  0725 07:09:54 16679 pod.go] The fs type is ext4\n[HYPER INFO  0725 07:09:54 16679 pod.go] WorkingDir is \n[HYPER INFO  0725 07:09:54 16679 pod.go] Image is /dev/mapper/docker-253:0-2490566-30e342681b5842bc790f8cb6b1f38562c576d9e074cc7da79f97d869959d78bf\n[HYPER INFO  0725 07:09:54 16679 pod.go] Container Info is \n&{30e342681b5842bc790f8cb6b1f38562c576d9e074cc7da79f97d869959d78bf /rootfs /dev/mapper/docker-253:0-2490566-30e342681b5842bc790f8cb6b1f38562c576d9e074cc7da79f97d869959d78bf ext4  [] [/bin/bash] map[]}\n[HYPER INFO  0725 07:09:54 16679 pod.go] container 0 created 30e342681b5842bc790f8cb6b1f38562c576d9e074cc7da79f97d869959d78bf, workdir , env: map[]\nPOD id is pod-GMkKPksKAf\n[HYPER INFO  0725 07:09:54 16679 hypervisor.go] main event loop got message 20(COMMAND_RUN_POD)\n[HYPER WARN  0725 07:09:54 16679 vm_states.go] got event during vm cleaning up\n[HYPER INFO  0725 07:09:54 16679 pod.go] Get the response from QEMU, VM id is vm-hvTPypPyGc!\n[HYPER INFO  0725 07:09:54 16679 hypervisor.go] main event loop got message 23(COMMAND_SHUTDOWN)\n[HYPER WARN  0725 07:09:54 16679 vm_states.go] got event during vm cleaning up\n[HYPER INFO  0725 07:10:04 16679 hypervisor.go] main event loop got message 3(EVENT_VM_TIMEOUT)\n[HYPER INFO  0725 07:10:04 16679 vm_states.go] Device removing timeout\n[HYPER INFO  0725 07:10:04 16679 qemu_process.go] quit watch dog.\n. The ubuntu images has already download and exist in host. I can run hyper pull command successfully.\nhost os is centos 6.6 and update kernel to 3.10.18\nI download the qemu2.0.0 and execute ./configure default and then make\n. @gnawux, should I edit the configure file when build qemu? \n. @gnawux \nI build qume2.0 with virtfs enabled and now work well. Thanks.\nBy the way , bring your own kernel means I could build any other linux kernels ? have docs  to follow this work? \n. @gnawux \ndoes any commands exsit that could copy files between host and hyper VMs,like docker cp ?\n. docker cp  copy file from contaner to host successfuly. But  with cp command to copy file from host to conainer ,I could not find the file inside ?\n. ",
    "chiefy": "Quick service @carmark , thanks! :+1: \n. Installed the package, but it looks like the daemon is having issues w/ VirtualBox:\n8/2/15 8:55:53.000 AM kernel[0]: org_virtualbox_SupDrvClient::initWithTask: Expected cookie 0x64726962 (VBoxSVC)\n8/2/15 8:55:53.000 AM kernel[0]: org_virtualbox_SupDrvClient::initWithTask: Expected cookie 0x64726962 (VBoxHeadless)\n8/2/15 8:55:53.000 AM kernel[0]: VBoxDrv: host_vmxon  -> vmx_use_count=1 rc=0\n8/2/15 8:55:53.000 AM kernel[0]: VBoxDrv: host_vmxoff -> vmx_use_count=0\n8/2/15 8:55:53.000 AM kernel[0]: vboxdrv: ffffff8238953020 VMMR0.r0\n8/2/15 8:55:53.000 AM kernel[0]: vboxdrv: ffffff8238b4f020 VBoxDDR0.r0\n8/2/15 8:55:53.000 AM kernel[0]: vboxdrv: ffffff81fe62a020 VBoxDD2R0.r0\n8/2/15 8:55:54.000 AM kernel[0]: VBoxDrv: host_vmxon  -> vmx_use_count=1 rc=0\n8/2/15 8:55:54.000 AM kernel[0]: VBoxDrv: host_vmxoff -> vmx_use_count=0\n8/2/15 8:55:54.216 AM com.apple.launchd[1]: (sh.hyper.hyper) Throttling respawn: Will start in 10 seconds\n. @carmark still having issue after restart:\n8/3/15 9:57:22.000 AM kernel[0]: org_virtualbox_SupDrvClient::initWithTask: Expected cookie 0x64726962 (VBoxSVC)\n8/3/15 9:57:22.000 AM kernel[0]: org_virtualbox_SupDrvClient::initWithTask: Expected cookie 0x64726962 (VBoxHeadless)\n8/3/15 9:57:23.205 AM com.apple.launchd[1]: (sh.hyper.hyper) Throttling respawn: Will start in 10 seconds\nRunning boot2docker in VB, seems fine otherwise?\n. bash\n~\n\u276f hyper run --attach --rm alpine:latest /bin/ash\nPOD id is pod-ahmTVgyoJs\n:beers:  thanks @carmark !\n. @carmark yes, Homebrew simply needs a set of source files at a specific tagged version w/ SHA hash. Tagging 0.3 to start would be fine. Thanks.\n. ",
    "jefby": "@carmark Hi , i compile the hyper code and run hyperd ,it can't work, why ? and how to set the config ??Thanks very much.\n```\n./hyperd\n[HYPER INFO  0802 15:36:21 04972 xen.go] Xen Driver Load failed: failed to initialize xen context\n[HYPER INFO  0802 15:36:21 04972 vm.go] Qemu Driver Loaded\n[HYPER INFO  0802 15:36:21 04972 hyperd.go] The config file is\n[HYPER ERROR 0802 15:36:21 04972 daemon.go] Read config file (/etc/hyper/config) failed, open /etc/hyper/config: no such file or directory\n[HYPER ERROR 0802 15:36:21 04972 hyperd.go] The hyperd create failed, open /etc/hyper/config: no such file or directory\n```\n. ",
    "Termina1": "And what about xhyve?\n. ",
    "puresoul": "Hi, first I try install by hyper.sh and got that issue, after that I compiled everything myself and problem still presist. \n. so, that pid isn't hyperd and I can't track process to that pid. Here is log with -v=3, only thing what I've done in vm was apt-get update. Vm stuck in the middle of that process.\nlog of hyperd http://pastebin.com/CMdZp6vJ\n. It produce more output but still same problem, do you have some another guess to try? Thanks\nhttp://pastebin.com/DFrWsZ9m\n. could you post what revisions of xen and hyper is verified to works. Then I'll checkout source, build and try it once more? Thanks\n. ok, so now i can't startup vm. Only that:\n```\n  root@w1:/etc/hyper# hyper run ubuntu\n  hyper ERROR: QEMU response data is nil\nroot@w1:/etc/hyper# cat /etc/hyper/config \n  Host= unix:///var/run/hyper.sock\n  Bios=/var/lib/hyper/bios-qboot.bin\n  Kernel=/var/lib/hyper/kernel\n  Initrd=/var/lib/hyper/hyper-initrd.img\n  Bridge=hyper0\n```\nhyperd:\nhttp://pastebin.com/Pt2DTmdm\n. Ok, so I got it finaly working. xen 4.6rc, hyper-0.2dev and vanila kernel+initrd from installer. One thing that i changed was qemu-system-x86_64 binary. Xen qemu-system-i386 was maybe the problem. Installed qemu from repo and it's working... You may close this issue, thanks\nEdit: hyper now don't use xen, but run on it's own with qemu from repo, so i try compile xen with \"--with-system-qemu=\"\nSo, recompile xen with other qemu won't help. Still got hyper runing in dom0 and no as domU\n. hyper vm wouldn't show in \"xl list\", so it's runing in dom0, and no at xen....\n. I'll try it, tahnks\n. so, after while i got a question. Maybe here is bad place for that. I can't get from xen qemu-system-x86_64 binary, and qemu-system-i386 isn't compatible\n[HYPER INFO  1102 12:06:00 01624 init_comm.go] [:45] [console] This kernel requires an x86-64 CPU, but only detected an i686 CPU.\n[HYPER INFO  1102 12:06:00 01624 init_comm.go] [:45] [console] Unable to boot - please use a kernel appropriate for your CPU.\n. On same machine. Installed stock hyper by curl/bash installation with xen 4.6. New bunch of errors occured. But that may be my own fault?\nhttp://pastebin.com/c32LiEmY\n. I'am not build xen for i386 and because as xen does not rely on qemu for CPU emulation, I don't understand what I'am doing wrong.\n. ok, another strange things. Just overwrite hyper with own compiled one. Now I got hyper vm/pod running in xen:\nroot@w1:/etc/hyper# xl list\n   Name                                        ID   Mem VCPUs   State   Time(s)\n   Domain-0                                     0  2048     2     r-----    2054.9\n   vm-mpTJiIAepG                                6   128     1     -b----       1.5\nNow network is broken and if try manage eth0, vm/pod again crashes...\n. that was about mounth old revision, by the way...\n. there may be some problem with hyper bridge settings. If I network-detach interface and then network-attach, network over xen bridge works. But this whole issue is totaly network related, because until I use it, pod/vm is stable. After I connect it and do some load, hyper or xen somehow kills vm/pod\n. Hyperd log from last testings\nhttp://pastebin.com/96WsWas8\n. So I compiled actual revision and same problem with network presist. But now some new stuff appers at log,\nhttp://pastebin.com/GC76KKLM\n. ok, I'll try. And another problem. The image somehow get corupted and I won't get shell anymore, after the pod (If I call it right) is killed. Need to delete it and pull new one. Don't know why...\n. ok, even updated hyperstart won't help. This is weird. Can I somehow enforce another network frontend/backend. Maybe it's related to netback/netfront\n. sorry, misstype. Ok, I'll try\n. I pull changes from git on hyper, runv and hyperstart. Recompile and start pod with 512mb. If I starts same apt-get with only main repo, it runs ok. After I add universe and multiverse, same situation. Freez and shutdown, it is maybe related to ubuntu image. I may try debian or anything else also....\nhttp://pastebin.com/9EXCMeih\n. won't resolv. Just drop hyper in favour of LXD. \n. ",
    "dhargitai": "Hi @carmark,\nSure.\nconsole\n$ hyper info\nImages: 31\nContainers: 0\nPODs: 0\nStorage Driver: vbox\nHyper Root Dir: /private/var/lib/hyper\nIndex Server Address: https://index.docker.io/v1/\nExecution Driver: VirtualBox\nTotal Memory: 10.0 GB\nOperating System: Darwin Kernel Version 13.4.0\nI'd like to build an image from this Dockerfile: https://gist.github.com/dhargitai/e6b9c74cfe64b25f42e2\n. Thank you @carmark \n. ",
    "darylteo": "I have the same issue here.\nImages: 42\nContainers: 2\nPODs: 2\nStorage Driver: vbox\nHyper Root Dir: /private/var/lib/hyper\nIndex Server Address: https://index.docker.io/v1/\nExecution Driver: VirtualBox\nTotal Memory: 8.0 GB\nOperating System: Darwin Kernel Version 14.4.0\n```\nFROM php:5.6-apache\nMAINTAINER Daryl Teo\nCOPY build.sh /\nRUN chmod +x /build.sh\nRUN /build.sh\nCOPY install.sh run.sh config_xdebug.sh /\nRUN chmod +x /install.sh /run.sh /config_xdebug.sh\nRUN /install.sh\nENV XDEBUG_PORT 9000\nENTRYPOINT [\"/run.sh\"]\nCMD [\"apache2-foreground\"]\n.\n!/bin/bash\nset -e\napt-get update\n--- begin php setup ---\necho 'Setting up PHP'\n--- end php setup ---\n--- begin php extensions ---\necho 'Setting up PHP extensions'\ngd installation\napt-get install -y \\\n    php5-gd \\\n    libjpeg62-turbo-dev \\\n    libpng12-dev\ndocker-php-ext-configure gd --with-jpeg-dir=/usr/include/\ndocker-php-ext-install gd\nxdebug\napt-get install -y \\\n    php5-xdebug\nmb_string required for mPDF library\ndocker-php-ext-install mbstring\npdo mysql adaptor\ndocker-php-ext-install pdo_mysql\n# curl\napt-get install -y libcurl\ndocker-php-ext-install curl\n# recode\napt-get install -y librecode-dev librecode0 php5-recode recode\ndocker-php-ext-install recode\ndocker-php-ext-install json\ndocker-php-ext-install mysql\ndocker-php-ext-install mysqli\ndocker-php-ext-install pdo_mysql\ndocker-php-ext-install pspell\ndocker-php-ext-install readline\ndocker-php-ext-install xsl\n--- end php extensions ---\n```\n. I haven't followed the issue lately and moved away from docker for production use, but keep it up anyway!\n. ",
    "chr4": "I can confirm this on osx/ virtualbox using even the simplest Dockerfiles:\n$ cat Dockerfile\nFROM ubuntu\nCMD \"echo hello\"\n$ hyper build .\nSending build context to Docker daemon 2.048 kB\nhyper ERROR: the out tar file is not exist\n. ",
    "robinroestenburg": "Same problem here, I get the same results from @chr4's minimal test.\nOutput from hyper info:\nImages: 5\nContainers: 0\nPODs: 0\nStorage Driver: vbox\n  Root Dir: /private/var/lib/hyper/vbox\n  Backing Filesystem: hfs+\nHyper Root Dir: /private/var/lib/hyper\nIndex Server Address: https://index.docker.io/v1/\nExecution Driver: vbox\nTotal Memory: 8.0 GB\nOperating System: Darwin Kernel Version 14.5.0\n. ",
    "mckelvin": "@chr4 +1\nkelvin@PANs-MacBook-Pro:/tmp/reproduce \u00bb ls\nDockerfile\nkelvin@PANs-MacBook-Pro:/tmp/reproduce \u00bb cat Dockerfile\nFROM ubuntu:14.04\nRUN ls\nkelvin@PANs-MacBook-Pro:/tmp/reproduce \u00bb hyper info\nImages: 5\nContainers: 0\nPODs: 0\nStorage Driver: vbox\n  Root Dir: /private/var/lib/hyper/vbox\n  Backing Filesystem: hfs+\nHyper Root Dir: /private/var/lib/hyper\nIndex Server Address: https://index.docker.io/v1/\nExecution Driver: vbox\nTotal Memory: 8.0 GB\nOperating System: Darwin Kernel Version 15.0.0\nkelvin@PANs-MacBook-Pro:/tmp/reproduce \u00bb hyper build -t \"reproduce-issue-33\" .\nSending build context to Docker daemon 2.048 kB\nhyper ERROR: the out tar file is not exist\nOutput of xtail /var/log/hyper/hyperd* while running hyper build -t \"reproduce-issue-33\" .: https://gist.github.com/mckelvin/010ff60c234def9cfde3\n\nI uninstalled the hyper:\n```\n$ sudo /opt/hyper/bin/uninstall-hyper.sh --purge\nVBoxManage: error: Machine 'hyper-mac-pull-vm' is not currently running\n0%...10%...20%...30%...40%...50%...60%...70%...80%...90%...100%\n$ tree /private/var/lib/hyper\n/private/var/lib/hyper\n\u2514\u2500\u2500 tar\n```\nand I'm sure no hyper/VirtualBox process was running via:\nps aux | grep hyper\nps aux | grep irtual\nthen reinstall again and again, but the issue still continues. :(\n. ALSO SEE: https://github.com/hyperhq/hyper/issues/59\n. ",
    "robhicks": "+1 \nOSX 10.11.1\nImages: 6\nContainers: 0\nPODs: 0\nStorage Driver: vbox\n  Root Dir: /private/var/lib/hyper/vbox\n  Backing Filesystem: hfs+\nHyper Root Dir: /private/var/lib/hyper\nIndex Server Address: https://index.docker.io/v1/\nExecution Driver: vbox\nTotal Memory: 16.0 GB\nOperating System: Darwin Kernel Version 15.0.0\n. ",
    "resouer": "we can close this issue as #123 is on. @gnawux \n. Just checkout https://github.com/hyperhq/hyper/blob/master/.travis.yml \n. stats, labels will be sent after start\n. lgtm\n. Fixed in #361\n. lgtm\n. lgtm\n. I will add a CrashHandler here\n. @heartlock You should add integration test to self-prove your code. A good example to follow is: https://github.com/hyperhq/hyperd/pull/349\n. There's existing project to integrate containerd + runv, see: https://blog.hyper.sh/runv-bring-isolation-to-docker.html\n. Root cause found:\nhyperd will skip volume provision if vol.Detail is nil or not provided, but runV will report error[1] in this case:\nif existed[v.Volume] || v.Detail == nil {\n            continue\n        }\nAnd on frakti side, it happened passed a invalid Detail (though reason is not clear for now). This has been fixed in https://github.com/kubernetes/frakti/pull/60.\nAnyhow, overall hyperd + runv process is right. \n[1] SB[vm-zFZOVpwrLe] Con[xxx] volume /var/lib/kubelet/pods/xxx does not exist in volume map. cc @gnawux :/  PLTA. Confirmed: status in pod info returned by hyperd is PodIP:[]string{\"10.244.1.195/24\"}\nthe /24  is unexpected.. Close this as this is by design.. @m-barthelemy Are you using Kubernetes with hyperd my I ask? which version?. Should be Errorf, CI is also complaining this.\n. nil error, nil value. will this broke the caller? \n. @YaoZengzeng This test is not enough, you need to check the pod status becomes Running at least\n. Sure, will add new commits to fix this\n. Then should we also allow v.Detail == nil?. ",
    "laijs": "LGTM\n. LGTM\n. LGTM\n. code LGTM\n. code LGTM\n. it is better to (automatically) pull the image only after the server report the image is missing(404).\notherwise\n1) the pulling may be blocked or failed due to network\n2) the (existing) image is changed (the user expects that the image must be the one in his currently repository)\n. LGTM, merge\n. test good, merge\n. #92 \n. LGTM\n. review & test good\n. LGTM\n. LGTM\n. LGTM\n. What happen if the runv is in the different GOPATH?\n. LGTM\n. good\n. LGTM\ntest good. but the 9p cache in the vm need to be disabled.\n. LGTM\n. LGTM\n. LGTM\n. #263 did this. although some enhancements(#278) are required.\n. updated, merge it.\n. updated @carmark \n. it changed the api. a new design without changing any api will be ready. close it.\n. then you should also update the Godeps.json\n. pause, unpause are done.\n. LGTM\n. LGTM\n. good\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. thanks for report.\nthe new hyperd creates /var/lib/hyper/hosts/pod-BtkAFzMFqC/ via tmpfs and causes the problem. we will fix it later.\n. The hyperd always use vfs for hosts. But the recent hyperd uses tmpfs for dir of the hosts file, but the path is still /var/lib/hyper/hosts/${pod-id}/hosts. it doesn't affect the previous instances in theory.\n(we try hard to make api/store stable, but it is not guaranteed since it is still under unstable development stage, and the newest hyperd/hyperctl are always recommended)\ncloud you give me the result of ls -l /var/lib/hyper/hosts/pod-BtkAFzMFqC/ and ls -ld /var/lib/hyper/hosts/pod-BtkAFzMFqC/ please?\n. the hosts file is missing. it was created from hyperd via tmpfs. it a known bug, we will fix it later.\n. github issue is the bugtracker\n. @zenny If you are interested in the possible fix before merged, you can try/retest it with https://github.com/hyperhq/hyper/pull/259 \nThanks.\n. I don't know the script /etc/init.d/hyperd, could you direct launch it please?\nhyperd --nondaemon -v=3\n. change the config /etc/hyper/config\nhttps://github.com/hyperhq/hyper/blob/master/package/dist/etc/hyper/config#L3-L4\nbut hyper doesn't support zfs storage driver yet.\n. Is /mnt/tank/HYPER/ a fresh new (empty) dir when the config is being changed?\nWhat's the behavior of the same operations for the recent hyperd/hyperctl?\n. You can find why the hyperd exit from the log /var/log/hyper/hyperd.INFO /var/log/hyper/hyperd.ERROR.\nCould you post it please?\nThe Root directory should be empty (except the kernel, hyper-initrd etc) when the first time it is relocated.\nthe saved state/config for the created pod includes abspath.\n. fixed by https://github.com/hyperhq/hypercli/pull/9\n. LGTM\n. LGTM\n. It is Ok in my environment, the init process of the container prints messages every second.\n```\n[root@sbox ~]# hyperctl run -dt aptech/erpnext_dev\nPOD id is pod-pCkxIQiORl\nTime to run a POD is 1968 ms\n[root@sbox ~]# hyperctl attach pod-pCkxIQiORl\n2016-04-14 10:39:25,319 INFO success: cron entered RUNNING state, process has stayed up for > than 10 seconds (startsecs)\n2016-04-14 10:39:26,989 INFO exited: mysqld (exit status 0; expected)\n2016-04-14 10:39:28,001 INFO spawned: 'mysqld' with pid 691\n2016-04-14 10:39:29,005 INFO success: mysqld entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)\n2016-04-14 10:39:34,539 INFO exited: mysqld (exit status 0; expected)\n2016-04-14 10:39:35,560 INFO spawned: 'mysqld' with pid 1337\n2016-04-14 10:39:36,564 INFO success: mysqld entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)\n``\n. it attaches successfully.\nthe hyperstart might be lack of something required by the combination of container processsupervisord+mysqld. LGTM\n. it has not yet been implemented..... LGTM\n. any update? @gnawux \n. https://github.com/hyperhq/runv/pull/205\n. @gnawux is right.\nthere isCalling POST /v0.5.0/vm/createbeforeCalling POST /v0.5.0/pod/create`.\nIn @Crazykev 's case, there might be something error caused the qemu vm failed.\nthe qemu stderr is disabled. so we can't find the reason why it failed. cloud you try it again with libvirt?\n. rebase it please\n. when the container tty is disabled, the ctrl-c is sent to hyperctl itself only. when ctrl-c is received, the hyperctl detaches without stopping the container. we should keep hyperctl running in this case.\n. #383: when creating new containers in sandbox(pod)\nhere: when creating a new pod via hyperctl run without \"-p pod.json\"\n. Ok, you can do it, but I failed to set the \"Assignees\" in the right panel of the github.\nThanks for your contribution.\nhttps://github.com/hyperhq/hyperd/blob/master/client/run.go#L198\n. change the return type of CmdSystemInfo() to be types.InfoResponse for the new gRPC api.\nand convert it into engine.Env for old api in getInfo()\n// old api: server/router/system/system_routes.go:18\n``` go\nfunc (s systemRouter) getInfo(ctx context.Context, w http.ResponseWriter, r http.Request, vars map[string]string) error {\n        env, err := s.backend.CmdSystemInfo()\n        if err != nil {\n                return err\n        }\n    return env.WriteJSON(w, http.StatusOK)\n\n}\n``\n. could you gofmt the code please\n. fixed by #497. Hello, thanks for using it and the report.\nDo you still hyper-xen? Could you retry it with verbose logs please? ashyperd --nondaemon -v=3`\nThere is a hyper xen faq, it shows that the xen needs to be recompiled with virtfs(./configure --with-extra-qemuu-configure-args=\"--enable-virtfs\")\n. sorry, the xen needs to be recompiled with ./configure --with-extra-qemuu-configure-args=\"--enable-virtfs\"\n. LGTM\n. move examples/hyperd to https://github.com/hyperhq/official-images\n. @gnawux gflag.PassAfterNonOption is not added to hyperctl exec yet. duplicated with #353 \n. fixed by #497. it is better to move the sharedir out from vmdir.\nctx.Close() removes the vmdir\npod.Cleanup() removes the sharedir\nThis patch works now, I will merge it (except the VMs has no pod).\nBut could you create PRs to implement above comments?\nPS:\nIn the future, all the sharedir management is moved to runv. After that\nsharedir can be in the vmdir.\n. could you add the client code or the test code in a new commit in this pr?\n. need rebase. see #482 . ```\nI1221 05:21:54.559450   22372 server.go:152] Calling GET /v0.7.0/images/get\ngithub.com/hyperhq/hyperd/integration\n/home/travis/gopath/src/github.com/hyperhq/hyperd/integration/hyper_test.go:517: too many arguments in call to s.client.StartPod\nFAIL    github.com/hyperhq/hyperd/integration [build failed]\n!!! Error in /home/travis/gopath/src/github.com/hyperhq/hyperd/hack/lib/test.sh:129\n  'go test github.com/hyperhq/hyperd/integration -check.vv -v' exited with status 2\nCall stack:\n  1: /home/travis/gopath/src/github.com/hyperhq/hyperd/hack/lib/test.sh:129 hyper::test::integration(...)\n  2: hack/test-cmd.sh:161 runTests(...)\n  3: hack/test-cmd.sh:191 main(...)\nExiting with status 1\nI1221 05:21:59.463920   22372 daemon.go:345] The daemon will be shutdown\nI1221 05:21:59.464034   22372 daemon.go:346] Shutdown all VMs\n+++ [1221 05:21:59] Clean up complete\n```. close it, and reopen it later with the hope that it will be tested with the newest hyperd. could you also add the signal api for process(exec) please?. could you add the client code or the test code in a new commit in this pr?\n. the go-flags parses and eats some flags after the imagename, this patch just recover the eaten flags. we hope the solution make go-flags stop parsing at the imagename, or change go-flags to other flags-parsing package.\nWe are looking for a full solution, thanks.\n. > Was it changed intentionally or by accident?\nhyper had been changed to be the client command line of the hyper cloud since the hyper cloud was announced.  And the controlling tool for hyperd(hypercontainer) was renamed to hyperctl at the same time. sorry for the confusion.\n. put them on package/debian\n. all checks have passed\n. Hello, crook \nThanks for the report!\n\nAnd the following has been set in /etc/libvirt/qemu.conf\nuser = \"root\"\ngroup = \"root\"\nclear_emulator_capabilities = 0\n\nWhen default, its qemu driver, not libvirt.\nthe qemu log should be /var/log/hyper/qemu/vm-XCxHKgKGSa.log\ncould you show it if it is not empty please?\nthanks\n. quote:\n[root@localhost ~]# cat /var/log/hyper/qemu/vm-koLUAvjzuO.log\nqemu-system-x86_64: /builddir/build/BUILD/qemu-2.6.1/target-i386/kvm.c:1713: kvm_put_msrs: Assertion ret == n' failed.\n. I hope we can run hypercontainer(with kvm) on Vmware nested VT-x.\ncould you find a proper '-cpu argument' for it?\n. could you squash the commits into one please. \n. add --logtostderr to hyperd in the travis test.\n. did you type any command after$sudo hyperctl attach pod-hyiQRxiLTI?\nsuch asls, your pod was started without-t, it will print nothing when attached.\n. no, the pod is running inside a VM. it is extremely hard to share the same network or even netns between host  and vm. it requires some very fancy features to enable the sharing such as pvcall(http://marc.info/?l=xen-devel&m=147639616310487&w=2).\n. 1\uff09 use network\n2)  use shared filesystem\n3)  use the slow stdio,hyperctl exec podid cmd | outer-process`\n. you can use the network. (socket/bind/listen/accept with the vm ip & host-bridge-ip),\n. LGTM. retest this please. #471  merged. LGTM. LGTM. exec..... LGTM. updated...... the default hykins is not removed yet. verified. from above result, there is a strange thing in the Godeps/Godeps.json:\n{\n                        \"ImportPath\": \"github.com/coreos/etcd/Godeps/_workspace/src/github.com/ugorji/go/codec\",\n                        \"Comment\": \"v2.2.0\",\n                        \"Rev\": \"e4561dd8cfb1163fb51afceca9c78aa89398e731\"\n                },\n                {\n                        \"ImportPath\": \"github.com/coreos/etcd/Godeps/_workspace/src/golang.org/x/net/context\",\n                        \"Comment\": \"v2.2.0\",\n                        \"Rev\": \"e4561dd8cfb1163fb51afceca9c78aa89398e731\"\n                },. right, we need to update the github.com/coreos/etcd in our vendor to prevent it. finished.. it seems one of the pod (ID: \"busybox\") was failed to be removed without any notification, but the next creations of pods failed.. you could remove all pods at the end of every test. LGTM. it seams it is file-volume (rather than dir-volume). cc @bergwolf also. ```\nhyperctl info\nImages: 4\nContainers: 0\nPODs: 1\nStorage Driver: rawblock\n  Backing Filesystem: xfs\n  Support Copy-On-Write: false\n  Block Filesystem: xfs\n  Block Size: 10GB\n...\n. retest it please @hykins. retest this please @hykins. rawblock test passed. PR was updated: version.h check and travis test.. retest this please @hykins. it seems it is duplicated with #537. it was auto closed when #581  was merged. Reopen it until it is confirmed fixed.. LGTM. failed to open /etc/passwd: No such file or directory. failed on reloading @Crazykev  . golang.org/x/sys contains vsocks patch which is not in the official repo. @bergwolf . `ls` didn't show the file size zero or not,  could you use `ls -Lal` instead please?. LGTM. retest this please, @hykins. LGTM. LGTM. I'm going to merge it. could you create new issues if there any problem unsolved. @gnawux @Crazykev . @Crazykev could you also review it pls?.\n16:55:25 I0411 16:55:25.569547    6283 vm_console.go:46] SB[vm-aRbFtiKhxu] [CNL] uptime 3.64 0.35\n16:55:25 I0411 16:55:25.574477    6283 vm_console.go:46] SB[vm-aRbFtiKhxu] [CNL] \n16:55:25 I0411 16:55:25.578552    6283 vm_console.go:46] SB[vm-aRbFtiKhxu] [CNL] hyper ctl append type 9, len 0\n16:55:25 I0411 16:55:25.582866    6283 vm_console.go:46] SB[vm-aRbFtiKhxu] [CNL] hyper_handle_event event EPOLLOUT, he 0x61d648, fd 3, 0x61d4c0\n16:55:25 I0411 16:55:25.586796    6283 json.go:330] SB[vm-aRbFtiKhxu] readVmMessage code: 14, len: 4\n16:55:25 I0411 16:55:25.586816    6283 json.go:330] SB[vm-aRbFtiKhxu] readVmMessage code: 14, len: 4\n16:55:25 I0411 16:55:25.586843    6283 json.go:330] SB[vm-aRbFtiKhxu] readVmMessage code: 9, len: 0\n16:55:25 I0411 16:55:25.586852    6283 json.go:231] SB[vm-aRbFtiKhxu] got cmd:14\n16:55:25 I0411 16:55:25.586861    6283 json.go:252] SB[vm-aRbFtiKhxu] get command NEXT: send 98, receive 8\n16:55:25 I0411 16:55:25.586867    6283 json.go:231] SB[vm-aRbFtiKhxu] got cmd:14\n16:55:25 I0411 16:55:25.586873    6283 json.go:252] SB[vm-aRbFtiKhxu] get command NEXT: send 98, receive 98\n16:55:25 I0411 16:55:25.586886    6283 json.go:314] SB[vm-aRbFtiKhxu] write 24 to hyperstart.\n16:55:25 I0411 16:55:25.586903    6283 json.go:231] SB[vm-aRbFtiKhxu] got cmd:9\n16:55:25 I0411 16:55:25.586929    6283 vm_states.go:171] SB[vm-aRbFtiKhxu] pod start successfully\n16:55:25 I0411 16:55:25.587105    6283 provision.go:303] Pod[service] sandbox init result: &api.ResultBase{Id:\"vm-aRbFtiKhxu\", Success:true, ResultMessage:\"wait init message successfully\"}\n16:55:25 I0411 16:55:25.587233    6283 vm_console.go:46] SB[vm-aRbFtiKhxu] [CNL] hyper_modify_event modify event fd 3, 0x61d648, event 8193\n16:55:25 I0411 16:55:25.592838    6283 vm_console.go:46] SB[vm-aRbFtiKhxu] [CNL] pid 328 exit normally, status 0\n16:55:25 \u001b[37mDEBU\u001b[0m[0165] container mounted via layerStore: /var/lib/hyper/overlay/271d3b86367459d4ccefc2efe9e003af870f232f4fe24820700c179df0e6742b/merged \n16:55:25 I0411 16:55:25.603064    6283 vm_console.go:46] SB[vm-aRbFtiKhxu] [CNL] hyper_handle_event event EPOLLIN, he 0x61d648, fd 3, 0x61d4c0\n16:55:25 I0411 16:55:25.607829    6283 vm_console.go:46] SB[vm-aRbFtiKhxu] [CNL] hyper ctl append type 14, len 4\n16:55:25 I0411 16:55:25.608739    6283 container.go:504] Pod[service] Con[(service)] create container c855786521db2911cc5f2cd64065042083192271530af9404005d3f75d287049 (w/: [])\n16:55:25 I0411 16:55:25.608849    6283 container.go:521] Pod[service] Con[c855786521db(service)] container info config &container.Config{Hostname:\"c855786521db\", Domainname:\"\", User:\"\", AttachStdin:false, AttachStdout:false, AttachStderr:false, ExposedPorts:map[nat.Port]struct {}(nil), PublishService:\"\", Tty:false, OpenStdin:false, StdinOnce:false, Env:[]string{\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"}, Cmd:(strslice.StrSlice)(0xc420c50c40), ArgsEscaped:false, Image:\"busybox:latest\", Volumes:map[string]struct {}(nil), WorkingDir:\"\", Entrypoint:(strslice.StrSlice)(nil), NetworkDisabled:true, MacAddress:\"\", OnBuild:[]string(nil), Labels:map[string]string{}, StopSignal:\"\"}, Cmd [/bin/sh -c ps aux], Args [-c ps aux]\n16:55:25 I0411 16:55:25.608875    6283 container.go:526] Pod[service] Con[c855786521db(service)] describe container\n16:55:25 I0411 16:55:25.608911    6283 container.go:534] Pod[service] Con[c855786521db(service)] mount id: 271d3b86367459d4ccefc2efe9e003af870f232f4fe24820700c179df0e6742b\n16:55:25 I0411 16:55:25.608965    6283 container.go:614] Pod[service] Con[c855786521db(service)] Container Info is \n16:55:25 &api.ContainerDescription{Id:\"c855786521db2911cc5f2cd64065042083192271530af9404005d3f75d287049\", Name:\"/service\", Image:\"sha256:00f017a8c2a6e1fe2ffd05c281f27d069d2a99323a8cd514dd35f228ba26d2ff\", Labels:map[string]string(nil), Tty:true, StopSignal:\"TERM\", RootVolume:(api.VolumeDescription)(0xc420db1040), MountId:\"271d3b86367459d4ccefc2efe9e003af870f232f4fe24820700c179df0e6742b\", RootPath:\"rootfs\", UGI:(api.UserGroupInfo)(nil), Envs:map[string]string{\"PATH\":\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"}, Workdir:\"/\", Path:\"/bin/sh\", Args:[]string{\"-c\", \"ps aux\"}, Rlimits:[]api.Rlimit{}, Sysctl:map[string]string(nil), Volumes:map[string]api.VolumeReference(nil), Initialize:false}\n16:55:25 I0411 16:55:25.608995    6283 container.go:755] Pod[service] Con[c855786521db(service)] configure dns\n16:55:25 I0411 16:55:25.609014    6283 container.go:762] Pod[service] Con[c855786521db(service)] Already has DNS config, bypass DNS insert\n16:55:25 I0411 16:55:25.609263    6283 networks.go:38] Pod[service] Nic[eth-default] prepare inf info: &api.InterfaceDescription{Id:\"eth-default\", Lo:false, Bridge:\"hyper0\", Ip:\"192.168.123.20\", Mac:\"52:54:60:70:29:b7\", Gw:\"192.168.123.1\", TapName:\"\"}\n16:55:25 I0411 16:55:25.609304    6283 provision.go:436] Pod[service] adding resource to sandbox\n16:55:25 I0411 16:55:25.609415    6283 volume.go:241] Pod[service] Vol[etchosts-volume] transit volume from state 0 to 1, ok\n16:55:25 I0411 16:55:25.609439    6283 volume.go:161] Pod[service] Vol[etchosts-volume] mount volume\n16:55:25 I0411 16:55:25.609459    6283 volumes.go:29] trying to bind dir /var/lib/hyper/hosts/service/hosts to /var/run/hyper/vm-aRbFtiKhxu/share_dir/JbfSLyboAc\n16:55:25 I0411 16:55:25.609597    6283 mount.go:58] dir /var/lib/hyper/hosts/service/hosts is bound to JbfSLyboAc\n16:55:25 I0411 16:55:25.609616    6283 volume.go:107] Pod[service] Vol[etchosts-volume] insert volume to sandbox\n16:55:25 I0411 16:55:25.609637    6283 context.go:430] SB[vm-aRbFtiKhxu] return volume add success for dir/nas etchosts-volume\n16:55:25 I0411 16:55:25.609653    6283 volume.go:115] Pod[service] Vol[etchosts-volume] volume inserted\n16:55:25 I0411 16:55:25.609670    6283 volume.go:241] Pod[service] Vol[etchosts-volume] transit volume from state 1 to 2, ok\n16:55:25 I0411 16:55:25.609732    6283 network_linux.go:1197] parse IP addr 192.168.123.20\n16:55:25 I0411 16:55:25.612135    6283 qmp_wrapper_amd64.go:17] send net to qemu at 24\n16:55:25 I0411 16:55:25.612175    6283 qmp_handler.go:298] got new session\n16:55:25 I0411 16:55:25.612202    6283 qmp_handler.go:227] Begin process command session\n16:55:25 I0411 16:55:25.612226    6283 qmp_handler.go:240] send cmd with scm (24 bytes) (1) {\"execute\":\"getfd\",\"arguments\":{\"fdname\":\"fdeth0\"}}\n16:55:25 I0411 16:55:25.612264    6283 container.go:872] Pod[service] Con[c855786521db(service)] begin add to sandbox\n16:55:25 I0411 16:55:25.612283    6283 volume.go:187] Pod[service] Vol[etchosts-volume] subcribe volume insert\n16:55:25 I0411 16:55:25.612298    6283 volume.go:191] Pod[service] Vol[etchosts-volume] the subscribed volume has been inserted, need nothing.\n16:55:25 I0411 16:55:25.612545    6283 container.go:894] Pod[service] Con[c855786521db(service)] finished container prepare, wait for volumes\n16:55:25 I0411 16:55:25.613149    6283 container.go:903] Pod[service] Con[c855786521db(service)] resources ready, insert container to sandbox\n16:55:25 I0411 16:55:25.613197    6283 container.go:74] SB[vm-aRbFtiKhxu] Con[c855786521db2911cc5f2cd64065042083192271530af9404005d3f75d287049] volume (fs mapping) etchosts-volume is ready\n16:55:25 I0411 16:55:25.613215    6283 container.go:105] SB[vm-aRbFtiKhxu] Con[c855786521db2911cc5f2cd64065042083192271530af9404005d3f75d287049] all images and volume resources have been added to sandbox\n16:55:25 I0411 16:55:25.609381    6283 servicediscovery.go:201] Pod[service] [Serv] commit IPVS service patch: \n16:55:25 -A -t 10.254.0.24:2834 -s rr\n16:55:25 -a -t 10.254.0.24:2834 -r 192.168.23.2:2345 -m -w 1\n16:55:25 I0411 16:55:25.613297    6283 json.go:231] SB[vm-aRbFtiKhxu] got cmd:6\n16:55:25 I0411 16:55:25.613316    6283 json.go:263] SB[vm-aRbFtiKhxu] delay version-awared command :6\n16:55:25 I0411 16:55:25.615447    6283 json.go:231] SB[vm-aRbFtiKhxu] got cmd:6\n16:55:25 I0411 16:55:25.615475    6283 json.go:263] SB[vm-aRbFtiKhxu] delay version-awared command :6\n16:55:25 I0411 16:55:25.617522    6283 json.go:231] SB[vm-aRbFtiKhxu] got cmd:6\n16:55:25 I0411 16:55:25.617547    6283 json.go:263] SB[vm-aRbFtiKhxu] delay version-awared command :6\n16:55:25 I0411 16:55:25.619636    6283 json.go:231] SB[vm-aRbFtiKhxu] got cmd:6\n16:55:25 I0411 16:55:25.619660    6283 json.go:263] SB[vm-aRbFtiKhxu] delay version-awared command :6\n16:55:25 I0411 16:55:25.621747    6283 json.go:231] SB[vm-aRbFtiKhxu] got cmd:6\n16:55:25 I0411 16:55:25.621771    6283 json.go:263] SB[vm-aRbFtiKhxu] delay version-awared command :6\n``. Doesgodep update github.com/hyperhq/runv/...+rm -rf vendor/github.com/hyperhq/runv/vendor/works?. oops, Godeps/Godeps.json can't be fixed viarm -rf vendor/github.com/hyperhq/runv/vendor/`. LGTM. retest this please, @hykins. updated. updated.\nIt should be tested with 2.8 or newer qemu. (just use the official qemu).\nAnd daxblock branch of the hyperstart.\nAnd in /etc/hyper/config.\nHypervisor=qemu # or # Hypervisor=libvirt\nStorageDriver=rawblock\nStorageOptions=daxblock. LGTM. it seems that the ro-mount for btrfs driver is not included.. LGTM. All the dependent packages are in the vendor/ directory. It is strange that this directory was not being searched when you built the hyperd.. LGTM. LGTM\nI think it fixes some latent bugs at least, although there is no bug referred in the commit changelog.. How about https://docs.travis-ci.com/user/languages/go/#Go-Import-Path. The binary cmd/protoc-gen-gogo/protoc-gen-gogo doesn't need to be added and pushed.. I think it is better that protoc-gen-gogo is moved into hack/. devicemapper cannot work well in the travis-ci.org. LGTM. LGTM. LGTM, p.sandbox is protected by p.resourceLock!. please use the name SandboxNameLocked(). LGTM. \"reconfigure runv with\", it should be hyper.\n. it should not fail when there is no libvirt-dev.\n. --with-xen will be silent turn off when there is no xen-dev installed. I think it should complain and fail.\n--with-xen & xen-dev  => xen\n--with-xen & no xen-dev  =>  fail\n--without-xen & xen-dev  => no xen\n--without-xen & no xen-dev  => no xen\nxen-dev  => xen\nno xen-dev  => no xen\nthe same for libvirt\n. \"vbox\" hypervisor, \"vbox\" is one of the driver name\n. cmds = append(append([]string{}, cmds...), c.Command...)\n. I think we should provide /pod/[un]pause, since we do it under the hood.\nwe may provide \"/container/pause\" in the future when we can pause it individually\n. setup\n. I think only p.volumes[v.Name] = volInfo is Ok,  we don't need to copy it one field by field if the volume existed.\n. It copies and keep DockerVolume. ignore my comments.\n. did all the store drivers obey it?\n. map<string,string> env might be a choice\n. workdir\n. why it has both commands and  args\n. why it has both image and imageID\n. use  repeated string option instead\nthe userpod.json will also be updated.\n. repeated processinfo processes\n. options is complicated, A struct is overkill.\nsee also https://github.com/opencontainers/runtime-spec/blob/master/specs-go/config.go#L93-L94\n. move rpcServer := serverrpc.NewServerRPC(d) into if\nand defer rpcServer.Stop()\nor other way avoid calling NewServerRPC()\n. make([]*types.ImageInfo, 0, len(images))\n. why \"req == nil\" is needed?\n. how to make it happen\n. it is not RunvPodSpec. it is hyperhq's official pod spec: UserPod.\nits code is just wrongly put at runv. it will be moved back to hyperd.\nUserPod will not be removed, any attempt will be rejected by xu. it is already an inuse user interface. \n. pod.UserPod is already user interface, it will not be removed(it will be moved back to hyperd)\nconvertToRunvPodSpec should be convertToUserPod for respecting the name.\nI suggest to change apitypes.UserPod to apitypes.Pod\n. UserPod doesn't belong to runv, it is just wrongly put at runv. there is no \"runv's UserPod\".\npod.UserPod is user interface used by hyperctl run -p and persistent struct saved on database, it is almost pure hyperd api. but because the cleanup was not completed when the runv is separated from hyper, pod.UserPod is still inuse in runv. this history is also one of the reason why pod.UserPod should be removed from runv.\nrunv StartPod() needs an internal pod config for everything instead of pod.UserPod + additional config fragments. the internal pod config is much more changeable than any user api such as pod.UserPod. Apitypes.UserPod is also user api/interface, it will not be used on StartPod() even pod.UserPod is removed from runv. \n. apitypes.UserPod is pod config for network transport, it is user API/ABI. but it is not seen by user directly.\npod.UserPod is pod config which can be created manually by user for composing a pod.\nI don't think apitypes.UserPod can replace pod.UserPod. and \"UserPod\" in apitypes.UserPod is confusing. apitypes.Pod is much better, like all other packagename.Pod.\n. in hypervisor/pod.go, there is User          UserUser field here.\nwe will have to add User as UserUser user = 13 ?\n. which id will be  used as pod id? this podID or podSpec.Id?\n. it needs to be split into 2 lines.\notherwise $res may contain other garbarge\n. stop is nil channel here, you should create one via:\nstop := make(chan bool, 1)\n. use ReadString('\\n')\n. comparing to existing definition, bool    timestamps = 3; seems better.\nthere is one blank at least before = and one blank exactly after =.\n. tag is going to be removed\n. ExitCode is for process(container's init process or exec process). the tag will be replaced by process Id.\n. its type is int32 \n. int32\n. define a grpc struct\n. int32\n. uint64 ?\n. why there is an additional empty line here?\n. it seems better if there is a bool noHang = 3.\nit can be added later if it is needed.\n. Hypervisor=libvirt\n. p.vm.KillContainer(containerId, syscall.SIGTERM) hardly fail. but the process may continue to run.\nplease wait/check the container's status(we haven't had such event) in several seconds and then kill SIGKILL if needed.\nor directly kill SIGKILL.\n. ?\n. NewStdWriter(errStream, stdcopy.Stderr)\nor swap the order\n. status.ResourcePath should also be removed from its definition.\n. pl.names is not possible to be nil\n. it should be Containers rather than ctnStartInfo\n. it might be a different value when it is call from dameon.Restore() -> daemon.createPodInternal() -> ....\n. where are the test code to call it?\n. unknown?\n. hypercontainer.io\n. the package name is hypercontainer\n. we use only use Hyper Dev Team <dev@hyper.sh> in other places.\ngrep \"Dev Team\" . -rIn\n./hyperd/package/centos/rpm/SPECS/hyper-container.spec:56:* Mon Aug 29 2016 Hyper Dev Team <dev@hyper.sh> - 0.6.2-1\n./hyperd/package/centos/rpm/SPECS/hyper-container.spec:58:* Thu Apr 28 2016 Hyper Dev Team <dev@hyper.sh> - 0.6-1\n./hyperd/package/centos/rpm/SPECS/hyperstart.spec:41:* Mon Aug 29 2016 Hyper Dev Team <dev@hyper.sh> - 0.6.2-1\n./hyperd/package/centos/rpm/SPECS/hyperstart.spec:43:* Thu Apr 28 2016 Hyper Dev Team <dev@hyper.sh> - 0.6-1\n./hyperd/package/centos/rpm/SPECS/qemu-hyper.spec:42:* Fri Jan 29 2016 Hyper Dev Team <dev@hyper.sh> - 2.4.1-2\n./hyperd/package/fedora/rpm/SPECS/hyper-container.spec:55:* Mon Aug 29 2016 Hyper Dev Team <dev@hyper.sh> - 0.6.2-1\n./hyperd/package/fedora/rpm/SPECS/hyper-container.spec:57:* Wed May 25 2016 Hyper Dev Team <dev@hyper.sh> - 0.6-1\n./hyperd/package/fedora/rpm/SPECS/hyperstart.spec:41:* Mon Aug 29 2016 Hyper Dev Team <dev@hyper.sh> - 0.6.2-1\n./hyperd/package/fedora/rpm/SPECS/hyperstart.spec:43:* Wed May 25 2016 Hyper Dev Team <dev@hyper.sh> - 0.6-1\n. please set the srcName to be \"\" in this case\n. when vol.Source is \"\", vol.Driver should be \"\", the hyperd will use its default volume driver\n. it seams better that it also returns the pod.UserVolume.\n. please check the readOnly here.\n. it might need to be a name with order as suffix or a random name to avoid name collision.\n. in golang, \"\" is the initial/default value of a string. please remove this line.\n. readOnly = false.\nand set it to true in the first branch below when needed.\n. add volDriver string\n. vol := pod.UserVolume{\n        Driver : volDriver,\n        Source: srcName,\n      ....\n}\nvolRef needs to be like this also.\n. move volDriver = \"\" up in one of the branches.\n. I prefer to generate the volume name here. \nfmt.Sprintf(\"vol-%d\", i); i is the order of the volumes.\n. just pod.UserVolume is Ok, don't need to be pointer\n. the order doesn't matter, but it is the source for the uniq identification.\n. speed and resouce are unimportant here.\nAnd in golang, pointer for small object is slower and uses more resource, golang always allocate heap memory for pointers.\n. Good catch! it is empty. ignore my request.\n. just add a volName argument to the parseVolumes()\n. use path/filepath\n. how about moving it to else branch ?\n. here are two \"_, volName =\", the value depends on the \"if else\" branches, could you move it into the branch?\n. please convert the source path to abspath\n. and please check the destPath, ensure it starts with /\n. please do it in a new pr.\n. just convert it to abspath\n. could you please fix the indent?. removed . fixed. changed again. it seams better to move the code into v.umount(). it supports all host filesystem. only btrfs supports cow. xfs supports cow soon.. we need to add loopback related code when use mount.Mount(). using mount commandline is much simpler. The 'Put()' function forgot to detach the loopdev. it needs to be fixed.. the code was copied from other drivers. but it seems the volume file should be removed.. \"--reflink=auto\", it always tries with ioctl(BTRFS_IOC_CLONE) and will fallback to normal copy(read-write) on non-btrfs. so we need to use directly syscall for xfs.\nioctl(4, BTRFS_IOC_CLONE, 0x3)          = -1 ENOTTY (Inappropriate ioctl for device)\nfadvise64(3, 0, 0, POSIX_FADV_SEQUENTIAL) = 0\nread(3, \"...\"..., 65536) = 252\nwrite(4, \"...\"..., 252) = 252. rawblockDriver.blockFs can't be fetched here, we can use 'probefs()' like dm.. how about --oci ?. when Exec, attachID is also changed. diff\n +type writeCloser struct {\n +  io.Writer\n +  io.Closer\n +}. so there may be a security problem among these two mount operations. The early existing container can possible write to the roofs of this one.. ",
    "ghost": "Problem is in installer. I quickly add Linux Mint support to bash installer script.\ninstall.sh\n``` sh\n!/bin/bash\nDescription:  This script is used to install hyper cli and hyperd\nUsage:\ninstall from remote\nwget -qO- https://hyper.sh/install | bash\ncurl -sSL https://hyper.sh/install | bash\ninstall from local\n./bootstrap.sh\nBASE_DIR=$(cd \"$(dirname \"$0\")\"; pwd); cd ${BASE_DIR}\nDEV_MODE=\"\"; SLEEP_SEC=10; SUPPORT_XEN=\"\";\nif [ $# -eq 1 -a \"$1\" == \"--dev\" ];then\n  DEV_MODE=\"-dev\"; SLEEP_SEC=3; echo \"[test mode]\"\nfi\nset -e\n#### Variable\nCURRENT_USER=\"$(id -un 2>/dev/null || true)\"\nBOOTSTRAP_DIR=\"/tmp/hyper-bootstrap-${CURRENT_USER}\"\n#### Parameter\nS3_URL=\"http://hyper-install${DEV_MODE}.s3.amazonaws.com\"\nPKG_FILE=\"hyper-latest${DEV_MODE}.tgz\"\nUNTAR_DIR=\"hyper-pkg\"\nSUPPORT_EMAIL=\"support@hyper.sh\"\n#### Constant\nSUPPORT_DISTRO=(debian ubuntu fedora centos linuxmint)\nLINUX_MINT_CODE=(rafaela rebecca qiana)\nUBUNTU_CODE=(trusty utopic vivid)\nDEBIAN_CODE=(jessie wheezy)\nCENTOS_VER=(6 7)\nFEDORA_VER=(20 21 22)\nColor Constant\nRED=tput setaf 1\nGREEN=tput setaf 2\nYELLOW=tput setaf 3\nBLUE=tput setaf 4\nWHITE=tput setaf 7\nLIGHT=tput bold\nRESET=tput sgr0\nError Message\nERR_ROOT_PRIVILEGE_REQUIRED=(10 \"This install script need root privilege, please retry use 'sudo' or root user!\")\nERR_NOT_SUPPORT_PLATFORM=(20 \"Sorry, Hyper only support x86_64 platform!\")\nERR_NOT_SUPPORT_DISTRO=(21 \"Sorry, Hyper only support ubuntu/debian/fedora/centos/linuxmint(17.x) now!\")\nERR_NOT_SUPPORT_DISTRO_VERSION=(22)\nERR_DOCKER_NOT_INSTALL=(30 \"Please install docker 1.5+ first!\")\nERR_DOCKER_LOW_VERSION=(31 \"Need Docker version 1.5 at least!\")\nERR_DOCKER_NOT_RUNNING=(32 \"Docker daemon isn't running!\")\nERR_DOCKER_GET_VER_FAILED=(33 \"Can not get docker version!\")\nERR_DOCKER_UNSUPPORTED_STORE_DRV=(34 \"Only docker storage driver 'aufs' and 'devicemapper' are supported!\")\nERR_QEMU_NOT_INSTALL=(40 \"Please install Qemu 2.0+ first!\")\nERR_QEMU_LOW_VERSION=(41 \"Need Qemu version 2.0 at least!\")\nERR_XEN_NOT_INSTALL=(50 \"Please install xen 4.5+ first!\")\nERR_XEN_GET_VER_FAILED=(51 \"Can not get xen version, xen daemon isn't running!\")\nERR_XEN_VER_LOW=(52 \"Sorry, hyper only support xen 4.5+\")\nERR_FETCH_INST_PKG_FAILED=(60 \"Fetch install package failed, please retry!\")\nERR_INST_PKG_MD5_ERROR=(61 \"Checksum of install package error, please retry!\")\nERR_UNTAR_PKG_FAILED=(62 \"Untar install package failed!\")\nERR_EXEC_INSTALL_FAILED=(70 \"Install hyper failed!\")\nERR_INSTALL_SERVICE_FAILED=(71 \"Install hyperd as service failed!\")\nERR_HYPER_NOT_FOUND=(72 \"Can not find hyper and hyperd after setup!\")\nERR_UNKNOWN_MSG_TYPE=98\nERR_UNKNOWN=99\n#### Function Definition\nmain() {\n  check_user\n  check_deps\n  check_hyper_before_install\n  if [[ -f install.sh ]] && [[ -d bin ]] && [[ -d boot ]] && [[ -d service ]];then\n    show_message debug \"Install from local ${BASE_DIR}/\"\n    BOOTSTRAP_DIR=\"${BASE_DIR}\"\n  else\n    show_message debug \"Install from remote\"\n    fetch_hyper_package\n  fi\n  stop_running_hyperd\n  install_hyper\n  start_hyperd_service\n  exit 0\n}\ncheck_hyper_before_install() {\n  if (command_exist hyper hyperd);then\n    echo \"${WHITE}\"\n    cat </dev/null);then\n        show_message error \"Hyper support ${LSB_DISTRO}( ${SUPPORT_CODE_LIST} ), but current is ${LSB_CODE}(${LSB_VER})\"\n        exit ${ERR_NOT_SUPPORT_DISTRO_VERSION[0]}\n      fi\n    ;;\n    ubuntu|debian)\n      if [ \"${LSB_DISTRO}\" == \"ubuntu\" ]\n      then SUPPORT_CODE_LIST=\"${UBUNTU_CODE[@]}\";\n      else SUPPORT_CODE_LIST=\"${DEBIAN_CODE[@]}\";\n      fi\n      if (echo \"${SUPPORT_CODE_LIST}\" | grep -v -w \"${LSB_CODE}\" &>/dev/null);then\n        show_message error \"Hyper support ${LSB_DISTRO}( ${SUPPORT_CODE_LIST} ), but current is ${LSB_CODE}(${LSB_VER})\"\n        exit ${ERR_NOT_SUPPORT_DISTRO_VERSION[0]}\n      fi\n    ;;\n    centos|fedora)\n      CMAJOR=$( echo ${LSB_VER} | cut -d\".\" -f1 )\n      if [  \"${LSB_DISTRO}\" == \"centos\" ]\n      then SUPPORT_VER_LIST=\"${CENTOS_VER[@]}\";\n      else SUPPORT_VER_LIST=\"${FEDORA_VER[@]}\";\n      fi\n      if (echo \"${SUPPORT_VER_LIST}\" | grep -v -w \"${CMAJOR}\" &>/dev/null);then\n        show_message error \"Hyper support ${LSB_DISTRO}( ${SUPPORT_VER_LIST} ), but current is ${LSB_VER}\"\n        exit ${ERR_NOT_SUPPORT_DISTRO_VERSION[0]}\n      fi\n    ;;\n    ) if [ ! -z ${LSB_DISTRO} ];then echo -e -n \"\\nCurrent OS is '${LSB_DISTRO} ${LSB_VER}(${LSB_CODE})'\";\n       else echo -e -n \"\\nCan not detect OS type\"; fi\n      show_message error \"${ERR_NOT_SUPPORT_DISTRO[1]}\"\n      exit ${ERR_NOT_SUPPORT_DISTRO[0]}\n    ;;\n  esac\n  echo -n \".\"\n}\ncheck_deps_docker() { #docker 1.5+ should be installed and running\n  if (command_exist docker);then\n    set +e\n    ${BASH_C} \"docker version > /dev/null 2>&1\"\n    if [ $? -ne 0 ];then\n      show_message error \"${ERR_DOCKER_NOT_RUNNING[1]}\\n\"\n      cat </dev/null | sed -ne 's/Server version:[[:space:]]([0-9]{1,})/\\1/p')\n    set -e\n    read DMAJOR DMINOR DFIX < <( echo ${DOCKER_VER} | awk -F\".\" '{print $1,$2,$3}')\n    if [ -z ${DMAJOR} -o -z ${DMINOR} ];then\n      show_message error \"${ERR_DOCKER_GET_VER_FAILED[1]}\"\n      display_support ${ERR_DOCKER_GET_VER_FAILED[0]}\n      exit ${ERR_DOCKER_GET_VER_FAILED[0]}\n    fi\n    if [ ${DMAJOR} -lt 1 ] || [ ${DMAJOR} -eq 1 -a ${DMINOR} -lt 5 ];then\n      show_message error \"${ERR_DOCKER_LOW_VERSION[1]} but current is ${DMAJOR}.${DMINOR}, please upgrade docker first!\"\n      exit ${ERR_DOCKER_LOW_VERSION[0]]}\n    fi\n    STORE_DRV=$(${BASH_C} \"docker info 2>/dev/null |sed -ne 's/Storage Driver: (.)/\\1/p'\")\n    if [[ $STORE_DRV != \"aufs\" ]] && [[ $STORE_DRV != \"devicemapper\" ]]; then\n        show_message error \"${ERR_DOCKER_UNSUPPORTED_STORE_DRV[1]}\\n\" && exit ${ERR_DOCKER_UNSUPPORTED_STORE_DRV[0]}\n    fi\n  else\n    show_message error \"${ERR_DOCKER_NOT_INSTALL[1]}\"\n    if [ \"${LSB_DISTRO}\" == \"ubuntu\" ];then\n      _OS=\"linux\"\n    fi\n    echo -e \"\\nInstructions for installing Docker on ${LSB_DISTRO}${_OS}\"\n    echo -e \"    https://docs.docker.com/installation/${LSB_DISTRO}${_OS}/\\n\"\n    exit ${ERR_DOCKER_NOT_INSTALL[0]}\n  fi\n  echo -n \".\"\n}\ncheck_deps_xen() {\n  set +e\n  ${BASH_C} \"which xl\" >/dev/null 2>&1\n  if [ $? -ne 0 ];then\n    show_message error \"${ERR_XEN_NOT_INSTALL[1]}\"\n    exit ${ERR_XEN_NOT_INSTALL[0]}\n  else\n    ${BASH_C} \"xl info\" >/dev/null 2>&1\n    if [ $? -eq 0 ];then\n      XEN_MAJOR=$( ${BASH_C} \"xl info\" | grep xen_major | awk '{print $3}' )\n      XEN_MINOR=$( ${BASH_C} \"xl info\" | grep xen_minor | awk '{print $3}' )\n      XEN_VERSION=$( ${BASH_C} \"xl info\" | grep xen_version | awk '{print $3}' )\n      show_message debug \"xen(${XEN_VERSION}) found\"\n      if [[ $XEN_MAJOR -ge 4 ]] && [[ $XEN_MINOR -ge 5 ]];then\n        PKG_FILE=\"hyper${SUPPORT_XEN}-latest${DEV_MODE}.tgz\"\n        UNTAR_DIR=\"hyper-pkg${SUPPORT_XEN}\"\n      else\n        show_message error \"${ERR_XEN_VER_LOW[1]}\"\n        exit ${ERR_XEN_VER_LOW[0]}\n      fi\n    else\n        show_message error \"${ERR_XEN_GET_VER_FAILED[1]}\"\n        exit ${ERR_XEN_GET_VER_FAILED[0]}\n    fi\n  fi\n  set -e\n}\ncheck_deps_qemu() { #QEMU 2.0+ should be installed\n  if (command_exist qemu-system-x86_64);then\n    local QEMU_VER=$(qemu-system-x86_64 --version | awk '{print $4}' | cut -d\",\" -f1)\n    read QMAJOR QMINOR QFIX < <( echo ${QEMU_VER} | awk -F'.' '{print $1,$2,$3 }')\n    if [ ${QMAJOR} -lt 2 ] ;then\n      show_message error \"${ERR_QEMU_LOW_VERSION[1]}\\n\" && exit ${ERR_QEMU_LOW_VERSION[0]}\n    fi\n  else\n    show_message error \"${ERR_QEMU_NOT_INSTALL[1]}\\n\" && exit ${ERR_QEMU_NOT_INSTALL[0]}\n  fi\n  echo -n \".\"\n}\ncheck_deps_initsystem() {\n  if [ \"${LSB_DISTRO}\" == \"ubuntu\" -a \"${LSB_CODE}\" == \"utopic\" ];then\n    INIT_SYSTEM=\"sysvinit\"\n  elif (command_exist systemctl);then\n    INIT_SYSTEM=\"systemd\"\n  else\n    INIT_SYSTEM=\"sysvinit\"\n  fi\n  echo -n \".\"\n}\nfetch_hyper_package() {\n  show_message info \"Fetch checksum and package...\\n\"\n  set +e\n  ${BASH_C} \"ping -c 3 -W 2 hyper-install${DEV_MODE}.s3.amazonaws.com >/dev/null 2>&1\"\n  if [ $? -ne 0 ];then\n    S3_URL=\"http://mirror-hyper-install${DEV_MODE}.s3.amazonaws.com\"\n  else\n    S3_URL=\"http://hyper-install${DEV_MODE}.s3.amazonaws.com\"\n  fi\n  local SRC_URL=\"${S3_URL}/${PKG_FILE}\"\n  local TGT_FILE=\"${BOOTSTRAP_DIR}/${PKG_FILE}\"\n  local USE_WGET=$( echo $(get_curl) | awk -F\"|\" '{print $1}' )\n  local CURL_C=$( echo $(get_curl) | awk -F\"|\" '{print $2}' )\n  show_message debug \"${SRC_URL} => ${TGT_FILE}\"\n  mkdir -p ${BOOTSTRAP_DIR} && cd ${BOOTSTRAP_DIR}\n  if [ -s ${TGT_FILE} ];then\n    if [ \"${USE_WGET}\" == \"true\" -a \"${DEV_MODE}\" == \"\" ];then\n      ${CURL_C} ${SRC_URL}.md5 2>&1 | grep --line-buffered \"%\" | sed -u -e \"s,.,,g\" | awk '{printf(\"\\b\\b\\b\\b%4s\", $2)}'\n    else\n      ${CURL_C} ${TGT_FILE}.md5 ${SRC_URL}.md5\n    fi\n    if [ -s \"${TGT_FILE}.md5\" ];then\n        NEW_MD5=$( cat ${TGT_FILE}.md5 | awk '{print $1}' )\n        OLD_MD5=$( md5sum ${TGT_FILE} | awk '{print $1}' )\n        if [[ ! -z ${OLD_MD5} ]] && [[ ! -z ${NEW_MD5} ]] && [[ \"${OLD_MD5}\" != \"${NEW_MD5}\" ]];then\n          show_message info \"${LIGHT}Found new hyper version, will download it now!\\n\"\n          ${BASH_C} \"\\rm  -rf ${BOOTSTRAP_DIR}/\"\n        elif [ ! -z ${OLD_MD5} -a \"${OLD_MD5}\" == \"${NEW_MD5}\" ];then #no update\n          ${BASH_C} \"\\rm  -rf ${BOOTSTRAP_DIR}/${UNTAR_DIR}\"\n        else\n          ${BASH_C} \"\\rm -rf ${BOOTSTRAP_DIR}/\"\n        fi\n    fi\n  elif [ -f ${TGT_FILE} ];then\n    ${BASH_C} \"\\rm -rf ${BOOTSTRAP_DIR}/*\"\n  fi\n  if [ ! -f ${TGT_FILE} ];then\n    \\rm -rf ${TGT_FILE}.md5 >/dev/null 2>&1\n    if [ \"${USE_WGET}\" == \"true\" -a \"${DEV_MODE}\" == \"\" ];then\n      ${CURL_C} ${SRC_URL}.md5 2>&1 | grep --line-buffered \"%\" | sed -u -e \"s,.,,g\" | awk '{printf(\"\\b\\b\\b\\b%4s\", $2)}'\n      ${CURL_C} ${SRC_URL} 2>&1 | grep --line-buffered \"%\" | sed -u -e \"s,.,,g\" | awk '{printf(\"\\b\\b\\b\\b%4s\", $2)}'\n    else\n      ${CURL_C} ${TGT_FILE}.md5 ${SRC_URL}.md5\n      ${CURL_C} ${TGT_FILE} ${SRC_URL}\n    fi\n    if [ $? -ne 0 ];then\n      show_message error \"${ERR_FETCH_INST_PKG_FAILED[1]}\" && exit \"${ERR_FETCH_INST_PKG_FAILED[0]}\"\n    else\n      MD5_REMOTE=$(cat ${TGT_FILE}.md5 | awk '{print $1}'); MD5_LOCAL=$(md5sum ${TGT_FILE} | awk '{print $1}')\n      if [ ${MD5_REMOTE} != ${MD5_LOCAL} ];then\n        echo \"required checksum: ${MD5_REMOTE}, but downloaded package is ${MD5_LOCAL}\"\n        show_message error \"${ERR_INST_PKG_MD5_ERROR[1]}\" && exit \"${ERR_INST_PKG_MD5_ERROR[0]}\"\n      fi\n    fi\n  fi\n  ${BASH_C} \"cd ${BOOTSTRAP_DIR} && tar xzf ${PKG_FILE}\"\n  if [ $? -ne 0 ];then\n    show_message error \"${ERR_UNTAR_PKG_FAILED[1]}\" && exit \"${ERR_UNTAR_PKG_FAILED[0]}\"\n  fi\n  BOOTSTRAP_DIR=\"${BOOTSTRAP_DIR}/${UNTAR_DIR}\"\n  show_message done \" Done\"\n  set -e\n}\ninstall_hyper() {\n  show_message info \"Installing \"\n  set +e\n  cd ${BOOTSTRAP_DIR}\n  ${BASH_C} \"./install.sh\" 1>/dev/null\n  if [ $? -ne 0 ];then\n    show_message error \"${ERR_EXEC_INSTALL_FAILED[1]}\" && exit \"${ERR_EXEC_INSTALL_FAILED[0]}\"\n  fi\n  echo -n \".\"\n  if [[ -f /usr/local/bin/hyper ]] && [[ -f /usr/local/bin/hyperd ]] && [[ ! -f /usr/bin/hyper ]] && [[ ! -f /usr/bin/hyperd ]] ;then\n    ${BASH_C} \"ln -s /usr/local/bin/hyper /usr/bin/hyper\"\n    ${BASH_C} \"ln -s /usr/local/bin/hyperd /usr/bin/hyperd\"\n  fi\n  if (command_exist hyper hyperd);then\n    install_hyperd_service\n    echo -n \".\"\n  else\n    show_message error \"${ERR_HYPER_NOT_FOUND[1]}\"\n    display_support ${ERR_HYPER_NOT_FOUND[0]}\n    exit ${ERR_HYPER_NOT_FOUND[0]}\n  fi\n  set -e\n  show_message done \" Done\"\n}\ninstall_hyperd_service() {\n  local SRC_INIT_FILE=\"\"\n  local TGT_INIT_FILE=\"\"\n  if [ \"${INIT_SYSTEM}\" == \"sysvinit\" ];then\n    if [ \"${LSB_DISTRO}\" == \"debian\" -a \"${LSB_CODE}\" == \"wheezy\" ];\n    then\n      SRC_INIT_FILE=\"${BOOTSTRAP_DIR}/service/init.d/hyperd.ubuntu\"\n    else\n      SRC_INIT_FILE=\"${BOOTSTRAP_DIR}/service/init.d/hyperd.${LSB_DISTRO}\"\n    fi\n    TGT_INIT_FILE=\"/etc/init.d/hyperd\"\n  elif [ \"${INIT_SYSTEM}\" == \"systemd\" ];then\n    SRC_INIT_FILE=\"${BOOTSTRAP_DIR}/service/systemd/hyperd.service\"\n    TGT_INIT_FILE=\"/lib/systemd/system/hyperd.service\"\n  fi\n  if [ -s ${SRC_INIT_FILE} ];then\n    ${BASH_C} \"cp ${SRC_INIT_FILE} ${TGT_INIT_FILE}\"\n    ${BASH_C} \"chmod +x ${TGT_INIT_FILE}\"\n  else\n    show_message error \"${ERR_INSTALL_SERVICE_FAILED[1]}\"\n    display_support ${ERR_INSTALL_SERVICE_FAILED[1]}\n    exit ${ERR_INSTALL_SERVICE_FAILED[0]}\n  fi\n}\nstop_running_hyperd() {\n  set +e\n  pgrep hyperd >/dev/null 2>&1\n  if [ $? -eq 0 ];then\n    echo -e \"\\nStopping running hyperd service before install\"\n    if [ \"${INIT_SYSTEM}\" == \"systemd\" ]\n    then ${BASH_C} \"systemctl stop hyperd\"\n    else ${BASH_C} \"service hyperd stop\";\n    fi\n    sleep 3\n  fi\n  set -e\n}\nstart_hyperd_service() {\n  show_message info \"Start hyperd service\\n\"\n  if [ \"${INIT_SYSTEM}\" == \"systemd\" ]\n  then ${BASH_C} \"systemctl start hyperd\"\n  else ${BASH_C} \"service hyperd start\";\n  fi\n  sleep 3\n  set +e\n  pgrep hyperd >/dev/null 2>&1\n  if [ $? -eq 0 ];then\n    if [ \"${SUPPORT_XEN}\" == \"-xen\" ];then\n      show_message success \"\\nhyperd for xen is running.\"\n    else\n      show_message success \"\\nhyperd for kvm/qemu is running.\"\n    fi\n    cat <<COMMENT\n\nTo see how to use hyper cli:\n  sudo hyper help\nTo manage hyperd service:\n  sudo service hyperd {start|stop|restart|status}\nTo get more information:\n  http://hyper.sh\nCOMMENT\n  else\n    show_message warn \"\\nhyperd isn't running.\"\n    cat < /dev/null 2>&1\n}\nget_curl() {\n  CURL_C=\"\"; USE_WGET=\"false\"\n  if (command_exist curl);then\n    if [ \"${DEV_MODE}\" != \"\" ];then CURL_C='curl -SL -o '; else CURL_C='curl -O --progress-bar -o '; fi\n  elif (command_exist wget);then\n    USE_WGET=\"true\"\n    if [ \"${DEV_MODE}\" != \"\" ];then CURL_C='wget -O '; else CURL_C='wget --progress=dot '; fi\n  fi\n  echo \"${USE_WGET}|${CURL_C}\"\n}\nshow_message() {\n  case \"$1\" in\n    debug)  if [ \"${DEV_MODE}\" == \"-dev\" ];then echo -e \"\\n[${BLUE}DEBUG${RESET}] : $2\"; fi;;\n    info)   echo -e -n \"\\n${WHITE}$2${RESET}\" ;;\n    warn)   echo -e    \"\\n[${YELLOW}WARN${RESET}] : $2\" ;;\n    done|success) echo -e \"${LIGHT}${GREEN}$2${RESET}\" ;;\n    error|failed) echo -e \"\\n[${RED}ERROR${RESET}] : $2\" ;;\n  esac\n}\n\nmain\n```\nNow it give an error that \"Can not get docker version!\"\nHow to fix it?\nP.S. installer code is not in this repo, right?\n. @laijs I know this is an old thread but I am new and get the same error when I run hyperctl info:\nhyperctl ERROR: An error occurred trying to connect: Get http://%2Fvar%2Frun%2Fhyper.sock/v0.8.1/list?item=pod: dial unix /var/run/hyper.sock: connect: no such file or directory\nand here is the log as you suggested:\nLog file created at: 2017/06/24 20:57:49\nRunning on machine: localhost\nBinary: Built with gc go1.8.3 for linux/amd64\nLog line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\nI0624 20:57:49.122851   18247 config.go:38] config file: %!(EXTRA string=/etc/hyper/config)\nI0624 20:57:49.123234   18247 config.go:72] [/etc/hyper/config] config items: &types.HyperConfig{ConfigFile:\"/etc/hyper/config\", Root:\"/var/lib/hyper\", Host:\"\", GRPCHost:\"\", StorageDriver:\"\", VmFactoryPolicy:\"\", Driver:\"qemu\", Kernel:\"/var/lib/hyper/kernel\", Initrd:\"/var/lib/hyper/hyper-initrd.img\", Bridge:\"\", BridgeIP:\"\", DisableIptables:false, EnableVsock:true, DefaultLog:\"\", DefaultLogOpt:map[string]string{}, logPrefix:\"[/etc/hyper/config] \"}\nI0624 20:57:55.170355   18247 daemon.go:215] The hypervisor's driver is qemu\nI0624 20:57:56.299793   18247 migration.go:23] Migrate lagecy persistent pod data, found: 0, migrated: 0\nI0624 20:57:56.310088   18247 hyperd.go:189] Hyper daemon: 0.8.1 v0.4.0-912-g7a07c71\nI0624 20:58:37.811799   18247 daemon.go:357] The daemon will be shutdown\nI0624 20:58:37.811826   18247 daemon.go:358] Shutdown all VMs\n. No it is the OS\n. Actually I had to compile from source because I was unable to install using the bash script.. I am using Linux korora 25 but It's just Fedora 25 under the hood.. I mean:\nuname -a\nLinux localhost.localdomain 4.11.5-200.fc25.x86_64 #1 SMP Wed Jun 14 17:17:29 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nOr is it a problem when using Fedora beacuse I also tried to install hyper on machine running project atomic/fedora25 but It complained saying it only supports Fedora 23..but i believe it is mentioned on github that fedora 25 is supported..\n. Thanks a lot @gnawux . It's now working!!\nHere are the issues and steps I took in case someone else gets the same problem..\n1. cd /etc/hyper  and uncomment  the Hypervisor line and assign a value accordingly(libvirt,kvm,qemu)\n2. enable hyper daemon with systemctl enable hyperd\n3. start hyper daemon with systemctl start hyperd\nN.B: step 2 and 3 only apply if you are using systemd, for example ubuntu 14  is still on init so adjust accordingly..\n4. systemctl status hyperd if you get running and enabled you are good to go\n5. sudo hyperctl list works!!\n. Yeah, I see..well I guess you can close this one. next stop Hypernetes!!. ",
    "nuxlli": ":+1:\n. ",
    "jstoja": "@feiskyer What command did you run to have this message?\n. What's your version of hyper ? (hyper version)\nHave you tried updating ?\n. ",
    "tuxknight": "Same problem \n[root@registry hyper]# hyper run ubuntu:latest\nhyper ERROR: Error from daemon's response: mkdir /var/lib/hyper/overlay/a7f45b97b668146329ddc343a121817a0e73916f2c413eeb8efe0c6b1c182a9a-init/merged/dev/shm: invalid argument\n[root@registry hyper]# hyper info\nImages: 4\nContainers: 0\nPODs: 0\nStorage Driver: overlay\n  Backing Filesystem: xfs\nHyper Root Dir: /var/lib/hyper\nIndex Server Address: https://index.docker.io/v1/\nExecution Driver: kvm\nTotal Memory: 3.7 MB\nOperating System: CentOS Linux 7 (Core)\n$ hyper version \nThe hyper version is 0.4.0\n. ",
    "huikang": "hi, @gnawux I am using centos7 too and tried add StorageDriver=devicemapper in /etc/hyper/config\nernel=/var/lib/hyper/kernel\nInitrd=/var/lib/hyper/hyper-initrd.img\nBios=/var/lib/hyper/bios-qboot.bin\nCbfs=/var/lib/hyper/cbfs-qboot.rom\nStorageDriver=devicemapper\nHowever, after that, I can not start hyperd. Anything wrong with my config? Thanks.\n. it works. thanks, @gnawux \n. ",
    "xiaods": "@Jimmy-Xu the report there is no insecure-registry parameter. \n. @carmark done.\n. @gnawux my point is remove \"docker pull\" in README or docs, only introduce \"hyper pull\". hyper need hyper way, don't need docker even it support docker cli.\n. hi team, we plan to try implement it in Saturday's Docker Global Hackathon day. are you interesting it?\n. @xlgao-zju yeah, i don't think hyperd need care you ipam. please carefully design your policy to manage the ip pool. etcd is good options.\n. currently the virtualbox is not working now\n. i don't think swarm is good match for hyper.  it should be alternative solution is build a swarmkit on hyperd.\n. why?\n. PR is welcome\n. install bash is up to date\n. i am not sure upstream want to upgrade compiler to go 1.7.\ngo build -tags \"static_build   libdm_no_deferred_remove exclude_graphdriver_btrfs\" -ldflags \"-X github.com/hyperhq/hyperd/utils.VERSION 0.6.2\"\nthe -X key/value should be update to key=value. it can fix the conflict\n. deep more info:\nless /var/log/hyper/hyperd.WARNING\nLog file created at: 2016/10/17 04:48:04\nRunning on machine: hadoop32\nBinary: Built with gc go1.5.1 for linux/amd64\nLog line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\nE1017 04:48:04.814880   18769 qemu_process.go:146] qemu-system-x86_64: -drive if=pflash,file=/var/lib/hyper/bios-qboot.bin,readonly=on: Could not open '/var/lib/hyper/bios-qboot.bin': No such file or directory\nE1017 04:48:04.815547   18769 qemu_process.go:150] exit status 1\nE1017 04:48:04.815602   18769 vm_states.go:424] VM start failed: try to start qemu failed, go to cleaning up\nW1017 04:48:14.799076   18769 qmp_handler.go:282] Initializer Timeout.\nE1017 04:48:14.799221   18769 qmp_handler.go:368] QMP initialize timeout\nE1017 04:48:14.941753   18769 qmp_handler.go:162] failed to connected to /var/run/hyper/vm-zSOEGSYCMH/qmp.sock dial unix /var/run/hyper/vm-zSOEGSYCMH/qmp.sock: connect: connection refused\nE1017 04:48:14.941796   18769 tty.go:139] Cannot connect to tty socket dial unix /var/run/hyper/vm-zSOEGSYCMH/tty.sock: connect: connection refused\nE1017 04:48:14.941768   18769 init_comm.go:126] Cannot connect to hyper socket dial unix /var/run/hyper/vm-zSOEGSYCMH/hyper.sock: connect: connection refused\nE1017 04:48:14.941855   18769 qmp_handler.go:361] QMP initialize failed\n. i just install hyper with curl install way with centos7 clean environment:\ncurl -sSL http://hypercontainer.io/install | bash\n. journalctl -u hyperd\n10\u6708 17 05:21:31 hadoop29 systemd[1]: Starting hyperd...\n10\u6708 17 05:21:31 hadoop29 hyperd[28827]: I1017 05:21:31.953395   28827 daemon.go:141] The config: kernel=/var/lib/hyper/kernel, initrd=/var/lib/hyper/hyper-initrd.img\n10\u6708 17 05:21:31 hadoop29 hyperd[28827]: I1017 05:21:31.953739   28827 daemon.go:143] The config: vbox image=\n10\u6708 17 05:21:31 hadoop29 hyperd[28827]: I1017 05:21:31.953753   28827 daemon.go:146] The config: bridge=, ip=\n10\u6708 17 05:21:31 hadoop29 hyperd[28827]: I1017 05:21:31.953763   28827 daemon.go:149] The config: bios=/var/lib/hyper/bios-qboot.bin, cbfs=/var/lib/hyper/cbfs-qboot.rom\n10\u6708 17 05:21:31 hadoop29 hyperd[28827]: E1017 05:21:31.953834   28827 daemondb.go:23] open leveldb file failed, resource temporarily unavailable\n10\u6708 17 05:21:31 hadoop29 hyperd[28827]: E1017 05:21:31.954083   28827 hyperd.go:143] The hyperd create failed, resource temporarily unavailable\nmissing   bios=/var/lib/hyper/bios-qboot.bin, cbfs=/var/lib/hyper/cbfs-qboot.rom\n. [dsxiao@hadoop29 hyper]$ hyperctl pull ubuntu\nUsing default tag: latest\nlatest: Pulling from library/ubuntu\nd5c7c4625536: Pull complete \n63e0c13f7819: Pull complete \n0bc1db706353: Pull complete \n906ea9b77b3f: Pull complete \n6bea758792bc: Pull complete \n88f79970fa20: Pull complete \nDigest: sha256:312986132029d622ae65423ca25d3a3cf4510de25c47b05b6819d61e2e2b5420\nStatus: Downloaded newer image for ubuntu:latest\n[dsxiao@hadoop29 hyper]$ hyperctl run -t ubuntu\nVM response data is nil\n. directly run daemon   hyperd -v3, get below error section\nI1017 07:21:16.764064   12128 qmp_handler.go:103] got a message {\"timestamp\": {\"seconds\": 1476703275, \"microseconds\": 776609}, \"event\": \"VSERPORT_CHANGE\", \"data\": {\"open\": true, \"id\": \"channel1\"}}\nI1017 07:21:16.764189   12128 qmp_handler.go:107] got event: VSERPORT_CHANGE\nI1017 07:21:16.764448   12128 qmp_handler.go:323] got QMP event VSERPORT_CHANGE\nI1017 07:21:45.769801   12128 hypervisor.go:29] vm vm-qZnSSRjfgF: main event loop got message 37(ERROR_INTERRUPTED)\nI1017 07:21:45.769914   12128 vm_states.go:317] Connection interrupted, quit...\nE1017 07:21:45.769942   12128 vm_states.go:287] Shutting down because of an exception: connection to VM broken\nI1017 07:21:45.769979   12128 context.go:258] VM vm-qZnSSRjfgF: state change from STARTING to 'DESTROYING'\nI1017 07:21:45.769990   12128 vm_states.go:36] VM has exit...\n. seems like #172 behavior\n. thanks\n. Cool. Close it.. ",
    "danielbodart": "sudo hyper info\nContainers: 6\nPODs: 3\nTotal Memory: 32896704 KB\nOperating System: Ubuntu 15.04\nuname -a\nLinux desktop 3.19.0-28-generic #30-Ubuntu SMP Mon Aug 31 15:52:51 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux\nsudo service hyperd status\n\u25cf hyperd.service - hyperd\n   Loaded: loaded (/lib/systemd/system/hyperd.service; disabled; vendor preset: enabled)\n   Active: active (running) since Tue 2015-09-15 06:54:53 BST; 1h 33min ago\n     Docs: http://docs.hyper.sh\n Main PID: 2828 (hyperd)\n   CGroup: /system.slice/hyperd.service\n           \u251c\u25002828 /usr/local/bin/hyperd\n           \u2514\u25005376 qemu-system-x86_64 -machine pc-i440fx-2.0,accel=kvm,usb=off -global kvm-pit.lost_tick_policy=discard -cpu host -drive if...\nSep 15 07:56:50 desktop hyperd[2828]: [HYPER INFO  0915 07:56:\nSep 15 07:56:50 desktop hyperd[2828]: [HYPER INFO  0915 07:56:\nSep 15 07:56:50 desktop hyperd[2828]: [HYPER INFO  0915 07:56:\nSep 15 07:56:50 desktop hyperd[2828]: [HYPER INFO  0915 07:56:\nSep 15 07:58:36 desktop hyperd[2828]: [HYPER INFO  0915 07:58:\nSep 15 07:58:36 desktop hyperd[2828]: [HYPER INFO  0915 07:58:\nSep 15 07:58:36 desktop hyperd[2828]: 2015/09/15 07:58:36 http: response.WriteHeader on hijacked connection\nSep 15 08:26:45 desktop hyperd[2828]: [HYPER INFO  0915 08:26:\nSep 15 08:26:45 desktop hyperd[2828]: [HYPER INFO  0915 08:26:\nSep 15 08:26:45 desktop hyperd[2828]: [HYPER INFO  0915 08:26:\ncat /proc/5376/cmdline\nqemu-system-x86_64-machinepc-i440fx-2.0,accel=kvm,usb=off-globalkvm-pit.lost_tick_policy=discard-cpuhost-driveif=pflash,file=/var/lib/hyper/bios-qboot.bin,readonly=on-driveif=pflash,file=/var/lib/hyper/cbfs-qboot.rom,readonly=on-realtimemlock=off-no-user-config-nodefaults-no-hpet-rtcbase=utc,driftfix=slew-no-reboot-displaynone-bootstrict=on-m128-smp1-qmpunix:/var/run/hyper/vm-rKeJAzBJCA/qmp.sock,server,nowait-serialunix:/var/run/hyper/vm-rKeJAzBJCA/console.sock,server,nowait-devicevirtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x2-devicevirtio-scsi-pci,id=scsi0,bus=pci.0,addr=0x3-chardevsocket,id=charch0,path=/var/run/hyper/vm-rKeJAzBJCA/hyper.sock,server,nowait-devicevirtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0-chardevsocket,id=charch1,path=/var/run/hyper/vm-rKeJAzBJCA/tty.sock,server,nowait-devicevirtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1-fsdevlocal,id=virtio9p,path=/var/run/hyper/vm-rKeJAzBJCA/share_dir,security_model=none-devicevirtio-9p-pci,fsdev=virtio9p,mount_tag=share_dir\n. Thanks a lot for the fast response!\n. Okay so adding the the resolve config fixes DNS lookups. \nHowever I think the default file system is not writeable by git.\nIf you run the standard ubuntu:latest (with the resolve pod file) image in hyper and then attach to the console:\n1. apt-get update\n2. apt-get install git\n3. git checkout https://github.com/hyperhq/hyper.git\nYou will see the same messages as my image.\n. Okay investigating further this seams to be a problem with certain virtual file systems in qemu, is it possible to change the default virtual file system used by hyper? What is the default (qcow2)? \n. Okay so if I create a \"raw\" volume and then run git in that folder it works fine.\nSo I'd like to try changing the default storage driver from aufs to overlayfs and see if that works. Any suggestions how I do that?\n. Well I'm building a multi-tenant build system on bare metal. The plan is to use Hyper to protect the base OS and of course users from each other.\nI wasn't planning for the users to have their own SSL certificate, my control software (and me remotely) would be the only people/machines with certificates.\n. Wow, that was fast.\nI assume this will fix it for all cases?\nI.e if the hyperd is not started or if I run a different command?\n. ",
    "CMGS": "@carmark \ngot by curl -sSL https://hyper.sh/install | bash \niptables was replaced by FirewallD in CentOS 7, so i think it cause this problem\n. [root@lothar ~]# iptables -t nat -nvL |grep Chain\nChain PREROUTING (policy ACCEPT 1944K packets, 1025M bytes)\nChain INPUT (policy ACCEPT 24 packets, 1244 bytes)\nChain OUTPUT (policy ACCEPT 2647 packets, 201K bytes)\nChain POSTROUTING (policy ACCEPT 2647 packets, 201K bytes)\nChain DOCKER (0 references)\nChain HYPER (0 references)\nChain OUTPUT_direct (1 references)\nChain POSTROUTING_ZONES (1 references)\nChain POSTROUTING_ZONES_SOURCE (1 references)\nChain POSTROUTING_direct (1 references)\nChain POST_public (2 references)\nChain POST_public_allow (1 references)\nChain POST_public_deny (1 references)\nChain POST_public_log (1 references)\nChain PREROUTING_ZONES (1 references)\nChain PREROUTING_ZONES_SOURCE (1 references)\nChain PREROUTING_direct (1 references)\nChain PRE_public (2 references)\nChain PRE_public_allow (1 references)\nChain PRE_public_deny (1 references)\nChain PRE_public_log (1 references)\nOn Thu, Nov 5, 2015 at 4:31 PM, Gao feng notifications@github.com wrote:\n\n@CMGS https://github.com/CMGS Can you show me the output of iptables -t\nnat -nvL |grep Chain, and check if xt_addrtype kernel module is available\non your centos?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/hyperhq/hyper/issues/115#issuecomment-153987035.\n\n\n-CMGS\nA simple coder.\nLove travel, sports especially outdoor sports and computer technology.\nHave a dream that one day can tour around.\n. OK\uff0cI will test it ASAP\nOn Wed, Nov 25, 2015 at 5:41 PM, Xu Wang notifications@github.com wrote:\n\n@CMGS https://github.com/CMGS\nfor centOS user, could you try the binary in this rpm\nhttps://s3.amazonaws.com/hyper-install/hyper-0.4-1.el7.centos.src.rpm\nI was working in build hyper native packages in hyper VMs (PR: #135\nhttps://github.com/hyperhq/hyper/pull/135 ), this is the first distro\npackage to build, and I need some tester to try it.\nPS: you need remove hyper and hyperd binary in /usr/local/bin, because rpm\nwill put binary in /usr/bin\nThank you.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/hyperhq/hyper/issues/115#issuecomment-159551285.\n\n\n-CMGS\nA simple coder.\nLove travel, sports especially outdoor sports and computer technology.\nHave a dream that one day can tour around.\n. I install those RPMs on my host, it seems work well, nice job!\nOn Sat, Jan 30, 2016 at 11:41 AM, Xu Wang notifications@github.com wrote:\n\n@CMGS https://github.com/CMGS\nBased on our tests and user reports, the following hyper and qemu RPMs\nwill fix hyper on CentOS, could you remove all the hyper binaries under\n/usr/local/bin, and install the following prebuilt RPMs:\nx86_64 binary packages:\n- hyper-0.4-2.el7.centos.x86_64.rpm\n  https://s3.amazonaws.com/hyper-install/hyper-0.4-2.el7.centos.x86_64.rpm\n- hyperstart-0.4-2.el7.centos.x86_64.rpm\n  https://s3.amazonaws.com/hyper-install/hyperstart-0.4-2.el7.centos.x86_64.rpm\n- qemu-hyper-2.4.1-2.el7.centos.x86_64.rpm\n  https://s3.amazonaws.com/hyper-install/qemu-hyper-2.4.1-2.el7.centos.x86_64.rpm\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/hyperhq/hyper/issues/115#issuecomment-177059830.\n\n\n-CMGS\nA simple coder.\nLove travel, sports especially outdoor sports and computer technology.\nHave a dream that one day can tour around.\n. ",
    "zhengxiaochuan-3": "@gnawux  I downloaded  binary with \" curl -sSL https://hyper.sh/install | bash \"\n[HYPER ERROR 1101 15:57:^@50 09130 qmp_handler.go] QMP initialize failed\n[HYPER ERROR 1101 15:57:^@50 09130 vm_states.go] read unix @->/var/run/hyper/vm-naBttYhBUN/qmp.sock: read: connection reset by peer\n[HYPER ERROR 1101 15:57:^@50 09130 vm_states.go] Shutting down because of an exception: Fail during init pod running environment\n[HYPER ERROR 1101 15:57:^@50 09130 tty.go] read tty data failed\n[HYPER ERROR 1101 15:57:^@50 09130 init_comm.go] read init data failed\n[HYPER ERROR 1101 15:57:^@50 09130 init_comm.go] read init message failed... read unix @->/var/run/hyper/vm-naBttYhBUN/hyper.sock: read: connection reset by peer\n[HYPER ERROR 1101 15:58:^@00 09130 vm_states.go] Shutting down because of an exception: vm terminating timeout\n[HYPER ERROR 1101 15:58:^@10 09130 pod.go] VM response data is nil\n[HYPER ERROR 1101 15:58:^@10 09130 server.go] Handler for POST /pod/run returned error: VM response data is nil\n[HYPER ERROR 1101 15:58:^@10 09130 server.go] HTTP Error: statusCode=500 VM response data is nil\n[HYPER ERROR 1101 16:04:^@35 09130 tty.go] read tty data failed\n[HYPER ERROR 1101 16:04:^@35 09130 vm_states.go] Shutting down because of an exception: connection to VM broken\n[HYPER ERROR 1101 16:04:^@35 09130 pod.go] VM response data is nil\n[HYPER ERROR 1101 16:04:^@35 09130 server.go] Handler for POST /pod/run returned error: VM response data is nil\n[HYPER ERROR 1101 16:04:^@35 09130 server.go] HTTP Error: statusCode=500 VM response data is nil\n[HYPER ERROR 1101 16:04:^@35 09130 init_comm.go] read init data failed\n[HYPER ERROR 1101 16:04:^@35 09130 init_comm.go] read init message failed... read unix @->/var/run/hyper/vm-KPoNOIBHDm/hyper.sock: read: connection reset by peer\n[HYPER ERROR 1101 16:04:^@35 09130 qmp_handler.go] get qmp welcome failed: read unix @->/var/run/hyper/vm-KPoNOIBHDm/qmp.sock: read: connection reset by peer\n[HYPER ERROR 1101 16:04:^@35 09130 qmp_handler.go] QMP initialize failed\n[HYPER ERROR 1101 16:05:^@16 09130 qmp_handler.go] QMP initialize timeout\n[HYPER ERROR 1101 16:05:^@16 09130 vm_states.go] QMP Init timeout\n[HYPER ERROR 1101 16:05:^@16 09130 vm_states.go] Shutting down because of an exception: Fail during init pod running environment\n[HYPER ERROR 1101 16:05:^@16 09130 qmp_handler.go] failed to connected to /var/run/hyper/vm-CLJiuNayuX/qmp.sock dial unix /var/run/hyper/vm-CLJiuNayuX/qmp.sock: connect: connection refused\n[HYPER ERROR 1101 16:05:^@16 09130 tty.go] Cannot connect to tty socket dial unix /var/run/hyper/vm-CLJiuNayuX/tty.sock: connect: connection refused\n[HYPER ERROR 1101 16:05:^@16 09130 init_comm.go] Cannot connect to hyper socket dial unix /var/run/hyper/vm-CLJiuNayuX/hyper.sock: connect: connection refused\n. @gnawux \n --- did you compile qemu from source\n      no,I just install qemu with this cmd \"apt-get install qemu\"\nall the daemon infos : \n[HYPER INFO  1101 16:32:46 03058 server.go] Calling POST /pod/run\n[HYPER INFO  1101 16:32:46 03058 job.go] +job podRun({\"id\":\"ubuntu-9105010387\",\"containers\":[{\"name\":\"ubuntu-9105010387\",\"image\":\"ubuntu\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}, no)\n[HYPER INFO  1101 16:32:46 03058 pod.go] {\"id\":\"ubuntu-9105010387\",\"containers\":[{\"name\":\"ubuntu-9105010387\",\"image\":\"ubuntu\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1101 16:32:46 03058 pod.go] podArgs: {\"id\":\"ubuntu-9105010387\",\"containers\":[{\"name\":\"ubuntu-9105010387\",\"image\":\"ubuntu\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1101 16:32:46 03058 pod.go] leveldb: not found\n[HYPER INFO  1101 16:32:46 03058 pod.go] Process the Containers section in POD SPEC\n[HYPER INFO  1101 16:32:46 03058 tags.go] LookupImage Name is ubuntu\n[HYPER INFO  1101 16:32:46 03058 container.go] ready to get the container(224e97d150b67792f4423ecd1fbe34eb451ba7b86c376a3c76fe1b18c9ff017c) info\n[HYPER INFO  1101 16:32:46 03058 pod.go] The config: kernel=/var/lib/hyper/kernel, initrd=/var/lib/hyper/hyper-initrd.img\nPOD id is pod-chNJTxyOyO\n[HYPER INFO  1101 16:32:46 03058 container.go] ready to get the container(224e97d150b67792f4423ecd1fbe34eb451ba7b86c376a3c76fe1b18c9ff017c) info\n[HYPER INFO  1101 16:32:46 03058 qemu_process.go] cmdline arguments: -machine pc-i440fx-2.0,accel=kvm,usb=off -global kvm-pit.lost_tick_policy=discard -cpu host -drive if=pflash,file=/var/lib/hyper/bios-qboot.bin,readonly=on -drive if=pflash,file=/var/lib/hyper/cbfs-qboot.rom,readonly=on -realtime mlock=off -no-user-config -nodefaults -no-hpet -rtc base=utc,driftfix=slew -no-reboot -display none -boot strict=on -m 128 -smp 1 -qmp unix:/var/run/hyper/vm-NunbJiqFZu/qmp.sock,server,nowait -serial unix:/var/run/hyper/vm-NunbJiqFZu/console.sock,server,nowait -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x2 -device virtio-scsi-pci,id=scsi0,bus=pci.0,addr=0x3 -chardev socket,id=charch0,path=/var/run/hyper/vm-NunbJiqFZu/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0 -chardev socket,id=charch1,path=/var/run/hyper/vm-NunbJiqFZu/tty.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1 -fsdev local,id=virtio9p,path=/var/run/hyper/vm-NunbJiqFZu/share_dir,security_model=none -device virtio-9p-pci,fsdev=virtio9p,mount_tag=share_dir\n[HYPER INFO  1101 16:32:46 03058 pod.go] Parsing envs for container 0: 0 Evs\n[HYPER INFO  1101 16:32:46 03058 pod.go] The fs type is dir\n[HYPER INFO  1101 16:32:46 03058 pod.go] WorkingDir is \n[HYPER INFO  1101 16:32:46 03058 pod.go] Image is /224e97d150b67792f4423ecd1fbe34eb451ba7b86c376a3c76fe1b18c9ff017c/rootfs\n[HYPER INFO  1101 16:32:46 03058 pod.go] Container Info is \n&{224e97d150b67792f4423ecd1fbe34eb451ba7b86c376a3c76fe1b18c9ff017c  /224e97d150b67792f4423ecd1fbe34eb451ba7b86c376a3c76fe1b18c9ff017c/rootfs dir  [] [/bin/bash] map[]}\n[HYPER INFO  1101 16:32:46 03058 pod.go] container 0 created 224e97d150b67792f4423ecd1fbe34eb451ba7b86c376a3c76fe1b18c9ff017c, workdir , env: map[]\n[HYPER INFO  1101 16:32:46 03058 hypervisor.go] main event loop got message 22(COMMAND_RUN_POD)\n[HYPER INFO  1101 16:32:46 03058 vm_states.go] got spec, prepare devices\n[HYPER INFO  1101 16:32:46 03058 vm.go] hyperHandlePodEvent pod pod-chNJTxyOyO, vm vm-NunbJiqFZu\n[HYPER INFO  1101 16:32:46 03058 vm.go] hyperHandlePodEvent pod pod-chNJTxyOyO, vm vm-NunbJiqFZu\n[HYPER INFO  1101 16:32:46 03058 context.go] VM vm-NunbJiqFZu: state change from  to 'STARTING'\n[HYPER INFO  1101 16:32:46 03058 hypervisor.go] main event loop got message 13(EVENT_INTERFACE_ADD)\n[HYPER INFO  1101 16:32:46 03058 qmp_wrapper.go] send net to qemu at 21\n[HYPER INFO  1101 16:32:46 03058 qmp_handler.go] got new session during initializing\n[HYPER INFO  1101 16:32:46 03058 qemu_process.go] qemu daemon pid 3131.\n[HYPER INFO  1101 16:32:46 03058 qemu_process.go] starting daemon with pid: 3131\n[HYPER WARN  1101 16:32:56 03058 qmp_handler.go] Initializer Timeout.\n[HYPER ERROR 1101 16:32:56 03058 qmp_handler.go] QMP initialize timeout\n[HYPER INFO  1101 16:32:56 03058 hypervisor.go] main event loop got message 32(ERROR_INIT_FAIL)\n[HYPER ERROR 1101 16:32:56 03058 vm_states.go] QMP Init timeout\n[HYPER ERROR 1101 16:32:56 03058 vm_states.go] Shutting down because of an exception: Fail during init pod running environment\n[HYPER INFO  1101 16:32:56 03058 context.go] VM vm-NunbJiqFZu: state change from STARTING to 'TERMINATING'\n[HYPER INFO  1101 16:32:56 03058 vm.go] Get the response from VM, VM id is vm-NunbJiqFZu!\n[HYPER INFO  1101 16:32:56 03058 hypervisor.go] main event loop got message 25(COMMAND_SHUTDOWN)\n[HYPER INFO  1101 16:32:56 03058 vm_states.go] got event during terminating\n[HYPER ERROR 1101 16:32:56 03058 init_comm.go] Cannot connect to hyper socket dial unix /var/run/hyper/vm-NunbJiqFZu/hyper.sock: connect: connection refused\n[HYPER INFO  1101 16:32:56 03058 hypervisor.go] main event loop got message 32(ERROR_INIT_FAIL)\n[HYPER INFO  1101 16:32:56 03058 vm_states.go] got event during terminating\n[HYPER ERROR 1101 16:32:56 03058 tty.go] Cannot connect to tty socket dial unix /var/run/hyper/vm-NunbJiqFZu/tty.sock: connect: connection refused\n[HYPER ERROR 1101 16:32:56 03058 init_comm.go] failed to connected to /var/run/hyper/vm-NunbJiqFZu/console.sock dial unix /var/run/hyper/vm-NunbJiqFZu/console.sock: connect: no such file or directory\n[HYPER ERROR 1101 16:32:56 03058 qmp_handler.go] failed to connected to /var/run/hyper/vm-NunbJiqFZu/qmp.sock dial unix /var/run/hyper/vm-NunbJiqFZu/qmp.sock: connect: connection refused\n[HYPER INFO  1101 16:32:56 03058 hypervisor.go] main event loop got message 32(ERROR_INIT_FAIL)\n[HYPER INFO  1101 16:32:56 03058 vm_states.go] got event during terminating\n[HYPER INFO  1101 16:33:06 03058 hypervisor.go] main event loop got message 3(EVENT_VM_TIMEOUT)\n[HYPER WARN  1101 16:33:06 03058 vm_states.go] VM did not exit in time, try to stop it\n[HYPER ERROR 1101 16:33:06 03058 vm_states.go] Shutting down because of an exception: vm terminating timeout\n[HYPER INFO  1101 16:33:06 03058 vm.go] Got response: 7: vm terminating timeout\n[HYPER INFO  1101 16:33:16 03058 qemu_process.go] kill Qemu... 3131\n[HYPER INFO  1101 16:33:16 03058 hypervisor.go] main event loop got message 2(EVENT_VM_KILL)\n[HYPER INFO  1101 16:33:16 03058 vm_states.go] Got VM force killed message, go to cleaning up\n[HYPER INFO  1101 16:33:16 03058 vm_states.go] VM has exit...\n[HYPER INFO  1101 16:33:16 03058 vm.go] Got response: 2: VM shut down\n[HYPER ERROR 1101 16:33:16 03058 pod.go] VM response data is nil\n[HYPER INFO  1101 16:33:16 03058 devicemap.go] need unmount aufs /224e97d150b67792f4423ecd1fbe34eb451ba7b86c376a3c76fe1b18c9ff017c/rootfs\n[HYPER INFO  1101 16:33:16 03058 job.go] -job podRun({\"id\":\"ubuntu-9105010387\",\"containers\":[{\"name\":\"ubuntu-9105010387\",\"image\":\"ubuntu\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}, no) ERR: VM response data is nil\n[HYPER INFO  1101 16:33:16 03058 devicemap.go] remove network card 0: 192.168.123.3\n[HYPER INFO  1101 16:33:16 03058 context.go] VM vm-NunbJiqFZu: state change from TERMINATING to 'DESTROYING'\n[HYPER ERROR 1101 16:33:16 03058 server.go] Handler for POST /pod/run returned error: VM response data is nil\n[HYPER ERROR 1101 16:33:16 03058 server.go] HTTP Error: statusCode=500 VM response data is nil\n[HYPER INFO  1101 16:33:16 03058 volume_linux.go] Ready to unmount the target : /var/run/hyper/vm-NunbJiqFZu/share_dir/224e97d150b67792f4423ecd1fbe34eb451ba7b86c376a3c76fe1b18c9ff017c/rootfs\n[HYPER INFO  1101 16:33:16 03058 hypervisor.go] main event loop got message 14(EVENT_INTERFACE_DELETE)\n[HYPER INFO  1101 16:33:16 03058 devicemap.go] interface 0 released\n[HYPER INFO  1101 16:33:16 03058 vm_states.go] Unplug interface return with true\n[HYPER INFO  1101 16:33:16 03058 hypervisor.go] main event loop got message 7(EVENT_CONTAINER_DELETE)\n[HYPER INFO  1101 16:33:16 03058 devicemap.go] container 0 umounted\n[HYPER INFO  1101 16:33:16 03058 vm_states.go] Unplug container return with true\n[HYPER INFO  1101 16:33:34 03058 hypervisor.go] main event loop got message 3(EVENT_VM_TIMEOUT)\n[HYPER INFO  1101 16:33:34 03058 vm_states.go] Device removing timeout\n. @gnawux \nsystem rebooted, but hyper run still failed.\nok,waiting for new version of binary hyper. trying to  Build from Source\nthanks a lot .\n. @gnawux \nI builded hyper from source on centos7.1 , no more this issue again.\n. [root@node-65-148 hyper-bootstrap-root]# hyper images\nREPOSITORY     TAG                 IMAGE ID                           CREATED             VIRTUAL SIZE\nubuntu         latest              a005e6b7dd01                2015-10-13 01:27:03            179.6 MB\nv4  still INFO messages : \n[root@node-65-148 hyper]# hyperd --nondaemon -v=4\n[HYPER INFO  1103 13:24:54 35125 hyperd.go] [:84] The config file is \n[HYPER INFO  1103 13:24:54 35125 docker.go] [:48] success to create docker\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:195] The config: kernel=/var/lib/hyper/kernel, initrd=/var/lib/hyper/hyper-initrd.img\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:197] The config: vbox image=\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:200] The config: bridge=, ip=\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:203] The config: bios=/var/lib/hyper/bios-qboot.bin, cbfs=/var/lib/hyper/cbfs-qboot.rom\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:979] devicemapper: driver version is 4.27.0\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:1060] Generated prefix: docker-253:0-1047406\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:1063] Checking for existence of the pool 'docker-253:0-1047406-pool'\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:373] [deviceset] constructDeviceIdMap()\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/002fa881df8af8679b36b85f052456483a8bec47ad270df58ee811bc224c3b08\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=4 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/0105f98ced6dc9e178b4b2aa3400759d0395c402c3718992561b11652a1eb6f6\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=2 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/0211c5b8e6e2e5856e76484e1c9b9d4bee6bbe42174c499580992c4146c206a6-init\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=27 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/37c00c2443e72f688be35c263ea6d31aaf41c3dd5e0ed016a1b90fbaaad45814\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=17 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/37c00c2443e72f688be35c263ea6d31aaf41c3dd5e0ed016a1b90fbaaad45814-init\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=16 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/3f2e332aed291fe3cf7ba7c032ada73519f268493f94d41c13834988fab4c7be-init\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=22 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/449549c1a2a2cffbc3df95c99bed86a8b3632511f571113399c212bb05059eb9\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=15 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/449549c1a2a2cffbc3df95c99bed86a8b3632511f571113399c212bb05059eb9-init\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=14 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/59613374cba132a876e5dc8d921ba9f20807f110091a5349f1b81f0f9bc443d2-init\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=25 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/5a8c8b6958aefb52534fc5d64439e28033aa19493649db8f9d8e7b7136ff3865\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=11 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/5a8c8b6958aefb52534fc5d64439e28033aa19493649db8f9d8e7b7136ff3865-init\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=10 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/60cf878e059f8ec321efc62aa0948acd576caf5cbf95770bc92538060d7c48c4\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=9 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/60cf878e059f8ec321efc62aa0948acd576caf5cbf95770bc92538060d7c48c4-init\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=6 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/66395c31eb82ba7f0a4efc97b3a18f1ca9afa82b4d19fba23f9f3891a844bcf4\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=3 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/8f421626b8714d0de19cd91731fa925087f243c965774b77bff6f1e3945f9bc8\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=21 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/8f421626b8714d0de19cd91731fa925087f243c965774b77bff6f1e3945f9bc8-init\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=20 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/a005e6b7dd0152d61a0f3d2b3aa42e93bbc5568facf435ebb3c022faf03c7085\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=5 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/a36a25aa1e7a8bef11ac66f2e170312b0cf814966e58be0dfdd29a7015e0dcfa\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=8 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/a36a25aa1e7a8bef11ac66f2e170312b0cf814966e58be0dfdd29a7015e0dcfa-init\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=7 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/b51f12d6bef1f3af6e5d7fee862a024a007d7cdefcc57c1ed40ff08fecab53fa-init\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=26 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/base\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=1 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/d8aeae00ee34ea254c40bcf43708a489ea91a1bf8f833b2831da2bba9ad9572b-init\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=23 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/e1d1cc219ffb22e83c3f8c9723467770d440fa64cc084eec397aa3056b491197\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=13 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/e1d1cc219ffb22e83c3f8c9723467770d440fa64cc084eec397aa3056b491197-init\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=12 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/f3f92244b20212e2ab0c8976306174ea8ba88e4a45b280d9d67969b69fc13b88\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=19 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/f3f92244b20212e2ab0c8976306174ea8ba88e4a45b280d9d67969b69fc13b88-init\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=18 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/fddcf3a340771d3273f559632e809e457750823533ceadd80e5a19213ee00811-init\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=24 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:347] Loading data for file /var/lib/hyper/devicemapper/metadata/transaction-metadata\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:368] Added deviceId=27 to DeviceIdMap\n[HYPER INFO  1103 13:24:54 35125 deviceset.go] [:390] [deviceset] constructDeviceIdMap() END\n[HYPER INFO  1103 13:24:54 35125 driver.go] [:137] [graphdriver] using prior storage driver \"devicemapper\"\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:629] Using graph driver devicemapper\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:649] Creating images graph\n[HYPER INFO  1103 13:24:54 35125 graph.go] [:77] Restored 5 elements\n[HYPER INFO  1103 13:24:54 35125 trusts.go] [:108] Reloaded graph with 3 grants expiring at 2017-03-22 19:04:46.713978458 +0000 UTC\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:674] Creating repository list\n[HYPER ERROR 1103 13:24:54 35125 daemon.go] [:253] Failed to load container 0211c5b8e6e2e5856e76484e1c9b9d4bee6bbe42174c499580992c4146c206a6: open /var/lib/hyper/containers/0211c5b8e6e2e5856e76484e1c9b9d4bee6bbe42174c499580992c4146c206a6/config.json: no such file or directory\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:259] Loaded container 37c00c2443e72f688be35c263ea6d31aaf41c3dd5e0ed016a1b90fbaaad45814\n[HYPER ERROR 1103 13:24:54 35125 daemon.go] [:253] Failed to load container 3f2e332aed291fe3cf7ba7c032ada73519f268493f94d41c13834988fab4c7be: open /var/lib/hyper/containers/3f2e332aed291fe3cf7ba7c032ada73519f268493f94d41c13834988fab4c7be/config.json: no such file or directory\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:259] Loaded container 449549c1a2a2cffbc3df95c99bed86a8b3632511f571113399c212bb05059eb9\n[HYPER ERROR 1103 13:24:54 35125 daemon.go] [:253] Failed to load container 59613374cba132a876e5dc8d921ba9f20807f110091a5349f1b81f0f9bc443d2: open /var/lib/hyper/containers/59613374cba132a876e5dc8d921ba9f20807f110091a5349f1b81f0f9bc443d2/config.json: no such file or directory\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:259] Loaded container 5a8c8b6958aefb52534fc5d64439e28033aa19493649db8f9d8e7b7136ff3865\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:259] Loaded container 60cf878e059f8ec321efc62aa0948acd576caf5cbf95770bc92538060d7c48c4\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:259] Loaded container 8f421626b8714d0de19cd91731fa925087f243c965774b77bff6f1e3945f9bc8\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:259] Loaded container a36a25aa1e7a8bef11ac66f2e170312b0cf814966e58be0dfdd29a7015e0dcfa\n[HYPER ERROR 1103 13:24:54 35125 daemon.go] [:253] Failed to load container b51f12d6bef1f3af6e5d7fee862a024a007d7cdefcc57c1ed40ff08fecab53fa: open /var/lib/hyper/containers/b51f12d6bef1f3af6e5d7fee862a024a007d7cdefcc57c1ed40ff08fecab53fa/config.json: no such file or directory\n[HYPER ERROR 1103 13:24:54 35125 daemon.go] [:253] Failed to load container d8aeae00ee34ea254c40bcf43708a489ea91a1bf8f833b2831da2bba9ad9572b: open /var/lib/hyper/containers/d8aeae00ee34ea254c40bcf43708a489ea91a1bf8f833b2831da2bba9ad9572b/config.json: no such file or directory\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:259] Loaded container e1d1cc219ffb22e83c3f8c9723467770d440fa64cc084eec397aa3056b491197\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:259] Loaded container f3f92244b20212e2ab0c8976306174ea8ba88e4a45b280d9d67969b69fc13b88\n[HYPER ERROR 1103 13:24:54 35125 daemon.go] [:253] Failed to load container fddcf3a340771d3273f559632e809e457750823533ceadd80e5a19213ee00811: open /var/lib/hyper/containers/fddcf3a340771d3273f559632e809e457750823533ceadd80e5a19213ee00811/config.json: no such file or directory\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:309] Loading containers: done.\n[HYPER INFO  1103 13:24:54 35125 docker.go] [:101] Daemon has completed initialization\n[HYPER WARN  1103 13:24:54 35125 hyperd.go] [:146] Driver xen is unavailable\nQemu Driver Loaded\n[HYPER INFO  1103 13:24:54 35125 hyperd.go] [:150] The hypervisor's driver is kvm\n[HYPER INFO  1103 13:24:54 35125 network_linux.go] [:238] bridge exist\n[HYPER INFO  1103 13:24:54 35125 iptables_linux.go] [:140] /usr/sbin/iptables, [--wait -t nat -C POSTROUTING -s 192.168.123.1/24 ! -o hyper0 -j MASQUERADE]\n[HYPER INFO  1103 13:24:54 35125 iptables_linux.go] [:140] /usr/sbin/iptables, [--wait -N HYPER]\n[HYPER INFO  1103 13:24:54 35125 iptables_linux.go] [:140] /usr/sbin/iptables, [--wait -t filter -C FORWARD -o hyper0 -j HYPER]\n[HYPER INFO  1103 13:24:54 35125 iptables_linux.go] [:140] /usr/sbin/iptables, [--wait -t filter -C FORWARD -i hyper0 -j ACCEPT]\n[HYPER INFO  1103 13:24:54 35125 iptables_linux.go] [:140] /usr/sbin/iptables, [--wait -t filter -C FORWARD -o hyper0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT]\n[HYPER INFO  1103 13:24:54 35125 network_linux.go] [:150] modprobe br_netfilter failed modprobe br_netfilter failed\n[HYPER INFO  1103 13:24:54 35125 iptables_linux.go] [:140] /usr/sbin/iptables, [--wait -t nat -N HYPER]\n[HYPER INFO  1103 13:24:54 35125 iptables_linux.go] [:140] /usr/sbin/iptables, [--wait -t nat -C OUTPUT -m addrtype --dst-type LOCAL ! -d 127.0.0.1/8 -j HYPER]\n[HYPER INFO  1103 13:24:54 35125 iptables_linux.go] [:140] /usr/sbin/iptables, [--wait -t nat -C PREROUTING -m addrtype --dst-type LOCAL -j HYPER]\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= tty\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= version\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= build\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= podRun\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= create\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= attach\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= serviceAdd\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= serviceList\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= auth\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= podCreate\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= exec\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= serviceDelete\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= commit\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= podStart\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= podStop\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= acceptconnections\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= images\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= imagesremove\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= serveapi\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= pull\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= push\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= podInfo\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= vmKill\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= list\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= serviceUpdate\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= info\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= rename\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= containerInfo\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= containerLogs\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= podRm\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:105] Engine Register: name= vmCreate\n[HYPER INFO  1103 13:24:54 35125 hyperd.go] [:192] Hyper daemon: 0.4.0 0\n[HYPER INFO  1103 13:24:54 35125 job.go] [:78] +job acceptconnections()\n[HYPER INFO  1103 13:24:54 35125 job.go] [:84] -job acceptconnections() OK\n[HYPER INFO  1103 13:24:54 35125 hyperd.go] [:224] Daemon has completed initialization\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:125] 37c00c2443e72f688be35c263ea6d31aaf41c3dd5e0ed016a1b90fbaaad45814\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:125] e1d1cc219ffb22e83c3f8c9723467770d440fa64cc084eec397aa3056b491197\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:125] 60cf878e059f8ec321efc62aa0948acd576caf5cbf95770bc92538060d7c48c4\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:125] 5a8c8b6958aefb52534fc5d64439e28033aa19493649db8f9d8e7b7136ff3865\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:125] a36a25aa1e7a8bef11ac66f2e170312b0cf814966e58be0dfdd29a7015e0dcfa\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:125] 8f421626b8714d0de19cd91731fa925087f243c965774b77bff6f1e3945f9bc8\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:125] f3f92244b20212e2ab0c8976306174ea8ba88e4a45b280d9d67969b69fc13b88\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:125] 449549c1a2a2cffbc3df95c99bed86a8b3632511f571113399c212bb05059eb9\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:128] Get the pod item, pod is pod-pod-AETARCvThi!\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:128] Get the pod item, pod is pod-pod-BYtvtQeADl!\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:128] Get the pod item, pod is pod-pod-NLRtmdhcMF!\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:128] Get the pod item, pod is pod-pod-QaunHhmVJU!\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:128] Get the pod item, pod is pod-pod-daIdtxTnVU!\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:128] Get the pod item, pod is pod-pod-fGEMLoWUNd!\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:128] Get the pod item, pod is pod-pod-shMCjqOZfn!\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:141] lock PodList\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:226] podArgs: {\"id\":\"ubuntu-4681519241\",\"containers\":[{\"name\":\"ubuntu-4681519241\",\"image\":\"ubuntu\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1103 13:24:54 35125 container.go] [:10] ready to get the container(37c00c2443e72f688be35c263ea6d31aaf41c3dd5e0ed016a1b90fbaaad45814) info\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:285] Found exist container ubuntu-4681519241 (37c00c2443e72f688be35c263ea6d31aaf41c3dd5e0ed016a1b90fbaaad45814), image: ubuntu\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:298] Process the Containers section in POD SPEC\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:301] trying to init container ubuntu-4681519241\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:153] leveldb: not found for pod-AETARCvThi\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:226] podArgs: {\"id\":\"ubuntu-latest-2755407662\",\"containers\":[{\"name\":\"ubuntu-latest-2755407662\",\"image\":\"ubuntu:latest\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1103 13:24:54 35125 container.go] [:10] ready to get the container(e1d1cc219ffb22e83c3f8c9723467770d440fa64cc084eec397aa3056b491197) info\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:285] Found exist container ubuntu-latest-2755407662 (e1d1cc219ffb22e83c3f8c9723467770d440fa64cc084eec397aa3056b491197), image: ubuntu:latest\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:298] Process the Containers section in POD SPEC\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:301] trying to init container ubuntu-latest-2755407662\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:153] leveldb: not found for pod-BYtvtQeADl\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:226] podArgs: {\"id\":\"ubuntu-latest-2579817665\",\"containers\":[{\"name\":\"ubuntu-latest-2579817665\",\"image\":\"ubuntu:latest\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1103 13:24:54 35125 container.go] [:10] ready to get the container(60cf878e059f8ec321efc62aa0948acd576caf5cbf95770bc92538060d7c48c4) info\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:285] Found exist container ubuntu-latest-2579817665 (60cf878e059f8ec321efc62aa0948acd576caf5cbf95770bc92538060d7c48c4), image: ubuntu:latest\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:298] Process the Containers section in POD SPEC\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:301] trying to init container ubuntu-latest-2579817665\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:153] leveldb: not found for pod-NLRtmdhcMF\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:226] podArgs: {\"id\":\"ubuntu-latest-1529084231\",\"containers\":[{\"name\":\"ubuntu-latest-1529084231\",\"image\":\"ubuntu:latest\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1103 13:24:54 35125 container.go] [:10] ready to get the container(5a8c8b6958aefb52534fc5d64439e28033aa19493649db8f9d8e7b7136ff3865) info\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:285] Found exist container ubuntu-latest-1529084231 (5a8c8b6958aefb52534fc5d64439e28033aa19493649db8f9d8e7b7136ff3865), image: ubuntu:latest\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:298] Process the Containers section in POD SPEC\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:301] trying to init container ubuntu-latest-1529084231\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:153] leveldb: not found for pod-QaunHhmVJU\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:226] podArgs: {\"id\":\"ubuntu-9435851299\",\"containers\":[{\"name\":\"ubuntu-9435851299\",\"image\":\"ubuntu\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1103 13:24:54 35125 container.go] [:10] ready to get the container(8f421626b8714d0de19cd91731fa925087f243c965774b77bff6f1e3945f9bc8) info\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:285] Found exist container ubuntu-9435851299 (8f421626b8714d0de19cd91731fa925087f243c965774b77bff6f1e3945f9bc8), image: ubuntu\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:298] Process the Containers section in POD SPEC\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:301] trying to init container ubuntu-9435851299\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:153] leveldb: not found for pod-daIdtxTnVU\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:226] podArgs: {\"id\":\"ubuntu-2226550459\",\"containers\":[{\"name\":\"ubuntu-2226550459\",\"image\":\"ubuntu\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1103 13:24:54 35125 container.go] [:10] ready to get the container(f3f92244b20212e2ab0c8976306174ea8ba88e4a45b280d9d67969b69fc13b88) info\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:285] Found exist container ubuntu-2226550459 (f3f92244b20212e2ab0c8976306174ea8ba88e4a45b280d9d67969b69fc13b88), image: ubuntu\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:298] Process the Containers section in POD SPEC\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:301] trying to init container ubuntu-2226550459\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:153] leveldb: not found for pod-fGEMLoWUNd\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:226] podArgs: {\"id\":\"ubuntu-latest-3207049895\",\"containers\":[{\"name\":\"ubuntu-latest-3207049895\",\"image\":\"ubuntu:latest\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1103 13:24:54 35125 container.go] [:10] ready to get the container(449549c1a2a2cffbc3df95c99bed86a8b3632511f571113399c212bb05059eb9) info\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:285] Found exist container ubuntu-latest-3207049895 (449549c1a2a2cffbc3df95c99bed86a8b3632511f571113399c212bb05059eb9), image: ubuntu:latest\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:298] Process the Containers section in POD SPEC\n[HYPER INFO  1103 13:24:54 35125 pod.go] [:301] trying to init container ubuntu-latest-3207049895\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:153] leveldb: not found for pod-shMCjqOZfn\n[HYPER INFO  1103 13:24:54 35125 daemon.go] [:163] unlock PodList\n[HYPER INFO  1103 13:24:54 35125 job.go] [:78] +job serveapi(unix:///var/run/hyper.sock)\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1283] Listening for HTTP on unix (/var/run/hyper.sock)\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering GET, /version\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering GET, /list\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering GET, /images/get\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering GET, /service/list\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering GET, /info\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering GET, /pod/info\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering GET, /container/info\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering GET, /container/logs\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /container/commit\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /pod/remove\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /exec\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /images/remove\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /service/add\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /vm/kill\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /service/update\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /service/delete\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /container/rename\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /image/create\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /image/build\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /image/push\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /pod/start\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /pod/create\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /pod/run\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /vm/create\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /attach\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /tty/resize\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /auth\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /container/create\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering POST, /pod/stop\n[HYPER INFO  1103 13:24:54 35125 server.go] [:1146] Registering OPTIONS, \n-----------------here i run the ubuntu image--------------\n[HYPER INFO  1103 13:24:59 35125 server.go] [:1034] Calling POST /pod/create\n[HYPER INFO  1103 13:24:59 35125 server.go] [:553] Args string is {\"id\":\"ubuntu-9981585516\",\"containers\":[{\"name\":\"ubuntu-9981585516\",\"image\":\"ubuntu\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}, \n[HYPER INFO  1103 13:24:59 35125 job.go] [:78] +job podCreate({\"id\":\"ubuntu-9981585516\",\"containers\":[{\"name\":\"ubuntu-9981585516\",\"image\":\"ubuntu\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}, )\n[HYPER INFO  1103 13:24:59 35125 pod.go] [:40] lock PodList\n[HYPER INFO  1103 13:24:59 35125 pod.go] [:226] podArgs: {\"id\":\"ubuntu-9981585516\",\"containers\":[{\"name\":\"ubuntu-9981585516\",\"image\":\"ubuntu\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1103 13:24:59 35125 pod.go] [:298] Process the Containers section in POD SPEC\n[HYPER INFO  1103 13:24:59 35125 pod.go] [:301] trying to init container ubuntu-9981585516\n[HYPER INFO  1103 13:24:59 35125 tags.go] [:133] LookupImage Name is ubuntu\n[HYPER INFO  1103 13:24:59 35125 deviceset.go] [:1186] [deviceset] AddDevice(hash=db2595a281231b8b00f041a5d9624855a27feded5cc98f5fe18e793b9c193908-init basehash=a005e6b7dd0152d61a0f3d2b3aa42e93bbc5568facf435ebb3c022faf03c7085)\n[HYPER INFO  1103 13:24:59 35125 deviceset.go] [:413] registerDevice(28, db2595a281231b8b00f041a5d9624855a27feded5cc98f5fe18e793b9c193908-init)\n[HYPER INFO  1103 13:24:59 35125 deviceset.go] [:1208] [deviceset] AddDevice(hash=db2595a281231b8b00f041a5d9624855a27feded5cc98f5fe18e793b9c193908-init basehash=a005e6b7dd0152d61a0f3d2b3aa42e93bbc5568facf435ebb3c022faf03c7085) END\n[HYPER INFO  1103 13:24:59 35125 deviceset.go] [:439] activateDeviceIfNeeded(db2595a281231b8b00f041a5d9624855a27feded5cc98f5fe18e793b9c193908-init)\n. maybe it is a mistake ,close it.\n. ",
    "crook": "any update?\n. dup with #383 \n. ok, I would like to take this enhancement. please assign this issue to me.\n. @laijs\uff0chere is the patch, please  help review.\nbtw, im still confused the above two issue,,can you explain more detail? thanka\n. what does hotadd mean?can you show en example?\ndo you mean the container can auto resan the newly attached volume?\n. BTW: my system is Fedora 24\nAnd the following has been set in /etc/libvirt/qemu.conf\nuser = \"root\"\ngroup = \"root\"\nclear_emulator_capabilities = 0\n. @laijs ,  my Fedora 24 is a VM running in Mac Vmware Fusion .\ni have wipe the old hyper env(/etc/livirt/qemu.conf isn't update, it seems doesn't matter since it's not use libvirt from your words), and try again.\nThis time hyperctl run complete, but the VM is pending status, and no qemu instance found in system\nMore log:\nhttp://pastebin.com/WzSymsB2\n. attach the hyper.ERROR\n```\n[root@localhost ~]# cat /var/log/hyper/hyperd.ERROR\nLog file created at: 2016/09/18 11:14:34\nRunning on machine: localhost\nBinary: Built with gc go1.6.3 for linux/amd64\nLog line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\nE0918 11:14:34.699797    1811 qmp_handler.go:181] get qmp welcome failed: read unix @->/var/run/hyper/vm-koLUAvjzuO/qmp.sock: read: connection reset by peer\nE0918 11:14:34.700514    1811 qmp_handler.go:361] QMP initialize failed\nE0918 11:14:34.700530    1811 vm_states.go:401] read unix @->/var/run/hyper/vm-koLUAvjzuO/qmp.sock: read: connection reset by peer\nE0918 11:14:34.700547    1811 vm_states.go:278] Shutting down because of an exception: Fail during init pod running environment\nE0918 11:14:34.700631    1811 run.go:86] VM vm-koLUAvjzuO start failed with code 7: Fail during init pod running environment\nE0918 11:14:34.700668    1811 server.go:170] Handler for POST /v1.17/pod/start returned error: VM vm-koLUAvjzuO start failed with code 7: Fail during init pod running environment\nE0918 11:14:34.700794    1811 init_comm.go:99] read init data failed\nE0918 11:14:34.700801    1811 init_comm.go:146] read init message failed... read unix @->/var/run/hyper/vm-koLUAvjzuO/hyper.sock: read: connection reset by peer\nE0918 11:14:34.700830    1811 tty.go:92] read tty data failed\nE0918 11:14:34.708863    1811 qemu_process.go:150] exit status 1\nE0918 11:14:44.702106    1811 vm_states.go:287] Shutting down because of an exception: vm terminating timeout\n[root@localhost ~]# df -h\nFilesystem               Size  Used Avail Use% Mounted on\n......\npod-egFltuQoNW-hosts     1.0M  4.0K 1020K   1% /var/lib/hyper/hosts/pod-egFltuQoNW\n```\n. The qemu command with '-cpu host' will make core dumped. after remove this option, the qemu VM can start.  what does '-cpu host ' means?  \nMy host is one VM(vendor_id: GenuineIntel, mode: ntel(R) Core(TM) i7-4870HQ CPU @ 2.50GHz) running on VMware Fusion.\n[root@localhost ~]# qemu-system-x86_64 -machine pc-i440fx-2.0,accel=kvm,usb=off -global kvm-pit.lost_tick_policy=discard -cpu host -kernel /var/lib/hyper/kernel -initrd /var/lib/hyper/hyper-initrd.img\nqemu-system-x86_64: /builddir/build/BUILD/qemu-2.6.1/target-i386/kvm.c:1713: kvm_put_msrs: Assertion `ret == n' failed.\nAborted (core dumped)\n[root@localhost ~]# qemu-system-x86_64 -machine pc-i440fx-2.0,accel=kvm,usb=off -global kvm-pit.lost_tick_policy=discard -kernel /var/lib/hyper/kernel -initrd /var/lib/hyper/hyper-initrd.im\nFound this similar 2 qemu bug:\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1339196\nhttps://bugzilla.redhat.com/show_bug.cgi?id=1368907\n. After disable intel VT-x/EPT on Vmware fusion, and run hyperd without kvm again:\nand after rebuild (hyper-initrd.img). this time the POD and VM came up.\nLog:\nhttp://pastebin.com/3qTMfS3r\n. I tried '-cpu core2duo', both is working. no matter with VT-X enabled.\nhttp://pastebin.com/us91L9Gj\n. @laijs @gnawux please help check the new version. Thanks\n. @laijs Thanks for your comments. and I have refined the codes. please help check\n. All is set, have a nice holiday:)\n. @laijs updated, User Abspath\n. what if '-v \"\":/hello' format? the driver is empty or VFS?\n. The parseVolume is like parsePortMapping, I prefer to keep consistent. And pointer is faster and less resouce\n. There is no order at all. User input '-v vol1 -v vol2', the order doesn't matter\n. make sense. But it's better to put all the codes into one function\n. There is already one volStr argument. User will be confused if add more volName, right?\n. The vol Name will be aded 5 random number in routine parseVolume, which is enough to make the name uniq. \n. make sense, do you need me also update the parsePortMapping routine? if no need. please help close the PR, thanks.\n. can't catch you\n. won't update this. got this puzzle if I don't use pointer (): https://www.reddit.com/r/golang/comments/2xmnvs/returning_nil_for_a_struct/\n. you are right. I should use path/filepath.Join\n. got you point. I will update\n. ok. #431 \n. so far, only support bind mount. i  would like add the named volume support. doyou think it is necessary?  the  hyper binary already has.\n. ",
    "gaocegege": "Hi, is there any progress? It is a fantastic feature!. ",
    "luluprat": "thnks for you reply\nbut they didn't work either\nsame error\nThe hyperd create failed, hyperd can not support docker's backing storage: vfs\n. i did :+1: curl -sSL https://hyper.sh/install | bash\n. are you planning to support ubuntu 15.04 ?\n. hello\ni tried with the right kernel and everything is fine\nit was a kernel from the cloud provider (specific)\n. i tried \nhyper run elasticsearch --memory=256\nthere is no error  but  i didn't have the prompt come back\n. ",
    "vinss1": "Hello there, did you fix the error, as I'm getting the same error. Any comments will help.\nThanks. ",
    "ptptptptptpt": "thx for reply.\nI've cleard old files, and installed the two rpms,  and  set StorageDriver=devicemapper, but...\nexecute 'systemctl start hyperd' ,  nothing output, and no hyperd process found.\nthen I execute 'hyperd' directly, hyperd started. \nhyper pull ubuntu:latest  ok.  BUT hyper run ubuntu:latest  NOT OK.\n\nhyper run ubuntu:latest\nPOD id is pod-kLUxapNTOm\n\n(after about 10 Secs...)\n\nSuccessfully attached to pod(pod-kLUxapNTOm)\nEnd of CmdExec(), Waiting for hijack to finish.\n\n/var/log/hyper/hyperd.ERROR\n\n[HYPER ERROR 1202 12:46:44 29486 qmp_handler.go] QMP initialize timeout\n[HYPER ERROR 1202 12:46:44 29486 vm_states.go] QMP Init timeout\n[HYPER ERROR 1202 12:46:44 29486 vm_states.go] Shutting down because of an exception: Fail during init pod running environment\n[HYPER ERROR 1202 12:46:44 29486 init_comm.go] Cannot connect to hyper socket dial unix /var/run/hyper/vm-tDIlfLtWUz/hyper.sock: connect: connection refused\n[HYPER ERROR 1202 12:46:44 29486 qmp_handler.go] failed to connected to /var/run/hyper/vm-tDIlfLtWUz/qmp.sock dial unix /var/run/hyper/vm-tDIlfLtWUz/qmp.sock: connect: connection refused\n[HYPER ERROR 1202 12:46:44 29486 tty.go] Cannot connect to tty socket dial unix /var/run/hyper/vm-tDIlfLtWUz/tty.sock: connect: connection refused\n[HYPER ERROR 1202 12:46:54 29486 vm_states.go] Shutting down because of an exception: vm terminating timeout\n[HYPER ERROR 1202 12:47:04 29486 pod.go] VM response data is nil\n[HYPER ERROR 1202 12:47:04 29486 server.go] Handler for POST /pod/start returned error: VM response data is nil\n[HYPER ERROR 1202 12:47:04 29486 server.go] HTTP Error: statusCode=500 VM response data is nil\n. PS,  qemu-x86_64 version is 2.4.1,  compile & install form source code.\n. 1, yes, I do use hyper-0.4-2, and hyperd.service is correct.\nrpm -qa | grep hyper\nhyperstart-0.4-1.el7.centos.x86_64\nhyper-0.4-2.el7.centos.x86_64\nsystemctl enable hyperd\nln -s '/usr/lib/systemd/system/hyperd.service' '/etc/systemd/system/multi-user.target.wants/hyperd.service'\nll /etc/systemd/system/multi-user.target.wants/hyperd.service\nlrwxrwxrwx 1 root root 38 Dec  2 13:30 /etc/systemd/system/multi-user.target.wants/hyperd.service -> /usr/lib/systemd/system/hyperd.service\ncat /etc/systemd/system/multi-user.target.wants/hyperd.service\n[Unit]\nDescription=hyperd\nDocumentation=http://docs.hyper.sh\nAfter=network.target\nRequires=\n[Service]\nExecStart=/usr/bin/hyperd\nMountFlags=slave\nLimitNOFILE=1048576\nLimitNPROC=1048576\nLimitCORE=infinity\n[Install]\nWantedBy=multi-user.target\n\n2, yes, I did\n\n./configure --enable-virtfs\n. '--nondaemon'  and '--nondaemon -v=1' both work. Now 'systemctl start hyperd' succeed.\n. 'hyper run ubuntu:latest' still fails.\ncat /var/log/hyper/hyperd.INFO\nLog file created at: 2015/12/02 13:46:02\nRunning on machine: sjs_90_62\nBinary: Built with gc go1.5.1 for linux/amd64\nLog line format: [IWEF]mmdd hh:mm:ss threadid file:line] msg\n[HYPER INFO  1202 13:46:02 14374 hyperd.go] The config file is \n[HYPER INFO  1202 13:46:02 14374 docker.go] success to create docker\n[HYPER INFO  1202 13:46:02 14374 daemon.go] The config: kernel=/var/lib/hyper/kernel, initrd=/var/lib/hyper/hyper-initrd.img\n[HYPER INFO  1202 13:46:02 14374 daemon.go] The config: vbox image=\n[HYPER INFO  1202 13:46:02 14374 daemon.go] The config: bridge=, ip=\n[HYPER INFO  1202 13:46:02 14374 daemon.go] The config: bios=/var/lib/hyper/bios-qboot.bin, cbfs=/var/lib/hyper/cbfs-qboot.rom\n[HYPER INFO  1202 13:46:02 14374 driver.go] [graphdriver] trying provided driver \"devicemapper\"\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] devicemapper: driver version is 4.29.0\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Generated prefix: docker-253:3-133275\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Checking for existence of the pool 'docker-253:3-133275-pool'\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] [deviceset] constructDeviceIdMap()\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/2332d8973c9393d58c03693bb4d8ec8bd853bafda3b897d48b391a1d0ba9ffb0\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=2 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/534f866d8a2e0067d4a32a6a09e96b286d75a984c7bcfb99ce15229eaa2e9792\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=17 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/534f866d8a2e0067d4a32a6a09e96b286d75a984c7bcfb99ce15229eaa2e9792-init\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=16 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/8170e0441d8b45605ca02599abbd5f858fca2d55efdb26a68be534c39558e92b\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=15 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/8170e0441d8b45605ca02599abbd5f858fca2d55efdb26a68be534c39558e92b-init\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=14 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/8bdbe93ae1f60e1305335643a9a8d3cfc405fee855d1b880337df430b4e2b0dd\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=13 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/8bdbe93ae1f60e1305335643a9a8d3cfc405fee855d1b880337df430b4e2b0dd-init\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=12 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/a467a7c6794fd7ebd5bd0e2dcb83a656ac8302e549c4a2cc29c524aea5c5623b\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=4 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/base\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=1 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/c35890c8956d50cdc280ce805097ae580c66b5d4070c252cd68e059196e2e45e\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=7 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/c35890c8956d50cdc280ce805097ae580c66b5d4070c252cd68e059196e2e45e-init\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=6 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/c3d46de1864289532a08b5f25eac18de41e14879147bc4ac05b73843a83a1664\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=9 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/c3d46de1864289532a08b5f25eac18de41e14879147bc4ac05b73843a83a1664-init\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=8 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/ca4d7b1b9a51f72ff4da652d96943f657b4898889924ac3dae5df958dba0dc4a\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=5 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/e752cee450407c0ccec0bb611b5b6e338176f3f70188dbb47ee538ec0165af3f\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=11 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/e752cee450407c0ccec0bb611b5b6e338176f3f70188dbb47ee538ec0165af3f-init\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=10 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/ea358092da773eff1664fd484edeffb0011f26b4f1dd34ad11b73db57c91d8ae\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=3 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Loading data for file /var/lib/hyper/devicemapper/metadata/transaction-metadata\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] Added deviceId=17 to DeviceIdMap\n[HYPER INFO  1202 13:46:02 14374 deviceset.go] [deviceset] constructDeviceIdMap() END\n[HYPER INFO  1202 13:46:02 14374 daemon.go] Using graph driver devicemapper\n[HYPER INFO  1202 13:46:02 14374 daemon.go] Creating images graph\n[HYPER INFO  1202 13:46:02 14374 graph.go] Restored 5 elements\n[HYPER INFO  1202 13:46:02 14374 trusts.go] Reloaded graph with 3 grants expiring at 2017-03-22 19:04:46.713978458 +0000 UTC\n[HYPER INFO  1202 13:46:02 14374 daemon.go] Creating repository list\n[HYPER INFO  1202 13:46:02 14374 daemon.go] Loaded container 534f866d8a2e0067d4a32a6a09e96b286d75a984c7bcfb99ce15229eaa2e9792\n[HYPER INFO  1202 13:46:02 14374 daemon.go] Loaded container 8170e0441d8b45605ca02599abbd5f858fca2d55efdb26a68be534c39558e92b\n[HYPER INFO  1202 13:46:02 14374 daemon.go] Loaded container 8bdbe93ae1f60e1305335643a9a8d3cfc405fee855d1b880337df430b4e2b0dd\n[HYPER INFO  1202 13:46:02 14374 daemon.go] Loaded container c35890c8956d50cdc280ce805097ae580c66b5d4070c252cd68e059196e2e45e\n[HYPER INFO  1202 13:46:02 14374 daemon.go] Loaded container c3d46de1864289532a08b5f25eac18de41e14879147bc4ac05b73843a83a1664\n[HYPER INFO  1202 13:46:02 14374 daemon.go] Loaded container e752cee450407c0ccec0bb611b5b6e338176f3f70188dbb47ee538ec0165af3f\n[HYPER INFO  1202 13:46:02 14374 daemon.go] Loading containers: done.\n[HYPER INFO  1202 13:46:02 14374 docker.go] Daemon has completed initialization\n[HYPER WARN  1202 13:46:02 14374 hyperd.go] Driver xen is unavailable\n[HYPER INFO  1202 13:46:02 14374 hyperd.go] The hypervisor's driver is kvm\n[HYPER INFO  1202 13:46:02 14374 network_linux.go] bridge exist\n[HYPER INFO  1202 13:46:02 14374 network_linux.go] modprobe br_netfilter failed modprobe br_netfilter failed\n[HYPER INFO  1202 13:46:02 14374 hyperd.go] Hyper daemon: 0.4.0 0\n[HYPER INFO  1202 13:46:02 14374 job.go] +job acceptconnections()\n[HYPER INFO  1202 13:46:02 14374 job.go] -job acceptconnections() OK\n[HYPER INFO  1202 13:46:02 14374 hyperd.go] Daemon has completed initialization\n[HYPER INFO  1202 13:46:02 14374 daemon.go] c35890c8956d50cdc280ce805097ae580c66b5d4070c252cd68e059196e2e45e\n[HYPER INFO  1202 13:46:02 14374 daemon.go] 534f866d8a2e0067d4a32a6a09e96b286d75a984c7bcfb99ce15229eaa2e9792\n[HYPER INFO  1202 13:46:02 14374 daemon.go] 8170e0441d8b45605ca02599abbd5f858fca2d55efdb26a68be534c39558e92b\n[HYPER INFO  1202 13:46:02 14374 daemon.go] c3d46de1864289532a08b5f25eac18de41e14879147bc4ac05b73843a83a1664\n[HYPER INFO  1202 13:46:02 14374 daemon.go] 8bdbe93ae1f60e1305335643a9a8d3cfc405fee855d1b880337df430b4e2b0dd\n[HYPER INFO  1202 13:46:02 14374 daemon.go] e752cee450407c0ccec0bb611b5b6e338176f3f70188dbb47ee538ec0165af3f\n[HYPER INFO  1202 13:46:02 14374 daemon.go] Get the pod item, pod is pod-pod-YhVypfByID!\n[HYPER INFO  1202 13:46:02 14374 job.go] +job serveapi(unix:///var/run/hyper.sock)\n[HYPER INFO  1202 13:46:02 14374 daemon.go] Get the pod item, pod is pod-pod-ZvqiWkpnWo!\n[HYPER INFO  1202 13:46:02 14374 server.go] Listening for HTTP on unix (/var/run/hyper.sock)\n[HYPER INFO  1202 13:46:02 14374 daemon.go] Get the pod item, pod is pod-pod-hGOFUQfogE!\n[HYPER INFO  1202 13:46:02 14374 daemon.go] Get the pod item, pod is pod-pod-kLUxapNTOm!\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering GET, /container/info\n[HYPER INFO  1202 13:46:02 14374 daemon.go] Get the pod item, pod is pod-pod-pKggcxNhCc!\n[HYPER INFO  1202 13:46:02 14374 daemon.go] Get the pod item, pod is pod-pod-vRySjkbabR!\n[HYPER INFO  1202 13:46:02 14374 pod.go] podArgs: {\"id\":\"ubuntu-latest-5152491391\",\"containers\":[{\"name\":\"ubuntu-latest-5152491391\",\"image\":\"ubuntu:latest\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1202 13:46:02 14374 container.go] ready to get the container(c35890c8956d50cdc280ce805097ae580c66b5d4070c252cd68e059196e2e45e) info\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering GET, /container/logs\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering GET, /info\n[HYPER INFO  1202 13:46:02 14374 pod.go] Found exist container ubuntu-latest-5152491391 (c35890c8956d50cdc280ce805097ae580c66b5d4070c252cd68e059196e2e45e), image: ubuntu:latest\n[HYPER INFO  1202 13:46:02 14374 pod.go] Process the Containers section in POD SPEC\n[HYPER INFO  1202 13:46:02 14374 pod.go] trying to init container ubuntu-latest-5152491391\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering GET, /images/get\n[HYPER INFO  1202 13:46:02 14374 daemon.go] leveldb: not found for pod-YhVypfByID\n[HYPER INFO  1202 13:46:02 14374 pod.go] podArgs: {\"id\":\"ubuntu-latest-4950975322\",\"containers\":[{\"name\":\"ubuntu-latest-4950975322\",\"image\":\"ubuntu:latest\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering GET, /list\n[HYPER INFO  1202 13:46:02 14374 container.go] ready to get the container(534f866d8a2e0067d4a32a6a09e96b286d75a984c7bcfb99ce15229eaa2e9792) info\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering GET, /pod/info\n[HYPER INFO  1202 13:46:02 14374 pod.go] Found exist container ubuntu-latest-4950975322 (534f866d8a2e0067d4a32a6a09e96b286d75a984c7bcfb99ce15229eaa2e9792), image: ubuntu:latest\n[HYPER INFO  1202 13:46:02 14374 pod.go] Process the Containers section in POD SPEC\n[HYPER INFO  1202 13:46:02 14374 pod.go] trying to init container ubuntu-latest-4950975322\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering GET, /service/list\n[HYPER INFO  1202 13:46:02 14374 daemon.go] leveldb: not found for pod-ZvqiWkpnWo\n[HYPER INFO  1202 13:46:02 14374 pod.go] podArgs: {\"id\":\"ubuntu-latest-4057894779\",\"containers\":[{\"name\":\"ubuntu-latest-4057894779\",\"image\":\"ubuntu:latest\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering GET, /version\n[HYPER INFO  1202 13:46:02 14374 container.go] ready to get the container(8170e0441d8b45605ca02599abbd5f858fca2d55efdb26a68be534c39558e92b) info\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /image/push\n[HYPER INFO  1202 13:46:02 14374 pod.go] Found exist container ubuntu-latest-4057894779 (8170e0441d8b45605ca02599abbd5f858fca2d55efdb26a68be534c39558e92b), image: ubuntu:latest\n[HYPER INFO  1202 13:46:02 14374 pod.go] Process the Containers section in POD SPEC\n[HYPER INFO  1202 13:46:02 14374 pod.go] trying to init container ubuntu-latest-4057894779\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /service/update\n[HYPER INFO  1202 13:46:02 14374 daemon.go] leveldb: not found for pod-hGOFUQfogE\n[HYPER INFO  1202 13:46:02 14374 pod.go] podArgs: {\"id\":\"ubuntu-latest-7160879927\",\"containers\":[{\"name\":\"ubuntu-latest-7160879927\",\"image\":\"ubuntu:latest\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1202 13:46:02 14374 container.go] ready to get the container(c3d46de1864289532a08b5f25eac18de41e14879147bc4ac05b73843a83a1664) info\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /tty/resize\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /container/create\n[HYPER INFO  1202 13:46:02 14374 pod.go] Found exist container ubuntu-latest-7160879927 (c3d46de1864289532a08b5f25eac18de41e14879147bc4ac05b73843a83a1664), image: ubuntu:latest\n[HYPER INFO  1202 13:46:02 14374 pod.go] Process the Containers section in POD SPEC\n[HYPER INFO  1202 13:46:02 14374 pod.go] trying to init container ubuntu-latest-7160879927\n[HYPER INFO  1202 13:46:02 14374 daemon.go] leveldb: not found for pod-kLUxapNTOm\n[HYPER INFO  1202 13:46:02 14374 pod.go] podArgs: {\"id\":\"ubuntu-latest-8512941493\",\"containers\":[{\"name\":\"ubuntu-latest-8512941493\",\"image\":\"ubuntu:latest\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /container/rename\n[HYPER INFO  1202 13:46:02 14374 container.go] ready to get the container(8bdbe93ae1f60e1305335643a9a8d3cfc405fee855d1b880337df430b4e2b0dd) info\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /exec\n[HYPER INFO  1202 13:46:02 14374 pod.go] Found exist container ubuntu-latest-8512941493 (8bdbe93ae1f60e1305335643a9a8d3cfc405fee855d1b880337df430b4e2b0dd), image: ubuntu:latest\n[HYPER INFO  1202 13:46:02 14374 pod.go] Process the Containers section in POD SPEC\n[HYPER INFO  1202 13:46:02 14374 pod.go] trying to init container ubuntu-latest-8512941493\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /pod/stop\n[HYPER INFO  1202 13:46:02 14374 daemon.go] leveldb: not found for pod-pKggcxNhCc\n[HYPER INFO  1202 13:46:02 14374 pod.go] podArgs: {\"id\":\"ubuntu-latest-0413979585\",\"containers\":[{\"name\":\"ubuntu-latest-0413979585\",\"image\":\"ubuntu:latest\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /auth\n[HYPER INFO  1202 13:46:02 14374 container.go] ready to get the container(e752cee450407c0ccec0bb611b5b6e338176f3f70188dbb47ee538ec0165af3f) info\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /pod/create\n[HYPER INFO  1202 13:46:02 14374 pod.go] Found exist container ubuntu-latest-0413979585 (e752cee450407c0ccec0bb611b5b6e338176f3f70188dbb47ee538ec0165af3f), image: ubuntu:latest\n[HYPER INFO  1202 13:46:02 14374 pod.go] Process the Containers section in POD SPEC\n[HYPER INFO  1202 13:46:02 14374 pod.go] trying to init container ubuntu-latest-0413979585\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /pod/run\n[HYPER INFO  1202 13:46:02 14374 daemon.go] leveldb: not found for pod-vRySjkbabR\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /service/add\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /vm/create\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /attach\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /container/commit\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /image/create\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /image/build\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering POST, /pod/start\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering DELETE, /vm\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering DELETE, /image\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering DELETE, /pod\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering DELETE, /service\n[HYPER INFO  1202 13:46:02 14374 server.go] Registering OPTIONS, \n[HYPER INFO  1202 13:46:08 14374 server.go] Calling POST /pod/create\n[HYPER INFO  1202 13:46:08 14374 server.go] Args string is {\"id\":\"ubuntu-latest-2246151429\",\"containers\":[{\"name\":\"ubuntu-latest-2246151429\",\"image\":\"ubuntu:latest\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}, \n[HYPER INFO  1202 13:46:08 14374 job.go] +job podCreate({\"id\":\"ubuntu-latest-2246151429\",\"containers\":[{\"name\":\"ubuntu-latest-2246151429\",\"image\":\"ubuntu:latest\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}, )\n[HYPER INFO  1202 13:46:08 14374 pod.go] podArgs: {\"id\":\"ubuntu-latest-2246151429\",\"containers\":[{\"name\":\"ubuntu-latest-2246151429\",\"image\":\"ubuntu:latest\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}\n[HYPER INFO  1202 13:46:08 14374 pod.go] Process the Containers section in POD SPEC\n[HYPER INFO  1202 13:46:08 14374 pod.go] trying to init container ubuntu-latest-2246151429\n[HYPER WARN  1202 13:46:08 14374 daemon.go] IPv4 forwarding is disabled. Networking will not work\n[HYPER INFO  1202 13:46:08 14374 tags.go] LookupImage Name is ubuntu:latest\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [deviceset] AddDevice(hash=e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1-init basehash=ca4d7b1b9a51f72ff4da652d96943f657b4898889924ac3dae5df958dba0dc4a)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] registerDevice(18, e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1-init)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [deviceset] AddDevice(hash=e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1-init basehash=ca4d7b1b9a51f72ff4da652d96943f657b4898889924ac3dae5df958dba0dc4a) END\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] activateDeviceIfNeeded(e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1-init)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [deviceset] AddDevice(hash=e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1 basehash=e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1-init)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] registerDevice(19, e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [deviceset] AddDevice(hash=e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1 basehash=e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1-init) END\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [devmapper] UnmountDevice(hash=e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1-init)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [devmapper] Unmount(/var/lib/hyper/devicemapper/mnt/e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1-init)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [devmapper] Unmount done\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [devmapper] deactivateDevice(e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1-init)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [devmapper] removeDevice START(docker-253:3-133275-e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1-init)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [devmapper] removeDevice END(docker-253:3-133275-e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1-init)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [devmapper] deactivateDevice END(e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1-init)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [devmapper] UnmountDevice(hash=e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1-init) END\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] activateDeviceIfNeeded(e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [devmapper] UnmountDevice(hash=e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [devmapper] Unmount(/var/lib/hyper/devicemapper/mnt/e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [devmapper] Unmount done\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [devmapper] deactivateDevice(e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [devmapper] removeDevice START(docker-253:3-133275-e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [devmapper] removeDevice END(docker-253:3-133275-e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [devmapper] deactivateDevice END(e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1)\n[HYPER INFO  1202 13:46:08 14374 deviceset.go] [devmapper] UnmountDevice(hash=e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1) END\n[HYPER INFO  1202 13:46:08 14374 container.go] ready to get the container(e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1) info\n[HYPER INFO  1202 13:46:08 14374 job.go] -job podCreate({\"id\":\"ubuntu-latest-2246151429\",\"containers\":[{\"name\":\"ubuntu-latest-2246151429\",\"image\":\"ubuntu:latest\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}, ) OK\n[HYPER INFO  1202 13:46:08 14374 server.go] Calling POST /pod/start\n[HYPER INFO  1202 13:46:08 14374 job.go] +job podStart(pod-VkkSGowjaN, , f7l7r0bj)\n[HYPER INFO  1202 13:46:08 14374 pod.go] Pod Run with client terminal tag: f7l7r0bj\n[HYPER INFO  1202 13:46:08 14374 pod.go] pod:pod-VkkSGowjaN, vm:\n[HYPER INFO  1202 13:46:08 14374 pod.go] podArgs: \n[HYPER INFO  1202 13:46:08 14374 vm.go] The config: kernel=/var/lib/hyper/kernel, initrd=/var/lib/hyper/hyper-initrd.img\n[HYPER INFO  1202 13:46:08 14374 container.go] ready to get the container(e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1) info\n[HYPER INFO  1202 13:46:08 14374 qemu_process.go] cmdline arguments: -machine pc-i440fx-2.0,accel=kvm,usb=off -global kvm-pit.lost_tick_policy=discard -cpu host -drive if=pflash,file=/var/lib/hyper/bios-qboot.bin,readonly=on -drive if=pflash,file=/var/lib/hyper/cbfs-qboot.rom,readonly=on -realtime mlock=off -no-user-config -nodefaults -no-hpet -rtc base=utc,driftfix=slew -no-reboot -display none -boot strict=on -m 128 -smp 1 -qmp unix:/var/run/hyper/vm-hnDZCFUNVu/qmp.sock,server,nowait -serial unix:/var/run/hyper/vm-hnDZCFUNVu/console.sock,server,nowait -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x2 -device virtio-scsi-pci,id=scsi0,bus=pci.0,addr=0x3 -chardev socket,id=charch0,path=/var/run/hyper/vm-hnDZCFUNVu/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0 -chardev socket,id=charch1,path=/var/run/hyper/vm-hnDZCFUNVu/tty.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1 -fsdev local,id=virtio9p,path=/var/run/hyper/vm-hnDZCFUNVu/share_dir,security_model=none -device virtio-9p-pci,fsdev=virtio9p,mount_tag=share_dir\n[HYPER INFO  1202 13:46:08 14374 qemu_process.go] starting daemon with pid: 15881\n[HYPER INFO  1202 13:46:08 14374 pod.go] Container Info is \n&{e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1 /rootfs /dev/mapper/docker-253:3-133275-e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1 ext4  [] [/bin/bash] map[]}\n[HYPER INFO  1202 13:46:08 14374 pod.go] configuring log driver [json-file] for pod-VkkSGowjaN\n[HYPER INFO  1202 13:46:08 14374 pod.go] configure container log to /var/run/hyper/Pods/pod-VkkSGowjaN/e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1-json.log\n[HYPER INFO  1202 13:46:08 14374 pod.go] configured logger for pod-VkkSGowjaN/e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1 (/ubuntu-latest-2246151429)\n[HYPER INFO  1202 13:46:08 14374 pod.go] Attach client f7l7r0bj before start pod\n[HYPER INFO  1202 13:46:08 14374 hypervisor.go] main event loop got message 32(COMMAND_ATTACH)\n[HYPER INFO  1202 13:46:08 14374 vm_states.go] attachment log-y5tkpdkw is pending\n[HYPER INFO  1202 13:46:08 14374 hypervisor.go] main event loop got message 32(COMMAND_ATTACH)\n[HYPER INFO  1202 13:46:08 14374 vm_states.go] attachment f7l7r0bj is pending\n[HYPER INFO  1202 13:46:08 14374 hypervisor.go] main event loop got message 22(COMMAND_RUN_POD)\n[HYPER INFO  1202 13:46:08 14374 vm_states.go] got spec, prepare devices\n[HYPER INFO  1202 13:46:08 14374 context.go] found container e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1 at 0\n[HYPER INFO  1202 13:46:08 14374 vm_states.go] attach pending client log-y5tkpdkw for e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1\n[HYPER INFO  1202 13:46:08 14374 vm_states.go] Connecting tty for e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1 on session 1\n[HYPER INFO  1202 13:46:08 14374 context.go] found container e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1 at 0\n[HYPER INFO  1202 13:46:08 14374 vm.go] hyperHandlePodEvent pod pod-VkkSGowjaN, vm vm-hnDZCFUNVu\n[HYPER INFO  1202 13:46:08 14374 vm_states.go] attach pending client f7l7r0bj for e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1\n[HYPER INFO  1202 13:46:08 14374 vm_states.go] Connecting tty for e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1 on session 1\n[HYPER INFO  1202 13:46:08 14374 context.go] VM vm-hnDZCFUNVu: state change from  to 'STARTING'\n[HYPER INFO  1202 13:46:08 14374 qmp_handler.go] got new session during initializing\n[HYPER INFO  1202 13:46:08 14374 hypervisor.go] main event loop got message 13(EVENT_INTERFACE_ADD)\n[HYPER INFO  1202 13:46:08 14374 qmp_wrapper.go] send net to qemu at 20\n[HYPER INFO  1202 13:46:08 14374 qmp_handler.go] got new session during initializing\n[HYPER WARN  1202 13:46:18 14374 qmp_handler.go] Initializer Timeout.\n[HYPER ERROR 1202 13:46:18 14374 qmp_handler.go] QMP initialize timeout\n[HYPER INFO  1202 13:46:18 14374 hypervisor.go] main event loop got message 36(ERROR_INIT_FAIL)\n[HYPER ERROR 1202 13:46:18 14374 vm_states.go] QMP Init timeout\n[HYPER ERROR 1202 13:46:18 14374 vm_states.go] Shutting down because of an exception: Fail during init pod running environment\n[HYPER INFO  1202 13:46:18 14374 context.go] VM vm-hnDZCFUNVu: state change from STARTING to 'TERMINATING'\n[HYPER INFO  1202 13:46:18 14374 vm.go] Get the response from VM, VM id is vm-hnDZCFUNVu!\n[HYPER INFO  1202 13:46:18 14374 hypervisor.go] main event loop got message 25(COMMAND_SHUTDOWN)\n[HYPER INFO  1202 13:46:18 14374 vm.go] Got response: 15: unexpected event during terminating\n[HYPER ERROR 1202 13:46:18 14374 init_comm.go] Cannot connect to hyper socket dial unix /var/run/hyper/vm-hnDZCFUNVu/hyper.sock: connect: connection refused\n[HYPER ERROR 1202 13:46:18 14374 init_comm.go] failed to connected to /var/run/hyper/vm-hnDZCFUNVu/console.sock dial unix /var/run/hyper/vm-hnDZCFUNVu/console.sock: connect: no such file or directory\n[HYPER INFO  1202 13:46:18 14374 hypervisor.go] main event loop got message 36(ERROR_INIT_FAIL)\n[HYPER ERROR 1202 13:46:18 14374 tty.go] Cannot connect to tty socket dial unix /var/run/hyper/vm-hnDZCFUNVu/tty.sock: connect: connection refused\n[HYPER WARN  1202 13:46:18 14374 vm_states.go] got unexpected event during terminating\n[HYPER INFO  1202 13:46:18 14374 hypervisor.go] main event loop got message 36(ERROR_INIT_FAIL)\n[HYPER WARN  1202 13:46:18 14374 vm_states.go] got unexpected event during terminating\n[HYPER ERROR 1202 13:46:18 14374 qmp_handler.go] failed to connected to /var/run/hyper/vm-hnDZCFUNVu/qmp.sock dial unix /var/run/hyper/vm-hnDZCFUNVu/qmp.sock: connect: connection refused\n[HYPER INFO  1202 13:46:28 14374 hypervisor.go] main event loop got message 3(EVENT_VM_TIMEOUT)\n[HYPER WARN  1202 13:46:28 14374 vm_states.go] VM did not exit in time, try to stop it\n[HYPER ERROR 1202 13:46:28 14374 vm_states.go] Shutting down because of an exception: vm terminating timeout\n[HYPER INFO  1202 13:46:28 14374 vm.go] Got response: 7: vm terminating timeout\n[HYPER INFO  1202 13:46:38 14374 qemu_process.go] kill Qemu... 15881\n[HYPER INFO  1202 13:46:38 14374 hypervisor.go] main event loop got message 2(EVENT_VM_KILL)\n[HYPER INFO  1202 13:46:38 14374 vm_states.go] Got VM force killed message, go to cleaning up\n[HYPER INFO  1202 13:46:38 14374 vm_states.go] VM has exit...\n[HYPER INFO  1202 13:46:38 14374 devicemap.go] need remove dm file/dev/mapper/docker-253:3-133275-e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1\n[HYPER INFO  1202 13:46:38 14374 devicemap.go] remove network card 0: 192.168.123.2\n[HYPER INFO  1202 13:46:38 14374 context.go] VM vm-hnDZCFUNVu: state change from TERMINATING to 'DESTROYING'\n[HYPER INFO  1202 13:46:38 14374 vm.go] Got response: 2: VM shut down\n[HYPER ERROR 1202 13:46:38 14374 pod.go] VM response data is nil\n[HYPER INFO  1202 13:46:38 14374 job.go] -job podStart(pod-VkkSGowjaN, , f7l7r0bj) ERR: VM response data is nil\n[HYPER ERROR 1202 13:46:38 14374 server.go] Handler for POST /pod/start returned error: VM response data is nil\n[HYPER ERROR 1202 13:46:38 14374 server.go] HTTP Error: statusCode=500 VM response data is nil\n[HYPER INFO  1202 13:46:38 14374 tty.go] a stdin closed, EOF\n[HYPER INFO  1202 13:46:38 14374 tty.go] Close tty f7l7r0bj\n[HYPER INFO  1202 13:46:38 14374 hypervisor.go] main event loop got message 14(EVENT_INTERFACE_DELETE)\n[HYPER INFO  1202 13:46:38 14374 devicemap.go] interface 0 released\n[HYPER INFO  1202 13:46:38 14374 vm_states.go] Unplug interface return with true\n[HYPER INFO  1202 13:46:38 14374 hypervisor.go] main event loop got message 9(EVENT_VOLUME_DELETE)\n[HYPER INFO  1202 13:46:38 14374 devicemap.go] blockdev /dev/mapper/docker-253:3-133275-e5db785cd9fb823b175165386360cda344b845ad960e4502ca3c4258a53e7ee1 deleted\n[HYPER INFO  1202 13:46:38 14374 vm_states.go] release volume return with true\n. > # cat /var/log/hyper/hyperd.ERROR \nLog file created at: 2015/12/02 13:46:18\nRunning on machine: sjs_90_62\nBinary: Built with gc go1.5.1 for linux/amd64\nLog line format: [IWEF]mmdd hh:mm:ss threadid file:line] msg\n[HYPER ERROR 1202 13:46:18 14374 qmp_handler.go] QMP initialize timeout\n[HYPER ERROR 1202 13:46:18 14374 vm_states.go] QMP Init timeout\n[HYPER ERROR 1202 13:46:18 14374 vm_states.go] Shutting down because of an exception: Fail during init pod running environment\n[HYPER ERROR 1202 13:46:18 14374 init_comm.go] Cannot connect to hyper socket dial unix /var/run/hyper/vm-hnDZCFUNVu/hyper.sock: connect: connection refused\n[HYPER ERROR 1202 13:46:18 14374 init_comm.go] failed to connected to /var/run/hyper/vm-hnDZCFUNVu/console.sock dial unix /var/run/hyper/vm-hnDZCFUNVu/console.sock: connect: no such file or directory\n[HYPER ERROR 1202 13:46:18 14374 tty.go] Cannot connect to tty socket dial unix /var/run/hyper/vm-hnDZCFUNVu/tty.sock: connect: connection refused\n[HYPER ERROR 1202 13:46:18 14374 qmp_handler.go] failed to connected to /var/run/hyper/vm-hnDZCFUNVu/qmp.sock dial unix /var/run/hyper/vm-hnDZCFUNVu/qmp.sock: connect: connection refused\n[HYPER ERROR 1202 13:46:28 14374 vm_states.go] Shutting down because of an exception: vm terminating timeout\n[HYPER ERROR 1202 13:46:38 14374 pod.go] VM response data is nil\n[HYPER ERROR 1202 13:46:38 14374 server.go] Handler for POST /pod/start returned error: VM response data is nil\n[HYPER ERROR 1202 13:46:38 14374 server.go] HTTP Error: statusCode=500 VM response data is nil\n. reboot dosen't help...\n\nanyhow, thanks for the anwser, I'll try again.\n. tried, and still failed...\nreplace qemu 2.4.1 by 2.3.1,  problem remains.\nNever mind. I intend to install a fedora 22 on my laptop later  :)\n. Expecting for that!  Thank you~\n. ",
    "jzarzuela": "Hi,\nI have version 10.11.2. So maybe that's why it doesn't work properly.\nThanks.\n. ",
    "mindscratch": "@gnawux I'm also on on Mac OS 10.11.2, looking forward to trying hyper.sh on it.\n. Looks like hyper.sh doesn't work with Mac OS 10.11.2 just yet.\n. ",
    "xlgao-zju": "Yes, we use etcd to manage the IP address in live migration. And I think we can close this issue.\n. I will take GET, /info for practice.\n. ping @carmark \n. OK, I will do it later.\n. @laijs updated\n. @laijs OK, I will do it.\n. @laijs code is updated\n. Code updated.\n. @resouer will do it after I finish the work about jenkins and frakti.\n. I update this pr. And we can create deb pkg by running make-deb.sh.\n. @laijs updated\n. Add other files to create hyperstart deb pkg. By running make-deb.sh, we can create hypercontainer pkg and hyperstart pkg. \nFIY: Run make-hyperstart-deb.sh for hyperstart deb pkg. And make-hypercontainer-deb.sh for hypercontainer deb pkg.\ncc @feiskyer \n. @feiskyer updated.\n. @feiskyer @gnawux updated.\n. updated\n. @gnawux OK, I will do it in another pr.\n. @gnawux will do.\n. @gnawux updated.\n. @feiskyer updated. @gnawux I can't see any container, after I issue hyperctl list container. And I tried several times, I got different containerIDs.. @gnawux I just tried other image@digest, it worked fine... So it maybe the issue of cleanup... :+1: So, should I close this issue?. this one is int64\n. ACK. ",
    "linfan": "Hyper version 0.4\n$ hyper version\nThe hyper version is 0.4.0\nQemu version 2.5.0\n$ qemu-system-x86_64 --version\nQEMU emulator version 2.5.0, Copyright (c) 2003-2008 Fabrice Bellard\nKernel version 3.10\n$ uname --kernel-release\n3.10.0-229.14.1.el7.x86_64\n. Have installed all 3 rpm packages, same issue still exist.\n```\n$ rpm -qa | grep hyper\nhyper-0.4-2.el7.centos.x86_64\nqemu-hyper-2.4.1-1.el7.centos.x86_64\nhyperstart-0.4-1.el7.centos.x86_64\n$ sudo systemctl daemon-reload\n$ sudo systemctl restart hyperd\n$ hyper run -dt busybox\nPOD id is pod-KBtlWvoJun\nhyper ERROR: Error from daemon's response: VM response data is nil\n```\nI'm trying recompile qemu 2.5.0 with --enable-virtfs flag, thanks for the hint.\n. That's a good catch !\nI actually forgot it and /usr/local/bin do have higher priority than /usr/bin !!\nThough it's still not the root cause:\n```\n$ sudo rm /usr/local/bin/qemu-\n$ sudo rm /usr/local/bin/hyper\n$ ls /usr/local/bin/\nivshmem-client  ivshmem-server\n$ hyper run -dt busybox\nPOD id is pod-UQesGhlqRN\nhyper ERROR: Error from daemon's response: VM response data is nil\n```\nRecompile qemu 2.5.0 still on going...\nCould it because the machine too small and vm creation take too long time? I'm running a t2.micro instance in AWS.\n. Have replaced the qemu with 2.5.0 (compiled with --enable-virtfs flag) and it works. AWS instance type is not a issue.\nMany thanks.\n. ",
    "robrotheram": "Hi @carmark Just a note, the config file permission was already set for root/root but what I changed was the user and group of the  QEMU processes inside the config file to root and it works thanks for your help\n. Just a note of working through the problem the main issue seems to be a permission issue. Hyper creates the vm under root as we can see from the following files:\nroot# ll /var/run/hyper/vm-VrJnrGviwn\ntotal 0\nsrwxrwxr-x 1 root root  0 Feb 18 09:05 console.sock\nsrwxrwxr-x 1 root root  0 Feb 18 09:05 hyper.sock\ndrwxr-xr-x 3 root root 80 Feb 18 09:05 share_dir\nsrwxrwxr-x 1 root root  0 Feb 18 09:05 tty.sock\nroot#\nThis causes qemu process to fail since the qemu process  it can not access them under default config settings, changing the config to set the qemu process to be root allows it to work. Is there any config setting to change the vm permissions so that the qemu process does not need to be root and be back to qemu user. \n. ",
    "kaoet": "+1 for the permission issue when using libvirt. Changing /etc/libvirt/qemu.conf makes it work.\nBut please fix this issue as running as root user is not the default configuration of libvirt. It also seems to be insecure to run libvirt as root.. ",
    "bergwolf": "LGTM!\n. LGTM. Thanks for the patch!\n. build fails because this needs https://github.com/hyperhq/runv/pull/232\ndaemon/pod.go:1234: undefined: \"github.com/hyperhq/runv/hypervisor/types\".E_CONTAINER_FINISHED\n. updated to move install and setup scripts into Dockerfile, as requested by @laijs \n. hyperhq/hyperd builder moved to https://github.com/hyperhq/official-images/pull/1\n. travis failed because this needs hyperhq/runv#269\n. Looks like https://github.com/hyperhq/hyperstart/issues/74\nAre you using a guest kernel with https://github.com/hyperhq/hyperstart/blob/master/build/kernel_patch/0001-HACK-9P-always-use-cached-inode-to-fill-in-v9fs_vfs_.patch ?\nAn easy way to verify is to run the test program mentioned in https://github.com/hyperhq/hyperstart/issues/74#issuecomment-214393486 inside your ubuntu container.\n. LGTM!\n. Looks ok on master now.\n[hypervsock@~]$sudo hyperctl run -d busybox\nPOD id is busybox-2567104319\nTime to run a POD is 4531 ms\n[hypervsock@~]$sudo hyperctl exec busybox-2567104319 ps aux\nPID   USER     TIME   COMMAND\n    1 root       0:00 /init\n    3 root       0:00 sh\n    4 root       0:00 ps aux\n[hypervsock@~]$pa|grep qemu\nroot      92814 26.8  7.1 553860 128072 ?       Sl   11:18   0:04 /usr/bin/qemu-system-x86_64 -name vm-EUsGoyVmkR -S -machine pc-i440fx-2.0,accel=kvm,usb=off -cpu host -m 128 -realtime mlock=off -smp 1,sockets=1,cores=1,threads=1 -uuid 229609d4-aac2-425d-affe-dadf534423b7 -nographic -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/domain-vm-EUsGoyVmkR/monitor.sock,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-reboot -boot strict=on -kernel /var/lib/hyper/kernel -initrd /var/lib/hyper/hyper-initrd.img -append console=ttyS0 panic=1 no_timer_check -device virtio-scsi-pci,id=scsi0,bus=pci.0,addr=0x3 -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x2 -fsdev local,security_model=none,id=fsdev-fs0,path=/var/run/hyper/vm-EUsGoyVmkR/share_dir -device virtio-9p-pci,id=fs0,fsdev=fsdev-fs0,mount_tag=share_dir,bus=pci.0,addr=0x4 -chardev socket,id=charserial0,path=/var/run/hyper/vm-EUsGoyVmkR/console.sock,server,nowait -device isa-serial,chardev=charserial0,id=serial0 -chardev socket,id=charchannel0,path=/var/run/hyper/vm-EUsGoyVmkR/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charchannel0,id=channel0,name=sh.hyper.channel.0 -chardev socket,id=charchannel1,path=/var/run/hyper/vm-EUsGoyVmkR/tty.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charchannel1,id=channel1,name=sh.hyper.channel.1 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -device vhost-vsock-pci,id=vsock0,bus=pci.0,addr=6,guest-cid=1024 -msg timestamp=on\nbergwolf  92918  0.0  0.1 112656  2316 pts/1    S+   11:18   0:00 grep --color=auto qemu\n[hypervsock@~]$pa|grep hyperd\nroot      92740  2.4  2.9 680356 52272 ?        Ssl  11:18   0:00 /usr/bin/hyperd --nondaemon --v=3 --log_dir=/var/log/hyper --registry_mirror=http://883cbacd.m.daocloud.io\nbergwolf  92941  0.0  0.1 112660  2324 pts/1    S+   11:18   0:00 grep --color=auto hyperd\n[hypervsock@~]$sudo kill -HUP 92740\n[hypervsock@~]$sudo service hyperd start\nRedirecting to /bin/systemctl start  hyperd.service\n[hypervsock@~]$sudo hyperctl list\nPOD ID               POD Name             VM name             Status\nbusybox-2567104319   busybox-2567104319   vm-EUsGoyVmkR       running\n[hypervsock@~]$sudo hyperctl exec busybox-2567104319 ps aux\nPID   USER     TIME   COMMAND\n    1 root       0:00 /init\n    3 root       0:00 sh\n    5 root       0:00 ps aux. retest this please @hykins. retest this please @hykins. @gao-feng updated and added ci case.. travis (https://travis-ci.org/hyperhq/hyperd/builds/204104859) shows #1196 passed but somehow the result was not showed here.. It is fs mapping per log entry volume (fs mapping) default-token-nhwpx_aa209b8e is ready\nHowever, I've tried following pod file and fs mapping for both file and dir are working:\n[hypervsock@examples]$cat file-mapping.pod\n{\n        \"containers\" : [{\n            \"image\": \"busybox\",\n            \"volumes\": [{\n                \"volume\": \"resolv.conf\",\n                \"path\": \"/etc/resolv.conf\",\n                \"readOnly\": false\n             },{\n                \"volume\": \"tmp\",\n                \"path\": \"/mnt/tmp\",\n                \"readOnly\": true\n             }]\n        }],\n        \"resource\": {\n            \"vcpu\": 1,\n            \"memory\": 256\n        },\n        \"files\": [],\n        \"volumes\": [{\n            \"name\": \"resolv.conf\",\n            \"source\": \"/etc/resolv.conf\",\n            \"format\": \"vfs\"\n        }, {\n            \"name\": \"tmp\",\n            \"source\": \"/tmp\",\n            \"format\": \"vfs\"\n        }],\n        \"tty\": true\n}\n```\n[hypervsock@examples]$sudo hyperctl run -p file-mapping.pod\nPOD id is pod-rDTcOQXEoq\nTime to run a POD is 5667 ms\n[hypervsock@examples]$sudo hyperctl exec -t pod-rDTcOQXEoq sh\n/ # df\nFilesystem           1K-blocks      Used Available Use% Mounted on\n/dev/sda              10474496     34908  10439588   0% /\ndevtmpfs                120484         0    120484   0% /dev\ntmpfs                   124748         0    124748   0% /dev/shm\nshare_dir             18307072  17153500   1153572  94% /etc/resolv.conf\nshare_dir             18307072  17153500   1153572  94% /mnt/tmp\nshare_dir                 1024         4      1020   0% /etc/hosts\n/ # ls /mnt/tmp/\ngit-static\ngo-build854471308\nsystemd-private-123d094739d54ab1aed99d3d0b9590af-systemd-machined.service-hPzwJy\nvWInh5u\n/ # cat /etc/resolv.conf\nGenerated by NetworkManager\nnameserver 192.168.214.2\n```\nWe need hyperd log to see what failed for fs mapping, because currently hyperstart ignores fs mapping failures when setting up containers (yeah, it should be fixed)... @feiskyer please save hyperd log when seeing this next time.\n. @feiskyer https://github.com/hyperhq/hyperstart/pull/271. lgtm!. lgtm!. hyperstart is asked to find user nobody:nobody\ntry to find the user: nobody:nobody\nhowever it should be user nobody and group nobody.\nspec passed to hyperd is\n\"process\":{\"id\":\"init\",\"user\":\"nobody:nobody\",\"terminal\":false,\"stdio\":1,\"stderr\":2,\"args\":[\"/sidecar\"],\"envs\":[{\"env\":\"PATH\",\"value\":\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"}],\"workdir\":\"/\"}\nit should contain \"user\":\"nobody\",\"group\":\"nobody\" instead.. @gnawux I can handle this one. docker config user field supports:\n--user=[ user | user:group | uid | uid:gid | user:gid | uid:group ]. I'll add regression for both when fixing https://github.com/hyperhq/hyperd/issues/548. It seems we have lost user group config specified in podfile. I'll add more fix here.\n[hypervsock@examples]$sudo hyperctl run -p user.pod\nPOD id is pod-YjSjNvpSAx\nTime to run a POD is 4537 ms\n[hypervsock@examples]$sudo hyperctl exec pod-YjSjNvpSAx ps\nPID   USER     TIME   COMMAND\n    1 root       0:00 /init\n    3 root       0:00 sh\n    4 root       0:00 ps\n[hypervsock@examples]$cat user.pod\n{\n        \"containers\" : [{\n            \"image\": \"busybox\",\n            \"user\": {\n                \"name\": \"nobody\",\n                \"group\": \"nobody\"\n            },\n            \"command\": [\"sh\"]\n        }]\n}. integration test timed out on aufs and overlay. @gnawux thx! I've updated the PR to cover #490 case.\n```\n[hypervsock@~]$sudo hyperctl run -p mysql.pod\nPOD id is pod-JPRHxqCRNZ\nTime to run a POD is 9159 ms\n[hypervsock@~]$sudo hyperctl exec pod-JPRHxqCRNZ df\nFilesystem     1K-blocks   Used Available Use% Mounted on\n/dev/sdb        10474496 446272  10028224   5% /\ndevtmpfs          119900      0    119900   0% /dev\ntmpfs             124748      0    124748   0% /dev/shm\nrootfs            119888  20248     99640  17% /lib/modules/4.9.11-hyper\n/dev/sda         1998672 130424   1747008   7% /var/lib/mysql\nshare_dir           1024      4      1020   1% /etc/hosts\n[hypervsock@~]$sudo hyperctl exec pod-JPRHxqCRNZ cat /etc/hosts\n127.0.0.1       localhost\n::1     localhost ip6-localhost ip6-loopback\nfe00::0 ip6-localnet\nff00::0 ip6-mcastprefix\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n. will do. It is random. Only saw it once. May relate to vm shutdown timing.. we would never read with zero length buffer:\n        buf := make([]byte, 512)\n        res := []byte{}\n        for read < needRead {\n                want := needRead - read\n                if want > 512 {\n                        want = 512\n                }\n                nr, err := conn.Read(buf[:want])\n``. libvirt ended up using qemu command line:-chardev socket,id=charchannel0,path=/var/run/hyper/vm-vBrtiilBzU/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charchannel0,id=channel0,name=sh.hyper.channel.0`\nwhile qemu driver uses:\n-chardev socket,id=charch0,path=/var/run/hyper/vm-oXLmDUBhSp/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0. full command lines comparison. \nlibvirt:\n/usr/bin/qemu-system-x86_64 -name vm-lWRHQAOeCW -S -machine pc-i440fx-2.0,accel=tcg,usb=off -cpu Broadwell,+abm,+pdpe1gb,+hypervisor,+rdrand,+f16c,+osxsave,+ss,+vme -m 128 -realtime mlock=off -smp 1,sockets=1,cores=1,threads=1 -uuid f2728ef7-a964-47ce-8f8e-e45703988361 -nographic -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/domain-vm-lWRHQAOeCW/monitor.sock,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-reboot -boot strict=on -kernel /var/lib/hyper/kernel -initrd /var/lib/hyper/hyper-initrd.img -append console=ttyS0 panic=1 no_timer_check -device virtio-scsi-pci,id=scsi0,bus=pci.0,addr=0x3 -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x2 -fsdev local,security_model=none,id=fsdev-fs0,path=/var/run/hyper/vm-lWRHQAOeCW/share_dir -device virtio-9p-pci,id=fs0,fsdev=fsdev-fs0,mount_tag=share_dir,bus=pci.0,addr=0x4 -chardev socket,id=charserial0,path=/var/run/hyper/vm-lWRHQAOeCW/console.sock,server,nowait -device isa-serial,chardev=charserial0,id=serial0 -chardev socket,id=charchannel0,path=/var/run/hyper/vm-lWRHQAOeCW/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charchannel0,id=channel0,name=sh.hyper.channel.0 -chardev socket,id=charchannel1,path=/var/run/hyper/vm-lWRHQAOeCW/tty.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charchannel1,id=channel1,name=sh.hyper.channel.1 -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 -msg timestamp=on\nqemu:\n/bin/qemu-system-x86_64 -machine pc-i440fx-2.0,usb=off -cpu core2duo -kernel /var/lib/hyper/kernel -initrd /var/lib/hyper/hyper-initrd.img -append console=ttyS0 panic=1 no_timer_check -realtime mlock=off -no-user-config -nodefaults -no-hpet -rtc base=utc,driftfix=slew -no-reboot -display none -boot strict=on -m 128 -smp 1 -qmp unix:/var/run/hyper/vm-oXLmDUBhSp/qmp.sock,server,nowait -serial unix:/var/run/hyper/vm-oXLmDUBhSp/console.sock,server,nowait -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x2 -device virtio-scsi-pci,id=scsi0,bus=pci.0,addr=0x3 -chardev socket,id=charch0,path=/var/run/hyper/vm-oXLmDUBhSp/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0 -chardev socket,id=charch1,path=/var/run/hyper/vm-oXLmDUBhSp/tty.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1 -fsdev local,id=virtio9p,path=/var/run/hyper/vm-oXLmDUBhSp/share_dir,security_model=none -device virtio-9p-pci,fsdev=virtio9p,mount_tag=share_dir -daemonize -pidfile /var/run/hyper/vm-oXLmDUBhSp/pidfile -D /var/log/hyper/qemu/vm-oXLmDUBhS.log. PRs for hyperd/runv travis are not merged yet\nhttps://github.com/hyperhq/hyperd/pull/611\nhttps://github.com/hyperhq/runv/pull/493\n. hykins/travis both use in-house built qemu now. Reopen if this is seen again.. the nfs test failure is due to guest clocksource issue fixed by https://github.com/hyperhq/runv/pull/494. I'll update runv deps to include it.. retest this please, @hykins. If we add a newline character at the end of https://github.com/feiskyer/ops/blob/master/kubernetes/examples/projected-volume.yaml#L28, e.g., - ls -laR /all && cat /all/podname && cat /all/secret-data && echo, then the file contents will be shown.\n```\n[hypervsock@kubelet]$kubectl logs test\n/all:\ntotal 0\ndrwxrwxrwt    3 root     root           120 May  5 06:32 .\ndrwxr-xr-x    3 root     root            82 May  5 06:32 ..\ndrwxr-xr-x    2 root     root            80 May  5 06:32 ..5985_05_05_14_32_45.093720545\nlrwxrwxrwx    1 root     root            31 May  5 06:32 ..data -> ..5985_05_05_14_32_45.093720545\nlrwxrwxrwx    1 root     root            14 May  5 06:32 podname -> ..data/podname\nlrwxrwxrwx    1 root     root            18 May  5 06:32 secret-data -> ..data/secret-data\n/all/..5985_05_05_14_32_45.093720545:\ntotal 8\ndrwxr-xr-x    2 root     root            80 May  5 06:32 .\ndrwxrwxrwt    3 root     root           120 May  5 06:32 ..\n-rw-r--r--    1 root     root             4 May  5 06:32 podname\n-rw-r--r--    1 root     root             5 May  5 06:32 secret-data\ntestadmin\n```\nA simple reproducer:\n[hypervsock@~]$sudo hyperctl run --rm busybox echo -n foobar\nfoobar[hypervsock@~]$\n[hypervsock@~]$sudo hyperctl run -d busybox echo -n foobar\nPOD id is busybox-9702672853\nTime to run a POD is 6017 ms\n[hypervsock@~]$sudo hyperctl logs busybox-9702672853\n[hypervsock@~]$sudo cat /var/run/hyper/Pods/busybox-9702672853/8dff3adc177161430cf1a103eb8ce21cfea813724da48b647b94b50d2c4cd16c-json.log\n[hypervsock@~]$\nIt seems the json file logger is missing container output when there is no terminating newline character.. I'm still looking for the root cause but yes it needs to be fixed.. retest this please, @hykins. I saw this once yesterday with rawblock+libvirt as well, when reproducing #557 . Root cause for the issue is that when kernel is doing the initial scsi scan and the device is being instantiated, hyperstart issues a rescan which returns immediately because the device is found in sysfs device list. However the device is still being initialized when hyperstart issues mount thus the failure.\nkernel issuing initial scsi scan:\nI0426 02:27:16.213205   22846 vm_console.go:96] SB[vm-pdLbyEynVG] [CNL] scsi 0:0:0:0: Direct-Access     QEMU     QEMU HARDDISK    2.4. PQ: 0 ANSI: 5\nhyperstart calls manual scsi scan that did not wait for initial scsi scan to finish:\nI0426 02:27:16.231822   22846 vm_console.go:96] SB[vm-pdLbyEynVG] [CNL] create directory /tmp/hyper/1416cf5306a146aaf4fd255e26d512dde0fe374ef2c92b7015902493407d3159\nI0426 02:27:16.232764   22846 vm_console.go:96] SB[vm-pdLbyEynVG] [CNL] create directory /tmp/hyper/1416cf5306a146aaf4fd255e26d512dde0fe374ef2c92b7015902493407d3159/devpts/\nI0426 02:27:16.238753   22846 vm_console.go:96] SB[vm-pdLbyEynVG] [CNL] hyper send mntns referenced event: normal\nI0426 02:27:16.243196   22846 vm_console.go:96] SB[vm-pdLbyEynVG] [CNL] create child process pid=335 in the sandbox\nI0426 02:27:16.247724   22846 vm_console.go:96] SB[vm-pdLbyEynVG] [CNL] path /sys/class/scsi_host/host0/scan\nI0426 02:27:16.456795   22846 vm_console.go:96] SB[vm-pdLbyEynVG] [CNL] finish scan scsi\nnormal scsi scan kernel message:\nscsi 0:0:0:0: Direct-Access     QEMU     QEMU HARDDISK    2.0. PQ: 0 ANSI: 5\nsd 0:0:0:0: Attached scsi generic sg0 type 0\nsd 0:0:0:0: [sda] 20971520 512-byte logical blocks: (10.7 GB/10.0 GiB)\nsd 0:0:0:0: [sda] Write Protect is off\nsd 0:0:0:0: [sda] Mode Sense: 63 00 00 08\nsd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA\nsd 0:0:0:0: [sda] Attached SCSI disk. sd_probe() does part of the probe work in scsi_sd_probe_domain, maybe we can change scsi scan code to wait for async sd probing jobs if it is a user requested scan.. The error happens when setting up container rootfs, rawblock rootfs is hot-plugged and hyperstart tries to mount it before device initialisation finishes.\nOther integration tests create containers during pod creation while TestSendContainerSignal is the only one separating pod and container creation. This seems to be the reason for its higher failure rate. When combined with pod creation, rootfs hotplug is coupled with other things like nic hotplug and configuration, so it can have more time to proceed.\ne.g., in job 384 TestSendContainerSignal, the time between root device hotplug success and INIT_NEWCONTAINER is less than 10ms, while in TestPauseAndUnpausePod, the time between root device hotplug success and INIT_NEWCONTAINER is more than 300ms.. Another incident in block IO code path, which did not result in test failure though:\nI0412 02:03:35.627608   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL] Task dump for CPU 0:\nI0412 02:03:35.632058   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL] init            R  running task        0   334      1 0x00000008\nI0412 02:03:35.636555   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  ffff880007603d60 ffffffff81075bb2 ffffffff8183e180 0000000000000087\nI0412 02:03:35.636725   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  0000000000000000 ffffffff810d9c70 ffff880007618200 ffffffff8183e180\nI0412 02:03:35.641159   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  0000000000000000 ffff880005bd7240 ffffffff8109bd2c 003b9aca00000000\nI0412 02:03:35.641212   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL] Call Trace:\nI0412 02:03:35.646191   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  <IRQ>  [<ffffffff81075bb2>] ? sched_show_task+0xd2/0x140\nI0412 02:03:35.646312   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff810d9c70>] ? rcu_dump_cpu_stacks+0x72/0xaa\nI0412 02:03:35.646417   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8109bd2c>] ? rcu_check_callbacks+0x5ec/0x730\nI0412 02:03:35.646545   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff810a4802>] ? update_wall_time+0x382/0x710\nI0412 02:03:35.646669   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8109e1f3>] ? update_process_times+0x23/0x50\nI0412 02:03:35.646779   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff810abb13>] ? tick_sched_timer+0x33/0x60\nI0412 02:03:35.646897   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8109e8e2>] ? __hrtimer_run_queues+0x92/0x100\nI0412 02:03:35.647004   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8109edd4>] ? hrtimer_interrupt+0x94/0x170\nI0412 02:03:35.647131   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff81039aa4>] ? smp_apic_timer_interrupt+0x34/0x50\nI0412 02:03:35.647248   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff814e6b27>] ? apic_timer_interrupt+0x87/0x90\nI0412 02:03:35.651879   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff81056cab>] ? __do_softirq+0x6b/0x1c0\nI0412 02:03:35.652001   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff81056ffc>] ? irq_exit+0x8c/0xa0\nI0412 02:03:35.652085   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff810241fc>] ? do_IRQ+0x4c/0xd0\nI0412 02:03:35.652221   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff814e6887>] ? common_interrupt+0x87/0x87\nI0412 02:03:35.656778   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  <EOI>  [<ffffffff8131b23d>] ? vp_notify+0xd/0x20\nI0412 02:03:35.656898   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8131780d>] ? virtqueue_notify+0xd/0x30\nI0412 02:03:35.657008   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff813874df>] ? virtscsi_kick_cmd+0x6f/0x80\nI0412 02:03:35.657132   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff81387a5b>] ? virtscsi_queuecommand+0x24b/0x2d0\nI0412 02:03:35.657251   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8137d08d>] ? scsi_dispatch_cmd+0x7d/0xd0\nI0412 02:03:35.657354   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8137fe02>] ? scsi_request_fn+0x272/0x560\nI0412 02:03:35.657467   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff81278c6a>] ? __blk_run_queue+0x2a/0x40\nI0412 02:03:35.657592   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8127fef3>] ? blk_execute_rq_nowait+0xa3/0x140\nI0412 02:03:35.657713   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8128065b>] ? blk_recount_segments+0xfb/0x180\nI0412 02:03:35.657811   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8127ffe6>] ? blk_execute_rq+0x56/0xc0\nI0412 02:03:35.657924   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff81274724>] ? bio_phys_segments+0x14/0x20\nI0412 02:03:35.658030   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8127fdcc>] ? blk_rq_map_kern+0xbc/0x140\nI0412 02:03:35.658142   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8127a6f4>] ? blk_get_request+0x74/0x100\nI0412 02:03:35.658244   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8137d1a9>] ? scsi_execute+0xc9/0x150\nI0412 02:03:35.658366   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8137d305>] ? scsi_execute_req_flags+0x85/0xf0\nI0412 02:03:35.658483   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff813819f4>] ? scsi_probe_and_add_lun+0x1e4/0xb60\nI0412 02:03:35.658597   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff81357a92>] ? dev_set_name+0x42/0x50\nI0412 02:03:35.663121   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff813826e9>] ? __scsi_scan_target+0xc9/0x490\nI0412 02:03:35.663297   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff81382c0d>] ? scsi_scan_channel.part.11+0x4d/0x70\nI0412 02:03:35.663425   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff81382d86>] ? scsi_scan_host_selected+0xc6/0x150\nI0412 02:03:35.663550   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff81384583>] ? store_scan+0xd3/0xe0\nI0412 02:03:35.663659   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff81190951>] ? kernfs_fop_write+0x101/0x190\nI0412 02:03:35.663763   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8112d59e>] ? __vfs_write+0x1e/0x120\nI0412 02:03:35.663875   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8112e1d8>] ? vfs_write+0xa8/0x1a0\nI0412 02:03:35.668358   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff8112f45d>] ? SyS_write+0x3d/0xa0\nI0412 02:03:35.668490   32374 vm_console.go:46] SB[vm-QtvdGRdmXm] [CNL]  [<ffffffff814e5efb>] ? entry_SYSCALL_64_fastpath+0x1e/0xad. All of these occurrences are accompanied with tsc clock drifting.\nclocksource: timekeeping watchdog on CPU0: Marking clocksource 'tsc' as unstable because the skew is too large:\nSo it is possible that these are not rcu_sched stalls but a clock source issue instead.. Looking at the first nfs test failure, nfs mount happened at \n12:15:59 I0409 12:15:58.848713    7385 vm_console.go:46] SB[vm-aJzUCJjJbP] [CNL] executing cmd mount.nfs4 -n 192.168.123.21:/export /tmp//export\n and was thought timeout even if less than 300 ms had passed wall-clock-wise, at \n12:15:59 I0409 12:15:59.102473    7385 vm_console.go:46] SB[vm-aJzUCJjJbP] [CNL] mount.nfs4: Connection timed out\nIn between, clocksource was switched from tsc to acpi_pm.\n12:15:59 I0409 12:15:59.052617    7385 vm_console.go:46] SB[vm-aJzUCJjJbP] [CNL] clocksource: timekeeping watchdog on CPU0: Marking clocksource 'tsc' as unstable because the skew is too large:\n12:15:59 I0409 12:15:59.055990    7385 vm_console.go:46] SB[vm-aJzUCJjJbP] [CNL] clocksource:                       'acpi_pm' wd_now: 6cc233 wd_last: 5b06f3 mask: ffffff\n12:15:59 I0409 12:15:59.059527    7385 vm_console.go:46] SB[vm-aJzUCJjJbP] [CNL] clocksource:                       'tsc' cs_now: 3a6b34e3893 cs_last: 3217d6ba8a8 mask: ffffffffffffffff\n12:15:59 I0409 12:15:59.085286    7385 vm_console.go:46] SB[vm-aJzUCJjJbP] [CNL] clocksource: Switched to clocksource acpi_pm\nWe can forcibly mark tsc clock as reliable to remove above tsc unstable warning and clock switching, but it does not affect the rcu_sched warning, because rcu kthread still thinks it takes too long to get scheduled.\nMaybe we should avoid using tsc as guest clocksource in the first place. With kvm, guest will use kvm-clock by default. W/o it, we have a few options, hpet, acpi_pm and tsc.. should be fixed by https://github.com/hyperhq/runv/pull/494. I've run nfs volume test with it for a few hundred times successfully and did not see any rcu stall warnings.. travis failed due to network issue:\nError trying v2 registry: Get https://gcr.io/v2/google_containers/k8s-dns-sidecar-amd64/blobs/sha256:fc5e302d8309a8deb3315452b428969aed4aa931189b10a3231ce48fc0687224: dial tcp 74.125.132.82:443: i/o timeout. retest this please, @hykins. hykins is still using the upstream qemu binary. @Jimmy-Xu is helping to change it.. @gao-feng updated. retest this please, @hykins. turns out it is a dup of https://github.com/hyperhq/hyperd/issues/601. fair enough, I'm dropping this. The hyperstart part needs rebase.. CI failures are because of readonly field parsing failure in hyperstart. We should merge  hyperhq/hyperstart#314 first and re-trigger CI here.. hykins was running too slow (>40 mins) and timed out. retest this please @hykins. There is an issue with aufs mount option length calculation. I've submitted https://github.com/hyperhq/hyperd/pull/642 to fix it. However, I am not sure if it is the root cause for the failure. I added some error logs so we can find out when we see the error again.. Please check your /etc/hyper/config for the Hypervisor option. Likely you are using qemu rather than libvirt, in which case virsh is not involved.. The hyeprhq/nfs-server-tester container failed to start as its hyperstart got HUP event after executing the first exec command. Then the nfs client container failed to connect to the nfs server.\n07:29:29 I0713 07:29:30.440442   10258 vm_console.go:96] SB[vm-gjFVAQMaPR] [CNL] hyper_handle_event event EPOLLHUP or EPOLLERR, he 0x1721f48, fd 12, 10\nIt does not seem like hyperd closed the connection. There might be something wrong in exec cleanup code.. Let's track the CI failure at https://github.com/hyperhq/hyperstart/issues/317. hykins is running too slow and timed out. Maybe we should raise the time limit?. LGTM!. I've bisected the failure to runv commit https://github.com/hyperhq/runv/pull/575/commits/36881991a0ccb0cd3ab9747ee749ce80dcafaeea. And the following change would fix it (https://travis-ci.org/bergwolf/hyperd/builds/287128848):\ndiff --git a/hypervisor/qemu/qemu.go b/hypervisor/qemu/qemu.go\nindex 2f8ad01..b18191c 100644\n--- a/hypervisor/qemu/qemu.go\n+++ b/hypervisor/qemu/qemu.go\n@@ -283,7 +283,7 @@ func (qc *QemuContext) AddNic(ctx *hypervisor.VmContext, host *hypervisor.HostNi\n        go func() {\n                // close tap file if necessary\n                ev, ok := <-waitChan\n-               syscall.Close(fd)\n+               glog.Infof(\"AddNic result %v %+v\", ok, ev)\n                if !ok {\n                        close(result)\n                } else {\nHowever, runv has been changed again to stop passing fd around. Yet the new runv code (after https://github.com/hyperhq/runv/pull/613) requires qemu (>2.7) to work and thus cannot run with travis ci yet.\n@gnawux @gao-feng shall we require newer qemu binary or continue supporting old ones?. FYI, travis only comes with trusty and xenial. And xenial stock qemu is v2.5. We'll need to provide newer version of qemu deb package to pass CI if we want to move pass v2.7.. @gnawux ci passed and this is based on hyperhq/runv#616 now.. ah, go_import_path should be the official way of doing the same thing. Thanks for pointing it out!. updated to use go_import_path instead.. LGTM!. OOM?\nPlease try to reproduce it with hyperctl run --memory=512. It's shown in hyperctl run --help:\n--memory=128       Memory size (MB) for the VM (default: 128). @1maginarium @linengier @joelmcdonald We have recently release v1.1.0 and it contains several bugfix on resource management. Would you please see if the problem still exists?. lgtm!. @enzian You can run docker in hyperd though you need to mount all cgroups mountpoints on your own. But after that, docker runs w/o issues.\nIMO, the main challenge of running kubernetes inside hyperd is the missing systemd. hyperstart runs as the init program in the guest and thus systemd cannot run (since it requires to be the init process as well). If you can setup kubernetes without systemd, you can run it inside hyperd.. For example this is the cgroups mountpoints in my local machine:\ntmpfs on /sys/fs/cgroup type tmpfs (ro,nosuid,nodev,noexec,mode=755)\ncgroup on /sys/fs/cgroup/unified type cgroup2 (rw,nosuid,nodev,noexec,relatime)\ncgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,xattr,name=systemd)\ncgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,devices)\ncgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,cpuset)\ncgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,pids)\ncgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,perf_event)\ncgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,hugetlb)\ncgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,cpu,cpuacct)\ncgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,blkio)\ncgroup on /sys/fs/cgroup/rdma type cgroup (rw,nosuid,nodev,noexec,relatime,rdma)\ncgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,freezer)\ncgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,net_cls,net_prio)\ncgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,memory)\nYou can find out a list of supported cgroups in /proc/cgroups and then mount them properly. I don't remember exactly what is required to run docker but all needed cgroups are already included in the hyperstart kernel image.\n```\n$cat /proc/cgroups\nsubsys_name    hierarchy       num_cgroups     enabled\ncpuset  3       2       1\ncpu     7       67      1\ncpuacct 7       67      1\nblkio   8       67      1\nmemory  12      101     1\ndevices 2       67      1\nfreezer 10      2       1\nnet_cls 11      2       1\nperf_event      5       2       1\nnet_prio        11      2       1\nhugetlb 6       2       1\npids    4       71      1\nrdma    9       1       1\n``. Did you upgrade from an older version of hyperd? You can remove/var/lib/hyper/lib/hyper.db` and try again.. Nice!. LGTM!. @kuthreecar \n\nfailed to load sandbox info: leveldb: not found key: SB-hello-world-0328860666\n\nThis is not a fatal error and is ignored during hyperd startup. Could you please post the full debug log (e.g. by starting hyperd in command line with sudo hyperd --v=9 --alsologtostderr), so that we can see what actually stopped hyperd from working? Thanks!. Updated. Also added waiting method for libvirtd initialization, since sometimes it takes longer than 5 second to start.\n. drop this line as well?. No, I don't think so. They were added by godeps but it looks to be a mistake. I removed these files but forgot to update Godeps.json. I'll drop them.. It was provided as an example of how to add an nfs volume to a pod.. Format=\"raw\" && Fstype =\"xfs\"? Also set spec.Fstype according to rawblockDriver.blockFs.. call rawblockDriver.CreateVolume() here? otherwise need mkfs?. Add a separator before containerId, like \"blocks_\"?. BTRFS_IOC_CLONE has been pulled to generic vfs layer as FICLONE, so ioctl(BTRFS_IOC_CLONE) should work for all file systems that support file clone.. d.active does not seem reliable upon daemon restart. e.g., after daemon restarts, d.active[id] is zero and damon tries to mount the device again, which will fail if a device is mounted before.. Possibly we can use xattr to save the refcount in the raw block file, and retrieve it in Get()/Put() when d.active[id] is zero.. yup.... hmm, good point! Impact of a second call to Close() is undefined. We should avoid it IMO.. They are different subvolumes. It needs to break its own chroot sandbox to access another container's rootfs, no?. Should call sb.SignalProcess() instead. Whenever we have a sandbox pointer, we should be calling sb operations directly instead of the vc APIs.. check for error?. ",
    "justin8": "ok, I'll give that a go. Thanks.\n. ",
    "zenny": "Yep, figured out that systemctl was missing. Installed in a new jessie machine and it works alright. Thanks @Jimmy-Xu\n. hi @gnawux, thanks.\nHowever when rebooted, I got an error:\nroot@hyper1:/var/lib/hyper# hyper list\nhyper ERROR: An error occurred trying to connect: Get http://%2Fvar%2Frun%2Fhyper.sock/v0.5.0/list?item=pod: dial unix /var/run/hyper.sock: connect: no such file or directory\nTo solve, I stopped the hyperd service and reinstalled. Maybe there is a lot of changes committed upstream, I guess.\n. Update: 'hyper attach' failed now:\n```\nroot@hyper1:/var/lib/hyper# hyper list\nPOD ID              POD Name                      VM name             Status\npod-iLQUZTuZtx      openresty-latest-0936741323                       pending\npod-haRlytzGwc      alpine-8038157849                                 succeeded\nroot@hyper1:/var/lib/hyper# hyper attach pod-haRlytzGwc\nhyper ERROR: Error from daemon's response: Can not find VM whose Id is !\n```\n'hyper attach --help' does not show up any extra options either.\nUPDATE: Ran another pod with 'hyper run -t alpine /bin/sh' command and then tried to attach from another terminal. This is what I got:\n```\nroot@hyper1:/var/lib/hyper# hyper list\nPOD ID              POD Name                      VM name             Status\npod-iLQUZTuZtx      openresty-latest-0936741323                       pending\npod-haRlytzGwc      alpine-8038157849                                 succeeded\npod-WEGMTEtnXY      alpine-5184284887             vm-cSywwOujqB       running\nroot@hyper1:/var/lib/hyper# hyper attach pod-WEGMTEtnXY\n/bin/sh: hy: not found\n/ #\n```\nUPDATE1: Exited from the attached pod and then tried to attach again while the terminal with 'hyper run -t alpine /bin/sh'  was still running, but it gave the following error:\n```\nroot@hyper1:/var/lib/hyper# hyper list\nPOD ID              POD Name                      VM name             Status\npod-iLQUZTuZtx      openresty-latest-0936741323                       pending\npod-haRlytzGwc      alpine-8038157849                                 succeeded\npod-WEGMTEtnXY      alpine-5184284887             vm-cSywwOujqB       running\nroot@hyper1:/var/lib/hyper# hyper attach pod-WEGMTEtnXY\n/bin/sh: hy: not found\n/ # exit\nroot@hyper1:/var/lib/hyper# hyper attach pod-WEGMTEtnXY\nhyper ERROR: Error from daemon's response: Can not find VM whose Id is !\nroot@hyper1:/var/lib/hyper# hyper list\nPOD ID              POD Name                      VM name             Status\npod-WEGMTEtnXY      alpine-5184284887                                 failed\npod-iLQUZTuZtx      openresty-latest-0936741323                       pending\npod-haRlytzGwc      alpine-8038157849                                 succeeded\n```\n. hyperctl attach does not work no matter what is used:\n```\nhyperctl list\nPOD ID              POD Name                      VM name             Status\npod-IsSuJCftUu      debian-sid-0430301703         vm-fWMrOywAst       running\n```\n```\nhyperctl attach debian-sid-0430301703\nhyperctl attach pod-IsSuJCftUu\n```\nmerely fails to do anything and even do not provide any root prompt. What a PITA?!\n. > curl -sSL https://hypercontainer.io/install | bash\nworks alright from the pre-compiled binary, but building from git breaks. The upstream changed the file location, it seems.\n. @gnawux that means the \"open source\" code over here is a crippled version which even does not address the basic requirements of containerization. The entire approach seems flawed in open source model where you do not cripple the source code nor features, but charge for a service. Thus, this does not seem to be an open source virtualization project like docker/rocket, qemu-kvm, bhyve, iocage among others. \n. > the public service and the open source project share the same code base.\n@gnawux Then why is there an omission of very essential parameters (like volume creation and destruction, fip and stats in the open source code?\nThat could be the reason there is not so much traction in this project though sounds interesting, just my guess. \nXiexie\n. @gnawux very confusing like Chinese characters because the documentation to this repo points to http://docs.hyper.sh and nowhere else. Do you mean that the documentation link is randomly selected? \nShall check hypernetes if that is so.\n. The problem with this repo is scattered bits of information over scattered sites, causing 'information pollution\". FreeBSD and ArchLinux are two best places to learn from, just my two _fen_s. :-)\nPS: Checked hypercontainer.io which is a ditto copy of hyper.sh. Old wine in a new bottle ;-)  \u65b0\u74f6\u88dd\u820a\u9152, \u65b0\u74f6\u88c5\u65e7\u9152 \u200e(x\u012bn p\u00edng zhu\u0101ng ji\u00f9 ji\u01d4)\n. However the pre-compiled binary (hyper, not hyperctl) works fine when reinstalled.\nIt could be that hyperctl searches for the hosts directory inside the pods!?\n. @laijs are you migrating to tmpfs from aufs, in that case? How will it affect the previous instances of hyper?\n. Here it comes:\n```\nls -l /var/lib/hyper/hosts/pod-BtkAFzMFqC/\ntotal 0\nls -ld /var/lib/hyper/hosts/pod-BtkAFzMFqC/\ndrwxr-xr-x 2 root root 4096 Apr 12 16:05 /var/lib/hyper/hosts/pod-BtkAFzMFqC/\n```\n. > the new hyperd creates /var/lib/hyper/hosts/pod-BtkAFzMFqC/ via tmpfs and causes the problem. we will fix it later.\nIs there a tentative roadmap when this aspect would be addressed? Is there a bugtracker like mantis related to this project?\n. @laijs Pulled from the latest commit of hyper including runv just a while ago, compiled, but hyperd fails to run:\n```\n/etc/init.d/hyperd start\n/etc/init.d/hyperd status\n```\nLog reports nothing except the following four lines:\n```\ncat /var/log/hyper/hyperd.ERROR\nLog file created at: 2016/04/14 08:50:12\nRunning on machine: MACHINE-GA-970A-D3\nBinary: Built with gc go1.6 for linux/amd64\nLog line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\n``\n. @laijs it is where the hyperd is located in the debian based distros and alpine linux, too which is equivalent to 'service hyperd start/stop/restart' or 'systemctl start/stop hyperd.service`' in systemd, fyi.\nOutput of direct execution of hyperd in nondaemon mode with 3 levels of verbosity:\n```\nhyperd --nondaemon -v=3\n3\n\u001bM\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\b\b\b\b\b\b\b\b\bndaemon -v=3\u001bM\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\n\u001b[C\u001b[C\nI0414 10:55:50.165991   17471 hyperd.go:108] The config file is \nI0414 10:55:50.166592   17471 daemon.go:137] The config: kernel=/mnt/storage/HYPER/kernel, initrd=/mnt/storage/HYPER/hyper-initrd.img\nI0414 10:55:50.166626   17471 daemon.go:139] The config: vbox image=\nI0414 10:55:50.166647   17471 daemon.go:142] The config: bridge=, ip=\nI0414 10:55:50.166666   17471 daemon.go:145] The config: bios=/mnt/storage/HYPER/bios-qboot.bin, cbfs=/mnt/storage/HYPER/cbfs-qboot.rom\n\u001b[37mDEBU\u001b[0m[0000] Using default logging driver none          \n\u001b[34mINFO\u001b[0m[0000] [graphdriver] using prior storage driver \"aufs\" \n\u001b[37mDEBU\u001b[0m[0000] Using graph driver aufs                    \n\u001b[34mINFO\u001b[0m[0000] Graph migration to content-addressability took 0.00 seconds \n\u001b[37mDEBU\u001b[0m[0000] Option DefaultDriver: bridge               \n\u001b[37mDEBU\u001b[0m[0000] Option DefaultNetwork: bridge              \n\u001b[34mINFO\u001b[0m[0000] Firewalld running: false                   \n\u001b[37mDEBU\u001b[0m[0000] Registering ipam driver: \"default\"         \n\u001b[33mWARN\u001b[0m[0000] Your kernel does not support swap memory limit. \n\u001b[37mDEBU\u001b[0m[0000] Cleaning up old shm/mqueue mounts: start.  \n\u001b[37mDEBU\u001b[0m[0000] Cleaning up old shm/mqueue mounts: done.   \n\u001b[34mINFO\u001b[0m[0000] Loading containers: start.                 \n\u001b[37mDEBU\u001b[0m[0000] Loaded container 61e3765ffe8c0d22b9d62ad3a0e0118fbcffae65980fe930b06b338cd6528b99 \n\u001b[37mDEBU\u001b[0m[0000] Loaded container 9d81493c3ab17e75c92878c0d49f1d1652c12ac93a465c8ee41195078114dd3e \n\u001b[37mDEBU\u001b[0m[0000] Loaded container b512775078fe3abd51a9839ce5e07514e037394ac68298b2d076d2a43416a66a \n\u001b[34mINFO\u001b[0m[0000] Loading containers: done.                  \nI0414 10:55:50.372695   17471 server.go:70] Server created for HTTP on unix (/var/run/hyper.sock)\nQemu Driver Loaded\nI0414 10:55:50.372806   17471 hyperd.go:195] The hypervisor's driver is \nI0414 10:55:50.372987   17471 network_linux.go:262] bridge exist\nI0414 10:55:50.375258   17471 iptables_linux.go:140] /sbin/iptables, [--wait -t nat -C POSTROUTING -s 192.168.123.1/24 ! -o hyper0 -j MASQUERADE]\nI0414 10:55:50.377344   17471 iptables_linux.go:140] /sbin/iptables, [--wait -N HYPER]\nI0414 10:55:50.381834   17471 iptables_linux.go:140] /sbin/iptables, [--wait -t filter -C FORWARD -o hyper0 -j HYPER]\nI0414 10:55:50.386168   17471 iptables_linux.go:140] /sbin/iptables, [--wait -t filter -C FORWARD -i hyper0 -j ACCEPT]\nI0414 10:55:50.389232   17471 iptables_linux.go:140] /sbin/iptables, [--wait -t filter -C FORWARD -o hyper0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT]\nI0414 10:55:50.395297   17471 network_linux.go:174] modprobe br_netfilter failed modprobe br_netfilter failed\nI0414 10:55:50.395473   17471 iptables_linux.go:140] /sbin/iptables, [--wait -t nat -N HYPER]\nI0414 10:55:50.398294   17471 iptables_linux.go:140] /sbin/iptables, [--wait -t nat -C OUTPUT -m addrtype --dst-type LOCAL ! -d 127.0.0.1/8 -j HYPER]\nI0414 10:55:50.401198   17471 iptables_linux.go:140] /sbin/iptables, [--wait -t nat -C PREROUTING -m addrtype --dst-type LOCAL -j HYPER]\nI0414 10:55:50.406275   17471 daemondb.go:220] got key from leveldb pod-container-pod-CLXAYVHBWl\nI0414 10:55:50.406373   17471 daemondb.go:220] got key from leveldb pod-container-pod-VGomPKZBCk\nI0414 10:55:50.406429   17471 daemondb.go:220] got key from leveldb pod-container-pod-mGkVWYVxar\nI0414 10:55:50.406480   17471 daemondb.go:220] got key from leveldb pod-pod-CLXAYVHBWl\nI0414 10:55:50.406557   17471 daemondb.go:220] got key from leveldb pod-pod-VGomPKZBCk\nI0414 10:55:50.406615   17471 daemondb.go:220] got key from leveldb pod-pod-mGkVWYVxar\nI0414 10:55:50.406690   17471 daemon.go:78] reloading pod pod-CLXAYVHBWl with args {\"id\":\"erpnext_dev-4222828621\",\"hostname\":\"\",\"containers\":[{\"name\":\"erpnext_dev-4222828621\",\"image\":\"aptech/erpnext_dev\",\"command\":[\"/bin/bash\"],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"labels\":{},\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}\nI0414 10:55:50.406958   17471 run.go:45] podArgs: {\"id\":\"erpnext_dev-4222828621\",\"hostname\":\"\",\"containers\":[{\"name\":\"erpnext_dev-4222828621\",\"image\":\"aptech/erpnext_dev\",\"command\":[\"/bin/bash\"],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"labels\":{},\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}\nI0414 10:55:50.407732   17471 daemondb.go:82] try get container list for pod pod-CLXAYVHBWl\nI0414 10:55:50.407905   17471 pod.go:228] loaded containers for pod pod-CLXAYVHBWl: [9d81493c3ab17e75c92878c0d49f1d1652c12ac93a465c8ee41195078114dd3e]\nI0414 10:55:50.408066   17471 pod.go:247] Found exist container erpnext_dev-4222828621 (9d81493c3ab17e75c92878c0d49f1d1652c12ac93a465c8ee41195078114dd3e), pod: pod-CLXAYVHBWl\nI0414 10:55:50.408154   17471 pod.go:280] do not need to create container erpnext_dev-4222828621 of pod pod-CLXAYVHBWl[0]\nI0414 10:55:50.408219   17471 pod.go:355] container name erpnext_dev-4222828621, image aptech/erpnext_dev\nI0414 10:55:50.408381   17471 pod.go:372] container info config &{9d81493c3ab1   false false false map[80/tcp:{}]  false false false [PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=erpnext.apt.local DEBIAN_FRONTEND=noninteractive MSQ_PASS=demopass FRAPPE_USER=frappe FRAPPE_BRANCH=master BENCH_BRANCH=master ADMIN_PASS=admin SITE_NAME=site1.local ERPNEXT_APPS_JSON=https://raw.githubusercontent.com/frappe/bench/master/install_scripts/erpnext-apps-master.json] 0xc820ffb3e0 false aptech/erpnext_dev map[/home/frappe/frappe-bench/sites/site1.local/:{} /var/lib/mysql:{}] /home/frappe  false  [] map[] }, Cmd [/bin/bash], Args []\nI0414 10:55:50.408636   17471 pod.go:395] Container Info is \n&{9d81493c3ab17e75c92878c0d49f1d1652c12ac93a465c8ee41195078114dd3e 9d81493c3ab17e75c92878c0d49f1d1652c12ac93a465c8ee41195078114dd3e    /home/frappe [] [/bin/bash] map[FRAPPE_USER:frappe ADMIN_PASS:admin HOSTNAME:erpnext.apt.local MSQ_PASS:demopass FRAPPE_BRANCH:master BENCH_BRANCH:master SITE_NAME:site1.local ERPNEXT_APPS_JSON:https://raw.githubusercontent.com/frappe/bench/master/install_scripts/erpnext-apps-master.json PATH:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin DEBIAN_FRONTEND:noninteractive] false}\nI0414 10:55:50.408956   17471 daemondb.go:91] try set container list for pod pod-CLXAYVHBWl: [9d81493c3ab17e75c92878c0d49f1d1652c12ac93a465c8ee41195078114dd3e]\nI0414 10:55:50.409225   17471 daemon.go:96] no existing VM for pod pod-CLXAYVHBWl: leveldb: not found\nI0414 10:55:50.409310   17471 daemon.go:78] reloading pod pod-VGomPKZBCk with args {\"id\":\"haproxy-alpine-9698708926\",\"hostname\":\"\",\"containers\":[{\"name\":\"haproxy-alpine-9698708926\",\"image\":\"haproxy:alpine\",\"command\":[\"/bin/sh\"],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"labels\":{},\"log\":{\"type\":\"\",\"config\":{}},\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}\nI0414 10:55:50.409449   17471 run.go:45] podArgs: {\"id\":\"haproxy-alpine-9698708926\",\"hostname\":\"\",\"containers\":[{\"name\":\"haproxy-alpine-9698708926\",\"image\":\"haproxy:alpine\",\"command\":[\"/bin/sh\"],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"labels\":{},\"log\":{\"type\":\"\",\"config\":{}},\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}\nI0414 10:55:50.409780   17471 daemondb.go:82] try get container list for pod pod-VGomPKZBCk\nI0414 10:55:50.409879   17471 pod.go:228] loaded containers for pod pod-VGomPKZBCk: [61e3765ffe8c0d22b9d62ad3a0e0118fbcffae65980fe930b06b338cd6528b99]\nI0414 10:55:50.409994   17471 pod.go:247] Found exist container haproxy-alpine-9698708926 (61e3765ffe8c0d22b9d62ad3a0e0118fbcffae65980fe930b06b338cd6528b99), pod: pod-VGomPKZBCk\nI0414 10:55:50.410047   17471 pod.go:280] do not need to create container haproxy-alpine-9698708926 of pod pod-VGomPKZBCk[0]\nI0414 10:55:50.410096   17471 pod.go:355] container name haproxy-alpine-9698708926, image haproxy:alpine\nI0414 10:55:50.410237   17471 pod.go:372] container info config &{61e3765ffe8c   false false false map[]  false false false [PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HAPROXY_MAJOR=1.6 HAPROXY_VERSION=1.6.4 HAPROXY_MD5=ee107312ef58432859ee12bf048025ab] 0xc820ffa980 false haproxy:alpine map[]  0xc820ffa9c0 false  [] map[] }, Cmd [/bin/sh], Args [/bin/sh]\nI0414 10:55:50.410375   17471 pod.go:395] Container Info is \n&{61e3765ffe8c0d22b9d62ad3a0e0118fbcffae65980fe930b06b338cd6528b99 61e3765ffe8c0d22b9d62ad3a0e0118fbcffae65980fe930b06b338cd6528b99     [] [/docker-entrypoint.sh /bin/sh] map[PATH:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HAPROXY_MAJOR:1.6 HAPROXY_VERSION:1.6.4 HAPROXY_MD5:ee107312ef58432859ee12bf048025ab] false}\nI0414 10:55:50.410648   17471 daemondb.go:91] try set container list for pod pod-VGomPKZBCk: [61e3765ffe8c0d22b9d62ad3a0e0118fbcffae65980fe930b06b338cd6528b99]\nI0414 10:55:50.410880   17471 daemon.go:96] no existing VM for pod pod-VGomPKZBCk: leveldb: not found\nI0414 10:55:50.410941   17471 daemon.go:78] reloading pod pod-mGkVWYVxar with args {\"id\":\"openresty-latest-5272813125\",\"hostname\":\"\",\"containers\":[{\"name\":\"openresty-latest-5272813125\",\"image\":\"ficusio/openresty:latest\",\"command\":[\"/bin/sh\"],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"labels\":{},\"log\":{\"type\":\"\",\"config\":{}},\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}\nI0414 10:55:50.411059   17471 run.go:45] podArgs: {\"id\":\"openresty-latest-5272813125\",\"hostname\":\"\",\"containers\":[{\"name\":\"openresty-latest-5272813125\",\"image\":\"ficusio/openresty:latest\",\"command\":[\"/bin/sh\"],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"labels\":{},\"log\":{\"type\":\"\",\"config\":{}},\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}\nI0414 10:55:50.411377   17471 daemondb.go:82] try get container list for pod pod-mGkVWYVxar\nI0414 10:55:50.411482   17471 pod.go:228] loaded containers for pod pod-mGkVWYVxar: [b512775078fe3abd51a9839ce5e07514e037394ac68298b2d076d2a43416a66a]\nI0414 10:55:50.411594   17471 pod.go:247] Found exist container openresty-latest-5272813125 (b512775078fe3abd51a9839ce5e07514e037394ac68298b2d076d2a43416a66a), pod: pod-mGkVWYVxar\nI0414 10:55:50.411652   17471 pod.go:280] do not need to create container openresty-latest-5272813125 of pod pod-mGkVWYVxar[0]\nI0414 10:55:50.411702   17471 pod.go:355] container name openresty-latest-5272813125, image ficusio/openresty:latest\nI0414 10:55:50.411842   17471 pod.go:372] container info config &{b512775078fe   false false false map[]  false false false [PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin OPENRESTY_VERSION=1.9.7.3 OPENRESTY_PREFIX=/opt/openresty NGINX_PREFIX=/opt/openresty/nginx VAR_PREFIX=/var/nginx] 0xc820ffbac0 false ficusio/openresty:latest map[] /opt/openresty/nginx/  false  [] map[] }, Cmd [/bin/sh], Args []\nI0414 10:55:50.412084   17471 pod.go:395] Container Info is \n&{b512775078fe3abd51a9839ce5e07514e037394ac68298b2d076d2a43416a66a b512775078fe3abd51a9839ce5e07514e037394ac68298b2d076d2a43416a66a    /opt/openresty/nginx/ [] [/bin/sh] map[OPENRESTY_VERSION:1.9.7.3 OPENRESTY_PREFIX:/opt/openresty NGINX_PREFIX:/opt/openresty/nginx VAR_PREFIX:/var/nginx PATH:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin] false}\nI0414 10:55:50.412226   17471 daemondb.go:91] try set container list for pod pod-mGkVWYVxar: [b512775078fe3abd51a9839ce5e07514e037394ac68298b2d076d2a43416a66a]\nI0414 10:55:50.412312   17471 daemon.go:96] no existing VM for pod pod-mGkVWYVxar: leveldb: not found\nI0414 10:55:50.412334   17471 daemon.go:106] 3 pod have been loaded\nI0414 10:55:50.412357   17471 daemon.go:108] container in pod pod-CLXAYVHBWl status: [0xc820ff0900]\nI0414 10:55:50.412377   17471 daemon.go:109] container in pod pod-CLXAYVHBWl spec: [{erpnext_dev-4222828621 aptech/erpnext_dev [] / [] false map[] [] [] [{/etc/hosts etchosts-volume false} {/home/frappe/frappe-bench/sites/site1.local/ 9d81493c3ab17e75c92878c0d49f1d1652c12ac93a465c8ee41195078114dd3e_home_frappe_frappe-bench_sites_site1.local_ false} {/var/lib/mysql 9d81493c3ab17e75c92878c0d49f1d1652c12ac93a465c8ee41195078114dd3e_var_lib_mysql false}] [{/etc/resolv.conf pod-CLXAYVHBWl-resolvconf 0644  }] never}]\nI0414 10:55:50.412425   17471 daemon.go:108] container in pod pod-VGomPKZBCk status: [0xc820ff0990]\nI0414 10:55:50.412444   17471 daemon.go:109] container in pod pod-VGomPKZBCk spec: [{haproxy-alpine-9698708926 haproxy:alpine [] / [] true map[] [] [] [{/etc/hosts etchosts-volume false}] [{/etc/resolv.conf pod-VGomPKZBCk-resolvconf 0644  }] never}]\nI0414 10:55:50.412477   17471 daemon.go:108] container in pod pod-mGkVWYVxar status: [0xc820ff0b40]\nI0414 10:55:50.412495   17471 daemon.go:109] container in pod pod-mGkVWYVxar spec: [{openresty-latest-5272813125 ficusio/openresty:latest [] / [] true map[] [] [] [{/etc/hosts etchosts-volume false}] [{/etc/resolv.conf pod-mGkVWYVxar-resolvconf 0644  }] never}]\nI0414 10:55:50.412682   17471 hyperd.go:234] Hyper daemon: 0.5.0 0\nI0414 10:55:50.412749   17471 server.go:199] Registering routers\nI0414 10:55:50.412791   17471 server.go:204] Registering GET, /container/info\nI0414 10:55:50.413046   17471 server.go:204] Registering GET, /container/logs\nI0414 10:55:50.413201   17471 server.go:204] Registering GET, /exitcode\nI0414 10:55:50.413318   17471 server.go:204] Registering POST, /container/create\nI0414 10:55:50.413482   17471 server.go:204] Registering POST, /container/rename\nI0414 10:55:50.413629   17471 server.go:204] Registering POST, /container/commit\nI0414 10:55:50.413757   17471 server.go:204] Registering POST, /container/kill\nI0414 10:55:50.413888   17471 server.go:204] Registering POST, /exec\nI0414 10:55:50.414007   17471 server.go:204] Registering POST, /attach\nI0414 10:55:50.414122   17471 server.go:204] Registering POST, /tty/resize\nI0414 10:55:50.414243   17471 server.go:204] Registering GET, /pod/info\nI0414 10:55:50.414356   17471 server.go:204] Registering GET, /pod/stats\nI0414 10:55:50.414478   17471 server.go:204] Registering GET, /list\nI0414 10:55:50.414601   17471 server.go:204] Registering POST, /pod/create\nI0414 10:55:50.414724   17471 server.go:204] Registering POST, /pod/labels\nI0414 10:55:50.414857   17471 server.go:204] Registering POST, /pod/start\nI0414 10:55:50.414985   17471 server.go:204] Registering POST, /pod/stop\nI0414 10:55:50.415098   17471 server.go:204] Registering POST, /pod/kill\nI0414 10:55:50.415201   17471 server.go:204] Registering POST, /pod/pause\nI0414 10:55:50.415318   17471 server.go:204] Registering POST, /pod/unpause\nI0414 10:55:50.415455   17471 server.go:204] Registering POST, /vm/create\nI0414 10:55:50.415587   17471 server.go:204] Registering DELETE, /pod\nI0414 10:55:50.415684   17471 server.go:204] Registering DELETE, /vm\nI0414 10:55:50.415772   17471 server.go:204] Registering GET, /service/list\nI0414 10:55:50.415906   17471 server.go:204] Registering POST, /service/add\nI0414 10:55:50.416038   17471 server.go:204] Registering POST, /service/update\nI0414 10:55:50.416194   17471 server.go:204] Registering DELETE, /service\nI0414 10:55:50.416303   17471 server.go:204] Registering GET, /images/get\nI0414 10:55:50.416439   17471 server.go:204] Registering POST, /image/create\nI0414 10:55:50.416562   17471 server.go:204] Registering POST, /image/load\nI0414 10:55:50.416679   17471 server.go:204] Registering POST, /image/push\nI0414 10:55:50.416789   17471 server.go:204] Registering DELETE, /image\nI0414 10:55:50.416882   17471 server.go:204] Registering GET, /_ping\nI0414 10:55:50.416985   17471 server.go:204] Registering GET, /info\nI0414 10:55:50.417114   17471 server.go:204] Registering GET, /version\nI0414 10:55:50.417232   17471 server.go:204] Registering POST, /auth\nI0414 10:55:50.417334   17471 server.go:204] Registering POST, /image/build\nI0414 10:55:50.417485   17471 server.go:95] API listen on /var/run/hyper.sock\n^CI0414 10:56:21.651074   17471 daemon.go:405] The daemon will be shutdown\nI0414 10:56:21.651150   17471 daemon.go:406] Shutdown all VMs\nroot@MACHINE-GA-970A-D3:~# exit\nexit\nScript done on Thu 14 Apr 2016 10:56:32 AM CEST\n```\n. @laijs Thanks for the useful pointer.\n. After changing the Root= parameter to the new location hyperd service did not get killed:\n```\ncat /etc/hyper/config\nRoot=/mnt/tank/HYPER/\nKernel=/mnt/tank/HYPER/kernel\nInitrd=/mnt/tank/HYPER/hyper-initrd.img\nBios=/mnt/tank/HYPER/bios-qboot.bin\nCbfs=/mnt/tank/HYPER/cbfs-qboot.rom\n```\n$ sudo hyper list\n[sudo] password for zenny: \nPOD ID              POD Name            VM name             Status\npod-KBaCetaWBb      alpine-5941507135                       pending\n$ sudo hyper start pod-KBaCetaWBb\nhyper ERROR: An error occurred trying to connect: Post http://%2Fvar%2Frun%2Fhyper.sock/v0.5.0/pod/start?podId=pod-KBaCetaWBb&tag=&vmId=: dial unix /var/run/hyper.sock: connect: no such file or directory\n```\n/etc/init.d/hyperd status\n\nhyperd is running\n\nservice hyperd stop\n\nStopping hyperd: hyperd                                                                                    [ OK ] \n\nservice hyperd status\n\nhyperd is running\n\nkillall hyperd\nhyperd: no process found\n```\n```\nservice hyperd restart\n\nStopping hyperd: hyperd                                                                                    [ OK ] \nStarting hyperd: hyperd                                                                                    [ OK ] \n\nservice hyperd stop\n\nStopping hyperd: hyperd                                                                                    [ OK ] \n\nservice hyperd status\n\nhyperd is not running\n```\n. > Is /mnt/tank/HYPER/ a fresh new (empty) dir when the config is being changed?\n\nNope I stopped the hyperd service, copied all from /var/lib/hyper /mnt/tank/HYPER with all acl's saved with -a, and made changes to the config file accordingly and restarted the hyperd daemon.\n\nWhat's the behavior of the same operations for the recent hyperd/hyperctl?\n\n```\nhyperctl list\nhyperctl ERROR: An error occurred trying to connect: Get http://%2Fvar%2Frun%2Fhyper.sock/v0.5.0/list?item=pod: dial unix /var/run/hyper.sock: connect: no such file or directory\n```\n. @laijs but even your log does not show it runs as it should. I also got, showing that it spawns every few second:\n```\n2016-04-14 15:01:33,321 INFO exited: mysqld (exit status 0; expected)\n2016-04-14 15:01:34,333 INFO spawned: 'mysqld' with pid 5859\n2016-04-14 15:01:35,339 INFO success: mysqld entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)\n2016-04-14 15:01:40,707 INFO exited: mysqld (exit status 0; expected)\n2016-04-14 15:01:41,719 INFO spawned: 'mysqld' with pid 6505\n2016-04-14 15:01:42,725 INFO success: mysqld entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)\n2016-04-14 15:01:48,116 INFO exited: mysqld (exit status 0; expected)\n2016-04-14 15:01:49,128 INFO spawned: 'mysqld' with pid 7151\n2016-04-14 15:01:50,135 INFO success: mysqld entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)\n2016-04-14 15:01:55,503 INFO exited: mysqld (exit status 0; expected)\n2016-04-14 15:01:56,515 INFO spawned: 'mysqld' with pid 7797\n2016-04-14 15:01:57,521 INFO success: mysqld entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)\n```\n. > the hyperstart might be lack of something required by the combination of container process supervisord+mysqld\nYep, but how can that process be implemented in hyper, that is the confusion. hyper does not seem as straight as docker. The normal process of starting that container in docker is:\nCreate and rund a mysql bench:\ndocker create -v /home/frappe/frappe-bench/sites/site1.local/ -v /var/lib/mysql --name erpdata aptech/erpnext_dev\nThen, run the app,\ndocker run -d -p 80:80 --name erpnext --volumes-from erpdata\nHow can this be implemented in hyper? \n@gnawux has posted a hint at https://github.com/hyperhq/hyper/issues/280#issuecomment-209855250, but leads nowhere, at least for me!\n. I restarted the pod with:\n# hyperctl start -c 2 -m 8192 pod-YGLRVtQDSW\nSuccessfully started the Pod(pod-YGLRVtQDSW)\nEven after increasing the resources to two cpus and 8GB of memory, I am still getting:\n```\nOpenJDK 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000fe84c000, 6770688, 0) failed; error='Cannot allocate memory' (errno=12)\n\nThere is insufficient memory for the Java Runtime Environment to continue.\nNative memory allocation (mmap) failed to map 6770688 bytes for committing reserved memory.\n```\n. Please tag this as bug.\nBTW, are resources thin provisioned or dedicated when one specifies, say --cpu=2 and --memory=8192?\nIn the meantime, I tried to create a new instances of container with:\n# hyperctl run --cpu=2 --memory=8192 debian:sid\nbut it didn't get attached on its own. However can be seen running in another console:\n```\nhyperctl list\nPOD ID              POD Name                      VM name             Status\npod-IsSuJCftUu      debian-sid-0430301703         vm-fWMrOywAst       running\n```\nAttaching the pod does not do anything:\n# hyperctl attach pod-IsSuJCftUu\nThe hyperd:ERROR log shows:\n```\nLog file created at: 2016/04/15 11:02:12\nRunning on machine: MACHINE-GA-970A-D3\nBinary: Built with gc go1.5.3 for linux/amd64\nLog line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\nE0415 11:02:12.727902   11506 pod.go:325] nosuchimagetag: No such image: debian:sid\nE0415 11:02:12.728480   11506 server.go:1154] Handler for POST /pod/create returned error: nosuchimagetag: No such image: debian:sid\nE0415 11:02:12.728495   11506 server.go:132] HTTP Error: statusCode=404 nosuchimagetag: No such image: debian:sid\nE0415 11:03:01.605841   11506 mount_linux.go:18] Couldn't run auplink before unmount: exec: \"auplink\": executable file not found in $PATH\nE0415 11:03:01.622414   11506 mount_linux.go:18] Couldn't run auplink before unmount: exec: \"auplink\": executable file not found in $PATH\nE0415 11:04:23.137363   11506 tty.go:78] read tty data failed\nE0415 11:04:23.137661   11506 init_comm.go:79] read init data failed\nE0415 11:05:33.335842   11506 tty.go:78] read tty data failed\nE0415 11:05:33.335945   11506 init_comm.go:79] read init data failed\nE0415 11:13:47.130996   11506 init_comm.go:79] read init data failed\nE0415 11:13:47.131069   11506 tty.go:78] read tty data failed\nE0415 11:49:47.606379   11506 tty.go:78] read tty data failed\nE0415 11:49:47.606412   11506 init_comm.go:79] read init data failed\nE0415 11:51:58.679959   11506 pod.go:325] nosuchimagetag: No such image: -m:latest\nE0415 11:51:58.680077   11506 server.go:1154] Handler for POST /pod/create returned error: nosuchimagetag: No such image: -m:latest\nE0415 11:51:58.680113   11506 server.go:132] HTTP Error: statusCode=404 nosuchimagetag: No such image: -m:latest\nE0415 11:51:58.807374   11506 tty.go:78] read tty data failed\nE0415 11:51:58.807405   11506 init_comm.go:79] read init data failed\nE0415 11:51:58.807589   11506 init_comm.go:117] read init message failed... EOF\nE0415 11:53:52.830544   11506 pod.go:325] nosuchimagetag: No such image: debian:jessie\nE0415 11:53:52.830722   11506 server.go:1154] Handler for POST /pod/create returned error: nosuchimagetag: No such image: debian:jessie\nE0415 11:53:52.830787   11506 server.go:132] HTTP Error: statusCode=404 nosuchimagetag: No such image: debian:jessie\nE0415 11:54:42.647709   11506 mount_linux.go:18] Couldn't run auplink before unmount: exec: \"auplink\": executable file not found in $PATH\nE0415 11:54:42.666501   11506 mount_linux.go:18] Couldn't run auplink before unmount: exec: \"auplink\": executable file not found in $PATH\nE0415 11:57:35.818653   11506 tty.go:78] read tty data failed\nE0415 11:57:35.818713   11506 init_comm.go:79] read init data failed\nE0415 12:24:13.523355   11506 init_comm.go:79] read init data failed\nE0415 12:24:13.523358   11506 tty.go:78] read tty data failed\nE0415 12:26:40.793095   11506 pod.go:325] nosuchimagetag: No such image: -c:latest\nE0415 12:26:40.793250   11506 server.go:1154] Handler for POST /pod/create returned error: nosuchimagetag: No such image: -c:latest\nE0415 12:26:40.793305   11506 server.go:132] HTTP Error: statusCode=404 nosuchimagetag: No such image: -c:latest\nE0415 12:26:40.906048   11506 tty.go:78] read tty data failed\nE0415 12:26:40.906059   11506 init_comm.go:79] read init data failed\nE0415 12:26:40.906177   11506 init_comm.go:117] read init message failed... EOF\nE0415 12:27:53.762378   11506 mount_linux.go:18] Couldn't run auplink before unmount: exec: \"auplink\": executable file not found in $PATH\nE0415 12:27:53.772151   11506 mount_linux.go:18] Couldn't run auplink before unmount: exec: \"auplink\": executable file not found in $PATH\n``\n. @gnawux as I have updated above at https://github.com/hyperhq/hyperd/issues/287#issuecomment-210407226, the resource allocation did not seem to have come into effect. Or did I miss something?\n. @Gnep I saw your last commit (https://github.com/hyperhq/hyperd/commit/88cea8d63f5def05ffdc72ecc82bed12a85fdde3) to the README.md, you have yet not omitted the docker part in theInstallation` section:\n\nEnsure you are running Linux (kernel 3.8 or later) and have* Docker (version 1.5 or later)* and QEMU (version 2.0 or later) installed. \n\nAnd the changes were not affected at http://docs.hypercontainer.io/get_started/install/build.html\nHowever, what is the https://github.com/hyperhq/hypercli repo, if not hyperctl?\nBefore I continue testing further, would you mind throwing some light on the roadmap and nomenclature to prevent further confusion while compiling and using the tools? Xiexie!\n. @gnawux Thanks for updating. It would be good if you have some roadmap documents.\n\nAnd could you share your case with us?\n\nI am planning to deploy in one of the production server in one of my datacenters. Namespace vulnerabilities didn't inspire me to deploy lxc or docker in production. The only part that I liked is the hw-based qemu-kvm isolation with this project above all,\n\nwe do not want to break your works when we do some changes.\n\nFrequent uninformed changes and  lack of update of proper documentation after the changes made life really a PITA.\n\nAlthough the project has not reach 1.0 yet (we want to make it reach 1.0 in this summer)\n\nGood news, hope to see you meeting that deadline.\nFrom what I witnessed with this project so far, your team lacks of someone with both technical and management skills who can field, canvass and manage your project more democratically with openness (could be me, shameless plug ;-) ). Just my two fen.\n. @Gnep Still the binary package (https://hyper-install.s3.amazonaws.com/hyper-latest.tgz) is not updated, fyi.\nAppreciate if you would advise Mr. Peng or @gnawux  to write back?\nMy PGP key is:\n```\n-----BEGIN PGP PUBLIC KEY BLOCK-----\nComment: http://openpgpjs.org\nxsBNBFcTxwEBCAC+G0MG+BHNGs8orGRobPV6jd+8RtT4XhXXEnuEjLA5uHz8\n1OulvUS/qiq58Jo/KEnTn19rtyNiN7GmrLvo14Q0+mpFQEfrnzj2NCr1bf8w\nl5r+CrIIb+xFEqf5dIHf3w1NNXgHwl6Z3QBflZsqaPHa8y5dhAqVlr1NS7EL\nVgCifutAppl2Fcl05p4F5pQLKHMYCO+5gPMnMfnOOe4BTch0VOg8N4qkv0Px\nJtSHjHucpivf4eJwznejYwDt/AtdyaB7LUC9N6yuLN+QYuB/mIo0YVU2wcgP\niwr8ITfDUz5Nx0MUm9hmTbOyj6ixNOVuYMmOvevCzzU0ULEkr99EMoAJABEB\nAAHNHFplbm55IDxnYXJieXRyYXNoQGdtYWlsLmNvbT7CwHIEEAEIACYFAlcT\nxwEGCwkIBwMCCRABOcPTK6+XKwQVCAIKAxYCAQIbAwIeAQAAD5gH/21f5PLm\nytP4rd9HLGKHTMQola/VKMoCMlA7zb1LLJKTCJayZmIproblTyWO8iSSkkaA\n89gIifuCTvMJ8vh4WLTUfO0gr+41uZhLScYqAOoqgctCPsyrHxV4QBYAzGf7\n1LAEymtYBSiKHhks4Jff190Czrfupz7AAuLxepS1/RIZbdmeYO2g8FWf4sIR\nZFKehNMSWlspxYGxXdAmGLX+xtHD+LNHqqnERsuatynR9oJ3G8WauD4CiNgW\nIRyfxf2xZVj7J+bGzg7dl7IJNmp0UDTLqqsF2TFpURyfIAAAhb3WkQAaV5n1\nosMST1BbCnWdGo5bjpReuBl3lQ5bIn3Gc3HOwE0EVxPHAQEIAL2Pq+od71kT\n/lRMt+XDryOc1XTT5DJW7BUMXOjXXOZfWsuGTrqU3O1XYPWYzoZy9L+6zpII\nOn/auicvkUblWvrXkt4CIVIU1qDk6KpDKVKBiINy5sk7cTyjumbqxPmnVBK2\nDHN27rLOnReCnFUmgIgbfgK0/un0oEnAHvsYdeg1ydipd2vVzx3aJ1TfQS1W\nIBWN125EO4nKQ5Kl1XV7nWvlv+ZvrOmOWVeSl9jpyZvLJDmks0E/AIF4QBJF\nK+NTME8+x7CwFDQwLGENXojeZOfsNHbln91KE1ZU1/QvzLHVqdZOo/s20Y7V\ntjdUsiUPpVQcsSpXLzGKPCWz90M3Be8AEQEAAcLAXwQYAQgAEwUCVxPHAgkQ\nATnD0yuvlysCGwwAAL9hCACP7CY1fivXEN4X+l/C56l/nARrNVoZvJr4QHnF\n9C/r5m6TLCMov0eOLg8IvZF7M0Ecyvq1IzNqbwQd+8mTA4tn+aND20fk2z08\nfloFL6fJykIyAGtRMwAb3HdC1pqexk/0pYxhoy9GtQzqvK/NbcPPdBDd1N7M\npKdXDVhXhx0R1K6UlMYfnyc9o171UYRPlFrmdBV7ZLC4KeBKqFEESKXaxyRg\nD7E1FXGl1pDMh2QJNM/n9gVLJb0+znBsPG4jUNOctAOhRwF9Z23qsU6AGpOu\nQhWG1alJz6d1T4sTgPdh+K1nMWNKGUzzayAKrRPTbnwLEijqqJPpIIDVzoai\npy73\n=JPvb\n-----END PGP PUBLIC KEY BLOCK-----\n``\n. @gnawux In that case, I suggest you to tag the tgz with a 'version no.-stable' and clarify that the precompiled package does not reflect the chanes in git repo, particularly the binary command itself (I meant your recent move tohyperctl`). This will help avoid confusion, I guess.\n. Gnep Yes I signed up a per your request, but have no idea how is it useful to me? You can derive my email from my PGP key and the username is Zenny.\nMy PGP key is:\n```\n-----BEGIN PGP PUBLIC KEY BLOCK-----\nComment: http://openpgpjs.org\nxsBNBFcTxwEBCAC+G0MG+BHNGs8orGRobPV6jd+8RtT4XhXXEnuEjLA5uHz8\n1OulvUS/qiq58Jo/KEnTn19rtyNiN7GmrLvo14Q0+mpFQEfrnzj2NCr1bf8w\nl5r+CrIIb+xFEqf5dIHf3w1NNXgHwl6Z3QBflZsqaPHa8y5dhAqVlr1NS7EL\nVgCifutAppl2Fcl05p4F5pQLKHMYCO+5gPMnMfnOOe4BTch0VOg8N4qkv0Px\nJtSHjHucpivf4eJwznejYwDt/AtdyaB7LUC9N6yuLN+QYuB/mIo0YVU2wcgP\niwr8ITfDUz5Nx0MUm9hmTbOyj6ixNOVuYMmOvevCzzU0ULEkr99EMoAJABEB\nAAHNHFplbm55IDxnYXJieXRyYXNoQGdtYWlsLmNvbT7CwHIEEAEIACYFAlcT\nxwEGCwkIBwMCCRABOcPTK6+XKwQVCAIKAxYCAQIbAwIeAQAAD5gH/21f5PLm\nytP4rd9HLGKHTMQola/VKMoCMlA7zb1LLJKTCJayZmIproblTyWO8iSSkkaA\n89gIifuCTvMJ8vh4WLTUfO0gr+41uZhLScYqAOoqgctCPsyrHxV4QBYAzGf7\n1LAEymtYBSiKHhks4Jff190Czrfupz7AAuLxepS1/RIZbdmeYO2g8FWf4sIR\nZFKehNMSWlspxYGxXdAmGLX+xtHD+LNHqqnERsuatynR9oJ3G8WauD4CiNgW\nIRyfxf2xZVj7J+bGzg7dl7IJNmp0UDTLqqsF2TFpURyfIAAAhb3WkQAaV5n1\nosMST1BbCnWdGo5bjpReuBl3lQ5bIn3Gc3HOwE0EVxPHAQEIAL2Pq+od71kT\n/lRMt+XDryOc1XTT5DJW7BUMXOjXXOZfWsuGTrqU3O1XYPWYzoZy9L+6zpII\nOn/auicvkUblWvrXkt4CIVIU1qDk6KpDKVKBiINy5sk7cTyjumbqxPmnVBK2\nDHN27rLOnReCnFUmgIgbfgK0/un0oEnAHvsYdeg1ydipd2vVzx3aJ1TfQS1W\nIBWN125EO4nKQ5Kl1XV7nWvlv+ZvrOmOWVeSl9jpyZvLJDmks0E/AIF4QBJF\nK+NTME8+x7CwFDQwLGENXojeZOfsNHbln91KE1ZU1/QvzLHVqdZOo/s20Y7V\ntjdUsiUPpVQcsSpXLzGKPCWz90M3Be8AEQEAAcLAXwQYAQgAEwUCVxPHAgkQ\nATnD0yuvlysCGwwAAL9hCACP7CY1fivXEN4X+l/C56l/nARrNVoZvJr4QHnF\n9C/r5m6TLCMov0eOLg8IvZF7M0Ecyvq1IzNqbwQd+8mTA4tn+aND20fk2z08\nfloFL6fJykIyAGtRMwAb3HdC1pqexk/0pYxhoy9GtQzqvK/NbcPPdBDd1N7M\npKdXDVhXhx0R1K6UlMYfnyc9o171UYRPlFrmdBV7ZLC4KeBKqFEESKXaxyRg\nD7E1FXGl1pDMh2QJNM/n9gVLJb0+znBsPG4jUNOctAOhRwF9Z23qsU6AGpOu\nQhWG1alJz6d1T4sTgPdh+K1nMWNKGUzzayAKrRPTbnwLEijqqJPpIIDVzoai\npy73\n=JPvb\n-----END PGP PUBLIC KEY BLOCK-----\n```\n. https://github.com/hyperhq/hyperd/issues/289 was closed, yet the problem persists as stated in https://github.com/hyperhq/hyperstart/pull/76#issuecomment-211780394\nLog:\n```\nzenny@HYPER-MACHINE:[/mnt/tank/HYPERBUILD/hyperhq/src/github.com/hyperhq/hyperd]:$ sudo ./hyperctl list\n[sudo] password for zenny: \nPOD ID              POD Name            VM name             Status\npod-nGrWzvoRNi      alpine-0989527933                       pending\nzenny@HYPER-MACHINE:[/mnt/tank/HYPERBUILD/hyperhq/src/github.com/hyperhq/hyperd]:$ sudo ./hyperctl start pod-nGrWzvoRNi\n./hyperctl ERROR: Error from daemon's response: VM vm-bbuWmTciLc start failed with code 7: Start POD failed\nzenny@HYPER-MACHINE:[/mnt/tank/HYPERBUILD/hyperhq/src/github.com/hyperhq/hyperd]:$ cat /var/log/hyper/hyperd.ERROR \nLog file created at: 2016/04/19 09:13:10\nRunning on machine: HYPER-MACHINE\nBinary: Built with gc go1.6 for linux/amd64\nLog line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\nE0419 09:13:10.126183   12946 vm_states.go:358] Shutting down because of an exception: Start POD failed\nE0419 09:13:10.129314   12946 vm_states.go:635] Start POD failed\nE0419 09:13:10.129363   12946 run.go:88] VM vm-bbuWmTciLc start failed with code 7: Start POD failed\nE0419 09:13:10.129372   12946 server.go:170] Handler for POST /v0.5.0/pod/start returned error: VM vm-bbuWmTciLc start failed with code 7: Start POD failed\nE0419 09:13:10.141460   12946 init_comm.go:75] read init data failed\nE0419 09:13:10.141544   12946 tty.go:103] read tty data failed\nzenny@HYPER-MACHINE:[/mnt/tank/HYPERBUILD/hyperhq/src/github.com/hyperhq/hyperd]:$ sudo ./hyperctl run -d busybox\n[sudo] password for zenny: \nUsing default tag: latest\nlatest: Pulling from library/busybox\n385e281300cc: Pull complete \na3ed95caeb02: Pull complete \nDigest: sha256:4a887a2326ec9e0fa90cce7b4764b0e627b5d6afcb81a3f73c85dc29cea00048\nStatus: Downloaded newer image for busybox:latest\nPOD id is pod-OZyNoqIJiX\n./hyperctl ERROR: Error from daemon's response: VM vm-TcTOqfpdJk start failed with code 7: Start POD failed\nzenny@HYPER-MACHINE:[/mnt/tank/HYPERBUILD/hyperhq/src/github.com/hyperhq/hyperd]:$ sudo ./hyperd --nondaemon --v=3 --config=\"/etc/hyper/config\"\nI0419 09:12:20.570689   12946 hyperd.go:108] The config file is /etc/hyper/config\nI0419 09:12:20.648048   12946 daemon.go:137] The config: kernel=/mnt/tank/HYPER/kernel, initrd=/mnt/tank/HYPER/hyper-initrd.img\nI0419 09:12:20.648120   12946 daemon.go:139] The config: vbox image=\nI0419 09:12:20.648157   12946 daemon.go:142] The config: bridge=, ip=\nI0419 09:12:20.648194   12946 daemon.go:145] The config: bios=/mnt/tank/HYPER/bios-qboot.bin, cbfs=/mnt/tank/HYPER/cbfs-qboot.rom\nDEBU[0000] Using default logging driver none          \nINFO[0000] [graphdriver] using prior tank driver \"aufs\" \nDEBU[0000] Using graph driver aufs                    \nINFO[0001] Graph migration to content-addressability took 0.07 seconds \nDEBU[0001] Option DefaultDriver: bridge               \nDEBU[0001] Option DefaultNetwork: bridge              \nWARN[0001] Running modprobe bridge br_netfilter failed with message: modprobe: WARNING: Module br_netfilter not found.\ninsmod /lib/modules/3.13.0-85-lowlatency/kernel/net/llc/llc.ko \ninsmod /lib/modules/3.13.0-85-lowlatency/kernel/net/802/stp.ko \ninsmod /lib/modules/3.13.0-85-lowlatency/kernel/net/bridge/bridge.ko \n, error: exit status 1 \nINFO[0001] Firewalld running: false                   \nDEBU[0001] Registering ipam driver: \"default\"         \nDEBU[0001] Cleaning up old shm/mqueue mounts: start.  \nDEBU[0001] Cleaning up old shm/mqueue mounts: done.   \nINFO[0001] Loading containers: start.                 \nDEBU[0001] Loaded container 16fedca36031c3778996e98f4978ac65e44ed79b1b68ad01cbcec265ae443e5d \nDEBU[0001] Loaded container 7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea \nDEBU[0001] Loaded container f14de5f26c178455eae3bedc68153f4e3292d83a42455e6692071fa4120b7d34 \nINFO[0001] Loading containers: done.                  \nI0419 09:12:21.839083   12946 server.go:70] Server created for HTTP on unix (/var/run/hyper.sock)\nQemu Driver Loaded\nI0419 09:12:21.839193   12946 hyperd.go:195] The hypervisor's driver is \nI0419 09:12:21.839382   12946 network_linux.go:245] create bridge hyper0, ip 192.168.123.0/24\nI0419 09:12:21.839886   12946 network_linux.go:366] Allocate IP Address 192.168.123.1 for bridge hyper0\nI0419 09:12:21.839926   12946 network_linux.go:517] ifaddmsg length 8\nI0419 09:12:21.843935   12946 iptables_linux.go:140] /sbin/iptables, [--wait -t nat -C POSTROUTING -s 192.168.123.1/24 ! -o hyper0 -j MASQUERADE]\nI0419 09:12:21.915421   12946 iptables_linux.go:140] /sbin/iptables, [--wait -t nat -I POSTROUTING -s 192.168.123.1/24 ! -o hyper0 -j MASQUERADE]\nI0419 09:12:21.922253   12946 iptables_linux.go:140] /sbin/iptables, [--wait -N HYPER]\nI0419 09:12:21.927825   12946 iptables_linux.go:140] /sbin/iptables, [--wait -t filter -C FORWARD -o hyper0 -j HYPER]\nI0419 09:12:21.948106   12946 iptables_linux.go:140] /sbin/iptables, [--wait -I FORWARD -o hyper0 -j HYPER]\nI0419 09:12:21.952352   12946 iptables_linux.go:140] /sbin/iptables, [--wait -t filter -C FORWARD -i hyper0 -j ACCEPT]\nI0419 09:12:21.959771   12946 iptables_linux.go:140] /sbin/iptables, [--wait -I FORWARD -i hyper0 -j ACCEPT]\nI0419 09:12:21.963443   12946 iptables_linux.go:140] /sbin/iptables, [--wait -t filter -C FORWARD -o hyper0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT]\nI0419 09:12:21.983308   12946 iptables_linux.go:140] /sbin/iptables, [--wait -I FORWARD -o hyper0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT]\nI0419 09:12:21.992518   12946 network_linux.go:174] modprobe br_netfilter failed modprobe br_netfilter failed\nI0419 09:12:21.992633   12946 iptables_linux.go:140] /sbin/iptables, [--wait -t nat -N HYPER]\nI0419 09:12:21.996187   12946 iptables_linux.go:140] /sbin/iptables, [--wait -t nat -C OUTPUT -m addrtype --dst-type LOCAL ! -d 127.0.0.1/8 -j HYPER]\nI0419 09:12:22.017007   12946 iptables_linux.go:140] /sbin/iptables, [--wait -t nat -I OUTPUT -m addrtype --dst-type LOCAL ! -d 127.0.0.1/8 -j HYPER]\nI0419 09:12:22.021562   12946 iptables_linux.go:140] /sbin/iptables, [--wait -t nat -C PREROUTING -m addrtype --dst-type LOCAL -j HYPER]\nI0419 09:12:22.028862   12946 iptables_linux.go:140] /sbin/iptables, [--wait -t nat -I PREROUTING -m addrtype --dst-type LOCAL -j HYPER]\nI0419 09:12:22.048088   12946 daemondb.go:220] got key from leveldb pod-container-pod-hNxHwMrDbT\nI0419 09:12:22.048183   12946 daemondb.go:220] got key from leveldb pod-container-pod-nGrWzvoRNi\nI0419 09:12:22.048235   12946 daemondb.go:220] got key from leveldb pod-container-pod-qRllmlNPJP\nI0419 09:12:22.048287   12946 daemondb.go:220] got key from leveldb pod-pod-nGrWzvoRNi\nI0419 09:12:22.048378   12946 daemon.go:78] reloading pod pod-nGrWzvoRNi with args {\"id\":\"alpine-0989527933\",\"hostname\":\"\",\"containers\":[{\"name\":\"alpine-0989527933\",\"image\":\"alpine\",\"command\":[\"/bin/sh\"],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":512},\"files\":[],\"volumes\":[],\"labels\":{},\"log\":{\"type\":\"\",\"config\":{}},\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}\nI0419 09:12:22.048671   12946 run.go:45] podArgs: {\"id\":\"alpine-0989527933\",\"hostname\":\"\",\"containers\":[{\"name\":\"alpine-0989527933\",\"image\":\"alpine\",\"command\":[\"/bin/sh\"],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":512},\"files\":[],\"volumes\":[],\"labels\":{},\"log\":{\"type\":\"\",\"config\":{}},\"tty\":true,\"type\":\"\",\"RestartPolicy\":\"\"}\nI0419 09:12:22.049467   12946 daemondb.go:82] try get container list for pod pod-nGrWzvoRNi\nI0419 09:12:22.049513   12946 pod.go:229] loaded containers for pod pod-nGrWzvoRNi: [7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea]\nI0419 09:12:22.049581   12946 pod.go:248] Found exist container alpine-0989527933 (7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea), pod: pod-nGrWzvoRNi\nI0419 09:12:22.049595   12946 pod.go:281] do not need to create container alpine-0989527933 of pod pod-nGrWzvoRNi[0]\nI0419 09:12:22.049609   12946 pod.go:356] container name alpine-0989527933, image alpine\nI0419 09:12:22.051137   12946 pod.go:373] container info config &{7383484ceb06   false false false map[]  false false false [] 0xc820fc9360 false alpine map[]   false  [] map[] }, Cmd [/bin/sh], Args []\nI0419 09:12:22.051401   12946 pod.go:396] Container Info is \n&{7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea 7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea     [] [/bin/sh] map[] false}\nI0419 09:12:22.051677   12946 daemondb.go:91] try set container list for pod pod-nGrWzvoRNi: [7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea]\nI0419 09:12:22.051983   12946 daemon.go:96] no existing VM for pod pod-nGrWzvoRNi: leveldb: not found\nI0419 09:12:22.052004   12946 daemon.go:106] 1 pod have been loaded\nI0419 09:12:22.052020   12946 daemon.go:108] container in pod pod-nGrWzvoRNi status: [0xc82112e000]\nI0419 09:12:22.052034   12946 daemon.go:109] container in pod pod-nGrWzvoRNi spec: [{alpine-0989527933 alpine [] / [] true map[] [] [] [{/etc/hosts etchosts-volume false}] [{/etc/resolv.conf pod-nGrWzvoRNi-resolvconf 0644  }] never}]\nI0419 09:12:22.052639   12946 hyperd.go:234] Hyper daemon: 0.5.0 0\nI0419 09:12:22.052657   12946 server.go:199] Registering routers\nI0419 09:12:22.052772   12946 server.go:204] Registering GET, /container/info\nI0419 09:12:22.059500   12946 server.go:204] Registering GET, /container/logs\nI0419 09:12:22.059704   12946 server.go:204] Registering GET, /exitcode\nI0419 09:12:22.059852   12946 server.go:204] Registering POST, /container/create\nI0419 09:12:22.060025   12946 server.go:204] Registering POST, /container/rename\nI0419 09:12:22.060185   12946 server.go:204] Registering POST, /container/commit\nI0419 09:12:22.060345   12946 server.go:204] Registering POST, /container/kill\nI0419 09:12:22.060503   12946 server.go:204] Registering POST, /exec\nI0419 09:12:22.060637   12946 server.go:204] Registering POST, /attach\nI0419 09:12:22.060767   12946 server.go:204] Registering POST, /tty/resize\nI0419 09:12:22.060907   12946 server.go:204] Registering GET, /pod/info\nI0419 09:12:22.061039   12946 server.go:204] Registering GET, /pod/stats\nI0419 09:12:22.061175   12946 server.go:204] Registering GET, /list\nI0419 09:12:22.061291   12946 server.go:204] Registering POST, /pod/create\nI0419 09:12:22.061421   12946 server.go:204] Registering POST, /pod/labels\nI0419 09:12:22.061560   12946 server.go:204] Registering POST, /pod/start\nI0419 09:12:22.061703   12946 server.go:204] Registering POST, /pod/stop\nI0419 09:12:22.061828   12946 server.go:204] Registering POST, /pod/kill\nI0419 09:12:22.061974   12946 server.go:204] Registering POST, /pod/pause\nI0419 09:12:22.062112   12946 server.go:204] Registering POST, /pod/unpause\nI0419 09:12:22.062670   12946 server.go:204] Registering POST, /vm/create\nI0419 09:12:22.062806   12946 server.go:204] Registering DELETE, /pod\nI0419 09:12:22.062893   12946 server.go:204] Registering DELETE, /vm\nI0419 09:12:22.062976   12946 server.go:204] Registering GET, /service/list\nI0419 09:12:22.063089   12946 server.go:204] Registering POST, /service/add\nI0419 09:12:22.063198   12946 server.go:204] Registering POST, /service/update\nI0419 09:12:22.063316   12946 server.go:204] Registering DELETE, /service\nI0419 09:12:22.063415   12946 server.go:204] Registering GET, /images/get\nI0419 09:12:22.063521   12946 server.go:204] Registering POST, /image/create\nI0419 09:12:22.063642   12946 server.go:204] Registering POST, /image/load\nI0419 09:12:22.063747   12946 server.go:204] Registering POST, /image/push\nI0419 09:12:22.063871   12946 server.go:204] Registering DELETE, /image\nI0419 09:12:22.063967   12946 server.go:204] Registering GET, /_ping\nI0419 09:12:22.064058   12946 server.go:204] Registering GET, /info\nI0419 09:12:22.064150   12946 server.go:204] Registering GET, /version\nI0419 09:12:22.064246   12946 server.go:204] Registering POST, /auth\nI0419 09:12:22.064333   12946 server.go:204] Registering POST, /image/build\nI0419 09:12:22.064470   12946 server.go:95] API listen on /var/run/hyper.sock\nI0419 09:12:47.305783   12946 server.go:152] Calling GET /v0.5.0/list\nI0419 09:12:47.305876   12946 pod_routes.go:52] List type is pod, specified pod: [], specified vm: [], list auxiliary pod: false\nI0419 09:13:09.305369   12946 server.go:152] Calling POST /v0.5.0/pod/start\nI0419 09:13:09.305455   12946 run.go:78] pod:pod-nGrWzvoRNi, vm:\nI0419 09:13:09.305478   12946 pod.go:71] lock pod for operation start\nI0419 09:13:09.305490   12946 pod.go:74] successfully lock pod for operation start\nI0419 09:13:09.305509   12946 vm.go:120] The config: kernel=/mnt/tank/HYPER/kernel, initrd=/mnt/tank/HYPER/hyper-initrd.img\nI0419 09:13:09.305764   12946 pod.go:709] container ID: 7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea, mountId 7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea\nI0419 09:13:09.306106   12946 qemu_process.go:63] cmdline arguments: -machine pc-i440fx-2.0,accel=kvm,usb=off -global kvm-pit.lost_tick_policy=discard -cpu host -drive if=pflash,file=/mnt/tank/HYPER/bios-qboot.bin,readonly=on -drive if=pflash,file=/mnt/tank/HYPER/cbfs-qboot.rom,readonly=on -realtime mlock=off -no-user-config -nodefaults -no-hpet -rtc base=utc,driftfix=slew -no-reboot -display none -boot strict=on -m 512 -smp 1 -qmp unix:/var/run/hyper/vm-bbuWmTciLc/qmp.sock,server,nowait -serial unix:/var/run/hyper/vm-bbuWmTciLc/console.sock,server,nowait -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x2 -device virtio-scsi-pci,id=scsi0,bus=pci.0,addr=0x3 -chardev socket,id=charch0,path=/var/run/hyper/vm-bbuWmTciLc/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0 -chardev socket,id=charch1,path=/var/run/hyper/vm-bbuWmTciLc/tty.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1 -fsdev local,id=virtio9p,path=/var/run/hyper/vm-bbuWmTciLc/share_dir,security_model=none -device virtio-9p-pci,fsdev=virtio9p,mount_tag=share_dir\nI0419 09:13:09.309285   12946 qemu_process.go:74] starting daemon with pid: 13079\nI0419 09:13:09.346809   12946 qmp_handler.go:167] connected to /var/run/hyper/vm-bbuWmTciLc/qmp.sock\nI0419 09:13:09.346916   12946 qmp_handler.go:177] begin qmp init...\nI0419 09:13:09.347062   12946 tty.go:157] tty socket connected\nI0419 09:13:09.347116   12946 tty.go:100] tty: trying to read 12 bytes\nI0419 09:13:09.347251   12946 init_comm.go:109] Wating for init messages...\nI0419 09:13:09.347335   12946 init_comm.go:72] trying to read 8 bytes\nI0419 09:13:09.387480   12946 init_comm.go:29] connected to /var/run/hyper/vm-bbuWmTciLc/console.sock\nI0419 09:13:09.387613   12946 init_comm.go:36] connected /var/run/hyper/vm-bbuWmTciLc/console.sock as telnet mode.\nI0419 09:13:09.391193   12946 volumes.go:28] trying to bind dir /mnt/tank/HYPER/hosts/pod-nGrWzvoRNi/hosts to /var/run/hyper/vm-bbuWmTciLc/share_dir/iVfeHKcqnR\nI0419 09:13:09.391477   12946 tank.go:77] dir /mnt/tank/HYPER/hosts/pod-nGrWzvoRNi/hosts is bound to iVfeHKcqnR\nI0419 09:13:09.391592   12946 pod.go:840] configuring log driver [json-file] for pod-nGrWzvoRNi\nI0419 09:13:09.391642   12946 pod.go:859] configure container log to /var/run/hyper/Pods/pod-nGrWzvoRNi/7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea-json.log\nI0419 09:13:09.391743   12946 pod.go:865] configured logger for pod-nGrWzvoRNi/7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea (/alpine-0989527933)\nI0419 09:13:09.391928   12946 vm.go:180] hyperHandlePodEvent pod pod-nGrWzvoRNi, vm vm-bbuWmTciLc\nI0419 09:13:09.391980   12946 hypervisor.go:29] main event loop got message 34(COMMAND_ATTACH)\nI0419 09:13:09.392043   12946 vm_states.go:281] attachment log-fqj0xw00 is pending\nI0419 09:13:09.392077   12946 hypervisor.go:29] main event loop got message 23(COMMAND_RUN_POD)\nI0419 09:13:09.392112   12946 vm_states.go:556] got spec, prepare devices\nI0419 09:13:09.392232   12946 context.go:244] #0 Container Info:\nI0419 09:13:09.392514   12946 context.go:247] \n{\n...|    \"Id\": \"7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea\",\n...|    \"MountId\": \"7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea\",\n...|    \"Rootfs\": \"\",\n...|    \"Image\": \"/7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea/rootfs\",\n...|    \"Fstype\": \"dir\",\n...|    \"Workdir\": \"\",\n...|    \"Entrypoint\": null,\n...|    \"Cmd\": [\n...|        \"/bin/sh\"\n...|    ],\n...|    \"Envs\": {},\n...|    \"Initialize\": false\n...|}\nI0419 09:13:09.392654   12946 devicemap.go:235] insert volume etchosts-volume to /etc/hosts on 0\nI0419 09:13:09.393396   12946 vm_states.go:72] initial vm spec: {\n        \"hostname\": \"alpine-0989527933\",\n        \"containers\": [\n            {\n                \"id\": \"7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea\",\n                \"rootfs\": \"\",\n                \"image\": \"/7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea/rootfs\",\n                \"fsmap\": [\n                    {\n                        \"source\": \"iVfeHKcqnR\",\n                        \"path\": \"/etc/hosts\",\n                        \"readOnly\": false,\n                        \"dockerVolume\": false\n                    }\n                ],\n                \"process\": {\n                    \"terminal\": true,\n                    \"stdio\": 1,\n                    \"args\": [\n                        \"/bin/sh\"\n                    ],\n                    \"workdir\": \"/\"\n                },\n                \"restartPolicy\": \"never\",\n                \"initialize\": false\n            }\n        ],\n        \"shareDir\": \"share_dir\"\n    }\nI0419 09:13:09.393519   12946 context.go:176] found container 7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea at 0\nI0419 09:13:09.393600   12946 vm_states.go:80] attach pending client log-fqj0xw00 for 7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea\nI0419 09:13:09.393656   12946 vm_states.go:304] Connecting tty for 7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea on session 1\nI0419 09:13:09.393735   12946 context.go:212] VM vm-bbuWmTciLc: state change from INIT to 'STARTING'\nI0419 09:13:09.397881   12946 hypervisor.go:29] main event loop got message 13(EVENT_INTERFACE_ADD)\nI0419 09:13:09.397980   12946 qmp_wrapper.go:90] send net to qemu at 20\nI0419 09:13:09.398127   12946 qmp_handler.go:370] got new session during initializing\nI0419 09:13:09.647189   12946 qmp_handler.go:186] got qmp welcome, now sending command qmp_capabilities\nI0419 09:13:09.647406   12946 qmp_handler.go:201] waiting for response\nI0419 09:13:09.647720   12946 qmp_handler.go:103] got a message {\"return\": {}}\nI0419 09:13:09.647855   12946 qmp_handler.go:210] got for response\nI0419 09:13:09.647905   12946 qmp_handler.go:213] QMP connection initialized\nI0419 09:13:09.647982   12946 qmp_handler.go:346] QMP initialzed, go into main QMP loop\nI0419 09:13:09.648095   12946 qmp_handler.go:137] Begin receive QMP message\nI0419 09:13:09.648114   12946 qmp_handler.go:225] Begin process command session\nI0419 09:13:09.648223   12946 qmp_handler.go:238] send cmd with scm (24 bytes) (1) {\"execute\":\"getfd\",\"arguments\":{\"fdname\":\"fdeth0\"}}\nI0419 09:13:09.649272   12946 qmp_handler.go:103] got a message {\"return\": {}}\nI0419 09:13:09.649679   12946 qmp_handler.go:243] sending command (1) {\"execute\":\"netdev_add\",\"arguments\":{\"fd\":\"fdeth0\",\"id\":\"eth0\",\"type\":\"tap\"}}\nI0419 09:13:09.650757   12946 qmp_handler.go:103] got a message {\"return\": {}}\nI0419 09:13:09.651080   12946 qmp_handler.go:243] sending command (1) {\"execute\":\"device_add\",\"arguments\":{\"addr\":\"0x5\",\"bus\":\"pci.0\",\"driver\":\"virtio-net-pci\",\"id\":\"eth0\",\"mac\":\"52:54:29:43:5a:30\",\"netdev\":\"eth0\"}}\nI0419 09:13:09.677339   12946 qmp_handler.go:103] got a message {\"return\": {}}\nI0419 09:13:09.677586   12946 qmp_handler.go:302] session finished, buffer size 1\nI0419 09:13:09.677644   12946 qmp_handler.go:305] success \nI0419 09:13:09.677696   12946 hypervisor.go:29] main event loop got message 15(EVENT_INTERFACE_INSERTED)\nI0419 09:13:09.677773   12946 vm_states.go:585] device ready, could run pod.\nI0419 09:13:09.752428   12946 init_comm.go:44] [console] Initializing cgroup subsys cpu\nI0419 09:13:09.753747   12946 init_comm.go:44] [console] Linux version 4.4.0-hyper+ (laijs@ubox) (gcc version 5.2.1 20151010 (Ubuntu 5.2.1-22ubuntu2) ) #0 SMP Mon Jan 25 01:10:46 CST 2016\nI0419 09:13:09.754172   12946 init_comm.go:44] [console] Command line: console=ttyS0 panic=1 no_timer_check\nI0419 09:13:09.754270   12946 init_comm.go:44] [console] \nI0419 09:13:09.754524   12946 init_comm.go:44] [console] x86/fpu: Legacy x87 FPU detected.\nI0419 09:13:09.754808   12946 init_comm.go:44] [console] x86/fpu: Using 'lazy' FPU context switches.\nI0419 09:13:09.755087   12946 init_comm.go:44] [console] e820: BIOS-provided physical RAM map:\nI0419 09:13:09.755607   12946 init_comm.go:44] [console] BIOS-e820: [mem 0x0000000000000000-0x000000000009fbff] usable\nI0419 09:13:09.756092   12946 init_comm.go:44] [console] BIOS-e820: [mem 0x000000000009fc00-0x000000000009ffff] reserved\nI0419 09:13:09.756615   12946 init_comm.go:44] [console] BIOS-e820: [mem 0x00000000000d0000-0x00000000000effff] ACPI NVS\nI0419 09:13:09.757139   12946 init_comm.go:44] [console] BIOS-e820: [mem 0x00000000000f0000-0x00000000000fffff] reserved\nI0419 09:13:09.757650   12946 init_comm.go:44] [console] BIOS-e820: [mem 0x0000000000100000-0x000000001fffffff] usable\nI0419 09:13:09.758164   12946 init_comm.go:44] [console] BIOS-e820: [mem 0x00000000feffc000-0x00000000feffffff] reserved\nI0419 09:13:09.758497   12946 init_comm.go:44] [console] NX (Execute Disable) protection: active\nI0419 09:13:09.758739   12946 init_comm.go:44] [console] DMI not present or invalid.\nI0419 09:13:09.758967   12946 init_comm.go:44] [console] Hypervisor detected: KVM\nI0419 09:13:09.759397   12946 init_comm.go:44] [console] e820: last_pfn = 0x20000 max_arch_pfn = 0x400000000\nI0419 09:13:09.759897   12946 init_comm.go:44] [console] x86/PAT: Configuration [0-7]: WB  WC  UC- UC  WB  WC  UC- WT\nI0419 09:13:09.760013   12946 init_comm.go:44] [console] MTRR: Disabled\nI0419 09:13:09.760399   12946 init_comm.go:44] [console] CPU MTRRs all blank - virtualized system.\nI0419 09:13:09.760714   12946 init_comm.go:44] [console] Using GB pages for direct mapping\nI0419 09:13:09.761042   12946 init_comm.go:44] [console] RAMDISK: [mem 0x1ff10000-0x1fffffff]\nI0419 09:13:09.761460   12946 init_comm.go:44] [console] ACPI: Early table checksum verification disabled\nI0419 09:13:09.761917   12946 init_comm.go:44] [console] ACPI: RSDP 0x00000000000F2960 000014 (v00 BOCHS )\nI0419 09:13:09.762593   12946 init_comm.go:44] [console] ACPI: RSDT 0x00000000000EFB95 000034 (v01 BOCHS  BXPCRSDT 00000001 BXPC 00000001)\nI0419 09:13:09.763312   12946 init_comm.go:44] [console] ACPI: FACP 0x00000000000EF1C0 000074 (v01 BOCHS  BXPCFACP 00000001 BXPC 00000001)\nI0419 09:13:09.764010   12946 init_comm.go:44] [console] ACPI: DSDT 0x00000000000EE040 001180 (v01 BOCHS  BXPCDSDT 00000001 BXPC 00000001)\nI0419 09:13:09.764303   12946 init_comm.go:44] [console] ACPI: FACS 0x00000000000EE000 000040\nI0419 09:13:09.765052   12946 init_comm.go:44] [console] ACPI: SSDT 0x00000000000EF234 0008E9 (v01 BOCHS  BXPCSSDT 00000001 BXPC 00000001)\nI0419 09:13:09.765777   12946 init_comm.go:44] [console] ACPI: APIC 0x00000000000EFB1D 000078 (v01 BOCHS  BXPCAPIC 00000001 BXPC 00000001)\nI0419 09:13:09.766416   12946 init_comm.go:44] [console] ACPI: RSDT 0x00000000000EFB95 000034 (v01 BOCHS  BXPCRSDT 00000001 BXPC 00000001)\nI0419 09:13:09.766678   12946 init_comm.go:44] [console] No NUMA configuration found\nI0419 09:13:09.767229   12946 init_comm.go:44] [console] Faking a node at [mem 0x0000000000000000-0x000000001fffffff]\nI0419 09:13:09.767683   12946 init_comm.go:44] [console] NODE_DATA(0) allocated [mem 0x1fefe000-0x1ff0ffff]\nI0419 09:13:09.768027   12946 init_comm.go:44] [console] kvm-clock: Using msrs 4b564d01 and 4b564d00\nI0419 09:13:09.768497   12946 init_comm.go:44] [console] kvm-clock: cpu 0, msr 0:1fefd001, primary cpu clock\nI0419 09:13:09.769454   12946 init_comm.go:44] [console] clocksource: kvm-clock: mask: 0xffffffffffffffff max_cycles: 0x1cd42e4dffb, max_idle_ns: 881590591483 ns\nI0419 09:13:09.769581   12946 init_comm.go:44] [console] Zone ranges:\nI0419 09:13:09.770082   12946 init_comm.go:44] [console]   DMA      [mem 0x0000000000001000-0x0000000000ffffff]\nI0419 09:13:09.770589   12946 init_comm.go:44] [console]   DMA32    [mem 0x0000000001000000-0x000000001fffffff]\nI0419 09:13:09.770767   12946 init_comm.go:44] [console]   Normal   empty\nI0419 09:13:09.771070   12946 init_comm.go:44] [console] Movable zone start for each node\nI0419 09:13:09.771323   12946 init_comm.go:44] [console] Early memory node ranges\nI0419 09:13:09.771850   12946 init_comm.go:44] [console]   node   0: [mem 0x0000000000001000-0x000000000009efff]\nI0419 09:13:09.772369   12946 init_comm.go:44] [console]   node   0: [mem 0x0000000000100000-0x000000001fffffff]\nI0419 09:13:09.772986   12946 init_comm.go:44] [console] Initmem setup node 0 [mem 0x0000000000001000-0x000000001fffffff]\nI0419 09:13:09.773272   12946 init_comm.go:44] [console] ACPI: PM-Timer IO Port: 0x608\nI0419 09:13:09.773742   12946 init_comm.go:44] [console] ACPI: LAPIC_NMI (acpi_id[0xff] dfl dfl lint[0x1])\nI0419 09:13:09.774307   12946 init_comm.go:44] [console] IOAPIC[0]: apic_id 0, version 17, address 0xfec00000, GSI 0-23\nI0419 09:13:09.774843   12946 init_comm.go:44] [console] ACPI: INT_SRC_OVR (bus 0 bus_irq 0 global_irq 2 dfl dfl)\nI0419 09:13:09.775397   12946 init_comm.go:44] [console] ACPI: INT_SRC_OVR (bus 0 bus_irq 5 global_irq 5 high level)\nI0419 09:13:09.775958   12946 init_comm.go:44] [console] ACPI: INT_SRC_OVR (bus 0 bus_irq 9 global_irq 9 high level)\nI0419 09:13:09.776521   12946 init_comm.go:44] [console] ACPI: INT_SRC_OVR (bus 0 bus_irq 10 global_irq 10 high level)\nI0419 09:13:09.777099   12946 init_comm.go:44] [console] ACPI: INT_SRC_OVR (bus 0 bus_irq 11 global_irq 11 high level)\nI0419 09:13:09.777571   12946 init_comm.go:44] [console] Using ACPI (MADT) for SMP configuration information\nI0419 09:13:09.777951   12946 init_comm.go:44] [console] smpboot: Allowing 1 CPUs, 0 hotplug CPUs\nI0419 09:13:09.778469   12946 init_comm.go:44] [console] e820: [mem 0x20000000-0xfeffbfff] available for PCI devices\nI0419 09:13:09.778834   12946 init_comm.go:44] [console] Booting paravirtualized kernel on KVM\nI0419 09:13:09.779767   12946 init_comm.go:44] [console] clocksource: refined-jiffies: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 1910969940391419 ns\nI0419 09:13:09.780389   12946 init_comm.go:44] [console] setup_percpu: NR_CPUS:8 nr_cpumask_bits:8 nr_cpu_ids:1 nr_node_ids:1\nI0419 09:13:09.781082   12946 init_comm.go:44] [console] PERCPU: Embedded 31 pages/cpu @ffff88001fc00000 s89560 r8192 d29224 u2097152\nI0419 09:13:09.781362   12946 init_comm.go:44] [console] KVM setup async PF for cpu 0\nI0419 09:13:09.781692   12946 init_comm.go:44] [console] kvm-stealtime: cpu 0, msr 1fc0d480\nI0419 09:13:09.782362   12946 init_comm.go:44] [console] Built 1 zonelists in Node order, mobility grouping on.  Total pages: 129161\nI0419 09:13:09.782571   12946 init_comm.go:44] [console] Policy zone: DMA32\nI0419 09:13:09.783106   12946 init_comm.go:44] [console] Kernel command line: console=ttyS0 panic=1 no_timer_check\nI0419 09:13:09.783135   12946 init_comm.go:44] [console] \nI0419 09:13:09.783603   12946 init_comm.go:44] [console] PID hash table entries: 2048 (order: 2, 16384 bytes)\nI0419 09:13:09.784816   12946 init_comm.go:44] [console] Memory: 507628K/523896K available (4225K kernel code, 452K rwdata, 1280K rodata, 860K init, 752K bss, 16268K reserved, 0K cma-reserved)\nI0419 09:13:09.785127   12946 init_comm.go:44] [console] Hierarchical RCU implementation.\nI0419 09:13:09.785551   12946 init_comm.go:44] [console]    Build-time adjustment of leaf fanout to 64.\nI0419 09:13:09.786048   12946 init_comm.go:44] [console]    RCU restricting CPUs from NR_CPUS=8 to nr_cpu_ids=1.\nI0419 09:13:09.786620   12946 init_comm.go:44] [console] RCU: Adjusting geometry for rcu_fanout_leaf=64, nr_cpu_ids=1\nI0419 09:13:09.786900   12946 init_comm.go:44] [console] NR_IRQS:4352 nr_irqs:256 16\nI0419 09:13:09.787247   12946 init_comm.go:44] [console]    Offload RCU callbacks from all CPUs\nI0419 09:13:09.787595   12946 init_comm.go:44] [console]    Offload RCU callbacks from CPUs: 0.\nI0419 09:13:09.787842   12946 init_comm.go:44] [console] Console: colour CGA 80x25\nI0419 09:13:09.788081   12946 init_comm.go:44] [console] console [ttyS0] enabled\nI0419 09:13:09.788451   12946 init_comm.go:44] [console] tsc: Detected 3422.964 MHz processor\nI0419 09:13:09.789128   12946 init_comm.go:44] [console] Calibrating delay loop (skipped) preset value.. 6845.92 BogoMIPS (lpj=3422964)\nI0419 09:13:09.789463   12946 init_comm.go:44] [console] pid_max: default: 32768 minimum: 301\nI0419 09:13:09.789753   12946 init_comm.go:44] [console] ACPI: Core revision 20150930\nI0419 09:13:09.791107   12946 init_comm.go:44] [console] ACPI: 2 ACPI AML tables successfully acquired and loaded\nI0419 09:13:09.791731   12946 init_comm.go:44] [console] Dentry cache hash table entries: 65536 (order: 7, 524288 bytes)\nI0419 09:13:09.792451   12946 init_comm.go:44] [console] Inode-cache hash table entries: 32768 (order: 6, 262144 bytes)\nI0419 09:13:09.793043   12946 init_comm.go:44] [console] Mount-cache hash table entries: 1024 (order: 1, 8192 bytes)\nI0419 09:13:09.793567   12946 init_comm.go:44] [console] Mountpoint-cache hash table entries: 1024 (order: 1, 8192 bytes)\nI0419 09:13:09.794162   12946 init_comm.go:44] [console] Last level iTLB entries: 4KB 512, 2MB 16, 4MB 8\nI0419 09:13:09.794685   12946 init_comm.go:44] [console] Last level dTLB entries: 4KB 512, 2MB 128, 4MB 64, 1GB 0\nI0419 09:13:09.800677   12946 init_comm.go:44] [console] Freeing SMP alternatives memory: 20K (ffffffff816ac000 - ffffffff816b1000)\nI0419 09:13:09.803964   12946 init_comm.go:44] [console] ..TIMER: vector=0x30 apic1=0 pin1=2 apic2=-1 pin2=-1\nI0419 09:13:09.905929   12946 init_comm.go:44] [console] smpboot: CPU0: AMD Phenom(tm) II X4 965 Processor (family: 0x10, model: 0x4, stepping: 0x3)\nI0419 09:13:09.906674   12946 init_comm.go:44] [console] Performance Events: Broken PMU hardware detected, using software events only.\nI0419 09:13:09.907252   12946 init_comm.go:44] [console] Failed to access perfctr msr (MSR c0010001 is ffffffffffffffff)\nI0419 09:13:09.907728   12946 init_comm.go:44] [console] x86: Booted up 1 node, 1 CPUs\nI0419 09:13:09.908198   12946 init_comm.go:44] [console] smpboot: Total of 1 processors activated (6845.92 BogoMIPS)\nI0419 09:13:09.908583   12946 init_comm.go:44] [console] devtmpfs: initialized\nI0419 09:13:09.912846   12946 init_comm.go:44] [console] clocksource: jiffies: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 1911260446275000 ns\nI0419 09:13:09.913277   12946 init_comm.go:44] [console] NET: Registered protocol family 16\nI0419 09:13:09.913637   12946 init_comm.go:44] [console] cpuidle: using governor ladder\nI0419 09:13:09.913900   12946 init_comm.go:44] [console] cpuidle: using governor menu\nI0419 09:13:09.914528   12946 init_comm.go:44] [console] ACPI: bus type PCI registered\nI0419 09:13:09.915032   12946 init_comm.go:44] [console] acpiphp: ACPI Hot Plug PCI Controller Driver version: 0.5\nI0419 09:13:09.915470   12946 init_comm.go:44] [console] PCI: Using configuration type 1 for base access\nI0419 09:13:09.915899   12946 init_comm.go:44] [console] PCI: Using configuration type 1 for extended access\nI0419 09:13:09.916760   12946 init_comm.go:44] [console] ACPI: Added _OSI(Module Device)\nI0419 09:13:09.917039   12946 init_comm.go:44] [console] ACPI: Added _OSI(Processor Device)\nI0419 09:13:09.917353   12946 init_comm.go:44] [console] ACPI: Added _OSI(3.0 _SCP Extensions)\nI0419 09:13:09.917797   12946 init_comm.go:44] [console] ACPI: Added _OSI(Processor Aggregator Device)\nI0419 09:13:09.919373   12946 init_comm.go:44] [console] ACPI: Interpreter enabled\nI0419 09:13:09.919645   12946 init_comm.go:44] [console] ACPI: (supports S0 S5)\nI0419 09:13:09.919922   12946 init_comm.go:44] [console] ACPI: Using IOAPIC for interrupt routing\nI0419 09:13:09.920747   12946 init_comm.go:44] [console] PCI: Using host bridge windows from ACPI; if necessary, use \"pci=nocrs\" and report a bug\nI0419 09:13:09.923656   12946 init_comm.go:44] [console] ACPI: PCI Root Bridge [PCI0] (domain 0000 [bus 00-ff])\nI0419 09:13:09.924330   12946 init_comm.go:44] [console] acpi PNP0A03:00: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]\nI0419 09:13:09.924884   12946 init_comm.go:44] [console] acpi PNP0A03:00: _OSC failed (AE_NOT_FOUND); disabling ASPM\nI0419 09:13:09.925269   12946 init_comm.go:44] [console] acpiphp: Slot [2] registered\nI0419 09:13:09.925594   12946 init_comm.go:44] [console] acpiphp: Slot [3] registered\nI0419 09:13:09.925796   12946 init_comm.go:44] [console] acpiphp: Slot [4] registered\nI0419 09:13:09.926073   12946 init_comm.go:44] [console] acpiphp: Slot [5] registered\nI0419 09:13:09.926332   12946 init_comm.go:44] [console] acpiphp: Slot [6] registered\nI0419 09:13:09.926626   12946 init_comm.go:44] [console] acpiphp: Slot [7] registered\nI0419 09:13:09.926901   12946 init_comm.go:44] [console] acpiphp: Slot [8] registered\nI0419 09:13:09.927158   12946 init_comm.go:44] [console] acpiphp: Slot [9] registered\nI0419 09:13:09.927439   12946 init_comm.go:44] [console] acpiphp: Slot [10] registered\nI0419 09:13:09.927728   12946 init_comm.go:44] [console] acpiphp: Slot [11] registered\nI0419 09:13:09.927991   12946 init_comm.go:44] [console] acpiphp: Slot [12] registered\nI0419 09:13:09.928284   12946 init_comm.go:44] [console] acpiphp: Slot [13] registered\nI0419 09:13:09.928575   12946 init_comm.go:44] [console] acpiphp: Slot [14] registered\nI0419 09:13:09.928833   12946 init_comm.go:44] [console] acpiphp: Slot [15] registered\nI0419 09:13:09.929134   12946 init_comm.go:44] [console] acpiphp: Slot [16] registered\nI0419 09:13:09.929429   12946 init_comm.go:44] [console] acpiphp: Slot [17] registered\nI0419 09:13:09.929734   12946 init_comm.go:44] [console] acpiphp: Slot [18] registered\nI0419 09:13:09.930026   12946 init_comm.go:44] [console] acpiphp: Slot [19] registered\nI0419 09:13:09.930315   12946 init_comm.go:44] [console] acpiphp: Slot [20] registered\nI0419 09:13:09.930628   12946 init_comm.go:44] [console] acpiphp: Slot [21] registered\nI0419 09:13:09.930925   12946 init_comm.go:44] [console] acpiphp: Slot [22] registered\nI0419 09:13:09.931219   12946 init_comm.go:44] [console] acpiphp: Slot [23] registered\nI0419 09:13:09.931523   12946 init_comm.go:44] [console] acpiphp: Slot [24] registered\nI0419 09:13:09.931826   12946 init_comm.go:44] [console] acpiphp: Slot [25] registered\nI0419 09:13:09.932133   12946 init_comm.go:44] [console] acpiphp: Slot [26] registered\nI0419 09:13:09.932432   12946 init_comm.go:44] [console] acpiphp: Slot [27] registered\nI0419 09:13:09.932732   12946 init_comm.go:44] [console] acpiphp: Slot [28] registered\nI0419 09:13:09.933035   12946 init_comm.go:44] [console] acpiphp: Slot [29] registered\nI0419 09:13:09.933347   12946 init_comm.go:44] [console] acpiphp: Slot [30] registered\nI0419 09:13:09.933655   12946 init_comm.go:44] [console] acpiphp: Slot [31] registered\nI0419 09:13:09.933956   12946 init_comm.go:44] [console] PCI host bridge to bus 0000:00\nI0419 09:13:09.934527   12946 init_comm.go:44] [console] pci_bus 0000:00: root bus resource [io  0x0000-0x0cf7 window]\nI0419 09:13:09.935104   12946 init_comm.go:44] [console] pci_bus 0000:00: root bus resource [io  0x0d00-0xadff window]\nI0419 09:13:09.935668   12946 init_comm.go:44] [console] pci_bus 0000:00: root bus resource [io  0xae0f-0xaeff window]\nI0419 09:13:09.936222   12946 init_comm.go:44] [console] pci_bus 0000:00: root bus resource [io  0xaf20-0xafdf window]\nI0419 09:13:09.936799   12946 init_comm.go:44] [console] pci_bus 0000:00: root bus resource [io  0xafe4-0xffff window]\nI0419 09:13:09.937412   12946 init_comm.go:44] [console] pci_bus 0000:00: root bus resource [mem 0x000a0000-0x000bffff window]\nI0419 09:13:09.938058   12946 init_comm.go:44] [console] pci_bus 0000:00: root bus resource [mem 0x20000000-0xfebfffff window]\nI0419 09:13:09.938489   12946 init_comm.go:44] [console] pci_bus 0000:00: root bus resource [bus 00-ff]\nI0419 09:13:09.939962   12946 init_comm.go:44] [console] pci 0000:00:01.1: legacy IDE quirk: reg 0x10: [io  0x01f0-0x01f7]\nI0419 09:13:09.940506   12946 init_comm.go:44] [console] pci 0000:00:01.1: legacy IDE quirk: reg 0x14: [io  0x03f6]\nI0419 09:13:09.941124   12946 init_comm.go:44] [console] pci 0000:00:01.1: legacy IDE quirk: reg 0x18: [io  0x0170-0x0177]\nI0419 09:13:09.941678   12946 init_comm.go:44] [console] pci 0000:00:01.1: legacy IDE quirk: reg 0x1c: [io  0x0376]\nI0419 09:13:09.942595   12946 init_comm.go:44] [console] pci 0000:00:01.3: quirk: [io  0x0600-0x063f] claimed by PIIX4 ACPI\nI0419 09:13:09.943194   12946 init_comm.go:44] [console] pci 0000:00:01.3: quirk: [io  0x0700-0x070f] claimed by PIIX4 SMB\nI0419 09:13:09.945717   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKA] (IRQs 5 10 11)\nI0419 09:13:09.946300   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKB] (IRQs 5 10 11)\nI0419 09:13:09.946856   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKC] (IRQs 5 10 11)\nI0419 09:13:09.947414   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKD] (IRQs 5 10 11)\nI0419 09:13:09.947871   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKS] (IRQs 9)\nI0419 09:13:09.948552   12946 init_comm.go:44] [console] ACPI: Enabled 16 GPEs in block 00 to 0F\nI0419 09:13:09.948797   12946 init_comm.go:44] [console] vgaarb: loaded\nI0419 09:13:09.949109   12946 init_comm.go:44] [console] SCSI subsystem initialized\nI0419 09:13:09.949445   12946 init_comm.go:44] [console] dmi: Firmware registration failed.\nI0419 09:13:09.949756   12946 init_comm.go:44] [console] PCI: Using ACPI for IRQ routing\nI0419 09:13:09.950382   12946 init_comm.go:44] [console] clocksource: Switched to clocksource kvm-clock\nI0419 09:13:09.950653   12946 init_comm.go:44] [console] pnp: PnP ACPI init\nI0419 09:13:09.951273   12946 init_comm.go:44] [console] pnp: PnP ACPI: found 5 devices\nI0419 09:13:09.957625   12946 init_comm.go:44] [console] clocksource: acpi_pm: mask: 0xffffff max_cycles: 0xffffff, max_idle_ns: 2085701024 ns\nI0419 09:13:09.958255   12946 init_comm.go:44] [console] pci 0000:00:05.0: BAR 6: assigned [mem 0x20000000-0x2003ffff pref]\nI0419 09:13:09.958803   12946 init_comm.go:44] [console] pci 0000:00:02.0: BAR 1: assigned [mem 0x20040000-0x20040fff]\nI0419 09:13:09.959368   12946 init_comm.go:44] [console] pci 0000:00:03.0: BAR 1: assigned [mem 0x20041000-0x20041fff]\nI0419 09:13:09.959900   12946 init_comm.go:44] [console] pci 0000:00:04.0: BAR 1: assigned [mem 0x20042000-0x20042fff]\nI0419 09:13:09.960418   12946 init_comm.go:44] [console] pci 0000:00:05.0: BAR 1: assigned [mem 0x20043000-0x20043fff]\nI0419 09:13:09.960901   12946 init_comm.go:44] [console] pci 0000:00:03.0: BAR 0: assigned [io  0x1000-0x103f]\nI0419 09:13:09.961385   12946 init_comm.go:44] [console] pci 0000:00:04.0: BAR 0: assigned [io  0x1040-0x107f]\nI0419 09:13:09.961855   12946 init_comm.go:44] [console] pci 0000:00:02.0: BAR 0: assigned [io  0x1080-0x109f]\nI0419 09:13:09.962325   12946 init_comm.go:44] [console] pci 0000:00:05.0: BAR 0: assigned [io  0x10a0-0x10bf]\nI0419 09:13:09.962791   12946 init_comm.go:44] [console] pci 0000:00:01.1: BAR 4: assigned [io  0x10c0-0x10cf]\nI0419 09:13:09.963118   12946 init_comm.go:44] [console] NET: Registered protocol family 2\nI0419 09:13:09.963732   12946 init_comm.go:44] [console] TCP established hash table entries: 4096 (order: 3, 32768 bytes)\nI0419 09:13:09.964209   12946 init_comm.go:44] [console] TCP bind hash table entries: 4096 (order: 4, 65536 bytes)\nI0419 09:13:09.964687   12946 init_comm.go:44] [console] TCP: Hash tables configured (established 4096 bind 4096)\nI0419 09:13:09.965136   12946 init_comm.go:44] [console] UDP hash table entries: 256 (order: 1, 8192 bytes)\nI0419 09:13:09.965601   12946 init_comm.go:44] [console] UDP-Lite hash table entries: 256 (order: 1, 8192 bytes)\nI0419 09:13:09.965928   12946 init_comm.go:44] [console] NET: Registered protocol family 1\nI0419 09:13:09.966371   12946 init_comm.go:44] [console] pci 0000:00:00.0: Limiting direct PCI/PCI transfers\nI0419 09:13:09.966786   12946 init_comm.go:44] [console] pci 0000:00:01.0: PIIX3: Enabling Passive Release\nI0419 09:13:09.967232   12946 init_comm.go:44] [console] pci 0000:00:01.0: Activating ISA DMA hang workarounds\nI0419 09:13:09.967698   12946 init_comm.go:44] [console] Trying to unpack rootfs image as initramfs...\nI0419 09:13:09.980494   12946 init_comm.go:44] [console] Freeing initrd memory: 960K (ffff88001ff10000 - ffff880020000000)\nI0419 09:13:09.981125   12946 init_comm.go:44] [console] futex hash table entries: 256 (order: 2, 16384 bytes)\nI0419 09:13:09.981875   12946 init_comm.go:44] [console] SGI XFS with ACLs, security attributes, no debug enabled\nI0419 09:13:09.982328   12946 init_comm.go:44] [console] 9p: Installing v9fs 9p2000 file system support\nI0419 09:13:09.983354   12946 init_comm.go:44] [console] Block layer SCSI generic (bsg) driver version 0.4 loaded (major 254)\nI0419 09:13:09.983653   12946 init_comm.go:44] [console] io scheduler noop registered\nI0419 09:13:09.983998   12946 init_comm.go:44] [console] io scheduler cfq registered (default)\nI0419 09:13:09.984486   12946 init_comm.go:44] [console] pci_hotplug: PCI Hot Plug PCI Core version: 0.5\nI0419 09:13:09.985059   12946 init_comm.go:44] [console] pciehp: PCI Express Hot Plug Controller Driver version: 0.4\nI0419 09:13:09.985732   12946 init_comm.go:44] [console] Warning: Processor Platform Limit event detected, but not handled.\nI0419 09:13:09.986211   12946 init_comm.go:44] [console] Consider compiling CPUfreq support into your kernel.\nI0419 09:13:09.987011   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKB] enabled at IRQ 10\nI0419 09:13:09.987522   12946 init_comm.go:44] [console] virtio-pci 0000:00:02.0: enabling device (0000 -> 0003)\nI0419 09:13:09.988529   12946 init_comm.go:44] [console] virtio-pci 0000:00:02.0: virtio_pci: leaving for legacy driver\nI0419 09:13:09.989560   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKC] enabled at IRQ 11\nI0419 09:13:09.990084   12946 init_comm.go:44] [console] virtio-pci 0000:00:03.0: enabling device (0000 -> 0003)\nI0419 09:13:09.991189   12946 init_comm.go:44] [console] virtio-pci 0000:00:03.0: virtio_pci: leaving for legacy driver\nI0419 09:13:09.992233   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKD] enabled at IRQ 11\nI0419 09:13:09.992762   12946 init_comm.go:44] [console] virtio-pci 0000:00:04.0: enabling device (0000 -> 0003)\nI0419 09:13:09.994053   12946 init_comm.go:44] [console] virtio-pci 0000:00:04.0: virtio_pci: leaving for legacy driver\nI0419 09:13:09.995270   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKA] enabled at IRQ 10\nI0419 09:13:09.995811   12946 init_comm.go:44] [console] virtio-pci 0000:00:05.0: enabling device (0000 -> 0003)\nI0419 09:13:09.997313   12946 init_comm.go:44] [console] virtio-pci 0000:00:05.0: virtio_pci: leaving for legacy driver\nI0419 09:13:09.998520   12946 init_comm.go:44] [console] Serial: 8250/16550 driver, 4 ports, IRQ sharing enabled\nI0419 09:13:10.024831   12946 init_comm.go:44] [console] 00:04: ttyS0 at I/O 0x3f8 (irq = 4, base_baud = 115200) is a 16550A\nI0419 09:13:10.059118   12946 init_comm.go:44] [console] brd: module loaded\nI0419 09:13:10.059332   12946 init_comm.go:44] [console] loop: module loaded\nI0419 09:13:10.060672   12946 init_comm.go:44] [console] scsi host0: Virtio SCSI HBA\nI0419 09:13:10.080969   12946 init_comm.go:44] [console] e1000: Intel(R) PRO/1000 Network Driver - version 7.3.21-k8-NAPI\nI0419 09:13:10.081360   12946 init_comm.go:44] [console] e1000: Copyright (c) 1999-2006 Intel Corporation.\nI0419 09:13:10.081757   12946 init_comm.go:44] [console] NET: Registered protocol family 10\nI0419 09:13:10.082781   12946 init_comm.go:44] [console] NET: Registered protocol family 17\nI0419 09:13:10.083112   12946 init_comm.go:44] [console] 9pnet: Installing 9P2000 support\nI0419 09:13:10.084638   12946 init_comm.go:44] [console] registered taskstats version 1\nI0419 09:13:10.086269   12946 init_comm.go:44] [console] Freeing unused kernel memory: 860K (ffffffff815d5000 - ffffffff816ac000)\nI0419 09:13:10.087397   12946 init_comm.go:44] [console] create directory /dev\nI0419 09:13:10.087608   12946 init_comm.go:44] [console] create directory /sys\nI0419 09:13:10.087804   12946 init_comm.go:44] [console] create directory /proc\nI0419 09:13:10.088031   12946 init_comm.go:44] [console] uptime 0.19 0.01\nI0419 09:13:10.088158   12946 init_comm.go:44] [console] \nI0419 09:13:10.088379   12946 init_comm.go:44] [console] create directory /dev/pts\nI0419 09:13:10.088606   12946 init_comm.go:44] [console] create directory /dev\nI0419 09:13:10.088945   12946 init_comm.go:44] [console] open hyper channel /dev/vport0p1\nI0419 09:13:10.089254   12946 init_comm.go:44] [console] send ready message\nI0419 09:13:10.089475   12946 init_comm.go:44] [console] hyper send type 8, len 0\nI0419 09:13:10.089621   12946 init_comm.go:82] read 8/8 [length = 0]\nI0419 09:13:10.089680   12946 init_comm.go:86] data length is 8\nI0419 09:13:10.089719   12946 init_comm.go:119] Get init ready message\nI0419 09:13:10.089804   12946 hypervisor.go:29] main event loop got message 5(EVENT_INIT_CONNECTED)\nI0419 09:13:10.089845   12946 vm_states.go:599] begin to wait vm commands\nI0419 09:13:10.089842   12946 init_comm.go:163] got cmd:1\nI0419 09:13:10.089889   12946 init_comm.go:72] trying to read 8 bytes\nI0419 09:13:10.089893   12946 init_comm.go:244] send command 1 to init, payload: '{\"hostname\":\"alpine-0989527933\",\"containers\":[{\"id\":\"7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea\",\"rootfs\":\"\",\"image\":\"/7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea/rootfs\",\"fsmap\":[{\"source\":\"iVfeHKcqnR\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"terminal\":true,\"stdio\":1,\"args\":[\"/bin/sh\"],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":false}],\"interfaces\":[{\"device\":\"eth0\",\"ipAddress\":\"192.168.123.2\",\"netMask\":\"255.255.255.0\"}],\"routes\":[{\"dest\":\"0.0.0.0/0\",\"gateway\":\"192.168.123.1\",\"device\":\"eth0\"}],\"shareDir\":\"share_dir\"}'.\nI0419 09:13:10.090023   12946 init_comm.go:257] write 512 to init, payload: '\u0001f{\"hostname\":\"alpine-0989527933\",\"containers\":[{\"id\":\"7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea\",\"rootfs\":\"\",\"image\":\"/7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea/rootfs\",\"fsmap\":[{\"source\":\"iVfeHKcqnR\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"terminal\":true,\"stdio\":1,\"args\":[\"/bin/sh\"],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":false}],\"interfaces\":[{\"device\":\"eth0\",\"ipAddress\":\"192.168.123.2\",\"netMask\":\"255.255.255.'.\nI0419 09:13:10.089917   12946 vm.go:262] Get the response from VM, VM id is vm-bbuWmTciLc!\nI0419 09:13:10.090141   12946 init_comm.go:262] message sent, set pong timer\nI0419 09:13:10.090225   12946 init_comm.go:44] [console] channel sh.hyper.channel.1, directory sh.hyper.channel.0\nI0419 09:13:10.090319   12946 init_comm.go:44] [console] \nI0419 09:13:10.090798   12946 init_comm.go:44] [console] open hyper channel /dev/vport0p2\nI0419 09:13:10.091882   12946 init_comm.go:44] [console] hyper_init_event hyper channel event 0x613578, ops 0x613320, fd 4\nI0419 09:13:10.092376   12946 init_comm.go:44] [console] hyper_add_event add event fd 4, 0x613320\nI0419 09:13:10.093029   12946 init_comm.go:44] [console] hyper_init_event hyper ttyfd event 0x613540, ops 0x6132e0, fd 5\nI0419 09:13:10.093554   12946 init_comm.go:44] [console] hyper_add_event add event fd 5, 0x6132e0\nI0419 09:13:10.094246   12946 init_comm.go:44] [console] hyper_init_event hyper signal event 0x613508, ops 0x613360, fd 3\nI0419 09:13:10.094770   12946 init_comm.go:44] [console] hyper_add_event add event fd 3, 0x613360\nI0419 09:13:10.095112   12946 init_comm.go:44] [console] hyper_loop epoll_wait 1\nI0419 09:13:10.095853   12946 init_comm.go:44] [console] hyper_handle_event get event 1, de 0x613578, fd 4. ops 0x613320\nI0419 09:13:10.096499   12946 init_comm.go:44] [console] hyper_handle_event event EPOLLIN, de 0x613578, fd 4, 0x613320\nI0419 09:13:10.096777   12946 init_comm.go:44] [console] hyper_event_read\nI0419 09:13:10.097120   12946 init_comm.go:44] [console] already read 8 bytes data\nI0419 09:13:10.097470   12946 init_comm.go:44] [console] hyper send type 14, len 4\nI0419 09:13:10.097572   12946 init_comm.go:82] read 8/8 [length = 0]\nI0419 09:13:10.097586   12946 init_comm.go:86] data length is 12\nI0419 09:13:10.097593   12946 init_comm.go:72] trying to read 4 bytes\nI0419 09:13:10.097603   12946 init_comm.go:82] read 12/12 [length = 12]\nI0419 09:13:10.097617   12946 init_comm.go:72] trying to read 8 bytes\nI0419 09:13:10.097626   12946 init_comm.go:163] got cmd:14\nI0419 09:13:10.097635   12946 init_comm.go:231] get command NEXT\nI0419 09:13:10.097648   12946 init_comm.go:234] send 512, receive 8\nI0419 09:13:10.097781   12946 init_comm.go:44] [console] get length 614\nI0419 09:13:10.098261   12946 init_comm.go:44] [console] read 504 bytes data, total data 512\nI0419 09:13:10.098644   12946 init_comm.go:44] [console] hyper send type 14, len 4\nI0419 09:13:10.098730   12946 init_comm.go:82] read 8/8 [length = 0]\nI0419 09:13:10.098747   12946 init_comm.go:86] data length is 12\nI0419 09:13:10.098758   12946 init_comm.go:72] trying to read 4 bytes\nI0419 09:13:10.098773   12946 init_comm.go:82] read 12/12 [length = 12]\nI0419 09:13:10.098790   12946 init_comm.go:72] trying to read 8 bytes\nI0419 09:13:10.098808   12946 init_comm.go:163] got cmd:14\nI0419 09:13:10.098821   12946 init_comm.go:231] get command NEXT\nI0419 09:13:10.098832   12946 init_comm.go:234] send 512, receive 512\nI0419 09:13:10.098853   12946 init_comm.go:257] write 102 to init, payload: '0\"}],\"routes\":[{\"dest\":\"0.0.0.0/0\",\"gateway\":\"192.168.123.1\",\"device\":\"eth0\"}],\"shareDir\":\"share_dir\"}'.\nI0419 09:13:10.099172   12946 init_comm.go:44] [console] hyper_loop epoll_wait 1\nI0419 09:13:10.099597   12946 init_comm.go:44] [console] hyper_handle_event get event 1, de 0x613578, fd 4. ops 0x613320\nI0419 09:13:10.099999   12946 init_comm.go:44] [console] hyper_handle_event event EPOLLIN, de 0x613578, fd 4, 0x613320\nI0419 09:13:10.100162   12946 init_comm.go:44] [console] hyper_event_read\nI0419 09:13:10.100304   12946 init_comm.go:44] [console] get length 614\nI0419 09:13:10.100672   12946 init_comm.go:44] [console] read 102 bytes data, total data 614\nI0419 09:13:10.100895   12946 init_comm.go:44] [console] hyper send type 14, len 4\nI0419 09:13:10.100988   12946 init_comm.go:82] read 8/8 [length = 0]\nI0419 09:13:10.101009   12946 init_comm.go:86] data length is 12\nI0419 09:13:10.101021   12946 init_comm.go:72] trying to read 4 bytes\nI0419 09:13:10.101035   12946 init_comm.go:82] read 12/12 [length = 12]\nI0419 09:13:10.101059   12946 init_comm.go:72] trying to read 8 bytes\nI0419 09:13:10.101074   12946 init_comm.go:163] got cmd:14\nI0419 09:13:10.101087   12946 init_comm.go:231] get command NEXT\nI0419 09:13:10.101099   12946 init_comm.go:234] send 102, receive 102\nI0419 09:13:10.110793   12946 init_comm.go:44] [console] 0 0 0 1 0 0 2 66 7b 22 68 6f 73 74 6e 61 6d 65 22 3a 22 61 6c 70 69 6e 65 2d 30 39 38 39 35 32 37 39 33 33 22 2c 22 63 6f 6e 74 61 69 6e 65 72 73 22 3a 5b 7b 22 69 64 22 3a 22 37 33 38 33 34 38 34 63 65 62 30 36 34 33 65 62 33 39 37 38 37 64 33 31 36 33 38 64 66 34 63 38 37 66 38 31 65 32 38 32 30 38 63 61 63 32 63 31 33 31 63 62 63 35 63 39 37 61 33 32 64 61 65 61 22 2c 22 72 6f 6f 74 66 73 22 3a 22 22 2c 22 69 6d 61 67 65 22 3a 22 2f 37 33 38 33 34 38 34 63 65 62 30 36 34 33 65 62 33 39 37 38 37 64 33 31 36 33 38 64 66 34 63 38 37 66 38 31 65 32 38 32 30 38 63 61 63 32 63 31 33 31 63 62 63 35 63 39 37 61 33 32 64 61 65 61 2f 72 6f 6f 74 66 73 22 2c 22 66 73 6d 61 70 22 3a 5b 7b 22 73 6f 75 72 63 65 22 3a 22 69 56 66 65 48 4b 63 71 6e 52 22 2c 22 70 61 74 68 22 3a 22 2f 65 74 63 2f 68 6f 73 74 73 22 2c 22 72 65 61 64 4f 6e 6c 79 22 3a 66 61 6c 73 65 2c 22 64 6f 63 6b 65 72 56 6f 6c 75 6d 65 22 3a 66 61 6c 73 65 7d 5d 2c 22 70 72 6f 63 65 73 73 22 3a 7b 22 74 65 72 6d 69 6e 61 6c 22 3a 74 72 75 65 2c 22 73 74 64 69 6f 22 3a 31 2c 22 61 72 67 73 22 3a 5b 22 2f 62 69 6e 2f 73 68 22 5d 2c 22 77 6f 72 6b 64 69 72 22 3a 22 2f 22 7d 2c 22 72 65 73 74 61 72 74 50 6f 6c 69 63 79 22 3a 22 6e 65 76 65 72 22 2c 22 69 6e 69 74 69 61 6c 69 7a 65 22 3a 66 61 6c 73 65 7d 5d 2c 22 69 6e 74 65 72 66 61 63 65 73 22 3a 5b 7b 22 64 65 76 69 63 65 22 3a 22 65 74 68 30 22 2c 22 69 70 41 64 64 72 65 73 73 22 3a 22 31 39 32 2e 31 36 38 2e 31 32 33 2e 32 22 2c 22 6e 65 74 4d 61 73 6b 22 3a 22 32 35 35 2e 32 35 35 2e 32 35 35 2e 30 22 7d 5d 2c 22 72 6f 75 74 65 73 22 3a 5b 7b 22 64 65 73 74 22 3a 22 30 2e 30 2e 30 2e 30 2f 30 22 2c 22 67 61 74 65 77 61 79 22 3a 22 31 39 32 2e 31 36 38 2e 31 32 33 2e 31 22 2c 22 64 65 76 69 63 65 22 3a 22 65 74 68 30 22 7d 5d 2c 22 73 68 61 72 65 44 69 72 22 3a 22 73 68 61 72 65 5f 64 69 72 22 7d \nI0419 09:13:10.111087   12946 init_comm.go:44] [console]  hyper_channel_handle, type 1, len 614\nI0419 09:13:10.111247   12946 init_comm.go:44] [console] online_cpu()\nI0419 09:13:10.111440   12946 init_comm.go:44] [console] online_memory()\nI0419 09:13:10.111664   12946 init_comm.go:44] [console] try to online memory1\nI0419 09:13:10.111903   12946 init_comm.go:44] [console] online memory1 result: success\nI0419 09:13:10.112130   12946 init_comm.go:44] [console] try to online memory2\nI0419 09:13:10.112371   12946 init_comm.go:44] [console] online memory2 result: success\nI0419 09:13:10.112602   12946 init_comm.go:44] [console] try to online memory3\nI0419 09:13:10.112845   12946 init_comm.go:44] [console] online memory3 result: success\nI0419 09:13:10.117136   12946 init_comm.go:44] [console] call hyper_start_pod, json {\"hostname\":\"alpine-0989527933\",\"containers\":[{\"id\":\"7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea\",\"rootfs\":\"\",\"image\":\"/7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea/rootfs\",\"fsmap\":[{\"source\":\"iVfeHKcqnR\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"terminal\":true,\"stdio\":1,\"args\":[\"/bin/sh\"],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":false}],\"interfaces\":[{\"device\":\"eth0\",\"ipAddress\":\"192.168.123.2\",\"netMask\":\"255.255.255.0\"}],\"routes\":[{\"dest\":\"0.0.0.0/0\",\"gateway\":\"192.168.123.1\",\"device\":\"eth0\"}],\"shareDir\":\"share_dir\"}, len 606\nI0419 09:13:10.120694   12946 init_comm.go:44] [console] call hyper_start_pod, json {\"hostname\":\"alpine-0989527933\",\"containers\":[{\"id\":\"7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea\",\"rootfs\":\"\",\"image\":\"/7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea/rootfs\",\"fsmap\":[{\"source\":\"iVfeHKcqnR\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"terminal\":true,\"stdio\":1,\"args\":[\"/bin/sh\"],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":false}],\"interfaces\":[{\"device\":\"eth0\",\"ipAddress\":\"192.168.123.2\",\"netMask\":\"255.255.255.0\"}],\"routes\":[{\"dest\":\"0.0.0.0/0\",\"gateway\":\"192.168.123.1\",\"device\":\"eth0\"}],\"shareDir\":\"share_dir\"}, len 606\nI0419 09:13:10.120964   12946 init_comm.go:44] [console] jsmn parse successed, n is 58\nI0419 09:13:10.121204   12946 init_comm.go:44] [console] token 0, type is 1, size is 5\nI0419 09:13:10.121459   12946 init_comm.go:44] [console] token 1, type is 3, size is 1\nI0419 09:13:10.121712   12946 init_comm.go:44] [console] hostname is alpine-0989527933\nI0419 09:13:10.121964   12946 init_comm.go:44] [console] token 3, type is 3, size is 1\nI0419 09:13:10.122158   12946 init_comm.go:44] [console] container count 1\nI0419 09:13:10.122326   12946 init_comm.go:44] [console] next container 7\nI0419 09:13:10.122455   12946 init_comm.go:44] [console] 1 name id\nI0419 09:13:10.122962   12946 init_comm.go:44] [console] container id 7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea\nI0419 09:13:10.123119   12946 init_comm.go:44] [console] 3 name rootfs\nI0419 09:13:10.123287   12946 init_comm.go:44] [console] container rootfs \nI0419 09:13:10.123428   12946 init_comm.go:44] [console] 5 name image\nI0419 09:13:10.123995   12946 init_comm.go:44] [console] container image /7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea/rootfs\nI0419 09:13:10.124138   12946 init_comm.go:44] [console] 7 name fsmap\nI0419 09:13:10.124278   12946 init_comm.go:44] [console] fsmap num 1\nI0419 09:13:10.124489   12946 init_comm.go:44] [console] maps 0 source iVfeHKcqnR\nI0419 09:13:10.124695   12946 init_comm.go:44] [console] maps 0 path /etc/hosts\nI0419 09:13:10.124914   12946 init_comm.go:44] [console] maps 0 readonly 0\nI0419 09:13:10.125148   12946 init_comm.go:44] [console] in maps incorrect dockerVolume\nI0419 09:13:10.125364   12946 init_comm.go:44] [console] parse pod json failed\nI0419 09:13:10.125504   12946 init_comm.go:44] [console] uptime 0.23 0.01\nI0419 09:13:10.125621   12946 init_comm.go:44] [console] \nI0419 09:13:10.125836   12946 init_comm.go:44] [console] hyper send type 10, len 0\nI0419 09:13:10.125909   12946 init_comm.go:82] read 8/8 [length = 0]\nI0419 09:13:10.125948   12946 init_comm.go:86] data length is 8\nI0419 09:13:10.125987   12946 init_comm.go:72] trying to read 8 bytes\nI0419 09:13:10.126026   12946 init_comm.go:163] got cmd:10\nI0419 09:13:10.126083   12946 init_comm.go:186] ack got, clear pong timer\nI0419 09:13:10.126122   12946 hypervisor.go:29] main event loop got message 44(ERROR_CMD_FAIL)\nE0419 09:13:10.126183   12946 vm_states.go:358] Shutting down because of an exception: Start POD failed\nI0419 09:13:10.129288   12946 context.go:212] VM vm-bbuWmTciLc: state change from STARTING to 'TERMINATING'\nE0419 09:13:10.129314   12946 vm_states.go:635] Start POD failed\nI0419 09:13:10.126363   12946 vm.go:262] Get the response from VM, VM id is vm-bbuWmTciLc!\nI0419 09:13:10.129346   12946 pod.go:83] unlock pod for operation start\nI0419 09:13:10.129355   12946 pod.go:86] successfully unlock pod for operation start\nE0419 09:13:10.129363   12946 run.go:88] VM vm-bbuWmTciLc start failed with code 7: Start POD failed\nE0419 09:13:10.129372   12946 server.go:170] Handler for POST /v0.5.0/pod/start returned error: VM vm-bbuWmTciLc start failed with code 7: Start POD failed\nI0419 09:13:10.129447   12946 init_comm.go:163] got cmd:4\nI0419 09:13:10.129458   12946 init_comm.go:244] send command 4 to init, payload: ''.\nI0419 09:13:10.129474   12946 init_comm.go:257] write 8 to init, payload: ''.\nI0419 09:13:10.129484   12946 init_comm.go:262] message sent, set pong timer\nI0419 09:13:10.130240   12946 init_comm.go:44] [console] hyper_loop epoll_wait 1\nI0419 09:13:10.130718   12946 init_comm.go:44] [console] hyper_handle_event get event 1, de 0x613578, fd 4. ops 0x613320\nI0419 09:13:10.131232   12946 init_comm.go:44] [console] hyper_handle_event event EPOLLIN, de 0x613578, fd 4, 0x613320\nI0419 09:13:10.131429   12946 init_comm.go:44] [console] hyper_event_read\nI0419 09:13:10.131811   12946 init_comm.go:44] [console] already read 8 bytes data\nI0419 09:13:10.132107   12946 init_comm.go:44] [console] hyper send type 14, len 4\nI0419 09:13:10.132204   12946 init_comm.go:82] read 8/8 [length = 0]\nI0419 09:13:10.132220   12946 init_comm.go:86] data length is 12\nI0419 09:13:10.132232   12946 init_comm.go:72] trying to read 4 bytes\nI0419 09:13:10.132246   12946 init_comm.go:82] read 12/12 [length = 12]\nI0419 09:13:10.132290   12946 init_comm.go:72] trying to read 8 bytes\nI0419 09:13:10.132305   12946 init_comm.go:163] got cmd:14\nI0419 09:13:10.132317   12946 init_comm.go:231] get command NEXT\nI0419 09:13:10.132327   12946 init_comm.go:234] send 8, receive 8\nI0419 09:13:10.132347   12946 init_comm.go:44] [console] get length 8\nI0419 09:13:10.132552   12946 init_comm.go:44] [console] 0 0 0 4 0 0 0 8 \nI0419 09:13:10.132941   12946 init_comm.go:44] [console]  hyper_channel_handle, type 4, len 8\nI0419 09:13:10.133216   12946 init_comm.go:44] [console] get DESTROYPOD message\nI0419 09:13:10.133498   12946 init_comm.go:44] [console] hyper send type 9, len 0\nI0419 09:13:10.133570   12946 init_comm.go:82] read 8/8 [length = 0]\nI0419 09:13:10.133583   12946 init_comm.go:86] data length is 8\nI0419 09:13:10.133599   12946 init_comm.go:72] trying to read 8 bytes\nI0419 09:13:10.133613   12946 init_comm.go:163] got cmd:9\nI0419 09:13:10.133627   12946 init_comm.go:167] got response of shutdown command, last round of command to init\nI0419 09:13:10.133647   12946 init_comm.go:186] ack got, clear pong timer\nI0419 09:13:10.133662   12946 hypervisor.go:29] main event loop got message 37(COMMAND_ACK)\nI0419 09:13:10.133679   12946 vm_states.go:800] [Terminating] Got reply to &{4 [] }: ''\nI0419 09:13:10.133709   12946 vm_states.go:802] POD destroyed \nI0419 09:13:10.133728   12946 qmp_handler.go:296] got new session\nI0419 09:13:10.133755   12946 qmp_handler.go:225] Begin process command session\nI0419 09:13:10.133781   12946 qmp_handler.go:243] sending command (1) {\"execute\":\"quit\"}\nI0419 09:13:10.134353   12946 qmp_handler.go:103] got a message {\"return\": {}}\nI0419 09:13:10.134433   12946 qmp_handler.go:103] got a message {\"timestamp\": {\"seconds\": 1461049990, \"microseconds\": 134330}, \"event\": \"SHUTDOWN\"}\nI0419 09:13:10.134497   12946 qmp_handler.go:107] got event: SHUTDOWN\nI0419 09:13:10.134512   12946 qmp_handler.go:152] Shutdown, quit QMP receiver\nI0419 09:13:10.134527   12946 qmp_handler.go:323] got QMP event SHUTDOWN\nI0419 09:13:10.134551   12946 qmp_handler.go:325] got QMP shutdown event, quit...\nI0419 09:13:10.134564   12946 hypervisor.go:29] main event loop got message 1(EVENT_VM_EXIT)\nI0419 09:13:10.134577   12946 vm_states.go:784] Got VM shutdown event while terminating, go to cleaning up\nI0419 09:13:10.134587   12946 vm_states.go:34] VM has exit...\nI0419 09:13:10.134606   12946 devicemap.go:419] need umount dir iVfeHKcqnR\nI0419 09:13:10.134706   12946 devicemap.go:462] need unmount aufs /7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea/rootfs\nI0419 09:13:10.134726   12946 devicemap.go:499] remove network card 0: 192.168.123.2\nI0419 09:13:10.134743   12946 context.go:212] VM vm-bbuWmTciLc: state change from TERMINATING to 'DESTROYING'\nI0419 09:13:10.135162   12946 pod.go:605] cleanup hosts for pod pod-nGrWzvoRNi failed, invalid argument\nI0419 09:13:10.138323   12946 volume_linux.go:43] Ready to unmount the target : /var/run/hyper/vm-bbuWmTciLc/share_dir/7383484ceb0643eb39787d31638df4c87f81e28208cac2c131cbc5c97a32daea/rootfs\nI0419 09:13:10.141402   12946 tty.go:441] Input byte chan closed, close the output string chan\nI0419 09:13:10.141440   12946 init_comm.go:46] console output end\nE0419 09:13:10.141460   12946 init_comm.go:75] read init data failed\nI0419 09:13:10.141485   12946 hypervisor.go:29] main event loop got message 43(ERROR_INTERRUPTED)\nI0419 09:13:10.141505   12946 vm_states.go:891] Connection interrupted while destroying\nE0419 09:13:10.141544   12946 tty.go:103] read tty data failed\nI0419 09:13:10.141575   12946 tty.go:164] tty socket closed, quit the reading goroutine EOF\nI0419 09:13:10.141605   12946 tty.go:131] tty chan closed, quit sent goroutine\nI0419 09:13:10.141620   12946 hypervisor.go:29] main event loop got message 43(ERROR_INTERRUPTED)\nI0419 09:13:10.141650   12946 vm_states.go:891] Connection interrupted while destroying\nI0419 09:13:10.156687   12946 hypervisor.go:29] main event loop got message 12(EVENT_BLOCK_EJECTED)\nI0419 09:13:10.156726   12946 devicemap.go:396] volume etchosts-volume umounted\nI0419 09:13:10.156743   12946 vm_states.go:456] Unplug block device return with true\nI0419 09:13:10.156758   12946 hypervisor.go:29] main event loop got message 14(EVENT_INTERFACE_DELETE)\nI0419 09:13:10.156774   12946 devicemap.go:387] interface 0 released\nI0419 09:13:10.156789   12946 vm_states.go:453] Unplug interface return with true\nI0419 09:13:10.165789   12946 hypervisor.go:29] main event loop got message 7(EVENT_CONTAINER_DELETE)\nI0419 09:13:10.165828   12946 devicemap.go:369] container 0 umounted\nI0419 09:13:10.165844   12946 vm_states.go:450] Unplug container return with true\nI0419 09:13:10.165856   12946 context.go:199] no more device to release/remove/umount, quit\nI0419 09:13:10.165901   12946 qemu_process.go:19] quit watch dog.\nI0419 09:47:29.285124   12946 server.go:152] Calling POST /v0.5.0/vm/create\nI0419 09:47:29.285333   12946 vm.go:120] The config: kernel=/mnt/tank/HYPER/kernel, initrd=/mnt/tank/HYPER/hyper-initrd.img\nI0419 09:47:29.285967   12946 qemu_process.go:63] cmdline arguments: -machine pc-i440fx-2.0,accel=kvm,usb=off -global kvm-pit.lost_tick_policy=discard -cpu host -drive if=pflash,file=/mnt/tank/HYPER/bios-qboot.bin,readonly=on -drive if=pflash,file=/mnt/tank/HYPER/cbfs-qboot.rom,readonly=on -realtime mlock=off -no-user-config -nodefaults -no-hpet -rtc base=utc,driftfix=slew -no-reboot -display none -boot strict=on -m 128 -smp 1 -qmp unix:/var/run/hyper/vm-TcTOqfpdJk/qmp.sock,server,nowait -serial unix:/var/run/hyper/vm-TcTOqfpdJk/console.sock,server,nowait -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x2 -device virtio-scsi-pci,id=scsi0,bus=pci.0,addr=0x3 -chardev socket,id=charch0,path=/var/run/hyper/vm-TcTOqfpdJk/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0 -chardev socket,id=charch1,path=/var/run/hyper/vm-TcTOqfpdJk/tty.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1 -fsdev local,id=virtio9p,path=/var/run/hyper/vm-TcTOqfpdJk/share_dir,security_model=none -device virtio-9p-pci,fsdev=virtio9p,mount_tag=share_dir\nI0419 09:47:29.286247   12946 server.go:152] Calling POST /v0.5.0/pod/create\nI0419 09:47:29.286393   12946 pod_routes.go:76] Args string is {\"id\":\"busybox-5112358495\",\"hostname\":\"\",\"containers\":[{\"name\":\"busybox-5112358495\",\"image\":\"busybox\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"labels\":{},\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}, autoremove false\nI0419 09:47:29.286494   12946 run.go:45] podArgs: {\"id\":\"busybox-5112358495\",\"hostname\":\"\",\"containers\":[{\"name\":\"busybox-5112358495\",\"image\":\"busybox\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"labels\":{},\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}\nI0419 09:47:29.289009   12946 daemondb.go:82] try get container list for pod pod-XPJnIRsvhP\nI0419 09:47:29.289108   12946 pod.go:229] loaded containers for pod pod-XPJnIRsvhP: []\nE0419 09:47:29.289312   12946 pod.go:309] nosuchimagetag: No such image: busybox:latest\nE0419 09:47:29.289339   12946 server.go:170] Handler for POST /v0.5.0/pod/create returned error: No such image: busybox:latest\nI0419 09:47:29.290183   12946 qemu_process.go:74] starting daemon with pid: 13818\nI0419 09:47:29.290355   12946 server.go:152] Calling POST /v0.5.0/image/create\nDEBU[2108] Trying to pull busybox from https://registry-1.docker.io v2 \nI0419 09:47:29.326299   12946 tty.go:157] tty socket connected\nI0419 09:47:29.326390   12946 tty.go:100] tty: trying to read 12 bytes\nI0419 09:47:29.326403   12946 qmp_handler.go:167] connected to /var/run/hyper/vm-TcTOqfpdJk/qmp.sock\nI0419 09:47:29.326467   12946 qmp_handler.go:177] begin qmp init...\nI0419 09:47:29.326429   12946 init_comm.go:29] connected to /var/run/hyper/vm-TcTOqfpdJk/console.sock\nI0419 09:47:29.326503   12946 init_comm.go:36] connected /var/run/hyper/vm-TcTOqfpdJk/console.sock as telnet mode.\nI0419 09:47:29.326314   12946 init_comm.go:109] Wating for init messages...\nI0419 09:47:29.326544   12946 init_comm.go:72] trying to read 8 bytes\nI0419 09:47:29.396018   12946 qmp_handler.go:186] got qmp welcome, now sending command qmp_capabilities\nI0419 09:47:29.396082   12946 qmp_handler.go:201] waiting for response\nI0419 09:47:29.396323   12946 qmp_handler.go:103] got a message {\"return\": {}}\nI0419 09:47:29.396344   12946 qmp_handler.go:210] got for response\nI0419 09:47:29.396354   12946 qmp_handler.go:213] QMP connection initialized\nI0419 09:47:29.396384   12946 qmp_handler.go:346] QMP initialzed, go into main QMP loop\nI0419 09:47:29.396408   12946 qmp_handler.go:137] Begin receive QMP message\nI0419 09:47:29.475465   12946 init_comm.go:44] [console] Initializing cgroup subsys cpu\nI0419 09:47:29.479392   12946 init_comm.go:44] [console] Linux version 4.4.0-hyper+ (laijs@ubox) (gcc version 5.2.1 20151010 (Ubuntu 5.2.1-22ubuntu2) ) #0 SMP Mon Jan 25 01:10:46 CST 2016\nI0419 09:47:29.479841   12946 init_comm.go:44] [console] Command line: console=ttyS0 panic=1 no_timer_check\nI0419 09:47:29.479947   12946 init_comm.go:44] [console] \nI0419 09:47:29.480199   12946 init_comm.go:44] [console] x86/fpu: Legacy x87 FPU detected.\nI0419 09:47:29.480599   12946 init_comm.go:44] [console] x86/fpu: Using 'lazy' FPU context switches.\nI0419 09:47:29.480961   12946 init_comm.go:44] [console] e820: BIOS-provided physical RAM map:\nI0419 09:47:29.481524   12946 init_comm.go:44] [console] BIOS-e820: [mem 0x0000000000000000-0x000000000009fbff] usable\nI0419 09:47:29.482110   12946 init_comm.go:44] [console] BIOS-e820: [mem 0x000000000009fc00-0x000000000009ffff] reserved\nI0419 09:47:29.482715   12946 init_comm.go:44] [console] BIOS-e820: [mem 0x00000000000d0000-0x00000000000effff] ACPI NVS\nI0419 09:47:29.483280   12946 init_comm.go:44] [console] BIOS-e820: [mem 0x00000000000f0000-0x00000000000fffff] reserved\nI0419 09:47:29.483806   12946 init_comm.go:44] [console] BIOS-e820: [mem 0x0000000000100000-0x0000000007ffffff] usable\nI0419 09:47:29.484302   12946 init_comm.go:44] [console] BIOS-e820: [mem 0x00000000feffc000-0x00000000feffffff] reserved\nI0419 09:47:29.484645   12946 init_comm.go:44] [console] NX (Execute Disable) protection: active\nI0419 09:47:29.484899   12946 init_comm.go:44] [console] DMI not present or invalid.\nI0419 09:47:29.485126   12946 init_comm.go:44] [console] Hypervisor detected: KVM\nI0419 09:47:29.485573   12946 init_comm.go:44] [console] e820: last_pfn = 0x8000 max_arch_pfn = 0x400000000\nI0419 09:47:29.486191   12946 init_comm.go:44] [console] x86/PAT: Configuration [0-7]: WB  WC  UC- UC  WB  WC  UC- WT\nI0419 09:47:29.486335   12946 init_comm.go:44] [console] MTRR: Disabled\nI0419 09:47:29.486718   12946 init_comm.go:44] [console] CPU MTRRs all blank - virtualized system.\nI0419 09:47:29.487032   12946 init_comm.go:44] [console] Using GB pages for direct mapping\nI0419 09:47:29.487382   12946 init_comm.go:44] [console] RAMDISK: [mem 0x07f10000-0x07ffffff]\nI0419 09:47:29.487826   12946 init_comm.go:44] [console] ACPI: Early table checksum verification disabled\nI0419 09:47:29.488282   12946 init_comm.go:44] [console] ACPI: RSDP 0x00000000000F2960 000014 (v00 BOCHS )\nI0419 09:47:29.489043   12946 init_comm.go:44] [console] ACPI: RSDT 0x00000000000EFB95 000034 (v01 BOCHS  BXPCRSDT 00000001 BXPC 00000001)\nI0419 09:47:29.489790   12946 init_comm.go:44] [console] ACPI: FACP 0x00000000000EF1C0 000074 (v01 BOCHS  BXPCFACP 00000001 BXPC 00000001)\nI0419 09:47:29.490546   12946 init_comm.go:44] [console] ACPI: DSDT 0x00000000000EE040 001180 (v01 BOCHS  BXPCDSDT 00000001 BXPC 00000001)\nI0419 09:47:29.490895   12946 init_comm.go:44] [console] ACPI: FACS 0x00000000000EE000 000040\nI0419 09:47:29.491672   12946 init_comm.go:44] [console] ACPI: SSDT 0x00000000000EF234 0008E9 (v01 BOCHS  BXPCSSDT 00000001 BXPC 00000001)\nI0419 09:47:29.492413   12946 init_comm.go:44] [console] ACPI: APIC 0x00000000000EFB1D 000078 (v01 BOCHS  BXPCAPIC 00000001 BXPC 00000001)\nI0419 09:47:29.493159   12946 init_comm.go:44] [console] ACPI: RSDT 0x00000000000EFB95 000034 (v01 BOCHS  BXPCRSDT 00000001 BXPC 00000001)\nI0419 09:47:29.493412   12946 init_comm.go:44] [console] No NUMA configuration found\nI0419 09:47:29.493971   12946 init_comm.go:44] [console] Faking a node at [mem 0x0000000000000000-0x0000000007ffffff]\nI0419 09:47:29.494419   12946 init_comm.go:44] [console] NODE_DATA(0) allocated [mem 0x07efe000-0x07f0ffff]\nI0419 09:47:29.494847   12946 init_comm.go:44] [console] kvm-clock: Using msrs 4b564d01 and 4b564d00\nI0419 09:47:29.495312   12946 init_comm.go:44] [console] kvm-clock: cpu 0, msr 0:7efd001, primary cpu clock\nI0419 09:47:29.496244   12946 init_comm.go:44] [console] clocksource: kvm-clock: mask: 0xffffffffffffffff max_cycles: 0x1cd42e4dffb, max_idle_ns: 881590591483 ns\nI0419 09:47:29.496370   12946 init_comm.go:44] [console] Zone ranges:\nI0419 09:47:29.496921   12946 init_comm.go:44] [console]   DMA      [mem 0x0000000000001000-0x0000000000ffffff]\nI0419 09:47:29.497358   12946 init_comm.go:44] [console]   DMA32    [mem 0x0000000001000000-0x0000000007ffffff]\nI0419 09:47:29.497513   12946 init_comm.go:44] [console]   Normal   empty\nI0419 09:47:29.497827   12946 init_comm.go:44] [console] Movable zone start for each node\nI0419 09:47:29.498059   12946 init_comm.go:44] [console] Early memory node ranges\nI0419 09:47:29.498568   12946 init_comm.go:44] [console]   node   0: [mem 0x0000000000001000-0x000000000009efff]\nI0419 09:47:29.499081   12946 init_comm.go:44] [console]   node   0: [mem 0x0000000000100000-0x0000000007ffffff]\nI0419 09:47:29.499657   12946 init_comm.go:44] [console] Initmem setup node 0 [mem 0x0000000000001000-0x0000000007ffffff]\nI0419 09:47:29.499932   12946 init_comm.go:44] [console] ACPI: PM-Timer IO Port: 0x608\nI0419 09:47:29.500391   12946 init_comm.go:44] [console] ACPI: LAPIC_NMI (acpi_id[0xff] dfl dfl lint[0x1])\nI0419 09:47:29.500944   12946 init_comm.go:44] [console] IOAPIC[0]: apic_id 0, version 17, address 0xfec00000, GSI 0-23\nI0419 09:47:29.501448   12946 init_comm.go:44] [console] ACPI: INT_SRC_OVR (bus 0 bus_irq 0 global_irq 2 dfl dfl)\nI0419 09:47:29.501988   12946 init_comm.go:44] [console] ACPI: INT_SRC_OVR (bus 0 bus_irq 5 global_irq 5 high level)\nI0419 09:47:29.502526   12946 init_comm.go:44] [console] ACPI: INT_SRC_OVR (bus 0 bus_irq 9 global_irq 9 high level)\nI0419 09:47:29.503097   12946 init_comm.go:44] [console] ACPI: INT_SRC_OVR (bus 0 bus_irq 10 global_irq 10 high level)\nI0419 09:47:29.503667   12946 init_comm.go:44] [console] ACPI: INT_SRC_OVR (bus 0 bus_irq 11 global_irq 11 high level)\nI0419 09:47:29.504149   12946 init_comm.go:44] [console] Using ACPI (MADT) for SMP configuration information\nI0419 09:47:29.504525   12946 init_comm.go:44] [console] smpboot: Allowing 1 CPUs, 0 hotplug CPUs\nI0419 09:47:29.505066   12946 init_comm.go:44] [console] e820: [mem 0x08000000-0xfeffbfff] available for PCI devices\nI0419 09:47:29.505391   12946 init_comm.go:44] [console] Booting paravirtualized kernel on KVM\nI0419 09:47:29.506329   12946 init_comm.go:44] [console] clocksource: refined-jiffies: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 1910969940391419 ns\nI0419 09:47:29.506938   12946 init_comm.go:44] [console] setup_percpu: NR_CPUS:8 nr_cpumask_bits:8 nr_cpu_ids:1 nr_node_ids:1\nI0419 09:47:29.507633   12946 init_comm.go:44] [console] PERCPU: Embedded 31 pages/cpu @ffff880007c00000 s89560 r8192 d29224 u2097152\nI0419 09:47:29.507886   12946 init_comm.go:44] [console] KVM setup async PF for cpu 0\nI0419 09:47:29.508183   12946 init_comm.go:44] [console] kvm-stealtime: cpu 0, msr 7c0d480\nI0419 09:47:29.508849   12946 init_comm.go:44] [console] Built 1 zonelists in Node order, mobility grouping on.  Total pages: 32201\nI0419 09:47:29.509033   12946 init_comm.go:44] [console] Policy zone: DMA32\nI0419 09:47:29.509564   12946 init_comm.go:44] [console] Kernel command line: console=ttyS0 panic=1 no_timer_check\nI0419 09:47:29.509583   12946 init_comm.go:44] [console] \nI0419 09:47:29.510036   12946 init_comm.go:44] [console] PID hash table entries: 512 (order: 0, 4096 bytes)\nI0419 09:47:29.511251   12946 init_comm.go:44] [console] Memory: 119800K/130680K available (4225K kernel code, 452K rwdata, 1280K rodata, 860K init, 752K bss, 10880K reserved, 0K cma-reserved)\nI0419 09:47:29.511548   12946 init_comm.go:44] [console] Hierarchical RCU implementation.\nI0419 09:47:29.511956   12946 init_comm.go:44] [console]    Build-time adjustment of leaf fanout to 64.\nI0419 09:47:29.512447   12946 init_comm.go:44] [console]    RCU restricting CPUs from NR_CPUS=8 to nr_cpu_ids=1.\nI0419 09:47:29.512996   12946 init_comm.go:44] [console] RCU: Adjusting geometry for rcu_fanout_leaf=64, nr_cpu_ids=1\nI0419 09:47:29.513267   12946 init_comm.go:44] [console] NR_IRQS:4352 nr_irqs:256 16\nI0419 09:47:29.513608   12946 init_comm.go:44] [console]    Offload RCU callbacks from all CPUs\nI0419 09:47:29.513939   12946 init_comm.go:44] [console]    Offload RCU callbacks from CPUs: 0.\nI0419 09:47:29.514130   12946 init_comm.go:44] [console] Console: colour CGA 80x25\nI0419 09:47:29.514352   12946 init_comm.go:44] [console] console [ttyS0] enabled\nI0419 09:47:29.514721   12946 init_comm.go:44] [console] tsc: Detected 3422.964 MHz processor\nI0419 09:47:29.515452   12946 init_comm.go:44] [console] Calibrating delay loop (skipped) preset value.. 6845.92 BogoMIPS (lpj=3422964)\nI0419 09:47:29.515784   12946 init_comm.go:44] [console] pid_max: default: 32768 minimum: 301\nI0419 09:47:29.516009   12946 init_comm.go:44] [console] ACPI: Core revision 20150930\nI0419 09:47:29.517357   12946 init_comm.go:44] [console] ACPI: 2 ACPI AML tables successfully acquired and loaded\nI0419 09:47:29.517965   12946 init_comm.go:44] [console] Dentry cache hash table entries: 16384 (order: 5, 131072 bytes)\nI0419 09:47:29.518576   12946 init_comm.go:44] [console] Inode-cache hash table entries: 8192 (order: 4, 65536 bytes)\nI0419 09:47:29.519126   12946 init_comm.go:44] [console] Mount-cache hash table entries: 512 (order: 0, 4096 bytes)\nI0419 09:47:29.519715   12946 init_comm.go:44] [console] Mountpoint-cache hash table entries: 512 (order: 0, 4096 bytes)\nI0419 09:47:29.520387   12946 init_comm.go:44] [console] Last level iTLB entries: 4KB 512, 2MB 16, 4MB 8\nI0419 09:47:29.520903   12946 init_comm.go:44] [console] Last level dTLB entries: 4KB 512, 2MB 128, 4MB 64, 1GB 0\nI0419 09:47:29.526938   12946 init_comm.go:44] [console] Freeing SMP alternatives memory: 20K (ffffffff816ac000 - ffffffff816b1000)\nI0419 09:47:29.530223   12946 init_comm.go:44] [console] ..TIMER: vector=0x30 apic1=0 pin1=2 apic2=-1 pin2=-1\nI0419 09:47:29.632283   12946 init_comm.go:44] [console] smpboot: CPU0: AMD Phenom(tm) II X4 965 Processor (family: 0x10, model: 0x4, stepping: 0x3)\nI0419 09:47:29.633007   12946 init_comm.go:44] [console] Performance Events: Broken PMU hardware detected, using software events only.\nI0419 09:47:29.633468   12946 init_comm.go:44] [console] Failed to access perfctr msr (MSR c0010001 is ffffffffffffffff)\nI0419 09:47:29.633867   12946 init_comm.go:44] [console] x86: Booted up 1 node, 1 CPUs\nI0419 09:47:29.634352   12946 init_comm.go:44] [console] smpboot: Total of 1 processors activated (6845.92 BogoMIPS)\nI0419 09:47:29.634739   12946 init_comm.go:44] [console] devtmpfs: initialized\nI0419 09:47:29.638642   12946 init_comm.go:44] [console] clocksource: jiffies: mask: 0xffffffff max_cycles: 0xffffffff, max_idle_ns: 1911260446275000 ns\nI0419 09:47:29.639045   12946 init_comm.go:44] [console] NET: Registered protocol family 16\nI0419 09:47:29.639370   12946 init_comm.go:44] [console] cpuidle: using governor ladder\nI0419 09:47:29.639615   12946 init_comm.go:44] [console] cpuidle: using governor menu\nI0419 09:47:29.640233   12946 init_comm.go:44] [console] ACPI: bus type PCI registered\nI0419 09:47:29.640759   12946 init_comm.go:44] [console] acpiphp: ACPI Hot Plug PCI Controller Driver version: 0.5\nI0419 09:47:29.641223   12946 init_comm.go:44] [console] PCI: Using configuration type 1 for base access\nI0419 09:47:29.641707   12946 init_comm.go:44] [console] PCI: Using configuration type 1 for extended access\nI0419 09:47:29.642586   12946 init_comm.go:44] [console] ACPI: Added _OSI(Module Device)\nI0419 09:47:29.642848   12946 init_comm.go:44] [console] ACPI: Added _OSI(Processor Device)\nI0419 09:47:29.643182   12946 init_comm.go:44] [console] ACPI: Added _OSI(3.0 _SCP Extensions)\nI0419 09:47:29.643602   12946 init_comm.go:44] [console] ACPI: Added _OSI(Processor Aggregator Device)\nI0419 09:47:29.645190   12946 init_comm.go:44] [console] ACPI: Interpreter enabled\nI0419 09:47:29.645418   12946 init_comm.go:44] [console] ACPI: (supports S0 S5)\nI0419 09:47:29.645795   12946 init_comm.go:44] [console] ACPI: Using IOAPIC for interrupt routing\nI0419 09:47:29.646552   12946 init_comm.go:44] [console] PCI: Using host bridge windows from ACPI; if necessary, use \"pci=nocrs\" and report a bug\nI0419 09:47:29.648988   12946 init_comm.go:44] [console] ACPI: PCI Root Bridge [PCI0] (domain 0000 [bus 00-ff])\nI0419 09:47:29.649672   12946 init_comm.go:44] [console] acpi PNP0A03:00: _OSC: OS supports [ExtendedConfig ASPM ClockPM Segments MSI]\nI0419 09:47:29.650218   12946 init_comm.go:44] [console] acpi PNP0A03:00: _OSC failed (AE_NOT_FOUND); disabling ASPM\nI0419 09:47:29.650608   12946 init_comm.go:44] [console] acpiphp: Slot [2] registered\nI0419 09:47:29.650897   12946 init_comm.go:44] [console] acpiphp: Slot [3] registered\nI0419 09:47:29.651184   12946 init_comm.go:44] [console] acpiphp: Slot [4] registered\nI0419 09:47:29.651481   12946 init_comm.go:44] [console] acpiphp: Slot [5] registered\nI0419 09:47:29.651782   12946 init_comm.go:44] [console] acpiphp: Slot [6] registered\nI0419 09:47:29.652068   12946 init_comm.go:44] [console] acpiphp: Slot [7] registered\nI0419 09:47:29.652360   12946 init_comm.go:44] [console] acpiphp: Slot [8] registered\nI0419 09:47:29.652653   12946 init_comm.go:44] [console] acpiphp: Slot [9] registered\nI0419 09:47:29.652952   12946 init_comm.go:44] [console] acpiphp: Slot [10] registered\nI0419 09:47:29.653251   12946 init_comm.go:44] [console] acpiphp: Slot [11] registered\nI0419 09:47:29.653545   12946 init_comm.go:44] [console] acpiphp: Slot [12] registered\nI0419 09:47:29.653836   12946 init_comm.go:44] [console] acpiphp: Slot [13] registered\nI0419 09:47:29.654135   12946 init_comm.go:44] [console] acpiphp: Slot [14] registered\nI0419 09:47:29.654427   12946 init_comm.go:44] [console] acpiphp: Slot [15] registered\nI0419 09:47:29.654726   12946 init_comm.go:44] [console] acpiphp: Slot [16] registered\nI0419 09:47:29.655021   12946 init_comm.go:44] [console] acpiphp: Slot [17] registered\nI0419 09:47:29.655323   12946 init_comm.go:44] [console] acpiphp: Slot [18] registered\nI0419 09:47:29.655638   12946 init_comm.go:44] [console] acpiphp: Slot [19] registered\nI0419 09:47:29.655936   12946 init_comm.go:44] [console] acpiphp: Slot [20] registered\nI0419 09:47:29.656227   12946 init_comm.go:44] [console] acpiphp: Slot [21] registered\nI0419 09:47:29.656528   12946 init_comm.go:44] [console] acpiphp: Slot [22] registered\nI0419 09:47:29.656829   12946 init_comm.go:44] [console] acpiphp: Slot [23] registered\nI0419 09:47:29.657126   12946 init_comm.go:44] [console] acpiphp: Slot [24] registered\nI0419 09:47:29.657415   12946 init_comm.go:44] [console] acpiphp: Slot [25] registered\nI0419 09:47:29.657720   12946 init_comm.go:44] [console] acpiphp: Slot [26] registered\nI0419 09:47:29.658009   12946 init_comm.go:44] [console] acpiphp: Slot [27] registered\nI0419 09:47:29.658313   12946 init_comm.go:44] [console] acpiphp: Slot [28] registered\nI0419 09:47:29.658618   12946 init_comm.go:44] [console] acpiphp: Slot [29] registered\nI0419 09:47:29.658918   12946 init_comm.go:44] [console] acpiphp: Slot [30] registered\nI0419 09:47:29.659212   12946 init_comm.go:44] [console] acpiphp: Slot [31] registered\nI0419 09:47:29.659517   12946 init_comm.go:44] [console] PCI host bridge to bus 0000:00\nI0419 09:47:29.660083   12946 init_comm.go:44] [console] pci_bus 0000:00: root bus resource [io  0x0000-0x0cf7 window]\nI0419 09:47:29.660654   12946 init_comm.go:44] [console] pci_bus 0000:00: root bus resource [io  0x0d00-0xadff window]\nI0419 09:47:29.661209   12946 init_comm.go:44] [console] pci_bus 0000:00: root bus resource [io  0xae0f-0xaeff window]\nI0419 09:47:29.661768   12946 init_comm.go:44] [console] pci_bus 0000:00: root bus resource [io  0xaf20-0xafdf window]\nI0419 09:47:29.662293   12946 init_comm.go:44] [console] pci_bus 0000:00: root bus resource [io  0xafe4-0xffff window]\nI0419 09:47:29.662873   12946 init_comm.go:44] [console] pci_bus 0000:00: root bus resource [mem 0x000a0000-0x000bffff window]\nI0419 09:47:29.663469   12946 init_comm.go:44] [console] pci_bus 0000:00: root bus resource [mem 0x08000000-0xfebfffff window]\nI0419 09:47:29.663837   12946 init_comm.go:44] [console] pci_bus 0000:00: root bus resource [bus 00-ff]\nI0419 09:47:29.665191   12946 init_comm.go:44] [console] pci 0000:00:01.1: legacy IDE quirk: reg 0x10: [io  0x01f0-0x01f7]\nI0419 09:47:29.665725   12946 init_comm.go:44] [console] pci 0000:00:01.1: legacy IDE quirk: reg 0x14: [io  0x03f6]\nI0419 09:47:29.666296   12946 init_comm.go:44] [console] pci 0000:00:01.1: legacy IDE quirk: reg 0x18: [io  0x0170-0x0177]\nI0419 09:47:29.666850   12946 init_comm.go:44] [console] pci 0000:00:01.1: legacy IDE quirk: reg 0x1c: [io  0x0376]\nI0419 09:47:29.667767   12946 init_comm.go:44] [console] pci 0000:00:01.3: quirk: [io  0x0600-0x063f] claimed by PIIX4 ACPI\nI0419 09:47:29.668331   12946 init_comm.go:44] [console] pci 0000:00:01.3: quirk: [io  0x0700-0x070f] claimed by PIIX4 SMB\nI0419 09:47:29.670367   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKA] (IRQs 5 10 11)\nI0419 09:47:29.670916   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKB] (IRQs 5 10 11)\nI0419 09:47:29.671464   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKC] (IRQs 5 10 11)\nI0419 09:47:29.672024   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKD] (IRQs 5 10 11)\nI0419 09:47:29.672488   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKS] (IRQs 9)\nI0419 09:47:29.673126   12946 init_comm.go:44] [console] ACPI: Enabled 16 GPEs in block 00 to 0F\nI0419 09:47:29.673340   12946 init_comm.go:44] [console] vgaarb: loaded\nI0419 09:47:29.673664   12946 init_comm.go:44] [console] SCSI subsystem initialized\nI0419 09:47:29.673990   12946 init_comm.go:44] [console] dmi: Firmware registration failed.\nI0419 09:47:29.674278   12946 init_comm.go:44] [console] PCI: Using ACPI for IRQ routing\nI0419 09:47:29.674890   12946 init_comm.go:44] [console] clocksource: Switched to clocksource kvm-clock\nI0419 09:47:29.675137   12946 init_comm.go:44] [console] pnp: PnP ACPI init\nI0419 09:47:29.675778   12946 init_comm.go:44] [console] pnp: PnP ACPI: found 5 devices\nI0419 09:47:29.682420   12946 init_comm.go:44] [console] clocksource: acpi_pm: mask: 0xffffff max_cycles: 0xffffff, max_idle_ns: 2085701024 ns\nI0419 09:47:29.682982   12946 init_comm.go:44] [console] pci 0000:00:02.0: BAR 1: assigned [mem 0x08000000-0x08000fff]\nI0419 09:47:29.683615   12946 init_comm.go:44] [console] pci 0000:00:03.0: BAR 1: assigned [mem 0x08001000-0x08001fff]\nI0419 09:47:29.684137   12946 init_comm.go:44] [console] pci 0000:00:04.0: BAR 1: assigned [mem 0x08002000-0x08002fff]\nI0419 09:47:29.684652   12946 init_comm.go:44] [console] pci 0000:00:03.0: BAR 0: assigned [io  0x1000-0x103f]\nI0419 09:47:29.685112   12946 init_comm.go:44] [console] pci 0000:00:04.0: BAR 0: assigned [io  0x1040-0x107f]\nI0419 09:47:29.685580   12946 init_comm.go:44] [console] pci 0000:00:02.0: BAR 0: assigned [io  0x1080-0x109f]\nI0419 09:47:29.686016   12946 init_comm.go:44] [console] pci 0000:00:01.1: BAR 4: assigned [io  0x10a0-0x10af]\nI0419 09:47:29.686342   12946 init_comm.go:44] [console] NET: Registered protocol family 2\nI0419 09:47:29.686951   12946 init_comm.go:44] [console] TCP established hash table entries: 1024 (order: 1, 8192 bytes)\nI0419 09:47:29.687482   12946 init_comm.go:44] [console] TCP bind hash table entries: 1024 (order: 2, 16384 bytes)\nI0419 09:47:29.687922   12946 init_comm.go:44] [console] TCP: Hash tables configured (established 1024 bind 1024)\nI0419 09:47:29.688351   12946 init_comm.go:44] [console] UDP hash table entries: 256 (order: 1, 8192 bytes)\nI0419 09:47:29.688823   12946 init_comm.go:44] [console] UDP-Lite hash table entries: 256 (order: 1, 8192 bytes)\nI0419 09:47:29.689187   12946 init_comm.go:44] [console] NET: Registered protocol family 1\nI0419 09:47:29.689615   12946 init_comm.go:44] [console] pci 0000:00:00.0: Limiting direct PCI/PCI transfers\nI0419 09:47:29.690066   12946 init_comm.go:44] [console] pci 0000:00:01.0: PIIX3: Enabling Passive Release\nI0419 09:47:29.690584   12946 init_comm.go:44] [console] pci 0000:00:01.0: Activating ISA DMA hang workarounds\nI0419 09:47:29.691076   12946 init_comm.go:44] [console] Trying to unpack rootfs image as initramfs...\nI0419 09:47:29.705316   12946 init_comm.go:44] [console] Freeing initrd memory: 960K (ffff880007f10000 - ffff880008000000)\nI0419 09:47:29.706017   12946 init_comm.go:44] [console] futex hash table entries: 256 (order: 2, 16384 bytes)\nI0419 09:47:29.706762   12946 init_comm.go:44] [console] SGI XFS with ACLs, security attributes, no debug enabled\nI0419 09:47:29.707274   12946 init_comm.go:44] [console] 9p: Installing v9fs 9p2000 file system support\nI0419 09:47:29.708337   12946 init_comm.go:44] [console] Block layer SCSI generic (bsg) driver version 0.4 loaded (major 254)\nI0419 09:47:29.708627   12946 init_comm.go:44] [console] io scheduler noop registered\nI0419 09:47:29.708998   12946 init_comm.go:44] [console] io scheduler cfq registered (default)\nI0419 09:47:29.709468   12946 init_comm.go:44] [console] pci_hotplug: PCI Hot Plug PCI Core version: 0.5\nI0419 09:47:29.710044   12946 init_comm.go:44] [console] pciehp: PCI Express Hot Plug Controller Driver version: 0.4\nI0419 09:47:29.710697   12946 init_comm.go:44] [console] Warning: Processor Platform Limit event detected, but not handled.\nI0419 09:47:29.711148   12946 init_comm.go:44] [console] Consider compiling CPUfreq support into your kernel.\nI0419 09:47:29.711897   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKB] enabled at IRQ 10\nI0419 09:47:29.712390   12946 init_comm.go:44] [console] virtio-pci 0000:00:02.0: enabling device (0000 -> 0003)\nI0419 09:47:29.713324   12946 init_comm.go:44] [console] virtio-pci 0000:00:02.0: virtio_pci: leaving for legacy driver\nI0419 09:47:29.714241   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKC] enabled at IRQ 11\nI0419 09:47:29.714738   12946 init_comm.go:44] [console] virtio-pci 0000:00:03.0: enabling device (0000 -> 0003)\nI0419 09:47:29.715814   12946 init_comm.go:44] [console] virtio-pci 0000:00:03.0: virtio_pci: leaving for legacy driver\nI0419 09:47:29.716854   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKD] enabled at IRQ 11\nI0419 09:47:29.717351   12946 init_comm.go:44] [console] virtio-pci 0000:00:04.0: enabling device (0000 -> 0003)\nI0419 09:47:29.718620   12946 init_comm.go:44] [console] virtio-pci 0000:00:04.0: virtio_pci: leaving for legacy driver\nI0419 09:47:29.719703   12946 init_comm.go:44] [console] Serial: 8250/16550 driver, 4 ports, IRQ sharing enabled\nI0419 09:47:29.743090   12946 init_comm.go:44] [console] 00:04: ttyS0 at I/O 0x3f8 (irq = 4, base_baud = 115200) is a 16550A\nI0419 09:47:29.771025   12946 init_comm.go:44] [console] brd: module loaded\nI0419 09:47:29.771305   12946 init_comm.go:44] [console] loop: module loaded\nI0419 09:47:29.772021   12946 init_comm.go:44] [console] scsi host0: Virtio SCSI HBA\nI0419 09:47:29.805216   12946 init_comm.go:44] [console] e1000: Intel(R) PRO/1000 Network Driver - version 7.3.21-k8-NAPI\nI0419 09:47:29.805246   12946 init_comm.go:44] [console] e1000: Copyright (c) 1999-2006 Intel Corporation.\nI0419 09:47:29.805255   12946 init_comm.go:44] [console] NET: Registered protocol family 10\nI0419 09:47:29.805263   12946 init_comm.go:44] [console] NET: Registered protocol family 17\nI0419 09:47:29.805271   12946 init_comm.go:44] [console] 9pnet: Installing 9P2000 support\nI0419 09:47:29.805278   12946 init_comm.go:44] [console] registered taskstats version 1\nI0419 09:47:29.805790   12946 init_comm.go:44] [console] Freeing unused kernel memory: 860K (ffffffff815d5000 - ffffffff816ac000)\nI0419 09:47:29.806894   12946 init_comm.go:44] [console] create directory /dev\nI0419 09:47:29.807125   12946 init_comm.go:44] [console] create directory /sys\nI0419 09:47:29.807381   12946 init_comm.go:44] [console] create directory /proc\nI0419 09:47:29.807578   12946 init_comm.go:44] [console] uptime 0.19 0.01\nI0419 09:47:29.807649   12946 init_comm.go:44] [console] \nI0419 09:47:29.807917   12946 init_comm.go:44] [console] create directory /dev/pts\nI0419 09:47:29.808114   12946 init_comm.go:44] [console] create directory /dev\nI0419 09:47:29.808449   12946 init_comm.go:44] [console] open hyper channel /dev/vport0p1\nI0419 09:47:29.808883   12946 init_comm.go:44] [console] send ready message\nI0419 09:47:29.809102   12946 init_comm.go:44] [console] hyper send type 8, len 0\nI0419 09:47:29.809351   12946 init_comm.go:82] read 8/8 [length = 0]\nI0419 09:47:29.809411   12946 init_comm.go:86] data length is 8\nI0419 09:47:29.809448   12946 init_comm.go:119] Get init ready message\nI0419 09:47:29.809579   12946 hypervisor.go:29] main event loop got message 5(EVENT_INIT_CONNECTED)\nI0419 09:47:29.809602   12946 vm_states.go:529] begin to wait vm commands\nI0419 09:47:29.809605   12946 init_comm.go:72] trying to read 8 bytes\nI0419 09:47:29.809831   12946 init_comm.go:44] [console] channel sh.hyper.channel.1, directory sh.hyper.channel.0\nI0419 09:47:29.809906   12946 init_comm.go:44] [console] \nI0419 09:47:29.810008   12946 init_comm.go:44] [console] open hyper channel /dev/vport0p2\nI0419 09:47:29.810578   12946 init_comm.go:44] [console] hyper_init_event hyper channel event 0x613578, ops 0x613320, fd 4\nI0419 09:47:29.810891   12946 init_comm.go:44] [console] hyper_add_event add event fd 4, 0x613320\nI0419 09:47:29.811327   12946 init_comm.go:44] [console] hyper_init_event hyper ttyfd event 0x613540, ops 0x6132e0, fd 5\nI0419 09:47:29.811655   12946 init_comm.go:44] [console] hyper_add_event add event fd 5, 0x6132e0\nI0419 09:47:29.812102   12946 init_comm.go:44] [console] hyper_init_event hyper signal event 0x613508, ops 0x613360, fd 3\nI0419 09:47:29.812379   12946 init_comm.go:44] [console] hyper_add_event add event fd 3, 0x613360\nI0419 09:47:30.706081   12946 init_comm.go:44] [console] tsc: Refined TSC clocksource calibration: 3423.033 MHz\nI0419 09:47:30.707036   12946 init_comm.go:44] [console] clocksource: tsc: mask: 0xffffffffffffffff max_cycles: 0x31574d6cac4, max_idle_ns: 440795369633 ns\nDEBU[2110] Increasing token expiration to: 0 seconds  \nDEBU[2111] Pulling ref from V2 registry: busybox:latest \nDEBU[2111] pulling blob \"sha256:385e281300cc6d88bdd155e0931fbdfbb1801c2b0265340a40481ee2b733ae66\" \nDEBU[2111] pulling blob \"sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\" \nDEBU[2111] Downloaded a3ed95caeb02 to tempfile /mnt/tank/HYPER/tmp/GetImageBlob044496314 \nDEBU[2112] Downloaded 385e281300cc to tempfile /mnt/tank/HYPER/tmp/GetImageBlob824912849 \nDEBU[2112] Applied tar sha256:1834950e52ce4d5a88a1bbd131c537f4d0e56d10ff0dd69e66be3b7dfa9df7e6 to 9305b0bc3c7251c6951f6753becb223e26dda490c7308aacf3307bc6475b82f5, size: 1112820 \nDEBU[2112] Applied tar sha256:5f70bf18a086007016e948b04aed3b82103a36bea41755b6cddfaf10ace3c6ef to 0404b7906f00f8c1ce6afacbfe9dd4eae9ab961cc98671004303320a669a2fc1, size: 0 \nI0419 09:47:33.428806   12946 server.go:152] Calling POST /v0.5.0/pod/create\nI0419 09:47:33.428874   12946 pod_routes.go:76] Args string is {\"id\":\"busybox-5112358495\",\"hostname\":\"\",\"containers\":[{\"name\":\"busybox-5112358495\",\"image\":\"busybox\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"labels\":{},\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}, autoremove false\nI0419 09:47:33.428910   12946 run.go:45] podArgs: {\"id\":\"busybox-5112358495\",\"hostname\":\"\",\"containers\":[{\"name\":\"busybox-5112358495\",\"image\":\"busybox\",\"command\":[],\"workdir\":\"/\",\"entrypoint\":[],\"ports\":[],\"envs\":[],\"volumes\":[],\"files\":[],\"restartPolicy\":\"never\"}],\"resource\":{\"vcpu\":1,\"memory\":128},\"files\":[],\"volumes\":[],\"labels\":{},\"log\":{\"type\":\"\",\"config\":{}},\"tty\":false,\"type\":\"\",\"RestartPolicy\":\"\"}\nI0419 09:47:33.429367   12946 daemondb.go:82] try get container list for pod pod-OZyNoqIJiX\nI0419 09:47:33.429412   12946 pod.go:229] loaded containers for pod pod-OZyNoqIJiX: []\nDEBU[2112] container mounted via layerStore: /mnt/tank/HYPER/aufs/mnt/8772dfdfe88b2a962ae1cc4275632c87398778e329d4cdef59f9ad2f96d4af30 \nI0419 09:47:33.466946   12946 pod.go:318] create container cd1b8f67f793383fb031caf6e61fcd2825d2d83805535653e5e8f5a53c95da58\nI0419 09:47:33.466999   12946 pod.go:356] container name busybox-5112358495, image busybox\nI0419 09:47:33.467049   12946 pod.go:373] container info config &{cd1b8f67f793   false false false map[]  false false false [] 0xc821bf16c0 false busybox map[]   true  [] map[] }, Cmd [sh], Args []\nI0419 09:47:33.467092   12946 pod.go:396] Container Info is \n&{cd1b8f67f793383fb031caf6e61fcd2825d2d83805535653e5e8f5a53c95da58 8772dfdfe88b2a962ae1cc4275632c87398778e329d4cdef59f9ad2f96d4af30     [] [sh] map[] true}\nI0419 09:47:33.467430   12946 daemondb.go:91] try set container list for pod pod-OZyNoqIJiX: [cd1b8f67f793383fb031caf6e61fcd2825d2d83805535653e5e8f5a53c95da58]\nI0419 09:47:33.468835   12946 server.go:152] Calling POST /v0.5.0/pod/start\nI0419 09:47:33.468959   12946 run.go:78] pod:pod-OZyNoqIJiX, vm:vm-TcTOqfpdJk\nI0419 09:47:33.469010   12946 pod.go:71] lock pod for operation start\nI0419 09:47:33.469053   12946 pod.go:74] successfully lock pod for operation start\nI0419 09:47:33.469093   12946 vm.go:157] find vm:vm-TcTOqfpdJk\nI0419 09:47:33.469187   12946 pod.go:709] container ID: cd1b8f67f793383fb031caf6e61fcd2825d2d83805535653e5e8f5a53c95da58, mountId 8772dfdfe88b2a962ae1cc4275632c87398778e329d4cdef59f9ad2f96d4af30\nI0419 09:47:33.471049   12946 volumes.go:28] trying to bind dir /mnt/tank/HYPER/hosts/pod-OZyNoqIJiX/hosts to /var/run/hyper/vm-TcTOqfpdJk/share_dir/RWEhfAenSg\nI0419 09:47:33.471273   12946 tank.go:77] dir /mnt/tank/HYPER/hosts/pod-OZyNoqIJiX/hosts is bound to RWEhfAenSg\nI0419 09:47:33.471340   12946 pod.go:840] configuring log driver [json-file] for pod-OZyNoqIJiX\nI0419 09:47:33.471388   12946 pod.go:859] configure container log to /var/run/hyper/Pods/pod-OZyNoqIJiX/cd1b8f67f793383fb031caf6e61fcd2825d2d83805535653e5e8f5a53c95da58-json.log\nI0419 09:47:33.471477   12946 pod.go:865] configured logger for pod-OZyNoqIJiX/cd1b8f67f793383fb031caf6e61fcd2825d2d83805535653e5e8f5a53c95da58 (/busybox-5112358495)\nI0419 09:47:33.471702   12946 vm.go:180] hyperHandlePodEvent pod pod-OZyNoqIJiX, vm vm-TcTOqfpdJk\nI0419 09:47:33.471768   12946 hypervisor.go:29] main event loop got message 34(COMMAND_ATTACH)\nI0419 09:47:33.471812   12946 vm_states.go:281] attachment log-aj2qdmnj is pending\nI0419 09:47:33.471846   12946 hypervisor.go:29] main event loop got message 23(COMMAND_RUN_POD)\nI0419 09:47:33.471881   12946 vm_states.go:556] got spec, prepare devices\nI0419 09:47:33.471971   12946 context.go:244] #0 Container Info:\nI0419 09:47:33.472106   12946 context.go:247] \n{\n...|    \"Id\": \"cd1b8f67f793383fb031caf6e61fcd2825d2d83805535653e5e8f5a53c95da58\",\n...|    \"MountId\": \"8772dfdfe88b2a962ae1cc4275632c87398778e329d4cdef59f9ad2f96d4af30\",\n...|    \"Rootfs\": \"\",\n...|    \"Image\": \"/8772dfdfe88b2a962ae1cc4275632c87398778e329d4cdef59f9ad2f96d4af30/rootfs\",\n...|    \"Fstype\": \"dir\",\n...|    \"Workdir\": \"\",\n...|    \"Entrypoint\": null,\n...|    \"Cmd\": [\n...|        \"sh\"\n...|    ],\n...|    \"Envs\": {},\n...|    \"Initialize\": true\n...|}\nI0419 09:47:33.472184   12946 devicemap.go:235] insert volume etchosts-volume to /etc/hosts on 0\nI0419 09:47:33.472346   12946 vm_states.go:72] initial vm spec: {\n        \"hostname\": \"busybox-5112358495\",\n        \"containers\": [\n            {\n                \"id\": \"cd1b8f67f793383fb031caf6e61fcd2825d2d83805535653e5e8f5a53c95da58\",\n                \"rootfs\": \"\",\n                \"image\": \"/8772dfdfe88b2a962ae1cc4275632c87398778e329d4cdef59f9ad2f96d4af30/rootfs\",\n                \"fsmap\": [\n                    {\n                        \"source\": \"RWEhfAenSg\",\n                        \"path\": \"/etc/hosts\",\n                        \"readOnly\": false,\n                        \"dockerVolume\": false\n                    }\n                ],\n                \"process\": {\n                    \"terminal\": false,\n                    \"stdio\": 1,\n                    \"stderr\": 2,\n                    \"args\": [\n                        \"sh\"\n                    ],\n                    \"workdir\": \"/\"\n                },\n                \"restartPolicy\": \"never\",\n                \"initialize\": true\n            }\n        ],\n        \"shareDir\": \"share_dir\"\n    }\nI0419 09:47:33.472432   12946 context.go:176] found container cd1b8f67f793383fb031caf6e61fcd2825d2d83805535653e5e8f5a53c95da58 at 0\nI0419 09:47:33.472475   12946 vm_states.go:80] attach pending client log-aj2qdmnj for cd1b8f67f793383fb031caf6e61fcd2825d2d83805535653e5e8f5a53c95da58\nI0419 09:47:33.472514   12946 vm_states.go:304] Connecting tty for cd1b8f67f793383fb031caf6e61fcd2825d2d83805535653e5e8f5a53c95da58 on session 1\nI0419 09:47:33.472618   12946 context.go:212] VM vm-TcTOqfpdJk: state change from INIT to 'STARTING'\nI0419 09:47:33.475178   12946 hypervisor.go:29] main event loop got message 13(EVENT_INTERFACE_ADD)\nI0419 09:47:33.475258   12946 qmp_wrapper.go:90] send net to qemu at 22\nI0419 09:47:33.475346   12946 qmp_handler.go:296] got new session\nI0419 09:47:33.475440   12946 qmp_handler.go:225] Begin process command session\nI0419 09:47:33.475512   12946 qmp_handler.go:238] send cmd with scm (24 bytes) (1) {\"execute\":\"getfd\",\"arguments\":{\"fdname\":\"fdeth0\"}}\nI0419 09:47:33.477128   12946 qmp_handler.go:103] got a message {\"return\": {}}\nI0419 09:47:33.477479   12946 qmp_handler.go:243] sending command (1) {\"execute\":\"netdev_add\",\"arguments\":{\"fd\":\"fdeth0\",\"id\":\"eth0\",\"type\":\"tap\"}}\nI0419 09:47:33.479921   12946 qmp_handler.go:103] got a message {\"return\": {}}\nI0419 09:47:33.480094   12946 qmp_handler.go:243] sending command (1) {\"execute\":\"device_add\",\"arguments\":{\"addr\":\"0x5\",\"bus\":\"pci.0\",\"driver\":\"virtio-net-pci\",\"id\":\"eth0\",\"mac\":\"52:54:bf:6c:f5:31\",\"netdev\":\"eth0\"}}\nI0419 09:47:33.487109   12946 qmp_handler.go:103] got a message {\"return\": {}}\nI0419 09:47:33.487301   12946 qmp_handler.go:302] session finished, buffer size 1\nI0419 09:47:33.487343   12946 qmp_handler.go:305] success \nI0419 09:47:33.487361   12946 hypervisor.go:29] main event loop got message 15(EVENT_INTERFACE_INSERTED)\nI0419 09:47:33.487393   12946 vm_states.go:585] device ready, could run pod.\nI0419 09:47:33.487468   12946 init_comm.go:163] got cmd:1\nI0419 09:47:33.487487   12946 init_comm.go:244] send command 1 to init, payload: '{\"hostname\":\"busybox-5112358495\",\"containers\":[{\"id\":\"cd1b8f67f793383fb031caf6e61fcd2825d2d83805535653e5e8f5a53c95da58\",\"rootfs\":\"\",\"image\":\"/8772dfdfe88b2a962ae1cc4275632c87398778e329d4cdef59f9ad2f96d4af30/rootfs\",\"fsmap\":[{\"source\":\"RWEhfAenSg\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"terminal\":false,\"stdio\":1,\"stderr\":2,\"args\":[\"sh\"],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":true}],\"interfaces\":[{\"device\":\"eth0\",\"ipAddress\":\"192.168.123.3\",\"netMask\":\"255.255.255.0\"}],\"routes\":[{\"dest\":\"0.0.0.0/0\",\"gateway\":\"192.168.123.1\",\"device\":\"eth0\"}],\"shareDir\":\"share_dir\"}'.\nI0419 09:47:33.487525   12946 init_comm.go:257] write 512 to init, payload: '\u0001m{\"hostname\":\"busybox-5112358495\",\"containers\":[{\"id\":\"cd1b8f67f793383fb031caf6e61fcd2825d2d83805535653e5e8f5a53c95da58\",\"rootfs\":\"\",\"image\":\"/8772dfdfe88b2a962ae1cc4275632c87398778e329d4cdef59f9ad2f96d4af30/rootfs\",\"fsmap\":[{\"source\":\"RWEhfAenSg\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"terminal\":false,\"stdio\":1,\"stderr\":2,\"args\":[\"sh\"],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":true}],\"interfaces\":[{\"device\":\"eth0\",\"ipAddress\":\"192.168.123.3\",\"netMask\":\"255.2'.\nI0419 09:47:33.487585   12946 init_comm.go:262] message sent, set pong timer\nI0419 09:47:33.488326   12946 init_comm.go:44] [console] hyper_loop epoll_wait 1\nI0419 09:47:33.489069   12946 init_comm.go:44] [console] hyper_handle_event get event 1, de 0x613578, fd 4. ops 0x613320\nI0419 09:47:33.489709   12946 init_comm.go:44] [console] hyper_handle_event event EPOLLIN, de 0x613578, fd 4, 0x613320\nI0419 09:47:33.489970   12946 init_comm.go:44] [console] hyper_event_read\nI0419 09:47:33.494687   12946 init_comm.go:82] read 8/8 [length = 0]\nI0419 09:47:33.494715   12946 init_comm.go:86] data length is 12\nI0419 09:47:33.494728   12946 init_comm.go:72] trying to read 4 bytes\nI0419 09:47:33.494743   12946 init_comm.go:82] read 12/12 [length = 12]\nI0419 09:47:33.494759   12946 init_comm.go:72] trying to read 8 bytes\nI0419 09:47:33.494774   12946 init_comm.go:163] got cmd:14\nI0419 09:47:33.494787   12946 init_comm.go:231] get command NEXT\nI0419 09:47:33.494799   12946 init_comm.go:234] send 512, receive 8\nI0419 09:47:33.494690   12946 init_comm.go:44] [console] already read 8 bytes data\nI0419 09:47:33.494819   12946 init_comm.go:44] [console] hyper send type 14, len 4\nI0419 09:47:33.494832   12946 init_comm.go:44] [console] get length 621\nI0419 09:47:33.494843   12946 init_comm.go:44] [console] read 504 bytes data, total data 512\nI0419 09:47:33.494858   12946 init_comm.go:44] [console] hyper send type 14, len 4\nI0419 09:47:33.494870   12946 init_comm.go:44] [console] pci 0000:00:05.0: BAR 6: assigned [mem 0x08040000-0x0807ffff pref]\nI0419 09:47:33.494882   12946 init_comm.go:44] [console] pci 0000:00:05.0: BAR 1: assigned [mem 0x08003000-0x08003fff]\nI0419 09:47:33.494941   12946 init_comm.go:44] [console] pci 0000:00:05.0: BAR 0: assigned [io  0x10c0-0x10df]\nI0419 09:47:33.496030   12946 init_comm.go:44] [console] ACPI: PCI Interrupt Link [LNKA] enabled at IRQ 10\nI0419 09:47:33.496921   12946 init_comm.go:44] [console] virtio-pci 0000:00:05.0: enabling device (0000 -> 0003)\nI0419 09:47:33.499226   12946 init_comm.go:44] [console] virtio-pci 0000:00:05.0: virtio_pci: leaving for legacy driver\nI0419 09:47:33.500872   12946 init_comm.go:82] read 8/8 [length = 0]\nI0419 09:47:33.500940   12946 init_comm.go:86] data length is 12\nI0419 09:47:33.500971   12946 init_comm.go:72] trying to read 4 bytes\nI0419 09:47:33.501018   12946 init_comm.go:82] read 12/12 [length = 12]\nI0419 09:47:33.501076   12946 init_comm.go:72] trying to read 8 bytes\nI0419 09:47:33.501123   12946 init_comm.go:163] got cmd:14\nI0419 09:47:33.501165   12946 init_comm.go:231] get command NEXT\nI0419 09:47:33.501201   12946 init_comm.go:234] send 512, receive 512\nI0419 09:47:33.501264   12946 init_comm.go:257] write 109 to init, payload: '55.255.0\"}],\"routes\":[{\"dest\":\"0.0.0.0/0\",\"gateway\":\"192.168.123.1\",\"device\":\"eth0\"}],\"shareDir\":\"share_dir\"}'.\nI0419 09:47:33.501602   12946 init_comm.go:44] [console] hyper_loop epoll_wait 1\nI0419 09:47:33.502069   12946 init_comm.go:44] [console] hyper_handle_event get event 1, de 0x613578, fd 4. ops 0x613320\nI0419 09:47:33.502465   12946 init_comm.go:44] [console] hyper_handle_event event EPOLLIN, de 0x613578, fd 4, 0x613320\nI0419 09:47:33.502674   12946 init_comm.go:44] [console] hyper_event_read\nI0419 09:47:33.502823   12946 init_comm.go:44] [console] get length 621\nI0419 09:47:33.503303   12946 init_comm.go:44] [console] read 109 bytes data, total data 621\nI0419 09:47:33.503686   12946 init_comm.go:44] [console] hyper send type 14, len 4\nI0419 09:47:33.503737   12946 init_comm.go:82] read 8/8 [length = 0]\nI0419 09:47:33.503759   12946 init_comm.go:86] data length is 12\nI0419 09:47:33.503768   12946 init_comm.go:72] trying to read 4 bytes\nI0419 09:47:33.503820   12946 init_comm.go:82] read 12/12 [length = 12]\nI0419 09:47:33.503877   12946 init_comm.go:72] trying to read 8 bytes\nI0419 09:47:33.503975   12946 init_comm.go:163] got cmd:14\nI0419 09:47:33.504016   12946 init_comm.go:231] get command NEXT\nI0419 09:47:33.504057   12946 init_comm.go:234] send 109, receive 109\nI0419 09:47:33.515784   12946 init_comm.go:44] [console] 0 0 0 1 0 0 2 6d 7b 22 68 6f 73 74 6e 61 6d 65 22 3a 22 62 75 73 79 62 6f 78 2d 35 31 31 32 33 35 38 34 39 35 22 2c 22 63 6f 6e 74 61 69 6e 65 72 73 22 3a 5b 7b 22 69 64 22 3a 22 63 64 31 62 38 66 36 37 66 37 39 33 33 38 33 66 62 30 33 31 63 61 66 36 65 36 31 66 63 64 32 38 32 35 64 32 64 38 33 38 30 35 35 33 35 36 35 33 65 35 65 38 66 35 61 35 33 63 39 35 64 61 35 38 22 2c 22 72 6f 6f 74 66 73 22 3a 22 22 2c 22 69 6d 61 67 65 22 3a 22 2f 38 37 37 32 64 66 64 66 65 38 38 62 32 61 39 36 32 61 65 31 63 63 34 32 37 35 36 33 32 63 38 37 33 39 38 37 37 38 65 33 32 39 64 34 63 64 65 66 35 39 66 39 61 64 32 66 39 36 64 34 61 66 33 30 2f 72 6f 6f 74 66 73 22 2c 22 66 73 6d 61 70 22 3a 5b 7b 22 73 6f 75 72 63 65 22 3a 22 52 57 45 68 66 41 65 6e 53 67 22 2c 22 70 61 74 68 22 3a 22 2f 65 74 63 2f 68 6f 73 74 73 22 2c 22 72 65 61 64 4f 6e 6c 79 22 3a 66 61 6c 73 65 2c 22 64 6f 63 6b 65 72 56 6f 6c 75 6d 65 22 3a 66 61 6c 73 65 7d 5d 2c 22 70 72 6f 63 65 73 73 22 3a 7b 22 74 65 72 6d 69 6e 61 6c 22 3a 66 61 6c 73 65 2c 22 73 74 64 69 6f 22 3a 31 2c 22 73 74 64 65 72 72 22 3a 32 2c 22 61 72 67 73 22 3a 5b 22 73 68 22 5d 2c 22 77 6f 72 6b 64 69 72 22 3a 22 2f 22 7d 2c 22 72 65 73 74 61 72 74 50 6f 6c 69 63 79 22 3a 22 6e 65 76 65 72 22 2c 22 69 6e 69 74 69 61 6c 69 7a 65 22 3a 74 72 75 65 7d 5d 2c 22 69 6e 74 65 72 66 61 63 65 73 22 3a 5b 7b 22 64 65 76 69 63 65 22 3a 22 65 74 68 30 22 2c 22 69 70 41 64 64 72 65 73 73 22 3a 22 31 39 32 2e 31 36 38 2e 31 32 33 2e 33 22 2c 22 6e 65 74 4d 61 73 6b 22 3a 22 32 35 35 2e 32 35 35 2e 32 35 35 2e 30 22 7d 5d 2c 22 72 6f 75 74 65 73 22 3a 5b 7b 22 64 65 73 74 22 3a 22 30 2e 30 2e 30 2e 30 2f 30 22 2c 22 67 61 74 65 77 61 79 22 3a 22 31 39 32 2e 31 36 38 2e 31 32 33 2e 31 22 2c 22 64 65 76 69 63 65 22 3a 22 65 74 68 30 22 7d 5d 2c 22 73 68 61 72 65 44 69 72 22 3a 22 73 68 61 72 65 5f 64 69 72 22 7d \nI0419 09:47:33.516114   12946 init_comm.go:44] [console]  hyper_channel_handle, type 1, len 621\nI0419 09:47:33.516775   12946 init_comm.go:44] [console] online_cpu()\nI0419 09:47:33.516953   12946 init_comm.go:44] [console] online_memory()\nI0419 09:47:33.520754   12946 init_comm.go:44] [console] call hyper_start_pod, json {\"hostname\":\"busybox-5112358495\",\"containers\":[{\"id\":\"cd1b8f67f793383fb031caf6e61fcd2825d2d83805535653e5e8f5a53c95da58\",\"rootfs\":\"\",\"image\":\"/8772dfdfe88b2a962ae1cc4275632c87398778e329d4cdef59f9ad2f96d4af30/rootfs\",\"fsmap\":[{\"source\":\"RWEhfAenSg\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"terminal\":false,\"stdio\":1,\"stderr\":2,\"args\":[\"sh\"],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":true}],\"interfaces\":[{\"device\":\"eth0\",\"ipAddress\":\"192.168.123.3\",\"netMask\":\"255.255.255.0\"}],\"routes\":[{\"dest\":\"0.0.0.0/0\",\"gateway\":\"192.168.123.1\",\"device\":\"eth0\"}],\"shareDir\":\"share_dir\"}, len 613\nI0419 09:47:33.524338   12946 init_comm.go:44] [console] call hyper_start_pod, json {\"hostname\":\"busybox-5112358495\",\"containers\":[{\"id\":\"cd1b8f67f793383fb031caf6e61fcd2825d2d83805535653e5e8f5a53c95da58\",\"rootfs\":\"\",\"image\":\"/8772dfdfe88b2a962ae1cc4275632c87398778e329d4cdef59f9ad2f96d4af30/rootfs\",\"fsmap\":[{\"source\":\"RWEhfAenSg\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"terminal\":false,\"stdio\":1,\"stderr\":2,\"args\":[\"sh\"],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":true}],\"interfaces\":[{\"device\":\"eth0\",\"ipAddress\":\"192.168.123.3\",\"netMask\":\"255.255.255.0\"}],\"routes\":[{\"dest\":\"0.0.0.0/0\",\"gateway\":\"192.168.123.1\",\"device\":\"eth0\"}],\"shareDir\":\"share_dir\"}, len 613\nI0419 09:47:33.524636   12946 init_comm.go:44] [console] jsmn parse successed, n is 60\nI0419 09:47:33.524803   12946 init_comm.go:44] [console] token 0, type is 1, size is 5\nI0419 09:47:33.525029   12946 init_comm.go:44] [console] token 1, type is 3, size is 1\nI0419 09:47:33.525265   12946 init_comm.go:44] [console] hostname is busybox-5112358495\nI0419 09:47:33.525578   12946 init_comm.go:44] [console] token 3, type is 3, size is 1\nI0419 09:47:33.525711   12946 init_comm.go:44] [console] container count 1\nI0419 09:47:33.525902   12946 init_comm.go:44] [console] next container 7\nI0419 09:47:33.526022   12946 init_comm.go:44] [console] 1 name id\nI0419 09:47:33.526491   12946 init_comm.go:44] [console] container id cd1b8f67f793383fb031caf6e61fcd2825d2d83805535653e5e8f5a53c95da58\nI0419 09:47:33.526681   12946 init_comm.go:44] [console] 3 name rootfs\nI0419 09:47:33.526830   12946 init_comm.go:44] [console] container rootfs \nI0419 09:47:33.526993   12946 init_comm.go:44] [console] 5 name image\nI0419 09:47:33.527549   12946 init_comm.go:44] [console] container image /8772dfdfe88b2a962ae1cc4275632c87398778e329d4cdef59f9ad2f96d4af30/rootfs\nI0419 09:47:33.527691   12946 init_comm.go:44] [console] 7 name fsmap\nI0419 09:47:33.527869   12946 init_comm.go:44] [console] fsmap num 1\nI0419 09:47:33.528085   12946 init_comm.go:44] [console] maps 0 source RWEhfAenSg\nI0419 09:47:33.528288   12946 init_comm.go:44] [console] maps 0 path /etc/hosts\nI0419 09:47:33.528461   12946 init_comm.go:44] [console] maps 0 readonly 0\nI0419 09:47:33.528681   12946 init_comm.go:44] [console] in maps incorrect dockerVolume\nI0419 09:47:33.528875   12946 init_comm.go:44] [console] parse pod json failed\nI0419 09:47:33.529050   12946 init_comm.go:44] [console] uptime 3.91 3.68\nI0419 09:47:33.529126   12946 init_comm.go:44] [console] \nI0419 09:47:33.529350   12946 init_comm.go:44] [console] hyper send type 10, len 0\nI0419 09:47:33.529439   12946 init_comm.go:82] read 8/8 [length = 0]\nI0419 09:47:33.529461   12946 init_comm.go:86] data length is 8\nI0419 09:47:33.529477   12946 init_comm.go:72] trying to read 8 bytes\nI0419 09:47:33.529488   12946 init_comm.go:163] got cmd:10\nI0419 09:47:33.529501   12946 init_comm.go:186] ack got, clear pong timer\nI0419 09:47:33.529519   12946 hypervisor.go:29] main event loop got message 44(ERROR_CMD_FAIL)\nE0419 09:47:33.529553   12946 vm_states.go:358] Shutting down because of an exception: Start POD failed\nI0419 09:47:33.529578   12946 context.go:212] VM vm-TcTOqfpdJk: state change from STARTING to 'TERMINATING'\nE0419 09:47:33.529606   12946 vm_states.go:635] Start POD failed\nI0419 09:47:33.529609   12946 init_comm.go:163] got cmd:4\nI0419 09:47:33.529635   12946 vm.go:262] Get the response from VM, VM id is vm-TcTOqfpdJk!\nI0419 09:47:33.529682   12946 pod.go:83] unlock pod for operation start\nI0419 09:47:33.529694   12946 pod.go:86] successfully unlock pod for operation start\nE0419 09:47:33.529704   12946 run.go:88] VM vm-TcTOqfpdJk start failed with code 7: Start POD failed\nE0419 09:47:33.529716   12946 server.go:170] Handler for POST /v0.5.0/pod/start returned error: VM vm-TcTOqfpdJk start failed with code 7: Start POD failed\nI0419 09:47:33.529645   12946 init_comm.go:244] send command 4 to init, payload: ''.\nI0419 09:47:33.529822   12946 init_comm.go:257] write 8 to init, payload: ''.\nI0419 09:47:33.529841   12946 init_comm.go:262] message sent, set pong timer\nI0419 09:47:33.530277   12946 server.go:152] Calling DELETE /v0.5.0/vm\nI0419 09:47:33.530349   12946 hypervisor.go:29] main event loop got message 26(COMMAND_SHUTDOWN)\nI0419 09:47:33.530383   12946 vm.go:81] Got response: 16: unexpected event during terminating\nI0419 09:47:33.530410   12946 init_comm.go:44] [console] hyper_loop epoll_wait 1\nI0419 09:47:33.530673   12946 init_comm.go:44] [console] hyper_handle_event get event 1, de 0x613578, fd 4. ops 0x613320\nI0419 09:47:33.531063   12946 init_comm.go:44] [console] hyper_handle_event event EPOLLIN, de 0x613578, fd 4, 0x613320\nI0419 09:47:33.531218   12946 init_comm.go:44] [console] hyper_event_read\nI0419 09:47:33.531560   12946 init_comm.go:44] [console] already read 8 bytes data\nI0419 09:47:33.531759   12946 init_comm.go:44] [console] hyper send type 14, len 4\nI0419 09:47:33.531921   12946 init_comm.go:82] read 8/8 [length = 0]\nI0419 09:47:33.531937   12946 init_comm.go:86] data length is 12\nI0419 09:47:33.531948   12946 init_comm.go:72] trying to read 4 bytes\nI0419 09:47:33.531962   12946 init_comm.go:82] read 12/12 [length = 12]\nI0419 09:47:33.531980   12946 init_comm.go:72] trying to read 8 bytes\nI0419 09:47:33.531995   12946 init_comm.go:163] got cmd:14\nI0419 09:47:33.532008   12946 init_comm.go:231] get command NEXT\nI0419 09:47:33.532025   12946 init_comm.go:234] send 8, receive 8\nI0419 09:47:33.532007   12946 init_comm.go:44] [console] get length 8\nI0419 09:47:33.532157   12946 init_comm.go:44] [console] 0 0 0 4 0 0 0 8 \nI0419 09:47:33.532426   12946 init_comm.go:44] [console]  hyper_channel_handle, type 4, len 8\nI0419 09:47:33.532639   12946 init_comm.go:44] [console] get DESTROYPOD message\nI0419 09:47:33.532841   12946 init_comm.go:44] [console] hyper send type 9, len 0\nI0419 09:47:33.532901   12946 init_comm.go:82] read 8/8 [length = 0]\nI0419 09:47:33.532917   12946 init_comm.go:86] data length is 8\nI0419 09:47:33.532935   12946 init_comm.go:72] trying to read 8 bytes\nI0419 09:47:33.532950   12946 init_comm.go:163] got cmd:9\nI0419 09:47:33.532965   12946 init_comm.go:167] got response of shutdown command, last round of command to init\nI0419 09:47:33.532980   12946 init_comm.go:186] ack got, clear pong timer\nI0419 09:47:33.532995   12946 hypervisor.go:29] main event loop got message 37(COMMAND_ACK)\nI0419 09:47:33.533009   12946 vm_states.go:800] [Terminating] Got reply to &{4 [] }: ''\nI0419 09:47:33.533082   12946 vm_states.go:802] POD destroyed \nI0419 09:47:33.533103   12946 qmp_handler.go:296] got new session\nI0419 09:47:33.533134   12946 qmp_handler.go:225] Begin process command session\nI0419 09:47:33.533159   12946 qmp_handler.go:243] sending command (1) {\"execute\":\"quit\"}\nI0419 09:47:33.533787   12946 qmp_handler.go:103] got a message {\"return\": {}}\nI0419 09:47:33.533884   12946 qmp_handler.go:103] got a message {\"timestamp\": {\"seconds\": 1461052053, \"microseconds\": 533777}, \"event\": \"SHUTDOWN\"}\nI0419 09:47:33.533913   12946 qmp_handler.go:107] got event: SHUTDOWN\nI0419 09:47:33.533928   12946 qmp_handler.go:152] Shutdown, quit QMP receiver\nI0419 09:47:33.533944   12946 qmp_handler.go:323] got QMP event SHUTDOWN\nI0419 09:47:33.533957   12946 qmp_handler.go:325] got QMP shutdown event, quit...\nI0419 09:47:33.533969   12946 hypervisor.go:29] main event loop got message 1(EVENT_VM_EXIT)\nI0419 09:47:33.533981   12946 vm_states.go:784] Got VM shutdown event while terminating, go to cleaning up\nI0419 09:47:33.533993   12946 vm_states.go:34] VM has exit...\nI0419 09:47:33.534009   12946 devicemap.go:419] need umount dir RWEhfAenSg\nI0419 09:47:33.534094   12946 vm.go:81] Got response: 2: VM shut down\nI0419 09:47:33.534119   12946 devicemap.go:462] need unmount aufs /8772dfdfe88b2a962ae1cc4275632c87398778e329d4cdef59f9ad2f96d4af30/rootfs\nI0419 09:47:33.534139   12946 devicemap.go:499] remove network card 0: 192.168.123.3\nI0419 09:47:33.534156   12946 context.go:212] VM vm-TcTOqfpdJk: state change from TERMINATING to 'DESTROYING'\nE0419 09:47:33.541585   12946 init_comm.go:75] read init data failed\nI0419 09:47:33.541624   12946 hypervisor.go:29] main event loop got message 43(ERROR_INTERRUPTED)\nI0419 09:47:33.541763   12946 vm_states.go:891] Connection interrupted while destroying\nE0419 09:47:33.541659   12946 tty.go:103] read tty data failed\nI0419 09:47:33.541785   12946 tty.go:164] tty socket closed, quit the reading goroutine EOF\nI0419 09:47:33.541810   12946 tty.go:131] tty chan closed, quit sent goroutine\nI0419 09:47:33.541687   12946 tty.go:441] Input byte chan closed, close the output string chan\nI0419 09:47:33.541835   12946 init_comm.go:46] console output end\nI0419 09:47:33.541845   12946 hypervisor.go:29] main event loop got message 43(ERROR_INTERRUPTED)\nI0419 09:47:33.541856   12946 vm_states.go:891] Connection interrupted while destroying\nI0419 09:47:33.544821   12946 volume_linux.go:43] Ready to unmount the target : /var/run/hyper/vm-TcTOqfpdJk/share_dir/8772dfdfe88b2a962ae1cc4275632c87398778e329d4cdef59f9ad2f96d4af30/rootfs\nI0419 09:47:33.550726   12946 hypervisor.go:29] main event loop got message 12(EVENT_BLOCK_EJECTED)\nI0419 09:47:33.550762   12946 devicemap.go:396] volume etchosts-volume umounted\nI0419 09:47:33.550774   12946 vm_states.go:456] Unplug block device return with true\nI0419 09:47:33.550784   12946 hypervisor.go:29] main event loop got message 14(EVENT_INTERFACE_DELETE)\nI0419 09:47:33.550792   12946 devicemap.go:387] interface 0 released\nI0419 09:47:33.550803   12946 vm_states.go:453] Unplug interface return with true\nI0419 09:47:33.557709   12946 hypervisor.go:29] main event loop got message 7(EVENT_CONTAINER_DELETE)\nI0419 09:47:33.557743   12946 devicemap.go:369] container 0 umounted\nI0419 09:47:33.557756   12946 vm_states.go:450] Unplug container return with true\nI0419 09:47:33.557770   12946 context.go:199] no more device to release/remove/umount, quit\nI0419 09:47:33.557809   12946 qemu_process.go:19] quit watch dog.\n```\n. ",
    "allencloud": "close this via #252 \n. Thanks for your careful code review. @gnawux @resouer \nMaybe I need to learn more details about how to run CI on my local machine.\n. Yes, of course.\n. Actually, I have already done the rebase thing, Mr Wang.\n. ",
    "szepeviktor": "Thank you. \n. ",
    "jdavisclark": "Looks like this is an inconsistent issue? same just happened after a hyper ps, but is now working correctly for fip commands\n. @gnawux my bad, I didn't even think to look for a CLI. I'll update mine in the morning to make sure I have the latest. If its still an issue, I'll reopen this in the right tracker.\nthanks.\n. ",
    "heartlock": "I will take POST, /image/load   POST, /auth    POST, /image/build\n. ",
    "asaif": "Here is my config file\nroot@dem1:~# cat /etc/hyper/config\nKernel=/var/lib/hyper/kernel\nInitrd=/var/lib/hyper/hyper-initrd.img\nBios=/var/lib/hyper/bios-qboot.bin\nCbfs=/var/lib/hyper/cbfs-qboot.rom\n. ",
    "Crazykev": "Yep, I fully understand the purpose, it's a good design BTW. I think maybe caused by environment configuration, I started it all over again with a new environment, and just could't reproduce it. Sorry for bother, I'll close this. \n. I try to produce a similar scene by deleting file /var/lib/hyper/kernel, it's worth to mention that VM hasn't been reclaimed correctly if something failed.\nPOD ID              POD Name             VM name          Status\npod-ipigGikWQU      busybox-1112725748   vm-YuvYKKGijG       pending\nThe client won't return and I can't delete this pod either. \n. @laijs updated and rebased\n. @laijs updated and rebased.\n. @feiskyer move command marshal to server. updated.\n. @bergwolf  yep, seems this patch has fixed it. It's the same problem. \n. @laijs added test case\n. @laijs updated and rebased.. Rebased.\n@gnawux either of us need to do that :wink: . @laijs  fixed.  Seems failure in CI not concerned with this patch.\n\n!!! Error in /home/travis/gopath/src/github.com/hyperhq/hyperd/hack/lib/test.sh:51\n  'sudo env PATH=$PATH hyperctl run --rm -a -p $1' exited with status 1\nCall stack:\n  1: /home/travis/gopath/src/github.com/hyperhq/hyperd/hack/lib/test.sh:51 hyper::test::run_attached_pod(...)\n  2: /home/travis/gopath/src/github.com/hyperhq/hyperd/hack/lib/test.sh:67 hyper::test::insert_file(...)\n  3: hack/test-cmd.sh:171 runTests(...)\n  4: hack/test-cmd.sh:191 main(...)\nExiting with status 1\n. @laijs travis can pass now.. @laijs yep, will do. @feiskyer Sorry for the delay.\nWe should operations VM first (if running), and update podspec later. or else, podspec needs to rollback if vm operations failed.\n\nIt is what is implemented. Won't rewrite service field in podspec until operation on VM succeed. And also ipvs rules in VM will rollback if operation failed.\n\nif podspec updated, it should also be saved to db so we don't lose it if pod restarted\n\nIndeed, I'll enhance it later.\n\nhow is ipvs rules being determined? \n\nCurrently, Each operation of service API will map to two types of operation on ipvs rules --- add and delete.  Generation code is here. real/virtualServer and protocol is determined by API arguments. Proxy algorithm is hard coded as NAT(masquerading), scheduling algorithm as RR and weight of real server as 1. \n\ncould them support configurations? e.g. algorithms, mode (DR/NAT/FULLNAT)\n\nI think part of them can support configuration if we need to. While I not sure about proxy algorithm, DR seems not meet our need, for can't support port mapping. I've not really concerned with FULLNAT before, I checked that alibaba LVS project just now, seems not updated  for a long time, and that part of code has not merged to upstream, I'm not sure we really need that part of feature. If we do, we may could use that version of ipvsadm and some kernel patch maybe.\nAs for others, scheduling algorithm could be configured, and weight of real server can be configured through API change. What do you think?. UPDATE:\n1. use hyperstart-exec feature, move runv part(https://github.com/hyperhq/runv/pull/409) to here.\n2. save change to db when service update succeed.\n3. add service field validation in podspec.\n4. fix a bug that pod factory forget to init db.\n@feiskyer PTAL . Yeap, will add soon.. I only find two types of container for now, service container and regular container. Hyperstart-exec container is a virtual container in hyperstart, won't affect this. I'm not sure others.. @gnawux So run with tty is not supported anymore?  Why we still leave this flag here. @gnawux I repeated exactly what I did (just start a hyperd and did what is above), everything is normal now.\nSo sorry, I can't reproduce it anymore. It's wired to me because operation is simple enough and this doesn't seem due to a config issue. \nAnyway, I'm gonna close this, if there is a change to meet it again, will dig deeper.. Anyone could help take a look at this test log? Something just not make sense.\nI add a test case here to test this api. And the order of operation to exec is:\nexecId, err := s.client.ContainerExecCreate(cName, []string{\"sh\", \"-c\", \"top\"}, false)\nerr = s.client.ContainerExecStart(cName, execId, nil, nil, nil, false)\nerr = s.client.ContainerExecSignal(cName, execId, sigKill)\nWhile test log in Jenkins is Create Exec->Kill Exec->Start Exec\n12:08:16 I0113 12:08:16.636046    7947 exec.go:12] create exec containerID:\"test-exec-signal\" command:\"sh\" command:\"-c\" command:\"top\" \n12:08:16 I0113 12:08:16.636073    7947 exec.go:34] Create Exec for container test-exec-signal\n12:08:16 I0113 12:08:16.636633    7947 vm_console.go:46] SB[vm-JrkUcrJWgM] [CNL] hyper_modify_event modify event fd 3, 0x61b568, event 1\n12:08:16 I0113 12:08:16.637094    7947 vm_console.go:46] SB[vm-JrkUcrJWgM] [CNL] hyper_handle_event event EPOLLOUT, he 0x1959eb8, fd 7, 0x61b500\n12:08:16 I0113 12:08:16.637109    7947 vm_console.go:46] SB[vm-JrkUcrJWgM] [CNL] write_to_stdin, seq 1\n12:08:16 I0113 12:08:16.637396    7947 vm_console.go:46] SB[vm-JrkUcrJWgM] [CNL] hyper_modify_event modify event fd 7, 0x1959eb8, event 0\n12:08:16 I0113 12:08:16.638507    7947 vm_console.go:46] SB[vm-JrkUcrJWgM] [CNL] pid 329 exit normally, status 0\n12:08:16 I0113 12:08:16.638846    7947 vm_console.go:46] SB[vm-JrkUcrJWgM] [CNL] pid 330 exit normally, status 0\n12:08:16 I0113 12:08:16.639016    7947 vm_console.go:46] SB[vm-JrkUcrJWgM] [CNL] pid 331 exit normally, status 0\n12:08:16 I0113 12:08:16.641498    7947 vm_console.go:46] SB[vm-JrkUcrJWgM] [CNL] hyper_install_process_stdio\n12:08:16 I0113 12:08:16.660299    7947 exec.go:108] ExecSignal with request containerID:\"test-exec-signal\" execID:\"exec-vcOJsBBZBr\" signal:9 \n12:08:16 I0113 12:08:16.660362    7947 exec.go:58] Kill Exec for container efdf3e785127da47bc287a0c761c24456e5062222c23b43f8c1ec317918d4b1b\n12:08:16 I0113 12:08:16.660388    7947 hypervisor.go:29] vm vm-JrkUcrJWgM: main event loop got message 16(GENERIC_OPERATION)\n12:08:16 I0113 12:08:16.660396    7947 vm_states.go:228] handle GenericOperation(SignalProcess) on state(RUNNING)\n12:08:16 I0113 12:08:16.660405    7947 init_comm.go:189] got cmd:24\n12:08:16 I0113 12:08:16.660432    7947 init_comm.go:281] send command 24 to init, payload: '{\"container\":\"efdf3e785127da47bc287a0c761c24456e5062222c23b43f8c1ec317918d4b1b\",\"process\":\"exec-vcOJsBBZBr\",\"signal\":9}'.\n12:08:16 I0113 12:08:16.660433    7947 exec.go:46] Start Exec for container test-exec-signal\n12:08:16 I0113 12:08:16.660457    7947 exec.go:117] Pod[busybox] Con[efdf3e785127] Exec[exec-vcOJsBBZBr] the sync chan is empty\n12:08:16 I0113 12:08:16.660475    7947 hypervisor.go:29] vm vm-JrkUcrJWgM: main event loop got message 16(GENERIC_OPERATION)\n12:08:16 I0113 12:08:16.660481    7947 vm_states.go:228] handle GenericOperation(AddProcess) on state(RUNNING)\n12:08:16 I0113 12:08:16.660521    7947 init_comm.go:294] write 127 to hyperstart.\nThe later kernel panic part could be fixed through https://github.com/hyperhq/hyperstart/pull/253, while that is not the original reason.\nBTW: That passed label on Hykins seems not reasonable either. @Jimmy-Xu . @Jimmy-Xu Yep, thanks. For now, I just couldn't understand the log behavior described in https://github.com/hyperhq/hyperd/pull/507#issuecomment-272595579, @gnawux could you help review this sometime?. @gao-feng Yes, this is the final reason here, and that is not the only error message I got when try to run this test over and over. When I try to debug, just couldn't understand the log in CI(and also in my local env). \nThis log in travis seems \"normal\" than jenkins, while handle GenericOperation(StartStdin) on state(RUNNING) is still behind ExecSignal with request.... \nAnd the failure in hykins is because when trying to kill process in hyperstart, cloud not find process with that exec-id, so I was wondering if there is some race condition.. Test case had some issues, too. Should work once hyperhq/hyperstart#257 be merged.. @gao-feng fixed. CI don't complain now.. Just notice that removing pod data from db is also not implement yet, so those test pods(including  ID:\"busybox\") is restored when hyperd start again. I guess this is the reason that when running  integration test the second time, many cases will fail.. @laijs I've tried to add removing pod/container from daemondb, it should work now.. @Jimmy-Xu Does hykins have a timeout, this test have run over 2 hours.. retest this please @hykins. @feiskyer fixed and CI passed.. @feiskyer I meet this too, should be fixed with #542 . @gnawux I'm not working on this, and also need some change in runv.. @feiskyer I've reverted changes about removing volumes from db, if it's needed, will open another PR to fix.\nalso /cc @gnawux do you have any comment on the disscusion?. I have no idea of why not a directory happens, while this one I can do something. @gnawux @gao-feng I agree there should be a approach to fix this. since there is already an issue related, I'll close this in favor a real solution.. find such log fragment, seems the reason why container start failed.\nI0315 03:14:30.698421   13728 vm_console.go:46] SB[vm-uSONjYYWGm] [CNL] try to find the user: nobody:nobody\nI0315 03:14:30.707577   13728 vm_console.go:46] SB[vm-uSONjYYWGm] [CNL] can't find the user: No such file or directory\nI0315 03:14:30.707791   13728 vm_console.go:46] SB[vm-uSONjYYWGm] [CNL] setup exec user failed. @gnawux updated and green now.. @gnawux Thanks, I've understood this. But I still think this is an(part of?) issue in hyperd, could we stop container just like we stop pod, if can't stop it with default(image specified) signal in graceful time, we need kill it with SIGKILL and return normally.\nI've tried in docker, container exit with status Exited (137), seems container have been killed.. @gnawux Glad to do this. Will do it later today or tomorrow. If rpcServer.Serve() failed at beginning, seems we do need glog.Fatalf.... yep, something like that, need that field in conatainer spec, and rootfs in container is read only.. gcr.io/google_containers/busybox:1.24. retest this please, hykins. here is complete v3 log. Hyperd is not serving because restore progress not finish yet. While the final reason I believe is a dead lock, RestoreContainer() in serial-hyperstart wait an response from hyperstart within lock, but socket connect failed and serial-hyperstart try to close with lock...\nI'll try to fix this.. @gnawux sure, I'll update it. Also need some cleanup code if hyperd restore failed. . I'm not sure how this happened, but the current one seems more reasonable, for I grep in hyperd repo, we didn't import github.com/opencontainers/image-spec directly\ncrazykev@CK:~/go-project/src/github.com/hyperhq/hyperd$ grep -r \"github.com\\/opencontainers\\/image-spec\"\nimage/tarexport/load.go:    ociv1 \"github.com/opencontainers/image-spec/specs-go/v1\"\nimage/tarexport/save-oci.go:    imgspec \"github.com/opencontainers/image-spec/specs-go\"\nimage/tarexport/save-oci.go:    ociv1 \"github.com/opencontainers/image-spec/specs-go/v1\"\nBinary file hyperd matches\nGodeps/Godeps.json:         \"ImportPath\": \"github.com/opencontainers/image-spec\",\nBinary file .git/index matches\nvendor/github.com/opencontainers/image-spec/specs-go/v1/image_index.go:import \"github.com/opencontainers/image-spec/specs-go\"\nvendor/github.com/opencontainers/image-spec/specs-go/v1/manifest.go:import \"github.com/opencontainers/image-spec/specs-go\". Seems this TestSuite.TestSendContainerSignal test have high possibility fail to response... . good to find that.. Seems there is some issue in hyperstart, I couldn't start a container(after exit a attach).\nI0418 10:48:42.098217    6683 vm_console.go:96] SB[vm-DPkMJaTycz] [CNL] recreate file ./etc/hosts\nI0418 10:48:42.098842    6683 vm_console.go:96] SB[vm-DPkMJaTycz] [CNL] recreate file ./etc/hostname\nI0418 10:48:42.099333    6683 vm_console.go:96] SB[vm-DPkMJaTycz] [CNL] recreate symlink ./etc/mtab to /proc/mounts\nI0418 10:48:42.099813    6683 vm_console.go:96] SB[vm-DPkMJaTycz] [CNL] container sets up init layer failed\nI0418 10:48:42.100067    6683 vm_console.go:96] SB[vm-DPkMJaTycz] [CNL] hyper send container inited event: error\nI0418 10:48:42.100319    6683 vm_console.go:96] SB[vm-DPkMJaTycz] [CNL] wait for setup container rootfs failed\nI0418 10:48:42.100762    6683 vm_console.go:96] SB[vm-DPkMJaTycz] [CNL] create child process pid=341 in the sandbox\nI0418 10:48:42.101124    6683 vm_console.go:96] SB[vm-DPkMJaTycz] [CNL] fail to enter container ns: Bad file descriptor\nI0418 10:48:42.101383    6683 vm_console.go:96] SB[vm-DPkMJaTycz] [CNL] hyper send enter container ns event: error\nI0418 10:48:42.101603    6683 vm_console.go:96] SB[vm-DPkMJaTycz] [CNL] hyper ctl append type 10, len 0\nI0418 10:48:42.101980    6683 vm_console.go:96] SB[vm-DPkMJaTycz] [CNL] hyper_handle_event event EPOLLOUT, he 0x61d648, fd 3, 0x61d4c0. @gnawux tried with hyperhq/hyperstart#287 and works for me.. This reminds another similar random test failure in frakti, which randomly missing stderr output. \nI tried with\nsudo ./hyperctl run --rm --name \"hello${i}\"  busybox blabla\nThere is a chance that stderr output exec failed: No such file or directory is missing. Is it also related with this?\n/cc @feiskyer . @laijs Cool, glad to help.. LGTM. @laijs Do you want have another look? . OK, I recheck current implement, if we start a running pod, there will be an ugly error\nhyperctl ERROR: Error from daemon's response: finished with errors: map[39c3371cd64c6a5c47e983cac3ce2529d9641b79f2eb0d0d201bea64d219b307:only CREATING container could be set to creatd, current: 3]\nCould you also fix this within this patch?. Return success or report pod is already running is fine, just don't try to add container to that pod again.. @gnawux Changes LGTM, although there is still little flaw in that error message, we can fix it later. This patch is more important.. @gnawux I think could be set to *creatd* in here should be could be set to *RUNNING*. CI passed, can I merge this?. Actually I never use this godep update (for not working as expected), when try to perform an update(on runv as an example) , I will \n1. remove all the entries of runv in hyperd/Godeps/Godeps.json\n2. use godep save or godep-save script in hack to re-vendor again.\nI'm not against this(don't like godep either...),  just try to offer a workaround if you don't want to change a vendor tool for 9 months.... Indeed, then that would be invent a wheel(write a update script) vs. change a tool \ud83d\ude06 . I test this with command hyperctl run --rm busybox echo hello, will delay 3 seconds to return, is this expected side-effect?. @gnawux Could we upgrade docker vendor to newer version, ie 1.12 ?. retest this please, hykins. @feiskyer Updated and CI passed.. merge this based on review an CI.. @laijs So how can we decide which tty without a client tag?\n. got it. I'll update this patch until related patch is merged.\n. yep, updated and rebased @feiskyer \n. Sure, rely too much on gofmt, fixed that.. yep, that's better. fixed.. @feiskyer intendedly, I just notice remove container shouldn't remove related volume from pod(at least there is no such logic here), so this should not store to db.. I'm not sure what the original design of this, could later created container use those volumes meant to be removed?  If not, yeah, we should remove it from pod and db. . @gnawux just in case someone happen to change that podfile, and not update this test.. yes, if graceful == 0, it is force kill directly:\n...\nforceKill := graceful == 0\nresChan := p.sandbox.WaitProcess(true, []string{c.Id()}, -1)\nerr := c.terminate(forceKill)\n...\nThis waitTime is the time we wait for container finish event before abort this mission(or try it with SIGKILL in non-zero graceful). If graceful == 0, it just wait 5s and won't kill again(return timeout error); if kill with image specified signal(graceful != 0), it will wait graceful time to kill with SIGKILL again.\nIs this 5 seconds too long?. Nop if we use docker:\n```\n$ docker stop --help\nUsage:  docker stop [OPTIONS] CONTAINER [CONTAINER...]\nStop a running container.\nSending SIGTERM and then SIGKILL after a grace period\n--help             Print usage\n  -t, --time=10      Seconds to wait for stop before killing it\n``\nor k8s way:\nhttps://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/api/v1alpha1/runtime/api.proto#L653-L658. I'm not sure I've understood why we need send this result back here?. OK, that should be fine, I was worried if we need to send this in restore.. Seems this is a wrong error message, could you also fix it here.\nI think this should beonly CREATED container could be set to RUNNING.p.statusLock == S_POD_PAUSED->p.status == S_POD_PAUSED.p.statusLock != S_POD_STARTING->p.status != S_POD_STARTING. I remember there is aS_POD_ERROR` state, should we filter it here?. Yep, more complicated than I thought.. ",
    "daehyeok": "FIxed mistyped word from\n--v=0                  Log level fro V logs\nto\n--v=0                  Log level for V logs\n. I removed xen to trying with KVM. but IIRC it was xen 4.6\n. Install xen 4.6 with apt then compile hyperd with ./configure --with-extra-qemuu-configure-args=\"--enable-virtfs\"\n-v=3 logs is\nI0616 14:25:32.369804    9063 server.go:70] Server created for HTTP on unix (/var/run/hyper.sock)\nI0616 14:25:32.371653    9063 xen.go:70] Xen capabilities: xen-3.0-x86_64 xen-3.0-x86_32p hvm-3.0-x86_32 hvm-3.0-x86_32p hvm-3.0-x86_64\nXen Driver Loaded.\nI0616 14:25:32.372079    9063 hyperd.go:208] The hypervisor's driver is xen\nI0616 14:25:32.372951    9063 network_linux.go:262] bridge exist\nI0616 14:25:32.393254    9063 iptables_linux.go:140] /sbin/iptables, [--wait -t nat -C POSTROUTING -s 192.168.123.1/24 ! -o hyper0 -j MASQUERADE]\nI0616 14:25:32.393537    9063 xen.go:92] got SIGCHLD, send msg to libxl\ngot child pid: 0\nI0616 14:25:32.410637    9063 iptables_linux.go:140] /sbin/iptables, [--wait -N HYPER]\nI0616 14:25:32.410818    9063 xen.go:92] got SIGCHLD, send msg to libxl\ngot child pid: -1\nI0616 14:25:32.427839    9063 iptables_linux.go:140] /sbin/iptables, [--wait -t filter -C FORWARD -o hyper0 -j HYPER]\nI0616 14:25:32.427910    9063 xen.go:92] got SIGCHLD, send msg to libxl\ngot child pid: -1\nI0616 14:25:32.444897    9063 iptables_linux.go:140] /sbin/iptables, [--wait -t filter -C FORWARD -i hyper0 -j ACCEPT]\nI0616 14:25:32.445070    9063 xen.go:92] got SIGCHLD, send msg to libxl\ngot child pid: -1\nI0616 14:25:32.462172    9063 iptables_linux.go:140] /sbin/iptables, [--wait -t filter -C FORWARD -o hyper0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT]\nI0616 14:25:32.462330    9063 xen.go:92] got SIGCHLD, send msg to libxl\ngot child pid: -1\nI0616 14:25:32.479996    9063 xen.go:92] got SIGCHLD, send msg to libxl\ngot child pid: -1\nI0616 14:25:32.497480    9063 iptables_linux.go:140] /sbin/iptables, [--wait -t nat -N HYPER]\nI0616 14:25:32.497555    9063 xen.go:92] got SIGCHLD, send msg to libxl\ngot child pid: -1\nI0616 14:25:32.514099    9063 iptables_linux.go:140] /sbin/iptables, [--wait -t nat -C OUTPUT -m addrtype --dst-type LOCAL ! -d 127.0.0.1/8 -j HYPER]\nI0616 14:25:32.514256    9063 xen.go:92] got SIGCHLD, send msg to libxl\ngot child pid: -1\nI0616 14:25:32.531015    9063 iptables_linux.go:140] /sbin/iptables, [--wait -t nat -C PREROUTING -m addrtype --dst-type LOCAL -j HYPER]\nI0616 14:25:32.531205    9063 xen.go:92] got SIGCHLD, send msg to libxl\ngot child pid: -1\nI0616 14:25:32.548261    9063 xen.go:92] got SIGCHLD, send msg to libxl\ngot child pid: -1\n. Oops i misunderstood. I thought it is option for hyperd...\n. ",
    "YaoZengzeng": "Ready for review, after the pr of \"runv #267\"  and \"hyperstart #121\" being merged.\n. OK\uff0c I\u2018ll do it.\n. Yes,I've tried it with \"repeated byte\",but there seems no tyep as \"byte\".And \"bytes\" is OK\n. No, it will return the whole bytes in buffer,and \"err = io.EOF\".And I have to process it and return the left bytes without '\\n'.But maybe we can just read the logs at once,like\n rpc ContainerLogs(ContainerLogsRequest) returns (ContainerLogsResponse) {}?\n. The channel \"stop\" seems useless, it doesn't be used anywhere in daemon.GetContainerLogs?\nAnd I want to use it to stop the daemon.GetContainerLogs if I dont want to get logs anymore.\nMay I use the \"stop\" in daemon.GetContainerLogs to notify the method to stop ?\n. ref to runv pr #251\n. Yes,I'll do it in next pr.\nJust call  \"SetOneContainerStatus\" in runv is fine.\n. It's necessary.The first time before \"daemon.WritePodAndContainers(pod.Id)\", we need put pod to update pod status.Then the spec of pod also being changed,so we need to update again to keep pod fresh.\n. ",
    "jgillich": "Any example for these crash messages in /var/log/messages? I can't find them, but I've had hyperd crash with no error two times now... Ok, seems like you're supposed to send the Podfile in the body. But what's the purpose of podArgs then?. Thanks, good to know. Also /container/create isn't documented currently.\nAnyway, I've decided to just learn Go and use the hyperd client directly, however I'm running in some issues. If I try to compile 0.7.0 with make, I get this:\n```\n$ make\nmake[1]: Entering directory '/home/jgillich/devel/src/golinux/vendor/github.com/hyperhq/hyperd'\ngo build -tags \"static_build   libdm_no_deferred_remove exclude_graphdriver_btrfs\" -ldflags \"-X github.com/hyperhq/hyperd/utils.VERSION=0.7.0\" hyperd.go\npanic: runtime error: slice bounds out of range\ngoroutine 1 [running]:\npanic(0x7e1940, 0xc4200101b0)\n    /usr/lib/golang/src/runtime/panic.go:500 +0x1a1\nmain.vendoredImportPath(0xc4204d9680, 0xc4202d1c01, 0x1a, 0x3, 0x0)\n    /usr/lib/golang/src/cmd/go/pkg.go:463 +0x66c\nmain.loadImport(0xc4202d1c01, 0x1a, 0xc420498730, 0x4e, 0xc4204d9680, 0xc4204b97e0, 0xc42047d740, 0x1, 0x1, 0x1, ...)\n    /usr/lib/golang/src/cmd/go/pkg.go:333 +0x9ea\nmain.(Package).load(0xc4204d9680, 0xc4204b97e0, 0xc420484000, 0x0, 0x0, 0x4)\n    /usr/lib/golang/src/cmd/go/pkg.go:940 +0x12b6\nmain.loadImport(0xc4202f9321, 0x26, 0xc420211380, 0x74, 0xc4204f5200, 0xc4204b97e0, 0xc4201bc4e0, 0x1, 0x1, 0x1, ...)\n    /usr/lib/golang/src/cmd/go/pkg.go:374 +0x470\nmain.(Package).load(0xc4204f5200, 0xc4204b97e0, 0xc420366700, 0x0, 0x0, 0x4)\n    /usr/lib/golang/src/cmd/go/pkg.go:940 +0x12b6\nmain.loadImport(0xc4203b2be1, 0x1c, 0xc42026f900, 0x4a, 0xc420202480, 0xc4204b97e0, 0xc4202e33e0, 0x1, 0x1, 0x1, ...)\n    /usr/lib/golang/src/cmd/go/pkg.go:374 +0x470\nmain.(Package).load(0xc420202480, 0xc4204b97e0, 0xc420115c00, 0x0, 0x0, 0xc)\n    /usr/lib/golang/src/cmd/go/pkg.go:940 +0x12b6\nmain.loadImport(0xc42048c181, 0x22, 0xc420449310, 0x47, 0xc42041c480, 0xc4204b97e0, 0xc4202a6000, 0x20, 0x20, 0x1, ...)\n    /usr/lib/golang/src/cmd/go/pkg.go:374 +0x470\nmain.(Package).load(0xc42041c480, 0xc4204b97e0, 0xc4203faa80, 0x0, 0x0, 0xc)\n    /usr/lib/golang/src/cmd/go/pkg.go:940 +0x12b6\nmain.loadImport(0xc420254931, 0x1f, 0xc4204114a0, 0x48, 0xc42030f200, 0xc4204b97e0, 0xc420255170, 0x1, 0x1, 0x1, ...)\n    /usr/lib/golang/src/cmd/go/pkg.go:374 +0x470\nmain.(Package).load(0xc42030f200, 0xc4204b97e0, 0xc420220000, 0x0, 0x0, 0xc)\n    /usr/lib/golang/src/cmd/go/pkg.go:940 +0x12b6\nmain.loadImport(0xc4201a1051, 0x20, 0xc420152a00, 0x41, 0xc4201b4000, 0xc4204b97e0, 0xc4201a1590, 0x1, 0x1, 0x1, ...)\n    /usr/lib/golang/src/cmd/go/pkg.go:374 +0x470\nmain.(Package).load(0xc4201b4000, 0xc4204b97e0, 0xc420189180, 0x0, 0x0, 0xc4201a2860)\n    /usr/lib/golang/src/cmd/go/pkg.go:940 +0x12b6\nmain.goFilesPackage(0xc42000c1b0, 0x1, 0x1, 0x3)\n    /usr/lib/golang/src/cmd/go/build.go:830 +0x87e\nmain.packagesAndErrors(0xc42000c1b0, 0x1, 0x1, 0x487598, 0x20, 0x7f3ac0)\n    /usr/lib/golang/src/cmd/go/pkg.go:1687 +0x4f7\nmain.packagesForBuild(0xc42000c1b0, 0x1, 0x1, 0x893e68, 0xc42001c080, 0x0)\n    /usr/lib/golang/src/cmd/go/pkg.go:1719 +0x75\nmain.runBuild(0xa27420, 0xc42000c1b0, 0x1, 0x1)\n    /usr/lib/golang/src/cmd/go/build.go:440 +0xd4\nmain.main()\n    /usr/lib/golang/src/cmd/go/main.go:181 +0x624\nMakefile:785: recipe for target 'build-hyperd' failed\nmake[1]:  [build-hyperd] Error 2\nmake[1]: Leaving directory '/home/jgillich/devel/src/golinux/vendor/github.com/hyperhq/hyperd'\nMakefile:359: recipe for target 'all-recursive' failed\nmake:  [all-recursive] Error 1\n```\nAny idea what's going on here?\nBefore that I've simply imported hyperd/client via Go, which resulted in all requests being made to /v/..., without a version, so all requests 404'd.\n. Ah I think I know why, I've used glide so hyperd is not below $GOPATH..I guess I have to somehow implement the makefile in my own project. Fun stuff.. Anyway, closing this. If you don't beat my to it, I will create a PR to update this soon-ish.. TIL go get does not have any version management, using Glide fixed it.. ping @gnawux :). I've tried setting that, but when I run my project with go run -ldflags \"-X github.com/hyperhq/hyperd/utils.VERSION=0.7.0\" main.go, it's still empty. hyperd is installed to vendor/ by glide, not sure if that makes a difference.. Aww, turns out when using the vendor directory, the path is relative from the main project.\nsh\ngo run -ldflags \"-X github.com/my/project/vendor/github.com/hyperhq/hyperd/utils.VERSION=0.7.0\" main.go\nWorks \ud83d\udc4d . Totally forgot to update godeps, but I don't know what's going on here:\n``\n $ godep save\ngodep: WARNING: Godep workspaces (./Godeps/_workspace) are deprecated and support for them will be removed when go1.8 is released.\ngodep: WARNING: Go version (go1.7) & $GO15VENDOREXPERIMENT= wants to enable the vendor experiment, but disabling because a Godep workspace (Godeps/_workspace) exists\ngodep: WARNING: Recorded major go version (go1.5) and in-use major go version (go1.7) differ.\ngodep: To record current major go version rungodep update -goversion`.\ngodep: Package (github.com/docker/docker/pkg/mflag) not found\n``. Have you considered using [glide](https://github.com/Masterminds/glide)? I'm in no rush with this PR, but I could maybe help with moving tovendor/`.. What do you mean? This is broken because the Godeps need updating.. Ah cool :). ",
    "Ddnirvana": "After using the newest code from git and compile hyperd from source codes,  I meet two problems using hyperctl.\n\nProblem 1\nI find that the pulling procedure will terminate before the actual pulling finished.\nLike this:\n\nddnirvana@ddPC:~$ hyperctl pull tomcat\nUsing default tag: latest\nlatest: Pulling from library/tomcat\nc28cbef85c39: Pull complete \n99f9a98f89c8: Pull complete \nd63e4aacbce8: Verifying Checksum \ndb12a40f67fd: Verifying Checksum \n0fc54345da07: Pulling fs layer \n32e09d69ee75: Pulling fs layer \n3339b4165c6d: Download complete \n54d43f138a73: Verifying Checksum \nfc28821752fe: Pulling fs layer \nc341484ab8e5: Pulling fs layer \n64a964a104dd: Verifying Checksum \nf54bd9444103: Download complete \n5fcdbb5d1dee: Download complete \n6bd53bf8e4ea: Download complete \n61beab82b39b: Download complete \n6099472c90b2: Download complete \n0ddac78e39bb: Verifying Checksum \n877822f67d63: Verifying Checksum \nb39df29eb8be: Download complete \n7abca3a2abee: Pulling fs layer \n47f05bc318c9: Download complete \n9c8adbc8edc0: Pulling fs layer \nd98f94fec048: Verifying Checksum \n02c64b4cd86b: Pulling fs layer \ndc0f2bfc7860: Download complete \nPulling repository docker.io/library/tomcat\nddnirvana@ddPC:~$ hyperctl images                                                                                                                                                                                                              \nREPOSITORY          TAG                 IMAGE ID            CREATED               VIRTUAL SIZE\nubuntu                latest               44f18eb3c13f        2016-07-09 02:39:43   124.9 MB\nhello-world          latest               95f1eedc264a        2016-07-02 03:39:27   1.848 kB\nbusybox              latest               b41c5284db84        2016-06-24 07:23:37   1.093 MB\nnone                   none               99f9a98f89c8        2016-06-10 06:08:24   171.3 MB\nnone                   none               f10563ca252d        2016-06-02 02:00:57   182.7 MB\n\nso maybe there is something like timeout in the hyper's pull procedure?\n\nProblem 2\nAnd, using this kind of hyperd, I find I can not run even hyperctl run busybox any more.. :(\nAnd I accidentally find that there are some log infos appeared in the dmesg after I run hyperctl runcommand:\n\n157612.737281] EXT4-fs (dm-7): mounted filesystem with ordered data mode. Opts: (null)\n[157613.470724] EXT4-fs (dm-7): mounted filesystem with ordered data mode. Opts: (null)\n[157613.966348] EXT4-fs (dm-7): mounted filesystem with ordered data mode. Opts: discard\n[157614.313190] device tap4 entered promiscuous mode\n[157614.313719] hyper0: port 5(tap4) entered forwarding state\n[157614.313737] hyper0: port 5(tap4) entered forwarding state\n[157614.513591] userif-3: sent link down event.\n[157614.513595] userif-3: sent link up event.\n[157614.866208] hyper0: port 5(tap4) entered disabled state\n[157614.868423] device tap4 left promiscuous mode\n[157614.868428] hyper0: port 5(tap4) entered disabled state\n[157614.873902] buffer_io_error: 182 callbacks suppressed\n[157614.873907] Buffer I/O error on dev dm-7, logical block 26214384, async page read\n[157614.873934] Buffer I/O error on dev dm-7, logical block 26214398, async page read\n[157614.873949] Buffer I/O error on dev dm-7, logical block 0, async page read\n[157614.873961] Buffer I/O error on dev dm-7, logical block 1, async page read\n[157614.873972] Buffer I/O error on dev dm-7, logical block 26214399, async page read\n[157614.874050] Buffer I/O error on dev dm-7, logical block 26214399, async page read\n[157614.874063] Buffer I/O error on dev dm-7, logical block 26214399, async page read\n[157614.874074] Buffer I/O error on dev dm-7, logical block 26214399, async page read\n[157614.874084] Buffer I/O error on dev dm-7, logical block 26214399, async page read\n[157614.874094] Buffer I/O error on dev dm-7, logical block 26214399, async page read\n[157615.066047] userif-3: sent link down event.\n[157615.066052] userif-3: sent link up event.\n[157922.095619] kvm [12589]: vcpu0 unhandled rdmsr: 0x1c9\n[157922.095624] kvm [12589]: vcpu0 unhandled rdmsr: 0x1a6\n[157922.095626] kvm [12589]: vcpu0 unhandled rdmsr: 0x1a7\n[157922.095628] kvm [12589]: vcpu0 unhandled rdmsr: 0x3f6\n[157922.152680] kvm [12589]: vcpu0 unhandled rdmsr: 0x606\n[157922.152693] kvm [12589]: vcpu0 unhandled rdmsr: 0x34\n[157942.595018] kvm [12601]: vcpu0 unhandled rdmsr: 0x1c9\n[157942.595023] kvm [12601]: vcpu0 unhandled rdmsr: 0x1a6\n[157942.595025] kvm [12601]: vcpu0 unhandled rdmsr: 0x1a7\n[157942.595027] kvm [12601]: vcpu0 unhandled rdmsr: 0x3f6\n[157942.654808] kvm [12601]: vcpu0 unhandled rdmsr: 0x606\n[157942.654821] kvm [12601]: vcpu0 unhandled rdmsr: 0x34\n[157964.902725] EXT4-fs (dm-7): mounted filesystem with ordered data mode. Opts: (null)\n[158049.722366] kvm [12659]: vcpu0 unhandled rdmsr: 0x1c9\n[158049.722371] kvm [12659]: vcpu0 unhandled rdmsr: 0x1a6\n[158049.722373] kvm [12659]: vcpu0 unhandled rdmsr: 0x1a7\n[158049.722375] kvm [12659]: vcpu0 unhandled rdmsr: 0x3f6\n[158049.779576] kvm [12659]: vcpu0 unhandled rdmsr: 0x606\n[158049.779590] kvm [12659]: vcpu0 unhandled rdmsr: 0x34\n\nAny advice about these problems? Thanks a lot!\n. @gao-feng  Thanks for your replying! The logs from hyperd are:\n\nroot@ddPC:/home/ddnirvana# hyperd -v=3 --nondaemon\nI0720 17:59:27.223318    4463 hyperd.go:121] The config file is \nI0720 17:59:27.223719    4463 daemon.go:141] The config: kernel=/var/lib/hyper/kernel, initrd=/var/lib/hyper/hyper-initrd.img\nI0720 17:59:27.223735    4463 daemon.go:143] The config: vbox image=\nI0720 17:59:27.223745    4463 daemon.go:146] The config: bridge=, ip=\nI0720 17:59:27.223754    4463 daemon.go:149] The config: bios=/var/lib/hyper/bios-qboot.bin, cbfs=/var/lib/hyper/cbfs-qboot.rom\nE0720 17:59:27.223798    4463 daemondb.go:23] open leveldb file failed, resource temporarily unavailable\nE0720 17:59:27.223989    4463 hyperd.go:158] The hyperd create failed, resource temporarily unavailable\n\nIt seems something is wrong.....\n. Thanks @gao-feng ! \nI start hyperd using hyperd -v=3 --nondaemon\nAnd then I run hyperctl run -t ubuntu:latest ,the logs printed are the following:\n\nI0721 09:03:59.716659   10996 pod.go:550] create container d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921\nI0721 09:03:59.716730   10996 pod.go:590] container name ubuntu-latest-3026073411, image ubuntu:latest\nI0721 09:03:59.716797   10996 pod.go:617] container info config &{d76da517adcd   false false false map[]  false false false [PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin] 0xc820aea760 false ubuntu:latest map[]   true  [] map[] }, Cmd [/bin/bash], Args []\nI0721 09:03:59.716929   10996 pod.go:641] Container Info is \n&{containerID:\"d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921\" commands:\"/bin/bash\" env:  1469063038 d573765f4e1dfc8957ef553be02253d7b9c918880a532ec09d9997f912cc3442   true}\nI0721 09:03:59.717451   10996 daemondb.go:91] try set container list for pod pod-YzrFNHTojU: [d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921]\nI0721 09:03:59.718170   10996 server.go:152] Calling GET /v0.6.0/pod/info\nI0721 09:03:59.719641   10996 server.go:152] Calling POST /v1.17/pod/start\nI0721 09:03:59.719720   10996 run.go:59] Run pod with tty attached\nI0721 09:03:59.719736   10996 run.go:67] pod:pod-YzrFNHTojU, vm:vm-BgDXhMAtjB\nI0721 09:03:59.719754   10996 pod.go:320] lock pod pod-YzrFNHTojU for operation start\nI0721 09:03:59.719767   10996 pod.go:323] successfully lock pod pod-YzrFNHTojU for operation start\nI0721 09:03:59.719781   10996 vm.go:229] find vm:vm-BgDXhMAtjB\nI0721 09:03:59.719811   10996 pod.go:951] container ID: d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921, mountId d573765f4e1dfc8957ef553be02253d7b9c918880a532ec09d9997f912cc3442\nI0721 09:03:59.772379   10996 dm.go:95] The filesytem type is ext4\nI0721 09:04:00.137684   10996 volumes.go:29] trying to bind dir /var/lib/hyper/hosts/pod-YzrFNHTojU/hosts to /var/run/hyper/vm-BgDXhMAtjB/share_dir/yuxhXREgeO\nI0721 09:04:00.138452   10996 storage.go:79] dir /var/lib/hyper/hosts/pod-YzrFNHTojU/hosts is bound to yuxhXREgeO\nI0721 09:04:00.138487   10996 pod.go:1109] configuring log driver [json-file] for pod-YzrFNHTojU\nI0721 09:04:00.138533   10996 pod.go:1137] configure container log to /var/run/hyper/Pods/pod-YzrFNHTojU/d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921-json.log\nI0721 09:04:00.138575   10996 pod.go:1143] configured logger for pod-YzrFNHTojU/d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921 (/ubuntu-latest-3026073411)\nI0721 09:04:00.138622   10996 hypervisor.go:29] vm vm-BgDXhMAtjB: main event loop got message 34(GENERIC_OPERATION)\nI0721 09:04:00.138639   10996 vm_states.go:296] handle GenericOperation(Attach) on state(INIT)\nI0721 09:04:00.138697   10996 vm_states.go:229] attachment d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921 is pending\nI0721 09:04:00.138735   10996 hypervisor.go:29] vm vm-BgDXhMAtjB: main event loop got message 34(GENERIC_OPERATION)\nI0721 09:04:00.138750   10996 vm_states.go:296] handle GenericOperation(Attach) on state(INIT)\nI0721 09:04:00.138765   10996 vm_states.go:229] attachment d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921 is pending\nI0721 09:04:00.138780   10996 pod.go:1195] Attach to container d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921 before start pod\nI0721 09:04:00.138833   10996 hypervisor.go:29] vm vm-BgDXhMAtjB: main event loop got message 21(COMMAND_RUN_POD)\nI0721 09:04:00.138851   10996 vm_states.go:443] got spec, prepare devices\nI0721 09:04:00.138870   10996 context.go:284] #0 Container Info:\nI0721 09:04:00.138951   10996 vm.go:162] hyperHandlePodEvent pod pod-YzrFNHTojU, vm vm-BgDXhMAtjB\nI0721 09:04:00.139047   10996 context.go:287] \n{\n...|    \"Id\": \"d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921\",\n...|    \"User\": \"\",\n...|    \"MountId\": \"d573765f4e1dfc8957ef553be02253d7b9c918880a532ec09d9997f912cc3442\",\n...|    \"Rootfs\": \"/rootfs\",\n...|    \"Image\": {\n...|        \"name\": \"\",\n...|        \"source\": \"/dev/mapper/docker-8:1-1704424-d573765f4e1dfc8957ef553be02253d7b9c918880a532ec09d9997f912cc3442\",\n...|        \"driver\": \"\",\n...|        \"option\": {\n...|            \"monitors\": null,\n...|            \"user\": \"\",\n...|            \"keyring\": \"\",\n...|            \"bytespersec\": 0,\n...|            \"iops\": 0\n...|        }\n...|    },\n...|    \"Fstype\": \"ext4\",\n...|    \"Workdir\": \"\",\n...|    \"Entrypoint\": null,\n...|    \"Cmd\": [\n...|        \"/bin/bash\"\n...|    ],\n...|    \"Envs\": {\n...|        \"PATH\": \"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n...|    },\n...|    \"Initialize\": true\n...|}\nI0721 09:04:00.139112   10996 devicemap.go:196] insert volume /dev/mapper/docker-8:1-1704424-d573765f4e1dfc8957ef553be02253d7b9c918880a532ec09d9997f912cc3442 source /dev/mapper/docker-8:1-1704424-d573765f4e1dfc8957ef553be02253d7b9c918880a532ec09d9997f912cc3442 fstype ext4\nI0721 09:04:00.139131   10996 devicemap.go:280] insert volume etchosts-volume to /etc/hosts on 0\nI0721 09:04:00.139445   10996 vm_states.go:67] initial vm spec: {\n        \"hostname\": \"ubuntu-latest-3026073411\",\n        \"containers\": [\n            {\n                \"id\": \"d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921\",\n                \"rootfs\": \"/rootfs\",\n                \"fstype\": \"ext4\",\n                \"image\": \"\",\n                \"fsmap\": [\n                    {\n                        \"source\": \"yuxhXREgeO\",\n                        \"path\": \"/etc/hosts\",\n                        \"readOnly\": false,\n                        \"dockerVolume\": false\n                    }\n                ],\n                \"process\": {\n                    \"terminal\": true,\n                    \"stdio\": 1,\n                    \"args\": [\n                        \"/bin/bash\"\n                    ],\n                    \"envs\": [\n                        {\n                            \"env\": \"PATH\",\n                            \"value\": \"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n                        }\n                    ],\n                    \"workdir\": \"/\"\n                },\n                \"restartPolicy\": \"never\",\n                \"initialize\": true\n            }\n        ],\n        \"shareDir\": \"share_dir\"\n    }\nI0721 09:04:00.139496   10996 context.go:216] found container d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921 at 0\nI0721 09:04:00.139510   10996 vm_states.go:75] attach pending client for d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921\nI0721 09:04:00.139522   10996 vm_states.go:247] Connecting tty for d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921 on session 1\nI0721 09:04:00.139533   10996 context.go:216] found container d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921 at 0\nI0721 09:04:00.139542   10996 vm_states.go:75] attach pending client for d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921\nI0721 09:04:00.139551   10996 vm_states.go:247] Connecting tty for d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921 on session 1\nI0721 09:04:00.139594   10996 context.go:252] VM vm-BgDXhMAtjB: state change from INIT to 'STARTING'\nI0721 09:04:00.139611   10996 qmp_handler.go:296] got new session\nI0721 09:04:00.139632   10996 qmp_handler.go:225] Begin process command session\nI0721 09:04:00.139655   10996 qmp_handler.go:243] sending command (1) {\"execute\":\"human-monitor-command\",\"arguments\":{\"command-line\":\"drive_add dummy file=/dev/mapper/docker-8:1-1704424-d573765f4e1dfc8957ef553be02253d7b9c918880a532ec09d9997f912cc3442,if=none,id=drive0,format=raw,cache=writeback\"}}\nI0721 09:04:00.142030   10996 hypervisor.go:29] vm vm-BgDXhMAtjB: main event loop got message 12(EVENT_INTERFACE_ADD)\nI0721 09:04:00.142058   10996 qmp_wrapper.go:90] send net to qemu at 23\nI0721 09:04:00.142085   10996 qmp_handler.go:296] got new session\nI0721 09:04:00.143147   10996 qmp_handler.go:103] got a message {\"return\": \"OK\\r\\n\"}\nI0721 09:04:00.143208   10996 qmp_handler.go:243] sending command (1) {\"execute\":\"device_add\",\"arguments\":{\"bus\":\"scsi0.0\",\"drive\":\"drive0\",\"driver\":\"scsi-hd\",\"id\":\"scsi-disk0\",\"scsi-id\":\"0\"}}\nI0721 09:04:00.145729   10996 qmp_handler.go:103] got a message {\"return\": {}}\nI0721 09:04:00.145779   10996 qmp_handler.go:302] session finished, buffer size 2\nI0721 09:04:00.145792   10996 qmp_handler.go:305] success \nI0721 09:04:00.145829   10996 qmp_handler.go:225] Begin process command session\nI0721 09:04:00.145868   10996 qmp_handler.go:238] send cmd with scm (24 bytes) (1) {\"execute\":\"getfd\",\"arguments\":{\"fdname\":\"fdeth0\"}}\nI0721 09:04:00.145910   10996 hypervisor.go:29] vm vm-BgDXhMAtjB: main event loop got message 9(EVENT_BLOCK_INSERTED)\nI0721 09:04:00.148466   10996 init_comm.go:68] [console] scsi 0:0:0:0: Direct-Access     QEMU     QEMU HARDDISK    2.0. PQ: 0 ANSI: 5\nI0721 09:04:00.150411   10996 init_comm.go:68] [console] sd 0:0:0:0: Attached scsi generic sg0 type 0\nI0721 09:04:00.150780   10996 qmp_handler.go:103] got a message {\"return\": {}}\nI0721 09:04:00.151037   10996 qmp_handler.go:243] sending command (1) {\"execute\":\"netdev_add\",\"arguments\":{\"fd\":\"fdeth0\",\"id\":\"eth0\",\"type\":\"tap\"}}\nI0721 09:04:00.152836   10996 init_comm.go:68] [console] sd 0:0:0:0: [sda] 209715200 512-byte logical blocks: (107 GB/100 GiB)\nI0721 09:04:00.154043   10996 init_comm.go:68] [console] sd 0:0:0:0: [sda] Write Protect is off\nI0721 09:04:00.156898   10996 qmp_handler.go:103] got a message {\"return\": {}}\nI0721 09:04:00.157133   10996 qmp_handler.go:243] sending command (1) {\"execute\":\"device_add\",\"arguments\":{\"addr\":\"0x5\",\"bus\":\"pci.0\",\"driver\":\"virtio-net-pci\",\"id\":\"eth0\",\"mac\":\"52:54:21:1a:97:10\",\"netdev\":\"eth0\"}}\nI0721 09:04:00.163013   10996 qmp_handler.go:103] got a message {\"return\": {}}\nI0721 09:04:00.163151   10996 qmp_handler.go:302] session finished, buffer size 1\nI0721 09:04:00.163165   10996 qmp_handler.go:305] success \nI0721 09:04:00.163177   10996 hypervisor.go:29] vm vm-BgDXhMAtjB: main event loop got message 14(EVENT_INTERFACE_INSERTED)\nI0721 09:04:00.163195   10996 vm_states.go:470] device ready, could run pod.\nI0721 09:04:00.163209   10996 init_comm.go:190] got cmd:1\nI0721 09:04:00.163299   10996 init_comm.go:281] send command 1 to init, payload: '{\"hostname\":\"ubuntu-latest-3026073411\",\"containers\":[{\"id\":\"d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921\",\"rootfs\":\"/rootfs\",\"fstype\":\"ext4\",\"image\":\"sda\",\"fsmap\":[{\"source\":\"yuxhXREgeO\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"terminal\":true,\"stdio\":1,\"args\":[\"/bin/bash\"],\"envs\":[{\"env\":\"PATH\",\"value\":\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"}],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":true}],\"interfaces\":[{\"device\":\"eth0\",\"ipAddress\":\"192.168.123.2\",\"netMask\":\"255.255.255.0\"}],\"routes\":[{\"dest\":\"0.0.0.0/0\",\"gateway\":\"192.168.123.1\",\"device\":\"eth0\"}],\"shareDir\":\"share_dir\"}'.\nI0721 09:04:00.163330   10996 init_comm.go:294] write 512 to init, payload: '\u0001\ufffd{\"hostname\":\"ubuntu-latest-3026073411\",\"containers\":[{\"id\":\"d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921\",\"rootfs\":\"/rootfs\",\"fstype\":\"ext4\",\"image\":\"sda\",\"fsmap\":[{\"source\":\"yuxhXREgeO\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"terminal\":true,\"stdio\":1,\"args\":[\"/bin/bash\"],\"envs\":[{\"env\":\"PATH\",\"value\":\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"}],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":true}],\"interfaces\":[{\"device\":\"'.\nI0721 09:04:00.163346   10996 init_comm.go:299] message sent, set pong timer\nI0721 09:04:00.163371   10996 init_comm.go:68] [console] sd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA\nI0721 09:04:00.164205   10996 init_comm.go:68] [console] hyper_loop epoll_wait 1\nI0721 09:04:00.165417   10996 init_comm.go:68] [console] hyper_handle_event get event 1, de 0x613578, fd 4. ops 0x613320\nI0721 09:04:00.168031   10996 init_comm.go:68] [console] pci 0000:00:05.0: BAR 6: assigned [mem 0x20040000-0x2007ffff pref]\nI0721 09:04:00.169033   10996 init_comm.go:68] [console] pci 0000:00:05.0: BAR 1: assigned [mem 0x20003000-0x20003fff]\nI0721 09:04:00.170323   10996 init_comm.go:68] [console] pci 0000:00:05.0: BAR 0: assigned [io  0x10c0-0x10df]\nI0721 09:04:00.194640   10996 init_comm.go:68] [console] ACPI: PCI Interrupt Link [LNKA] enabled at IRQ 10\nI0721 09:04:00.195856   10996 init_comm.go:68] [console] virtio-pci 0000:00:05.0: enabling device (0000 -> 0003)\nI0721 09:04:00.198274   10996 init_comm.go:68] [console] virtio-pci 0000:00:05.0: virtio_pci: leaving for legacy driver\nI0721 09:04:00.221717   10996 init_comm.go:68] [console] hyper_handle_event event EPOLLIN, de 0x613578, fd 4, 0x613320\nI0721 09:04:00.221950   10996 init_comm.go:68] [console] hyper_event_read\nI0721 09:04:00.222426   10996 init_comm.go:68] [console] already read 8 bytes data\nI0721 09:04:00.223003   10996 init_comm.go:68] [console] hyper send type 14, len 4\nI0721 09:04:00.223128   10996 init_comm.go:106] read 8/8 [length = 0]\nI0721 09:04:00.223145   10996 init_comm.go:110] data length is 12\nI0721 09:04:00.223156   10996 init_comm.go:96] trying to read 4 bytes\nI0721 09:04:00.223172   10996 init_comm.go:106] read 12/12 [length = 12]\nI0721 09:04:00.223190   10996 init_comm.go:96] trying to read 8 bytes\nI0721 09:04:00.223209   10996 init_comm.go:190] got cmd:14\nI0721 09:04:00.223223   10996 init_comm.go:253] get command NEXT\nI0721 09:04:00.223234   10996 init_comm.go:256] send 512, receive 8\nI0721 09:04:00.223362   10996 init_comm.go:68] [console] get length 671\nI0721 09:04:00.223756   10996 init_comm.go:68] [console] read 504 bytes data, total data 512\nI0721 09:04:00.224067   10996 init_comm.go:68] [console] hyper send type 14, len 4\nI0721 09:04:00.224843   10996 init_comm.go:106] read 8/8 [length = 0]\nI0721 09:04:00.224869   10996 init_comm.go:110] data length is 12\nI0721 09:04:00.224882   10996 init_comm.go:96] trying to read 4 bytes\nI0721 09:04:00.224901   10996 init_comm.go:106] read 12/12 [length = 12]\nI0721 09:04:00.224922   10996 init_comm.go:96] trying to read 8 bytes\nI0721 09:04:00.224936   10996 init_comm.go:190] got cmd:14\nI0721 09:04:00.224944   10996 init_comm.go:253] get command NEXT\nI0721 09:04:00.224952   10996 init_comm.go:256] send 512, receive 512\nI0721 09:04:00.224964   10996 init_comm.go:294] write 159 to init, payload: 'eth0\",\"ipAddress\":\"192.168.123.2\",\"netMask\":\"255.255.255.0\"}],\"routes\":[{\"dest\":\"0.0.0.0/0\",\"gateway\":\"192.168.123.1\",\"device\":\"eth0\"}],\"shareDir\":\"share_dir\"}'.\nI0721 09:04:00.225328   10996 init_comm.go:68] [console] hyper_loop epoll_wait 1\nI0721 09:04:00.225982   10996 init_comm.go:68] [console] hyper_handle_event get event 1, de 0x613578, fd 4. ops 0x613320\nI0721 09:04:00.226598   10996 init_comm.go:68] [console] hyper_handle_event event EPOLLIN, de 0x613578, fd 4, 0x613320\nI0721 09:04:00.226846   10996 init_comm.go:68] [console] hyper_event_read\nI0721 09:04:00.227073   10996 init_comm.go:68] [console] get length 671\nI0721 09:04:00.227472   10996 init_comm.go:68] [console] read 159 bytes data, total data 671\nI0721 09:04:00.227873   10996 init_comm.go:68] [console] hyper send type 14, len 4\nI0721 09:04:00.227992   10996 init_comm.go:106] read 8/8 [length = 0]\nI0721 09:04:00.228008   10996 init_comm.go:110] data length is 12\nI0721 09:04:00.228019   10996 init_comm.go:96] trying to read 4 bytes\nI0721 09:04:00.228035   10996 init_comm.go:106] read 12/12 [length = 12]\nI0721 09:04:00.228053   10996 init_comm.go:96] trying to read 8 bytes\nI0721 09:04:00.228068   10996 init_comm.go:190] got cmd:14\nI0721 09:04:00.228076   10996 init_comm.go:253] get command NEXT\nI0721 09:04:00.228083   10996 init_comm.go:256] send 159, receive 159\nI0721 09:04:00.244983   10996 init_comm.go:68] [console] 0 0 0 1 0 0 2 9f 7b 22 68 6f 73 74 6e 61 6d 65 22 3a 22 75 62 75 6e 74 75 2d 6c 61 74 65 73 74 2d 33 30 32 36 30 37 33 34 31 31 22 2c 22 63 6f 6e 74 61 69 6e 65 72 73 22 3a 5b 7b 22 69 64 22 3a 22 64 37 36 64 61 35 31 37 61 64 63 64 36 33 35 65 61 65 65 65 65 61 37 62 36 66 63 31 66 62 36 38 39 35 35 66 31 37 30 39 32 39 66 37 35 30 32 31 31 32 34 38 33 63 36 34 35 32 39 33 34 39 32 31 22 2c 22 72 6f 6f 74 66 73 22 3a 22 2f 72 6f 6f 74 66 73 22 2c 22 66 73 74 79 70 65 22 3a 22 65 78 74 34 22 2c 22 69 6d 61 67 65 22 3a 22 73 64 61 22 2c 22 66 73 6d 61 70 22 3a 5b 7b 22 73 6f 75 72 63 65 22 3a 22 79 75 78 68 58 52 45 67 65 4f 22 2c 22 70 61 74 68 22 3a 22 2f 65 74 63 2f 68 6f 73 74 73 22 2c 22 72 65 61 64 4f 6e 6c 79 22 3a 66 61 6c 73 65 2c 22 64 6f 63 6b 65 72 56 6f 6c 75 6d 65 22 3a 66 61 6c 73 65 7d 5d 2c 22 70 72 6f 63 65 73 73 22 3a 7b 22 74 65 72 6d 69 6e 61 6c 22 3a 74 72 75 65 2c 22 73 74 64 69 6f 22 3a 31 2c 22 61 72 67 73 22 3a 5b 22 2f 62 69 6e 2f 62 61 73 68 22 5d 2c 22 65 6e 76 73 22 3a 5b 7b 22 65 6e 76 22 3a 22 50 41 54 48 22 2c 22 76 61 6c 75 65 22 3a 22 2f 75 73 72 2f 6c 6f 63 61 6c 2f 73 62 69 6e 3a 2f 75 73 72 2f 6c 6f 63 61 6c 2f 62 69 6e 3a 2f 75 73 72 2f 73 62 69 6e 3a 2f 75 73 72 2f 62 69 6e 3a 2f 73 62 69 6e 3a 2f 62 69 6e 22 7d 5d 2c 22 77 6f 72 6b 64 69 72 22 3a 22 2f 22 7d 2c 22 72 65 73 74 61 72 74 50 6f 6c 69 63 79 22 3a 22 6e 65 76 65 72 22 2c 22 69 6e 69 74 69 61 6c 69 7a 65 22 3a 74 72 75 65 7d 5d 2c 22 69 6e 74 65 72 66 61 63 65 73 22 3a 5b 7b 22 64 65 76 69 63 65 22 3a 22 65 74 68 30 22 2c 22 69 70 41 64 64 72 65 73 73 22 3a 22 31 39 32 2e 31 36 38 2e 31 32 33 2e 32 22 2c 22 6e 65 74 4d 61 73 6b 22 3a 22 32 35 35 2e 32 35 35 2e 32 35 35 2e 30 22 7d 5d 2c 22 72 6f 75 74 65 73 22 3a 5b 7b 22 64 65 73 74 22 3a 22 30 2e 30 2e 30 2e 30 2f 30 22 2c 22 67 61 74 65 77 61 79 22 3a 22 31 39 32 2e 31 36 38 2e 31 32 33 2e 31 22 2c 22 64 65 76 69 63 65 22 3a 22 65 74 68 30 22 7d 5d 2c 22 73 68 61 72 65 44 69 72 22 3a 22 73 68 61 72 65 5f 64 69 72 22 7d \nI0721 09:04:00.245414   10996 init_comm.go:68] [console]  hyper_channel_handle, type 1, len 671\nI0721 09:04:00.245929   10996 init_comm.go:68] [console] sd 0:0:0:0: [sda] Attached SCSI disk\nI0721 09:04:00.246144   10996 init_comm.go:68] [console] online_cpu()\nI0721 09:04:00.246467   10996 init_comm.go:68] [console] online_memory()\nI0721 09:04:00.246899   10996 init_comm.go:68] [console] try to online memory1\nI0721 09:04:00.247432   10996 init_comm.go:68] [console] online memory1 result: success\nI0721 09:04:00.247854   10996 init_comm.go:68] [console] try to online memory2\nI0721 09:04:00.248392   10996 init_comm.go:68] [console] online memory2 result: success\nI0721 09:04:00.248811   10996 init_comm.go:68] [console] try to online memory3\nI0721 09:04:00.249328   10996 init_comm.go:68] [console] online memory3 result: success\nI0721 09:04:00.258423   10996 init_comm.go:68] [console] call hyper_start_pod, json {\"hostname\":\"ubuntu-latest-3026073411\",\"containers\":[{\"id\":\"d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921\",\"rootfs\":\"/rootfs\",\"fstype\":\"ext4\",\"image\":\"sda\",\"fsmap\":[{\"source\":\"yuxhXREgeO\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"terminal\":true,\"stdio\":1,\"args\":[\"/bin/bash\"],\"envs\":[{\"env\":\"PATH\",\"value\":\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"}],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":true}],\"interfaces\":[{\"device\":\"eth0\",\"ipAddress\":\"192.168.123.2\",\"netMask\":\"255.255.255.0\"}],\"routes\":[{\"dest\":\"0.0.0.0/0\",\"gateway\":\"192.168.123.1\",\"device\":\"eth0\"}],\"shareDir\":\"share_dir\"}, len 663\nI0721 09:04:00.267326   10996 init_comm.go:68] [console] call hyper_start_pod, json {\"hostname\":\"ubuntu-latest-3026073411\",\"containers\":[{\"id\":\"d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921\",\"rootfs\":\"/rootfs\",\"fstype\":\"ext4\",\"image\":\"sda\",\"fsmap\":[{\"source\":\"yuxhXREgeO\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"terminal\":true,\"stdio\":1,\"args\":[\"/bin/bash\"],\"envs\":[{\"env\":\"PATH\",\"value\":\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"}],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":true}],\"interfaces\":[{\"device\":\"eth0\",\"ipAddress\":\"192.168.123.2\",\"netMask\":\"255.255.255.0\"}],\"routes\":[{\"dest\":\"0.0.0.0/0\",\"gateway\":\"192.168.123.1\",\"device\":\"eth0\"}],\"shareDir\":\"share_dir\"}, len 663\nI0721 09:04:00.267815   10996 init_comm.go:68] [console] jsmn parse successed, n is 67\nI0721 09:04:00.268291   10996 init_comm.go:68] [console] token 0, type is 1, size is 5\nI0721 09:04:00.268764   10996 init_comm.go:68] [console] token 1, type is 3, size is 1\nI0721 09:04:00.269147   10996 init_comm.go:68] [console] hostname is ubuntu-latest-3026073411\nI0721 09:04:00.269570   10996 init_comm.go:68] [console] token 3, type is 3, size is 1\nI0721 09:04:00.269874   10996 init_comm.go:68] [console] container count 1\nI0721 09:04:00.270099   10996 init_comm.go:68] [console] next container 8\nI0721 09:04:00.270303   10996 init_comm.go:68] [console] 1 name id\nI0721 09:04:00.270958   10996 init_comm.go:68] [console] container id d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921\nI0721 09:04:00.271198   10996 init_comm.go:68] [console] 3 name rootfs\nI0721 09:04:00.271582   10996 init_comm.go:68] [console] container rootfs /rootfs\nI0721 09:04:00.271847   10996 init_comm.go:68] [console] 5 name fstype\nI0721 09:04:00.272256   10996 init_comm.go:68] [console] container fstype ext4\nI0721 09:04:00.272463   10996 init_comm.go:68] [console] 7 name image\nI0721 09:04:00.272781   10996 init_comm.go:68] [console] container image sda\nI0721 09:04:00.272999   10996 init_comm.go:68] [console] 9 name fsmap\nI0721 09:04:00.273221   10996 init_comm.go:68] [console] fsmap num 1\nI0721 09:04:00.273649   10996 init_comm.go:68] [console] maps 0 source yuxhXREgeO\nI0721 09:04:00.274054   10996 init_comm.go:68] [console] maps 0 path /etc/hosts\nI0721 09:04:00.274397   10996 init_comm.go:68] [console] maps 0 readonly 0\nI0721 09:04:00.274872   10996 init_comm.go:68] [console] in maps incorrect dockerVolume\nI0721 09:04:00.275244   10996 init_comm.go:68] [console] parse pod json failed\nI0721 09:04:00.275582   10996 init_comm.go:68] [console] uptime 1.76 1.40\nI0721 09:04:00.275701   10996 init_comm.go:68] [console] \nI0721 09:04:00.276162   10996 init_comm.go:68] [console] hyper send type 10, len 0\nI0721 09:04:00.276252   10996 init_comm.go:106] read 8/8 [length = 0]\nI0721 09:04:00.276263   10996 init_comm.go:110] data length is 8\nI0721 09:04:00.276273   10996 init_comm.go:96] trying to read 8 bytes\nI0721 09:04:00.276295   10996 init_comm.go:190] got cmd:10\nI0721 09:04:00.276306   10996 init_comm.go:209] ack got, clear pong timer\nI0721 09:04:00.276319   10996 hypervisor.go:29] vm vm-BgDXhMAtjB: main event loop got message 38(ERROR_CMD_FAIL)\nE0721 09:04:00.276329   10996 vm_states.go:278] Shutting down because of an exception: Start POD failed\nI0721 09:04:00.276525   10996 context.go:252] VM vm-BgDXhMAtjB: state change from STARTING to 'TERMINATING'\nE0721 09:04:00.276534   10996 vm_states.go:518] Start POD failed\nI0721 09:04:00.276541   10996 init_comm.go:190] got cmd:4\nI0721 09:04:00.276550   10996 init_comm.go:281] send command 4 to init, payload: 'null'.\nI0721 09:04:00.276563   10996 init_comm.go:294] write 12 to init, payload: '\u0004\n                                                                             null'.\nI0721 09:04:00.276570   10996 init_comm.go:299] message sent, set pong timer\nI0721 09:04:00.276585   10996 vm.go:275] Get the response from VM, VM id is vm-BgDXhMAtjB!\nI0721 09:04:00.276600   10996 pod.go:332] unlock pod pod-YzrFNHTojU for operation start\nI0721 09:04:00.276606   10996 pod.go:335] successfully unlock pod pod-YzrFNHTojU for operation start\nE0721 09:04:00.276612   10996 run.go:77] VM vm-BgDXhMAtjB start failed with code 7: Start POD failed\nE0721 09:04:00.276628   10996 server.go:170] Handler for POST /v1.17/pod/start returned error: VM vm-BgDXhMAtjB start failed with code 7: Start POD failed\n2016/07/21 09:04:00 http: response.WriteHeader on hijacked connection\n2016/07/21 09:04:00 http: response.Write on hijacked connection\nI0721 09:04:00.276869   10996 server.go:152] Calling GET /v0.6.0/list\nI0721 09:04:00.276897   10996 pod_routes.go:52] List type is container, specified pod: [pod-YzrFNHTojU], specified vm: [], list auxiliary pod: false\nI0721 09:04:00.276974   10996 init_comm.go:68] [console] hyper_loop epoll_wait 1\nI0721 09:04:00.277203   10996 server.go:152] Calling GET /v0.6.0/exitcode\nI0721 09:04:00.277220   10996 exec.go:14] Get container id d76da517adcd635eaeeeea7b6fc1fb68955f170929f7502112483c6452934921, exec id \nI0721 09:04:00.277719   10996 init_comm.go:68] [console] hyper_handle_event get event 1, de 0x613578, fd 4. ops 0x613320\nI0721 09:04:00.278257   10996 init_comm.go:68] [console] hyper_handle_event event EPOLLIN, de 0x613578, fd 4, 0x613320\nI0721 09:04:00.278533   10996 init_comm.go:68] [console] hyper_event_read\nI0721 09:04:00.278906   10996 init_comm.go:68] [console] already read 8 bytes data\nI0721 09:04:00.279283   10996 init_comm.go:68] [console] hyper send type 14, len 4\nI0721 09:04:00.279386   10996 init_comm.go:106] read 8/8 [length = 0]\nI0721 09:04:00.279397   10996 init_comm.go:110] data length is 12\nI0721 09:04:00.279405   10996 init_comm.go:96] trying to read 4 bytes\nI0721 09:04:00.279420   10996 init_comm.go:106] read 12/12 [length = 12]\nI0721 09:04:00.279445   10996 init_comm.go:96] trying to read 8 bytes\nI0721 09:04:00.279480   10996 init_comm.go:190] got cmd:14\nI0721 09:04:00.279492   10996 init_comm.go:253] get command NEXT\nI0721 09:04:00.279501   10996 init_comm.go:256] send 12, receive 8\nI0721 09:04:00.279621   10996 init_comm.go:68] [console] get length 12\nI0721 09:04:00.279961   10996 init_comm.go:68] [console] read 4 bytes data, total data 12\nI0721 09:04:00.280347   10996 init_comm.go:68] [console] hyper send type 14, len 4\nI0721 09:04:00.280432   10996 init_comm.go:106] read 8/8 [length = 0]\nI0721 09:04:00.280442   10996 init_comm.go:110] data length is 12\nI0721 09:04:00.280456   10996 init_comm.go:96] trying to read 4 bytes\nI0721 09:04:00.280468   10996 init_comm.go:106] read 12/12 [length = 12]\nI0721 09:04:00.280482   10996 init_comm.go:96] trying to read 8 bytes\nI0721 09:04:00.280497   10996 init_comm.go:190] got cmd:14\nI0721 09:04:00.280509   10996 init_comm.go:253] get command NEXT\nI0721 09:04:00.280519   10996 init_comm.go:256] send 12, receive 12\nI0721 09:04:00.280774   10996 init_comm.go:68] [console] 0 0 0 4 0 0 0 c 6e 75 6c 6c \nI0721 09:04:00.281137   10996 init_comm.go:68] [console]  hyper_channel_handle, type 4, len 12\nI0721 09:04:00.281391   10996 init_comm.go:68] [console] get DESTROYPOD message\nI0721 09:04:00.281670   10996 init_comm.go:68] [console] hyper send type 9, len 0\nI0721 09:04:00.281734   10996 init_comm.go:106] read 8/8 [length = 0]\nI0721 09:04:00.281747   10996 init_comm.go:110] data length is 8\nI0721 09:04:00.281759   10996 init_comm.go:96] trying to read 8 bytes\nI0721 09:04:00.281772   10996 init_comm.go:190] got cmd:9\nI0721 09:04:00.281781   10996 init_comm.go:194] got response of shutdown command, last round of command to init\nI0721 09:04:00.281789   10996 init_comm.go:209] ack got, clear pong timer\nI0721 09:04:00.281801   10996 hypervisor.go:29] vm vm-BgDXhMAtjB: main event loop got message 31(COMMAND_ACK)\nI0721 09:04:00.281808   10996 vm_states.go:636] [Terminating] Got reply to &{4   [] 859542176512}: ''\nI0721 09:04:00.281824   10996 vm_states.go:638] POD destroyed \nI0721 09:04:00.281854   10996 qmp_handler.go:296] got new session\nI0721 09:04:00.281870   10996 qmp_handler.go:225] Begin process command session\nI0721 09:04:00.281885   10996 qmp_handler.go:243] sending command (1) {\"execute\":\"quit\"}\nI0721 09:04:00.282118   10996 qmp_handler.go:103] got a message {\"return\": {}}\nI0721 09:04:00.282158   10996 qmp_handler.go:103] got a message {\"timestamp\": {\"seconds\": 1469063040, \"microseconds\": 282098}, \"event\": \"SHUTDOWN\"}\nI0721 09:04:00.282178   10996 qmp_handler.go:107] got event: SHUTDOWN\nI0721 09:04:00.282187   10996 qmp_handler.go:152] Shutdown, quit QMP receiver\nI0721 09:04:00.282196   10996 qmp_handler.go:323] got QMP event SHUTDOWN\nI0721 09:04:00.282201   10996 qmp_handler.go:325] got QMP shutdown event, quit...\nI0721 09:04:00.282209   10996 hypervisor.go:29] vm vm-BgDXhMAtjB: main event loop got message 1(EVENT_VM_EXIT)\nI0721 09:04:00.282215   10996 vm_states.go:620] Got VM shutdown event while terminating, go to cleaning up\nI0721 09:04:00.282221   10996 vm_states.go:36] VM has exit...\nI0721 09:04:00.282230   10996 devicemap.go:487] remove network card 0: 192.168.123.2\nI0721 09:04:00.282248   10996 context.go:252] VM vm-BgDXhMAtjB: state change from TERMINATING to 'DESTROYING'\nI0721 09:04:00.282290   10996 hypervisor.go:29] vm vm-BgDXhMAtjB: main event loop got message 13(EVENT_INTERFACE_DELETE)\nI0721 09:04:00.282302   10996 devicemap.go:442] interface 0 released\nI0721 09:04:00.282311   10996 vm_states.go:359] Unplug interface return with true\nI0721 09:04:00.282320   10996 context.go:239] no more device to release/remove/umount, quit\nI0721 09:04:00.282343   10996 qemu_process.go:23] quit watch dog.\nE0721 09:04:00.374707   10996 init_comm.go:99] read init data failed\nE0721 09:04:00.374717   10996 tty.go:92] read tty data failed\nI0721 09:04:00.374749   10996 tty.go:153] tty socket closed, quit the reading goroutine EOF\nI0721 09:04:00.374762   10996 tty.go:440] Input byte chan closed, close the output string chan\nI0721 09:04:00.374787   10996 tty.go:120] tty chan closed, quit sent goroutine\nI0721 09:04:00.374794   10996 init_comm.go:70] console output end\n\nI find some erro logs are:\n\nE0721 09:04:00.276612   10996 run.go:77] VM vm-BgDXhMAtjB start failed with code 7: Start POD failed\nE0721 09:04:00.276628   10996 server.go:170] Handler for POST /v1.17/pod/start returned error: VM vm-BgDXhMAtjB start failed with code 7: Start POD failed\n\n. hello @gao-feng ,  I have tried using the newest hyperstart code and I find the result is still the same....\nActually, I tried run runv using kernel and initrd in /var/lib/hyper and it could work.\n\nroot@ddPC:/mycontainer# runv spec\nroot@ddPC:/mycontainer# runv --kernel /var/lib/hyper/kernel --initrd /var/lib/hyper/hyper-initrd.img --driver=qemu start -b $(pwd) test\n/ # ls\nbin   dev   etc   home  proc  root  sys   tmp   usr   var\n\nThe following is logs from hyperd, I still can see parse pod json failed...\n\nI0721 12:17:28.163988    2411 pod.go:1109] configuring log driver [json-file] for pod-oFAoMjYvWA\nI0721 12:17:28.164028    2411 pod.go:1137] configure container log to /var/run/hyper/Pods/pod-oFAoMjYvWA/e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e-json.log\nI0721 12:17:28.164068    2411 pod.go:1143] configured logger for pod-oFAoMjYvWA/e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e (/ubuntu-latest-9281403192)\nI0721 12:17:28.164139    2411 hypervisor.go:29] vm vm-OBmZxSrPLb: main event loop got message 34(GENERIC_OPERATION)\nI0721 12:17:28.164163    2411 vm_states.go:296] handle GenericOperation(Attach) on state(INIT)\nI0721 12:17:28.164210    2411 vm_states.go:229] attachment e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e is pending\nI0721 12:17:28.164256    2411 hypervisor.go:29] vm vm-OBmZxSrPLb: main event loop got message 34(GENERIC_OPERATION)\nI0721 12:17:28.164275    2411 vm_states.go:296] handle GenericOperation(Attach) on state(INIT)\nI0721 12:17:28.164291    2411 vm_states.go:229] attachment e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e is pending\nI0721 12:17:28.164308    2411 pod.go:1195] Attach to container e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e before start pod\nI0721 12:17:28.164349    2411 hypervisor.go:29] vm vm-OBmZxSrPLb: main event loop got message 21(COMMAND_RUN_POD)\nI0721 12:17:28.164363    2411 vm_states.go:443] got spec, prepare devices\nI0721 12:17:28.164380    2411 context.go:284] #0 Container Info:\nI0721 12:17:28.164382    2411 vm.go:162] hyperHandlePodEvent pod pod-oFAoMjYvWA, vm vm-OBmZxSrPLb\nI0721 12:17:28.164523    2411 context.go:287] \n{\n...|    \"Id\": \"e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e\",\n...|    \"User\": \"\",\n...|    \"MountId\": \"b574be3716f3bdafb67b18e6378d79af11ed6802fe202f1bb102d9f1831f028c\",\n...|    \"Rootfs\": \"/rootfs\",\n...|    \"Image\": {\n...|        \"name\": \"\",\n...|        \"source\": \"/dev/mapper/docker-8:1-1704424-b574be3716f3bdafb67b18e6378d79af11ed6802fe202f1bb102d9f1831f028c\",\n...|        \"driver\": \"\",\n...|        \"option\": {\n...|            \"monitors\": null,\n...|            \"user\": \"\",\n...|            \"keyring\": \"\",\n...|            \"bytespersec\": 0,\n...|            \"iops\": 0\n...|        }\n...|    },\n...|    \"Fstype\": \"ext4\",\n...|    \"Workdir\": \"\",\n...|    \"Entrypoint\": null,\n...|    \"Cmd\": [\n...|        \"/bin/bash\"\n...|    ],\n...|    \"Envs\": {\n...|        \"PATH\": \"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n...|    },\n...|    \"Initialize\": true\n...|}\nI0721 12:17:28.164575    2411 devicemap.go:196] insert volume /dev/mapper/docker-8:1-1704424-b574be3716f3bdafb67b18e6378d79af11ed6802fe202f1bb102d9f1831f028c source /dev/mapper/docker-8:1-1704424-b574be3716f3bdafb67b18e6378d79af11ed6802fe202f1bb102d9f1831f028c fstype ext4\nI0721 12:17:28.164600    2411 devicemap.go:280] insert volume etchosts-volume to /etc/hosts on 0\nI0721 12:17:28.164928    2411 vm_states.go:67] initial vm spec: {\n        \"hostname\": \"ubuntu-latest-9281403192\",\n        \"containers\": [\n            {\n                \"id\": \"e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e\",\n                \"rootfs\": \"/rootfs\",\n                \"fstype\": \"ext4\",\n                \"image\": \"\",\n                \"fsmap\": [\n                    {\n                        \"source\": \"zxTYGIDYaR\",\n                        \"path\": \"/etc/hosts\",\n                        \"readOnly\": false,\n                        \"dockerVolume\": false\n                    }\n                ],\n                \"process\": {\n                    \"terminal\": true,\n                    \"stdio\": 1,\n                    \"args\": [\n                        \"/bin/bash\"\n                    ],\n                    \"envs\": [\n                        {\n                            \"env\": \"PATH\",\n                            \"value\": \"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"\n                        }\n                    ],\n                    \"workdir\": \"/\"\n                },\n                \"restartPolicy\": \"never\",\n                \"initialize\": true\n            }\n        ],\n        \"shareDir\": \"share_dir\"\n    }\nI0721 12:17:28.164984    2411 context.go:216] found container e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e at 0\nI0721 12:17:28.164999    2411 vm_states.go:75] attach pending client for e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e\nI0721 12:17:28.165021    2411 vm_states.go:247] Connecting tty for e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e on session 1\nI0721 12:17:28.165035    2411 context.go:216] found container e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e at 0\nI0721 12:17:28.165045    2411 vm_states.go:75] attach pending client for e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e\nI0721 12:17:28.165054    2411 vm_states.go:247] Connecting tty for e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e on session 1\nI0721 12:17:28.165102    2411 context.go:252] VM vm-OBmZxSrPLb: state change from INIT to 'STARTING'\nI0721 12:17:28.165119    2411 qmp_handler.go:296] got new session\nI0721 12:17:28.165144    2411 qmp_handler.go:225] Begin process command session\nI0721 12:17:28.165170    2411 qmp_handler.go:243] sending command (1) {\"execute\":\"human-monitor-command\",\"arguments\":{\"command-line\":\"drive_add dummy file=/dev/mapper/docker-8:1-1704424-b574be3716f3bdafb67b18e6378d79af11ed6802fe202f1bb102d9f1831f028c,if=none,id=drive0,format=raw,cache=writeback\"}}\nI0721 12:17:28.166468    2411 hypervisor.go:29] vm vm-OBmZxSrPLb: main event loop got message 12(EVENT_INTERFACE_ADD)\nI0721 12:17:28.166526    2411 qmp_wrapper.go:90] send net to qemu at 26\nI0721 12:17:28.166565    2411 qmp_handler.go:296] got new session\nI0721 12:17:28.170146    2411 qmp_handler.go:103] got a message {\"return\": \"OK\\r\\n\"}\nI0721 12:17:28.170213    2411 qmp_handler.go:243] sending command (1) {\"execute\":\"device_add\",\"arguments\":{\"bus\":\"scsi0.0\",\"drive\":\"drive0\",\"driver\":\"scsi-hd\",\"id\":\"scsi-disk0\",\"scsi-id\":\"0\"}}\nI0721 12:17:28.172360    2411 qmp_handler.go:103] got a message {\"return\": {}}\nI0721 12:17:28.172454    2411 qmp_handler.go:302] session finished, buffer size 2\nI0721 12:17:28.172477    2411 qmp_handler.go:305] success \nI0721 12:17:28.172522    2411 qmp_handler.go:225] Begin process command session\nI0721 12:17:28.172563    2411 qmp_handler.go:238] send cmd with scm (24 bytes) (1) {\"execute\":\"getfd\",\"arguments\":{\"fdname\":\"fdeth0\"}}\nI0721 12:17:28.172521    2411 hypervisor.go:29] vm vm-OBmZxSrPLb: main event loop got message 9(EVENT_BLOCK_INSERTED)\nI0721 12:17:28.174524    2411 init_comm.go:68] [console] scsi 0:0:0:0: Direct-Access     QEMU     QEMU HARDDISK    2.0. PQ: 0 ANSI: 5\nI0721 12:17:28.175629    2411 qmp_handler.go:103] got a message {\"return\": {}}\nI0721 12:17:28.175875    2411 qmp_handler.go:243] sending command (1) {\"execute\":\"netdev_add\",\"arguments\":{\"fd\":\"fdeth0\",\"id\":\"eth0\",\"type\":\"tap\"}}\nI0721 12:17:28.178832    2411 qmp_handler.go:103] got a message {\"return\": {}}\nI0721 12:17:28.179103    2411 qmp_handler.go:243] sending command (1) {\"execute\":\"device_add\",\"arguments\":{\"addr\":\"0x5\",\"bus\":\"pci.0\",\"driver\":\"virtio-net-pci\",\"id\":\"eth0\",\"mac\":\"52:54:99:c5:77:90\",\"netdev\":\"eth0\"}}\nI0721 12:17:28.179385    2411 init_comm.go:68] [console] sd 0:0:0:0: Attached scsi generic sg0 type 0\nI0721 12:17:28.187040    2411 qmp_handler.go:103] got a message {\"return\": {}}\nI0721 12:17:28.187127    2411 init_comm.go:68] [console] sd 0:0:0:0: [sda] 209715200 512-byte logical blocks: (107 GB/100 GiB)\nI0721 12:17:28.187255    2411 qmp_handler.go:302] session finished, buffer size 1\nI0721 12:17:28.187277    2411 qmp_handler.go:305] success \nI0721 12:17:28.187291    2411 hypervisor.go:29] vm vm-OBmZxSrPLb: main event loop got message 14(EVENT_INTERFACE_INSERTED)\nI0721 12:17:28.187311    2411 vm_states.go:470] device ready, could run pod.\nI0721 12:17:28.187333    2411 init_comm.go:190] got cmd:1\nI0721 12:17:28.187418    2411 init_comm.go:281] send command 1 to init, payload: '{\"hostname\":\"ubuntu-latest-9281403192\",\"containers\":[{\"id\":\"e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e\",\"rootfs\":\"/rootfs\",\"fstype\":\"ext4\",\"image\":\"sda\",\"fsmap\":[{\"source\":\"zxTYGIDYaR\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"terminal\":true,\"stdio\":1,\"args\":[\"/bin/bash\"],\"envs\":[{\"env\":\"PATH\",\"value\":\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"}],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":true}],\"interfaces\":[{\"device\":\"eth0\",\"ipAddress\":\"192.168.123.2\",\"netMask\":\"255.255.255.0\"}],\"routes\":[{\"dest\":\"0.0.0.0/0\",\"gateway\":\"192.168.123.1\",\"device\":\"eth0\"}],\"shareDir\":\"share_dir\"}'.\nI0721 12:17:28.187452    2411 init_comm.go:294] write 512 to init, payload: '\u0001\ufffd{\"hostname\":\"ubuntu-latest-9281403192\",\"containers\":[{\"id\":\"e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e\",\"rootfs\":\"/rootfs\",\"fstype\":\"ext4\",\"image\":\"sda\",\"fsmap\":[{\"source\":\"zxTYGIDYaR\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"terminal\":true,\"stdio\":1,\"args\":[\"/bin/bash\"],\"envs\":[{\"env\":\"PATH\",\"value\":\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"}],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":true}],\"interfaces\":[{\"device\":\"'.\nI0721 12:17:28.187469    2411 init_comm.go:299] message sent, set pong timer\nI0721 12:17:28.188034    2411 init_comm.go:68] [console] hyper_loop epoll_wait 1\nI0721 12:17:28.188899    2411 init_comm.go:68] [console] hyper_handle_event get event 1, de 0x613578, fd 4. ops 0x613320\nI0721 12:17:28.189689    2411 init_comm.go:68] [console] hyper_handle_event event EPOLLIN, de 0x613578, fd 4, 0x613320\nI0721 12:17:28.191888    2411 init_comm.go:68] [console] pci 0000:00:05.0: BAR 6: assigned [mem 0x20040000-0x2007ffff pref]\nI0721 12:17:28.192966    2411 init_comm.go:68] [console] pci 0000:00:05.0: BAR 1: assigned [mem 0x20003000-0x20003fff]\nI0721 12:17:28.193904    2411 init_comm.go:68] [console] pci 0000:00:05.0: BAR 0: assigned [io  0x10c0-0x10df]\nI0721 12:17:28.218878    2411 init_comm.go:68] [console] ACPI: PCI Interrupt Link [LNKA] enabled at IRQ 10\nI0721 12:17:28.219700    2411 init_comm.go:68] [console] virtio-pci 0000:00:05.0: enabling device (0000 -> 0003)\nI0721 12:17:28.221715    2411 init_comm.go:68] [console] virtio-pci 0000:00:05.0: virtio_pci: leaving for legacy driver\nI0721 12:17:28.246144    2411 init_comm.go:68] [console] hyper_event_read\nI0721 12:17:28.246422    2411 init_comm.go:68] [console] already read 8 bytes data\nI0721 12:17:28.246743    2411 init_comm.go:68] [console] hyper send type 14, len 4\nI0721 12:17:28.246857    2411 init_comm.go:106] read 8/8 [length = 0]\nI0721 12:17:28.246881    2411 init_comm.go:110] data length is 12\nI0721 12:17:28.246894    2411 init_comm.go:96] trying to read 4 bytes\nI0721 12:17:28.246910    2411 init_comm.go:106] read 12/12 [length = 12]\nI0721 12:17:28.246929    2411 init_comm.go:96] trying to read 8 bytes\nI0721 12:17:28.246947    2411 init_comm.go:190] got cmd:14\nI0721 12:17:28.246959    2411 init_comm.go:253] get command NEXT\nI0721 12:17:28.246970    2411 init_comm.go:256] send 512, receive 8\nI0721 12:17:28.247047    2411 init_comm.go:68] [console] get length 671\nI0721 12:17:28.247493    2411 init_comm.go:68] [console] read 504 bytes data, total data 512\nI0721 12:17:28.247827    2411 init_comm.go:68] [console] hyper send type 14, len 4\nI0721 12:17:28.247903    2411 init_comm.go:106] read 8/8 [length = 0]\nI0721 12:17:28.247914    2411 init_comm.go:110] data length is 12\nI0721 12:17:28.247927    2411 init_comm.go:96] trying to read 4 bytes\nI0721 12:17:28.247943    2411 init_comm.go:106] read 12/12 [length = 12]\nI0721 12:17:28.247960    2411 init_comm.go:96] trying to read 8 bytes\nI0721 12:17:28.247991    2411 init_comm.go:190] got cmd:14\nI0721 12:17:28.248003    2411 init_comm.go:253] get command NEXT\nI0721 12:17:28.248016    2411 init_comm.go:256] send 512, receive 512\nI0721 12:17:28.248033    2411 init_comm.go:294] write 159 to init, payload: 'eth0\",\"ipAddress\":\"192.168.123.2\",\"netMask\":\"255.255.255.0\"}],\"routes\":[{\"dest\":\"0.0.0.0/0\",\"gateway\":\"192.168.123.1\",\"device\":\"eth0\"}],\"shareDir\":\"share_dir\"}'.\nI0721 12:17:28.248388    2411 init_comm.go:68] [console] hyper_loop epoll_wait 1\nI0721 12:17:28.249043    2411 init_comm.go:68] [console] hyper_handle_event get event 1, de 0x613578, fd 4. ops 0x613320\nI0721 12:17:28.249651    2411 init_comm.go:68] [console] hyper_handle_event event EPOLLIN, de 0x613578, fd 4, 0x613320\nI0721 12:17:28.249894    2411 init_comm.go:68] [console] hyper_event_read\nI0721 12:17:28.250118    2411 init_comm.go:68] [console] get length 671\nI0721 12:17:28.250514    2411 init_comm.go:68] [console] read 159 bytes data, total data 671\nI0721 12:17:28.250823    2411 init_comm.go:68] [console] hyper send type 14, len 4\nI0721 12:17:28.250908    2411 init_comm.go:106] read 8/8 [length = 0]\nI0721 12:17:28.250925    2411 init_comm.go:110] data length is 12\nI0721 12:17:28.250935    2411 init_comm.go:96] trying to read 4 bytes\nI0721 12:17:28.250949    2411 init_comm.go:106] read 12/12 [length = 12]\nI0721 12:17:28.250961    2411 init_comm.go:96] trying to read 8 bytes\nI0721 12:17:28.250974    2411 init_comm.go:190] got cmd:14\nI0721 12:17:28.250982    2411 init_comm.go:253] get command NEXT\nI0721 12:17:28.250989    2411 init_comm.go:256] send 159, receive 159\nI0721 12:17:28.267217    2411 init_comm.go:68] [console] 0 0 0 1 0 0 2 9f 7b 22 68 6f 73 74 6e 61 6d 65 22 3a 22 75 62 75 6e 74 75 2d 6c 61 74 65 73 74 2d 39 32 38 31 34 30 33 31 39 32 22 2c 22 63 6f 6e 74 61 69 6e 65 72 73 22 3a 5b 7b 22 69 64 22 3a 22 65 33 38 36 61 65 31 30 35 35 62 64 36 62 30 32 36 34 63 65 38 65 35 65 37 38 39 36 62 65 36 30 65 38 34 63 39 65 33 36 62 63 33 64 35 37 32 65 65 61 34 33 61 62 64 39 39 36 31 62 33 33 30 65 22 2c 22 72 6f 6f 74 66 73 22 3a 22 2f 72 6f 6f 74 66 73 22 2c 22 66 73 74 79 70 65 22 3a 22 65 78 74 34 22 2c 22 69 6d 61 67 65 22 3a 22 73 64 61 22 2c 22 66 73 6d 61 70 22 3a 5b 7b 22 73 6f 75 72 63 65 22 3a 22 7a 78 54 59 47 49 44 59 61 52 22 2c 22 70 61 74 68 22 3a 22 2f 65 74 63 2f 68 6f 73 74 73 22 2c 22 72 65 61 64 4f 6e 6c 79 22 3a 66 61 6c 73 65 2c 22 64 6f 63 6b 65 72 56 6f 6c 75 6d 65 22 3a 66 61 6c 73 65 7d 5d 2c 22 70 72 6f 63 65 73 73 22 3a 7b 22 74 65 72 6d 69 6e 61 6c 22 3a 74 72 75 65 2c 22 73 74 64 69 6f 22 3a 31 2c 22 61 72 67 73 22 3a 5b 22 2f 62 69 6e 2f 62 61 73 68 22 5d 2c 22 65 6e 76 73 22 3a 5b 7b 22 65 6e 76 22 3a 22 50 41 54 48 22 2c 22 76 61 6c 75 65 22 3a 22 2f 75 73 72 2f 6c 6f 63 61 6c 2f 73 62 69 6e 3a 2f 75 73 72 2f 6c 6f 63 61 6c 2f 62 69 6e 3a 2f 75 73 72 2f 73 62 69 6e 3a 2f 75 73 72 2f 62 69 6e 3a 2f 73 62 69 6e 3a 2f 62 69 6e 22 7d 5d 2c 22 77 6f 72 6b 64 69 72 22 3a 22 2f 22 7d 2c 22 72 65 73 74 61 72 74 50 6f 6c 69 63 79 22 3a 22 6e 65 76 65 72 22 2c 22 69 6e 69 74 69 61 6c 69 7a 65 22 3a 74 72 75 65 7d 5d 2c 22 69 6e 74 65 72 66 61 63 65 73 22 3a 5b 7b 22 64 65 76 69 63 65 22 3a 22 65 74 68 30 22 2c 22 69 70 41 64 64 72 65 73 73 22 3a 22 31 39 32 2e 31 36 38 2e 31 32 33 2e 32 22 2c 22 6e 65 74 4d 61 73 6b 22 3a 22 32 35 35 2e 32 35 35 2e 32 35 35 2e 30 22 7d 5d 2c 22 72 6f 75 74 65 73 22 3a 5b 7b 22 64 65 73 74 22 3a 22 30 2e 30 2e 30 2e 30 2f 30 22 2c 22 67 61 74 65 77 61 79 22 3a 22 31 39 32 2e 31 36 38 2e 31 32 33 2e 31 22 2c 22 64 65 76 69 63 65 22 3a 22 65 74 68 30 22 7d 5d 2c 22 73 68 61 72 65 44 69 72 22 3a 22 73 68 61 72 65 5f 64 69 72 22 7d \nI0721 12:17:28.267734    2411 init_comm.go:68] [console] sd 0:0:0:0: [sda] Write Protect is off\nI0721 12:17:28.268204    2411 init_comm.go:68] [console]  hyper_channel_handle, type 1, len 671\nI0721 12:17:28.269479    2411 init_comm.go:68] [console] sd 0:0:0:0: [sda] Write cache: enabled, read cache: enabled, doesn't support DPO or FUA\nI0721 12:17:28.269711    2411 init_comm.go:68] [console] online_cpu()\nI0721 12:17:28.270076    2411 init_comm.go:68] [console] online_memory()\nI0721 12:17:28.270432    2411 init_comm.go:68] [console] try to online memory1\nI0721 12:17:28.270813    2411 init_comm.go:68] [console] online memory1 result: success\nI0721 12:17:28.271147    2411 init_comm.go:68] [console] try to online memory2\nI0721 12:17:28.271536    2411 init_comm.go:68] [console] online memory2 result: success\nI0721 12:17:28.271856    2411 init_comm.go:68] [console] try to online memory3\nI0721 12:17:28.272275    2411 init_comm.go:68] [console] online memory3 result: success\nI0721 12:17:28.277690    2411 init_comm.go:68] [console] call hyper_start_pod, json {\"hostname\":\"ubuntu-latest-9281403192\",\"containers\":[{\"id\":\"e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e\",\"rootfs\":\"/rootfs\",\"fstype\":\"ext4\",\"image\":\"sda\",\"fsmap\":[{\"source\":\"zxTYGIDYaR\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"terminal\":true,\"stdio\":1,\"args\":[\"/bin/bash\"],\"envs\":[{\"env\":\"PATH\",\"value\":\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"}],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":true}],\"interfaces\":[{\"device\":\"eth0\",\"ipAddress\":\"192.168.123.2\",\"netMask\":\"255.255.255.0\"}],\"routes\":[{\"dest\":\"0.0.0.0/0\",\"gateway\":\"192.168.123.1\",\"device\":\"eth0\"}],\"shareDir\":\"share_dir\"}, len 663\nI0721 12:17:28.283048    2411 init_comm.go:68] [console] call hyper_start_pod, json {\"hostname\":\"ubuntu-latest-9281403192\",\"containers\":[{\"id\":\"e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e\",\"rootfs\":\"/rootfs\",\"fstype\":\"ext4\",\"image\":\"sda\",\"fsmap\":[{\"source\":\"zxTYGIDYaR\",\"path\":\"/etc/hosts\",\"readOnly\":false,\"dockerVolume\":false}],\"process\":{\"terminal\":true,\"stdio\":1,\"args\":[\"/bin/bash\"],\"envs\":[{\"env\":\"PATH\",\"value\":\"/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"}],\"workdir\":\"/\"},\"restartPolicy\":\"never\",\"initialize\":true}],\"interfaces\":[{\"device\":\"eth0\",\"ipAddress\":\"192.168.123.2\",\"netMask\":\"255.255.255.0\"}],\"routes\":[{\"dest\":\"0.0.0.0/0\",\"gateway\":\"192.168.123.1\",\"device\":\"eth0\"}],\"shareDir\":\"share_dir\"}, len 663\nI0721 12:17:28.283430    2411 init_comm.go:68] [console] jsmn parse successed, n is 67\nI0721 12:17:28.283878    2411 init_comm.go:68] [console] token 0, type is 1, size is 5\nI0721 12:17:28.284340    2411 init_comm.go:68] [console] token 1, type is 3, size is 1\nI0721 12:17:28.284747    2411 init_comm.go:68] [console] hostname is ubuntu-latest-9281403192\nI0721 12:17:28.285107    2411 init_comm.go:68] [console] token 3, type is 3, size is 1\nI0721 12:17:28.285368    2411 init_comm.go:68] [console] container count 1\nI0721 12:17:28.285619    2411 init_comm.go:68] [console] next container 8\nI0721 12:17:28.285821    2411 init_comm.go:68] [console] 1 name id\nI0721 12:17:28.286492    2411 init_comm.go:68] [console] container id e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e\nI0721 12:17:28.286731    2411 init_comm.go:68] [console] 3 name rootfs\nI0721 12:17:28.287039    2411 init_comm.go:68] [console] container rootfs /rootfs\nI0721 12:17:28.287269    2411 init_comm.go:68] [console] 5 name fstype\nI0721 12:17:28.287696    2411 init_comm.go:68] [console] container fstype ext4\nI0721 12:17:28.288200    2411 init_comm.go:68] [console] sd 0:0:0:0: [sda] Attached SCSI disk\nI0721 12:17:28.288401    2411 init_comm.go:68] [console] 7 name image\nI0721 12:17:28.288633    2411 init_comm.go:68] [console] container image sda\nI0721 12:17:28.288808    2411 init_comm.go:68] [console] 9 name fsmap\nI0721 12:17:28.288977    2411 init_comm.go:68] [console] fsmap num 1\nI0721 12:17:28.289258    2411 init_comm.go:68] [console] maps 0 source zxTYGIDYaR\nI0721 12:17:28.289509    2411 init_comm.go:68] [console] maps 0 path /etc/hosts\nI0721 12:17:28.289757    2411 init_comm.go:68] [console] maps 0 readonly 0\nI0721 12:17:28.290050    2411 init_comm.go:68] [console] in maps incorrect dockerVolume\nI0721 12:17:28.290312    2411 init_comm.go:68] [console] parse pod json failed\nI0721 12:17:28.290532    2411 init_comm.go:68] [console] uptime 1.74 1.39\nI0721 12:17:28.290622    2411 init_comm.go:68] [console] \nI0721 12:17:28.290902    2411 init_comm.go:68] [console] hyper send type 10, len 0\nI0721 12:17:28.290987    2411 init_comm.go:106] read 8/8 [length = 0]\nI0721 12:17:28.291008    2411 init_comm.go:110] data length is 8\nI0721 12:17:28.291019    2411 init_comm.go:96] trying to read 8 bytes\nI0721 12:17:28.291045    2411 init_comm.go:190] got cmd:10\nI0721 12:17:28.291058    2411 init_comm.go:209] ack got, clear pong timer\nI0721 12:17:28.291074    2411 hypervisor.go:29] vm vm-OBmZxSrPLb: main event loop got message 38(ERROR_CMD_FAIL)\nE0721 12:17:28.291085    2411 vm_states.go:278] Shutting down because of an exception: Start POD failed\nI0721 12:17:28.291311    2411 context.go:252] VM vm-OBmZxSrPLb: state change from STARTING to 'TERMINATING'\nE0721 12:17:28.291322    2411 vm_states.go:518] Start POD failed\nI0721 12:17:28.291330    2411 init_comm.go:190] got cmd:4\nI0721 12:17:28.291343    2411 init_comm.go:281] send command 4 to init, payload: 'null'.\nI0721 12:17:28.291354    2411 vm.go:275] Get the response from VM, VM id is vm-OBmZxSrPLb!\nI0721 12:17:28.291360    2411 init_comm.go:294] write 12 to init, payload: '\u0004\n                                                                             null'.\nI0721 12:17:28.291369    2411 init_comm.go:299] message sent, set pong timer\nI0721 12:17:28.291377    2411 pod.go:332] unlock pod pod-oFAoMjYvWA for operation start\nI0721 12:17:28.291383    2411 pod.go:335] successfully unlock pod pod-oFAoMjYvWA for operation start\nE0721 12:17:28.291390    2411 run.go:77] VM vm-OBmZxSrPLb start failed with code 7: Start POD failed\nE0721 12:17:28.291412    2411 server.go:170] Handler for POST /v1.17/pod/start returned error: VM vm-OBmZxSrPLb start failed with code 7: Start POD failed\n2016/07/21 12:17:28 http: response.WriteHeader on hijacked connection\n2016/07/21 12:17:28 http: response.Write on hijacked connection\nI0721 12:17:28.291632    2411 server.go:152] Calling GET /v0.6.0/list\nI0721 12:17:28.291660    2411 pod_routes.go:52] List type is container, specified pod: [pod-oFAoMjYvWA], specified vm: [], list auxiliary pod: false\nI0721 12:17:28.291767    2411 init_comm.go:68] [console] hyper_loop epoll_wait 1\nI0721 12:17:28.291961    2411 server.go:152] Calling GET /v0.6.0/exitcode\nI0721 12:17:28.291981    2411 exec.go:14] Get container id e386ae1055bd6b0264ce8e5e7896be60e84c9e36bc3d572eea43abd9961b330e, exec id \nI0721 12:17:28.292269    2411 init_comm.go:68] [console] hyper_handle_event get event 1, de 0x613578, fd 4. ops 0x613320\nI0721 12:17:28.293070    2411 init_comm.go:68] [console] hyper_handle_event event EPOLLIN, de 0x613578, fd 4, 0x613320\nI0721 12:17:28.293287    2411 init_comm.go:68] [console] hyper_event_read\nI0721 12:17:28.293566    2411 init_comm.go:68] [console] already read 8 bytes data\nI0721 12:17:28.293851    2411 init_comm.go:68] [console] hyper send type 14, len 4\nI0721 12:17:28.293945    2411 init_comm.go:106] read 8/8 [length = 0]\nI0721 12:17:28.293954    2411 init_comm.go:110] data length is 12\nI0721 12:17:28.293959    2411 init_comm.go:96] trying to read 4 bytes\nI0721 12:17:28.293967    2411 init_comm.go:106] read 12/12 [length = 12]\nI0721 12:17:28.293977    2411 init_comm.go:96] trying to read 8 bytes\nI0721 12:17:28.293996    2411 init_comm.go:190] got cmd:14\nI0721 12:17:28.294005    2411 init_comm.go:253] get command NEXT\nI0721 12:17:28.294014    2411 init_comm.go:256] send 12, receive 8\nI0721 12:17:28.294140    2411 init_comm.go:68] [console] get length 12\nI0721 12:17:28.294471    2411 init_comm.go:68] [console] read 4 bytes data, total data 12\nI0721 12:17:28.294748    2411 init_comm.go:68] [console] hyper send type 14, len 4\nI0721 12:17:28.294821    2411 init_comm.go:106] read 8/8 [length = 0]\nI0721 12:17:28.294840    2411 init_comm.go:110] data length is 12\nI0721 12:17:28.294848    2411 init_comm.go:96] trying to read 4 bytes\nI0721 12:17:28.294869    2411 init_comm.go:106] read 12/12 [length = 12]\nI0721 12:17:28.294880    2411 init_comm.go:96] trying to read 8 bytes\nI0721 12:17:28.294892    2411 init_comm.go:190] got cmd:14\nI0721 12:17:28.294900    2411 init_comm.go:253] get command NEXT\nI0721 12:17:28.294908    2411 init_comm.go:256] send 12, receive 12\nI0721 12:17:28.295124    2411 init_comm.go:68] [console] 0 0 0 4 0 0 0 c 6e 75 6c 6c \nI0721 12:17:28.295508    2411 init_comm.go:68] [console]  hyper_channel_handle, type 4, len 12\nI0721 12:17:28.295757    2411 init_comm.go:68] [console] get DESTROYPOD message\nI0721 12:17:28.296029    2411 init_comm.go:68] [console] hyper send type 9, len 0\nI0721 12:17:28.296154    2411 init_comm.go:106] read 8/8 [length = 0]\nI0721 12:17:28.296168    2411 init_comm.go:110] data length is 8\nI0721 12:17:28.296180    2411 init_comm.go:96] trying to read 8 bytes\nI0721 12:17:28.296191    2411 init_comm.go:190] got cmd:9\nI0721 12:17:28.296201    2411 init_comm.go:194] got response of shutdown command, last round of command to init\nI0721 12:17:28.296214    2411 init_comm.go:209] ack got, clear pong timer\nI0721 12:17:28.296226    2411 hypervisor.go:29] vm vm-OBmZxSrPLb: main event loop got message 31(COMMAND_ACK)\nI0721 12:17:28.296236    2411 vm_states.go:636] [Terminating] Got reply to &{4   [] 859550868128}: ''\nI0721 12:17:28.296254    2411 vm_states.go:638] POD destroyed \nI0721 12:17:28.296264    2411 qmp_handler.go:296] got new session\nI0721 12:17:28.296278    2411 qmp_handler.go:225] Begin process command session\nI0721 12:17:28.296290    2411 qmp_handler.go:243] sending command (1) {\"execute\":\"quit\"}\nI0721 12:17:28.296613    2411 qmp_handler.go:103] got a message {\"return\": {}}\nI0721 12:17:28.296673    2411 qmp_handler.go:103] got a message {\"timestamp\": {\"seconds\": 1469074648, \"microseconds\": 296592}, \"event\": \"SHUTDOWN\"}\nI0721 12:17:28.296699    2411 qmp_handler.go:107] got event: SHUTDOWN\nI0721 12:17:28.296713    2411 qmp_handler.go:152] Shutdown, quit QMP receiver\nI0721 12:17:28.296727    2411 qmp_handler.go:323] got QMP event SHUTDOWN\nI0721 12:17:28.296784    2411 qmp_handler.go:325] got QMP shutdown event, quit...\nI0721 12:17:28.296799    2411 hypervisor.go:29] vm vm-OBmZxSrPLb: main event loop got message 1(EVENT_VM_EXIT)\nI0721 12:17:28.296809    2411 vm_states.go:620] Got VM shutdown event while terminating, go to cleaning up\nI0721 12:17:28.296818    2411 vm_states.go:36] VM has exit...\nI0721 12:17:28.296833    2411 devicemap.go:487] remove network card 0: 192.168.123.2\nI0721 12:17:28.296846    2411 context.go:252] VM vm-OBmZxSrPLb: state change from TERMINATING to 'DESTROYING'\nI0721 12:17:28.296884    2411 hypervisor.go:29] vm vm-OBmZxSrPLb: main event loop got message 13(EVENT_INTERFACE_DELETE)\nI0721 12:17:28.296893    2411 devicemap.go:442] interface 0 released\nI0721 12:17:28.296900    2411 vm_states.go:359] Unplug interface return with true\nI0721 12:17:28.296906    2411 context.go:239] no more device to release/remove/umount, quit\nI0721 12:17:28.296921    2411 qemu_process.go:23] quit watch dog.\nE0721 12:17:28.383330    2411 init_comm.go:99] read init data failed\nE0721 12:17:28.383330    2411 tty.go:92] read tty data failed\nI0721 12:17:28.383371    2411 tty.go:440] Input byte chan closed, close the output string chan\nI0721 12:17:28.383385    2411 tty.go:153] tty socket closed, quit the reading goroutine EOF\nI0721 12:17:28.383408    2411 init_comm.go:70] console output end\nI0721 12:17:28.383430    2411 tty.go:120] tty chan closed, quit sent goroutine\n\n. @gao-feng  No\u2026\u2026 But I replaced the original images with the new built one by mv build/hyper-initrd.img /var/lib/hyper/ and mv build/kernel /var/lib/hyper/\n. HI @gao-feng  I also tried changing the contents in the /etc/hyper/config like this:\n\nKernel=/home/ddnirvana/devlop/gopath/src/github.com/hyperhq/hyperstart/build/kernel\nInitrd=/home/ddnirvana/devlop/gopath/src/github.com/hyperhq/hyperstart/build/hyper-initrd.img\n\nBut it's still the same result  :(\n. Brilliant! It works fine! @gao-feng  Thank you so so much!~  But it seems it's a bug in hyperd to use outdated default config file \uff1a)\n. hi ,@gnawux. Thanks for your quickly response! But  release                : 4.4.0-98-generic actually is the version of the domain0 kernel, and the xen version I used isxen_version            : 4.9.0(You can also see this in the previous result of xl info)..\n. hi, @gnawux   Please notify me when you finish updating the code :) Thanks very much!. ",
    "mchiappero": "Great, thank you!\n. ",
    "hzmangel": "ref #319\n. @gnawux Just checked docker code, they only modify the output string in client side. I have made similar changes, which reverted the server / daemon side change and only modified print part of CLI side.\nPlease help to review. Thanks.. @gnawux Sure thing, I have just pushed the new squashed commit.. Sorry, forgot to remove this, updating now.. Sorry, found this after push.... ",
    "sameo": "@gnawux I got it from the hyperd README.md.\nAre you going to update the static builds ?\n. @gnawux I was trying to run hyperd on Clear Linux, and I got it figured out. So please don't bother generating static files, at least not for me :)\n. ",
    "ZYNCMA": "Maybe something like rsplitN is better\n. Close this issue as it has been fixed in v0.7.0\n. ",
    "vitan": "I am using curl -sSL http://hypercontainer.io/install | bash to bypass it\n. I found the following log:\nI1020 23:27:56.489530   31322 init_comm.go:68] [console] maps 0 readonly 0\nI1020 23:27:56.489724   31322 init_comm.go:68] [console] in maps incorrect dockerVolume\nI1020 23:27:56.490459   31322 init_comm.go:68] [console] parse pod json failed\nI1020 23:27:56.492315   31322 init_comm.go:68] [console] uptime 5.50 0.00\nif we don't detail the volume map , the error will be triggered and the pod is pending always.\n. @gnawux  @laijs  have a look please. I am not sure why My PR triggering other unittest failed. wip: trigger the unittest for debug. ",
    "wahmedswl": "Thanks. ",
    "GrantLajs": "Is there anything wrong with my qemu?\nHere's the qemu build script:\n$cat build.sh\n```\n!/bin/bash\nif [ $(uname -m) = i686 ]; then\n   QEMU_ARCH=i386-softmmu\nelse\n   QEMU_ARCH=x86_64-softmmu\nfi\nmkdir -vp build &&\ncd        build &&\n../configure --prefix=/usr               \\\n             --sysconfdir=/etc           \\\n             --target-list=$QEMU_ARCH    \\\n             --audio-drv-list=alsa       \\\n             --enable-virtfs             \\\n             --docdir=/usr/share/doc/qemu-2.7.0 &&\nunset QEMU_ARCH &&\nmake\n```\n. /etc/hyper/config:\n```\nconfigurations for hyperd\nRoot directory for hyperd\nRoot=/var/lib/hyper/\nSpecify the hypervisor: libvirt, qemu, qemu-kvm, kvm, xen, vbox (for linux)\nvbox (for mac).\n\"kvm\"  is equivalent to \"qemu-kvm\" which uses qemu with kvm acceleration.\n\"qemu\" is equivalent to \"qemu-kvm\" when the system enables kvm, otherwise\nthe hypervisor is \"qemu-tcg\" (qemu without kvm acceleration).\nWhen Hypervisor is not set, the hyperd will try to probe \"qemu-kvm\" or \"xen\"\nas the containers' hypervisor according to the host, if the host doesn't\nsupport any hardware-assisted technology, it will use \"qemu-tcg\".\n\nHypervisor=qemu\nBoot kernel\nKernel=/var/lib/hyper/kernel\nBoot initrd\nInitrd=/var/lib/hyper/hyper-initrd.img\nBIOS image, qboot bios will accelarate the bootup\nBios=/var/lib/hyper/bios-qboot.bin\nCBFS coreboot fs for boot image, if it is set, Kernel and Initrd will be ignored\nCbfs=/var/lib/hyper/cbfs-qboot.rom\nBoot CDROOM for \"vbox\" hypervisor (for mac only)\nVbox=/opt/hyper/static/iso/hyper-vbox-boot.iso\nStorage driver for hyperd, valid value includes devicemapper, overlay, and aufs\nStorageDriver=overlay\nBridge device for hyperd, default is hyper0\nBridge=\nBridge ip address for the bridge device\nBridgeIP=\nIf the host IP is provided, a TCP port will be listened for, same as the '--host' option\nHost=\nThis is only useful for hypernetes, to disable the iptables setup by hyperd\nDisableIptables=false\n```\n. Any help is appreciated. Thanks!\n. @gao-feng \nAfter comment the Bios and Cbfs config, and rerun the hyper run command. I got the following logs:\n```\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.457+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:49.457+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:49.457+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.457+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.457+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.458+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:49.458+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:49.458+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.458+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.458+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.458+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:49.458+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'add'\n2016-11-14 04:50:49.458+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device '253:2' does not have property 'DRIVER'\n2016-11-14 04:50:49.458+0000: 1268: debug : udevGetDeviceProperty:139 : Found property key 'SUBSYSTEM' value 'bdi' for device with sysname '253:2'\n2016-11-14 04:50:49.459+0000: 1268: debug : udevGetDeviceType:1245 : Could not determine device type for device with sysfs name '253:2'\n2016-11-14 04:50:49.459+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d20c00 /sys/devices/virtual/bdi/253:2\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.459+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.459+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:49.459+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'add'\n2016-11-14 04:50:49.459+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'DRIVER'\n2016-11-14 04:50:49.459+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_BUS'\n2016-11-14 04:50:49.459+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_SERIAL'\n2016-11-14 04:50:49.459+0000: 1268: debug : udevGetDeviceSysfsAttr:208 : udev reports device 'dm-2' does not have sysfs attr 'device/vendor'\n2016-11-14 04:50:49.459+0000: 1268: debug : udevGetDeviceSysfsAttr:208 : udev reports device 'dm-2' does not have sysfs attr 'device/model'\n2016-11-14 04:50:49.459+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_TYPE'\n2016-11-14 04:50:49.459+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_DRIVE_FLOPPY'\n2016-11-14 04:50:49.459+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_CDROM'\n2016-11-14 04:50:49.459+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_DRIVE_FLASH_SD'\n2016-11-14 04:50:49.459+0000: 1268: debug : udevKludgeStorageType:1037 : Could not find definitive storage type for device with sysfs path '/sys/devices/virtual/block/dm-2', trying to guess it\n2016-11-14 04:50:49.459+0000: 1268: debug : udevKludgeStorageType:1046 : Could not determine storage type for device with sysfs path '/sys/devices/virtual/block/dm-2'\n2016-11-14 04:50:49.459+0000: 1268: debug : udevProcessStorage:1167 : Storage ret=-1\n2016-11-14 04:50:49.459+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d1e6a0 /sys/devices/virtual/block/dm-2\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.459+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.459+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.468+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:49.468+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.468+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.468+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.468+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.468+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.468+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.468+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.468+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.468+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:49.468+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:49.468+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:49.468+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.468+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.468+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.468+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.468+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.469+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.469+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.469+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.469+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.469+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.469+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.469+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.469+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.469+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.469+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.469+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.469+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.469+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.469+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.469+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.469+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.555+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:49.555+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:49.555+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.555+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.555+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.624+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:49.624+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'change'\n2016-11-14 04:50:49.624+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'DRIVER'\n2016-11-14 04:50:49.624+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_BUS'\n2016-11-14 04:50:49.624+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_SERIAL'\n2016-11-14 04:50:49.624+0000: 1268: debug : udevGetDeviceSysfsAttr:208 : udev reports device 'dm-2' does not have sysfs attr 'device/vendor'\n2016-11-14 04:50:49.624+0000: 1268: debug : udevGetDeviceSysfsAttr:208 : udev reports device 'dm-2' does not have sysfs attr 'device/model'\n2016-11-14 04:50:49.624+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_TYPE'\n2016-11-14 04:50:49.624+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_DRIVE_FLOPPY'\n2016-11-14 04:50:49.624+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_CDROM'\n2016-11-14 04:50:49.624+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_DRIVE_FLASH_SD'\n2016-11-14 04:50:49.624+0000: 1268: debug : udevKludgeStorageType:1037 : Could not find definitive storage type for device with sysfs path '/sys/devices/virtual/block/dm-2', trying to guess it\n2016-11-14 04:50:49.624+0000: 1268: debug : udevKludgeStorageType:1046 : Could not determine storage type for device with sysfs path '/sys/devices/virtual/block/dm-2'\n2016-11-14 04:50:49.624+0000: 1268: debug : udevProcessStorage:1167 : Storage ret=-1\n2016-11-14 04:50:49.624+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d20c00 /sys/devices/virtual/block/dm-2\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.624+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.624+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.666+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:49.666+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'change'\n2016-11-14 04:50:49.666+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'kvm' does not have property 'DRIVER'\n2016-11-14 04:50:49.666+0000: 1268: debug : udevGetDeviceProperty:139 : Found property key 'SUBSYSTEM' value 'misc' for device with sysname 'kvm'\n2016-11-14 04:50:49.666+0000: 1268: debug : udevGetDeviceType:1245 : Could not determine device type for device with sysfs name 'kvm'\n2016-11-14 04:50:49.666+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d21a10 /sys/devices/virtual/misc/kvm\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.666+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.667+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.667+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:49.667+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:49.667+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.667+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 2 event(s)\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.667+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:49.667+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:49.667+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.667+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:49.667+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'add'\n2016-11-14 04:50:49.667+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'xfs' does not have property 'DRIVER'\n2016-11-14 04:50:49.667+0000: 1268: debug : udevGetDeviceProperty:139 : Found property key 'SUBSYSTEM' value 'module' for device with sysname 'xfs'\n2016-11-14 04:50:49.667+0000: 1268: debug : udevGetDeviceType:1245 : Could not determine device type for device with sysfs name 'xfs'\n2016-11-14 04:50:49.667+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d1ad40 /sys/module/xfs\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.667+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.668+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 2 event(s)\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.668+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:49.668+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:49.668+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.668+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:49.668+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'add'\n2016-11-14 04:50:49.668+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device ':t-0000184' does not have property 'DRIVER'\n2016-11-14 04:50:49.668+0000: 1268: debug : udevGetDeviceProperty:139 : Found property key 'SUBSYSTEM' value 'slab' for device with sysname ':t-0000184'\n2016-11-14 04:50:49.668+0000: 1268: debug : udevGetDeviceType:1245 : Could not determine device type for device with sysfs name ':t-0000184'\n2016-11-14 04:50:49.668+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d1a800 /sys/kernel/slab/:t-0000184\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.668+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 2 event(s)\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.668+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:49.668+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:49.668+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.668+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:49.668+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'add'\n2016-11-14 04:50:49.668+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device ':t-0000480' does not have property 'DRIVER'\n2016-11-14 04:50:49.668+0000: 1268: debug : udevGetDeviceProperty:139 : Found property key 'SUBSYSTEM' value 'slab' for device with sysname ':t-0000480'\n2016-11-14 04:50:49.668+0000: 1268: debug : udevGetDeviceType:1245 : Could not determine device type for device with sysfs name ':t-0000480'\n2016-11-14 04:50:49.668+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d1a360 /sys/kernel/slab/:t-0000480\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.668+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 2 event(s)\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.668+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:49.668+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:49.668+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.668+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:49.668+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'add'\n2016-11-14 04:50:49.668+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device ':t-0000400' does not have property 'DRIVER'\n2016-11-14 04:50:49.668+0000: 1268: debug : udevGetDeviceProperty:139 : Found property key 'SUBSYSTEM' value 'slab' for device with sysname ':t-0000400'\n2016-11-14 04:50:49.668+0000: 1268: debug : udevGetDeviceType:1245 : Could not determine device type for device with sysfs name ':t-0000400'\n2016-11-14 04:50:49.668+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d1ad40 /sys/kernel/slab/:t-0000400\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.668+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.669+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 2 event(s)\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.669+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:49.669+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:49.669+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.669+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:49.669+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'add'\n2016-11-14 04:50:49.669+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device ':t-0000208' does not have property 'DRIVER'\n2016-11-14 04:50:49.669+0000: 1268: debug : udevGetDeviceProperty:139 : Found property key 'SUBSYSTEM' value 'slab' for device with sysname ':t-0000208'\n2016-11-14 04:50:49.669+0000: 1268: debug : udevGetDeviceType:1245 : Could not determine device type for device with sysfs name ':t-0000208'\n2016-11-14 04:50:49.669+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d1a800 /sys/kernel/slab/:t-0000208\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.669+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 2 event(s)\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.669+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:49.669+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:49.669+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.669+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:49.669+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'add'\n2016-11-14 04:50:49.669+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device ':t-0000152' does not have property 'DRIVER'\n2016-11-14 04:50:49.669+0000: 1268: debug : udevGetDeviceProperty:139 : Found property key 'SUBSYSTEM' value 'slab' for device with sysname ':t-0000152'\n2016-11-14 04:50:49.669+0000: 1268: debug : udevGetDeviceType:1245 : Could not determine device type for device with sysfs name ':t-0000152'\n2016-11-14 04:50:49.669+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d1acd0 /sys/kernel/slab/:t-0000152\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.669+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 2 event(s)\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.669+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:49.669+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:49.669+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.669+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:49.669+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'add'\n2016-11-14 04:50:49.669+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device ':t-0000144' does not have property 'DRIVER'\n2016-11-14 04:50:49.669+0000: 1268: debug : udevGetDeviceProperty:139 : Found property key 'SUBSYSTEM' value 'slab' for device with sysname ':t-0000144'\n2016-11-14 04:50:49.669+0000: 1268: debug : udevGetDeviceType:1245 : Could not determine device type for device with sysfs name ':t-0000144'\n2016-11-14 04:50:49.669+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d1ad40 /sys/kernel/slab/:t-0000144\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.669+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 2 event(s)\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.669+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:49.669+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:49.669+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.669+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:49.669+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'add'\n2016-11-14 04:50:49.669+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'xfs_inode' does not have property 'DRIVER'\n2016-11-14 04:50:49.669+0000: 1268: debug : udevGetDeviceProperty:139 : Found property key 'SUBSYSTEM' value 'slab' for device with sysname 'xfs_inode'\n2016-11-14 04:50:49.669+0000: 1268: debug : udevGetDeviceType:1245 : Could not determine device type for device with sysfs name 'xfs_inode'\n2016-11-14 04:50:49.669+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d1a800 /sys/kernel/slab/xfs_inode\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.669+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 2 event(s)\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.669+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.670+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:49.670+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:49.670+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.670+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:49.670+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'add'\n2016-11-14 04:50:49.670+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device ':t-0000528' does not have property 'DRIVER'\n2016-11-14 04:50:49.670+0000: 1268: debug : udevGetDeviceProperty:139 : Found property key 'SUBSYSTEM' value 'slab' for device with sysname ':t-0000528'\n2016-11-14 04:50:49.670+0000: 1268: debug : udevGetDeviceType:1245 : Could not determine device type for device with sysfs name ':t-0000528'\n2016-11-14 04:50:49.670+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d1a360 /sys/kernel/slab/:t-0000528\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.670+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.670+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:49.670+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'add'\n2016-11-14 04:50:49.670+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device ':t-0000472' does not have property 'DRIVER'\n2016-11-14 04:50:49.670+0000: 1268: debug : udevGetDeviceProperty:139 : Found property key 'SUBSYSTEM' value 'slab' for device with sysname ':t-0000472'\n2016-11-14 04:50:49.670+0000: 1268: debug : udevGetDeviceType:1245 : Could not determine device type for device with sysfs name ':t-0000472'\n2016-11-14 04:50:49.670+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d21820 /sys/kernel/slab/:t-0000472\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.670+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.670+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.889+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:49.889+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:49.889+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.889+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.889+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:49.891+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:49.891+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:49.891+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:49.891+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:49.891+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:49.891+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:49.891+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:49.891+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:49.891+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:49.891+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:49.891+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:49.891+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:49.891+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'change'\n2016-11-14 04:50:49.891+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'kvm' does not have property 'DRIVER'\n2016-11-14 04:50:49.891+0000: 1268: debug : udevGetDeviceProperty:139 : Found property key 'SUBSYSTEM' value 'misc' for device with sysname 'kvm'\n2016-11-14 04:50:49.891+0000: 1268: debug : udevGetDeviceType:1245 : Could not determine device type for device with sysfs name 'kvm'\n2016-11-14 04:50:49.892+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d21920 /sys/devices/virtual/misc/kvm\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:49.892+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:49.892+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.360+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:50.360+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:50.360+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.360+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:50.361+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:50.361+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:50.361+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.365+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:50.365+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'remove'\n2016-11-14 04:50:50.365+0000: 1268: debug : udevRemoveOneDevice:1314 : Failed to find device to remove that has udev name '/sys/devices/virtual/block/dm-2'\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:50.365+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:50.365+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.368+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:50.368+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:50.368+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:50.368+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 2 event(s)\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.368+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:50.368+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:50.368+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.368+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:50.368+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'remove'\n2016-11-14 04:50:50.368+0000: 1268: debug : udevRemoveOneDevice:1314 : Failed to find device to remove that has udev name '/sys/devices/virtual/bdi/253:2'\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:50.368+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:50.368+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.369+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:50.369+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.369+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.369+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.369+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.369+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.369+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.369+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.369+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.369+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.369+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.369+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:50.369+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'remove'\n2016-11-14 04:50:50.369+0000: 1268: debug : udevRemoveOneDevice:1314 : Failed to find device to remove that has udev name '/sys/devices/virtual/block/dm-2'\n2016-11-14 04:50:50.369+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:50.370+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:50.370+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.570+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:50.570+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.570+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.570+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.570+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.570+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.570+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.570+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.570+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.570+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:50.570+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:50.570+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:50.571+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.571+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:50.571+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:50.571+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:50.571+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.571+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:50.571+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'add'\n2016-11-14 04:50:50.571+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device '253:2' does not have property 'DRIVER'\n2016-11-14 04:50:50.571+0000: 1268: debug : udevGetDeviceProperty:139 : Found property key 'SUBSYSTEM' value 'bdi' for device with sysname '253:2'\n2016-11-14 04:50:50.571+0000: 1268: debug : udevGetDeviceType:1245 : Could not determine device type for device with sysfs name '253:2'\n2016-11-14 04:50:50.571+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d20c00 /sys/devices/virtual/bdi/253:2\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:50.571+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.571+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:50.571+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'add'\n2016-11-14 04:50:50.571+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'DRIVER'\n2016-11-14 04:50:50.571+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_BUS'\n2016-11-14 04:50:50.571+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_SERIAL'\n2016-11-14 04:50:50.571+0000: 1268: debug : udevGetDeviceSysfsAttr:208 : udev reports device 'dm-2' does not have sysfs attr 'device/vendor'\n2016-11-14 04:50:50.571+0000: 1268: debug : udevGetDeviceSysfsAttr:208 : udev reports device 'dm-2' does not have sysfs attr 'device/model'\n2016-11-14 04:50:50.571+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_TYPE'\n2016-11-14 04:50:50.571+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_DRIVE_FLOPPY'\n2016-11-14 04:50:50.571+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_CDROM'\n2016-11-14 04:50:50.571+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_DRIVE_FLASH_SD'\n2016-11-14 04:50:50.571+0000: 1268: debug : udevKludgeStorageType:1037 : Could not find definitive storage type for device with sysfs path '/sys/devices/virtual/block/dm-2', trying to guess it\n2016-11-14 04:50:50.571+0000: 1268: debug : udevKludgeStorageType:1046 : Could not determine storage type for device with sysfs path '/sys/devices/virtual/block/dm-2'\n2016-11-14 04:50:50.571+0000: 1268: debug : udevProcessStorage:1167 : Storage ret=-1\n2016-11-14 04:50:50.571+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d1e6a0 /sys/devices/virtual/block/dm-2\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:50.571+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:50.571+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.582+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:50.582+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.582+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.582+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.582+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.582+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.582+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.582+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.582+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.583+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:50.583+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:50.583+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:50.583+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:50.583+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.592+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:50.592+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'change'\n2016-11-14 04:50:50.592+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'DRIVER'\n2016-11-14 04:50:50.592+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_BUS'\n2016-11-14 04:50:50.592+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_SERIAL'\n2016-11-14 04:50:50.592+0000: 1268: debug : udevGetDeviceSysfsAttr:208 : udev reports device 'dm-2' does not have sysfs attr 'device/vendor'\n2016-11-14 04:50:50.592+0000: 1268: debug : udevGetDeviceSysfsAttr:208 : udev reports device 'dm-2' does not have sysfs attr 'device/model'\n2016-11-14 04:50:50.592+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_TYPE'\n2016-11-14 04:50:50.592+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_DRIVE_FLOPPY'\n2016-11-14 04:50:50.592+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_CDROM'\n2016-11-14 04:50:50.592+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_DRIVE_FLASH_SD'\n2016-11-14 04:50:50.592+0000: 1268: debug : udevKludgeStorageType:1037 : Could not find definitive storage type for device with sysfs path '/sys/devices/virtual/block/dm-2', trying to guess it\n2016-11-14 04:50:50.592+0000: 1268: debug : udevKludgeStorageType:1046 : Could not determine storage type for device with sysfs path '/sys/devices/virtual/block/dm-2'\n2016-11-14 04:50:50.592+0000: 1268: debug : udevProcessStorage:1167 : Storage ret=-1\n2016-11-14 04:50:50.592+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d1c6b0 /sys/devices/virtual/block/dm-2\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:50.592+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:50.592+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.921+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:50.921+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:50.921+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:50.921+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:50.921+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.927+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:50.927+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:50.927+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:50.927+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 2 event(s)\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.927+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:50.927+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:50.927+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.927+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:50.927+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'remove'\n2016-11-14 04:50:50.927+0000: 1268: debug : udevRemoveOneDevice:1314 : Failed to find device to remove that has udev name '/sys/devices/virtual/bdi/253:2'\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:50.927+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.927+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.928+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:50.928+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'remove'\n2016-11-14 04:50:50.928+0000: 1268: debug : udevRemoveOneDevice:1314 : Failed to find device to remove that has udev name '/sys/devices/virtual/block/dm-2'\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:50.928+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:50.928+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.929+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:50.929+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'remove'\n2016-11-14 04:50:50.929+0000: 1268: debug : udevRemoveOneDevice:1314 : Failed to find device to remove that has udev name '/sys/devices/virtual/block/dm-2'\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:50.929+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:50.929+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:50.976+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:50.976+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:50.976+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:50.976+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:50.976+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:50.976+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:50.976+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:50.976+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:50.976+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:50.976+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=6 events=1\n2016-11-14 04:50:50.976+0000: 1268: debug : virNetlinkEventCallback:472 : dispatching to max 0 clients, called from event watch 6\n2016-11-14 04:50:50.976+0000: 1268: debug : virNetlinkEventCallback:485 : event not handled.\n2016-11-14 04:50:50.976+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:50.976+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:50.976+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.976+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.976+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.977+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:50.977+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:50.977+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:50.977+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:50.977+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:50.977+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:50.977+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:50.977+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:50.977+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:50.977+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:50.977+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:50.977+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:50.977+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n...\n2016-11-14 04:50:50.991+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n2016-11-14 04:50:51.000+0000: 1268: debug : virEventPollRunOnce:651 : Poll got 1 event(s)\n2016-11-14 04:50:51.000+0000: 1268: debug : virEventPollDispatchTimeouts:433 : Dispatch 0\n2016-11-14 04:50:51.000+0000: 1268: debug : virEventPollDispatchHandles:479 : Dispatch 8\n2016-11-14 04:50:51.000+0000: 1268: debug : virEventPollDispatchHandles:493 : i=0 w=1\n2016-11-14 04:50:51.000+0000: 1268: debug : virEventPollDispatchHandles:493 : i=1 w=2\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollDispatchHandles:493 : i=2 w=3\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollDispatchHandles:493 : i=3 w=4\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollDispatchHandles:493 : i=4 w=5\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollDispatchHandles:493 : i=5 w=6\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollDispatchHandles:493 : i=7 w=8\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollDispatchHandles:493 : i=8 w=9\n2016-11-14 04:50:51.001+0000: 1268: info : virEventPollDispatchHandles:507 : EVENT_POLL_DISPATCH_HANDLE: watch=9 events=1\n2016-11-14 04:50:51.001+0000: 1268: debug : udevEventHandleCallback:1543 : udev action: 'change'\n2016-11-14 04:50:51.001+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'DRIVER'\n2016-11-14 04:50:51.001+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_BUS'\n2016-11-14 04:50:51.001+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_SERIAL'\n2016-11-14 04:50:51.001+0000: 1268: debug : udevGetDeviceSysfsAttr:208 : udev reports device 'dm-2' does not have sysfs attr 'device/vendor'\n2016-11-14 04:50:51.001+0000: 1268: debug : udevGetDeviceSysfsAttr:208 : udev reports device 'dm-2' does not have sysfs attr 'device/model'\n2016-11-14 04:50:51.001+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_TYPE'\n2016-11-14 04:50:51.001+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_DRIVE_FLOPPY'\n2016-11-14 04:50:51.001+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_CDROM'\n2016-11-14 04:50:51.001+0000: 1268: debug : udevGetDeviceProperty:124 : udev reports device 'dm-2' does not have property 'ID_DRIVE_FLASH_SD'\n2016-11-14 04:50:51.001+0000: 1268: debug : udevKludgeStorageType:1037 : Could not find definitive storage type for device with sysfs path '/sys/devices/virtual/block/dm-2', trying to guess it\n2016-11-14 04:50:51.001+0000: 1268: debug : udevKludgeStorageType:1046 : Could not determine storage type for device with sysfs path '/sys/devices/virtual/block/dm-2'\n2016-11-14 04:50:51.001+0000: 1268: debug : udevProcessStorage:1167 : Storage ret=-1\n2016-11-14 04:50:51.001+0000: 1268: debug : udevAddOneDevice:1413 : Discarding device -1 0x7f63a5d1d6d0 /sys/devices/virtual/block/dm-2\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventRunDefaultImpl:305 : running default event implementation\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollCleanupTimeouts:526 : Cleanup 0\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollCleanupTimeouts:562 : Found 0 out of 0 timeout slots used, releasing 0\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollCleanupHandles:575 : Cleanup 9\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=0 w=1, f=6 e=1 d=0\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=1 w=2, f=8 e=1 d=0\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=2 w=3, f=11 e=1 d=0\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=3 w=4, f=12 e=1 d=0\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=4 w=5, f=13 e=1 d=0\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=5 w=6, f=14 e=1 d=0\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=6 w=7, f=15 e=0 d=0\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=7 w=8, f=15 e=1 d=0\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollMakePollFDs:401 : Prepare n=8 w=9, f=19 e=1 d=0\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollCalculateTimeout:338 : Calculate expiry of 0 timers\n2016-11-14 04:50:51.001+0000: 1268: debug : virEventPollCalculateTimeout:371 : No timeout is pending\n2016-11-14 04:50:51.001+0000: 1268: info : virEventPollRunOnce:641 : EVENT_POLL_RUN: nhandles=8 timeout=-1\n```\n. Sorry, the logs are long. @gao-feng Do you know how I can check \"qemu supports virtfs\"?\n. @gao-feng  It's failed, and here's the output.\n```\n$qemu-system-x86_64 -fsdev local,id=virtio9p,path=/tmp,security_model=none -device virtio-9p-pci,fsdev=virtio9p,mount_tag=hello\nqemu-system-x86_64: -device virtio-9p-pci,fsdev=virtio9p,mount_tag=hello: 'virtio-9p-pci' is not a valid device model name\n```\n. I'll try it again. Thank you Feng!\n. After reconfig the qemu source, the problem solved, and pod can run.\nSo appreciate you @gao-feng  and @gnawux .\nI still face a new problem, but will start a new thread to track.\nThanks!\n. here's the qemu process\uff1a\n```\n$sudo ps aux | grep qemu\nroot      20479  1.7  0.1 1049040 99228 ?       Sl   17:31   0:02 /usr/local/bin/qemu-system-x86_64 -machine pc-i440fx-2.0,accel=kvm,usb=off -global kvm-pit.lost_tick_policy=discard -cpu host -kernel /var/lib/hyper/kernel -initrd /var/lib/hyper/hyper-initrd.img -append console=ttyS0 panic=1 no_timer_check -realtime mlock=off -no-user-config -nodefaults -no-hpet -rtc base=utc,driftfix=slew -no-reboot -display none -boot strict=on -m 128 -smp 1 -qmp unix:/var/run/hyper/vm-OFdQkvhhDh/qmp.sock,server,nowait -serial unix:/var/run/hyper/vm-OFdQkvhhDh/console.sock,server,nowait -device virtio-serial-pci,id=virtio-serial0,bus=pci.0,addr=0x2 -device virtio-scsi-pci,id=scsi0,bus=pci.0,addr=0x3 -chardev socket,id=charch0,path=/var/run/hyper/vm-OFdQkvhhDh/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0 -chardev socket,id=charch1,path=/var/run/hyper/vm-OFdQkvhhDh/tty.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1 -fsdev local,id=virtio9p,path=/var/run/hyper/vm-OFdQkvhhDh/share_dir,security_model=none -device virtio-9p-pci,fsdev=virtio9p,mount_tag=share_dir -daemonize -pidfile /var/run/hyper/vm-OFdQkvhhDh/pidfile -D /var/log/hyper/qemu/vm-OFdQkvhhDh.log\nadmin     23923  0.0  0.0 112652   980 pts/1    S+   17:33   0:00 grep --color=auto qemu\n```\n$ls  /var/log/hyper\nhyperd.INFO  hyperd.rs7b01008.root.log.INFO.20161114-173017.18531  qemu\nlogs from /var/log/hyper/hyperd.rs7b01008.root.log.INFO.20161114-173017.18531\n```\nBinary: Built with gc go1.5.1 for linux/amd64\nLog line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\nI1114 17:30:17.079651   18531 daemon.go:141] The config: kernel=/var/lib/hyper/kernel, initrd=/var/lib/hyper/hyper-initrd.img\nI1114 17:30:17.079949   18531 daemon.go:143] The config: vbox image=\nI1114 17:30:17.079960   18531 daemon.go:146] The config: bridge=, ip=\nI1114 17:30:17.079968   18531 daemon.go:149] The config: bios=, cbfs=\nI1114 17:30:20.316967   18531 hyperd.go:193] The hypervisor's driver is\nI1114 17:30:20.447406   18531 hyperd.go:245] Hyper daemon: 0.7.0 0\nI1114 17:31:21.880395   18531 init_comm.go:142] Wating for init messages...\nI1114 17:31:21.880399   18531 qmp_handler.go:177] begin qmp init...\nI1114 17:31:21.917555   18531 qmp_handler.go:186] got qmp welcome, now sending command qmp_capabilities\nI1114 17:31:21.917650   18531 qmp_handler.go:201] waiting for response\nI1114 17:31:21.918125   18531 qmp_handler.go:210] got for response\nI1114 17:31:21.918162   18531 qmp_handler.go:213] QMP connection initialized\nI1114 17:31:21.918238   18531 qmp_handler.go:346] QMP initialzed, go into main QMP loop\nI1114 17:31:21.918253   18531 qmp_handler.go:137] Begin receive QMP message\nI1114 17:31:22.895995   18531 pod.go:557] create container d5eb2c11a4a419629174f12cb017e5356ae88ce3ca8f8e22ef08b4002cd45e7e\nI1114 17:31:22.896151   18531 pod.go:597] container name ubuntu-latest-7178817114, image ubuntu:latest\nI1114 17:31:22.896362   18531 pod.go:624] container info config &{d5eb2c11a4a4   false false false map[]  false false false [PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin] 0xc8213541e0 false ubuntu:latest map[]   true  [] map[] }, Cmd [bash], Args []\nI1114 17:31:22.898271   18531 run.go:68] pod:pod-hyiQRxiLTI, vm:vm-OFdQkvhhDh\nI1114 17:31:22.898330   18531 vm.go:235] find vm:vm-OFdQkvhhDh\nI1114 17:31:22.898372   18531 pod.go:960] container ID: d5eb2c11a4a419629174f12cb017e5356ae88ce3ca8f8e22ef08b4002cd45e7e, mountId 6656654be480c31b05a33f127efed2d8de6d3c60d72b5fd799467ffd36cccf71\nI1114 17:31:23.287452   18531 vm_states.go:439] got spec, prepare devices\nI1114 17:31:23.287563   18531 vm_states.go:75] attach pending client for d5eb2c11a4a419629174f12cb017e5356ae88ce3ca8f8e22ef08b4002cd45e7e\nI1114 17:31:23.287632   18531 qmp_handler.go:296] got new session\nI1114 17:31:23.293245   18531 qmp_handler.go:302] session finished, buffer size 1\nI1114 17:31:23.307277   18531 qmp_handler.go:296] got new session\nI1114 17:31:23.316099   18531 qmp_handler.go:302] session finished, buffer size 1\nI1114 17:31:24.016185   18531 init_comm.go:152] Get init ready message\nI1114 17:31:24.016276   18531 init_comm.go:225] got cmd:1\nI1114 17:31:24.016338   18531 vm_states.go:480] begin to wait vm commands\nI1114 17:31:24.016647   18531 init_comm.go:225] got cmd:0\nI1114 17:31:24.022707   18531 init_comm.go:225] got cmd:14\nI1114 17:31:24.023462   18531 init_comm.go:225] got cmd:14\nI1114 17:31:24.026944   18531 init_comm.go:225] got cmd:14\nI1114 17:31:24.156678   18531 init_comm.go:225] got cmd:9\nI1114 17:31:24.157022   18531 vm_states.go:506] pod start success\nI1114 17:31:24.160280   18531 init_comm.go:225] got cmd:14\nI1114 17:31:24.161129   18531 init_comm.go:225] got cmd:14\nI1114 17:31:24.162697   18531 init_comm.go:225] got cmd:9\nI1114 17:31:24.162720   18531 init_comm.go:198] hyperstart API version:4242, VM hyperstart API version: 4242\nI1114 17:31:54.162971   18531 init_comm.go:225] got cmd:12\nI1114 17:31:54.168610   18531 init_comm.go:225] got cmd:14\nI1114 17:31:54.170164   18531 init_comm.go:225] got cmd:14\nI1114 17:31:54.172901   18531 init_comm.go:225] got cmd:9\nI1114 17:32:24.173208   18531 init_comm.go:225] got cmd:12\nI1114 17:32:24.178371   18531 init_comm.go:225] got cmd:14\n```\n. VM log is empty.\n$ll /var/log/hyper/qemu/\ntotal 0\n-rw-r--r-- 1 root root 0 Nov 14 17:31 vm-OFdQkvhhDh.log\n. Appreciate any response very much!\n. I can get output with \"run -t\", though can NOT attach to a pod with \"run -d\".\n$sudo hyperctl run -t ubuntu:latest ls /\nbin   dev  home  lib64  mnt  proc  run   selinux  sys  usr\nboot  etc  lib   media  opt  root  sbin  srv      tmp  var\nIt's weird.\n. Thank you @laijs  so much!\nAs you said, there has output after I typed commands. And it looks better after I start pod with -d -t.\n. I see. Thank @laijs  very much!\n. Thanks for response.\nCan I use domain socket, since the ip of vm and host could be in different ip domain?\nAnd, shared filesystem is not good for protocol communication; stdio is lack of intercommunication.\n. Is there any plan in future?\nDevice is visible to docker, but is disabled to vm without driver.\n. ",
    "marcosnils": "@gnawux yes, forgot to comment that installing libvirt0 fixes the issue.. ```# configurations for hyperd\nRoot directory for hyperd\nRoot=/var/lib/hyper/\nSpecify the hypervisor: libvirt, qemu, qemu-kvm, kvm, xen, vbox (for linux)\nvbox (for mac).\n\"kvm\"  is equivalent to \"qemu-kvm\" which uses qemu with kvm acceleration.\n\"qemu\" is equivalent to \"qemu-kvm\" when the system enables kvm, otherwise\nthe hypervisor is \"qemu-tcg\" (qemu without kvm acceleration).\nWhen Hypervisor is not set, the hyperd will try to probe \"qemu-kvm\" or \"xen\"\nas the containers' hypervisor according to the host, if the host doesn't\nsupport any hardware-assisted technology, it will use \"qemu-tcg\".\n\nHypervisor=libvirt\nBoot kernel\nKernel=/var/lib/hyper/kernel\nBoot initrd\nInitrd=/var/lib/hyper/hyper-initrd.img\nBIOS image, qboot bios will accelarate the bootup\nBios=/var/lib/hyper/bios-qboot.bin\nCBFS coreboot fs for boot image, if it is set, Kernel and Initrd will be ignored\nCbfs=/var/lib/hyper/cbfs-qboot.rom\nBoot CDROOM for \"vbox\" hypervisor (for mac only)\nVbox=/opt/hyper/static/iso/hyper-vbox-boot.iso\nStorage driver for hyperd, valid value includes devicemapper, overlay, and aufs\nStorageDriver=overlay\nBridge device for hyperd, default is hyper0\nBridge=\nBridge ip address for the bridge device\nBridgeIP=\nIf the host IP is provided, a TCP port will be listened for, same as the '--host' option\nHost=\nThis is only useful for hypernetes, to disable the iptables setup by hyperd\nDisableIptables=false\nVmFactoryPolicy defines the policies to create factories\nVmFactoryPolicy = [FactoryConfig,]*FactoryConfig\nFactoryConfig   = {[\"cache\":NUMBER,][\"template\":(true|false),]\"cpu\":NUMBER,\"memory\":NUMBER}\nExamples:\nVmFactoryPolicy={\"cache\":10, \"cpu\":1, \"memory\":128}\nVmFactoryPolicy={\"cpu\":3, \"memory\":1024}\nVmFactoryPolicy={\"template\":true, \"cpu\":1, \"memory\":128}\nVmFactoryPolicy={\"cache\":1, \"template\":true, \"cpu\":1, \"memory\":128}\nVmFactoryPolicy={\"cache\":10, \"template\":true, \"cpu\":1, \"memory\":128},{\"template\":true, \"cpu\":3, \"memory\":1024}\nIt is recommended to specify the \"cache\" when VmFactoryPolicy is set,\notherwise it is a less efficient factory\nVmFactoryPolicy=\n[Log]\nPodLogPrefix=/var/run/hyper/Pods\nPodIdInPath=true\n```. @gao-feng tried recompiling the initrd image following hyperstart README instructions several times. Always getting the same error.\nThe initrd.img gets generated and has a \"reasonable\" size, but it doesn't work :( . Yes, I copy both he kernel and the initrd.img to that directory. . @gao-feng this initrd works. So, what should I do in order to build a proper one from the hyperstart repo?. ",
    "kfox1111": "Curious what issues you have hit. Do you have any logs?. Where did you do the benchmark? Running the tar within the container at /var/lib/hyper with the passed through volume will perform vastly differently then in the rest of the container as it will be doing cow operations.. ",
    "kadogo": "Perfect I will wait then ^^\nThanks.. ",
    "matti": "to me it seems like (see my recent issues) that hyperd is not really tested with anything else than with centos -- I used centos and the pre-built packages and everything worked fine.\nShould I just use centos and forget ubuntu etc?. @gnawux thanks ok!. (I'm only compiling from the source since there are no pre-built binaries). So can I use an older xen? Which version and how should I install it?. (Can I install packages or should I install it from the source, the docs are very thin here @gnawux). yes instead of qemu.. (I'm only compiling from the source since there are no pre-built binaries). so the docs should be updated if I'm reading you correctly?. (related https://github.com/hyperhq/hyperd/issues/467). any pointers where to look for a solution?. Okay, so this seems to work:\ndocker run -it --privileged --net=host -v /var/lib/hyper myimage hyperd\nSo: -v /dev:/dev is not needed and also --cap-add=ALL is not (because of --privileged)\nIMO this looks very promising, it's so convenient to run hyperd as a docker image!. @neerdoc didn\u2019t do any IO benchmarks. Just compared startup times (ran uptime from alpine image) - from around 6s on GCP, virtualbox to 1.7s with baremetal kabylake 3.8ghz... @gnawux can you help me out building one? or is it not doable anymore?\nmake\nMaking all in mac_installer\nmake[1]: Nothing to be done for `all'.\ngo build -tags \"static_build   exclude_graphdriver_btrfs libdm_no_deferred_remove\" -ldflags \"-X github.com/hyperhq/hyperd/utils.VERSION=0.8.0 -X github.com/hyperhq/hyperd/utils.GITCOMMIT=`git describe`\" hyperd.go\nhyperd.go:11:2: cannot find package \"github.com/docker/docker/opts\" in any of:\n    /usr/local/Cellar/go/1.8/libexec/src/github.com/docker/docker/opts (from $GOROOT)\n    /Users/mpa/.go/src/github.com/docker/docker/opts (from $GOPATH)\nhyperd.go:20:2: cannot find package \"github.com/docker/docker/pkg/parsers/kernel\" in any of:\n    /usr/local/Cellar/go/1.8/libexec/src/github.com/docker/docker/pkg/parsers/kernel (from $GOROOT)\n    /Users/mpa/.go/src/github.com/docker/docker/pkg/parsers/kernel (from $GOPATH). ",
    "neerdoc": "@matti Did you benchmark your setup? I followed this to create a hyper-in-docker (hid)  installation of the 0.8.1 version and it worked fine, but when running a simple benchmark I get very low disk-IO performance. I.e., 'tar xf linux-4.12.5.tar.xz' takes around 15 seconds on host, also around 15 seconds in docker but around 200 seconds on hid. All my setup uses btrfs (host, docker and hyperd).. ",
    "SergeyOvsienko": "When the bug fix is planned?\n. tnx. @gnawux In new version hyperd there will be native support iptables? Kubernetes is not our case)\nI do not want to write the wrapper for iptables myself. @gnawux I'm interested in new versions? Or in future versions there will be no more support iptables native?. @gnawux tnx. I am too very interested). ",
    "nickdoikov": "hello @gnawux  , is there any info about approx 0.9 release date ? . @gnawux could you please provide a previous version number where this feature was worked?. @gnawux  it is not clear for me now.\ndo hyperd will support native iptables in future versions?\n It is simple question , but it is very important for us.  \nWhy you kill functionality that allow to use hyperd without kubernetes?\nSometimes it is enough to build own schedulers and use common iptables based port forwarding.. @gnawux thanks , so  i can downgrade to 0.7 and build own ecosystem holding in mind that in future i will be capable to upgrade to 0.9 where  port mapping management functionality will be added back?\ni mean that  i will  be capable to manage it via REST or cli , not only via kubelet in 0.9 ?. manage port publishing ? \nor you mean general REST/cli management  ?\nMy question was about port publishing functionality  and your response not so clear for me, sorry , cloud please clarify  :)\nfor me is not a problem that mechanisms was changed, i`m worried about functionality (iptables REST/CLI management) ..  what you mean when talking about \"interface\"?\nrest endpoints structure? or what ?  not clearly understand you.. thanks !\nit is clear now .\nis there any chance that release 0.9 will be done in 2017 ? . @cool, many thanks. \nActually we had plans to use hyper in paas platform instead of currently used docker  engine and containers,hyper provide simplicity and excellent resource isolation level.\nthat's why i'm very interested in hyper release perspectives.\nThank you one more time.. Hello @gnawux \na small update :\nWe have tested 0.7 and iptables works fine, but this version contain interactive terminal issues.\nso we will waiting for 0.8.2 or 0.9.0  hope this will happened soon.\n . ",
    "punitagrawal": "Thanks for the quick response.\nI found out what I was doing wrong - instead of <GOPATH>/src I had cloned the repository in <GOPATH>. I think that was why I got the build failures.\nSorry for the noise. I can build when I correctly follow the instructions.\nFor reference, the go version on the system -\n% go version\ngo version go1.8.3 linux/amd64. ",
    "giabar": "No solution for this issue?. ",
    "jiangpengcheng": "should this be written to the systemd unit file?. ",
    "haoyixin": "@gnawux i did as you said, but it's not fixed.. ",
    "qianyuqiao": "I have the same problem,even worse. of course not , now on windows platform .only win10 and windows server support windows containers while other versions use docker in vitualbox instead . ",
    "minz1027": "@gnawux Hi, can you help with the issue or point me to someone who knows about it? Thanks!. @gnawux Hi Xu, thanks for your reply! I have some questions about this, \n\nDoes it mean the configurable vol size thing will only work for devicemapper?\nDo you want me to just add a size field to the pod spec or pass it all the way to construct a dm?. @gnawux Hi Xu, I have the code to pass size through cmd line and pod spec. But the thing is I found that the size property or storage for the daemon is set when creating a new daemon. https://github.com/hyperhq/hyperd/blob/20ab5d6fc8c38259f6d481667027c4cc381d0e2c/daemon/daemon.go#L136\n\nBy the time we tried to create a pod, the daemon should be already created and running. How can we override the storage property in that case?. @gnawux Hmm.. I am not sure if that will solve my problem. My problem is when I try to hyperctl run with a very large image(~10G), I will get no space left on the disk error when pulling the image. \nIt doesn't seem to be related to the volume.\nWhat I want is more like setting the dm.basesize in docker daemon.\nhttps://docs.docker.com/engine/reference/commandline/dockerd/. @gnawux Hi Xu, my PR got merge yay! I wonder when would you guys do release with that change? Thanks! . @gnawux Hi Xu, do you have a date when this will be merged?. @gnawux Interesting! I actually had the same performance issue when running hyperd on aws vm. So using runV with the xen pv mode will be better for that use case? Do we have permission to use the xen pv mode on aws vm?\nBTW, have you guys investigated how this can be integrated with kubernetes?. ",
    "antoineherzog": "yeah, it would be very helpful! ?\nalso it would be very helpful to have a tutorial to install hyper in Ubuntu with packet.net\nIt fell very complicated right now.. @gnawux . ",
    "gogolxdong": "@gnawux deployed hyperd on physical machine end up with:\nhyperctl run -t --cpu 4 --memory 8192 centos7_go\nwrk -t12 -c400 -d30s http://192.168.123.5\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     6.56ms    2.86ms 222.43ms   91.27%\n    Req/Sec     5.09k   777.00    46.00k    97.56%\n  1823029 requests in 30.10s, 226.01MB read\nRequests/sec:  60569.22\nTransfer/sec:      7.51MB\ndocker run -it centos7_go \nwith 24cores 32G memory\nwrk -t12 -c400 -d30s http://172.17.0.3:1323\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     3.12ms    5.28ms 218.95ms   95.94%\n    Req/Sec    11.85k     1.61k   20.74k    76.04%\n  4254182 requests in 30.10s, 527.42MB read\nRequests/sec: 141341.39\nTransfer/sec:     17.52MB\ncompared with cc-time of docker run -it --cpus 4 --memory 8589934592 centos7_go\n```\n  Thread Stats   Avg      Stdev     Max   +/- Stdev\n    Latency     8.92ms    6.34ms 232.35ms   91.85%\n    Req/Sec     3.83k   410.75    15.51k    95.00%\n  1373596 requests in 30.10s, 170.30MB read\nRequests/sec:  45636.34\nTransfer/sec:      5.66MB\n. ",
    "xuchenhao001": "Now that I know more about the kernel from hyperhq/hyperstart, I built my s390x kernel from torvalds/linux in s390x/ubuntu docker container:\nbash\nmake defconfig\nmake -j 8\nand then i integrated kernel, kernel_config and initrd.img modules.tar with hyperhq/hyperstart.\nAfter that, i got a hyper-initrd.img.\nThen, i tried to run ubuntu image by: hyperctl run -t s390x/ubuntu\nbut it didn't work:\nbash\nNov 01 09:23:55 marist02 hyperd[9436]: time=\"2017-11-01T09:23:55Z\" level=info msg=\"Graph migration to content-addressability took 0.00 seconds\"\nNov 01 09:23:55 marist02 hyperd[9436]: time=\"2017-11-01T09:23:55Z\" level=info msg=\"Firewalld running: false\"\nNov 01 09:23:55 marist02 hyperd[9436]: time=\"2017-11-01T09:23:55Z\" level=warning msg=\"Your kernel does not support swap memory limit.\"\nNov 01 09:23:55 marist02 hyperd[9436]: time=\"2017-11-01T09:23:55Z\" level=info msg=\"Loading containers: start.\"\nNov 01 09:23:55 marist02 hyperd[9436]: time=\"2017-11-01T09:23:55Z\" level=info msg=\"Loading containers: done.\"\nNov 01 09:24:29 marist02 hyperd[9436]: E1101 09:24:29.258757    9436 json.go:370] read tty data failed\nNov 01 09:24:29 marist02 hyperd[9436]: E1101 09:24:29.259145    9436 json.go:427] SB[vm-NwUOEtsyii] tty socket closed, quit the reading goroutine: read unix @->/var/run/hyper/vm-NwUOEtsyii/tty.sock: read: connection reset by peer\nNov 01 09:24:29 marist02 hyperd[9436]: E1101 09:24:29.259182    9436 json.go:570] SB[vm-NwUOEtsyii] get hyperstart API version error: hyperstart closed\nNov 01 09:24:29 marist02 hyperd[9436]: E1101 09:24:29.259217    9436 vm_states.go:288] SB[vm-NwUOEtsyii] hyperstart failed: hyperstart closed\nNov 01 09:24:29 marist02 hyperd[9436]: E1101 09:24:29.259225    9436 vm_states.go:246] SB[vm-NwUOEtsyii] Shutting down because of an exception: %!(EXTRA string=connection to vm broken)\nNov 01 09:24:29 marist02 hyperd[9436]: E1101 09:24:29.259268    9436 vm_states.go:226] SB[vm-NwUOEtsyii] Start POD failed: hyperstart closed\nNov 01 09:24:29 marist02 hyperd[9436]: E1101 09:24:29.259288    9436 run.go:38] ubuntu-0536492129: failed to add pod: hyperstart closed\nNov 01 09:24:29 marist02 hyperd[9436]: E1101 09:24:29.259302    9436 server.go:170] Handler for POST /v0.8.1/pod/create returned error: hyperstart closed\nNov 01 09:24:29 marist02 hyperd[9436]: E1101 09:24:29.259515    9436 json.go:137] read init data failed\nNov 01 09:24:29 marist02 hyperd[9436]: E1101 09:24:29.259530    9436 json.go:171] SB[vm-NwUOEtsyii] error when readVmMessage() for ready message: read unix @->/var/run/hyper/vm-NwUOEtsyii/hyper.sock: read: connection reset by peer\nNov 01 09:24:29 marist02 hyperd[9436]: E1101 09:24:29.259561    9436 qmp_handler.go:183] get qmp welcome failed: read unix @->/var/run/hyper/vm-NwUOEtsyii/qmp.sock: read: connection reset by peer\nNov 01 09:24:29 marist02 hyperd[9436]: E1101 09:24:29.259601    9436 qmp_handler.go:363] QMP initialize failed\nNov 01 09:24:29 marist02 hyperd[9436]: E1101 09:24:29.261501    9436 qemu_process.go:157] exit status 1. @gnawux Thanks for your attention! Here is the logs after set -v=4:\n```bash\n$ hyperctl run -t s390x/ubuntu\n...\n-- The start-up result is done.\nNov 01 12:11:40 marist02 hyperd[9839]: time=\"2017-11-01T12:11:40Z\" level=debug msg=\"Using default logging driver none\"\nNov 01 12:11:40 marist02 hyperd[9839]: time=\"2017-11-01T12:11:40Z\" level=debug msg=\"[graphdriver] trying provided driver \"overlay\"\"\nNov 01 12:11:40 marist02 hyperd[9839]: time=\"2017-11-01T12:11:40Z\" level=debug msg=\"Using graph driver overlay\"\nNov 01 12:11:41 marist02 hyperd[9839]: time=\"2017-11-01T12:11:41Z\" level=info msg=\"Graph migration to content-addressability took 0.00 seconds\"\nNov 01 12:11:41 marist02 hyperd[9839]: time=\"2017-11-01T12:11:41Z\" level=debug msg=\"Option DefaultDriver: bridge\"\nNov 01 12:11:41 marist02 hyperd[9839]: time=\"2017-11-01T12:11:41Z\" level=debug msg=\"Option DefaultNetwork: bridge\"\nNov 01 12:11:41 marist02 hyperd[9839]: time=\"2017-11-01T12:11:41Z\" level=info msg=\"Firewalld running: false\"\nNov 01 12:11:41 marist02 hyperd[9839]: time=\"2017-11-01T12:11:41Z\" level=debug msg=\"Registering ipam driver: \"default\"\"\nNov 01 12:11:41 marist02 hyperd[9839]: time=\"2017-11-01T12:11:41Z\" level=warning msg=\"Your kernel does not support swap memory limit.\"\nNov 01 12:11:41 marist02 hyperd[9839]: time=\"2017-11-01T12:11:41Z\" level=debug msg=\"Cleaning up old shm/mqueue mounts: start.\"\nNov 01 12:11:41 marist02 hyperd[9839]: time=\"2017-11-01T12:11:41Z\" level=debug msg=\"Cleaning up old shm/mqueue mounts: done.\"\nNov 01 12:13:45 marist02 hyperd[9839]: E1101 12:13:45.699033    9839 qmp_handler.go:183] get qmp welcome failed: read unix @->/var/run/hyper/vm-oOMYrhTfNz/qmp.sock: read: connection reset by peer\nNov 01 12:13:45 marist02 hyperd[9839]: E1101 12:13:45.700238    9839 qmp_handler.go:363] QMP initialize failed\nNov 01 12:13:45 marist02 hyperd[9839]: E1101 12:13:45.699061    9839 json.go:137] read init data failed\nNov 01 12:13:45 marist02 hyperd[9839]: E1101 12:13:45.699110    9839 json.go:370] read tty data failed\nNov 01 12:13:45 marist02 hyperd[9839]: E1101 12:13:45.699797    9839 qemu_process.go:157] exit status 1\nNov 01 12:13:45 marist02 hyperd[9839]: E1101 12:13:45.700669    9839 vm_states.go:288] SB[vm-oOMYrhTfNz] read unix @->/var/run/hyper/vm-oOMYrhTfNz/qmp.sock: read: connection reset by peer\nNov 01 12:13:45 marist02 hyperd[9839]: E1101 12:13:45.700759    9839 vm_states.go:246] SB[vm-oOMYrhTfNz] Shutting down because of an exception: %!(EXTRA string=connection to vm broken)\nNov 01 12:13:45 marist02 hyperd[9839]: E1101 12:13:45.700452    9839 json.go:171] SB[vm-oOMYrhTfNz] error when readVmMessage() for ready message: read unix @->/var/run/hyper/vm-oOMYrhTfNz/hyper.sock: read: connection reset by peer\nNov 01 12:13:45 marist02 hyperd[9839]: E1101 12:13:45.700562    9839 json.go:427] SB[vm-oOMYrhTfNz] tty socket closed, quit the reading goroutine: read unix @->/var/run/hyper/vm-oOMYrhTfNz/tty.sock: read: connection reset by peer\nNov 01 12:13:45 marist02 hyperd[9839]: 2017/11/01 12:13:45 http: panic serving @: send on closed channel\nNov 01 12:13:45 marist02 hyperd[9839]: goroutine 51 [running]:\nNov 01 12:13:45 marist02 hyperd[9839]: net/http.(conn).serve.func1(0xc4205b80a0)\nNov 01 12:13:45 marist02 hyperd[9839]:         /usr/local/go/src/net/http/server.go:1697 +0xc2\nNov 01 12:13:45 marist02 hyperd[9839]: panic(0x80ee2b40, 0x811cec40)\nNov 01 12:13:45 marist02 hyperd[9839]:         /usr/local/go/src/runtime/panic.go:491 +0x26e\nNov 01 12:13:45 marist02 hyperd[9839]: github.com/hyperhq/hyperd/vendor/github.com/hyperhq/runv/hypervisor.(VmContext).reportVmFault(...)\nNov 01 12:13:45 marist02 hyperd[9839]:         /root/go/src/github.com/hyperhq/hyperd/vendor/github.com/hyperhq/runv/hypervisor/report.go:66\nNov 01 12:13:45 marist02 hyperd[9839]: github.com/hyperhq/hyperd/vendor/github.com/hyperhq/runv/hypervisor.(VmContext).startPod(0xc42066c6e0, 0xc420191260, 0x805c6652)\nNov 01 12:13:45 marist02 hyperd[9839]:         /root/go/src/github.com/hyperhq/hyperd/vendor/github.com/hyperhq/runv/hypervisor/vm_states.go:225 +0x2fa\nNov 01 12:13:45 marist02 hyperd[9839]: github.com/hyperhq/hyperd/vendor/github.com/hyperhq/runv/hypervisor.(Vm).InitSandbox(0xc420064320, 0xc420191260, 0xc4200ba8f0, 0x80)\nNov 01 12:13:45 marist02 hyperd[9839]:         /root/go/src/github.com/hyperhq/hyperd/vendor/github.com/hyperhq/runv/hypervisor/vm.go:231 +0x4e\nNov 01 12:13:45 marist02 hyperd[9839]: github.com/hyperhq/hyperd/daemon/pod.(XPod).createSandbox(0xc4200ba8f0, 0xc4211232c0, 0xc4210036b0, 0xc4200ba8f0)\nNov 01 12:13:45 marist02 hyperd[9839]:         /root/go/src/github.com/hyperhq/hyperd/daemon/pod/provision.go:272 +0x33a\nNov 01 12:13:45 marist02 hyperd[9839]: github.com/hyperhq/hyperd/daemon/pod.CreateXPod(0xc420190ae0, 0xc4211232c0, 0x0, 0x0, 0x0)\nNov 01 12:13:45 marist02 hyperd[9839]:         /root/go/src/github.com/hyperhq/hyperd/daemon/pod/provision.go:36 +0x100\nNov 01 12:13:45 marist02 hyperd[9839]: github.com/hyperhq/hyperd/daemon.(Daemon).CreatePod(0xc420228070, 0x0, 0x0, 0xc4211232c0, 0xc4211232c0, 0x0, 0x0)\nNov 01 12:13:45 marist02 hyperd[9839]:         /root/go/src/github.com/hyperhq/hyperd/daemon/run.go:36 +0x27e\nNov 01 12:13:45 marist02 hyperd[9839]: github.com/hyperhq/hyperd/daemon.(Daemon).CmdCreatePod(0xc420228070, 0xc4205f00b0, 0xac, 0x200, 0xc4205f00b0, 0xac)\nNov 01 12:13:45 marist02 hyperd[9839]:         /root/go/src/github.com/hyperhq/hyperd/daemon/server.go:260 +0xc8\nNov 01 12:13:45 marist02 hyperd[9839]: github.com/hyperhq/hyperd/server/router/pod.(podRouter).postPodCreate(0xc42108f890, 0x3ffa085dfc0, 0xc4205b22d0, 0x818ee4a0, 0xc42114a000, 0xc42112c600, 0xc4205b2150, 0x818efaa0, 0xc4205b22d0)\nNov 01 12:13:45 marist02 hyperd[9839]:         /root/go/src/github.com/hyperhq/hyperd/server/router/pod/pod_routes.go:70 +0x20a\nNov 01 12:13:45 marist02 hyperd[9839]: github.com/hyperhq/hyperd/server/router/pod.(podRouter).(github.com/hyperhq/hyperd/server/router/pod.postPodCreate)-fm(0x3ffa085dfc0, 0xc4205b22d0, 0x818ee4a0, 0xc42114a000, 0xc42112c600, 0xc4205b2150, 0x3ffa085dfc0, 0xc4205b22d0)\nNov 01 12:13:45 marist02 hyperd[9839]:         /root/go/src/github.com/hyperhq/hyperd/server/router/pod/pod.go:26 +0x5a\nNov 01 12:13:45 marist02 hyperd[9839]: github.com/hyperhq/hyperd/server.versionMiddleware.func1(0x3ffa085ac08, 0xc420012018, 0x818ee4a0, 0xc42114a000, 0xc42112c600, 0xc4205b2150, 0x810c71ac, 0xe)\nNov 01 12:13:45 marist02 hyperd[9839]:         /root/go/src/github.com/hyperhq/hyperd/server/middleware.go:156 +0x166\nNov 01 12:13:45 marist02 hyperd[9839]: github.com/hyperhq/hyperd/server.(Server).corsMiddleware.func1(0x3ffa085ac08, 0xc420012018, 0x818ee4a0, 0xc42114a000, 0xc42112c600, 0xc4205b2150, 0x80e4e9e0, 0x811c86c8)\nNov 01 12:13:45 marist02 hyperd[9839]:         /root/go/src/github.com/hyperhq/hyperd/server/middleware.go:134 +0xa2\nNov 01 12:13:45 marist02 hyperd[9839]: github.com/hyperhq/hyperd/server.(Server).userAgentMiddleware.func1(0x3ffa085ac08, 0xc420012018, 0x818ee4a0, 0xc42114a000, 0xc42112c600, 0xc4205b2150, 0x8103f7a0, 0x100000080fca580)\nNov 01 12:13:45 marist02 hyperd[9839]:         /root/go/src/github.com/hyperhq/hyperd/server/middleware.go:117 +0xd4\nNov 01 12:13:45 marist02 hyperd[9839]: github.com/hyperhq/hyperd/server.(Server).makeHTTPHandler.func1(0x818ee4a0, 0xc42114a000, 0xc42112c600)\nNov 01 12:13:45 marist02 hyperd[9839]:         /root/go/src/github.com/hyperhq/hyperd/server/server.go:169 +0x1cc\nNov 01 12:13:45 marist02 hyperd[9839]: net/http.HandlerFunc.ServeHTTP(0xc421127560, 0x818ee4a0, 0xc42114a000, 0xc42112c600)\nNov 01 12:13:45 marist02 hyperd[9839]:         /usr/local/go/src/net/http/server.go:1918 +0x42\nNov 01 12:13:45 marist02 hyperd[9839]: github.com/hyperhq/hyperd/vendor/github.com/gorilla/mux.(Router).ServeHTTP(0xc42101e050, 0x818ee4a0, 0xc42114a000, 0xc42112c600)\nNov 01 12:13:45 marist02 hyperd[9839]:         /root/go/src/github.com/hyperhq/hyperd/vendor/github.com/gorilla/mux/mux.go:114 +0xde\nNov 01 12:13:45 marist02 hyperd[9839]: github.com/hyperhq/hyperd/server.(routerSwapper).ServeHTTP(0xc42114dc50, 0x818ee4a0, 0xc42114a000, 0xc42007c300)\nNov 01 12:13:45 marist02 hyperd[9839]:         /root/go/src/github.com/hyperhq/hyperd/server/router_swapper.go:29 +0x78\nNov 01 12:13:45 marist02 hyperd[9839]: net/http.serverHandler.ServeHTTP(0xc4210e9380, 0x818ee4a0, 0xc42114a000, 0xc42007c300)\nNov 01 12:13:45 marist02 hyperd[9839]:         /usr/local/go/src/net/http/server.go:2619 +0x96\nNov 01 12:13:45 marist02 hyperd[9839]: net/http.(conn).serve(0xc4205b80a0, 0x818ef9e0, 0xc42004e440)\nNov 01 12:13:45 marist02 hyperd[9839]:         /usr/local/go/src/net/http/server.go:1801 +0x6f8\nNov 01 12:13:45 marist02 hyperd[9839]: created by net/http.(Server).Serve\nNov 01 12:13:45 marist02 hyperd[9839]:         /usr/local/go/src/net/http/server.go:2720 +0x264\nNov 01 12:13:45 marist02 hyperd[9839]: E1101 12:13:45.701139    9839 json.go:570] SB[vm-oOMYrhTfNz] get hyperstart API version error: hyperstart closed\n. At `/var/log/hyper/qemu/vm-oOMYrhTfN.log` I found logs about vm:\nqemu-system-s390x: -device virtio-9p-ccw,fsdev=virtio9p,mount_tag=share_dir: 'virtio-9p-ccw' is not a valid device model name\n. I rebuild qemu with:bash\ngit clone git://git.qemu.org/qemu.git\ncd qemu\ngit checkout v2.10.1\ngit submodule init\ngit submodule update --recursive\n./configure --prefix=/usr --enable-virtfs\nmake\nmake install\nbut it seems not working with the same error:bash\n-- The start-up result is done.\nNov 02 09:37:39 marist02 hyperd[27626]: time=\"2017-11-02T09:37:39Z\" level=debug msg=\"Using default logging driver none\"\nNov 02 09:37:39 marist02 hyperd[27626]: time=\"2017-11-02T09:37:39Z\" level=debug msg=\"[graphdriver] trying provided driver \"overlay\"\"\nNov 02 09:37:39 marist02 hyperd[27626]: time=\"2017-11-02T09:37:39Z\" level=debug msg=\"Using graph driver overlay\"\nNov 02 09:37:39 marist02 hyperd[27626]: time=\"2017-11-02T09:37:39Z\" level=info msg=\"Graph migration to content-addressability took 0.00 seconds\"\nNov 02 09:37:39 marist02 hyperd[27626]: time=\"2017-11-02T09:37:39Z\" level=debug msg=\"Option DefaultDriver: bridge\"\nNov 02 09:37:39 marist02 hyperd[27626]: time=\"2017-11-02T09:37:39Z\" level=debug msg=\"Option DefaultNetwork: bridge\"\nNov 02 09:37:39 marist02 hyperd[27626]: time=\"2017-11-02T09:37:39Z\" level=info msg=\"Firewalld running: false\"\nNov 02 09:37:39 marist02 hyperd[27626]: time=\"2017-11-02T09:37:39Z\" level=debug msg=\"Registering ipam driver: \"default\"\"\nNov 02 09:37:39 marist02 hyperd[27626]: time=\"2017-11-02T09:37:39Z\" level=warning msg=\"Your kernel does not support swap memory limit.\"\nNov 02 09:37:39 marist02 hyperd[27626]: time=\"2017-11-02T09:37:39Z\" level=debug msg=\"Cleaning up old shm/mqueue mounts: start.\"\nNov 02 09:37:39 marist02 hyperd[27626]: time=\"2017-11-02T09:37:39Z\" level=debug msg=\"Cleaning up old shm/mqueue mounts: done.\"\nNov 02 09:39:02 marist02 hyperd[27626]: E1102 09:39:02.179870   27626 hypervisor.go:37] SB[vm-WLrdFtGHVq] watch hyperstart timeout\nNov 02 09:39:02 marist02 hyperd[27626]: E1102 09:39:02.180992   27626 json.go:570] SB[vm-WLrdFtGHVq] get hyperstart API version error: hyperstart closed\nNov 02 09:39:02 marist02 hyperd[27626]: E1102 09:39:02.181026   27626 vm_states.go:288] SB[vm-WLrdFtGHVq] watch hyperstart timeout\nNov 02 09:39:02 marist02 hyperd[27626]: E1102 09:39:02.181035   27626 vm_states.go:246] SB[vm-WLrdFtGHVq] Shutting down because of an exception: %!(EXTRA string=connection to vm broken)\nNov 02 09:39:02 marist02 hyperd[27626]: E1102 09:39:02.181133   27626 json.go:370] read tty data failed\nNov 02 09:39:02 marist02 hyperd[27626]: E1102 09:39:02.181148   27626 json.go:427] SB[vm-WLrdFtGHVq] tty socket closed, quit the reading goroutine: read unix @->/var/run/hyper/vm-WLrdFtGHVq/tty.sock: use of closed network connection\nNov 02 09:39:02 marist02 hyperd[27626]: E1102 09:39:02.181160   27626 vm_states.go:226] SB[vm-WLrdFtGHVq] Start POD failed: hyperstart closed\nNov 02 09:39:02 marist02 hyperd[27626]: E1102 09:39:02.181178   27626 run.go:38] ubuntu-6203458613: failed to add pod: hyperstart closed\nNov 02 09:39:02 marist02 hyperd[27626]: E1102 09:39:02.181194   27626 server.go:170] Handler for POST /v0.8.1/pod/create returned error: hyperstart closed\nNov 02 09:39:02 marist02 hyperd[27626]: E1102 09:39:02.185606   27626 json.go:137] read init data failed\nNov 02 09:39:02 marist02 hyperd[27626]: E1102 09:39:02.185730   27626 json.go:171] SB[vm-WLrdFtGHVq] error when readVmMessage() for ready message: EOF\nNov 02 09:39:02 marist02 hyperd[27626]: E1102 09:39:02.185891   27626 qmp_handler.go:141] QMP exit as got error: EOF\n:disappointed: . No, I found there is a new log file at `/var/log/hyper/qemu`, but it has no content:bash\n$ root@marist02:/var/log/hyper/qemu# ls -ltr\ntotal 4\n-rw-r--r-- 1 root root 126 Nov  2 03:18 vm-doHDBOZdv.log\n-rw-r--r-- 1 root root   0 Nov  2 09:38 vm-WLrdFtGHV.log\n(the first one is created before rebuild my qemu with `--enable-virtfs`). @gnawux \nAfter a long period of working on `qemu-system-s390x`, now i have something found:\nWhen I use libvirt to start a guest vm (Ubuntu) on s390x, i got qemu running parameters like this:\n/usr/bin/qemu-system-s390x -name ubuntu16.04 -S -machine s390-ccw-virtio-xenial,accel=kvm,usb=off -m 1024 -realtime mlock=off -smp 1,maxcpus=2,sockets=2,cores=1,threads=1 -object iothread,id=iothread1 -uuid 4db4b6b7-145b-4942-8833-509da364e32b -nographic -no-user-config -nodefaults -chardev socket,id=charmonitor,path=/var/lib/libvirt/qemu/domain-ubuntu16.04/monitor.sock,server,nowait -mon chardev=charmonitor,id=monitor,mode=control -rtc base=utc -no-shutdown -boot strict=on -kernel /var/lib/libvirt/images/kernel.ubuntu -initrd /var/lib/libvirt/images/initrd.ubuntu -device virtio-scsi-ccw,id=scsi0,devno=fe.0.0002 -drive file=/home/ubuntu/zhengxil/xchxubj/kvm/ubuntu.qcow2,format=raw,if=none,id=drive-virtio-disk0 -device virtio-blk-ccw,scsi=off,devno=fe.0.0000,drive=drive-virtio-disk0,id=virtio-disk0,bootindex=2 -drive file=/home/ubuntu/zhengxil/xchxubj/kvm/xenial-server-s390x.iso,format=raw,if=none,id=drive-scsi0-0-0-1,readonly=on -device scsi-cd,bus=scsi0.0,channel=0,scsi-id=0,lun=1,drive=drive-scsi0-0-0-1,id=scsi0-0-0-1,bootindex=1 -netdev tap,fd=25,id=hostnet0,vhost=on,vhostfd=27 -device virtio-net-ccw,netdev=hostnet0,id=net0,mac=52:54:00:9c:58:88,devno=fe.0.0001 -chardev pty,id=charconsole0 -device sclpconsole,chardev=charconsole0,id=console0 -device virtio-balloon-ccw,id=balloon0,devno=fe.0.0003 -msg timestamp=on\nOn the other hand, when I type `hyperctl run -t s390x/ubuntu:16.04`, it hangs for a while. And at that time, I start an another ssh and check the qemu process with `ps -ef | grep qemu`:\n/usr/bin/qemu-system-s390x -machine s390-ccw-virtio,accel=kvm,usb=off -cpu host -kernel /var/lib/hyper/kernel -initrd /var/lib/hyper/hyper-initrd.img -append \"console=ttyS1 panic=1 no_timer_check\" -realtime mlock=off -no-user-config -nodefaults -enable-kvm -rtc base=utc,clock=vm,driftfix=slew -no-reboot -display none -boot strict=on -m 128 -smp 1 -qmp unix:/var/run/hyper/vm-ESajskTODD/qmp.sock,server,nowait -chardev socket,id=charconsole0,path=/var/run/hyper/vm-ESajskTODD/console.sock,server,nowait -device sclpconsole,chardev=charconsole0 -device virtio-serial-ccw,id=virtio-serial0 -device virtio-scsi-ccw,id=scsi0 -chardev socket,id=charch0,path=/var/run/hyper/vm-ESajskTODD/hyper.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0 -chardev socket,id=charch1,path=/var/run/hyper/vm-ESajskTODD/tty.sock,server,nowait -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1 -fsdev local,id=virtio9p,path=/var/run/hyper/vm-ESajskTODD/share_dir,security_model=none -device virtio-9p-ccw,fsdev=virtio9p,mount_tag=share_dir -daemonize -pidfile /var/run/hyper/vm-ESajskTODD/pidfile -D /var/log/hyper/qemu/vm-ESajskTOD.log\nRoughly speaking, there are a lot of differences between this two.\nHowever, as I know, s390x do not support `tty`, but `pty` instead. \nSo if I can configure the default parameters with hyperctl to start `qemu-system-s390x`?\nHere is my xml file for libvirt:\n\nubuntu\n2097152\n1048576\n2\n\nhvm\n\n\n/usr/bin/qemu-system-s390x\n\n\n\n\n\n\n\n\n\n\n\n\nThanks!. @gao-feng Thanks! But as i know, there is no ttyS device on s390x machine:bash\n$ ls /dev\nautofs         chsc     cpu_dma_latency  dasda1  disk      fd         hwrng    lightnvm      loop1  loop5   mem               network_latency     prandom  sclp        stderr  tty       vhost-net\nblock          clp      cuse             dasda2  dm-0      full       initctl  log           loop2  loop6   memory_bandwidth  network_throughput  ptmx     sclp_line0  stdin   ttysclp0  zero\nbtrfs-control  console  dasd_eer         dasdb   dm-1      fuse       kmsg     loop-control  loop3  loop7   mqueue            null                pts      shm         stdout  urandom\nchar           core     dasda            dasdb1  ecryptfs  hugepages  kvm      loop0         loop4  mapper  net               port                random   snapshot    sys-vg  vfio\nSo which one should I use? . @pmorjan Thank you for your help!\nI have read your [code](https://github.com/hyperhq/runv/blob/b761a8c0aba5abc5aa672a327d97d001c17f62fa/hypervisor/qemu/qemu_s390x.go#L41) about this old example. It's working for runv to start or remove an vm.\nHowever, when I start a vm from command `hyperctl run -t s390x/ubuntu:16.04 `, it seems that hyperd cannot read any data from qemu. Just like the error log before:\nE1102 09:39:02.181133   27626 json.go:370] read tty data failed\nI think if there is something wrong with these lines in your code:\n\"-append\", \"\\\"console=ttyS1 panic=1 no_timer_check\\\"\",\n\"-chardev\", fmt.Sprintf(\"socket,id=charch0,path=%s,server,nowait\", ctx.HyperSockName),\n\"-device\", \"virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0\",\n\"-chardev\", fmt.Sprintf(\"socket,id=charch1,path=%s,server,nowait\", ctx.TtySockName),\n\"-device\", \"virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1\",\nBecause there is no ttyS device on s390x at all.. @gnawux Of course I do. here is the qemu process detail:bash\n/usr/bin/qemu-system-s390x \\\n-machine s390-ccw-virtio,accel=kvm,usb=off \\\n-cpu host \\\n-kernel /var/lib/hyper/kernel \\\n-initrd /var/lib/hyper/hyper-initrd.img \\\n-append \"console=ttyS1 panic=1 no_timer_check\" \\\n-realtime mlock=off \\\n-no-user-config \\\n-nodefaults \\\n-enable-kvm \\\n-rtc base=utc,clock=vm,driftfix=slew \\\n-no-reboot \\\n-display none \\\n-boot strict=on \\\n-m 128 \\\n-smp 1 \\\n-qmp unix:/var/run/hyper/vm-ESajskTODD/qmp.sock,server,nowait \\\n-chardev socket,id=charconsole0,path=/var/run/hyper/vm-ESajskTODD/console.sock,server,nowait \\\n-device sclpconsole,chardev=charconsole0 \\\n-device virtio-serial-ccw,id=virtio-serial0 \\\n-device virtio-scsi-ccw,id=scsi0 \\\n-chardev socket,id=charch0,path=/var/run/hyper/vm-ESajskTODD/hyper.sock,server,nowait \\\n-device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0 \\\n-chardev socket,id=charch1,path=/var/run/hyper/vm-ESajskTODD/tty.sock,server,nowait \\\n-device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1 \\\n-fsdev local,id=virtio9p,path=/var/run/hyper/vm-ESajskTODD/share_dir,security_model=none \\\n-device virtio-9p-ccw,fsdev=virtio9p,mount_tag=share_dir \\\n-daemonize \\\n-pidfile /var/run/hyper/vm-ESajskTODD/pidfile \\\n-D /var/log/hyper/qemu/vm-ESajskTOD.log\nI was just query the validity of some parameters:bash\n-append \"console=ttyS1 panic=1 no_timer_check\" \\\nand\n-chardev socket,id=charch0,path=/var/run/hyper/vm-ESajskTODD/hyper.sock,server,nowait \\\n-device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0 \\\n-chardev socket,id=charch1,path=/var/run/hyper/vm-ESajskTODD/tty.sock,server,nowait \\\n-device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1 \\\n```. @pmorjan Yes, I'm on version v0.8.1. And my qemu running parameters look the same as yours.\nMy host is on LPAR. For kernel and initrd, I simply download an iso from ubuntu offical website, and extract boot/kernel.ubuntu & boot/initrd.ubuntu from iso to /var/lib/hyper/kernel & /var/lib/hyper/hyper-initrd.img. Is that right?\nSince that I successfully run ubuntu installer on s390x with this two files, I take them for granted as kernel & hyper-initrd.img.. @pmorjan Ok, thank you! I will check it up.\nWhat's more, how did you build your kernel & hyper-initrd.img on s390x? If you can provide your kernel_config file at hyperstart project?\nThank you!. @pmorjan Thank you!!! It works for me!! :smile: \nWhat's more, I tried to build a linux kernel v4.12.4 based on your config, luckily, it still works!\nAlso, thank you @gnawux for your help! . @bergwolf Thanks! It solved my problem.\nFor those who have the same problem, my command is hyperctl run --memory=512 -d tomcat:8\nBy the way, what's the default memory size of hyper containers?. So can I specify my pod's dns like hyperctl run --dns=10.96.0.10 -t ubuntu:16.04?. Ok, then what should I do if I want to start another hyper/docker container in a hyper pod?\nWhen I use docker, I simply mount /var/run/docker.sock to my container and that works.. ",
    "pmorjan": "The console device is ttysclp0. Here is an old example of qemu parameters from of a working docker/ hyperstart/runv setup that includes the two unix sockets:\n-machine s390-ccw-virtio,accel=kvm,usb=off \\\n    -cpu host \\\n    -kernel /var/lib/runv/kernel \\\n    -initrd /var/lib/runv/initrd \\\n    -append \"console=ttyS1 panic=1 no_timer_check\" \\\n    -realtime mlock=off \\\n    -no-user-config \\\n    -nodefaults \\\n    -enable-kvm \\\n    -no-reboot \\\n    -display none \\\n    -boot strict=on \\\n    -m 512 \\\n    -smp 4 \\\n    -qmp unix:/var/run/hyper/vm-rnqlfpzLLe/qmp.sock,server,nowait \\\n    -device virtio-scsi-ccw,id=scsi0 \\\n    -chardev socket,id=charconsole0,path=/var/run/hyper/vm-rnqlfpzLLe/console.sock,server,nowait \\\n    -device sclpconsole,chardev=charconsole0 \\\n    -device virtio-serial-ccw,id=virtio-serial0 \\\n    -chardev socket,id=charch0,path=/var/run/hyper/vm-rnqlfpzLLe/hyper.sock,server,nowait \\\n    -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0 \\\n    -chardev socket,id=charch1,path=/var/run/hyper/vm-rnqlfpzLLe/tty.sock,server,nowait \\\n    -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1 \\\n    -fsdev local,id=virtio9p,path=/var/run/hyper/vm-rnqlfpzLLe/share_dir,security_model=none \\\n    -device virtio-9p-ccw,fsdev=virtio9p,mount_tag=share_dir \\\n    -daemonize \\\n    -pidfile /var/run/hyper/vm-rnqlfpzLLe/pidfile \\\n    -D /var/log/hyper/qemu/vm-rnqlfpzLLe.log\n. @xuchenhao001 It restored my old build environment and can confirm that the qemu config works as expected. I successfully ran a container via hyperctl and hyperd(v0.8.1) on s390x. This is the qemu command line of a running hyperd container:\n/usr/bin/qemu-system-s390x \\\n    -machine s390-ccw-virtio,accel=kvm,usb=off \\\n    -cpu host \\\n    -kernel /data/runv/dist/kernel \\\n    -initrd /data/runv/dist/initrd \\\n    -append \"console=ttyS1 panic=1 no_timer_check\" \\\n    -realtime mlock=off \\\n    -no-user-config \\\n    -nodefaults \\\n    -enable-kvm \\\n    -rtc base=utc,clock=vm,driftfix=slew \\\n    -no-reboot \\\n    -display none \\\n    -boot strict=on \\\n    -m 128 \\\n    -smp 1 \\\n    -qmp unix:/var/run/hyper/vm-FDpmPVFtfA/qmp.sock,server,nowait \\\n    -chardev socket,id=charconsole0,path=/var/run/hyper/vm-FDpmPVFtfA/console.sock,server,nowait \\\n    -device sclpconsole,chardev=charconsole0 \\\n    -device virtio-serial-ccw,id=virtio-serial0 \\\n    -device virtio-scsi-ccw,id=scsi0 \\\n    -chardev socket,id=charch0,path=/var/run/hyper/vm-FDpmPVFtfA/hyper.sock,server,nowait \\\n    -device virtserialport,bus=virtio-serial0.0,nr=1,chardev=charch0,id=channel0,name=sh.hyper.channel.0 \\\n    -chardev socket,id=charch1,path=/var/run/hyper/vm-FDpmPVFtfA/tty.sock,server,nowait \\\n    -device virtserialport,bus=virtio-serial0.0,nr=2,chardev=charch1,id=channel1,name=sh.hyper.channel.1 \\\n    -fsdev local,id=virtio9p,path=/var/run/hyper/vm-FDpmPVFtfA/share_dir,security_model=none \\\n    -device virtio-9p-ccw,fsdev=virtio9p,mount_tag=share_dir \\\n    -daemonize \\\n    -pidfile /var/run/hyper/vm-FDpmPVFtfA/pidfile \\\n    -D /var/log/hyper/qemu/vm-FDpmPVFtf.log\nI guess there is something wrong in your kernel / initrd setup.\nBut does it make sense not to follow upstream development of hyperd and stay on the backlevel 0.8.1 version? I did a quick look and it seems hyperd now depends on the lightweight kvmtool  which is not available on s390x (as far as I know). On the other hand the overhead of Qemu on s390x is smaller compared to x86 (no BIOS) and it's well tested. Making upstream hyperd work on s390x again could be a challenge. \n. @xuchenhao001 I'm not sure if your kernel will work at all. Does it have the 9p/virtio modules builtin? If not then you have to include them into your initrd and patch hyperstart/src/init to load them. 9pfs is the filesytem type used for the rootfs. . @xuchenhao001 The important config options are as follows:\nCONFIG_NET_9P=y\nCONFIG_NET_9P_VIRTIO=y\nCONFIG_VIRTIO_BLK=y\nCONFIG_VIRTIO_NET=y\nCONFIG_FSCACHE=y\nCONFIG_9P_FS=y\nSee the attached config for vanilla 4.8.0 that worked for me:\nconfig-4.8.0-s390x.gz\n$ ./hyperctl   run -t alpine sh\n/ # uname -a\nLinux alpine-2987341563 4.8.0 #6 SMP Wed Nov 22 12:38:13 CET 2017 s390x Linux\n/ # lsmod\nModule                  Size  Used by    Not tainted\n/ #. ",
    "m-barthelemy": "Hi,\nthanks for the quick reply.\nWill try a fresh nw build using the official tool. Closing this one.. Hi,\nIs there anything I could do to help fixing this one? Any additional information I could provide?\n. Negative, I get that error using both hyperctl and Hyperd API.. ",
    "yangwanli2017": "I have the same erros:\nE1225 04:46:07.084291   10970 qmp_handler.go:183] get qmp welcome failed: read unix @->/var/run/hyper/vm-OjTvltcBdS/qmp.sock: read: connection reset by peer\nE1225 04:46:07.084723   10970 qmp_handler.go:364] QMP initialize failed\nE1225 04:46:07.084745   10970 vm_states.go:207] SB[vm-OjTvltcBdS] read unix @->/var/run/hyper/vm-OjTvltcBdS/qmp.sock: read: connection reset by peer\nE1225 04:46:07.084751   10970 vm_states.go:164] SB[vm-OjTvltcBdS] Shutting down because of an exception: %!(EXTRA string=connection to vm broken)\nE1225 04:46:07.084868   10970 vm_states.go:144] SB[vm-OjTvltcBdS] Start POD failed: rpc error: code = Unavailable desc = transport is closing\nE1225 04:46:07.085770   10970 qemu_process.go:159] exit status 1\nE1225 04:46:07.086531   10970 sandbox.go:106] StartPod fail, response: &api.ResultBase{Id:\"vm-OjTvltcBdS\", Success:false, ResultMessage:\"Response Chan is broken\"}\nFailed to load the container after created, err: &os.PathError{Op:\"open\", Path:\"/run/runv/mycontainer/state.json\", Err:0x2}\n. qemu version-2.4.1 , and CentOS 7.2 .\nBut I use your release rpm:\nhyper-container-1.0.0-1.el7.centos.x86_64.rpm\nhyperstart-1.0.0-1.el7.centos.x86_64.rpm\nqemu-hyper-2.4.1-3.el7.centos.x86_64.rpm \n\nhyperd  errors:\nWARN[0000] devmapper: Usage of loopback devices is strongly discouraged for production use. Please use --storage-opt dm.thinpooldev or use man docker to refer to dm.thinpooldev section. \nWARN[0000] devmapper: Base device already exists and has filesystem xfs on it. User specified filesystem  will be ignored. \nINFO[0000] [graphdriver] using prior storage driver \"devicemapper\" \nINFO[0000] Graph migration to content-addressability took 0.00 seconds \nINFO[0000] Firewalld running: false                   \nINFO[0000] Loading containers: start.                   \nINFO[0000] Loading containers: done.                  \nE1226 06:21:34.732926   31949 qmp_handler.go:183] get qmp welcome failed: read unix @->/var/run/hyper/vm-yZzcdxDGVB/qmp.sock: read: connection reset by peer\nE1226 06:21:34.733495   31949 qmp_handler.go:364] QMP initialize failed\nE1226 06:21:34.732943   31949 json.go:141] read init data failed\nE1226 06:21:34.733521   31949 vm_states.go:372] SB[vm-yZzcdxDGVB] read unix @->/var/run/hyper/vm-yZzcdxDGVB/qmp.sock: read: connection reset by peer\nE1226 06:21:34.733539   31949 vm_states.go:329] SB[vm-yZzcdxDGVB] Shutting down because of an exception: %!(EXTRA string=connection to vm broken)\nE1226 06:21:34.733560   31949 json.go:175] SB[vm-yZzcdxDGVB] error when readVmMessage() for ready message: read unix @->/var/run/hyper/vm-yZzcdxDGVB/hyper.sock: read: connection reset by peer\nE1226 06:21:34.733029   31949 json.go:401] read tty data failed\nE1226 06:21:34.733581   31949 json.go:458] SB[vm-yZzcdxDGVB] tty socket closed, quit the reading goroutine: read unix @->/var/run/hyper/vm-yZzcdxDGVB/tty.sock: read: connection reset by peer\nE1226 06:21:34.733613   31949 json.go:601] SB[vm-yZzcdxDGVB] get hyperstart API version error: hyperstart closed\nE1226 06:21:34.733615   31949 vm_states.go:309] SB[vm-yZzcdxDGVB] Start POD failed: hyperstart closed\nE1226 06:21:34.733753   31949 run.go:34] ubuntu-6967856279: failed to add pod: hyperstart closed\nE1226 06:21:34.733764   31949 server.go:170] Handler for POST /v1.0.0/pod/create returned error: hyperstart closed\nE1226 06:21:34.733881   31949 qemu_process.go:159] exit status 1\n. ",
    "lemonli": "I got the same problem, and then I replace my qemu with https://hypercontainer-download.s3-us-west-1.amazonaws.com/qemu-hyper/qemu-hyper-2.4.1-3.el7.centos.x86_64.rpm.\nNow everything goes well. :)\n. ",
    "teawater": "The code has something wrong.. Have something wrong with build.. Have something wrong with build.. I really hate use master as work branch.  So close it and open other pr for the issue.. Use https://github.com/hyperhq/hyperd/pull/717 handle the issue.. @laijs r u ok with the new version? :). I can make sure that status lock is used safely.. What about set it inside your app?\nls -al /proc/sys/fs/inotify/max_user_watches\n-rw-r--r-- 1 root root 0 5\u6708   8 16:36 /proc/sys/fs/inotify/max_user_watches. With this patch, I got some qemu doesn't quit issue.. Hi @spearl ,\nI think the reason that you got the issue is after XPod.Remove timeout in https://github.com/hyperhq/hyperd/blob/60a4dd858f9a5db29e2878d67276134a93648505/daemon/pod/decommission.go#L61 , it will set the status of the pod to S_POD_NONE in https://github.com/hyperhq/hyperd/blob/60a4dd858f9a5db29e2878d67276134a93648505/daemon/pod/decommission.go#L71\nThen XPod.cleanup will give up clean the pod because the status of the pod is S_POD_NONE in https://github.com/hyperhq/hyperd/blob/60a4dd858f9a5db29e2878d67276134a93648505/daemon/pod/decommission.go#L551\nEven if the current patch can let XPod.Remove call decommissionResources release something.  But There is still another function p.removeSandboxFromDB() need to be called.\nSo what about update https://github.com/hyperhq/hyperd/blob/60a4dd858f9a5db29e2878d67276134a93648505/daemon/pod/decommission.go#L551 let XPod.cleanup can handle the S_POD_NONE but not just give it up?\nThanks,\nHui. Hi @spearl \nI would like to handle this issue in XPod.cleanup but not XPod.Remove because XPod.cleanup is designed to clean the resource usage when VM quit.\nI think If VM doesn't quit, just release the resource usage in XPod.Remove is not a good idea.\nAbout the code in XPod.cleanup.\nI think you can remove the check of S_POD_NONE in\nhttps://github.com/hyperhq/hyperd/blob/60a4dd858f9a5db29e2878d67276134a93648505/daemon/pod/decommission.go#L551\nAnd update following part to change p.status to S_POD_STOPPING if p.status is not S_POD_NONE.\nhttps://github.com/hyperhq/hyperd/blob/60a4dd858f9a5db29e2878d67276134a93648505/daemon/pod/decommission.go#L555\nThen following part become useable.\nhttps://github.com/hyperhq/hyperd/blob/60a4dd858f9a5db29e2878d67276134a93648505/daemon/pod/decommission.go#L574\nThanks,\nHui. @gnawux I tried to use the docker/distribution/reference when I made the patch.  But the docker/distribution/reference that is included by hyperd doesn't have proper code to handle the issue.\nThere are too many private elements around it.. @njlie @gnawux I reproduced this issue in my part and working on it.. @gnawux Updated commit log.\nAnd it passed the test-cmd.sh in travis-ci.\nI tried to test it in my local part but got too many network issues.. @gnawux \nI found ContainerCreate is the slower function than PodCreate with the high load test in the begin of this year.\nI think maybe the vmtemplate and vmcache help PodCreate a lot.. @gnawux the reason is inf.descript.Id is not set anywhere in hyperd.. @gnawux I made a mistake with this part.\nPlease see the comments of https://github.com/hyperhq/hyperd/pull/697/commits/a5ac58d322dc58ed52713dfcd3b4359d39c87348 about this.. ",
    "vbmade2000": "@preytaren Can you point to that part of doc ?. ",
    "preytaren": "@vbmade2000 example part on the container spec page\uff0c  https://docs.hypercontainer.io/reference/containers.html. ",
    "linengier": "a7c41708ef58 is my docker image which pull form my registry\nhyperctl images\nREPOSITORY                        TAG                 IMAGE ID            CREATED               VIRTUAL SIZE\n127.0.0.1:5000/linhaidong/hello   v1                  a7c41708ef58        2018-01-16 15:50:14   122.8 MB\n. when hyperd runing, all operation is normal. When I restart hyperd, the hyperctl list cmd can't list container which I create by running  hyperctl run cmd.\nhyperd runing:\n$ hyperctl list\nPOD ID              POD Name            VM name             Status\nhello3              hello3              vm-EcbpSCOwia       running\nhello4              hello4              vm-TpUJhFLJsC       running\n$ hyperctl stop hello3\nSuccessfully shutdown the POD: hello3!\n$ hyperctl list\nPOD ID              POD Name            VM name             Status\nhello3              hello3                                  failed\nhello4              hello4              vm-TpUJhFLJsC       running\n$ hyperctl start hello3\nSuccessfully started the Pod(hello3)\nhyperd restart:\n$ hyperctl list\nPOD ID              POD Name            VM name             Status\n$hyperctl start hello3\nhyperctl ERROR: Error from daemon's response: The pod(hello3) can not be found, please create it first\n$hyperctl run -d --name hello3 a7c41708ef58\nhyperctl ERROR: Error from daemon's response: Conflict. The name \"/hello3\" is already in use by container 88390a41f913fc76518d18292f26bb2aa113d13da890530e5aca3fabe837d347. You have to remove (or rename) that container to be able to reuse that name.\n. ",
    "joelmcdonald": "I had a similar issue with container name conflicts appearing in the hyperd INFO, WARNING and ERROR logs (/var/log/hyper/hyperd.*).\nIt turned out to be some orphan hosts (/var/lib/hyper/hosts/) and containers (/var/lib/hyper/containers/) hanging around after a hyperd service restart. These were enough to trigger the errors and cause contracts to fail, but would never appear in the POD list (hyperctl list)... just like you're describing.\nAfter I deleted everything from the hosts and containers directories, and restarted hyperd, everything worked fine.\nIt would be good if a hyperd service restart somehow triggered a graceful stop of any active PODs, and then started them again afterwards. Theory needs testing, but I think this would avoid creating the orphans.. good question, it was installed by remote script as part of setting up a codius host... https://codius.s3.amazonaws.com/hyper-bootstrap.sh\n'hyperctl info' shows Library Version: 1.02.146-RHEL7 (2018-01-22). Happy to try, I'll come back with the results. @gnawux I'm having trouble with the make... returns lots of \"cannot use description in field value\" type errors\nPre-requisites all installed ok, though I did have to add a symlink to resolve a failed file path.\nBelow path looks very circular... I get 10+ of these on screen, then make [2] fails with error 2, and make [1] fails with error 1.\nHave I missed something?\n[root@auscodius hyperd]# make\nMaking all in cmd\nmake[1]: Entering directory/src/github.com/hyperhq/hyperd/cmd'\nMaking all in hyperd\nmake[2]: Entering directory /src/github.com/hyperhq/hyperd/cmd/hyperd'\ngo build -gcflags=\"if [ \"\" != \"\" ]; then echo \"-N -l\"; else echo \"\"; fi\" -tags \"static_build    exclude_graphdriver_btrfs libdm_no_deferred_remove\" -ldflags \"-X github.com/hyperhq/hyperd/utils.VERSION=1.0.0 -X github.com/hyperhq/hyperd/utils.GITCOMMIT=git describe --dirty --always --tags 2> /dev/null || true`\" hyperd.go\ngithub.com/hyperhq/hyperd/vendor/github.com/hyperhq/hyperd/daemon/pod\n/usr/lib/golang/src/github.com/hyperhq/hyperd/vendor/github.com/hyperhq/hyperd/daemon/pod/persist.go:359:11: cannot use c.descript (type \"github.com/hyperhq/hyperd/vendor/github.com/hyperhq/hyperd/vendor/github.com/hyperhq/runv/api\".ContainerDescription) as type \"github.com/hyperhq/hyperd/vendor/github.com/hyperhq/hyperd/vendor/github.com/hyperhq/hyperd/vendor/github.com/hyperhq/runv/api\".ContainerDescription in field value`. Installed OK, service started OK...\nI'll load a pod, restart and see if it errors\n`[root@auscodius centos]# systemctl status hyperd\nhyperd.service - hyperd\n   Loaded: loaded (/usr/lib/systemd/system/hyperd.service; enabled; vendor preset: disabled)\n   Active: active (running) since Tue 2018-07-10 08:47:11 UTC; 11s ago\n     Docs: http://docs.hypercontainer.io\n Main PID: 12671 (hyperd)\n   CGroup: /system.slice/hyperd.service\n           \u2514\u250012671 /usr/bin/hyperd --log_dir=/var/log/hyper\nJul 10 08:47:11 auscodius.com systemd[1]: Started hyperd.\nJul 10 08:47:11 auscodius.com systemd[1]: Starting hyperd...\nJul 10 08:47:11 auscodius.com hyperd[12671]: time=\"2018-07-10T08:47:11Z\" level=warning msg=\"devmapper: Usage of loopback devices is strongly discouraged for production use. Please use --storage-opt dm.thinpooldev or us...pooldev section.\"\nJul 10 08:47:11 auscodius.com hyperd[12671]: time=\"2018-07-10T08:47:11Z\" level=warning msg=\"devmapper: Base device already exists and has filesystem xfs on it. User specified filesystem  will be ignored.\"\nJul 10 08:47:11 auscodius.com hyperd[12671]: time=\"2018-07-10T08:47:11Z\" level=info msg=\"[graphdriver] using prior storage driver \"devicemapper\"\"\nJul 10 08:47:11 auscodius.com hyperd[12671]: time=\"2018-07-10T08:47:11Z\" level=info msg=\"Graph migration to content-addressability took 0.00 seconds\"\nJul 10 08:47:11 auscodius.com hyperd[12671]: time=\"2018-07-10T08:47:11Z\" level=info msg=\"Firewalld running: true\"\nJul 10 08:47:11 auscodius.com hyperd[12671]: time=\"2018-07-10T08:47:11Z\" level=info msg=\"Loading containers: start.\"\nJul 10 08:47:11 auscodius.com hyperd[12671]: time=\"2018-07-10T08:47:11Z\" level=info msg=\"Loading containers: done.\"\nHint: Some lines were ellipsized, use -l to show in full.`. Unfortunately, the issue is still there in the new rpms.\n\nupgraded and started hyperd\nsuccessfully loaded test POD\nhyperctl list showed POD active\nrestarted hyperd\nhyperctl list showed nothing\nhosts and containers were still present in /var/lib/hyper/hosts/ and /var/lib/hyper/containers/\nlogs confirm name conflict error (below)\n\n[root@auscodius centos]# more /var/log/hyper/hyperd.INFO\nLog file created at: 2018/07/10 08:52:36\nRunning on machine: auscodius\nBinary: Built with gc go1.8 for linux/amd64\nLog line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg\nI0710 08:52:36.477023   13221 config.go:39] config file: %!(EXTRA string=/etc/hyper/config)\nI0710 08:52:36.477346   13221 config.go:74] [/etc/hyper/config] config items: &types.HyperConfig{ConfigFile:\"/etc/hyper/config\", Root:\"/var/lib/hyper\", Host:\"\", GRPCHost:\"\", StorageDriver:\"\", StorageBaseSize:\"\", VmFactoryPolicy:\"\", Driver:\"\n\", Kernel:\"/var/lib/hyper/kernel\", Initrd:\"/var/lib/hyper/hyper-initrd.img\", Bridge:\"\", BridgeIP:\"\", DisableIptables:false, EnableVsock:false, DefaultLog:\"\", DefaultLogOpt:map[string]string{}, logPrefix:\"[/etc/hyper/config] \"}\nI0710 08:52:36.631031   13221 daemon.go:225] The hypervisor's driver is\nI0710 08:52:36.653608   13221 migration.go:23] Migrate lagecy persistent pod data, found: 0, migrated: 0\nI0710 08:52:36.653858   13221 persist.go:57] layout loading finished\nE0710 08:52:36.654224   13221 persist.go:100] Pod[l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa] failed to load inf info of : leveldb: not found\nW0710 08:52:36.654536   13221 daemon.go:82] Got a unexpected error when creating(load) pod l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa, leveldb: not found\nI0710 08:52:36.654706   13221 hyperd.go:189] Hyper daemon: 1.0.0\nI0710 08:52:44.537987   13221 list.go:94] got list request for pod (pod: , vm: )\nE0710 08:53:15.119353   13221 server.go:170] Handler for GET /pod/info returned error: Can not get Pod info with pod ID(l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa)\nI0710 08:53:15.122847   13221 vm_states.go:301] SB[vm-YhvXsEcVAg] startPod: &json.Pod{Hostname:\"l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa\", DeprecatedContainers:[]json.Container(nil), DeprecatedInterfaces:[]json.NetworkInf(nil),\nDns:[]string(nil), DnsOptions:[]string(nil), DnsSearch:[]string(nil), DeprecatedRoutes:[]json.Route(nil), ShareDir:\"share_dir\", PortmappingWhiteLists:(*json.PortmappingWhiteList)(0xc4208195f0)}\nI0710 08:53:19.510273   13221 vm_states.go:304] SB[vm-YhvXsEcVAg] pod start successfully\nI0710 08:53:19.510309   13221 provision.go:285] Pod[l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa] sandbox init result: <nil>\nE0710 08:53:19.512929   13221 container.go:411] Pod[l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa] Con[(l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa__app)] Conflict. The name \"/l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5m\nlioa__app\" is already in use by container 0931f8ea35412777a4bd22e76a50f52ccd760383d55dc91cc4b3eb6df2583e6a. You have to remove (or rename) that container to be able to reuse that name.\nI0710 08:53:19.512963   13221 vm_states.go:332] SB[vm-YhvXsEcVAg] poweroff vm based on command: vm.Kill()\nE0710 08:53:19.512982   13221 run.go:34] l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa: failed to add pod: Conflict. The name \"/l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa__app\" is already in use by container 0931f8ea3541277\n7a4bd22e76a50f52ccd760383d55dc91cc4b3eb6df2583e6a. You have to remove (or rename) that container to be able to reuse that name.\nE0710 08:53:19.512997   13221 server.go:170] Handler for POST /pod/create returned error: Conflict. The name \"/l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa__app\" is already in use by container 0931f8ea35412777a4bd22e76a50f52ccd76038\n3d55dc91cc4b3eb6df2583e6a. You have to remove (or rename) that container to be able to reuse that name.\nI0710 08:53:19.513193   13221 qemu_process.go:93] kill Qemu... 13334\nI0710 08:53:19.513243   13221 context.go:199] SB[vm-YhvXsEcVAg] VmContext Close()\nI0710 08:53:19.513277   13221 qmp_handler.go:344] quit QMP by command QMP_QUIT\nE0710 08:53:19.513293   13221 qmp_handler.go:141] QMP exit as got error: read unix @->/var/run/hyper/vm-YhvXsEcVAg/qmp.sock: use of closed network connection\nI0710 08:53:19.513335   13221 decommission.go:536] Pod[l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa] got vm exit event\nI0710 08:53:19.513342   13221 etchosts.go:97] cleanupHosts /var/lib/hyper/hosts/l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa, /var/lib/hyper/hosts/l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa/hosts\nE0710 08:53:19.513395   13221 json.go:601] SB[vm-YhvXsEcVAg] get hyperstart API version error: hyperstart closed\nE0710 08:53:19.513427   13221 json.go:141] read init data failed\nE0710 08:53:19.513439   13221 json.go:601] SB[vm-YhvXsEcVAg] get hyperstart API version error: hyperstart closed\nW0710 08:53:19.513450   13221 hypervisor.go:47] SB[vm-YhvXsEcVAg] keep-alive test end with error: hyperstart closed\nE0710 08:53:19.513482   13221 json.go:401] read tty data failed\nE0710 08:53:19.513496   13221 json.go:458] SB[vm-YhvXsEcVAg] tty socket closed, quit the reading goroutine: read unix @->/var/run/hyper/vm-YhvXsEcVAg/tty.sock: use of closed network connection\nI0710 08:53:19.517880   13221 server.go:388] getting image: calerobertson/test-manifest@sha256:7c8b236c5f01e5abc78c9500cce7c974ffcedd980e60c2d3a5f1a44c8455ae2d:\nI0710 08:53:19.519579   13221 decommission.go:578] Pod[l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa] pod stopped\nI0710 08:53:22.206354   13221 server.go:406] got image: calerobertson/test-manifest@sha256:7c8b236c5f01e5abc78c9500cce7c974ffcedd980e60c2d3a5f1a44c8455ae2d\nI0710 08:53:22.207930   13221 server.go:388] getting image: docker.coil.com/codius-moneyd@sha256:4c02fc168e6b4cfde90475ed3c3243de0bce4ca76b73753a92fb74bf5116deef:\nI0710 08:53:23.005769   13221 server.go:406] got image: docker.coil.com/codius-moneyd@sha256:4c02fc168e6b4cfde90475ed3c3243de0bce4ca76b73753a92fb74bf5116deef\nI0710 08:53:23.007600   13221 vm_states.go:301] SB[vm-njySycyLVm] startPod: &json.Pod{Hostname:\"l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa\", DeprecatedContainers:[]json.Container(nil), DeprecatedInterfaces:[]json.NetworkInf(nil),\nDns:[]string(nil), DnsOptions:[]string(nil), DnsSearch:[]string(nil), DeprecatedRoutes:[]json.Route(nil), ShareDir:\"share_dir\", PortmappingWhiteLists:(*json.PortmappingWhiteList)(0xc42083b1d0)}\nI0710 08:53:27.387600   13221 vm_states.go:304] SB[vm-njySycyLVm] pod start successfully\nI0710 08:53:27.387630   13221 provision.go:285] Pod[l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa] sandbox init result: <nil>\nE0710 08:53:27.390305   13221 container.go:411] Pod[l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa] Con[(l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa__app)] Conflict. The name \"/l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5m\nlioa__app\" is already in use by container 0931f8ea35412777a4bd22e76a50f52ccd760383d55dc91cc4b3eb6df2583e6a. You have to remove (or rename) that container to be able to reuse that name.\nI0710 08:53:27.390330   13221 vm_states.go:332] SB[vm-njySycyLVm] poweroff vm based on command: vm.Kill()\nE0710 08:53:27.390345   13221 run.go:34] l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa: failed to add pod: Conflict. The name \"/l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa__app\" is already in use by container 0931f8ea3541277\n7a4bd22e76a50f52ccd760383d55dc91cc4b3eb6df2583e6a. You have to remove (or rename) that container to be able to reuse that name.\nE0710 08:53:27.390358   13221 server.go:170] Handler for POST /pod/create returned error: Conflict. The name \"/l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa__app\" is already in use by container 0931f8ea35412777a4bd22e76a50f52ccd76038\n3d55dc91cc4b3eb6df2583e6a. You have to remove (or rename) that container to be able to reuse that name.\nI0710 08:53:27.390481   13221 qemu_process.go:93] kill Qemu... 13348\nI0710 08:53:27.390529   13221 context.go:199] SB[vm-njySycyLVm] VmContext Close()\nI0710 08:53:27.390555   13221 qmp_handler.go:344] quit QMP by command QMP_QUIT\nE0710 08:53:27.390571   13221 qmp_handler.go:141] QMP exit as got error: read unix @->/var/run/hyper/vm-njySycyLVm/qmp.sock: use of closed network connection\nI0710 08:53:27.390611   13221 decommission.go:536] Pod[l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa] got vm exit event\nI0710 08:53:27.390618   13221 etchosts.go:97] cleanupHosts /var/lib/hyper/hosts/l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa, /var/lib/hyper/hosts/l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa/hosts\nI0710 08:53:27.390626   13221 etchosts.go:101] cannot find /var/lib/hyper/hosts/l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa/hosts\nI0710 08:53:27.390671   13221 decommission.go:578] Pod[l2xvchk27rbrnh3mc3y4p3iaeoonjzur2u24qxuod2iaqd5mlioa] pod stopped\nE0710 08:53:27.390682   13221 json.go:601] SB[vm-njySycyLVm] get hyperstart API version error: hyperstart closed\nE0710 08:53:27.390706   13221 json.go:401] read tty data failed\nE0710 08:53:27.390714   13221 json.go:458] SB[vm-njySycyLVm] tty socket closed, quit the reading goroutine: read unix @->/var/run/hyper/vm-njySycyLVm/tty.sock: use of closed network connection\nE0710 08:53:27.390730   13221 json.go:141] read init data failed\nE0710 08:53:27.390737   13221 json.go:601] SB[vm-njySycyLVm] get hyperstart API version error: hyperstart closed\nW0710 08:53:27.390744   13221 hypervisor.go:47] SB[vm-njySycyLVm] keep-alive test end with error: hyperstart closed. Pretty sure that's what I did... stopped and removed all PODs, stopped hyperd, deleted all hosts and containers, installed new rpms with --force option, started hyperd, etc per above\nDid I miss anything?. sorry... just read the second half of your message. I'll purge those files, apply the new binaries and retest\n. OK, purged /var/lib/hyper/* but left kernel and hyper-initrd.img, reinstalled new binaries, started service and retested...\nThe issue is still there. Logs report container name conflict, old containers and host are still present, but nothing shows in hyperctl list. No rush, it will be 24 hours before I have a chance to retest any further updates.\nWhat's the path for leveldb file?. Within /var/lib/hyper/lib/hyper.db I have 3 .ldb files, 1 .log file, and a CURRENT, LOCK, LOG and MANIFEST-000010 file.\nOnly the log file has been updated since the service was restarted.\nhyperd service status\nhyperd:     active (running) since Tue 2018-07-10 10:11:02 UTC; 1h 13min ago\nfile list of hyper.db\n[root@auscodius hyper.db]# ls -la\ntotal 36\ndrwxr-xr-x 2 root root  139 Jul 10 10:11 .\ndrwxr-xr-x 3 root root   50 Jul 10 09:57 ..\n-rw-r--r-- 1 root root 4205 Jul 10 10:01 000002.ldb\n-rw-r--r-- 1 root root  192 Jul 10 10:08 000005.ldb\n-rw-r--r-- 1 root root 5790 Jul 10 10:11 000008.ldb\n-rw-r--r-- 1 root root  152 Jul 10 10:12 000009.log\n-rw-r--r-- 1 root root   16 Jul 10 10:11 CURRENT\n-rw-r--r-- 1 root root    0 Jul 10 09:57 LOCK\n-rw-r--r-- 1 root root 2301 Jul 10 10:11 LOG\n-rw-r--r-- 1 root root  657 Jul 10 10:11 MANIFEST-000010\n. ",
    "1maginarium": "Any luck on this?. hyper-container.x86_64 1.1.0-1.el7 installed hyperstart.x86_64 1.1.0-1.el7 installed qemu-hyper.x86_64 2.4.1-3.el7.centos installed\nThis issue is very easily reproduced by simply stopping or restarting hyperd / the server while a pod is running.\nThen they turn into stale, zombie pods, that cannot be seen in hyperctl list only hyperctl info under pods. They also show up in systemctl status hyperd -l as to their pod names, but can not be found or deleted under any circumstance. This prevents self-test from being successful, and prevents any pods from actually being able to run in HyperD.. ",
    "svarlamov": "Running it within the container while it's actually running seems to work (thanks Hyper.sh support):\nbash\necho fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf && sudo sysctl -p\nHowever, it doesn't seem like we have a way to set this at build-time.... @teawater That also works. For now I just have it our entrypoint script. Opted out of including it in the source as that would not be nice for users/devs who run locally. I have also tried the exact 'fix' described in the following reddit post for consul on windows -- which is very similar in that it's VM-based -- but I get the same error as before...\nhttps://www.reddit.com/r/docker/comments/89nhkw/i_am_trying_to_run_a_containerized_consul_on/\n-p 8300:8300 -p 8301:8301 -p 8301:8301/udp -p 8302:8302/udp -p 8302:8302 -p 8400:8400 -p 8500:8500 -p 53:53/udp. ",
    "lifupan": "close this pr and resend later.. ",
    "enzian": "@bergwolf any pointers as to where I can find the mountpoints needed?. hmm, I'll try that one and report back here :-). ok, here's what I've tried so far:\nI used the docker:stable-dind image and wantet to run this like this:\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  labels:\n    run: my-shell\n  name: my-shell\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: my-shell\n  strategy:\n  template:\n    metadata:\n      labels:\n        run: my-shell\n    spec:\n      containers:\n      - args:\n        - sh\n        image: docker:stable-dind\n        imagePullPolicy: Always\n        name: my-shell\n        resources:\n          limits:\n            cpu: 500m\n            memory: 1048Mi\n        volumeMounts:\n        - mountPath: /sys/fs/cgroup\n          name: cgroup-storage\n        - mountPath: /var/run\n          name: docker-sock\n      volumes:\n      - emptyDir: {}\n        name: cgroup-storage\n      - emptyDir: {}\n        name: docker-sock\nThen I attach to the pod in kubernetes and try running dockerd which then fails with:\nWARN[2018-06-19T14:56:45.325877326Z] could not change group /var/run/docker.sock to docker: group docker not found\ncan't create unix socket /var/run/docker.sock: listen unix /var/run/docker.sock: bind: no such device or address\nI'm not sure what's going wrong and google did not help much so far. Why would dockerd not create a socket in this directory?. ",
    "spearl": "Thanks for getting back so quickly @gnawux. I put something together quickly ^ that I'd love any feedback on. Hi @teawater , thanks for responding! \np.removeSandboxFromDB() is actually already being called synchronously inside of the call to p.removeFromDB() here: https://github.com/hyperhq/hyperd/blob/60a4dd858f9a5db29e2878d67276134a93648505/daemon/pod/persist.go#L243\nSo the only missing call is the one to p.decommissionResources().\nIf we remove the check for S_POD_NONE do we still need to check for S_POD_STOPPED? Would we still need to change the status to S_POD_STOPPING in the else? What purpose does this check serve? The comment explaining it above is cut off mid sentence.\nAlso, what is point of this check? https://github.com/hyperhq/hyperd/blob/60a4dd858f9a5db29e2878d67276134a93648505/daemon/pod/decommission.go#L574\nThe status will never be S_POD_NONE at this point since it always gets changed to S_POD_STOPPING above. Which means that the status will get changed from S_POD_NONE to S_POD_STOPPED if we simply remove the check above.. Sounds good. Just made those changes and took out the sync call in the remove() method. ",
    "traviscrist": "For this release #733 was merged in, is #685 still in testing? . Any updates on the timeline for the 1.1 release? Thanks!. Any updates on the 1.1 release? Thanks!. @gnawux any updates on the plans for a 1.1 hyperd release? Thanks!. ",
    "Chen8132": "I think no log is about this issue. Just stop there when I execute hyperctl load -i /tmp/a.tar -r aaa. I actually do not know what is the use of -n option and -r option and want to figure it out.. My tar file is from docker save. Maybe it causes the problem?. I find hyperctl exec and attach command do not get respond too.. I don't think attach command has -t option.. exec also doesn't work with -t option. Both attach and exec still do not work after I try what you said. I don't think they will make it in my server. My system is centOS 7. . I have tried ubuntu image and centos image. Nothing in logs.. just show leveldb: not found when it starts. ",
    "sjkeerthi": "guest platform . ",
    "jonaagenilsen": "Not delicate.. but\n```\nyum list installed | egrep \"hyper-|hyperstart\"\nhyper-container.x86_64               1.1.0-1.el7                       installed\nhyperstart.x86_64                    1.1.0-1.el7                       installed\n```. ",
    "opahopa": "yeah know that, thanks. wandering about existing of any kind of --version flag in hyperctl/hyperd, since running yum list installed | egrep \"hyper-|hyperstart\" certainly must take more execution time. (in my use case it is relatively important). > And checked with the code\n\n```\nhyperctl version\nThe hyperctl version is 1.1.0\n```\nIt's supported\n\nhaha thanks! tried different options but not without -. (since it's not in help / docs). ",
    "kuthreecar": "@bergwolf\nThanks, since I was deployed it on aws, I relaunched a instance and reinstalled hyperd. now have no problem for that, I may close this issue first, thanks.. "
}
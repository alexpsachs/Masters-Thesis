{
    "saileshmittal": "@kramasamy check now.\n. shipit\n. shipit. Just add a readme file in scripts/ to say what \"compile\" directory is for at a high level.\n. @kramasamy Can also fix the tracker_unittest?\n. Apart from minor nits, it looks good. Merge after fixing them.\n. One of the challenges is that only one stream is considered. The scenario when a component consumes or emits more than one stream is not even handled, and need to thought through.\n. Fixed in #243 \n. shipit\n. shipit\n. shipit\n. shipit\n. LGTM, merging\n. Merging. Not waiting for Travis since its just docs being changed.\n. Thanks for doing this. Merging.\n. Delete the comments, then shipit \ud83d\udc4d \n. Fixed in #385 \n. This includes changes in the SPI. Won't this break existing scheduler code?\nIf yes, we probably should keep the existing SPI, and add new interfaces for serial calls. OR we break the SPI this time and communicate to others.\nIn either case, may we should keep the async version around as well.\n. ok \ud83d\udc4d \n. Fixes #385 and #226 \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. The tuning section is very small, because it refers to the other sections, like backpressure and spout lag. That is the reason I included it here. An in-depth tuning section can be added later. This is just a primer on tuning. Let me know if you still think that this can be a independent section.\nThese are all the guidelines that we provide internally (minus summingbird/tsar). Please let me know if I missed anything.\n. Troubleshooting and tuning sections are expected to grow over time. It would be hard to put everything here in one shot.\nPlease review the format it is written in. Anyone who adds to these guides should follow a single format.\n. @kramasamy Checked and confirmed. It skips over them and UI only shows the topologies with new versions.\n. @billonahill , so the idea is that since we are early on in the process, we can make breaking changes now rather than later.\nNote that this is a breaking change, and any older topology (with previous execution state) can not be read by new Tracker that is based on this execution state.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d thanks for fixing these, @lewiskan . I'll add some more documentation and add you as reviewer in those.\n. \ud83d\udc4d \n. No, it is not okay. Thats why I pixelated those. I have checked all in /website/src/img/ and these were the only two that had some internal info.\nIf we want to remove them complete from history, it will be a little messy, because one would need to force push the whole repo, including tags, etc, and then others would need to rebase their repo, not merge it.\nThis is a tool for that (https://rtyley.github.io/bfg-repo-cleaner/). If you think that this is sensitive and needs to be done, let me know and I'll do it. \ncc @kramasamy \n. \ud83d\udc4d \n. lgtm \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. Dropping this because it should fail Tracker, as it happens now. All changes should be backward compatible to proto, otherwise there might be some weird errors.\n. @joestein Looks like this PR was closed along with others when we cleaned up the repo. Please see this thread for more details (https://groups.google.com/forum/#!topic/heron-users/Uuvi9jnsFg4) and reopen the PR when ready.\n. No blocked. I will test it today, and if all goes well, I will send an email on mailing list to notify everyone and do it about two hours after that, or may be after 24 hours.\n. @lucperkins @kramasamy Requesting for comments.\n. @lucperkins There are two parts to UI documentation. One is the runbook which lists how to deploy UI, and configurations. This is meant for the admins of UI, and it need not contain how the UI looks like. The second part is usage, which is more for topology developers, who want to use UI to get info about their topologies. They wouldn't want to know how to deploy the UI themselves.\nThis doc is the second one - usage. The one that you pointed to would be the runbook.\n. @billonahill addressed all the above comments. Can you check once more?\nThanks.\n. @lucperkins Yes, that will be my next PR. Wanted to make changes in modules. The next PR will remove redundant info from runbook page, and add info about some of the configs. I'll also add link in each doc to the other one.\n. @billonahill , @lucperkins do you guys have any more comments?\n. \ud83d\udc4d \n. Apart from minor nits, LGTM \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @billonahill Unfortunately, we don't have integration tests for tracker and UI, but I have verified that it works internally.\n. Had cleaned up local machine states and the master worked. Closing this PR since it is not required.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @kramasamy @billonahill @lucperkins Could you guys take a look?\n. @lucperkins Addressed comments.\n. @alanngai We use 2 space indent. Could you change your setting to take that into account? It will decrease the diff length quite a bit.\nAlso, please try to create multiple PRs for different modules, so it will be easier to review.\n. Yes, please go ahead and change the rules.\n1. Indentation to 2 spaces\n2. Ignore imports. This is because of the way bazel and pex work. The imports have to have the scope starting from root directory of project.\n3. Ignore Exception rules. Most python code is lenient on exceptions.\n4. Length should be 100 chars per line. We need to fix them if they are violated.\n5. Unused params should be deleted.\nPlease create issues for the parts of codes that are not straightforward.\nIf you know how to enable these checks based on modules (cli or tracker or ui or common), we should enable it selectively for those that have been taken care of. Otherwise we'll enable once everything is compliant.\n. I would recommend option 2 (relax that check) and creating an issue.\n. \ud83d\udc4d \n. @billonahill could you create an issue and link that with this TODO?\n. @kramasamy Comma-separated list is not yet allowed. #763 only allows \"host:port\" format for a single ZK server.\n. @maosongfu , @billonahill could you take a look at this PR?\n. Wrong PR\n. \ud83d\udc4d \n. @samek Tracker does not create the nodes automatically if they are not already present. The latest version of config file states that:\n(https://github.com/twitter/heron/blob/master/heron/config/src/yaml/tracker/heron_tracker.yaml)\n``` yaml\n\nTo use 'localzk', launch a zookeeper server locally\nand create the following paths:\n1. /heron/topologies\n2. /heron/executionstate\n3. /heron/pplans\n4. /heron/tmasters\n5. /heron/schedulers\n\n-\ntype: \"zookeeper\"\nname: \"localzk\"\nhostport: \"localhost:2181\"\nrootpath: \"/heron\"\ntunnelhost: \"localhost\"\n```\nSo you would need to create those nodes to have Tracker work. One of the reasons we do it this way is that Tracker is completely readonly and does not modify any of the states.\n. @lucperkins This is better. But the sidebar still scrolls to the top after a click. Can it remain at its current position?\n. If you scroll down to the last dropdown menu, and select a topic from there, the sidebar will jump back to the top. Although the dropdown would still be open, you would need to scroll down to go to the selected topic.\n. \ud83d\udc4d \nThis is much better. There is still a small delay when it opens the dropdown though.\n. @lucperkins okay, lgtm. shipit.\n. @alanngai , As I mentioned before, we are not following any particular python code styles, but we will be following twitter guidelines eventually. One of the things I wanted to mention is the 2-space indentation, that you have already done, and 4-space indent for wrapped lines, like lines following an open bracket. Can you add that check as well?\n. Otherwise LGTM. Can you also fix why travis build is failing?\n. Can you add a comment as to why this is needed?\n. Remove these twitter specific configs.\n. Remove twitter specific configs\n. Remove twitter specific configs\n. Packer is not available outside twitter. Remove these commands. This may make this aurora file unusable, but it should not be here.\n. Ditto with Packer.\n. There will only be more questions about Packer, how it works, how can it be replaced, etc.. If you can remove Packer and still keep a sane aurora file, then that might work. Packer must be removed for sure.\n. Sort the import modules.\n. Sort\n. Is there a change in these lines?\n. May be point to https://github.com/twitter/heron\n. \"import define and options\" should be removed since it is not used anymore.\n. Please import in separate lines since we are doing that everywhere. Also sort them.\n. We don't have \"dc\" anymore, right?\n. Don't use this styling. Use python's standard docstring style instead. See how other functions have docstrings.\ndef func:\n  \"\"\"\n  Description of what this function does.\n  \"\"\"\n. I see this comment is not for this function after all. Scratch the previous comment. Can you leave one or two blank lines between this and next function?\n. Added.\n. These are over the network calls to other components which we haven't gotten to mock yet.\n. Change function name so that we know its a check.\nExample, check_java_home_set or is_java_home_set..\n. Use the standard way of printing an error.\nExample, \"JAVA_HOME not set\"\n. I think you forgot to push the commit.\n. sort\n. import Log as Log is redundant. Just import Log is enough.\n. remove as Log\n. Why error?\n. If this is not an error, may be return True?\n. return True?\n. remove as Log\n. Should this change to use Log?\n. remove as Log\n. remove comment\n. remove as Log\n. remove as Log\n. This will fail anywhere other than your Mac. Can you make this generic? Like relative to home dir?\n. Typo?\n. Will this work outside Twitter env?\n. Remove commented out code\n. Should it also take additional args that a topology might take?\n. Since all positional arguments and options for the topology main (the \"unknown args\" in cli) are passed through this means, adding just the java env variables would be inconsistent with that behavior.\nThe current behavior is that all the arguments and options that need to go to the main method are part of cli submit command.\n. on ~~a~~\n. Is this intentional?\n. Move this to a separate function, preferably in PackerUtils class. The function would return pkg from a uri. It should also verify that URI is valid. This line can throw an IndexOutOfBoundException.\n. Make a function for this, like above.\n. Make heron.aurora is a constant variable. Hard coding the filename might not be a good idea, but let it be a there for now. We may want to pick up that name from configs later on.\n. IS_PRODUCTION\nAlso, use toString or String.valueOf method instead of bool + \"\" to convert bool to str.\n. This comment can be removed. I think the TODO comment above this can also be removed.\n. There should be no execution state written yet, right?\n. Should delete topology here, since it was written before.\n. Is there a synchronous statemgr call available? It looks like we create a future and immediately wait for it. This is a lot of code duplication that we can avoid if we wrap it in a util function or have a sync method.\n. I see a few issues with this approach:\n1. lock/unlock/acquire/release are terms often used with multithreading. Overloading them here is quite confusing.\n2. acquire and delete have side effects other than just acquiring/releasing some lock construct. These actually write and delete data from ZK nodes.\n3. The acquires and releases are happening from different files. There is no clear owner of the \"lock\".\nA better way to model this problem might be \"initiateSubmission\", \"commitSubmission\" and \"rollBackSubmission\". Killing topology can similarly have initiation and commit phases.\n. Checking for \"existsOrNot\" is not similar to concurrent programming. For example, file locks may be able to restrict users from writing to and reading from a given file, but they are never used to check whether a file exists or not. \nOur use case is to check for existence of a topology. If present, abort, if absent, go ahead. Encapsulating it inside locks doesn't feel right IMHO.\n. This is heron-shell url. It does not depend on the schedulers.\n. done\n. -D looked fine to me. topology-jvm-property is ambiguous, in the sense that this property is passed only for local execution, and not used thereafter.\nI recommend discarding this PR and keeping the option as is for now.\n. Added comment. The user should not be able to see outside this dir for sandboxing reasons. Hence absolute path and parent accessing is disallowed.\n. Sorted for all sections of this file.\n. Good point. Also updated the comments.\n. Okay. Could you rename it to something like @maosongfu asked? I also feel that the name --topology-jvm-property is a bit confusing.\n. Not picking up one, but assuming that we pick, lets say --topology-main-jvm-property, then the command would look something like\nheron submit local topology.tar arg1 --topology-main-jvm-propertyhello=world\nIs that the right intention?\n. If this was working without this line, why do we need this?\n. Will it work with any other python? like python2.6 ?\n. Can you change dc to cluster? Since that is what we are now calling it.\n. Same as above - dataCenter -> cluster\n. LGTM. Shipit after renaming. \ud83d\udc4d \n. Is this not being read anymore?\n. Saw your dummyCliPath change. That was what I was expecting.\nGood to merge.\n. Okay. --topology-main-jvm-property sounds good. Can you change the way these options are parsed to resemble what you mentioned above?\n. Change javaDefines variable name.\n. Done.\n. +1 for || true. Nice trick.\n. I removed that code, and some other files that are not being used anymore.\n. What does m stands for mBolt?\n. incrValue never used?\n. Rename to builder\n. Rename\n. Rename\n. This looks like a bug to me. It should increment the value somewhere. May be @maosongfu or @vikkyrk know about this?\nIf we don't know, can you add a comment with a FIXME and link a issue with this?\n. I agree, but this PR may not be the right place to do that. Thats why I was suggesting creating an issue and linking it with a FIXME comment, and then we can dig deeper as to what this method does.\nI would recommend against a logical change in these kind of PRs.\n. Try to change as much as you can. These will never get changed in future. If you change at 2-3 places, that would be 2-3 places with better naming. So only limit to the variables that this PR touches.\n. Is it okay to continue even if isSuccessful is false?\n. @maosongfu optional is fine. I don't think there would be more than one link per topology.\n. Yes. Currently the viz link is generated by tracker itself, using the config. The idea is viz links would be same irrespective of schedulers, so probably it is ok to leave that in tracker.\n. There should be a check after this too.\n. @billonahill  This is the next image - clicking on a component. And the next is clicking on an instance. Should I add a note in 7 and 8, like \"See below\"?\n. Some have 2 space indentation while some have 4 in this file..\n. Extra line in the beginning. Also, we should add the OSS comments here.\n. Class variables should start with capital letter. May be all caps?\n. Convert to docstring?\n. 1. The outer bracket is redundant. Please delete it.\n2. You don't need to escape single quotes, since they are inside double quotes, and python can handle them.\n3. Make the sentences in comments start with capital letters. It will be easier to know where a sentence begins, and where is the continuation from previous line.\n. That might suggest an idea that this has to be run from command line somewhere. The image is just to exhibit what is expected. Besides, it would be much easier to add an image if something changes - imagine something more visual than a command line.\n. Do you have any suggestions? The idea is that these \"circles\" can be clicked. I am not sure what else to call them here. The image helps in understanding this by having an arrow pointing to the circle.\nHow about The circles can be clicked for...?\n. Rename to toBeOverwritten or overwrite\n. toBeOverwritten or overwrite\n. This file is just renamed. I have not changed its content. It has a lot of old and incomplete information. I thought we could do them in two different PRs, but I am working on completing this, so this PR will be ready for review in a couple more hours.\n. I think those are earlier docs. I have changed it to --- in the files I touched. Its more markdown-ish to use --- instead of &mdash;.\n. I created the links, but when I click them, the actual place is hidden behind the top bar. @lucperkins or @msbarry , do you know if we can do something about that?\n. That should be a part of concepts documentation. \n. Good catch. Also added description for inputs.\n. Thats a typo. component is required, but instance can be repeated or omitted.\n. A 4-space indent makes it a fixed-width font.\n. The parameters for the query language become more obvious as we go forward, so I think the examples are sufficient. For example, RATE takes a multivariate timeseries as argument, then it will be redundant to type RATE(<multivariate>) followed by RATE(TS(...)).\nI think the examples are sufficient.\n. Missed those. Updated.\n. That would break the concept of a \"cluster\" as such. The assumption here is that if there are more than one ZK servers, they will be different \"clusters\". Do you think there is a scenario where this assumption is wrong?\n. Created a ticket to add the support later: https://github.com/twitter/heron/issues/767\n. Seems its not easy to customize the output format. If you can make it work with the RawDescriptionHelpFormatter, then this would not be needed. Otherwise, its okay to have it.\n. +1 for refactoring. Should we call it cli_helper or just helper?\n. 4 spaces.\n. Put it in alphabetical order.\nIn other places, we used globs so that we don't need to add every file manually.\nsrcs = glob(\n        [\"**/*.py\"],\n        exclude = [\"main.py\"],\n    ),\nI won't mind doing that here either.\n. ",
    "kramasamy": "@joestein - The file env_exec.sh in scripts is generated. You need to first run \n./bazel_configure.py\nin your repo before kicking off the build.\n. @joestein - Can you provide a brief summary about this commit? \n. Can you also mention which flavor of Linux and flavor you are building? Based on those, you might have to add some entries in tools/bazel.rc for linking some libraries such as libunwind, librt etc. \n. You can build directly the release packages using\nbazel build --config=<platform> release:packages\nYesterday, I checked in some fixes to build these packages using bazel builtin's pkg_tar\n. If the vagrant image has limited resources such as memory or cores, you might have to pass other options to bazel as well. This is because bazel is very aggressive in spawning multiple builds and it will soon run out of resources. I would encourage you to check the following files\n.travis.yml\n/tools/travis-ci/bazel.rc\n. @joestein - It looks like ubuntu config did not exist. Will add it to the fix. For release packages, it builds fine in Mac and Centos. It is possible the bazel binaries shipped for ubuntu does not contain the pkg_tar rules. \n. I pushed the change #33 to take care of the config for ubuntu and it should work now. Please sync and try it out.\n. Working on trying to reproduce #34 in Ubuntu.\n. @joestein - we fixed #34 and please sync and compile. you should be able to build release packages.\n. @joestein - nice to know that you are able to build release packages!\n. @ajorgensen - can you create a new issue and give a pull request?\n. @ajorgensen - If they are in the same repo, we might have to checkout the repo twice?\n. @joestein - should I pull this request and move the tools around in another PR?\n. @saileshmittal - comments have been added\n. @ajorgensen - I disabled openssl in #40 since it is not needed for Heron. It compiled seamlessly in El Capitan and passed all the unit tests.\n. @lucperkins - We have resolved the confusion between Storm Config and Heron Config. All the config are available thru' Storm Config - backward compatible Storm Config + any additional config that are specific for Heron. Can you change the document accordingly and send another PR?  \nIt was a great find - which otherwise would have been confusing!\n. @lperkins any update on this?\n. Fixes #46 \n. Fixes #54 \n. Fixed by #53 \n. :+1: \n. @ativilambit - thanks for the comments. @vikkyrk will clean this up and send another pull request.\n. @ativilambit - this is not a working in OSS since it was written for a older version of Kafka 0.7.2. @joestein and co are working on a version that will work on 0.8.x and 0.9.x of Kafka.\n. @yshivakrishna - Thanks for the request. Let me ask somebody to review it and pull the request!\n. We checked and it is not needed.\n. @yshivakrishna - Nice catch. packer is a twitter internal tool. We will not be using it OSS. We will delete those.\n. Packer is removed from OSS.\n. :+1: \n. --config-path allows the config either to be a directory or a file, hence it is more flexible and able to accommodate different schedulers. In the case of aurora, the config is going to be a directory since it requires a few files for config.\n. @jingwei - Is this resolved?\n. :+1: \n. Fixed in #128 where all the jars in lib/ directory organized by their directories.\n. As we discussed, the information consists of cluster/role/tag\n- cluster refers to the name of the cluster the job needs to be submitted\n- role can mean a group \n- tag - additional information (some times it is useful to tag a topology as dev, prod and staging)\nif the cluster name is local - it means local scheduler and role and tag can be made optional or mandatory based on cluster config.\n. :+1: \n. Fixed in #92 \n. @ashvina - let me try out in my end and get back to you.\n. @ashvina - I checked in a Dockerfile for Ubuntu15.10. You can directly build the artifacts from the heron directory\ndocker/build-artifacts.sh ubuntu15.10 0.1.0-SNAPSHOT ~/output\nSometimes, it is possible that docker container might hang especially during bazel compilation. In those cases, do the following\ndocker run -it heron-compiler:ubuntu15.10\nOnce you are in the container, do the following:\ncd /root\ngit clone <heron-repo>\nbazel build --config=ubuntu heron/...\nbazel test --config=ubuntu heron/...\nbazel build --config=ubuntu release:packages\nIt will build the tarballs at bazel-bin/release. Hope this helps.\n. :+1: \n. @nlu90 - have you taken care of all the comments by @maosongfu?\n. :+1: \n. This is great! @jingwei will take a look at this.\n. @billonahill - Is this similar to what you fixed?\n. Fixed in #216\n. Fixed in #176 \n. Fixed in #291 \n. This seems to be sandboxing issue - the following command seems to work as reported by @supunkamburugamuve \nbazel build --config=ubuntu scripts/packages:binpkgs  --verbose_failures  --genrule_strategy=standalone --ignore_unsupported_sandboxing --sandbox_debug --spawn_strategy=standalone\n. Partially this is fixed - using a directory structure that categorizes the jars. However, logging jars and a few others are hard coded - need to remove it.\n. It has been moved under lib/3rdparty - and the 3rdparty libraries are underneath.\n. After discussion, we converged on generating override.yaml for the config properties overridden by CLI or .heronrc. This file will be shipped with the other config to override the static config present in files.\n. :+1: \n. Fixed in #136 \n. Fixed in #232 \n. Fixed in #128 \n. :+1: \n. @ashvina - We did a recent checkin where the scheduler architecture is streamlined now. Everything about a cluster is read from a bunch of config files. Try the following\ncheckout the new master and run\nbazel run --config=darwin -- release/packages:heron-client-install.sh --user\nThis will install the client of heron at your home directory ~/.heron and the executable heron-cli3 in your ~/bin will have a link ~/.heron/bin/heron-cli3.\nThis installation has already a bunch of configs built in for the local scheduler and they can be found in \n~/.heron/conf/local/.  Those config files define the classes needed for that cluster. You can parse these config files to get an instance of IScheduler. \nThis config is available from the client when you launch and the entire config is carried with the topology package and made available in the containers as well. In the container, they will be stored in\n./heron-conf/\nand several parse routines are available to read those files\nhttps://github.com/twitter/heron/tree/master/heron/spi/src/java/com/twitter/heron/spi/common\nSchedulerMain also reads from those files and you can make an instance of the IScheduler class.\n. With local installation, you can run some topologies right away using the examples using the following command \nheron-cli3 submit local ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology\nYou won't be able to see the output unless you can get to the logs. The logs are in \n~/.herondata/topologies/local//ExclamationTopology/log-files\nYou can kill the topology using the following command\nheron-cli3 kill local ExclamationTopology\nLet me know if you run into issues. \n. We are working on updating the documentation for the new config.\n. Yes. newscheduler is the way forward. It provides a lot of flexibility and clean configuration.\n. Fixed in #132 \n. :+1: \n. Fixed in #150 \n. :+1: \n. :+1: \n. :+1: \n. @windie - I was wondering if you are interested in this.\n. @windie - yes he picked it up. Perhaps, you could help testing this. I am testing this in a Mac.\n. Fixes #139 and #151\n. Fixed in #150 \n. :+1: \n. Fixed in #155 pull request.\n. :+1: \n. Fixed in #178 \n. It should be called 'Getting Started Guide'. \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. Fixes #180 \n. Fixed in #576 \n. @cckellogg - can you take a look at this issue?\n. @cckellogg - do you have an update on this?\n. Fixed in #565 \n. :+1: \n. Fixed in #203 \n. bundled into two jars - heron-api.jar and heron-storm.jar (contains storm-compat and localmode jars). Fixed in #223 \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. That is a nice suggestion @ashvina.\n. Fixed in #225 \n. :+1: \n. This is considered as not an issue - since local scheduler is for development at this stage.\n. Fixed in #232 \n. :+1: \n. :+1: \n. :+1:  - once you move sendToTmaster RuntimeManager\n. :+1: \n. :+1: \n. :+1: \n. \ud83d\udc4d \n. This is great. I will look at it in the night.\nSent from my iPhone\n\nOn Apr 1, 2016, at 12:50 PM, Prabhu Inbarajan notifications@github.com wrote:\ngit clone \ngit checkout -b prabhu/heron\n./bazel-configure.py\ncd scripts\n./setup-intellij.sh\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n. Files changes include some of the commits that we did. I would suggest do the following - \n\n```\ngit clone \ngit checkout -b prabhu/intellj\n\ngit commit -am \ngit push origin prabhu/intellij\n```\nOnce the branch is pushed, please create a pull request. We are verifying it and soon you will get som e feedback.\n. @prabhuinbarajan - \nheron/scheduler is the old code which will be removed as we move to newscheduler, once we finish testing with Aurora.\nheron/schedulers is the new code that are specific to each scheduler\nheron/newscheduler is the abstract scheduler that invokes scheduler specific code\nbazel will not build anything in heron/scheduler, the old code, since it does not have any BUILD file.\nAdded you to the slack account\n. #291 contains this PR\n. :+1: \n. Fixed in #262 \n. It should be under the directory ~/.m2/repository/com/twitter/heron/\nls -l ~/.m2/repository/com/twitter/heron/\n. thanks @maosongfu. The AckingTopology is not a good example, I will add ExclamationTopology to the starting guide.\n. Fixed in #268 \n. :+1: \n. :+1: \n. @maosongfu @ashvina made a comment about whether we can use the Hadoop API as opposed to using command line tool.\n. @ashvina - this classpath is mainly to run Launcher and Uploader on the cli. This does not have to passed all the way to the sandbox, isn't?\n. working on it - should have a PR in 30 minutes!\n. PR is at https://github.com/twitter/heron/pull/272 - note that i have not added a validation whether the classpath value is valid - working on it.\n. My answers are - \nfor 2) @ashvina confirmed that is only being on the cli side. \nfor 3) the bigger picture is the use of heronrc or user specified heronrc. Such situations occur even in other programs such as bazel etc (where the same argument is needed for multiple invocations). There is issue outstanding to do it - https://github.com/twitter/heron/issues/138, this will be fixed in a different PR.\nfor 4) scheduler_classpath is probably not apt, something like cli-classpath might be more relevant. Another question is how the topology config class path is provided when invoking the cli that you think will cause confusion?\nfor 5) we can do extra_jars as well - but I wanted to be consistent in ensuring that the user provided classpath is always the first.\n. for 3) we need both #272 and #138 - sometimes, people want to give in the command line just to override. This is a common practice typically in the Unix world - it will be better to go some existing convention and practice.\nfor 4) if we just have one single classpath at the cli - both submission and topology development will it be clean?\n. 3) when #138 is implemented cli will read .heronrc and it will be the defaults and if any command line is provided it will override the defaults. Either override or the defaults (depending on what is provided) will be passed on to the implementation of #272 \n4) I am open for a different name as well - as long as it is simple and easy to remember\n. @ashvina - currently all scheduler jars are prebuilt and shipped when you run heron-client-install.sh. With hadoop it is possible that packaged version might not be the right version since hadoop environments defer. Yes this approach requires adding jars to git only if it needed for linking and packing. If you are invoking command line programs, the jars are not needed.\nDefinitely this approach might not be ideal solution - since internally we have only a very few schedulers. There are pros and cons for this solution - \nPROS  include fixes the version of the jar used, self contained and does not require installing other packages\nCONS are version mismatch, fat heron-client-install (even for schedulers that you might not be using).\n. @ashvina - While most of the proposal sounds fine, I would suggest not to copy the hadoop jars into $(HERON_HOME)/lib/scheduler. It is possible that Heron might be installed in a system directory such as /usr/local/heron - which might not have permission to copy.  Instead, if we could include the HADOOP_CLASSPATH to be in the classpath in the CLI (interally), probably it might be easier. Let me check with the team tomorrow and make a concrete proposal. \n. @ashvina - After several discussion, here is a proposal - \n- add an option in either client.yaml or scheduler.yaml for the additional classpaths.\n- potentially override this with a cli option called --additional-scheduler-classpath or --additional-heron-classpath for all the CLI commands\n. @maosongfu - I am going to be putting up a PR with cli option\n--extra-heron-classpath\nThis will be added to all the commands (except version and help). Let me know\n. yes. Fixed in #1245 \n. @ashvina - yes after our meeting @maosongfu figured out this and we are working on a fix - so that it is set in LaunchRunner``rather than in the implementation of``ILaunch```.\n. Fixed in #275 \n. Thanks @prabhuinbarajan - if you could take care of this and sent a PR, it will be great!\n. Fixed in #277 \n. Prabhu - just one comment, instead of changing the directory structure, can we change the package itself? InLaunchRunnerTest.javayou could change the package name toimport com.twitter.heron.schedulerinstead ofimport com.twitter.heron.scheduler.service`. We are not going to have service there.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @prabhuinbarajan - how can we verify this?\n. @billonahill - from a functionality standpoint, there is no clear separation. For example, scheduler jars go both into client and core. In such cases, what value does the functional separation add?\n. @billonahill - thanks. @prabhuinbarajan - it look like functional modules are nice to think about - seems to have an advantage. It will be great if you could accommodate this.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d  (shipit)\n. @billonahill - release/BUILD file is going away. In fact that entire directory will be gone.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. bazel_configure.py should be self contained. If it depends on packages then the user has to install those packages. If the semvar.py is just one file, can we include it in bazel_configure.py. This could be temporary since newer versions of bazel has support for auto detection.\nFor detecting gcc and g++ version, typically you could use gcc --version (in all platforms) and the output could be parsed to get the appropriate value. Similarly for cpp/ld etc. For compiling in other platforms, we use docker. Check the top level docker directory.\n. @billonahill - should we change the gcc version to check gcc >= 4.8.1 and that should make travis happy? all the compilers from 4.8 is c++11x complaint.\n. \ud83d\udc4d \n. Ignoring this pull request - since it messed up commits in the branch!\n. \ud83d\udc4d \n. \ud83d\udc4d \n. Fixed in #332 \n. Let us do this for 0.13.4 since I would like to a code freeze after this - only documentation and bug fixes for 0.14.0\n. @maosongfu - sounds good!\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @billonahill - I checked with hazel and it does not see to have a plugin for it. Not sure how to enforce it in bazel.\n. Fixed in #329 \nNow the output looks as \n```\nusage: heron   ...\nAvailable commands:\n    activate           Activate a topology\n    deactivate         Deactivate a topology\n    help               Prints help for commands\n    kill               Kill a topology\n    restart            Restart a topology\n    submit             Submit a topology\n    version            Print version of heron-cli\nGetting more help: \n  heron help  Prints help and options for \nFor detailed documentation, go to http://heronstreaming.io\n```\n. Fixed in #649 \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @lucperkins and @billonahill - I was wondering if this is fixed. If you any additional help, @bmhatfield can pitch in?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @lewiskan - Can you merge with master and update the PR?\n. \ud83d\udc4d \n. @billonahill - the release process generates a file called RELEASE and this file is included in all of the tarballs and packages. heron cli reads from this RELEASE to display the information. Similarly we will make other components to read this as well - for any version checks. \n. We need to document the release process - we can use a variation of our internal process.\n. opened a new issue to track documentation https://github.com/twitter/heron/issues/363\n. @lucperkins - The branch has conflicts. Could you merge and update?\n. @lucperkins - Can you provide an update to PR based on @billonahill 's comments\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @billonahill - should we use bazel do this? is the linkchecker a shell or python script?\n. \ud83d\udc4d \n. Fixed in #354 \n. Fixed in #393 \n. @ashvina - can you review this - since you will have some impact?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. Fixed in #364 \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @billonahill and @ajorgensen - I have taken care of both of your feedback. It was a easy to merge into one single file for compilation. For docker, as @ajorgensen mentioned, there is a file called compile-docker.sh which sets up the docker environment.\n. \ud83d\udc4d \n. :+1: \n. Fixed in #379 \n. Fixed in #381 \n. \ud83d\udc4d \n. @saileshmittal - can you review this change?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @billonahill and @maosongfu - can you share the experiences?\n. Fixed in #392 \n. Fixed in #388 \n. @nlu90 - can you take care of this?\n. @billonahill - I recommended version based on some of compiling in our nest machines and mac machines. I took the minimum and recommended it. Now it looks like jenkin machines have only 1.9.6. Since it seems to be work, we should lower our version to 1.9.6?\n. Fixed in #399 \n. this is what centos5 machines report\ncmake --version\ncmake version 2.6-patch 4\nvery differently.\n. thanks for the suggestion.\n. Fixed in #399 \n. the reason why python2.7 is required is because of pex and bazel. one thing we can do in all our python scripts is to use\n!/usr/bin/env python2.7\nthis will ensure all the scripts use python2.7 executable only. If python2.7 does not exist, it will bail out. Thoughts?\n. in fact when i looked at tools/bazel.rc - we already force python to be python2.7 - which means bazel seems to be correctly using python2.7. bazel_configure.py needs to just verify whether python2.7 is present in the path.\n. probably a combination of @nlu90 approach with #!/usr/bin/env python2.7 - should ensure then we use python2.7 uniformly.\n. Fixed in #399 \n. \ud83d\udc4d \n. @billonahill - seems to have failed the test\n//heron/instance/tests/java:spout-instance_unittest  \u001bFAILED\u001b in 22.5s\nthis is not a flaky test - not sure why.\n. I restarted the bazel build and it was fine. Looks like that test is flaky.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. Can we do the same for Aurora as well?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. fixed in #454 and #448\n. Fixed in #496 \n. Fixed in #496 #493 \n. Fixed in #512 \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. Darin - REEF essentially provides an abstraction layer and allows it to run\nboth on in YARN and Mesos.  We will test and give some feedback.\nOn Wed, Apr 20, 2016 at 6:03 PM Darin notifications@github.com wrote:\n\nThe fact your using REEF is interesting. I know it also has the ability to\nschedule jobs for Mesos, but haven't used it. Would there be good reasons\nnot to use it for Mesos (Currently being worked) as well in the name of\nless code?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/heron/pull/419#issuecomment-212676637\n. @ashvina - Any update on this?\n. @ashvina - thanks. let us know if you need any help.\n. @ashvina - PR #518 is already merged.\n. @ashvina - Just wondering if you need any other help with REEF scheduler, let us know. \n. @ashvina - Here are the answers\n- I have added some unit tests for REEF scheduler today. I am working with our cluster teams to develop deployment scripts and test the implementation.\n\nThis is awesome - will definitely improve reliability.\n- Could you please point me at the solution for client classpath. I have not been able to follow all the discussions lately.\nOption 1: Add a config in client.yaml and this assumes that the class path is the same across different computers in the organization.\nOption 2: Add a command line arg --extra-scheduler-classpath which augments the class path while submitting, activating, deactivating & killing a topology. \nCurrent thinking is to provide option 1 and if needed provide option 2. Both the options could lead to conflicts. We can solve them when it occurs.\n- After rebasing on the latest commits, I am getting a new error: Could not find topology info for topology: AckingTopology, cluster: default and environ: heron. I am debugging this and will reach out to your team if needed. Please let me know if you are aware any issue related to this.\nWe have not been seeing this issue. Just wondering when and where do you see this?\n- Regarding Scheduler/AM HA, I have hit an open issue in REEF. This feature might need to wait a bit.\nCan you please elaborate this?\n- I am aware of pending tasks and issues. It will help me prioritize if you could help me understand the acceptance criteria for this pull request. Thanks!\nPR looks good. It will be nice if you could add a note below review comments that have been fixed. Typically, for the TODOs, we open tracking issues so that we do not drop it.\nWe don't have strict acceptance criteria. Since community will be very interested in YARN and probably get it into production, it will be nice to ensure some reasonable quality. \n- Include unit tests so that we can exercise regularly\n- Include integration tests, if applicable\n- Please test in a distributed environment for a few days and see if any issues come up\nLet us know if you need help on this.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @billonahill - can you merge this branch with the latest master and kick ci build to ensure that travis-ci passes?\n. \ud83d\udc4d \n. Ship it \n. @billonahill - i will fix the trailing characters in the next PR. For catching Exception, I think it will be better to catch specific exceptions!\n. Here is the command for checkstyle\nbazel build --config=darwin --experimental_action_listener=tools/java:compile_java heron/uploaders/src/java/...\n. \ud83d\udc4d \n. \ud83d\udc4d \n. thanks @lewiskan and @supunkamburugamuve - let me get the ubuntu build streamlined with the appropriate options. a few clarifications - \n- for installing libtool-2.4.6 - what command did you use?\n- i will see if libunwind can brought into the code base itself so that it is self reliant\n- what are the errors in gperftools that occurred so that i can understand\n. \ud83d\udc4d  - after the travis ci fix goes in. please merge with the master and rerun the travis ci to ensure everything is fine - before checkin.\n. I tried several variations of the following - \n```\nCheck whether there are any uncommited changes\nif [ -z ${HERON_TREE_STATUS+x} ];\nthen\n  cmd=\"git diff-index --quiet HEAD -- || true\"\n  status=$($cmd)\n  if [[ ${status} == 0 ]];\n  then\n    tree_status=\"Clean\"\n  elif\n    tree_status=\"Modified\"\n  else\n    tree_status=\"Unknown\"\n  fi\nelse\n  tree_status=${HERON_TREE_STATUS}\nfi\n```\nand it did not work!\n. thank you all for the suggestion. Going with @ajorgensen suggestion!\n. @billonahill - to clarify your question \n${HERON_TREE_STATUS+x}\nit checks whether HERON_TREE_STATUS is empty or non-empty - according to the website \n. @ajorgensen - As you suggested, I have added \nset -eu\n. \ud83d\udc4d \n. @billonahill - can you merge with the latest master and update the PR to ensure that travis-ci passes?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. +1 - it will be good to fix.\n. Fixed in #549 and #563 \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. Looks good to me!\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. Fixed in #503 \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @ajorgensen - you might want to check if this is ok with you.\n. @lewiskan - has pretty much completed the task. We can upload the jars only after 0.14.0 jars are officially published.\n. Before statically linking - the libraries used were as follows: \nubuntu@ip-172-31-10-44:~/heron2/bazel-out/local_linux-fastbuild/bin/heron/tmaster/src/cpp$ ldd heron-tmaster\n    linux-vdso.so.1 =>  (0x00007ffe57fb5000)\n    libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f5aedc51000)\n    libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f5aed94b000)\n    libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f5aed72d000)\n    libunwind.so.8 => /usr/lib/x86_64-linux-gnu/libunwind.so.8 (0x00007f5aed512000)\n    librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f5aed30a000)\n    libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f5aecf45000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007f5aedf55000)\n    libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f5aecd2f000)\n    liblzma.so.5 => /lib/x86_64-linux-gnu/liblzma.so.5 (0x00007f5aecb0d000)\n    libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f5aec909000)\nAfter statically linking - the libraries used were as follows:\nubuntu@ip-172-31-10-44:~/heron/bazel-out/local_linux-fastbuild/bin/heron/tmaster/src/cpp$ ldd heron-tmaster\n    linux-vdso.so.1 =>  (0x00007ffd8b9ad000)\n    libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f3d95ba3000)\n    libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f3d9589d000)\n    libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f3d9567f000)\n    librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f3d95477000)\n    libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f3d95261000)\n    libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f3d94e9c000)\n    /lib64/ld-linux-x86-64.so.2 (0x00007f3d95ea7000)\nThis eliminates the dynamic dependencies of libunwind and liblzma linking\n. @supunkamburugamuve - for centos we have tested it with centos7, these are the binaries that we publish.\ncentos5 and centos6 will required compiled binaries of bazel or users have to download bazel and compile it. The bazel release binaries do not work directly in either of those. We do have centos5 version of heron working inside Twitter - using bazel that we compiled for centos5.\nI would think it might be suffice to support centos7 - let us know if it should be otherwise.\n. @supunkamburugamuve - sounds good!. thanks for the write up.\n. After investigation, we realized that it might not be possible. gperftools uses libunwind and it does not provide an option to specify the path for custom libunwind.\n. Suggest to split the trouble shooting and tuning of topologies into different pages.\n. perhaps give all the guidelines that we provide internally included as well?\n. Let us keep in different pages - we have enough material to add already.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @nlu90 - one way or the other merge is going to conflict, I think.\n. \ud83d\udc4d \n. @maosongfu - did you run through style checks?\n. \ud83d\udc4d\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @billonahill - when you get a chance, please take a look at this. As discussed, there is no default during installation. The user has to provide either\n--user, installs in the user home directory\n--system, installs in the /usr/local/heron directory\n--prefix, installs in the directory prefix provided\n. When a runtime exception is thrown - instead of System.exit(1) - will the executor exit as well?\n. Sounds good!\n. \ud83d\udc4d \n. duplicate of #187 \n. \ud83d\udc4d \n. @billonahill - based on the discussions we had, packages will be installed in ~/heron/package/version, if --user option is provided\n```\n/Users/kramasamy/heron\n\u2514\u2500\u2500 client\n    \u251c\u2500\u2500 0.13.3\n    |-- master\n```\n. @maosongfu - update the PR with your comments. @lewiskan suggested that ~/heron might clash with people who have checked out github heron repo.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @saileshmittal - can you check if this works with old topologies? I mean it does not have to read them - but it should not crash the old UI or tracker - and gracefully skip it.\n. Sounds good!\n. \ud83d\udc4d \n. @saileshmittal - can you include the feedback from @maosongfu?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @prabhuinbarajan and @billonahill - can you review this PR?\n. @supunkamburugamuve - can you please update this PR so that we can merge it?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @billonahill - this is stored in zookeeper though!\n. \ud83d\udc4d \n. Fixed in #557 \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. glad you got those! \ud83d\udc4d \n. \ud83d\udc4d \n. note that - this is not a working cluster. The reason why each file is created with localfs_uploader.yaml or locals_statemgr.yaml is because you can have zk_uploader.yaml later.\nclient.yaml is optional.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @saileshmittal - can you take a look at this?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. please check with travis-ci before checking in.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. Ship it\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @caofangkun - Will you be interested in tackling this or something interesting?\n. Sure go for it. It will be nice if you could write a simple wiki or a design doc. We will be happy to review it. Github is not allowing me to assign the issue. I have invited you to be a collaborator - once you accept it, it might.\n. Yup - the issue is assigned to you.\n. Now heron support python topologies. Please try it out and let us know. It is in the master now - soon we will have a release.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @maosongfu - this is in nest machine - unless /usr/bin/python points to python 2.7, we will have this issue.\n. this fix is just a syntactic sugar - no change in logic.\n. \ud83d\udc4d \n. Fixed in #594 \n. Fixed in #598 \n. Fixed in #600 \n. \ud83d\udc4d \n. Ship it\n. yes indeed.. Fixed in #619 \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. Awesome! Go for it!\nSent from my iPhone\n\nOn Jun 22, 2016, at 3:26 PM, Christian Stewart notifications@github.com wrote:\nInterested in doing this, will see if I can get it working\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @paralin - any update on this? if you need help let us know.\n. @moomou @prabhuinbarajan @paralin - all expressed interest for running Heron on Kubernetes. Please let me know so that we can have an initial meeting and colloborate on this effort.\n. @paralin @moomou @prabhuinbarajan - we could have a video chat and I can add you guys to the committer slack channel.\n. @paralin @moomou - I have invited both of you the committer slack channel. Once you accept the invite - we can discuss it and have an initial video chat.\n. Prabhu/Palin/Paul - please include Wei in your kubernetes development.\nOn Aug 24, 2016, at 5:40 AM, Wei-Ting Kuo notifications@github.com wrote:\nWill the discussion or development be transparent? Sorry I'm not familiar with Heron. I was a Storm user and I'm currently trying to port other frameworks to kubernetes now. Quite interested in your approach, would be a good inspiration when I port other frameworks. Got the chance to audit or contribute?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @waitingkuo - we have invited you to our slack contributor's channel. Please join and you will be able to discuss it in kubernetes channel\n. @nicknezis - initial version of heron in k8s is already in 0.14.9. We are adding documentation and tutorial in the upcoming version. Please check #1946 . Heron already runs in Kubernetes - closing the issue.. @vidit-bhatia - bringing this to the attention of @ashvina and @avflor - who are microsoft folks.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. Congrats @billonahill - you are the PEX expert!\n. @lucperkins - can you please take a look at this PR?\n. ship it\n. looks good to me!\n. @saileshmittal - can you take a look at this?\n. @maosongfu - can you take look at this?\n. \ud83d\udc4d \n. Waiting for the ci to finish before merging.\n. \ud83d\udc4d \n. @sijie - we removed it during cleaning purposes. But the idea of contrib will speed up development. We are looking at the following directory structure for contrib\n\ncontrib/<component>/<software-version>/src/<language>/....\nFor example, a spout for distributed log will be \ncontrib/spout/dlog-0.1.1/src/java/com/twitter/dlog/<code>\nAnother example, of a scheduler is\ncontrib/scheduler/yarn-1.0.1/src/java/com/twitter/scheduler/yarn/...\nHope this helps.\n. @khurrumnasimm - @sijie is in charge of this. @sijie wondering if you are ok with @khurrumnasimm can contribute?. Can you please do bazel clean and compile everything?\nSent from my iPhone\n\nOn May 13, 2016, at 8:00 AM, Supun Kamburugamuve notifications@github.com wrote:\nAfter syncing with the master, the following error happens.\n0: [libprotobuf ERROR google/protobuf/message_lite.cc:123] Can't parse message of type \"heron.proto.api.Topology\" because it is missing required fields: id, name, state\n0: F0513 14:43:24.454993 11468 stmgr-main.cpp:70] Corrupt topology defn file\n0: * Check failure stack trace: *\n0: @ 0x52672a google::LogMessage::Fail()\n0: @ 0x52865f google::LogMessage::SendToLog()\n0: @ 0x52630f google::LogMessage::Flush()\n0: @ 0x528f9e google::LogMessageFatal::~LogMessageFatal()\n0: @ 0x40d3af main\n0: @ 0x7f3a045d2ec5 __libc_start_main\n0: @ 0x40cdff (unknown)\n0: @ (nil) (unknown)\n0: WARNING: Logging before InitGoogleLogging() is written to STDERR\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\n. Did bazel clean work?\n. I would suggest to finish the slurm scheduler - check it in and then work\non torque\nOn Sat, May 14, 2016 at 6:15 PM Supun Kamburugamuve \nnotifications@github.com wrote:\nYes, it worked, I'll focus on the unit tests and Torque scheduler. Before\ntorque I think I need to get the CentOs 6 build working.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/heron/pull/647#issuecomment-219260481\n. @supunkamburugamuve - thanks. I was wondering if you add some documentation for Slurm. It will be nice. I will request @maosongfu to take a look at this.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. do you want to make it use python cli?\n. I mean something like python argparse so that the order of names do not matter and it is more clear. Let us do that in the next PR.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @saileshmittal - Can you incorporate the feedback from @billonahill and merge this in?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @Ishiihara - I have invited you to collaborate. Please accept and I will be able to assign it.\n. @Ishiihara - any update on this?\n. @skanjila - we have done for Java but not scala yet. Welcome any contributions. Since Heron in the process of moving to Apache incubation - please use apache dev mailing lists. Sent a subscribe message to dev-subscribe@heron.incubator.apache.org and you can join the slack at https://heronstreaming.herokuapp.com/\n\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. Ship it\n. @billonahill is working on this in issue #799 \n. Fixed in #747 \n. Fixed in #747 \n. Fixed in #747 \n. Fixed in #747 \n. We keep the committers since they can contribute even if they are not it twitter.\n. @billonahill - done!\n. Handing it off to @joestein since he is updating the Mesos Scheduler along with Vagrant.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @saileshmittal - any update on this?\n. \ud83d\udc4d \n. @maosongfu - is this ready to merge?\n. ship it\n. \ud83d\udc4d \n. \ud83d\udc4d nice work!\n. Ship it\n. \ud83d\udc4d \n. @nlu90 - this is fixed - can you close the issue?\n. \ud83d\udc4d \n. Probably local scheduler implementation could be a good place to start.\n. @supunkamburugamuve and @maosongfu - wondering if this is a priority that needs to be done. If so, I would like to assign a milestone so that we can get this completed.\n. @lucperkins - Has all the feedback been addressed?\n. in order to get syntax highlighting while rendering locally - you should do \npip install Pigments\n. @maosongfu - can you review this and see if this can be checked in?\n. @lucperkins - we are planning to get this one as soon as we can. @maosongfu - are we ready to get this  in?\n. Code block sometimes does not work for some people. Not sure why is that.\n. @lucperkins - I was wondering if you could send some screen shots of the new website.\n. Looks good to me! Others have any comments?\n. +1 for what @lucperkins said.\n. looks good to me. Let us check in scala code and we can enable their compilation and testing, once we go into newer bazel. Thoughts?\n. \ud83d\udc4d \n. do you want to call it local mode? I thought we moved away from that terminology - since it will be confusing.\n. Sure - as long as we don't break the storm compatibility and not breaking the running code - I am fine.\n. @saileshmittal - can you review this?\n. @saileshmittal - can you take care of this before 0.14.0 goes out?\n. @billonahill - as a first step, you need to install docker in your mac and then do the following on the top level directory of the local heron repo\ndocker/build-artifacts.sh centos7 master ~/centos\n. Fixed in #842 - thanks @tysonnorris \n. \ud83d\udc4d \n. @joestein - looks like it failed in check styles. I was wondering if you could fix those and update the PR.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @billonahill - this is completed I believe. Can we close this?\n. can you explain when the java docs are built? If you could introduce a bazel dependency on protobuf it will work fine.\n. Fixed in #763 \n. we can merge it after the first release - \n. \ud83d\udc4d \n. thanks @jmcomets for the patch. Can you please fill out the CLA agreement \nhttp://twitter.github.io/heron/docs/contributors/community/\nfor submitting the patch?\n. thanks for the fix @frewsxcv - can you please fill up the CLA agreement at\nhttp://twitter.github.io/heron/docs/contributors/community/\nfor submitting the patch.\n. @lucperkins - is this relevant still? If so, can you please merge and update the PR?\n. @saileshmittal - have all the review comments taken care of.\n. closing this - since @taishi8117 finished it.\n. @lucperkins - is this still needed?\n. Let us wait until what @lucperkins says.\nOn Friday, August 12, 2016, Taishi Nojima notifications@github.com wrote:\n\nShould we merge this?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/pull/795#issuecomment-239605149, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAWcRJ_NXxtwrlRf_aXl2967ZXFjC_S2ks5qfWIwgaJpZM4InBFU\n.\n. @lucperkins - it will be nice if you could add a screenshot.\n. looks good to me! @maosongfu @nlu90 @billonahill @objmagic @saileshmittal - any comments?\n. @lucperkins - sure. let us know once you are done.\n. thanks @nickmerwin - for the patch. Can you please fill out the CLA agreement\n\nhttp://twitter.github.io/heron/docs/contributors/community/\nfor submitting the patch? It is pretty straight forward.\n. @DarinJ - thanks for giving it a spin. I was wondering if you had a chance to try heron tracker and heron ui. If not, please do and see if the topologies that you launched show up in UI.\n. @DarinJ - Awesome. Let us know if you run into issues with HDFS uploader.\n. @joestein - would you have some bandwidth to resolve the conflicts?\n. @DarinJ - just wondering if you had a chance to spin this up on a mesos cluster and see if everything works well?\n. \ud83d\udc4d \n. Just wondering if you followed the instructions in the page \nhttp://twitter.github.io/heron/docs/developers/compiling/mac/\n. @cybernet - moving to bazel > 0.2 will require some additional work especially the WORKSPACE. If you could correct those and send a PR - it would be great.\n. Please fill out the CLA agreement\nhttp://twitter.github.io/heron/docs/contributors/community/\nfor accepting the patches. It is pretty easy to do.\n. To be precise - \n- Building Heron requires Java JDK8\n- For writing topologies and running them - JDK7 is sufficient.\n. @billonahill - do you have any comments on this?\n. yes - we are working on getting this fixed.\n. Fixed in #869 \n. @billonahill - since java checkstyles do not take time, are we close this pull request?\n. our use case for drpc were none - hence we did not choose to implement it. However - if there is a lot of interest - we will be happy to accept contributions!\n. @lewiskan - can you please investigate how we can get source code artifacts?\n. the c++ binaries need to be statically linked - we do mostly static option and because of this standard libraries and libunwind are not statically linked.\n. Congrats - @qiuyij  - on your first commit!\n. Please merge yourself - since you have permission to do so.\n. This is nice - ship it!\n. @lucperkins  - if you can resolve the conflicts and we are ready to merge.\n. @smxysu3 - Please follow the instructions for compiling on Ubuntu and let us know. \nhttp://twitter.github.io/heron/docs/developers/compiling/linux/#building-on-ubuntu-14.04:34fe9f2d9cc3cd7e319ca75e7da08332\nand let us know\n. @bretlowery - Wondering during the installation whether you used the --user option?  If that is the case it installs heron under ~/.heron.  Let us know.\n. @mlinge - I was wondering if you try to compile the code and installed the prebuilt binaries. What version of ubuntu are you using? Ubuntu binaries work for Ubuntu versions >= 14.04\n. @mlinge - we have not encountered this error before. It could be partially because we have not run heron in 16.04 at all.\n@billonahill - any clues on why PEX might be complaining?\n. @mlinge - checking if this issue is resolved so that we can close the issue.\n. If you get a chance can you use the DockerFile.ubuntu14.04 and see if the error occurs. There is a script called \ndocker/build-artifacts.sh\nwhich you can use to build artifacts for ubuntu\ncheers\n/karthik\n\nOn Jun 21, 2016, at 12:01 AM, Aleksandar Vitorovic notifications@github.com wrote:\nI get the same error when running 'heron version', although I am using Ubuntu 14.04 (I upgraded from Ubuntu 12.04).\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/issues/831#issuecomment-227356925, or mute the thread https://github.com/notifications/unsubscribe/AAWcRK2AfHhevk609BrlnXRjfbyt22V2ks5qN4xagaJpZM4IpPTL.\n. @longdafeng - you should not connect to maven.twttr.com. At the top of the repo - there is a WORKSPACE file, with the following entries - \n\nmaven_server(\n  name = \"default\",\n  url = \"http://central.maven.org/maven2/\",\n)\nmaven_server(\n  name = \"twitter-maven\",\n  url = \"http://maven.twttr.com\",\n)\nRemove the maven_server entry that has maven.twttr.com and try it. You should connect by default to maven central. \n. Reopening this - so that we can fix this. @billonahill or @maosongfu - any idea why thrift should 0.5.0-1?\n. @phynman - since heron uses native code, you can build and deploy in the same platforms only. For example, if you build in centos7, you need to deploy in centos7 or above.\n. @goldenhoo - maven.twttr.com http://maven.twttr.com/ is a twitter internal maven repository. You need to use the central maven repository. Wondering which version of Heron are you trying to compile.\ncheers\n/karthik\n\nOn Apr 12, 2017, at 12:53 AM, goldenhoo notifications@github.com wrote:\nThanks in advance.\nMy question:\ngoldens-MacBook-Air:heron golden$ bazel build --config=darwin heron/...\nERROR: /Users/golden/heron/heron/metricsmgr/src/thrift/BUILD:5:1: no such package '@org_apache_thrift_libthrift//jar': Failed to fetch Maven dependency: Could not transfer artifact org.apache.thrift:libthrift:jar:0.5.0-1 from/to org_apache_thrift_libthrift (http://maven.twttr.com http://maven.twttr.com/): Connect to maven.twttr.com:80 timed out and referenced by '//heron/metricsmgr/src/thrift:thrift_scribe_java'.\nERROR: Analysis of target '//heron/metricsmgr/src/thrift:thrift-java' failed; build aborted.\nINFO: Elapsed time: 13.361s\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/issues/832#issuecomment-293501891, or mute the thread https://github.com/notifications/unsubscribe-auth/AAWcRAFWpG03Mf1r37JLRhQnD1Owx4ukks5rvIMTgaJpZM4IpPs9.\n\n\n. @danielschonfeld - thanks for pointing out. We are in the process of updating it.\n. @danielschonfeld - yes. Please look at the local, aurora and slurm scheduler implementation.\n. duplicate of #809 \n. @longdafeng - there is no need to mv heron-core.tar.gz to ~/.heron - heron-client-install.sh --user will automatically install heron-core.tar.gz. stream manager is started automatically when you launch the topology.\n@maosongfu - can you please take a look at this?\n. #505 is prioritized for next release.\n. @thedrow - looks like travis-ci failed due to syntax error in travis.yml\n. @thedrow - We thought Ubuntu precise can serve as a common denominator. You could build heron for Ubuntu 15.10 as well. There is a docker file in docker/ directory.\n. @thedrow - regarding ccache, IMO, we could wait until bazel supports it.\n. @Jonathan-Wei - the simplest would be to get automake in your path and see if it works. Meanwhile we are working on finding the root cause of the issue.\n. @daxwang - we are working on moving on to bazel 0.2.3. This is a partial working branch that fixed the workspace issues. We are working on fixing the java toolchain issues - once they are fixed, we will officially cut over to 0.2.3\n. Closing this in favor of #869 \n. @billonahill - I tried the changes in my branch and it works. Able to build centos7 binaries.\n. @maosongfu / @nlu90 - could you help on the simulation mode?\n. @nlu90 - can you please check and get to the bottom of the issue?\n. @aaronshan - this is because the file times are set to be epoch - while making the tar packages. Would appreciate a patch.\n. If you want to compile for centos 6, you need to compile bazel. Bazel binaries do not work for centos6. Otherwise rest of the code should compile without any issues. We have tried in centos5 and it works.\n. @nlu90 - can you check and give some pointers to why the tests have failed?\n. @alanngai - the documentation can be found in the web as well.\nhttp://twitter.github.io/heron/docs/getting-started/\n. @alanngai - can you do the following?\nbazel build --config=darwin --verbose_failures heron/...\nThis will prevent verbose messages when a failure occurs in bazel.\n. @alanngai - I checkout a fresh version of your branch in my iMac running El Capitan 10.11.3 and it was successful\nNFO: From Linking third_party/yaml-cpp/libyaml-cxx.a:\n/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/libtool: file: bazel-out/local-fastbuild/bin/third_party/yaml-cpp/_objs/yaml-cxx/third_party/yaml-cpp/empty.pic.o has no symbols\nwarning: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/libtool: warning for library: bazel-out/local-fastbuild/bin/third_party/yaml-cpp/libyaml-cxx.a the table of contents is empty (no object file members in the library define global symbols)\nINFO: From Executing extra_action //tools/java:checkstyle_java on //heron/schedulers/tests/java:HeronMasterDriverTest:\nStarting audit...\nAudit done.\nINFO: From Executing extra_action //tools/java:checkstyle_java on //heron/examples/src/java:examples-unshaded:\nStarting audit...\nAudit done.\nINFO: Elapsed time: 498.984s, Critical Path: 489.77s\n. @alanngai - perhaps you should checkout a new version of your branch and try it out.\n. Here you go \nKarthiks-iMac:heron karthikz$ ./bazel_configure.py \nPlatform Darwin\nUsing C compiler          : /usr/bin/gcc (4.2.1)\nUsing C++ compiler        : /usr/bin/g++ (4.2.1)\nUsing C preprocessor      : /usr/bin/cpp (7.3.0)\nUsing C++ preprocessor    : /usr/bin/cpp (7.3.0)\nUsing linker              : /usr/bin/ld\nUsing Automake            : /opt/twitter/Cellar/automake/1.14.1/bin/automake (1.14.1)\nUsing Autoconf            : /opt/twitter/Cellar/autoconf/2.69/bin/autoconf (2.69)\nUsing Make                : /usr/bin/make (3.81)\nUsing CMake               : /opt/twitter/Cellar/cmake/3.0.2/bin/cmake (3.0.2)\nUsing Python2             : /System/Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7 (2.7.10)\nUsing archiver            : /usr/bin/libtool\nUsing coverage tool       : /usr/bin/gcov\ndwp                       : not found, but ok\nUsing nm                  : /usr/bin/nm\nobjcopy                   : not found, but ok\nobjdump                   : not found, but ok\nUsing strip               : /usr/bin/strip\nWrote the environment exec file scripts/compile/env_exec.sh\n. @alanngai - great to hear. It will be great if you could merge the conflicts and update the PR. @nlu90 - can you another look at it so that we can merge this?\n. Alan - we are working through the last bug. If that clears, we will merge this PR in. \n\nOn Jul 8, 2016, at 5:06 PM, Alan Ngai notifications@github.com wrote:\ncreated #1053 to track @objmagic's suggesting\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @alanngai - just to clarify what @objmagic and @cckellogg pointed out\n- Merge with the current master\n- There is an exception that occurs in tracker which @cckellogg pointed out to reverting the changes in a single file. Let us revert this first or fix the error occurring in this file.\n- @objmagic mentioned that there are still python style checker errors occurring in your code. Could you run the style checker again and please fix those?\n- Once these are done, we will merge the code and then address the changes to enable python checker by default.\n. @alanngai - thanks let us do 1) first and merge it. I will request @objmagic to take care of 2).\n. @objmagic - regarding the exception trace for 2) - can you fix in a separate PR and send a pull request?\n. @objmagic - if that is the case, this PR is ready to be merged.\n. @Jonathan-Wei - you can read the sections on Deployment for understanding how to deploy it multi-node cluster. \n- Since Heron does not have a scheduler of its own - you need to have a scheduler installed. Currently, we support Aurora/Mesos - you need to install Aurora as in http://aurora.apache.org/\n- You need a zookeeper installed in some node.\n\nOnce you have both follow the configuration \nhttps://github.com/twitter/heron/tree/master/heron/config/src/yaml/conf/aurora\n. @Jonathan-Wei - we are working on testing it and cover a few corner cases. Once it is done - we will merge to master. Similarly native mesos scheduler is also in the works #803 \n. @Jonathan-Wei - We have just merged the REEF scheduler on YARN to the master. It is still experimental and we are working on the documentation. You are welcome to try it out and let us know.\n. :+1: \n. Great point. Configs that we supply are examples which is bundled in the package. The expectation is that the user provided config will be in different directory - for example - you can have a directory called \"~/clusters\" under which two different directories 'devcluster' and 'prodcluster'. When you issue a command, you need to do the following:\nheron activate devcluster --config-path ~/clusters ExclamationTopology\nnote that --config-path option need to be provided in all the commands. To avoid repetition, we are working on a user specific ~/.heronrc (that allows specifying the config path once). The commands will use ~/.heronrc in order to determine correct config-path.\nHope this helps!\n. @DarinJ - any other suggestions are welcome as well!\n. @nlu90 - what will it take to make it thread safe?\n. Could you explain the difference and also how the performance will be affected?\nSent from my iPhone\n\nOn Jun 4, 2016, at 12:13 AM, Neng Lu notifications@github.com wrote:\nI discussed this issue with maosong.\nMaking it thread safe introduces performance overhead. Instead, we can add synchronization mechanism in the bolt/spout code if needed.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @lucperkins - you can start with the design goals in the Heron Architecture section for Heron Design Goals.\n. @lucperkins - Any update on this?\n. Closing this since it has been answered.\n. @caofangkun - When I looked the log of travis ci, it seems to failing in pants. I was wondering if you could enable some logs so that we can identify why the error message is being issued. \n\nBased on @billonahill comments - I recommend add some debugging to see what file isn't being copied, or to set should_log to return True in tracer.py\n. I am also getting this warnings - \nbazel build --config=ubuntu heron/...\nWARNING: /Users/kramasamy/workspace/heron24/3rdparty/zookeeper/BUILD:82:16: in includes attribute of cc_library rule //3rdparty/zookeeper:zookeeper-cxx: 'include' resolves to '3rdparty/zookeeper/include' not in 'third_party'. This will be an error in the future.\nWARNING: /Users/kramasamy/workspace/heron24/3rdparty/libevent/BUILD:100:16: in includes attribute of cc_library rule //3rdparty/libevent:libevent-cxx: 'include' resolves to '3rdparty/libevent/include' not in 'third_party'. This will be an error in the future.\nWARNING: /Users/kramasamy/workspace/heron24/3rdparty/gflags/BUILD:49:16: in includes attribute of cc_library rule //3rdparty/gflags:gflags-cxx: 'include' resolves to '3rdparty/gflags/include' not in 'third_party'. This will be an error in the future.\nWARNING: /Users/kramasamy/workspace/heron24/3rdparty/gtest/BUILD:94:16: in includes attribute of cc_library rule //3rdparty/gtest:gtest-cxx: 'include' resolves to '3rdparty/gtest/include' not in 'third_party'. This will be an error in the future.\nWARNING: /Users/kramasamy/workspace/heron24/3rdparty/gperftools/BUILD:71:16: in includes attribute of cc_library rule //3rdparty/gperftools:tcmalloc-cxx: 'include' resolves to '3rdparty/gperftools/include' not in 'third_party'. This will be an error in the future.\nWARNING: /Users/kramasamy/workspace/heron24/3rdparty/jansson/BUILD:46:16: in includes attribute of cc_library rule //3rdparty/jansson:jansson-cxx: 'include' resolves to '3rdparty/jansson/include' not in 'third_party'. This will be an error in the future.\nWARNING: /Users/kramasamy/workspace/heron24/3rdparty/gmock/BUILD:83:16: in includes attribute of cc_library rule //3rdparty/gmock:gmock-cxx: 'include' resolves to '3rdparty/gmock/include' not in 'third_party'. This will be an error in the future.\nWARNING: /Users/kramasamy/workspace/heron24/3rdparty/glog/BUILD:54:16: in includes attribute of cc_library rule //3rdparty/glog:glog-cxx: 'include' resolves to '3rdparty/glog/include' not in 'third_party'. This will be an error in the future.\nWARNING: /Users/kramasamy/workspace/heron24/3rdparty/protobuf/BUILD:141:16: in includes attribute of cc_library rule //3rdparty/protobuf:protobuf-cxx: 'include' resolves to '3rdparty/protobuf/include' not in 'third_party'. This will be an error in the future.\nWARNING: /Users/kramasamy/workspace/heron24/3rdparty/yaml-cpp/BUILD:127:16: in includes attribute of cc_library rule //3rdparty/yaml-cpp:yaml-cxx: 'include' resolves to '3rdparty/yaml-cpp/include' not in 'third_party'. This will be an error in the future.\n. When I looked at the travis-ci logs, it seems to be failing with java tests and looking at each of the logs shows the following - \n```\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\n\nClass not found: [rotating-map_unittest]\nBazelTestRunner exiting with a return value of 2\nJVM shutdown hooks (if any) will run now.\nThe JVM will exit once they complete.\n-- JVM shutdown starting at 2016-06-16 04:33:10 --\n```\n. Awesome! The only issue remaining is the warnings which is not fixed even in bazel 0.3.0\n. @caofangkun - this PR is really coming out very well. I see some additional warnings - \nINFO: From Executing extra_action //tools/cpp:checkstyle_cpp on //heron/common/tests/cpp/threads:spcountdownlatch_unittest:\nIgnoring external/local_config_cc/cc_wrapper.sh; not a valid file name (cc, h, cpp, cu, cuh)\nINFO: From Executing extra_action //tools/cpp:checkstyle_cpp on //heron/proto:proto-cxx:\nIgnoring external/local_config_cc/cc_wrapper.sh; not a valid file name (cc, h, cpp, cu, cuh)\nINFO: From Executing extra_action //tools/cpp:checkstyle_cpp on //heron/common/src/cpp/network:network-cxx:\nIgnoring external/local_config_cc/cc_wrapper.sh; not a valid file name (cc, h, cpp, cu, cuh)\nINFO: From Executing extra_action //tools/cpp:checkstyle_cpp on //heron/common/src/cpp/network:network-cxx:\nIgnoring external/local_config_cc/cc_wrapper.sh; not a valid file name (cc, h, cpp, cu, cuh)\nINFO: From Executing extra_action //tools/cpp:checkstyle_cpp on //heron/common/src/cpp/network:network-cxx:\nIgnoring external/local_config_cc/cc_wrapper.sh; not a valid file name (cc, h, cpp, cu, cuh)\nINFO: From Executing extra_action //tools/cpp:checkstyle_cpp on //heron/common/src/cpp/zookeeper:zookeeper-cxx:\nIgnoring external/local_config_cc/cc_wrapper.sh; not a valid file name (cc, h, cpp, cu, cuh)\nINFO: From Executing extra_action //tools/cpp:checkstyle_cpp on //heron/common/tests/cpp/network:switch_unittest:\nIgnoring external/local_config_cc/cc_wrapper.sh; not a valid file name (cc, h, cpp, cu, cuh)\nINFO: From Executing extra_action //tools/cpp:checkstyle_cpp on //heron/proto:proto_execution_state_cc:\nIgnoring external/local_config_cc/cc_wrapper.sh; not a valid file name (cc, h, cpp, cu, cuh)\nINFO: From Executing extra_action //tools/cpp:checkstyle_cpp on //heron/common/tests/cpp/network:order_unittest:\nIgnoring external/local_config_cc/cc_wrapper.sh; not a valid file name (cc, h, cpp, cu, cuh)\nINFO: From Executing extra_action //tools/cpp:checkstyle_cpp on //heron/common/tests/cpp/network:switch_unittest:\nIgnoring external/local_config_cc/cc_wrapper.sh; not a valid file name (cc, h, cpp, cu, cuh)\nINFO: From Executing extra_action //tools/cpp:checkstyle_cpp on //heron/common/tests/cpp/network:switch_unittest:\nIgnoring external/local_config_cc/cc_wrapper.sh; not a valid file name (cc, h, cpp, cu, cuh)\nINFO: From Executing extra_action //tools/cpp:checkstyle_cpp on //heron/common/tests/cpp/network:packet_unittest:\nIgnoring external/local_config_cc/cc_wrapper.sh; not a valid file name (cc, h, cpp, cu, cuh)\nINFO: From Executing extra_action //tools/cpp:checkstyle_cpp on //heron/common/tests/cpp/network:order_unittest:\nIgnoring external/local_config_cc/cc_wrapper.sh; not a valid file name (cc, h, cpp, cu, cuh)\nINFO: From Executing extra_action //tools/cpp:checkstyle_cpp on //heron/common/tests/cpp/network:order_unittest:\nIgnoring external/local_config_cc/cc_wrapper.sh; not a valid file name (cc, h, cpp, cu, cuh)\nINFO: From Executing extra_action //tools/cpp:checkstyle_cpp on //heron/stmgr/src/cpp:grouping-cxx:\nIgnoring external/local_config_cc/cc_wrapper.sh; not a valid file name (cc, h, cpp, cu, cuh)\nIt will be nice if we could take care of it as well.\n. @caofangkun - this occurs in my Mac laptop.\n. @caofangkun - travis-ci seems to have failed. Can you please check?\n. @caofangkun - can you update your branch since yours seems to have conflicts?\n. Looks good to me! \ud83d\udc4d \n. ship it \ud83d\udc4d \n. @javi1104 - this has been fixed in 0.14.3 pre-release. Heron tracker polls the zk until some topologies come up. You are right in that it is a ready only service!\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @billonahill - awesome. Can we fix the scripts where we untar? They are the installer scripts.\n. @lucperkins - do you have a site where I can look at this?\n. @lucperkins - sounds good.\n. @lucperkins - any update on this?\n. @lucperkins - any update on this?\n. When we build manually we provide the config= but in IDEA you need to query the platform and accordingly pass the platform. \nfor ubuntu, it is --config=ubuntu\nfor mac, it is --config=darwin\nfor centos, it is --config=centos\nIt will be great if you could provide a patch to fix this.\n. \ud83d\udc4d \n. @lucperkins - if you are fine with this PR, we can merge it. @billonahill is on vacation next week.\n. @lewiskan - can you incorporate comments by @lucperkins?\n. @objmagic - can you test if all the functions of the UI is working?\n. \ud83d\udc4d \n. @nlu90 - thanks for tracking this. can you fix and send a PR?\n. @objmagic - @zuyu and I are working with the travis folks.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @maosongfu - let us review this before we get into other conflicts.\n. @ashvina - please create the issues in github and make a link in the TODO areas in the code. Once it is done, we can merge it.\n. @ashvina - In another PR, could you add documentation for REEF scheduler so that we can add a page in the website - this will be very important for people to use.\n. @maosongfu and @billonahill - if you don't have any other comments, we can merge it. Let me know.\n. @vivilife - Just wondering if you have installed libunwind. We have a docker file for centos7 under the docker/ directory. Please check what are the dependencies that you need to install before compiling in centos7.\n. @ajorgensen - it will be nice if you could push this soon into main repo. @avflor is working alternate and interesting packing algorithms.\n. Should we create something like an 'spi' similar to 'api' - make it a separate package? spi and api are for completely different audience. Let me know your thoughts!\n. Alternatively, if you check in whatever you have, I add a separate package called 'spi' in the next iteration.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @billonahill - can you please review this?\n. @billonahill - can you please check this as well?\n. @lewiskan - is there a possible to make scripts generic so that we update the version in place?\n. Please go ahead and merge it.\nSent from my iPhone\n\nOn Jun 19, 2016, at 6:35 PM, Runhang Li notifications@github.com wrote:\nCI tests done, head reset to ad1bfce, ready for merge.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @ashvina - I will be nice to add this documentation so that others can use it.\n. We might need to extend this a bit - for example, with multiple packing algorithms, the resource consumption might differ. it might be good to list the resources required for each packing algorithm as well? \n. Or it might could be at a fixed location and downloaded when the containers start - similar to Aurora.\n. @hellowBigData - Can you give an example program of how you submit it?\n. @hellowBigData - thanks for the snippet. We will look into this. In our use cases, we do not submit programmatically. Hence, we did not implement this functionality.\n. @avitorovic - thanks for the feedback.\n. Ship it\n. fixed in #1397 \n. @lewiskan - a few questions. Is the image has been built already? If so what is its name?\n. @lewiskan and @tysonnorris - can we close this?\n. @maosongfu and @nlu90 - can you please review this?\n. this looks good to me. @maosongfu and @nlu90 - are you fine with merging it?\n. @lilalinda - that configuration should be in client.yaml - not in scheduler.yaml. The documentation needs to be changed. If you could provide a patch that will be great!\n. @lilalinda - When you use file:/// semantics, it refers to local file. The slave node will not be able to pick up the heron-core.tar.gz - from whichever machine it resides. The key is to place heron-core.tar.gz in a place where it is accessible from even slave machines. For example, you could place it in a webserver and instead of file:/// you could use http://. Alternatively, you could place it in HDFS and adjust your config heron.aurora file.\n\nThere is a detailed blog post on how to install and run it in Aurora\nhttp://streamanalytics.blogspot.com/2016/06/deploying-heron-on-cluster-of-machines.html\nand also a pull request to update the documentation\nhttps://github.com/twitter/heron/pull/992\nWe will continue to refine the documentation so that it is easy to install and run.\n. Close this issue since we are tracking the runtime issue in #1027 \n. @lucperkins - can you please take a look at this?\n. \ud83d\udc4d \n. @lucperkins - some additional documentation, please review.\n. @caofangkun - this is also coming from unknown repository. Can you close this PR and resubmit as a new one so that we can verify it?\n. @objmagic - whenever you get some time, please take a look at this.\n. @lucperkins - additional documentation for REEF/YARN scheduler. Can you please review?\n. @ashvina - are you lacking any software dependencies? \n. @lucperkins - Is this ready to go?\n. Fixed in #942 - we need to launch the site.\n. @lucperkins - can we relaunch the doc site?\n. This is completed.\n. @objmagic - can you investigate the possibility of making role as a mandatory argument?\n. looks good!\n. \ud83d\udc4d \n. @caofangkun - thanks for the contribution.\n. @crazyLT - We don't use Trident at Twitter. Hence we choose not to implement Trident. But since we are API compatible with Storm, implementing Trident should not be hard. Happy to receive contributions.\n. @caofangkun - When I run this, I get the following:\n[tw-mbp-kramasamy heron24 (heron-974)]$ bazel build --config=darwin heron/...\nSLF4J: The following set of substitute loggers may have been accessed\nSLF4J: during the initialization phase. Logging calls during this\nSLF4J: phase were not honored. However, subsequent logging calls to these\nSLF4J: loggers will work as normally expected.\nSLF4J: See also http://www.slf4j.org/codes.html#substituteLogger\nSLF4J: org.eclipse.aether.internal.impl.DefaultRepositorySystem\nSLF4J: org.eclipse.aether.internal.impl.DefaultRemoteRepositoryManager\nSLF4J: org.eclipse.aether.internal.impl.DefaultTransporterProvider\nSLF4J: org.apache.maven.repository.internal.DefaultVersionResolver\nSLF4J: org.eclipse.aether.transport.file.FileTransporter\nSLF4J: org.eclipse.aether.internal.impl.DefaultArtifactResolver\nSLF4J: org.apache.maven.repository.internal.DefaultArtifactDescriptorReader\nSLF4J: org.eclipse.aether.internal.impl.DefaultMetadataResolver\nSLF4J: org.eclipse.aether.internal.impl.DefaultRepositoryConnectorProvider\nSLF4J: org.eclipse.aether.transport.http.HttpTransporter\nSLF4J: org.eclipse.aether.internal.impl.DefaultRepositoryLayoutProvider\nSLF4J: org.eclipse.aether.internal.impl.DefaultRepositoryEventDispatcher\nSLF4J: org.eclipse.aether.internal.impl.DefaultOfflineController\nSLF4J: org.eclipse.aether.internal.impl.DefaultUpdatePolicyAnalyzer\nSLF4J: org.apache.maven.repository.internal.DefaultVersionRangeResolver\nSLF4J: org.eclipse.aether.internal.impl.DefaultUpdateCheckManager\nSLF4J: org.eclipse.aether.connector.basic.BasicRepositoryConnector\n. Hmm... still i am getting this. Note that I am using Mac\n[tw-mbp-kramasamy heron24 (heron-974)]$ bazel clean --expunge \nINFO: Starting clean (this may take a while). Consider using --expunge_async if the clean takes more than several minutes.\n[tw-mbp-kramasamy heron24 (heron-974)]$ bazel build --config=darwin heron/...\n.\nSLF4J: The following set of substitute loggers may have been accessed\nSLF4J: during the initialization phase. Logging calls during this\nSLF4J: phase were not honored. However, subsequent logging calls to these\nSLF4J: loggers will work as normally expected.\nSLF4J: See also http://www.slf4j.org/codes.html#substituteLogger\nSLF4J: org.eclipse.aether.internal.impl.DefaultRepositorySystem\nSLF4J: org.eclipse.aether.internal.impl.DefaultRemoteRepositoryManager\nSLF4J: org.eclipse.aether.internal.impl.DefaultTransporterProvider\nSLF4J: org.apache.maven.repository.internal.DefaultVersionResolver\nSLF4J: org.eclipse.aether.transport.file.FileTransporter\nSLF4J: org.eclipse.aether.internal.impl.DefaultMetadataResolver\nSLF4J: org.eclipse.aether.internal.impl.DefaultRepositoryConnectorProvider\nSLF4J: org.eclipse.aether.transport.http.HttpTransporter\nSLF4J: org.eclipse.aether.internal.impl.DefaultRepositoryLayoutProvider\nSLF4J: org.eclipse.aether.internal.impl.DefaultRepositoryEventDispatcher\nSLF4J: org.eclipse.aether.internal.impl.DefaultUpdatePolicyAnalyzer\nSLF4J: org.eclipse.aether.internal.impl.DefaultUpdateCheckManager\nSLF4J: org.eclipse.aether.connector.basic.BasicRepositoryConnector\n. @supunkamburugamuve - can you try this if you get a chance?\n. @caofangkun - wondering if you are facing the issue that I am facing.\n. Looks good to me. @billonahill - if you are ok with it, we can merge it.\n. @caofangkun - Another option is to compile entire thrift 0.9.3 similar to protobuf that we have. However, thrift compilation takes a long time time though. We will be better off with the current approach.\n. @nlu90 - sounds good!\n. \ud83d\udc4d \n. @SuriyaaKudoIsc - closing since the recommendation is that README is a noun and there is no need for this change.\n. \ud83d\udc4d \n. Looks good!\n. :+1: \n. \ud83d\udc4d\n. @kartik894 - I believe you are using the new Window bolt from Storm 1.0.0+. We have not added the support for it yet. We are looking into this.\n. @kartik894 - would you be interested in contributions for Windows Bolt in Heron?\n. :+1: \n. Can you sign the CLA?\nSent from my iPhone\n\nOn Jun 29, 2016, at 5:43 PM, Neng Lu notifications@github.com wrote:\n\ud83d\udc4d Thanks for the patch!\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. :+1: \n. \ud83d\udc4d \n. @ashvina - Can you please respond?\n. @ideal-hp - let me know so we can close the issue.\n. closing since the issue seems to have been resolved.\n. @nlu90 - can you take a look at this?\n. @caofangkun - can we use bazel to build the docker images?\n\nhttp://www.bazel.io/docs/be/docker.html\nIt will be easier to use a single build system as other software will be moving to bazel\n. @ajorgensen - any thoughts on this?\n. :+1: \n. @ashvina - with the new commit @zhangzhonglai pushed - can you check?\n. @zhangzhonglai - can you update the PR since there are some conflicts? Then I can merge it.\n. @gustavopinto - thanks for the note\n1 - We keep history for window of time. Otherwise, git log grows unchecked and the repo checkout time becomes larger. Furthermore, when a software is open sourced, we don't want to expose the history due to confidential reasons. Based on practical experience, a history more than a year is used very rarely.\n2 - In our case, the core developers pretty much know the history. Again as I said in 1) very rarely there is a need to go back in history beyond a certain time.\n3 - New comers tend to work with the provided environment. In an environment, where the software is continuously tested and everything seems to be working pretty well, new comers hardly tend to look at the history - unless it is really needed, again not more than a given window of time.\n4 - Software evolution using a history is valuable. It is difficult to understand only from code. For such evolution documentation is more helpful.\nJust to reiterate, these are my opinions based on practical experience.\n. :+1: \n. @mhajibaba - This is because when you used HDFS uploader, your aurora file heron.aurora needs to be changed to use hdfs command line. Essentially, you need to change the fetch_heron_system and fetch_heron_package to use hdfs command line in file\nhttps://github.com/twitter/heron/blob/master/heron/config/src/yaml/conf/aurora/heron.aurora\n. @mhajibaba - we fixed one more issue with respect to YARN - especially when heron-shell is started - https://github.com/twitter/heron/pull/1140\n. @severun - we are working on seeing how much effort to add shell bolt. Will keep you posted.\n. @caofangkun - can you please help with this?\n. @caofangkun - any update on this?\n. @severun - We are bit resource crunched on this. Also there are some folks who are doing native C++ topologies. Perhaps that might satisfy your need - not sure.\n. @severun - furthermore, the shell bolt implementation in Storm is bit convoluted. We are rethinking this design and see how we can support it cleanly.\n. looks good to me \ud83d\udc4d \n. @lilalinda - I see an issue in the command line \n$ heron submit --verbose aurora/hadoopuser/test ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology\nAfter --verbose, it should be the name of the cluster (you seem to have given a path). For (e.g.) if you are using the cluster named aurora, you should use\n$ heron submit --verbose aurora ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology\nAlso there is no need to have a master node and 2 slave nodes - instead all three nodes can be cluster. Heron does not have the notion of master.\n. @lilalinda - sure it is fine to keep the configuration and pass cluster/role/env - triplet. As @maosongfu pointed out, can you share the log files?\n. @lilalinda - just wondering if you have installed libunwind in both the master and the slave nodes.\n. That is great to know libunwind is there. Sure, look forward to the logs. We are in Pacific Time.\n. @ajorgensen - can you help by looking at the logs - what could be going wrong? you are the closest since your environment is similar to @lilalinda \n. @lilalinda - thanks for figuring out the issue. I was wondering if you could provide feedback on how to improve this. We released 0.14.1 which is stable as well.\n. @caofangkun - this is related with the #1024 \n. looks good to me. @billonahill - does this PR looks ok?\n. Looks good!\n. @daxwang - should we close this, if this is not an issue?\n. @benley - this is really awesome. As @billonahill mentioned, it will be great have a PR and a debug option. It will be nice if you could integrate these rules permanently into Bazel. Currently, Bazel lacks the ability to build self contained python binaries.\n. Another suggestion would be to upgrade to new version of PEX?  PEX has new versions. If we could use the repository of PEX for code https://github.com/pantsbuild/pex based on a particular release and your rules using git repo rules in bazel WORKSPACE, we can seamlessly upgrade PEX as new versions are being released.\n. @benley - let us see if we could resolve this bug.\n. @benley - we are in the process of upgrading the pex. Please check out the pull request #1052 \n. @benley - with PEX upgraded, it might be good to get pex rules upgraded. Let us know when you are ready.\n. Thanks. Look forward to it.\nSent from my iPhone\n\nOn Jul 14, 2016, at 11:47 AM, Benjamin Staffin notifications@github.com wrote:\nI've got my copy of pex upgraded now. I want to do a little more testing before diving in with heron; stay tuned.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @benley - any update on this project? Let us know - we will be happy to help if you need anything.. @windie - Is this for Storm 1.0.0+?\n. @windie - one of the unit tests seems to have failed in travis-ci. Can you run the unit tests in your machine and see if everything passes? To run unit tests, do the following\n\nbazel test --config=darwin heron/...\n. @windie - restarted. once it finishes successfully, we can merge it.\n. @ajorgensen - I was wondering if you could help.\n. looks good to me!\n. :+1: \n. @ashvina - can you take a look at this?\n. @ajorgensen - looks like the unit tests failed.\n. @lucperkins and @lewiskan - any suggestions on this are welcome.\n. Fixed in #1168 \n. @objmagic - let us create one github issue for each. Each issue can be prefixed by [heron-explorer] so that you can distinguish. Otherwise it will be difficult to track.\n. nice work @caofangkun. @prabhuinbarajan - It will be great if you could review this.\n. @prabhuinbarajan - any luck with this?\n. @caofangkun - it will be great if you could send a patch.\n. Assigned - thank you very much!\n. Looks good to me! Once CI finishes, I will merge it.\n. @caofangkun - I was wondering if bazel works out of box with centos6 - when I tried it did not. I had to compile bazel from the sources and install it. Let me know if you have tried it building using docker.\n. I was trying to build using the docker file - there seems to be a python2.7 issue\nStep 17 : ADD compile-platform.sh /compile-platform.sh\n ---> 8536d40275d7\nRemoving intermediate container 04776f18e153\nSuccessfully built 8536d40275d7\nRunning build in container\nBuilding heron with version 0.14.1 for platform centos6\nExtracting source\n/usr/bin/env: python2.7: No such file or directory\nCleaning up scratch dir\nYou need to install python2.7 - check these out\nhttps://github.com/h2oai/h2o-2/wiki/installing-python-2.7-on-centos-6.3.-follow-this-sequence-exactly-for-centos-machine-only\n. In order to test docker for centos6 you could do the following -\ndocker/build-artifacts.sh centos6 0.14.1 ~/centos6\n. @caofangkun - It looks like gcc and g++ installed in centos6 using yum is 4.4.5 - while we require 4.8+. Looks like it will require installing devtoolset-2 - which is pretty huge.\n. @caofangkun - any update on this?\n. @caofangkun - I ran with the docker file and I am getting the following error.\nchecking for C compiler default output file name... \nconfigure: error: in `/gcc-4.8.2':\nconfigure: error: C compiler cannot create executables\nSee `config.log' for more details.\nThe command '/bin/sh -c tar -zxvf gcc-4.8.2.tar.gz     && cd gcc-4.8.2      && tar -zxvf ../mpc-1.0.1.tar.gz && tar -jxvf ../mpfr-3.1.4.tar.bz2     && tar -jxvf ../gmp-5.1.3.tar.bz2 && tar -jxvf ../isl-0.11.1.tar.bz2     && tar -zxvf ../cloog-0.18.0.tar.gz     && mv mpc-1.0.1  mpc && mv mpfr-3.1.4 mpfr && mv gmp-5.1.3 gmp && mv isl-0.11.1 isl     && mv cloog-0.18.0 cloog     && ./configure --prefix=/usr/local/gcc' returned a non-zero code: 77\nCleaning up scratch dir\n. @caofangkun - any update on this please?\n. @objmagic - can you check with our internal version as @billonahill pointed out so that we can merge this?\n. @objmagic and @nlu90 - can this PR be merged if internal CIs work?\n. @windie - good one. Can somebody review this PR?\n. @windie - does this cover all the java tests?\n. @maosongfu - can you take a quick look whether all the java tests are covered?\n. @windie - please do update the other java code as well just to ensure consistency\n. @windie - provided more feedback.\n. @windie - overall comment is to keep the assignments of temporary variables for deps and use those variables wherever needed. Otherwise, the dependency lists are too long and hard to read. Once you fix these, we could merge the PR.\n. \ud83d\udc4d \n. Looks good to me!\n. @nlu90 @objmagic - can one of you review this?\n. @supunkamburugamuve - just wondering if you could add a documentation page on how to use the uploader. Very similar to other pages.\n. @ashvina - Can you help review the PR?\n. @supunkamburugamuve - please use the documentation format like we have for other uploaders to ensure consistency.\n. :+1: \n. @objmagic - can you check what is the issue?\n. @sirinath - it looks like the bazel support is experimental. However, you are welcome to try it out and provide a patch.\n. @objmagic - can you cut and paste the checkstyle errors for @avflor?\n. @avflor - one broader comment is to write a documentation about this packing algorithm.\n- What does the algorithm do?\n- When is it useful and its pros and cons?\n- How to enable it?\n. @ajorgensen - can you review this PR since you are the closest to write a new packing algorithm?\n. @avflor @ajorgensen @ashvina - I think the consensus to keep the packing algorithms and schedulers orthogonal - the PR for this is here\nhttps://github.com/twitter/heron/pull/1079\n. Is FFD packing algorithm is good enough to merge?\n. Can we open an issue for tracking?\nSent from my iPhone\n\nOn Jul 15, 2016, at 7:12 AM, Andrew Jorgensen notifications@github.com wrote:\n@avflor I think aside from my initial comments it looks good to me. I have other aspirations to have those default padding values be configurable as well but I don't believe that needs to happen in this pass.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @avflor - excellent. @ajorgensen @maosongfu - if you are fine with the PR, we can merge it.\n\n@avflor - should we check in the documentation as a part of this PR as we discussed earlier or it will go in a separate PR?\n. Ok. Please create a github issue and reference it here.\nSent from my iPhone\n\nOn Jul 16, 2016, at 7:44 PM, avflor notifications@github.com wrote:\n@kramasamy Karthik, what do you think about me submitting the documentation PR as a separate one next week? I need some more time to look into the tools and the setup. In the meantime, I have comments on how to use the algorithm in the FirstFitDecreasing.java class.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @billonahill - Will this functionality good for heron explorer?\n. \ud83d\udc4d \n. Looks good to me!\n. @maosongfu - why the claim that it is not scalable? I understand the fault tolerance aspect of it but not sure the scalability part of it.\n. @maosongfu - however when the job is scheduled, it knows upfront the number of containers and the resources it requires. As Mesos keeps giving those offers, we could grab those resources and schedule containers.\n. Sounds good to me. Let us wait until @ashvina takes a look at it.\n. @maosongfu - can we move this scheduler to contrib directory since it is experimental and also does not cover fault tolerance?\n. one of the model that we discussed was the possibility of using a contrib directory - the code is allowed to check into the directory especially those are experimental / getting started. once they attain production quality, we move that code from contrib to main directories. This will allow for faster iteration in contrib rather than the PR waiting to be merged for a long time. This is a model that is followed in other open source projects such as Hadoop. \n\nLooks like a good idea IMO.\n. @maosongfu @ashvina - I am fine with getting Mesos to the core rather than being contrib. As a policy we can adapt the following (LMK if this is ok)\n- contrib directory will contains features/debugging tools/other software that are addons to heron\n- core software will continue to be developed in the master and can reside on their own branches/forks until it attains quality before merging into master\nAny other items that is not covered by these two?\n. @ashvina - Regarding Mesos on Apache REEF, we are good if the project supports it and to some extend proved in production. The key aspect with Mesos is the fault tolerance - native support tend to favor framework scheduler approach for long running jobs while framework per job works for short and terminating jobs.\n. thanks @ashvina for this - it is a good idea to keep packing algorithms and schedulers orthogonal.\n. \ud83d\udc4d \n. @nlu90 - this PR works even with bazel 0.2.3 in my mac.\n. \ud83d\udc4d \n. @windie - now you could work on #524 \n. @maosongfu - I verified with bazel 0.2.3 and it works. Essentially it works with both bazel 0.2.3 and bazel 0.3.0. In Twitter environment, we can continue to use bazel 0.2.3\n. \ud83d\udc4d \n. @mycFelix - can you share your configuration? @ashvina - can you help with this?\n. @mycFelix - awesome! great to know that it works.\n. @maosongfu @ajorgensen @billonahill - can you review this PR?\n. @nlu90 and @maosongfu is this good to go?\n. @nlu90 - we need to fix the styling as well in order for the copyright to go through.\n. looks good - ship it.\n. @mycFelix - have you started the heron-tracker and heron-ui as indicated in the documentation? Look at the getting started guide at http://twitter.github.io/heron/docs/getting-started/\n. @mycFelix - this is a pending feature request. Here is the relevant issue #187 \n. @avflor - is this complete? if it is, please close this.\n. @objmagic - is this completed?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. can we call this packer itself instead of package? the external tool that will be shipped will be called heron-packer\n. @nlu90 - can you please add several unit tests?\n. Please add unit tests. Are you planning in the next PR or this PR?\n. \ud83d\udc4d\n. \ud83d\udc4d\n. We provide the same guarantees as storm - at least once and at most once.\nCheers\n/Karthik \nSent from my iPhone\n\nOn Jul 21, 2016, at 3:00 AM, Majid Hajibaba notifications@github.com wrote:\n@maosongfu your link about guarantees message processing work just on Storm not heron!!!\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. looks good \ud83d\udc4d \n. @ajorgensen - is there a way to display the host information in addition to the container id? This information is lacking.\n. @avflor - looks good to me. @billonahill - let us know if this is ok to go.\n. in tools/bazel.rc, I see darwin config enables check styles but not sure why it does not enforce it.\n. @caofangkun - this is great. One broad comment is we need to retain the copyright information of Storm files.\n. @caofangkun - if check style is an issue due to copyright, please update your branch from the master.\n. @objmagic - please do not close this. we need to think about the design and communicate it to @caofangkun \n. @cliffyg - thanks for the pull request. Since it is the first time you are issuing a pull request - can you sign the CLA form? It is pretty straightforward and entirely can be done on the web.\n\nhttps://engineering.twitter.com/opensource/cla\n. @cliffyg - thanks. merging the fix in. thanks for the contribution.\n. @benley - any updates on this?\n. @benley - any progress on this?. @benley - wondering if you would like to merge these rules into Bazel?. Thanks @benley - we could take care of maintaining the rules. The only issue that I am facing with it - when I run it the first time I get some errors. However, if we run bazel again, it seems to work. Not sure what is the issue - if you could help, it will be great.\nhttps://github.com/benley/bazel_rules_pex/issues/37\n. If the change is for the underlying Heron API - we should change it across all the class as @billonahill pointed out.\n. @caofangkun - we already have Java Style. Please check https://github.com/twitter/heron/blob/master/tools/java/src/com/twitter/bazel/checkstyle/JavaCheckstyle.java whether it satisfies your need.\n. @prabhuinbarajan - there are some errors. can you please check?\n. @objmagic - can you please review this?\n. @taishi8117 @objmagic - can you review this?\n. @prabhuinbarajan - please address the final set of comments.\n. @objmagic - we need to documentation in the web pages. that could be another PR?\n. @prabhuinbarajan and @objmagic - current README is fine. But we need some detailed documentation to be put in the web page. Can you open an issue to track this? We can do the entire documentation in a separate PR.\n. @objmagic - that is not the intended use of heronrc. it used only for optional parameters. In your example, topology file name, topology class are positional arguments or rather required arguments. For example, the following is a valid .heronrc file\nheron:submit:local --verbose\nWhen you submit a command\nheron submit local ~/.heron/examples/heron-examples.jarcom.twitter.heron.examples.ExclamationTopology ExclamationTopology\nautomatically verbose option is enabled. In order words, verbose option is enabled for all the topologies being submitted to local cluster.\n. @objmagic - are we adding a verbose option? or it is going to be in a later PR?\n. \ud83d\udc4d \n. @caofangkun - can you please fix conflicts and ensure all the tests work?\n. \ud83d\udc4d \n. @windie - curious to know what caused this issue.\n. @windie - we have not tried with homebrew environment actually. Since the bazel files are self sufficient, there is no need to use CFLAGS etc. In fact, I would advice to try it out without setting any of the CFLAGS, CXXFLAGS, etc\n. our current master works with both bazel 0.3.0 and bazel 0.2.3.\n. @windie - one more suggestion would be to use the binaries from the release page to construct a brew formula. With this approach, you won't have the need to compile any code at all when you do brew install - it will be fast and easy and no need to even install bazel.\n. @windie - would you be interested in taking a look at this?\n. @windie - this is to support time windows - tumbling window, moving window, etc. it will be awesome, if you could help.\n. @maosongfu - can you take a look at this?\n. @ajorgensen - some syntax errors in travis-ci. please take a look.\n. @ajorgensen - if everything is fine, can you merge it?\n. @ajorgensen @billonahill - is this good to go? I am trying to get 0.14.3 release quickly.\n. @ajorgensen @billonahill - are we getting close to complete this PR?\n. thanks @billonahill \n. @chris-pardy - this is fixed in the PR https://github.com/twitter/heron/pull/1133 and it will be available in the next release 0.14.2\n. \ud83d\udc4d \n. \ud83d\udc4d  @maosongfu do you have any feedback on this?\n. @taishi8117 - a few feedback.\n. \ud83d\udc4d @billonahill - do you have any other feedback?\n. @taishi8117 - one unit test enough to cover sufficiently all the cases?\n. \ud83d\udc4d \n. @kiril-me - Since this is the first time you are contributing, can you please sign the CLA? It is pretty straight forward - https://engineering.twitter.com/opensource/cla\n. @nlu or @objmagic - can you please review this?\n. Thanks @yanxz\n. @caofangkun - can you please check what @billonahill suggested and let us know?\n. @mycFelix - that could be because of trident? currently heron does not support trident.\n. @mycFelix - understood. we will make it a high priority for this. Meanwhile, if you could contribute for this, it will be awesome. I have requested @objmagic to help on this.\n. OK. @caofangkun - can you help us on this?\n. Actually we have internal Kafka Spout which works cleanly for us. The storm kafka spout has diverged. We have a pull request that we need to work on \nhttps://github.com/twitter/heron/pull/804\nto get the Kafka spout in. We are bumping this a priority for the next release - 0.14.3. \n. @mycFelix - @nlu90 has patched the heron api to fix the missing API so that storm-kafka spout can work. Please try it out and let us know. For the bigger picture, we are discussing whether to align the spouts to storm distribution or create heron specific spouts.\n. @nlu90 - can you please add that method and give a custom jar for @mycFelix to try it out?\n. @mycFelix - I have invited you to our heron slack group. Please check your gmail for an invitation - it might help speed up things.\n. this is fixed in #1299 \n. In addition to this - we might need additional fields to indicate - \n- Indicate whether the topology is written in C++/Java/Python/Go etc\n- Higher level framework used as Summingbird, TSAR, SQL, etc\nThis will be helpful later I believe.\n. @billonahill - I was more worried about the migration within Twitter.  If we can consolidate the changes into one release - it makes it little less painful. Otherwise, we can do it a different PR?\n. Fine with me.\n\nOn Jul 25, 2016, at 5:39 PM, Bill Graham notifications@github.com wrote:\nI was advocating not blocking/coupling this PR with the feature you're talking about. It's best to separate features into different PRs/issue for better tracking.\nWe can put other things in this release if we want to, but let's deal with it separately.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @taishi8117 - can you look at the tracker code as well to ensure these changes do not affect? This is because tracker code use these protos as well.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. Looks good!\n. @daxwang - wondering what is the use case for tuple.getSourceTask()?\n. @avflor - the approach is fine with me. @maosongfu, @ajorgensen and @billonahill can you please check this PR?\n. One broader question - will it affect the existing tuning configuration of current running jobs? Will it require lesser resources than what is provided.\n. @avflor - thanks for the detailed explanation. conservatively, can we code this as a separate algorithm - something like Enhanced Round Robin. I can ask a few topologies to turn on the Enhanced Round Robin and see how it works before we roll out widely. If the existing algorithm is changed and we don't know its behavior, it will be difficult to roll back - unless we roll back the release. Thoughts?\n. @avflor - it is definitely possible to configure each topology with a different packing algorithm. All you have to do is during heron submit pass the option\n\n--config-property \"heron.class.packing.algorithm=<packing-classname>\"\nit should work. let me know if this does not work.\n. @avflor - you can override any config at the command line using --config-property as long as you know its implications. The only inconvenience is you have to type it every time you submit in the CLI. With this PR #1119, you could setup an .heronrc file in your home directory and set the config property there and you don't have to do in command line every time.\n. @avflor - we already enforce checkstyle when you compile. probably the empty line rule might not be there.\n. @ajorgensen - if you have any additional comments, let us know - so that we can merge it.\n. @avflor - can you please resolve the conflicts?\n. @maosongfu - can the documentation be added in a separate PR? @nlu90 - please create another issue to track this.\n. @nlu90 - you might have to resolve conflicts.\n. \ud83d\udc4d As @ashvina mentioned, please create the SSL issue - which we can resolve later.\n. :+1: \n. \ud83d\udc4d \n. It should be  \"No topologies in cluster\"\n. Look good. Ship it!\n. @nlu90 / @ajorgensen - Should we merge this PR?\n. \ud83d\udc4d \n. \ud83d\udc4d  merge it if @billonahill is fine with it.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @nlu90 - is this done?\n. Rohan - \nthe documentation is in for Marathon DC/OS. It is in the master but officially not published yet.\ncheers\n/karthik\n\nOn May 22, 2017, at 5:07 PM, Rohan Agarwal notifications@github.com wrote:\nAny updates on this? Some documentation would be really helpful.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/issues/1173#issuecomment-303253220, or mute the thread https://github.com/notifications/unsubscribe-auth/AAWcREL4JGVJ6qSLVT8cuc1q9LQs_HtVks5r8iNagaJpZM4JWlx0.\n\n\n. Currently, we don\u2019t have this functionality but should be easy to add. Currently, we accept a bunch of user key value pairs and pass it on. We could use this mechanism to pass it to marathon scheduler. The only question is which parameters go where.  If you are interested, we welcome contributions to tackle this.\ncheers\n/karthik\n\nOn May 22, 2017, at 6:45 PM, Rohan Agarwal notifications@github.com wrote:\nThanks @kramasamy https://github.com/kramasamy. Is there any way to provide custom Marathon properties (like job constraints) to the Scheduler?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/issues/1173#issuecomment-303266710, or mute the thread https://github.com/notifications/unsubscribe-auth/AAWcRF7uJwDQldzQqDatlGouvwWDcApAks5r8jo2gaJpZM4JWlx0.\n\n\n. \ud83d\udc4d \n. @nlu90 - if everything looks good, can you merge it?\n. @wking1986 - it looks like a PEX issue. When pex is exploding the zip file, it seems to be writing into a directory where permission is denied. Are you running Mesos locally? Can you do ls -al of the directory where you have heron-executor.stderr logs. You should see a directory called .pex there. If that is not there, there is some issue.\n. thanks @objmagic - @wking1986 - what user do you run your topologies as?\n. @ashvina - can you please take a look why the tests failed?\n. I think it is ok to break backward compatibility - since a bunch of changes have been there for proto and further additions are coming as well.\n. I think this is a good time to change - if needed, so that we can minimize incompatible migrations.\n\nOn Jul 29, 2016, at 10:15 AM, Bill Graham notifications@github.com wrote:\n@kramasamy I was also wondering about that. Internally we don't have any production jobs on OSS heron, by the looks of the task tracker. If we can break backward compatability we could do the right thing. @taishi8117 do you want to work on a patch to refactor our proto classes to use the pattern that @objmagic showed above?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Ship it \ud83d\udc4d\n. @tysonnorris - can you please help?\n. Looks good to me. @objmagic - please review.\n. @saifrahmed - do you want to try this repo? https://github.com/kramasamy/heron-starter\n\nIf that is not what you intended, can you point out to some examples of how you do it? It will be even better if you could contribute, we will be happy to review and merge - it will be useful for others as well.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d If @objmagic is good with this, let us merge this.\n. \ud83d\udc4d \n. @objmagic - do you have any comments on this?\n. @taishi8117 - can you please take care of @objmagic feedback and we should be good to merge it.\n. @daxwang - thanks for the use case, definitely interesting. Will discuss this with our team and keep you posted.\n. @objmagic - sure.\n. \ud83d\udc4d \n. @objmagic - while this is interesting, it might not be needed since bazel itself parallelizes the build. With two level parallelization it might be hard to identify where the bug should be.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @ashvina - can you please resolve the conflicts?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @windie - I would suggest to use a stable version such as v1.0.1, then when we upgrade we can do the next version - it is very predictable.\n. thanks @windie - really appreciate the contribution.\n. @windie - we can add you into our slack group of heron committers. But I don't know your email address.\n. Invited - please remove your email from the comment.\n. @windie - this should go into heron/storm - since this is storm api 1.0.0 compatible. Heron api under heron/api - is not supposed to be used by the users - since it is a hidden API.\n. @nlu90 @objmagic - can you co-review the PR and discuss the design?\n. @maosongfu - can you review this?\n. @windie - can you please add some detailed documentation for the types of windows supported? We want to verify and close this as soon as possible.\n. @windie - how can we write unit tests for this?\n. @windie - any update on this?\n. ok sounds good. Can we do separate PR for excluding style checking?\n. @maosongfu - any idea which directories under heron/storm is written by us? I can take care of separating it out.\n. probably good to keep the heron executor simple with minimal dependencies?\n. just the move to proto3 might not help - we need to enable the cpp implementation for python. will work on this in the next PR.\n. Bill - it would be good to track this issue so that we don't drop it on the floor.\n\nOn Aug 5, 2016, at 1:19 PM, Bill Graham notifications@github.com wrote:\n@ashvina good observation. I'd love to remove then and that was my initial intent, but the executor needs that info and is written in python. Serializing it means we don't need to change the executor code, so I kept in there for now. Without it we'd need to reimplement that logic in python to reconstruct. I don't love having the logic implemented in two places, but I agree we might want to do that instead before we ship scaling, but in a later patch.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \ud83d\udc4d  @ashvina - do you want to take a quick look before merging it?\n. \ud83d\udc4d \n. @lukess - thanks for the contribution. Can you please accept the Twitter CLA?\n\nhttps://engineering.twitter.com/opensource/cla\n. @objmagic - can you please comment why this is needed?\n. please add this comment in the code.\n. \ud83d\udc4d \n. Ship it!\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. It looks fine to me - rest of the stuff \ud83d\udc4d  (once you fix the comment)\n. \ud83d\udc4d \n. @objmagic - is this ready to be merged?\n. Ok - sounds good. I will merge it tomorrow.\n. @xcf1992 - the com.twitter.heron.api is an internal API and you are discouraged to use it. You should use either backtype.storm.* or org.apache.storm.* \n. @maosongfu - can you please take a look at this?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @objmagic - any comments?\n. But @taishi8117 needs to pull from the master and then only it will pass, isn't?\n. Ship it.\n. \ud83d\udc4d \n. Awesome! finally it passed.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d  @objmagic - can you check this as well?\n. thanks @ajorgensen - just wondering who would like to pick this up @ashvina, @ajorgensen, @nlu90 \n. @lukess - as suspected the python check style does not seem to accept long lines.\n. \ud83d\udc4d \n. @maosongfu - can you please take a look at this?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. +1\n. :+1: \n. ui, tracker, cli and explorer are working. Let me know if this PR is good to go.\n. @kylozw - this issue has been reported sporadically. We did a fix and is in the master. If you compile and install from the master, the issue should go away. Please try it and let us know.\n. When building - you need to first configure bazel. You need to run\n./bazel_configure.py\nCheers\n/Karthik\nOn Fri, Aug 19, 2016 at 6:04 PM Mark Li notifications@github.com wrote:\n\nshould be resolved in #1276 https://github.com/twitter/heron/pull/1276\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/1280#issuecomment-241156397, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAWcRCgaF53ripK7Y6gGjCcqPtCaPq4Jks5qhjaTgaJpZM4JoQt9\n.\n. \ud83d\udc4d\n. Looks good - \ud83d\udc4d\n. Sorry to get back on this late. If I remember correctly, the reason why we did Config runtime is to future proof any other runtime config that we might need.  Now passing only Topology.TopologyAPI makes it restrictive. This means we need to change current implementations to use this interface and later if we need any additional stuff, we might have put back Config runtime - again leading to more changes - IMO.\n. +1 - let us go with the refactoring.\n. @objmagic @taishi8117 - any update on this?\n. How can you set the variable at run time?\nOn Aug 22, 2016, at 1:03 PM, Mark Li notifications@github.com wrote:\nThis PR makes heron-executor takes environmental variable HERON_PROFILE so that we can profile Python topology at run-time.\nNot really sure if HERON_PROFILE is a good name candidate here.\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/twitter/heron/pull/1293\nCommit Summary\nTake profiling env var.\nFile Changes\nM heron/executor/src/python/heron_executor.py (9)\nPatch Links:\nhttps://github.com/twitter/heron/pull/1293.patch\nhttps://github.com/twitter/heron/pull/1293.diff\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. It will work only in local mode - not in distributed mode, correct?\nOn Aug 22, 2016, at 3:16 PM, Mark Li notifications@github.com wrote:\none can set this env var in shell before submitting topology, and heron-executor takes this env var at run-time.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @taishi8117 - that is a good suggestion. I am looking for something similar to Java, where you could enable it for a particular spout/bolt - during runtime. Is it possible?\n. @objmagic - my concern is to have some consistency across the languages.\n. @objmagic - is this done?\n. I will be great if we could do this cleanup before it is too late - IMO.\n. @objmagic - can you create issues for @billonahill 's suggestions so that we can track them.\n. @ashvina and @billonahill - who wants to pick this up.\n. @ashvina and @billonahill - if you could collaborate and come with a plan, it will be great!\n. \ud83d\udc4d \n. @nlu90 - This patch will suffice for running Storm kafka spout. Or we need this for heron-kafka spout as well?\n. @nlu90 - instead of introducing another set of configuration for the spout, I was wondering, can we think about making it simpler. Looking at the config - kafka spout - seems to adding a lot of config and hard coded as well. We want to move away from hard coding the config into the topology. We can introduce the notion of config per component - in a cleaner fashion.\n. ship it \ud83d\udc4d \n. @objmagic - can you fix the travis?\n. @objmagic - can you resolve the conflicts and update the PR?\n. @objmagic - is this ready to go?\n. looks good to me \ud83d\udc4d @billonahill and @maosongfu - do you have any comments?\n. @ashvina - can you resolve and update so that we can merge it?\n. looks good to me \ud83d\udc4d \n. furthermore, these statements are not seen by the user since heron executor is automatically started by the scheduler\n. Can we create this node automatically similar to other nodes?\nOn Aug 25, 2016, at 3:37 PM, Bill Graham notifications@github.com wrote:\nYeah, you need to manually create state/local/packingplans in ZK.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @billonahill - this is great. I was wondering if there is a way to discourage people using an API - like warnings etc. I see sometimes people are using directly the Heron API - which should be completely discouraged. If we can find a way to enforce, it will be great!\n. @billonahill - ok. I think it is better than the current state which is nothing.\n. \ud83d\udc4d \n. @billonahill - this is great!.\n. @billonahill - should we merge this?\n. sorry - I did not see the change in status at the top :(\n. \ud83d\udc4d \n. \ud83d\udc4d \n. nice cleanup. Look good to me \ud83d\udc4d \n. @zuyu - do you have a quick step hash function that we can for uniform distribution - despite the presence of skew.\n. @zuyu - can we use it for other data types?\n. @taishi8117 - if you could take a look at this, it will be awesome.\n. it looks fine to me \ud83d\udc4d \n. let us wait until @taishi8117 gives feedback.\n. Can you identify which integration test is unstable?\nOn Aug 30, 2016, at 10:04 AM, Mark Li notifications@github.com wrote:\nseems integration test is very unstable. restarted CI...\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/pull/1309#issuecomment-243508505, or mute the thread https://github.com/notifications/unsubscribe-auth/AAWcRF-ktaKuwqo-QKkdfSkGYCrJ9TAjks5qlGKcgaJpZM4JvL0V.\n. @waitingkuo - I think the actual fix could be for the tracker to wait until those directories/zk nodes are created by polling every second. Once we have this - independent of whether the tracker starts first or later, it will correctly.\n. @waitingkuo - sure please try this out. If you want to just install the tools, do the following \n\nbazel run --config=<your-platform> -- scripts/packages:heron-tools-install.sh --user\nand then run heron-tracker and heron-ui before launching topologies and see if that works.\n. Please send a pull request\n\nOn Aug 30, 2016, at 5:31 AM, Wei-Ting Kuo notifications@github.com wrote:\ndocker-compose work normally after I add this\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @ashvina - that is a start. But we need to continue to be mindful of it during the code review. Then we need to clean up our existing code.\n. @nlu90 - can we write a build file so that we can build using bazel under the config, if needed.\n. @nlu90 - the reason why I suggested BUILD file, we can occasionally use it for release purposes and also ensure that the code build manually.\n. I was wondering do they have unit tests as well\n. should we change src/jvm to src/java directory to be consistent with others?\n. @nlu90 - can you make changes as @mycFelix mentioned?\n. Ship it!\n. \ud83d\udc4d \n. @hsc - yes it is on its way. closing this.\n. \ud83d\udc4d \n. Java topologies\nOn Aug 31, 2016, at 8:46 AM, Mark Li notifications@github.com wrote:\nIs this Python or Java topologies?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/issues/1322#issuecomment-243807515, or mute the thread https://github.com/notifications/unsubscribe-auth/AAWcRPVgvmWL7fCL9czeXlwo1YkMl7Rnks5qlaHHgaJpZM4JxvDm.\n. Ship it \ud83d\udc4d \n. We are working on a kafka spout for Heron - check the pull request \n\nhttps://github.com/twitter/heron/pull/1317\nOnce it is merged, we will publish a JAR in maven central it will seamlessly work with Heron\n\nOn Sep 2, 2016, at 12:42 AM, khushboo13 notifications@github.com wrote:\nHi,\nWhile using kafka spout with heron I am getting following exception.\n[2016-09-02 13:08:37 +0530] storm.kafka.DynamicBrokersReader INFO: Read partition info from zookeeper: GlobalPartitionInformation{partitionMap={0=10.14.24.194:9092, 1=10.14.24.192:9092}}\n[2016-09-02 13:08:37 +0530] com.twitter.heron.instance.HeronInstance SEVERE: Exception caught in thread: SlaveThread with id: 12\njava.lang.NoClassDefFoundError: storm/trident/spout/ISpoutPartition\nat java.lang.ClassLoader.defineClass1(Native Method)\nat java.lang.ClassLoader.defineClass(ClassLoader.java:760)\nat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\nat java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\nat java.net.URLClassLoader.access$100(URLClassLoader.java:73)\nat java.net.URLClassLoader$1.run(URLClassLoader.java:368)\nat java.net.URLClassLoader$1.run(URLClassLoader.java:362)\nat java.security.AccessController.doPrivileged(Native Method)\nat java.net.URLClassLoader.findClass(URLClassLoader.java:361)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\nat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\nat storm.kafka.trident.GlobalPartitionInformation.getOrderedPartitions(GlobalPartitionInformation.java:54)\nat storm.kafka.KafkaUtils.calculatePartitionsForTask(KafkaUtils.java:215)\nat storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:80)\nat storm.kafka.ZkCoordinator.getMyManagedPartitions(ZkCoordinator.java:69)\nat storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:135)\nat backtype.storm.topology.IRichSpoutDelegate.nextTuple(IRichSpoutDelegate.java:67)\nat com.twitter.heron.instance.spout.SpoutInstance.produceTuple(SpoutInstance.java:271)\nat com.twitter.heron.instance.spout.SpoutInstance.access$100(SpoutInstance.java:42)\nat com.twitter.heron.instance.spout.SpoutInstance$1.run(SpoutInstance.java:176)\nat com.twitter.heron.common.basics.WakeableLooper.executeTasksOnWakeup(WakeableLooper.java:142)\nat com.twitter.heron.common.basics.WakeableLooper.runOnce(WakeableLooper.java:74)\nat com.twitter.heron.common.basics.WakeableLooper.loop(WakeableLooper.java:64)\nat com.twitter.heron.instance.Slave.run(Slave.java:169)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ClassNotFoundException: storm.trident.spout.ISpoutPartition\nat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n1249,1-8 81%\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/issues/1327, or mute the thread https://github.com/notifications/unsubscribe-auth/AAWcRDT4YYK1aNWNqHYma35bIssknMvaks5ql9NwgaJpZM4JzcuD.\n. We are very close to merging it in the master. The JAR should be available next week. @nlu90 - can you publish this jar into maven central early next week?\n. @khushboo13 - we are almost ready to get the release out and push the heron-kafka spout into maven. I will try to get this done tomorrow and keep you posted.\n. @khushboo13 - based on my understanding, you need a kafka spout with the name space backtype.storm, is that correct? Our rational for supporting org.apache.storm is that it is going to be API moving forward. Hence we thought we will support the spouts for org.apache.storm.\n. Nice one!\n. @objmagic - do not merge this unless @ashvina says yes.\n. \ud83d\udc4d \n. Looks good to me! @ashvina and @avflor - can you review this please?\n. \ud83d\udc4d \n. Ship it!\n. \ud83d\udc4d \n. @ashvina - that is fine.\n. \ud83d\udc4d \n. Fixed in #1339 \n. please write a comment - that it is used for local testing only \ud83d\udc4d \n. other than that it looks fine. @taishi8117 - can you just take a quick look at it?\n. \ud83d\udc4d \n. Should we merge this or wait until the licensing issue is resolved?\n. Should we merge this?\n. @ashvina - if this is not needed, please close this.\n. to show heron version, do the following \n\ncmdline> heron version\n. @ashvina - can help with this?\n. @HosiYuki - Does @mycFelix answer work for you?\n. @HosiYuki - the situation * is fixed in the master. common-cli and curator jars are already baked into other jars. @maosongfu - can you please comment?\n. @ashvina - is this config a part of scheduler.yaml?\n. @jomsdev - thanks for the PR. I was wondering if you could sign the CLA since you are committing for the first time - \nhttps://engineering.twitter.com/opensource/cla\n. @jomsdev - just wondering if this is tried in 0.14.2\n. @jomsdev - just wondering if this issue is resolved. if so we can close this.\n. If you could please try with 0.14.3 and see if that works or not. Then we can close the issue\n\nOn Sep 15, 2016, at 3:00 PM, Jordi Montes Sanabria notifications@github.com wrote:\nI could install it compiling it from source. However, using the script that you provide (0.14.02) I couldn't. It was using a fresh Ubuntu.\nI would close it and take it into account only if anyone else complains about the same problem.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Thanks for the PR. @jomsdev - I was wondering if you could share the issues that you faced with Ubuntu installation so that we can fix it.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. is this out of master?\n. closing the issue.\n. \ud83d\udc4d \n. \ud83d\udc4d  @ashvina @avflor - do you have any comments on this?\n. \ud83d\udc4d \n. \ud83d\udc4d - nice cleanup!\n. \ud83d\udc4d \n. @au80 - Can you try the following?\n\nbazel build --config=centos --verbose_failures heron/...\nit give an idea what caused the failure.\n. @Au80 - weird, we did not encounter such an issue. How about checking the docker/Dockerfile.centos7  file and see if all the software that is being installed in there is available in your machine?\n. @Au80  - Can you use docker to build your centos build and see if it works? Check the documentation in the following URL and follow the directions - for compiling from source\nhttps://github.com/twitter/heron/tree/master/docker\n. the command should be \ndocker/build-artifacts.sh centos7 master ~/heron-centos\nIn the above command, \n- centos7 is the platform for which the Dockerfile.centos7 exists, \n- master represents the version since you are building from the master (but you can give anything)\n- heron-centos - is the name of the output directory where the final packages will be stored.\nHope this helps.\n. @Au80 - it looks like you aborted it in the middle with ^C. Is that correct? It takes ~20 mins compile actually.\n. @Au80 - are you connected to internet, while compiling? We download a bunch of jar packages, python packages which compiling.\n. @mtunique - just wondering what platform you are building with?\n. @mtunique - perhaps I was not clear, whether you are building with centos, ubuntu or mac? If so, can you specify the version?\n. @mtunique - I just compiled in mac and everything seems to be ok. I was wondering if you have docker installed in your machine. If so, you can try compiling it in docker and see if that passes. For docker compilation, do the following\ndocker/build-artifacts.sh ubuntu14.04 master ~/heron-output\nand see if it works?\n. @mtunique - also wondering what command did you give while compiling?\n. @mtunique - oh! ok. Let us try to compile in command line and see if that works in your machine. At the root directory of heron, do the following:\n./bazel_configure.py\nbazel build --config=ubuntu heron/...\n. @mtunique - ok cool. Can you send a pull request?\n. @mtunique - thanks for the PR. Can you accept the CLA agreement since this is the first time, you are initiating a pull request?\nhttps://engineering.twitter.com/opensource/cla\n. @mtunique - once the CI finishes, I will merge it.\n. ship it. We will merge it once the CI finishes.\n. ship it. \ud83d\udc4d \n. @ajorgensen - can you please review this?\n. @moomou - can you fill up the CLA agreement since it is the first time you are committing?\nhttps://engineering.twitter.com/opensource/cla\n. @moomou - can you please sign the CLA and also incorporate the feedbacks by @ajorgensen?\n. @moomou - a gentle reminder to sign the CLA\n. @moomou - is this ready to be merged?\n. cool - once the ci finishes, i will merge this.\n. \ud83d\udc4d \n. @objmagic - it will be good to add the check as @billonahill suggested. \n. \ud83d\udc4d \n. Nice! \ud83d\udc4d \n. @mycFelix - Definitely, it is an interesting use case. However, this is more embedded in the application logic, isn't?\n. @mycFelix - As far as I remember, Kafka spout used to store the offsets in ZK. That became a source of issue for us. At a large scale, the spouts were hitting ZK hard that ZK was unable to keep up. The functionality of today-start-offset is with the spouts isn't? - since the spouts are the ones that read the offset actually isn't?\n. \ud83d\udc4d \n. @tobecontinued - thanks for the pull request. Can you please sign the CLA since it is your first commit?\nhttps://engineering.twitter.com/opensource/cla\n. please sign the CLA since it is your first commit \nhttps://engineering.twitter.com/opensource/cla\n. ship it \ud83d\udc4d \n. \ud83d\udc4d \n. @ashvina and @billonahill - is this ready to go?\n. @congwang - some failure in unit tests.\n. @congwang - we might need similar metrics between stream managers as well? @pankajgupta - can you comment?\n. @yunfanfighting - Heron has C++ and Java code. Due to C++, the code has to be explicitly ported to another platform before you can run on it.\n. @yunfan123 - yes, you can. We already support it. Check the documentation at \nhttps://github.com/twitter/heron/tree/master/docker\n. @ashvina - should we add the documentation for 0.14.2 since we are already at 0.14.3? 0.14.4 is going to go out soon.\n. thanks @mycFelix - let me check with @ashvina if he really needs it. \n. @ashvina - sounds good.\n. ship it \ud83d\udc4d \n. \ud83d\udc4d \n. @lucperkins - can you review this PR please?\n. thanks @lucperkins - this looks excellent. @vidit-bhatia - please review and use the contents provided by @lucperkins \n. @vidit-bhatia - just wondering if you had a chance to take a look at this.\n. @vidit-bhatia - please update the pull request with the content of @lucperkins - then I can merge it.\n. \ud83d\udc4d  - once CI passes.\n. \ud83d\udc4d \n. \ud83d\udc4d - once CI passes.\n. \ud83d\udc4d  ship it.\n. For us it takes around 20-30 mins to compile the entire codebase. Are you having difficulty downloading some jars?\n\nOn Sep 21, 2016, at 10:51 PM, yunfan123 notifications@github.com wrote:\nI use docker to compile the heron.\nStill waiting for 11 jobs to complete:\nRunning (standalone):\nExecuting extra_action //tools/cpp:checkstyle_cpp on //third_party/zoo\nkeeper:zookeeper-cxx, 59306 s^M^M\nExecuting extra_action //tools/cpp:checkstyle_cpp on //third_party/lib\\\nevent:libevent-cxx, 59306 s^M^M\nExecuting extra_action //tools/cpp:checkstyle_cpp on //third_party/glo\\\ng:glog-cxx, 59306 s^M^M\nExecuting extra_action //tools/cpp:checkstyle_cpp on //heron/proto:pro\\\nto_physical_plan_cc, 59145 s^M^M\nExecuting extra_action //tools/cpp:checkstyle_cpp on //heron/proto:pro\\\nto_stmgr_cc, 59144 s^M^M\nExecuting extra_action //tools/cpp:checkstyle_cpp on //heron/proto:pro\\\nto_topology_cc, 59135 s^M^M\nExecuting extra_action //tools/cpp:checkstyle_cpp on //heron/proto:pro\\\nto_stats_cc, 59081 s^M^M\nExecuting extra_action //tools/cpp:checkstyle_cpp on //third_party/jan\\\nsson:jansson-cxx, 58959 s^M^M\nExecuting extra_action //tools/cpp:checkstyle_cpp on //third_party/gpe\nThe jobs checkstyle_cpp takes near 16 hours. \nIt seems not so necessary to have this job.\nCan I skip job like this? And how can I do this.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Checkstyle cpp ensures that the coding style is enforced. As long as you can access maven central we are good to go. Not sure what is going wrong. One possibility is to try compiling one directory at a time under .../heron and isolate what the problem is.\nOn Sep 21, 2016, at 11:11 PM, yunfan123 notifications@github.com wrote:\n@kramasamy , all of left tasks is checkstyle_cpp. What task is this task do? \nI'm in China, but I'm sure my compile env can access maven.twttr.com and http://central.maven.org/maven2/ normally.\nIs there any jars from other maven central?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @yunfan123 - there are prebuilt docker images. Just wondering what platform are you compiling it for?\n. Ok - which version of Ubuntu are you trying to build on?\n. I built on Ubuntu 14.04 and it is pretty good.\n. It will be great if you could send a PR request for this.\n. @chris-pardy - Can you provide an elaborate description about what this PR does and how is intend to be used so that we can understand.\n. one more meta question - is a single docker container runs the entire topology or fragments of the topologies run in multiple containers constitute a single topology?\n. @chris-pardy - one more question, you might know this already. In the case of Aurora, you need to supply *.aurora file. In the aurora file, you need to have explicit instructions to download the docker image from the registry before you can start it, correct? If the topology image is different for different topologies, how can you specify different image URLs for different topologies - since the aurora file is part of the configuration and is static. \n\nBTW, If you are interested in the heron committers slack channel,  I would need your email address.\n. Got it. The heron.aurora file need to be changed to use docker pull if we are using docker uploader for that cluster configuration. Furthermore, launching of the executor needs to be changed as well  - the docker container should be informed on how to start the heron executor.\n. @chris-pardy - one more question, can we do docker all the way where we run the images directly in the container? similar to what is used at http://aurora.apache.org/documentation/latest/reference/configuration/#docker-object\nThe docker image when it starts needs to start heron-executor and a clean way to view logs in the running container.  This will be really useful since Kubernetes/ECS can leverage this e2e docker for deploying containers. thoughts?\n. @chris-pardy - one thought we had was - if we build a standard base image then we can do docker all the way including the launch of container and within the container starting various process in the appropriate sequence etc.  Of course, enterprise can construct a new image based on this image - to address their needs - especially with respect to their needs.\n. @chris-pardy - I build heron using docker for all our releases and it seems to doing fine. Typically, it takes 30-35 minutes to make a release. If you are doing it for the first time, docker image building takes some time since it installs various software. But after the first time, it should go faster.\n. @ajorgensen - any update on this PR?\n. @chris-pardy - no worries. We are moving into a direction where the docker images are prebuilt for each heron release and is available for you (based on your's and John's). The only remaining aspect is how to upload the job jar and pull it in the docker image that is running. We are currently using an URL based approach. Your PR gave a lot of valuable insights. Currently we are running seamlessly with this approach in DC/OS - Marathon.. @chris-pardy - meta comment. Do we need two configs for the path? A single config will be sufficient, isn't?\n. @chris-pardy - can you please fill in the CLA?  https://engineering.twitter.com/opensource/cla\n. @chris-pardy - no idea why the ci did not kick in. Try another commit and update the pull request?\n. @chris-pardy - some Java style issue, \"unused import\"\n. This seems to be simple fix - \n```\ngit diff\ndiff --git a/heron/schedulers/src/java/com/twitter/heron/scheduler/aurora/AuroraScheduler.java b/heron/schedulers/src/java/com/twitter/heron/scheduler/aurora/AuroraScheduler.java\nindex 64ecffb..82db52c 100644\n--- a/heron/schedulers/src/java/com/twitter/heron/scheduler/aurora/AuroraScheduler.java\n+++ b/heron/schedulers/src/java/com/twitter/heron/scheduler/aurora/AuroraScheduler.java\n@@ -14,7 +14,6 @@\npackage com.twitter.heron.scheduler.aurora;\n-import java.io.File;\n import java.nio.charset.Charset;\n import java.util.ArrayList;\n import java.util.HashMap;\n@@ -34,7 +33,6 @@ import com.twitter.heron.common.basics.FileUtils;\n import com.twitter.heron.proto.scheduler.Scheduler;\n import com.twitter.heron.scheduler.UpdateTopologyManager;\n import com.twitter.heron.spi.common.Config;\n-import com.twitter.heron.spi.common.ConfigKeys;\n import com.twitter.heron.spi.common.Context;\n import com.twitter.heron.spi.common.Misc;\n import com.twitter.heron.spi.packing.PackingPlan;\n```\nBut after fixing the checkstyle, it seems to be failing in the unit test\n. @chris-pardy - looks good. there were some flaky integration tests. we are taking a look at them.\n. @chris-pardy - is this ready to go?\n. \ud83d\udc4d \n. @congwang and @maosongfu - CI is failing.\n. \ud83d\udc4d \n. @congwang @maosongfu - looks like all the tests passed, should we go ahead and merge it?\n. @congwang - is this completed?\n. @wangli1426 - this is definitely interesting. @billonahill and @avflor - can you please take a look at this proposal?\n. @taishi8117 and @objmagic -  can you please take a look at this?\n. +1. @billonahill @ashvina @avflor - can you please look at this change?\n. @ashvina - is this good to go?\n. \ud83d\udc4d  ship it\n. @chris-pardy - checkstyle did not pass in python\n. @chris-pardy - let me give it a try with your code in my machine.\n. @chris-pardy - I was able to reproduce the error. When I looked at the BUILD file, I realized that you need to use one pex_test macro per test file. Hence I modified the BUILD file into \n```\nload(\"/tools/rules/pex_rules\", \"pex_test\")\npex_test(\n    name = \"configloader_unittest\",\n    srcs = [\n        \"configloader_unittest.py\",\n    ],\n    deps = [\n        \"//heron/statemgrs/src/python:statemgr-py\",\n    ],\n    reqs = [\n        \"py==1.4.27\",\n        \"pytest==2.6.4\",\n        \"unittest2==0.5.1\",\n    ],\n    size = \"medium\",\n)\npex_test(\n    name = \"zkstatemanager_unittest\",\n    srcs = [\n        \"zkstatemanager_unittest.py\",\n    ],\n    deps = [\n        \"//heron/statemgrs/src/python:statemgr-py\",\n    ],\n    reqs = [\n        \"py==1.4.27\",\n        \"pytest==2.6.4\",\n        \"unittest2==0.5.1\",\n    ],\n    size = \"medium\",\n)\npex_test(\n    name = \"statemanagerfactory_unittest\",\n    srcs = [\n        \"statemanagerfactory_unittest.py\",\n    ],\n    deps = [\n        \"//heron/statemgrs/src/python:statemgr-py\",\n    ],\n    reqs = [\n        \"py==1.4.27\",\n        \"pytest==2.6.4\",\n        \"unittest2==0.5.1\",\n    ],\n    size = \"medium\",\n)\n```\nAnd all the unit tests pass.\n. nice - thanks for fixing this @nlu90 \n. @nlu90 - is this good to go?\n. Please check out the docker uploaded PR it might do part of the stuff that you might need.\n\nOn Oct 9, 2016, at 11:00 PM, mou notifications@github.com wrote:\nI will start to look into this.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @caniszczyk - permission granted. Anybody can comment on it? Can you point this to kubernetes contributors so that we can get some feedback?\n. @billonahill - is this completed or should be moved to subsequent releases?\n. @billonahill - can we do simpler one? Wondering if a solution that creates an ephemeral node in ZK with the topology name by the client or whoever is responsible and hold on to it until it is done?\n. Gave one feedback - other than that it looks good \ud83d\udc4d \n. \ud83d\udc4d \n. thanks for the contribution @reconditesea - since this is the first time, you are sending a PR, can you please sign the CLA agreement?\n\nhttps://engineering.twitter.com/opensource/cla\n. @reconditesea - any reason why closed this? \n. looks good to me. @maosongfu / @congwang - can you take a quick look at it?\n. @objmagic - are there any flaky integration tests? I am also getting the same as well\n. @taishi8117 - can you help? @objmagic - can you add some logs and see what is happening?\n. ok - if the tests pass, we should merge it.\n. \ud83d\udc4d \n. @billonahill - what is the advantage of the new location vs old location?\n. @billonahill - understood.\n. @maosongfu - the title says that it is the stream manager instead of metrics manager? Can you please correct?\n. @maosongfu - is this completed?\n. @maosongfu - what issue does that resolve?\n. @maosongfu - instead of rejecting the large packet, can we gracefully consume it? In the PR, you are reading the packet size and ignoring it. But it might not work in a TCP connection since you have to consume the entire packet before discarding it, correct?\n. @maosongfu - if you look at the problem more deeper, it is initiated because user chose to define a metrics that is pretty large. When you collect the metrics and it is beyond a certain size, we should reject right there. This will force the user rethink the metrics and how it needs to be structured into smaller chunks. Thoughts?\n. @maosongfu - it is ok to safeguard the server in general -  when some large packets are thrown in. Do all servers need protection like this from various Heron components perspective? If we introduce this, then you need to look at the batch size and number of tuples send from stmgr to Heron instance and vice versa and the config becomes complex.\nIf this is a purely a metrics manager issue, my suggestion would be to restrict the size of the metriics at the point of receiving it - probably in the api call itself?\n. @maosongfu - if all the java servers have this protection, we might be restricting everybody just for one component's behavior, is that correct?\n. For simplicity, let us restrict the size of the metrics received - then we can tackle the problem of server protection differently or in different PR, thoughts?\n. @maosongfu - do we need this PR or we can close it?\n. thanks @windie for the PR - @objmagic / @maosongfu, can you please check this?\n. \ud83d\udc4d \n. Ship it\n. \ud83d\udc4d  - can you check if this is similar to Java?\n. \ud83d\udc4d \n. thanks @dmarchand - this is a great PR, we have been wanting to do this. Great to see you picked this up.\n. ship it - once CI passes.\n. @objmagic - let us investigate why this happened\n. @objmagic - can you launch other topologies and see how they work?\n. @objmagic - Is this PR relevant?\n. @nlu90 - can you take a quick look at this?\n. Let us add it - we need to support both pre-1.0 and post 1.0 versions, if that is supported in both.\n. @billonahill @maosongfu @avflor - when you get some time, can you provide feedback on the proposal?\n. @wangli1426 - @maosongfu has gone on a urgent trip to China. He will be back soon. Meanwhile, I am looking at your source code and proposal.. @wangli1426 - I was looking at the source code. Why have you not defined a elastic spout?. @maosongfu - can we get both? Can we know the container that is in back pressure and also the instance that triggered the back pressure? Sometimes, the back pressure is initiated by stmgr only due to network issues. Wondering if we can capture this as well.\n. @maosongfu - how to distinguish between the two?\n. Also how can we keep track of the back pressure for the last 3 min, 10 min, 1 hrs, 3 hrs, etc? so that we can bring this information up in the UI.\n. \ud83d\udc4d \n. @ashvina - yes that is the approach. It maximizes the use of existing messages and code path.\n. Looks good to me! @maosongfu - can you take a look at the instance changes? @nlu90 - can you take a look at the simulator code?\n. New changes look good to me. \ud83d\udc4d \n. Looks good \ud83d\udc4d \n. \ud83d\udc4d \n. @avflor @ashvina - can you let us know if you have any comments?\n. @ashvina - can we provide a detailed explanation?\n. @ashvina - thanks. Looks like some syntax in compilation.\n. \ud83d\udc4d \n. @billonahill - is this atmost once or atleast once semantics?\n. @moomou - any update on this?. \ud83d\udc4d \n. @tobecontinued - you have signed CLA before. no need to sign again. once ci finishes, I will merge.\n. \ud83d\udc4d \n. \ud83d\udc4d  @ashvina @avflor - any comments?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. Good one! Thanks for fixing this.\n. @ashvina  - does it mean that the the master containers - running TM, scheduler, etc will require more resources than the other data containers - in the case of homogenous containers?\n. thanks @ashvina \n. @mycFelix - please test this and let us know, how it works.\n. @nlu90 = any other questions?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. Looks good to me. \ud83d\udc4d \n. @objmagic - any update on this PR?. @congwang - how is the performance after moving to size in MB?. \ud83d\udc4d . @ashvina - can you please check?. \ud83d\udc4d . \ud83d\udc4d . \ud83d\udc4d . @avflor - can you please check and let me know?. \ud83d\udc4d . @ashvina - can you please check?. @fabianmenges - the unit tests failed. Can you please take a look?. @ajorgensen - Can you please take a look at this PR since your team uses it heavily?. @billonahill - wondering if your feedbacks have been taken care off, before I merge it?. @billonahill - wondering if your review recommendations have been taken care?. @fabianmenges - can you please review the changes @billonahill proposed?. @billonahill @objmagic - According to what I have read proto2 and proto3 are not compatible (from binary perspective). But @congwang is using proto2 binary format using proto3 version - which should be binary compatible. @congwang - I would suggest to run a test whether they are compatible or not. Also, it will be great to quantify the performance improvement.. @congwang - any update?. Sure - go for it.. @maosongfu - can you respond?. @anty - yes heron runs in centos5. It requires bazel to be compiled in centos5 and you need to ensure that the environment of compilers, make, cmake, etc confirms to version specified in the website http://heronstreaming.io. In fact, we run this in production. . @anty  - for centos7, please take a look at the docker file that we use at docker/Dockerfile.centos7 - it contains all the necessary commands and dependencies to run in centos7. hope this helps.. @chgl - if you have signed a CLA before there is no need to sign again.. Looks like you are committing to this code base, first time.. @supunkamburugamuve - everything looks ok?. \ud83d\udc4d . @congwang - Is this resolved?. @maosongfu - can you review this code?. @objmagic - are you sure this is the right title for the PR? the changes seem to be reflecting dry run.. @objmagic - got it.. @objmagic - Wondering if you could make the packing plan more readable. Also, it would be good to include information about the total number of containers, total number of instance for each component, total memory per container, total cpu per container and the aggregate total memory and cpu for the entire topology.. @objmagic - can you include the command as well? Also you need add documentation to the website as well.. Nice work @objmagic and thanks to @billonahill for guiding.. @objmagic @billonahill - is this PR ready to go? @objmagic - can we fix the conflicting files and also ensure that the CI passes? Also please add some unit tests for dry-run?. LGTM. @supunkamburugamuve - If you don't want to review this yet. Please change the title \nDO NOT MERGE - Stmgr Ability to specify the network interface to bind to\nso that somebody does not accidently merge it.. @supunkamburugamuve - any update on this?. @congwang - single quote takes precedence our double quotes. I am using a convention that the scripts use single quote - since some of them require it. I am trying to make it consistent - across all of them.. @congwang - any more comments?. @cckellogg - let us do another patch to appropriately report the status of the topologies in red. \ud83d\udc4d . @billonahill - the proposal is to change the color from RED to normal and display the state as 'ORPHANED'. @cckellogg is doing it in a different PR. Any suggestions for this state other than 'ORPHANED'?. @jrcrawfo - thanks for the nice PR. @nlu90 @billonahill @maosongfu @congwang - can you please review this?. @jrcrawfo - one question. Does the docker guarantee that the environment variable HOST is set for each container?. @ajorgensen - can you please take a look at this PR? It is docker related.. @jrcrawfo - thanks for the update. Looks like a good list of items to complete. Will let you know soon if there are any additional items.. @jrcrawfo - Also, you could build docker images using bazel build tool directly. \nhttps://bazel.build/versions/master/docs/be/docker.html\nIt will be nice if we can build them as part of package building and also include them into the release process.. @jrcrawfo - any update on this?. Ok cool - let us know if you need help.. We can add you into our committers slack - I need your email address though.. invited to slack.. @jrcrawfo - is this PR ready for another round of review?. thanks @jrcrawfo - let us know when it is ready!. @jrcrawfo - any update on this?. thanks @jrcrawfo - reviewing it now.. @jrcrawfo - Had a few comments. Otherwise, it looks good to me. I am going to test in my Mac and will let you know if I find any issues.. @jrcrawfo - For documentation, we can use a new PR. Otherwise, this PR will become huge.. @jrcrawfo - I tested your pull request in my local mac and see some errors in heron-ui. Any idea what might have gone wrong?\n93069880&endtime=1493073480&max=true (127.0.0.1) 45.52ms\nERROR: 04/24/2017 15:38:53 -0700 Uncaught exception GET /topologies/metrics/timeline?cluster=local&environ=default&topology=ET&metric=memory&component=*&instance=*&starttime=1493069880&endtime=1493073480&max=true (127.0.0.1)\nHTTPServerRequest(protocol='http', host='0.0.0.0:8889', method='GET', uri='/topologies/metrics/timeline?cluster=local&environ=default&topology=ET&metric=memory&component=*&instance=*&starttime=1493069880&endtime=1493073480&max=true', version='HTTP/1.1', remote_ip='127.0.0.1', headers={'Connection': 'keep-alive', 'Accept-Language': 'en-us', 'Accept-Encoding': 'gzip, deflate', 'Referer': 'http://0.0.0.0:8889/topologies/local/default/ET', 'Host': '0.0.0.0:8889', 'Accept': 'application/json,*/*', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_3) AppleWebKit/602.4.8 (KHTML, like Gecko) Version/10.0.3 Safari/602.4.8'})\nTraceback (most recent call last):\n  File \"/Users/karthikz/.pex/install/tornado-4.0.2-py2.7-macosx-10.12-intel.egg.a4e07cdf2aaf1b7e934b1ad19ed5af2921b6184f/tornado-4.0.2-py2.7-macosx-10.12-intel.egg/tornado/web.py\", line 1334, in _execute\n    result = yield result\n  File \"/Users/karthikz/.pex/install/tornado-4.0.2-py2.7-macosx-10.12-intel.egg.a4e07cdf2aaf1b7e934b1ad19ed5af2921b6184f/tornado-4.0.2-py2.7-macosx-10.12-intel.egg/tornado/gen.py\", line 628, in run\n    value = future.result()\n  File \"/Users/karthikz/.pex/install/tornado-4.0.2-py2.7-macosx-10.12-intel.egg.a4e07cdf2aaf1b7e934b1ad19ed5af2921b6184f/tornado-4.0.2-py2.7-macosx-10.12-intel.egg/tornado/concurrent.py\", line 109, in result\n    raise_exc_info(self._exc_info)\n  File \"/Users/karthikz/.pex/install/tornado-4.0.2-py2.7-macosx-10.12-intel.egg.a4e07cdf2aaf1b7e934b1ad19ed5af2921b6184f/tornado-4.0.2-py2.7-macosx-10.12-intel.egg/tornado/gen.py\", line 631, in run\n    yielded = self.gen.throw(*sys.exc_info())\n  File \"heron/tools/ui/src/python/handlers/api/metrics.py\", line 85, in get\n    results = yield futures\n  File \"/Users/karthikz/.pex/install/tornado-4.0.2-py2.7-macosx-10.12-intel.egg.a4e07cdf2aaf1b7e934b1ad19ed5af2921b6184f/tornado-4.0.2-py2.7-macosx-10.12-intel.egg/tornado/gen.py\", line 628, in run\n    value = future.result()\n  File \"/Users/karthikz/.pex/install/tornado-4.0.2-py2.7-macosx-10.12-intel.egg.a4e07cdf2aaf1b7e934b1ad19ed5af2921b6184f/tornado-4.0.2-py2.7-macosx-10.12-intel.egg/tornado/concurrent.py\", line 109, in result\n    raise_exc_info(self._exc_info)\n  File \"/Users/karthikz/.pex/install/tornado-4.0.2-py2.7-macosx-10.12-intel.egg.a4e07cdf2aaf1b7e934b1ad19ed5af2921b6184f/tornado-4.0.2-py2.7-macosx-10.12-intel.egg/tornado/gen.py\", line 464, in callback\n    result_list = [i.result() for i in children]\n  File \"/Users/karthikz/.pex/install/tornado-4.0.2-py2.7-macosx-10.12-intel.egg.a4e07cdf2aaf1b7e934b1ad19ed5af2921b6184f/tornado-4.0.2-py2.7-macosx-10.12-intel.egg/tornado/concurrent.py\", line 109, in result\n    raise_exc_info(self._exc_info)\n  File \"/Users/karthikz/.pex/install/tornado-4.0.2-py2.7-macosx-10.12-intel.egg.a4e07cdf2aaf1b7e934b1ad19ed5af2921b6184f/tornado-4.0.2-py2.7-macosx-10.12-intel.egg/tornado/gen.py\", line 633, in run\n    yielded = self.gen.send(value)\n  File \"heron/tools/common/src/python/access/heron_api.py\", line 753, in fetch_max\n    raise tornado.gen.Return(result)\nUnboundLocalError: local variable 'result' referenced before assignment. \n. thanks @jrcrawfo. @objmagic will run the entire test suite in the production environment and give his final approval.. Looks good! Let us merge it once the CI finishes!. @ajorgensen - wondering if there is a network connectivity/partitioning issue here?. @ajorgensen - valid point. We need to check if the tmaster is reconnecting if the partitioning happens or not? @congwang / @maosongfu ?. Thanks @ajorgensen - for the detailed investigation. This is indeed very helpful. @congwang @ajorgensen - can you work together to identify the case and find a fix?. \ud83d\udc4d\n\nOn Jan 10, 2017, at 11:58 AM, Andrew Jorgensen notifications@github.com wrote:\nI am not sure. If the problem is that the tmaster connection resets and it immediately tries to write the znode before the old one has expired then yes I think that this diff will help. I can apply this locally and give it a try.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. yes - please remove the host name,. thanks @objmagic for pointing it out. updated it.. \ud83d\udc4d . \ud83d\udc4d . Yes - we provide the binaries for only centos 7 only. If you want to run on centos 6, you need to compile and install. Please follow the instructions in the web page.\nOn Jan 10, 2017, at 9:06 AM, Thomas Shields notifications@github.com wrote:\n@objmagic okay, thanks. I noticed after filing this issue that the compilation instructions for Heron (which I hadn't looked at before since I was just installing with the script) mention it can only be compiled on CentOS 7. I'm on CentOS 6. Could that be the source of the issue?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \ud83d\udc4d . @nlu90 - can you fix the CI and check the code in?. @dmarchand - the library is published and pushed to maven. Please check and close the issue accordingly.. \ud83d\udc4d . @objmagic - I don't think these files will be needed for centos7. You can build the binary scripts/packages:binpkgs. Can you verify and see if this necessary?. Heron client includes core for running local cluster.\nOn Feb 21, 2017, at 10:17 PM, Bill Graham notifications@github.com wrote:\nI don't know why the packing files are duplicated in heron_client.bzl but this seems incorrect to me.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @objmagic - have you tested the package for centos7 before removing these files?. \ud83d\udc4d . @leonardgithub - I went to mvn website and able to download the jar. What is the specific issue that you are seeing?\nOn Mar 30, 2017, at 12:02 AM, Mark Li notifications@github.com wrote:\n@leonardgithub https://github.com/leonardgithub Mailing list is at https://groups.google.com/forum/#!forum/heron-users https://groups.google.com/forum/#!forum/heron-users\n@kramasamy https://github.com/kramasamy Can you have a look the Maven Jar issue @leonardgithub https://github.com/leonardgithub mentioned above?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/issues/1760#issuecomment-290320708, or mute the thread https://github.com/notifications/unsubscribe-auth/AAWcRKdtwcHaQ_E1yHYRKxWy8iF7ghoDks5rq1N0gaJpZM4Ma7io.\n\n\n. @congwang - good PR. Cleans a lot of code. Just gave a few comments. \ud83d\udc4d  - once the CI is green, please merge it.. @congwang @maosongfu - we need some reliable information the container is indeed in some kind irrecoverable situation before we can start the container. If we restart the container without not much evidence, it could lead to inaccurate results.  If @congwang feels that the new back pressure algorithm has improved, it might be worth a try before adding autorestart. One fear is that if this code (which is kind of a hacky solution), it might not come out later.\n. @avflor - wondering what is the ETA for the SLA/Health Manager so that we can do clean integration?. Not convinced this is the right approach for dealing with the back pressure - container. Did we collect data points about any other issues that leads to it?\n\nOn Jun 6, 2017, at 9:56 PM, bed debug notifications@github.com wrote:\nbeg for review\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/pull/1770#issuecomment-306686498, or mute the thread https://github.com/notifications/unsubscribe-auth/AAWcRNWVj_RrDC-6QjQEFndu3Z2hD9VLks5sBi1kgaJpZM4Mi0HI.\n\n\n. @huijunw - for a stop gap implementation, this PR seems to touching a lot of code. It is going to be hard to take it out later. Also, I would recommend to add a configuration - which enable this feature. by default that value should be turned off.. @huijunw - instead of adding parameters in heron submit, can you use the \n--config-property option for these values\nThis is advantageous\n already the mechanism is in place to pass the config values\n no need to introduce a specific feature like command line args\n you can ship with internally with specific values to enable this feature.\n it will reduce a lot of code of passing these parameters around\n. @huijunw - do we have to implement this logic in Tmaster? Why can't we do a separate process - this allows for a clean migration to Dhalion as well - when it becomes available.. \ud83d\udc4d  - once the CI passes.. Yes, it is being used for config generation. When we move to 0.4.4 or 0.5.0 of bazel, we can look at removing them.\n\nOn Mar 22, 2017, at 9:53 AM, bed debug notifications@github.com wrote:\nDo we use this directory any more https://github.com/twitter/heron/tree/master/config https://github.com/twitter/heron/tree/master/config ?\nIf we don't use it, we may remove/cleanup it\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/issues/1777, or mute the thread https://github.com/notifications/unsubscribe-auth/AAWcRLRPkG74PpkeqkI4MwWaO48_npu1ks5roVIOgaJpZM4Mlguo.\n\n\n. Thanks @kkdoon - it looks great. A couple of questions \n\n\nIf the back pressure is initiated by an instance, will we be able to see the instance that generated the back pressure visually, when we click on the back pressure metric\n\n\nIf the back pressure is initiated by the stream manager, we should be able to see it visually as well. In this case, when we click on the back pressure metric, it could show the container shaded? - so we can see right away that it is a stream manager initiated back pressure.. @kkdoon - for point 2, please check with @congwang and see if there is any metrics for stream manager based back pressure.. @objmagic - any update on this?. @billonahill - for trident instead of using their stateful processing, we should map into our exactly once processing. This will be a cleaner implementation rather than implementing two stateful processing algorithms, IMO.. \ud83d\udc4d looks good to me.. @Detoo - we will merge it once all the tests pass.. @huijunwu - wondering if you ran the tests a few times before the flakiness is gone.. @maosongfu @billonahill - is this ready to be merged?. @maosongfu - sounds good!. @nlu90 - it is not being include anywhere. all the compilation were successful and so did tests. It is a file that includes all the files in that directory.. sort by each field is good enough and we have powerful search as well.. \ud83d\udc4d . @ananthgs - can you write a good concise PR description about how it works with ECS?. @ananthgs - you might want to add a detailed documentation of how to set it up and use, as described in other schedulers for heron.io. However, this can be done in a different PR.. @maosongfu - this might be ok for unrecoverable exceptions. But for recoverable exceptions, you should not die, just like that. How can you distinguish between the two?. @ajorgensen - @srkukarni also encountered this when he was working with C++ api for Heron.. @nlu90 - this limits some of the resource specifications where you could specify componentRam etc - which makes Heron tuneable?. @maosongfu - but that exposes the underlying heron api which we should not do. Heron API will undergo a lot of changes as other higher level apis are being developed. You can think of the api such as setComponentRam, etc as storm extensions and let them reside in the Storm config itself - as it was orginally.. -1 - even I am not happy with this PR - since it is exposing Heron API, which is something that will change extensively. Instead a storm extensions config - extensions to the Config of Storm is a good enough. Not sure why this was needed in the first place.. @aahmed-se - there is a PR waiting to be merged actually. https://github.com/twitter/heron/pull/1776. Let use -- since it is standard in python/gnu/hazel etc.\n\n\nSent from my iPhone\n\nOn May 12, 2017, at 10:43 PM, Maosong Fu notifications@github.com wrote:\nThis change can break the interfaces provided by Heron. Are there ways we could:\nprecisely match the name of optional args' key, or\ndistinguish the scope heron itself's configs from the scope of user's configs, just like bazel using -- as a separator?\n\u2014\nYou are receiving this because your review was requested.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @objmagic already -- works in heron client and it is a consistent behavior across the language. . @billonahill - great pr. Wondering if there are other tests that require changes as well to take advantage of the framework.. @jrcrawfo - looks good to me. Can you please address @nlu90's feedback so that we can merge it?. @jrcrawfo - please address the comments of @nlu90 so that we can merge it.. Ideally it will be nice to merge this - but we have some additional work to get this completed.\nOn May 31, 2017, at 3:47 AM, Runhang Li notifications@github.com wrote:\nThis is not for 0.14.8. And please fix merge conflict. Also someone please review it.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @srkukarni - Should we merge this?. @srkukarni - Should we merge this?. Please take a look at following repo\n\nwww.github.com/kramasamy/heron-starter\nCheers\n/karthik\n\nOn May 29, 2017, at 11:29 PM, jrpspam notifications@github.com wrote:\nOkay,, found example of how to do it. Building jar w/ dependencies takes a lot of pom.xml to get working. Absolutely nothing obvious about how to do it. Might be very nice if the heron examples came w/ an actual working pom.xml that one could use as a starting point. I had to add entire build section to pom as below:\n4.0.0\ncom.neustar\ncnam\njar\n1.0-SNAPSHOT\ncnam\nhttp://maven.apache.org\njunit\njunit\n3.8.1\ntest\n https://mvnrepository.com/artifact/com.twitter.heron/heron-storm \n\ncom.twitter.heron\nheron-storm\n0.14.6\n\n\n${project.artifactId}\n\n\nmaven-compiler-plugin\n3.0\n\n1.6\n      1.6\n\n\n\norg.codehaus.mojo\nexec-maven-plugin\n1.2.1\n\ncom.twitter.storm.Topology\ncompile\n\n\n\nmaven-assembly-plugin\n\n\nmake-assembly\npackage\n\nsingle\n\n\n\n\n\njar-with-dependencies\n\n\n\nbiz.neustar.cnamheron.CnamTopology\n\n\n\n\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @nlu90 @maosongfu @billonahill - there could be a couple of issues.\n\n\nInternet could be spotty during the travis-ci run\npyyaml is actually referred as PyYAML, which might not be issue\n\nHowever, I have fixed pyyaml to be PyYAML in the build files (just to be sure). Ran it a few times and so far it is not able to reproduce.. @SidduMirji - we currently do not use thrift at all in Heron. Hence we have not added any bazel rule for thrift.. Sorry! I did not realize it.. @jrcrawfo - is this ready to go? . Ship it!. ship it.. @cckellogg - can you add some comments?. @huijunw - one more possible probably \"simple\" solution (this assumes that zombie container lost connection to other containers). In the heartbeat sent to TMaster from stmgr include the number of active connections to other containers. For a zombie container, it might be 0 or very low and TMaster is also getting pings from container Y (which is reincarnation of X). When these two conditions satisfy - send a kill to the zombie container. The advantage of this approach is that it does not have to kill the new container. thoughts?\n. @huijunw - don't want to identify the underlying issue since this website related build. Or we are completely removing website related stuff out of the CI.. @huijunw @nlu90 - instead of removing the line, can you comment it out? otherwise once we identify the root cause, we uncomment it.. Perhaps another CI that runs weekly or every other day might help.\n\nOn Jul 20, 2017, at 4:23 PM, Luc Perkins notifications@github.com wrote:\n@billonahill The issue, though, is that the website-related stuff is adding a lot of extra time (several minutes to each build) and a lot of extra brittleness that is imposing frustration and other costs on engineers. I do agree that periodic checks are a good thing, but performing these actions on every build, across many thousands of builds a year, seems excessive. I'll see if I can come up with a better solution that both uses automation to keep the documentation up to our standards but at a more reasonable cost.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @lucperkins - some error in javadocs. Can you please fix it so that we can check it in?. @billonahill - protobuf is already out.. This is addressed by the PR #2833 - let me know if anything else is remaining. @huijunw - this is needed for multiple different deployment modes.. @billonahill - the reason why mirror will not work is because the repo that you pulling down streamlio/bazel_rules_pex will in turn pull remote repositories - under the hood in bazel. This makes it hard to do 'twitterify' style script.. @billonahill - good idea. After some investigation, I found out that there is no need to use bazel git_repository for fetching the pex rules. In fact bazel discourages it. Instead, we can use http_archive to fetch the tar ball for even rules - something like this\n\n```\npex_rules_version = \"0.3.8\"\npex_rules_url_prefix = \"https://github.com/streamlio/bazel_rules_pex/archive/\"\npex_rules_url = pex_rules_url_prefix + pex_rules_version + \".tar.gz\"\npex_rules_sha256 = \"071b75d24611910b9fd250366688581c813b62680267760b23389912714b6e72\"\nfor pex rules\nhttp_archive(\n  name = \"io_bazel_rules_pex\",\n  url = pex_rules_url,\n  strip_prefix = \"bazel_rules_pex-\" + pex_rules_version,\n  sha256 = pex_rules_sha256,\n)\nload(\"@io_bazel_rules_pex//pex:pex_rules.bzl\", \"pex_repositories\")\npex_repositories()\n```\nFixed the branch with these changes, please take a look at it - whether it makes it easy.. @billonahill - we can enable it for the whole repo in the next PR if needed. Merging it.. Basically, we need the following when we submit a topology \n[INFO]: Using config file under /Users/karthikz/.heron/conf/local\n[INFO]: Launching topology: 'etc'\n[INFO]: Uploading topology jar to \n[INFO]: Scheduling topology in \n[INFO]: Successfully launched topology 'etc'\n[INFO]: Elapsed time: 2.405s.\nProviding minimal information as each step is complete - so that user knows at a high level what is the current step, if that step is successful and the total time it took to complete. We print Elapsed time for all the commands just - to give an idea for the user - similar to what bazel does.\n. Elapsed time is needed since the jobs are varying size, operating over different schedulers and different networks. I would feel comfortable keeping it. In fact it was added based on feedback from teams. On the config file, if you have multiple clusters and different config paths being provided, it is good to display so that you have an idea which config file is being used - since there are typically multiple of them.. +1. We need this for every release.. Ship it!. @tomncooper  - please send a PR for this fix.. Ship it - once the CI passes.. Let us merge the corresponding PR.\n\n@cckellogg https://github.com/cckellogg @kramasamy https://github.com/kramasamy what is the next step for this?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/pull/2220#issuecomment-330095793, or mute the thread https://github.com/notifications/unsubscribe-auth/AAWcRJugLvN13IO1NlPG6AExKGpg9t5eks5sjZU2gaJpZM4O81lS.\n\n\n. This document is not fully flushed out yet.. @huijunwu - this PR broke the executor unit_test and did not pass the CI. The reason is because --verbose addition in heron_executor.py is not captured in the test. The fix is in #2261 . @yesimsure - we compile Heron multiple times everyboday and seems to work. Wondering if you are able to connect maven.twttr.com (repo). Try http://maven.twttr.com/ in a browser and navigate and you will find libthrift.jar. @jerrypeng - we can add it in the subsequent PRs?. @aahmed-se - what is the use case?. +1 - please check why CI failed.. @nlu90 - nice graphs.. Yes. I have closed #2301\n\nOn Sep 18, 2017, at 10:41 AM, bed debug notifications@github.com wrote:\ndoes this pr replace #2301 https://github.com/twitter/heron/pull/2301?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/pull/2316#issuecomment-330298507, or mute the thread https://github.com/notifications/unsubscribe-auth/AAWcRG9YXXnUgKXEtV5JIhdRXGjNBNBIks5sjqsvgaJpZM4PacEW.\n\n\n. I don\u2019t think so - let me investigate and see.\n\nOn Sep 20, 2017, at 12:14 AM, Maosong Fu notifications@github.com wrote:\n@kramasamy https://github.com/kramasamy Do you know any conflicts/issues preventing running python tests with the bazel upgrade?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/issues/2328#issuecomment-330765810, or mute the thread https://github.com/notifications/unsubscribe-auth/AAWcRDYh5QnA_o8BBawvktSAVMnJ1uzEks5skLtCgaJpZM4PdLRn.\n\n\n. @dancollins34 - you are welcome to pick this up. The current simulator code is under heron/simulator - it emulates the topology running inside a single process. In order to add this support, you need to add the checkpoint manager (that stores state in storage) into the simulator. furthermore, it needs to incorporate the checkpointing and replay logic. Please join the slack board so that I can share the exactly once implementation - design document.. @jerrypeng @cckellogg @aahmed-se - is this PR needs to be merged?. @nlu90 @huijunw - these changes are backward compatible withh bazel 0.5.4 and also works with bazel 0.6.0 as well.. @cckellogg - can we refine the error messages to \n\nUnable to connect the kubernetes schheduler or api server\nInvalid config\n. Waiting for @maosongfu at Twitter to approve. @objmagic @huijunw - any comments?. Nice functionality!. @aahmed-se - a bunch of checkstyle errors that you need to fix.. This will not work because the link is not correct.\n\nSent from my iPhone\n\nOn Oct 24, 2017, at 11:54 AM, Runhang Li notifications@github.com wrote:\n@objmagic approved this pull request.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @huijunw - is this in travis or internally at Twitter. For travis, the maximum time goes into compilation and preparation of packages.. fixed in #2513 . Awesome!\n\nSent from my iPhone\n\nOn Nov 2, 2017, at 8:29 AM, Michael Schmidt notifications@github.com wrote:\nYAY it worked (:\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. It is not needed.\nOn Nov 3, 2017, at 12:56 AM, Sanjeev Kulkarni notifications@github.com wrote:\nIt should be removed\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @lucperkins - no need this for anymore since cmake dependency has been removed.. @huijunw @maosongfu @nwangtw - can you use this PR to test within Twitter?. @huijunw @nwangtw - wondering if you have verified this PR?. if it is not used - should we remove the library from the jar? If the application programs link against this with different version of kryo then you might have a clash.. It is incremental benefit as we move towards removing tar balls out of the source code base\nyaml cpp does not make use make/cmake/autoconf/automake tools to setup and compile. Instead, it directly uses bazel - because of that bazel can parallelize the compilation of the yaml cpp files - thus reducing the compilation time. While it might be small in yaml-cpp, as we convert other projects it will get better and better.. Purpose of this pull request\n- Make the compilation faster for yaml cpp\n\nHow would this pull request resolve the issue\n- This pull request untars the binary tar ball into actual files and directly uses bazel to compile. Hence yaml cpp does not depend on make/automake/cmake/autoconfig etc. Since bazel uses parallelism underneath, it speeds up compilation\nWhat would be when the pull request is merged: (Issues resolved? What's the expected results?)\n- The expected result is in the increase faster compilation. This is a part of the broader effort to remove tar balls out of the code base. If Twitter were to mirror selective github repos, we can further simplify the PR using new_git_repository rules. It can be done in a later PR once repos are setup.\n. @huijunw - I was wondering if this has been tested in a clustered environment. Will it be security hole?. @srkukarni - where is the merge conflict?. @srkukarni - this is just test where the dependencies do not mater?. @mjschmidt - yes, we have reduced the size of the docker container using slim debian8. We are planning to release in the next iteration.. Is it not in 0.17.0 release?\nSent from my iPhone\n\nOn Nov 20, 2017, at 7:49 PM, Michael Schmidt notifications@github.com wrote:\n@kramasamy\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @mjschmidt - Thanks for finding this. Please fix and send a pull request.. I think so.\n\nOn Thu, Dec 28, 2017 at 5:35 AM Michael Schmidt notifications@github.com\nwrote:\n\nIs this issue done then?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/2584#issuecomment-354289219, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAWcRFB0HqYOVuYJvtJV2deCdT2pZ22aks5tE5kHgaJpZM4QpGJq\n.\n. @mjschmidt  - closing this issue. Reopen if there is a problem.. @nwangtw - do you need this target inside Twitter?. @nwangtw - currently we publish the jars to maven automatically. Furthermore, python api's are being published to pypi. Hence heron api with just tar balls of jars might not make sense.. @cckellogg - kubernetes changes for the other yamls will be changed in the later PR.. @lucperkins - anything to be added here.. @aahmed-se - is this PR needs to be merged?. @mjschmidt - we don't remember adding anything to UI so far actually.. Ok - let us know if we need to fix anything!\nOn Dec 26, 2017, at 7:25 AM, Michael Schmidt notifications@github.com wrote:\nThis looks like its not actually a recent change, it was august. So it must be a mirroring issue on our side. I will make the change in my free time at home.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @mjschmidt - emptyDir is a stateful set concept that k8s uses. When the pod migrates to another machine it gets a fresh empty directory.. It will be nice to provide some examples of how to use this feature.\n\nOn Tue, Jan 2, 2018 at 11:20 PM Boyang Jerry Peng notifications@github.com\nwrote:\n\n@jerrypeng commented on this pull request.\nIn heron/api/src/java/com/twitter/heron/api/bolt/WindowedBoltExecutor.java\nhttps://github.com/twitter/heron/pull/2653#discussion_r159369854:\n\n\nif (topoConf.containsKey(WindowingConfigs.TOPOLOGY_BOLTS_WINDOW_CUSTOM_EVICTOR)) {\nevictionPolicy = (EvictionPolicy)\ntopoConf.get(WindowingConfigs.TOPOLOGY_BOLTS_WINDOW_CUSTOM_EVICTOR);\n} else {\nevictionPolicy = getEvictionPolicy(windowLengthCount, windowLengthDurationMs);\n}\n+\nif (topoConf.containsKey(WindowingConfigs.TOPOLOGY_BOLTS_WINDOW_CUSTOM_TRIGGER)) {\ntriggerPolicy = (TriggerPolicy)\ntopoConf.get(WindowingConfigs.TOPOLOGY_BOLTS_WINDOW_CUSTOM_TRIGGER);\n} else {\ntriggerPolicy = getTriggerPolicy(slidingIntervalCount, slidingIntervalDurationMs, manager,\nevictionPolicy, topoConf);\n}\n+\n+\n\n\nPlace remove extra line\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/pull/2653#pullrequestreview-86305878,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAWcRGuVfwFLq2mjTCfxoTMhhmfWrcy3ks5tGyongaJpZM4RRUjm\n.\n. @jerrypeng - can you please re-review again?. @dancollins34 - some errors have been occurring in the unit tests. Can you please fix them?. @dancollins34 - can you add examples of how to use these under examples/ and storm-compatibility-examples? This will help developers give some code that they can understand and also quickly copy and modify if needed.. @dancollins34 - it will be nice if you could add some documentation on how to use this in a separate PR.. @jerrypeng - Can I merge this?. #2655 merged. @supunkamburugamuve - is this ready to go?. @joshfischer1108 - thanks for the great contribution. I was wondering if you could add a few examples. . @joshfischer1108 - PR looks good to me. Are there any additional commits that is coming along or ready to merge?. @joshfischer1108 - state is implemented differently with Heron. After merging this branch, it will be great to add documentation on how to use it. It will be nice to translate into a blog as well. Thanks for the great contribution.. If you could remove the check for env_exec.sh - in setup-intellij.sh and send a PR - it will be great. We have to make the entire compilation work with python3 - currently works with python2.7+ but not python3. @sreev - it will be great if you could sent out the PR for removing env_exec.sh?. @mjschmidt - streamlet examples do not contain ExclamationTopology. Instead try heron-api-examples.jar for ExclamationTopology. @mjschmidt - docker container is mainly to run in a cluster and currently it does not have heron client installed since the image size gets bigger. you can view the single docker container as a one node cluster and submit topology from the outside - let me see if I can improve the experience.. Michael - If this is for dev with all the dependencies - we have one already.  Checkout the containers\n\nstreamlio/build-centos7\nstreamlio/build-ubuntu14.04\nstreamlio/build-centos6\nPlease try it out and let us know.\n\nOn Jan 23, 2018, at 12:15 PM, Michael Schmidt notifications@github.com wrote:\nNot the way I was hoping to do this docker file but what ever, you can use this as template for providing a container in which people can do local dev work with heron 0.17.3\nI don't mind maintaining it.\n`FROM centos:centos7\nDockerfile author / maintainer\nMAINTAINER mjschmidt mjschmidt\nThis is passed to the heron build command via the --config flag\nENV TARGET_PLATFORM centos\nENV bazelVersion 0.9.0\nRUN yum -y upgrade\nRUN yum -y update\nRUN yum -y install \nautomake \ncurl \ncmake \nopenssl-devel \nfile \ngcc \ngcc-c++ \ngit \nkernel-devel \nlibtool \nmake \npatch \npython \npython-devel \npython-setuptools \nzip \nunzip \nvim \nwget \nwhich\nRUN yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel\nENV JAVA_HOME /usr/lib/jvm/java-1.8.0\nRun git clone https://github.com/twitter/heron.git https://github.com/twitter/heron.git\nRUN curl -LO http://github.com/twitter/heron/releases/download/0.17.3/heron-install-0.17.3-centos.sh https://github.com/twitter/heron/releases/download/0.17.3/heron-install-0.17.3-centos.sh\nRUN chmod +x heron-install-0.17.3-centos.sh\nRUN ./heron-install-0.17.3-centos.sh\n@todo https://github.com/todo I want to get heron completely compiled from github at some point... unfortunately dockering inside of docker is hard and the container wants to do docker things\nRUN heron-install-0.17.3-centos.sh\nRUN heron/docker/scripts/build-artifacts.sh centos7 0.17.3 ~/heron-release\nRUN ~/heron-release/heron-install-0.17.3-centos7.sh\nADD bazelrc /root/.bazelrc\nADD scripts/compile-platform.sh /compile-platform.sh\n`\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/issues/2667#issuecomment-359916156, or mute the thread https://github.com/notifications/unsubscribe-auth/AAWcRIm_bquv2uZb0HZDggmWKhDVira4ks5tNj3LgaJpZM4Rh9be.\n\n\n. It will be solved in 0.17.5 \n\nOn Jan 26, 2018, at 5:09 AM, Michael Schmidt notifications@github.com wrote:\n@kramasamy I think this is solved for the 17.4 or 17.5 release right?\nThis can also be found at orchapod/herondev\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Yes - this is solved!\n\nSent from my iPhone\n\nOn Feb 23, 2018, at 5:01 AM, Michael Schmidt notifications@github.com wrote:\nI think this is solved now? I checked the container and it looks good.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. Will move it\nOn Jan 18, 2018, at 3:52 AM, Josh Fischer notifications@github.com wrote:\n@kramasamy the yaml files to assemble the topologies are inheron/examples/src/resources/*yaml.\nIf you want to move them, now would be the time, but then you will also have to modify\ntools/rules/heron_examples.bzl\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @dancollins34 - sorry got distracted into something else. @nwangtw @huijunw - can you take a look at this?. @nwangtw - Do you have any feedback on this? I remember you mentioned  you had two comments. If you have can you specify them so that @dancollins34 - can take care of it? If not, I can merge them tomorrow.. @dancollins34 - thanks for the excellent contribution. We can merge it if there are no feedbacks tomorrow.. @dancollins34 - when you get a chance, can you address @nwangtw comments?. @lucperkins  - is this ready to go?. @Code0x58 - Thanks for the review. Let me upgrade to 3.6 on this PR.. @wxl24life - env_exec.sh file is not needed anymore. If you could remove the if block in \n\nhttps://github.com/streamlio/streamlio/blob/master/deploy/kubernetes/helm/templates/broker.yaml#L84\nand send a PR that will be awesome!. Sorry wrong link - check if this works\nhttps://github.com/twitter/heron/blob/master/scripts/setup-intellij.sh#L4 \n. @nwangtw - this is probably not the right approach to implement this. Rather than adding more options - we could do the following -\n\n\nwhen heron kill issued for a topology, write a znode under that topology that topology is going to be killed. This is an atomic operation\n\n\nOnce the znode is written, subsequent submit can clear the state before launching the topology.\n. Ship it!\n\n\nSent from my iPhone\n\nOn Jan 25, 2018, at 12:36 PM, Eren Avsarogullari notifications@github.com wrote:\n@erenavsarogullari commented on this pull request.\nIn WORKSPACE:\n\n@@ -876,3 +876,18 @@ new_http_archive(\n     urls = [\"https://releases.hashicorp.com/nomad/0.7.0/nomad_0.7.0_linux_amd64.zip\"],\n     build_file = \"third_party/nomad/nomad.BUILD\",\n )\n+\n+# scala integration\n+rules_scala_version=\"5cdae2f034581a05e23c3473613b409de5978833\" # update this as needed\n+\n+http_archive(\n+             name = \"io_bazel_rules_scala\",\nAddressed.\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @jrcrawfo - any update on this PR?. @nwangtw - Let us chat on tuesday so that we can discuss its implications. It is very common for people to ack/fail in another thread. If we don't support it - it is a limitation.. @srkukarni - can you address @maosongfu concerns?. @maosongfu - can you approve the PR after @srkukarni has addressed the concerns?. @maosongfu @srkukarni - is this PR has come to a conclusion?. @erenavsarogullari - looks good to me. @skanjila - can you do a quick review for this PR so that I can merge.. @erenavsarogullari - looks good to me. @skanjila - can you review this?. @jerrypeng @srkukarni - Can you respond to this?. @jerrypeng - why does this test fail?. It could be transient failure as well - let me kick off the build again. \nOn Feb 8, 2018, at 11:46 AM, Josh Fischer notifications@github.com wrote:\n@joshfischer1108 commented on this pull request.\nIn eco/src/java/com/twitter/heron/eco/parser/EcoParser.java https://github.com/twitter/heron/pull/2703#discussion_r167047602:\n\n}\n\n\nprivate EcoTopologyDefinition loadTopologyFromYaml(Yaml yaml, InputStream inputStream) {\nreturn (EcoTopologyDefinition) yaml.load(inputStream);\nprivate EcoTopologyDefinition loadTopologyFromYaml(Yaml yaml, InputStream inputStream,\nInputStream propsIn,\nboolean envFilter) throws IOException {\nByteArrayOutputStream bos = new ByteArrayOutputStream();\nint b;\nwhile ((b = inputStream.read()) != -1) {\nbos.write(b);\n}\n+\nString yamlDefinitionStr = bos.toString();\n// properties file substitution\nif (propsIn != null) {\nLOG.info(\"Performing property substitution.\");\n@kramasamy https://github.com/kramasamy Ok I will end some high level logging for each one of the steps. I will do this after master comes back to a passing state.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/pull/2703#discussion_r167047602, or mute the thread https://github.com/notifications/unsubscribe-auth/AAWcRPduePVYnh1gOPFJYQU9pjqO46KZks5tS08RgaJpZM4R1Abt.\n\n\n. Thanks for the PR. Reviewing it.. @dancollins34 - can you please remove the dependency and update the PR as we discussed?. @dancollins34  - it will be nice to add some docs on how to run streamlets in simulation mode. it will be very useful for the user.. Thanks @erenavsarogullari for the PR - had a comment.. Once the travis ci passes - we can merge it. @aahmed-se - can you please fix the travis ci issue?. @lucperkins - is this ready to go?. Now the question is what happens if the heron api server moves to different node because nomad choose to?. @jerrypeng - if is not about the service discovery mechanism - it is about the core file. Should core file be available in the same node as api server - if that it the case when the api server moves - how are we going to move the core file or the assumption is that the core file needs to be available at neutral location - accessible from all nodes.. @akozich - thanks for the PR. waiting for the ci to finish.. @jrcrawfo - wondering if you have some feedback.. @huijunw @nwangtw - not sure what is the intention of this PR. Ideally instead of doing all the warning etc - heron update itself should take care of itself. If heron update has an issue in terms of getting resources etc, it should revert itself - making the operation as though it did not change anything. Can you explain the conditions when it can enter a weird state?. @huijunw - talked with @nwangtw about a cleaner approach. we can discuss it offline.. @joshfischer1108 - here is the fix.. @asudhindra - please do submit a PR. thanks.. @erenavsarogullari - can you resolve the conflicts?. thanks @erenavsarogullari - once it finishes the CI we can merge it.. probably /runtime_config/update - will be more apt. It gives more room to expand as well.. @lucperkins - can you review this?. @lucperkins - can you take a quick pass at it?. @cckellogg - wondering if the values are being read in the instance.. @lucperkins - wondering if you can share the preview?. @lucperkins - is this ready to go?. @erenavsarogullari - can you respond to @nwangtw feedback?. @skanjila - looks good to me. Once @nwangtw approves it we can merge it.. For todo items - can you open issues so that we can track - otherwise it might get dropped in the midst of several things.\n\nOn Mar 10, 2018, at 12:34 AM, Ning Wang notifications@github.com wrote:\n@nwangtw commented on this pull request.\nIn heron/api/src/scala/com/twitter/heron/streamlet/scala/Builder.scala:\n\n+//  limitations under the License.\n+\n+package com.twitter.heron.streamlet.scala\n+\n+\n+import com.twitter.heron.streamlet.scala.impl.BuilderImpl\n+\n+\n+/*\n+  * Builder is used to register all sources. Builder thus keeps track\n+  * of all the starting points of the computation dag and uses this\n+  * information to build the topology\n+  /\n+object Builder {\n+  def newBuilder(): Builder =\n+    new BuilderImpl(com.twitter.heron.streamlet.Builder.newBuilder())\nSuggest to use alias,\n\nimport com.twitter.heron.streamlet.{Builder => JavaBuilder}\n...\nnew BuilderImpl(JavaBuilder.newBuilder())\nIn heron/api/src/scala/com/twitter/heron/streamlet/scala/Builder.scala:\n\n\n/**\n\n\nAll sources of the computation should register using addSource.\n\n\n*\n\n\n@param supplier The supplier function that is used to create the streamlet\n\n\n\n\n@return a Streamlet representation of the supplier object\n\n\n*/\ndef newSourceR: Streamlet[R]\n+\n+\n/**\n\n\nCreates a new Streamlet using the underlying generator\n\n\n*\n\n\n@param generator The generator that generates the tuples of the streamlet\n\n\n\n\n@return  a Streamlet representation of the source object\n\n\n*/\ndef newSourceR: Streamlet[R]\nFunction signature doesn't match comment.\n\n\nIn heron/api/src/scala/com/twitter/heron/streamlet/scala/converter/ScalaToJavaConverter.scala:\n\n@@ -33,8 +38,21 @@ object ScalaToJavaConverter {\n       override def get(): T = f()\n     }\n\n+\n+\n+  def toJavaSourceT: com.twitter.heron.streamlet.Source[T] = {\nPlease use import alias instead of full path\nIn heron/api/src/scala/com/twitter/heron/streamlet/scala/converter/ScalaToJavaConverter.scala:\n\n@@ -27,6 +32,17 @@ object ScalaToJavaConverter {\n       override def get(): T = f()\n     }\n\n+\n+  def toJavaSourceT: com.twitter.heron.streamlet.Source[T] = {\n+    new com.twitter.heron.streamlet.Source[T] {\n+      override def setup(context: Context): Unit = source.setup(context)\n+\n+      override def get(): Collection[T] = scala.collection.JavaConverters.asJavaCollectionConverter(source.get).asJavaCollection\nhmm. interesting. another note is that \"asJavaCollectionConverter(...).asJavaCollection\" looks a bit strange. from function name, it seems asJavaCollectionConverter(...) should return a Java collection, hence the following asJavaCollection looks a bit confusing. Any idea why this is needed?\nIn heron/api/src/scala/com/twitter/heron/streamlet/scala/impl/BuilderImpl.scala:\n\n+//  You may obtain a copy of the License at\n+//\n+//  http://www.apache.org/licenses/LICENSE-2.0\n+//\n+//  Unless required by applicable law or agreed to in writing, software\n+//  distributed under the License is distributed on an \"AS IS\" BASIS,\n+//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+//  See the License for the specific language governing permissions and\n+//  limitations under the License.\n+package com.twitter.heron.streamlet.scala.impl\n+\n+import com.twitter.heron.streamlet.scala.{Builder, Source, Streamlet}\n+import com.twitter.heron.streamlet.scala.converter.ScalaToJavaConverter\n+\n+\n+class BuilderImpl(builder: com.twitter.heron.streamlet.Builder) extends Builder {\nSuggest to use import alias\n\nIn heron/api/tests/scala/com/twitter/heron/streamlet/scala/impl/BuilderImplTest.scala:\n\n+import scala.collection.mutable.ListBuffer\n+\n+/*\n+  * Tests for Scala Builder Implementation functionality\n+  /\n+class BuilderImplTest extends BaseFunSuite {\n+\n+\n+  test(\"BuilderImpl should support streamlet generation from a user defined supplier function\") {\n+    val supplierStreamletObj = Builder.newBuilder.newSource(() => Math.random).setName(\"Supplier_Streamlet_1\").setNumPartitions(20)\n+    assert(supplierStreamletObj.isInstanceOf[Streamlet[Int]])\n+    assertEquals(\"Supplier_Streamlet_1\", supplierStreamletObj.getName)\n+    assertEquals(20, supplierStreamletObj.getNumPartitions)\n+  }\n+\n+  test(\"BuilderImpl should support streamlet generation from a user defined source function\") {\ndescription seems to be wrong.\n\nIn heron/api/tests/scala/com/twitter/heron/streamlet/scala/impl/BuilderImplTest.scala:\n\n+//  you may not use this file except in compliance with the License.\n+//  You may obtain a copy of the License at\n+//\n+//  http://www.apache.org/licenses/LICENSE-2.0\n+//\n+//  Unless required by applicable law or agreed to in writing, software\n+//  distributed under the License is distributed on an \"AS IS\" BASIS,\n+//  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+//  See the License for the specific language governing permissions and\n+//  limitations under the License.\n+package com.twitter.heron.streamlet.scala.impl\n+\n+\n+import com.twitter.heron.streamlet.scala.{Builder,Streamlet,Source}\n+import com.twitter.heron.streamlet.scala.common.BaseFunSuite\n+import org.junit.Assert.{assertEquals, assertTrue}\nsort imports\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. +1 @ashvina and @srkukarni - comments. Thanks @ashvina for the raising the issue. I would think either 2) and 3) will be great!  1) seems very non-deterministic when somebody configures to a value of 0. . @srkukarni @aahmed-se - you need to reduce the memory as well (for instances) so that we can run in a docker container.. @bjonnh - it is currently used only at Twitter for collecting metrics - we could potentially - make it a conditional compilation.. @maosongfu @nwangtw @huijunw - thoughts?. @bjonnh - wondering what is the CPU values that you give when you compile heron?. @bjonnh - here is the PR to fix the issue - \n\nhttps://github.com/twitter/heron/pull/2801. @bjonnh - the PR that I put out is pretty straight forward - if you can verify if it is able to compile using the branch - it will be great!. @bjonnh - can you describe your environment? Machine CPU, operating system and version of the operating system.. It seems the PR that I put out has passed compilation, unit tests and most of the integration tests in Mac OS X, Ubuntu and Debian.. @bjonnh - can you try this with 0.17.8 and see if this works?. We are working to see if we can remove scribe support.\nSent from my iPhone\n\nOn Apr 5, 2018, at 1:12 AM, Horatio notifications@github.com wrote:\nEncounter the same error in my Debian Machine.\nHeron: 0.17.8\nOS: Debian Testing (upgraded recently)\nCPU: Intel(R) Core(TM) i5-2520M CPU @ 2.50GHz\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. You can read a lot of papers and talks on Heron. First paper on Heron \n\nhttps://dl.acm.org/citation.cfm?id=2742788\ncontains a lot of information. If you have any additional questions, please join the slack and we can further clarify.\n. @huijunw - once we have some API that allows for setting the DefaultCpu to lower values per instance - then we can have more instances per containers. Ideally, we should move to a world where instances are assigned the defaults and possibly override by the user. Rest of the calculations such as the number of containers, cpu per container memory per container etc automatically.. @jcoyne - we are working on correcting this issue for the upcoming release 0.17.8. Will keep you posted.. @nwangtw - can you do double instead of float consistently across C++ and Java.. @nwangtw - k8 represents all variants of linux in bazel.. @jerrypeng - can you fix the CI?. Great find - wondering if you could get a PR updating the doc.\nSent from my iPhone\n\nOn Mar 23, 2018, at 7:34 PM, Justin Coyne notifications@github.com wrote:\nYou're right. Should that be added to the getting started guide?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @erenavsarogullari @nwangtw - is this ready to be merged?. @sijie - what is the solution for this? We are unable to run an s3 connector.. @ashvina - the reason we proposed this approach is because we already have some config such as topology.project, topology.team, etc. Instead of changing the design approach we were just augmenting what was already there.. @erenavsarogullari - is this PR ready to go?. Sure - will merge it.\nOn Sat, Apr 7, 2018 at 12:58 PM Eren Avsarogullari notifications@github.com\nwrote:\n@kramasamy https://github.com/kramasamy Thanks for following this. Yes,\nit is ready for review.\nAlso, Scala Streamlet Integration Test Execution Time looks 17s as\nfollows:\n// Integration Pipeline Summary\nheron build integration_test  0:01:25\nheron install 0:04:52\nheron tests install   0:00:19\nheron integration_test local  0:04:10\nheron integration_test http-server initialization 0:00:00\nheron integration_test scala  0:00:17\nheron integration_test java   0:13:11\nheron integration_test python 0:03:25\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/apache/incubator-heron/pull/2826#issuecomment-379495209,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AAWcRMNJPFQ1k4uNLboENxuuwsv5O6s-ks5tmRpNgaJpZM4TBIoF\n.\n. +1 . Ok. Can we have the two configs at heron also available in storm namespace - users in storm namespace should not use heron namespace.\n\nImplementing a stateful interface is fine.. It feels weird to use Heron namespace config - which is always needed for stateful topologies. \nThe second call is needed for serializing the state? If the state is always a hash map - should we register it rather than letting the programmer to do it. Again we don't want to expose anything in Heron API at the storm level. \n. @jerrypeng - my key argument the user needs to be aware that a configuration from Heron API required for using with Storm API - which is not a good developer experience. API namespace should be clean.. It might be easier if you could help test in yarn cluster - since we don't have access to it at streamlio. Just running the integration tests will help.\n\nOn Apr 5, 2018, at 9:06 AM, Ashvin notifications@github.com wrote:\nAlso, I think it will be worthwhile sharing the tests executed to verify the change. In this case I believe unit tests and integration tests may not be sufficient. I am thinking of verifying heron on YARN cluster. What do you think about this?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @joshfischer1108 - if you approve the PR, we can merge it.. Thanks @ajorgensen for the PR. Will things continue to function as it is - if we do not exercise this functionality.. @ajorgensen - we can always depend on the open ssl to be compiled similar to other packages like glog/gperftools. We can even dynamically link it. Now the question is how to match the version of what we compile - with the version available in the production.. @ajorgensen - can you provide some idea on how to run this so that we can test?. @ajorgensen - any reason for closing this?. +1 for eco heron and eco storm - tried several examples and UI and it works file.. @nwangtw - is this ready to be merged?. Will take a look in an hour\nOn Apr 8, 2018, at 12:30 PM, Eren Avsarogullari notifications@github.com wrote:\ncc @kramasamy @nwangtw\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @ajorgensen - once the ci passes, we can merge it.. @sreev - the changes are not just for asf-site - other code files have been changed as well. Can you remove those changes?. @sreev - should we remove this file? @huijunwu might be needing this.. I agree - introducing scheduler dependency into heron core - is not a good idea.. @huijunw - can you explain why Aurora scheduler jar is needed in the core - not clear why?. @srkukarni - can you please take a look at this?. @comes5 - can you include what error you are getting?. Alternatively you could use the docker image at\nstreamlio/build-ubuntu14.0:latest\nit has everything installed already.\n\n. @comes5 - in the new PR #2879 - we are removing the scribe and the thrift executable. Once this is merged, this error will go away.. @nlu90 - done. updating the master.. @nwangtw - can you resolve conflicts if this is good to go.. @joshfischer1108 - ready to merge?. @ajorgensen - is this still an issue?. We can merge it\nSent from my iPhone\n\nOn May 17, 2018, at 7:54 AM, Andrew Jorgensen notifications@github.com wrote:\nIt's an improvement I think since there is a potential for a double free bug here if packets fail to send but I haven't seen a crash in production related to this yet\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @ajorgensen - here it is\nbazel build --config=darwin <targets>. @jerrypeng @maosongfu @nwangtw - can you respond?. @mjschmidt - this is due to the underlying bolt implementation that streamlet maps into. Currently these bolts do not ack or fail when the topology is set to ATLEAST_ONCE. We need to fix this. . @nlu90 - Do you do background cleaning of snapshots or on demand basis? It might be easier to do a background cleaning.. background cleaning might be easier in the case of storage system like hadoop - which takes amount of time to delete several files and reclaim. Furthermore, it makes the checkpointing at tmaster immediate rather than waiting for cleanup to finish. . Ok got it.. @cckellogg - can you check this?. Thanks for the PR. Looks like it is green across the board.. @placeacall - we don't support python3 packages yet.. Sure go for it!. @Code0x58 - is this PR ready to merge?. @Code0x58 - there is no need to run bazel configure anymore. It can be used to check whether all the requirements are present.. @Code0x58 - thanks for the cleanup.. Sounds good @Code0x58 . @Code0x58 - the docker changes are merged I think.. @Code0x58 - could you update the conflicts so that we can merge it?. Looks good!. @yaoliclshlmch - once scala PR gets merged, we should try to rebuild this.. @Code0x58 @erenavsarogullari - is this good to go?. @Yitian-Zhang - the logs will be local during the submission process. You can see the logs using \n\nheron submit <cluster> --verbose\nHope this helps.. heron --verbose gives the logs until the submission of the job is done - either success or failure. After that the logs of scheduler will give some idea about what happened. Once the scheduler schedules the job - tmaster, stmgr, instances log are available.. @kalimfaria - can you please take care of the failing test? Once the ci passes we can merge it.. @maosongfu - might know this.. @Glorfischi - it will be great if you could send a quick PR.. @Code0x58 - great. this is a needed.. @nwangtw @Glorfischi - is this ready to be merged?. @nwangtw - can you please check if this is ok to merge it? @Glorfischi - have you fixed all the feedback.. @nlu90 @kalimfaria - is this ready to be merged?. I would look at the heron job submission path on the client side. It directly writes into zookeeper - which might not close the connection correctly. Does this occur in your custom scheduler or in Aurora scheduler?. Looks good to me - let us merge it once the CI passes.. @thomas4g - do we have to do for other docker containers as well?. OK sounds good.. @kalimfaria - how does this work with respect to non aurora environment?. @nlu90 - you don't need BUILD, *.tar.gz and empty.cc at all. All you need cereal.BUILD - nothing else.. Those files can be safely removed.. @nlu90 - the files BUILD, empty.cc can be removed.. Can you change it to \"Heron is the next generation real time stream processing system, developed at Twitter.\n. We split these sections into two - Installing Bazel and Configuring Bazel. Our ./bazel_configure.py does not install bazel at all.\n. @joestein - You need to ensure that gcc and g++ installed >= 4.8.2 (in other words C++11x). You might have to do \napt-get install gcc-4.8 g++-4.8\nPlease check the .travis.yml for the packages that we install.\n. You need to set the environment variables CC and CXX point to your gcc and g++ compilers.\n. @joestein - a suggestion. It will be convenient if you could create another directory called tools/vagrant and put the vagrant version of bazel.rc there. This will allow us to evolve the build options/test options differently for different environments\n. @joestein - if you are installing gcc-4.8 and g++-4.8, you don't need g++, there might be two g++ in the system - could interfere\n. @joestein - for ubuntu apt-get install libunwind7-dev is sufficient. I tried in a docker image on ubuntu and it worked. For travis ci, I had to resort to downloading the source for libunwind since it did not work.\n. Can you move this file to tools/docker/bazel.rc\n. Can you move the files in heron-conf to heron/config? heron/config contains already several configuration files\n. smf1?\n. If this is a vagrant image zkhost should be local?\n. What is the difference between config_file and config_path?\n. Do we need these options? \n. Should this be an example config?\n. It was already there - hence I left it as it was\n. We need it for backward compatibility - once all the topologies are moved to use the new one, then we can get rid of it. That is why the TO DO.\n. it is removed\n. Not currently until all the topologies are moved.\n. setting up logging is not a good idea here. This function just builds the command line config. Add it in to main? \n. Please rephrase it to \nFor checking the status and logs of the topology, use the working directory %s\n. What happens if the ShellUtils.runAsyncProcess does not succeed?\n. Quick question - what do you mean by config? From my perspective, config is something you read from files, such as where the executor binary is located, default values for RAM, etc. \nContainer Id is user passed. Note that all the options passed thru' the command line will use options parsing later.\n. Note that the config_overrides is still around except we are not using it cleanly yet. Config overrides requires some additional thinking. It potentially needs to handle several arguments especially the ones passed from command line using --config-property since any of the key value properties can be overridden.\n. Instead of putting the code in main - can we have methods called\n- prepare()\n  -post\nwithin prepare, you can validate if topology is running and return boolean. Similarly a post method at the end.\n. call this method prepare?\n. isManageSuccess is a not a good name. Let us keep it simple and isSuccessful?\n. You are validating whether topology is running or not at several places. Instead of duplicating the functionality everywhere, can we use a function similar isTopologyRunning that I wrote in LocalRuntimeManager.java - that can be called at multiple places.\n. Should we call this prepare method?\n. should we encapsulate in post?\n. Retain this function somewhere else so that it can be reused whenever you need.\n. Overall - I feel it is a bit hacky IMO.\n. You are using enum but converting into string again? Is it unsafe?\n. again converting from string to enum? It might be uniform to keep it as a string?\n. @maosongfu - Good suggestion from @osgigeek. can we take care of this in this PR?\n. IMO - enum does not really buy anything - since the strings are being passed from heron cli command, it is guaranteed that those strings will be valid.\n. There is already an issue tracking to do this change. https://github.com/twitter/heron/issues/127\n. Instead of passing these extra args ILauncher and IPacking - when you instantiate them in the calling function, pack them into runtime config - so that you can access it. This allows all object instances are passed thru' the runtime - so that it is uniform - for example SchedulerManagerAdaptor is passed that way.\n. Can you put this file in 3rdparty/commons? Already there is commons-io and since this is similar it might make sense to push it there?\n. Change to use the new 3rdparty/commons:commons-cli-java based on previous comments.\n. Is the indent space follows some coding standard. I have been using 4 spaces for second or more lines.\n. Some comment before the function - it is self obvious - let us keep the consistency of adding a comment for each function.\n. comment before the function name?\n. if ParseException is being caught - why we need it here?\n. Do you want to use the java style documentation for consistency?\n. Java style documentation?\n. instead of string formatter - can we use string builder so that we don't have to count the number of formatting arguments - often it takes a couple of iterations to get this right?  String Builder now will be good especially with the CLI options, thoughts?\n. some formatting, i guess\n. I used this comment to divide code into macro blocks so that it is easy to read.\n. Can you add some comments in the code - so that readers can figure out the logic quickly?\n. Any unit test for the handler?\n. Comment - form and send the http request\n. Comment - traverse the list of metrics\n. We will do in the future PR - https://github.com/twitter/heron/issues/204\n. Remove this TODO line\n. Instead of doing another map - why can't we use the Config structure?  Config auroraProperties? No need for yet another map.\n. This assumes that the uploader is always local file system - since this config is specific to local file system uploader. Aurora launcher should work for any uploader.\n. It looks like auroraProperties is not needed. All you are doing with it is construct the Aurora command. Can't we directly form the aurora command from Context. Perhaps, it might be good to write a Command Builder?\n. Use generic java path class for this?\n. This assumes that the uploader is local file system. Is there a way this can be made generic? One possible approach is to have these lines as a part of the uploader config and insert these lines into a generic aurora file and the merged file can be use to submit an Aurora job?\n. Should we use SHARDS or CONTAINERS? Let us use one terminology consistently. I think we are using containers in most places.\n. Do we need this? This is only applicable for local scheduler, not for aurora.\n. builder.setCluster - rather than setDc. \n. Very simple solution. Why can't we make the core URL and topology URL as bind parameters (similar to others) that we can pass? Uploaders already provide a URL  for topology and core URL is a configuration parameter that we already specify in scheduler.yaml  This makes it easily generic and not depend on the type of uploader?\n. Config structure is pretty easy to use. Furthermore, with your approach, you have introduced new set of keys some of which are different from the keys that we use consistently for Config Keys. This adds more confusion. If iterator is something desired - we can add it, not a big issue.\n. Perhaps I did not clearly explain my solution. We provide both the core uri and the topology uri as a parameter to Aurora command using bind properties - this will translate into\nheron_core_release_url = '{{CORE_PACKAGE_URI}}'\nheron_topology_jar_url = '{{TOPOLOGY_PACKAGE_URI}}'\nThese two can be passed as a bind parameter in the aurora command line - like you do other properties. Note this is not bug prone since the topology URI is provided returned by uploader and core package URI is provided from scheduler.yaml - as a config. Once these parameters are passed to the Aurora command line, it will use your approach for fetching both core package URI and topology package URI using CURL. (There is no assumption here that it will be downloaded before). Since CURL supports several protocols - this approach will satisfy 99% of use cases out of the box - without the user having to modify anything. For the case of packer or something that is not main stream, user can modify the Aurora file to interpret the URL accordingly (whatever it may be). This approach works cleanly for most of the cases.\n. If you use the URL approach, there is no need for any kind of customization for the user. You cannot expect an user to modify the Aurora Launcher according to their own needs - it is not a good design. All uploaders have been standardized to return an URI - even we can make Packer Uploader return an URI. This URI can be passed using Aurora bind variables and let the logic in heron.aurora file interpret it. Again Packer is a corner case, not a common case.\n. That is fine - instead of using hardcoded \"/\" - you could use something like the following\nnew File(Context.heronConf(config), \"heron.aurora\").getPath()\nheron.aurora is hardcoded, we need to add it to keys and defaults file.\n. It is defined in proto for backward compatibility. But moving forward we are using cluster only. Please change it to use as cluster.\n. Furthermore, the URL approach for both core and topology packages should be applicable for Mesos and it is already use in local scheduler as well. Uniform approach for all the schedulers - make it easy to understand, configure and deploy - works in 99% of the cases.\n. This will change to \ncmdline = 'curl {{TOPOLOGY_PACKAGE_URI}} -o topology.tar.gz && tar zxf topology.tar.gz'\n. Some of the variable names are different from the names that we use consistently.\n. This core release package Uri should be accessed by\nContext.corePackageUri(config)\nDon't use hard coded constants. We already have this example in local scheduler - where the config looks the following\nheron.package.core.uri:                      file://${HERON_DIST}/heron-core.tar.gz\nit is in client.yaml. But for aurora, the value can be different but the key should be the same.\n. Is value the same? - if you are providing a vagrant example, it should be different value, isn't? I remember you used before file://vagrant/.herondata or something on those lines. Whatever it is make sure that it is reflected here correctly. this is the value that will be passed to Aurora.\n. Do we really need HERON_PACKAGE_NAME and TOPOLOGY_PKG_NAME? After all it is the same value since you have changed to URI. \n'curl %s -o heron-core.tar.gz && tar zxf heron-core.tar.gz' % heron_core_release_uri\n'curl %s -o topology.tar.gz && tar zxf topology.tar.gz' % heron_topology_jar_uri'\nIt reduces the number of bindings by 2 and not even necessary.\n. check keys.yaml - let us keep the naming as same as possible. Otherwise, it will start diverging. Note that the keys.yaml also defines the key for runtime variables as well.\n. This core uri config should be either in client.yaml or scheduler.yaml. For aurora, since you don't need any ${} variables, you can use scheduler.yaml.\n. Note that we distinguish config into static config read from files, command line config user provide or through overrides and also runtime config. Take a look at keys.yaml - you have every key name defined - already. Use those key names to be consistent in your map rather than using a variation of these names (e.g) we refer ROLE and you use RUN_ROLE and a few more of those - when you change those, you need to change those in heron.aurora file as well. \nAlso consider using the names with SANDBOX_**** since they are more appropriate.\n. Why ZK_NODE and ZK_ROOT? we already have something called STATEMGR_ROOT and STATEMGR_CONNECTION_STRING? In you config, you don't even use ZK - you are using local state mgr. ZK_ROOT and ZK_NODE are not being used in heron.aurora file at all.\n. If the name is fixed as I mentioned about - both TOPOLOGY_PKG_NAME AND HERON_PACKAGE_NAME is not needed.\n. note that we have defined environ as being flexible. it is possible that user might not even its value and provides a value that is different from prod. Open source Aurora allows for test, dev, staging and prod\n. I did a complete revamp to change from Dc to Cluster - now we are introducing again DC. It is a very simple change that we can change it in this PR itself. Otherwise, we will never get to this issue unless some error happens.\n. spelling mistake\n. can the cluster be read from config file - which needs to have a statemgr config one per cluster.\n. should we name it as aurora cluster?\n. the default name of the cluster should be \"local\" - not localhost.\n. What is this config for?\n. postActivate will be called when connection is null - which is not desirable, right?\n. connection null means - postDeactivate will be called.\n. Just wondering why do you need this information?\n. It is not clear to me why this is needed - when no scheduler is running you invoke postActivate and when scheduler is running it is not invoked. it seems very confusing to me.\n. done\n. done.\n. It is an error because the subparser is not registered or not recognized.\n. done\n. changed to info - which is the right thing to do.\n. done\n. This code should not be in Runtime. Runtime is an accessor for runtime config - not to do any action\n. keep a separate file as TMasterUtils?\n. I am assuming this is for Aurora only that does not have a scheduler, isn't?\n. This is removed because cleanState method in RuntimeManagerRunner.java which is called when a topology is killed?\n. It is kind of not intuitive that actual restart is being called in postRestart? Similarly for others. I am assuming that the intention of post methods is to clean up resources, etc.\n. this method is used only in RuntimeManager means we can put it there.\n. ok.\n. Nice one! \n. If you look at some of the template that we have - (e.g) launch. there are three methods - preLaunch, launch and postLaunch. While the submitter has these nice three methods, I was wondering why runtime manager is a bit different where the actual action is not abstracted as a method - whose implementation might be shared or somebody can override if needed.   This suggestion might be a bit involved - probably not right now.\n. Instead of doing this, please use mktemp -d - so that the directories do not clash when multiple users are installing the api simultaneously in a shared server.\nhttp://stackoverflow.com/questions/4632028/how-to-create-a-temporary-directory\n. If the scheduler does not exist, should it return false - it is possible that scheduler is not started or not reachable, due to network issue, right? In this case, is it valid to call postKill? In the Aurora since you have a dummy scheduler, it will respond, if it is there and that will trigger the postKill (where it kills the Aurora job).\n. instead of this you could use\nbazel build heron/...\nthis will build the entire set of jars\n. cd ../heron should not be there\nWhen I try to run ./setup-intellij.sh scripts, it was unable to find the directory. It looks there is cd .. already in ./setup-intellij.sh that brings to the top level WORKSPACE directory - from where bazel needs to be run.\n. Why it is called second try? - It is the first try.\n. you are putting an URI but you are passing java Object which is kind of non intuitive. I think if it is an URI - you should have a generic URI. Java object is too generic - take a look at this  https://docs.oracle.com/javase/7/docs/api/java/net/URI.html\n. See if we can use a generic URI class as mentioned in https://docs.oracle.com/javase/7/docs/api/java/net/URI.html\n. why is this private not public?\n. @maosongfu - Even packer triple can be converted into some URI - as long as it can be interpreted. for (e.g) it can be something packer://tuple0/tuple1/tuple2 or something on those lines. Then everything fits into a generic URI model.\n@osgigeek - @maosongfu refers triple as a tuple with 3 items. @maosongfu - correct me if I have misunderstood.\n. @osgigeek @maosongfu - sounds good.\n. all the context functions do not have get as a prefix - should we stick to that convention? - just\nhadoopConfigDirectory will do\n. same as above.\n. do we have default values for these or the config is always expected?\n. no idea - typically hdfs command line tool knows where to get the config. Not sure please check with hadoop folks - I am fine with either way\n. I know - we should probably change all context functions at some point, until then let us keep the convention and stick with it?\n. good point - need to come up with some generic way of representing path.\n. done\n. We need to revisit whether this is needed. Java home is in two places.\n. done\n. since the tests are fixed with one single test topology and this an evolution of the existing program, I just kept what was there to get it running. Perhaps, we can generalize when we have different tests.\n. user_classpath is used to ensure consistency that user provided jars are the first in java class path. Perhaps, renaming to user_jars might be much better?\n. why this is undo instead of close?\n. Do you want to take care of the suggestion where the uploaders are organized together and sorted alphabetically?\n. Spelling mistake - heron.statemgr.zookeeper.is.initialize.tree\n. should we change to use formatter {0} {1}?\n. please change to use formatter\n. I am assuming these values of 20 is to introduce spaces so that everything is aligned - in the output!\n. This entire directory release have to be removed - once we finish an internal release. \n. @billonahill - since this is coming from bazel, I was wondering if we should deviate from it. This will get the fixes from bazel if needed - if we deviate probably then we might not be able to leverage. Thoughts?\n. Please follow twitter coding standards!\n. @ajorgensen - bazel build files are 4 spaces for indentation.\n. Some comments in the code would be nice!\n. bazel indentation is 4 spaces\n. nice! thanks for fixing this.\n. we can checkin the changes.\n. should we make it case insensitive comparison?\n. @prabhuinbarajan - can you please give us an idea of what would be involved?\n. @prabhuinbarajan - thanks @billonahill - if you think it will lead to cleaner design, let us go for it. Leave it to your recommendation! I am not much of intelliJ person.\n. too much spacing here.\n. Is the indentation correct?\n. After error - should we exit?\n. in other return calls we are returning dict()\n. I would prefer different configuration for both - one goes into scheduler.yaml and the other goes into statemgr.yaml.\n. Let me so know so that I can add the other two configuration!\n. semver is a third party python package, i think it should move to 3rdparty.\n. please move all the files pertaining to semver to 3rdparty/semver - Also, since this package is used as it is, it will create .pyc files when we run bazel_configure.py. We need to ensure that these .pyc files are not generated in the source and even if they are generated and it should be accidentally checked in.\n. there is no more heron-cli - rather it is just 'heron'\n. There is no more heron-cli\n. no more heron-cli\n. Is this URL generic enough to handle all scheduler cases?\n. sounds good!\n. All the *.bzl files are in tools/rules\n. If you change the location of location javadoc.bzl - then you need to \nload(\"/tools/rules/javadoc\", \"javadoc\")\n. no need for an empty line\n. should we remove the comment code or this is going to reenabled in later PRs\n. Do we need the commented code?\n. same as above\n. same as above\n. There are a few more differences especially when we output - I will try to merge and see how it goes.\n. Here is the another difference - since the OUTPUT_DIRECTORY is a bit different.\n. Config overrides are applicable for all the commands - not only for submit.\n. it should be RELEASE file - since mac is case sensitive, it worked. but i changed it to RELEASE (all in caps)\n. from the RELEASE file that ships with the packages, is translated into release.yaml - which is packaged with every topology. hope this explanation helps!\n. Instead of this - which when I read - is cleaning a directory implies that override_config is a directory - while it is not. To improve readability, I think it might be good to use\nshutil.rmtree(os.path.dirname(override_config))\n. same as before\n. same as before\n. same as before.\n. no need for this function - if we are using the above comment.\n. Is this the right place?\n. good idea. let me do this.\n. sounds good!\n. @lewiskan - is this check deterministic? One can install anaconda anywhere and the path might not have the string anaconda in their path.  Does the Anaconda python provide a version string which indicates that it is anaconda python and its version?\n. This is already merged. Probably a good idea to remove it, merge with the master and update the PR.\n. this option is to pass the JVM when running the topology main during submission. Let me know if name is not appropriate.\n. What is the alternative suggestion?\n. This should override_config_file - consistently change in other places as well.\n. override_config_file\n. override_config_file\n. Override_config_file\n. Ditto\n. override_config_file\n. should we add this in the function extract_common_args?\n. Instead of doing generated_release_files - you can use a filegroup similar to :RELEASE ?\n. IMO first, -D is very inconsistent with respect to the other options. Second, it is not clear whether -D is for the topology main or run the scheduler main. Hence, the attempt to distinguish and make it consistent with other options.\n. Let us enumerate the choice\n--topology-jvm-property\n--topology-main-jvm-property\n--topology-main-jvm-args\nAlso the help string can explain cleanly as well\n```\nusage: heron submit [options] cluster/[role]/[environ] topology-file-name topology-class-name [topology-args]\nRequired arguments:\n  cluster/[role]/[env]  Cluster, role, and environ to run topology\n  topology-file-name    Topology jar/tar/zip file\n  topology-class-name   Topology class name\nOptional arguments:\n  --config-path (a string; path to cluster config; default: \"/Users/kramasamy/.heron/conf\")\n  --config-property (key=value; a config key and its value for overriding; default: [])\n  --deploy-deactivated (a boolean; default: \"false\")\n  --topology-jvm-property (property=value; JVM system property for executing topology main; default: [])\n  --verbose (a boolean; default: \"false\")\n```\n. perhaps - we could do a safe check - as in Convert.getDouble. I can add Convert.getBoolean - to be on the safer side?\n. Added a safe check - let me know if this looks ok. @maosongfu is waiting for this PR to merge.\n. If you want to emphasize local execution\n--topology-local-jvm-property\n--topology-main-jvm-property\n--topology-local-execution-jvm-property\nany other suggestions?\n. As per @saileshmittal suggestions, if we were to add properties to modify the behavior of JVM invocation during topology main and also during scheduler execution - here are a few choices\n--topology-jvm-property\n--topology-local-jvm-property\n--topology-main-jvm-property\n--topology-local-execution-jvm-property\n--heron-jvm-property\n--heron-local-jvm-property\n--scheduler-jvm-property\n--scheduler-local-jvm-property\n--system-jvm-property\n--system-local-jvm-property\nLet us pick one pair to consistently represent both.\n. or \n--topology-main-jvm-property \"hello=world\"\nthis is consistent with rest of the other args that we have - for example \n--config-property\n. This ensures that everything uses python2.7 - otherwise, something works in python2.7 while others might not. Ensuring consistency - that is all.\n. python2.7 is an explicit link to actual 2.7 version. This is installed as a part of python 2.7 installation.\n. These are module files and cannot be run standalone. main.py is the driver for all the modules. It has the #!/usr/bin/env python2.7\n. As mentioned before, we are going to look for only python2.7 version.\n. I was wondering if this can into a different PR - might be easier to track and probably be able to figure out spout-instance-unittest\n. my bad - this is needed for checkstyle - should be fine,\n. spelling mistake\n. perhaps changeTopologyState or transitionTopologyState?\n. Can we change the containerId to a generic string? Let the individual scheduler implementation convert it into whatever the type needs to be?\n. whenever we fail to kill - should we clean the state or it should be externally cleanable?\n. transistionTopologyState might be better?\n. Picking\n--topology-main-jvm-property\nsince it looks close enough.\n. +1 - nice suggestion.\n. every implementation - need not change to integer. they will change to whatever suitable. for example, docker container ids are identified by a string.\n. sounds good!\n. sounds good!\n. Is the indentation 4 spaces or 2 spaces? We thought 2 spaces were the norm at twitter - either way I am fine - just checking.\n. like the following \n```\n--topology-main-jvm-property \"hello=world\"\n```\n. Can we make it a single config file? Two for tracker is kind of overkill\n. tmaster containerId is always 0, isn't?\n. there is a heron core uri config that you could use to extract the name - instead of explicitly specifying the name.\n. Will it be always on the localhost? or should we use hostname?\n. instead of changing the permissions of the bazel output files, can we change the dest file, instead?\n. @billonahill - i have been following this convention for all utility classes based on an article that I read in the web.\nhttp://stackoverflow.com/questions/14398747/hide-utility-class-constructor-utility-classes-should-not-have-a-public-or-def\n. If you are starting the UI in a ubuntu machine and want to reach it using your mac with a URL, localhost might not work. The solution is a specific to only localhost. If you could come up something more generic - it can be use across machines as well.\n. sounds good - i will use only private constructor with an empty method\n. parse_config_file - spelling mistake?\n. Check the existence of path\n. It is never used but still checkStyle complained about the parameter name being N\n. m is a convention that is used for member variables in a class. We are not following it for every class but this could be a start. The reason why I started doing this is because the input parameters in functions sometimes clash with the member variables.\n. we need to change for every place - we can do it another PR - this is relevant to check style where the line is more than 100 chars\n. I don't know if the method is being used - but the parameter N/incrValue is not used in the function.\n. https://github.com/twitter/heron/issues/459\n. there is bldr, cbldr, boltBldr, obldr, sbldr, gfbldr - we might have to change a lot of places.\n. This is not relevant for this PR - since this is concerned with checkstyle\n. done\n. done\n. checkstyle - does not allow this, gives errors.\n. done\n. that will be awesome!\n. this is due to the check style errors - it does not want the member variables and local variables to be the same name - even if you qualify it.\n. good point, we will make it a standard.\n. do you want to enforce the directory hierarchy in the header guards?\n. return is on the same line - is this desired for code readability?\n. throw RuntimeException - since check style will complain\n. I used \nLOG.log(Level.INFO, \"Incarnating ourselves as {0} with task id {1}\", new Object[]{physicalPlanHelper.getMyComponent(), physicalPlanHelper.getMyTaskId())}\n. done.\n. done\n. done\n. done.\n. done.\n. done\n. done\n. done\n. done\n. done\n. Do we need copyright information?\n. Supun - it will be great if you could have a separate page for each platform. That will help users to identify the platform and follow the instructions more easily.\n. CentOS 7\n. it is just centos\n. @supunkamburugamuve - gperftools is already part of the code base. The only dependency it requires is the libunwind. If libunwind is downloaded, compiled and installed or just installed, gperftools in the source code base will automatically compile and link.\n. alternatively they can do \napt-get install libunwind8 libunwind-setjmp0-dev\n. alternatively they can do \nsudo apt-get install libtool\n. ok sounds good!\n. Agreed! Removing it - since it will be overwhelming work!\n. ignore_unsupported_sandboxing is already there in tools/bazel.rc for centos and ubuntu. Are there any other options that needs to be there always?\n. --genrule_strategy=standalone\n--spawn_strategy=standalone\ncould be added to tools/bazel.rc.\n. This link should be docs, I think - since it points to the entire documentation as opposed to getting started.\n. we are thinking about adding \n--genrule_strategy=standalone \n--spawn_strategy=standalone\n--sandbox_debug\ninto tools/bazel.rc - if that is the case, you probably don't need these options here?\n. Can you rephrase the error message as  \"Failed to store topology definition\"?\n. Failed to clean topology state \n. instead of hard coding these values - can we define a few constants?\n. same as previous comment\n. @billonahill  - the travis-ci uses a different bazel.rc as opposed to tools/bazel.rc. The file it actually uses are tools/travis/bazel.rc\n. Is this relevant?\n. go for it.\n. @supunkamburugamuve - we have added those options in tools/bazel.rc - hence you probably don't need it in the scripts.\n. insert an empty line before the comment\n. @nlu90 and @billonahill you can run using bazel - but only one instance of bazel can be run at given time in a single repo.\n. @billonahill - scripts/packages:heron-client-install.sh does not depend on integration-test/src/java:local-integration-tests_deploy.jar\n. let us version 0 (since version numbers are never negative)\n. let us also use some value for tag as well.\n. should we move into heron/uploaders/src/java/com/twitter/heron/uploader/utils?\n. You might want to ensure that the tests are able to run both in\n- bazel\n- command line\nCan you ensure that?\n. All the contents of aurora logging properties has moved here?\n. Need to link to docs page.\n. done.\n. done.\n. done.\n. done.\n. Instead we can keep only the generic part - omit the version and time etc?\n. done.\n. done.\n. done.\n. done.\n. done.\n. This PR is a great start. travis-ci failed due to errors in java styling issues. I would suggest you to sync with the master and run the bazel command. We are enforcing styles by default.\n. We need to ensure that the documentation does not drift when cli is updated. it is part of the review burden. The documentation already existed - all I did it is to enhance it and ensure its correctness. Furthermore, having this in the docs helps people to understand the usability of the system - whenever they do evaluations, etc.\n. Working directory is not needed for distributed scheduler - this is mainly for local scheduler.\n. you mentioned - you use a distributed file system. LocalFileSystemUploader is used only for local file systems.\n. If the file system is a distributed file system, you need to give the path in the distributed file system.\n. this function is for local file system - you need to upload the jars to the distributed file system and download from the distributed file system.\n. We have changes those utils - please sync with the master and recompile.\n. Are these changes required?\n. Heron is very flexible - it can work with a shared directory as well as individual directories. The draw back of working on individual directories - is the fact that you will not be able to see other topologies run by others - unless the tracker and UI is shared and configured to pick up all the individual state manager directories.\n. done.\n. Can you please change to Centos 7?\n. For centos 7, there is no need to compile the bazel. The binary releases will work. But for centos 6, you will need it.\n. good catch @objmagic - since we are not supporting the implementation of --trace-execution. I would suggest to remove the line 43 and just return args2?\n. The one in Local Scheduler is being changed to be closer to HdfsContext - that is why @maosongfu suggested this.\n. @maosongfu - could you give explanation for @supunkamburugamuve?\n. Comment is not appropriate.\n. I would suggest rename 'hpc' directory to 'slurm' directory. The reason is these are example files that named for each scheduler.\n. change 'hpc' to 'slurm' - as the convention is to keep the name of the scheduler.\n. the file name has a spelling mistake - it should be \"SlurmController.java\"\n. It should be cluster I think - since we changed everything to use the term cluster.\n. done\n. done.\n. done\n. done.\n. done.\n. done\n. done\n. done\n. done.\n. done.\n. done\n. done.\n. done.\n. done\n. done\n. done.\n. done.\n. done\n. done.\n. done.\n. done.\n. There is no notion of Heron scheduler - instead we piggy back on existing schedulers like Mesos, YARN, etc.\n. This essentially configures how heron works with existing scheduler.\n. done.\n. done.\n. done.\n. Should you raise exception here?\n. can you add some comment for this config?\n. add some comment for this config?\n. we should standardize on --- not the mdash.\n. When I grepped for it - I did not see anything.\n. done.\n. done.\n. Fixed all the mdashes to use ---\n. when we do heron cli - heron shows up highlighted color (red). I have not seen the highlighted color - used with links.\n. That text was present before - I am removing the link.\n. ran make linkchecker as well to catch but it looks fine to me.\n. @lucperkins - why the heron version is hard coded. It might be hard to maintain this.\n. @saileshmittal - can you incorporate this?\n. space required between $getting_started_url and 'for'\n. Thanks for the fix. In order for the travis ci to compile correctly, you need to download bazel 0.2.3 in the .travis.yml \nhttps://github.com/twitter/heron/blob/master/.travis.yml\nCan you make the changes and update the PR?\n. This might not be enough - we might have to package this jar along with storm and api jars as well - so that we can publish to maven and goes into the install scripts.\n. why is this help needed?\n. since it is across packages - not sure how to share it in bazel.\n. @lewiskan - can you remove the email address? Instead can we point to the google groups?\n. @maosongfu - Just wondering if you have taken care of @billonahill comments?\n. Can we use the {{% bazelVersion %}} for these and several others as well? That will save several changes when we change bazel version later?\n. Sounds fair.\n. We need to get this jar into heron-api install scripts - so that when a user installs he/she gets everything.\n. OSTYPE is a predefined environment variable. For mac\necho $OSTYPE\ndarwin14\nand for Centos\necho $OSTYPE\nlinux-gnu\n. how could you distinguish between different flavors of linux? We support both centos and ubuntu.\n. One issue with $OSTYPE might be the return value 'linux-gnu' for both Centos and Ubuntu and we cannot distinguish between the two.\n. +1 - we should make this as a generic script. @caofangkun - would appreciate if you could detect the OS and include it in get_all_heron_paths()\n. @caofangkun - can you include @billonahill feedback on the docs?\n. +1 - makes it more readable\n. +1 - makes it more readable\n. +1\n. +1\n. @caofangkun - recommend using $PLATFORM rather than $PLAT_FORM - easier to read?\n. @caofangkun - any thoughts about this feedback?\n. @caofangkun - Wondering why you need the 'go' lang package?\n. @caofangkun - if you install python-pygments above, why we are uninstalling and installing again?\n. Adding API docs - along side 'Get Started' is not a good idea. Instead, it might be better if you could add it at the top next to 'Docs'. Instead of calling it Docs, call it just an 'API'\n. @ashvina - Do you have any comments on this?\n. Heron when installed - actually prints the full path of '~' - should we change to ${HOME}?\n. @caofangkun - you need to just do \ntest --test_output=errors\nonly once.\n. our convention is put each option in a separate line to improve readability. please put the test option in a separate line.\n. This seems to be repeating code in main.py also?\n. There is not much comment in the code. Can you add function level comments? and code comments where it is not intuitive for the whole code.\n. Do we need this comment?\n. There is a lot of repetition of code between heron cli and heron explorer. Can you refactor and put them into common area so that it can be shared?\n. I would suggest to keep these - multiple lists.\n. instead use heron_java_proto_files() + spi_deps_files + local_statemgr_deps_files + ... \nit is modular and we know what dependencies are there.\n. keep these as I said before.\n. instead use common_deps_files + spi_deps_files + locafs_deps_files + ... \n. instead use heron_java_proto_files() + common_deps_files + spi_deps_files + ...\n. this line and the following should be \nscl enable devtoolset-2 bash \"gcc --version\"\notherwise, it does not work. Instead it reports the older gcc version\n. Can you open an issue to track this?\nSent from my iPhone\n\nOn Jul 13, 2016, at 9:04 PM, Maosong Fu notifications@github.com wrote:\nIn heron/schedulers/src/java/com/twitter/heron/scheduler/mesos/framework/MesosFramework.java:\n\n\nProtos.ExecutorID executorID,\nProtos.SlaveID slaveID,\nbyte[] data) {\n// TODO(mfu): TO handle this message\nLOG.info(\"Received framework message.\");\n}\n  +\n@Override\npublic void disconnected(SchedulerDriver schedulerDriver) {\n// TODO(mfu): TO handle this failure\nLOG.info(\"Disconnected!\");\n}\n  +\n@Override\npublic void slaveLost(SchedulerDriver schedulerDriver, Protos.SlaveID slaveID) {\n// TODO(mfu): TO handle this failure\n  True the topology will not work normally if a slave lost. The recovering mechanism is complicated and reconcile() is needed. Prefer not to handle this in this iteration consider this scheduler implementation does not aim for production.\n\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @windie - if you could take care of this, we can merge this PR. The rationale for this feedback is keep the dependencies modular rather than a long list - make it easy to understand.\n. same as above - keep the common_deps_files, spi_deps_files, etc and include the appropriate deps - otherwise it is a long list.\n. same as above.\n. same as above.\n. do you think callbacks can be defined as a list outside and assign this outside list to callbacks = listofcallbacks. This will provide space per line and you don't need to introduce new lines.\n. Sounds good to me. @objmagic - can you create an issue for tracking?\n. it should \"slightly bigger than\"\n. Can you describe a few scenarios when this packing is useful?\n. with 0.3.0 @bazel_tools is not supported I believe. Our repo can compile both in bazel 0.2.3 and bazel 0.3.0. Can you please ensure that it compiles with bazel 0.3.0 as well?\n. Can you add a lot of comments so that we can understand?\n. It will be great if you could rename this file to be heronrc.sample or SAMPLE .heronrc.sample will be hidden in the code and people might not see it.\n. Cool - travis ci works with bazel 0.2.3\n. this will print the exception stack - rather than giving an error. Probably, a nicely formatted message why this happened might be more informative.\n. same as before.\n. same as before.\n. same as before.\n. same as before.\n. This needs to be the copyright file from Storm - not Twitter copyright.\n. Please keep the Storm copyright files. In general, if you use a file from Storm codebase you need to retain the same copyright.\n. same as before.\n. Retain the original copyright.\n. If you have written it - you could use Twitter copyright or if you have taken code from Storm repo, we could retain the Storm copyright.\n. This needs to have Storm Copyright.\n. This needs to have Storm copyright information\n. This needs to have Storm copyright information.\n. This needs to have to Storm Copyright.\n. If this file is written by you, this copyright is fine or if the file is taken from Apache Storm, please use their copyright.\n. Use the Storm copyright\n. use the storm copyright.\n. Please use the storm copyright\n. Please use the storm copyright\n. Please use the storm copyright\n. Please use the storm copyright\n. Please use the storm copyright\n. Please use the storm copyright\n. Please use the storm copyright\n. Please use the storm copyright\n. Please use the storm copyright\n. Instead of specifying everything under spi - I would suggest module by module - helps to keep track of it in a cleaner fashion.\n. Do we need this deps since the lib refers to the same file?\n. There were a bunch of other heron instance options. Wondering if they are not needed?\n. Why abc?\n. Pickle is used as serializer at the tuple level or something else? pickle could be the cause of performance?\n. this files seems to be repeated in multiple PRs?\n. Can we read this value from the configuration file? I am sure this constant exists already somewhere. Please do not hard code value.\n. alternatively - you could separate the main into main.py and keep the remaining code as heron-executor.py. In this case, you could compile heron-executor as lib and main.py as pex-binary with heron-executor as lib for linking. Feel free to punt this suggestion if it is not worthwhile.\n. Can you add a huge separator comment between big groups of configuration? For example, before instance specific configuration - can you add a huge comment so that when we read the file we can get to specific group quickly?\n. same as before?\n. ok - at some point it might be nice to split up this file into different files from a modularization perspective.\n. especially it takes 32 arguments in the main :(  - we should augment it with options to improve readability.\n. @billonahill - just wondering how the args[5] is processed in the new version?\n. Can you add a file level comment so that we know what is it used for?\n. I was wondering - instead of starting with the name 'heron' - can we start with some scrambled version such as \"A89109120120\" - a UUID and the likelihood of it clashing is very very less, isn't? Just an idea - not sure how feasible it is?\n. do you need this file since it is already checked in?\n. can you move this common area - if everybody is interested in adding version information? heron-tracker, ui, cli, explorer, packer, etc.\n. This is duplicated code - we need to find a home in common\n. This import is vey unintuitive - as it imports config as utils. Can we make it more readable?\n. Should be change it to --topology_binary and change it in the Java file? We can do it the next PR.\n. Do we need 'At the time this article is written....'?  Please remove - we will forget when we make a release.\n. To confirm Mesos cluster is ready.\n. instead of 'to make it use' change it to 'to use'\n. When you log.debug - it prints only when the verbose option is enabled?\n. What is the difference here? In case raise Exception and terminate - where as in the Log.error case, it will print the error and continue with the execution?\n. signal handler is appearing two times?\n. why use pylint and disable?\n. can we add a unit test for task hook?\n. can we refactor this code?\n. why we are still calling TOPOLOGY_JAR_FILE? probably TOPOLOGY_BINARY_FILE\n. Also - what is PYHERON_INSTANCE_BINARY? Should we pass this - or we can package this as a part of the core and since core is already downloaded unzipped - all we have to locate this file - this might be better approach - reduces the number of parameters in heron executor invocation. In fact, you can package python instance binary in heron core package and it will available to you similar to TMASTER_BINARY?\n. This profiling is available only if we use local scheduler, correct? If that is the case do we need it?\n. Let us keep the line length to a fixed value - whatever it is today. otherwise, people will write longer lines.\n. why call pyheron_st_instance? simply calling heron-python - will make it easier to remember and also  sticks to the convention?\n. probably call \n\nSANDBOX_PYTHON_INSTANCE_BINARY and\nheron.binaries.sandbox.python.instance\n. please rename to Context.pythonInstanceSandboxBinary()\n. same as before - name change.\n. cool - please ignore this.\n. Instead of SANDBOX_PYHERON_INSTANCE_BINARY please call it \nSANDBOX_PYTHON_INSTANCE_BINARY?\nHeron name is redundant since the entire code is Heron.\n. this does not seem to be correct. if the --verbose option is provided by the user then only we add the --verbose option to the java programs.\n. this is not the correct logic - only if the --verbose is provided, then enable the debug level option.\n. this should be ok.\n. can you import as Log rather than log? then the subsequent changes are not required.\n. this should be Log.debug.\n. same here as well.\n. why give false here? should we pass the verbose value that was passed?\n. I think it is a good idea to use logging uniformly - what is rationale for using do print?\n. @objmagic - it is ok since it is always looking in the user's home directory. \n@prabhuinbarajan - it might be good to change it to \nos.path.join(os.path.expanduser('~'), '.heronrc')\n. I think unrecognized is fine.\n. Is there a way not to hard code this path?\n. can you open a PR for this?\n. do we need this with the new change?\n. I was not clear - I meant the variable \"resource\"\n. A bunch of the files seems to be instance related. Why we are putting these in heron/common?\n. this should not be allowed IMO since positional arguments are always required.\n. use Log.debug(\"Topology deployment status %s\" % (opt.get_config(...))\nso that it is displayed even in the case of active vs paused.\n. Why change to IHeronSerializer?\n. do you want to use Log.debug(traceback.print_exc())\n. why call st_heron_instance? simply calling heron_instance will suffice?\n. same as before heron_instance.py and stmgr_client.py\n. can you break the line in the argument as compared to in the middle of invocation?\n. ok.\n. it will be good if you can do else if and compare with jar - even though we filter it at the top. As people add more code - people will tend to forget and they might not add here. always good to add more checks.\n. keep it a full name 'pyheron_integration_core'\n. leave a space between JAVA and PYTHON\n. instead of this - can we do the following - \naddress = socker.gethostbyname(socket.gethostname()) if not command_line_args['address'] else command_line_args['address']\n. or instead of the long line - \naddress = socket.gethostbyname(socket.gethostname())\nif command_line_args['address']:\n   address = command_line_args['address']\neasy to read as well, thoughts? \n. @lukess - if the user knows that there are multiple NICs, you address port combination will help, isn't? 0.0.0.0 might be security loophole - since allows for all ip address.\n. Also, you might want to make the same changes to heron-tracker as well?\n. @lukess - that is bad. It does this in the case of multiple NICs using socket.gethostbyname(), is that correct?\n. in my case it does the following\nheron-ui\nINFO: 08/12/2016 18:11:09 -0700 Listening at http://127.0.0.1:8889\nINFO: 08/12/2016 18:11:09 -0700 Using tracker url: http://localhost:8888\n. @lukess - can you check how does your /etc/hosts file looks like?\n. it is listening in IPv6 and IPv4 in port *.8889\n. please change the name to common-pyheron - just to be consistent.\n. it should be heron-examples-python? we need to rename heron-examples in java to heron-examples-java?\n. please change it to heronstreaming@gmail.com\n. Pyheron package allows a developer to write python topology in Heron. Pyheron is backward compatible with the popular python API called streamparse. It can be run in various clusters including Mesos/Aurora, Mesos/Marathon, YARN, etc.\n. you should use DEFAULT_ADDRESS\n. should it address=address?\n. @lukess - please consider a seperate PR for the tracker address. \n. This is because the user defined class path should come earlier than the system or heron class path.\ncheers\n/karthik\n\nOn Aug 15, 2016, at 9:36 AM, Mark Li notifications@github.com wrote:\nIn heron/cli/src/python/execute.py https://github.com/twitter/heron/pull/1245#discussion_r74790579:\n\n@@ -54,7 +54,7 @@ def heron_class(class_name, lib_jars, extra_jars=None, args=None, java_defines=N\n   # the java opts must be passed as part of the list\n   all_args = [config.get_java_path(), \"-client\", \"-Xmx1g\"] + \\\n              java_opts + \\\n-             [\"-cp\", config.get_classpath(lib_jars + extra_jars)]\n-             [\"-cp\", config.get_classpath(extra_jars + lib_jars)]\n  em.. why switching order here?\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/pull/1245/files/88942cede245a9d6f1442e04388dc3189772eebc#r74790579, or mute the thread https://github.com/notifications/unsubscribe-auth/AAWcRNhbHFyTSRI-xhUmTgJWpuh1-4Uaks5qgJWcgaJpZM4Jh24I.\n. Is this the overall resource requirement for the entire plan?\n. I am assuming this is the resource requirement for the entire container?\n. Just wondering if this is a request send by Topology Master to Executor or something else?\n. If this is indeed a request - do we need a response? In general, for control messages, it will always good to send a response back.\n. Instead of two classes - one for serialization and the other deserialization - can we have all of them in a single class? It will be easy to read and any modification will be localized - makes it easy to read and review code - in case one is changed but not the other?\n. thanks for the clarification.\n. sounds good.\n. Since this is a nice tool - it might be good to separate this out in a separate file - so that it works for any state manager. We can make it work for any state manager?\n. Nice cleanup!\n. Instead of do_print - can we use our logging API - for consistency?\n. +1 cool - nice.\n. Instead of having the main there - could we separate this into tool called - \n\nheron-zk\nthat can be used to show the contents of state managers -  this tool can take the name of the cluster and use the appropriate config file for state manager and display the contents - will be useful for troubleshooting? just a thought, we could do this in python also?\n. ok sounds good - let us worry about the tool later.\n. From a terminology perspective what does HeronWorker mean?\n. I think just contrib should work in a general fashion. As we add more spouts and bolts into contrib area, you might not want to change this file again and again\n. Is this for testing - when you mentioned 'local'?\n. Why a new configuration variable? Will there be any issues when mixing packing algorithms?\n. should it be jvm_metrics or pyvm_metrics?\n. heron tools is repeated twice.\n. what does this file do?\n. Instead of explicitly declaring an iterator - you can use auto?\n. again use \"auto\"\n. probably \"auto\" again.\n. @mycFelix - should we need this since we have released 0.14.3 already and 0.14.4 is going to be released this week or early next week?\n. Is 0.14.2 version necessary?\n. Please change this to be !instance_name.empty()\n. Is this config used in scheduler.yaml? If that is the case it is needed in keys.yaml and you need an accessor function in one of the Context/Config to access this config. If there are some default values to be supplied for this argument, you need to add this in defaults.yaml. It is a messy design - we need to cleanup some point in time.\n. if the user does not provide a role in the command line - we supply the role as \"default\" and the environment as \"default\"\n. the base image should be a heron core image - that already contains the core image of heron?\n. It is at the CLI level - by the time it invokes the submitter which in turn calls the uploader, it might be too late.\n. you mean scheduler will download the docker image for heron core and the docker image for topology? or the heron core is a tar image (like we have it today) and the docker image for the topology will know where to download the core when docker image is starting? Is that correct?\n. got it from the detailed explanation.\n. Similar to this file, can you add a sample for heron.aurora - if somebody wants to use docker uploader. You could add this under the directory heron/config/src/yaml/aurora - with the file name heron.aurora.docker.\n. can you write a page documentation similar to other uploaders so that we have information about how to use this?\n. Instead of using environment variable, a better approach would be to check if these new configs are present, if these are present, use those - otherwise, default to what it is used to be. Alternatively, we can make this config mandatory - with the default pointing to heron.aurora file in the cluster directory. \n. +1 please use --config-property, that is a better approach.\n. What is this TODO? Can you comment on this?\n. SendMessage might be deleting memory underneath - we need to be careful here.\n. ignore this.\n. Please remove the TODO - since there is no comment what needs to be done.\n. TODO without comment\n. By default, this is not installed in /etc/heron. It is installed in where the cluster config resides. Location of where the config resides is provided as configPath in SubmitterMain - you need to use this configPath. Probably, a better approach would be if the config is not present, fall back to the default approach?\n. sounds good. perhaps, we can make the comment little more verbose \n- in the sense that these two parameters are optional\n- Any situations it is used as well\n. @chris-pardy - in the case of both the config being absent, how does this work? Is the JOB_TEMPLATE_DIR set to default somewhere?\n. @chris-pardy - how does this work? If the docker image is supposed to contain both the core image as well as topology image, we need to just use docker pull to get the image, is it correct?\n. ## @billonahill - should we just keep only --config-property? IMO - it might not be a good idea to provide API since our goal long term is to remove them from being hard coded.\n. If you are unable to connect, can you please print a log message for the host/port that you are unable to connect. I know it is not there previously - but it will be nice to have.\n. you might want to change the example configuration files or given a commented example in\nheron/tools/config/src/yaml/tracker/heron_tracker.yaml\n. for local file system, there is a file system call that you can use open system call with OCREAT - that automatically creates if a local file does not exist or fails if the file exists. It might be better solution, I think\n. which means it will copy to another directory - which incurs overhead and also a configuration of directory where the file needs to be moved as well. Probably, it might be a good idea to have NullUploader.\n. If you need the heron binaries in a container, we can build it as a part of the release process and ship it. Let me know if this will simplify it.\n. great! if you and @chris-pardy can let us what will be needed, we can get this done.\n. it should be Received a terminal or Received terminals\n. why the change from 15 to 25?\n. Most of the times the integration tests not flaky, I believe.\n. @billonahill - this might not work since there is no target defined in this file?\n. Instead of calling it UpdatableComponent should we call it IUpdatable to keep interface names consistent.\n. Wondering if there is more code to be added here?\n. ok\n. my bad - i did not realize it is at end\n. Good point - this needs to be addressed. If the function execution exceeds beyond a certain time - some action needs to be taken.\n. Does this assume that all the containers are homogeneous?\n. Would be nice if you could add some comments?\n. Fine with me. I was thinking since it was exposing some corner conditions - it might be worthwhile. Leave it to you.\n. The include files are in the hdrs now. Realized this is not needed in the srcs actually.. yes - to keep it simple in the grand scheme of things.. I don't mind this. If we use your suggestion, somebody later might think it is needs to be cleaned up since a single assignment will suffice.. Can you add a comment about this line why this needed to improve readability?. This is a hardcoded value. Can we move this to a generic config so that it is flexible?. Same as before - should we hard code this?. Just wondering if this is needed?. it should be stmgrBinary?. is there a better name than 'remote' - something like 'toSandboxMode' or 'toContainerMode' - something on the lines.. instead of 'toLocalMode' - can we call 'toClientMode' or something better on those lines.. it should be stmgrBinary?. Why this requires true?. @billonahill - the previous version was doing one level substitution and it was not generic.. If sandbox is not needed any more, do we need this?. stmgrBinary?. For remote - 'toClusterMode' is more appropriate I think.. Some comments - will be nice.. @jrcrawfo - one possible approach is to use a configuration file. Perhaps in the Marathon Scheduler file - @nlu90 can you help?. this does not seem to be correct. if you delete the iter, how can we do iter++. instead of for - you could use while similar to the mSentPackets (that follows)?. Is this standard c++? I thought it is GNU C++ extension. If this is part of the regular c++ standard, it is fine?. Remove extra log information that you introduced for debugging.. ditto. Instead of LOG(ERROR), use PLOG(ERROR) and you won't need strerror(errno). Instead of multiplying by 1024* 1024 - every time, can you define a constant in one of the include file called \nconst int MB = 1024 * 1024;\nsomewhere in the include file. This is not 100 MB it should be 100 * 1024 * 1024. got it. my bad.. can you please open an issue to track this?. Why the need to switch args for this one and several subsequent functions?. Again switching of arguments?. Got it!. @congwang - can you print the stmgr id as well?. @congwang - here also please print the stmgr id. Also, bin/java - you might not want to hard code it. You might want to do two path join - we can make it windows proof.. Another issue could be Twitter internal JVM version detection needs to be different since on the version string, it does something different - we have to look at the next line and parse it to determine the version.. @objmagic - the intention of com.twitter.heron.xxx is to hide implementation details. Nobody should access the com.twitter.heron.xxx api - since it is bound to change. Developers should stick with org.apache.storm.xxx or backtype.storm.xxx apis only.. I agree that we should not mix backtype api (which is storm < 1.0) and org.apache.storm api (storm >= 1.0) together.. +1. @jrcrawfo - I think what @nlu90 is referring to is - if any other environment (other than Docker) is setting HOST environment variable, this could fail. Hence I was wondering if we can add a check to see if we are running inside a docker environment. If we do, then we can safely assume that HOST variable is set - something along the lines of the following thread\nhttp://stackoverflow.com/questions/23513045/how-to-check-if-a-process-is-running-inside-docker-container\n. Since this is a test wondering if we need to get the HOST from environment variable - just socket.gethostname() should be fine?. Why does the pom file exist here? What is it for?. @objmagic - does not feel write to be in this directory. . Are errno's negative? Otherwise it might not work. Instead I would suggest to return NOT_OK.. It should SP_NOTOK. Here is the file common/src/cpp/basics/sprcodes.h. What does << stmgr_port means? Does it just print stmgr_port when the test fails or succeeds?. @huijunw - can you add some meaningful message so that it says what it is printing?. Can you return SP_NOTOK to be consistent with other functions?. Instead of using global variable - can you the function directly where a port is needed?. Can you add comments in the Start_Base method and also in options that new port is set when port 0 is provided.. Do we need to need to have a global variable for saving the port? Can we add something in the class itself?. Is this a local file system or it expects zookeeper?. Do we require tunneling? If not, please remove the tunnel configuration.. Instead of adding code to this function, can we make a overall function called get_host - in which we can check whether it is running in docker or ECS or use socket.gethostname() and return the hostname. . There is a lot of hard coding of values here. Not sure why you need them?. @ananthgs - why is this URL hard coded?. @ananthgs - Is this image a generic ubuntu image downloaded from docker hub? or specially created image? If this is specially created image - what does it contain other than the basic OS?. Why these ports are hard coded? Can we ask ECS to provide a dynamic port?. If the assumption is that we are going to pass 0 always. If that is the case, port can be returned. You will get to race condition here - since the server_port will be modified by the thread and server port is used in a different thread.. the comment does not read well. it should be rephrased as \n// If the count is greater than target, then wait until the count reaches the value of target.. I am assuming d is duration - can make it a descriptive name?. @huijunw - instead of pointer, can you use reference?. @huijunw - instead of * can you use reference like sp_uint32& port?  It is much more easy to read.. @huijunw - do you need this message?. use reference instead of pointer for both port and latch?. why this check?. @huijunwu - I saw this fragment of code repeating in another PR. Can you write a small utility function in sockutils.cpp - getPort - given the socket and use that function here?. Use references instead of pointers. why this check?. This code is being repeated in the other PRs as well. Can you please abstract it to a function in sock utils.cpp and call that function here.. Got it. But it will be good idea to make it a meaningful function.. weird error though - let us stick with pointer. Ok got it.. use nullptr?. How can you guarantee that port_ is initialized to 0?. It is the same for other PRs as well.. Do you need a string array for this, if the URL is going to always one?. Should we use constants instead of hard coded 5 secs?. Update the print out if changed to constant. @cckellogg - ubuntu is misspelled.. if you are using auto there is no need for auto*. Why the tools is twice?. Please rename the file to build heron docker image.. something like docker/build-docker-image.sh - it is confusing with build-docker.sh. Seems inconsistent in the documentation.. No check is required.. use mkdir -p . run_build_docker please change build_docker_image. If this is not supported - when multiple containers/pods run in the same machine, how do they divvy up the disk space - is there a formula?. got it. We can add this once the feature is available, the tracking issue is\nhttps://github.com/twitter/heron/issues/1943. This is required for Python packaging - so that we can submit to pypi. When we package it, the packager specifies that the readme file should either README or README.txt or README.rst (not README.md). Link is added.. @huijunw - why is this metrics port needed? Tmaster is already receiving all the metrics, correct?. If not, there is some kind of white list that can be include the back pressure metrics. I already see the back pressure metrics in UI.. @huijunw - what does the --secret do? Do we need only when the restart back pressure is enabled? or it can be enabled in all the cases?. I would recommend to use --config-property in heron submit which is a generic mechanism which is already in place for passing feature specific parameters.. All the parameter code passing will be gone - if --config-property is used.. Please move this to heron_internals.yaml.\n```\n\nTo autorestart containers, if they go into unrecoverable back pressure\n\nTime duration the container should be in back pressure\nheron.container.backpressure.duration.min: 10\nTime interval between contiguous restarts\nheron.container.backpressure.interval.min: 20\n```\n. This block of code is not needed.. This is not needed.. Not needed.. If clause is not needed since the secret could be enabled by default in all cases.. this is not needed.. Shared secret with HeathManager. 0, 20 is not needed.. No change required.. no change required.. No change here also.. No changes required in the file. Logic should move to Health Manager process.. Logic should move to HealthManager process. No changes in this file.. no changes in this file. No changes in this file as well.. No changes in this file. +1 - let us use Metrics Cache Manager. . if the argument is not valid, we exit right away - since it is a mandatory argument. Yes it is a mandatory argument. Is this test flaky?. Can you make the package name as com.twitter.heron.common.testhelpers or something similar - you don't want to give the impression that it is some kind of test.. similarly the target should //heron/common/src/java:testhelpers-java. I would suggest not to move the file - this is going to be used in DC/OS and ECS later. Hence the original place was more apt and we take a dependency on jackson and we shade jackson where it is being used (like instance).. Please change the copyright?. WindowInfo - can we come up with a better name?. Can we have a better name than WindowInfo?. @ashvina - wondering why there are so many dependencies?. @ashvina - is the health manager one per topology or for all topologies?. If the health manager is per topology, we can enable MetricsCacheManager Sink by default, similar to Tmaster sink.. Better log message - is this needed?. Better log message?. What is this samplebolt for? Can you add more comments?. Can we do dotted notation, in a cascading fashion?. can you move it to the top - all the version so that it is all in one place.. do you want to keep at 8888 similar to tracker?. Why this file is called BUILDX?. If you don't need this, please remove it.. agreed - the repo can be easily duplicated since once it is deployed the changes will be none or minimal. you can have a copy of that repo internally at Twitter. In twitterify script, you could change the repo to refer to the internal repo.. @objmagic - let us get this branch work internally first before we can merge the changes. I have been testing and it seems to work - except a java test failure in one of the travis-ci. mirror will not be sufficient since the remote repository URLs have to change to internal twitter URLs.. It is controlling the number of jobs that can run in parallel. Docker containers in Mac by default comes with 4 cores and 2 GB of memory. Within that constraint, we are able to run 25 jobs without bazel terminating with the error 'EOF'.. We might need this as well so that we know what file that is being used always.. this should be Log.info since it gives an idea about how long it look for all the heron commands to execute - always independent of whether there is verbose.\n. What happens when you relaunch the topology after changing code? Then the new code will not take effect?. please move this file into tools/rules - where we keep all the bazel rule files. It is not ideal in the new version of bazel but eventually when we move to next version of bazel we can move this around.. Again this should be in tools/rules as well - until we move it later versions of bazel.. +1. you should use ${HOME}/.herondata/checkpoints. you should use ${HOME}/.herondata/checkpoints. self._initialized_global_metrics?. generate_pom.cc should go into tools - this looks a generation tool that can be shared by others.. It should be a sub block. new gflags use cmake now - instead of configure/make in the older versions.. Instead of long name distributedlog, can we use just dlog?. dlog-java?. dlog-java?. move one line up. aggregate all common and instance references?. can you change the target pytest-py to something like instance-tests-py. @jerrypeng - comment about how these values are being chosen will be helpful. Are these values configurable by the user?. downloaded?. @jerrypeng - that is fine. please just add a comment saying that we are using arbitrary values since it is going to transient. Later when somebody reads the code, they might not be able to know the context.. you should rename the file 'RemapCustomGrouping.java'?. Similarly the class as well.. please rename it to RemapStreamlet.java and also the class.. BiFunction - should be Bifunction ?. +1. this patch is to ensure we use a fast path for C++11x so that performance is not affected. The patch was originally done by @congwang - just redid it for protobuf 3.4.0. Help should be appropriate?. view cluster configuration or properties?. @huijunw - this is for docker sandbox image.. There is no notion of worker child process in heron - can you rephrase it?. the same as before. it should be OnReconnectTimer() - Reconnect is one word.. Should be ReconnectTimer?. can you rename the file to bookkeeper-apiserver.yaml? just to make it easy.. It should Apache before BookKeeper. please change it to bookkeeper-apiserver.yaml - so that it is easy to remember. @huijunw - this target conf-yaml includes every yaml file and also every aurora file.. This value is hard coded, probably not a good idea. Since you already have the yaml config, you can use this value - whatever this value is. Since most of the time this value is not changed, it automatically becomes the default value. \nAlso the logic seems to be incorrect. If you are allowing for overrides, the overridden value should be used - but is that there somewhere?. @erenavsarogullari - ByteAmount is a convenience class that we use to make it more readable. Any idea why we need to change ByteAmount?. removed this since the image already sets JAVA_HOME.. Yes indeed - it gives some overview of what has been installed.. @lucperkins  - working on hosting the open source charts in a different place. It should be done today.. @cckellogg - you might want to change heron.aurora file that launches heron_executor as well.. n1-standard-4 and there is no need for 2 SSDs per machine. Since we use stateful sets, there is no need for it.. Again SSDs are not needed since we use stateful sets. The stateful sets automatically create the necessary persistent claim templates.. local ssd count - is not needed.. We need to add a repo - \nhelm repo add heron-charts . @jerrypeng - are you using the heron-install.sh script or just untaring the heron.tar.gz?. @jerrypeng - for nomad case, we can add a config that in nomad/scheduler.yaml - similar to local and it will work. For aurora nothing will break since the core_package_uri will be in a URL that is stored somewhere else.\n. the core logic - can either curl or use symbolic - which is controlled by a config. It is a question of telling nomad which one to use. For Aurora, the deployment is that - heron-core.tar.gz is served from a file server (like webserver or packer) . We use 4 spaces indentation.. How do you distinguish the case between \n- Normal running topology\n- Topology in a partial state. @dancollins34 - we all use java 8 by default - which means we are ok.. @nwangtw - use to generate library jars for 7. After twitter moved to Java 8, we also moved to Java 8 - by default you need Java 8 to compile and even to run your topologies.. @joshfischer1108 - API can be used for topologies running as well in cluster - correct?. @lucperkins - is this change ok?. @cckellogg - you mean heron_tools_bin_files - probably leave it for now until we don't need it.. yes for now - Let us leave the old one and remove it later.. @cckellogg - we need both since one is used for heron binary packaging while the other is used for docker container.. @erenavsarogullari - is it not possible to use maven jar?. @huijunw - thanks for the PR - should we use the same algorithm for both?. Please remove the extra line.. Please remove the extra space.. please remove the extra line. same as before.. LOG.info -  I am assuming that it prints information always during eco topology submission?. Do we need this - if we are changing the config later in the same file?. should we have something like /download/core rather /downloadcore. It will be nice to print the highlevel steps that eco topologies do - during submission. Here the topology might not be in the running state?. This code is in scheduler core - you cannot assume that it is aurora specific. It is used by other schedulers. Please change this. Confirmation with the user should not be for all cases. If the user knows the behavior - it is annoying everytime to say yes/no - furthermore think about the fact that heron update is running in CI/CD - yes/no will not work very well.. This verification can be done at the cli level itself - rather than doing it in the core.. Do you need this another warning?. What is this message for?. foundData - does not sound like a good name. Any other choices?. furthermore, there is no way for the user to verify whether the resources are available. Resource checking should be automatic. This is aurora specific. In other schedulers, we can ask for extra containers - rather than \"aurora like update\". this data is all about executionState - is it correct? if so we can name it as\nexecutionStateExists\nwill do?\n. @nwangtw - this is not data. this is metadata actually. this is what we call executionState (it consists of logical, physical plan and additional information). alternatively a better one is \nstaleExecutionData\n. @nwangtw - can you make this as a function?. this message might not be misleading - since we are not killing the topology I think.. is this variable \nhasStaleExecutionData. @erenavsarogullari - instead of forward slash - can you use Java Path class - to keep it more generic?. @nwangtw - if the intention is to convert to - toScalaStreamlet - that will be fine.. @skanjila - any idea why the compilation errors occurred?. @ajorgensen - yes we have only heron.tar.gz and heron-install.sh -- please update the PR.. @nwangtw - why is the validation code removed?. Spelling mistake - it should be \"topology id\". @nwangtw - what happens if both the update component parallelism and also user run time config is provided simultaneously.. use vector and no need to allocate memory.. just use component_parallelism instead of component_parallelisms. leave it as component_name - so that it is informative.. This seems to be the case across the board.. @ashvina - this default is for all instances for a particular topology. if your instance needs more memory - you can override using setComponentRam - which is the next PR that will be coming up. . good catch it is not needed.. @nwangtw - i need to take my comment back. this is needed to ensure all linux variants are grouped under one target which is k8 - otherwise, it is ubuntu, centos and more variations.. @jerrypeng - is this needed for 'nomad' direct deployment?. ok got it - let us fix the CI and merge it.. @ashvina - any reason why --verbose is removed?. it should org.apache.heron now - instead of org.apache.heron. same as before.. @Code0x58 - wondering if you had a chance to test this with py3 topologies?. It might be worth trying and if it works - we can use json dumps and load.. Let us use one parameter and comma separated. heron.scheduler.properties is a good one.. Why this extra copy?. Ok. thanks.. @nwangtw - stmgr runs in a single thread. hence loopExit is fine in the context of sigterm handler.. @nwangtw - stmgr runs in a single thread. hence loopExit is fine in the context of sigterm handler.. @Glorfischi - can you please do those changes for other unRegister events as well?. Sure will change.. I am removing those rule files from the code base itself.. I am removing those rule files from the code base itself.. do you need an auto* ? just auto event will be fine, isn't?. same here and other places.. This is not the right fix. SCRATCH_DIR is used to create directory. Multiple RUN commands increase the number of layers which increase the docker size - you need to optimize to get them into one single RUN command. same as above. same as above. same as above. same as above. ",
    "joestein": "I got further along, thanks for the help so far. I am better matching now what is in \n.travis.yml\n/tools/travis-ci/bazel.rc\n/docs/developers/compiling.md\nNew issue stuck at, I will tackle it later tonight if I have time or tomorrow, let me know if you see anything else I am doing wrong\n==> master: ERROR: /vagrant/3rdparty/libevent/BUILD:11:1: declared output '3rdparty/libevent/lib/libevent_openssl.a' was not created by genrule. This is probably because the genrule actually didn't create this output, or because the output was a directory and the genrule was run remotely (note that only the contents of declared file outputs are copied from genrules run remotely).\n==> master: ERROR: /vagrant/3rdparty/libevent/BUILD:11:1: not all outputs were created.\nthanks!\n. looks like that last commit (had to try one more thing before leaving the office) got the build to pass :)\nINFO: Elapsed time: 746.802s, Critical Path: 663.02s.\nwill try the release packages and get it working on Mesos next error to get past \nERROR: /vagrant/release/BUILD:1: Extension file not found: 'tools/build_defs/pkg/pkg.bzl'.\nERROR: error loading package 'release': Extension file not found: 'tools/build_defs/pkg/pkg.bzl'.\n. manual run fail after successful build trying to make packages\n~/bin/bazel build --config=ubuntu --define RELEASE=0.1.0-SNAPSHOT release:packages\n..........\nWARNING: Config values are not defined in any .rc file: ubuntu\nINFO: Found 1 target...\nINFO: From Linking heron/stmgr/src/cpp/heron-stmgr:\n/usr/bin/ld: bazel-out/local_linux-fastbuild/genfiles/3rdparty/gperftools/lib/libtcmalloc.a(libtcmalloc_internal_la-sampler.o): undefined reference to symbol 'log@@GLIBC_2.2.5'\n//lib/x86_64-linux-gnu/libm.so.6: error adding symbols: DSO missing from command line\ncollect2: error: ld returned 1 exit status\nERROR: /vagrant/heron/stmgr/src/cpp/BUILD:104:1: Linking of rule '//heron/stmgr/src/cpp:heron-stmgr' failed: gcc failed: error executing command /usr/bin/gcc -o bazel-out/local_linux-fastbuild/bin/heron/stmgr/src/cpp/heron-stmgr -B/usr/bin/ -Wl,-z,relro,-z,now -no-canonical-prefixes -pass-exit-codes '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' ... (remaining 2 argument(s) skipped).\nTarget //release:packages failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 122.601s, Critical Path: 114.47s.\nalso tried\n~/bin/bazel --bazelrc=tools/travis-ci/bazel.rc build --config=ubuntu --define RELEASE=0.1.0-SNAPSHOT release:packages\nINFO: Reading 'startup' options from /vagrant/tools/travis-ci/bazel.rc: --host_jvm_args=-Xmx2500m --host_jvm_args=-Xms2500m --batch\nWARNING: Running Bazel server needs to be killed, because the startup options are different.\nSending SIGTERM to previous Bazel server (pid=2001)... done.\nWARNING: Config values are not defined in any .rc file: ubuntu\nERROR: /vagrant/release/BUILD:1: Extension file not found: 'tools/build_defs/pkg/pkg.bzl'.\nERROR: error loading package 'release': Extension file not found: 'tools/build_defs/pkg/pkg.bzl'.\nINFO: Elapsed time: 0.768s.\nwill dig more tonight/tomorrow let me know if anything sticks out, thanks!\n. @kramasamy success! full build on a mesos cluster in VM works\nvagrant up\nwoo!\n```\n==> master: Target //release:packages up-to-date:\n==> master:   bazel-bin/release/heron-api.tar.gz\n==> master:   bazel-bin/release/heron-cli.tar.gz\n==> master:   bazel-bin/release/heron-core.tar.gz\n==> master:   bazel-bin/release/heron-metrics-api.tar.gz\n==> master:   bazel-bin/release/heron-storm-compat.tar.gz\n==> master:   bazel-bin/release/heron-tracker.tar.gz\n==> master: ____Elapsed time: 121.238s, Critical Path: 113.26s\n```\nmesos started also successfully will try to run heron on mesos next chance I get\n. Hey @jingwei thanks for the review, sorry I have been traveling should have time this weekend to start through things\n. superseded by #638 \n. @maosongfu yup, in progress refactoring, added you to https://github.com/elodina/heron\n. @maosongfu @DarinJ here are the latest changes from previous version to work on latest trunk https://github.com/elodina/heron/tree/v2 starting testing in a few hours hopefully or tomorrow am else\n. @maosongfu I add you maybe email invite didn't go through you should see it here https://github.com/elodina then accept should get you in\n. @kramasamy unfortunately I won't though I thought I would sorry @mhausenblas if you or @DarinJ or someone else can help getting this over the goal line maybe @aShevc could maybe too dunno but I won't be able to myself.\n. ",
    "ajorgensen": "I also have a docker setup to compile heron (haven't tested it on master yet with new bazel). I can put it up in a PR but just wanted to add it here as a reference point.\nDockerfile\n```\nFROM docker.crash.io/base-trusty\nRUN apt-get update && apt-get -y install \\\n      python \\\n      python2.7-dev \\\n      curl \\\n      build-essential \\\n      automake \\\n      libtool \\\n      git \\\n      unzip \\\n      oracle-java8-jdk \\ \n      libssl-dev \\\n      cmake \\\n      software-properties-common \\\n      libunwind8 \\\n      libunwind-setjmp0-dev\nDefine commonly used JAVA_HOME variable\nENV JAVA_HOME /usr/lib/jvm/java-8-oracle\nADD build.sh /build.sh\n```\nbuild.sh\n``` bash\n!/bin/bash\nset -o nounset\nset -o errexit\necho \"Building heron with version $HERON_VERSION\"\nmkdir /scratch\ncd /scratch\necho \"Extracting source\"\ntar --strip-components 1 -C . -xf /src.tar.gz\necho \"Preparing source for build\"\nif [-d \".git\"]; then\n  git clean -fdx\nfi\n./bazel_configure.py\n./bazel --batch clean\necho \"Creating release packages\"\n./bazel --batch build -c opt --python2_path /usr/bin/python2.7 --define RELEASE=$HERON_VERSION release:packages\nfor file in ./bazel-genfiles/release/*.tar.gz; do\n  filename=$(basename $file)\n  mv $file /dist/${filename/unversioned/$HERON_VERSION}\ndone\n```\nbuild-artifacts.sh\n``` bash\n!/bin/bash\nset -o errexit\nrealpath() {\n  echo \"$(cd \"$(dirname \"$1\")\"; pwd)/$(basename \"$1\")\"\n}\nrun_build() {\n  RELEASE_TAR=$(realpath $1)\n  ARTIFACT_DIR=$(realpath $2)\n  HERON_VERSION=$3\necho \"Building heron-compiler docker container\"\n  docker build -t heron-compiler $(dirname $0)\nmkdir -p $ARTIFACT_DIR\nTAR_DIR=\"$(realpath $RELEASE_TAR)\"\n  docker run \\\n    --rm \\\n    -e HERON_VERSION=$HERON_VERSION \\\n    -v \"$ARTIFACT_DIR:/dist\" \\\n    -v \"$TAR_DIR:/src.tar.gz:ro\" \\\n    -t \"heron-compiler\" /build.sh\n}\ncase $# in\n  3)\n    run_build \"$@\"\n    ;;\n*)\n    echo 'usage:'\n    echo 'to build heron'\n    echo \"  $0 HERON_SOURCE_TARBALL OUTPUT_DIRECTORY HERON_VERSION \"\n    echo \"  \"\n    echo \"NOTE: If running on OSX, the output directory will likely need to \"\n    echo \"      be under /Users so virtualbox has access to.\"\n    exit 1\n    ;;\nesac\n```\n. @kramasamy i'd be happy to. It's currently setup to be contained in a separate repo similar to this project (https://github.com/stevendborrelli/mesos-docker-builders) but I think I can modify it to add it to the heron repo and be run from there.\n. Looks like ci has failed on this commit (https://github.com/twitter/heron/commit/62cf520d9aa0adc27671fb2c749e577c381e4450), not sure why it works on my machine and not ci and vice versa.\n. Sounds good, I'll close this PR then as I dont think its needed.\n. Duplicate of https://github.com/twitter/heron/pull/49\n. :+1: \n. The intermediate layers are actually cached so subsequent builds should not be replaced but use the cached layers from the previous run. So building with ubuntu and then centos and then ubuntu again you should not have to pay the building penalty on the second ubuntu run.\nI'm going to add a tag to the docker container for each platform to help separate the images however I think its good change anyway.\n. There's a lot here so its a bit hard to reason about the impact of the change, but overall it looks good to me.\n. I dont think this was actually the issue in the end, going to close for now.\n. Yes this is resolved. The cli arguments were unclear, should be fixed some with documentation.\n. This looks awesome, I  was able to try it out and it looks like it setup the project perfectly.\n. One thing i did notice is that heron/config/src is missing from the source list. This contains some configuration yaml files which I'm guessing are not picked up when generating the source paths. They would be handy to add so that they come up in the omnisearch bar.\n. This looks good to me!\n. This is expected behavior, probably should add some docs to make it clear just in case people put files there.\n. This looks good to me. I ran rm *.iml; rm -rf .idea and then ran the setup script and it looks like it set everything up correctly.\nIt looks like you have a few lines commented out in scripts/setup-intellij.sh that could be cleaned up but other than that I think this looks good.\nEdit: Would it be possible to add the 3rd party folder as well?\n. @kramasamy I updated the spacing, let me know if there are any other style guide violations.\n. I added a few comments, please let me know if they make sense or need more clarification\n. Looks good.\n. This looks good to me.\n. I definitely will. I instrumented it in a task hook that we have and will port that code over and put up a PR when i get a chance.\n. I'll fix these ASAP, how do i run checkstyle to make sure i've gotten everything?\n. For example this works:\n``` bash\n!/usr/bin/env bash\nset -e\ncmd=\"cat /does/not/exist\"\n$($cmd || true)\necho \"status: $?\"\n```\nOutputs:\n```\n:!bash /private/tmp/foo.sh\ncat: /does/not/exist: No such file or directory\nstatus: 0\n```\nI think you need to change status=$($cmd) to status=$($cmd || true) and remove it from where you set cmd=\n. Looks good to me.\n. Looks good \ud83d\udc4d \n. This looks good to me!\n. @maosongfu I am away until Tuesday but I can take a look at it then. \n. Just wanted to add a bit of a note here. This change caused a pretty significant performance degradation (30% or so) in our topologies. The reason is kind of interesting.\nIn our case we are using kafka which uses log4j directly. log4j-over-slf4j replaces all of the log4j methods and routes them through slf4j so that they can be dispatched to another logger. When this dependency was removed it caused that to no longer be the case. Heron was setting the log level to INFO for slf4j which also ended up setting the log4j level to INFO as well. When the dependency was removed this was no longer the case because the two logging mechanisms were separated. log4j defaults to a log level of DEBUG which meant that kafka (and other dependencies in a worse way for some other topologies) started to output all of their debug log to essentially /dev/null because it is the default for log4j. So we could not see the increase in log messages in our heron logs and had to profile the topology when it was in a bad state vs a good state to see that kafka was spending a tremendous amount of time logging out debug messages.\nI think we should consider adding this dependency back in to align all of the logging to the same place, otherwise we could have multiple loggers writing to different places on different log levels which can be very confusing and is not apparent without some pretty serious digging.\nThe message that you put in the description of this issue happens in our topologies as well but does not have any impact on the runtime of the topology. My feeling is a simple log message about multiple bindings is far better than accidentally turning on debug mode for any dependency that uses log4j explicitly.\n. It is but this will mean anyone using Kafka or anytl dependency  that uses\nlog4j directly will need to either add the log4j-over-slf4j dependency or\nhave a log4j.propeties file. My issue there is that failure case is not at\nall apaarent and very hard to find whereas the error you saw was very clear\nand easy to fix. The only way I found this was by asking why the\nperformance of my topology degraded enough times to finally uncover the\nissue. The lack of visibility here I think is reason enough to revert this\nchange and if there's a conflict you can add an explicit exclude. If\nsomeone has a better way of handling this I'd love to hear it.\n. I think we need either one or the other, log4j-over-slf4j delegates all log4j calls to slf4j so when we set the log level it gets applied to both. slf4j-log4j12 delegates all slf4j calls to log4j so we would have to set the log level in log4j land (I believe) and add log4j as a dependency. Given that we've already chosen the former route with how we set the log level and use slf4j I think we necessarily need to use log4j-over-slf4j in order to get a unified logging experience that is consistent and work around cases where another dependency has chosen to go the other way.\n. Correct. However the heron UI provides links directly into the log files which means it is required that the logs go through heron in order to be written into the correct place. Going through log4j will result in logs not being accessible from the heron ui so the end user would have a worse experience and would have to find the logs manually each time. In order for that all to work heron necessarily needs to handle all of the logging to make sure its accessible.\nI believe the default log4j logger actually logs to stdout but because we were using slf4j to handle the logging it was never actually getting to a place where I could see it.\n. Heron is the runtime. I don't agree with you the log4j-over-slf4j is a topology dependency. You are running topology code inside the heron runtime it should be heron's responsibility to configure how that logging works. I also do not agree that it would be better if log4j should be configured separately, it would potentially break the linking between the heron logs and the ui in a way that is not immediately apparent.\nAdditionally the failure case for not including log4j-over-slf4j is much worse than the failure case for including it. If any dependency uses log4j directly it will silently start spewing DEBUG logging information which depending on the dependency can cause serious performance degradations in the topology without any indication as to why. In order to figure out what was causing the problem I had to profile the app and go through the call tree to see that kafka was spending a lot of time writing debug information.\nI think that if you are running your code inside someone else's runtime (Hadoop, Heron, Storm, etc) that it is their responsibility to configure the logging system to do the right thing.\nslf4j is supposed to be a facade that allows you to write code against a common interface and then the place where you are running the code can configure how that information gets logged out. The runtime decides what logging system it wants to use and configures it accordingly, if you only include slf4j-api or use log4j directly this just works its when dependencies start including log4j-over-slf4j or slf4j-log4j12 directly and configuring logging that this breaks down because the runtime is suddenly not able to make the decisions it needs to to work.\n. For posterity and to summarize a conversation in chat, the path forward here is to provide the logging bridge as a runtime dependency on the classpath rather than keeping it as a compile time dependency. This will allow people to point to a different logging bridge if they need to depending on their use case. I can look into making this change or someone else would be more than welcome to. I'll open up another issue to track it and we can close this issue.\n. Correct I was missing a required config file specifically uploader.yaml. I'm not sure if theres a list of the minimum config thats needed to actually run the heron cli. If you turn on the debug logging when running a cli command you can see it checking to see if theres a list of config files and if they are not there it just continues. For example:\n[2016-06-09 18:58:35 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config stream\n[2016-06-09 18:58:36 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config\n[2016-06-09 18:58:36 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config stream\n[2016-06-09 18:58:36 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config\n[2016-06-09 18:58:36 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Config file /home/ajorgensen/heron-client/conf/production/cluster.yaml does not exist\n[2016-06-09 18:58:36 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Config file /home/ajorgensen/heron-client/conf/production/client.yaml does not exist\n[2016-06-09 18:58:36 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Config file /home/ajorgensen/heron-client/conf/production/packing.yaml does not exist\n[2016-06-09 18:58:36 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Config file /home/ajorgensen/heron-client/conf/production/scheduler.yaml does not exist\n[2016-06-09 18:58:36 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Config file /home/ajorgensen/heron-client/conf/production/statemgr.yaml does not exist\n[2016-06-09 18:58:36 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Config file /home/ajorgensen/heron-client/conf/production/uploader.yaml does not exist\n[2016-06-09 18:58:36 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /home/ajorgensen/heron-client/release.yaml\n[2016-06-09 18:58:36 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /home/ajorgensen/heron-client/release.yaml\n[2016-06-09 18:58:36 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /tmp/tmpreYOcn/override.yaml\n[2016-06-09 18:58:36 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /tmp/tmpreYOcn/override.yaml\nSo I think we can do two things here, first is make the classloader exception message a little more descriptive. Secondly we should either supply defaults or fail the command if a required config file does not exist. In this case we could default to the NullUploader in the event that the uploader.yaml file is not supplied which would circumvent the exception case. I still think making the exception better is worthwhile because if the class for the uploader is provided but mistyped they'll still hit that exception path.\n. Yes, it reconnects fine but will disconnect again after then next tick tuple and round of tuples being emitted. I have not yet looked into whether those tuples are lost during this incident or if they end up getting sent eventually. Regardless of any tuples getting lost, it can take a few seconds to reconnect which can cause some backpressure depending on the workload of the bolt itself.\n. Interesting, good find! I'll give that a try.\n. @jrpspam This looks like there might be an issue with the connection between the stream manager and the component instance. When this happens can you look at the process list (ps aux | grep heron) and confirm that the stmgr process is still running? You can also run netstat -tulpn | grep <port number> to make sure there is still a process listening on that port you see in the logs.\nYou can also look at the stmgr logs directly (should be in the same folder) to make sure there wasnt an exception or something.. To add an anecdote to this currently we are using a memory aware packing algorithm because we've found that we can get container sizes down by a pretty significant amount by taking memory into account. There are of course trade-offs here depending on the cpu requirements of each of the components but in general for topologies that have a few large components we've found that by sorting the components by memory required first and then using a round robin approach to place them in containers can reduce the total memory overhead where the number of containers is greater than or equal to the number of the instances of the largest component.\nAs an example if we have a topology that has the following characteristics:\nSpot1: 2 x 500Mb RAM \nBolt1: 2 x 500Mb RAM\nBolt2 4 x 2Gb RAM\nwith 3 containers a simple round robin might get you:\n[S1_1, B1_2, B2_3] [S1_2, B2_1,B2_4] [B1_1, B2_2]\nThe largest container in this case would be the second one (1 Spot and 2 Bolt2s) with adds up to 4.5Gb of RAM needed. In the case of Aurora all of the containers need to be the same so we require 4.5Gb * 3 = 13.5 Gb of RAM total. If we take memory into account and increase the number of containers by one we can get away with\n[B2_1, S1_1][B2_2, S1_2][B2_3, B1_1][B2_4, B1_2]\nWhich in this case we only need 2.5Gb per container or 10Gb of RAM total for a savings of 3.5Gb. \nI'm curious your thoughts on bin packing and the dimensions that you'd most care to optimize for.\n. Sorry I should have been more clear, I have a implementation of bin packing for memory but it's not on master. I will try to get into a state that can be put into a pr when I have some time. \nControlling for CPU is a bit more difficult. For the JVM you can pretty easily set the min and max heap size which controls for memory but there is no mechanism that I know of to control for CPU in the same way. For meoss and Aurora we have cgroups which provide CPU usage isolation to control for noisy neighbors but I'm not aware of a mechanism that could be applied to a specific processes in containers although I wonder if cgroups could be used for this as well.\nI like the idea of the packing algorithm being able to determine the optimal number of containers so it uses the fewest overall resources. I think ideally it'd be nice to not have the containers need to be homogenous which would cut back on waste even more, I believe you can get this when running and managing tasks on meoss but it's not possible on Aurora. \n. I was thinking that we just make it look the same as heron-api and treat it the same way. Using the genrule in the PR i was able to generate the jar and upload it to a private artifactory instance and include it in a project.\n. Sounds great. Thanks!\n. @kramasamy I added the spi jar to the heron-api-install.sh flow and tested it out locally. I added the spi jar to the list returned by heron_api_lib_files, let me know if thats not the appropriate place to put it. It seemed logical to group them together and as far as I could tell the only place that method was used was in the generation of heron-api.tar.gz and the install script.\n. This looks good to me. Thanks for the quick turn around!\n. @tysonnorris that sounds great! I'm just going to run through it locally and verify that it works but the code changes look good to me.\n. Looks good, thanks!\n. I really like the idea of having as much of the build process as possible through bazel, I think it would make providing different docker containers for different targets easier. It seems totally possible to do this in an iterative approach starting with the docker build step and then replacing other steps by either building out the functionality ourselves or if bazel starts to provide more docker integrations then using those.\n. Can you check if core dumps are turned on ulimit -c. If that is set to 0 you can turn it on and re-run the job to see if it will give us some more information about what is going on. Here's an article on how to turn it on for everyone http://www.akadia.com/services/ora_enable_core.html. You can also see where Ubuntu will store the core dumps by looking at cat /proc/sys/kernel/core_pattern which on my machines is set to /proc/%p/cwd/core.%p which should put the core dump in the root directory of the sandbox of the job.\nWe generally have core dumps turned off as they have the potential to fill up the disk really fast depending on the failure and only turn them on when necessary to gather more information. It's also possible to run it with a specific limit to help curb this but I've generally found we only need core dumps in very rare cases.\n. Interesting, I'll take a look and see whats going on. @pradeepchemical1 thanks for the analysis!\n. @pradeepchemical1 Do you have any more of the exception handy? I tried reproducing off master and I was not able to get the same error message and was able to successfully upload the topology to s3.\n. I was not able to reproduce the issue with the code you provided which is really strange. I am not opposed to upgrading the sdk version, that feels like a good idea regardless. It's just strange to me that I cannot reproduce the error. Also the 400 Bad Request seems pretty nondescript. Out of curiosity, what region is your s3 instance in do you know? It looks like the boto project had a similar looking exception that was entirely based on what region the s3 instance was in due to some authentication mechanism changes (https://github.com/boto/boto/issues/2916).\nI can go ahead and update the dependency version, it would just be nice to understand the why as well.\n. Let me try to create a s3 bucket in ap-south-1 and give it a try. I'll report back if I find anything. Either way it sounds like upgrading fixes your issue.\n. Yup thats exactly what the problem was, with 1.10.66 I was able to query a bucket in us-east-1 but was not able to query a bucket in ap-south-1. They must have changed some authentication or request structure for that region in the client.\nCaught an AmazonServiceException, which means your request made it to Amazon S3, but was rejected with an error response for some reason.\nError Message:    Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: D44DD531D7B8FE50)\nHTTP Status Code: 400\nAWS Error Code:   400 Bad Request\nError Type:       Client\nRequest ID:       D44DD531D7B8FE50\nAlso confirmed that upgrading to 1.11.14 fixed the issue. Thanks for reporting the issue and for all the information!\n. @kramasamy yeah it looks like they updated their dependency on http_core, I updated the PR with that change, tests should pass now.\n. @avflor I think aside from my initial comments it looks good to me. I have other aspirations to have those default padding values be configurable as well but I don't believe that needs to happen in this pass. \n. Opened one here: https://github.com/twitter/heron/issues/1084\n. PR looks good to me!\n. @kramasamy that information is available. I'm not sure from a UI perspective where the best place to put that information would be, my feeling is it would look too cluttered in the hover over. We could put that information under the Instance Metrics when you click on a specific instance in a container. It feels a bit weird to get container level information (host in this case) by clicking on a specific instance but I'm not sure where else in the UI it should go.\n. I'll leave pulling out the host information for another PR, I think some thought needs to go into where that information would be best located.\n. Looks like it, I missed the \ud83d\udc4d comment. \n. This implementation should be working now. I would like to get some feedback on the last commit (https://github.com/twitter/heron/pull/1128/commits/12258d10bb1038adc280a17f6b43bcafe1877836). We need to assume that the topology is valid in the kill case in order for the command not to fail. This was the quickest way of solving that I could find without doing a larger refactoring.\n. @billonahill Made the changes you suggested and fixed all the tests. Let me know if it looks good to you.\n. @joelanford as far as I know this is likely still an issue, the scheduler code architecture changed a bit and I didnt have time to update this PR so I closed it. The issue is still open https://github.com/twitter/heron/issues/1126 if you want to take a stab at it.. As far as i can tell reflectasm does not have any compiled dependencies: https://mvnrepository.com/artifact/com.esotericsoftware/reflectasm and it is the only compiled dependency listed for https://mvnrepository.com/artifact/com.esotericsoftware/kryo/3.0.3 \n. That sounds great. Is there a task that can be run to check for missing dependencies or is it more of a manual process?\n. @maosongfu did you want to block this PR on the dependency check or is this ok to merge? If you want to include other dependencies in this change whats the best way to flesh that out and I can go ahead and do that.\n. We can keep this open, we're still on 0.14.0 so I can just cherry pick this commit to get us up and running. Its not really blocking me I just wanted to check back to see if there was anything I should be doing on my end.\n. I vote to merge as is and not couple an audit of dependencies with it.\n. There is actually a heron version string that is passed in whose purpose was to be the version that the release was tagged with. What are peoples thoughts about\neither\n- removing that and allowing the override as I have implemented here\nor\n- removing the git parsing for the version string\n(See: https://github.com/twitter/heron/pull/1227/files#diff-489b3a1dc33fb4234394f371a8e67fe1R104)\nIt appears that HERON_VERSION is only used in naming the file which feels a bit misleading.\n. I can do it or someone else is more than welcome to pick it up if they have time. I might have some time this weekend to work on it.\n. Looks like this may have been fixed but not deployed? Serving the docs off of master locally gives the expected outcome.\n. @kramasamy @objmagic is there a plan to deploy the change? Looks like it was fixed 6 days ago but is still broken in production\n. Ok sounds good. Thanks!\nOn Thursday, September 8, 2016, Mark Li notifications@github.com wrote:\n\n@ajorgensen https://github.com/ajorgensen actually, we are going to\nrelease 0.14.3 soon. I will build website after 0.14.3.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/1358#issuecomment-245756155, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAfHU_JZBDW3YFyaETxq0yI4Stm4xk3cks5qoIYzgaJpZM4J4SXu\n.\n\n\nAndrew Jorgensen\n@ajorgensen\n. This looks good to me.\n. I'm curious how you are getting the log files out of the docker sandbox and into a place where they can be accessed by heron/aurora. We played around with this approach at first but found that getting at the log files in the docker container was more complex than was worth the effort of running docker. If your answer is you're not then this implementation will break multiple components the of the UI that allow for streaming the logs from the aurora container \n. Sorry I must have misunderstood the purpose then, is the purpose of to use docker as a transportation mechanism for a tarball? My experience with docker and aurora is limited so sorry if this is a naive question but how does the/home/<role>/<topology> folder (which correct me if i'm wrong is in the docker container)  get mounted in such a way that it can be access in the aurora sandbox? I see your aurora command is referencing by its absolute path /home/<role>/<topology>. I see your comment here https://github.com/twitter/heron/pull/1419/files#diff-99cb9acf72a2f3e634670ac6ecab2593R30 so does that mean this is unusable out of the box until someone writes a scheduler to build up another container that mounts the aurora sandbox correct?\n. Sorry for the delay i wasn't able to verify this change locally this weekend. I'll do that today.\n. I am getting the following when trying to compile on ubuntu 14.04 with docker:\nERROR: /scratch/heron/uploaders/src/java/BUILD:120:12: in deps attribute of java_binary rule //heron/uploaders/src/java:docker-uploader-unshaded: deps not allowed without srcs; move to runtime_deps?\nERROR: Analysis of target '//scripts/packages:tarpkgs' failed; build aborted.\nHere's the command:\n./docker/build-artifacts.sh ubuntu14.04 docker ~/Downloads/heron-artifacts/heron-docker\nDo you mind just verifying that it works for you? \nI also tried upgrading bazel to 0.3.1 but get the same thing:\n```\nbazel build --config=ubuntu^C\nbazel version\nBuild label: 0.3.1\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\nBuild timestamp: 1469783392\nBuild timestamp as int: 1469783392\nbazel build -c opt --config=ubuntu //heron/uploaders/src/java:docker-uploader-unshaded\nERROR: /scratch/heron/uploaders/src/java/BUILD:120:12: in deps attribute of java_binary rule //heron/uploaders/src/java:docker-uploader-unshaded: deps not allowed without srcs; move to runtime_deps?\nERROR: Analysis of target '//heron/uploaders/src/java:docker-uploader-unshaded' failed; build aborted.\nINFO: Elapsed time: 2.733s\n``\n. @I see whats going on, this.tarignore` file is a bit too aggressive: https://github.com/twitter/heron/blob/master/docker/.tarignore#L2\nDo you mind updating that file with this diff:\n``` diff\ndiff --git a/docker/.tarignore b/docker/.tarignore\nindex f8fc0f4..154db68 100644\n--- a/docker/.tarignore\n+++ b/docker/.tarignore\n@@ -1,5 +1,5 @@\n .git\n-docker\n+/docker\n*.pyc\n```\n. Was able to get a little bit farther. The container is now exiting with a error code when attempting to be run:\n\nI was able to run the container directly with docker and see the topology resources were in it. I'll see if i can keep playing with it to get the container to actually run in aurora and not exist with an error.\n. ~~@congwang why was this closed? Was it fixed somewhere else or are you closing as will not fix?~~\nHere is the PR https://github.com/twitter/heron/pull/1549 @congwang you can reference the issue # in the PR and it will automatically close the issue when merged FYI and that way there is a connection between the two.\nSee https://help.github.com/articles/closing-issues-via-commit-messages/\n. @objmagic awesome thanks for the info. @congwang thanks for fixing this!\n. This commit removed the usage of the stream id and the stream component from the metrics and is just reporting them under default: https://github.com/twitter/heron/commit/557c9804af0c23952b3df7e29715f01e338884f8#diff-ee6176f97f13bdf403187bbce3dfa83bR99\n. This was fixed by this commit: https://github.com/twitter/heron/commit/c9c7ffa4a89df6d8db0f5a42ac30c0a4f65e925e#diff-3088e54faf934e2335757cfd6482089fR65. I can try tomorrow. Ill also try to work out a reproducible test case for you if I can. \n. No we have both of those still at the default. \n@congwang what would happen if the spout could produce tuples faster than the bolt could consume them and the maxSpoutPending value was high? Would the spout know to stop calling nextTuple or would the buffer in the stream manager continue to increase until it ran out of memory?\n. @congwang were you able to reproduce the issue with the topology I sent you?. Oh ok. You should be able to create a simple pom file and build it with maven. Let me see if i can put one together for you.. @congwang sorry about that. I've emailed you the same project but with a working pom.xml file now. Let me know if you have any trouble building the topology. Seems simple enough. Did you give it a try with the test topology? If you\nput together a patch I can also apply it to our internal build and try it\nout on a production topology to verify.\nAndrew Jorgensen\n@ajorgensen\nOn Wed, Jan 4, 2017 at 4:58 PM, Cong Wang notifications@github.com wrote:\n\nI guess we need the following fix:\ndiff --git a/heron/common/src/cpp/network/connection.cpp b/heron/common/src/cpp/network/connection.cpp\nindex c03ea8d..90cfbf3 100644\n--- a/heron/common/src/cpp/network/connection.cpp\n+++ b/heron/common/src/cpp/network/connection.cpp\n@@ -240,6 +240,8 @@ void Connection::handleDataWritten() {\nsp_int32 Connection::readFromEndPoint(sp_int32 fd) {\n   sp_int32 bytesRead = 0;\n+  if (mUnderBackPressure)\n+    return 0;\n   while (1) {\n     sp_int32 read_status = mIncomingPacket->Read(fd);\n     if (read_status == 0) {\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/1567#issuecomment-270498875, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAfHU-PWnkSiEEANha_Ajd622lUzf7pCks5rPBX1gaJpZM4KzOop\n.\n. I deployed one of our topologies with this change and I think we're still seeing the stream manager memory usage spike up.\n\n\nAnything I can do to dig into where its coming from?. I think it was closed because it turned out to not solve the original issue, im not sure if the commits here can still be used but likely that can.. @objmagic CPU (at least in the aurora case) is not measured in physical cores. In aurora's case resource isolation is the amount of CPU time you get per 100ms. So if you ask for 4.0 CPU you get 400ms of actual cpu time for every 100ms cycle. So you could also ask for 1.5 CPU which would mean you get 150ms of cpu time per 100ms cycle. I believe this ultimately trickles down into cgroups and how it measure cpu time, but that is why it is a double value and not something like an integer.. I don't think so. If the topology is running and you used || then the deploy would never run. If the topology is not running and you used && then the deploy would never run. The only case where && would work is if the topology is already running and the only case where || would work is in the case that that topology is not running I believe, unless I'm missing something.\nYou could hypothetically just do heron kill; heron submit and it would do the right thing and just ignore whether the kill succeeded or not but you lose the ability to tell if the whole operation succeeded or not for instance if there is an exception in the kill command. Having a redeploy or kill-if-running option feels cleaner and more intentional to me.. ldd --version\nldd (Ubuntu EGLIBC 2.19-0ubuntu6.9) 2.19\nCopyright (C) 2014 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\nWritten by Roland McGrath and Ulrich Drepper.\nDoes that answer your question?. I think it was missing symbols because I opened the core dump on another machine. Here is the backtrace from the core dump when opened in gdb on the same machine:\n```(gdb) bt\n0  0x00007f302e629c37 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56\n1  0x00007f302e62d028 in __GI_abort () at abort.c:89\n2  0x00000000004b4717 in google::DumpStackTraceAndExit () at src/utilities.cc:150\n3  0x00000000004abaca in google::LogMessage::Fail () at src/logging.cc:1478\n4  0x00000000004ad9ff in google::LogMessage::SendToLog (this=0x7fff1cf06d20) at src/logging.cc:1432\n5  0x00000000004ab6af in google::LogMessage::Flush (this=this@entry=0x7fff1cf06d20) at src/logging.cc:1301\n6  0x00000000004ae33e in google::LogMessageFatal::~LogMessageFatal (this=0x7fff1cf06d20, __in_chrg=)\nat src/logging.cc:2013\n\n7  0x000000000040e37b in heron::stmgr::StMgr::CheckTMasterLocation(EventLoop::Status) ()\n8  0x0000000000493231 in EventLoopImpl::handleTimerCallback(long, short) ()\n9  0x0000000000490f8b in EventLoopImpl::eventLoopImplTimerCallback(int, short, void*) ()\n10 0x000000000049c39c in event_process_active_single_queue (activeq=0x1cee020, base=0x1cfe000) at event.c:1350\n11 event_process_active (base=) at event.c:1420\n12 event_base_loop (base=0x1cfe000, flags=0) at event.c:1621\n13 0x0000000000409416 in main ()\n```. Here is the log file from one of the containers that was experiencing the issue. Its interesting that only 3 out of 10 were experiencing this problem.\nfilename log-files/heron-stmgr-stmgr-2.mesos-worker2204.answers.log.INFO.20170102-145608.121106\nI0102 14:56:08.113024 121106 stmgr.cpp:80] Init Stmgr\nI0102 14:56:08.218334 121106 heron-zkstatemgr.cpp:546] Setting watch on tmaster location \nI0102 14:56:08.218350 121106 zkclient.cpp:177] Checking if /heron/tmasters/topology exists\nI0102 14:56:08.218374 121106 stmgr.cpp:175] Fetching TMaster Location\nI0102 14:56:08.218380 121106 zkclient.cpp:239] Getting zknode /heron/tmasters/topology\nI0102 14:56:08.218400 121106 stmgr.cpp:187] Creating StmgrServer\nI0102 14:56:08.218592 121106 stmgr.cpp:217] Creating tuple cache \nI0102 14:56:08.218633 121106 metricsmgr-client.cpp:49] Connected to metrics manager\nI0102 14:56:08.219760 121106 metricsmgr-client.cpp:86] Successfully registered ourselves to the metricsmgr\nI0102 14:56:08.219766 121106 metricsmgr-client.cpp:95] Do not have a TMasterLocation yet\nI0102 14:56:08.221850 121108 zkclient.cpp:307] ZKClient GlobalWatcher called with type SESSION_EVENT and state CONNECTED_STATE\nI0102 14:56:08.221858 121108 zkclient.cpp:318] Got a new session id: 168992150425614812\nI0102 14:56:08.222769 121106 heron-zkstatemgr.cpp:523] Setting watch on tmaster location succeeded: no node\nE0102 14:56:08.222812 121106 heron-zkstatemgr.cpp:304] Error getting tmaster location because the tmaster does not exist\nI0102 14:56:08.222817 121106 stmgr.cpp:261] TMaster Location Fetch failed with status 2000\nI0102 14:56:08.222820 121106 stmgr.cpp:262] Retrying after 10000000 micro seconds \nI0102 14:56:13.096352 121106 stmgr-server.cpp:171] Got new connection 0x187fe00 from 127.0.0.1:45406\nI0102 14:56:13.096390 121106 stmgr-server.cpp:171] Got new connection 0x187ff80 from 127.0.0.1:45407\nI0102 14:56:13.096618 121106 stmgr-server.cpp:267] Got HandleRegisterInstanceRequest from connection 0x187ff80 and instance container_2_topology-events_12\nI0102 14:56:13.096623 121106 stmgr-server.cpp:306] New instance registered with us container_2_topology-events_12\nI0102 14:56:13.096686 121106 stmgr-server.cpp:267] Got HandleRegisterInstanceRequest from connection 0x187fe00 and instance container_2_bolt_32\nI0102 14:56:13.096691 121106 stmgr-server.cpp:306] New instance registered with us container_2_bolt_32\nI0102 14:56:13.096849 121106 stmgr-server.cpp:171] Got new connection 0x1880100 from 127.0.0.1:45409\nI0102 14:56:13.097143 121106 stmgr-server.cpp:267] Got HandleRegisterInstanceRequest from connection 0x1880100 and instance container_2_topology-ios-events_2\nI0102 14:56:13.097149 121106 stmgr-server.cpp:306] New instance registered with us container_2_topology-ios-events_2\nI0102 14:56:13.098670 121106 stmgr-server.cpp:171] Got new connection 0x1880280 from 127.0.0.1:45410\nI0102 14:56:13.098938 121106 stmgr-server.cpp:267] Got HandleRegisterInstanceRequest from connection 0x1880280 and instance container_2_topology-events_22\nI0102 14:56:13.098945 121106 stmgr-server.cpp:306] New instance registered with us container_2_topology-events_22\nI0102 14:56:13.098958 121106 stmgr.cpp:315] We haven't received tmaster location yet, so tmaster_client_ hasn't been createdOnce we get the location, it will be started\nI0102 14:56:18.223721 121106 stmgr.cpp:175] Fetching TMaster Location\nI0102 14:56:18.223739 121106 zkclient.cpp:239] Getting zknode /heron/tmasters/topology\nE0102 14:56:18.224812 121106 heron-zkstatemgr.cpp:304] Error getting tmaster location because the tmaster does not exist\nI0102 14:56:18.224817 121106 stmgr.cpp:261] TMaster Location Fetch failed with status 2000\nI0102 14:56:18.224819 121106 stmgr.cpp:262] Retrying after 10000000 micro seconds \nI0102 14:56:28.225303 121106 stmgr.cpp:175] Fetching TMaster Location\nI0102 14:56:28.225322 121106 zkclient.cpp:239] Getting zknode /heron/tmasters/topology\nE0102 14:56:28.226440 121106 heron-zkstatemgr.cpp:304] Error getting tmaster location because the tmaster does not exist\nI0102 14:56:28.226447 121106 stmgr.cpp:261] TMaster Location Fetch failed with status 2000\nI0102 14:56:28.226449 121106 stmgr.cpp:262] Retrying after 10000000 micro seconds \nI0102 14:56:38.226729 121106 stmgr.cpp:175] Fetching TMaster Location\nI0102 14:56:38.226747 121106 zkclient.cpp:239] Getting zknode /heron/tmasters/topology\nE0102 14:56:38.227967 121106 heron-zkstatemgr.cpp:304] Error getting tmaster location because the tmaster does not exist\nI0102 14:56:38.227973 121106 stmgr.cpp:261] TMaster Location Fetch failed with status 2000\nI0102 14:56:38.227975 121106 stmgr.cpp:262] Retrying after 10000000 micro seconds \nI0102 14:56:48.228669 121106 stmgr.cpp:175] Fetching TMaster Location\nI0102 14:56:48.228688 121106 zkclient.cpp:239] Getting zknode /heron/tmasters/topology\nE0102 14:56:48.229796 121106 heron-zkstatemgr.cpp:304] Error getting tmaster location because the tmaster does not exist\nI0102 14:56:48.229804 121106 stmgr.cpp:261] TMaster Location Fetch failed with status 2000\nI0102 14:56:48.229805 121106 stmgr.cpp:262] Retrying after 10000000 micro seconds \nI0102 14:56:58.230870 121106 stmgr.cpp:175] Fetching TMaster Location\nI0102 14:56:58.230887 121106 zkclient.cpp:239] Getting zknode /heron/tmasters/topology\nE0102 14:56:58.231976 121106 heron-zkstatemgr.cpp:304] Error getting tmaster location because the tmaster does not exist\nI0102 14:56:58.231984 121106 stmgr.cpp:261] TMaster Location Fetch failed with status 2000\nI0102 14:56:58.231988 121106 stmgr.cpp:262] Retrying after 10000000 micro seconds \nI0102 14:57:08.232578 121106 stmgr.cpp:175] Fetching TMaster Location\nI0102 14:57:08.232589 121106 zkclient.cpp:239] Getting zknode /heron/tmasters/topology\nE0102 14:57:08.233913 121106 heron-zkstatemgr.cpp:304] Error getting tmaster location because the tmaster does not exist\nI0102 14:57:08.233921 121106 stmgr.cpp:261] TMaster Location Fetch failed with status 2000\nI0102 14:57:08.233922 121106 stmgr.cpp:262] Retrying after 10000000 micro seconds \nI0102 14:57:18.234390 121106 stmgr.cpp:175] Fetching TMaster Location\nI0102 14:57:18.234408 121106 zkclient.cpp:239] Getting zknode /heron/tmasters/topology\nE0102 14:57:18.235612 121106 heron-zkstatemgr.cpp:304] Error getting tmaster location because the tmaster does not exist\nI0102 14:57:18.235620 121106 stmgr.cpp:261] TMaster Location Fetch failed with status 2000\nI0102 14:57:18.235622 121106 stmgr.cpp:262] Retrying after 10000000 micro seconds \nI0102 14:57:28.235913 121106 stmgr.cpp:175] Fetching TMaster Location\nI0102 14:57:28.235932 121106 zkclient.cpp:239] Getting zknode /heron/tmasters/topology\nE0102 14:57:28.237000 121106 heron-zkstatemgr.cpp:304] Error getting tmaster location because the tmaster does not exist\nI0102 14:57:28.237009 121106 stmgr.cpp:261] TMaster Location Fetch failed with status 2000\nI0102 14:57:28.237010 121106 stmgr.cpp:262] Retrying after 10000000 micro seconds \nI0102 14:57:38.237598 121106 stmgr.cpp:175] Fetching TMaster Location\nI0102 14:57:38.237618 121106 zkclient.cpp:239] Getting zknode /heron/tmasters/topology\nE0102 14:57:38.238720 121106 heron-zkstatemgr.cpp:304] Error getting tmaster location because the tmaster does not exist\nI0102 14:57:38.238726 121106 stmgr.cpp:261] TMaster Location Fetch failed with status 2000\nI0102 14:57:38.238729 121106 stmgr.cpp:262] Retrying after 10000000 micro seconds \nI0102 14:57:48.239404 121106 stmgr.cpp:175] Fetching TMaster Location\nI0102 14:57:48.239424 121106 zkclient.cpp:239] Getting zknode /heron/tmasters/topology\nE0102 14:57:48.240520 121106 heron-zkstatemgr.cpp:304] Error getting tmaster location because the tmaster does not exist\nI0102 14:57:48.240530 121106 stmgr.cpp:261] TMaster Location Fetch failed with status 2000\nI0102 14:57:48.240533 121106 stmgr.cpp:262] Retrying after 10000000 micro seconds \nI0102 14:57:58.240674 121106 stmgr.cpp:175] Fetching TMaster Location\nI0102 14:57:58.240692 121106 zkclient.cpp:239] Getting zknode /heron/tmasters/topology\nE0102 14:57:58.241816 121106 heron-zkstatemgr.cpp:304] Error getting tmaster location because the tmaster does not exist\nI0102 14:57:58.241823 121106 stmgr.cpp:261] TMaster Location Fetch failed with status 2000\nI0102 14:57:58.241827 121106 stmgr.cpp:262] Retrying after 10000000 micro seconds \nF0102 14:58:08.219342 121106 stmgr.cpp:152] Could not fetch the TMaster location in time. Exiting.\nHere is a snippet of a log from another container:\nI0102 15:12:43.855743 114846 zkclient.cpp:239] Getting zknode /heron/tmasters/topology\nE0102 15:12:43.856377 114846 heron-zkstatemgr.cpp:304] Error getting tmaster location because the tmaster does not exist\nI0102 15:12:43.856384 114846 stmgr.cpp:261] TMaster Location Fetch failed with status 2000\nI0102 15:12:43.856386 114846 stmgr.cpp:262] Retrying after 10000000 micro seconds \nW0102 15:12:43.873224 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-3 running at mesos-worker2084:31423 due to: 1\nI0102 15:12:43.873244 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:44.226778 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-5 running at mesos-worker2030:31673 due to: 1\nI0102 15:12:44.226799 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:44.227692 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-2 running at mesos-worker2030:31427 due to: 1\nI0102 15:12:44.227700 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:44.876132 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-3 running at mesos-worker2084:31423 due to: 1\nI0102 15:12:44.876155 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:45.227958 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-5 running at mesos-worker2030:31673 due to: 1\nI0102 15:12:45.227978 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:45.228839 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-2 running at mesos-worker2030:31427 due to: 1\nI0102 15:12:45.228845 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:45.877647 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-3 running at mesos-worker2084:31423 due to: 1\nI0102 15:12:45.877677 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:46.229532 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-5 running at mesos-worker2030:31673 due to: 1\nI0102 15:12:46.229552 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:46.230432 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-2 running at mesos-worker2030:31427 due to: 1\nI0102 15:12:46.230439 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:46.879498 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-3 running at mesos-worker2084:31423 due to: 1\nI0102 15:12:46.879516 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:47.231680 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-5 running at mesos-worker2030:31673 due to: 1\nI0102 15:12:47.231698 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:47.233083 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-2 running at mesos-worker2030:31427 due to: 1\nI0102 15:12:47.233088 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:47.881393 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-3 running at mesos-worker2084:31423 due to: 1\nI0102 15:12:47.881440 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:48.233335 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-5 running at mesos-worker2030:31673 due to: 1\nI0102 15:12:48.233353 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:48.234001 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-2 running at mesos-worker2030:31427 due to: 1\nI0102 15:12:48.234007 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:48.882557 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-3 running at mesos-worker2084:31423 due to: 1\nI0102 15:12:48.882612 114846 stmgr-client.cpp:107] Retrying again...\nI0102 15:12:49.215821 114846 tmaster-client.cpp:186] Sending heartbeat\nW0102 15:12:49.235723 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-5 running at mesos-worker2030:31673 due to: 1\nI0102 15:12:49.235741 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:49.235965 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-2 running at mesos-worker2030:31427 due to: 1\nI0102 15:12:49.235970 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:49.884268 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-3 running at mesos-worker2084:31423 due to: 1\nI0102 15:12:49.884287 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:50.238018 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-5 running at mesos-worker2030:31673 due to: 1\nI0102 15:12:50.238035 114846 stmgr-client.cpp:107] Retrying again...\nW0102 15:12:50.238272 114846 stmgr-client.cpp:99] Could not connect to stmgr stmgr-2 running at mesos-worker2030:31427 due to: 1\nHere is a snippet from the tmaster logs:\nI1219 23:26:33.567201 95596 physical-plan-helper.cpp:85] Topology State: 1\nI1219 23:26:33.567203 95596 stmgrstate.cpp:93] Sending a new physical plan to stmgr stmgr-1\nI1219 23:26:33.567247 95596 stmgrstate.cpp:93] Sending a new physical plan to stmgr stmgr-10\nI1219 23:26:33.567276 95596 stmgrstate.cpp:93] Sending a new physical plan to stmgr stmgr-2\nI1219 23:26:33.567303 95596 stmgrstate.cpp:93] Sending a new physical plan to stmgr stmgr-3\nI1219 23:26:33.567332 95596 stmgrstate.cpp:93] Sending a new physical plan to stmgr stmgr-4\nI1219 23:26:33.567358 95596 stmgrstate.cpp:93] Sending a new physical plan to stmgr stmgr-5\nI1219 23:26:33.567384 95596 stmgrstate.cpp:93] Sending a new physical plan to stmgr stmgr-6\nI1219 23:26:33.567411 95596 stmgrstate.cpp:93] Sending a new physical plan to stmgr stmgr-7\nI1219 23:26:33.567438 95596 stmgrstate.cpp:93] Sending a new physical plan to stmgr stmgr-8\nI1219 23:26:33.567463 95596 stmgrstate.cpp:93] Sending a new physical plan to stmgr stmgr-9\nE1225 21:56:53.524971 95596 packet.cpp:152] Remote end has done a shutdown\nI1225 21:56:53.526180 95596 tmaster.cpp:573] StMgr stmgr-4 disconnected from us\nE1225 21:56:53.531491 95596 packet.cpp:152] Remote end has done a shutdown\nW1225 21:56:53.531535 95596 tmasterserver.cpp:54] Unknown connection closed on us from 10.150.174.143:57467, possibly metrics mgr\nI1225 21:57:04.400070 95596 tmaster.cpp:359] Got a register stmgr request from stmgr-4\nI1225 21:57:04.400094 95596 tmaster.cpp:401] All stmgrs have connected with us\nI1225 21:57:04.400393 95596 zkclient.cpp:266] Setting zknode /heron/pplans/topology\nI1225 21:57:04.406215 95596 tmaster.cpp:459] Successfully wrote new assignment to state\nI1225 21:57:04.406246 95596 tmaster.cpp:480] To distribute new pplan:\nI1225 21:57:04.406250 95596 physical-plan-helper.cpp:62] Printing Physical Plan\nI1225 21:57:04.406252 95596 physical-plan-helper.cpp:63] Topology Name: topology\nI1225 21:57:04.406255 95596 physical-plan-helper.cpp:64] Topology Id: topology22da2412-7d2a-456d-ace6-54beb8854254\n. It looks like maybe the tmaster zknode expired and was not re-initialized? I am not sure why some nodes handled that better and other did not. But it looks like both the containers that had the issue and the ones that did not both detected that the tmaster zknode no longer existed. \nIts possible that the containers that were having issues crashed for some other reason and then could not re-start because the tmaster node did not exist. I do not see anything in the tmaster logs that indicates there was a problem talking to zookeeper. \nDo you know how that /heron/tmasters/ node is managed? Does it automatically go away when the connection between the tmaster and zookeeper is terminated? Is it possible that the connection was temporarily dropped due to some networking interference and the tmaster just never reconnected?. It's definitely possible and should be expected given we are operating in AWS although a prolonged outage would be unexpected. Does the tmaster gracefully re-connect if the connection is terminated momentarily for some reason? I would be surprised to see separate machine segmented from the entire zookeeper ensemble for multiple days.\nMy hypothesis is that there was a disruption between the tmaster and the zookeeper cluster that resulted in the znode being removed and never re-established but I dont know exactly how to prove that other than manually causing a network segmentation against a topology either in production or running locally.. If it reconnects does it restore the znode for the tmaster?\nOn Wed, Jan 4, 2017 at 7:34 PM Maosong Fu notifications@github.com wrote:\n\nThe tmaster will try to re-connect automatically; too many failures will\nlead to the failure of heron-executor, which will then be re-scheduled to\nanother host. So does the stream mgr.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/1653#issuecomment-270528424, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAfHU4u4MzAVN-1Eo55YO3dYWYij3MZfks5rPDp3gaJpZM4LZJLj\n.\n. Cool that sounds good. I can try to launch the topology locally and segment\nthe connection between zookeeper and the tmaster and nodes to see if I can\nreproduce what we saw. Might just be an edge case somewhere because as far\nas I know this is the only time I've seen it happen.\n\nOn Wed, Jan 4, 2017 at 8:41 PM Maosong Fu notifications@github.com wrote:\n\nThe znode of TMasterLocation is an ephemeral node so it will be cleaned\nautomatically when the connection between TMaster and zookeeper breaks. And\nwhen TMaster restarts/reconnects, it will try to write its location to the\nsame node.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/1653#issuecomment-270541008, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAfHU0c3DpUFx4i2pOC-ACgZvj4QW_3Iks5rPEpfgaJpZM4LZJLj\n.\n. We've had this happen a few more times over the last 5 days so I am going to look into this a little more. I was able to segment the tmaster from zookeeper locally and confirm that the znode was properly recreated. I'm going to keep looking to see if i can find the case where this error occurs.. I was able to find a running topology that showed this issue. I was able to confirm that the tmaster is running but there is no zknode in /heron/tmasters/<topology-name>\n\nI1116 20:06:44.640955 127856 stmgrstate.cpp:93] Sending a new physical plan to stmgr stmgr-1\nI1116 20:06:44.641252 127856 stmgrstate.cpp:93] Sending a new physical plan to stmgr stmgr-2\nI1116 20:06:44.684420 127856 metricsmgr-client.cpp:86] Successfully registered ourselves to the metricsmgr\nI1116 20:06:44.684438 127856 metricsmgr-client.cpp:92] Sending TMaster Location to metricsmgr\nI1130 21:11:31.130848 127880 zkclient.cpp:307] ZKClient GlobalWatcher called with type SESSION_EVENT and state CONNECTING_STATE\nI1130 21:11:31.130864 127880 zkclient.cpp:337] Re-connecting to the zookeeper\nI1130 21:11:31.134097 127880 zkclient.cpp:307] ZKClient GlobalWatcher called with type SESSION_EVENT and state CONNECTED_STATE\nI think I would have expected to see a line like: I0109 18:40:53.746345 63264 zkclient.cpp:203] Creating zknode /heron/tmasters/topology if the znode had actually been re-created when the tmaster reconnected. \nSo it looks like there is some edge case here where the tmaster does not properly re-create its znode. Im not sure if its in the interaction between zookeeper and the tmaster or whether there is actually a branch that doesn't re-create the node after re-initializing the connection.\nI was also able to confirm that hard restarting (kill -9) the tmaster node was able to restore the topology to a working state, here are the logs from the stream managers perspective:\nI0109 18:40:53.366755 12357 zkclient.cpp:239] Getting zknode /heron/tmasters/heron-topology\nE0109 18:40:53.368134 12357 heron-zkstatemgr.cpp:304] Error getting tmaster location because the tmaster does not exist\nI0109 18:40:53.368140 12357 stmgr.cpp:261] TMaster Location Fetch failed with status 2000\nI0109 18:40:53.368144 12357 stmgr.cpp:262] Retrying after 10000000 micro seconds \nE0109 18:40:53.621537 12357 tmaster-client.cpp:97] Could not connect to tmaster at worker2343:31089\nI0109 18:40:53.621558 12357 tmaster-client.cpp:99] Will retry again...\nI0109 18:40:53.878615 12388 zkclient.cpp:59] ZKClient CallWatcher called with type CREATED_EVENT and state CONNECTED_STATE\nI0109 18:40:53.878635 12388 zkclient.cpp:62]  for path /heron/tmasters/heron-topology\nI0109 18:40:53.878672 12357 heron-zkstatemgr.cpp:546] Setting watch on tmaster location \nI0109 18:40:53.878681 12357 zkclient.cpp:177] Checking if /heron/tmasters/heron-topology exists\nI0109 18:40:53.878697 12357 stmgr.cpp:175] Fetching TMaster Location\nI0109 18:40:53.878703 12357 zkclient.cpp:239] Getting zknode /heron/tmasters/heron-topology\nI0109 18:40:53.879802 12357 heron-zkstatemgr.cpp:523] Setting watch on tmaster location succeeded: ok\nI0109 18:40:53.879892 12357 stmgr.cpp:277] Fetched TMasterLocation to be worker2343:31089\nI0109 18:40:53.879901 12357 stmgr.cpp:288] New tmaster location same as the current one. Nothing to do here... \nI0109 18:40:53.879906 12357 metricsmgr-client.cpp:105] Sending TMaster Location to metricsmgr\nI0109 18:41:03.368957 12357 stmgr.cpp:175] Fetching TMaster Location\nI0109 18:41:03.368980 12357 zkclient.cpp:239] Getting zknode /heron/tmasters/heron-topology\nI0109 18:41:03.370076 12357 stmgr.cpp:277] Fetched TMasterLocation to be worker2343:31089\nI0109 18:41:03.370086 12357 stmgr.cpp:288] New tmaster location same as the current one. Nothing to do here... \nI0109 18:41:03.370091 12357 metricsmgr-client.cpp:105] Sending TMaster Location to metricsmgr\nI0109 18:41:03.623334 12357 tmaster-client.cpp:89] Connected to tmaster running at worker2343:31089\nI0109 18:41:03.623780 12357 tmaster-client.cpp:139] Registered successfully with Tmaster\nI0109 18:41:03.623785 12357 stmgr.cpp:332] Received a new physical plan from tmaster\nI0109 18:41:03.629701 12357 tmaster-client.cpp:174] Got a new assignment\nI0109 18:41:03.629714 12357 stmgr.cpp:332] Received a new physical plan from tmaster\nI0109 18:41:13.623984 12357 tmaster-client.cpp:186] Sending heartbeat\n. I seem to be able to reproduce this locally with the following procedure:\n\nDeploy a topology locally with zookeeper as the state manager\nStop zookeeper for 30 seconds\nRestart zookeeper\n\nYou should see the tmaster reconnect to zookeeper and then the stream manager disconnect from the tmaster. The tmaster never re-establishes its znode in zookeeper and so the stream manager cannot properly reconnect. It is interesting that during the time zookeeper is down the tmaster and stream manager appear to be able to communicate just fine, its only when the state change event comes from zookeeper after they reconnect that they decide they can no longer talk to each other. . I also found something interesting this blog post which might be causing the issue:\n\ntry {\nzk.create(path, data, CreateMode.EPHEMERAL)\n} catch {\n// The ephemeral node hasn't been deleted yet.\ne: NodeExistsException => zk.set(path, data)\n}\nSomething weird was happening with this code. I would restart the service, and when the the second case was triggered (the ephemeral node hadn't been deleted yet), after a little while the ZooKeeper node with my data would disappear, as if my session had expired!\nOf course the reason the node disappeared is obvious in hindsight: if you didn't create the ephemeral node in your session, your session doesn't own the node, so when the owning session expires, the node will be deleted. This situation becomes more common when you don't close your ZooKeeper session explicitly when you shut down your service, so you're relying on the ZooKeeper server quorum itself to expire ephemeral nodes. And if your server can restart faster than the session timeout, usually 30 seconds, then you'll run into this situation.\n\nIf when the tmaster starts up and it is simply checking if the node exists this is not enough, it actually has to re-create it because it is a different session and znodes are tied to a specific session.. I am not sure. If the problem is that the tmaster connection resets and it immediately tries to write the znode before the old one has expired then yes I think that this diff will help. I can apply this locally and give it a try.. I tried the change locally but looking at the code it looks like it should actually be doing the right thing so i'll need to look at it a bit more.\nAs another data point i've also seen this happen:\nI0112 15:14:40.304189 65510 zkclient.cpp:239] Getting zknode /heron/topologies/heron-topology\nE0112 15:14:40.304669 65510 heron-zkstatemgr.cpp:359] Error getting topology because the topology does not exist\nE0112 15:14:40.304677 65510 tmaster.cpp:224] For topology heron-topology Error getting topology Errsettingorcode is 2000\nI've seen this happen a few times where the topologies node is removed from zookeeper somehow which causes the issues with the tmaster starting up and it can go into an endless loop.. I haven't really been able to reproduce this locally, however one extra piece of information from a recent incident is that the zookeeper server seems to have noticed that the connection to zookeeper timed out but the tmaster did not:\nI0112 17:21:39.505532 50349 stmgrstate.cpp:93] Sending a new physical plan to stmgr stmgr-1\nI0112 17:22:59.248440 50365 zkclient.cpp:307] ZKClient GlobalWatcher called with type SESSION_EVENT and state CONNECTING_STATE\nI0112 17:22:59.248457 50365 zkclient.cpp:337] Re-connecting to the zookeeper\nI0112 17:23:02.334136 50365 zkclient.cpp:307] ZKClient GlobalWatcher called with type SESSION_EVENT and state CONNECTED_STATE\nE0112 22:55:32.104410 50349 packet.cpp:152] Remote end has done a shutdown\nI0112 22:55:32.104485 50349 tmaster.cpp:573] StMgr stmgr-1 disconnected from us\nE0113 02:27:28.202004 50349 packet.cpp:152] Remote end has done a shutdown\nW0113 02:27:28.202072 50349 tmasterserver.cpp:54] Unknown connection closed on us from 1.1.1.1:34466, possibly metrics mgr\nand from zookeepers perspective:\n2017-01-12 22:55:32,000 [myid:4] - INFO  [SessionTracker:ZooKeeperServer@358] - Expiring session 0x1586177286bfece, timeout of 30000ms exceeded\n2017-01-12 22:55:32,001 [myid:4] - INFO  [ProcessThread(sid:4 cport:-1)::PrepRequestProcessor@487] - Processed session termination for sessionid: 0x1586177286bfece\n2017-01-12 22:55:32,003 [myid:4] - INFO  [CommitProcessor:4:NIOServerCnxn@1008] - Closed socket connection for client /1.1.1.1:34644 which had sessionid 0x1586177286bfece\nI am not sure what happened between 17:23:02 and 22:55:32 on the tmaster but it seems like it was no longer able to communicate with zookeeper or the stream manager and then never recovered.\n. The piece of information that we need from the resources json output is the max container size for cpu, ram, and disk. Said another way, we need to know the container size in terms of cpu, memory, and disk. We use this information to determine what availability zone has enough capacity in our mesos cluster to handle the deploy. It looks like from the raw output we can just grab one of the containers in the container-list to get that information.\nOne more thing to consider is that it would be beneficial if the only piece of information that is output on stdout for that command is the json structure. That will make it easier to consume since we just have to put the output through a json parser rather than trying to picture the line output that has the data in it.\nFeature looks really awesome, thanks for pinging me about our needs and let me know if i can help out in any way.\n. That makes sense to me. I think unless we have a concrete case where someone needs a csv its not worth the time to try and get the format to work with csv/tsv. @billonahill we dont actually use ECS at all in our environment. I looked through the changes and they all look pretty self contained so I dont think there would be any negative impact to our usage of heron with mesos.. Thanks for reviewing. Yeah this was an interesting one to track down, learned a lot about python and its interaction with subprocesses and also about stderr vs stdout buffering.. @objmagic ah sorry I just copied another pex test block and didnt look too closed. I'll remove that dependency if its not actually used.\nI believe all of the other pex_test blocks in this file have this as a requirement, this test seems to work without it though.. @objmagic @kramasamy I ran the tests locally and they all pass, the failures looked to be caused by \n```\nJava HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000eab00000, 40894464, 0) failed; error='Cannot allocate memory' (errno=12)\n\nThere is insufficient memory for the Java Runtime Environment to continue.\nNative memory allocation (mmap) failed to map 40894464 bytes for committing reserved memory.\nAn error report file with more information is saved as:\n```\nLooks like the ci machines may need some more memory or is there some other recourse?. @objmagic yeah I wonder if these changes impacted the interaction in the integration tests. I'll take a look and see if something jumps out. All of the other prs look like they have passing tests so I think its somewhat safe to say this change introduced some difference in the way the integration tests work.. @objmagic it looks like the processes are no longer being properly cleaned up during the integration tests which leaves them around ultimately consuming all the memory available. I'll look into why this is happening.. Latest commit should fix the issue, going to wait for it to go through ci to confirm but I was able to see the issue and the resolution locally as well, it just turned out my machine has more memory than the ci machines do :). @kramasamy I updated the list by doing an ls on the output directory.. It looks like the same test is failing on master, given that this is a comment change it shouldn't have any impact on existing tests.. I believe the failing test here isn't related to the code I added/modified. All of the tests run and pass locally.. @kramasamy yes the intention is for everything to function as is and only in the case that a certificate and private key path are provided will an ssl connection be established between stream managers.\nAnyone running a production environment should always have their openssl version up to date to make sure it has the latest patches, bug fixes, etc. As long as the version running on the production machine is ABI compatible with 1.1.0 then this should work as intended. I believe openssl tries to be as backwards compatible as possible (https://abi-laboratory.pro/tracker/timeline/openssl/). 1.1.0 was released in 2016 which was the largest breaking change they've made and until then its been close to 100%. I think anyone running a version older than 1.1.0 will have bigger problems then not being able to run heron with ssl but assuming the methods that we're using are still in that version it should just work. \nI am working on getting this change into our production environment now so i will hopefully have some metrics to share with you soon regarding any performance impact, etc. I have tested this out extensively locally and confirmed that the data is transferred over ssl when a certificate key and path are supplied through heron_internals.yaml. This is missing a config value for the trust store, I will add that in as well.. @kramasamy yes, we just got a high volume topology running with the change yesterday. There are one or two tweaks to this I need to make and I will provide more detailed instructions. I am currently on vacation so I will not be able to provide that until mid next week but will get it to you asap.. It's not complete and it has diverged enough from our implementation that I need to cherry-pick the commits again. I figured it'd be best to open a new PR with the full set of changes but I can push them here if you'd rather.. This was my mistake.. It's an improvement I think since there is a potential for a double free bug here if packets fail to send but I haven't seen a crash in production related to this yet. I think this change makes the pointer ownership semantics cleaner and easier to reason about.. @nlu90 fixed. Is there a bazel command just run checkstyle?. @kramasamy  Oh interesting... the checkstyles never run for me during a normal build. For example if I introduce the same checkstyle issue and run\nbazel build //heron/common/src/cpp/.. \nIt builds successfully. Its not until I run the scripts/packages directive that the checkstyle actually fires. Or am I misunderstanding and there are specific checkstyle targets that can be run?\n. You missed this line bazel-bin/release\n. That's a bit difficult. Docker cannot add files that are not under the folder that the Dockerfile is in so it would not be able to access ../tools/docker/bazel.rc. ~~I can try to symlink the file in before build, I think that would work.~~ I'll have to copy it in and then remove it.\n. Extra space at the top of this file.\n. Agree, these aurora files will likely need to be somewhat specific to the person deploying. We are planning on migrating to sacker  at some point but there is currently no way to plug in different package managers into aurora. Maybe just provide a sample aurora file and then people can customize it to their needs.\n. Interesting, good to know I will fix that. In this case the performance impact is likely negligible though correct because this isn't really a hot looped code. Just wanted to make sure I understood fully.\n. added\n. Strange i dont see that spacing locally. Let me see whats up.\n. Should be fixed now\n. Do we want to pull this out into another script like compile-docker.sh You can then re-use the export of the env vars and collapse these if statements down to 1 statement each \nif [ $PLATFORM = \"darwin\" ]; then\n  docker/compile-darwin.sh $HERON_VERSION $SCRATCH_DIR $SOURCE_TARBALL $OUTPUT_DIRECTORY \nelse\n  docker/compile-docker.sh $HERON_VERSION $SCRATCH_DIR $SOURCE_TARBALL $OUTPUT_DIRECTORY \nfi\nThe other plus side of that is this script is then just a wrapper/dispatch/environment setup and then the actual logic for doing the compiling is co-located in another place.\n. I'm fine with not coupling these two scripts. They look the same but are explaining two separate concepts (compiling on darwin vs in docker).\n. Any reason not to do LOG.log(Level.FINE,...?\n. You can ignore a particular command using particular_script || true I would vote for making exceptions for specific commands rather than remove the protection that set -e provides.\n. Another thing i like to use on shell scripts is set -u which will cause the script to error if you try and use any unset variables. This is nice because it helps avoid the bug where you use a variable that hasn't been set and its just empty. Not sure if that makes sense here but might be worth adding as well. This article highlights some of the benefits of using flags to help prevent bugs in bash scripts (http://redsymbol.net/articles/unofficial-bash-strict-mode).\nNot saying we should add all of those just saying its a good resource and can help with some bugs that can be a bit nasty to track down.\n. I believe that because its an || status will be the result of $cmd in the success case and true in the failure case. I believe this keeps the same functionality as before where in the success case it will be == 0 and in the failure case it will not.\nFor example this will print Not Clean if the command fails\n``` bash\n!/usr/bin/env bash\nset -e\ncmd=\"cat /does/not/exist\"\nstatus=$($cmd || true)\nif [[ ${status} == 0 ]]; then\n  tree_status=\"Clean\"\nelse\n  tree_status=\"Not Clean\"\nfi\necho \"tree_status: $tree_status\"\n```\nThis would print Clean given that the git command succeeds. \n``` bash\n!/usr/bin/env bash\nset -e\ncmd=\"git diff-index --quiet HEAD --\"\nstatus=$($cmd || true)\necho $?\nif [[ $? == 0 ]]; then\n  tree_status=\"Clean\"\nelse\n  tree_status=\"Not Clean\"\nfi\necho \"tree_status: $tree_status\"\n```\n@kramasamy I actually think that status var isnt what you want. That will be the output of the git command which afaik will never equal zero. If you want to check the status code of the command to make sure it succeeded then you can check if $? == 0 as i did in the second example above. $? is the status code of the previously run command\nHere is another alternative which may actually get you what you want:\n``` bash\n!/usr/bin/env bash\nset -e\nfail_command=\"cat /does/not/exist\"\nsuccessful_command=\"git diff-index --quiet HEAD --\"\nset +e\n$($fail_command)\nfail_status=$?\nset -e\nset +e\n$($successful_command)\nsuccess_status=$?\nset -e\necho $fail_status\necho $success_status\nif [[ ${success_status} == 0 ]]; then\n  tree_status=\"Clean\"\nelse\n  tree_status=\"Not Clean\"\nfi\necho \"tree_status: $tree_status\"\n```\n``` console\n:!bash foo.sh\ncat: /does/not/exist: No such file or directory\nfail status: 1\nsuccess status: 0\ntree_status: Clean\n``\n. I was curious about that as well. My thought was that the heron-ui will always be listening on localhost (127.0.0.1), I dont see any params to change that, just the port and tracker url. Hostname would work but technically its just listening to all traffic for port (default 8889) on localhost I believe. \n. That's true, I think theres a different between listening address and the address where you can reach it at. We know the listening address, the address where it is reachable is different. I suppose the ip address of the host would probably be most appropriate.\n. What if instead of random you use timestamp or some other temporal value? That way my would be easier to tell which artifact is the newest. Curios your thoughts on that. \n. My thought is that this would live along side the api and storm jars and can be added as dependency the same way as those two. I think generally anything that coding against the spi interfaces will be expected to be run from the heron-cli or something like that right?\n. Ah sorry I misunderstood. I can look into adding it there as well.\n. Would the wordbatchin here help clarify at all?heron.instance.set.data.tuple.batch.size.bytesfor instance? Or doesdata tupleinherently mean that its a batch of tuples generated from the user code.\n. If this value does not exist in heron_internal.yaml for example if someone has bundled an older version of heron_internals.yaml with their topology for overriding purposes what happens? It looks to me like this code will throw a null pointer exception in the event that the value does not exist in heron_internals right?\n. Should this be>=`\n. Wont these always get overwritten with the config values in the constructor? Why set them here to such a large value?\n. awesome, thanks for the clarification!\n. Sounds awesome!\n. Do you mind sorting these alphabetically, it helps avoid accidentally adding a dependency twice. \nhttps://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/\n. Extra space intentional? \n. Extra space intentional?\n. No thats fine, if thats how it is in other scripts then lets keep it consistent.\n. If you cd into the docker directory then this command will not work should be ./start-docker in your example\n. This ip address will likely not work for anyone else. Is there a better way to link these two containers together without specifying a static ip address?\nFor example, I am using Docker for mac which runs docker outside of a virtualization so I can access the tracker from http://localhost:8888. If there's no out of the box solution for more easily describing the relationship between the two containers then can you put a note in the README that the user will likely need to change the tracker_url to what matches up with their system\n. FFD is a pretty esoteric term. What if we expanded it to FirstFitDecreasing? I feel like expanding out the name would make it more discoverable.\n. So this calculation is not generalizable to other schedulers correct? For example Aurora only has 1 extra container, I am not familiar with YARN but it looks like in this case there are 2 extra containers correct?.\n. I've been thinking about this a little more recently and would be curious what your thoughts are about generic schedulers for all platforms and whether or not we should start splitting some of the implementations up based on scheduling technologies (yarn, aurora, mesos, etc). I think that trying to generalize the requirements of all of those platforms into one algorithm might actually be making this more complex than it needs to be and certainly some schedulers will have different trade-offs and be able to make certain optimizations based on how they work. I don't think we need to do that in this pass but I think its worth thinking about how that would work and whether the level effort/complexity that splitting them out into separate scheduler implementations might introduce. @kramasamy I'd be curious of your thoughts as well.\n. I think it is, if the execution state doesn't exist we cant properly validate the runtime so my proposal is that we default to true. If we return false the command will fail and we'd be in the same situation where the command will fail to run (see: https://github.com/twitter/heron/blob/master/heron/scheduler-core/src/java/com/twitter/heron/scheduler/RuntimeManagerMain.java#L309).\nThe manifestation of the bug is that the execution state got deleted from zookeeper, since that happens we cannot really do this validation step. Do you know what this validation was meant to protect against? It seems to be that its meant to make it so you cannot deploy a topology twice to two different cluster, role, environments correct? The layout of zookeeper already precludes the topology from being deployed twice anyway, i guess this would protect against accidentally deploying over/killing someone else's topology but as I mentioned if the execution state has been deleted then we cannot validate that requirement.\n. No I think you're right, schedulerLocationExists reads better. I'll change all of these to be that form.\n. I see what you're saying now. So the idea would be that if for some reason the execution state was gone then some commands (restart, active, deactivate) would just simply fail and would require the topology to be killed and then submitted it order to use those commands?\nA larger change that could be made would be to heron itself write the execution state. The topology master could write the execution state as an ephemeral node in zookeeper so that its always there if the topology is running. This would fix the edge case around deleting that piece of state from zookeeper and might simplify things a bit.\n. Yeah. It doesn't feel great to tell people they have to restart their topology in order to do some commands. In the end its probably not the end of the world but feels like there might be a different change to automatically recover from this scenario. The nice part about possibly having the tmaster manager that state is that it can be restored because its encoded in the arguments or in a protobuf object somewhere that could be re-used to automatically recover and restore the missing state.\nThat being said I think we can make an incremental improvement here and think more about some sort of recovery mechanism.\n. It's all state.\n. The other way I could do this is surround these all with try/catch blocks and specifically catch exceptions. I dont know if there is a \"safe\" version of the zookeeper interaction where if the node does not exist it doesn't throw an exception but I can look. In the worst case I think this will increase the number of zookeeper operations on kill from 5 to 10 which didnt feel too impactful to me. \nIf you'd rather we could just \"fix\" the curator case and not throw an exception here: https://github.com/twitter/heron/blob/master/heron/statemgrs/src/java/com/twitter/heron/statemgr/zookeeper/curator/CuratorStateManager.java#L213-L215 and then remove the explicit checks for if the path exists in the delete case.\nThe difference here is where the decision for the success of a delete is made, whether by the delegate or the SchedulerStateMangerAdaptor. It might be cleaner to push it down to the delegate to make that decision but might lead to inconsistencies between IStateManager implementations.\nEDIT: I think just fixing the curator implementation to not raise an exception when the node does not exist will be the easiest thing to do and then we can revert the check if the path exists before calling delete. Does that satisfy your comment? This will mean that it is possible for there to be a difference between IStateManager implementations on how they handle this failure.\n. Sorry is vpu vores a typo that should say cpu cores?\n. Extra new lines.\n. Same typo here it looks like\n. extra new lines\n. So this will say that two ram requirements for different components will be equal correct. IE new RamRequirement(\"foo\", 10).equals(new RamRequirement(\"bar\", 10)) would return true. The hashCode on the other hand takes the component name into account. So while two of these would be equal they would not hash to the same value.\nJust want to make sure this is intentional\n. This might have been a poor merge conflict resolution, good catch.\n. I agree. This will actually still fail if the scheduler state is missing which isn't great but that seemed to be a bit more to unravel but I can see if theres a way to succeed in that case as well. \n. It is deleted a few lines below this (https://github.com/twitter/heron/pull/1128/files#diff-6564229e92c7d6359355807e020d0972R237)\n. I like that. I didn't see that new data structure, let me refactor this based on your suggestions. \n. It might also be easier if there was a deleteState method on IStateManager that takes this enum. All of the state lives in a single location so there really isn't a huge need to define all of the different delete methods for the different locations deleteTMasterState, deleteSchedulerLocation, etc.\nWhat are you thoughts on that instead of having this big switch statement?\n. My thought was that we could remove the need for the switch by just\ndeleting based on the 'dir' property on enum. Let's just table that idea\nfor now.\n\nAndrew Jorgensen\n@ajorgensen\n. Purely nit picky organization thing but can you keep HERON_UPLOADER_S3_ACCESS_KEY and HERON_UPLOADER_S3_SECRET_KEY grouped together instead of putting keys between then?\n. (nit): Extra newline here is not needed.\n. You are missing a , at the end of this line.\n. ~~I believe this is supposed to just be container = Docker(image = '{{heron_topology_image_uri}}') according to the docs here: http://aurora.apache.org/documentation/latest/features/containers/~~\n. Sorry  it looks like the docs differ in a few places on this one. I tried this: http://livewyer.com/blog/2015/04/13/deploying-docker-containers-using-apache-aurora and it seemed to parse correctly.\nI think you do need to interpolate the {{heron_topology_image_uri}} though instead of just having it be a string.\n. Ok. Coincidentally we dont have docker turned on in our aurora cluster and the aurora vagrant setup seems to not be working for me so im trying to figure out a way i can test this change out. With the missing comma I got up to the point where aurora is telling me that docker is not enabled. Going to try the local vagrant route again and see if i can verify this all works.\nI see what you mean about the interpolation. I believe you're correct given that its already interpolated it should be able to just be used as a normal variable.\n. The base here actually has to be one with python installed in it. Aurora does some crazy stuff to get a aurora executor running inside the docker container so it can monitor its process and do health checking and things. The default ubuntu container does not have python installed so you'll need to set python:2.7 or something similar as the base container. Also adding a comment to that effect would be good.\n. The container attempted to run yes but overall it did not work. I still need to do a bit more tweaking I think to get it to get the heron binaries into the running container but working on it. \nHow are you transporting the heron binaries into the container?\n. Do you mean the sandbox in aurora? AFAIK you dont have a mount point inside the docker container into the aurora sandbox and any command you setup in aurora will be run into the docker container itself which means that you need to be able to get to the binaries inside the docker container.\n. yeah I think building out the full docker support wouldn't be too much more work from where this PR is now and would actually simplify how all this works. \n@chris-pardy if you dont have time to pick that up I can see if i can find some extra time to do that but I think that combined with what you have here for dynamically generating the container with the topology artifacts would be good. I'll manually generate a container with the heron binaries in it and use that as the base to test that out. If we were able to generate that and push it up to the public docker repository so its just available for folks that would be awesome.\n. This is meant to be machine readable, all of the other formats are human readable. As such I think the raw byte amount would be easier for a machine to parse than having to know the format of toString() and parse accordingly.. Can you be more specific? It looks sorted to me.. I've never used ObjectWriter but from what I can tell the benefit is it has a formatted output via mapper.withDefaultPrettyPrinter(). As i mentioned in another comment I want this output to be machine readable. If someone wants the output to be printed nicely it would be as simple as \nheron submit --dry-run --dry-run-format=json ... | python -m json.tool\nSorry I didnt make the machine readability of this output clear in the description.\nIn terms of re-usability I'm not sure it ever needs to be re-used but it should be safe to call the render method multiple times and get the correct result and I believe that ObjectMapper is thread safe.. No worries! Thought i was going crazy :smile: . FWIW here's a comment on StackOverflow regarding re-usability.\n\nIt's fine to use a single instance per application provided you don't call any configuration methods after it's been made visible i.e. you should do all your initialization inside a static block.\nhttps://stackoverflow.com/questions/18611565/how-do-i-correctly-reuse-jackson-objectmapper\n\nGiven that the documentation mentions that it's thread-safe and I am not storing any of the intermediate steps in instance variables I think this should be safe from both a re-usability standpoint and thread-safety standpoint as written but I can defer to someone who has more jackson experience to weigh in as well.\n. Great suggestion, thanks!. ",
    "ativilambit": "This link doesnt work for me\n. Is this a working version? I would like to do a test run with it in our environment\n. ",
    "objmagic": "749 should fix this\n. fixed via https://github.com/twitter/heron/pull/1493. i believe this issue was resolved long time ago. closing.... heron-explorer can do this. closing this issue.. Bumping this. We often have customers complaining metrics on UI do not show up. Most of the times it is because their topology is too large.. @billonahill \nYou are right. UI <-> Tracker also has design problem/performance issue.. thanks guys!\nclose via #738, #762, and #760\n. Some errors are fixed. More errors arise because there is no BUILD file under heron/contrib. Some Java files cannot even be built. \nFor example:\n./../contrib/kafka/src/java/com/twitter/heron/storage/StormMetadataStore.java:30: \nerror: class MetadataStore is public, should be declared in a file named \nMetadataStore.java\n  2 public abstract class MetadataStore { \n                          ^^^^\n. Yes but the patch only fixes some errors. Running javadocs.sh still give me errors. I would like to wait #638 is closed, while I'll fix some more errors not caused by building.\n. removing heron/contrib is necessary\n. relevant: #716 \n. closed via #716 \n. one can turn on set -e since javadoc.sh exits with 0 now. This means Travis CI test won't fail on make site\n. close via #739 \n. this issue can be resolved now since 0.14.0 is released.\n. might be related:\n\nApple fully supports static libraries; if you want to create one, just start with the appropriate Xcode project or target template.\nApple does not support statically linked binaries on Mac OS X. A statically linked binary assumes binary compatibility at the kernel system call interface, and we do not make any guarantees on that front. Rather, we strive to ensure binary compatibility in each dynamically linked system library and framework.\n\nfrom https://developer.apple.com/library/mac/qa/qa1118/_index.html\n. fixed via #1090 \n. should be resolved via https://github.com/twitter/heron/pull/1564. @moomou feel free to go ahead! You can always discuss with us on mailing list, join Heron user Slack group, or continue on this GitHub issue.\n. closed via https://github.com/twitter/heron/commit/7e05847b9039fb4e37c666dffbf9f6e56ff443eb\n. would be good to add BUILD files to build files under heron/contrib. This is necessary for #370.\n. I think this can be closed now\n. I want to know what benefits you could have by having native Scala topology support, since Scala interoperates with Java well?\n. any idea on this PR?\n. removed --trace-execution option temporarily\n. a small example can be found here\n. close this since #690 is merged?\n. bad links should be gone after #734, #738, #739 \n. btw, maybe remove Vikas?\n. merged. close #370 \n. @lucperkins seems that javadocs.sh in this commit didn't use my commited version on master. Is Javadocs problem gone now? Do warnings still exist?\n. @lucperkins thanks, but it would be good if we can also figure out how to remove warnings emitted by javadocs.sh\n. Syntax highlighting works fine now, and this PR lgtm. We can merge it now.\n. see PR #734 \n. make serve makes no difference. \n. close since I am dumb and don't RTFM\n. @lucperkins only syntax highlighting isn't working. code block is displayed as plain text\n. One needs to do\nBash\n$ pip install Pygments\nto see syntax highlighting when developing locally.\nThanks @cckellogg and we can close this issue now.\n. Looks good to me\n. @lucperkins I merged #732 into master. Can you rebase this PR against master so that we can merge it?\n. so maybe \"Local simulation\" or just \"Simulation\"?\n. @lucperkins \nHugo Static Site Generator v0.15 BuildDate: 2015-11-25T22:29:07-08:00\n. @lucperkins thanks. I remember I did this and there was no such error last week. I installed again today, and the error is gone...\n. btw, numbering under Patch Acceptance Process on page heron/docs/contributors/community/ is wrong\n. close via #745 \n. Thanks! Merged\n. \ud83d\udc4d\n. LGTM.\n. @maosongfu @kramasamy this PR was closed rather than merged\n. A PR was just merged. Would you mind rebase against mater?\n. thanks for linking javadocs to make them accessible. This should fix #749, cc @nlu90 \n. any idea why CI failed?\n. I added set -e in commit https://github.com/twitter/heron/commit/7fb03ebc352cacfaac0ccaaae6006fccf0f3b449 and the CI worked fine. However, I failed to realize the error happens nondeterministically. The actual problem may be related to parallel building of protobuf Java files.\nThis means some PRs could be blocked by this problem. An immediate fix could be to remove set -e again. \n. I believe this issue was fixed via #968. Feel free to close this.\n. close. #797 and #841 will fix this.\n. +1\n. Should we close since @taishi8117 is working on similar task, which does not use the code in this PR?\n. This looks good to me. I think we should merge it.\n. looks beautiful! Thanks Luc!\n. cf #798 \n. resolved via https://github.com/twitter/heron/pull/943\n. @billonahill this is the temporary solution I said. what do you think?\n. when you are on Heron introduction page and follow link leading you to API's javadoc, you'll get 404. So yes, the only way is to open website/public/api/index.html and manually find the doc of class you want to read. This is not convenient for people developing website locally.\n. Also, I can go from Heron page to Java doc API page if static/api exists. However, when I rm -rf static/api, and make serve again, I can still go to Java doc API page from Heron page. I can only imagine there is some caching mechanism going on inside hugo\n. @lucperkins I pulled and tested your branch. I can still open the symlinked javadoc API page using hugo (I can open http://localhost:1313/heron/api). Hugo showed same errors though.\nI am not sure if this is the case for other people. Either symlinking or cp -rf is fine to me, as long as one can go to API page from Heron page when developing on local machine.\n. close via #943 \n. can you rebase it against master? If the link checker in master does not complain, I think we can merge this.\n. well, actually we should merge with master...\n. this PR becomes quite messy. I tried to clean things up/pick out actual commits, but I failed. Not sure what to do here now...\n@lucperkins shall we open a new PR?\n. @lucperkins \nSounds good! You can PR and I can include it in #1265 \nThanks a lot!\n. \ud83d\udc4d\n. related to #1146 \n@joestein do you still want to work on this?\n. Thanks to @nlu90, we can close this via https://github.com/twitter/heron/pull/1317\n. +1\n. hi @samek, Mesos scheduler is still under development. See #803 \n. @billonahill I got it, and I'll fix it soon.\n. +1\n. @kramasamy \nas for your concern in this comment, version number in config.yaml is hardcoded currently. As far as I know, YAML variables are set statically. Of course, one can write a script to generate config.yaml dynamically from template to set heron to the output of git describe --tags --abbrev=0\n. This PR is better to be merged after #815 \n. cf #827 \n. close because of #827 \n. @kramasamy @lucperkins \nI cannot click the sidebar. I'm not sure if this is the same for others: https://objmagic.ml/heron/docs/getting-started/\n. This PR looks very promising. It solves the problem asked by @kramasamy. \nThanks!\n. If I understand this PR correctly, one can put arbitrary version number in config.yaml and CI will correct it. However, note that CI is not able to commit the corrected config.yaml. This means version number field is unnecessary and could be omitted.\n. @lucperkins  Nice. It works fine for me now.\n. Thanks. Built and published!\n. This is due to OS version issue. We will try to use low OS version to build releasing binary.. https://www.whatsmydns.net/#A/maven.twttr.com \nDNS resolution of maven.twttr.com is poisoned in mainland China. IP address location is either in Italy or in Azerbaijan.\nIf anyone encounters this issue, you are advised to use proxy or VPN. \nOr you can try adding the following entry into /etc/hosts (I don't guarantee it will work).\n199.16.156.245 maven.twttr.com. fixed via #928 \n. looks good to me. I see that config file specifies two-space indent, which is good. Can you make sure that all the other configs also conform to Twitter's Python coding style here?\n. @alanngai sorry for the confusion. I mean checkstyle.ini.\n. @alanngai there is some confusion going on. I mean it is good that you followed Twitter's Python coding rule and enforced indentation level to 2 at here: https://github.com/OpsClarity/heron-1/blob/078dbad4f627079f8369749990c58937cddbddeb/tools/python/checkstyle.ini#L266 . I was not talking about checkstyle.ini itself.\nsorry about the confusion...\n. @alanngai no. just want to make sure your config file has correct config options, according to Twitter's Python style\n. @alanngai sorry. updated.\n. @alanngai \nOur goal after merging this PR is to enable Python style checker. Checkout #587 and you can see currently we disable Python style checker in CI.\nYou can refer to #587 and run \nbazel build --config=darwin --experimental_action_listener=tools/python:compile_python heron/cli/src/python/...\nWe want this command have exit status 0.\n. @kramasamy @alanngai @cckellogg @nlu90 \nAs for 1), we are actually talking about two errors:\n1. https://github.com/twitter/heron/pull/849#issuecomment-230960262, which is fixed by @nlu90 \n2. https://github.com/twitter/heron/pull/849#issuecomment-230960639, which we had no idea in this morning. \nHowever, I just realized I have seen the same error message before. I checked with @nlu90 this afternoon and we believe that this error happens because metrics info is refreshed every one minute, and if you fire up heron-tracker and heron-ui immediately after you submit the topology, this exception will show up. If you refresh the UI after one minute you open the heron-ui/heron-tracker, you will not see any error message.\nAnother reason that makes me more confident in my conclusion is that @alanngai 's change on heron.py is purely syntax level change and involves no change on the semantic of the program. If we really want to consider the above scenario in the above paragraph an issue, we can merge this issue first, and I can submit a new PR to make sure no exception will be thrown even if there is no metrics information.\n. @kramasamy and yes, I can look into 2) after we merge this.\n. @alanngai @kramasamy \nI think we can go ahead and merge this PR now.\nI will keep track of related issues after merging this PR. Namely:\n1. fix https://github.com/twitter/heron/pull/849#issuecomment-231932982\n2. fix https://github.com/twitter/heron/pull/849#issuecomment-231834711 (\"errors that I don't have enough context to fix (labelled with FIXME)\")\n3. #578 #579 #580 #581 #582 #583 #584\n. Close this since we already fixed all the Python style errors after Pylint PR (#849 )\n. http://lucperkins.github.io/heron/docs/install.html\nthis still gives me 404 now\n. \ud83d\udc4d\n. See Bill's suggestion about how to improve local integration tests\nhttps://github.com/twitter/heron/pull/1296#issuecomment-241902050\n. prioritize this. At least comprehensive unit test for CLI is critical.. cli unit test is done.. Please fix ({{< ref \"#stream-manager\" >}}) before merging this.\n. @kramasamy this should be good to go\n. I tested and ran the UI. There is no problem on UI. I also used grep to verify that all usage of instances of Graph and TopologyDAG are irrelevant to this change (it is easy to verify since there are very few occurrences of them)\n. What's our plan on this issue? It seems that no one is working on it even though it's of high priority... (I am not focusing on this right now)\n. any update on this? should this be included in 0.14.5?\n. cc @congwang who could help look into this. moving to #1631 and close this.. Thanks, @lucperkins! I updated script so that wget will be installed if there is no wget. \nI'll also deliberately make some errors in Java documentation and break some links to see if script can catch it during CI\n. @billonahill The CI passed. See: https://travis-ci.org/twitter/heron/builds/138415101\nI will proceed to add comments to javadocs.sh and break some docs and links to see if CI could catch it. \n. CI \"successfully\" exited after I introduced documentation syntax error. See CI log. I will try breaking links next.\n. One downside of this PR is that there is no good way to limit the depth of checking if we want to check existence of external webpages.\nSay we limit depth to 2 to prevent wget from getting sucked into infinite levels down when checking external webpages. This means that for local webpages, we also search only two levels down, but I believe if we want to do an exhaustive checking on local webpages, the actual depth has to be much larger than 2.\n. Yes, currently external checking option is off. Also, after some study, I am pretty sure we cannot only limit depth for external checking. \n. CI script successfully displayed non-existing links and exited with 1. See the end of CI log.\n. If everyone is happy with this PR, I'll reset head of this branch to https://github.com/twitter/heron/pull/943/commits/ad1bfce5449613b2b28d5408e125d089a0d47305 so that we can merge.\n. CI tests done, head reset to ad1bfce, ready for merge.\n. internal people said it would be good to have this feature. cc @kramasamy \n. need more time to investigate why adding comments can make CI fail...\n. I simply restarted CI and CI now passed. I noted some difference between the past two CI logs. The first CI log displays some message saying \"implausibly old time stamp 1970-01-01 08:00:00\", but there is no such message in the second CI log. Maybe it's related to #910.\n. Is Apache 2.0 the correct license to use? Also note that other scripts under script/travis do not have any license in header.\n. this may come as a surprise, but we require Python indentation level to be 2. It would be good if you are willing to adopt our coding style.\n. Nice work \ud83d\udc4d \nbut I would like to run the CI a few more times to make sure the problem is indeed gone.\n. Feel free to merge this\n. Thanks. I'll fix all the problems, and submit two PRs separately.\n. close this because we are going to split this PR\n. Just pushed a commit that extends /topologies API JSON result with role. This API change will not break any other parts of the code.\n. all problems fixed. Please review again so that we can merge.\n. Please accept CLA first: https://engineering.twitter.com/opensource/cla\n. By \"repo name\", do you mean the name which is displayed here?\n\nIf so, I don't think it is possible... \n(well, of course, you can use curl to get the source of forked GitHub project page and find the string \"ashvina/heron-streaming\", and then blah blah blah. However, this sounds absurd to me...)\n. Thanks for pointing it out. I didn't mean this problem is absurd. I mean my half-joking solution is absurd.\nI think changing forked repo name is a rare scenario, and it is not necessary to fix your issue. If you bother to change forked repo name, you can also change the baseurl field easily.\n. I believe this is a problem related to publishing GitHub page. The GitHub page has to be published under twitter.github.io/heron (team_url/project_name). This means baseurl has to be heron for us. If you have modified repo name on GitHub and want to publish your own webpage on GitHub page, you may need to modify baseurl field.\nmaybe @lucperkins can have a better answer...\n. - fixed lots of bugs, styles, etc...\n- added tests using Mock\n- added cluster command\n- rebased against branch tracker-with-role\n. rebased against https://github.com/twitter/heron/pull/979 again\n. fix bugs, add version subcommand, add various tiny improvements, and re-rebased against master.\n. change committed this morning is lost when rebasing..\n. let me see..\n. curl http://localhost:1313/heron/docs/developers/compiling/ returns the following with status code 200:\n<pre>\n</pre>\nThis means wget will think the link to this page is not broken. I am not sure why curl gives me the above result, while curl -I http://twitter.github.io/heron/docs/developers/compiling/ gives me HTTP/1.1 404 Not Found\n\ncc @lucperkins,\nwould you like to comment on this issue?\n. remove semicolon: \nhttps://github.com/twitter/heron/blob/master/heron/cli/src/python/utils.py#L170\n. close via #1006 \n. @cckellogg can you have a look at this PR?\n. @severun The person outside Twitter (@caofangkun) who volunteered to work on this is not active, and internally we don't have people working on it.\n. @billonahill \nI patched the script. See: https://github.com/twitter/heron/pull/1029/commits/fd832aca4bd61183a5c7b57e35c1c25bf2a9ed09\n@kramasamy \nI updated the website design as you suggested. Please see: https://objmagic.ml/heron/\n. Thanks for the fix!!! I fixed this in a commit of PR for heron-explorer (https://github.com/twitter/heron/pull/989/commits/db9b88a71d64a391e9b417312e61f04f2f291f70) but forgot to patch it back on tracker-with-role branch.\n. customizable default value for tracker_url\n. ok.\n. @caofangkun \nHi, thanks for the review. \n\ndoes release/release-process.md should also change 0.14.0 to 0.14.1?\n\nI don't think we need to because we are talking about an example in release/release-process.md, which is independent from the version number we are using.\n\nand should we provide update-version.sh {version_number} shell script to update version?\n\nsee https://github.com/twitter/heron/pull/827\n. @kramasamy @caofangkun \nI tested out and this looks fine. Please remove the duplicate shebang and I will merge this.\n. @kramasamy yes\n. @kramasamy @billonahill \nInternal CI passed. I can also build and run on Docker following steps here.\nI'll go ahead to merge it if everyone think it's fine.\n. @benley do you want to have a look at this PR?\n. @benley I applied your patch: https://github.com/twitter/heron/pull/1052/commits/37f9d7498c535dc13c0fec5e8b3c19c494ea1ce6 and I can build fine.\n. Travis CI, internal CI, Docker build all passed, again.\n. I reviewed this PR yesterday. It looks good to me.\n. Thx! Please sign the CLA first.\n. I checked out your branch and ran\nbazel build --config=darwin --experimental_action_listener=tools/java:compile_java heron/packing/...\nI can see 9 style errors.\n. @ashvina are you the same guy as @avflor? and who is @avrilia? what is going on here?\n. @avflor please fix two Java Docs error here. You should use &lt; and &gt;.\n. @atibon what do you mean by \"not right\"? Could you please tell us which particular part of metrics is incorrect in the table above?\n. interesting. cc/ @cckellogg . Could you please have a look at this problem?\n. @atibon I don't think you need to, and I understand it is not realistic for you to collect all the information under hundreds of sandbox dirs. If there is really something wrong with the metrics you see on Heron UI, I'd say it's our mistake to have heron-ui display incorrect information. That being said, it is not necessary for you to dig into metrics.json.*, if you think our heron-ui fails to present you correct information.\nLet's keep this issue open to keep track of what is going on with the Heron UI for you :)\n. LGTM\n. cf. #587 \n. small issue: https://github.com/twitter/heron/pull/1081#discussion_r70904671\n. now I think it is not necessary to make this change: https://github.com/twitter/heron/pull/1081#discussion_r70904671\n. #578 #579 #580 #581 #582 #583 #584 #1091 have been all resolved.\nwaiting #1096 CI to finish and I'll PR to enable Python style checking tomorrow morning.\n. forgot to fix styles under /test\n. also fixed style under tests via https://github.com/twitter/heron/pull/1085/commits/5de04def12a67059c9f384986ee756cd13879672\n. this is blocking #581 and #1072 \n. completed. close now.\n. #1204 has just been merged. Please first merge with master and change to use log module under heron/common/src/python/utils. You can refer to heron-explorer to see how to config and use this log module.\n. This PR is quite trivial and I talked with @kramasamy about this on phone. CI passed so I'll go ahead and merge this.\n. CI spotted some error and I will fix it later.\n. @yanxz Hi, Python style checking was just enabled yesterday and I am sorry I forgot fix Python coding style under integration_test.\nwill do it now.\n. Thanks for the issue. A PR has been submitted to fix this issue: https://github.com/twitter/heron/pull/1100\n. Hi, @yanxz \n1100 is merged. Could you please pull the master branch, do bazel clean, and try again?\n. fixed via #1142 \n. FYI, it works on my machine..\n. @billonahill is this still happening? if not, we can close this.. @billonahill I see. I was asking because I see some of your PRs failed first CI pass due to style errors. It seems that this never happened to me.. @caofangkun \nThanks a lot! @nlu90 and I will review this tomorrow.\n. I'm reviewing the patch. It seems that theses code are almost the same as in storm. I noticed you removed handleMetrics from ShellBolt.java. What's the purpose? Instead of a PR with no context or explanation provided, could you please list all the differences so that we can have a better review of this patch?\n. I see the following error message when compiling\nERROR: /Users/rl/heron/heron/storm/src/java/BUILD:19:1: Java compilation in rule '//heron/storm/src/java:storm-compatibility-unshaded' failed: java failed: error executing command external/local_jdk/bin/java -Xbootclasspath/p:external/bazel_tools/third_party/java/jdk/langtools/javac.jar -client -jar external/bazel_tools/tools/jdk/JavaBuilder_deploy.jar ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nheron/storm/src/java/org/apache/storm/multilang/ISerializer.java:53: warning: [rawtypes] found raw type: Map\n  Number connect(Map conf, TopologyContext context)\n                 ^\n  missing type arguments for generic class Map<K,V>\n  where K,V are type-variables:\n    K extends Object declared in interface Map\n    V extends Object declared in interface Map\nerror: warnings found and -Werror specified\nheron/storm/src/java/org/apache/storm/multilang/NoOutputException.java:25: warning: [serial] serializable class NoOutputException has no definition of serialVersionUID\npublic class NoOutputException extends Exception {\n       ^\nheron/storm/src/java/org/apache/storm/multilang/JsonSerializer.java:62: warning: [rawtypes] found raw type: Map\n  public Number connect(Map conf, TopologyContext context)\n                        ^\n  missing type arguments for generic class Map<K,V>\n  where K,V are type-variables:\n    K extends Object declared in interface Map\n    V extends Object declared in interface Map\nheron/storm/src/java/org/apache/storm/multilang/JsonSerializer.java:65: warning: [unchecked] unchecked call to put(K,V) as a member of the raw type HashMap\n    setupInfo.put(\"pidDir\", context.getPIDDir());\n                 ^\n  where K,V are type-variables:\n    K extends Object declared in class HashMap\n    V extends Object declared in class HashMap\nheron/storm/src/java/org/apache/storm/multilang/JsonSerializer.java:66: warning: [unchecked] unchecked call to put(K,V) as a member of the raw type HashMap\n    setupInfo.put(\"conf\", conf);\n                 ^\n  where K,V are type-variables:\n    K extends Object declared in class HashMap\n    V extends Object declared in class HashMap\nheron/storm/src/java/org/apache/storm/multilang/JsonSerializer.java:67: warning: [unchecked] unchecked call to put(K,V) as a member of the raw type HashMap\n    setupInfo.put(\"context\", context);\n                 ^\n  where K,V are type-variables:\n    K extends Object declared in class HashMap\n    V extends Object declared in class HashMap\nheron/storm/src/java/org/apache/storm/multilang/JsonSerializer.java:76: warning: [unchecked] unchecked call to put(K,V) as a member of the raw type HashMap\n    obj.put(\"id\", boltMsg.getId());\n           ^\n  where K,V are type-variables:\n    K extends Object declared in class HashMap\n    V extends Object declared in class HashMap\nheron/storm/src/java/org/apache/storm/multilang/JsonSerializer.java:77: warning: [unchecked] unchecked call to put(K,V) as a member of the raw type HashMap\n    obj.put(\"comp\", boltMsg.getComp());\n           ^\n  where K,V are type-variables:\n    K extends Object declared in class HashMap\n    V extends Object declared in class HashMap\nheron/storm/src/java/org/apache/storm/multilang/JsonSerializer.java:78: warning: [unchecked] unchecked call to put(K,V) as a member of the raw type HashMap\n    obj.put(\"stream\", boltMsg.getStream());\n           ^\n  where K,V are type-variables:\n    K extends Object declared in class HashMap\n    V extends Object declared in class HashMap\nheron/storm/src/java/org/apache/storm/multilang/JsonSerializer.java:79: warning: [unchecked] unchecked call to put(K,V) as a member of the raw type HashMap\n    obj.put(\"task\", boltMsg.getTask());\n           ^\n  where K,V are type-variables:\n    K extends Object declared in class HashMap\n    V extends Object declared in class HashMap\nheron/storm/src/java/org/apache/storm/multilang/JsonSerializer.java:80: warning: [unchecked] unchecked call to put(K,V) as a member of the raw type HashMap\n    obj.put(\"tuple\", boltMsg.getTuple());\n           ^\n  where K,V are type-variables:\n    K extends Object declared in class HashMap\n    V extends Object declared in class HashMap\nheron/storm/src/java/org/apache/storm/multilang/JsonSerializer.java:86: warning: [unchecked] unchecked call to put(K,V) as a member of the raw type HashMap\n    obj.put(\"command\", msg.getCommand());\n           ^\n  where K,V are type-variables:\n    K extends Object declared in class HashMap\n    V extends Object declared in class HashMap\nheron/storm/src/java/org/apache/storm/multilang/JsonSerializer.java:87: warning: [unchecked] unchecked call to put(K,V) as a member of the raw type HashMap\n    obj.put(\"id\", msg.getId());\n           ^\n  where K,V are type-variables:\n    K extends Object declared in class HashMap\n    V extends Object declared in class HashMap\nheron/storm/src/java/org/apache/storm/multilang/JsonSerializer.java:137: warning: [unchecked] unchecked method invocation: method setTuple in class ShellMsg is applied to given types\n    shellMsg.setTuple((List) msg.get(\"tuple\"));\n                     ^\n  required: List<Object>\n  found: List\nheron/storm/src/java/org/apache/storm/multilang/JsonSerializer.java:137: warning: [unchecked] unchecked conversion\n    shellMsg.setTuple((List) msg.get(\"tuple\"));\n                      ^\n  required: List<Object>\n  found:    List\nheron/storm/src/java/org/apache/storm/task/ShellBolt.java:112: warning: [rawtypes] found raw type: Map\n  public void prepare(Map stormConf, TopologyContext context,\n                      ^\n  missing type arguments for generic class Map<K,V>\n  where K,V are type-variables:\n    K extends Object declared in interface Map\n    V extends Object declared in interface Map\nheron/storm/src/java/org/apache/storm/task/ShellBolt.java:378: warning: [unchecked] unchecked cast\n            _process.writeTaskIds((List<Integer>) write);\n                                                  ^\n  required: List<Integer>\n  found:    Object\nheron/storm/src/java/org/apache/storm/utils/ShellProcess.java:70: warning: [rawtypes] found raw type: Map\n  public Number launch(Map conf, TopologyContext context) {\n                       ^\n  missing type arguments for generic class Map<K,V>\n  where K,V are type-variables:\n    K extends Object declared in interface Map\n    V extends Object declared in interface Map\nheron/storm/src/java/org/apache/storm/utils/ShellProcess.java:98: warning: [rawtypes] found raw type: Map\n  private ISerializer getSerializer(Map conf) {\n                                    ^\n  missing type arguments for generic class Map<K,V>\n  where K,V are type-variables:\n    K extends Object declared in interface Map\n    V extends Object declared in interface Map\nheron/storm/src/java/org/apache/storm/utils/ShellProcess.java:107: warning: [rawtypes] found raw type: Class\n      Class klass = Class.forName(serializer_className);\n      ^\n  missing type arguments for generic class Class<T>\n  where T is a type-variable:\n    T extends Object declared in class Class\n1 error\n20 warnings\n. Since we pass -Werror flag to javac, I need to suppress all these warnings above to build. I am not sure why Travis CI can work.\n. Could you please put class RandomSentenceSpout, SplitSentence,  and WordCount all into WordCountShellTopology, just like we did for existing examples? See ExclamationTopology.java\n. @caofangkun I'll close this PR if there is no further activity.\n. ping @caofangkun \n. hi @benley, any update on this PR?\n. not sure why no CI is running...\n. I will review it as soon as possible on Monday...\n. Here is my review of this PR. Please bear with me.\nREADME.md\nFirst of all, your README.md makes little sense to me. What do you mean by \"pattern\"? I can barely understand what you mean by \"precedence\"? Do you mean \"order\"? What is \"To run/Test\"? What is cp.heronrc.sample $HOME/.heronrc? Do you mean cp .heronrc.sample $HOME/.heronrc?\nCode\nPlease fix indentation. We enforce Python indentation level to be 2. There are simply too many parts of your code that need to be fixed, and I don't have time to add specific comment and suggestion to each of them.\nTest\nPlease add test under heron/cli/test/python. Running test manually is not a viable approach in such a large project.\n. Since you group config by \"precedence\", I think it would be better to put each piece of comment on a separate line and stop supporting trailing comment. For example:\n```\ncommand specific\nactivate: --config-path cmd-activate- \nrole and command specific\nsubmit:devcluster/ads/PROD --config-path cmd-submit-role\nactivate:devcluster/ads/PROD --config-path cmd-activate-role\nglobal\n--config-path hello \n: --config-path hello-global\n``\n. Also, please trypylint --rcfile=heron/tools/python/checkstyle.ini heron/cli/src/python/argparser.pyto lint your code. You can usepip install pylint --userto installpylint.\n. It looks much better now. I think there are two things left to do:\n1. address new comments\n2. Please have a look at your [README.md](https://github.com/prabhuinbarajan/heron-1/blob/3f2b67e44350a74e102421ee68b0478f6520d972/heron/cli/src/python/README.md) and fix it with [correct Markdown grammar](https://guides.github.com/features/mastering-markdown/)\n. CI errors because Git merge was not performed correctly. Please fix.\n. you can restart CI on the upper right corner ofTravis CI panel, although I'm not sure if you have the permission\n. 1. It seems thatheronwill make using.heronrcmandatory? Does it fallback to current parser? Please provide tests for the case that.heronrccannot be found or be parsed correctly. \n2. Address my other comments.\n. @prabhuinbarajan thanks, I will check this.\n. @kramasamy My another concern is that user will not be able to know the existence of.heronrc, or to say, the possibility of using.heronrcto make his life easier. How could user know except reading the source code?\n. do you mean put them as comment/documentation in the source code, or add a way to display them when user entershelpsubcommand?\n. I think this PR is in good shape now. #1204 has just been merged and you need to rebase against master. Specifically, please use log module underheron/common/src/python/utils. You can refer to [heron-explorer](https://github.com/twitter/heron/blob/master/heron/explorer/src/python/main.py#L31) to see how I config and use this log module.\n. 1. Please rebase to resolve conflict with master.\n2. Please use the log module --- see the explanation above.\n. Still this does not work at all.\nHere is my.heronrc`:\nheron:submit:local --verbose  --topology-file-name ~/.heron/examples/heron-examples.jar  --topology-class-name com.twitter.heron.examples.ExclamationTopology --topology-name ExclamationTopology\nNeither heron submit local nor heron submit works for me.\n. LGTM \ud83d\udc4d \n. related to https://github.com/twitter/heron/issues/1595. I will revert this PR by the end of next Monday if @prabhuinbarajan does not reply or does not want to fix his code.. team has reached consensus that we will remove heronrc entirely from codebase since the code has a very porr quality since the very beginning and no one uses it.. It already has a verbose option, and it works fine.\n. Can you provide the reason why this PR is necessary? Please explain why your PR is necessary in the future.\n. ping @caofangkun. It seems that this PR is dead. Do you want to close this?\n. close since author does not provide reason why this PR is necessary and does not respond questions.\n. there is no error when killing topology. @kramasamy tried again and the error is gone. However, I'll improve the logging.\n. can be combined with #1159 now\n. closed via #1204 \n. resolved via https://github.com/twitter/heron/pull/1140\n. \ud83d\udc4d \n. I read this PR and I think it is well written. Please address the comments and we can merge it.\n. \ud83d\udc4d \n. looks very good to me. Please use formatter as much as possible.\n. this looks good to me.\n. As for making this webpage reachable from main page, you can modify toc.yaml.\n. \ud83d\udc4d \n. @caofangkun any progress on this?\n. @mycFelix I am working on this. I will greatly appreciate if you can provide a repository/zip which I can use to reproduce the problem.\n. @mycFelix can I have your pom.xml? which Storm version are you using?\n. @mycFelix No problem. Can you also provide me your e-mail address so I can invite you to our Slack dev group?\n. so I was tracking down (no-pun-intended) the root of this problem by dumping each thread's stack. I noticed although IO loop in heron-tracker was stopped, filestatemanager thread was still running (See filestatemanager #L71). We should register SIGINT in heron-tracker properly and recursively stop all the running loop\n. \ud83d\udc4d \n. \ud83d\udc4d \n. closed via #1204 \n. @kramasamy Refactored to reuse version fetching function\n. @kramasamy \n. Right. Will fix. \nKindly,\nRunhang\n\nOn Jul 26, 2016, at 19:38, Karthik Ramasamy notifications@github.com wrote:\nIt should be \"No topologies in cluster\"\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @kramasamy fixed\n. > W: Some index files failed to download. They have been ignored, or old ones used instead.\nThe command \"sudo -E apt-get -yq --no-install-suggests --no-install-recommends --force-yes install gcc-4.8 g++-4.8 python2.7 wget pkg-config zip zlib1g-dev\" failed and exited with 100 during .\n\nrestarted CI.\n. merged since CI is OK\n. @benley you might be interested in this..\n. \ud83d\udc4d \n. Which version of Heron are you using, or what is the git commit id of the Heron you have built? Line 501 of heron-executor.py of either the latest release version or the current master head should not raise error.\n. @kramasamy actually it is because that the OS call that sets executor as process group lead does not have permission. See heron-executor.py#L501 and setpgrp manual\n. good rationale. \n\ud83d\udc4d \n. I suggest we make the following change instead. Either both custom_grouping_object and CustomGroupingObjectType appear or neither exists.\nprotobuf\nmessage CustomGroupingObject {\n    required byte custom_grouping_object = 1;\n    required CustomGroupingObjectType type = 2;\n}\n...\n{\n   ...\n   optional CustomGroupingObject obj = 5;\n}\nHowever, if this change breaks backward compatibility a lot, your change is fine. \n. @taishi8117 then it's fine. Let's wait other people's opinion though.\n. btw, it would be good to add some comments like \"if you change this protobuf file, you need to make change accordingly for foo.py, bar.java, wee.cpp\"\n. Let's be careful when upgrading to Protobuf 3.0 or making any change related to Protobuf. If someone makes change on Protobuf, I suggest he should at least:\n1. Clean build the whole project\n2. bazel test --config=darwin heron/...\n3. Run integration test under integration-test\n4. Submit a topology and manually verify heron-explorer, heron-tracker, and heron-ui work well\n5. Pass internal CI (maybe this is optional)\n. flat buffer looks interesting! I agree we should investigate carefully before making move here...\n. @kramasamy thinks for milestone 0.14.3, we should upgrade to Protobuf 3.0.0 and use C++ implementation underhood for Python protobuf. Investigating flatbuffer may take much longer time. cc @maosongfu \n. cross-reference: #1217 \n. we are going to upgrade to 3.1.0\ncf: https://github.com/twitter/heron/pull/1591. \ud83d\udc4d \n. should be good if you\n1. add tests on task hooks.\n2. fix CI\n. #1204 has just been merged. Please first merge with master and change to use log module under heron/common/src/python/utils. You can refer to heron-explorer to see how to config and use this log module.\n. @taishi8117 because we are going to use log.set_logging_level, I just imported log module. \nI agree it's kind of ugly. Feel free to give me your suggestion.\n. maybe I did not explain clearly. Actually, if log module is not used, you can just from log import Log... \nI'll merge this after removing the last commit. \n. Looks good to me. Please address my comments and we should be good to go.\n. #1204 has just been merged. Please first merge with master and change to use log module under heron/common/src/python/utils. You can refer to heron-explorer to see how to config and use this log module.\n. I think we can resolve conflict and merge this. As for https://github.com/twitter/heron/issues/1216, let's open a separate PR.\n. Other than some place which use string concatenation instead of formatter, this PR looks fine.\n. This PR will be refined after integration test PR.\n. \ud83d\udc4d \n. close this since this may be merged with an incoming PR.\n. @maosongfu I have question. Do we need to add -v to tar? Is it necessary to list the files in tar file? Similarly, if -s is not provided to curl, curl will show progress bar. Is progress bar necessary?\n. @maosongfu never mind. I now think those messages are necessary.\n. @kramasamy btw, just as a reminder, please do not merge any other Python PR before this one.\n. I'm not sure how action listener in Bazel works in detail, but when you build project, bazel will say, for example, \"46 files found by checkstyle\" (when building target xxx). We can then use multiple processes to check style to accelerate. \nAs for the bug reporting, Pylint usually dumps out information which clearly indicates which module has bad style. Parallelism will not create much hassle here. \n. @billonahill @kramasamy I just used time to get the difference. It turns out you are right --- the difference is negligible. Let's close this PR.\nI should really time it before submitting this PR...\n. I thought #1140 already fixed the problem? If not, could you please explain to me?\n. @ashvina thanks a lot! This makes sense to me now.\n. Thanks! Please resolve the conflict and we can merge this.\n. This issue is quite annoying, but good news is that people on Bazel mailing list suggests Bazel 0.3.1 may have fixed this.\n. see: https://groups.google.com/forum/?utm_medium=email&utm_source=footer#!msg/bazel-discuss/rnAudK0OKcc/9ISbJFY4CAAJ\n. Bazel team has updated brew so we can do brew upgrade bazel to upgrade Bazel to 0.3.1. 0.3.1 works fine on my machine, but I guess we again need more effort to run it on our internal machine...\n. This is Bazel's bug. Please do brew upgrade bazel to upgrade to 0.3.1\nMark this issue as resolved now.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @kramasamy will do when we have time...\n. I agree with Bill. It's a good idea to introduce more granularity at this stage. By the way, you can make a dependency graph on your tnojima/python_topo branch by:\nBash\n$ bazel query 'deps(heron/common/src/python:common)' --output graph > heron-py-common.dot\n$ dot -Tpng heron-py-common.dot > heron-py-common.png\n\nAs we can see, we could introduce tons of unnecessary Protobuf dependencies even if we just want to use log.\n. Just as a reference, here is the output of command tree heron/common/src/python/\n```\nheron/common/src/python/\n\u251c\u2500\u2500 BUILD\n\u251c\u2500\u2500 basics\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 init.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 event_looper.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 gateway_looper.py\n\u251c\u2500\u2500 constants.py\n\u251c\u2500\u2500 handler\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 access\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 init.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 fetch.py\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 heron.py\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 query.py\n\u251c\u2500\u2500 network\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 init.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 heron_client.py\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 protocol.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 socket_options.py\n\u251c\u2500\u2500 pex_loader.py\n\u2514\u2500\u2500 utils\n    \u251c\u2500\u2500 init.py\n    \u251c\u2500\u2500 config.py\n    \u251c\u2500\u2500 log.py\n    \u251c\u2500\u2500 metrics\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 init.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 metrics.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 metrics_helper.py\n    \u251c\u2500\u2500 misc\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 init.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 communicator.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 custom_grouping_helper.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 outgoing_tuple_helper.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 pplan_helper.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 serializer.py\n    \u251c\u2500\u2500 topology\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 init.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 custom_grouping.py\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 task_hook.py\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 topology_context.py\n    \u251c\u2500\u2500 tracker_access.py\n    \u2514\u2500\u2500 tuple.py\n8 directories, 32 files\n``\n. Besides introducing more granularity (addingBUILDfile per subdirectory), I think we should also be careful on when to include dependency//heron/proto:proto-py. Please identify precisely the **subdirectories** that require Protobuf. Do not add Protobuf dependency on toplevel BUILD file underheron/common/src/python.\n. @taishi8117 Do you think #1252 resolved this issue completely? \n. @maosongfu agree\n. would be good if @taishi8117 can profile again with this patch. We need to do experiment to get data. \n. it is possible that user simply imports theLogobject without calling functionconfigure. This may cause exception \"No handler found\". So we at top level add alogging.basic_config()to prepare a default handler so thatLogwill always be properly initialized.\n. I made some small changes on website. Can we hold this PR for a little while?\n. @lucperkins I just merged in #1226. Can you build the website again? \n. \ud83d\udc4d \n. If you pass--verboseflag on command line, logging level is set tologging.DBEUG, otherwise it islogging.INFO.\n. or do you mean you want more granularity? like command-line argument--verbose=LEVEL`?\n. add some comments. should be good to go after you fix them\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. restart CI after #1237 \n. \ud83d\udc4d \n. \ud83d\udc4d \n. Code looks good, but I will add unit tests for this PR.\n. Yes. But I'm sorry that unit tests are hard to write and I'm not in good health condition now. I may add some later. Integration test which Taishi now is writing could be helpful though.\nAnyway, I'd say we can merge this PR.\n\nOn Aug 11, 2016, at 00:30, Karthik Ramasamy notifications@github.com wrote:\n@objmagic - is this ready to be merged?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @kramasamy looks fine. merged.\n. restarted CI\n. oh right. @taishi8117 please rebase against master\n. restarted CI\n. please rebase against master if necessary\n. fix comment and \ud83d\udc4d \n. \ud83d\udc4d \n. Building and testing on your local machine can catch style error. Did you do it...?\n. Yes. If you work on OS X, you can do bazel build --config=darwin heron/... and then bazel test --config=darwin heron/... before PR.\n. > when the same argument is provided at multiple levels, this was supposed to clobber the argument list and enforce the precedence\n\nDo you mean for .heronrc file like:\n........ --config-property a=b\n...\n...\n........ --config-property c=d --config-property a=b\nwe should dedup list --config-property a=b --config-property c=d --config-property a=b to --config-property a=b --config-property c=d? Is this what you meant by \"clobber\"?\nAlso, can you elaborate what you mean by \"precedence\"? and why we should preserve it?\n. #1198 is merged. Merging with master and restarting CI now.\n. \ud83d\udc4d \n. Please provide your Heron version, cli version (heron version), environment, etc...\nWhat exactly is the command you were using to submit your topology?\n. @billonahill sorry. See objmagic.ml/heron\nAs before, JavaScript does not work on my website so navbar on the left is dead... Maybe because of some HTTPS issues cc @lucperkins \n. No. Just a build\n. yeah, so if you use HTTPS (https://twitter.github.io/heron/docs/getting-started/), navbar on the left is dead... (although on this page you will fall back to HTTP from the HTTPS main page)\n. yeah, not a big deal. But my domain (http://objmagic.github.io/heron) enforces/redirects HTTPS so people cannot navigate on my GitHub Heron page.\n. @lucperkins @billonahill no worry. I will submit a new PR and fix this...\n. site rebuilt with HTTPS JQuery. \nYou can view at objmagic.ml/heron.\nPyHeron related page is here.\nSorry for the hassle (I config to use CDN to deliver my blog hosted on GitHub using HTTPS just to be cool...)\n. will update this PR when #1266 is merged.\n. This PR is good to go.\n. Thanks \ud83d\udc4d  LGTM and I will merge it when CI passes.\n. just need a quick +1 \n. I ran this test 1000 times using a for-loop and nothing happened (yes, with --cache_test_results=no flag passed to bazel test) . However, sometimes it fails on our internal CI.\n. failed internal CI again\n. repeated this test on internal Linux environment for 30mins but still can't reproduce the error...\n. finally reproduced this error on Linux machine:\n```\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\n\nJUnit4 Test Runner\n.....E.\nTime: 4.13\nThere was 1 failure:\n1) testGetProcessBuilder(com.twitter.heron.spi.utils.ShellUtilsTest)\njava.lang.AssertionError\n    at org.junit.Assert.fail(Assert.java:86)\n    at org.junit.Assert.assertTrue(Assert.java:41)\n    at org.junit.Assert.assertTrue(Assert.java:52)\n    at com.twitter.heron.spi.utils.ShellUtilsTest.testGetProcessBuilder(ShellUtilsTest.java:132)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n    at com.google.testing.junit.runner.junit4.CancellableRequestFactory$CancellableRunner.run(CancellableRequestFactory.java:84)\n    at org.junit.runner.JUnitCore.run(JUnitCore.java:160)\n    at org.junit.runner.JUnitCore.run(JUnitCore.java:138)\n    at com.google.testing.junit.runner.junit4.JUnit4Runner.run(JUnit4Runner.java:103)\n    at com.google.testing.junit.runner.BazelTestRunner.runTestsInSuite(BazelTestRunner.java:149)\n    at com.google.testing.junit.runner.BazelTestRunner.main(BazelTestRunner.java:90)\nFAILURES!!!\nTests run: 6,  Failures: 1\nBazelTestRunner exiting with a return value of 1\nJVM shutdown hooks (if any) will run now.\nThe JVM will exit once they complete.\n-- JVM shutdown starting at 2016-11-08 00:40:06 --\n```\n. \ud83d\udc4d \n. \ud83d\udc4d \n. Style error:\n[ERROR]\nheron/statemgr/zookeeper/curator/CuratorStateManager.java:328: Don't System.exit(),\nthrow a RuntimeException() [RegexpSinglelineJava]\nFYI, bazel runs Java style checker on my local machine. That said, #1110 does not happen to me.\n. \ud83d\udc4d \n. this breaks since the hardcoded level is changed:\nhttps://github.com/twitter/heron/blob/master/heron/common/src/python/utils/config.py#L130\n. \ud83d\udc4d LGTM\n. \ud83d\udc4d \n. should be resolved in https://github.com/twitter/heron/pull/1276, but keep this open for a while for possible future reference\n. @kylozw if you have any other question, you can open a separate issue or ask on Heron's mailing list\n. closing this issue since I believe this problem is gone.\n. \ud83d\udc4d \n@kramasamy any final comment? I think license related PR needs your final review\n. \ud83d\udc4d \n. restarted CI\n. \ud83d\udc4d \n. ah, didn't notice this. I am just used to using four backticks... will change to three now.\n. @billonahill no. It is because a new line was not inserted before the code block.\n. LGTM\n\ud83d\udc4d \n. looks good. we can also display this list on the website.\n. \ud83d\udc4d \n. cf. #1217 \n. not really sure how to start investigating this issue\n. @kramasamy this should be the most important and the last issue. And I will investigate this issue.\n. I am getting a bit lost when investigating this issue. Event loop could stop running, but not all the time. When I decrease parallelism to 1, acking latency (complete latency) is much slower, but still growing.\n. one can set this env var in shell before submitting topology, and heron-executor takes this env var at run-time.\n. You're right... Maybe this feature is mostly targeting Heron developer?\n. @kramasamy I think that is possible by adding this profiling option into API. And we don't need to add this option in command-line tool options.\n. temporarily close this since we need to reach consensus on design\n. I googled around but found no tool that can enforce license style for Python. If someone has suggestion, I'll be glad to give a try.\n. integer being type of container's id looks reasonable to me. any reason why String type shows up occasionally? maybe this happened during some refactor before?\n. FYI, @maosongfu wanted me to improve the local integration test since he spotted some potential flaws. I have to admit that I spent some time on it but felt there is no fundamentally design/programming flaw. Maybe you would like to have a quick look?\n. @billonahill thx for the suggestions. I'll take care of them at some time.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. pretty sure Travis will fail again on integration test. Can you have a look at this PR? @taishi8117 \n. ShuffleGrouping in the integration tests fails after 25 attempts.\n. Rebased.\n. need more time to look into integration test failure. we can do 0.14.3 before this PR.\n. Yes!\n\nOn Sep 1, 2016, at 21:41, Karthik Ramasamy notifications@github.com wrote:\n@objmagic - is this ready to go?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. @taishi8117 I have addressed your comments in the latest commit.\n. seems integration test is very unstable. restarted CI...\n. seems to me each one of them failed before (retried > 25 times)\n. restarted CI and now it's fine.\n. I cannot reproduce this problem on OS X. Will try Docker some time...\n. Ah, I see. If I do rm -r ~/.herondata, I can see the problem.\n\nThere are three solutions that I can think of:\n1. do mkdir as you said in filestatemanager.py\n2. do mkdir when we install Heron, just like how we create ~/.heron and ~/.herontools\n3. emit an useful error message to tell user that he needs to actually run a topology before starting tracker\n. LGTM. Please sign CLA. \n. LGTM if Python style is fixed.\n. Just curious, did this actually cause memory leak before? \n. Is this Python or Java topology?\n. This error was discussed before. It is because metrics for these components are not available.\n. Related: https://github.com/twitter/heron/blob/master/heron/tmaster/src/cpp/manager/tmetrics-collector.cpp#L112\n. \ud83d\udc4d if CI passes\n. resolved in #1331 but we will rebuild the website later. \n. \ud83d\udc4d \n. Please fix failed unit test\n. \ud83d\udc4d \n. My bad. I should not merge this...\n. sorry I didn't notice the \"DO NOT MERGE\". This is very inappropriate. Apology...\n. \ud83d\udc4d \n. Not sure if I should change Log.error to Log.warn here: https://github.com/twitter/heron/pull/1336/files#diff-409c349551fd1e7ad3611d8544571cffR148\nIn my opinion, we should have different logging level for \"Metrics not available\" (Log.warn) and \"Unknown component\" (Log.error). However, we have no way to distinguish since we only have OK and NOTOK. Maybe we can add another specific error number, like METRICS_NOT_AVAILABLE?\n. will merge this when internal integration test passes\n. I think this is fine. Sorry again for the hassle.\n\nOn Sep 4, 2016, at 16:09, Ashvin notifications@github.com wrote:\nHi @objmagic. I think there is no need to revert this patch. I tested #1329 with the local and yarn schedulers. While it works on local, it has some issues on the YARN scheduler. I will have a fix ready for YARN today. So the build will be stable then. Thoughts?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. \ud83d\udc4d \n. @billonahill \n\nhttps://github.com/twitter/heron/commit/dbe056322528ea034e5f84c7f52e000a00738092#diff-2a669e084143b86b5766a49245f8e357R620\nMaybe because of this if expression should not be nested under if state_manager_class == 'com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager':? \n. @billonahill \nHow about using os.path.split to split path, substituting wildcard variable with corresponding variable in config, and finally using os.path.join to join them back?\n. change jvm_metrics to py_metrics. removed dup license header. \n. \ud83d\udc4d \n. @jomsdev Logger problem should be fixed via https://github.com/twitter/heron/pull/1276 which was after 0.14.2\n. \ud83d\udc4d \n. this was fixed in https://github.com/twitter/heron/pull/1331\n. @ajorgensen will build the website now\n. @ajorgensen actually, we are going to release 0.14.3 soon. I will build website after 0.14.3.\n. \ud83d\udc4d \n. \ud83d\udc4d  if we can make sure it also works in the headless-YARN application scenario\n. \ud83d\udc4d \n. May I know which version of Heron you were using? What does heron-tracker version give you?\n. CI failed. You can run bazel test --config=centos heron/... to check.\n. Thx for reporting this issue. I will look into it.\n. @kramasamy @billonahill should we perform cpp code style checking on third-party code?\n. \ud83d\udc4d  I will see if we can disable cpp code style checking on third-party code.\n. Can you briefly describe how you improved the performance? Can we have some numbers that shows the performance difference?\n. Internal CI passed. Please wait Travis CI to finish.. this seems easily fixable after we introduced information propagation from Java side to Python side.. \ud83d\udc4d \n. should use LOG.warning here\n. document how to use cluster.yaml\n. if the topology already exists, the error message is hidden in a huge trunk of stacktrace. And at the very end, we see:\nERROR: Failed to launch topology 'ExclamationTopology' because User main failed with status 1. Bailing out...\nTraceback (most recent call last):\n  File \"heron/tools/cli/src/python/submit.py\", line 149, in launch_topologies\n    launch_a_topology(cl_args, tmp_dir, topology_file, defn_file)\n  File \"heron/tools/cli/src/python/submit.py\", line 114, in launch_a_topology\n    java_defines=[]\n  File \"heron/tools/cli/src/python/execute.py\", line 73, in heron_class\n    raise RuntimeError(err_str)\nRuntimeError: User main failed with status 1. Bailing out...\nThis piece of error message does not help user understand what's going wrong.\n. closed via #1571 #1610 . Is there any test for ZK curator lock?\n. \ud83d\udc4d \n. close via #1468 \n. resolves https://github.com/twitter/heron/issues/1465\n. fixing some other failed integration tests\n. unsure about Travis integration test failure. Some integration tests succeed while some failed. Restarting Travis CI now...\n. the all groping one is failing for me\n. @kramasamy fixed via https://github.com/twitter/heron/pull/1468/commits/9b4005906bd1c140fd679d739007aceec4a3b695. Wait for CI.\n. need more fix.\n. still failing. need more time to know why\n. still debugging. Somehow the process is never called in integration_test_bolt.py. This means that result cannot be posted to HTTP server and the test kept failing to fetch HTTP. After 25 times, the whole test fails. \nReally appreciate if someone can have a look at it. You can reproduce this error by doing ./scripts/run_integration_test.sh.\n. @maosongfu and I are debugging.\n. Bug should be fixed now.\n. closed via https://github.com/twitter/heron/pull/1481\n. LGTM \ud83d\udc4d \n. @maosongfu agree with Taishi's comment. This should be a MM design issue. will look into this today.\n. Java and Python code handle NOT_OK in exactly the same way:\nJava: https://github.com/twitter/heron/blob/master/heron/instance/src/java/com/twitter/heron/network/MetricsManagerClient.java#L177\nPython:\nhttps://github.com/twitter/heron/blob/master/heron/instance/src/python/network/metricsmgr_client.py#L80\n. will do.\n. we did some basic investigation and believe it's because of the inefficiency of Python Protobuf library.\n. And please use mailing list if you have more questions: https://groups.google.com/forum/#!forum/heron-users\n. Please use mailing list if you have question: https://groups.google.com/forum/#!forum/heron-users\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @maosongfu it is an internal topology called \"nlu-ex\". Besides execution_state, its cluster and environ variables are also None.\n. sure. considering it happens only internally, we'd better continue this discussion on internal Slack channel.\n. update: we could not find this topology on Aurora. However, for some unknown reasons, when talking to Zookeeper, tracker gets this topology's info (name only, no any other info)\n. @kramasamy Do you mean \"is this PR related to viz deployment issue\"?\nIt is not related to the viz code. viz are fine now. we still don't understand why some ghost topology could show up in zookeeper...\n. close this PR since it's because of some ghost topologies in our internal ZK. @billonahill bombed them out and we think this PR is not necessary anymore.\n. > CuratorStateManager.java:80: Line is longer than 100 characters (found 155). [LineLength]\nPlease fix CI error.\n. LGTM \ud83d\udc4d \n. just curious, SUM means sum of the metrics in a window, right?\n. thanks for the explanation. \nLGTM \ud83d\udc4d \n. \ud83d\udc4d \n. need to add an API that returns back pressure info in heron-tracker. i can help on this.\n. LGTM \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. fix Python style and \ud83d\udc4d \n. do we need to change FirstFitDecreasingPackingTest.java accordingly?\n(and all CI tests have passed...)\n. You are right I forgot to check if arguments match.\n. \ud83d\udc4d \n. Besides one minor comment, this PR looks good to me.\n. \ud83d\udc4d if style is fixed\n. \ud83d\udc4d \n. should be fine when we rebuilt website\n. because input file is only on your local machine but not on the cluster machine?\n. Did this happen during test_kill_stmgr?\n. resolved via #1541 \n. cc @congwang \n. @ajorgensen resolved via https://github.com/twitter/heron/pull/1549\n. besides comments and style issues, \ud83d\udc4d \ud83d\udc4d \ud83d\udc4d \ud83d\udc4d \ud83d\udc4d \n. I'm OK with logging everything when in verbose, but we should specify what should not be omitted when logger is not in verbose mode. For ZK,  I'd like to see some important information gets printed. For example, the ZK hostport that gets connected to.\n+1 for not logging HttpURLConnection when not in verbose mode.\nP.S.: I was fixing Python logging months ago and realized that carefully assigning proper logging level to every log is not an easy job. And that If you think some levels are fine, not everyone will agree... (\"Why I have to add --verbose to get blah blah printed out!?\")\n. ok make sense. \ud83d\udc4d for setting ZK to warning.\n. - >I'm inclined to set all of org.apache to INFO actually, but org.apache.zookeeper to WARN\nUnderstood now. Agree \ud83d\udc4d \n- you should also remove --verbose flags in integration test in this PR so that those logs you don't want to see will actually be gone\nintegration-test//src/python/local_test_runner/main.py:75:  splitcmd = [tracker_path, '--verbose', '--port=%s' % tracker_port]\nintegration-test//src/python/local_test_runner/test_kill_tmaster.py:30:  splitcmd = [heron_cli_path, 'restart', '--verbose', test_cluster, topology_name, str(shard_num)]\nintegration-test//src/python/local_test_runner/test_scale_up.py:50:      heron_cli_path, 'update', '--verbose', test_cluster, topology_name,\nintegration-test//src/python/local_test_runner/test_template.py:301:      heron_cli_path, 'submit', '--verbose', '--', test_cluster, test_jar_path,\nintegration-test//src/python/local_test_runner/test_template.py:311:  splitcmd = [heron_cli_path, 'kill', '--verbose', test_cluster, topology_name]\nintegration-test//src/python/test_runner/main.py:277:  cmd = \"%s update --config-path=%s %s %s %s --verbose\" %\\\n. Yes those logs should be removed.\n. \ud83d\udc4d \n. Pleases do the same for SpoutOutputCollectorImpl.java\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. //heron/common/tests/cpp/network:switch_unittest also sometimes times out. Maybe mark all tests under //heron/common/tests/cpp/networ as flaky so that Bazel could rerun them?. constantly fails both Travis CI and internal CI now. We should prioritize this.. resolved by #1830 #1815 . @billonahill what tag name do you want to use? 0.14.5-website?\n. \ud83d\udc4d \n. Preview effect:\n\n. @congwang has informed @kramasamy that this is not a issue that could be resolved soon. Delay this to 0.15.0.. \ud83d\udc4d \n. @billonahill thanks for the suggestion. This PR still needs improvement and I'll push more commits later.. After discussion with @billonahill, we agreed that there are many other refactoring that have to be done. I'll address them in later PRs. For now, I'll push more to address some of the issues in this PR. We should then merge this PR to avoid making this PR too large. . It should be ready now. I'll let Bill review.. close #1593 and keep working on this.. pushed one important commit: https://github.com/twitter/heron/pull/1571/commits/002dde6c52c031840ba71ac62a916e146e6ca527\nprobably the only existing comment left to be resolved: https://github.com/twitter/heron/pull/1571#discussion_r91440353\ncomment? @billonahill . will also tackle https://github.com/twitter/heron/issues/1594 here. See reason: https://github.com/twitter/heron/issues/1594#issuecomment-265649190. resolved #1594 via https://github.com/twitter/heron/pull/1571/commits/cc99a7868303f5952afef3629fc50dba245f49ec. I think all the work I want to finish in this PR have been done. More comments are welcomed. @billonahill . will fix integration test errors later. LGTM \ud83d\udc4d . \ud83d\udc4d . fix the CI and then \ud83d\udc4d . related to https://github.com/twitter/heron/pull/1217. >  The better fix would be to throw an exception with a clear error message as is being done in #1462.\nThis might be hard, since there is no way to get \"clear error message\" by just reading shelled-out's program's stdout/stderr outputs. I think improvement in #1726 is good enough.\n\nShellUtils also has this pattern in runSyncProcess where helpful failure messaging doesn't occur without passing --verbose.\n\nthis is probably worth doing. Notice when shelling out Aurora process, we already add --verbose if user runs heron submit with --verbose. What we need to do here is to identify which shelled-out processes worth having --verbose passed to them.. this should be resolved. so the proto2 of protobuf-3.x also has improvement? \nAnd we should run internal integration test to make sure everything works fine as before. Also, it would be great to give some concrete numbers that show how much more tups/secs we get. close this PR and will keep working on #1571. I see, because we need to pass --component_parallelism to RuntimeManagerMain. Same with restart.py which needs to pass extra argument --container-id.\nAnother difference is that different action requires different jar libs.\nBut I think there is chance we can refactor this (can be done after #1593). @billonahill Thanks! agree that OO approach is nice. will PR.. although #1571 is getting too large, i think it is a good chance to resolve this issue in #1571 since #1571 already involves changes of restart.py and update.py.. resolved in https://github.com/twitter/heron/pull/1571/commits/cc99a7868303f5952afef3629fc50dba245f49ec in #1571 . @kramasamy any comment on this issue?. although it is hard to work on/maintain heronrc parser, it works alright. let's delay this issue.... But seems that we don't run it in a separate thread so far?. refactored so that LaunchRunner no longer implements Callable in https://github.com/twitter/heron/pull/1571. resolved via #1599 and #1609 . \ud83d\udc4d . It should be fine to build on CentOS 5.\n\nCould you please give us output of ./bazel_configure.py?\nThe above error message has no useful information (weird even with --verbose_failures). It would be good if we have detailed logs. 1. how about using destination instead of connection?\nplease update sample.yaml\nplease sign CLA at https://engineering.twitter.com/opensource/cla. resolved via #1620. Thanks.. i believe this has been resolved by https://github.com/twitter/heron/pull/1708. resolved via #1617 . \ud83d\udc4d . \ud83d\udc4d . restarted CI due to some flaky integration test. @kramasamy just opening a random PR to test bot. bot is running on aws now and has nothing to do with the Heron project itself.. reviews addressed via https://github.com/twitter/heron/pull/1618/commits/1cf64a5ce2019f577ebf341dfd2f6c0e34e588ba. #1571 and #1618 refactored control flow of submitter and runtime manager in a way that enables us to easily implement dry-run --- we propagate packing plan information all the way up using exception. Java side (submitter and runtime manager) further propagates info back to Python side (heron client) using stdout with special return code associated (for example, 200).\n\nRendering of packing plan should be done on Java side.. \nA glance of the prototype. Still need lots of polish, of course.. This is raw renderer. Other renders could print info in a much better way.\nWill update soon\n. preview of output of table formatter:\n\n. @billonahill that's a good idea. But I'm wondering what's that gonna look like for topology with thousands of instances running. It is not a problem to display such big table on terminal (we can use ncurse to enable scrolling). However, the amount of information will be overwhelming. What useful information do we expect user to get immediately under this detailed mode? \nAnd another question, since user updates topology's \"component parallelism\", any reason why he needs to reason about how each instance is running in container?. From @ajorgensen:\n\nOne feature that we added to our fork of heron was a way to output the result of the packing plan in json format. This allowed us to do some intelligent scheduling for availability zones in AWS\n\nYes, you'll have JSON render.. and @billonahill, what does the field cpu mean here? and why it has type double?. updated. now table does colorful and styled formatting:\n\n. more screenshots:\n\n. On Jan 9, 2017, at 09:38, Bill Graham notifications@github.com wrote:\nIn the enlarged containers which instances are added? Can you visually\nrepresent that?\nYes, I have just improved the visualization of this.\nI like the rendering but do you think it would simplify things by\ncondensing states enlarged, reduced and modified all into just modified?\nThen you could see which instances were added and removed in each of the\nmodified.\nYes I have realized that enlarged and reduced should be better generalized\nto 'modified'\n. Updated:\n\n. @billonahill no instance resource is modified in container 1 and container 2. They are modified because only the requiredResource was modified. And yes, I will add some container number and max container info at the top(update: hold off this as discussed with @billonahil). And feel free to give me more suggestion.. resolved via #1571 #1618 #1629 #1675 #1676 \nfollow-up:\n- [ ] #1622 \n- [ ] #1667 \n- [x] #1668 \n- [ ] #1673 \n- [x] #1692 \n- [x] small comments https://github.com/twitter/heron/pull/1629#discussion_r99461641. \ud83d\udc4d . \ud83d\udc4d . imo only SubmitterMain and RuntimeManageMain should extend AbstractMain. let's leave SchedulerMain later.. actually, will finish this PR after @billonahill's work on refactoring configs is merged. \ud83d\udc4d . same situation happened this week when I was debugging with @congwang when deploying a topology. I'll spend some time to implement this. And expose this info via an API endpoint on tracker.. related #1646, which adds very basic support of displaying topology states. > Many unit tests are required, even for the renderers\nshould I save the sample output into a txt file, and unit test loads the file and compares two outputs? @billonahill . @kramasamy it will be finished by the end of today. Bill is on PTO and he will review it later.. @billonahill this PR now should be ready for another review.. all comments addressed except https://github.com/twitter/heron/pull/1629#discussion_r96979988, which I'd like to see @billonahill's comment.. @billonahill \n\nOutput of heron help submit\n```\nusage: heron submit [options] cluster/[role]/[env] topology-file-name topology-class-name [topology-args]\n\nRequired arguments:\n  cluster/[role]/[env]  Cluster, role, and environment to run topology\n  topology-file-name    Topology jar/tar/zip file\n  topology-class-name   Topology class name\nOptional arguments:\n  --config-path CONFIG_PATH\n                        Path to cluster configuration files\n  --config-property PROPERTY=VALUE\n                        Configuration properties that overrides default\n                        options\n  --deploy-deactivated DEPLOY_DEACTIVATED\n                        Deploy topology in deactivated mode\n  --extra-launch-classpath CLASS_PATH\n                        Additional JVM class path for launching topology\n  --topology-main-jvm-property PROPERTY=VALUE\n                        JVM system property for executing topology main\n  --dry-run             Enable dry-run mode\n  --dry-run-format DRY_RUN_FORMAT\n                        Dry-run to display resource packing plan. Available\n                        options: raw, table. Default option: table\n  --verbose VERBOSE     Verbose mode. Increases logging level to show debug\n                        messages\n```\n\nOutput of heron help update\n```\nusage: heron update [options] cluster/[role]/[env]  --component-parallelism \n\nRequired arguments:\n  cluster/[role]/[env]  Cluster, role, and environment to run topology\n  topology-name         Name of the topology\nOptional arguments:\n  --component-parallelism COMPONENT_PARALLELISM\n                        Component name and the new parallelism value colon-\n                        delimited: [component_name]:[parallelism]\n  --config-path CONFIG_PATH\n                        Path to cluster configuration files\n  --config-property PROPERTY=VALUE\n                        Configuration properties that overrides default\n                        options\n  --dry-run             Enable dry-run mode\n  --dry-run-format DRY_RUN_FORMAT\n                        Dry-run to display resource packing plan. Available\n                        options: raw, table. Default option: table\n  --verbose VERBOSE     Verbose mode. Increases logging level to show debug\n                        messages\n``. update: tackling https://github.com/twitter/heron/pull/1629#discussion_r97118602, which @billonahill and I decide to not rush ourselves and take some time to resolve.. @billonahill fixed. Thanks for all the reviews! Waiting for the CI to pass.. All tests passed on local machine but not on Travis CI. Updated CI script so that I can see detailed error report.. \ud83d\udc4d . LGTM. Make sure you run internal integration test.. @billonahill I think the key point here is thatheron kill` returning 1 have overloaded meanings:\n\nfailed to kill; topology is still running\n   in which case we cannot do heron submit\ntopology does not exist\n   in which case we can go ahead and do heron submit\n\nAnother solution to this feature request is to assign different proper exit code for different scenarios.. \ud83d\udc4d . prioritize this issue by adding it into milestone 0.14.6. @nlu90 can you show me how to reproduce this?. @nlu90 \nlaunched AckingTopology locally and didn't see any issue. Here are some screenshots:\n\n\nwill close this issue if we cannot reproduce the problem\n. delay this to 0.15.0. \ud83d\udc4d . \ud83d\udc4d . \ud83d\udc4d . resolved via #1655 . resolved via https://github.com/twitter/heron/pull/1575 but this change was introduced after 0.14.5 so it was not included in the latest website build.. I can push website change directly to website branch in the future.. @mycFelix fixed via #1652. Please check out http://twitter.github.io/heron/docs/operators/deployment/schedulers/yarn/. Not sure if important but please do not expose internal service hostname to the public . got paged this morning and confused by these unclear log messages again.... resolves #1651. LGTM \ud83d\udc4d Please investigate CI failure.. we've seen similar issues when we were doing some internal experiments. will investigate.\ncc @congwang . @thomas4g that is very possible. The dynamically linked libstdc++ on your machine probably doesn't have the GLIBCXX of version that heron-tmaster requires.. this issue definitely has a very low priority, but I created it just to keep track of this problem.. resolved via #1666. reminder to myself: should finish #1668 first. will be tackling this in https://github.com/twitter/heron/pull/1629. #1629 implemented an example, but we still needs to specify some extra paths to the resource. Google's Bazel example looks simpler. We need to investigate why, and change to use resource for all BUILD files.. seems we have done this before for java_library: https://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/common/Resource.java. @billonahill notice it uses getResourceAsStream  and resources label. reminder to myself: check ZK clean steps of OSS version, both restart and kill. The process of heron kill looks fine to me. The steps are:\n1. issue Aurora command job killall. Command returns 0 even if job does not exist\n2. clean ZK state if Aurora command exits with 0\nheron restart also LGTM. The steps are:\n1. if user wants to restart container 0 (TMaster) or every container, ZK will clean up TMasterLocation\n2. issue Aurora command to restart containers\n. resolved via https://github.com/twitter/heron/pull/1695. LGTM.. close since I merged #1629 with this. It is a trivial change and we already did this in internal CI months ago.. can you check out the CI?. Thanks. I'll just restart the CI.. The PR CI is fine. Feel free to merge it.. The PR CI check passed. Merged.. compiling the regular expression and matching should be fast; and I observe no significant overhead when switching to regex header check. \ud83d\udc4d . btw, it would be good if we can also resolve https://github.com/twitter/heron/issues/1626 in this PR.... @dmarchand I believe this is our mistake. \n@billonahill I will update the OSS release process both internally and on GitHub.\n@kramasamy Can you publish 0.14.5 to Maven?. @ajorgensen iirc you showed interest in JSON format. Please free feel to elaborate your need, if there is any, so that I can incorporate it.\nThe table output is here.\nThe raw output is here.. propose a design for CSV format first. Shall we do something like:\ntask_id, component name, container id, CPU, RAM, Disk\nSo, for the following table format:\n```\nContainer 1\nCPU: 5.0, RAM: 11GB, Disk: 5GB\n====================================================\n| component | task ID | CPU | RAM (GB) | disk (GB) |\n\n|  exclaim1 |       1 | 1.0 |        3 |         1 |\n...\n====================================================\n```\nCSV output (excluding the header) should be something like:\n1, exclaim1, 1, 1, 3, 1\nThis is kind of shitty and there should be a better design. Any idea? @billonahill . resolved in #1695. finally got some useful logs. See attachment.\nci_logs.tar.gz\n. Each spout sends out ten tuples exactly.\n1691-investigation $ grep \"taskid\" container_1_ab-spout-1_2.log | wc -l\n      10\n1691-investigation $ grep \"taskid\" container_1_ab-spout-1_3.log | wc -l\n      10\n1691-investigation $ grep \"taskid\" container_1_ab-spout-2_4.log | wc -l\n      10\n1691-investigation $ grep \"taskid\" container_1_ab-spout-2_5.log | wc -l\n      10\n1691-investigation $ grep \"taskid\" container_1_ab-spout-2_6.log | wc -l\n      10\n1691-investigation $ grep \"taskid\" container_1_ab-spout-1_1.log | wc -l\n      10\n. grep \"taskid\" container_1_identity-bolt_8.log | wc -l gives us 64.\ngrep \"taskid\" container_1_identity-bolt_8.log | grep 5 | wc -l gives us 14.\ncontainer_1_ab-spout-2_5 has some problem?. ci.log:[2017-02-03 23:03:01 +0000] com.twitter.heron.integration_test.core.IntegrationTestSpout INFO:  Received a fail with MessageId: __integration_test_mock_message_id_79af2046-68eb-401e-99c3-f850dd546d69\nci.log:[2017-02-03 23:03:01 +0000] com.twitter.heron.integration_test.core.IntegrationTestSpout INFO:  Received a fail with MessageId: __integration_test_mock_message_id_9ccb1355-95c0-42bf-a8d8-bffe5a8b60b7\nci.log:[2017-02-03 23:03:31 +0000] com.twitter.heron.integration_test.core.IntegrationTestSpout INFO:  Received a fail with MessageId: __integration_test_mock_message_id_79af2046-68eb-401e-99c3-f850dd546d69\nci.log:[2017-02-03 23:03:31 +0000] com.twitter.heron.integration_test.core.IntegrationTestSpout INFO:  Received a fail with MessageId: __integration_test_mock_message_id_9ccb1355-95c0-42bf-a8d8-bffe5a8b60b7\ncontainer_1_ab-spout-2_5.log:[2017-02-03 23:03:01 +0000] com.twitter.heron.integration_test.core.IntegrationTestSpout INFO:  Received a fail with MessageId: __integration_test_mock_message_id_79af2046-68eb-401e-99c3-f850dd546d69\ncontainer_1_ab-spout-2_5.log:[2017-02-03 23:03:01 +0000] com.twitter.heron.integration_test.core.IntegrationTestSpout INFO:  Received a fail with MessageId: __integration_test_mock_message_id_9ccb1355-95c0-42bf-a8d8-bffe5a8b60b7\ncontainer_1_ab-spout-2_5.log:[2017-02-03 23:03:31 +0000] com.twitter.heron.integration_test.core.IntegrationTestSpout INFO:  Received a fail with MessageId: __integration_test_mock_message_id_79af2046-68eb-401e-99c3-f850dd546d69\ncontainer_1_ab-spout-2_5.log:[2017-02-03 23:03:31 +0000] com.twitter.heron.integration_test.core.IntegrationTestSpout INFO:  Received a fail with MessageId: __integration_test_mock_message_id_9ccb1355-95c0-42bf-a8d8-bffe5a8b60b7\ncontainer_1_ab-spout-2_5 receives 4 fails and re-emits 4 tuples. working on #1687 . resolved via https://github.com/twitter/heron/pull/1815. code LGTM. CI integration test fails.. That integration test is failing, but we should be able to merge this PR. I have some PRs waiting this to be merged.. LGTM.. I see this is part of 0.14.6 milestone. Any progress on this?\nand @maosongfu, should I update viz code to reflect this metrics?. PR is huge and has repetitive pattern so I just glanced through. LGTM. Nice use of enum to remove default configs. . Also addressed some comments from @mycFelix in https://github.com/twitter/heron/pull/1697/commits/1e34ccc490bf6fd88a5169e744e699f34f71be0d. build is fine on CI, unit tests passed locally. Website build breaks the CI because of some package issues, which will be solved in #1703. \nmerging now.. display of internal uploader command, scheduler command under info mode. Also consider improve ShellUtils.. related https://github.com/twitter/heron/issues/1588. @lucperkins I understand you are not at Twitter anymore, but we will really appreciate if you can help us look into this issue.. fixed via #1703 . this seems to be the root cause: https://github.com/jakubpawlowicz/clean-css/issues/888. CI is still using node -v v0.10.36. We need 0.12.x. @billonahill comments addressed.. @billonahill any more feedback?. note to team: this PR is merged into master, but we will also incorporate it in 0.14.6. I'll do that.. LGTM.. any idea why? @billonahill @kramasamy . any idea? @billonahill @kramasamy . will remove. and rename. . why include heron-core (heron-dist) inside of client? internal build rule does not include it in client package? https://github.com/twitter/heron/blob/master/scripts/packages/BUILD#L254. @kramasamy Good to know. Thanks!. @huijunw because of Bazel's own issue, we have to write a separate BUILD file for CentOS 5 (and cannot reuse the one for Mac). This issue does not happen on CentOS 7. Since we are migrating to CentOS 7, we can remove these two historical files. . https://github.com/twitter/heron/pull/1726/commits/4c46bb49aeee15225a0a16b6fec5dc14037b5ceb addressed https://github.com/twitter/heron/issues/1699. still want to talk to @billonahill about this.. @billonahill most reviews about Python programs are addressed. Working on tuning ShellUtils now.. @billonahill updated. We can discuss about the output in-person tomorrow (don't want to upload terminal recording clip since it contains internal service info). @mycFelix Looks good to me. I think putting ByteAmountUnit inside of ByteAmount might be better, but it's totally up to you. We can merge it if @ashvina gives a ship-it.. is there any update on this? seems we have very few things left to do for this PR? @mycFelix @billonahill . @huijunw yes, it will remain in TMaster. As for the URL path, do you have any suggestion? Is /topologies/stmgrsregsummary good?. updated. See https://github.com/twitter/heron/issues/1754 discussion. ping for review @billonahill @huijunw . I have updated. . can be combined into https://github.com/twitter/heron/issues/1595. NPE in CI. cc @nlu90, who is handling similar issues now. @billonahill it is too late, but since we haven't announced yet I can perform some git operations to get it in.... @billonahill done with the git surgery. we can get it in 0.14.6. @huijunw info is really an aggregation of all the informations.. @billonahill \nCurrent API endpoint topologies/executionstate gives you\n{\n  status: \"Running\",\n  release_username: \"rli\",\n  has_tmaster_location: true,\n  jobname: \"EX\",\n  release_version: \"rli/tm-stmgr-registration\",\n  submission_user: \"rli\",\n  submission_time: 1489523952,\n  viz: \"\",\n  has_scheduler_location: true,\n  has_physical_plan: true,\n  cluster: \"local\",\n  release_tag: \"\",\n  environ: \"default\",\n  role: \"rli\"\n}\nWe should split API endpoint topologies/executionstate into two endpoints:\n topologies/metadata:\n{\n  release_version: \"rli/tm-stmgr-registration\",\n  cluster: \"local\",\n  release_tag: \"\",\n  environ: \"default\",\n  submission_user: \"rli\",\n  release_username: \"rli\",\n  submission_time: 1489523952,\n  viz: \"\",\n  role: \"rli\",\n  jobname: \"EX\"\n}\n topologies/runtimestate\n{\n  has_tmaster_location: true,\n  stmgrs_reg_summary: {\n    registered_stmgrs: [\n      \"stmgr-1\",\n      \"stmgr-2\"\n    ],\n    absent_stmgrs: [ ]\n  },\n  has_scheduler_location: true,\n  has_physical_plan: true\n}\nIn topologies/runtimestate, we add stmgrs_reg_summary (to @billonahill I will change it to OO style later)\nAs for topologies/info, it is an aggregation of all the infos\n{\n  name: \"EX\",\n  tmaster_location: ...\n  physical_plan: ...\n  runtime_state: ...\n  scheduler_location: ...\n  logical_plan: ...\n  execution_state: ...\n  id: \"EX26810c0f-c992-4962-b01a-4c482609f8f8\",\n  metadata: {}\n}. Updated view of result of topologies/runtimestate\n{\n  has_tmaster_location: true,\n  stmgrs: {\n    stmgr-1: {\n      is_registered: true\n    },\n    stmgr-2: {\n      is_registered: true\n    }\n  },\n  has_scheduler_location: true,\n  has_physical_plan: true\n}. I got other tasks to do. Can you PR to temporarily disable it? See https://github.com/twitter/heron/pull/1700. i will look into it when I finish other tasks. cc @ashvina and @avflor who might care. @huijunwu do you also run our internal CI?. btw, please use mailing list for question next time.. @leonardgithub Mailing list is at https://groups.google.com/forum/#!forum/heron-users\n@kramasamy Can you have a look the Maven Jar issue @leonardgithub mentioned above?. @huijunw we support C++11. minor comments:\n1. We should upgrade to at least https://github.com/bazelbuild/bazel/releases/tag/0.4.3. The coverage report part looks helpful.\n\nWe probably need to special attention to make Bazel 0.4.x work in Twitter internal env.. My two cents:\nOne of the most common alerts we get is back pressure, and one of the most typical resolutions is to restart the containers that have trouble and wait for the topology to come back.\n\n\n\nBased on my on-call experience, this is not always true. I'd say around half of the time restarting does not resolve back-pressure. \nTo make this PR more convincing, we should at least have\n1.  detailed analysis of what auto-restart can really solve (e.g. bad host), and what auto-restart cannot solve (e.g. long de/serialization time). \n2. how often does each scenario happen? We'd better have statistics. Also, IMHO at least people who work on this PR should all have decent enough on-call experience.\n3. measure if auto-restart can help users in most of the scenarios. \nAlso I'm wondering if heron will fall into auto-restart loop hell if auto-restart does not solve the problem. Do we have any measure to prevent this from happening?\n. Most importantly, as far as I know, no one has tested this auto-restart feature in Twitter internal environment, not even for a long enough period of time (feel free to correct me if there is any)\nI cannot believe anyone here wants to merge this PR without testing it and measuring its performance thoroughly in Twitter internal production environment.. LGTM. Feel free to merge.. I was working with @kkdoon on this so I will review later.. Targeting 0.15.0+\nSorry for not reviewing this PR... Probably will do in May.. probably merge this in later this week, or next week.. I've been testing this PR internally this week, and it functions well. I am in favor of merging this soon.\nDoes anyone here have any more comment?. Merged. Will fix the BP name and CSS change in another PR.. @billonahill oh I misread and thought self.jvm_version was already added before. This is good implementation \ud83d\udc4d . putting this into 0.14.7 for OSS testing. Is it OK? @billonahill . \ud83d\udc4d . @congwang this is Java client of stmgr.. LGTM except a minor comment.. probably related https://github.com/twitter/heron/commit/1ff359b12bef4fa581cc64e0deb676ec54aa819a#diff-ae50e69b7e07fc33c6bb1ccfddc1c3f9. @nlu90 this is removed by @cckellogg a year ago: https://github.com/twitter/heron/commit/a412e4b81e51a0868a05d85da91ff4c048f20edf#diff-e49e2896d307ce58674cd955979d8912L303\nRemoving this from 0.14.7.. @cckellogg can you look into this? It is actually quite useful.. @nlu90 \nI don't think we can. Many jars in heron-core.tar.gz have the heron/api/src/java:api-java dependencies. This api-java dependency will bring in Kryo. \n\"//heron/metricsmgr/src/java:metricsmgr-unshaded\" -> \"//heron/api/src/java:api-java\"\n\"//heron/statemgrs/src/java:localfs-statemgr-unshaded\" -> \"//heron/api/src/java:api-java\"\n\"//heron/schedulers/src/java:mesos-scheduler-unshaded\" -> \"//heron/api/src/java:api-java\"\n\"//heron/instance/src/java:instance-java\" -> \"//heron/api/src/java:api-java\"\n\"//heron/schedulers/src/java:slurm-scheduler-unshaded\" -> \"//heron/api/src/java:api-java\"\n\"//heron/schedulers/src/java:marathon-scheduler-unshaded\" -> \"//heron/api/src/java:api-java\"\n\"//heron/packing/src/java:binpacking-packing-unshaded\" -> \"//heron/api/src/java:api-java\"\n\"//heron/scheduler-core/src/java:scheduler-unshaded\" -> \"//heron/api/src/java:api-java\"\n\"//heron/metricscachemgr/src/java:metricscachemgr-unshaded\" -> \"//heron/api/src/java:api-java\"\n\"//heron/metricsmgr/src/java:metricsmgr-java\" -> \"//heron/api/src/java:api-java\"\n\"//heron/statemgrs/src/java:statemgrs-java\" -> \"//heron/api/src/java:api-java\"\n\"//heron/schedulers/src/java:local-scheduler-unshaded\" -> \"//heron/api/src/java:api-java\"\n\"//heron/scheduler-core/src/java:scheduler-java\" -> \"//heron/api/src/java:api-java\"\n\"//heron/common/src/java:common-java\" -> \"//heron/api/src/java:api-java\"\n\"//heron/statemgrs/src/java:zookeeper-statemgr-unshaded\" -> \"//heron/api/src/java:api-java\"\n\"//heron/spi/src/java:heron-spi\" -> \"//heron/api/src/java:api-java\"\n\"//heron/packing/src/java:roundrobin-packing-unshaded\" -> \"//heron/api/src/java:api-java\"\n\"//heron/spi/src/java:utils-spi-java\" -> \"//heron/api/src/java:api-java\"\n\"//heron/spi/src/java:packing-spi-java\" -> \"//heron/api/src/java:api-java\"\nThis means we have to have both api-java-neverlink and api-java. This difference propagates all the way back so that we have many xxx-neverlink and xxx.. @billonahill @ttim @nlu90 \nthe \"xxx\" vs \"xxx_nl\" craziness is eliminated now. I confirmed that Kryo is not used in heron-core, which means Kryo should not be linked into any fatjar at anywhere. heron-api.jar and heron-storm.jar depends on it, but this Kryo dependency will only be specified via POM.\nLet's now talk about whether we should exclude backtype.storm from heron-storm.jar. It seems to me that we have not reached a conclusion. cc @kramasamy . I have cleaned up this PR quite a lot based on our discussion. Please review again.. Thanks. Please sign CLA first: https://engineering.twitter.com/opensource/cla. Also enable https://github.com/twitter/heron/pull/1773 ?. this is trivial so I'll just go ahead and merge it. possibly. can investigate later.. @pingzh If you are following the tutorial, you should have downloaded heron-client-install-0.14.7-darwin.sh. Why heron version is still at 0.14.6? \nAnyway, I installed this script on my machine and your command works for me.. @pingzh \nCan you run again with --verbose?\nAlso, heron-tools-install-0.14.7-darwin.sh is available now.. @pingzh what is heron version? Is it 0.14.7 now?. I cannot reproduce this issue so I cannot answer your question. . cannot reproduce so close for now. currently topologies are sorted by names when you open heron-ui. When user submits his topology, it would be better if he sees his topology at the very top of the ui list.. @billonahill \n\nbut not for others\n\nI will assume \"others\" mean people who open heron-ui but did not submit a new job. In this case, nothing has changed: he still has to use search bar or scroll up and down to find his job. I think in most cases, people open heron-ui because they just submitted a job.\nCan you give me an example of sorting topology by submission time by default being not helpful to a particular group of users? What particular benefits do we have from sorting topology by names?. ok then. Thanks for the effort! Merged.. Unit test failed. Log excerpt is here:\n18:09:27 FAIL: //heron/ckptmgr/tests/java:CheckpointManagerServerTest (see /var/lib/jenkins/.cache/bazel/_bazel_jenkins/fe81da57b028a6fdc8903fcaa04d2374/execroot/heron-mirror-release_7/bazel-out/local-opt/testlogs/heron/ckptmgr/tests/java/CheckpointManagerServerTest/test.log)\n18:09:27 ____From Testing //heron/ckptmgr/tests/java:CheckpointManagerServerTest:\n18:09:27 ==================== Test output for //heron/ckptmgr/tests/java:CheckpointManagerServerTest:\n18:09:27 JUnit4 Test Runner\n18:09:27 .May 15, 2017 1:09:25 AM com.twitter.heron.common.network.HeronClient start\n18:09:27 INFO: Connecting to endpoint: /127.0.0.1:59373\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.ckptmgr.CheckpointManagerServer onConnect\n18:09:27 INFO: Got a new connection from host:port /127.0.0.1:34527\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.ckptmgr.CheckpointManagerServer handleSaveInstanceStateRequest\n18:09:27 INFO: Got a save checkpoint request for checkpointId checkpoint_id  component component_name instance instance_id on connection /127.0.0.1:34527\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.ckptmgr.CheckpointManagerServer handleSaveInstanceStateRequest\n18:09:27 INFO: Saved checkpoint for checkpointId checkpoint_id compnent component_name instance instance_id\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.common.network.HeronServer stop\n18:09:27 INFO: Closing connected channel from client: /127.0.0.1:34527\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.common.network.HeronServer stop\n18:09:27 INFO: Removing all interest on channel: /127.0.0.1:34527\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.ckptmgr.CheckpointManagerServer onClose\n18:09:27 SEVERE: Got a connection close from remote socket address: /127.0.0.1:34527\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.common.network.SocketChannelHelper forceFlushWithBestEffort\n18:09:27 INFO: Forcing to flush data to socket with best effort.\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.common.network.HeronClient stop\n18:09:27 INFO: To stop the HeronClient.\n18:09:27 .May 15, 2017 1:09:25 AM com.twitter.heron.common.network.HeronClient start\n18:09:27 INFO: Connecting to endpoint: /127.0.0.1:40491\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.ckptmgr.CheckpointManagerServer onConnect\n18:09:27 INFO: Got a new connection from host:port /127.0.0.1:50254\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.ckptmgr.CheckpointManagerServer handleCleanStatefulCheckpointRequest\n18:09:27 INFO: Got a clean request from oldest_checkpoint_preserved: \"checkpoint_id\"\n18:09:27 clean_all_checkpoints: true\n18:09:27  running at host:port /127.0.0.1:50254\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.ckptmgr.CheckpointManagerServer handleCleanStatefulCheckpointRequest\n18:09:27 INFO: Dispose checkpoint successful\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.common.network.HeronServer stop\n18:09:27 INFO: Closing connected channel from client: /127.0.0.1:50254\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.common.network.HeronServer stop\n18:09:27 INFO: Removing all interest on channel: /127.0.0.1:50254\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.ckptmgr.CheckpointManagerServer onClose\n18:09:27 SEVERE: Got a connection close from remote socket address: /127.0.0.1:50254\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.common.network.SocketChannelHelper forceFlushWithBestEffort\n18:09:27 INFO: Forcing to flush data to socket with best effort.\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.common.network.HeronClient stop\n18:09:27 INFO: To stop the HeronClient.\n18:09:27 .May 15, 2017 1:09:25 AM com.twitter.heron.common.network.HeronClient start\n18:09:27 INFO: Connecting to endpoint: /127.0.0.1:44505\n18:09:27 May 15, 2017 1:09:25 AM com.twitter.heron.common.network.HeronClient handleConnect\n18:09:27 SEVERE: Failed to FinishConnect to endpoint: /127.0.0.1:44505\n18:09:27 java.net.ConnectException: Connection refused\n18:09:27    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n18:09:27    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n18:09:27    at com.twitter.heron.common.network.HeronClient.handleConnect(HeronClient.java:244)\n18:09:27    at com.twitter.heron.common.basics.NIOLooper.handleSelectedKeys(NIOLooper.java:115)\n18:09:27    at com.twitter.heron.common.basics.NIOLooper.access$000(NIOLooper.java:32)\n18:09:27    at com.twitter.heron.common.basics.NIOLooper$1.run(NIOLooper.java:45)\n18:09:27    at com.twitter.heron.common.basics.WakeableLooper.executeTasksOnWakeup(WakeableLooper.java:142)\n18:09:27    at com.twitter.heron.common.basics.WakeableLooper.runOnce(WakeableLooper.java:74)\n18:09:27    at com.twitter.heron.common.basics.WakeableLooper.loop(WakeableLooper.java:64)\n18:09:27    at com.twitter.heron.ckptmgr.CheckpointManagerServerTest$2.run(CheckpointManagerServerTest.java:276)\n18:09:27    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n18:09:27    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n18:09:27    at java.lang.Thread.run(Thread.java:748)\n18:09:27 \n18:09:27 Exception in thread \"pool-3-thread-2\" java.lang.AssertionError: Connection with server failed\n18:09:27    at org.junit.Assert.fail(Assert.java:88)\n18:09:27    at com.twitter.heron.ckptmgr.CheckpointManagerServerTest$SimpleCheckpointManagerClient.onConnect(CheckpointManagerServerTest.java:313)\n18:09:27    at com.twitter.heron.common.network.HeronClient$3.run(HeronClient.java:253)\n18:09:27    at com.twitter.heron.common.basics.WakeableLooper.triggerExpiredTimers(WakeableLooper.java:151)\n18:09:27    at com.twitter.heron.common.basics.WakeableLooper.runOnce(WakeableLooper.java:76)\n18:09:27    at com.twitter.heron.common.basics.WakeableLooper.loop(WakeableLooper.java:64)\n18:09:27    at com.twitter.heron.ckptmgr.CheckpointManagerServerTest$2.run(CheckpointManagerServerTest.java:276)\n18:09:27    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n18:09:27    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n18:09:27    at java.lang.Thread.run(Thread.java:748)\n18:09:27 E.May 15, 2017 1:09:27 AM com.twitter.heron.common.network.HeronClient start\n18:09:27 INFO: Connecting to endpoint: /127.0.0.1:50998\n18:09:27 May 15, 2017 1:09:27 AM com.twitter.heron.ckptmgr.CheckpointManagerServer onConnect\n18:09:27 INFO: Got a new connection from host:port /127.0.0.1:57664\n18:09:27 May 15, 2017 1:09:27 AM com.twitter.heron.ckptmgr.CheckpointManagerServer handleStMgrRegisterRequest\n18:09:27 INFO: Got a StMgr register request from stmgr_id running on host:port /127.0.0.1:57664\n18:09:27 May 15, 2017 1:09:27 AM com.twitter.heron.common.network.HeronServer stop\n18:09:27 INFO: Closing connected channel from client: /127.0.0.1:57664\n18:09:27 May 15, 2017 1:09:27 AM com.twitter.heron.common.network.HeronServer stop\n18:09:27 INFO: Removing all interest on channel: /127.0.0.1:57664\n18:09:27 May 15, 2017 1:09:27 AM com.twitter.heron.ckptmgr.CheckpointManagerServer onClose\n18:09:27 SEVERE: Got a connection close from remote socket address: /127.0.0.1:57664\n18:09:27 May 15, 2017 1:09:27 AM com.twitter.heron.common.network.SocketChannelHelper forceFlushWithBestEffort\n18:09:27 INFO: Forcing to flush data to socket with best effort.\n18:09:27 May 15, 2017 1:09:27 AM com.twitter.heron.common.network.HeronClient stop\n18:09:27 INFO: To stop the HeronClient.\n18:09:27 .May 15, 2017 1:09:27 AM com.twitter.heron.common.network.HeronClient start\n18:09:27 INFO: Connecting to endpoint: /127.0.0.1:42091\n18:09:27 May 15, 2017 1:09:27 AM com.twitter.heron.ckptmgr.CheckpointManagerServer onConnect\n18:09:27 INFO: Got a new connection from host:port /127.0.0.1:58053\n18:09:27 May 15, 2017 1:09:27 AM com.twitter.heron.ckptmgr.CheckpointManagerServer handleGetInstanceStateRequest\n18:09:27 INFO: Got a get checkpoint request for checkpointId checkpoint_id  component component_name taskId 1 on connection /127.0.0.1:58053\n18:09:27 May 15, 2017 1:09:27 AM com.twitter.heron.ckptmgr.CheckpointManagerServer handleGetInstanceStateRequest\n18:09:27 INFO: Get checkpoint successful for checkpointId checkpoint_id component component_name taskId 1\n18:09:27 May 15, 2017 1:09:27 AM com.twitter.heron.common.network.HeronServer stop\n18:09:27 INFO: Closing connected channel from client: /127.0.0.1:58053\n18:09:27 May 15, 2017 1:09:27 AM com.twitter.heron.common.network.HeronServer stop\n18:09:27 INFO: Removing all interest on channel: /127.0.0.1:58053\n18:09:27 May 15, 2017 1:09:27 AM com.twitter.heron.ckptmgr.CheckpointManagerServer onClose\n18:09:27 SEVERE: Got a connection close from remote socket address: /127.0.0.1:58053\n18:09:27 May 15, 2017 1:09:27 AM com.twitter.heron.common.network.SocketChannelHelper forceFlushWithBestEffort\n18:09:27 INFO: Forcing to flush data to socket with best effort.\n18:09:27 May 15, 2017 1:09:27 AM com.twitter.heron.common.network.HeronClient stop\n18:09:27 INFO: To stop the HeronClient.\n18:09:27 \n18:09:27 Time: 3.218\n18:09:27 There was 1 failure:\n18:09:27 1) testRegiseterTMaster(com.twitter.heron.ckptmgr.CheckpointManagerServerTest)\n18:09:27 java.lang.AssertionError\n18:09:27    at org.junit.Assert.fail(Assert.java:86)\n18:09:27    at org.junit.Assert.assertTrue(Assert.java:41)\n18:09:27    at org.junit.Assert.assertTrue(Assert.java:52)\n18:09:27    at com.twitter.heron.ckptmgr.CheckpointManagerServerTest.testRegiseterTMaster(CheckpointManagerServerTest.java:235)\n18:09:27    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n18:09:27    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n18:09:27    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n18:09:27    at java.lang.reflect.Method.invoke(Method.java:498)\n18:09:27    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)\n18:09:27    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n18:09:27    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)\n18:09:27    at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n18:09:27    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n18:09:27    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)\n18:09:27    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)\n18:09:27    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)\n18:09:27    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n18:09:27    at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)\n18:09:27    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)\n18:09:27    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)\n18:09:27    at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)\n18:09:27    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)\n18:09:27    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\n18:09:27    at org.junit.runners.ParentRunner.run(ParentRunner.java:309)\n18:09:27    at com.google.testing.junit.runner.junit4.CancellableRequestFactory$CancellableRunner.run(CancellableRequestFactory.java:89)\n18:09:27    at org.junit.runner.JUnitCore.run(JUnitCore.java:160)\n18:09:27    at org.junit.runner.JUnitCore.run(JUnitCore.java:138)\n18:09:27    at com.google.testing.junit.runner.junit4.JUnit4Runner.run(JUnit4Runner.java:114)\n18:09:27    at com.google.testing.junit.runner.BazelTestRunner.runTestsInSuite(BazelTestRunner.java:152)\n18:09:27    at com.google.testing.junit.runner.BazelTestRunner.main(BazelTestRunner.java:91)\n18:09:27 \n18:09:27 FAILURES!!!\n18:09:27 Tests run: 5,  Failures: 1\n18:09:27 \n18:09:27. ping @kramasamy. Can you give an approval?. closed via #1836 . Shall we also update comments here?. @maosongfu add something like \"will fail fast if Error is encountered\" at here. @maosongfu oh ok then.. Nice work.\n(actually I am happy to see at the first time that not properly piping I/O from subprocess could lead to such bad error...). @ajorgensen last comment: why this dependency?. please fix CI.. @ajorgensen \nI just read this PR again but I failed to understand why memory usage issue could happen. However, I observed that integration tests were running fine for first 3~4 tests, but started having problem afterwards.\nLet's try this:\n1. sleep 5 seconds after an integration test finishes\n2. run ps and dump its output on terminal. Just want to make sure processes spawned by test have exited.\n3. sleep another 5 seconds\n4. run next integration test\nWhat do you think?. @ajorgensen Agree. I am quite lost here since I did not expect such non-invasive change could blow up memory... We have to fix this issue to get this important PR merged.. @ajorgensen you are right. nice observation.. This reminds me that I may need to use thread as well in result.py to prevent deadlock.. CI is good now. Merged. Thanks! @ajorgensen . The issue this PR addresses is not really urgent, so we can leave this PR open until Monday when @billonahill can give his comments. Still targeting this in 0.14.7.2 though.. @huijunw I deleted your comment because it exposed too much Twitter internal information, e.g., hostname.. @huijunw You can just change 0.01 to 0.03. I did this for Python test for before.. ping @kramasamy. Any more comments?. @huijunwu LGTM. Feel free to merge this.. LGTM. I just realized you did not sign https://engineering.twitter.com/opensource/cla. Can you sign it?. @srkukarni we can cherry-pick to not include your patch for internal release. tbh any PR into master should be allowed if it is ok. Sounds good. Last thing I want to say: we are going to \"verify those codes\" and release them anyway. You cannot avoid this. It will be always be dangerous. It is just we don't have enough manpower to verify internally.. My intuition is that this issue requires us to do a comprehensive analysis of current design of Tracker API endpoints. For example, we need to figure out how to improve API so that thousands of API calls to Tracker can be batched into one API call. That said, I think this issue could not be solved in a short amount of time. However, this could be the most often complaint we receive from customers.. Unfortunately, this does not solve the root cause at all. What if user passes some argument with prefix heron? You will see the same error. And more annoyingly, this problem is not due to our mistake, not due to our users either.\nThe root cause is that Python 2.7 argparser module is full of crazy shit. Excuse me again for the foul language, but if you have experience with using other command line parsers in other languages, and you have ever read through the entire documentation of argparser APIs, you have no choice but wholeheartedly agree with me. Python 3 fixed this problem by allowing a (allow_abbrev=False) which can give you exact match. But this solution to me is like fixing a wall of shit by throwing at it more shit.\nI have no strong opinion with @kramasamy's suggestion of using --, as long as you can implement it and add comprehensive tests! Another way of solving this problem is here. Also, I prefer to solve this issue by not requesting changes from users' side (or you can say by not breaking Heron client's current behavior). Imagine users come to ask us \"hey my script does not work. what has happened?\", and you have to say \"we did the following breaking change and you need to change accordingly\"... \nThat said, -- solution is not as good as this one. Let us solve this issue as transparent to users as possible.. Also, your PR forgot to change all the places that access argument values. For example, here.. ok. whatever is fine to me. . @maosongfu @srkukarni I agree we need to fix failed unit tests. But please do not worry about our internal migration process since currently release is always cherry-picked. That said, we can always control what we want to put into our next release.. Also: CI.. Thanks. Merged!. are we good on this? @kramasamy . @jrcrawfo @kramasamy Is this the one you want to include in the next release? If so, please address comments asap.. @jrcrawfo merge with master to fix CI error\n@nlu90 can you review it? @kramasamy wants in next release.. Two comments:\n1. There is no unit test/integration test. Is this PR done?\n2. I think when this PR is ready to be merged, I will run an internal integration test on it.. @srkukarni Understood. Ping me when you are ready for an internal integration test.. This PR is quite large but I'll find some time today to review.. This PR is quite large but I'll find some time today to review.. This is not for 0.14.8. And please fix merge conflict. Also someone please review it.. LGTM. Please merge this soon.. would like to have these metrics back. It will help us debugging stream manager.. Merged. Thanks!!. @billonahill here is one way I come up with but it is not very elegant\ndo ./bin/heron-executor --help and you will see\n```\nusage: heron-executor [-h]\n                      shard topology_name topology_id topology_defn_file\n                      zknode zkroot tmaster_binary stmgr_binary\n                      metricsmgr_classpath instance_jvm_opts classpath\n                      master_port tmaster_controller_port tmaster_stats_port\n                      heron_internals_config_file component_rammap\n                      component_jvm_opts_in_base64 pkg_type topology_bin_file\n                      heron_java_home shell_port heron_shell_binary\n                      metricsmgr_port cluster role environ instance_classpath\n                      metrics_sinks_config_file scheduler_classpath\n                      scheduler_port python_instance_binary\n                      metricscachemgr_classpath metricscachemgr_masterport\n                      metricscachemgr_statsport\npositional arguments:\n  shard\n  topology_name\n  topology_id\n  topology_defn_file\n  zknode\n  zkroot\n  tmaster_binary\n  stmgr_binary\n  metricsmgr_classpath\n  instance_jvm_opts\n  classpath\n  master_port\n  tmaster_controller_port\n  tmaster_stats_port\n  heron_internals_config_file\n  component_rammap\n  component_jvm_opts_in_base64\n  pkg_type\n  topology_bin_file\n  heron_java_home\n  shell_port\n  heron_shell_binary\n  metricsmgr_port\n  cluster\n  role\n  environ\n  instance_classpath\n  metrics_sinks_config_file\n  scheduler_classpath\n  scheduler_port\n  python_instance_binary\n  metricscachemgr_classpath\n  metricscachemgr_masterport\n  metricscachemgr_statsport\n```\nWe can count the number of lines after \"positional arguments\".\nBetter solution could be refactoring heron-executor first and add a `heron-executor --arity so that we print the number of arguments it accepts.. @billonahill \n$ sed -n 's/command_to_start_executor = \\(.*\\)/\\1/p' < heron.aurora | tr ' ' '\\n' | grep -c \"{{\"\n35. LGTM. See https://github.com/twitter/heron/issues/1918. We should really break this long line into separate lines so that diff could make better sense.. CI won't check it so merge now.. sometimes we use it to benchmark in cluster so maybe we don't want to throttle it all the time?. ok. lgtm. So I checked our schedulers. Some of them does truncation using FileUtils.getBaseName, some of them invoke executorCommandArgs to invoke getBaseName indirectly. I think we should do truncation for all schedulers and do it once at one place only.. minor comment: copyright year should be 2017.. @maosongfu I think this is one is slightly different.. Also here, which is used by other schedulers indirectly.. also please fix style error in CI.. @srkukarni yes you are right.. LGTM.. lgtm.. seems 0.02ms in https://github.com/twitter/heron/pull/1736 is not enough. delta is 0.043866872787475586 here.. LGTM in general. I feel there is some security issues here. For examples, it seems that you can use ../../../../.. to access files else where. This is not allowed in Twitter internal environment since info sec team has program which keeps doing such HTTP request to scan for such security hole (we've seen HTTP request like ../../../../../../../../../../etc/passwd). And I guess if they find it they will ping our team. . ok sounds good. . LGTM. Please go ahead to merge it.. LGTM. Some other customers also reported that execute latency is becoming red.\nTargeting 0.14.9.1.. @billonahill if this looks good now, we can add more tests now.. have you tried locally, since we don't have unit test for heron-shell. Also, have you tried locally that it will work?. @huijunw don't forget to PR twitter internal repo. I searched around the web but found no official comment on overhead. Several blog posts say enabling heap profiling introduces little overhead. I have also tested on @ttim's large devel topology and I see no performance degradation. \nThis PR simply adds shell support. For tracker and UI support, we can do in another patch.. Any comment on this?. @srkukarni Agree. I am on PTO and will find time to do so. Also, exclamation topology is throttled now so I will try word count one instead.. lgtm.. Seems it is not correct to use sizeof here. We printed out memory pool stat using getSpaceUsed\nI0714 04:03:08.716549 63460 stmgr.cpp:131] N5heron5proto6system13HeronTupleSetE - nums: 1 bytes used: 2990914\nI0714 04:03:08.716555 63460 stmgr.cpp:131] N5heron5proto5stmgr19TupleStreamMessage2E - nums: 1 bytes used: 131063\nI0714 04:03:08.716559 63460 stmgr.cpp:131] N5heron5proto6system14HeronTupleSet2E - nums: 550 bytes used: 69140211\nI0714 04:03:08.716563 63460 stmgr.cpp:131] N5heron5proto5stmgr24StartBackPressureMessageE - nums: 1 bytes used: 274\nI0714 04:03:08.716567 63460 stmgr.cpp:131] N5heron5proto5stmgr23StopBackPressureMessageE - nums: 1 bytes used: 274\nI0714 04:03:08.716570 63460 stmgr.cpp:131] N5heron5proto5stmgr19StrMgrHelloResponseE - nums: 1 bytes used: 72\nTuples of type TupleStreamMessage2 are using more than 60MB (stream manager using around 400MB in total). We believe memory pool is not really limited because of sizeof. This means using sizeof is wrong and should be removed.\nHowever, notice that getSpaceUsed could be heavy, as said in protobuf computation.. I went through lots of discussion with @huijunw and @ttim (thanks them!). Here are some discussions result\n\n\nLooping through vector and sum all getSpaceUsed calls results are quite heavy. If we want to do a cap here, how can we determine if we should put it back or delete in a fast way? I was thinking to use a thread to calculate mempool size constantly, so when every time we can release, we don't need to loop over to sum message size. Yes, this idea is naive because i did not consider thread-safety, blah blah. \n\n\n50MB limit is probably not what we want here. As you can see, N5heron5proto5stmgr19TupleStreamMessage2E counts most of the usage. What we are doing here is equivalent to limit whole mempool to  around 50MB...\n\n\nWe managed to get a stacktrace. See it here. As we can see, add_data_tuple calls append, which calls reserve in string library, which calls _S_create. When we do heap profiling, we see _S_create is the largest source of heap memory usage. Putting this observation here if you are interested details of heap usage.\n\n\nThis observation leads to one point: Message object, which is being constantly put back and taken out from mempool, will keep growing because every time the number of tuples you put is more than ever before, we need to reserve more memory. But we believe it will stop at a certain size.. and to answer question why stmgr memory usage issue gets worse when we switch to 0.14.9.1 from 0.14.8. I think the reason seems to be https://github.com/twitter/heron/pull/1914 starts using mem pool more.. instead of POST, can we keep uniform to use GET? So URL should be http://localhost:53969/killexecutor?secret=ExclamationTopology8c04f42b-e5f4-43b7-9674-8546697c3820. @huijunw make sense. \nLGTM.. LGTM.. LGTM. By the way, I just checked heron-api.jar and noticed Google Guava shaded inside of it. We don't want to do that either.. @nlu90 I'm wrong. heron-storm.jar still has guava dependency\n\n. Sure. Let's merge this one now.. @huijunw https://developers.google.com/protocol-buffers/docs/reference/cpp/google.protobuf.message_lite#MessageLite.GetCachedSize.details. @huijunw we should use ByteSize instead of GetCachedSize. In fact, before https://github.com/twitter/heron/commit/557c9804af0c23952b3df7e29715f01e338884f8, we use ByteSize only.. @srkukarni can you explain why we use GetCachedSize? I think its API description is not very clear. Will it return the correct current byte size of a message?. @srkukarni makes sense. I was worried if we do not call ByteSize first we won't get correct value of GetCachedSize. Your proposal looks good. We need to carefully make sure ByteSize will be called at the very first. Then use GetCachedSize all the way until we release it.\nFor perf, we may need more time to fix the viz.\nThis proposal may help #2253 as well.. not necessary.. not really useful. I have a PR ready and will test it later.. should be closed via #2054 and an internal patch.. Clear does not release memory back to memory allocator. See documentation.\n\nClear() avoids freeing memory, assuming that any memory allocated to hold parts of the message will be needed again to hold the next message. If you actually want to free the memory used by a Message, you must delete it.\n\nbtw, @congwang and I just realized this on Friday. . This argument sounds strange to me. If this needs to be true all the time, how shall we change packing algorithm? The only way is to pass --config-property?. depends on #2061 #2066 . It seems that we are not going to reach an agreement here soon. I have asked @ttim to add --config-property support in tsar config.. LGTM. . @srkukarni Thanks. Add configuration variable takes time because there is circular dependency problems.. @srkukarni config-cxx -> network-cxx -> basics-cxx. Inside basics-cxx, mempool.h/cpp needs to use heron-internals-config-reader., which is in config-cxx. Thus circular dependency.. @srkukarni I have added a method to set limit of memory pool externally. We then set limit of memory pool in stream manager's Init. So we can avoid the circular dependency problem here.. @congwang I agree. This solution does not look nice to me either.\n@srkukarni @congwang I have a theory about the cause of \"memory leak\". If one does not call Clear on Message, used space in Message will not be zero'ed out. So next time we add tuple in it, Message thinks it does not have enough space and will enlarge the strings in itself. C++ STL will reserve extra space in string when you append (just like how it uses some heuristics to reserve extra space for vector). Thus we are always allocating extra memory here. \nIf my theory is correct, https://github.com/twitter/heron/pull/2057 should solve the issues, along with this PR.\nSee our heap leak check graph below and backtrace we get from core dump for details.\n\n. @srkukarni You are right. Before your PR, Clear is already called properly for all messages.. @srkukarni \nWe have a tentative implementation that we tested a bit on local environment. The patch is here. Does the implementation look OK?\nhttps://github.com/twitter/heron/compare/huijunw/testLocalZombie...objmagic:huijunw/testLocalZombie?expand=1\nnew: https://github.com/twitter/heron/compare/master...objmagic:rli/zombie-host-no-test?expand=1. @srkukarni @kramasamy @huijunw let's move discussion to https://github.com/twitter/heron/pull/2480. @srkukarni your comments have been addressed . LGTM.. pretty sure we need some time to make Python3 default inside twtr. ok sounds good. TBR 4:30PM.. @huijunw I once mentioned this idea with @maosongfu and @ttim but they think (and I was convinced) that using in queue size is more accurate.. @srkukarni see #2224 for a fix. We are not making it configurable. Just check if a single data tuple could be larger than the size limit of a set of data tuple.. a related mailing list thread https://groups.google.com/forum/#!searchin/protobuf/reuse$20memory%7Csort:relevance/protobuf/TSzsFu3DPmQ/9BjFXpkdAc0J. Did some experiment:\nLOG(INFO) << _message->ByteSize();\n  LOG(INFO) << _message->SpaceUsed();\n  _message->Clear();\n  LOG(INFO) << _message->ByteSize();\n  LOG(INFO) << _message->SpaceUsed();\ngives us\nI0828 14:10:23.306396 2502742976 stmgr-server.cpp:437] 97\nI0828 14:10:23.306404 2502742976 stmgr-server.cpp:438] 832\nI0828 14:10:23.306416 2502742976 stmgr-server.cpp:440] 0\nI0828 14:10:23.306422 2502742976 stmgr-server.cpp:441] 832. @srkukarni It seems that we have no choice but to use SpaceUsed here. We can do some benchmark here to measure its performance.. Some benchmarking for SpaceUsed. Size is in bytes. It looks not bad @srkukarni \nI0828 15:22:59.759600 2502742976 stmgr-server.cpp:439] size: 1105717\nI0828 15:22:59.759625 2502742976 stmgr-server.cpp:440] time: 5 microseconds\nI0828 15:22:59.967779 2502742976 stmgr-server.cpp:439] size: 1250892\nI0828 15:22:59.967838 2502742976 stmgr-server.cpp:440] time: 10 microseconds\nI0828 15:23:00.128224 2502742976 stmgr-server.cpp:439] size: 1250892\nI0828 15:23:00.128247 2502742976 stmgr-server.cpp:440] time: 5 microseconds\nI0828 15:23:00.290628 2502742976 stmgr-server.cpp:439] size: 1250892\nI0828 15:23:00.290647 2502742976 stmgr-server.cpp:440] time: 4 microseconds\nI0828 15:23:00.485448 2502742976 stmgr-server.cpp:439] size: 2150892\nI0828 15:23:00.485477 2502742976 stmgr-server.cpp:440] time: 6 microseconds\nI0828 15:23:00.642523 2502742976 stmgr-server.cpp:439] size: 2150892\nI0828 15:23:00.642571 2502742976 stmgr-server.cpp:440] time: 5 microseconds\nI0828 15:23:00.818881 2502742976 stmgr-server.cpp:439] size: 2150892\nI0828 15:23:00.818902 2502742976 stmgr-server.cpp:440] time: 5 microseconds\nI0828 15:23:00.970676 2502742976 stmgr-server.cpp:439] size: 3100892\nI0828 15:23:00.970695 2502742976 stmgr-server.cpp:440] time: 4 microseconds\nI0828 15:23:01.183621 2502742976 stmgr-server.cpp:439] size: 4000892\nI0828 15:23:01.183640 2502742976 stmgr-server.cpp:440] time: 3 microseconds\nI0828 15:23:01.350108 2502742976 stmgr-server.cpp:439] size: 4000892\nI0828 15:23:01.350129 2502742976 stmgr-server.cpp:440] time: 4 microseconds\nI0828 15:23:01.512719 2502742976 stmgr-server.cpp:439] size: 4000892\nI0828 15:23:01.512740 2502742976 stmgr-server.cpp:440] time: 4 microseconds\nI0828 15:23:01.721442 2502742976 stmgr-server.cpp:439] size: 4000892\nI0828 15:23:01.721462 2502742976 stmgr-server.cpp:440] time: 3 microseconds\nI0828 15:23:01.896715 2502742976 stmgr-server.cpp:439] size: 4000892\nI0828 15:23:01.896742 2502742976 stmgr-server.cpp:440] time: 7 microseconds\nI0828 15:23:02.010160 2502742976 stmgr-server.cpp:439] size: 4000892\nI0828 15:23:02.010188 2502742976 stmgr-server.cpp:440] time: 6 microseconds\nI0828 15:23:02.057590 2502742976 stmgr-server.cpp:439] size: 4000892\nI0828 15:23:02.057623 2502742976 stmgr-server.cpp:440] time: 6 microseconds\nI0828 15:23:02.272822 2502742976 stmgr-server.cpp:439] size: 4000892\nI0828 15:23:02.272848 2502742976 stmgr-server.cpp:440] time: 6 microseconds\nI0828 15:23:02.437445 2502742976 stmgr-server.cpp:439] size: 4000892\nI0828 15:23:02.437464 2502742976 stmgr-server.cpp:440] time: 4 microseconds\nI0828 15:23:02.597956 2502742976 stmgr-server.cpp:439] size: 4000892\nI0828 15:23:02.598012 2502742976 stmgr-server.cpp:440] time: 8 microseconds. word count, parallelism=20\nStmgr CPU user time doubled, Data Tuples from Instances dropped ~25%.\nWe need to fix https://github.com/twitter/heron/issues/1908 first to see more metrics.. @srkukarni @huijunw wanted to add this because when a topology just started, it seems physical plan message was so large that instance rejected it because of this limit.\nThis limit was defined here: https://github.com/twitter/heron/pull/2118/files\nA solution we made internally at Twitter is to increase this value to 64Mbytes\n. still needs some work... whitespace separator is gone.. tmaster unittest being flaky. Twitter internal test passed.. resolved in latest Heron topologies (>0.15.0). closed via #2312 . @srkukarni others directly or indirectly use SchedulerUtils.executorCommandArgs, which I have modified.. LGTM. Also looking forward to sending intra message via shared memory! Always want to see its implementation in real world code.. @maosongfu no. that's user's topology and we cannot experiment this on their topology. If I have time I can probably ask someone to help to test.. done. You forgot to update WORKSPACE file. cc @srkukarni . sorry i broke this branch when resolving the conflict... will try to restore it.. have you changed Python instance as well?. This rarely happens so I'd like to keep the current state and wait if we see more of this error.. LGTM.. ok.\n. Right, otherwise it is confusing to others\n. done, see #809 \n. maybe refactor to \n\"If it's not convenient to check the output and logs in intellij console, you can save them to a local file by choosing Run -> Edit Configurations.... as shown in the following image:\"\n. ({{< ref \"#stream-manager\" >}}) fails to work now\n. yes. let me check again...\n. I'm thinking maybe it's because we should first build and then make site? see ci.sh\n. ah, I forgot this. Let me add comment and submit a new PR.\n. understood. I can add self.get_argumemt_role() and use it\n. This was the first way of refactoring I tried. I believe this breaks consistency because these pieces of code use url_concat on optional parameters: https://github.com/twitter/heron/blob/master/heron/ui/src/python/handlers/access/heron.py#L324\n. I think this is indeed a better design. I'll make the change.\n. agree. I'll make the change.\n. It is because not only heron/ui depends on handler, heron/explorer will also depend on handler. We think it would be better if we move handler to heron/common. It might be confusing that heron/explorer depends on part of the code in heron/ui.\n. this is solved. see https://github.com/twitter/heron/pull/979/commits/022410fdc5f05e0f7f66a4604cfdac48aad676ab\n. fixed via https://github.com/twitter/heron/pull/979/commits/757878ab582d3ed57eff02fa850939f006cbad81\n. solved.\n. solved.\n. yes, but link checker will complain it is a broken link.\n. ok. this is better.\n. I'll try.\n. also I need to figure out why link checker did not complain http://localhost:8888 during CI.\n. sorry, my word usage is wrong! The code below has the right logic.\n. fixed in https://github.com/twitter/heron/pull/1029/commits/464d811fe90fe26878e024a44ce9bac26a8ad68a!\n. thanks for the suggestion. fixed in https://github.com/twitter/heron/pull/979/commits/882ac3fbfe21c1f4e2dcc2b32ccd6dd4c7460543\n. thanks. fix via https://github.com/twitter/heron/pull/979/commits/fe75a271eac45a53195045663c9161319c264295\n. what does \"is run\" mean?\n. according to https://github.com/twitter/heron/pull/973#issuecomment-230018371, use \"State Manager\"\n. fixed via https://github.com/twitter/heron/pull/989/commits/86963e51094e627dcd80710ceb9d1f38e140a5b8\n. resolved via https://github.com/twitter/heron/pull/989/commits/a8d40cdff47a85e9013328154ffda90d9b5ebd41\n. why reverting this file can make style checker work?\n. that's a good suggestion. will do.\n. moved utils.py to common/ so that cli and explorer can share it. See commit https://github.com/twitter/heron/pull/989/commits/0d5896073f28f27a8336cdbe428d549aceb08c2f\n. please remove duplicate Bash shebang...\n. I think it's trivial so I put it here: https://github.com/twitter/heron/issues/1072#issuecomment-232838919\n. please follow https://github.com/twitter/heron/pull/1086/files#diff-e6e2ba32ba0d8f08be1e918dc5d8f95aR64 \n. main does not return anything. Please add return 0 at the end of control flow in main.\n. Should this be argparser instead of argparse?\n. Maybe hrc_parse is a better module name?\n. remove commented out code\n. remove commented-out print\n. line too long\n. value, command, env = '', '', ''\n. please sort import order. \n. are the above three variables even used?\n. if m is not None:\n. remove unnecessary blank line\n. see my suggestion: https://github.com/twitter/heron/pull/1119#issuecomment-235355454 \n. why argparser module can be invoked as a Python program? If this is for test purpose, please do not run test manually and add test under heron/cli/tests to test this module.\n. I see no need to run this module as a separate Python program. Please remove this shebang. \n. remove unnecessary import\n. remove unnecessary import\n. Please use to surround the above two commands so that they are formatted properly\n. indent comment properly\n. unnecessary parenthesis\n. please use Log module underheron/commonby addingfrom heron.common.src.python.color import Log, then you can useLog.warn(...). No space surrounding assignment operator is required. Please writeprog='heron'likeformatter_class=SubcommandHelpFormatterbelow.\n. problem is that implementation of argument parser of each CLI tool is different (because they are written by different people). You cannot reuse thisversion.pyforheronandheron-explorer.\n. putting them intoutilsmodule underheron/commonlooks strange to me. I can remove theseversion.pyand put this simple logic of adding parser insidemain.py.\n. agree on this. I understand it's exhausting to fix issues caused by renaming, but please fix them.\n. Correct. See: [main.py#L204](https://github.com/twitter/heron/blob/master/heron/tracker/src/python/main.py#L204)\n.p,filters,expressionsare not used.\n. agree.--topology_jardefinitely confuses people who submit Python topology.--topology_binarysounds good to me, or we can add--topology_pexoption and group it with--topology_jaroption into a [mutually exclusive group](https://docs.python.org/2.7/library/argparse.html#mutual-exclusion)\n. Thanks for the documentation. Not related to your PR, but I want to say this function has always been quite flaky. Because of the way this function uses to getpath,\n1.heron-uicannot haveversionsubcommand (#1175)\n2.  In this PR, @taishi8117 moved this file into a new subfolder (which is OK) and he had to increment these hardcoded levels.\n. \"WARN\" is not necessary here.Log.warnwill add aWarnat the beginning.\n. clean up commented out code\n. clean up this print\n. maybe useLog.debug(traceback.format_exc())?\n. remove this comment\n. Please use formatter instead of string concatenation...\n. Change all string concatenation to formatter...\n. same. use formatter.\n. maybekey not in [constants.LIVE, constants.LATEST, constants.COUNTER]?\n. In Tornado application, if an exception is raised, it's just raised (stack trace could be printed out) but program will not terminate. If we doLog.error, the program continue execution and [return an emptyretvalue without metrics information](https://github.com/twitter/heron/pull/1169/files/77f9e4362294273e1bd8a2e51d6f3cea9c8f2556#diff-bb948157f88754a9d9b0b27a2df041dbR142), which is ok for the UI side.\n. maybe I'm just being picky, but please remove \"the\"...\n. we are not really using argumentssignumandframe.\n. do you meansignal_handlerappears both here and a file below? It is becauseheron-trackerneeds to additionally stopfilestatemanagerandzkstatemanager, butheron-uidoes not need to.\n.signal_handlerinheron-trackerhas to performapplication.stop(), butheron-uidoes not have to.\n. please rename toheronrc.sample. we don't want to see this file gets hidden.\n. please remove whitespace surrounding assignment operator\n. same as above\n. can you dodict_ns = dict(namespace)first? so you don't have to writedicteverywhere.\n. equivalent toif not command:. equivalent toif not app:. equivalent toif not env:. you're right. I didn't look carefully.\n. thanks. I misremembered. It should bevars(https://github.com/twitter/heron/blob/master/heron/explorer/src/python/main.py#L172)\n. Please see [set_verboseimplementation](https://github.com/twitter/heron/pull/1204/files/47421526742c3418cdc1b62b7b6216ce808c7f57#diff-d7d30dde4ab9b43859ead4b233f151a7R85). I agree this name is confusing. I can change toset_logging_level.\n. Right. We compare to see if the logger objectLoghas theDEBUGlevel set. If it is set, we add--verboseflag to invoke Java program.\n. this is basically about propagating logging level from Python executor down to Java scheduler.\n. Good suggestion but log module has the logger object calledLog`. There will be name clashing.\n\nOn Aug 2, 2016, at 12:05, Karthik Ramasamy notifications@github.com wrote:\nIn heron/explorer/src/python/main.py:\n\n@@ -28,7 +28,7 @@\n import heron.explorer.src.python.topologies as topologies\n import heron.common.src.python.utils.config as config\n import heron.explorer.src.python.version as version\n-from heron.common.src.python.color import Log\n+import heron.common.src.python.utils.log as log\ncan you import as Log rather than log?\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n. log module has a logger object Log and a function configure. I first get the logger object out (see: https://github.com/twitter/heron/pull/1204/files#diff-762d509cb715b724772c61e8aae7f089R33) and then call log.configure under main function.\n. thanks. will fix.\n. thanks. will fix.\n. because in this class, no verbose value has been set. I just put false here. cc @ashvina do you have any suggestion?\n. if it's hard to propagate verbose flag to YARN scheduler, I'm not sure what to do. Maybe just put false here?\n. this should be Log.debug\n. @kramasamy Do you think it's fine to hardcode this file location? I'm OK with this and just want to know your opinion.\n. There is no verbose option in HeronExecutorTask's field.\n. no verbose option in HeronTaskConfiguration\n. I'll appreciate if you can add a verbose option in YARN scheduler.\n. Should we use logging for all the do_print and print? Do we need to add functionalities in log.py? Maybe we can improve #1204 and merge #1204 first, and finally use log.py in this PR?\n. please use formatter\n. maybe \"Invalid\" instead of \"Unrecognized\"? I'm not quite sure on this.\n. @ashvina Fantastic! We can merge #1208 first and make change on this PR accordingly.\n. OK.\n. Agree. Maybe you can refer to how I did this in explorer?\n. Do you mean user can specify which rc file to be used on command-line? If so, please add a test for this.\n. Log.debug\n. Log.error\n. at least in this file, you can do from import heron.common.src.python.utils.log import Log since log module is not used elsewhere.\n. If RC file does not exist, we should silently fall back to normal argument parser. That said, should we use Log.debug here?\n. I am not sure if you can call it argparser.py since it only contains HeronRCArgumentParser. Should we rename it to, for example, rcargparser.py? \n. That's a good point. Please do it.\nOn Aug 5, 2016, at 08:20, Prabhu Inbarajan notifications@github.com wrote:\nIn heron/comon/src/python/argparser.py:\n\n@@ -0,0 +1,196 @@\n+# Copyright 2016 Twitter. All rights reserved.\nI am wondering if we should also name the class to be more generic , say HeronArgumentParser instead of making it specific to HeronRC. this also provides the fallback behavior in case the RC file is not found.\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. right. I think Log.error(\"File %s does not exist\", effective_rc) is better.\n. Agree. This keeps consistency and should be fine.\n. fixed via https://github.com/twitter/heron/pull/1226/commits/c0b26aa8e841acce182ac804cb07f1cb5b237e74\n. fixed via https://github.com/twitter/heron/pull/1226/commits/c0b26aa8e841acce182ac804cb07f1cb5b237e74\n. Please use @abstractmethod decorator.\n. also wondering... we don't have to even follow the naming convention from Factory method of Java\n. 1. is used to log...\n2. It accepts an optional argument which specifies logging level\n. start a new line before \"Warning\"\n. maybe use NOT rather than NOT?\n. does not exist\n. is used to\n. same as https://github.com/twitter/heron/pull/1231/files#r74167181\n. just \"Currently, ... is still experimental\" should be fine...\n. Why use \"Moreover\" here?\n. can you use ordered or unordered list instead of \"either ... or ...\" here?\n. not really important, but to be consistent, 'S' should be in lower case\n. same, lower case (forgive my being picky...)\n. joining a list of one element?\n. also, better rename it to cmd. args does not make sense to me.\n. Not necessary because main execution failed. Not really being a troll, but a user can mistakenly submit a PEX even without if __name__ = \"__main__\". It's also possible that this PEX has some module importing problem. In this case, user still will see error message about main...\n\nAlso, the if topology_class_name == '-' looks good to me for now, but I feel somehow this hack could be shaky under some circumstances.\n. should be traceback.format_exc()). Fix all the same errors if you think there are more.\n. how about Log.error(\"Error when loading a topology: %s\", str(ex))\n. better to rename testsJarPath to something else\n. \"su orting\"?\n. what's the point of doing this? Can you explain why?\n. \ud83d\udc4d  this is easier to read. \n. looks fine to me. \ud83d\udc4d \n. em.. why switching order here?\n. better to add a line Log.error('Unknown subcommand: %s' % command)\n. do_print is changed to Log. See: https://github.com/twitter/heron/pull/1252/files\nRebasing against master should fix this.\n. maybe passing runners to create_parser? For example, do kill.create_parser(subparsers, self.runners) and inside kill module associates string kill with a lambda function like the following:\nPython\nrunners['kill'] = lambda *args: run(*args)\nBack in main, refactor run to:\ndef run(command, *rest_args):\n    try:\n        return self.runners[command](command, *rest_args)\n    except:\n        Log.error('Unknown subcommand: %s' % command)\n        return 1\n. agree...\n. keys() is not necessary here. Feel free to ignore since this is trivial...\n. we should specify license somewhere? Is this related to #1259 ?\n\n. logging.info(\"Killing topology: %s\", splitcmd)\nand again down below\nlogging.info(splitcmd)\n. umm, to keep uniformity, should we surround assignment operator using whitespace?\n. should these be Log.debug?\n. oh I forgot heron-executor logs to a file. Then I think it's ok to do Log.info here.\n. Why mention \"Hadoop projects\" here?\n. fixed via: https://github.com/twitter/heron/pull/1336/commits/18b6dd1011f8d5234686afe4814b837378595afd#diff-409c349551fd1e7ad3611d8544571cffR148\n. Thanks!!!\nfixed via https://github.com/twitter/heron/pull/1336/commits/18b6dd1011f8d5234686afe4814b837378595afd#diff-b5454de1597b4cda2547ba9c6ca12541R125\n. right. will do.\n. update via https://github.com/twitter/heron/pull/1336/commits/8ddd534675296dfe2032edd6bbf7b0d2c5650bc8. Can you have a look?\n. can we use Log here?\n. oops. py_metrics\n. maybe use Log.debug(traceback.format_exc(ex))?\n. still camel case\n. Good question. Since self.tunnel is only set at this line and I tried a meaningless command to open a subprocess, the Popen still gave me back an object instead of a None, I think not checking None should be fine here.\n. python\nimport subprocess\np = subprocess.Popen(['ssh', 'twitter.com']) # should fail\np is still an object instead of a None.\n. but I think we can add this check just to be safe...\n. why change 6 to 7 here?\n. Since you removed this part, can you use parse_known_args as in heron/tools/explorer/src/python/main.py#L172, and check if there is any unknown args?\n. I don't understand this comment. Can you explain?\n. since we didn't write any help message for any argument, it doesn't really matter if we'll print help message here...\n. Thanks for pointing out. Agreed.\n. Agree. If @dmarchand thinks it's worth the effort, feel free to refer to https://docs.python.org/dev/library/argparse.html#action\n. the subdir is not streamparse now.\n. I see lots of occurrences of streamparse. Is this PR up-to-date?\n. complicated to explain but in short --- because it is built with zip_safe=False\n. I am thinking any tool in the future that uses zip_safe=False can reuse this method, and naming this method in this way then makes sense.\n. actually, i realized heron-explorer was moved to the same directory as heron. This means this method is no longer needed.\n. right. will remove it\n. I briefly mentioned it here and I am not sure how to have a better wording here, so I came up with a variable name hoping it is self-explaining...\n. removed via https://github.com/twitter/heron/pull/1507/commits/e72fe571568deba0aed16582d8367e10a48b4e2e\n. Can we return False and True, instead of strings...?\n. ok\n. resolved via https://github.com/twitter/heron/pull/1541/commits/0238865309396146ecb897aac5eec9260b7015d8\n. resolved via https://github.com/twitter/heron/pull/1541/commits/593f35ad033a133938057f381844285804be373a\n. resolved via https://github.com/twitter/heron/pull/1541/commits/dc185672ac8d8b9bd773ecf9246c4a32ef6a3bc0\n. duplicate except ... as e?\n. how about using try .. except .. finally clause? https://docs.python.org/2.7/tutorial/errors.html#defining-clean-up-actions\n. \"for convenience\" of what? i don't quite follow here...\n. \ud83d\udc4d \n. I created a symlink called CONTRIBUTING.md at root directory pointing to website/content/docs/contributors/community.md. I don't know why GitHub diff displays something like that.\n. we cannot link to site. We have to put a CONTRIBUTING.md (symlink fine) under root.\n. Agree. Will do!. what do you mean by \"this logic\"? And I agree we can have a super-class of those exceptions.. just let exception be thrown out since it won't be part of the expected workflow.. Divide two longs could yield 0, which I guess is not what we want to see...?. what's the usage of this function since the one below looks perfect?. should be the last comment so bear with me...\nDo we also need a decreaseBy, maybe it's useful when we scale down?. ok \ud83d\udc4d . forgot this! will do. forgot to refactor this part. UploaderException is already created.. No. And I checked this part of the code. Whether we can connect to the tunnel is independent from connectionString.  We will use connectionString later. a tuple of format (return_code, stdout, stderr) is fine. But I feel execute is the module that actually executes the Java command, and get the return code, stdout, and stderr from it. It makes more sense to also dumps out the message and stop there. Only return code will be propagated.\nPushing this logic one level above means duplicated code that exercises the same logic here.. 1. main.py will not output any extra thing after the module it calls finishes executing. This makes sure the message other modules print out will be the last message. \n\nAs for \"logging consistency\", can you explain a bit more? . What kind of prefix text? logging or comment?. I am testing if the exception will be thrown out. If no exception is thrown out, the control flow reaches fail, which will fail the test.\n\nAnd this SubmitterMainTest shows the problem I mentioned days ago. Inside the function testSubmitTopology, we are testing multiple statements to see if they throw exceptions. If we want to use Test(expected=xxxException.class), we need to split this testSubmitTopology into multiple sub-tests (and duplicate the code that sets up the test).. oh I see.... I'm not sure how to send all this all the way back to main.py. Looking at run in submit.py, we expect this run function to return a boolean value.. I see. Seems related to https://github.com/twitter/heron/pull/1571#discussion_r90493084. I'll think about how to design \"a response object\".. discussed internally and will only include stderr if retcode != 0 && retcode < 100. agree. and this is important for https://github.com/twitter/heron/issues/1462#issuecomment-258550269. still TODO.. resolved in #1593. resolved in #1593. resolved in #1593. Yes. this run function still needs improvement, which I will do after I figured out how to resolve the issue @billonahill asked in https://github.com/twitter/heron/pull/1571#discussion_r90506972. This was not addressed in https://github.com/twitter/heron/pull/1593. Here is the submit.py in #1593. Basically, main.py calls submit.run and will exit (nothing else will be printed out after submit.run). That said, main is at the top level and submit.run is at the second level. Do we even have to eliminate this \"log-and-return-null|false\" pattern at second level? It seems ok to me that submit.run has this \"log and return\" pattern, or maybe I failed to understand your comment, or maybe I already have addressed your comment? \nstill kind of confused on this issue.. resolved in #1593. resolved in #1593. This is fixed. I refactored down to LaunchRunner.java in #1593. However, notice that I did not not go even deeper down to setPackingPlan, setExecutionState, etc because this anti-pattern is deeply nested. Not sure if we can stop at the newly if-null-then-throw-exception pattern \ud83d\ude05. these tests are fixed in #1593. See: SubmitterMainTest.java. I did not and put URI info inside of the exception message (For example, see Hdfsuploader.java). Any argument why catcher needs to access name?. > For all of these exception messages, assume they get caught at the highest level, up in the python main and rendered to the user without any of the surrounding context that we have when reading the code.\nvery good suggestion! I followed this suggestion in #1593, but I'm sure there are parts I did not do perfectly. Feel free to point out.\nTemporarily mark this one as resolved.. resolved in #1593 . resolved in #1593. See response.py and SubmitterMain.java. the code that renders response based on status code has been improved a lot in response.py in #1593, which I am sure it needs more comments.... resolved in #1593 . resolved in #1593. See response.py. resolved in #1593. See HdfsUploader.java. resolved in #1593. See Hdfsuploader.java.. that means we need to change to Callable<Void> here. And we have to return a null at the very end.. this will be encapsulated later into a response object. Since we have different types of responses, there is no way to know which type we want at this stage.. Yes, I am not satisfied with these two names. HeronError and InvokationError are fine (note that we have Python topology so JavaMainError is not a good name...). it subclasses Response to have defn_file in its constructor. as discussed, will address in another PR.. resolved. resolved. resolved. resolved. resolved. resolved. fixed at https://github.com/twitter/heron/pull/1571/commits/25444541d998af57e00a4de6d02a9abc80a5a35f#diff-1043de1ec8f8e4c0194d4e6035e654cfR331. resolved. refactored so that LaunchRunner does not implement Callable. opened an issue related to https://github.com/twitter/heron/issues/1596. . fixed via https://github.com/twitter/heron/pull/1571/commits/8964cfc28fb18001c6c856eaa6125dc809d41393. fixed via https://github.com/twitter/heron/pull/1571/commits/8964cfc28fb18001c6c856eaa6125dc809d41393. not necessarily uploaderClass. Instantiation of statemgrClass and launcherClass could also fail.\nI'll just remove exception from TopologySubmissionException constructor here.. or we can use three try...catch clause here.... dup of https://github.com/twitter/heron/pull/1571#discussion_r91195786. used three try...catch clause: https://github.com/twitter/heron/pull/1571/commits/12a41b1836f0b799a30387f7f8602d2e35ca9a30 \nmark this one as resolved unless there is more question. sorry for merging this too fast. I'll do this.. actually, returning enum might not be what we want, since we need to put value as string into config. See: https://github.com/twitter/heron/pull/1599/files#diff-1043de1ec8f8e4c0194d4e6035e654cfL80. Yes. How about we assign 0 to Ok, 1 to InvocationError, and 100 for HeronError at https://github.com/twitter/heron/pull/1571/files/dfae5d760fc355bf5a562dad9e12be57f383fbf6#diff-a90956032bc120bbfaad04664668a46aR20. still some refactor left: push response object back to main.py so we can combine some logic.. I feel UploaderException means unexpected error that happens during uploading package, but I think it is fine to extend the meaning.... sure. msg is from stdout and detailed_msg is from stderr. Invocation error message resides in stderr, so we Log.error it.. will add documentation . good suggestion. will do.\nnot rendering msg here because InvocationError implies there should be no information in stdout. will add documentation.. where do you think is appropriate to put this? I am really bad at finding an appropriate class to add code\ud83d\ude14. i don't understand the assert part. Where and how should I assert this? Sometimes shelled-out program exits with > 0 && < 100, sometimes it could exit with >=100. . how about instantiation of state manager and launcher class? should we also create exception for them?. resolved. resolved. resolved. resolved. resolved. resolved via https://github.com/twitter/heron/pull/1571/commits/e15d3cb03b7d7edba71c2aec133bdbf212ec5abb. resolved via https://github.com/twitter/heron/pull/1571/commits/e15d3cb03b7d7edba71c2aec133bdbf212ec5abb. resolved via https://github.com/twitter/heron/pull/1571/commits/e15d3cb03b7d7edba71c2aec133bdbf212ec5abb. maybe, which I guess will timeout Travis or internal CI. resolved. I'll move to use LOG.log(Level.FINE, ..., e). what is the problem with javadocs?. mark as resolved.. @mycFelix any reason why using enum is not a good idea?. @mycFelix imo Bill has better arguments. I will submit a PR to use enum.. I am not sure if it will return a lowercase one or an uppercase one.. yes that will be easier. I realized that I am just cargo-culting.. oh shit. done via https://github.com/twitter/heron/pull/1571/commits/dc8ec7697539ecd27a854d6491ba9bd21198403c. how about packageFile, since this class is PackageType? filename is too general. @mycFelix Every one is welcomed to review.. I am not sure about using Preconditions.checkState. I don't see any particular benefit we could have. Besides, if we use it here, does that mean we should use it elsewhere? Does that mean we should use Guava as much as possible?. then accordingly, TopologySubmissionException should be SubmissionException?. yes. resolved. resolved. resolved. will do this in another PR. will do this in another PR. consistency is nice, but does that mean we should put statement Preconditions.checkNotNull... inside of a try...catch clause. If exception NullPointerException gets thrown out, we catch it and throw a TMasterException out instead? (and none of the checkNotNull accepts exception in arguments]. resolved via https://github.com/twitter/heron/pull/1618/commits/9379ed9cfdca138f46c635e3e836a81f260bcf1b. resolved via https://github.com/twitter/heron/pull/1618/commits/3e55954414684441e5172550ebda2a4697c89a0b. what does this mean? searched around but still couldn't find explanation.... yes. I forgot this when refactoring.. actually, the config value you got is the dry-run format that's going to be used. So, String here.. didn't see your comment below.. make sense. will do.. let's do it in #1622 . agree. will do in this PR or #1622 . right. I noticed many duplicated code in this PR and decide to leave that for #1622 . this function is used only in submit.py and update.py. Passing --dry-run to other subcommands will get command line parsing errors (assuming that bad .heronrc parser does not break things here...). in my opinion, when new return code class is added (for example, dry-run here), we use number larger than the current largest number (100). I am using 200 here. 130, 150, or other numbers larger than 100 is OK.. in your suggestion, HeronError is using number larger than 200 instead of 100, which means we need to modify all the codes that return 100.. not the case for help and version subcommands. I feel it is OK to let them return a boolean as always, instead of making them to return a response object. and heron help deactivate will not show anything about --dry-run.. I see. make sense. Dry-run code does not have to be a range, while invocation error code needs to be in a range. . actually how about \n```\nelif status_code == 100:\n  return Status.HeronError\nelif status_code == 200:\n  return Status.DryRun\nelse:\n  # some assertion failure. I see.. Leaving this at #1622. Mark this as resolved now.. unit test added: https://github.com/twitter/heron/pull/1629/files#diff-2fd9d07404811bde28fe7deae9abf3edR48. comment updated here: https://github.com/twitter/heron/pull/1629/files#diff-a90956032bc120bbfaad04664668a46aR29. resolved via https://github.com/twitter/heron/pull/1658. fixed. See https://github.com/twitter/heron/pull/1629/files#diff-3db76a95a638602566f48a8d69b39780R30 https://github.com/twitter/heron/pull/1629/files#diff-b068406a7b6af7b7b4433ec0b01aec01R28 https://github.com/twitter/heron/pull/1629/files#diff-d8f8ef57c1b89944ba0c7725e25ad2b6R28. fixed. see: https://github.com/twitter/heron/pull/1629/files#diff-5aa882aa5a656504af61600c3013d31b. done. see https://github.com/twitter/heron/pull/1629/files#diff-5aa882aa5a656504af61600c3013d31b. done. see https://github.com/twitter/heron/pull/1629/files#diff-d1d1365a41878729fde9e63ed4a847a9. fixed. See https://github.com/twitter/heron/pull/1629/files#diff-21f2217fff70a9489b1ab27923cc039e and https://github.com/twitter/heron/pull/1629/files#diff-b48f5f206bbbc8bf893c8ac3ef7a0bc5. done.. done. see: https://github.com/twitter/heron/pull/1629/files#diff-33e2d5d0d27172c9f4b557d15cc3a3f0R40. done. see https://github.com/twitter/heron/pull/1629/files#diff-a90956032bc120bbfaad04664668a46a. done via https://github.com/twitter/heron/pull/1629/files#diff-c3a12db5e886d4de25105d68c3e96929R124. @billonahill I don't understand. If this is the case, what output format doesString render()have? Table or raw, or anything else?. The reason of doing so is that later in #1622 (AbstractMain), I'd like to have a common abstract method likeprotected abstract String renderDryRunResponse(DryRunResponse)inAbstractMain`. \nSubmitterMain will then override the abstract with a method String renderDryRunResponse(SubmitDryRunResponse) and RuntimeManagerMain will have String renderDryRunResponse(UpdateDryRunResponse).\nIt is of course still OK to follow your suggestion, but I'd like to see naming here look more consistent (SubmitterMain renders SubmitDryRunResponse instead of DryRunResponse).. yeah I just realized this... will really mock here.... yeah I just realized this... will really mock here.... I was following the existing structure:\n\n. I was following https://github.com/twitter/heron/blob/master/heron/common/tests/java/com/twitter/heron/common/config/Constants.java. And include the test files like this.\nDo you have any example of what you just suggested?. @billonahill we have our custom rules java_tests, which forbids resources. See: https://github.com/twitter/heron/blob/master/tools/rules/java_tests.bzl. and I searched the codebase. No test is using the resources way. If we choose to do so, should we refactor all the tests that are doing data?. leaving this in https://github.com/twitter/heron/issues/1668. Should we add some documentation here? What is bindings here? I need to read aurora job create -h to know what it is. Considering this is an argument of an interface function, I think it is necessary to add some docs here.. when containerId is -1, the entire job will also be restarted according to here. renamed to resources.. I checked out the PackingTestUtils. I think the method it provides is not really flexible to build up the scenario I want to test. removed these two methods since I didn't really use it.. fixed via https://github.com/twitter/heron/pull/1629/commits/9cbe6df7c99b2c204cf5b2e8407305f02940795c. resolved. see: https://github.com/twitter/heron/pull/1629/files#diff-9d7c5b93dadaa33d2766bdfdad746d08\nminor comment: i have to mock the behavior of Context instead of config object since mocking config does not work... (NPE) (fixed). I've been trying to do a really mock test here. This is what I have done so far: https://github.com/twitter/heron/pull/1629/files#diff-581c3ba72d069451257c5910ad628e4eR216\nBut as discussed, I can't really work out a correct unit test because I keep encountering NPE. See the log. I was following your example at RuntimeManagerRunnerTest but it seems that it did not work in my case.... resolved via https://github.com/twitter/heron/pull/1629/commits/641f439aaa7246b8363b2d874bf85bbe0f917e07. thanks to @billonahill 's help, this is resolved.. I'd like to have only two components: exclaim1 and word. However, your code above will create six individual components: componentName-1, componentName-2, componentName-3, componentName-4, componentName-5, componentName-6.. forgot to remove this.. agree. will refactor.. resolved via https://github.com/twitter/heron/pull/1629/commits/addd480372c7c098930748733961e6237cf45f38. resolved via https://github.com/twitter/heron/pull/1629/commits/3be17b3309d59eb60b5326d4e37c0f9c8529d3b1. resolved via https://github.com/twitter/heron/pull/1629/commits/3be17b3309d59eb60b5326d4e37c0f9c8529d3b1. Thanks for pointing this out. Leaving this in https://github.com/twitter/heron/issues/1668. resolved. see: https://github.com/twitter/heron/pull/1629/files#diff-d46e068b4c18dd1146772b304b784859R456 and https://github.com/twitter/heron/pull/1629/files#diff-1043de1ec8f8e4c0194d4e6035e654cfR519. forgot this yesterday and resolved today.. see reply above https://github.com/twitter/heron/pull/1629#discussion_r97118115. mark this as resolved since I didn't touch this class eventually.. as discussed with @billonahill, we will tackle this in the current PR.. Thanks. Fixed via https://github.com/twitter/heron/pull/1629/commits/868a6cb36045a45defbb34a2bd289f9ae2c52ef5. Thanks. Fixed via https://github.com/twitter/heron/pull/1629/commits/868a6cb36045a45defbb34a2bd289f9ae2c52ef5. done via https://github.com/twitter/heron/pull/1629/commits/a20cfec70032091e95321d3bee783b17c43865c4!. done via https://github.com/twitter/heron/pull/1629/commits/4fe8a8f834e984ba6cc1e3eff71e5932186cbaee!. done via https://github.com/twitter/heron/pull/1629/commits/4fe8a8f834e984ba6cc1e3eff71e5932186cbaee!. done via https://github.com/twitter/heron/pull/1629/commits/17fb6beba1bded0c310022138cc032cdab3683ca. fixed via https://github.com/twitter/heron/pull/1629/commits/b92871cac5069d67164f3f84be88111843f8ad74. fixed via https://github.com/twitter/heron/pull/1629/commits/b92871cac5069d67164f3f84be88111843f8ad74. fixed via https://github.com/twitter/heron/pull/1629/commits/01745ff5dd56717998f16e5ffd9a017745e5873b. the generic array, type erasure, and casting stuff are quite annoying here, but I found the solution.... We can't use enum here since Java does not have such API. See: https://docs.oracle.com/javase/7/docs/api/java/io/PrintStream.html. However, I agree we can use the .name().\nAnd yes, I should allow some hours for reviewers to review. Sorry I've been slow and hurrying myself too much on dry-run.. This also drives me into total nuts. I will do this.\nP.S.: to change this behavior in IDE, go Preference -> Editor -> Copyright -> Copyright Profiles. i was attempting to use puppycrawl checkstyle to also check Python header, but failed. This is some leftover during refactoring and not needed now. will remove.. done https://github.com/twitter/heron/pull/1679/commits/927772d44cc601998c337ae2bac59732c2a176db. done https://github.com/twitter/heron/pull/1679/commits/927772d44cc601998c337ae2bac59732c2a176db. yes this is annoying. will fix when i get chance.. this looks useful https://aantron.github.io/better-enums/tutorial/Conversions.html#Strings and easy to include in, if we bother to use it at all.... just pointing out. i have no opinion since C++ language does not have the capability of allowing people to use enum reflection easily.. agree. Thanks. I didn't review carefully here. @billonahill do you have any particular reason to add this check?. what do you mean?. you can add a reply next time instead of making an inline comment.. what are the tokens and recursive expansion here...?. because I also want to test loading from InputStream here: https://github.com/twitter/heron/pull/1710/files/4f721caaa991ff8c3cc156ebab9ce9b294993720#diff-535d4127d634ce5fd544311ba8e7fb2cR63. is this TODO addressed?. @mycFelix Bill is on vacation so he may not have spare time to reply you. We can definitely add ByteAmountUnit (and as a enum class to keep things simple). Another approach is to make 2048 into 2048 * 1024. We sacrifice some clarity here but can avoid extending current APIs. Thoughts? @ashvina  . @mycFelix actually, now I think @ashvina's suggestion is better. It would be nice to add a enum class ByteAmountUnit in the same style of SystemConfigKey to expose MB and GB in a typed manner.. I'm confused here. Shouldn't DRIVER_MEMORY accept value of type int, according to reef?. because this variable is also used in other modules. . makes sense. will do.. I don't think you can just cast here...?. @mycFelix I now think I am not the right person to review this PR. You can think about Bill's review and wait for @ashvina's comment.. what is process string? do you mean the command? we already do info-level logging of command here. . if it is a directory, we remove the directory. If it is a temporary file, we remove the entire temporary directory that contains this temporary file.. > Does shutil.rmtree(os.path.dirname(cur_file)) fail if cur_file is a directory\nYes. See below for API documentation. For /usr/lib, you'll get /usr.\n\nos.path.dirname(path)\nReturn the directory name of pathname path. This is the first element of the pair returned by passing path to the function split().. most functions return list of responses because for some reason I don't know we can launch multiple topologies. help and version return a single response object.\n\nWe can just make them all to return a list of response objects.. no particular reason. I'll make it an instance method.. i don't understand. you mean log succ_context and err_context? why?. render could also render a list of responses so it should be static method.. The reason I implemented in this way is that all Java logging go to stderr and exception message/dry-run response go to stdout. That said, stderr always go before stdout (I am using always but not 100% sure this is the case). I may follow this answer to make some improvement.. i'm sticking with my original approach.. if there is no further question, I'll mark this as resolved.. mark as resolved.. This function needs very careful review... @billonahill . only internal uploader tool needs saves all redirected I/O using string builder.. you are right. we should use LOG here.. adding another signature actually means we will add three or four more signatures... https://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/utils/ShellUtils.java#L63. it will be started later. . this is used for internal uploader tool. if you --json to internal uploader tool, it will return a JSON result and we want to catch that. fixed.. fixed. fixed. fixed. fixed. fixed. fixed. fixed. https://github.com/twitter/heron/pull/1726/commits/ee0b4f2d4ca72eb0a133dcf67670399f50a17287. https://github.com/twitter/heron/pull/1726/commits/ee0b4f2d4ca72eb0a133dcf67670399f50a17287. https://github.com/twitter/heron/pull/1726/commits/ee0b4f2d4ca72eb0a133dcf67670399f50a17287. some leftover here?. If I understand correctly, using amount for argument name instead of bytes makes better sense? (100_MB means one hundreds megabytes. ). @congwang agree with @maosongfu. add id so when we grep it we can also see the id. ok... std::string(length_str)? Fails to compile using Clang (Apple LLVM version 8.0.0 (clang-800.0.38)).\nError message:\nERROR: .../heron/tmaster/src/cpp/BUILD:3:1: C++ compilation of rule '//heron/tmaster/src/cpp:tmaster-cxx' failed: cc_wrapper.sh failed: error executing command external/local_config_cc/cc_wrapper.sh -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -Wthread-safety -Wself-assign -fcolor-diagnostics -fno-omit-frame-pointer '-std=c++0x' -MD -MF ... (remaining 60 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nheron/tmaster/src/cpp/manager/stats-interface.cpp:165:46: error: no matching conversion for functional-style cast from 'std::ostringstream' (aka 'basic_ostringstream<char>') to 'std::string' (aka 'basic_string<char, char_traits<char>, allocator<char> >')\n  http_response->AddHeader(\"Content-Length\", std::string(length_str));. oh my mad. definitely typo. Also, shall we tell everyone there is grep -B? What if people don't know? Do you just tell them to RTFM? Just how hard is it to add a stream id? If you do not do it, I will do it in another PR.. heron-ui is using it. If necessary I will refactor heron-ui in another PR. In the meantime, let us keep this API here.. imo deprecating execution state could be done in another PR. Updated: https://github.com/twitter/heron/pull/1734/commits/ee1a73010632b39572e6c53c1299c41545e80637\nAlso see: https://github.com/twitter/heron/issues/1779. it is from here. Modified means there are instances added/removed from the container.. +1. you still need to parse its output. This could be flaky compared with a call of System.getProperty(\"java.version\") which is a stable API.. should it be * 1000?. shall we check if key is null before fetching value?. not used.. Not really, but since heron-storm.jar does not include backtype now. We can only use org.apache.storm. However, GlobalMetrics is not in org.apache.storm.\nActually, I have already explained this in this PR.. I have already explained this in the PR: \"Removed some unused protobuf build targets.\".. @nlu90 Timur is right. I am thinking com.twitter.xxx is just \"implementation\" in this context.. This sounds like a better move, but I feel like that needs more discussion, planning, and effort for implementation.. which one do you think we should keep? I explained why we need both here: https://github.com/twitter/heron/pull/1811#issuecomment-295926431. will fix.. @nlu90 what do you think? Do you remember the com.twitter.heron.shaded.do.not.use that I showed you during the stand-up yesterday?. I am not familiar with the heron <-> storm compatibility story here. But isn't it OK if our storm is a superset of their storm? By \"superset\" I mean including more metrics API like GlobalMetircs?. What if user accesses some APIs in com.twitter.heron.xxx? Is it OK? If we shade, user will see something like org.apache.storm.shaded.do.not.use.com.twitter.heron.xxx, and he will not use it either intentionally or accidentally for sure.. This change, along with the introduction of GlobalMetrics, is here because I changed to only include \"org/apache/**\" in storm lib. See: https://github.com/twitter/heron/pull/1811/files#diff-cf85344880ea3bb51b03100c97fb7a82R22\nThis means I have to drop any use of backtype in examples. This is related to https://github.com/twitter/heron/pull/1811#discussion_r112565244. How about api_deps_files_neverlink_kryo?. That's true. Let's hold off this.. @billonahill probably @nlu90 can give you a better answer? We think because this is not really used anywhere.. @kramasamy thanks. I think your word means it is OK to shade com.twitter.heron.xxx.. @billonahill @ttim btw, there is only one Kryo that is being used in this entire codebase: Kryo 3.0.3 (as shown in WORKSPACE file). If our backtype (storm < 1.0) is using Kryo 2.x, then it is confusing to me now.. actually, current backtype.storm is bundled with Kryo 3.0.3.. @kramasamy I see. If backtype is storm < 1.0 and org.apache.storm is storm >= 1.0, then we should not put them together in heron-storm.jar. This PR will only include org.apache.storm and will drop backtype.storm. cc @billonahill . resolved via https://github.com/twitter/heron/pull/1811/commits/85de0d9e67a98003a00e4d8cd5ee9eb21991e516. resolved via https://github.com/twitter/heron/pull/1811/commits/85de0d9e67a98003a00e4d8cd5ee9eb21991e516. I don't think there is need to exclude Guava dependency now (similarly, heron-storm.jar has Guava dependency that is impossible to remove. so I have to bundle and shade it). That's why I need to shade all com.google.*.. forgot to change this.. https://github.com/twitter/heron/pull/1811/commits/b27da030962bc4e1f8792547f4bc31b717e3bf43. https://github.com/twitter/heron/pull/1811/commits/440e82e00dc2eab925c3fe11fc3d3a53456c4c46. done here https://github.com/twitter/heron/pull/1811/commits/91ef2db266f5dd18c5164c07048ee05edea6ab64. https://github.com/twitter/heron/pull/1811/commits/09e11f03c8104d43e546001803a76c3d101d85a8. Because the dependency Kryo will be specified in POM file, instead of being bundled (and shaded) into fatjar. This is the same for open-sourced Heron. When you publish to Maven, you also want to publish this POM file.. fixed https://github.com/twitter/heron/pull/1811/commits/909b32b1be591a9286e5574fd1a7b96c4e0e5594. cc @billonahill . so where do you suggest to put it?. fixed via https://github.com/twitter/heron/pull/1811/commits/d08acae850c541d3177c0eb0b7df3b2f6f26326f. well, it is not necessary. I can revert this anyway.. better remove this.. @billonahill \n\nCan we use generics in the State API for better type safety? (Map<, >)\n\nOut of curiosity, what is the meaning of \"type safety\" in this sentence? In what way could user implement the State interface so that certain \"type safety\" could be lost?. @billonahill do you mean that <K extends Serializable> will simply put constraint on K, while Map<Serializable, Serializable> will cast key or value to Serializable and thus we will lose some type information about key or value?. @billonahill \n\nThis provides compile time checks in user code for their types\n\nSorry for bugging in this thread, but can you be more specific? Do you mean with K, V we can also provide compile-time type checks to make sure both that String is extending Serializable and that the type of key is extending K (String)?. PLOG?. can we use std::chrono::seconds::zero() here?. How about adding a bufsize=1 here for better performance?. I use while loop because of this issue. It seems that Python 2.7 fixed this issue but as you can see in the thread some people are still reporting seeing this issue in 2.7. @kkdoon ideally we should adjust CSS to make that column wider.. Just curious since I don't have a strong opinion here: in this kind of scenario, shall we use greaterThan API instead?. also ByteAmount here?. like other questions, shall we also do ByteAmount here? I assume you are following some principle here: what are converted to ByteAmount, what are converted. Can you elaborate a bit?. why? and what is \"this\"?. no, none of the subprocess will block in heron-shell. And even if it blocks, server will return a HTTP timeout. @kkdoon We can include this in next release if we can do the CSS fix today or tomorrow.. final int?. @srkukarni can we merge our PR first, you merge with master, and continue with your refactor?. yeah better to have prefix. there is an self.configs above.. \"exists\" -> \"exist\". remove this.. this. It is not used. Pretty sure it is some leftover when we move from close-sourced to open-sourced Heron.. At first I thought we don't need to copy all the env variables, but now I think we do need all env vars. For example, Java program could fetch environmental variables.. addressed in https://github.com/twitter/heron/pull/1983/commits/f18b0a417bb96f0b473db62eea39b12705a59f3f. Using string looked better at first. But when I was refactoring to use string, I realized the command string is too long and I have to break them into multiple lines, but I forgot to add a space at the end of some lines, which made me spend like 5 minutes comparing why assertion failed. I suggest we use list, which makes spacing clearer.\n\n. yes later. . https://github.com/twitter/heron/pull/1983/commits/4de4efbb19ade9fada1a6adbe6c9b6bd7e8cbfd7. Add newline at end of the file.. exit 1 here?. exit 1 early here?. jar tf gives you sorted result, but just in case I can add a sort here.. resolved via https://github.com/twitter/heron/pull/2023/commits/4e04b678ab612dd3b64ba100c993613c21ec1011. i see little possibility that we will need to remove in the future. And if we want to enable remove here, we may want to switch to use dict. find and remove a tuple in a list is quite awkward.. I was worried if doing this needs to pull in lots of dependencies from schedulers. But I can try though.. You are right. I was not clear in my message above: yes, I was worried about circular dependencies as well when reading BUILD files.. This is explained above: https://github.com/twitter/heron/pull/2073#issue-243279094. it is config driven. The initial value is 512, and will be overridden here: https://github.com/twitter/heron/pull/2073/files#diff-5ff5ed00c50afd2302f522a9181e50a1R90\nAs for why initial value cannot be read from config, see the discussion between Sanjeev and I about circular dependency.. @billonahill Updated here: https://github.com/twitter/heron/pull/2073/commits/01acca884a923b0fbe556e266586b8bd54019c76. oh you are right. this is typo.. ok, do you want me to add an if to see if exception is OutOfMemoryError?. lots of unnecessary newlines here. We need to test if we can build inside of Twitter environment. Inside Twitter, there is no access to outside network. This means we cannot fetch pex rules repo.. Sorry @kramasamy, can you clarify \"duplicated\" and \"deploy\" here? I can't follow the context.... @kramasamy Understood. Sounds good - but we need to set up mirror first. I will give a shipit now but we should make sure mirror is set up first (cc @huijunw since I'm on PTO.). libtensorflow?. Yes, we must follow existing POM files. The generated POM files must be the same one generated now.\nPlease read https://github.com/twitter/heron/pull/1811 if you want to know why.\nAnd why a C++ program to do this job? A Python program should do well.. I could not find ByteSizeLong API in generated code. Let me try again.... seems this is introduced in 3.1.0 and then deprecated in 3.4.0. also, ByteSize calculates size of serialized message, which seems will be slightly larger than actual size in memory.. so if the message is 60MB but only little part of it is used to hold the incoming message (because incoming message is small), we will not delete it because I believe serialized size is much smaller than 60MB.  i'm not sure about. \"Found exception when initializing state managers\". result symptom:. ",
    "lucperkins": "I'll add some of this material to the doc @objmagic is writing later, but for now I'll close this\n. I'm happy to pitch in on this but I wanted to wait to make sure that other\ndocumentation work isn't currently underway (as that can create a lot of\nmerge headaches). Let me know when you feel like it's safe to begin making\nchanges and I'll get these fixed quickly.\nOn Mon, May 23, 2016 at 9:56 AM Karthik Ramasamy notifications@github.com\nwrote:\n\n@lucperkins https://github.com/lucperkins and @billonahill\nhttps://github.com/billonahill - I was wondering if this is fixed. If\nyou any additional help, @bmhatfield https://github.com/bmhatfield can\npitch in?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/heron/issues/324#issuecomment-221030531\n. Totally agreed! I was just resolving the merge conflict for now. Should've made that more clear :smile:\n\nI'll push those changes later today.\n. @billonahill It's ready for review!\n. @billonahill My bad. I thought you meant that make assets and gulp build were redundant. Now there's a strict separation between make setup and make pages et al.\n. +1. This approach is much cleaner. No need for people working on docs or static assets to even know about Gulp.\n. A few general issues. First, why was this doc added when this doc already exists? Is it intended to replace that doc or supplement it? If the latter, there needs to be links to that doc in this doc and a clarification of what each doc accomplishes. If the former, where is the actual usage information for Heron UI? How do I deploy Heron UI? On which host and port does it run once it's deployed?\nThere's a lot of really great info here, but these things need to be clarified.\n. @saileshmittal Okay, that makes sense, but I think that two things need to happen here:\n1. The previous Heron UI runbook doc that I linked to needs to have any redundant material removed to make way for this doc\n2. This new doc needs to contain links to the runbook doc\n. +1\n. +1\n. +1\nOne thing to be a bit wary of in the future is letting the line length grow well beyond 80 characters. At some point we should go through and make sure that this is standardized, but we can slay that dragon another day.\n. +1\n. +1\n. +1\n. Yes, it has. Feel free to merge.\nOn Sun, May 22, 2016 at 9:33 AM Karthik Ramasamy notifications@github.com\nwrote:\n\n@lucperkins https://github.com/lucperkins - Has all the feedback been\naddressed?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/heron/pull/722#issuecomment-220841886\n. @objmagic My apologies. Not sure how that happened. I've re-added those lines to the script in the most recent commit.\n. To be honest, I'm not really sure that there's much we can do. The only way\nI can think of to remove all those errors would be to actually import the\nProtobuf Java library into the Heron codebase itself, and I'm not sure\nthat's really worth it. Right now there are Javadocs errors but until it\ncan be shown that they actually make a difference in readers' experience of\nthe docs I'm strongly inclined to think that it's of very minor importance.\nOn Sun, May 22, 2016 at 8:21 PM objmagic notifications@github.com wrote:\n@lucperkins https://github.com/lucperkins thanks, but it would be good\nif we can also figure out how to remove warnings emitted by javadocs.sh\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/heron/pull/722#issuecomment-220879885\n. @ashvina Could you let me know when this lands? I'd love to add documentation for this and also use this as potential material for the \"Custom Schedulers\" doc.\n. How are you viewing that page? Are you viewing it by running \"make serve\"?\n. @objmagic What do you mean by cannot be rendered? Do you mean that it isn't showing up or only that the syntax highlighting isn't working?\n. @objmagic @kramasamy Try experimenting with other Pygments themes. To select a different one, simply change the pygmentsStyle config in website/config.yaml. This is simply the one I selected when I was first working on the project and I have no problem with changing it. Just let me know if one ends up working better (in general) than others.\n. @objmagic @kramasamy I've tried this out with a variety of different Pygments themes and as far as I can tell YAML highlighting doesn't seem to work in any of them. I think this is something we'll need to come back to later.\n. @kramasamy Here are some screenshots for the current new aesthetic: https://www.dropbox.com/s/9xisamyjplgkqx3/heron-new-look.png?dl=0\nhttps://www.dropbox.com/s/iay0o8c5vqmd266/heron-docs-page.png?dl=0\n. I also took the liberty of adding a 404 page:\n\nhttps://www.dropbox.com/s/kikmxtnzxgpu5hl/heron-404.png?dl=0\n. I'll see what I can do re: issues 2 and 3. Re: issue 1, the collapsible\nnavigation bar is still there, but it only shows up on smaller screens (as\nit was previously). You can adjust your browser window to see the effect.\nOn Sun, May 22, 2016 at 10:11 PM cckellogg notifications@github.com wrote:\n\nOverall it looks fine but I have few suggestions.\n- The collapsible side navigation bar is gone this might be useful for\n  people reading on phones or tablets\n- The list text seem way too small on all of the documentation pages -\n  for example on \"Getting Started\" heron-client-install and\n  heron-tools-install are very small and hard to read\n- Can the tweets list be extended to stretch down close to the footer?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/heron/pull/732#issuecomment-220888702\n. That is definitely something that we would like to have eventually but\nunfortunately hasn't yet been implemented in any iteration of the site.\nOn Sun, May 22, 2016 at 10:27 PM cckellogg notifications@github.com wrote:\nI see that the main navigation on the top collapses as adjust my browser.\nWhen I click on Getting Started a side navigation now shows up but when I\nadjust my browser that side navigation disappears - I think it use to\ncollapse and appear at the top but I could be imaging things.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/heron/pull/732#issuecomment-220890109\n. @billonahill Here's an image of lists with the current CSS:\n\n\nTo be honest, I'm not sure how much smaller we should make that. As for making that fixed-width code styling, I agree 100% but want to cover all of that in a big, separate PR, because there are a lot of little fixes like that.\n. @billonahill Totally agree, but fortunately that screenshot you linked is from an earlier commit \ud83d\ude04\n. @objmagic Which version of Hugo are you using?\n$ hugo version\n. @objmagic Have you run pip install pygments? That isn't in the make setup steps on master but it's being added in #732.\n. +1\n. Hmmm. As far as I can tell that will break all the links to the Javadocs.\nWe're going to need to use Gulp or something else to get those files into\nthe static folder.\nOn Wed, May 25, 2016 at 8:25 AM Bill Graham notifications@github.com\nwrote:\n\n@lucperkins https://github.com/lucperkins the contents of public/* are\nwhat's published to the site so putting the javadocs in static/* means they\ndon't get published. I'm going to revert that change for now.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/heron/pull/750#issuecomment-221611016\n. @objmagic There was a really obscure Python issue caused by some new files moved into the directory. I'm hoping that this most recent commit fixes it.\n. @billonahill It broke my environment, too. These pip permissions issues are so thorny and almost completely inscrutable to me. My most-likely-suboptimal solution was to uninstall and reinstall pip and sudoize all the pip stuff in those scripts. I'll see if I can get some help from a Python-savvy engineer (which I am not).\n. @kramasamy What in particular needs to be documented that's not in this document? https://github.com/twitter/heron/blob/master/website/content/docs/contributors/custom-metrics-sink.md\n\nAre you looking for more material on the currently available sinks?\n. Oops, I accidentally wiped out my fork repo. Closing this for now but will re-open soon.\n. +1\n. +1\n. :+1:\n. @kramasamy Yes, this is still relevant, though not critical. It's meant to address pip issues that may arise when running make setup and to delete the linkchecker-out.csv file once the linkchecking process is finished (that file is superfluous once the linkchecker script is finished and should be removed so that it doesn't accidentally get committed to Git).\n. Awesome! This will come in handy majorly down the line. Major +1.\n. @kramasamy @taishi8117 @objmagic Yes, these changes are still valid. Please merge if the changes look okay.\n. @kramasamy Here's an example:\n\n. @kramasamy Actually, I'd recommend visiting the site on my fork: http://lucperkins.github.io/heron/docs/getting-started/\n. @billonahill Yeah, unfortunately my personal site is showing the changes in #835 at the moment.\n. As someone who worked on docs about things like implementing custom\nschedulers and metrics, it would've been immensely beneficial to be able to\nmove seamlessly between the locally running site and the Javadocs.\nAs for Hugo, it's not being flaky, it's just not built to do what we'd like\nit to without a workaround. I'm going to continue to look for a better\nsolution but I think this is an approach that's both beneficial to doc\nwriters and has no real drawbacks.\nOn Wed, May 25, 2016 at 8:00 PM objmagic notifications@github.com wrote:\n\nwhen you are on Heron introduction page and follow link leading you to\nAPI's javadoc, you'll get 404. So yes, the only way is to open\nwebsite/public/api/index.html and manually find the doc of class you want\nto read.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/heron/pull/798#issuecomment-221765555\n. @objmagic You could be right that there's some caching mechanism in place. I'll dig into some Hugo internals to see what's going on there. What I know for sure is that removing the static dir and running make site && make serve will not bring about the desired result, and that's the result I'm working toward.\n\n@billonahill I have no problem with symlinking or any other approach as long as I can clone, cd website && make site && make serve and be able to navigate between universes without issue. I'll explore this approach today and see if I can get it to go.\n. @billonahill So I tried out a symlink approach and bizarrely enough it works just fine and yet the Hugo server throws Symbolic links for directories not supported, apparently because Go's filesystem API is weird about symlinks. See if you can reproduce using these steps:\nbash\n$ cd website\n$ rm -rf public static\n$ make site\n$ make serve\n$ open http://localhost:1313/heron/api\nSo I suppose this can be seen as an improvement but it also produces a sub-optimal UX in its own right. I'll explore further.\n. @objmagic Sorry, I haven't done a rebase in a long time. I'll get it figured out.\n. @objmagic Yeah, I'm also having trouble rebasing. There really aren't a lot of changes here so I'll submit a new PR and close this.\n. +1\n. @objmagic @kramasamy I found the issue. The accordion relies on jQuery and the jQuery URL currently being used (http://code.jquery.com/jquery-2.2.1.min.js) is HTTP rather than HTTPS, and so GitHub pages is refusing to fetch that resource. That URL also works via HTTPS so I'll submit a PR to change that.\n. @objmagic @kramasamy See PR #828\n. @kramasamy Actually, hold off on shipping this. I'm realizing now that the script currently alphabetizes all of the config.yaml params, which we really don't want. I'll see if I can undo that behavior.\n. @billonahill How is this approach different from what you're proposing? This approach determines the current Heron version via Git tags and then updates the version value in the website config.\nWhether this script is called during the broader Heron release process is up to others, but as for the make site process this seems to do precisely what you're calling for.\n. @billonahill But the issue with that approach is that there's nothing to ensure that that happens. @kramasamy explicitly asked me to create a script that automates keeping the two in sync. To be honest, I don't really mind either way; I'm just taking cues from the project lead.\n. @objmagic Try a running version of the site with this change here: http://lucperkins.github.io/heron/docs/getting-started/\n. @saileshmittal Could you explain your issue a little bit further? What exactly is happening? \n. @saileshmittal I added a scrollTo line in sidebar.js to address this issue. Give it a try at the same link above. It's not perfect but I think that it does help.\n. @saileshmittal Unfortunately, there's not much that can be done about that.\n. @billonahill To be honest, I had reservations about the pull request model for this and my doubts have been confirmed. I tried to merge with upstream/gh-pages here and that generated 100+ merge conflicts, which makes sense because there will always and by necessity be merge conflicts on this branch because we're dealing with generated content, not source content. It's not clear that merging makes sense to begin with because there's nothing to be gained by merging with files that are meant to be simply obliterated when make site is run.\nAnd so I think that the current approach of force pushing the generated public folder to the upstream/gh-pages branch is the only viable approach.\n. @billonahill Done. I can see now that I misunderstood your initial request. Makes perfect sense.\n. I'm happy to write this up. @kramasamy, get in touch with me and let me know what information you'd like to see included.\n. @kramasamy @objmagic Fixed that missing space in the script and the URL I provided above should work now. Even if it doesn't, it's no big deal. I think this is ready to ship.\n. @maosongfu As you can see from the current version of this doc, the API versions aren't being properly inserted into the doc. This was an oversight on my part. I'll make this work properly in another PR tomorrow. Make sure that those changes get pushed before you push these changes.\n. I can only display one branch at a time on my personal fork, but I can update it tomorrow\n. Yes, I submitted the requested changes\nOn Fri, Jun 24, 2016 at 10:29 PM Karthik Ramasamy notifications@github.com\nwrote:\n\n@lucperkins https://github.com/lucperkins - any update on this?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/pull/911#issuecomment-228513278, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/ABc9oDrP3pLmg2R58GyYtXWnvgNiJ7WFks5qPLysgaJpZM4I00Id\n.\n. Because make linkcheck is called whenever you make the site, users will get errors if they don't have wget installed. I think that wget installation should be added to setup.sh.\n. I made some small, nitpicky comments. One more general thing that needs to happen, though, is that capitalization needs to be made consistent with the rest of this document. Terms like Metrics Manager and Stream Manager need to be capitalized. Consult the other parts of the document to determine what should be capitalized.\n. README is commonly used as a noun. I'm not sure that I really see the value of this change, personally.\n. @objmagic That most recent commit is a new build that takes your changes into account.\n. @objmagic Could you explain what the purpose of this PR is? Is this simply a new site build PR (as we've done in the past) or have you changed the underlying source as well?\n. @objmagic I'm not sure why that would be, but is that an issue? The heronstreaming.io URL resolves to HTTP.\n. @objmagic Oh, okay, I see what you mean now. Perhaps it would be best if I were to submit a new build PR instead. Would you be opposed to that?\n. I've performed a significant re-write of this section. Please re-submit with these changes:\n\n7. Debugging Java topologies\nThe jar containing the code for building the topology, along with the spout and bolt \ncode, is deployed in the containers. A Heron Instance is started in each container, with each\nHeron Instance responsible for running a bolt or a spout. One way to debug Java code is to write debug logs to the log files for tracking and debugging purposes.\nLogging is the preferred mode for debugging as it makes it easer to find issues in both the short and long term in the topology. If you want to perform step-by-step debugging of a JVM\nprocess, however, this can be achieved by enabling remote debugging for the Heron Instance.\nFollow these steps to enable remote debugging:\n1. Add Java options to enable debugging on all the Heron Instances that will be started. This can be achieved by adding the options \"agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n\". Here's an example:\njava\n   conf.setDebug(true);\n   conf.setMaxSpoutPending(10);\n   conf.put(Config.TOPOLOGY_WORKER_CHILDOPTS, \"-XX:+HeapDumpOnOutOfMemoryError\");\n   conf.setComponentJvmOptions(\"word\",\n       \"-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n\");\n   conf.setComponentJvmOptions(\"exclaim1\",\n        \"-agentlib:jdwp=transport=dt_socket,address=8888,server=y,suspend=n\");\n2. Use the steps as given in the tutorial to set up remote debugging in Eclipse.\nTo set up remote debugging with IntelliJ, see the remote debugging instructions .\n3. Once the topology is activated, start the debugger at localhost:{PORT} if in standalone local deployment or {IP}/{HOSTNAME}:{PORT} for multi-container remote deployment, and you will then be able to debug the code step by step.\n. @kramasamy @jrcrawfo This page has already been added to the docs:\nhttps://twitter.github.io/heron/docs/operators/deployment/schedulers/kubernetes/\nhttps://github.com/twitter/heron/blob/master/website/content/docs/operators/deployment/schedulers/kubernetes.md\nI'd recommend closing this PR.. The Kubernetes support in Heron has changed dramatically from the initial iteration and we now have documentation for that new setup here: https://twitter.github.io/heron/docs/operators/deployment/schedulers/kubernetes/.. I'm going to close the PR and propose a new one that simply removes all website publishing logic from the Travis CI jobs.\nAs it stands, the website isn't being published by the CI job anyway, so any website-related failures will cause builds to fail and any successful website builds won't go public anyway.. @objmagic Cool. Feel free to merge pending the CI run.. @billonahill The issue, though, is that the website-related stuff is adding a lot of extra time (several minutes to each build) and a lot of extra brittleness that is imposing frustration and other costs on engineers. I do agree that periodic checks are a good thing, but performing these actions on every build, across many thousands of builds a year, seems excessive. I'll see if I can come up with a better solution that both uses automation to keep the documentation up to our standards but at a more reasonable cost.. @kramasamy @billonahill That could work, too, Karthik. I'm doing some exploring today. I think I may have a solution that only does a basic site build (without static assets) and checks links. I'll submit a PR hopefully by EOD.. I'm working on it. The errors are pretty obscure to me.\nOn Fri, Jul 21, 2017 at 03:50 Karthik Ramasamy notifications@github.com\nwrote:\n\n@lucperkins https://github.com/lucperkins - some error in javadocs. Can\nyou please fix it so that we can check it in?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/pull/2090#issuecomment-316970903, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ABc9oEn_usfS_TIgVk5dYaAo1_I3nfLIks5sQIKPgaJpZM4Oeu0A\n.\n. @billonahill I've been discussing this with Ali. The intention, from both of our perspectivces, is absolutely to have a single Javadoc generation process (which could also be utilized by the website build process) and not multiple. I'll discuss further with him and see if there's a workable approach that can be included with this PR.. I'm going to cancel this PR and start from scratch. New PR forthcoming.. My understanding is that these docs are not currently displayed in the sidebar because they're either not fully baked or not currently up to date.\n\n@sanjeev @kramasamy Are these docs more or less up to date?. I'm going to close this for now. PR #2444 will add the Kubernetes doc (as well as the TOC reference) and future PRs will address the other schedulers.. I'm going to merge this for now and iterate this upcoming week, because there continue to be gaps.. @srkukarni This is ready for review. Unfortunately this needs to be a little bit quick and dirty while I finish up the Java DSL docs. But it'd be nice to get this out there to pass on to interested parties.. @ashvina Yep, just a work in progress \ud83d\ude04 This PR will eventually include practical docs for both Python and Java as well as a more full-fledged theoretical discussion of Dhalion. I'll be in touch with Karthik and Sanjeev and we can push forward on this when the time comes \ud83d\udc4d  Just wanted to make sure that people are aware that the work is underway.. I'm going to close this, as we're changing all of the terminology surrounding the new API from \"DSL\" to \"Streamlets\". I'm going to cancel this PR for now given its advanced age and start a new one from scratch.. @srkukarni I do see your point, but I also feel like this is very confusing as well: https://twitter.github.io/heron/docs/developers/java/effectively-once/#specifying-delivery-semantics. I'll close this for now but in the future we should try to be more aggressive about standardizing on terminology. The conceptual surface area of Heron is already quite intimidating for most users and we should do whatever we can to eliminate potential sources of confusion.. Somehow I ended up creating this PR from the wrong branch. Closing this and reopening.. Something like this:\njava\nbuilder.newKVSource(() -> new KeyValue<>(\"player1\", 1234))\n        .formattedLog((k, v) -> String.format(\"Player: %s. Score: %d\", k, v));. @srkukarni Now that I think about it, it seems like it'd be better to just use the consume operator for this. Like so:\njava\nbuilder.newKVSource(() -> new KeyValue<>(\"player1\", 1234))\n        .consume(s -> LOG.info(/* formatted string */));\nI don't think that providing a new operator is much of a win. Happy to close this if you agree.. @srkukarni I'm experimenting with this locally and I've noted a regression in that the developer can no longer use setName and setNumPartitions on either Streamlets or KVStreamlets. It looks like those methods have been somehow depublicized in this implementation.. @srkukarni Never mind. This was completely an error on my part. My experiments with this modified API have been 100% successful thus far.. @srkukarni I'm 100% in favor of not using a tuple for that. To me, one of the core benefits of the Streamlet API is that you can ditch tuples entirely.. @jerrypeng +1 to allowing for windowed operations for non-KV streamlets. I've tried this out locally and it works as expected. Just made one comment re: variable naming. Another thing I'd recommend is enabling the same thing for reduceByWindow. There doesn't seem to be a compelling reason not to allow for that same logic there.. @srkukarni Got it. If that's standard usage, I'd say go for it.. @srkukarni I do like the addition of a collections interface here, but I'm not sure how I feel about making Collection<T> get() the sole method for retrieving data from a source. The current T get() method makes a lot more sense for me when it comes to basically all of the use cases that I've worked with. What would be some use cases where the collections interface would be, in fact, required? I suppose batch processing might be an example, but even then it's not yet clear to me what the collections construct yields. I'm open to being convinced, especially because this is a new problem domain for me, but would like to hear more about the rationale.. @srkukarni Okay, as long as the simple SerializableGenerator option exists, I don't see any reason to oppose this. +1. @srkukarni Awesome! In the future, I think we should strive to offer this pattern for pretty much all user-facing constructors.. @nwangtw I took your advice and changed the serializer into an enum. Kryo is still set as the default. I'll continue to confer with project leads on that.. @erenavsarogullari Nice! I just noticed some of these spelling errors the other day. LGTM, but make sure to wait on approval from @srkukarni.. @srkukarni This is again ready for review. I've removed the Resources class but have left the ByteAmount class (this is an issue that I'll deal with soon, but it's beyond the scope of this PR).. @erenavsarogullari I can ask about that. I've manually restarted the build for now.. @erenavsarogullari Sounds good to me. Glad to have your help!. @joshfischer1108 Okay, I see the issue in CI. Run the following and then commit and push the resulting changes:\nbash\n$ rm -rf website/public\n$ git submodule update --init\nIn general, don't commit changes to that website/public folder, as the version control and publishing processes are separate.. @joshfischer1108 Please feel to get in touch (luc@streaml.io) about issues you've had getting set up, For now, though, this PR will not pass CI if there are changes to that folder.. @srkukarni At the moment, none of the links work because the site was generated with a different base URL. Those links will all be broken until I'm able to publish the site anew. Will do ASAP.. @joshfischer1108 Yes, this is a known issue. I'm updating the site as fast as I can.. Done\n. I'd suggest moving this into the Makefile so that make setup covers this. I think that's a bit cleaner.\n. As mentioned below, I'd like to see linkchecker installed here\n. For future reference, you can simply use --- instead of &mdash;\n. I did move it but I also made some changes (I essentially just made the script context independent by making all directories relative to the root using git rev-parse --show-toplevel in case we want to move the script again).\n. This is actually not an issue, as these configs are all case insensitive. I really wish this were more clean in the Hugo docs, though. Alas.\n. Include a link to the class definition in the Javadocs. The same goes for other class names.\n. I think the methods would be better as a list or table\n. As in the spouts doc, I think the methods are much more clear if they're in the form of a list or table rather than a set of sentences.\n. failure\n. This should be:\n```\ntitle: Heron Tracker REST API\n```\n. This should be:\n```\ntitle: Heron Tracker\n``\n. This image should be placed in the/assets/imgfolder, notsrc/img(the URL can stay the same here)\n. @billonahill The confusion surrounding that is my fault. With a previous Hugo config I had to use\u2014but now the same thing can be achieved using---`, so we should use that throughout.\n. What do you mean by \"internal\" here? Make sure to clarify because there are a lot of moving pieces at work in Heron.\n. I'm not sure how others feel about this, but it might be better to call this doc \"Intro to Heron Cluster Configuration.\" \"Internal\" is just a bit too vague for my tastes. Then, the first few sentences could be something like this:\n\"Heron clusters can be configured at two different levels:\n1. The system level --- System-level configurations apply to the whole Heron cluster rather than to any specific topology (e.g. logging configs).\n2. The component level --- Component-level configurations enable you to establish default configurations for different topology components.\nNeither system- nor component-level configurations can be overridden by topology developers.\"\n. This is related to #798. It's a proposed (and hopefully temporary) solution\nthat will enable us to use Javadocs locally and on the live site without\nissue.\nOn Wed, May 25, 2016 at 7:53 PM Bill Graham notifications@github.com\nwrote:\n\nIn website/scripts/javadocs.sh\nhttps://github.com/twitter/heron/pull/800#discussion_r64685454:\n\n-$JAVADOC $FLAGS -d $JAVADOC_OUTPUT_DIR $GEN_FILES $HERON_SRC_FILES $BACKTYPE_SRC_FILES $APACHE_SRC_FILES\n+$JAVADOC -quiet \\\n-  -windowtitle \"Heron Java API\" \\\n-  -doctitle \"The Heron Java API\" \\\n-  -overview $OVERVIEW_HTML_FILE \\\n-  -d $JAVADOC_OUTPUT_DIR $GEN_FILES $HERON_SRC_FILES $BACKTYPE_SRC_FILES $APACHE_SRC_FILES\n  +\n  +cp -rf $JAVADOC_OUTPUT_DIR $JAVADOC_STATIC_OUTPUT_DIR/api\n\nwhy do we need 2 copies of the javadocs in multiple folders?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/heron/pull/800/files/8fde8341a3f8f1df41b109d1558404d0e7b6e78d#r64685454\n. A few things:\n- s/Not/Note\n- s/zookeeper/ZooKeeper\n- s/readonly/read-only\n- \"without creating a /topologies node\" instead \"without creating /topologies node\"\n- s/rootpath/root path\n. @billonahill Done\n. A few things:\n- s/zookeeper/ZooKeeper\n- s/readonly/read-only\n- without creating a /topologies node (instead of just \"without creating /topologies node)\n- s/rootpath/root path\n. The result of that discussion was that \"you\" is okay in tutorial-style docs but not quite as welcome in things like REST API docs or Javadoc. In this case, I think it's appropriate.\n. Indent should be two spaces here\n. s/to/in\n. the Heron API\n. s/to/into\n. \"releases page\" should be a hyperlink to https://github.com/twitter/heron/releases\n. Remove \"named\" here (since the sentence starts with \"The name of...\"\n. Once you've downloaded the script, run it with the --user flag set\n. s/to/into\n. the Heron APIs\n. Remove \"just\"\n. Enable the --verbose flag\n. This needs to be wrapped in a bash ... block like the other shell commands below it\n. s/launches/launch\n. For code snippets in normal text you only need one backtick, not three. This header should look like this:\n\n```\nStep 3 - Create a bundle.jar for each heron-api and heron-storm artifact\n```\nMake sure to make those changes throughout.\n. There should be an empty line after headers like this. That improves readability dramatically.\n. Mac OS X\n. Names of buttons should be in bold face, like this:\nClick the **Upload Bundle** button...\n. Also, these items should be a list:\n``` markdown\nFor each artifact...:\n\nFrom the...\nThen click...\n``\n. \"Check to ensure that the local version of the Heron topology compiles by deleting your local~/.heronapi, updating the projectpom.xml` file, and pulling from Maven Central\"\n. In general, some of these steps don't feel like proper sentences. This one should be something more like this:\n\nGo to Maven's [search page](http://search.maven.org/), search for \"com.twitter.heron\", and verify that the proper version exists\n. That should look something like this:\n``\n1.{artifact-version}.jar`\netc.\n```\nIn general, any time something is a file name, directory name, etc., it should be in backticks.\n. You now have two step 3s and you need an empty line below this header\n. This needs to have a period at the end. There are some other sentences here that also don't end in periods.\n. 1. curl should be in backticks, i.e. curl\n2. I'd advise against using \"for example\" and \"such as\" in the same sentence. Instead, this sentence could be \"For example, if the binary is hosted in HDFS,...\"\n. hdfs should be in backticks, i.e. hdfs (same for curl)\n. the uploader.yaml config file\n. capitalize Aurora and add a comma after \"deployments\"\n. \"documentations\" isn't a widely used term. Instead, say \"can be found in the documentation for the HDFS and S3 uploaders\"\n. the heron.aurora config file\n. s/scheduler/schedulers\n. The YARN scheduler\n. the Apache REEF framework\n. In general, the use of articles needs to be enforced. It needs to be the YARN scheduler, a YARN scheduler, etc. Please make the appropriate changes throughout, the REEF framework, etc.\n. s/cli/CLI and provide a link to the \"Managing Heron Topologies\" document\n. All bulleted list items should end in a period if they're full sentences.\n. AM HA should be defined or a hyperlink to an external source should be added\n. This section is not rendering properly in your rendered docs. Did you make any changes since submitting this PR that may have caused that?\n. This should be:\nbash\n$ heron kill yarn AckingTopology\nThat goes for the other commands as well.\n. This should just be ### Log File Location (remove the ### after the heading)\n. Colon after this sentence, and also add an empty line between this and the ordered list items\n. the heron submit command\n. creates a .defn file\n. What do you mean by \"packed\" here?\n. The heron-executor process\n. Capitalize Heron here\n. on container 0\n. capitalize Heron\n. Remove comma\n. Gateway and Slave shouldn't be capitalized here as far as I know. Also, this should be \"the gateway thread and the slave thread. The gateway...\"\n. the slave thread\n. establishes\n. Should be either just \"Heron\" or \"a Heron cluster,\" not \"a Heron\"\n. on your machine\n. Yep. Done.. Okay, will change.. Sorry, forgot to add that the PR is a work in progress. @huijunw As far as I know, idempotency and effectively once correspond to different concerns. A topology can be idempotent and process tuples more than once, which is a problem if you have effectively once requirements.. Please note that this is a work in progress. Again, this is a work in progress. I'll ping you as soon as it's in reviewable shape.. Got it. Yep. Both of these files (along with two others) seem to be results of errant merges. I've taken care of this in my latest commit.. Done. Works for me either way. I'll shorten it to just streamlet.. This is just a WIP for now. I'll ping you when it's ready for review.. Got it. I'd suggest that we brainstorm some alternative names for identity. Maybe initialValue or something like that?. It probably is less efficient than a lot of other alternatives, but it's only happening every half second and I like that it's just a one liner, which keeps the code clean. We could do that, but there is a separate example topology that covers sources. Up to you.. Agreed. That would be very cool. We can iterate on these a lot in the coming weeks.. It\u2019s eventually logged in the consume operator. In this case I\u2019m using it mostly for debugging so it can be removed.. I conferred with @srkukarni on this and he agreed that the performance improvements associated with using Kryo are significant enough (with no apparent drawbacks) that Kryo should be the default. As for the multiple calls to useKryoSerializer, I feel that this is always a potential issue when using the builder pattern but I'll see if there's a way to prevent this.. Good idea. Done.. You\u2019re right. I was misidentifying the problem. Using KryoSerializer.class instead of instantiating a KryoSerializer seems to have fixed the issue. You can still set the serializer more than once but again, I believe this is an issue in all builder pattern classes.. This doesn't require any dependencies that aren't currently included with the examples. In fact, one of the Topology API examples relies on this as well: \nhttps://github.com/twitter/heron/blob/master/examples/src/java/com/twitter/heron/examples/api/ComponentJVMOptionsTopology.java#L29\nI'm happy to change this to use a raw byte amount, though.. Another possibility might be to move this into the heron-api library.. Actually wait, we can't do that because the basics lib is supposed to depend only on the JDK. Never mind.. I think it might be a bit premature to have a separate utils class for the streamlet API. I like this require function but I'd say let's keep it in the StreamletImpl class for now. If a handful of utility functions emerge then a util class will become necessary.. Related to my comment on the StreamletUtils class above, I'd say include this with the StreamletImplTest for now. We can create a separate test suite for streamlet utils at a future time when a separate utils class makes more sense. The test is a good idea, though.. This change is already included in an existing PR: https://github.com/twitter/heron/pull/2559/files#diff-59aa6c12ff571579c86e35a77cafe55eR95. I'd recommend leaving this change out of this PR to avoid conflict (I also would prefer for this enum to remain protected and restricted to the StreamletImpl class).. In general, I think that the changes suggested in this PR are already covered in #2559. But I do think these tests would be nice to have. Could you possibly re-submit this or a different PR with just the tests once #2559 is merged?. Yep, sounds good!. @kramasamy Awesome! Let me know how that turns out. FYI this is just a work in progress at the moment.. Could you rename this enum to StreamletNamePrefix? I think it makes more sense that way.. In this case I would say no, because you\u2019re not specifying a type per se but rather just providing a string. ",
    "vikkyrk": "Looks good to me.\n. :+1: \n. Need to look into 0.9 API. Will review over the weekend.\n. :+1: \n. :+1: \n. @ashvina By design SchedulerMain is supposed to be run as a separate process, in which case you wouldn't have access to any of its methods once launched. Are you trying to use SchedulerMain as a library?\n. @ashvina So I am guessing you have a separate main program that want to launch scheduler in the same process, is that correct? I guess we could make it more flexible for such cases to use Scheduler as a library, instead of going through main method. Let me know if that is what you are looking for.\n. Ok cool, I have assigned it to myself. Will keep this issue posted as I make progress\n. :+1: \n. :+1:  other than the comment\n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. Fixed comments and a bug\n. :+1: \n. :+1: \n. :+1: \n. https://github.com/twitter/heron/pull/406\n. For Aurora it happens on the server side, so we don't get these shell commands.\n. \ud83d\udc4d \n. Left a few comments, but looks good otherwise\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @maosongfu @billonahill Not sure, I just moved those files to a different directory. If you \"git log --follow BaseBatchBolt.java\" you will hit a different commit by @maosongfu. https://reviewboard.twitter.biz/r/360711/ . IMO it can be removed.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. Will fix all comments\n. Fixed\n. Fixed\n. Fixed\n. fixed\n. Actually this needs more investigation as to why it didn't work for me. Would like to know if it is just an issue for me or everyone else on localhost.   \"http://tw-mbp-vikasr:49791/....\" didn't work from the java code. (worked through browser). I changed to 'localhost' and it worked form the java code too.\n. fixed\n. fixed\n. Because parsing help options can also throw exception, but technically it won't because there are no required ones. Didn't want to add another try catch block and just rethrow exception.\n. Private methods need not have that style. \n. Private methods need not have that style\n. Can you add more comments as to what this script is doing?\n. http://docs.oracle.com/javase/7/docs/technotes/guides/logging/overview.html#a1.3 \n\"\" represents root logger which is inherited by all loggers.\n. Will fix\n. Fixed\n. fixed\n. Fixed\n. Fixed as much as I can. We will have to go another pass across all files and fix this \n. separate PR\n. separate PR will come\n. Fixing\n. fin.close can throw Exception, so we need to catch that as well. It will throw NPE if fin is not defined.\n. Fixed\n. just use this.collector = collector. Better than naming it aCollector. Same at other places\n. Just delete this lines?\n. build/c++11 is a general rule that includes many things. Don't think we can disable certain rules in that without changing the code.\n. Fixing...\n. ",
    "yshivakrishna": "I had created this pull request as I was having problems building using bazel  (I hadn't used _deploy suffix). So, I am not sure if this pull request is valid.\n. ",
    "cckellogg": ":+1: \n. :+1: \n. :+1: \n. Not needed anymore\n. :+1: \n. \ud83d\udc4d \n. uploading again\n. Overall it looks fine but I have few suggestions. \n- The collapsible side navigation bar is gone this might be useful for people reading on phones or tablets \n- The list text seem way too small on all of the documentation pages - for example on \"Getting Started\" heron-client-install and heron-tools-install are very small and hard to read\n- Can the tweets list be extended to stretch down close to the footer?\n. I see that the main navigation on the top collapses as adjust my browser. When I click on Getting Started a side navigation now shows up but when I adjust my browser that side navigation disappears - I think it use to collapse and appear at the top but I could be imagining things.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. Can we address the font colors in another patch? This was preexisting code and I'm not sure what it means either. Here the code for the colors (https://github.com/twitter/heron/blob/master/heron/tools/ui/resources/static/js/alltopologies.js) lines 20 - 31.. @jrcrawfo what do you think of this approach?. What about adding a flag in the config on whether or not to download the heron core into the docker container?. I added a pull request to compile and build a docker image for running heron. https://github.com/twitter/heron/pull/1934. . ship it. @jrcrawfo will you take a look at this when you get the chance?. Putting up a different pull request. The tornado web server will take care of that it. The following request \nhttp://localhost:9999/../../../../../../../../../../etc/passwd. will be converted to /etc/pass and then fail because there is no handler to handle that route. This works the same as the file FileDataHandler in the shell (https://github.com/twitter/heron/blob/master/heron/shell/src/python/handlers/filedatahandler.py). \ud83d\udc4d . \ud83d\udc4d . @kramasamy comments where? More javadocs?. There will be some future patches to combine everything - possibly into a connectors directory that will contain spouts and sink bolts.. @huijunw we will add a doc soon. The purpose of this is to add another way to submit topologies and to provide an option that makes it easier to control (submit/kill/etc..) topologies within a clusters (DCOS and kubernetes). For example, users must have their ssh keys on a node in the cluster so they can access zookeeper when submitting or killing. The api server would fix this issue.. \ud83d\udc4d . Looks good to me. We could have a second level that groups things by type connectors/pulsar, connectors/jdbc, etc... The source (spout) and sink (bolt) would be together in the second level in case they need to share common code. This is what other projects do as well. Thoughts?. Some places we are using bookkeeper and come places we are using dlog should we standardize on one in the code base for consistency?. @jrcrawfo what do you think of this change?. It is a little different and strange but I think we need an easier way to get started with examples. Currently, the user needs to know where the examples are installed and the fully qualified class name. This is a lot to ask to get started. I'm open to suggestions on how make this better.. #2310 . \n\n. I say we keep this file since it makes it easy to build custom images on your laptop for testing.\nthe ci-docker assumes certain file naming which is not correct if you use build-artifacts.sh. Thoughts?. If we build locally on the laptop those binaries will be for osx and not ubuntu or centos right?. Going to move this somewhere else.. This is different. The issue I was seeing earlier was with the RoundRobinPackingPlan. It would be helpful to have a better to have more meaningful error message.. What about using stateful sets instead? We would only have to make one rest call to start the topology then. Also, pods will have a predictable name too. The trick is to find the ordinal for the pod and then we can use that as the shard?. We have been using stateful sets for zookeeper for a while so I'm not concerned with the beta part. Also, replicasets are in beta too. What i like about the stateful sets is that it groups everything together (with replica sets we would have to create one for each container/pod) and they provides a predictable name for the pod. The only thing that changes in the heron command currently is the shard-id right?\nStateful sets will give each pod an ordinal and if one of those pods does down it will restart with the same ordinal. \nThoughts? \n. @jrcrawfo is this patch ready with the replica sets?. Will fix in another pr.. Do we need to add pod affinity so bookies are not scheduled on the same node?. Should we reduce the container padding too? Config.setContainerRamPadding. Can we add versions to the  js file names? \nAlso, there a few more dependencies:\n//cdnjs.cloudflare.com/ajax/libs/d3/3.4.11/d3.min.js\n//cdnjs.cloudflare.com/ajax/libs/d3-tip/0.6.3/d3-tip.min.js\nSee files:\nconfig.html\nexception.html\ntopologies.html\ntopology.html\n. Thank you for the pull request. This one https://github.com/twitter/heron/pull/2513 does the same thing and came in first so we are planning on merging this. Will this one work for you?. this file needs to be updated too (https://github.com/twitter/heron/blob/master/deploy/kubernetes/gcp/bookkeeper-apiserver.yaml). @jerrypeng will you review this too. What is the state of the topology? This will happen when the state is unknown. The tracker will return an empty logical and physical plan. The ui should probably be more graceful in this situation. . @jerrypeng will this effect nomad?. It's related to this where the client is setting the wrong http patch header.\nhttps://github.com/kubernetes-client/java/issues/127. Yes, I see them being passed to the open/prepare methods of the components.\n[2018-02-28 09:18:07 -0800] [STDOUT] stdout: open config\n:{topology.component.rammap=word:536870912,exclaim1:536870912, topology.team.environment=default, topology.container.disk=2147483648, topology.container.ram=1073741824, topology.enable.message.timeouts=true, topology.debug=true, topology.max.spout.pending=1000000000, topology.container.cpu=1.0, topology.name=acking, topology.team.name=chris, topology.component.parallelism=2, topology.stmgrs=2, topology.worker.childopts=-XX:+HeapDumpOnOutOfMemoryError, topic=ads/default/my-topic-spout, topology.reliability.mode=ATLEAST_ONCE, topology.message.timeout.secs=10}\n[2018-02-28 09:18:07 -0800] [STDOUT] stdout: prepare config\n:{topology.component.rammap=word:536870912,exclaim1:536870912, topology.team.environment=default, topology.container.disk=2147483648, topology.container.ram=1073741824, topology.enable.message.timeouts=true, topology.debug=true, topology.max.spout.pending=1000000000, topology.container.cpu=1.0, topology.name=acking, topology.team.name=chris, topology.component.parallelism=2, topology.stmgrs=2, topology.worker.childopts=-XX:+HeapDumpOnOutOfMemoryError, topic=ads/default/my-topic-bolt, topology.reliability.mode=ATLEAST_ONCE, topology.message.timeout.secs=10}\n. The use case for the tag is to provide more information and context to a topology's function. For example, a topology could be an analytics topology, data-ingestion topology, machine-learning topology, example topology etc... This tag can be surfaced in the ui and explorer to provide more context about the topology's purpose. Also, this could be used for metric aggregation across tags. For example, how much resources are my analytics topologies using. Thoughts?. @ashvina any suggestions how to implement this feature?. This would solve some of the use cases but not the metrics one. For example, in prometheus you can attach labels to metrics. Exposing the tag in the config would allow an operator or developer to group metrics by tag/category.. Will rethink the approach and address at a later time.. Will this break the explorer?. @sebastienpattyn93 what version for heron are you running? What is the yaml for apiserver deployment? It looks like the heron-apiserver is configured wrong.  I don't think there has been a release since the packaging has changed. This class does not exist org.apache.heron.uploader.dlog.DLUploader. If you change the org.apache to com.twitter it should work.. should we update all the tools templates to the new proxy urls too?\nhttps://github.com/apache/incubator-heron/blob/master/deploy/kubernetes/helm/templates/tools.yaml\nhttps://github.com/apache/incubator-heron/blob/master/deploy/kubernetes/minikube/tools.yaml\nhttps://github.com/apache/incubator-heron/blob/master/deploy/kubernetes/general/tools.yaml\n. why are you keeping self.dc?\n. Can we get rid of this?\n. What is a zone? Can that be named something better?\n. Can the optional string dc = 6 be removed?\n. remove comment out code\n. change name to cluster\n. should this be cluster?\n. remove all the smf1 and atla references\n. what is ya_parser? what is clargs can we name these better?\n. namespace is a weird name maybe arguments\n. should we have a constants/defaults file for these default values it will be easier to maintain\n. if this class is only going have static methods there would be a private constructor. Maybe there should be a TMasterClient Interface for sending commands then there can be a HttpTMasterClient implementation\n. This can be a List<> and you can just do List<> cmd = Arrays.asList()\n. This method can be remove and replaced with the following\nLOG.info(\"cmdline=\" + auroraCmd);\nreturn 0 == ShellUtils.runProcess(isVerbose,  auroraCmd.toArray(), new StringBuilder(), new StringBuilder());\n. final HttpURLConnection connection = \nIt's good practice to declare variables final if they will not be modified. It's makes the easier to read and understand.\n. I agree it seems strange to launch the task on a post operation. My assumption would be the task kill, activate, deactivate, etc would have already happened why a postAction is called. There should be methods added that are explicit in indication that a task is being launched. Maybe add onKill, onActivate, etc..\n. final List cmd = Arrays.asList();\n. move 249-252 into the initLogger(boolean isVerbose) then set the level in there\nfinal Level level = isVerbose ? Level.ALL : Level.INFO;\n. Same as the comment above consider using LOG.log(Level.FINE, \"string {0}\", param) to avoid unnecessary string concatenation. Same for instance in the files below.\n. Why is \"\" used in getLogger. According to the java doc (A name for the logger. This should be a dot-separated name and should normally be based on the package name or class name of the subsystem, such as java.net or javax.swing) \n. This has to be camel case PygmentsCodeFences (https://gohugo.io/extras/highlighting/#code-fences)\n. We should not be afraid to break consistency if we think it makes the code better and we can refactor that file going forward. I like Neng's suggestion and your first thought when you did the refactor.\n. do we need a way to get topologies by cluster and environment too? Should we have an internal method that each of the public ones call?\n_get_topologies(cluster, role = None, env = None)\n. Out of curiosity why are we changing the module naming.\n. I reverted this file and got it to work. For this patch can we revert this file and then try to improve it in another patch?\n. removing. how about build-heron-docker-image.sh?. fixing. We are planning on publishing images for different operating systems so we came up with the convention heron:heron-version-os-version. So we would publish a release image like this streamlio/heron:0.14.8-ubuntu14.04. What do you think?. Should we prefix these new args with 'sm' or something like that?. They are all checked exceptions and the initialize method does not support throwing an exception. I'm not sure what else to do, any ideas?. Will change all Object references to StorageObject. Can I get a version number for the Config or should I supply the version number from the StoredObject?. Will update that section of the javadoc and add an example.. I don't think adding a storage object version would be useful to the user.. Will update with the next commit.. done. done. ok. I wanted something different until we collapse it so it doesn't interfere with the tracker and ui default ports.. will remove. this should be BKDownloader.class instead of Downloader.class. typo in the key? should it be heron.uploader.bk? . the downloader extracts the tar.gz on the file. There is a test for this (ExtractorTests under heron/downloaders/tests). spelling mistake? heron. Is it possible to remove the guava dependency for downloader?. Yes that works.. I will update the name.. It is there from the old WebSink sink class. This provides an endpoint for the prometheus service to scrape metrics from.  I added this class so both the WebSink and PrometheusSink could share the http server code. Thoughts? . The metrics for prometheus need to be in a certain format for exporting (https://prometheus.io/docs/instrumenting/exposition_formats/). This why the http server is used. I think keeping it here makes it less invasive with the core code and less stress on the tmaster service. Thoughts?. maybe BaseStreamlet?. is there a better name? java methods usually don't have underscores. Maybe doBuild()? Make this protected too since it should never be called externally?. Should this be immutable? Also maybe add a static create(K key, V value) method for connivence? . why StreamletImpl and not just Streamlet?. Can this and all the ones below be KVStreamlets?. no left over from debugging. will remove. should this be removed too cp $SCRIPT_FILE $SCRIPT_OUT_FILE?. Does it make sense for the Builder interface to have a build() method on it so we don't have to cast here?. @huijunw do you have any more concerns? we would like to get this in. thanks.. I tried without the default on some of the examples and seems to not work. It seems like the default is needed.\nmsa/exclaim1/container_1_exclaim1_3:\n{execute-time-ns/word/default=4.36168882E8, __fail-count/word/default=0.0, __execute-latency/default=10412.989280683745, __process-latency/default=9611.879413660563, __fail-count/default=0.0, __process-latency/word/default=9611.879413660563, __execute-count/word/default=41887.0, __execute-latency/word/default=10412.989280683745, __ack-count/word/default=41887.0, __tuple-serialization-time-ns/default=9.5455816E7, __ack-count/default=41887.0, __execute-count/default=41887.0, __auto/selected_items=4.0, __out-queue-full-count=103.0, __emit-count/default=41887.0, __execute-time-ns/default=4.36168882E8}\nIn what cases would I not see default at the end? \nThe other option is i can go through all the metric names and if check to see if \"__execute-time-ns\" is in the name.\n. This will only delete the symlink and not any of the potential files/directories under it.. I will rename it to something better.. I wanted to separate the create/clean directory and the tar.gz file extraction. This decouples things for when a scheduler would like to use an installed version of the code. The extraction of the core.tar.gz is not needed but you had to pass those values in previous. Thoughts?. There is a Google Storage uploader too, should we add doc on the website for that?. let's update the image to be heron/heron: or heron/heron:latest. Let's update this to a heron image heron/heron:latest. should we add or Minikube here?\nA Kubernetes cluster with at least 3 nodes or Minikube. We should remove this because the prometheus sink will always flatten the metrics.. spelling mistake? should just be .js right?. Can we update the image here too heron/heron:latest?. The flag we set is called shard so i updated the variable name to match. We can probably do this better but that is probably a bigger change that we can address in another pr.. this should be ENV HERON_HOME /heron/heron-core/. we can remove line 31. Do we need this file? . should we make this a for loop so we fail after sometime?. Should there be a bookkeeper dependency in the chart?. This needs to be a string because we use an environment variable for the shard id for statefuel sets. Kubernetes needs to be able to send a string as the shard id. alignment is off. alignment looks off. alignment looks off. could metaData.get here ever return null?. Maybe use a util here? FileHelper.copy(in, path)?. Can we add some useful error messages here? There are some private helper methods (that could be refactored out) in the TopologyResources to help with this.. let's add a method that we can use below when forming the uri. I don't like setting a static variable here. \nprivate String getHostnameOrIp(Config config);\nAlso you can call config.getString(key, defaultValue) so we don't need the string util.\n. can we do a little refactoring and have method call getHostnameOrIp() so we are not setting a static variable like this. Then lines 119/120 becomes:\nString uri = String.format(\"http://%s:%s/api/v1/file/download/%s\", getHostnameOrIp(), getPort(), fileName);. I think we should not be setting static state here.\nfinal String downloadHostName = getDownloadHostName();\nif (StringUtil.isNotBlank(downloadHostName) {\n  return downloadHostName;\n}\nreturn hostname != null ? hostname : ip.toString();\nalso could ip ever be null here?\n. if these keys do not exist will false just be returned?. is this needed anymore?. Yes the heron_tools_bin_files it looks like the new one replaces it.. Should we add a static util method isNotEmpty()?. what about no short name or \"rc\"? \nAlso instead of doing a list you can do something like this\nfinal Option property = Option.builder(\"rc\")\n        .argName(\"comp:property=value\")\n        .numberOfArgs(2)\n        .valueSeparator()\n        .desc(\"use value for given property\")\n        .build();\nthen you can pass values like this:\n-rc word:prop1=value1 -rc word:prop2=value2\n. ",
    "maosongfu": ":+1: \n. :+1: \n. :+1: \n. Fixes #95\n. Fixes #95\n. Fixes: #140 \n. Fixes #116 \n. fixes: #154 \n. fixes: #165 #162 \n. Fixes: #342 \n. will review it tonight\n. :+1: \n. Fixes: #263 \n. :+1: \n. :+1: \n. Fixes: #205 \n. Fixs #145 \n. :+1: \n. :+1: \n. Fixes: #170 \n. Fixes: #162 \n. :+1: \n. :+1: \n. Fixes: #168 \n. +1 to @osgigeek \n. Fixes: #172 \n. :+1: \n. \"A minor typo in the log message: Existing should be Exiting?\" is fixed in: #177 \nException catching will be handled soon.\n. Fixes: #476 \n.  Note that heron.proto.api.Topology, included in PhysicalPlan, reflect the actual dynamic running state of the topology.\nQuery this value in state manager guarantees the correct running state of the topology.\nhttps://github.com/twitter/heron/blob/master/heron/proto/physical_plan.proto#L39\nhttps://github.com/twitter/heron/blob/master/heron/proto/topology.proto#L14\n. Tests done.\n. Fixes: #294 \n. Fixes: #201 \n. :+1: \n. Fixes: #627 \n. Fixes: #644 \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. :+1: \n. Updates:\n1. Uniform the scheduler abstraction: run NullScheduler (a dummy scheduler) in Aurora implementation\n2. Include NullLauncher and NullScheduler into building target.\n3. Uniform the path for activate/deactivate request.\n4. Remove unnecessary config.\n. #402 makes ISchedulerClient, allowing people to manage topology via uniform ISchedulerClient interface.\n. Hi,\nSimply saying, the topology is running fine. Congratulations!\nThe failure metric is a bit different from exceptions/errors; it is increased when topology invokes collector.fail(tuple) explicitly or timeout.\nThe topology you running, AckingTopology, https://github.com/twitter/heron/blob/master/heron/examples/src/java/com/twitter/heron/examples/AckingTopology.java#L87, will report failure via collector.fail(tuple) explicitly for half tuples. So the metric in fact is showing the expected running status.\n@kramasamy Should we use another sample topology in getting started documentation to avoid confusion? For instance, the ExclamationTopology\n. Yeah I met this issue a lot. And the workaround I did is to install them manually.\n. :+1: \n. Integrated test done.\nUnit tests will be added once enabling all unit tests for scheduler.\n. Hi,\nLet's understand the problems clearly first and then design the solution.\n1. Should we provide \"Ability to configure client classpath\"?\n   I think so, as @ashvina mentioned the issue. We need to provide a way to specify extra classpath for running scheduler.\n2. Is this classpath used only on client side?\n   @ashvina said yes, but I want to confirm with this. Implementation of IScheduler (In your case, the driver or application master part)will run on server side; does the classpath also need to be specified for the Implementation of IScheduler too? If so, current #272 is not enough; Launcher needs to get this classpath to correctly construct the command to start SchedulerMain, i.e. correctly set the value of classpath for SchedulerMain.\n3. How to provide \"Ability to configure client classpath\"?\n   #272 gets classpath argument from cli and pass it to execute java_class. \n   It is not a good solution: it requires people to specify this classpath every time they submit a topology, which is unneeded and annoying.\n   We should provide a config for extra_scheduler_class, (in this case, the default value can be hadoop related jars) and cli will pick up this config when trying to run scheduler related JVM process. Also, we can also provide --classpath option to override the default value to provide more flexibility.\n4. When providing an option to override the value, I don't think \"-classpath\" is a good name, since topology developers can also set additional classpath for their topologies instances in topology Config; the name \"classpath\" causes confusion. \"scheduler_classpath\" can be a better one; or any other good names eliminating confusion with classpath for topology instances.\n5. A small implementation question on #272 \n   why add one more argument in method execute.heron_class(..), rather than directly pass the value to arguments \"extra_jars\"? (Will point it out in the PR too.)\n. 2) I am curious how to make it happen LOL, since in earlier MesosScheduler, I hard-coded the classpath to construct the SchedulerMain command.\n3) then #138 should be the correct solution for this issue. #272 seems hacky and not good. Furthermore, #272 does not seem to prepare for #138; it is not extensible and may need to be 90% rewritten when doing the #138 \n4) topology developers can suppose they can also set the classpath to start instances in cli via --classpath option.\n. 3) I agree we need to provide config option to override the value. I just doubt whether the change in #272 is easy for future maintenence or extensibility.\n4) topology developers will still get confused. Let's figure out a better option name than --classpath.\n. Other concerns;\n1. w.r.t #138.\nAccording to the description, \"\u201c.heronrc\u201d is a simple configuration file to support repeated overriding options, instead of typing those options\", I don't think it is a suitable place to save the default value of --scheduler-classpath option, since this value varies basing on different scheduler, rather than varies basing on different users. For instance, yarn-scheduler requires hadoop jar in classpath while mesos-scheduler requires mesos jar. \u201c.heronrc\u201d is designed to save something like \u201crole\u201d or repeated overriding options basing on a particular user.\nFurthermore, I doubt the idea to have \u201c\u201c.heronrc\u201d\u201d:\n1) \u201c.heronrc\u201d can reduce repeated overriding options. But then the overriding options will be spitted into 2 parts: one part is in \u201c.heronrc\u201d while another one is in command line. It makes debug harder.\n2) I also doubt whether \u201c.heronrc\u201d does reduce repeated overriding options a lot. \n- Practically, people have a script (or copy-parse the command) to invoke heron-cli; they don\u2019t have to type the repeated options again and again.\n- There are very few options fitting with \u201c.heronrc\u201d; most of them should be taken care by heron/config. I doubt the need to add one extra file.\n2.\n@ashvina When we run client side\u2019s scheduler code (Launcher, Uploader or RuntimeManager), we \u201c$HERON_HOME/lib/scheduler\u201d and\u201d $HERON_HOME/lib/uploader\u201d into the classpath (we do something similar when running IScheduler on sever side). One possible workaround is to build a fat jar including necessary dependencies and put it under lib/scheduler folder. What do you think of this workaround?\n. Fixes: #305 \n. tx can you also do similar things in killAuroraJob() and restartAuroraJob()\n. :+1: \n. :+1: \n. These \"exception logging\" are expected rather than \"real exception\", as you see we kill the topology successfully. May compress them into DEBUG log rather than INFO log, or to log more helpful information.\n. Fixes: #292 \n. Fixes: #294 \n. I tried with the script and it works well for me.\n. :+1: \n. :+1: \n. I have updated the interfaces of ILaucnher; you may need to resolve potential conflicts with lastest master branch beforeing checking in.\n. only documentation and bug fixes for 0.14.0  -- also tests, integration tests or unit tests?\n. Close this Pull Request according to #338 .\n. Discussed within the team, and decided to just document it down (Solution 3). Will re-consider to solve this issue in future when it is really needed.\nClosed Pull Request #316 too.\n. @saileshmittal \nHi,\nYou are right, changing SPI is possible to break existing scheduler code.\nBut consider these:\n1. From the start of design, we decided to make all calls in SchedulerStateManagerAdaptor synchronized  and there might be some miscommunications or other unknown reasons, it is in async style, which is hard to reason and use in Scheduler implementation case.\n2. I checked current scheduler implementations in the repo and fixed the breaking parts. Async interfaces may not be a good idea in SchedulerStateManagerAdaptor: IStateManager is fully async, and we could expose IStateManager from SchedulerStateManagerAdaptor if in future async methods are really needed.\nConsider we are still in an early stage, it is ok to break things rather than keep async version for backward compatibility concern (which may not even exist at current stage).\n. What are these:\n\u2502   \u251c\u2500\u2500 localfs_statemgr.yaml\n\u2502   \u251c\u2500\u2500 localfs_uploader.yaml\n\u2502   \u251c\u2500\u2500 metrics_sinks.yaml\n\u2502   \u2514\u2500\u2500 roundrobin_packing.yaml\nIf they are examples, they should be in example folder; if they are shared config, please classify them clearly.\n. :+1: \n. @lewiskan Do you mean the Compilation fails when enabling some options? We can compile it sucessfully.\n. HI,\nIt does make sense. Feel free to make a Pull Request for it. \nThe only file you need to change is: https://github.com/twitter/heron/blob/master/heron/common/src/java/com/twitter/heron/common/utils/metrics/JVMMetrics.java\nAdd BufferPoolMXBean related metrics similarly as other JVM metrics and it will work..\n. Will checkout to my mac and review it.\n. @ashvina Hi, are u making your fork repo private? I could not checkout your branch. Can u create a branch in heron repo for this PR?\n. @ashvina Agree that those issues can be incrementally resolved. But is it feasible to resolve my comments starting with \"TODO(mfu): ...\" with adding actual TODO plan/comments? Currently those comments of mine, to some extent, are more like views or opinions, rather than actual TODO plan/comments.\n. This PR looks good to me. But I reviewed only the functional aspects; please resolve other reviewers' comments before checking in the code.\n. BTW, unit tests for schedulers are enabled now under folder: heron/schedulers/tests/java/...\nAnd unit tests for AuroraScheduler and LocalScheduler are there.\nYou can also add unit tests for reef scheduler on YARN there.\n. Fixes: #454 \n. Hi @ashvina \n-- Can we please discuss before you commit any changes?\nAs mentoined in the description, this Pull Request is considered only as a Code Review and will never be checked in; I used the PR to put my comments since it is easier for me to do that. That's also why this PR is \"closed\" and not pushed to your branch. Sorry if any confusion.\n-- The scheduler works on YARN and I have tested it many times.\nWe can discuss together. As I mentioned in the reviews (TODO comments), a lot of cases, especially failure cases, could be overlooked.\n-- why associating a name with TODO comment is needed\nThis is a convention used in Twitter. Associating a name with TODO indicates the person who left the TODO; others can fix and remove it. We can discuss and come up a uniform way to write TODO.\nSorry if any confusion. Please treat this PR as a Review, and resolve my changes or TODO(mfu) in your branch. We may discuss in this PR.\n. +1 for Andrew's solution.\n. looks good to me\n. Fixes: #455 \n. \ud83d\udc4d \n. Fixes: #461 \n. Fixes: #460 \n. \ud83d\udc4d \n. \ud83d\udc4d . Looks good to me.\n. looks good to me. :+1: \n. \ud83d\udc4d \n. :+1: \n. Yes I run it through style checks and passed.\n. :+1: \n. This PR involves API changes, I suspect whether it will break current existing topology, especially for the changes in backtype.storm package, whose interfaces are expected to be same as open source storm's.\n. Yeah please run some topologies for testing, especially those storm topology in source repo.\n. \ud83d\udc4d \n. This PR involves API changes, I suspect whether it will break current existing topology, especially for the changes in backtype.storm package, whose interfaces are expected to be same as open source storm's.\n. Yeah please run some topologies for testing, especially those storm topology in source repo.\n. \ud83d\udc4d \n. When the signature can not fit into one line. It is better to put every parameter in a new line.\n. \ud83d\udc4d \n. This is a feature requested by @ashvina , for ReefScheduler. Could u take a look at it?\n. @kramasamy Good question. We do all the stop() and close() in a finally block, so they will be closed as well. Or a safest way to add a DefaultUncaughtExceptoinHandler and put \"System.halt(1)\" at the end; but it can potentially hide issues. (It is what heron-instance and metrics mgr are doing now.)\n. How about making it similar to JAVA hierarchy?\n$ echo $JAVA_HOME\n/Library/Java/JavaVirtualMachines/jdk1.8.0_66.jdk/Contents/Home\nWe could have:\n/Users/kramasamy/heron\n-- heron-0.13.3\n----- client\n----- tool \n----- ...\n-- heron-master\n....\n. lgtm. :+1: \n. \ud83d\udc4d \n. When I wrote BaseBatchBolt or IBatchBolt, they didn't take generic type T; it is just something like:\npublic abstract class BaseRichBolt implements IBatchBolt {}.\nAccording to the git log:\ne6eb7310 (Vikas Kedigehalli 2015-01-21 22:58:48 +0000 1) package com.twitter.heron.integration_test.core;\ne6eb7310 (Vikas Kedigehalli 2015-01-21 22:58:48 +0000 2) \ne6eb7310 (Vikas Kedigehalli 2015-01-21 22:58:48 +0000 3) import com.twitter.heron.api.bolt.BaseRichBolt;\ne6eb7310 (Vikas Kedigehalli 2015-01-21 22:58:48 +0000 4) \ne6eb7310 (Vikas Kedigehalli 2015-01-21 22:58:48 +0000 5) // We keep this since we want to be consistent with earlier framework to reuse test topologies\ne6eb7310 (Vikas Kedigehalli 2015-01-21 22:58:48 +0000 6) public abstract class BaseBatchBolt extends BaseRichBolt implements IBatchBolt {\ne6eb7310 (Vikas Kedigehalli 2015-01-21 22:58:48 +0000 7) }\n@vikkyrk may have a better idea on this.\n. I checked with storm:\nhttps://github.com/apache/storm/blob/master/storm-core/src/jvm/org/apache/storm/coordination/IBatchBolt.java#L26\nThe reason heron has IBatchBolt is that storm has, so at that time I just want to keep the same as storm for backward compabitliy\nBut we don't really use \"T\" now for heron.\n. Normally, the config folder contains 7 yaml files:\n$ ls -al heron/config/src/yaml/conf/local\ntotal 80\ndrwxr-xr-x   9 mfu  staff    306 Apr 20 14:58 .\ndrwxr-xr-x  11 mfu  staff    374 Mar 18 15:07 ..\n-rw-r--r--   1 mfu  staff    182 Apr 12 17:24 client.yaml\n-rw-r--r--   1 mfu  staff  11270 Mar  7 17:50 heron_internals.yaml\n-rw-r--r--   1 mfu  staff   4138 Mar 13 19:11 metrics_sinks.yaml\n-rw-r--r--   1 mfu  staff    147 Mar  7 17:50 packing.yaml\n-rw-r--r--   1 mfu  staff    569 Apr 20 14:58 scheduler.yaml\n-rw-r--r--   1 mfu  staff    404 Mar  7 17:50 statemgr.yaml\n-rw-r--r--   1 mfu  staff    379 Apr  4 16:53 uploader.yaml\n1. Can u make the example config folder contain all necessary yaml files?\n2. Can u make the yaml named consistently? Remove \"localfs_\" prefix; or add \"sample_\" prefix for all.\n. Fair. Then can u add a README to describe this and add also the missing the sample scheduler.yaml?\n. \ud83d\udc4d \n. lgtm. \ud83d\udc4d \n. Do we require a minimal version of python to build/run heron?\nIf so, is it feasible that we enforce python >= 2.7 or otherwise throw exceptions? python 2.4 is final on November 30, 2004, really old.\n. @nlu90 IIRC, twitter internal has python2.7. Can we forcibly change the version of python on a host (some prepare work on host) before we building heron? For instance, jenkins building allows specification of JAVA or Python version.\nAfter 2.4, python provides a lot of useful features we are very likely to use in feature.\n.  lgtm. Ship it. :+1: \n. :+1: \n. @ajorgensen Could u please check whether your S3Uploader can also make use of this utils method?\n. In fact, heron does not depend on log4j-over-slf4j.\nHeron currently contains those libraries for some twitter specific concerns. We will see whether we can remove them\n. True. Though zookeeper itself depends on slf4j so heron-with-zk depends also on slf4j.\n. Yeah feel free to submit a pull request for that.\n. Sure. It seems relying on a pretty old version of heron master; @joestein could u rebase and fix with latest master branch? BTW, could u add me to your forked repo so I can checkout your code and review on my laptop? thanks\n. @joestein Hi I still could not open page https://github.com/elodina/heron; I don't think I have the permission yet.\n. @joestein Thanks I can access now. Will review it today.\n. Hi @joestein I reviewed the code and I noticed that the mesos scheduler is on top of a pretty old version of heron scheduler spi, for instance, the schedulers location (currently they are under heron/schedulers/...); and the IScheduler interface, for instance, no more onActivate() or onHealthCheck(). Is it feasible that you refactor it to align with latest scheduler spi?\nThanks.\n. Confirmed with Twitter Messaging team. They are happy to contribute the DistributedLog spout.\n. Hi @supunkamburugamuve \nI have just merged pull request: #706 , and now you can use SchedulerUtils.setupWorkingDirectory(...) as an utils method; no longer duplicated code in Launcher/Scheduler.\n. What do you mean \"make it use python cli\"?\nThis pr rename ports' name from \"port1, port2 ...\" to more descriptive names.\n. lgtm. \ud83d\udc4d \n. @nlu90 Now the state manager supports ssh tunnel connection, rather than only direct access.\n. This PR mentions a very good problem, but the solution may not work. We rely on Zookeeper for leader election to void multiple Scheduler running as master at the same time; and a Scheduler exits when failing to set SchedulerLocation on zookeeper.\nThis PR will cause: once a Scheduler fails unexpectedly, then no any new scheduler can start again. (Fails to set Scheduler Location on zookeeper since it is not permanent and not cleaned by any one.)\nLet's discuss and figure out a more complete and sound solution.\n. @kramasamy can u review this?\n. \ud83d\udc4d \n. Can u also add unit tests?\n. \ud83d\udc4d \n. :+1: \n. Need we add other similar classes, for instance, some test bolts, from storm too?\n. #715 has merged. Now accessing:\nRuntime.instanceDistribution(runtime)\nRuntime.numContainers(runtime);\ncan get appropriate values, rather than NPE.\n. looks good to me. \ud83d\udc4d \n. Another alternative can be: start a monit process for every executor and restart the executor if it fails.\n. We have a hierarchy fault tolerant design:\n1. Heron-instance fails and exits, it if has any uncaught exceptions. And heron-executor will retry to restart the failed heron-instance.\n2. Heron-executor fails and exits (including cleaning all processes spawned by the heron-executor) if it exceeds the maximum amount of attempts trying to restart a heron-executor. And then it is supposed Scheduler should restart the heron-executor for fault tolerance.\n3. BTW, we also expect heron-scheduler to restart if it fails by certain mechanisms, for instance, monit, though in LocalScheduler we didn't do it for the sake of simplicity. That's also the reason LocalScheduler is for testing, rather than production.\n. Just reviewed the code. @ashvina Did u try this reef scheduler with a distibured environment inside microsoft?\n. \ud83d\udc4d \n. :+1: \n. :+1: \n. Could u merge with latest master branch and fix the travis-ci building too?\n. Yeah, @saileshmittal we need to move to standard zookeepers style, also what current java part supports:\nzk-host1:port1, zk-host2:port2, ....\nhttps://zookeeper.apache.org/doc/r3.4.6/api/org/apache/zookeeper/ZooKeeper.html#ZooKeeper(java.lang.String, int, org.apache.zookeeper.Watcher)\nconnectString - comma separated host:port pairs, each corresponding to a zk server. e.g. \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002\" If the optional chroot suffix is used the example would look like: \"127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002/app/a\" where the client would be rooted at \"/app/a\" and all paths would be relative to this root - ie getting/setting/etc... \"/foo/bar\" would result in operations being run on \"/app/a/foo/bar\" (from the server perspective).\n. \ud83d\udc4d \n. To create a new pull request to trigger the travis-ci building.\n. \ud83d\udc4d \n. Debug completed.\n. \ud83d\udc4d \n. Also, I can try to reduce the results-pulling-interval in integration tests, while increasing the amount of pulling attempts. But this may reduce no more than 5 mins; not significant.\n. Hmm interesting, according to the log:\nmake site   0:02:00\nscripts/travis/build.sh 0:29:01\nscripts/travis/test.sh      0:12:58\n\nthe whole Travis-Ci building can be completed in 44mins, much shorter than expected.\n. Yeah originally the timeout is 50mins and every building in the afternoon (maybe that's a busy time and travis-ci is shared host?) will exceed the timeout and fail.\n. @joestein \nCorrect me if i'm wrong, It seems, the implementation launches the SchedulerMain(including MesosScheduler and Tmaster) locally, and then talk with Mesos Master to start other containers on Mesos Slave's machines?\n. @aShevc \nThanks for the reply. \nBut I am wondering how to guarantee fault tolerance of Scheduler & TMaster? Let's say, what happen when Scheduler & TMaster fail? Anyone to restart it?\nAlso, does it also require there is no firewall between the host doing submission and the hosts running rest containers?\n. Close it due to replicated with #1067 \n. #871 \n. Short answer: yes.\nFull answer:  We specified building heron with JAVA8 (so you have to run with JAVA8) due to twitter internal concerns. But heron is wirtten using only JAVA7 features, so people can build it with JAVA7 and run with JAVA7.\n. It is possible a topology fails start normally, for instance, failing to start a component due to dependencies lacking, though it is submitted successfully.\nCan you check the logs under: \n~/.herondata/topologies/{cluster}/{role}/{topologyName}/log-files/\nand \n~/.herondata/topologies/{cluster}/{role}/{topologyName}/ heron-executor.stderr  && heron-executor.stdout\n?\nCan be a good idea to add this kind of debugging in doc.\n. @AMirFirouzi \nIt is possible a topology fails start normally, for instance, failing to start a component due to dependencies lacking, though it is submitted successfully.\nCan you check the tmaster logs under:\n~/.herondata/topologies/{cluster}/{role}/{topologyName}/log-files/\nand\n~/.herondata/topologies/{cluster}/{role}/{topologyName}/ heron-executor.stderr && heron-executor.stdout\n?\n. @jipinxieshen \nChecked the heron-executor.stdout provided by @aaronshan , and found:\nheron-tmaster exited with status 256\nstmgr-1 exited with status 256\nSo pretty sure the issue is caused by lacking dependencies to start C++ program; but it is wired that log-info is missing in heron-executor.stderr, so currently no way to know what those dependencies are.\nYou can diagnose by run the stream mgr's cmd directly in terminal, check the output and figure out the lacking dependencies.\nWe are working in progress to relief your issues too:\n1. #505 \n2. #853\n. #853 is fixed. Now people can figure out reasons failed to start process from:\n~/.herondata/topologies/{cluster}/{role}/{topologyName}/ heron-executor.stdout\n. @lilalinda \nheron-executor.stderr or heron-executor.stdout is in\n.herondata/topologies/local/USER/ExclamationTopology/\n, rather than in \n.herondata/topologies/local/USER/ExclamationTopology/log-files/\n. @Udit93 \nhttps://github.com/twitter/heron/pull/902/files\n```\nIt is also possible that the host has an issue with resolving localhost. To check, run the following command in a shell.\n$ python -c \"import socket; print socket.gethostbyname(socket.gethostname())\"\nTraceback (most recent call last):\n  File \"\", line 1, in \nsocket.gaierror: [Errno 8] nodename nor servname provided, or not known\nIf the output looks like a normal IP address, such as 127.0.0.1, you don't have this issue. If the output is similar to the above, you need to modify the /etc/hosts file to correctly resolve localhost, as shown below.\nRun the following command, whose output is your computer's hostname.\n$ python -c \"import socket; print socket.gethostname()\"\nOpen the /etc/hosts file as superuser and find a line containing\n127.0.0.1   localhost\nAppend your hostname after the word \"localhost\" on the line. For example, if your hostname was tw-heron, then the line should look like the following:\n127.0.0.1   localhost   tw-heron\nSave the file. The change should usually be reflected immediately, although rebooting might be necessary depending on your platform.\n```\n. It is more Twitter internal concern. BTW, the latest thrift 0.9.* is not compatible with 0.5.*\n. Duplicated: #912 \n. Yeah the doc is a little outdated. And we are working to improve it. Same issue reported here: #809 \n. Stream mgr is started by heron-executor, rather than by scheduler directly.\nIt is possible a topology fails start normally, for instance, failing to start a component (stream mgr) due to dependencies lacking (some native library), though it is submitted successfully.\nCan you check the logs under: \n~/.herondata/topologies/{cluster}/{role}/{topologyName}/log-files/\nand \n~/.herondata/topologies/{cluster}/{role}/{topologyName}/ heron-executor.stderr && heron-executor.stdout\n?\nCan be a good idea to add this kind of debugging in doc.\n. Yes, we noticed this earlier: #505 \nIt is nice to add:\n1. How to investigate normal failures with logs,\n2. Document clearly the dependencies (native system libraries) c++ executable relies on\ninto the getting-started guide. @kramasamy \n. #853 is also working in progress.\n. #853 is fixed. Now people can figure out reasons failed to start process from:\n~/.herondata/topologies/{cluster}/{role}/{topologyName}/ heron-executor.stdout\n. @jieliu3 \n1. Now you can check \"~/.herondata/topologies/{cluster}/{role}/{topologyName}/ heron-executor.stdout\" to figure out the reason failing to start a component.\n\"2. Document clearly the dependencies (native system libraries) c++ executable relies on into the getting-started guide\"\nThis is working in progress. Pull request is here: #877 \n. Resolved.\n. Duplicated: #910 \n. Can u paste the content of heron-executor.stderr and heron-executor.stdout in:\n~/.herondata/topologies/{cluster}/{role}/{topologyName}/\nThe issues should be logged there.\n. @aaronshan \nChecked the heron-executor.stdout, and found:\nheron-tmaster exited with status 256\nstmgr-1 exited with status 256\nSo pretty sure the issue is caused by lacking dependencies to start C++ program;  but it is wired that log-info is missing in heron-executor.stderr, so currently no way to know what those dependencies are.\nYou can diagnose by run the stream mgr's cmd directly in terminal, check the output and figure out the lacking dependencies.\nWe are working in progress to relief your issues too:\n1. #505 \n2. #853 \n. @aaronshan \n./heron-core/bin/heron-stmgr ExclamationTopology ExclamationTopology890e8b1c-ca60-485a-b00c-0a7c24350287 ExclamationTopology.defn LOCALMODE /home/q/heron/heron-0.14.0/herondata/repository/state/local stmgr-1 container_1_word_2,container_1_exclaim1_1 52697 40818 49201 ./heron-conf/heron_internals.yaml\n. #853 is fixed. Now people can figure out reasons failed to start process from:\n~/.herondata/topologies/{cluster}/{role}/{topologyName}/ heron-executor.stdout\n. Resolved\n. BTW, we currently have built-in support for aurora and slurm. Mesos and YARN are working in process and coming soon.\n. #862 \n. #853 is fixed. Now people can figure out reasons failed to start process from:\n~/.herondata/topologies/{cluster}/{role}/{topologyName}/ heron-executor.stdout\n. Hi Kevin,\nThere is no plan to make collector thread-safe in near future, since we\nexpect topology developers would handle the concurrent issues if they\ncreate more threads by their own.\nAlternatively, got any thoughts on doing HTTP posts without this?\nWe have higher level framework summingbird (\nhttps://github.com/twitter/summingbird/) providing the feature you want,\nand it achieves this feature by:\n1. Maintain a thread-safe queue to keep the results of http posts request.\n2. Create a bounded thread-pool: each thread will do the actual post\nrequest and put the results into the thread-safe queue mentioned above.\n3. Add the tick-frequency for the final bolt and bind with actual work on\nthe thread-safe result queue. Then the results can be handled in main\nthread.\nHope it helps.\n2016-12-06 4:52 GMT+08:00 Kevin Chen notifications@github.com:\n\nAre there any plans to do this in the near future? Alternatively, got any\nthoughts on doing HTTP posts without this? (mostly on a final bolt,\nsynchronous requests would be undesirable)\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/859#issuecomment-264973550, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AC422pG3A5wCdnrhGWMO7OgzaE6PF9IUks5rFHmDgaJpZM4Ithfp\n.\n\n\n-- \nWith my best Regards\n\nFu Maosong\nTwitter Inc.\nMobile: +001-415-244-7520\n. You can use  conf.setNumWorkers(workersCount), (https://github.com/twitter/heron/search?utf8=%E2%9C%93&q=setNumWorkers), in storm-compatible api, which is translated into number of containers/stream mgrs in heron when converting as a heron job.\n. There are output on the terminal indicating whether a topology was submitted successfully nor not.\nFurthermore:\n1. You can check the working directory (printed at the end of output)\n2. Use heron-tracker or heron-ui, as getting started.\nAnother trick can be using \"ps\" command, though not reliable.\n. @lucperkins sure.\n. Rebased with master branch. Will open a new pull request to solve the commit tag conflicts.\n. Looks good to me. :+1: \n. @nlu90 Can u review this and make sure it also works in Twitter Env?\n. \ud83d\udc4d \n. @billonahill This can happen when the classname to create is empty. @ajorgensen Triggered this exception by not passing a value for the required config.\n. Another alternative can be: have a config-checker and throw meaningful exceptions if required config is not provided.\n. Hi,\nAccording to the logs, it failed to invoke Aurora.onScheduler(...)/ Can u add the flag \"--verbose\" when submitting the job and share the verbose output?\n. 1. Check whether the topology is running normally. You can do it via checking the log-files folder\n2. Heron trakcer feeds data for heron-ui. You need to start both of them with correct state manager configuration: https://github.com/twitter/heron/blob/master/heron/config/src/yaml/tracker/heron_tracker.yaml\n. BTW, I added a pull request: #884 , which logs the stderr of a spawned process even without \"--verbose\" flag.\n. @aaronshan @wking1986 \nAll scheduler implementations share similar working-directory (sandbox) structure. For aurora, can u go to the heron-executor.stdout && log-files folder in sandbox folder? (not in ~/.herondata/topologies/{cluster}/{role}/{topologyName}/ heron-executor.stdout)?\n1. You can use Aurora page to navigate to the webpage showing sandbox content (http://aurora.apache.org/documentation/latest/getting-started/tutorial/, click \"chroot browse\")\n2. You can ssh to the target sandbox host and enter the sandbox folder.\n. @aaronshan Can u enter the sandbox folder, at the same level as stderr you opened, which has the same structure as working directory in LocalScheduler, and check the content in heron-executor.stdout?\n. @nlu90 \nDo you know why \"curl: (6) Couldn't resolve host 'hdfs:'\"?\nAccording to the modified heron.aurora file, \"curl\" is commented and not even used.\n@aaronshan \nCan u double check the actual command when running \"fetch_user_package\"?\n20160608214214\nOn aurora page, you can click the name of process and get it.\n. @wking1986 Awesome! Aslo, native mesos scheduler and yarn scheduler are coming soon too! Pull requests are being reviewed.\n. @aaronshan \nHi,\nAccording to the log, heron-executor failed to start a heron-instance process.\nCan u try to run the command directly:\n/home/q/java8/jdk1.8.0_91/bin/java -Xmx320M -Xms320M -Xmn160M -XX:MaxPermSize=128M -XX:PermSize=128M -XX:ReservedCodeCacheSize=64M -XX:+CMSScavengeBeforeRemark -XX:TargetSurvivorRatio=90 -XX:+PrintCommandLineFlags -verbosegc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCCause -XX:+PrintPromotionFailure -XX:+PrintTenuringDistribution -XX:+PrintHeapAtGC -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:ParallelGCThreads=4 -Xloggc:log-files/gc.container_1_word_2.log -XX:+HeapDumpOnOutOfMemoryError -Djava.net.preferIPv4Stack=true -cp ./heron-core/lib/instance/*:heron-examples.jar com.twitter.heron.instance.HeronInstance ExclamationTopology ExclamationTopology603f5dd1-da30-46ac-8e6b-01650fd35cfe container_1_word_2 word 2 0 stmgr-1 31719 31300 ./heron-conf/heron_internals.yaml\nand check the output?\n. @aaronshan So what was the issue?\n. @aaronshan \nYou can specifiy the disk_per_container in Config to override the default one: https://github.com/twitter/heron/blob/master/heron/api/src/java/com/twitter/heron/api/Config.java#L266\nAs aurora shows, it failed to schedule containers with requested disk. It is related to the Aurora Resource Management we can rarely do anything.\n. @jiandongjia @aaronshan \nYou migh get more insights from Aurora Offical Website: http://aurora.apache.org/ \n. @kartik894 \nIt is caused by invalid config value in statemgr.yaml when trying to connect zookeeper:\nheron.statemgr.root.path: \"hdfs:///heron\"\nYou can try with: /heron\nOr check zookeeper for path format.\n. \ud83d\udc4d \n. #893 \n. \ud83d\udc4d \n. Why closed?\n. \ud83d\udc4d \n. It seems after:\n1. specify the -xms explicitly\n2. +XX:+AlwaysPreTouch always takes the full ram at the start\nwill make two integration tests topology (which specify most instances, one specifies 6 while another specifies 7; and 1GB ram by default per instance) exceeds the ram limitation of travis-ci building env, which is 7.5GB. (https://docs.travis-ci.com/user/ci-environment/#Virtualization-environments)\n. @billonahill \nI printed the stderr & stdout and found:\n```\n2016-06-14 01:59:27: Running container_1_ab-spout-1_2 process as /usr/lib/jvm/java-8-oracle/bin/java -Xmx832M -Xms832M -Xmn416M -XX:MaxPermSize=128M -XX:PermSize=128M -XX:ReservedCodeCacheSize=64M -XX:+AlwaysPreTouch -XX:+CMSScavengeBeforeRemark -XX:TargetSurvivorRatio=90 -XX:+PrintCommandLineFlags -verbosegc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCCause -XX:+PrintPromotionFailure -XX:+PrintTenuringDistribution -XX:+PrintHeapAtGC -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:ParallelGCThreads=4 -Xloggc:log-files/gc.container_1_ab-spout-1_2.log -Djava.net.preferIPv4Stack=true -cp ./heron-core/lib/instance/*:integration-tests.jar com.twitter.heron.instance.HeronInstance 1465869225_IntegrationTest_MultiSpoutsMultiTasks_eb838a2e-2d88-4139-9a47-db58ed4747d6 1465869225_IntegrationTest_MultiSpoutsMultiTasks_eb838a2e-2d88-4139-9a47-db58ed4747d670137c81-59b2-43ac-8ea8-bf948d571446 container_1_ab-spout-1_2 ab-spout-1 2 1 stmgr-1 49765 35598 ./heron-conf/heron_internals.yaml\n2016-06-14 01:59:27: Logging pid 25527 to file container_1_ab-spout-1_2.pid\n2016-06-14 01:59:27: container_1_ab-spout-1_3 exited with status 256\n2016-06-14 01:59:27: container_1_ab-spout-1_3 stdout: -XX:+AlwaysPreTouch -XX:+CMSScavengeBeforeRemark -XX:+HeapDumpOnOutOfMemoryError -XX:InitialHeapSize=872415232 -XX:MaxHeapSize=872415232 -XX:MaxNewSize=436207616 -XX:MaxTenuringThreshold=6 -XX:NewSize=436207616 -XX:OldPLABSize=16 -XX:ParallelGCThreads=4 -XX:+PrintCommandLineFlags -XX:+PrintGC -XX:+PrintGCCause -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -XX:+PrintPromotionFailure -XX:+PrintTenuringDistribution -XX:ReservedCodeCacheSize=67108864 -XX:TargetSurvivorRatio=90 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC \n\nThere is insufficient memory for the Java Runtime Environment to continue.\nNative memory allocation (mmap) failed to map 436207616 bytes for committing reserved memory.\nAn error report file with more information is saved as:\n/home/travis/.herondata/topologies/local/heron-staging/1465869225_IntegrationTest_MultiSpoutsMultiTasks_eb838a2e-2d88-4139-9a47-db58ed4747d6/hs_err_pid25520.log\n2016-06-14 01:59:27: container_1_ab-spout-1_3 stderr: Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=128M; support was removed in 8.0\nJava HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=128M; support was removed in 8.0\nJava HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x00000000cc000000, 436207616, 0) failed; error='Cannot allocate memory' (errno=12)\n```\nThe test topology does fail due to limited ram.\n. This pull request is used for testing and I will close now since the issues have been found.\n. Hi, \nIt is a typo. Pull request to fix is here: https://github.com/twitter/heron/pull/907\n. Fixed: #907 \n. \ud83d\udc4d \n. Travis-ci building also shows this WARNING during untar:\n[2016-06-14 02:05:18 +0000] com.twitter.heron.spi.common.ShellUtils FINE:  $> [tar, -xvf, /home/travis/.herondata/topologies/local/heron-staging/1465869225_IntegrationTest_OneBoltMultiTasks_de8c043e-7d22-4ac4-ba17-902fbba5db81/topology.tar.gz]  \nintegration-tests.jar\n1465869225_IntegrationTest_OneBoltMultiTasks_de8c043e-7d22-4ac4-ba17-902fbba5db81.defn\n./heron-conf/\n./heron-conf/heron_internals.yaml\ntar: ./heron-conf/heron_internals.yaml: implausibly old time stamp 1970-01-01 00:00:00\n./heron-conf/packing.yaml\ntar: ./heron-conf/packing.yaml: implausibly old time stamp 1970-01-01 00:00:00\n./heron-conf/uploader.yaml\ntar: ./heron-conf/uploader.yaml: implausibly old time stamp 1970-01-01 00:00:00\n./heron-conf/client.yaml\ntar: ./heron-conf/client.yaml: implausibly old time stamp 1970-01-01 00:00:00\n./heron-conf/scheduler.yaml\ntar: ./heron-conf/scheduler.yaml: implausibly old time stamp 1970-01-01 00:00:00\n./heron-conf/statemgr.yaml\ntar: ./heron-conf/statemgr.yaml: implausibly old time stamp 1970-01-01 00:00:00\n./heron-conf/metrics_sinks.yaml\ntar: ./heron-conf/metrics_sinks.yaml: implausibly old time stamp 1970-01-01 00:00:00\n./heron-conf/release.yaml\ntar: ./heron-conf/release.yaml: implausibly old time stamp 1970-01-01 00:00:00\n./heron-conf/override.yaml\ntar: ./heron-conf: implausibly old time stamp 1970-01-01 00:00:00\n. Fixed: #985 \n. Fixed: #940 \n. @ashvina Could u resolve the conflicts?\n. This Pull request looks good to me. \n@ashvina can u add issues to trace remaining TODO in this pull request?\n1. Co-locate TMaster and Scheduler\n2. Handle the case TMaster fails (ApplicationMaster for YARN job)\n3. To avoid uploading the heron-core release package in every topology submission\n4. To avoid uploading reef scheduler individually in every topology submission since it is already in heron-core release package.\n. Does it run normally after reconnecting to the stream mgr?\n. Hi\nThis issue can be easily reproduced by a spout with:\n```\n  public static final int N = 1024;\npublic void nextTuple() {\n    byte[] b = new byte[100 * N * N];\n    new Random().nextBytes(b);\n    collector.emit(new Values(b));\n  }\n```\nInvestigating what is the issue. We do a mini-batch on heron-instance side and it seems stream mgr has troubles to deal with a packet (batch) greater than a value.\n@ajorgensen During the time of investigation, you can have this workaround: change this config (https://github.com/twitter/heron/blob/master/heron/config/src/yaml/conf/examples/heron_internals.yaml#L199) to a relatively small number, for instance, 20. (The key is to make this value * each tuple size <= 50M)\n. It turns out that stream mgr rejects to parse a too-huge protobuf (default limitation is 64M) and closes the connection to instance.\nError info:\n[libprotobuf ERROR google/protobuf/io/coded_stream.cc:171] A protocol message was rejected because it was too big (more than 67108864 bytes).  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\nAnd here is the comments in coded_stream.h\nHint: If you are reading this because your program is printing a warning about dangerously large protocol messages, you may be confused about what to do next. The best option is to change your design such that excessively large messages are not necessary. For example, try to design file formats to consist of many small messages rather than a single large one. If this is infeasible, you will need to increase the limit. Chances are, though, that your code never constructs a CodedInputStream on which the limit can be set. You probably parse messages by calling things like Message::ParseFromString(). In this case, you will need to change your code to instead construct some sort of ZeroCopyInputStream (e.g. an ArrayInputStream), construct a CodedInputStream around that, then call Message::ParseFromCodedStream() instead. Then you can adjust the limit. Yes, it's more work, but you're doing something unusual.\nHowever, this error message is not redirected to file and stream mgr is run as a daemon process so it is tricky to figure it out.\nHere are what we could do:\n1. Redirect stdout&stderr in c++ program to glog (or file).\n2. Increase the limitation of protobuf size to parse (or make it configurable, but it is discouraged from google)\n3. Force the protobuf size (sent from heron-instance )smaller than the limitation.\n. Fixed: #960 \n. Duplicated with #1094 \n. :+1: This does make sense\n. @nlu90 \nCan u take a look at the why the local integration test case \"KILL_TMASTER\" failed? \nAlso, I noticed that:\n1. If \"ERROR:root:Failed to get expected and actual results\", the local integration script will not kill the test topology.\n2. When invoking:\nsubmitTopology(\n        params['cliPath'], \n        params['cluster'],\n        params['testJarPath'], \n        params['topologyClassPath'],\n        params['topologyName'],\n        params['readFile'],\n        params['outputFile']\n    )\nthe local integration script considers the submission fails only when it throws exceptions, without taking care the case when return code is -1.\nBoth of them seem are kind of bugs.\n. Yes, it was caused by some flaky tests and non-related to your pull request.\n. Waiting for the fix of flaky unit tests: #961 \n. @ashvina Currently travis-ci does not provide the feature to fetch data from the building container.\n. \ud83d\udc4d \n. looks good to me. \ud83d\udc4d \n. Comments on the demo:\n1. Remove the unnecessary logs printed\n2. Separate heron-explorer's dependencies on heron-ui\n3. Some results can be printed in a better format, for instance, print the list of topologies running on a specific cluster in a table.\n. @nlu90 Can u take a look at this?\n. Fixed: #980 \n. Closing because this is a question, not an issue. This is a good discussion for the mailing list please.\n. This will not work.\nCurrently the workflow is:\n1. Use thrift executable in third_patry (https://github.com/twitter/heron/tree/master/third_party/thrift) to generate the java source code from thrift files. And those thrift executables are version 0.5.1\n2. Then add thrift-java dependencies (which is changed in this pull request) to compile the scribe-sink.\nSo chaning the thrift-java dependencies alone will not solve the problem: you have to align the thrift compiler (generating the java source code) and thrift-java dependencies to the same version. This is traced by issue: #912 . You may re-assign the issue to yourself if you want.\n@kramasamy made this workflow. He may have better ideas on it. BTW, I am not convinced on current solutions having two thrift pre-compiled executables directly since c++ binareis are not portable and the linux one may not be able to run on all linux platform.\n. looks good to me. \ud83d\udc4d \n. @nlu90 Need we also change the one in simulator: https://github.com/twitter/heron/blob/master/heron/simulator/src/java/com/twitter/heron/simulator/instance/SpoutInstance.java#L181 ?\n. @sharmaarun Can u also check whether you set a limitation on the vagrant\n. That means # of containers you specified in topology config is more than\nthe # of instances.\n2016-06-30 5:56 GMT-07:00 Kartik Sathyanarayanan notifications@github.com:\n\nHi,\nI get the following error upon submitting a topology in Heron.\nException in thread \"main\" java.lang.RuntimeException: More containers allocated than instance. Bailing out.\n    at com.twitter.heron.packing.roundrobin.RoundRobinPacking.getBasePacking(RoundRobinPacking.java:246)\n    at com.twitter.heron.packing.roundrobin.RoundRobinPacking.pack(RoundRobinPacking.java:57)\n    at com.twitter.heron.scheduler.LaunchRunner.call(LaunchRunner.java:121)\n    at com.twitter.heron.scheduler.SubmitterMain.callLauncherRunner(SubmitterMain.java:465)\n    at com.twitter.heron.scheduler.SubmitterMain.submitTopology(SubmitterMain.java:418)\n    at com.twitter.heron.scheduler.SubmitterMain.main(SubmitterMain.java:315)\nCould you help me with this please? Altering the task resources in\nheron.aurora isn't solving the problem.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/1012, or mute the thread\nhttps://github.com/notifications/unsubscribe/AC422hhPVrUKjm5fc-uekvq5NydlUVGBks5qQ70bgaJpZM4JCIS5\n.\n\n\nWith my best Regards\nFu Maosong\nTwitter Inc.\nMobile: +001-415-244-7520\n. \ud83d\udc4d \n. Can you also add a unit test to verify the corner cases trying to solve in https://github.com/twitter/heron/tree/master/heron/metricsmgr/tests/java/com/twitter/heron/metricsmgr ?\n. Thanks for the contribution. Looks good to me. :+1: \n. @mhajibaba You may need to customize the HDFSUploader to return http:///webhdfs/v1/ URI\n. @mhajibaba \nFor heron-core package, you could configure the uri directly in config: https://github.com/twitter/heron/blob/master/heron/config/src/yaml/conf/aurora/client.yaml#L2\nFor topology package, https://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/uploader/IUploader.java#L48, the IUploader.uploadPackage() returns the topology URI, so you may need to change the HDFSUploader a little bit. Or a pull request to make HDFSUploader more generic is welcome.\n. Also, make sure your JAVA_HOME is set correctly.\n. @lilalinda \nCan u share the log files in failed container's sandbox? They are:\n-- heron-executor.stderr\n-- heron-executor.stdout\n-- all files in log-files folder\n. @lilalinda \nBy design, heron-executor spawns children processes and restart them if any failure happens unless the amount of retry exceeds the limitation.\nAccording to the log in stderr:\n*** Aborted at 1467703906 (unix time) try \"date -d @1467703906\" if you are using GNU date ***\nPC: @           0x4d37ce google::protobuf::io::CodedInputStream::PopLimit()\n*** SIGTERM (@0x3e900006a17) received by PID 27173 (TID 0x7f404adcf780) from PID 27159; stack trace: ***\n    @     0x7f404a3a6330 (unknown)\n    @           0x4d37ce google::protobuf::io::CodedInputStream::PopLimit()\n    @           0x466c5b heron::proto::system::HeronDataTupleSet::MergePartialFromCodedStream()\n    @           0x467011 heron::proto::system::HeronTupleSet::MergePartialFromCodedStream()\n    @           0x4560a1 heron::proto::stmgr::TupleMessage::MergePartialFromCodedStream()\n    @           0x4d2287 google::protobuf::MessageLite::ParseFromArray()\n    @           0x490b8d IncomingPacket::UnPackProtocolBuffer()\n    @           0x42104b Server::dispatchMessage<>()\n    @           0x492c45 Server::OnNewPacket()\n    @           0x48cec9 Connection::handleDataRead()\n    @           0x48a0dc BaseConnection::handleRead()\n    @           0x48f94d EventLoopImpl::handleReadCallback()\n    @           0x498fcc event_base_loop\n    @           0x409416 main\n    @     0x7f4049bcff45 __libc_start_main\n    @           0x40d82f (unknown)\n    @                0x0 (unknown)\nThe stream mgr exited ~ 09:31:46.\nAccording to the log in heron-executor.stdout:\n2016-07-05 09:31:39: Logging pid 27173 to file stmgr-1.pid\n2016-07-05 09:31:39: Running metricsmgr-1 process as /usr/lib/jvm/java-8-oracle/bin/java -Xmx1024M -XX:+PrintCommandLineFlags -verbosegc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCCause -XX:+PrintPromotionFailure -XX:+PrintTenuringDistribution -XX:+PrintHeapAtGC -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:+PrintCommandLineFlags -Xloggc:log-files/gc.metricsmgr.log -Djava.net.preferIPv4Stack=true -cp ./heron-core/lib/metricsmgr/* com.twitter.heron.metricsmgr.MetricsManager metricsmgr-1 31288 ExclamationTopology ExclamationTopology8c1920b3-e596-4af1-a626-d2457c5f9689 ./heron-conf/heron_internals.yaml ./heron-conf/metrics_sinks.yaml\n2016-07-05 09:31:39: Logging pid 27174 to file metricsmgr-1.pid\n2016-07-05 09:31:46: Executor terminated; exiting all process in executor.\nIt seems the heron-executor exited directly without restarting the failed stream mgr.\nAnd I found in the thermos_runner.INFO:\nI0705 09:31:38.587270 27058 runner.py:845] Forking Process(launch_heron_executor)\nI0705 09:31:46.608158 27058 thermos_runner.py:160] Thermos runner got signal 10, shutting down.\nI0705 09:31:46.608392 27058 thermos_runner.py:161] Interrupted frame:\nI0705 09:31:46.610755 27058 thermos_runner.py:164]   File \"/usr/lib/python2.7/runpy.py\", line 162, in _run_module_as_main\nI0705 09:31:46.610853 27058 thermos_runner.py:164]     \"__main__\", fname, loader, pkg_name)\nI0705 09:31:46.610932 27058 thermos_runner.py:164]   File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\nI0705 09:31:46.611006 27058 thermos_runner.py:164]     exec code in run_globals\nI0705 09:31:46.611077 27058 thermos_runner.py:164]   File \"/var/lib/mesos/slaves/a12ea196-4416-4904-b123-e26b2ba82f22-S3/frameworks/1e37e686-4cc7-465d-bc0c-a2b6e1f286f4-0000/executors/thermos-hadoopuser-test-ExclamationTopology-1-961438f4-a36c-4a66-94d9-d4ce0ad443b5/runs/69b97000-1547-4262-9be9-9ece49c254a5/thermos_runner.pex/__main__.py\", line 25, in <module>\nI0705 09:31:46.611149 27058 thermos_runner.py:164]     bootstrap_pex(__entry_point__)\nI0705 09:31:46.611217 27058 thermos_runner.py:164]   File \"/var/lib/mesos/slaves/a12ea196-4416-4904-b123-e26b2ba82f22-S3/frameworks/1e37e686-4cc7-465d-bc0c-a2b6e1f286f4-0000/executors/thermos-hadoopuser-test-ExclamationTopology-1-961438f4-a36c-4a66-94d9-d4ce0ad443b5/runs/69b97000-1547-4262-9be9-9ece49c254a5/thermos_runner.pex/.bootstrap/_pex/pex_bootstrapper.py\", line 98, in bootstrap_pex\nI0705 09:31:46.611290 27058 thermos_runner.py:164]     pex.PEX(entry_point).execute()\nI0705 09:31:46.611367 27058 thermos_runner.py:164]   File \"/var/lib/mesos/slaves/a12ea196-4416-4904-b123-e26b2ba82f22-S3/frameworks/1e37e686-4cc7-465d-bc0c-a2b6e1f286f4-0000/executors/thermos-hadoopuser-test-ExclamationTopology-1-961438f4-a36c-4a66-94d9-d4ce0ad443b5/runs/69b97000-1547-4262-9be9-9ece49c254a5/thermos_runner.pex/.bootstrap/_pex/pex.py\", line 324, in execute\nI0705 09:31:46.611448 27058 thermos_runner.py:164]     self._wrap_coverage(self._wrap_profiling, self._execute)\nI0705 09:31:46.611558 27058 thermos_runner.py:164]   File \"/var/lib/mesos/slaves/a12ea196-4416-4904-b123-e26b2ba82f22-S3/frameworks/1e37e686-4cc7-465d-bc0c-a2b6e1f286f4-0000/executors/thermos-hadoopuser-test-ExclamationTopology-1-961438f4-a36c-4a66-94d9-d4ce0ad443b5/runs/69b97000-1547-4262-9be9-9ece49c254a5/thermos_runner.pex/.bootstrap/_pex/pex.py\", line 257, in _wrap_coverage\nI0705 09:31:46.611659 27058 thermos_runner.py:164]     runner(*args)\nI0705 09:31:46.611751 27058 thermos_runner.py:164]   File \"/var/lib/mesos/slaves/a12ea196-4416-4904-b123-e26b2ba82f22-S3/frameworks/1e37e686-4cc7-465d-bc0c-a2b6e1f286f4-0000/executors/thermos-hadoopuser-test-ExclamationTopology-1-961438f4-a36c-4a66-94d9-d4ce0ad443b5/runs/69b97000-1547-4262-9be9-9ece49c254a5/thermos_runner.pex/.bootstrap/_pex/pex.py\", line 289, in _wrap_profiling\nI0705 09:31:46.611821 27058 thermos_runner.py:164]     runner(*args)\nI0705 09:31:46.611890 27058 thermos_runner.py:164]   File \"/var/lib/mesos/slaves/a12ea196-4416-4904-b123-e26b2ba82f22-S3/frameworks/1e37e686-4cc7-465d-bc0c-a2b6e1f286f4-0000/executors/thermos-hadoopuser-test-ExclamationTopology-1-961438f4-a36c-4a66-94d9-d4ce0ad443b5/runs/69b97000-1547-4262-9be9-9ece49c254a5/thermos_runner.pex/.bootstrap/_pex/pex.py\", line 367, in _execute\nI0705 09:31:46.611958 27058 thermos_runner.py:164]     return self.execute_entry(self._pex_info.entry_point)\nI0705 09:31:46.612025 27058 thermos_runner.py:164]   File \"/var/lib/mesos/slaves/a12ea196-4416-4904-b123-e26b2ba82f22-S3/frameworks/1e37e686-4cc7-465d-bc0c-a2b6e1f286f4-0000/executors/thermos-hadoopuser-test-ExclamationTopology-1-961438f4-a36c-4a66-94d9-d4ce0ad443b5/runs/69b97000-1547-4262-9be9-9ece49c254a5/thermos_runner.pex/.bootstrap/_pex/pex.py\", line 425, in execute_entry\nI0705 09:31:46.612095 27058 thermos_runner.py:164]     runner(entry_point)\nI0705 09:31:46.612162 27058 thermos_runner.py:164]   File \"/var/lib/mesos/slaves/a12ea196-4416-4904-b123-e26b2ba82f22-S3/frameworks/1e37e686-4cc7-465d-bc0c-a2b6e1f286f4-0000/executors/thermos-hadoopuser-test-ExclamationTopology-1-961438f4-a36c-4a66-94d9-d4ce0ad443b5/runs/69b97000-1547-4262-9be9-9ece49c254a5/thermos_runner.pex/.bootstrap/_pex/pex.py\", line 430, in execute_module\nI0705 09:31:46.612236 27058 thermos_runner.py:164]     runpy.run_module(module_name, run_name='__main__')\nI0705 09:31:46.612348 27058 thermos_runner.py:164]   File \"/usr/lib/python2.7/runpy.py\", line 180, in run_module\nI0705 09:31:46.612421 27058 thermos_runner.py:164]     fname, loader, pkg_name)\nI0705 09:31:46.612488 27058 thermos_runner.py:164]   File \"/usr/lib/python2.7/runpy.py\", line 72, in _run_code\nI0705 09:31:46.612555 27058 thermos_runner.py:164]     exec code in run_globals\nSo there was something happened at ~ 09:31:46. It seems mesos sent caught SIGNAL 10 and sent SIGTERM to heron-executor. Heron-executor will then clean (kill, send SIGTERM to all spawned processes). Stream mgr was also killed and printed the stack trace above. So I don't think the death of stream mgr is a result rather than a cause.\nSo I have questions:\n1. Did u run \"heron kill ...\" to kill the job at ~ 09:31:46 ?\n2. What does SIGNAL 10 mean in aurora/mesos?\n3. Could u check whether the job hit aurora limitation (for instance, used more ram than required) and was killed by aurora?\nAlso, currently the stderr & stdout of stream mgr can lose: (https://github.com/twitter/heron/issues/958)\nSo can u try to change here: https://github.com/twitter/heron/blob/master/heron/executor/src/python/heron-executor.py#L359\nto forcibly redirect the logs to a file and share the content of the file with us?\nThanks \n. @wuye9036 Aurora is open sourced: https://github.com/apache/aurora\n. @lilalinda Cool nice to know that. Let's dig it deeper when you get a chance. Similarly, we may need your log-files to figure out what happened.\n. We have considered whether to keep this interface a long time ago and decided not to support it. Can I know what the benefits are to \"hash this tuple to different thread\" basing on the taskid, instead of its value , i.e. tuple.hashCode()?\n. @nlu90 Can u take a look at this?\n. looks good to me. :+1: \n. This does make sense.  :+1: \n. @avflor \nTry with: bazel build --config=darwin --experimental_action_listener=tools/java:compile_java heron/...\n. @avflor Maybe you need \"bazel clean\" first? @billonahill may have better ideas on this.\n. looks good to me. \ud83d\udc4d \n. You may try with heron-tracker: http://twitter.github.io/heron/docs/operators/heron-tracker/\nAnd here is the RESTful API: http://twitter.github.io/heron/docs/operators/heron-tracker-api/\nBut heron-tracker provides only filtered metrics: https://github.com/twitter/heron/blob/master/heron/config/src/yaml/conf/examples/metrics_sinks.yaml#L44\nIf you want to get more metrics, one simple way is to look at metrics.json.* under working directory.\n. Should we change it into protected so later we can use it in Unit test?\n. :+1: \n. @kramasamy \nScalability can be considered from a lot of point of views. For instance, this scheduler would not cache offer, which means it may take a long time to schedule a big topology, since the tasks need to be scheduled one by one with new incoming offers.\n. @kramasamy \nConsider Mesos keeps giving offers slowly, it will take a long time to receive enough offers to fully schedule a big topology. Also, it is just one case. There are still a lot of potential improvement for scalability.\n. @kramasamy It is not experimental; it is a scheduler aiming at getting started, similar to LocalScheduler. Should we also move all schedulers to contrib/ directory? Or we may create a new repo called: heron-schedulers, similar to storm or presto? since they are all on top of heron. \n. @kramasamy \nOn the one hand, Hadoop may not be a good example to follow; people always complain its huge mono-repo. For more current github projects, for instance, presto, they would like to split them into small sub repos, making each one clear.\nOn the other hand, looking at some popular open source projects, I could not see \u201ccontrib\u201d folder: jQuery, MongoDB, Redis, Puppet, Ruby on Rails, Jenkins. Django has a contrib folder and the role of folder is already explained here: https://docs.djangoproject.com/en/dev/ref/contrib/\n```\nDjango aims to follow Python\u2019s \u201cbatteries included\u201d philosophy. It ships with a variety of extra, optional tools that solve common Web-development problems.\nThis code lives in django/contrib in the Django distribution. This document gives a rundown of the packages in contrib, along with any dependencies those packages have.\n```\nSo, I am not convinced by the opinion that contrib folder is for experimental / getting started. I think it is more for tools building on top of the core part.\nTo summarize, it can be a good idea to move all scheduler-implementations and similar things to an individual repo (to an individual folder is debatable). But whether to move them bases on the nature of those projects, rather than the quality of those projects.\n. @ashvina \nIssue created: #1077 \nI also heard about that Apache REEF supports Mesos also. But I could not find enough documentation or status on that. It would be great if we can make heron run on Mesos basing on REEF at production quality. Just don't know how hard it is or how long it will take.\n. @ashvina @kramasamy @nlu90 \nAny more concerns I need to handle beforing merging this pull request?\n. looks good to me. \ud83d\udc4d \n. @kramasamy @nlu90 \nDid we verify this change not to break Twitter's internal building environment for Heron?\n. \ud83d\udc4d \n. @nlu90 Can u take a look at this?\n. \ud83d\udc4d \n. :+1: Looks good to me\n. Is it possible that the key is no longer valid after earlier callback's operation?\n. It is hard to assume what would be in callback.handleXXXX(), since it is an interface and implemented outside NIOLooper. Adding a isValid check is more safe, IMO.\n. I see your point now.\nAdmittedly, it can work normally if just removing the second isValid() check. But it would implicitly assume the order of different operations, which is not guaranteed; it will be harder to reason, trace or maintain. \nIt is better to keep every block self-inclusive, instead of implicitly relying on earlier not-guaranteed logic.\n. You should instantiate Jedis on server side rather than on client. \nIn other words, you can instantiate Jedis in open(), https://github.com/twitter/heron/blob/master/heron/api/src/java/com/twitter/heron/api/spout/ISpout.java#L56, or prepare(), https://github.com/twitter/heron/blob/master/heron/api/src/java/com/twitter/heron/api/bolt/IBolt.java#L54, depending on spout or bolt.\nAlso, it is not an issue. It is better to ask in mailing list. Close it since it is not an issue.\n. @mycFelix Could u check whether you install heron in a system directory requiring sudo permission? Normally we install heron in user home dir.\n. You don't need sudo; instead I think you should:\n$ ./heron-client-install-VERSION-PLATFORM.sh --user\n. @ashvina yes when people try to install heron in system dir requiring sudo permission, heron will try to do some temp work in HERON_HOME, which is also in system dir potentially requiring sudo permission. May need to think a way to handle it. There are no simple ways to disable heron-shell unless changing the source code and re-build a new release.\n. doImmediateAcks is an internal implementation and could be changed in future.\nFor:\n why collector.ack called by user in a bolt is different from ack method in spout?\nHow I ensure that my message is processed by last bolt?\nIs there any need to implement acking in heron or it guarantees message processing?\nThis doc could help: http://storm.apache.org/releases/current/Guaranteeing-message-processing.html\n. Close since it is not an issue. Better to ask in google group.\n. @mhajibaba Sorry I think I haven't explained clearly. Heron has compatible-api with storm and provides same guarantees as storm, so concept is shared and you can use heron acking mechanism as storm's.\n. @yanxz That's awesome. I will assign this issue to you. Could you also sign the CLA: https://engineering.twitter.com/opensource/cla?\n. Here are steps for trying mesos scheduler locally:\n- Install heron by building latest master from the scratch(The release version does not contain mesos scheduler)\n  bazel run \u2014config=darwin \u2014verbose_failures \u2014 scripts/packages:heron-client-install.sh \u2014user\n1. Install mesos locally\n   https://mesosphere.com/blog/2014/07/07/installing-mesos-on-your-mac-with-homebrew/\n2. Change the config in heron/config/src/yaml/conf/mesos/scheduler.yaml\n   \"heron.mesos.native.library.path\" to the dir you mesos installed in step 1\n3. Play with mesos scheduler\n   heron submit mesos \u2014verbose ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology\n   heron kill mesos ExclamationTopology\n. Fixed: #1142 \n. @objmagic @nlu90 \n. Hi,\nWe had a discussion a long time ago and we decided to use only java.util.logging within heron code base. The existence of any other logging libraries is for other 3rd party libraries that heron uses.\n. @caofangkun We have a formatter tool for java style for IntelliJ IDEA, will check it into the repo.\n. @atibon \nFirst question, are u using aurora scheduler or mesos scheduler?\n\" some PC has been assigned 5 container, and some PC has been assigned 1 container.\"\nWe rely on aurora/mesos to do the actual scheduling. So how to assign containers for PC is determined by aurora/mesos.\n\"And Total CPU used in a container is bigger than 3. Maybe something is wrong.\"\nFor using aurora/mesos scheduler, we pack topology request into actual aurora/mesos request to ask them to do the actual scheduling. Could u share the resource requirement on aurora ui or mesos ui? Then we can check whether it is an issue related to resource request transformation, or it is something related to aurora/mesos.\nAlso, I don't understand \"upload picture failed...\"\n. :+1:  Looks good to me.\n. Very good question. We discussed this a long time ago.\nWakeableLooper is designed non-thread-safe, and we rely on higher level logic to handle this concern. \nFor instance, if you take a look at heron-instance source code, you will see there are two threads, Gateway and Slave, and each of them has one WakeableLooper. Data communication between them will be done within a thread-safe Queue, LinkedTransaferQueue; and WakeableLooper will register events in its own thread to listen on that Queue. \nSimply saying, those events-registered interfaces should not be invoked in other threads.\n. Let's keep current design for now. It is a low level library shared in a lot of places; need more careful experiments and thoughts. \nFor instance, you mentioned \"Maybe get some overhead\", could we quantify the overhead, in basic library benchmark, simple topoogies and normal production topologies?\n. By the way PriorityQueue seems hard to implement cancel timer.\nTrue. We don't provide interfaces like cancelTimer(..); will definitely review this data structure if we need to add this interface in future.\n. Can u also modify the website to make it able to navigate to this page?\n. The content looks good to me. :+1: \n. Nice catch. Thanks for the contribution. Could you sign the CLA first?\nhttps://engineering.twitter.com/opensource/cla\n. Thanks.\n. @mycFelix Thanks for telling us this; it is super helpful. \nSo what you think next steps are? Since it is more an issue of storm kafka-spout (it relies on some config set in-explicitly), and we could barely do a lot of things on that. One option can be have our own Kafka Spout (can be modified from existing storm one) and put into heron-contrib repo or folder.\n. looks good to me. Please make sure the change does not break any existing components.\n. tuple.getSourceComponent() is implemented, though tuple.getSourceTask() is not implemented.\n. Can u add the unit tests for the Marathon Scheduler?\n. Can we also add docs for this Marathon Scheduler?\n. @nlu90 We may need to check completely whether all necessary dependencies are included at one shot. There might be more than just this one.\n. @ajorgensen True. I just want to make sure whether there are still other potential missing dependencies. Let's have a complete check?\n. @ajorgensen If you are hurry with this PR you could go with it for now and we have another potential pull request. @nlu90 is looking at this. Neng what's your opinion on this?\n. Let's consider this upgrade together carefully, since we had a plan/thoughts tryting to replace protobuf with flat-buffer or something similar. If to replace, then this upgrade seems less meaningful.\n. Let's discuss the priorities in hand in a meeting. Not sure how much workload is involved in this upgradation; also, to improve the protobuf-related performance issues for python instance seems not that critical for now, compared with others.\n. @kramasamy @nlu90 @windie I will propose to put all of these on top of heron in another repo or in a folder exclude style checking.\n. @windie @kramasamy @billonahill It may not be a good idea to exclude everything in /heron/storm since there are still codes owned by us in this folder, especially for the parts converting storm into heron. Is it possible in the separate PR that we split this folder into two: one is for codes owned by us enforcing style checks, while another is for codes from outside excluding style checks.\n. @kramasamy\nCurrently these are written by us:\nheron/heron/storm/src/java/\n- backtype/storm\n- org/apache/storm\nThey are converting storm job into heron job.\n. Please don't merge it unless everything is verified properly.\n. Nice catch. Thanks for the contribution. Could you sign the CLA first?\nhttps://engineering.twitter.com/opensource/cla\n. This is because by default we are trying to supply java utility logging (j.u.l) for slf4j.\n@nlu90 is looking at the duplicated logging dependencies which is required by Twitter internally while not needed for open source. We are thinking to remove unneeded dependencies.\n. @nlu90 could have better ideas on this.\n. I don't think it is related to heron.api; could u share the log-files in your sandbox?\nBTW,  heron.api is not encouraged to use for now since it is not stable and would be changed quite a bit in near future.\n. There is no performance difference between backtype and heron.api.\nI want the logs mentioned in here: http://twitter.github.io/heron/docs/getting-started-troubleshooting/\n~/.herondata/topologies/{cluster}/{role}/{TopologyName}/heron-executor.stdout\n~/.herondata/topologies/{cluster}/{role}/{TopologyName}/log-files/\n. According to the container_1_g_6.log.0, it is a known issue:\n[2016-08-10 16:10:01 -0700] com.twitter.heron.instance.HeronInstance SEVERE:  Exception caught in thread: SlaveThread with id: 13 \njava.lang.NoClassDefFoundError: com/esotericsoftware/reflectasm/ConstructorAccess\n    at com.esotericsoftware.kryo.Kryo$DefaultInstantiatorStrategy.newInstantiatorOf(Kryo.java:1233)\n    at com.esotericsoftware.kryo.Kryo.newInstantiator(Kryo.java:1078)\n    at com.esotericsoftware.kryo.Kryo.newInstance(Kryo.java:1087)\n    at com.esotericsoftware.kryo.serializers.CollectionSerializer.create(CollectionSerializer.java:107)\n    at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:111)\n    at com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:40)\n    at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:793)\n    at com.twitter.heron.api.serializer.KryoSerializer.deserialize(KryoSerializer.java:49)\n    at com.twitter.heron.instance.bolt.BoltInstance.handleDataTuple(BoltInstance.java:170)\n    at com.twitter.heron.instance.bolt.BoltInstance.readTuplesAndExecute(BoltInstance.java:217)\n    at com.twitter.heron.instance.bolt.BoltInstance$1.run(BoltInstance.java:142)\n    at com.twitter.heron.common.basics.WakeableLooper.executeTasksOnWakeup(WakeableLooper.java:142)\n    at com.twitter.heron.common.basics.WakeableLooper.runOnce(WakeableLooper.java:74)\n    at com.twitter.heron.common.basics.WakeableLooper.loop(WakeableLooper.java:64)\n    at com.twitter.heron.instance.Slave.run(Slave.java:169)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ClassNotFoundException: com.esotericsoftware.reflectasm.ConstructorAccess\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    ... 18 more\nwhich is reported by @ajorgensen in https://github.com/twitter/heron/issues/1165. And it is fixed in latest master branch and release 0.14.2.\n@xcf1992 Can u give it a try on 0.14.2? Thanks.\n. +1. It is also useful to show the trends of metrics as curve, just similar to the heron-viz.\n. config.setContainerRamRequested(512 * MEGABYTE);\nAs the error indicated, you need to increase the size of ram for every container -- I think you are using RoundRobinPackingAlgorithm, the ram for every instance would be calculated like: (containerRam - daemonServiceRam)/(# of instances on that container)\n. According to the source: https://github.com/twitter/heron/blob/master/heron/packing/src/java/com/twitter/heron/packing/roundrobin/RoundRobinPacking.java#L77\nIt requires 2GB ram for daemon process. You may reduce this value if you want.\n. :+1: looks good to me\n. I remember most state managers could create the nodes automatically if\nthose nodes do not exist?\nhttps://github.com/twitter/heron/blob/master/heron/statemgrs/src/java/com/twitter/heron/statemgr/zookeeper/curator/CuratorStateManager.java#L124\nThe packingplans seem not included.\n2016-08-25 19:40 GMT-07:00 Karthik Ramasamy notifications@github.com:\n\nCan we create this node automatically similar to other nodes?\n\nOn Aug 25, 2016, at 3:37 PM, Bill Graham notifications@github.com\nwrote:\nYeah, you need to manually create state/local/packingplans in ZK.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub, or mute the thread.\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/1303#issuecomment-242610856, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AC422p2MjGIfooGNgHydVG9cRJDovkvgks5qjlIcgaJpZM4Jtj-6\n.\n\n\nWith my best Regards\nFu Maosong\nTwitter Inc.\n. Here is a benchmark report for popular hash functions: https://lonewolfer.wordpress.com/2015/01/05/benchmarking-hash-functions/\n. Yes it did cause memory leak; but this message does not occur a lot and every time the size leaked is small. So the leak would build up slowly.\n. LGTM :+1: \n. Solved at #1464 \n. Solved at #1429 \n. @objmagic Added detailed descriptions for major changes and performance improvement.\n. This looks good to me. :+1: \n. #1445 \n. Resolved.\n. looks good to me. \ud83d\udc4d \n. @kramasamy Updated.\n. #1475 \n. #1481 \n. @kramasamy \nAdded in the description.\n. @kramasamy In fact, here by saying \"reject\" I mean \"discard\". It works in TCP connnection (in fact we tested in TCP connection). It still consumes the entire packet in byte array but not to parse (deserialize) them. On the other hand, the client later would also limit the size of message to send.\nBesides that, do you have any recommandation how to gracefully consume it? In production, the MetricsManager server can throw exceptoin (OOM Error) when trying to parse it. Or should we try to discard the data byte by byte when reading it from socket, to avoid requiring the large byte array?\n. @kramasamy \nThat is a good point; in fact discussed with Bill about this yesterday. \nThe key is: server could not rely on/expect on client side's behaviors too much; that's not safe. Similarly in practice, the security check is always required on server side, no matter whether client side has the security check or not. For heron, we could not prevent people writing a client sending large packets (well, in fact we can prevent this by careful code reviews and asking people to take their own risks if to use clients not reviewed by us....).\nSo the plan is 2-step:\n1. Allow server to dicard packets\n2. Allow client to discard packets in future pull requests\nThoughts?\n. @kramasamy \n\nDo all servers need protection like this from various Heron components perspective? Do all servers need protection like this from various Heron components perspective? If we introduce this, then you need to look at the batch size and number of tuples send from stmgr to Heron instance and vice versa and the config becomes complex.\n\nIn this pull request's implementation, yes all java servers will have this protection. But by default, the value is Integer.MAX_VALUE; people can use the default value by not passing this config.\n\nIf this is a purely a metrics manager issue, my suggestion would be to restrict the size of the metriics at the point of receiving it - probably in the api call itself?\n\nThis issue sourced from Metrics Manager, since for data tuple, we have already done size limitation, for instance, # of tuples in a batch, max size in bytes for a batch, and so on. So if we do the control on client's side solely, then we need to update all clients; or all metrics mgr clients, (java, python, ...).\nThoughts?\n. > I think you need to handle the OOM exception from unpackMessage()\nOn the one hand, OOM is an error (not a checked exception) should be bubbled up rather than be caught and handled. On the other hand, this pull request is only trying to resolve the crashes caused by large packets. In the case you mentioned \" it needs to be handled anyway regardless to size, in theory OOM could happen even for allocating 1 byte.\", the appropriate solution I believe should be to increase the memory for JVM.\n\nAlso, why 131072? The max IPv4 packet size is 65536 (needs GRO).\n\nJust consider 128k is a decent value. Sorry, for packet I mean, the IncomingPacket object defined by us, rather than the IPv4 packet. I could modify the name if needed.\n. sounds good. I would close this pull request and open a new one.\n. Let's close it.\n. LGTM :+1: \n. Can we also check metrics-mgr's log?\n. Any updates on this issue?\n. Is the metrics-mgr's log still exist or deleted?\nIt is a bit different from the case @taishi8117  mentioned: java instance could occasionally get the exception \"connection refused\" when Metrics Manager was not yet initialized; but here we received a NOT_OK response, which rarely happens.\n. Yes they are handling in the same way. \nBut this rarely happens in java instances. Is there a way to get the log to figure out what happened?\n. We can investigate why the execution-state is missing; it should not happen.\n. @wangli1426 We are also working on a stateful processing proposal internally and will discuss it this Friday. Will sync with u after that.\n. @wangli1426 The dicussion on Friday was productive and we got a lot of action items.  For instance, we would come up with the proposal of stateful interfaces exposed to topology developers, which includes and not limits what you mentioned in this issue. Will public the doc once we finish the first proposal draft.\n. @objmagic \nThat means, in a given window:\nFor \"SUM\", when we received multiple values for a metric_name, we would sum them up and consider the aggregated value as the actual value.\nFor \"AVG\", we would consider the mean value as the actual value.\nFor \"LAST\", we would consider the last value received as the actual value.\n. @kramasamy \nyes both of them have been added in the whitelist.\n. \"__server/__time_spent_back_pressure_initiated\": SUM   -- is the one initiated by stmgr\n \"__time_spent_back_pressure_by_compid\": SUM -- is the one triggered by instance\nNotice these are the prefix of the metrics_name. The actual full metric name would contain the instance id.\nSo the back_pressure time for each instance at every collection interval is kept for a period time (currently expire after 3 hours) and people can query it via heron-tracker's RESTful API.\n. step 1 is solved: #1503 \n. :+1: looks good to me\n. thanks for the contribution @tobecontinued - since this is the first time, you are sending a PR, can you please sign the CLA agreement?\nhttps://engineering.twitter.com/opensource/cla\n. @tobecontinued Sorry you don't need to sign it again.\n. \ud83d\udc4d \n. Process death could lead to tuples dropping and then re-emitting. In earlier integration tests, we consider process death a kind of failure. But seems this assumption is not longer true.. The example to use ack is in AckingTopology: https://github.com/twitter/heron/blob/master/heron/examples/src/java/com/twitter/heron/examples/AckingTopology.java\nAnd WordCountTopology is not aimed at that.\nTo remove ack() makes more sense.\n. looks good to me. :+1: \n. It provides the flexibility to run it in a separate thread to enhance performance.. Yes we found that running it in the same thread can provide acceptable performance so far.. The failed ci can be caused by some tricky test cases. @billonahill @objmagic Can you help on this?\nThis pull request looks good to me now. @nlu90 Can you check whether the new guava dependency looks ok to you?. The tmaster will try to re-connect automatically; too many failures will lead to the failure of heron-executor, which will then be re-scheduled to another host. So does the stream mgr.. The znode of TMasterLocation is an ephemeral node so it will be cleaned automatically when the connection between TMaster and zookeeper breaks. And when TMaster restarts/reconnects, it will try to write its location to the same node.. I don't understand why \"they cannot occur at the same time\"?\nwakeUp() can be expected to be invoked in other threads rather than the one waiting.. @pankajroark Hi Can you add more comments on it, and describe the case this pull request covers in this pr's description?. looks good to me. \ud83d\udc4d . The old closed source heron does clean topology info if topology is no longer running. But I don't think it happens in the generic OSS heron scheduler abstraction. \nAurora case can just be a coincidence since it returns 0 even if job does not exist. But when to return 0 can be different for other resources manager.. We can have all 3 different metrics: # of data_tuple, ack_tuple and fail_tuple?. Pull request is here: #1695 . Did you test it locally with null config-value? @huijunw . Looks good to me. \ud83d\udc4d . Let's also log when an instance receives the tuples from stmgr, which is here:\nhttps://github.com/twitter/heron/blob/master/heron/instance/src/java/com/twitter/heron/network/StreamManagerClient.java#L291. looks good to me. \ud83d\udc4d . One thing to notice is that: scheduler will also be run in server side, for instance, yarn-scheduler in container-0. If we exclude scheduler jars from heron-core, then we need to take care of downloading necessary scheduler jars properly.\nBut I agree at least heron-core should not contain all different implementations of scheduler.. @billonahill Yeah this pull request is not for merging for now.. \ud83d\udc4d This would be helpful if we can print HOSTNAMEs of both old and new stmgrs.. Agree that \"leave the configs and update the framework to look exclusively at the packing plan, but that's a larger effort\" is the best solution.. lgtm. \ud83d\udc4d . I think this metric should not exceed 60s. This value is directly reported by stmgr so there might be some calculation errors in stmgr.. It is likely having a version mismatch between heron-cli topology building against with and the heron-cli to submit topology. Could you check it and make them match?. Can we justify the reasons to move to bazel 0.4.x? There are already too many pending issues now and it is good if we can prioritize them.\nAlso, as @objmagic mentioned above, it is probable Bazel 0.4.x can not simply work in Twitter internal env, it could break current Twitter Heron internal building process.. Hi, Did we verify whether it works in local scheduler and aurora scheduler?\nOne simple way is to make bolt.execute() slow (sleep a few seconds every time processing a tuple). Then the bolt will trigger back-pressure soon.. The motivation of this pull request is to handle some unrecoverable situations. There are a lot of in-application unrecoverable situations, which are transparent to aurora so aurora has no ways to handle them. For instance, slow hosts (symptoms can be high execute-latency; building up lag; unrecoverable back-pressure due to the whole topology consumes slower than the data incoming rate; and so on.)\nIt is expected that we could detect all cases mentioned above but it is good to start from a concrete case. We discussed with a lot of Twitter internal consumers and considered \"an instance causing back-pressure for a long period of time without any indication to recover\" is a good case to start. \n@congwang The back-pressure algorithm did improve a lot. But in most cases the back-pressure is caused by slow hosts; we can't do much without moving the containers.\n@billonahill +1. We need to verify this first before merging it into master branch.. @congwang We want to provide a mechanism to restart containers in unrecoverable bad situations. And it is pluggable what situations to track and how to determine whether a situation is recoverable or not. \nHere back-pressure is a case to start with since:\n1. it can benefit customers (customers want it to reduce ongoing operation cost)\n2. customers can use it in practice so it is easier to verify whether this restarting mechanism does help. @objmagic \nThere is an internal ticket with action items (including test plans) and ETA. \nSimply saying, this pull request will not be merged unless:\n1. It is verified working in Twitter distributed env\n2. It is verified by Twitter internal customers (Prasad) that it does provide values\n3. Oss heron is in production inside Twitter.. @kramasamy \nDefinitely we need to figure out the root cause for a back-pressure.  But this pull request does not prevent that and it can benefit topology developers:\n1. It provides a mechanism to trigger handlers when encountering abnormal cases. We investigated and discussed with a lot of customers at Twitter. They mentioned the most common abnormal case is back-pressure and the it is always caused by slow hosts - so they recommended the handlers  could be \"restarting that container on another host\". Notice, it does not prevent other handlers; the reason we provide this handler is basing on customers' feedback.\n2. This feature is not enabled by default: only when people provide specific flags explicitly and manually can they use this feature. So by default, it will not hurt. Furthermore, restarting a container does not prevent further investigation: we have logs; we have metrics. In future, we could also add something like \"heapdump\", \"snapshot\" before restarting to help debug. Besides those, keeping an instance running will not help debug too much, since we have limited interactive runtime debugging solutions.\n3. It will reduce the down-time of a topology, consider at Twitter env, most topologies are actually in \"down-time\" if they can not recover in certain mins (that's also what this pull request designs for). It will also reduce the maintenance cost. Customers want this feature very much.\n. lgtm. \ud83d\udc4d . Please DON'T MERGE this pull request since it is not tested and could potentially break twitter ci building.. Previously we had discussion on this. We thought this error is super abnormal (not due to EventBus corrupt events) and we should LOG(FATAL) to exit the process rather than be resilient to it.. Looks good. I noticed that, during HandleConnectionClose(..), the instance_metric will not be cleaned (removed); will it be a problem too?\nVerifying the changes ahead of merging.. After testing, this pull request does not solve the ongoing issue. WILL NOT MERGE this pull request.\n. Close this pull request due to #1801 . lgtm. \ud83d\udc4d . lgtm. \ud83d\udc4d . LGTM. . @kramasamy This Pull Request looks good tome. Pending for @billonahill to see whether it contains all changes he requested.. Can you explain more about this issue? For instance, background, current status, goal and the justification.. \ud83d\udc4d . @kramasamy It will fail fast for error(https://docs.oracle.com/javase/7/docs/api/java/lang/Error.html). In JAVA, error is different from exception (both extends Throwable, https://docs.oracle.com/javase/7/docs/api/java/lang/Throwable.html); errors means unrecoverable things.\nIt will still try to release resources gracefully for other exceptions.\nFurthermore, it is heron-instance and this logic is in UncaughtException. When we hit this, we have no ideas how to \"recover\"; we can only exit gracefully with best effort.. @objmagic What comments you think to update?. @objmagic I think it is same as the comment I did on Ln236?. I saw there are:\ncom.twitter.heron.api.Config.setEnableAcking(conf, true); \nand \ncom.twitter.heron.api.Config.setNumStmgrs(conf, 1);\nFor those storm samples, do you want to use storm traditional configs:\nconf.setNumAckers(1);\nand\nconf.setNumWorkers(1);\nIn this way we could avoid using heron directly in storm examples.. LGTM. \ud83d\udc4d . @kramasamy People can still access Heron specific config, for instance componentRam, like: https://github.com/twitter/heron/pull/1842/files#diff-b3d9497f7486aba1bca1cc5387948820R60. @kramasamy \nFirst, lets distinguish high level APIs and storm-extension level APIs:\n- APIs in this pull request are storm-extension and are low-level. Definitely we can have another API jar called storm-api-extension providing these interfaces. But then why not just call heron? \n- For high level APIs. I would favor for another high level APIs jar, which is on top of heron, providing features to make develpers life easier, for instance, window supports.\nSecond, lets make sure the goal of heron-storm-compatibility: it is designed for easy migration from storm to heron; it is not for providing more features: new features are in heron.\nIf people want heron features, they should use heron API. These interfaces do not exist in Apache Storm, and people should use these interfaces explicitly with heron Config to reflect they are using some features on Heron, rather than Storm. \nMixture of these two makes things ugly and hard to trace, i,.e, Hard to reason and hard to debug.\nAlso, even if we expose heron API features under storm namespace, it is in fact still heron-api, since: \n-  we own and maintain this part of code. \n- people need to bundle their projects with heron's jars.  (When people develop project with apache-storm, they will have no ways to access these APIs.)\nOn the one hand, it can not provide more benefits than simply calling heron api. On the other hand, it increases maintainence cost.\n. @kramasamy \nExtensions to the Config of Storm by adding more interfaces explicitly, is a bit different from making Config of Storm inheriting from Config of Heron directly.\nI saw @billonahill comments, and it did mention some good points.\nCurrently the Config of Storm inherits from Config of Heron directly, which is error-prone and actually it does become a bug during topologies migration inside Twitter.\nSo even if we want to add extensions to the Config of Storm, it would be better to:\n1. Revert Config of Storm back extending Map, rather than Config of heron. (What this pull request does) \n2. Add those features as methods explicitly. \nAnd I saw @nlu90 has created an issue for it: https://github.com/twitter/heron/issues/1845\n. So far:\n1. No one is using heron-python for prod\n2. The heron-python does not meet the production level of reliability and stablity.\n3. They are not with high-priority.\nI would sugguest we can turn off thoese heron-python unit tests or mark them flaky for now, if it hurts normal travis building. Also, we would ask people to take their own risks for using heron-python.. @objmagic That is dangerous. Even though we can cherry-pick for now but eventually we need to verify them. Also, if we need to fix some bugs, we would have to apply changes on latest code merged, otherwise the code would get diluted. Current deal is, except code for bugs-fixing, non-dead code can be merged only when heron is running fine in production env without any new issues for one week. . This change can break the interfaces provided by Heron. Are there ways we could:\n- precisely match the name of optional args' key, or\n- distinguish the scope heron itself's configs from the scope of user's configs, just like bazel using -- as a separator?\n. +1 for @kramasamy suggestion if it can work. Plz fix eariler broken unit tests in https://github.com/twitter/heron/pull/1831 first rather than hurry for new codes. Otherwise it will block and slow down the whole heron migration process at Twitter; and consequently slow down the whole stateful processing features.. According to: https://github.com/twitter/heron/blob/master/heron/tools/ui/resources/static/js/alltopologies.js#L21\nThe red font means: the tmaster_location does not exist in zookeeper\nThe orange font means: the physical_plan does not exist in zookeeper. @billonahill +1. . Good point. \ud83d\udc4d . Can you add more context in the description? \nFor instance:\n- How current heron works\n- What is the problem of current way\n- How this pull request works\n- What will change?\nThanks. + @billonahill  who works on IUpdate().\nThere are two places storing TopologyState: one is on the node Topology; and another is on the node PhysicalPlan, which includes another copy of Topology.\nBy design, the copy in PhysicalPlan should reflect the actual runtime status of a topology while the node Topology contains meta-data only for first time start-up. \nIt is documented in the comments:\nhttps://github.com/twitter/heron/blob/master/heron/proto/physical_plan.proto#L42\nhttps://github.com/twitter/heron/blob/master/heron/proto/topology.proto#L6\nI suspect in this case it fails to query correct state.. +1. It is happening now on master branch: https://travis-ci.org/twitter/heron/builds/243842085?utm_source=email&utm_medium=notification. For aurora scheduler, it seems we have that port already:\nhttps://github.com/twitter/heron/blob/master/heron/config/src/yaml/conf/aurora/heron.aurora#L31. I am thinking from actual use scenarios:\nIn most cases, people don't want to re-build and re-deploy a topology just to pick up those debugging jvm options; it would be awesome if we can restart a particular heron-instance with new jvm options at runtime.\nSo how about allowing heron-shell to accept a request to restart a particular heron-instance with new jvm options at runtime. (It can bring security concerns though). @billonahill sounds good.. > couldn't batches contain tuples from a mix of sources\n-- I think this is due to the exactly-once algorithm. We need to be able to distinguish and control tuples from different sources, so we can control the markers.. @kramasamy Not really. We are using thrift for ScrbieSink in MetricsMgr: https://github.com/twitter/heron/tree/master/heron/metricsmgr/src/thrift. @billonahill @srkukarni BTW you can also disable the tunnel in config.. Here are my thoughts:\n1. Will different zk endpoints be an issue for data sync across different components? For instance, entries created on cli needs to be read by Tmaster.\n\n\nOne workaround can be: setting up a local tunneling proxy, which forwards the local one to another zk endpoint you expect, i.e. using forwarding to achieve different zk endpoints.\n\n\nNot sure for cpp/python parts, but when designing the zk-statemanager for java, it does accept a list of host:port as connecting-string (https://github.com/twitter/heron/blob/master/heron/statemgrs/src/java/com/twitter/heron/statemgr/zookeeper/ZkUtils.java#L51). In other words, you can pass multiple endpoints and zk-statemanager will pick up the first available one.. Agree with @objmagic . Please make sure it works in LocalScheduler before merging.. Can we come up with a plan to carefully evaluate the performance implications of this? Stmgr is very performance-critical.. @nlu90 Can you also explain in the description what will happen when hitting the limit of mempool?. @ashvina Sure. It is desired to handle by the tools. But:\n\nSometimes it is not easy or efficient for tools to do the aggregation. For example, at Twitter, we leverage a lot on wildcard to do pattern matching, and it does not work well on aggregation if there are metrics missing. Also, with wildcard pattern matching, the aggregation is inefficient.\nFor some metrics, people would care more on the value aggregated on all streams. For instance, \"execute-ns\", which indicates time spending on execute(). It is good to provide those metrics.. @ashvina From overhead point of view, doing aggregation on heron-instance side will be cheapest , most scalable and easy to implement: it will be an in-memory java hashmap get() and put(); also the aggregation will be done distributedly in different heron-instances.\n\n\nWhat if the aggregation takes place at container or topology level.\n\nCurrently the common practice is that metrics-sinks in metricsmgr can handle the metrics in their own ways. For instance, scribe-sink/graphite-sink will send those metrics to Scribe/Graphite directly. It is unlikely to perform the aggregation at topology level.\nIf to do the aggregation at container level, i.e. at metrics-mgr, the implementation can be hard and tricky: it need to specify the metrics to aggregate, and the aggregation is on streams rather than time-series.. @billonahill I have reported this to Twitter Lgeal and they are still working on it. . @billonahill  We checked with Twitter Legal and we are still waiting for their response.. @huijunw \nI think statefulstorage is for the backend to store checkpoints in stateful processing.\nAnd uploader/downloader are for topology tarball storage.. LGTM. \ud83d\udc4d . LGTM. \ud83d\udc4d . Sorry but I don't understand the description: \n\nphysical plans are not going to be download all at the same time causing some components to start sending tuples earlier than others. By initializing the queues earlier allows tuples to be start processing earlier\n\nIf one instance hasn't got the physical plan, it does not know who it is and let alone to process incoming tuples. I don't see the point for an instance to accept data tuples before getting the physical plan. . @jerrypeng \nThanks for sharing the context. I think we need to investigate the root cause, rather than commit something tentatively, since we don't know whether it does fix the issues, or potentially it can introduce other bugs.\nIf you need to skip this for local development, you could have it on a seprate branch or patch.. @jerrypeng @srkukarni \nI went through the code and I found the issue. I filed a pull request to fix it: #2279. With that fix, on the one hand, it makes more sense; on the other hand, we don't need to specify magic default values outside the Communicator.. I don't know why but I can not add @jerrypeng as a reviewer. So at you @jerrypeng here for review this pull request.. \ud83d\udc4d LGTM. @srkukarni @jerrypeng Already requested and the timeout was increased: now it is 150mins. Let me know if it does not work. . @pankajroark @ nwangtw , who might be interested in reviewing this too.. @srkukarni Thanks. It looks good to me. Please make sure we get +1 from other reviewers before merging.. We have it under com.twitter.heron namespace: https://github.com/twitter/heron/blob/master/heron/simulator/src/java/com/twitter/heron/simulator/Simulator.java\nDo you think we need to move it to heron api? \nOr add a wrapper on it? (For this, heron api package would still depend on simulator jar, so why don't users use heron.simulator namespace directly?). @srkukarni This is a huge change. \n@nlu90 Could we plan a time and try this branch within the distributed env to see whether this pull request will work fine?. @cckellogg @objmagic \nChecked the log, it seems the heron-shell's DownloadHandler fails to respond the request from heron-tracker, and throws exceptions :\n[E 170915 05:35:14 http1connection:53] Uncaught exception\n    Traceback (most recent call last):\n      File \"/var/lib/mesos/slaves/e9952b74-f26a-412e-9e22-4dbcd96c1f17-S5987/frameworks/201104070004-0000002563-0000/executors/thermos-adreview-service-prod-adreview_cn_processor-5-90d3c247-8d44-4af1-a8a0-1efa3f44332c/runs/39a9d3fd-8b8d-49ac-9799-c0c33f32a9b6/sandbox/.pex/install/tornado-4.0.2-py2.7-linux-x86_64.egg.ac906166e1f6becd21c962ec755200af39fec42d/tornado-4.0.2-py2.7-linux-x86_64.egg/tornado/http1connection.py\", line 234, in _read_message\n        delegate.finish()\n      File \"/var/lib/mesos/slaves/e9952b74-f26a-412e-9e22-4dbcd96c1f17-S5987/frameworks/201104070004-0000002563-0000/executors/thermos-adreview-service-prod-adreview_cn_processor-5-90d3c247-8d44-4af1-a8a0-1efa3f44332c/runs/39a9d3fd-8b8d-49ac-9799-c0c33f32a9b6/sandbox/.pex/install/tornado-4.0.2-py2.7-linux-x86_64.egg.ac906166e1f6becd21c962ec755200af39fec42d/tornado-4.0.2-py2.7-linux-x86_64.egg/tornado/httpserver.py\", line 282, in finish\n        self.delegate.finish()\n      File \"/var/lib/mesos/slaves/e9952b74-f26a-412e-9e22-4dbcd96c1f17-S5987/frameworks/201104070004-0000002563-0000/executors/thermos-adreview-service-prod-adreview_cn_processor-5-90d3c247-8d44-4af1-a8a0-1efa3f44332c/runs/39a9d3fd-8b8d-49ac-9799-c0c33f32a9b6/sandbox/.pex/install/tornado-4.0.2-py2.7-linux-x86_64.egg.ac906166e1f6becd21c962ec755200af39fec42d/tornado-4.0.2-py2.7-linux-x86_64.egg/tornado/web.py\", line 1884, in finish\n        self.execute()\n      File \"/var/lib/mesos/slaves/e9952b74-f26a-412e-9e22-4dbcd96c1f17-S5987/frameworks/201104070004-0000002563-0000/executors/thermos-adreview-service-prod-adreview_cn_processor-5-90d3c247-8d44-4af1-a8a0-1efa3f44332c/runs/39a9d3fd-8b8d-49ac-9799-c0c33f32a9b6/sandbox/.pex/install/tornado-4.0.2-py2.7-linux-x86_64.egg.ac906166e1f6becd21c962ec755200af39fec42d/tornado-4.0.2-py2.7-linux-x86_64.egg/tornado/web.py\", line 1904, in execute\n        **self.handler_kwargs)\n      File \"/var/lib/mesos/slaves/e9952b74-f26a-412e-9e22-4dbcd96c1f17-S5987/frameworks/201104070004-0000002563-0000/executors/thermos-adreview-service-prod-adreview_cn_processor-5-90d3c247-8d44-4af1-a8a0-1efa3f44332c/runs/39a9d3fd-8b8d-49ac-9799-c0c33f32a9b6/sandbox/.pex/install/tornado-4.0.2-py2.7-linux-x86_64.egg.ac906166e1f6becd21c962ec755200af39fec42d/tornado-4.0.2-py2.7-linux-x86_64.egg/tornado/web.py\", line 180, in __init__\n        self.initialize(**kwargs)\n    TypeError: initialize() got an unexpected keyword argument 'path'. Another thought is to see whether we could integrate with existing tool/service providing similar features, like LogLens.. @aahmed-se @nlu90 I have encountered this issue and I doubt whether those python test methods, like def test_all_zk_supports_comma_separated_hostports(self), are executed. I tried two changes:\n- I put a time.sleep(10) in one test method but the whole test class can still quickly finish and pass in 1s. \n- I also tried to raise an exception explicitly and the test can still pass\n@nlu90 I think we can update the description of this issue to make it more clear.. @kramasamy Do you know any conflicts/issues preventing running python tests with the bazel upgrade?. LGTM. \ud83d\udc4d . @objmagic Did we verify the problematic topology mentioned by Pankaj?. @objmagic Thanks! That would be awesome! Good job!. I am also wondering the APIs are different?. LGTM.. Quick questions: \nIs it feasible to pass env variables by using TOPOLOGY_WORKER_CHILDOPTS and TOPOLOGY_COMPONENT_JVMOPTS in Config: https://github.com/twitter/heron/blob/master/heron/api/src/java/com/twitter/heron/api/Config.java#L51, rather than have a specific Config in this pull request to achieve it?\nWhat are the major benefits to have a specific Config to allow users to set topology environment properties? (Can you also put this in the description of this pull request). @jerrypeng sounds fair enough. Looks good to me. Can you also add the justification you mentioned in the description of this pull request?. @huijunw @objmagic We construct the arguments for heron-executor on aurora file, i.e. on the heron-cli side. We need to make sure a heron-cli will always deploy with the corresponding version of heron-core; otherwise, it will fail.. BTW, executeTasksOnWakeup is not thread-safe; we expect thread coordination from higher level externally. Can we also have the error message in the description?. @jerrypeng @srkukarni . @huijunwu @objmagic Can we check will it break AuroraScheduler?. @srkukarni @kramasamy . #2520 . \ud83d\udc4d  LGTM. @jerrypeng @srkukarni . @kramasamy \nCan we have following parts in the description?\nCurrent issue:\nThe purpose of this pull request:\nHow would this pull request resolve the issue:\nWhat would be when the pull request is merged: (Issues resolved? What's the expected results?)\n. The change looks good but why the travis-ci fails continuously?. @nwangtw could you please review this?. Tested with actual Twitter distributed env.. It would be good if we can add a default value here to prevent the NPE caused by the missing config:\nhttps://github.com/twitter/heron/blob/8ce680d14a6fc6070f7e0d635cb933058d3fce5d/heron/spi/src/java/com/twitter/heron/spi/common/Context.java#L109,\nlike: \ncfg.getBooleanValue(Key.SCHEDULER_IS_SERVICE, false);. LGTM. But lets see how other reviewers say. Do you think we should add back the check of \"TICK_TUPLE_SECS\" in BoltInstance here: https://github.com/twitter/heron/blob/master/heron/instance/src/java/com/twitter/heron/instance/bolt/BoltInstance.java#L305, for backward compatibility to avoid making such thing broken in future?\ncc: @jerrypeng, what do you think?. @nwangtw sounds good.. Also, could we add unit tests or integration tests to catch this issue and avoid it in future? @kramasamy @cckellogg . Also, plz make sure no unit tests will break. LGTM.. Two comments:\n1. Do you think we could make:\nif (System.getenv(\"HOST\") != null) {\n        this.hostname = System.getenv(\"HOST\");\n     } else {\n        this.hostname = InetAddress.getLocalHost().getHostName();\n     } \nas an utlity method rather than duplicate the code multiple times?\n\nUse a more specific system env name like HERON_SERVER_HOST rather than the generic HOST to avoid any unexpected system injection?. @jrcrawfo \nFair. Will give a ship it once:\nThe utility method is added.\nThe Travis Ci build can pass.. @jerrypeng \n\nCan we have some performance tests and show the number? For instance, against the exclamation topology for the throughput/per cpu\n\n\nI am thinking whether this implementation is doing in the right way: it changes the interfaces' expected behaviors. In other words, it would break the interfaces, in some non-functional requirements, like performance, or design philosophy. \nI have some other thoughts to achieve the thread-safe support, for instance: adding new interfaces providing methods safeAck(...), safeFail(...), safeEmit(...), etc. and people can call those methods if they need thread-safety.\n\n\n@srkukarni @kramasamy @nwangtw Thoughts?. @skanjila \nSuppose an instance is expected to receive data in such sequences: tuple_A, tuple_B, tuple_C, stateful-checkpoint-request, tuple_D, ...\nCan we guarantee the instance will do a stateful-checkpoint exactly after the completion of processing tuple_A, tuple_B, tuple_C and before the start of processing tuple_D, if tuple_A, tuple_B, tuple_C, tuple_D are processed in different threads?. @srkukarni \nPreviously heron-instance processed data in a single thread so it will process data in the order it receives, unless users create multi-threads by themselves. Then it is fair to ask them to take care of the order.\nBut after this PR heron-instance allows process data in multi-threads by itself but asking people to guarantee the output order. Then the current effectively-once implementation does not sound like in a good shape or a complete solution. \nIt sounds like:\nPreviously we provide a queue implementation without guaranteeing any thread-safety so if people want concurrency they have to take care both add() and remove. But now we offer a concurrent queue solution/implementation providing thread-safe add() without thread-safe remove(). Definitely, we could ask customers to guarantee the remove() thread-safe on their own but, but the implementation does not sound like an actual complete concurrent queue.\nFurthermore, for the case: tuple_A, tuple_B, tuple_C, stateful-checkpoint-request, tuple_D, ...\nI am not talking about heron to complete processing tuple_A, tuple_B, tuple_C, stateful-checkpoint-request, tuple_D in order; \nMy point is heron-instance should guarantee do a stateful-checkpoint exactly after the completion of processing tuple_A, tuple_B, tuple_C and before the start of processing tuple_D, so A, C, B, checkpoint, D is ok. This can be achieved by using lock/mutex/semaphore/etc.\n. @srkukarni @jerrypeng @kramasamy @nwangtw \nWe need to stop all data processing in a heron-instance and then do a checkpoint, and then recover the data processing.\nMore specifically:\nIn: https://github.com/twitter/heron/blob/c65e312c078e52222b6ea797662715dc03674b36/heron/instance/src/java/com/twitter/heron/instance/bolt/BoltInstance.java\n```\n    if (bolt instanceof IStatefulComponent) {\n      ((IStatefulComponent) bolt).preSave(checkpointId);\n    }\ncollector.sendOutState(instanceState, checkpointId);\n\nAnd:\nIn https://github.com/twitter/heron/blob/28478557d50718725491c2b6a631dea1dc56f485/heron/instance/src/java/com/twitter/heron/instance/spout/SpoutInstance.java\n    if (spout instanceof IStatefulComponent) {\n      ((IStatefulComponent) spout).preSave(checkpointId);\n    }\ncollector.sendOutState(instanceState, checkpointId);\n\n```\nWe need to make sure these two blocks will not be executed concurrently with data-processing. Otherwise, we will get a dirty state.\n. One the one hand:\nCurrently, we claim all interfaces are not thread-safe so users will take full responsibility to do the multi-threads, for instance, getting wrong results. Admittedly, merging this pull request will not break the single-thread case, but this new pull request claims some interfaces thread-safe. So it sounds like a queue implementation has only: 1. thread-safe push() without thread-safe pop() 2. will still work in the single-thread case, and it claims itself a thread-safe concurrent queue.\nOn the other hand:\none thing I mentioned above is:\n```\nif (spout instanceof IStatefulComponent) {\n      ((IStatefulComponent) spout).preSave(checkpointId);\n    }\ncollector.sendOutState(instanceState, checkpointId);\n\n```\nAbove block should be executed atomically. So we would need to make sure the state is flushed out before any furthermore processing. And currently based on this implementation, it seems no way to guarantee that inside the topology if users do emit in multi-threads. So users will no way to implement a correct effective-once topology with the support of thread-safety in this pull request.\nAn example is: an instance completes a checkpoint, but some later tuples are processed and emitted in another thread - users have no way to guarantee the checkpoint is flushed before the flush of later processed data tuples' emits, since this execution is out of the scope of the topology. And then leading a wrong state of downstream instances.\nMeanwhile, it is achievable (achieved in some other stream processing engines) to prevent the concurrent execution of data processing and checkpoint in a heron-instance level. As I mentioned above, \n\nThis can be achieved by using lock/mutex/semaphore/etc.\n\nI like the idea supporting thread-safe interfaces but we should carefully design & implement it and should not break any existing features (like effective-once).\n. Do we have unit tests for this?. Can we:\n1. have pre-check whether the role has enough resources before doing the actual scale-up/down (Careful, by-pass this step if the environment is not prod)?\n\nreject the request directly if the role does not have enough resources, we would, rather than potentially to put the topology in a wired state.. In fact, I have submitted a pull request for this:#2729 previously but could not pass the ci-tests due to some flaky tests. I will close mine.. In fact, I have submitted a pull request for this:https://github.com/twitter/heron/pull/2729  previously but could not pass the ci-tests due to some flaky tests. I will close mine.. It is a POST request changing topology state. Should we use a more expressive endpoint name, like update_runtime_config?. /runtime_config/update sgtm.. Thanks for the explanation. At the same time, I am also looking at the code and thinking about whether we would have better options for a fix.. Currently, heron-instance does gather the metrics gc-count and gc-time-ms for all types of gc in JVMMetrics (https://github.com/twitter/heron/blob/a905adde3888f508dfd6c9b57e1959f093810674/heron/common/src/java/com/twitter/heron/common/utils/metrics/JVMMetrics.java#L411).\n\nAnd the metric name is: __jvm-gc-count/{GarbageCollectorMXBeanName}.\nFor instance, for G1GC:\n__jvm-gc-time-ms/G1-Old-Generation\n__jvm-gc-count/G1-Old-Generation\n__jvm-gc-time-ms/G1-Young-Generation\n__jvm-gc-count/G1-Young-Generation\nNotice, these metrics are accumulative values so you would have to do a rate if you want to know the gc behavior for every particular interval.\nAlso, we have gc metrics summing up all types of gc for a jvm process:\n__jvm-gc-collection-time-ms\n__jvm-gc-collection-count\nAttached an example generated by the file-sink in metrics-mgr: \nmetrics.txt\nlocalhost:1/exclaim1/container_1_exclaim1_1 is using G1GC by adding \"-XX:-UseConcMarkSweepGC -XX:+UseG1GC -XX:MaxGCPauseMillis=200\" to Config.TOPOLOGY_WORKER_CHILDOPTS\n. @srkukarni Any idea what happened?. +1 to do the rate limit on stmgr side. Or even do the rate limit on gateway side is better than on output_collector(slave) side.. Had a discussion with Huijun, and met some new concerns. Hold it for now until we better understand it.. +1 to Neng.\nNormally, in the description we will:\n1. Current status/Problem statement (Why we need this pull request)\n2. What this pull request does \n3. How will the pull request solve the problem (, and how the new status will look like)\n4. Test coverage? etc\nThanks. Change the extension to yaml?\n. Currently we put ConfigLoader source file in heron/common/src/..., and put only config files in the config folder. Could we also put the source code into common src and leave only config files in config folder?\n. Also, could we make SystemConfig.java (heron/common/src/java/com/twitter/heron/common/utils/misc/SystemConfig.java) inherits from this class to reduce the duplicate codes?\n. done\n. Added a safe check.\n. Please pass the container id as a config, rather than a optional argument for cmd, which makes the cmd interfaces ugly and hard to extend in future.\n. What I mean is, to have a generic solution to parse optional arguments in python side and feed them uniformly to the JAVA main method, rather than to have optional argument check inside the JAVA main method.\nI say \"put it in config\", since in earlier design, we have a generic solution that passing the config-override, which is a String including all optional arguments to JAVA main program, and JAVA main program parse this config-override, picking all necessary optional arguments it needs.\n. sounds good. :+1: \n. 1. It is a duplicate method already defined in IStateManager, though in async version. Even if we need a sync version, we shall not put it in LocalRuntimeManager.\n2. A rule of thumb: add a method when we need it. And currently this method is not used.\n3. The correct way should be to add a sync version in some utility class, which should be handled in future pull request to make this one focused.\n. It is not that easy to come up with appropriate signature of prepare() and post(); so I would like to put this in future refactor pull request, rather than complicating this one.\n. It is not that easy to come up with appropriate signature of prepare() and post(); so I would like to put this in future refactor pull request, rather than complicating this one.\n. done\n. It is not doing the same validating at several places; it is getting whether topology is running or not in several places; and due to the interfaces are async so extra codes used to get the value. Future pull request should provide a sync version.\n. It is not that easy to come up with appropriate signature of prepare() and post(); so I would like to put this in future refactor pull request, rather than complicating this one.\n. It is not that easy to come up with appropriate signature of prepare() and post(); so I would like to put this in future refactor pull request, rather than complicating this one.\n. It is used to construct the endpoint of command request, which is a String. It is a utility method which separate the construction details\n. The command is converted into Enum once it enters the program. It enhances safety during runtime.\n. Make sense. It is good to inject IRuntimeManager, put RuntimeManagerRunner.call() in a try block, and close any resources in a finally block externally.\n. @osgigeek True. It requires changes also on sever sides; we can open an issue and enhance this in future.\n. Enum is to avoid missing of developers, for instance, sometime passing the wrong String valie, i.e. passing the topologyName as Command which I did earlier and spent quite some time in testing to figure out the issues.\n. @kramasamy This should be in future PR to avoid complicating this PR too much, since redesign the interfaces between server and client are not a trivial work.\n. @osgigeek Injected IRuntimeManager. Put main logic in a try block and released the resources in a finally block.\nIt can guarantee the release of resources, also enhancing the testability. Thanks\n. Updated. Thanks\n. @kramasamy did this and he mentioned in #127 . He would take care of this in future PR.\n. Good idea. Done.\n. Looks good. But are there any reasons that the location value stored by TMaster fails? Will this also be an issue for distributed production mode?\n. We may need investiate this to identify the root cause. IIRC, for local hostname, for instance, tw-mbp-vikasr, it uses etc/hosts to resolve to local host. Can u try add the mapping in etc/hosts and see whether it works?\n. Hi, this PR is unnecessary.\n1. In run_process(), it will invoke \" do_print(\"Running %s process as %s\" % (name, ' '.join(cmd)))\" to print the info.\n2. Don't use print directly; use do_print method.\n. why we have config value in source code?\n. Why we need a new yaml file, instead of just putting in current yaml files?\n. Customization on AuroraScheduler is required if people want to use other uploader; implementation is specific to different scenarios.\n. Map auroraProperties contains only the properties-binding for aurora command.\nReasons not using Config Structure:\n1. Those properties binding are not static; cannot read directly from a static file. Call it Config is a little misleading\n2. Config strucuture is complicated (one more layer on top of map, requiring complicated codes to construct it with some conventions) and unnecessary compared with a local variable map -- putting all properties to bind in a map, iterate them to construct the aurora command later.\n3. The Config structure is not only complicated, but also not feasible in this case: it does not provide the iterator interface.\n. 1. Can't we directly form the aurora command from Context?\n   The aurora command follows specific syntax, for instance, \"--bind\" is needed for every property binding. Put the properties in a map and then construct the command can reduce size of code and make the logic cleaner.\n2. Perhaps, it might be good to write a Command Builder?\n   The aurora command follows specific syntax, not to share with other resources manager, so it is unnecessary to make it a specific class, which increases the cost to understand, use and maintain. \n   It may be better to have a separate method (not class) to construct this command inside this class. Or when adding the AuroraRuntimeManager in future PR, it may be better to have a specific AuroraUtilities class containing all this command construction methods. Will make it when it is needed.\n. getHeronAuroraPath() returns a String representing the aurora file location, which is required for the aurora command to submit a job: aurora job create ....... *.aurora\n. This is defined in proto file. May open an issue for this change if needed. Will not include in this PR.\n. done\n. Currently, we are using both. SHARD_ID for a container is an Integer, the index, for instance, 1,2,3. CONTAINER_ID is a String, for instance, \"container_1\". Currently these concept are used mixly in nearly everywhere, e.g. executor, stream mgr and so on. It is a good idea to make it consistent.\n. Modifications are always needed for different uploader. Modifications are always needed. Let's compare the modifications required for \"generic\" solution you mentioned and current PR one.\n\"Generic\" solution:\n1. Modify the uploader config for appropriate command.\n2. Modify the AuroraLauncher, since the command contains variable and people have to modify the AuroraLauncher to feed with appropriate arguments\nThis PR solution:\n1. Modify the aurora file for appropriate command.\n2. Modify the AuroraLauncher, since the command contains variable and people have to modify the AuroraLauncher to feed with appropriate arguments\nFurthermore, the \u201cgeneric\u201d solution has following problems:\n1. It adds dependencies between aurora file and config file. People can get confused: \u201cgiven that I always have to modify something in aurora file, why I should modify the config file rather than modify the aurora file directly\u201d?\n2. The aurora file to use will be generated in runtime (may in some tmp folder) \u2013 hard to debug or trace.\nI don\u2019t think the \u201cgeneric\u201d solution is more generic.  And I doubt whether there is a simple generic solution and whether it is worthy.\n. The solution you mentioned assumes that sandbox downloads dependencies from URI. This is a fair assumption, though in some cases it does not work, for instance Packer. \nUnder this assumption:\n1. No much difference from current PR one with the generic one you mentioned, as compared above. The only thing in AuroraLauncher potentially causing  confusion may be  the config \"heron.uploader.localfs.file.system.directory\", which in fact is just the prefix URI to topologies REPO, rather than binding only with local file system. Let me change it to something like \"heron.uploader.topologies.repo.uri\"\n2. heron.aurora now is also placed in conf folder. Changing on aurora file directly is easier to understand, maintain. And also reduce the complexity to pass \u201ccommand line\u201d config value, which is bug prone.\n. Let me make this config a better name to avoid the confusion.\n. Aurora requires some runtime binding, which are variables and shall not be considered as Config. Introducing new set of keys can clearly decouple the dependencies and provide enough flexibility\n. I updated the PR and current one is already doing \"heron_core_release_url = '{{CORE_PACKAGE_URI}}'\". Can u refer to the latest changes?\n. Isse created: #215. Bad to include this change inside this PR.\n. done\n. I updated the PR and current one is already doing \"heron_core_release_url = '{{CORE_PACKAGE_URI}}'\". Can u refer to the latest changes?\n. This one is consistent with original one. Not sure whether some inconsistent variables name introduced during the last big refactor of Scheduler.\n. sounds good. done.\n. sounds good. done.\n. Removed the one in uploader.yaml\n. sounds good. done.\n. removed it in uploader.yaml\n. sounds good. done.\n. sounds good. done.\n. According to the latest aurora, IsProduction is not required. Removed it.\n. sounds good. Done\n. To indicate whether there is no scheduler running. For instance, True for AuroraScheduler and False (by default) for LocalScheduler.\n. It is decided that prepare and post shall be invoked even when there is no scheduler_rest_endpoint (it is expected to have no scheduler running). Not invoking post in this case is a bug.\n. Yes, if there is not running scheduler and it is designed to have no scheduler running, for instance, AuroraScheduler, connection is null; and postDeactivate shall still be invoked.\nOnly when \"no_scheduler_running\" && \"it is expected to have scheduler running\", the method returns false early, i.e. postDeactivate will not be invoked.\n. it is likely that I didn't explain the scheduler abstraction to u clearly earlier. Will explain that.\nIt is not clear to me why this is needed - when no scheduler is running you invoke postActivate and when scheduler is running it is not invoked. it seems very confusing to me.\nA: Please double check the code. postActivate is invoked too when scheduler is running. We follow prepareActivate-onActivate-postActivate, and only when earlier steps failure will not to invoke postActivate (return false directly.)\n. This is an utils method shared by a lot of scheduler; do u have any recommendation where to put this method?\n. This one is for cases no scheduler exists; Aurora is one example of this case.\n. Given that this method will be used in all activate/deactivate implementation, I am considering to put it inside RuntimeManager, and invoke it after preapre...(), before onActivate/onDeactivate. How do you think about it?\n. Yes. These cleanings are needed for all scheduler implementation, so put it inside RuntimeManagerRunner rather than duplicating them in every implementation.\n. Consider AuroraScheduler as an example of no_scheduler_endpoint implementation, so this will be put either in prepare() or post(). The reason I didn't put it prepare() is to avoid false negative, i.e. prepare() returns true while post() returns false -> the whole restart return falses while in fact the restart does take effect.\nGiven this tradeoff, I would like put actual restart in postRestart(). What do you think of this tradeoff?\n. createHttpConnection() will just create a URLConnection instance, i.e. does not establish the actual network connection. (https://docs.oracle.com/javase/7/docs/api/java/net/URL.html#openConnection())\nSo as long as SchedulerLocation is stored in state manager, it creates the instance. If the location is not reachable, it throws exceptions in later's sending http request to scheduler, which has already handled the case you mentioned.\n. sounds good. Done\n. sounds good. Done.\n. sounds good. Done. In fact, if we pass isVerbose true, ShellUtils.runProcess(..) logs the command already. So even no need to duplicately log the command.\n. sounds good. Done.\n. This is an utils class; private constructor added.\nAdmittedly, to add MasterClient Interface and HttpTMasterClient is a good and clean point. But I prefer not to add TMasterClient Interface now (at least not in this PR) since currently TMasterServer accepts only http request and it is ok to treat it as a simple utils class. Can create an issue for future tracing and make this enhancement in future if needed. Similarly, we can also have SchedulerClient interface and HttpSchedulerClient implementation. Requiring more comprehensive design on it.\n. Issue created: #251 \n. Is it feasible to invoke loggerInit (https://github.com/twitter/heron/blob/master/heron/common/src/java/com/twitter/heron/common/utils/logging/LoggingHelper.java#L18) rather than put another method here?\n. Sorry, I mean, \"secondly, try to submit a topology\". Will correct it.\n. I was also considering about this problem. I didn't consider java URI class is a good option since it is inconvenient to use when taking a look at the usage;\nI was considering whether to use \u201cString\u201d v.s. \u201cObject\u201d.\nObject may be too generic, but \u201cString\u201d may not cover all scenarios. For instance, packer, it returns a triple  rather than a simple String. Admittedly, we can convert this triple into a String format.\nTell me how you think about this tradeoff: Make it more specific to be more intuitive, or make it generic to cover all cases.\n. same as above.\n. same as @osgigeek . Inteface allows only methods with public modifier, and by default they are public, i.e. public modifier is duplicated.\n. @osgigeek sounds good. Updated.\n. hadoopConfigDirectory is more C/C++ style, while \"get prefix\" is JAVA style.\n. same as above.\n. This config is always expected. Do you have any recommendations where to put a sample config file for HdfsUploader?\n. OK I can remove get prefix for now.\n. Added sample config yaml.\n. why add another argument \"user_classpath\"? why not just pass the value to extra_jars?\nThis change is making the method hard to understand.\n. Should we pass this config via Topology Config or heron-cli config option? \n. It has both undo() and close().\nundo() is invoked in failure cases while close() is invoked in any cases.\n. sounds great. done.\n. Also created utils methods to avoid duplication in CuratorStateManager too.\n. commons-logging-java has already in 3rdparty/logging/BUILD; no need to add extra jars.\n. joda-time has already in bazel built-in 3rdparty folder: ~/.bazel/base_workspace/third_party/; you may use it via something like \"\"@bazel_tools//third_party:joda-time\". No need to add extra jars.\n. Could u add comments for previousVersionFilePath and pathPrefix? These two variable are not intuitive to understand.\n. Are u trying to log only error message? It could be better to log also the stack trace.\nYou can use this method: https://docs.oracle.com/javase/7/docs/api/java/util/logging/Logger.html#log(java.util.logging.Level,%20java.lang.String,%20java.lang.Throwable)\nSame for rest logging calls in SEVERE level\n. A minor thing to note. Consider using the logger api\nlog(Level level, String msg, Object[] params)\nthe advantage of using that API is you wont incur the overhead of string concatenations unless log level is enabled. Here you still do all the string cat work even if log levels are at ERROR.\nSame for rest logging calls.\n. So this method always returns true?\n. Add newline at the end of the file.\n. I agree the performance impact is likely negligible. Just a better practice.\n. Please refer to the latest change: I put the lock acquire(set topology def in state manager) and release outside.\n. Please refer to the latest change: I put the lock acquire(set topology def in state manager) and release outside.\n. Good idea. I have discussed with Karthik about this a long time ago. And will make it in next PR (make calls in SchedulerStateManagerAdaptor sync).\n. Current the lock is acquired when submitting a topology and released when:\n1. killed a topology\n2. failed to submit a topology. \nSo it may not able to encapsulated it in submit. Or do you have other ideas on this?\n. Sounds good. Done.\nI am thinking this: Should we make TopologyLock a pure utils class or a normal object can be instantiated?\nThe lifecycle of this lock is from the submission to death of a topology,\nrather than just in one phrase. Prefer to consider it as a utils class. \nWhat do you think?\n. We need proxy in two places:\n1. Talking with state manager\n2. Talking with scheduler\nShould we consider to apply one proxy to both of them, (or more in future)? Or should we explicitly distinguish them?\nIn practice, applying with the same proxy is good enough. What's your opinions?\n. Oh I understand now. This solution looks good but TopologyAPI.Topology is a class generated by protobuf.\nWe may have a customized Topology class encapsulating these as you mentioned and use this in other places. But I would prefer not to put this refactor here since currently TopologyAPI.Topology is used anywhere and this refactor will involve a lot of files changed; not a trivial work.\n. Sounds good. Let's separate them.\nThe state manager case is a little tricky. Can u first add only the scheduler proxy, i.e. changing the config name more specific for scheduler, for instance, heron.scheduler.proxy.connection.string and so on.\n. 1. lock/unlock/acquire/release are terms used in concurrent programming rather than just multi-threading. You can consider TopologyLock here as a file locking (https://en.wikipedia.org/wiki/File_locking), while the persistent storage is state manager rather than local file system.\n2. acquire and delete have side effects other than just acquiring/releasing some lock construct. These actually write and delete data from ZK nodes.\n   Yes, similar to the file locks, restricts submission by allowing only one user or process access to the state manager's node at any specific time.\n3. The acquires and releases are happening from different files. There is no clear owner of the \"lock\".\n   A topology owns the lock. Currently the lock is acquired when submitting a topology and released when:\n4. killed a topology\n5. failed to submit a topology. \nA better way to model this problem might be \"initiateSubmission\", \"commitSubmission\" and \"rollBackSubmission\". Killing topology can similarly have initiation and commit phases.\nCurrently  in fact, the submission and killing are doing these  separated phases, except write ahead log or call these methods explicitly. Do you consider we need to add these methods explicitly?\n. Our use case is not to check for existence of a topology. As the source code shows, it is trying to setTopologyNode -- it is used to restrict multiple users to submit a given topology at the same time.\nOne concern is: the behavior is undefined when a topology is submitted and killed from different client at the same time. Multi-killing may or may not be an issue, depending whether we guarantee only best effort killing.\nAnother possible solution is to have an extra specific node on state manager (a shared persistent storage) as lock. Any submission or killing need to acquire the lock and release it when it is done. This can restrict only one submission or killing happen at any same time. The tradeoff is we need to put extra node on state manager. And two more interfaces will be added to SchedulerStateManagerAdaptor or even in IStateManager: acquireLock and releaseLock.\n. will be reenabled in later PRs.\n. sounds good.\n. removed\n. release or release.yaml?\n. release or release.yaml? Can we make them consistent?\n. Why not generate an appropriate yaml-formatted release-version file during building & releasing, and then simply copy it to the topology.tar.gz during submission? Why we need such complex translation during submission? It is easier to reason and maintain, since we would modify and generate release-version info in one shot.\nThen during submission, we can read this config and use when needed.\n. What's the difference between \"config_path\" and \"config_files\"? Could u rename it to a more verbose name and add comments to clearly distinguish them?\n. Is it used?\n. Is it feasible to have a common target at the top, such as \"common_files\" or something else to avoid including this hard-coded filename in all building targets?\n. Please rename it to an appropriate argument name to avoid any confusion witht the heron-instance's jvm options.\n. I know what this config is for; but the name can mislead topology developers that they can set the jvm configs for instance via this config. As for \"topology-jvm-property\", it is easy to be considered the properties for running topology, i.e. instances,\n. Can u put the default value in defaults yaml rather than in source code?\n. Sounds good. Done.\n. Yeah u mentioned a very good point.\nCurrently different commands refer to different http endpoints, so different connections are required.\nBut it is really wired that we can not invoke another method after calling one method in HttpServiceSchedulerClient. \nI have changed HttpServiceSchedulerClient, so now:\n1. It accepts SchedulerLocation in constructor rather than HttpConnection. And HttpConnection is created and closed inside method \"protected boolean requestSchedulerService(Command command, byte[] data)\". So the issue u mentioned is no longer there.\n2. It contains \" createHttpConnection(Command command)\" and \"getCommandEndpoint(String schedulerEndpoint, Command command)\".\n. Fair. changed.\n. On the one hand, AuroraUtils class is already there.\nOn the other hand, it can decouple AuroraScheduler from the actual Aurora Commands, which may help in future if we need to modify the actual Aurora Commands, for instance, add new options to create an aurora job.\nLet me know what you think about it.\n. Thanks. Corrected.\n. Changed to transitionTopologyState. Tx.\n. It is designed to be an Integer, right? Then we should make it as specific as possible. It does not make sense to change it to a generic string and then every implementation converts it back to Integer; not safe.\n. What you suggest can lead to a situation that all states are cleaned while the job is still running, which is not expected:\n1. Heron considers the job not running since all states are cleaned, so no way to kill the job again via \"heron kill\". It can be hard to clean the running job, since the communication to Resource Manager can be complicated.\n2. The job runs abnormally since states are cleaned; but it does not release the resources.\nI prefer the \"heron kill\" with the semantics:\n1. Best effort\n2. No false positive. False negative is fine.\n3. When \"heron kill\" fails, it can be re-run and eventually it returns true and does all killing related things completely.\n. We can discuss this later; it is not related to this PR.\n. changed.\n. sounds good. Refactored AuroraUtils into AuroraController as suggested.\n. extra_scheduler_classpath? Same for below.\n. Yes, tmaster containerId is always 0. -1 means to restart the whole job, including container 0, containging TMaster\n. It is not necessary to start TMaster before all other containers. \nBut the topology will start emit and process data only after all instances in all containers, including TMaster, are ready.\n. This config is no longer needed.\n. Why not read value from Resource in PackingPlan?\n. Why convert it into int value? Can Reef support double value of cpu requirement?\n. Doubt whether it will work in distributed environment. In normal cases, we would request ports as a kind of resource from resource manager (Mesos or Aurora) ahead, and then use these pre-reserved ports to scheduler a container to guarantee the availability and isolation of ports resources.\n. 1024 will be more fair.\n. Better to make it into one line: \"Failed to start topology {0} due to {1}\".......\n. Better to make it into one line: \"Failed to start topology {0} due to {1}\".......\n. TMaster (or container 0) is designed to co-locate in the same host as Scheduler. Otherwise, it can not handle some failure cases nicely, for instance, network partition. You may leave as it for now but add an issue and TODO for that.\n. https://docs.oracle.com/javase/7/docs/api/java/util/logging/Logger.html#log%28java.util.logging.Level,%20java.lang.String,%20java.lang.Throwable%29\n. Should we fix this command, or remove \"set -e\" directly?\nIt can hide potential errors in future.\n. sounds good. updated.\n. It is a bug; PR is here: https://github.com/twitter/heron/pull/460\n. I noticed that Karthik changed the variable name in another PR. Should I leave the change to Karthik or make the change here?\n. Renamed it to incrValue. Rest changes w.r.t to check style in this file will still leave back to Karthik's PR.\n. Can u add comments about what it is for then we can better maintain it in future.\n. \"all instances get\" -> \"all containers get\"\n. in fact it also spawns stream-mgr, which is related to stream routing, but not related to user code.\n. sounds good. done.\n. The original one logs the stacktrace of the Throwable. Could u double check whether the new one also does that? Or just the exception message.\n. Add a comment here: short sleep to avoid busy looping\n. Add a comment here: short sleep to avoid busy looping\n. Add a comment here: short sleep to avoid busy looping\n. Add a comment here: short sleep to avoid busy looping\n. Use Log.log(Level.Severe, ... {0}, ...) style\n. Use Log.log(Level.Severe, ... {0}, ...) style\n. Use Log.log(Level.Severe, ... {0}, ...) style\n. Use Log.log(Level.Severe, ... {0}, ...) style\n. Use Log.log(Level.Severe, ... {0}, ...) style\n. rename it to aTopology?\n. It passes the style checks; style checks apply only on source code. Need we add it?\n. Can we make environ = 6? similarly, make role = 7 and release_state = 9?\n. Can we also remove uploader_version?\n. rephrased to \"Failed to set topology definition\"\n. sounds good. Done.\n. @saileshmittal . Do you want the it \"optional\", or \"repeated\" for more than one link?\n. In Twitter case, there are two: heron-aurora page link and heron-viz page link, right?\n. updated. \n. Yes. Addressed.\n. New line at the end\n. make sense. Updated.\n. Default is true, inconsistent with thr PR's title. Can u change the title/decription of the PR?\n. the utils-spi-java dependencies are huge, for instance, even guava. I just don't want to complicate uploader dependencies.\n. Will we then have too many fragmented utils folder? In fact, a lot of class in current spi/utils folder are not shared by different components, though shared by different classes of the same components, for instance, different implementation of uploader or scheduler.\n. If the code is built with bazel, it will work in both cases:\n1. bazel\n2. command line.\nBut if it is not built by bazel, it may not work. And in fact, earlier didn't work either since it uses a wrong relative config file path.\n. Yes.\n. Currently this one is not even used; locally we log everything to console. It is good to remove it now and add if really needed in future..\n. Hi @ashvina \nCurrently the issue is that we need to build Config before instantiating a SchedulerMain instance to do some static prepare setup, for instance, logging setup.\nIs that possible that you build the Config on your local client side and pass it as argument to SchederulMain constructor? Just similar to how you pass role, cluster and other arguments earlier. The main difference is not passing primitive java type.\n. In java/cpp, this is in OutgoingPacket. Should we follow the same pattern as java/cpp to have both: \"IncomingPacket\", and \"OutgoingPacket\", and providing similar interfaces?\n. True. it would be great if you can create a pull request for that.\n. done.\n. done.\n. This config is no longer needed/used in latest branch.\n. We have a new utils method executorCommand() in SchedulerUtils; you may use that to reduce duplicated code.\n. You need to have a new line at the end to pass java style check.\n. You need to have a new line at the end to pass java style check.\n. Just to confirm: if you specify this config as False, there will be no running scheduler process as service. In other words, hpc needs to take care of failure of executor by itself, for instance, to restart a failed executor.\n. LocalFileSystemStateManager is more used for testing; it may not be able to handle some corner cases, for instance, master election for potential multiple running tmaster (which should not happen). It is recommended to use the zookeeper one. (CuratorStateManager, sample conf under localzk folder)\n. Just to confirm: Not sure whether the API provided for NFS is the same as local fs. If not, you may need to write your own Uploader to use the appropriate API.\nAlso, LocalFileSystemUploader is more used for testing. Not sure whether it is robust enough for corner cases.\n. It is recommended to write Context in style similar to HdfsContext: https://github.com/twitter/heron/blob/master/heron/uploaders/src/java/com/twitter/heron/uploader/hdfs/HdfsContext.java\nThat is:\nPut java constant CONFIG_KEY_NAME at the top and call config.getStringValue(CONFIG_KEY_NAME) simply.\nThis PR's implementation is a little hard to trace or understand. (We may also refactor current Context similar to HdfsContext later too.)\n. Can we invoke the above runProcess(..)? They look a little duplicated.\n. Add the comment that: if failed to read from file, it returns an empty list of String\n. So it assumes the format of file? Could we also document down the format?\nAlso, it seems only the first line of the content is used?\n. You may directly log hpcCmd.toString. No need to construct the String command individually just for LOG. Also, I believe LEVEL.fine is a better Logging.LEVEL here.\n. It can be better to have a individual method for this data formation. Also, it can be better to construct and return a new List of String, instead of changing in-place.\n. This file can be removed if the HPCContext follows the style of HDFSContext or ZkContext\n. This file can be removed if the HPCContext follows the style of HDFSContext or ZkContext\n. Given that this method is non-trivial and shared in some different places. Should we move it to utils? What do you think about it?\n. You can also use: Runtime.numContainers(runtime);\n. Just to confirm: is this createJob a blocking call?\n. Any plan to implement this feature?\n. Given that this method is non-trivial and shared in some different places. Should we move it to utils? What do you think about it?\n. Given that this method is non-trivial and shared in some different places. Should we move it to utils? What do you think about it?\n. If it is not a service, for the whole lifecycle of a topology, this method should be invoked only once -> so the case \"overwrite\" should not happen. Should we set \nreturn setData(getSchedulerLocationPath(topologyName), location.toByteArray(), isService); ?\nOr should we have something like:\n    if(!isService){\n      AssertTrue(Scheduler.SchedulerLocation.notExist)\n    }\n. Should we also update the interface of deleteSchedulerLocation?\n. BTW, we accept \"Config config\" from IStateManager.initialzie(), from where we can also read \"isService\", need we pass it as parameter from this method?\n. Add comments why to suppressWarnings\n. Make it as a field of the class.\n. Make it as a static final constant at the top since it is shared by a lot methods.\n. Can u add unit tests for the rest public methods?\n. Let me do that. I will move them.\n. Currently we are write the default value in the source code: https://github.com/twitter/heron/blob/master/heron/statemgrs/src/java/com/twitter/heron/statemgr/zookeeper/ZkContext.java\n, via method: public Boolean getXXXValue(String key, boolean defaultValue)\n. corrected.\n. This file has already executorCommand() above. Could u use it directly?\n. Consider using the logger api\nlog(Level level, String msg, Object[] params)\nthe advantage of using that API is you wont incur the overhead of string concatenations unless log level is enabled. \n. Consider using the logger api\nlog(Level level, String msg, Object[] params)\nthe advantage of using that API is you wont incur the overhead of string concatenations unless log level is enabled. \n. same as above\n. String.format(\"--ntasks=%d\", containers)  ?\n. File file = topologyWorkingDirectory == null ? null : new File(topologyWorkingDirectory);\nreturn 0 == ShellUtils.runSyncProcess(isVerbose, false, slurmCmd, stdout, stderr, file);\n. Consider using the logger api\nlog(Level level, String msg, Object[] params)\nthe advantage of using that API is you wont incur the overhead of string concatenations unless log level is enabled.\n. could u add a sample slurmCmdArray as comment here?\n. Can u break down the executorCommand(...) by invoking this method executorCommandArgs(...)? You can just copy-and-parse the snippet of code in executorCommand(...) and put it here.\n. commands.add(FileUtils.getBaseName(Context.topologyDefinitionFile(config)))\nNo need to introduce apache dependencies.\n. commands.add(Runtime.instanceDistribution(runtime));\nArgument packing is duplicated here.\n. This will print the hashcode of the array executorCmd\nLOG.log(Level.FINE, \"Executor command line: {0}\", Arrays.toString(executorCmd));\n. LOG.log(Level.SEVERE, \"Failed to read the Slurm Job id from file: {0}\", jobIdFile);\n. The config is removed, no longer used/needed.\n. I don't think LocalFileSystemStateManager will work in distributed environment,\n. Here you may need to specify the JAVA_HOME in target slave hos running heron.\n. Will getFreePort() work in distributed environment? Need we request the ports ahead beofre deployment and then use the reserved ports provided? Otherwise, how could we guarantee and isolate the resource?\n. https://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/common/ShellUtils.java#L202\n,\nhttps://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/common/ShellUtils.java#L180\nor,\nhttps://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/utils/SchedulerUtils.java#L393\n,\nhttps://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/utils/SchedulerUtils.java#L355\ncan be used.\n. https://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/common/ShellUtils.java#L202\n,\nhttps://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/common/ShellUtils.java#L180\nor,\nhttps://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/utils/SchedulerUtils.java#L393\n,\nhttps://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/utils/SchedulerUtils.java#L355\ncan be used.\n. Though we do a check for \"null\", according to the design/comments of this method, it should return empty list when there are no job pages associated.\n. Fair.\n. sounds good.\n. Then could remove this config, or provide a stub value for it with comments?\n. Should we also update the class name of source code from LocalMode to SimulatorMode?\n. Should we also update the class name of source code from LocalMode to SimulatorMode?\nSame for below's LocalMode usage.\n. Updated.\n. No, it just works as that config not specified; then we use the default value.\n. What if the connectionString is a list of zkservers, separated by comma?\n. In heron-internals.yaml, configs are separated in two parts: generic for any component, and component-specific.\nAny configurations in heron-internals.yaml currently can not be overriden by heron-cli, for instance, the configurations for heron-instance; only config related to scheduler, for instance, uploader, scheduler etc, can be overriden, whose docs are located under deployment section: https://github.com/twitter/heron/blob/master/website/content/docs/operators/deployment/configuration.md\n. For \"internal\", I mean it is config applies to the whole cluster, and is transparent to topology developers (it is impossible for them to override it)\n. sounds good. updated.\n. The doc is a little outdated: IConfigLoader or IRuntimeManager is no longer there. You don't need to update them in this PR, but can u add an issue so we can trace and correct Scheduler-related docs?\n. This leads to the issue that storm-compatibility, (or anything with simulator), contains the whole heron-instance dependencies, which is not expected since it complicates the storm-compatibility's dependencies a lot.\nOne alternative can be breaking current heron-instance build target into 2 parts: shared part can be used by other components, and the non-shared part.\n. Can u make it into two classes: IncomingPacket & OutgoingPacket with similar interfaces in c++/java IncomingPacket & OutgoingPacket?\n. sounds good; updated.\n. sounds good; updated.\n. sounds good; updated.\n. We should have only one heron version. And api, spi, cli or others should use that version number too. It makes all sub-components in heron with consistent version.\n. heron-spi\n. heron-spi\n. We should have only one heron version. And api, spi, cli or others should use that version number too. It makes all sub-components in heron with consistent version.\n. done.\n. done\n. done\n. We determine whether the logging level is Level.FINE or Level.INFO, basing whether a verbose flag is passed. \nBy design, we don't want to show the stdout of a spawned process unless a verbose flag is passed, since:\n1. We should provide more meaningful INFO in itself, rather than rely on the stdout of a spawned process.\n2. Also it is more controllable, since we have not ideas what the stdout can be. (potentially very large as you mentioned)\nBut we want to show stederr no matter the verbose flag being passed or not, since it is more considered as a kind of error message. (Level.SERVER)\n. Agree. Another question is: should we keep one single troubleshooting page for the whole heron, or keep two: one for topology developers while another for heron developers (or operators).\n. Can we also print out the content of \"parts\"?\n. Can u also add some sample errors output, so people can associate their errors with this trouble-shooting?\n. Sorry this is just for debugging. Will push mergable commits later.\n. Sorry this is just for debugging. Will push mergable commits later.\n. Sorry this is just for debugging. Will push mergable commits later.\n. Sorry this is just for debugging. Will push mergable commits later.\n. java.util.Optional is since 1.8 and currently we are still on JDK1.7\n. https://github.com/twitter/heron/issues/945\n. True. The PackingPlan (https://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/common/PackingPlan.java) is exactly what you want: strongly typed objects, like a Container that has a Collection of Instances, which each has typed \"Resource\".\nCurrently PackingPlan is constructed by providing the actual value directly like:\npublic ContainerPlan(String id,\n                         Map<String, InstancePlan> instances,\n                         Resource resource) {\n      this.id = id;\n      this.instances = instances;\n      this.resource = resource;\n    }\nso we need to come up with some intermediate variables for conversion, i.e. \"maps of Strings to Maps of Strings to Longs\" you mentioned. It can be a good idea to build PackingPlan via Builder patter, allowing people to edit it with typed safety.\nIssue created here:\nhttps://github.com/twitter/heron/issues/946\n. It was but not now. It is possible to be used in future. I can remove it if you want.\n. Move this to packing_deps_files below?\n. Let us not directly depend on utils-spi-java; it contains too many deep dependencies. \nTwo options can be considered:\n1. Add more specific building target in spi/src/java:BUILD\n2. Re-org classes in utils, for instance, put SchedulerUtils into spi/src/java/scheduler or something similar.\n. Let us not directly depend on utils-spi-java; it contains too many deep dependencies. \nTwo options can be considered:\n1. Add more specific building target in spi/src/java:BUILD\n2. Re-org classes in utils, for instance, put SchedulerUtils into spi/src/java/scheduler or something similar.\n. Let us not directly depend on utils-spi-java; it contains too many deep dependencies. \nTwo options can be considered:\n1. Add more specific building target in spi/src/java:BUILD\n2. Re-org classes in utils, for instance, put SchedulerUtils into spi/src/java/scheduler or something similar.\n. data tuple set inherently means it is a batch since it is defined in proto as a batch of tuples with meta data.\n. Yes it throws a NPE. I would like to solve it in a new pull request since all configs in heron_internals.yaml share the same concern; it is good to solve them uniformly at the same shot:\nhttps://github.com/twitter/heron/issues/970\n. LOL. Good catch. Will change it.\n. Yes they are always get overwritten in the constructor. Will uniform the initialization in construct in the new commit.\n. 1.\nhttps://github.com/facebookarchive/scribe/blob/master/if/scribe.thrift is using \",\" rather than \";\".\n2.\n Good to keep consistent for all: enum ResultCode above is still using \",\"\n. Just to indicate this is a test case. You may consider it as a tag that Test Tool can take it into account when generating the test report.\n. Sure. Comments added.\n. Can we check and throw exceptions here if config does not contain \"filename-output\" or \"file-maximum\"? Or provide some defaults values?\n. We have already logged the location of working directory when a topology is submitted successfully via LocalScheduler: https://github.com/twitter/heron/blob/master/heron/schedulers/src/java/com/twitter/heron/scheduler/local/LocalLauncher.java#L83\nA better alternative can be log the working directory no matter a topology is submitted successfully or not. \nEven if you want to log the working directory for every time running runASyncProcess, the logging level should be Level.FINE\n. Yes those places should have been checked.\n. @ashvina can provide better ideas when this logging should be.\nIs there a way to easily adjust the log level.\nPeople could submit topology with verbose flag, and then expect the logging level be Level.ALL (https://github.com/twitter/heron/blob/master/heron/scheduler-core/src/java/com/twitter/heron/scheduler/SubmitterMain.java#L275). \nAdmittedly this is not fully done, for instance, not yet done in SchedulerMain or HeronExecutorTask. Pull request for them are welcome.\n. We had discussion for this earlier.\nCurrent style, fetching actual key value from files, is unnecessary and complicates the code base, which is bad. Eventually we would refactor all these fetching-from-files to specifying keys' value directly in Keys.java, as this pull request. \nWe have done this similarly in Context, for instance, https://github.com/twitter/heron/blob/master/heron/schedulers/src/java/com/twitter/heron/scheduler/aurora/AuroraContext.java\n. Fixed.\n. Fixed.\n. Fixed\n. Added the job link.\n. Comments added in the source code: The lifecycle of Topology is independent from Scheduler, for instance, Scheduler may crash, restart and pick up already running topology (for fault tolerance).\n. sounds good.\n. The mesos master will reclaim the offer back if we fail to launch a task with that offer.\n. True the topology will not work normally if a slave lost. The recovering mechanism is complicated and reconcile() is needed. Prefer not to handle this in this iteration consider this scheduler implementation does not aim for production.\n. Fixed.\n. Updated the implementation. It iterates with all offers now.\n. First-fit is a reasonable strategy in practice. Method generateLaunchableTasks(..) is already separated to allow people customize their own assigning strategy. We could evolve it later if any practical concerns raised.\n. https://github.com/twitter/heron/issues/1077\n. Sounds good. Updated.\n. Good catch. Added synchronized block: \nsynchronized (toScheduleTasks) {}\nin critical blocks inside some public methods to avoid concurrent issues.\n. Added synchronized block: \nsynchronized (toScheduleTasks) {}\noutside to guarantee toScheduleTasks will not be changed.\n. Sounds good. Addressed by invoking handlesMesosFailure(TaskId)\n. Added synchronized block: \nsynchronized (toScheduleTasks) {}\noutside to guarantee toScheduleTasks will not be changed in other threads. So taskId will not be null now.\n. Fixed.\n. Updated to log at WARNING level. \nAdded a comment in the source code:\n// TODO(mfu): Don't know what should be done here. Simply ignore this status update for now.\nI am hesitate to kill the task here since it is also possible to kill tasks belonging to other jobs.\nThis case should rarely happen.\n. isTerminated needs to be within the synchronized block; otherwise the check of isTerminated and the update of Terminated could lead to concurrent bugs. It may be a little confusing to synchronized (toScheduleTasks); changed it to synchronized (this) to make it clear.\n. handleMesosFailure is a protected method invoked in a public method, which is within the synchronized block; so handleMesosFailure and killJob will not concurrently executed.\nIt may not be a good idea to add synchronized block in an internal method, i.e. private or protected.\n. Not sure how necessary to do that; this can be controlled by higher level logic. Currently the critical parts are within synchronized block and can not execute concurrently; we also have isTerminated checking at the start of those critical parts. So logically, tasksId will not be updated by other threads once killJob() is initiated.\n. I would prefer using synchronized blocks over synchronized methods, since:\n1. It clearly shows what object is used as lock\n2. It could provide more flexibility because it is kind of an internal implementation people can use other object as lock or narrow the block scope in future, whereas syncing a public method would is more kind of determining an interface, not easy to evolve.\n. Not sure whether to put cleanState(...) here or in the Utils class since it is more like an utils method, providing an aggregation of primitive methods.\n. Need to clean all state? Or only explicitly clean execution_state and topology to make it more specific and clear?\n. This style of implementation could lead to one more remote call. Could we figure out a more-efficient way?\nSimilar to below.\n. Not directly related to this pull request. But I think it is better \"this will soon be refactored to get this from the TMaster instead of args\"\n. Change from \"install heron\" to \"Get heron with Mesos Scheduler implementation\" or someting like it. Otherwise this seems like a little duplicated with section \"Compile Heron\" below.\nOr we maybe move this section to \"Compile heron\" and merge them together. \n. When submitting a topology, people can also specify the customized config path. The getting-started webpage provides an example. \nLater release would contain mesos scheduler implementation, and people can provide a config and submit a topology without recompiling heron.\n. Can u also run the heron-tracker and heron-ui, then put their running screenshot into this doc?\n. Can u add more doc here showing a running topology? For instance, where is the scheduler-working-directory, how does it look like, how can people use it to debug. Where's the log of a running heron-instance, what's the sample log content?\n. I remember you came across some issues when tyring to run the mesos cluster. Could u also share them here as potential trouble shooting?\n. sounds reasonable.\n. Could we change \"jave_class_name\" and \"jave_object\" to a more generic name, rather than adding a new filed \"python_class_name\", which is not extensible.\n. +1\n. Can we simply kill the whole group and resubmit the whole group in this case?\n. So only RAM is taken into account? Should we rename it to avoid confusion?\n. Why not use or refactor the utils method already exists?\n. It is better to put this change into a separate pull request.\n. I am wondering:\n1. Can we use only one method: void removeContainers(Set existingContainers, Set containersToRemove);\n1. Why the arguments for addContainers & removeContainers are not similar, i.e. why removeContainers  needs existingContainers while addContainers  does not?\n. The name of \"ScalableScheduler\" indicates it is a scheduler interface. However, it is for \"Scalable\" features only. It could be better to remove Scheduler to make it \"Scalable\" or \"IScalable\".  Then actual scheduler can : \nclass XScheduler implements IScheduler, IScalable {..}\n. +1 for IScalable. And it also allows the flexbility to have one more interface  IScalableScheduler that extends IScheduler and IScalable in future if needed.\n. Removed.\n. Definitely.\n. Add a new line at the end?\n. Add a new line at the end?\n. Should we make it separate class or a nested class?\n. In current implementation, we enforce the thread-name the same as sink-id. Do you think we need to add an extra map to record it?\n. Sorry but I don't understand. This implementation logs exception at the start once; and then followings are handling logic with a different if/then ... Those are not causes but handling logics?\n. Sounds good. Will do that.\n. Added comments in the source code to avoid confusion. sinkId is more representative, since it is defined inside the metrics_sink.yaml\n. sounds good. Will do it.\n. Both this counter and droppedExceptionsCount are per message payload. \nEvery time we collect the metrics, we would invoke IMetric.getAndReset(), send the message out and these two counter will reset to 0.\n. sounds good. Will do it.\n. will do it in another pr\n. Is it possible to add all these configs as a sample for this sink in metrics_sinks.yaml (https://github.com/twitter/heron/blob/master/heron/config/src/yaml/conf/examples/metrics_sinks.yaml)?. Can you add comments for all these configs to explain what they are for?. This variable name does not match the config name. It is good to make them consistent or comment down the reason.. Should we also print the content in the file?. Should we also bubble up this exception or solely log it down?. What is this variable for? Could we add comment for it?. Can you tell me what are the differences between:\n1. \nMap<String, Object> sourceMap = new HashMap<>();\nprocessMetrics(\"\", record.getMetrics(), sourceMap);\nmetrics.put(source, sourceMap);\n2.\nprocessMetrics(\"\", record.getMetrics(), metrics);\n. 1. Comment for this method? \n2.\nAlso, the argument Iterable<MetricsInfo> metrics will hide the field protected ConcurrentHashMap<String, Object> metrics, which is bad.\n\nWhy making it a static method? Also, why passing the out as a argument, which is pretty C/C++ style?. After reviewing this sink, I found there is no limitation on the size of this Map metrics. In some cases, for instance, users export metrics with new name at an interval, it is possible the size of metrics will grow unboundedly and eventually lead to OOM, which is a bad design.. Can you add a comment here to indicate this library is used only for websink, since we would later split the all these sinks from the spi parts.. Given the RuntimeException contains the error msg, extra SEVERE LOG seems duplicated and unnecessary.. On the one hand, not sure whether it would affect our semantics: originally it is fail-fast but now it is dropping. On the other hand, we would also need to export metrics for this behavior; otherwise, it is hard to trace and debug.\n\nAlso, did we really witness this data corruption, since protobuf is pretty neat.. I am still not convinced by the idea of pull request. This method returns nothing. So this failure will be no way visible to higher layer's logic: we can not handle this dropping. \nI would still prefer the fail-fast.\nCould you describe more on the scenarios of corruption issue?. Check entry.getKey() too since the key can also be null.\nhttps://docs.oracle.com/javase/7/docs/api/java/util/Map.html#put(K,%20V). sum of metrics value/ sum of invocation count. @huijunw At the start of every metrics-collection-cycle, it would reset the value as 0; then at the end of every metrics-collection-cycle, the metric value would be: sum of metrics value every time recorded/ sum of invocation count => the average value of this metric recorded during the cycle.. Can we merge L202 and L203 into one line:\nsourceCache = (Map) sourceObj;\n. sounds good.. Should we use heron raw api directly @kramasamy \nEven if we try to use heron api in examples, should we add a new example rather than reuse this ExclamationTopology?. Should we pass high_watermark and low_watermark from constructor, or similar to max_packet_size, reading config inside CreateClient(...) method?\nOn the one hand, it is not necessary to change the signature of a class's constructor. On the other hand, It will match the style of max_packet_size.. Just wondering: how does the \"max_packet_size\" config work?\nAlso, IIRC, c++ reads this: https://github.com/twitter/heron/blob/master/heron/config/src/yaml/conf/test/test_heron_internals.yaml, as a test config.. OK Sounds good to me.. Can we also log down the stmgr-id here? like \"Another stmgr {stmgr-id} exists at ....\". Same as the comment above.. @congwang It is better to have a log having all necessary info self-contained, than relying on earlier logs. It is possible that people mistakenly hurt this log without knowing it.. Also one mirror comment, (maybe not relate to this pr): when logging \"Got a register stmgr request from \", it is better to log \"Got a register stmgr request from {stmgr-id} at {host:port}\". @congwang Yeah we read context for debugging. My concern is: people can change/remove the log \"Got a register stmgr request from ...\" without knowing this change will affect another log.. I would think having IStatefulComponent is good enough; no need IStatefulBolt or IStatefulSpout:\n- IStatefulBolt does not add more methods than IStatefulComponent and IRichBolt. A bolt can simply implement IStatefulComponent if it wants Stateful feature; on the contrast, IUpdatable interface looks better (https://github.com/twitter/heron/blob/master/heron/api/src/java/com/twitter/heron/api/topology/IUpdatable.java).\n- IStatefulBolt extends IRichBolt. Then how about IBasicBolt? Need we add it too? And how about others, ? It can either lead to an explosion of APIs, which in fact IMO are duplicated; or it can lead to more refactoring on existing topologies to support these features (for instance, current bolt extends IBaseBasicBolt, which provides different APIs from IRichBolt).\nIn a word, I think this wrapper complicates things without bringing much benefits.\n. Similar to above.. +1 for: it should be a topology specific config:\n- if people want a component to be stateless, they can simply not to store any states, even with this config is set true\n- it does not make sense to make this config per component; the whole topology DAG is a computing job and the global state will be correct only when all components have set topology.component.state.enabled true.. +1 Map<<K extends Serializable>, <V extends Serializable>> . @nlu90 Heron feeds the state during void initState(State state); so it always has the reference for the state. . I would prefer to halt() directly here, since:\n1. It is more clear to know the process will fail fast without tracing other parts of the code.\n2. It reduces extra operations, which are not guaranteed to execute during Error. For instance, catching exceptions will use memory, and it might not work if we encounter java.lang.OutOfMemoryError: Java heap space. Make senses. Updated.. There was an intentional optimization to reduce one  System.nanoTime() call per tuple execution. In the new commit this optimization is removed to simplify the reasoning. \nNow it is much easier to understand.. Please check with latest code. No currentTime there now. One the one hand, it seems we don't have such Constants class; I don't see how necessary to add a class for a single filed;\non the other hand, this class is designed to abstract out the shared actions/implementations, which can be a good place to put a shared field.\nBTW, I would prefer this class being an abstract class to an interface, since IMO this class is more used to reduce duplicated code, rather than to define some required behaviors.. No rush for this pull request. Will also wait for Bill's comments.. we should either add a \"if\" or update the description of this pull request.. It implies that we would only use java default serialization/deserialization for state encoding. It is good to have an interface for it, similar to tuple serialization/deserialization. Users could provide an implemented one, or a default one will be used (java default or kryo). . return stmgr.getHostName() + \":\" + stmgr.getShellPort();. I can not find an override toString() method for ContainerRestart. Could we check we get a meaningful print-out?\nIf so, can you add a comment for a sample event.toString() in the code?. I can not find an override toString() method for ContainerRestart. Could we check we get a meaningful print-out?\nIf so, can you add a comment for a sample event.toString() in the code?. no need to make it synchronized:\n- it is a private method\n- the only entry is get() above, a synchronized public method. Can we use a more type-safe method rather than a cast (int) policyConfig.getConfig....?. getShellUrl can return null. A safety check is required here. Otherwise, the debug could be hard.. plz document down clear in comments whether it will:\n- return null\n- return \"\"\n- or throw exceptions\nfor this method in abnormal case. A log is required here to tell no resolver exists. Or throw exceptions if it means some really bad things.. final. final. final. I would recommend breaking this method into several smaller methods.. safer to return an empty List; would not rely on an external safety check.. @huijunw @ashvina \n+1 for the proposal above.\nFor scheduler service for Aurora, it violates part of Twitter DC's security design, so I would say not betting on it for near future, at least for the Twitter environment. . Need we throw an exception if we could not initialize stateMgr and stateMgrAdaptor? Otherwise, the AuroraHeronShellController is in a wrong state.. call the variable host or hostname. ip is confusing here.. Have a try-catch-finally block and put con.disconnect() in finally. This implementation here is really ugly and bad.\nOne alternative is to use constant enum;\nanother alternative is to use a boolean to decide which one to use since you have only two cases and it will fail otherwise.\nPolymorphism can be another one.\n. given you have only 2 options, you can consider this is a boolean. You could evolve from it in future if needed, but I seriously doubt that.. Bubble up a RuntimeException to external logic. Otherwise, the program will execute with a bad state but it is no way executing normally and recovering with such a bad state.. java style is getTaskId(). Similar to other methods in this interface.. Need it be public?. Do we also need to consider other types of window, for instance, SessionWindows?. Same as above. Do we need to consider other types too?. Use switch - case - default instead of if - else if - else if - else?. Good to specify a constant field externally in a Constant class, rather than use a String constant directly; easy to trace and understand.. Same as above. abstract this into another private method?. Do we need to ack?. Do we need to ack?. Same as above.. Also Log No name found for xxx. Use the default name xxx instead for easy tracing.. It seems the method doBuild()s in ConsumerStreamlet, FilterStreamlet and other xxxStreamlets have a lot of duplicates. Can we try to abstract them out to reduce duplications?. Same as above.. Can we move the String consumer to a Constant class? Same to below.. It is good to have a method to convert config.get(Config.TOPOLOGY_RELIABILITY_MODE) to enum and then compare two enum values. Comparing two Strings is error prone.. Do we need the LOG.info(...) here? . Do we need a LOG.info(..) here?. Do we need a LOG.info(..) here?. it does not break current code. But I am wondering why we need this change?. I would argue let IWindowedBolt extends IBolt, then \n- we no longer need setBolt(..., IStatefulWindowBolt, ...) method.\n- it follows the convention we used for low-level heron stateful API as: https://github.com/twitter/heron/blob/master/examples/src/java/com/twitter/heron/examples/api/StatefulWordCountTopology.java\n- IStatefulWindowedBolt should be a type of IBolt. This method signature could potentially lead to ambiguous signature matching.. Same as above.. Will users mistakenly use the same key for their own states leading to a conflict? Same for below.\nAlso, should we have another separate xxxState class wrapping up these fields, and provide restore(..) and getState(...)?. If people want to override the value, they would put it in the yaml file. \nCurrently, we just put 5 as the default value and use it.. Added. Thanks.. - This value is hard coded, probably not a good idea.\nWe need to handle backward compatibility; this is why we have this value in source code. Different places would use their own configs, not necessarily sync up with OSS Heron repo. This implementation can prevent bringing down running Heron service.\n\n\nSince you already have the yaml config, you can use this value - whatever this value is. Since most of the time this value is not changed, it automatically becomes the default value.\nWe need to handle backward compatibility. This implementation also provides the flexibility about what you mentioned.\n\n\nIf you are allowing for overrides, the overridden value should be used - but is that there somewhere?\nconfig.getIntegerValue(JOB_MAX_KILL_ATTEMPTS, 5); means we would try to look up the value for the key JOB_MAX_KILL_ATTEMPTS in the config. If people supply value for this key heron will use it, otherwise to override the default value, it will use the default value 5. This implementation allows overrides, and the sample aurora/scheduler.yaml does the overrides.\n\n\n. The variable name processName is a bit confusing. It seems what you refer to is instanceId, rather than the processName. Similarly, I would recommend changing the request structure so we can have instance_id_to_restart here. Shall we need to release/close the resource after open?. if (b != null && b) is good enough, or if (b != null && b.booleanValue()). LOG.severe does not lead to fail-fast. We would need to throw an exception here.\nFurthermore, the start() returns void, we need to handle it carefully so the process will exit with non-zero exit code (throwing exceptions can be a good idea.). Several comments:\n1. Based on the design/comment/signature of this method, it should return Null or False and the failure handling should be outside this method.\n\n\nFor\nLOG.severe(\"creating zk metricscache node failed, fail fast\");\nthrow new RuntimeException();\nA better implementation is:\nthrow new RuntimeException(\"creating zk metricscache node failed, fail fast\");\n\n\nSome cases do not get covered here and they can lead to strange states and never get recovered. For instance, What if: there is a network partition between the host running metrics-cache and zk-server? Based on this implementation this metrics-cache will hang forever without notifying others.\nA better fail-fast can be to exit when losing connections to zk. Then the scheduler can eventually scheduler it to a healthy host. The key diff is: this design would notify others when facing unexpected failures.. Added.. Added.. I don't think we need to make it so complicated. We will not do any recovering here,\nso we can simply:\nLog(...);\n// The ReleaseResourcesExplicitlyIfNeeded() can be a bit tricky. \n// If you want to release resources explicitly, make sure set a timeout to void unexpected hanging\n// Also, it is possible that it can have other unexpected behaviors\n// If all resources can eventually be released by the death of a process, \n// we are ok not to clean it explicitly\nReleaseResourcesExplicitlyIfNeeded(); \nRuntime.getRuntime().halt(1);. You can put this halt(1) in finally. This should be fine Level Logging. This should be fine Level Logging. Should we return a list to preserve order or set?. If we need to add containers, and stderr.length() <= 0, should we consider it as an error and throw an exception?. Good to make this parsing logic as a method below. Good to add more comments to illustrate the remapping logic.. It is good to comment clearly:\n\nWill the map-to-return include all container-ids to add, no matter whether requesting a remapping? \nOr\nWill the map-to-return include only the container-ids requesting a remapping?\nOr\nWill the map-to-return include at least all container-ids requesting a remapping, but also potentially include container-ids not requesting a remapping?. But don't you want a list of ids with increasing order? \n\nIt could potentially help the remapping with less items.. This implementation tries to keep the ratio of #(instances)/#(containers) always the same during re-scaling.\nThe issue here is:\nDuring each re-scale, we would update only # of instances in topology def, and # of containers will not be updated. So the # of containers fetched by TopologyUtils.getNumContainers(topology) is not accurate. So if we continues scaling-up a topology, we would end up:\nincreasing the # of instances in one single container boundlessly.\nOne thought is always getting # of instances and containers based on currentPackingPlan. This will reflect the correct values.\nBut it also has some issues: we will try to keep the ratio of #(instances)/#(containers) the same with last currentPackingPlan, rather than the original one. And every time we would ceil the # of containers, since the # of containers has to be an integer. And after several times of scale-up, it can lead to:\nThe # of instances in per container decreases and eventually there will be only one instance per container.\nThe ideal case is to always keep the ratio of #(instances)/#(containers) the same as the original ratio. But it seems our current state updating is kind of messy, and I am not sure whether there is a simple way to get it. Or one thought is to insert this original ratio as a config during submission.. - I am curious about how the original number of container is evaluated? \nIt is set by customers by is put into the topology-level config: https://github.com/twitter/heron/blob/master/heron/api/src/java/com/twitter/heron/api/Config.java#L356. Scheduler will look up this config to know # of containers to spawn.\nOne issue is there is a bug when updating the topology: https://github.com/twitter/heron/blob/master/heron/scheduler-core/src/java/com/twitter/heron/scheduler/UpdateTopologyManager.java#L317. It will update only the new # of instances, but not the new # of containers.\nAlso, I don't think it is a good idea to update the topology def here; I think it should be unchanged once a topology is submitted. All runtime changes should apply to the PhycicalPlan.\n\nI agree with maosong that the ratio may or may not be the best option here.\nRatio is ok if we can keep the ratio same as the original one. \nAnother thought is we could also accept updated count of containers when updating the topology, rather than accept updated count of instances for component. To make it, we can reuse existing routine: the component name is container or stmgr, like:\n--component-parallelism container:100. +1 for unit tests. Particularly, the unit tests should guarantee situations below not to happen:\ncount of instances in a container grows boundlessly\ncount of instances in a container reduces to 1 eventually. The implementation of TopologyUtils.getTotalInstance(), https://github.com/twitter/heron/blob/57310fe58a808ee9cf88b91121d9d8822698bee1/heron/api/src/java/com/twitter/heron/api/utils/TopologyUtils.java#L130, seems same?. If we remove the topology-update on stateManager, it seems we could remove or clean a lot of codes, like:\nprivate TopologyAPI.Topology getUpdatedTopology(...)\nstatic TopologyAPI.Topology mergeTopology(...)\nprivate static boolean updateComponent(...)\nAnd a lot of unit tests.\nCan we also do that? . Good to have a comment here saying:\nWill throw exceptions internally if failed to xxxx. Good to have a comment here saying:\nWill throw exceptions internally if failed to xxxx. Need to think about whether to rollback the packing_plan on zk if failure happens.. Would be much better if we can print meaningful log to notify users when unrecoverable bad things happen, like:\nFailing to update topology. The topology can be in a strange stage. Please check carefully or redeploy the topology.. Need to check whether we successfully add containers.. I think this check quota logic should be at the upper level rather than at this level. i.e. in AuroraScheduler.addContainers(...). We can simply get the env from Context (https://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/common/Context.java#L36). No need to ask Aurora Status.\n. I mean, we can do this \ntry {\n      JsonNode jsonStatus = askAuroraStatus();\n     ...\n}\nin AuroraScheduler.addContainers(...) rather than in AuroraCLIController.addContainers(...). I mean, only when users try to update a topology in prod env, which can be parsed from the Context, we will try to askAuroraQuota(); we don't need the methods askAuroraStatus() and isProd(). @nwangtw it should be safe; this should be verified ahead to come to the actual update stage.\nFurthermore, topology-name is unique for a cluster, so there will be not topologies with the same name in one cluster.. I agree it is a bit overkill, if we are to use this implementation.\n\nBut the point is: it is pretty common that users forget requesting more resources and then fail with insufficient resources. It is a super bad experience.\nAnother thought is: before the actual update, we could remind customers of providing enough resources. For instance, to print: Please make sure there are sufficient resources to update this job? [Y/N] and proceed when users input Y\n@nwangtw @huijunw Thoughts?. 2 minor comments:\n1. Why pass the flag running in the constructor, rather than in the Config runtime?\n2. Name the boolean as isRunning will be more expressive and more java style.\n. OK Fair enough.. Why not LOG.error(...)?. In general, it is a bad idea to require a console to be assigned as this very much impacts the flexibility of your application. Many ways of invoking Java will not assign a console (i.e. System.console() returns null), and your application is unusable in those instances.\n(https://stackoverflow.com/questions/20982664/why-does-system-console-return-null-for-a-command-line-app)\nYou can always use System.in and System.out instead, as follows:\nString qq;\nScanner scanner = new Scanner(System.in);\nqq = scanner.nextLine();\nSystem.out.println(qq);. I would suggest having an option (can be overridden by config-override) to bypass the prompt so it is easier to invoke the update command in other scripts or process, like ci, Jenkins.\nOr we can have this in a future pull request.. Karthik mentioned a good point. Running can be confusing since we have used it already for topology state Running, Paused, etc.\nMaybe we could rename it to isExisting?. WakeableLooper is designed not to be thread-safe; concurrent scenarios should be handled in outer logic.\nIf the purpose of this pull request is to support thread-safety, using this concurrent structure is not safe enough, if thinking this from an application point of view, since the actual Runnable a removed reference pointing to might not run normally, for instance, running with a dirty state.\nIf the purpose is to avoid task-removal during execution, I would suggest not to use a Concurrent data structure to avoid confusion.\nBTW, IIRC, previously we could only add a task to tasksOnWakeup when executing tasks from it, but we could not remove a task.. IIRC, previously we could only add a task to such kind of tasks-list when executing tasks from it, but we could not remove a task.\nOn the other hand, WakeableLooper is designed not to be thread-safe; so it should be executed with only one thread.. Also, if the purpose is to avoid task-removal during execution, why not find out such code snippets and correct them?. Based on the comment above: We pre-get the size to avoid execute the tasks added during execution\nWill we encounter the scenario: remove a task and then add a task -> we would execute the tasks added during this round of execution? Otherwise, not sure whether we would lead to a thread spinning.. Can we add comments for terminateAllTasksOnWakeup and terminateAllExitTasks? Though I can understand them after I go through the code, it is still better to have more comments.. Can you remove this comment (and similar comments below), and add comments at the head to declare:\n1. Most methods except the onExit() are not thread-safe.\n2. People should handle the concurrent scenarios in their business logic rather than in this class.. ",
    "caofangkun": "\nProject: \n  Eclipse plugin for Bazel support, e4b\nBolg \n  Bazel IDE Support\n. #1047 add .project .classpath and setup-eclipse.sh for Eclipse IDE support\n. Hi @kramasamy \nIt's interesting to me.\nplease assign this issue to me and I will keep track of it.\n. Since Bazel 0.2.3, Bazel  warns if a cc rule's includes attribute points out of third_party\nhttps://github.com/bazelbuild/bazel/commit/f0a18846461d4b7ecc76c36a659ff1e37f93d7dc\n. Hi @kramasamy @taishi8117 \nThank you very much for your kind help. CI passed ... at last! \n. Hi @kramasamy \n\nAccording to code of https://github.com/bazelbuild/bazel/commit/f0a18846461d4b7ecc76c36a659ff1e37f93d7dc#diff-b1e05be607e9de9e40f306162531dc81R431\nI did the following changes, and the warnings gone.\ntwitter-heron git:(heron-816-1) \u2717 mv 3rdparty third_party\ntwitter-heron git:(heron-816-1) \u2717 sed -i \"s/3rdparty/third_party/g\" `grep 3rdparty -rl ./`\nShould i fix this in this PR or file a new issue and fix it later?\n. Hi @kramasamy\nI checked the  travis-ci logs but not find  any not a valid file name (cc, h, cpp, cu, cuh) warnings. \n. Hi @billonahill I have included your feedback in d74c1dc , Could you please have a review?\n. Hi @taishi8117 , Thank you so much for your help, I have fixed it as you suggested.\n. @kramasamy I'm working on the conficts now, will fix it soon :)\n. Thank you @taishi8117 After merge the CI passed now.\n. The fix has been merged as #985 , please close this issue.\n. @jiandongjia  This issue has been fixed and merged , Please close it .Thank you .\n. Hi @billonahill  I have accepted  the CLA .\n. Recreated PR at #985 and  close this PR.\n. Hi @billonahill \nsorry to have bothered you, it's my fault. I deleted the fork  several days ago, and the pull request came from an unknown repository.\nI don't know how to recover this , so I recreated new PRs.\n will never happen again.\n. Hi @kramasamy  @nlu90 \nI'm sorry , I deleted the fork yesterday\uff0c now the pull request came from an \"unknown repository\".\nSo I re-create the pull-request as  #982 , and I will delete this PR.\n. Hi @kramasamy  I have recreated PR at #984 and will close this PR.\n. @kramasamy\nDo you clean up before build ? Could you try as following again?\n\u279c  twitter-heron git:(heron-974) bazel clean --expunge    \n\u279c  twitter-heron git:(heron-974) bazel build --config=darwin heron/...\n. Hi @kramasamy \nI\u2018m using Ubuntu, and did show no more warning messages when I updated slf4j version. \nI do not have a mac -_-#! ,so I couldn't figure out what happening on Mac.\n. Hi @maosongfu \nThank you so much for your clear and helpful explanation.\n I'll  look into this issue and try to find out how to solve it.\n. HI @kramasamy \n- 1. compile and install thrift 0.9.3 and use which thrift eg /usr/local/bin/thrift to generate java code , \n  but this consumes too much time on compiling thrift;\n- 2.use thrift executables like third_party/thrift/thrift-linux-x86_64.exe to generate code at runtime , but this may  not proper  run on all linux-dist platforms;\nSo why don't we :\n- 3. do not generate code at compile time , just use thrift 0.9.3 to generate java code locally, and use generated java code directlly? \n. Hi @nlu90 \nThis PR is ready for reivew now.\nI would be really grateful if you could spend some time to review. \n. as described at https://github.com/bazelbuild/bazel/issues/642 \nthird_party  is likely  required for a license\n. :+1: \n. Hi @kramasamy \nconvert Makefile build to bazel looks good overall, but about docker_build rule, there are a few features missing:\n- does not support docker run \n- does not support docker push\n- does not support docker stop\nMaybe there is no immediately need to convert traditional Dockerfile-based docker build to  bazel docker_build rule .\nBut it's indeed better using bazel BUILD file build,run,push or stop docker images/instances by invoking shell commands like build-docker.sh and so on.\n.  error curl: (1) Protocol hdfs not supported or disabled in libcurl  is likely from : ShellUtils/curlPackage\nbut curl could not operate hdfs directly.\nApache Hadoop provided webhdfs rest api \nto support curl files on hdfs\n@mhajibaba Could you have a try?  don't forget modify hdfs-site.xml and add following config:\n<property>\n <name>dfs.webhdfs.enabled</name>\n <value>true</value>\n</property>\n. @kramasamy \nOk, I'll keep track :)\n. Hi @objmagic \ndoes release/release-process.md  should also change 0.14.0 to 0.14.1?\nand should we provide update-version.sh {version_number} shell script to update version?\n. Hi @kramasamy \nPlease assign this to me , I will fix it.\n. Hi @kramasamy \nAs discussed at https://github.com/bazelbuild/bazel/issues/172  bazel 0.3.0 fixed this bug.\nI am doing further testing.\n. \u279c  docker git:(heron-149) ./build-artifacts.sh centos6 0.14.1 ~/centos6\nGenerating source tarball\n./build-artifacts.sh: line 140: docker/compile-docker.sh: No such file or directory\nCleaning up scratch dir\n. Hi @kramasamy\nSorry for late reply... thank you for your patience and helpful feedbacks. \nNow this built well on my local machine (that is Ubuntu 14.04), \nCould you please have a try docker build on your mac os ?\nIf any problem still exists, tell me , I wll keep tracking down :) \n. :+1: \n. Hi @atibon \nThis is not a heron issue.  \nIt's more likely MAT's/Eclipse MAT's bug.\nPlease check more detail log following of the NullPointerException.\n. Thank you @kramasamy ,  the branch is already up to date.\n. Hi @billonahill \nWhat about the following changes ?\n1.  rename com.twitter.heron.api.Config to com.twitter.heron.api.HeronConfig\n2. rename com.twitter.heron.common.config.Config  to com.twitter.heron.common.config.CommonConfig \n3. rename com.twitter.heron.spi.common.Config to  com.twitter.heron.spi.common.SpiCommonConfig\n. @nlu90 \nApache Storm provided external tool storm-rename-hack to deal with namespace compatibility.\n. Hi @kramasamy I have fixed this bug. \nThe Travis CI still not pass yet, I will  get into the root of the matter to find out why.\n. @kramasamy If file /etc/redhat-release exits then It should be CentOS .\n. @kramasamy OK, I will do it .\n. fixed.\n. fixed.\n. fixed\n. fixed\n. Yes ,we can call \"tail\" shell. but I do not think that make any difference.\nCode Sample:\n```\n!/usr/bin/python\n-- coding: utf-8 --\nimport subprocess \nfilename = '/tmp/result.log'\ncommand='tail -1000f '+filename\npopen=subprocess.Popen(command,stdout=subprocess.PIPE,stderr=subprocess.PIPE,shell=True)\nwhile True:\n    line=popen.stdout.readline().strip()\n    print line\n```\n. fixed\n. fixed\n.  seconds difference is enough here\n===> Starting heron build at 2016-06-22 03:11:50\n['bazel', '--bazelrc=tools/travis-ci/bazel.rc', 'build', 'heron/...'] writing to heron_build.txt\n1472 seconds 5946 log lines\n. fixed\n. lines is List  , print it line by line will be much readable.\neg: \n- print lines  will be \n[\"INFO: Reading 'startup' options from /home/ablecao/workspace/twitter-heron/tools/travis-ci/bazel.rc: --host_jvm_args=-Xmx2500m --host_jvm_args=-Xms2500m --batch\", '____(06-23 11:01:59.475) Loading package...]\n-  print line by line  will be \n```\n_(06-23 11:00:02.695) Loading package: heron/common/tests/cpp/network\n(06-23 11:00:02.735) Loading package: heron/cli/src/python\n_(06-23 11:00:02.765) Loading package: heron/ui/resources\n(06-23 11:00:04.142) Loading...\n_(06-23 11:00:04.332) Loading package: tools/defaults\n_(06-23 11:00:04.396) Loading package: @bazel_tools//tools/objc\n____(06-23 11:00:04.449) Loading package: third_party/java\n``\n. @kramasamyprint '\\n'.join(lines)is better, I have fixed this as request. :)\n. @kramasamy \nneedgoto installhugoon Ubuntu.go get -u -v github.com/spf13/hugo. @kramasamy \nYes , it's not necessary installpython-pygmentshere.\nfixed.\n. Hi @billonahill \nrelative path ofdetect_os_type.sh` here is not proper indeed. \nI have fixed it. Thank you for pointing out :)\n. Hi @kramasamy \nYou are right.\n Fixed.\n. ",
    "ashvina": "The docker file approach works. Thanks !\n. Thanks Karthik. I will look forward to the updated docs.\nThis seems to be a major change. A number of classes have been removed (for e.g. IConfigLoader). Can I assume that any new scheduler should be added to newscheduler and the old scheduler code is outdated and should be ignored?\n. This issue is not applicable anymore.\n. Is it feasible to use try-with-resources (https://docs.oracle.com/javase/tutorial/essential/exceptions/tryResourceClose.html) in such cases?\n. I am running Scheduler as a separate process. The JVM will run the Scheduler component only and no other Heron component. However instead of invoking the scheduler as a shell/java command, I am invoking the main method directly from a \"handle\" I receive from the cluster/resource manager. \n. @vikkyrk Yes. This is correct. Thanks. \n. Yes. The classpath configuration is for adding hadoop jars at runtime on the cli. The intention is to avoid packaging the jars with Heron distribution.\n. @kramasamy I am not sure if I understand the suggestion. \nIs the fat jar built during client-install time? If so, it assumes that the hadoop version is known before hand. Also this requires the jars to be added to git. I am doing something similar currently and it does not seem to be the right solution.\nOr are you suggesting to copy the required jars to lib/scheduler until a long term fix is available?\n. @kramasamy - Following your suggestions, for now I am thinking of adding all the required jars, including hadoop version dependent jars, to 3rdparty jar folder so that tests can be run. All hadoop jars are NOT packaged, only versioned independent api classes are packaged. That way the packaged version of the scheduler is small and does not include any version dependent classes. Before submitting a Topology, the cluster version specific jobs are copied to $HERON_HOME/lib/scheduler folder (Note: these jars are needed by the client to submitting job to the cluster. Hadoop jars are already available on the cluster nodes and hence not needed there). This way any conflicts are avoided too.\nFor submitting a hadoop job, jars set in HADOOP_CLASSPATH are included in the hadoop client classpath. There is a different option for copying user jar on all the nodes where job will run. I think a solution matching this would be simple and complete. Changes via .heronrc may not be needed. Alternatively I am inclining towards adding a script to copy required jars from HADOOP_HOME to  lib/scheduler.\nDoes this sound ok?\n. @kramasamy this issue can be closed now?\n. +1 works for me too.\nI have one observation. I have used maven eclipse plugin in the past to\ngenerate project files from existing source. It is very fast and existence\nof java compilation errors does not fail creation of project files. When\nused for the first time the intellij script for Heron takes a very long\ntime and fails if there are compilation errors. Is it possible to avoid\ncode compilation for generating IDE files?\nThanks,\nAshvin\nOn Thu, Apr 14, 2016 at 6:38 PM, Maosong Fu notifications@github.com\nwrote:\n\nI tried with the script and it works well for me.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/heron/pull/291#issuecomment-210242373\n. There are a couple of ways this patch can be tested. Following are the steps to test using light weight MiniYarnCluster\n1. Start yarn on default port: $ bin/hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.2-tests.jar minicluster -rmport 8032\n2. Copy hadoop client jars in .heron/lib/scheduler (this is a temp step till user classpath is supported): commons-collections-3.2.2.jar commons-configuration-1.6.jar commons-logging-1.1.3.jar hadoop-auth-2.7.2.jar hadoop-yarn-client-2.7.2.jar hadoop-yarn-common-2.7.2.jar htrace-core-3.1.0-incubating.jar .   commons-compress-1.4.1.jar avro-1.7.4.jar commons-lang-2.6.jar commons-lang3-3.4.jar hadoop-yarn-api-2.7.2.jar hadoop-common-2.7.2.jar netty-all-4.0.23.Final.jar jackson-mapper-asl-1.9.13.jar jackson-   core-asl-1.9.13.jar\n3. Submit topology: $ heron submit reef ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.AckingTopology AckingTopology\n4. Check yarn ui and processes: http://localhost:8088/node\n5. For worker HA, atleast a single node pseudo cluster is needed.\n. @kramasamy The patch was reviewed by @maosongfu and @osgigeek. I have resolved some. I am working on Driver HA and improving request synchronization. Some others have been marked as TODO notes. In my opinion these issues can be incrementally resolved.\n\nI think the check failing on Travis could be related to maven integration. I am looking at this now.\n. @kramasamy @maosongfu - I have updated the TODO comments with the action items. Please take a look. \nNote: This branch depends on merge request 518 (from @maosongfu) to be accepted into master.\n. @kramasamy - Thanks for asking !\nI have added some unit tests for REEF scheduler today. I am working with our cluster teams to develop deployment scripts and test the implementation.\n- Could you please point me at the solution for client classpath. I have not been able to follow all the discussions lately.\n- After rebasing on the latest commits, I am getting a new error: Could not find topology info for topology: AckingTopology, cluster: default and environ: heron. I am debugging this and will reach out to your team if needed. Please let me know if you are aware any issue related to this.\n- Regarding Schduler/AM HA, I have hit an open issue in REEF. This feature might need to wait a bit.\n- I am aware of pending tasks and issues. It will help me prioritize if you could help me understand the acceptance criteria for this pull request.\n  Thanks!\n. > Option 1: Add a config in client.yaml and this assumes that the class path is the same across different computers in the organization.\n\nOption 2: Add a command line arg --extra-scheduler-classpath which augments the class path while submitting, activating, deactivating & killing a topology.\nCurrent thinking is to provide option 1 and if needed provide option 2. Both the options could lead to conflicts. We can solve them when it occurs.\n\n@kramasamy - Option 1 is fine. For submitting a yarn job, user adds yarn config files to the classpath too. Custom classpath is not limited to jar files only.\n. > Regarding Scheduler/AM HA, I have hit an open issue in REEF. This feature might need to wait a bit.\n\nCan you please elaborate this?\n\n@kramasamy - REEF supports AM HA on .Net/Windows only. Java implementation is work in progress (REEF-345). In absence of this feature if REEF driver crashes, topology scheduler here, YARN will shut down all the containers allocated to the topology.\n. Hi @maosongfu ,\nI am looking at the changes in 783aff9. It seems there is some misunderstanding. The scheduler works on YARN and I have tested it many times. Can we please discuss before you commit any changes?\nAlso I am not sure if I understand why associating a name with TODO comment is needed. Once open sourced anyone should be able to work on a enhancement. It may not feasible to reach a \"owner\" for understanding the issue or waiting for the person to fix the issue. \nThanks,\nAshvin\n. Great ! Thanks for the clarifications. I am reviewing the pull request.\n. Thats correct. I just tested Heron-with-zk without log4j-over-slf4j. It works without any issues. \n. @maosongfu Any update on this? I could submit a pull request for removing the dependency on log4j-over-slf4j?\n. Hi @ajorgensen Thanks for the details\nThe issue reported is not a ignorable log message. Topology deployment on YARN cluster fails if both slf4j-log4j binding and log4j-over-slf4j are present. The applications quits to avoid infinite loop, search for preempting StackOverflowError in the description. \nIs it feasible to add log4j configuration instead of adding a dependency?\n. Two doubts:\n1. What if the users of log4j provides a properties file and uses custom appenders? They will need to debug why appenders are not working and reconfigure log properties. Sometimes it is not feasible.\n2. The default log4j appender is ConsoleAppender. I thought all console writes are redirected to .out files? Then the logs should not got unnoticed.\nIf at all this is inevitable, do you think shading this dependency will work?\n. @maosongfu what do you think about this issue?\n. The log4j-over-slf4j bridge is a topology dependency. Dependencies for some use cases should not become platform dependencies. Classpath management becomes hard when Heron dependencies are integrated with runtime dependencies. In such cases \"hard-coding\" topology dependencies would add to the challenge. \nMoreover there are many deployment specific logging integrations. Would'nt it be better if log4j bridge be a configurable runtime dependency instead of packaging it with Heron? I.e. allow users to easily remove this bridge at runtime.\n. This is useful for Reef Scheduler also.\n\ud83d\udc4d\n. @maosongfu I have tried the reef scheduler with example topologies in our distributed environment, HDInsight. After deploying heron binaries, a few config changes, including state manager conf and tracker conf, are needed. \n. @kramasamy @maosongfu any updates on this pull request?\n. @maosongfu Done. As you may have seen, the conflict did not exist when the pull request was created. It is resolved now. Thanks !\n. @billonahill In my opinion we are mixing two different interfaces in the proposed builder. Methods in the add or remove instance section are specific to update (or may be submit) topology process. Where Methods in the set resource settings section are related to constraints specified by the user in connection with the packing algorithm. For e.g. current RoundRobin implementation will need number of containers as a resource setting, while FFD will not use it. So I think any resource setting (constraints) specific methods should not be part of this interface.\n. @kramasamy I am working on the doc and planning to post the first draft by today evening.\n. Sample doc page: http://ashvina.github.io/heron/docs/operators/deployment/schedulers/yarn/. I think the doc is missing commands and information about log directories. I am adding these details.\n. Hi @maosongfu. I am looking at AuroraLauncher. It uses Scheduler as a library ref. Which means scheduler is not needed by topology master or instances and hence there will be no issues if YARN scheduler keeps the TMaster in a different container.\n. @maosongfu Pull request #967 fixes #961. Please take a look\n. @maosongfu I am looking at this. Do you know if the build logs are available somewhere?\nI have been able to reproduce it using --runs_per_test. The previous information is still useful. \n. @kramasamy I am following the instructions here: https://github.com/twitter/heron/blob/master/website/README.md. I am not getting any errors while making or serving the site. The pages are generated with some missing html markups. \nWill retry...\n. I am referring to hard coded value of baseurl=/heron in https://github.com/twitter/heron/blob/master/website/config.yaml. Here heron is the git repository name. A fork may have a different name. It is rare, not sure if I will call it absurd. \nFor now I will close this issue.\n. @zhangzhonglai pb is an instance of java.lang.ProcessBuilder. A deadlock in this class seems unlikely. The log messages you shared are generated before heron executor is launched. Some heron log files are also present in NodeManager's local directory (for e.g. <NM_LOCAL_DIR>/usercache/heron/appcache/application_1466548964728_0004/container_1466548964728_0004_01_000002/log-files). Could you please check and share some of those logs.\n. @zhangzhonglai It would be helpful if you can share some information about your YARN cluster. Thanks.\n. @zhangzhonglai  BTW, I have put some information about running the YARN scheduler on this doc page: https://github.com/ashvina/heron/blob/yarnSchedulerDoc-947/website/content/docs/operators/deployment/schedulers/yarn.md. \n. @zhangzhonglai Thanks for providing the details. This seems like failure to launch executor. The YARN scheduler is expected have retried launching the job and generated logs. I will take a look at that scenario again.\n. @zhangzhonglai Hi. I think #884 addresses a similar need. Could you please take a look.\n. Looks good to me. A user can reach initial app logs from RM UI and find out working directory from that. Capturing Java process logs in a file seems reasonable too.\n:+1: \n. \ud83d\udc4d \n. Good catch. Looks good to me.\n:+1: \n. LGTM \ud83d\udc4d \nIf possible add a test for config verification.\n. Yes. I can see the errors after \"bazel clean\".\nOn Mon, Jul 11, 2016 at 1:36 PM, Maosong Fu notifications@github.com\nwrote:\n\n@avflor https://github.com/avflor Maybe you need \"bazel clean\" first?\n@billonahill https://github.com/billonahill may have better ideas on\nthis.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/pull/1059#issuecomment-231856675, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AJkbjKcI9T_ITc4xiFi585q0ZdwVoBRNks5qUqlagaJpZM4JJrSl\n.\n. @kramasamy @maosongfu \nI agree with @maosongfu now. I had a couple of conversations with some OSS developers here. A contrib directory is primarily used for \"add-on\" features. For e.g. scripts and tools for debugging. There are a few projects where the contrib directory is used for un-important and un-supported code. However that may result in hard to manage \"trash\" and may be discouraging for new contributors. A scheduler implementation is core to Heron. So I think the Mesos scheduler should be added to core. \n\nHowever, I am not sure how to manage \"not-for-production\" contributions. A experimental feature may starve for users if left in its own branch. So I would prefer merging them in master. But how will a user know if he is using a experimental feature? Should we print a warning on the console.\nWith respect to this patch, I think a few minor issues are pending in addition to fault tolerance cases. Also, I may not be able to deploy and test this on a mesos cluster.\n. The scheduler code seems fine now. There are a few TODO items. Are you planning to create issues to help track them?\n. @kramasamy @maosongfu \nI have one orthogonal question related to the Mesos scheduler. Apache REEF seems to support Mesos also. The YARN scheduler is based on REEF. I think a lot of code can be reused if we developed a Mesos Scheduler based on REEF. What do you think about it? I can give it a shot if you'd like.\n. I did not mean to move packing plan classes to schedulers. Sorry for the confusion. I am suggesting that scheduler specific computation be outside packing algorithm. For e.g. method RoundRobinPacking.getLargestContainerRam should be in moved to AuroraScheduler and should be used in createAuroraProperties. \nI can submit a patch to clarify in detail. \n. @billonahill I created a patch for this https://github.com/ashvina/heron/commit/59234cf9acc3acfc5f43d2d3e4d3d742457a0be7\n. Hi @mycFelix. It seems, on the client node yarn-site.xml is missing a yarn classpath config.  Can you please check if yarn-site.xml contains yarn.application.classpath config. An example the value of this config is below:\n$HADOOP_HOME/hadoop-2.7.2/etc/hadoop,\n             $HADOOP_HOME/hadoop-2.7.2/share/hadoop/common/lib/*,\n             $HADOOP_HOME/hadoop-2.7.2/share/hadoop/common/*,\n             $HADOOP_HOME/hadoop-2.7.2/share/hadoop/hdfs,\n             $HADOOP_HOME/hadoop-2.7.2/share/hadoop/hdfs/lib/*,\n             $HADOOP_HOME/hadoop-2.7.2/share/hadoop/hdfs/*,\n             $HADOOP_HOME/hadoop-2.7.2/share/hadoop/yarn/lib/*,\n             $HADOOP_HOME/hadoop-2.7.2/share/hadoop/yarn/*,\n             $HADOOP_HOME/hadoop-2.7.2/share/hadoop/mapreduce/lib/*,\n             $HADOOP_HOME/hadoop-2.7.2/share/hadoop/mapreduce/*,\n             $HADOOP_HOME/hadoop-2.7.2/contrib/capacity-scheduler/*.jar,\n             $HADOOP_HOME/hadoop-2.7.2/share/hadoop/yarn/*,\n             $HADOOP_HOME/hadoop-2.7.2/share/hadoop/yarn/lib/*\nIn case yarn-site.xml has this config, then please retry by adding the yarn-site.xml to the hadoop-yarn-common.jar.\njar uf  $HERON_HOME/lib/scheduler/hadoop-yarn-common.jar yarn-site.xml\n. Great.\n@mycFelix I am planning to add the error to a FAQ section on YARN doc page (http://twitter.github.io/heron/docs/operators/deployment/schedulers/yarn/). Please feel free to contribute if the doc page is missing an important step.\n. @billonahill Thanks. I will update the patch in a bit.\n. I think #1010 is similar to this issue?\nI will investigate the deployment flow for non admin user accounts.\nOn Monday, July 18, 2016, mycFelix notifications@github.com wrote:\n\nso.... I need to execute 'chmod 777 /home/.pex'?\nIs there any other way?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/1103#issuecomment-233317090, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AJkbjBoaegF1HE9vf08UEGtS1ZMII7CDks5qW3RTgaJpZM4JOhBb\n.\n. @mycFelix The Heron client uses REEF client to submit the topology. The REEF library is waiting for some internal events. I am working on that issue with the REEF team. However the wait does not cause topology submission failure. Please ignore that for now. \n. Hi @mycFelix, I am also looking at the permission denied error. In my test the user submitting the topology was also a user on YARN cluster and existed on all YARN nodes. I do not get the error. Does that work for you?\n. Thanks @mycFelix \nI don't think there is anything wrong with the YARN clusters. This is specific to Heron components on the YARN nodes, https://github.com/twitter/heron/blob/master/website/content/docs/operators/heron-shell.md. \n\n@maosongfu In case heron-shell is not needed/used, can it be disabled? Or at least temporarily.\nHave you noticed the permission error on Aurora clusters?\n. @mycFelix No. Heron client installation is needed on one node only. The client will package the required bits and provide to RM. \n. Thanks for the patch. I will testing it now.\n. LGTM :+1: \nI deployed a topology on a YARN cluster. .pex directory is created inside container's working directory. I do not see the permission denied error any more. \n. I do not see any users of this code: https://github.com/ashvina/heron/commit/7b6a5b97b4e1033bbdb21f36ad5fa06474fa8e25\n. LGTM \ud83d\udc4d \n@nlu90 do you think it is worth creating an issue for SSL?\n. The change looks good to me. It would be great if someone could provide some clarity on the question I posted above.\n. @kramasamy : I found an issue in SlurmLauncherTest (#1192) and have created a PR #1193 for the same. Am looking at other failures.\n. @supunkamburugamuve \nCould you please verify if these issues exist and if the fix proposed in #1193 is ok?\n. Hi @objmagic,\n@kramasamy debugged this issue ( thanks! ). Below is a explanation in his words. We had this discussion on slack\n```\nprevious to the refactor\nheron-executor depended only pyyam\nhttps://github.com/twitter/heron/commit/a9d4861813eb27a692dce7bfdeb8d3ad75fc0059\nheron executor refactor and unit tests (#1134) \u00b7 twitter/heron@a9d4861 \u00b7 GitHub\n* switch order of monitor process * committing the unit tests * committing the unit tests * fix unit tests * updating comments * change most methods to be private * One last c... \naccording to BUILD file\nwith the refactor it also includes - statemanager etc\nthat pulls in zookeeper changes\nnow when heron-executor itself is run - it needs a pex directory to explode itself\nin this case it uses ~/.pex\nwhen heron executor executes process like heron-shell etc\nthey use the container .pex directory\nand hence you have two .pex directories\n```\n1140 fixed the heron-shell issue. This one fixes the new error after the refactor commit.\n. LGTM. CI failures need to be fixed.\n. @billonahill methods like c.t.h.spi.packing.PackingPlan.getInstanceDistribution() seem to be serializing packing plan into strings. Will this change make such custom methods redundant and removable?\n. :+1: LGTM\n. :+1: Thanks for this contribution\nI tested the classpath on local yarn cluster by adding YARN jars and config xml files. It worked as expected. There is no need to copy jars anymore. I also tested with by adding heron:submit:* --extra-launch-classpath <value> to .heronrc\n. \ud83d\udc4d LGTM\nI tested on a local yarn cluster. Incorrect paths are flagged correctly. \n. Now, the signature of initialize method in IPacking and IRepacking is not same as the one in IScheduler, ILauncher, etc. Is that ok?\n. @avflor @billonahill I agree with your comments. Currently there is no need.\nHowever I am worried about the cost of changing interface. A small change would require changing all implementations of IPacking, changing all test cases where methods are invoked directly, and potentially changing [power]mocks where methods could be invoked using reflection. The current pattern has enough headroom for future enhancements. It has some qualities of the Spring framework, where in the dependencies are injected into the implementation. For e.g. REEF is based on such a pattern.\nMy impression is that the confusion is created by incompleteness of the runtime config object. In this case I would prefer keeping the config object complete over changing the interface. IMO this change introduces a tight coupling.\n. @billonahill Totally agree with you regarding untyped-giant-HashMap. I find it hard to debug and ugly. And DI is out of scope for this change. For now this interface change looks ok. \n. I too can free up some cycles for this improvement. May be this can be done incrementally, one interface at a time.\nBTW, I spoke to a couple of folks here about Guice. Dagger and Tang seem to have evolved from Guice. Both systems provide static compile time checks which seem to be a major limitation of Guice. So I think we could use the newer solutions. I used tang in the YARN scheduler. Most of the configuration errors will be flagged by the IDE itself. It reduced number of required iterations. If you think it helps, I can create a small patch as an example. Although the Tang community is very active, there is limited support available outside.\nMisc references on Dagger\n1. https://dig.floatingsun.net/dagger-vs-guice-8c9fbae4712e#.e1223p2x7\n1. https://blog.gouline.net/dagger-2-even-sharper-less-square-b52101863542#.pkksdqat1\n. @kramasamy The merge conflicts are resolved now. Thanks !\n. @billonahill @maosongfu Thanks for quickly addressing this issue.\n. Nice :+1: \n. Is a \"Best Practices\" doc page a expected deliverable for this issue? \n. LGTM \ud83d\udc4d \n. @kramasamy I can review it partially. I have authored a part of this commit (https://github.com/billonahill/heron/pull/2) which @billonahill had reviewed earlier. \n. Hi @objmagic. I think there is no need to revert this patch. I tested #1329 with the local and yarn schedulers. While it works on local, it has some issues on the YARN scheduler. I will have a fix ready for YARN today. So the build will be stable then. Thoughts?\n. @billonahill \nIs the user expected to configure ${HOME} in some config file? The path is getting resolved incorrectly:\n09/08/2016 22:31:45 -0700:INFO: Logging pid 50263 to file heron-executor-0.pid\n 09/08/2016 22:31:45 -0700:INFO: Connecting to file state with rootpath: /home//.herondata/repository/state/yarn\n 09/08/2016 22:31:45 -0700:INFO: Registered state watch for packing plan changes with state manager <heron.statemgrs.src.python.filestatemanager.FileStateManager object at 0x10f8db450>.\n 09/08/2016 22:31:45 -0700:INFO: State watch triggered for packing plan change. New PackingPlan:\n. @billonahill Thanks for the pointers.\nThis issue is similar to one seen in #1140. Many times YARN application is headless and there is no home directory for the user. Also it is possible the user launching the topology is different from the executor's user. In my test case, YARN set the home as /home. I can thinking of fixing the issue by updating the YARN configuration for now. But I am not sure if resolving the path on the executor is a good idea. What do you think about constructing the state manager directory location on the client and let all components use the same? \n. @mycFelix Your answer is complete and accurate. Thanks !\nWould you like to contribute to this detail to the YARN scheduler doc?\n. @HosiYuki as @mycFelix mentioned, logs would be helpful. In addition check the following\n1. libunwind and python 7 installed? I.e. the topology should be deployable in local mode on all yarn nodes.\n2. if you are using local state manager (single node YARN), the root path config must specifie absolute home path. For e.g {HOME} should be replaced with /home/userDir in heron.statemgr.root.path:         ${HOME}/.herondata/repository/state/${CLUSTER}. Default location of config file ~/.heron/conf/yarn/statemgr.yaml\n. From the logs NoClassDefFoundError: org/apache/avro/io/DatumReader. Seems the avro jar needs to be added to extra-launch-classpath.\n. @HosiYuki The avro ClassNotFound error is coming in evaluator logs. Which means the yarn application classpath may be missing in yarn-site.xml. Could you please check that. \nSample yarn-site config is below\n<property>\n        <name>yarn.application.classpath</name>\n        <value>\n            $HADOOP_HOME/etc/hadoop,\n            $HADOOP_HOME/share/hadoop/common/lib/*,\n            $HADOOP_HOME/share/hadoop/common/*,\n            $HADOOP_HOME/share/hadoop/hdfs,\n            $HADOOP_HOME/share/hadoop/hdfs/lib/*,\n            $HADOOP_HOME/share/hadoop/hdfs/*,\n            $HADOOP_HOME/share/hadoop/yarn/lib/*,\n            $HADOOP_HOME/share/hadoop/yarn/*,\n            $HADOOP_HOME/share/hadoop/mapreduce/lib/*,\n            $HADOOP_HOME/share/hadoop/mapreduce/*,\n            $HADOOP_HOME/contrib/capacity-scheduler/*.jar,\n            $HADOOP_HOME/share/hadoop/yarn/*,\n            $HADOOP_HOME/share/hadoop/yarn/lib/*\n        </value>\n    </property>\n. @silence-liu as @mycFelix mentioned, can you share some details from the yarn user logs ?\n. I think we should not add this to heron doc. Its a very yarn specific\nconfig and is expected to change in future yarn versions.\n. @kramasamy The yarn application classpath is part of yarn config file yarn-site.xml. It is one of the many yarn related configurations like security, queues etc. Similarly many hdfs related configurations belong to hdfs-site.xml and need not be documented for heron's YARN scheduler.\n. @HosiYuki I have not seen this issue before.\n1. Are you seeing too many orphaned HeronInstances, i.e. instances present even after the yarn application is killed. Do you also see any executor processes?\n2. Could you check the executor and shell logs files in the namenodes's local directory\n. @HosiYuki I got occupied with other work and lost track of this issue. Are you still observing this issue? In case you plan to submit new topology, please kill any heron related yarn applications and heron processes on all nodes. Command: yarn application -kill yarn_application_id\n. @amirfirouzi - That is right, installation of heron binaries on the YARN nodes is not required. You may find additional details here:http://twitter.github.io/heron/docs/operators/deployment/schedulers/yarn/. @amirfirouzi - Once the heron scheduler starts, all logs are redirected to a heron log file in the container's local directory. You should find heron-core binaries, log files, etc in this directory. Could you please share what you see there?\nRegarding additional containers, YARN cluster must have sufficient resources. Otherwise new containers will not be allocated. Please check if this is the case.. Hi @yesimsure \nThe topology submission is successful. The REEF client does not exit on its own. Using Ctrl-C to terminate the submission client after you see this message Topology WordCountTopology is running is fine.\nI am not sure what happened to the topology. Did you see any metrics initially? Could you please check and share the container logs?. LGTM, fix failures and Ship it !\n(Apologies for the typo)\n. Interesting... is it the other way around? I am trying this on Java 7\n. LGTM \ud83d\udc4d \n. Very useful change, improves quality of the code \ud83d\udc4d \n. @kramasamy I do not have any pending changes related to this PR. In addition to the CI, I have also tested this change locally.\n. com.twitter.heron.packing.Container is a helper class used by packing algorithms. This change does not impact or is impacted by that class. \n. Just one minor comment, otherwise LGTM. Thanks for the contribution !\n. :+1: \n. @kramasamy In my opinion, the second option to resolve classpath (using jar copy) could be kept in the document for some time. We could remove it after 2-3 releases.\n. LGTM \ud83d\udc4d \n. LGTM \ud83d\udc4d \n. Two small suggestions. Let me know what you think.\n1. You could link this PR with the issue by including Fixes ISSUE_NUMBER in the description of this PR.\n2. It would be better to git squash the commits to avoid confusion and keep commit history readable. \n. LGTM \ud83d\udc4d \n. @billonahill Unit tests for addition and removal of containers are included now.\n. LGTM \ud83d\udc4d \n. \ud83d\udc4d \n. @silence-liu The YARN scheduler does not depend on HDFS uploader. It instead uses the file system configured in core-site.xml. Please make sure core-site.xml is in the client's class path and is correctly configured. In this case the scheduler failed to find reef job on the AM (/tmp/2016_09_28_14_48_07_reef-job-1/reef-job-8790805129276842877.jar). Missing core-site.xml is the most likely issue.\n. @silence-liu This could be a memory related failure (see 2 GB of 2.1GB virtual memory used). This node has 1 GB physical memory. \nI have a request, if possible please attach text instead of images. You could do that using syntax markdown.\n. LGTM \ud83d\udc4d \n. I think I can address the second observation. \nRegarding observation 1, most of the configurations provided to REEF are heron configs and are not optional. For e.g. there is no default value of a topology name. These are provided to launcher by heron config loader which is independently unit tested. I am not sure if this kind of test is applicable here?\n. Updated the unit test. It will fail if unexpected config is found.\ncc @billonahill \n. LGTM \ud83d\udc4d \n. LGTM \ud83d\udc4d \n. @billonahill Are BoltInstance and SpoutInstance internal/core classes? If yes, is interface UpdateableInstance needed? Can the method updateTopologyContext be added to IInstance? \n. The approach looks good to me. My understanding is that the UpdateManager will delete the physical plan and upload a new packing plan. TMaster will use the new packing plan to create a new physical plan and inform all the instances using control message. This control message is received by Slave to update the HeronInstance. Is this correct?\n. LGTM \ud83d\udc4d \n. @kramasamy - Updated the PR description and javadoc for the YARN AM.\n. @kramasamy - Not necessary. A container with instances is typically 4GB/4cores in size. The master out of the box will be 2GB/1 core. The memory allocated to master is configurable. \n. @nlu90 - All questions are welcomed :-)\nYes, one HeronMasterDriver is launched for each topology. In case of YARN, each topology is a YARN application and each app needs an app-master (called Driver in our implementation).\n. LGTM \ud83d\udc4d \n. +1\nAs topology is paused during scaling, it would be useful to know what is the downside of restarting a StreamManager.\n. I see one issue with the proposed approach. As scale up results in increase in number of containers (mostly), i think scale down should prioritize reducing number of containers. \nThe above approach will result in the following if 1 each of A and B is added and removed\nT0: deploy 4 A to B\nPacking: (A A B)   (A A B)\nT1: scale up 1 A and 1 B\nnew packing:  (A A B) (A A B) (A B)\nT2: scale down A by 1\nnew packing:  (A B) (A A B) (A B)\nT3: scale down B by 1\nnew packing:  (A A B) (A) (A B)\n. @billonahill - In the example, event T3 is scaling-down component B. So it will be (A A B) (A) (A B) if allocation factors mentioned in the description is used.\nIncrease in number of containers can increase the total resource cost of a topology, specially for Aurora. In Aurora all containers will be of same size (the largest one). So the allocation factors approach can result in higher cost. For e.g. if the cost at deployment is 100 GB, say cost after temporary scaling-up is 200 GB, cost after restoring original size could still be 200 GB.\nAlso should we design the default scale down approach for a specific use case? Is scaling up a single component and scaling it down expected to be the most common operation? IMO, no.  A user might scale up all the components.\nI think scale down should try to come as close to the packing algo as possible.\n. Thinking aloud...\nA scale-down event will result in some disruption as processes are killed, some in-memory state will be lost etc. How about performing rebalancing and defragmentation (GC) when scale down is invoked?\n. @avflor - +1 looks good to me\n@billonahill @wangli1426 - Could you please help me understand the impact of redistributing process? Restarting an instance would require rebuilding in-memory state from an external state store. Is there any other heron system or user level cost? Could you please share some more details?\n. @wangli1426 - I am not aware of anyone working on HeronInstance migration. It is a good idea and was discussed informally a couple of times in the past. \n@billonahill - you mentioned this in one of the comments above, \"moving and restarting 1000s of process when adding or removing only a few, for example, would incur unnecessary cost\". I am not sure if I understand this. If scale down is removing only a few instances, then very container plans will change. Which means defragmentation will impact few processes only.\n. Folks, would it be worth creating a list of \"popular\" use cases, and compare the algorithms against that list?\nAlso, what do you think about adding a policy configuration to choose between container optimization and balanced scale down?. LGTM \ud83d\udc4d \n. @billonahill - I was using heron update command for all the scaling related steps. Repacking happens on the client. As a new client process is launched for each update action, I dont think this issue is caused by reuse of PackingPlanBuilder. I have not been able to look into the issue in detail. I will try to add a test case for this issue tomorrow.\n. LGTM \ud83d\udc4d . LGTM \ud83d\udc4d . This was needed, thanks !\nI find the runtime config object management confusing too. It is slightly related to this change, but could be addressed in a different PR. Currently two config objects are used in most components. My understanding is that the initial instance of config object will be built/loaded by the ConfigConfig class. The other instance, runtime, would be loaded by the *Main classes. If this is correct, should we remove any runtime specific keys from ConfigKeys and keys.yaml? For e.g. COMPONENT_RAMMAP. The user will never provide these values. Adding these strings to keys.yaml seems unnecessary and may become confusing later.. @billonahill Thanks for the suggestions. I will take a look at the logs and share what I see.\nI am assuming this is a bug? I.e. the stream manager should not wait for the buffers to drain and the tuples in transit will be lost for non-acking topologies.. As pointed out by @billonahill  and based on discussion on Slack, the Dhalion project api will be moved outside Heron. Any Heron specific Dhalion integration work will be moved to heron directory from the contrib directory. \nSo I will close this PR once a new PR is created.. Hi @huijunw I will soon push updates to resolve some of the review comments.. @maosongfu Could you please elaborate on how the customers use the aggregated metrics. Also graphite and other tools provide easy mechanism to aggregate metrics. Do we really need to worry about the intent inside the code? In my opinion metrics aggregation is better handled by tools. . Thanks @maosongfu \nWith the proposed change, every heron instance will be doing extra work for every tuple. Is this overhead less than the wildcard aggregate inefficiency? What if the aggregation takes place at container or topology level. For instance, the metrics cache could aggregate and push this information to sinks.. \ud83d\udc4d LGTM. Marking the PR as ready to merge after discussing with @huijunw . Good catch. The solution is the other way around, numbers need not be specified as strings. I am testing the fix locally. Will create a PR soon.. I am investigating this issue now.. @lucperkins Thanks for this PR ! \nThe doc looks generally looks good to me. We are still developing and improving the feature. I think it would be better to mark this feature as a \"beta\" feature till we have tested and hardened it enough. The document will also need a section on \"How to create and manage Self-regulating topologies\". . @huijunw This one is tricky because the metrics cache is returning empty structures. \nThe metric provider contract should be the same for all metrics providers. I.e. for easy development and maintenance of sensors, all metrics providers must return the same ComponentMetrics structure. In this case (BackPressureSensor.java#L90), the sensor is querying stmgr component and backpressure metric. Accordingly it expects the metrics provider to return one metric in the provided component. However, the metrics cache is returning empty structures. This seems like a common issue and could be handled by MetricsCacheMetricsProvider. Instead of adding logic in all sensors to filter the empty structures, I would suggest fixing the MetricsCacheMetricsProvider. You are most familiar with the MetricsCacheMetricsProvider code. What do you think?. @huijunw Current Dhalion.MetricsProvider interface contract is to return value of a given metricName for all instances of a component. Hence the interface definition. It is not back pressure specific. \nIn this case the metric name includes the instance name, container_1_backpressurebolt_1. Hence the metric provider should return just one value specific to the component, instance, metric-name. However the implementation of metrics cache provider is returning multiple empty values too.\nAs earlier, i think the fix should go in the metrics provider.. @huijunw The inclusion of instance id in metric name is not enforced by dhalion. I think that discussion is outside the scope for this PR.\nCan you please explain why is the metrics-cache returning empty structures when the metric is not available? for e.g. \nmetric {\n  instance_id: \"stmgr-2\"\n}. There is one check in MetricsCacheMetricsProvider to avoid adding empty entries: https://github.com/twitter/heron/blob/master/heron/healthmgr/src/java/com/twitter/heron/healthmgr/sensors/MetricsCacheMetricsProvider.java#L122\nI think a similar check is required here too:  https://github.com/twitter/heron/blob/master/heron/healthmgr/src/java/com/twitter/heron/healthmgr/sensors/MetricsCacheMetricsProvider.java#L127. That should fix the problem and avoid adding filters in all the sensors.. Any topology component specific metric provided by stream manager will need this change. For e.g. BackPressureSensor and BufferSizeSensor. . There are two inconsistencies here. \n\n\nThe topology component metrics related to back pressure and buffer size are owned by stream manager.\n\n\nThe tracker response is different from the tmaster response. \n\n\nAs a result special handling is required in some sensors. There are atleast 3 different ways to solve this :-). And it is not worth debating on which one is the best. For now, i will try to implement what @huijunw suggests. It will impact all the sensors that consume component metrics from the stream manager.. Hi @jerrypeng \nAt one point the resources for examples were reduced (see: 1f5b57b905e7a8ac56c14895c55b07667ceeb358). I think some users had requested that change to be able to run low footprint topologies. \nWhat do you think about using configurable resources instead of increasing for all the example topologies? Also did you see the OOME in existing examples?. @jerrypeng Thanks for your comments. \nFirst, I agree that the way ports are currently managed is error prone. Thanks for taking this up.\nHowever, this PR is addressing some future needs and I don't understand them well. Personally I find this solution overfitting. I think it would be better for one more reviewer to take a look.. Currently, packing algorithm is executed before container allocation and a logical-container id is pre-allocated by the packing algorithm. The user needs to know this logical-container-id in cases like deadlocks where container restarts are required. The packing algorithm also makes resource (cpu, ram) suggestions. \n\u00a0\nThe actual resource allocated by schedulers can be different for from the required resources. For e.g. all Aurora containers are of same size and YARN containers cannot be smaller than the limit configured by the YARN admin. Currently, the packing plan created by packing algorithm is updated by the scheduler based on actual allocation. I think we could employ the same reasoning and pattern for container id too. The container id suggested by packing algorithm is updated by the scheduler based on actual allocation by the scheduler. I believe the logical-container-id is not used anywhere else. \nIn long term, it would be useful to find a way to assign logical container id to Aurora containers. Each container should fetch details of the instances it needs to execute. I think @srkukarni proposed something similar earlier. . @kramasamy What do you think about retaining heron-core.tgz?\nhttps://github.com/twitter/heron/blob/master/scripts/packages/template_bin.sh#L128\n. Hi @nwangtw. If I understand correctly, rate limit is configured at the topology level. Which means TPS of individual components cannot be controlled, and the max rate to be configured will depend on the component producing maximum tuples. In one of the use cases we were working with, we needed to tune output of each components differently. \nWhat do you think about extending this configuration to components? \nI also agree with @srkukarni. If the output is throttled, input queues of the component will fill up and cause back-pressure. Back-pressure will be seen as soon as rate limit is applied and I don't think it matters where the rate limit is applied. Supporting all the languages would be better than creating a language base feature compatibility matrix.. Is can take a look at this issue. There a few places where a whole number value of cpu is assumed. It seems to me that we can get rid of this assumption.. This is what I have seen in the code so far. A container will always request for a whole number of cores. However, the instance inside the container may still be requesting 0.5 core. \nFor e.g. The default value of heron.resources.instance.cpu is 1. So the default container (of 4 instances) will request 4 cores. If the value of heron.resources.instance.cpu is changed to 0.5, the same default container (4 instances) will request 2 cores, or 3 cores for 5 instances. This seems fine to me.\nHi @aahmed-se, can you provide more details about the whole number test-case you mentioned in the description?\n. setContainerCpuRequested sets the max number of cpu's to be allocated to be a container. The default cpu allocation per instance, as i mentioned earlier, is 1. Did you configure instance limit too? \nWhat kind of resource profile are you looking at? And packing plan?. Discussed with @aahmed-se. RoundRobinPacking supported the behavior in which a larger instance can be forced in a small container. RRPacking packing has been deprecated (75c25a08bb5ebd10b76f7b5ededd9eb2f4e9180a). ResourceComplaintRRPacking does not allow this. Whole-number-core does not seem to be the issue. \nAlso 0.5 core assignment to a container seems like a very special case and will need additional tuning, as the defaults resource assignments are more generous.\nI will fix the logging issue in a PR. . This issue is not caused by RCRR packing.\nI launched a topology with 192 ram allocated to a bolt counter. I found the following in the executor logs.\n[INFO]: component name: counter, ram request: 201326592, total jvm size: 192M, cache size: 64M, metaspace size: 128M\nThe executor correctly received ram size. Which means packing worked as expected.\nHowever the issue still exists, because the process fails to launch:\n[INFO]: container_1_counter_3 stdout: Invalid maximum heap size: -Xmx0M\nThe culprit is https://github.com/twitter/heron/blob/master/heron/executor/src/python/heron_executor.py#L562\n# TO DO (Karthik) to be moved into keys and defaults files\n    code_cache_size_mb = 64\n    java_metasize_mb = 128\n...\n      total_jvm_size = int(self.component_ram_map[component_name] / (1024 * 1024))\n      heap_size_mb = total_jvm_size - code_cache_size_mb - java_metasize_mb\n      Log.info(\"component name: %s, ram request: %d, total jvm size: %dM, \"\n               \"cache size: %dM, metaspace size: %dM\"\n               % (component_name, self.component_ram_map[component_name],\n                  total_jvm_size, code_cache_size_mb, java_metasize_mb))\n@kramasamy may know more about the reason.\n@jerrypeng @nwangtw, you may want look at this issue before adding any new configs to correctly estimate jvm size.. I think this PR is incomplete.\nsee: https://github.com/twitter/heron/blob/master/heron/packing/src/java/com/twitter/heron/packing/builder/PackingPlanBuilder.java#L360. Thanks  @cckellogg \nThis is very interesting. However, in the use case you mentioned, the tag/category seems like external to heron and a topology. It is also possibly that a user would like to update topology's tag without restarting the topology. I can also imagine that the tag will be assigned by an operator who may not have the ability to update topology code. Hence, I think making the tag a Config may not address the requirement. \n@kramasamy ?. Since this is a UI/explorer feature and no other heron component will ever use this data, how about adding an UI action/command to \"decorate\" topology with labels? Tracker could persist the data in a new topology sub-node in state manager. . Hi @cckellogg \nEven in the use case you just mentioned, a tag seems external to topology logic. In my opinion, using topology config for tag is odd. I believe a topology developer will not know how its metrics will be grouped. Topologies can be shared and deployed in different environments. A developer cannot be expected to think of all those use cases in advance. \nIt is possible my understanding is flawed. I am hoping others will present their opinion and we can move forward.\n@maosongfu @nwangtw , what do you think about this?. I am planning to merge this PR today. Please let me know if anyone has additional comments. Thanks !. Also, I think it will be worthwhile sharing the tests executed to verify the change. In this case I believe unit tests and integration tests may not be sufficient. I am thinking of verifying heron on YARN cluster. What do you think about this?. heron-core does not contains yarn or aurora scheduler (https://github.com/apache/incubator-heron/blob/master/tools/rules/heron_core.bzl#L37). To me it seems it should not contain any other schedulers too. The executor does not need any scheduler jars, since the scheduler starts before executors and the scheduler itself launches all the executors. When scheduler is running as a service it is contacted using scheduler independent http calls. In the other non-service cases, the scheduler is embedded in the client (https://github.com/apache/incubator-heron/blob/master/tools/rules/heron_client.bzl#L32). Perhaps this needs to be asked in a new issue. \n@maosongfu, @kramasamy, @neng WDYT?\nw.r.t. this issue, the solution would be add the non-service scheduler (aurora in this case) to the healthmgr jar. However I am not sure if inclusion of aurora jar is sufficient. Any files needed by aurora scheduler need to be present on container-0. Is that feasible?\n@huijunw Are you able to run the healthmgr in container-0. @huijunw I am looking at this issue now. Will try to publish new dhalion version today/tomorrow.. ",
    "nlu90": "@kramasamy yes, this pull request is based on the checked in code\n. #97 \n. #371 \n. #249 \n. #250 \n. #546\n. shipit\n. resolved\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. #749 \n. This change breaks our twitter internal's heron build by not generating the release.yaml file.\nCan you also publish an internal RB to fix the issue?\n. removed utils.clean_dir()\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. Hi, I'm fixing the check style in heron/storm dir. There might be some potential conflicts between this PR and #512.\nFeel free to merge this PR and I'll handle those conflicts.\n. \ud83d\udc4d \n. #557 \n. #565 \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. @billonahill LGTM \ud83d\udc4d \n. \ud83d\udc4d \nI happen to encountered the problem while doing travis check for integration test.\n. @maosongfu We do enforce python2.7 to build targets and rules. But for attribute 'tools' in the rule of 'extra_action', bazel somehow treats it differently and doesn't use python2.7 provided in the bazel.rc file. Instead, it generats a file using /usr/bin/python.\nIf we throw exceptions on python2.4, then we will have problems building heron internally at Twitter.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. #395 \n. \ud83d\udc4d \n. \ud83d\udc4d \n. If the scheduler is a long running service, setting the location as ephemeral node is totally fine and can avoid multiple scheduler running as master problem. \nBut in our Aurora Scheduler case, it's not a long running service.  Thus we are not setting the scheduler location at all. We should find some way to persist the location in this case in order for tracker/ui get it.\n. #685 \n. :+1\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @maosongfu sure, I'll add the unit test once other updates are finished\n. \ud83d\udc4d \n. @maosongfu We can add them later when needed.\n. I further cleaned other unused or missing import for all the cli files\n. \ud83d\udc4d \n. The code block formatting also not working for me. But @kramasamy seems has an working version\n. :shipit: \n. #346 \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. git rev-parse --show-toplevel can locate the heron root dir.\n[tw-mbp-nlu contributors (master)]$ git rev-parse --show-toplevel\n/Users/nlu/workspace/heron\n. \ud83d\udc4d \n. \ud83d\udc4d \nAnd please fill out the CLA agreement\nhttp://twitter.github.io/heron/docs/contributors/community/\nfor accepting the patches. It is pretty easy to do.\n. @serejja once you've signed the CLA, could you reply here so that we can merge this change?\n. @serejja Thanks for the patch!\n. \ud83d\udc4d \n. @billonahill I looked into the log, the local integration test takes 5 minutes, the integration test takes around 7 minutes, the unit test takes about 4 minutes.\nAnd I relaunched the ci job and found that it took about 30 mins to finish from checking web contents, checking code style and running unit tests. And remaining 12~15 mins to finish all the integration test.\nSo we need to investigate why 30 mins are needed for all building and test.\n. \ud83d\udc4d \n. @samek For now, what  I've found is the heron_tracker.yaml you pasted is different from the 0.14.0 release as shown below:\nyaml\nstatemgrs:\n  -\n    type: \"file\"\n    name: \"local\"\n    rootpath: \"~/.herondata/repository/state/local\"\n    tunnelhost: \"localhost\"\n  -\n    type: \"zookeeper\"\n    name: \"localzk\"\n    hostport: \"localhost:2181\"\n    rootpath: \"/heron\"\n    tunnelhost: \"localhost\"\nNotice the name localzk and the root path /heron are different from what you provided. And I'm testing it on Mac OS  X Yosemite. So can you double check if the correct version of heron-tool is installed?\n. Interesting. Can you paste the content of ~/.herontools/release.yaml here?\n. I tested on my mac, in order to make the local zk work, you need to manually create the znode \"/heron/topologies\" in your zk. Once it's created, everything should be fine.\n. @saileshmittal should we add this into our doc for future reference?\n. \ud83d\udc4d \n. We have this similar problem when compiling the binaries on one platform and executing it on another platform. One thing we can do is recompiling the whole heron project on your host, and then install the compiled the binaries.\n. \ud83d\udc4d \n. Hi @samek, what do you mean by \"can't see anything happening beyond second bolt\"?\nDoes it mean your bolt logs/prints some information during the execution but you can't find it in Intellij's Console? Our simple ExclamationTopology example can output log and other prints to IntelliJ's console as follows:\nJun 02, 2016 5:04:53 PM com.twitter.heron.localmode.executors.StreamExecutor handleInstanceExecutor\nSEVERE: Nobody consumes stream: id: \"default\"\ncomponent_name: \"exclaim2\"\nJun 02, 2016 5:04:53 PM com.twitter.heron.localmode.executors.StreamExecutor handleInstanceExecutor\nSEVERE: Nobody consumes stream: id: \"default\"\ncomponent_name: \"exclaim2\"\nJun 02, 2016 5:04:53 PM com.twitter.heron.localmode.executors.StreamExecutor handleInstanceExecutor\nSEVERE: Nobody consumes stream: id: \"default\"\ncomponent_name: \"exclaim2\"\nnathan\nnathan\nbertels\nOr you just need a guide on how to setup your IntelliJ for development?\n. @samek \nFor Intellij debugging, Bolts and Spouts are running as separate threads in simulator. So if you want to add breakpoints inside a bolt/spout, you need to set the Suspend Policyof the breakpoint to Thread. Right click on your break point and you can see:\n\nFor the output issue, one thing I can suggest is saving the output to a file and check the file. To set it, chose Run -> Edit Configurations.... and you can see\n\n. Hi @alanngai, based on the branch of  alan/pylint-rcfile from your heron fork. I found the following problems:\n1. In BUILD file: \"help.py\", should be \"cli_helper.py\" on line 12\n2. In main.py: import heron.cli.src.python.help as cli_help should be import heron.cli.src.python.cli_helper as cli_helper on line 26, and change the following occurrence of cli_help to cli_helper.\n3. In main.py: cli_help.create_parser(subparsers) only provides one argument while the method in cli_helper.py requires 3 parameters: def create_parser(subparsers, action, help):. I think we should either remove this line, or find the proper arguments for it. \n4. In submit.py: javaDefines=cl_args['topology_main_jvm_property']) on line 175 should be java_defines=cl_args['topology_main_jvm_property'])\n5. In submit.py: javaDefines=[] on line 106 should be java_defines=[]\nAfter that, I can launch launch a topology successfully. But there might be other problems.\n. I tried with other heron operations and  found following problems: \nkill is not invoked correctly.\nbash\n[tw-mbp-nlu heron (alan/pylint-rcfile)]$ heron kill local nlu-ex\nINFO: heron_class 10\nINFO: heron_class 20\n[2016-06-09 14:23:44 -0700] com.twitter.heron.spi.utils.TMasterUtils SEVERE:  Topology is already activateed  \nINFO: Successfully activate topology 'nlu-ex'\nINFO: Elapsed time: 2.971s.\ndeactivate is not invoked correctly.\nbash\n[tw-mbp-nlu heron (alan/pylint-rcfile)]$ heron deactivate local nlu-ex\nINFO: heron_class 10\nINFO: heron_class 20\n[2016-06-09 14:27:26 -0700] com.twitter.heron.spi.utils.TMasterUtils SEVERE:  Topology is already activateed  \nINFO: Successfully activate topology 'nlu-ex'\nINFO: Elapsed time: 2.935s.\nThey are all bind to activate instead. Other operations restart, version and activate are behaving correctly. The problem relates to the changes you made in kill.py and deactivate.py that reusing the cli_helper's run method.\nCould you fix those problems and let me know?\n. @alanngai any updates?\n. \ud83d\udc4d \n. The heron-cli part looks fine. But for the heron-ui/heron-tracker part, I see the following errors when looking around the topology page. @alanngai Can you fix the issue?\n21 Jun 2016 11:44:33-ERROR:web.py:1407: Uncaught exception GET /topologies/metrics/timeline?cluster=local&environ=default&topology=nlu-ex&metric=gc&component=*&instance=*&starttime=1466523840&endtime=1466534640 (::1)\nHTTPServerRequest(protocol='http', host='localhost:8889', method='GET', uri='/topologies/metrics/timeline?cluster=local&environ=default&topology=nlu-ex&metric=gc&component=*&instance=*&starttime=1466523840&endtime=1466534640', version='HTTP/1.1', remote_ip='::1', headers={'Accept-Language': 'en-US,en;q=0.8,zh-CN;q=0.6,zh;q=0.4,ja;q=0.2,zh-TW;q=0.2', 'Accept-Encoding': 'gzip, deflate, sdch', 'Host': 'localhost:8889', 'Accept': 'application/json,*/*', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/49.0.2623.87 Safari/537.36', 'Connection': 'keep-alive', 'Referer': 'http://localhost:8889/topologies/local/default/nlu-ex', 'Cookie': '_ga=GA1.1.59782163.1463778064'})\nTraceback (most recent call last):\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/web.py\", line 1334, in _execute\n    result = yield result\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/gen.py\", line 628, in run\n    value = future.result()\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/concurrent.py\", line 109, in result\n    raise_exc_info(self._exc_info)\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/gen.py\", line 631, in run\n    yielded = self.gen.throw(*sys.exc_info())\n  File \"/Users/nlu/.pex/code/5674180e6b6e436e31a29124f90f1b52e728dcfa/heron/ui/src/python/handlers/api/metrics.py\", line 85, in get\n    results = yield futures\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/gen.py\", line 628, in run\n    value = future.result()\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/concurrent.py\", line 109, in result\n    raise_exc_info(self._exc_info)\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/gen.py\", line 464, in callback\n    result_list = [i.result() for i in children]\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/concurrent.py\", line 109, in result\n    raise_exc_info(self._exc_info)\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/gen.py\", line 631, in run\n    yielded = self.gen.throw(*sys.exc_info())\n  File \"/Users/nlu/.pex/code/5674180e6b6e436e31a29124f90f1b52e728dcfa/heron/ui/src/python/handlers/access/heron.py\", line 566, in fetch\n    results = yield futures\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/gen.py\", line 628, in run\n    value = future.result()\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/concurrent.py\", line 109, in result\n    raise_exc_info(self._exc_info)\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/gen.py\", line 464, in callback\n    result_list = [i.result() for i in children]\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/concurrent.py\", line 109, in result\n    raise_exc_info(self._exc_info)\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/gen.py\", line 631, in run\n    yielded = self.gen.throw(*sys.exc_info())\n  File \"/Users/nlu/.pex/code/5674180e6b6e436e31a29124f90f1b52e728dcfa/heron/ui/src/python/handlers/access/heron.py\", line 366, in get_metrics\n    raise tornado.gen.Return((yield fetch_url_as_json(request_url)))\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/gen.py\", line 628, in run\n    value = future.result()\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/concurrent.py\", line 109, in result\n    raise_exc_info(self._exc_info)\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/gen.py\", line 631, in run\n    yielded = self.gen.throw(*sys.exc_info())\n  File \"/Users/nlu/.pex/code/5674180e6b6e436e31a29124f90f1b52e728dcfa/heron/ui/src/python/handlers/access/fetch.py\", line 45, in fetch_url_as_json\n    http_response = yield tornado.httpclient.AsyncHTTPClient().fetch(fetch_url)\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/gen.py\", line 628, in run\n    value = future.result()\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/concurrent.py\", line 111, in result\n    raise self._exception\nHTTPError: HTTP 404: Not Found\n. @alanngai sure. You'll need to build and start the heron-tracker/heron-ui on your localhost.\nStep 1: build heron-tracker/heron-ui, in top dir of the heron project:\nbazel run --config=darwin -- scripts/packages:heron-tools-install.sh --user\nStep 2: start heron-tracker/heron-ui, run the following cmds in two separate terminals:\nheron-tracker and heron-ui\nStep 3: open a broswer and enter localhost:8889\nStep 4: launch a local topology and check the output of heron-tracker/heron-ui terminal\n. @alanngai did you merge with the most recent master? I can build the heron project with the new bazel\n[tw-mbp-nlu heron (neng/clean-spi-dir)]$ bazel build --config=darwin heron/...\nINFO: Found 333 targets...\nINFO: Elapsed time: 0.381s, Critical Path: 0.09s\n. @alanngai \n1. Can you try to install the 0.2.3 bazel again? You can down it from here. And pick either bazel-0.2.3-installer-darwin-x86_64.sh or bazel-0.2.3-installer-linux-x86_64.sh to install based on your machine type.\n2. Before build the heron repo, did you run ./bazel_configure.py to setup the environment correctly?\nThese're the two things came into my mind.\n. @alanngai Is there any update? I checked out the branch alan/pylint-rcfile from your heron fork(the latest commit is: commit 2376b6dea135e9296e8b4163c7c7ed642181f72e), and built with bazel 0.2.3 on my own mac. It worked fine:\n[tw-mbp-nlu heron-1 (alan/pylint-rcfile)]$ bazel build --config=darwin heron/...\nINFO: Found 331 targets...\n...\n...\n...\nStarting audit...\nAudit done.\nINFO: Elapsed time: 969.019s, Critical Path: 862.65s\n. @alanngai My macos version is: OS X Yosemite, 10.10.5. \nI'll send the whole build log to you via email. Here are the output of bazel and ./bazel_configuration.py cmd\n[tw-mbp-nlu heron_main (master)]$ bazel version\n.\nBuild label: 0.2.3\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Tue May 17 14:22:21 2016 (1463494941)\nBuild timestamp: 1463494941\nBuild timestamp as int: 1463494941\n[tw-mbp-nlu heron_main (master)]$ ./bazel_configure.py \nPlatform Darwin\nUsing C compiler          : /usr/bin/clang (7.0.2)\nUsing C++ compiler        : /usr/bin/clang++ (7.0.2)\nUsing C preprocessor      : /usr/bin/cpp (7.0.2)\nUsing C++ preprocessor    : /usr/bin/cpp (7.0.2)\nUsing linker              : /usr/bin/ld\nUsing Automake            : /opt/twitter/Cellar/automake/1.14.1/bin/automake (1.14.1)\nUsing Autoconf            : /opt/twitter/Cellar/autoconf/2.69/bin/autoconf (2.69)\nUsing Make                : /usr/bin/make (3.81)\nUsing CMake               : /opt/twitter/Cellar/cmake/3.0.1/bin/cmake (3.0.1)\nUsing Python2             : /opt/twitter/Cellar/python/2.7.8_1/Frameworks/Python.framework/Versions/2.7/bin/python2.7 (2.7.8)\nUsing archiver            : /usr/bin/libtool\nUsing coverage tool       : /usr/bin/gcov\ndwp                       : not found, but ok\nUsing nm                  : /usr/bin/nm\nobjcopy                   : not found, but ok\nobjdump                   : not found, but ok\nUsing strip               : /usr/bin/strip\nWrote the environment exec file scripts/compile/env_exec.sh\n. @kramasamy once this PR is updated, I'll look into it\n. Hi, @alanngai Based on the log output of heron-ui and heron-tracker. I found two problems:\nThe first one is from heon-ui:\nTraceback (most recent call last):\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/web.py\", line 1334, in _execute\n    result = yield result\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/gen.py\", line 628, in run\n    value = future.result()\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/concurrent.py\", line 109, in result\n    raise_exc_info(self._exc_info)\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/gen.py\", line 631, in run\n    yielded = self.gen.throw(*sys.exc_info())\n  File \"/Users/nlu/.pex/code/b661095b1351239862044a4429b3d51c6d46d19d/heron/ui/src/python/handlers/api/metrics.py\", line 52, in get\n    results = yield futures\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/gen.py\", line 628, in run\n    value = future.result()\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/concurrent.py\", line 109, in result\n    raise_exc_info(self._exc_info)\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/gen.py\", line 464, in callback\n    result_list = [i.result() for i in children]\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/concurrent.py\", line 109, in result\n    raise_exc_info(self._exc_info)\n  File \"/Users/nlu/.pex/install/tornado-4.0.2-py2.7-macosx-10.10-intel.egg.eb452d50fb45f95017438ce5b0db2edc504b3a97/tornado-4.0.2-py2.7-macosx-10.10-intel.egg/tornado/gen.py\", line 175, in wrapper\n    yielded = next(result)\n  File \"heron/common/src/python/handler/access/heron.py\", line 396, in get_comp_metrics\n    create_url(METRICS_URL_FMT),\nTypeError: url_concat() takes exactly 2 arguments (1 given)\nIt's caused by a recent change from us, I'll fix it.\n. The second one is from heron-tracker:\n06 Jul 2016 18:40:10-WARNING:web.py:1811: 404 GET /topologies/metricsquery?cluster=local&environ=default&starttime=1467844800&query=RATE%28TS%28word%2C%2A%2C__jvm-gc-collection-time-ms%29%29&endtime=1467855600&topology=nlu-ex (::1) 95.23ms\nTraceback (most recent call last):\n.\n.\n.\n  File \"heron/tracker/src/python/metricstimeline.py\", line 114, in getMetricsTimeline\n    raise Exception(metricResponse.status.message)\nException: Unknown component\n@cckellogg Can you help have a look?\n. @Jonathan-Wei the powermock java library is missing.\nCan you check the dir bazel-bin/3rdparty/java and see if any powermock jars in it? Like the following\nshell\n[tw-mbp-nlu java]$ ls | grep powermock\nlibpowermock-ijar.jar\nlibpowermock.jar\nlibpowermock.jar-2.params\nlibpowermock.jar_manifest_proto\nlibpowermock.jdeps\n. @Jonathan-Wei \nUnlike Storm, heron itself is a library.\nTo run topologies in a distributed environment, you need to setup:\n1. scheduler for scheduling jobs \n2. uploader for uploading and downloading your topology file, heron-core file\n3. statemanager for managing the states during topology running\n. @lohithsamaga If you keep seeing following messages:\nshell\n2016-06-03 14:55:31: heron-tmaster exited with status 32512 \n2016-06-03 14:55:31: stmgr-1 exited with status 32512\nIt means your topology is not started successfully. And based on the status code, 32512 implies that certain c++ lib is missing.\nCan you check the log files of tamster and stmgr in ~/.herondata/topologies/{cluster}/{user}/{topology} to find out more details?\n. @XciD the OutgoingTupleCollection is not thread safe.\n. @kramasamy I discussed this issue with maosong.\nMaking it thread safe introduces performance overhead. Instead, we can add synchronization mechanism in the bolt/spout code if needed.\n. \ud83d\udc4d \n. Hi @hennarah, I assume you are referring the Quick Start Guide page containing the following two lines,\nheron-client-install-0.13.7-PLATFORM.sh\nheron-tools-install-0.13.7-PLATFORM.sh\nThese above 2 are not URL links, the PLATFORM is variable. You need to go to out github release page to choose the binary suitable for your platform: ubuntu or darwin.\n. @kramasamy What are the changes need to be made for  our Twitter internal building/testing environment?\n. @jiandongjia Is this a duplicate of #885 ?\n. \ud83d\udc4d \n. @wking1986 \nCould you check logs to see if your topology is actually running? Sometimes the pplan missing is due to topology not running correctly. If this is the case, you can kill the topology and submit it again and see if the issue resolves.\n. @jiandongjia It's probably due to the user testuser doesn't have permission to access the .pex dir under your home directory. Can you check the permission of the .pex dir? And change it if necessary.\nAnd also, you can try to submit a topology using your own user to see if there's other problems\n. @jiandongjia By looking at the error message, the aurora is trying to call safe_mkdir method which is defined as follows:\n``` python\ndef safe_open(filename, args, *kwargs):\n  \"\"\"Safely open a file.\nsafe_open ensures that the directory components leading up the\n  specified file have been created first.\n  \"\"\"\n  safe_mkdir(os.path.dirname(filename))\n  return open(filename, args, *kwargs)  # noqa: T802\n```\nIt's trying to mkdir at certain place, and it's probably some place in your dir.\n. @kartik894 can you check the client.yaml in your aurora conf dir? You should  see the following 2 confs:\n``` yaml\nWhether role/env is required to submit a topology. Default value is False.\nheron.config.is.role.required:               True\nheron.config.is.env.required:                True\n```\nChange the value to False if your aurora cluster doesn't require the role and env to be provided. Otherwise, you need to provide those two arguments when submitting the topology like follows:\nheron submit aurora/{YOUR_ROLE}/{YOUR_ENV} --config-path ~/.heron/conf/ ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. Twitter's environment is based on old version of thrift, please make sure the upgrade doesn't break our internal users before merge it.\n. @billonahill In the Context.java code, the timestamp is fetched as follows:\njava\n  public static String buildTimeStamp(Config cfg) {\n    return cfg.getStringValue(ConfigKeys.get(\"BUILD_TIMESTAMP\"));\n  }\nWe either make it a string in yaml or fetch it as a long in the code. I'm fine with either solution, let me know your idea.\n. @billonahill Updated. Timestamp now is Long.\n. @maosongfu I relaunched the test yesterday and it passed. This is a flaky test due to python file open and read operations. It's not caused by this PR. And for your suggestion, we can refine the test in a separate pr\n. LGTM \ud83d\udc4d \n. once the \"\\n\".join(lines) change is made, it will be good to be merged\n. @maosongfu can you have a look?\n. 1. Please separate your changes for heron-tracker in another PR\n2. fix the problem when a non-existing role provided the program throws exceptions, and checks if any other information missing will also cause the exception\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @kramasamy @maosongfu This change may break Twitter internal heron usage. Please do not merge it until we verify it also works with Twitter environment.\n. \ud83d\udc4d \n. @maosongfu Can you have a look at this PR?\n. @kartik894 \nCan you check the following:\n1. tmaster's log to see if there's any problem? \n2. If the zookeeper node for tmaster's location exists?\nThe stream manager is having problems to fetching tmaster's location from zookeeper\n. \ud83d\udc4d \n. @billonahill There are some platform-dependent bahavior during the build. I tried to use scripts/package/BUILD for centos building and encountered the following error\nbazel build --config=centos scripts/packages:heron-client-install.sh\nWARNING: Output base '/home/nlu/.cache/bazel/_bazel_nlu/3f912366ea080f2f98e424559c0207ed' is on NFS. This may lead to surprising failures and undetermined behavior.\nINFO: Found 1 target...\nERROR: /home/nlu/heron/scripts/packages/BUILD:100:1: null failed: build_tar failed: error executing command bazel-out/host/bin/external/bazel_tools/tools/build_defs/pkg/build_tar '--flagfile=bazel-out/local-fastbuild/bin/scripts/packages/heron-core-lib-instance.args': com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n  File \"/home/nlu/.cache/bazel/_bazel_nlu/3f912366ea080f2f98e424559c0207ed/heron/bazel-out/host/bin/external/bazel_tools/tools/build_defs/pkg/build_tar.runfiles/__main__/../bazel_tools/tools/build_defs/pkg/build_tar.py\", line 92\n    mode = 0o755 if os.access(f, os.X_OK) else 0o644\n               ^\nSyntaxError: invalid syntax\nTarget //scripts/packages:heron-client-install.sh failed to build\nUse --verbose_failures to see the command lines of failed build steps.\nINFO: Elapsed time: 3.326s, Critical Path: 0.66s\n. @billonahill Here's the output with --verbose_failures option\nbazel build --config=centos scripts/packages:heron-client-install.sh --verbose_failures\nWARNING: Output base '/home/nlu/.cache/bazel/_bazel_nlu/3f912366ea080f2f98e424559c0207ed' is on NFS. This may lead to surprising failures and undetermined behavior.\nINFO: Found 1 target...\nERROR: /home/nlu/heron/scripts/packages/BUILD:192:1: null failed: build_tar failed: error executing command \n  (cd /home/nlu/.cache/bazel/_bazel_nlu/3f912366ea080f2f98e424559c0207ed/heron && \\\n  exec env - \\\n  bazel-out/host/bin/external/bazel_tools/tools/build_defs/pkg/build_tar '--flagfile=bazel-out/local-fastbuild/bin/scripts/packages/heron-client-conf-aurora.args'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n  File \"/home/nlu/.cache/bazel/_bazel_nlu/3f912366ea080f2f98e424559c0207ed/heron/bazel-out/host/bin/external/bazel_tools/tools/build_defs/pkg/build_tar.runfiles/__main__/../bazel_tools/tools/build_defs/pkg/build_tar.py\", line 92\n    mode = 0o755 if os.access(f, os.X_OK) else 0o644\n               ^\nSyntaxError: invalid syntax\nTarget //scripts/packages:heron-client-install.sh failed to build\nINFO: Elapsed time: 2.542s, Critical Path: 0.16s\n+1 for single BUILD file if possible.\n. @caofangkun ./python/pylint/BUILD is the only build file in third_party directory doesn't have licenses([\"notice\"]) in the head. So once it's fixed, everything should be fine.\n. \ud83d\udc4d  Thanks for the patch!\n. @atibon How did you build and install the tracker and ui?\nCan you try the following command \nbazel run --config=darwin -- scripts/packages:heron-tools-install.sh --user\nthen start heron-tracker and heron-ui in the terminal? And see if it works?\n. @atibon any updates?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @severun Currently, heron doesn't support ShellBolt.\n. @caofangkun How is the ShellBolt support going on? If you need any help, please let us know.\n. \ud83d\udc4d  \n@windie This seems a typo when we added the new package namespace for storm compatibility\n. \ud83d\udc4d  for internal jenkins CI\n. \ud83d\udc4d \n. LGTM \ud83d\udc4d \n. Please don't merge this PR until we verified this change doesn't break Twitter's internal building environment for Heron.\nAlso, we just upgraded to bazel 0.2.3, it should be a stable building tool for some time.\n. \ud83d\udc4d  once the cpu and disk issue is filed and comments are added\n. @kramasamy LGTM\n. @kramasamy Can you have a look at this PR?\n. \ud83d\udc4d \n. @kramasamy I refactored the code and added many comments.\nI'll add unit test in the next commit\n. @kramasamy unit tests for LocalBlobstore and LocalMetastore are added\n. Will close this PR for it's opened too long ago and now we have the downloaders spi.. For the license issue, since many code in the heron/storm directory is borrowed from Storm project, we need to respect the original license from Storm project. \nAs we updated the license in #1080, please retain the original license.\n. \ud83d\udc4d \n. I just looked into the spi-jar genrule, only files in utils and common are included in the spi jar.\n```\njava_binary(\n    name = \"spi-unshaded\",\n    srcs = glob([\n        \"/spi/utils//.java\",\n        \"/spi/common//.java\",\n    ]),\n    deps = utils_deps_files,\n)\ngenrule(\n    name = \"heron-spi-jar\",\n    srcs = [\":spi-unshaded_deploy.jar\"],\n    outs = [\"heron-spi.jar\"],\n    cmd  = \"cp $< $@\",\n)\n```\n. The original change is made in #937 \n. @caofangkun These duplicated code is for namespace compatibility. We want to support both the old and new storm namespace. If you have any good idea on how to maintain the namespace compatibility without duplicated code, please let me know.\n. @caofangkun The hack you provided is replacing namespace from \"backtype.storm\" to \"org.apache.storm\". It's not for namespace compatibility but for namespace replacement.\nOnce replaced, people will not be able to use the old \"backtype.storm\" namespace with the new release of heron-api/heron-storm.\n. Hi @mycFelix, can you try to use the heron-api and heron-storm jars in the following zip file?\nheron-api-storm.zip\nAnd let me know if there's any problem\n. @mycFelix Thanks for the feedback! Here is the new customized jar:\nheron-api-storm.zip\n. @mycFelix Yes, it's due to the different interfaces.\nTn storm, the interface of MultiScheme is public Iterable<List<Object>> deserialize(ByteBuffer ser);,\nwhile in heron, we use Iterable<List<Object>> deserialize(byte[] ser); \nThe two methods accept different type of parameters. There are 2 ways we can solve the probelm:\n1. update the interface in Heron to use ByteBuffer; This may cause a lot of effort to migrate.\n2. update the storm-kafka code to provide byte[] to the method; This requires we diverge from the storm kafka spout.\nLet me know your thoughts about the problem.\n. #1299 \n. @kramasamy Created the documentation issue #1173\n. \ud83d\udc4d \n. @maosongfu @ajorgensen I created an issue #1171 for checking the missing dependency and put it into the 0.14.3 milestone. \nWe can merge this PR and track the problem in that issue.\n. @kramasamy yeah, I compared the jars in WORKSPACE file with the old heron repo. Most of the used jars remain the same.\n. \ud83d\udc4d \n. @avflor yeah, as we discussed in a previous refactor PR #1079 to take cpus and disks into consideration.\n. @kramasamy we need this for heron-kafka spout as well\n. @kramasamy If we want to introduce the notion of config per component, what are the changes needed?\n. @kramasamy \n1. For release purpose, you can use the pom.xml file and maven to do a binary release. And also since the code will finally be moved into a separate repo, we should keep it decoupled with other part of the heron repo. Adding a build file also requires changes in the WORKSPACE file to include the extra binary dependencies, changes in other build scripts to integrate with the release scripts. Those changes need to be reverted once the code is moved. I think it's not a good practice.\n2. This PR also includes the unit tests\n3. I'm fine with either. I can change it to src/java\n. @mycFelix, thanks for mentioning it, I've updated it.\n@kramasamy the BUILD file for bazel is also added.\n. @khushboo13 The code is merged here #1317 \n. @khushboo13 we updated the existing storm-kafka to work with heron\n. @mycFelix yeah \ud83d\udc4d \n. @kramasamy I added the checkstyle for apache code\n. @mycFelix nice point\n. Just a reminder, the license here is not the Apache License. It will fail checkstyle if the bazel BUILD file is added and travis conducts the integration test check.\n. @reconditesea Yeah,  it's possible to do that.\n. @kramasamy Our storm api doesn't have TupleUtils class. We can add one if we want to make the storm-hdfs works without any change.\n. @kramasamy I didn't find the TupleUtils class in storm's 0.9.6 and 0.9.7 source code. I'll assume this class is only added for newer 1.0.x storm.\n. @otter-in-a-suit we have merged the fix for this issue. Can you test it out with the current master or with the 0.14.5 release?\n. I'm new to yarn scheduler, so my question might be noob. \nIs the HeronMasterDriver luanched in separate containers for each topology? Will there be a case that multiple tmasters running in the same container after this change?\n. LGTM. LGTM \ud83d\udc4d . \ud83d\udc4d \n. \ud83d\udc4d . @objmagic you can look into our internal new heron-ui page to see the problem by selecting a topology, clicking one component and checking the metrics.\nYou may also try to reproduce it locally.. \ud83d\udc4d . all the twitter internal tests are passed. \ud83d\udc4d . \ud83d\udc4d \nSorry for mistakenly closed the PR... These changes are merged from another PR: #1820, thus closing this duplicate one. \ud83d\udc4d . \ud83d\udc4d . \ud83d\udc4d . LGTM. \ud83d\udc4d . also a general comment: libs (api, common) now have normal and never-link versions, is that possible to keep only one version?. @huijunw what's the reason to link this PR with the class loader issue? They are separate ones.. \ud83d\udc4d . Is this config.h file included anywhere? Those places should also be updated.. Any concrete examples?. LGTM. 1. FileUtils.deleteDir unit tests are added\n2. All failed unit tests due to refactoring are fixed\n3. Refactored CheckpoingManagerServerTest to use CountDownLatch instead of Thread.sleep()\n4. Changed the general OBJECT type to MAP type for the ConfigKey.\nThough the ConfigKey brings type check, I feel it is complicated to compose the code and hard to generalize for situations like nested configs and complex value types. We need to investigate if this part could be improved. \n. Refactor the code:\n1. rename localFileSystemBackend to \"localFileSystemStorage\" in LocalFileSystemStorageTest.java\n2. rename CheckpointManager.start to CheckpointManager.startAndLoop in CheckpointManager.java. After some investigation, the cause of the failure is:  Client and server are running in separate threads and client trying to connect to the server before the server is started.  \nThe fix is to let the client connect to server only when the server is started.. @billonahill \nThis PR keeps storing ByteAmount objects for ram related settings if you take a closer look at the changes. E.g, https://github.com/twitter/heron/pull/1842/files#diff-932b57c1a82df361bb7446cf9380e87cR58. \nThe only difference is now user needs to call the static method provided in heron.api.Config to set them. This is reasonable because those ram settings(container ram, component ram) are more concepts in Heron not in Storm. The previous way of the config works just mix concepts from heron and storm together, while now the config for storm and config for heron are separated by breaking this inheritance.\nBetter typing is a good idea and we do need to move toward to it. But given the fact that our users are blocked by this bug for migration now and Duration object introduces a lot of changes in other places, I would prefer we do those changes in another separate PR. This could benefit our users' migration progress as well as us to keep issues separate.\n. @billonahill Thanks for the idea and the discussion. I've created two issues #1845, #1846  to tracking the problems we found during our discussion.\nI'll merge this PR to unblock the release process for now.. @kramasamy \nIf those apis will change extensively, keep them out of users' reach will be the best strategy. But Heron APIs are also exposed through Inheritance. Users can access those public methods/apis in Heron Config through a Storm Config object. \nAlso, in the first place in our internal heron, the storm config doesn't extend the heron.api.Config at all. It's Config extends HashMap<String, Object>, and our users are all using the provided static methods setting those heron unique confs.. #1835 . \ud83d\udc4d . As a discussion result, we will ask our users to add \"--\" to fix the problem. And this solution is verified working internall. So I'll close this pr. Split this PR into 2 separate smaller ones: #1861 #1862, for easier and fast reviewing. This PR will be closed. LGTM, but please fix the failed test before merge the changes.\n. the last commit a86f8b4 includes \n1. some code refactoring in the IStateManager.java for easier reading\n2. updated getStatefulCheckpoint to getStatefulCheckpoints. @huijunw This only adds the binary into the heron-core.tar.gz. They will not be started at all.\nAnd later, we have a switch to start them if the topology is configed to be stateful. Otherwise, they won't be started.. After some investigation, we found that the UI is showing the status correctly. For all these topologies, the tmaster locations are missing.\nNow the question is why those tmaster locations disappear from the zk statemgr. The facts we have are:\n1. the tmaster location node in zk is an ephemeral node\n2. there's no logic in the code that delete the tmaster location from zk\n3. once a tmaster restarted, it will registered its location to zk\nThe current suspicion is that there's network problem between the tmaster and zk. It causes the zk think the seesion is closed thus delete the tmaster locaiton. But in the meantime, the tmaster is running fine.\n. @srkukarni I submitted the topology to production aurora cluster for the testing. #1872 . @srkukarni added stateful.yaml into other schedulers. \nThis stateful.yaml contains config for LocalFileSystemStorage. People can change it whenever necessary. @billonahill  updated. \ud83d\udc4d  much cleaner!. two separate PRs will be sent instead:\n1. config updates\n2. schedulers and executor chagnes to start ckptmgr. Verified with internal AuroraScheduler starting ckptmgr if the topology is stateful.. LGTM except my question of return null. @jrpspam the NoClassDefFoundError is caused by the IRichSpout file available at compile time but missing at runtime. I would suggest you to check the correct mvn command to build a deployment jar with all the classes packaged.\nYou probably need to look up the usage of maven-dependency-plugin and maven-jar-plugin. Here are the links:\nhttps://maven.apache.org/plugins/maven-jar-plugin/\nhttps://maven.apache.org/plugins/maven-dependency-plugin/\n. @huijunw Those metrics are calculated on the run. You can check the commit I mentioned to see how they are calculated.. I remember there are other places having the same problem, please also fix them.. @objmagic the issue you mentioned will be fixed in another PR. @billonahill \nThis integration test: \"BasicTopologyOneTaskScaleUp\" is flaky. I've seen it fails the internal CI several times. Any ideas about this test?. I also saw this test failed internally. If you like, I can share the failed build number and log with you. unit tests added. +1. Added unit test for piper.\nUpdated the PR title and description.\nAdded comments for the stateful controller.. Before we merge this pr into master, let's test with some user topologies to see if there's any performance degradation. \nFor debugging the issue, we can make a customized release based on the branch.\n. @maosongfu  Updated the description.. @huijunw I checked the code, there's a dispatchMessage method in client.h amd server.h which acquires memory from the mempool for messages. @huijunw Simply limiting the resources used by stmgr to me is not a good solution. It may cause performance issues if the stmgr really needs so many resources.. @huijunw This suggestion in general is good. But before we add this functionality, we need to first analyze what leads to the large heap and see if we bound the mempool will the problem still occur.. @objmagic Once this PR is merged, the heron-api.jar won't have Google Guava any more. Notice I removed the guava dependency from common's basics-java target\nThe shading rule in heron-api is used for shading the protobuf deps. The guava comes from heron-spi component. And most of the case are using the annotation @VisableForTesting. \nGiven it comes from a separate component and the current common component is blocking new production releases, I suggest we do a separate cleaning for the heron-spi component in a later PR.. Twitter internally verified that the fix works.. @billonahill \nThis doesn't break existing customer. For the following reasons:\n1. Anyone using ByteBuffer will facing the same problem here since ByteBuffer is not serializable. So there should be no existing case using it correctly with the topology.\n2. The wrapped tuple is passed into heron internal system via OutputCollector, and it accepts the tuple of List<Object>. So the byte array is fine to be passed.\nAlso Added the @Override annotation to RawScheme's methods. \ud83d\udc4d . \ud83d\udc4d . The assumption here is once the executor in zombie host's container X is killed, it will not be restarted by Aurora. But the executor in normal host's container Y is killed, Aurora restarts it. Thus, only one stmgr for a particular container is running eventually and resolve the conflicting connection issue.\n. @srkukarni Updated.  Thanks for this suggestion. +1 for cleaning the duplicate config logic. . Verified with twitter's environment, the bazel 0.5.4 can compile the project successfully.\nPlease fix the falied check for travis here so that we can approve it.. \ud83d\udc4d . @sijie Did you add @PrepareForTest(Extractor.class) at the start of your test case? Like this:\nhttps://github.com/twitter/heron/blob/master/heron/scheduler-core/tests/java/com/twitter/heron/scheduler/client/SchedulerClientFactoryTest.java#L81. Great! we succeeded to remove the singleton! LGTM now.. @srkukarni The fetcher abstraction is to allow heron client to download topology files from remote storage like Packer within twitter and submit it. We need to update the heron client is because the topology's main method must be executed locally to generate information for topology submission.. It looks good to me now.. Sure, I agree we should move to the proposed way to manage them. Let's make the change.. Here are the performance metrics comparison on Exclamation topology:\nFor CPU usage:\n\nFor Spout Emit Count:\n\nFor Bolt Execute Count:\n\nGiven the CPU usage is almost the same, the topology's performance is slightly better for the bufferevnet version. \n. @srkukarni Yeah, here's the longer time: 3 day result\nFor CPU usage:\n\nFor Spout emit count:\n\nFor Bolt execute count:\n\n. Some more metrics with some more fixes in the sanjeevk/moveto_bufferevent branch:\nCPU:\n\nSpout Emit Count:\n\nBolt Execute Count:\n\nAnd I also investigated that the CPUs are different for the two topologies' containers. The 0.15.2 release topology is running with better and faster CPUs compared to those used by bufferevent release topology. \nThe trends for both are pretty similar and there's no more fluctuation for the bufferevent release topology. Further more experiment shows that given same cpus, the bufferevent version topology has better performance and is more stable than the current release. So this change is good to be merged now.\nCPU usage:\n\nSpout Emit Count:\n\nBolt Execute Count:\n\n. @jerrypeng  This change in general is a really good one.  My only question is once this change merged, does the heron-client.tar.gz still has heron-examples.jar with it? The heron-examples.jar is really useful for verification and debugging purpose. Hope we can keep this unchanged.. This requires some changes inside Twitter's related repo correspondingly to avoid issues. Please don't merge it until we verifies it works fine with twitter environment.. Verified. The pex change doesn't cause any building problems.. Please don't merge it until we verify it doesn't cause any problem.. @srkukarni The rebase work causes confusions to the PR. It is closed accidentally when i tried to update the branch. I'll create a new PR for this work and reference this pr.. @aahmed-se I've change the statemanagerfactory.py code to make it only create object but not actually connecting to zk: #2327 \nThe issue here is why in an obvious failure test case, bazel doesn't report the failure.. @aahmed-se  Yes. I'm converting it to a real unit case and making the test fail on purpose. But the failure is not reported.. Thanks for fixing the issue!. LGTM. We also need to update some internal tests to work.\nPlease merge it after the travis ci is passed.. \ud83d\udc4d . @huijunw I suggest 1). we make sure things are not broken with any such dev tool upgrades; 2).  with the first point guaranteed, we move as fast as we can to help the oss community.. \ud83d\udc4d . Just a reminder, if we will do similar repeated exception handling in the future, we need to find out a cleaner way.. any update on this PR? it failed the ci test.. IIRC, the previous C++ Binding PR needs to do some work about logging. Is the logging issue solved here or there will be further PRs?. \ud83d\udc4d . @jrcrawfo Ping for this PR, any updates?. @bornej It is possible to utilizing multithreads in Heron even without this patch. The only requirement is that you need to ack/fail the processed tuple in the original main thread for correctness. If your topology is AT-MOST-ONCE semantics, then you don't even need to worry about this problem.\n@srkukarni After reviewing this long converstaion and the changes made, it looks good to me. But I think we need to mention the thread-safe acking to users very carefully, ensures them that EFFECTLY-ONCE might be broken if they don't implement preSave carefully.. @kramasamy @nwangtw @huijunw Any ideas on this PR?. LGTM \ud83d\udc4d . What's the background of removing this Math.rounding()? . \ud83d\udc4d . @ajorgensen seems several unit tests failed. Could you update it and for a further review?. @kramasamy  Ping. @kramasamy Let's try to merge this first connector and then keep adding more in the future. Two more suggestions for this PR:\n\n\nremove the cereal related files in third_party\n\n\nrename the directory connectors to heron-connectors\n\n\n. +1 for instance & api's java part.\ncpp & python part seem no changes needed. \ud83d\udc4d . @comes5 In short, Yes. You'll need to build the whole project to get the core jars for testing. But since bazel is smart enough to do the incremental building, it will only build targets affected by your changes after the initial full building.. Please also clean up the scribe and thrift test directory.. LGTM. Let's see if @kramasamy has any comments.. @kramasamy  This change may break your downloader usage, please update your code accordingly using the new downloader.yaml file.. There's some code style problem. Please fix it before we can merge it.\nheron/common/src/cpp/network/connection.cpp:65:  Line ends in whitespace.  Consider deleting these extra spaces.  [whitespace/end_of_line] [4]\nTotal errors found: 1. here is some discussion about the __lll_lock_wait error.\nThe comment here claims it's single thread mode. But from the dump we found there're actually 2 threads. So we need to update the comments to correctly reflect what the code does.. @kramasamy \n1. On demand basis cleaning is not working in localfsstorage because the checkpoints are stored locally and they can't receive the delete requests from tmaser by design.\n\nWhat do you mean by \"backgroud cleaning\"? Spawn another thread and keeps monitoring # of checkpoints? This sounds too overkill.. @kramasamy This change only needed by local filesystem storage. For other storage systems, no change is needed and the original design works fine.. @nwangtw @jerrypeng Could you have a look at this PR?. Closing this PR due to confusing commits and code not cleaned up. \nA new PR will be posted for this feature with better commit information and clean code.. @huijunw Could you take a look?. @sijie  Could you help take a look whenever you have some time?. LGTM. Based on the description, this is a test PR. Why it's merged into master branch? @huijunw . Duplicate PRs. Will close this one.. One general question is why there'll be duplicated instance state info? Can we prevent the duplicated state being generated at the first place?\n. @nwangtw Please take a look.. @nwangtw @jerrypeng Please take a look.. @nwangtw @yaoliclshlmch Please help take a look. I tried to update it and found I don't have the permission.\n\n@kramasamy  Do you have the permission for changing that?. @kramasamy Could you also delete the deprecated heron-users google group? This group has been receiving unrelated posts(e.g. furniture selling) for quite a long time and no one asks heron questions. Since we are preparing for the first Apache release, it's a good time to migrate all the discussion to the mailing list at this time.. The refactoring looks fine to me. \nAs Jerry mentioned, this might break the DSL abstraction. But we can merge it for now and see . @sleepy-brook  Heron currently doesn't have native Kafka spout, but you can utilize the Kafka spout from Storm to compose your topology. \nhttps://github.com/apache/storm/tree/master/external/storm-kafka-client. In general, it looks good to me. One thing I suggest to try is verifying those metrics with an actual running topology in our environment.. the scripts/travis directory also contains build scripts. Could you clarify the difference between this PR's scripts and the ones in that directory?. LGTM. ",
    "billonahill": "@kramasamy in #910 and #939 we suppressed the warnings when untarring but bazel still sets bad timestamps. This is a bazel issue.\n. @objmagic are you sure this is the reason? I've also heard that the problem with large topologies in the UI is that the UI has to make a very large number of concurrent requests to tracker fetch all the metrics it needs to render. When the UI requests metrics from tracker, is tracker pulling and aggregating from TMaster synchronously?. Instead of language based modules, what do folks think about functional-based modules? Like one for core, cli, tools, etc?\n. Right we'd want a way to discover all the parts of the functional modules. They could be api, client, core, tools, which are the 4 build artifacts produced by bazel. Or they could be the lower level packages:\nls bazel-bin/heron/\napi          cli2         controller   executor     localmode    newscheduler proto        shell        statemgrs    storm        tracker      uploaders\ncli          common       examples     instance     metricsmgr   packing      schedulers   spi          stmgr        tmaster      ui\n@kramasamy do you know how to do a bazel query command for all deps of each of those?\n. @kramasamy maybe it's better than to not use client, core but instead to break out by logical grouping like what we see in ls bazel-bin/heron. The benefit is that when working in a functional area you can see both the python, java, bash, perl, etc. parts together as part of the same client module, which might share higher level build targets. Splitting up by language fragments the logical groupings of the project.\n. that sounds reasonable to me @prabhuinbarajan \n. @prabhuinbarajan that seem fine, but when I build the project and run idea, the java class deps don't resolve and I've got red squigglies.\nAlso, can you try merging from master and pushing. I want to see if that fixes your travis CI build failures.\n. Actually, java classes do resolve for me now. I had to re-configure by JDK version and now I'm all good.\n. I had to go to 'Project Structure' and 'Project SDK' and set it.\n. Acutally from TopologyContextImpl the com.twitter.heron.api.generated package is not found:\nimport com.twitter.heron.api.generated.TopologyAPI;\nAlso I when I right click at the top-level to add a new file, I can't. \n. Thanks looks good, thanks! I still can't create a file under the top level project dir, but that's not a big deal.\n+1\n. @prabhuinbarajan I've found that any files that aren't under 3rdparty or heron are not recognized as being project filed. This means you get a warning when editing them and their git changes don't appear. I think we want the top level dir to be a content root.\n. @prabhuinbarajan and @maosongfu try working on files that aren't below heron/ or 3rdparty/. They're unrecognized, which causes all sorts of problems. They need to be part of a content root.\nEditing a file show's it's not recognized in the project:\n\nYou can't see git history:\n\nYou can't add new files:\n\n. Everything should be included, since they're all fair game to be worked on.\n. Also commented out some things in release/BUILD that are breaking things. @kramasamy is that entire BUILD going away?\n. @kramasamy the user doesn't need to install semver since it's committed in the repo and referenced directly via it's path.\nDo we have any docs for how we launch instances on Docker? What's in docker/ is just how we build. What I need to know is what the output of gcc --version looks like on each platform, since the parsing logic will be different. It's very different between OSx and rhel5 for example.\n. The script is working and has failed travis CI due to a too low version:\nFAILED:  /usr/bin/gcc-4.8 is version 4.8.1 which is less than the required version 4.8.2\nhttps://travis-ci.com/twitter/heron/builds/23718358#L2169\nEither the CI host is non-compliant or our min version for gcc of 4.8.2 is too strict.\nI've added more checks. Here's what the script currently does on mac, centos and ubuntu.\n```\n$ CMAKE=/Applications/CMake.app/Contents/bin/cmake PYTHON2=/usr/bin/python2.7 ./bazel_configure.py\nPlatform Darwin\nUsing C compiler          : /usr/bin/gcc (4.2.1)\nUsing C++ compiler        : /usr/bin/g++ (4.2.1)\nUsing C preprocessor      : /usr/bin/cpp (6.0)\nUsing C++ preprocessor    : /usr/bin/cpp (6.0)\nUsing linker              : /usr/bin/ld\nUsing Automake            : /opt/twitter_mde/homebrew/Cellar/automake/1.15/bin/automake (1.15)\nUsing Autoconf            : /opt/twitter_mde/homebrew/Cellar/autoconf/2.69/bin/autoconf (2.69)\nUsing Make                : /usr/bin/make (3.81)\nUsing CMake               : /Applications/CMake.app/Contents/bin/cmake (3.5.1)\nUsing Python2             : /System/Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7 (2.7.10)\nUsing archiver            : /usr/bin/libtool\nUsing coverage tool       : /usr/bin/gcov\ndwp                       : not found, but ok\nUsing nm                  : /usr/bin/nm\nobjcopy                   : not found, but ok\nobjdump                   : not found, but ok\nUsing strip               : /usr/bin/strip\nWrote the environment exec file scripts/compile/env_exec.sh\n$ cd docker && ./build-artifacts.sh centos7 0.12.0 .\n...\nPlatform Linux\nUsing C compiler          : /usr/bin/gcc (4.8.5)\nUsing C++ compiler        : /usr/bin/g++ (4.8.5)\nUsing C preprocessor      : /usr/bin/cpp (4.8.5)\nUsing C++ preprocessor    : /usr/bin/cpp (4.8.5)\nUsing linker              : /usr/bin/ld.bfd\nUsing Automake            : /usr/bin/automake (1.13.4)\nUsing Autoconf            : /usr/bin/autoconf (2.69)\nUsing Make                : /usr/bin/make (3.82)\nUsing CMake               : /usr/bin/cmake (2.8.11)\nUsing Python2             : /usr/bin/python2.7 (2.7.5)\nUsing archiver            : /usr/bin/ar\nUsing coverage tool       : /usr/bin/gcov\nUsing dwp                 : /usr/bin/dwp\nUsing nm                  : /usr/bin/nm\nUsing objcopy             : /usr/bin/objcopy\nUsing objdump             : /usr/bin/objdump\nUsing strip               : /usr/bin/strip\nWrote the environment exec file scripts/compile/env_exec.sh\n$ cd docker && ./build-artifacts.sh ubuntu14.04 0.12.0 .\nPlatform Linux\nUsing C compiler          : /usr/bin/gcc-4.8 (4.8.4)\nUsing C++ compiler        : /usr/bin/g++-4.8 (4.8.4)\nUsing C preprocessor      : /usr/bin/cpp-4.8 (4.8.4)\nUsing C++ preprocessor    : /usr/bin/cpp-4.8 (4.8.4)\nUsing linker              : /usr/bin/ld.bfd\nUsing Automake            : /usr/bin/automake-1.14 (1.14.1)\nUsing Autoconf            : /usr/bin/autoconf (2.69)\nUsing Make                : /usr/bin/make (3.81)\nUsing CMake               : /usr/bin/cmake (2.8.12.2)\nUsing Python2             : /usr/bin/python2.7 (2.7.6)\nUsing archiver            : /usr/bin/ar\nUsing coverage tool       : /usr/bin/gcov-4.8\nUsing dwp                 : /usr/bin/dwp\nUsing nm                  : /usr/bin/nm\nUsing objcopy             : /usr/bin/objcopy\nUsing objdump             : /usr/bin/objdump\nUsing strip               : /usr/bin/strip\nWrote the environment exec file scripts/compile/env_exec.sh\n$ cd docker && ./build-artifacts.sh ubuntu15.10 0.12.0 .\nBuilding heron with version 0.12.0 for platform ubuntu\nExtracting source\nPlatform Linux\nUsing C compiler          : /usr/bin/gcc-5 (5.2.1)\nUsing C++ compiler        : /usr/bin/g++-5 (5.2.1)\nUsing C preprocessor      : /usr/bin/cpp-5 (5.2.1)\nUsing C++ preprocessor    : /usr/bin/cpp-5 (5.2.1)\nUsing linker              : /usr/bin/ld.bfd\nUsing Automake            : /usr/bin/automake-1.15 (1.15)\nUsing Autoconf            : /usr/bin/autoconf (2.69)\nUsing Make                : /usr/bin/make (4.0)\nUsing CMake               : /usr/bin/cmake (3.2.2)\nUsing Python2             : /usr/bin/python2.7 (2.7.10)\nUsing archiver            : /usr/bin/ar\nUsing coverage tool       : /usr/bin/gcov-5\nUsing dwp                 : /usr/bin/dwp\nUsing nm                  : /usr/bin/nm\nUsing objcopy             : /usr/bin/objcopy\nUsing objdump             : /usr/bin/objdump\nUsing strip               : /usr/bin/strip\nWrote the environment exec file scripts/compile/env_exec.sh\n``\n. @kramasamy I moved semver to 3rdparty. We've got*.pycin.gitignoreso those won't be committed. I added that todocker/.tarignore` as well.\nWhat about the Travis CI version of gcc (see my comment above). Should we downgrade the required version to 4.8.1 or upgrade travis to 4.8.2?\n. Related issue #321 for clean-up in client usage and cli docs.\n. Related comment in #320 \n. +1\nMaven has a plugin to enforce license headers on all source files.\nhttp://code.mycila.com/license-maven-plugin/\nIf such a thing exists for bazel we should use it.\n. Also, there's a lot of text describing commands in this doc that should probably be thinned out and moved into the cli usage:\nhttps://github.com/twitter/heron/blob/master/website/content/docs/operators/heron-cli.md\nAnd these usage examples in that doc don't seem correct, since they have what should be optional overrides shown as required preceding the required topology.\n$ heron-cli activate <activator-overrides> <topology>\n$ heron-cli deactivate <deactivator-overrides> <topology>\n$ heron-cli restart <restarter-overrides> <topology> [shard]\n$ heron-cli kill <killer-overrides> <topology>\nThat page also shows invalid version and classpath commands:\n$ ~/bin/heron-cli3 version\nINFO: Elapsed time: 0.000s.\n$ ~/bin/heron-cli3 --version\nusage: heron-cli3 <command> <options> ...\nheron-cli3: error: too few arguments\n$ ~/bin/heron-cli3 classpath\nusage: heron-cli3 <command> <options> ...\nheron-cli3: error: argument <command> <options>: invalid choice: 'classpath' (choose from 'activate', 'deactivate', 'help', 'kill', 'ps', 'restart', 'submit', 'version')\n. Reopening to also address the heron-cli documentation inconsistencies in my second comment.\n. No need to follow the bazel docs that closely. We can change it to be more neutral.\n\nIn general, contributions that fix bugs or add features (as opposed to stylistic, refactoring, or \"cleanup\" changes) are preferred. Please check on the dev list (heron-dev@googlegroups.com) before investing a lot of time in a patch.\n. Thanks @lewiskan but I think I've gotten most taken care of now. I forgot to link to this ticket. What's remaining is the following:\n- Getting javadocs built and fixing those links (#347)\n- Getting user documentation committed and fixing those links (#346)\n\nThen a few other items like getting linkchecker to run as part of the site release (#349) and integrating java style checking (#348).\nLet me know if any of these are in your wheelhouse. I'm working on the javadocs ticket with @lucperkins. You can see other issues we need to complete at milestone v0.13.3:\nhttps://github.com/twitter/heron/milestones/v0.13.3\n. @kramasamy this is blocked on #346 which is to get the developer docs written and properly linked. Those are the last of the broken links. @nlu90 are you working on #346?\n. Fixed! Thanks @nlu90 for the final push!\n. Thanks @lewiskan looks good.\nFWIW, it's great to put TODOs in the docs so we can search for what we have left to do, we should just enclose them in   in the md to keep them from rendering. Also fyi I started tagging issues with milestone v0.0.1 that we want to have done before going public. If you have any tickets you're aware of please do the same.\n. What's the process for updating the semver version when cutting a release? Where does that info get set/stored?\n. What is the release process and how do we set the semver version?\n. We should discuss building the docs in README.rb or website/README.rb but not both. This latest commit seems to keep the latter file in place.\n. @lucperkins let's move the documentation for how to build the docs into website/README.md and link to that from README.md.\n. @lucperkins let me know when you're ready for a review. \n. @lucperkins please see my comment above re make setup.\n. Thanks @lucperkins!\n. +1\n. @kramasamy these does either still need to be added, or the pages that references them needs to not do so.\n. @nlu90 once the links above have pages and work, please enable the CI link checker with the one liner change discussed in .travis.yml in #709.\n. PR #355 added the javadoc build target. What's left is to integrate that with website/make and then fix the broken links listed above. Reassigning to @lucperkins who's kindly offered to help with that part.\n. @lucperkins is this issue resolved or still pending?\n. Fixed\n. I had to build linkchecker for my mac and it's a python executable. We can try committing it and see how it works for others.\n. Great. I was thinking the we could build the site via the website/makefile for now as described here:\nhttps://github.com/twitter/heron/pull/355/files\nthen we publish website/public to a github subtree like this:\nhttps://gohugo.io/tutorials/github-pages-blog/\nbut open for other suggestions.\n. Yeah, I went through the same exercise with gh-pages. :) We already have a branch with a placeholder:\nhttp://billonahill.github.io/heron/\nhttp://twtiter.github.io/heron/\nIf we could run linkchecker on the files on disk without the hugo server that would be awesome. For now if we get the publish to website/public working with link checking we're good. We can add/update the subtree when we're ready to go live.\nI don't think we need github to do the check, since re-publishing the website will probably be a manual process when we cut a release. Still, it should be as automated as possible.\n. That's great that we can use pip install. No need to support windows/linux for now. Let's name the make target 'site' so it builds and verified the site locally. Publish is different, and we'll probably do that separately. I've committed an outline here:\nhttps://github.com/twitter/heron/blob/master/website/Makefile#L17\nSo we have something like this:\nsite:\n    @make assets\n    @make pages\n    @make javadoc\n    @make linkcheker\nEach has it's own make target above that can be run individually as needed. Also, the pip install of linkchecker would be done in setup.\n. Fixed.\n. +1\n. Will commit since the javadoc part is ready and the doc make part is a comments out wip.\n. +1\n. Handled in PR #678 using a submodule\n. Fixed in #678 \n. We should add build files below contrib and get that building as well. See my comment in #638. Do you have a PR for the fixed errors?\n. PR #638 is a larger contribution that might take a while, so we probably don't want to block on it. It also adds a lot of new code. It would be best for us to get the existing contrib/ code building and clean before taking new submissions there. That would also help reduce the scope of PR #638 to only it's changes without a bunch of unrelated cleanup of pre-existing code.\n. We should open another ticket to track getting this enabled, or re-open this one:\nhttps://github.com/twitter/heron/blob/master/website/javadocs.sh#L5\n. @objmagic sounds good. Please do so and this can be closed out. You can open another ticket for removing the warnings.\n. btw, when new comments come in you could update the existing PR instead of closing and making a new one. It makes it easier to track the history of the discussion and the evolution of the patch.\n. @lewiskan makes sense. I think there's enough logic happening that we should add a linkchecker.sh script to handle this logic, which make calls. That will simplify things, because it can fail and generate output and then return 1.\n. Thanks @prabhuinbarajan, option 2 seems better to me. Bonus if we can edit the top level files like bazel_configure.py and have the be recognized project filed. I think that might make the top level the root though.\nHaving to re-build the idea project upon new folders isn't a big deal, but we should make it as easy as possible. Like you should be able to re-run the script and it wipes out the old idea project files and builds a new one. One command to basically refresh the project.\n. @prabhuinbarajan in PR #404 I'm adding support for checkstyles and copyright headers, which we should also include in the IntelliJ setup.\nFor the styles you can follow the instructions in https://github.com/twitter/heron/pull/404/files#diff-e3ab259ab2bf07b0feecc1e8eda32bec to include tools/java/src/com/twitter/bazel/checkstyle/HeronIDEA.xml in the project and see how that should appear in the generated project files. \nFor the latter we should be able to just include the following files, which I've included below:\n.idea/copyright/\n\u251c\u2500\u2500 Heron.xml\n\u2514\u2500\u2500 profiles_settings.xml\nHeron.xml:\n<component name=\"CopyrightManager\">\n  <copyright>\n    <option name=\"myName\" value=\"Heron\" />\n    <option name=\"notice\" value=\"// Copyright &amp;#36;today.year Twitter. All rights reserved.&#10;//&#10;// Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);&#10;// you may not use this file except in compliance with the License.&#10;// You may obtain a copy of the License at&#10;//&#10;//    http://www.apache.org/licenses/LICENSE-2.0&#10;//&#10;// Unless required by applicable law or agreed to in writing, software&#10;// distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&#10;// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&#10;// See the License for the specific language governing permissions and&#10;// limitations under the License.\" />\n  </copyright>\n</component>\nprofile_settings.xml:\n<component name=\"CopyrightManager\">\n  <settings default=\"Heron\">\n    <module2copyright>\n      <element module=\"All\" copyright=\"Heron\" />\n    </module2copyright>\n  </settings>\n</component>\n. Fixed.\n. Done in PR #368\n. I took the required version from the compile.md documentation. Should we be upgrading automake on the internal hosts, or downgrading the required version? @kramasamy do you know why is 1.11.1 required?\n. Yes, let's lower it.\n. In what OS are you seeing this?\n. Docker's centos reports 2.8.11:\nhttps://github.com/twitter/heron/pull/306#issuecomment-208559907\nI think the correct fix would be to replace the '-patch ' with a dot when building the semver, so it's valid.\n. I took the required version from the compile.md documentation. Should we be upgrading python on the internal hosts, or downgrading the required version? @kramasamy do you know why is 2.7 required?\n. Actually, the python env checking is kinda suspect. I'm not sure why we do it, since nothing from the script execution persists to the running of bazel. For example, I have to do the following on a mac:\nPYTHON2=/usr/bin/python2.7 ./bazel_configure.py\nwhich makes the check script happy. But I then run bazel, which I suspect gets whatever python is in my environment (by luck it happens to also be python 2.7.10:\n$ python --version\nPython 2.7.10\n$ which python2\n[nothing]\n. I like @nlu90's approach more than just relying on the configure_bazel.sh approach, although if we're going to tell users to run configure_bazel.sh to verify the env in advance, we should check for pyhton2.7 if that's what the scripts will require. \n. To just verify that python2.7 is set we can just do the command @nlu90 initially proposed above, but without the '2.7' optional variable at the end:\nenv_map['PYTHON2'] = discover_tool('python2.7', 'Python2', 'PYTHON2')\n. \ud83d\udc4d\n. \ud83d\udc4d\n. @kramasamy where do you see he failed test? Travis CI completed successfully with this change.\n. The initial command I mentioned above is now working for me so closing. We're all good.\n. Fixing the styles in #410. Closing this and will resubmit.\n. Shipped.\n. Shipped\n. @kramasamy if there's good reason why Exception needs to be caught, let's add an IllegalCatch exclude for the specific class to suppressions.xml:\nhttps://github.com/twitter/heron/blob/master/tools/java/src/com/twitter/bazel/checkstyle/suppressions.xml#L10\nWhy the RegexpSingleline check fail? That one we should typically fix.\n. Just realized your plan is to refactor to not catch Exception instead of suppressing the check, which is of course a better approach.\n. +1\n. Thanks @kramasamy. Will close this out since it diagnosed the issue. RB #438 has some changes to expose errors better.\n. Great, thanks @ajorgensen!\n. @lewiskan are you building on an ubuntu host? If not, you need to build with docker:\ncd docker\n./build-artifacts.sh ubuntu15.10 0.12.0 .\n./build-artifacts.sh ubuntu14.04 0.12.0 .\n./build-artifacts.sh centos7 0.12.0 .\n. @prabhuinbarajan if you want to move tools/java/src/com/twitter/bazel/checkstyle/HeronIDEA.xml under scripts/resources/idea and tweek it to make things easier go for it. At that point I suspect tools/java/src/com/twitter/bazel/checkstyle/README.md will no longer be relevant/necessary so please update or delete.\n. Cool. I'm unable to see diffs the links above, but I'll take a look once you put up the PR.\n. Looks good. Let's just also delete tools/java/src/com/twitter/bazel/checkstyle/HeronIDEA.xml and we'll be good to go.\n. @kramasamy I think you want to || true part in in the script, not in the cmd that you shell out to.\n. @kramasamy I typically see that check as this (with the quotes):\nif [ -z \"${HERON_TREE_STATUS}\" ];\n. @kramasamy done. CI looks good.\n. \ud83d\udc4d\n. +1\n. You'll also want to update the index and fix the broken links to developer docs that are referenced in Issue #346.\n. \ud83d\udc4d\n. \ud83d\udc4d\n. :+1: \n. I suggest not using the atypical this.foo = aFoo and instead using the more idiomatic this.foo = foo.\n. :+1: \n. :+1: \n. +1\n. :+1: \n. Current count of heron/ alone is 949:\n$ bazel build heron/... &> build_warnings.txt\n$ grep \"warnings$\" build_warnings.txt | awk '{ sum+=$1} END {print sum}'\n949\nAnd here's the breakdown of warning types:\n$ egrep \"\\.java:[0-9]+: warning: \" build_warnings.txt | egrep -o \"(\\[.*])\" | sort | uniq -c\n   3 [cast]\n 251 [deprecation]\n 354 [rawtypes]\n  91 [serial]\n  37 [static]\n   6 [try]\n 215 [unchecked]\n. PR #485 fixes heron/common and a few other easy ones.\n. PR #489 adds serializableVersionID to all serializable classes.\n. PR #490 disabled warnings from thrift and proto code-gened files.\nNow we're down to these, which are mostly from passing un-typed Map config objects around. Working on that next:\n$ bazel clean && bazel build heron/... &> build_warnings.txt\n$ egrep \"\\.java:[0-9]+: warning: \" build_warnings.txt | egrep -o \"(\\[.*])\" | sort | uniq -c\n 334 [rawtypes]\n 214 [unchecked]\n. PR #511 removes rawtypes warnings for Maps, Lists and Sets.\n$ egrep \"\\.java:[0-9]+: warning: \" build_warnings.txt | egrep -o \"(\\[.*])\" | sort | uniq -c\n 154 [rawtypes]\n 144 [unchecked]\n. PR #539 removes rawtypes warnings from Config, Kryo and Serialization classes. Leaves us with this:\n$ egrep \"\\.java:[0-9]+: warning: \" build_warnings.txt | egrep -o \"(\\[.*])\" | sort | uniq -c\n 119 [unchecked]\n. PR #544 removes all warnings from integration-test except for unchecked.\nPR #548 makes javac fail on warnings other than unchecked.\n. PR #549 removes more unchecked warnings from c.t.bazel.\n. It's been quite a while since I've written cpp, but lgtm.\n:+1: \n. Issue #893 is to add maven publish process to the release process. Issue #821 is to also publish source. PR #890 updates docs to show how to reference heron artifacts when developing.\nClosing this ticket.\n. I'm fairly sure that this change is backwards compatible but not positive. I suspect users would get compiler warnings but not a failure. This is what I saw when working on this PR when I added generics to classes link ReducerMetrics and CombinerMetrics but not the classes that use then.\nStill though, I'll run some tests next week before committing these changes to be sure.\n. I'm not seeing how adding generics could break anything. If generics are added to an interface or class and the consumer doesn't use generics with their objects, that's allowed for backward compatibility. That's why generics could be added to the Collections API in java 6 without breaking consumers. Legacy code would get the \"using raw types\" warning, but not exception.\nSee https://docs.oracle.com/javase/tutorial/java/generics/rawTypes.html \n. Same comment as #515, I think this is safe but I'll run some tests to make sure.\n. See comments in PR #515, this change is safe. ComponentConfigurationDeclarer has always taken a generic parameter, it just hadn't used it itself.\n. +1, assuming these are only used for RPC and not stored on disk ever.\n. Got it. If we're ok with a breaking change at this point then the change seems fine.\n. The travis build.sh mentioned above should also be updated to build integration-test/....\n. :+1: \n. OK, I just simplified things and removed the unused T's, which is a safe change since it's not used.\n. :+1: \n. :+1: \n. @nlu90 any follow-up comments?\n. :+1: \n. I'm going to get python bazel linting integrated into bazel. @vikkyrk will get the eggs we need.\n@vikkyrk if you could commit the eggs somewhere below 3rdparty/python (or 3rdparty/eggs?) and provide a sample command line to test a *.py file I can take it from there on the bazel side.\n. is it ok to release dc names?\n. Oh I see now. I was looking at the deleted image when I saw the names. Good that they're gone now.\n:+1: \n. :+1: \n. Update on this ticket is that I've been chasing down a number of issues getting the build to work on mac, twitter CI and travis. I'm making progress and hopefully close, but around every corner I seem to git a snag. Fingers crossed...\nNo need to review my changes until I ask, since I'm still making updates.\n. OK, I finally got the build/tests to pass on darwin, travis and twitter CI. This patch is ready to ship again.\n. @kramasamy omg, I'm not sure I want that title! :)\n. :+1:  but you've got a travis failure\n. Seems like the integration test is reading /home/travis/.heron/heron-0.13.4-18-gb9c0fd2/client/release.yaml which has the commit message included, which isn't valid yaml because of the ':'.\n@kramasamy or @nlu90 do you know why the test would read release.yml? If that's in fact the intent, how is that file created? Commit messages would need to be quoted in that file.\n. @bmhatfield try wrapping the commit message in escaped quotes here:\nhttps://github.com/twitter/heron/blob/master/scripts/release/status.sh#L62\necho \"HERON_BUILD_COMMIT_MSG \\\"${commit_msg}\\\"\"\n. Great catch, thanks!\n:+1: \n. These don't seem to abide by the java styles. Please build with --experimental_action_listener=tools/java:compile_java to verify.\nAlso we should change heron/... to {heron,contrib}/... in scripts/build/travis/build.sh to get build/test coverage as well as update get_heron_java_paths in scripts/get_all_heron_paths.sh to get IDEA integration.\n. :+1: \n. :+1: \n. @saileshmittal are we blocked on anything else for this? If this changes requires some manual re-basing step we should do it this week.\n. Thanks @saileshmittal!\n. :+1: \n. :+1: \n. I can't see why that's happening. This is where the magic is supposed to happen:\nhttps://github.com/twitter/heron/blob/master/tools/bazel.rc#L37\nDid you try doing bazel clean first?\nIf that's not it could you try running the same target with ---experimental_action_listener=tools/java:compile_java.\n. Please don't commit python eggs. Instead use the reqs attribute to specify them in the pex targets. See #657 for an example.\n. The patch now passes the CI on travis and Twitter CI. It's almost all egg/whl removal with a few other buried changes:\n- tools/rules/pex_rules.bzl\n  The pex_test target wasn't getting the pypi location updated when running in twitter. It also needed /bin and /usr/bin in it's path for chmod and exec, used by the pycrypto build.\n- integration-test/src/python/test_runner/main.py\n  Unrelated, but I made changes to the test runner to show test times and improved log info\n. That's great @Ishiihara. I'm unable to. I think @kramasamy needs to change an admin setting on github so anyone can be assigned.\n. :+1: @saileshmittal if that version works for tracker, but please verify both travis CI and twitter CI integration tests pass.\n. :+1: \n. @lucperkins made one more tweak to Makefile to make localhost links resolve better. Without --server links were kicking me out to github.\n. \ud83d\udc4d \n. #690 was auto-closed by the git history cleanup and never submitted. #709 actually included the fix, but the check is currently disabled until #346 is done and the bad links are fixed.\n. Another approach we could consider would be changing the Java style checker in CI so it could run once over all files in the repo and disabling the compile-triggered checker. We'd still want the latter for developer compiling target that they're working on. We could probably do this with an new option to say not to use the extra_action_file to get the class list, but to instead take a find expression.\nhttps://github.com/twitter/heron/blob/master/tools/java/src/com/twitter/bazel/checkstyle/JavaCheckstyle.java#L51\n. Can we alphabetize these lists?\n. \ud83d\udc4d \n. Good call @lucperkins done.\nAlso added a link to the linkchecker issue:\nhttps://github.com/wummel/linkchecker/pull/657\n. \ud83d\udc4d \n. \ud83d\udc4d \nWhy can we not merge this until the warnings are fixed? We have warnings currently right, and we ignore them?\n. Possibly related to #655\n. Please merge from master and verify again that Travis CI works with these changes. I just merges the linkcheck travis CI work to master.\n. Please update .gitignore too please:\nhttps://github.com/twitter/heron/blob/master/website/.gitignore#L1\n. Overall styles look good to me.\nOnly comment I have is that the bullet text has always seemed kinda huge to me. See the heron-client-install-<version>-darwin.sh on the getting started page (which should be fixed-width code formatted IMO too btw).\n. +1, I agree let's not get into that fixed-width stuff on this PR.\nThose bullet sizes look fine. In https://www.dropbox.com/s/iay0o8c5vqmd266/heron-docs-page.png?dl=0 the size difference between regular text and bullet text is more pronounced.\n. \ud83d\udc4d \n. @kramasamy done. See latest commit files under heron/examples/src/scala.\n. @alanngai here's where the linter gets kicked off:\nhttps://github.com/twitter/heron/blob/master/tools/java/src/com/twitter/bazel/checkstyle/PythonCheckstyle.java#L84\nWhich invokes pylint (https://www.pylint.org/) via:\nhttps://github.com/twitter/heron/blob/master/3rdparty/python/pylint/BUILD#L5\n. https://github.com/twitter/heron/blob/master/website/content/docs/developers/data-model.md#L6 also references spouts.html and bolts.html and should be updated.\nPlease run the following to verify all links are good:\ncd website && make site\nAlso please change the ';' to an \"&&\" to enable link checks here:\nhttps://github.com/twitter/heron/blob/master/.travis.yml#L52\n. Fix it and ship it: just remote 'special' from Tuple description\n\ud83d\udc4d \n. @objmagic this is ready for review. I'm adding all the required jars to the classpath and first building to assure they're there. Also adding backtype and org.apache Storm classes. Once this merges we need to fix new javadoc errors found in backtype and apache and re-enable fail on error.\n. @kramasamy can you include steps to reproduce?\n. Fixes #740.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. Fix it and ship it: the the\n\ud83d\udc4d \n. @lucperkins the contents of public/* are what's published to the site so putting the javadocs in static/* means they don't get published. I'm going to revert that change for now.\n. Fix it and ship it.\n\ud83d\udc4d \n. \ud83d\udc4d \n. Ugh. I tested this last night and it make setup broke my environment so I didn't want to push it until after the launch today, since I'm unable to build this. I should have commented as such.\n. @saileshmittal done. Merging.\n. Failure is caused by bazel build logic in javadocs.sh and ends with this:\n``\n/bin/mkdir -p '/home/travis/.cache/bazel/_bazel_travis/030a6cc9b27b34f2fe0e137b942f59bf/heron/bazel-out/local_linux-fastbuild/genfiles/3rdparty/protobuf/include/google/protobuf/compiler'\n /usr/bin/install -c -m 644  google/protobuf/compiler/code_generator.h google/protobuf/compiler/command_line_interface.h google/protobuf/compiler/importer.h google/protobuf/compiler/parser.h google/protobuf/compiler/plugin.h google/protobuf/compiler/plugin.pb.h '/home/travis/.cache/bazel/_bazel_travis/030a6cc9b27b34f2fe0e137b942f59bf/heron/bazel-out/local_linux-fastbuild/genfiles/3rdparty/protobuf/include/google/protobuf/compiler'\n/bin/mkdir -p '/home/travis/.cache/bazel/_bazel_travis/030a6cc9b27b34f2fe0e137b942f59bf/heron/bazel-out/local_linux-fastbuild/genfiles/3rdparty/protobuf/include/google/protobuf/io'\n /usr/bin/install -c -m 644  google/protobuf/io/coded_stream.h google/protobuf/io/gzip_stream.h google/protobuf/io/printer.h google/protobuf/io/tokenizer.h google/protobuf/io/zero_copy_stream.h google/protobuf/io/zero_copy_stream_impl.h google/protobuf/io/zero_copy_stream_impl_lite.h '/home/travis/.cache/bazel/_bazel_travis/030a6cc9b27b34f2fe0e137b942f59bf/heron/bazel-out/local_linux-fastbuild/genfiles/3rdparty/protobuf/include/google/protobuf/io'\n/bin/mkdir -p '/home/travis/.cache/bazel/_bazel_travis/030a6cc9b27b34f2fe0e137b942f59bf/heron/bazel-out/local_linux-fastbuild/genfiles/3rdparty/protobuf/include/google/protobuf/stubs'\n /usr/bin/install -c -m 644  google/protobuf/stubs/atomicops.h google/protobuf/stubs/atomicops_internals_arm_gcc.h google/protobuf/stubs/atomicops_internals_arm_qnx.h google/protobuf/stubs/atomicops_internals_atomicword_compat.h google/protobuf/stubs/atomicops_internals_macosx.h google/protobuf/stubs/atomicops_internals_mips_gcc.h google/protobuf/stubs/atomicops_internals_pnacl.h google/protobuf/stubs/atomicops_internals_x86_gcc.h google/protobuf/stubs/atomicops_internals_x86_msvc.h google/protobuf/stubs/common.h google/protobuf/stubs/platform_macros.h google/protobuf/stubs/once.h google/protobuf/stubs/template_util.h google/protobuf/stubs/type_traits.h '/home/travis/.cache/bazel/_bazel_travis/030a6cc9b27b34f2fe0e137b942f59bf/heron/bazel-out/local_linux-fastbuild/genfiles/3rdparty/protobuf/include/google/protobuf/stubs'\nmake[3]: Leaving directory/tmp/protobuf.yNys8/protobuf-2.5.0/src'\nmake[2]: Leaving directory /tmp/protobuf.yNys8/protobuf-2.5.0/src'\nmake[1]: Leaving directory/tmp/protobuf.yNys8/protobuf-2.5.0/src'\n/tmp/protobuf.yNys8/protobuf-2.5.0/libtool: line 1099: ldconfig: command not found\n\u001b[32mINFO: \u001b[0mFrom ProtocJava heron/proto/proto_stmgr_java_src.srcjar:\ncommon.proto: File not found.\nstats.proto: File not found.\ntopology.proto: File not found.\nphysical_plan.proto: File not found.\ntuple.proto: File not found.\nstmgr.proto: Import \"common.proto\" was not found or had errors.\nstmgr.proto: Import \"stats.proto\" was not found or had errors.\nstmgr.proto: Import \"topology.proto\" was not found or had errors.\nstmgr.proto: Import \"physical_plan.proto\" was not found or had errors.\nstmgr.proto: Import \"tuple.proto\" was not found or had errors.\nstmgr.proto:16:12: \"heron.proto.system.PhysicalPlan\" is not defined.\nstmgr.proto:24:12: \"heron.proto.system.Instance\" is not defined.\nstmgr.proto:30:12: \"heron.proto.system.Status\" is not defined.\nstmgr.proto:32:12: \"heron.proto.system.PhysicalPlan\" is not defined.\nstmgr.proto:36:12: \"heron.proto.system.PhysicalPlan\" is not defined.\nstmgr.proto:41:12: \"heron.proto.system.HeronTupleSet\" is not defined.\nstmgr.proto:52:12: \"heron.proto.system.Status\" is not defined.\nstmgr.proto:58:12: \"heron.proto.system.HeronTupleSet\" is not defined.\n\u001b[31m\u001b[1mERROR: \u001b[0m/home/travis/build/twitter/heron/heron/proto/BUILD:109:1: error executing shell command: 'set -e\nrm -rf bazel-out/local_linux-fastbuild/genfiles/heron/proto/proto_stmgr_java_src.srcjar.srcs\nmkdir bazel-out/local_linux-fastbuild/genfiles/heron/proto/proto_stmgr_java_src.srcjar.srcs\nbazel...' failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped).\n\u001b[32mINFO: \u001b[0mBuilding complete.\n\u001b[1A\u001b[K\u001b[32mINFO: \u001b[0mElapsed time: 99.296s, Critical Path: 93.25s.\nmake[1]:  [javadocs] Error 1\nmake[1]: Leaving directory `/home/travis/build/twitter/heron/website'\nmake:  [site] Error 2\ntravis_time:end:08439c54:start=1464142104187590686,finish=1464142210583845582,duration=106396254896\n\u001b[0K\n\u001b[31;1mThe command \"(cd website && make site) && scripts/travis/build.sh && scripts/travis/test.sh\" exited with 2.\u001b[0m\nDone. Your build exited with 1.\n```\n. No, it's still disabled:\nhttps://github.com/twitter/heron/blob/master/website/scripts/javadocs.sh#L2\nInvoking bazel the way that script does causes protobuf compile errors. Needs investigation.\n. All links that are not markdown links appear to be external to linkchecker, which includes all javadocs links. These are all broken links somewher in the docs:\nls: public/api/com/twitter/heron/api/TopologyBuilder: No such file or directory\nls: public/api/com/twitter/heron/spi/scheduler/IConfig.html: No such file or directory\nls: public/api/com/twitter/heron/spi/scheduler/IConfigLoader.html: No such file or directory\nls: public/api/com/twitter/heron/spi/scheduler/IRuntimeManager.html: No such file or directory\nls: public/api/com/twitter/heron/spi/scheduler/IUploader.html: No such file or directory\nls: public/api/metrics/com/twitter/heron/metricsmgr/sink/FileSink.html: No such file or directory\nls: public/api/metrics/com/twitter/heron/metricsmgr/sink/GraphiteSink.html: No such file or directory\nls: public/api/metrics/com/twitter/heron/metricsmgr/sink/ScribeSink.html: No such file or directory\nls: public/api/scheduler/com/twitter/heron/scheduler/aurora/AuroraConfigLoader.html: No such file or directory\nls: public/api/scheduler/com/twitter/heron/scheduler/aurora/AuroraLauncher.html: No such file or directory\nls: public/api/scheduler/com/twitter/heron/scheduler/aurora/AuroraTopologyRuntimeManager.html: No such file or directory\nls: public/api/scheduler/com/twitter/heron/scheduler/local/LocalConfigLoader.html: No such file or directory\nls: public/api/scheduler/com/twitter/heron/scheduler/local/LocalLauncher.html: No such file or directory\nls: public/api/scheduler/com/twitter/heron/scheduler/local/LocalScheduler.html: No such file or directory\nls: public/api/scheduler/com/twitter/heron/scheduler/local/LocalTopologyRuntimeManager.html: No such file or directory\nls: public/api/scheduler/com/twitter/heron/scheduler/local/LocalUploader.html: No such file or directory\nls: public/api/scheduler/com/twitter/heron/scheduler/mesos/MesosConfigLoader.html: No such file or directory\nls: public/api/scheduler/com/twitter/heron/scheduler/mesos/MesosLauncher.html: No such file or directory\nls: public/api/scheduler/com/twitter/heron/scheduler/mesos/MesosScheduler.html: No such file or directory\nls: public/api/scheduler/com/twitter/heron/scheduler/mesos/MesosTopologyRuntimeManager.html: No such file or directory\nls: public/api/scheduler/com/twitter/heron/scheduler/util/DefaultConfigLoader.html: No such file or directory\nls: public/api/scheduler/index.html: No such file or directory\nls: public/img/heron-deployment.png: No such file or directory\n. Thanks @jmcomets!\n. \ud83d\udc4d \n. The python BUILD parts that I commented on in #658 have been addressed. I have not reviewed the code itself.\n. @lucperkins looks like your site isn't showing the changed version anymore, but the formats above LGTM.\n. If hugo is flaky couldn't one just view them as files in the browser? This is how I've typically viewed local javadocs.\nopen website/public/api/index.html\n. The issue as I see it re hugo is that while many of us can browse from docs to javadocs without issue (for example you can go from http://localhost:1313/heron/docs/developers/data-model/ to http://localhost:1313/heron/api/com/twitter/heron/api/tuple/Tuple.html and integration is seamless), some people get 404s with the javadocs links and we don't know why. And then the issue mysteriously goes away. Let me know if I'm mis-assessing the issue here, but if that is our issue we should understand why we see that behavior.\nIt would be great to understand why hugo can load javadocs from website/static, but not website/public/api, but if that's not possible, could we instead symlink from the latter to the former instead of copy?\n. @maosongfu ran a build without stylechecks (was that just java, or python, c too?) and the build time dropped by 20-30 minutes so we should pursue optimizations here.\n1. One question we have is why do the java checkstyles seem to happen serially? Maybe this has something to do with the action events/listener model. Can we parallelize this?\n2. Another possible solution: create a checkstyle mode that checks the entire codebase once, before the build proceeds without checkstyle checks. Theory is that a one-time check would be faster than many smaller ones. Challenge with that is getting the equivalent of find . -name \"*.java\" relative to heron root from bazel.\n. For item 2 above, theory confirmed. If would implement something like this, maybe as a sh_binary that relies on the checkstyle java_library, we'd be set.\n``\n$ time java -cp checkstyle-6.18-all.jar com.puppycrawl.tools.checkstyle.Main \\\n  -c tools/java/src/com/twitter/bazel/checkstyle/coding_style.xml \\find {heron,tools,integration-test}/. -name \"*.java\"`\nStarting audit...\nAudit done.\nreal    0m5.016s\nuser    0m13.773s\nsys 0m0.517s\n```\n. As expected this is proving difficult to do in bazel. The sh_binary target needs to be able to get the checkstyle jars and configs as well as all java files in the repo. I can get the jars, but 2 and 3 are proving tricky. Trying some hacks like this:\nBUILD:\nsh_binary(\n    name = \"checkstyle_all_java\",\n    srcs = [\"checkstyle.sh\"],\n    data = ['//3rdparty/java:checkstyle'],\n)\ncheckstyle.sh:\n```\n!/bin/bash\nJARS=find . -name \"*\\.jar\" | tr '\\n' \":\"\nTODO: This is janky and doesn't actually work. Can't get all java files from this location\nJAVA_FILES=find ../{heron,tools,integration-test}/. -name \"*.java\"\necho pwd: pwd\necho classpath: $JARS\necho JAVA_FILES: $JAVA_FILES\nTODO: coding_style.xml doesn't actually resolve here\ntime java -cp $JARS com.puppycrawl.tools.checkstyle.Main \\\n  -c tools/java/src/com/twitter/bazel/checkstyle/coding_style.xml \\\n  $JAVA_FILES\n```\nThen run with:\n$ bazel run :checkstyle_all_java\n. Thanks @nlu90 that's such a good hack and I think that will work for me.\nTo expand on item 2 above, the idea is that for the CI build only, we do two things:\n1. Remove the current checkstyles that happen via the javac listener here:\nhttps://github.com/twitter/heron/blob/master/tools/travis-ci/bazel.rc#L19\n2. Add a one-time call to a target like the one I'm describing above to check the entire repo in one shot, before we proceed to the rest of the build, that no longer includes checkstyle verification.\n. @maosongfu in #818 the java checkstyle is done once for the entire repo at the start, which takes 7 seconds, but the build still take 42 minutes. How did you assert that 20-30 minutes would be saved without checkstyles?\n. Investigation in #818 shows the the checkstyles are not the cause of the delay. #854 shows more detailed timings. The build is in fact spent in the main build:\n```\n===> Task duration summary for scripts/travis/build.sh\n===========================================================\nheron build 0:23:07\nheron test non-flaky    0:03:41\nheron test flaky    0:01:26\nheron build tarpkgs 0:00:25\nheron build binpkgs 0:00:22\n===> Task duration summary for scripts/travis/test.sh\nheron build integration-test    0:00:32\nheron client install    0:00:19\nheron integration-test local    0:05:17\nheron integration-test  0:06:50\n===> Task duration summary for scripts/travis/ci.sh\nmake site   0:02:00\nscripts/travis/build.sh 0:29:01\nscripts/travis/test.sh      0:12:58\n```\n\n. These timings don't include a few more minutes of setup for the host. The build has been taking about 47 or so minutes for a while. Are you saying you'd expect longer?\n. I tried doubling the parallelism in #857 but that didn't help. We must be using all available cores already.\n. Closing this for now since travis ci max time has been extended and the bulk of the time spent seems to be the compile. We can reopen if there are suggestions for how to speed up compile.\n. @huijunw what is the answer?. That's really strange. the _pex.py file can't find another instance of itself. Can you try changing Trace.should_log to return True and see what you get:\nhttps://github.com/twitter/heron/blob/master/3rdparty/pex/pex/tracer.py#L64\nYou'll also want to pass --verbose_failures to bazel build.\n. @zuyu looks like 3rdparty/pex/_pex failed to execute without much messaging as to why. Specfically this:\ncd /private/var/tmp/_bazel_zzhang/3964dd02d192385870c24d7a89344f18/heron && \\\n  exec env - \\\n  bazel-out/local_darwin-fastbuild/bin/3rdparty/pex/_pex --not-zip-safe --entry-point \\\n  heron.ui.src.python.main bazel-out/local_darwin-fastbuild/bin/heron/ui/src/python/heron-ui.pex bazel-out/local_darwin-fastbuild/bin/heron/ui/src/python/heron-ui.pex.manifest\nBest way to debug would be to add some print statements into 3rdparty/pex/_pex to see identify it's failing and improve the messaging.\n. Thanks for the report @efenderbosch.\nRelates to #747. @kramasamy do you have that image?\n. I'm not that familiar with the syntax of either but these changes seem reasonable.\n. This patch is not yet ready to be merged. It needs more testing, since not all files are being checked it seems.\n. This review changes CI to audit all java files at once. The java checkstyle takes 7 seconds, but the build still take 42 minutes.\n. Yes, this change makes not much of a difference. I think the java checkstyles are a red herring. They seem like the cause of slownedd because they log every time a compile happens, but the don't actually take much time. Here's the timing summary with the checkstyles moved out, which is similar to master (see comments #854):\nTask duration summary for scripts/travis/ci.sh\n===================================================\nmake site   0:02:02\nscripts/travis/build.sh 0:25:58\nscripts/travis/test.sh  0:12:48\nI'm included to cancel this pull request.\n. Thanks for clarifying. Really we want to have the release in the docs align with the git tag. So if you checkout a git tag for a release, the docs are properly labeled. With this approach they won't be. Could we instead update the release process to include changing the version in config.yaml? Either automatically via a release script (ideal) or manually via documented process.\n. @kramasamy I don't think we should merge this as is. See my comments above.\n. The release tag in git should already include an updated config.yaml file to have a matching version.\n. Confirmed with @kramasamy that we will update the site version as part of the release process before we create the release tag. To be documented as part of #363. Closing this out.\n. @mlinge pex files include the dependencies (in this case pyyaml) so reinstalling it on your machine shouldn't make a difference. I would recommend looking at the version currently being bundled and checking its compatibility with your version of Ubuntu.\n. @maosongfu Heron currently requires a patched version of thrift 0.5.0 that is only available at maven.twttr.com, which is a public repo. @longdafeng if you're environment can't connect there, you can download the artifact and upload it to your local maven repo.\n. @phynman bazel needs a maven repo to pull artifacts from. You either need access to maven.twttr.com and central.maven.org (see the WORKSPACE file), or you can use your own maven repo if you have one.\n. maven.twttr.com is in fact a public repo and is used by heron only for libthrift (http://maven.twttr.com/org/apache/thrift/libthrift/0.5.0-1/). See the WORKSPACE file. It does seem to be down though. I'm investigating.\n. maven.twttr.com seems to be available again(https://www.webpagetest.org/result/170412_QX_15Z9/1/details/#waterfall_view_step1). I'm guessing there was an intermittent availability issue.\n. FYI, dns appears to be incorrect in China:\nhttps://www.whatsmydns.net/#A/maven.twttr.com\n. @Jonathan-Wei best way to debug this would be to follow the code from this line in bazel_configure.py:\nenv_map['AUTOMAKE'] = discover_tool('automake', 'Automake', 'AUTOMAKE', '1.9.6')\nIt calls discover_tool, which fails to find anything in discover_program. Add some print debugging in discover_program and real_program_path to uncover the issue: \nhttps://github.com/twitter/heron/blob/master/bazel_configure.py#L212\n. You're comparing two different things. bazel_configure.py is checking /opt/local/bin/cmake, but you're checking /usr/local/bin/automake.\n. @lucperkins looks like this merge has conflicts. Could you pull from upstream and see how things look?\n. @lucperkins if you're getting a merge conflict, I will too and we won't be able to commit. (I confirmed this)\nI suspect the conflicts are because your remote has build date tags inserted into pages that conflict with the upstream remote. To resolve you can reset with the upstream remote and rebuild and you won't have conflicts.\nAssuming origin is your fork and upstream is twitters:\n```\n$ cd workspace/heron\n$ git pull upstream/master\n$ cd website/public\n$ git pull upstream/gh-pages\n$ git reset upstream/gh-pages --hard\npush this to your gh-pages fork and verify your remote is in sync with the upstream\n$ git status # should show everything clean\n$ cd .. && make site && cd -\ncommit to your fork again and you should have a clean diff for a PR. You might need to force push:\n$ git push -f origin gh-pages\n```\nNow you can make a clean PR.\n. That's great, thanks for the patch. I'll review this shortly. In the meanwhile would you please accept the CLA yet from here:\nhttp://twitter.github.io/heron/docs/contributors/community/\n. Looks good @insomniacslk, thanks for the patch!\nWould you please accept the CLA linked to from http://twitter.github.io/heron/docs/contributors/community/.\n. Great, this is so much cleaner. Thanks @insomniacslk.\n. Great find @zhangzhonglai. Would you mind submitting a patch?\n. Great, glad it's working @Jonathan-Wei. Thanks for the patch @zhangzhonglai.\n. This can be found in the logs now:\n```\n===> Task duration summary for scripts/travis/build.sh\n===========================================================\nheron build 0:23:07\nheron test non-flaky    0:03:41\nheron test flaky    0:01:26\nheron build tarpkgs 0:00:25\nheron build binpkgs 0:00:22\n===> Task duration summary for scripts/travis/test.sh\nheron build integration-test    0:00:32\nheron client install    0:00:19\nheron integration-test local    0:05:17\nheron integration-test  0:06:50\n===> Task duration summary for scripts/travis/ci.sh\nmake site   0:02:00\nscripts/travis/build.sh 0:29:01\nscripts/travis/test.sh      0:12:58\n```\n. This made no difference. Closing.\n. Thanks @zhangzhonglai for the patch!\n. Looks good. Would you please accept the CLA as described here:\nhttp://twitter.github.io/heron/docs/contributors/community/\n. Thanks @castomer!\n. Please update bazel version for the website here too:\nhttps://github.com/twitter/heron/blob/master/website/config.yaml#L24\nAnd in the md files we should reference it via:\n{{% bazelVersion %}}\n. Just left one last small comment. Fix it and ship it.\n\ud83d\udc4d \n. Have you searched for the problem? Looks like this might be related:\nhttp://stackoverflow.com/questions/28967785/attribute-error-installing-with-pip/30310206#30310206\n. Looks like we already have a troubleshooting guide that much of this content could be included in:\nhttps://github.com/twitter/heron/blob/master/website/content/docs/developers/troubleshooting.md#L2\n. @maosongfu and I just discussed and he agrees we should combine all troubleshooting on the troubleshooting page and we link to that from here if people run into issues doing the setup. Let's move this over and then ship. Better to get the organization right from the start and push it tomorrow if needed.\n. \ud83d\udc4d \n. One last comment. Fix it and ship it.\n\ud83d\udc4d \n. +1 the above exception should never happen.\n@ajorgensen can you describe a simple way to reproduce? Does this happen when a given implementation class is not specified or not found in the yaml?\n. @maosongfu we'll also need to do checks with reflection to make sure classes are present.\n. @kartik894 I responded to your issue #888. Let's keep these two issues separate pls.\n. @chatterjeesubarna I've answered your question on the mailing list. Please don't double post questions on both the mailing list and git issues. Also, troubleshooting questions are best handled on the mailing list. Git issues should just be for bugs or feature requests.. @bjmota would you please ask troubleshooting questions on the mailing list? Github issues should be used for filing bugs and feature requests.. @maosongfu is this only for processes on the scheduler, or those on the client? If the latter can you show what the console logs look like on the heron client with this change? Seems like this could potentially produce a lot of output. \n. \ud83d\udc4d \n. Looks like you're swapped the ordering of [options] and cluster/[role]/[environ]:\n```\n$ heron help submit\nusage: heron submit [options] cluster/[role]/[environ] topology-file-name topology-class-name [topology-args]\nRequired arguments:\n  cluster/[role]/[env]  Cluster, role, and environ to run topology\n  topology-file-name    Topology jar/tar/zip file\n  topology-class-name   Topology class name\nOptional arguments:\n  --config-path (a string; path to cluster config; default: \"/Users/billg/.heron/conf/\")\n  --config-property (a string; a config property; default: [])\n  --deploy-deactivated (a boolean; default: \"false\")\n  --verbose (a boolean; default: \"false\")\n```\nIf that's not the cause, this is the relevant code to troubleshoot FYI:\nhttps://github.com/twitter/heron/blob/master/heron/cli/src/python/main.py#L155\n. What do you see when you add --verbose or debugging statements to the cli?\n. Try adding some debug lines to the client code to see if this is a bug or an area where we need to improve output logging.\n. If this does the trick, adding a reference to client.yaml in the cli output would be a great patch to help clarify this for future users.\n. @kartik894, @nlu90 see #894 for enhanced error logging patch.\n. \ud83d\udc4d \nPlease verify that <version>{{% heronVersion %}}</version> renders properly when running make serve if you haven't already.\n. @lewiskan I suggest we document the currently manual process first, as is. Then we can work on automating the various parts. That way we're clear and consistent with our manual releases until we have automation.\n. Keeping this open unit the rest of the release process is documented.\n. @maosongfu done. I'm echoing back cluster_role_env, from which we can infer parts.\n. Looks good, thanks!\n. \ud83d\udc4d \n. @taishi8117 you typically will want to keep the same branch and PR and not close and reopen, since it makes tracking the comments, edit history, issue deps, etc. difficult. You should be able to resolve conflicts and update the branch and have it work out with the existing PR.\n. \ud83d\udc4d \n. Can we dynamically override the default ram in the CI test via an environment variable or something like that?\n. Closing because this is a question, not an issue. This is a good discussion for the mailing list please.\n. Thanks @cranst0n, looks good! Would you please accept the CLA as described here:\nhttp://twitter.github.io/heron/docs/contributors/community/\n. Thanks!\n. This is a bazel issue. All timestamps get reset upon packaging. Need to investigate more to see if we can change this.\nhttps://github.com/bazelbuild/bazel/issues/1299\nhttps://groups.google.com/forum/#!topic/bazel-discuss/cprd0UhRMAs\nhttps://github.com/bazelbuild/bazel/search?utf8=%E2%9C%93&q=198001010000\n. These are warnings that can be ignored. Untarring with --warning=no-timestamp should suppress the warning. See https://www.gnu.org/software/tar/manual/html_node/warnings.html\n. Ideally we'd fix the timestamps but that looks tricky due to bazel magic. Instead we suppressed the warnings for now. Will close this, but we can revisit to see if it's still an issue after upgrading to the latest bazel.\n. \ud83d\udc4d \n. Closing, not a bug. Please use the mailing list for discussions.\n. That's a bug that we don't pass --config=[os] to bazel on that line. I suggest we fix by defaulting to --config=darwin (the most common case), but allowing that to be overridden with a flag.\n. Looks good. Once last comment left, fix it and ship it.\n\ud83d\udc4d \n. Thanks @lewiskan!\n. Should timestamp actually be a number?\n. Why would we not want timestamp to be numeric?\n. Seems like it should always be a long to me.\n. \ud83d\udc4d \n. Seems reasonable. Would you please provide a patch?\n. Thanks @lucperkins.\nAs a follow-up we should probably clean up the layout of this page (e.g., bullets and indents):\nhttp://lucperkins.github.io/heron/docs/getting-started-troubleshooting/\n. Thanks!\n\ud83d\udc4d \n. Not an issue, closing. Please discuss in the heron-users forum.\nhttps://groups.google.com/forum/#!forum/heron-users\n. This approach seems reasonable to me. I think it would be good to support multiple packing algorithms, each which could be run in a dry-run mode for inspection by topology developers. We should expand the output to show a standard report of the plan and what the resource usage would look like if deployed. Users could manually inspect what the physical plan would look like with any of the algorithms before deploying. Ultimately the best algorithm could even be dynamically detected by the system at deploy time.\n. Is this because newer bazel not allow _? The rest of the targets in the repo seem to use a lowercased-dash-delimited convention instead of CamelCase. Could we also follow that here? \n. \ud83d\udc4d \n(not so sure how I feel about bazel's design decision there :) )\n. @kramasamy I added comments in the code explaining why I'm using --help.\n. Investigating CI failure...\n. Looks good!\nWould you please accept the CLA as discussed here:\nhttp://twitter.github.io/heron/docs/contributors/community/\n. @caofangkun please don't close and re-open new PRs for small revisions, since it makes it harder to track things like discussions and deps across the multiple PRs with the same scope.\n. No problem! I didn't realize the repo was messed up, so opening a new PR makes sense now. Some folks have a habit of closing and re-opening for minor changes, so I wanted to make sure that's not what was happening.\n. Looks good, I just left all the same comments in #869 before seeing this so maybe we just get them all in the same PR.\n. Seems better to exclude checking public links if it meant we could not specify a depth for internal links. Not sure if that's possible with wget. \n. \ud83d\udc4d \nnice!\n. I've also been thinking about this as a way to simplfy the packing algorithms. I'd like to reduce the number of Maps and Collections passed around in that code and replace with a builder, specifically the Map<Integer, List<InstanceId>> resource that is used, as well as some of our shared PackingUtil static helpers. I've looked at the code a bit and I think an impl like this would work to encapsulate building/modifying packing plans:\n```\npublic class PackingPlanBuilder {\n  PackingPlanBuidler()\n  PackingPlanBuidler(PackingPlan existingPacking)\n// topology settings\n  void setTopologyId(String topologyId);\n// set resource settings\n  void setDefaultInstanceResource(Resource resource);\n  void setRequestedComponentRam(String componentName, long ram);\n  void setRequestedContainerRam(long ram);\n  void setRequestedContainerRamPadding(long ram);\n// add or remove instances\n  void addInstance(Integer containerId, InstanceId instance);\n  void removeInstance(Integer containerId, InstanceId instance);\n  void removeAnyInstance(String componentName);\n// build container plan sets by summing up instance resources\n  // mostly impl from PackingUtils.buildContainerPlans\n  PackingPlan build() throws InvalidPackingExcepton;\n}\n```\ncc/ @avflor @ashvina who have also been thinking about this.\n. @ashvina the current workflow for both packing algorithms is that based on number of containers, the algos basically call addInstance to bind InstanceIds to containers. Once that's complete the build process converts InstanceIds into InstancePlans to make the PackingPlan. This stage requires the notion of instance and container resources and is currently handled in PackingUtils.buildContainerPlans. \nThe only real difference between InstanceIds and InstancePlans is that the latter has resources, so it's not clear to me how we can have a packing plan builder that takes InstanceIds without taking some notion of resources. How do you suggest we change the API?\n. I spoke with @avflor and @ashvina to clarify that the placement/removal logic will still be up to the algo, the builder just handles the add/remove calls as directed. Also the resource settings included are generic across packing algo, like default instance resources and resource per component. These are also set by the packing algo based on user settings or configs. They're required to generate a packing plan with resources from the InstanceIds.\nOne thing we did realize is that RoundRobinPacking handles plan creation differently than the others (as a result it can over-allocate) so that class might not be able to upgrade to use the packing builder. We might just deprecate that one if we're able to replace it with the ResourceCompliantRRPacking impl.\n. I would suggest that the user could configure a different packing algorithm\nto see it's output, or pass a non-default packing class as another optional\narg.\nOn Thu, Aug 4, 2016 at 12:03 PM, Karthik Ramasamy notifications@github.com\nwrote:\n\nWe might need to extend this a bit - for example, with multiple packing\nalgorithms, the resource consumption might differ. it might be good to list\nthe resources required for each packing algorithm as well?\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/948#issuecomment-237651288, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AABczttzSzfATrqfT6_Mrad_F-585nUgks5qcjd5gaJpZM4I4sE0\n.\n. Thanks @caofangkun looks good! Merging.\n. @nlu90 when verifying we need to assert that we can build and deploy a topology in source, which still requires thrift 0.5.0. I suspect there could be difficulties there.\n. Ping.\n. @objmagic could you check why linkchecker didn't catch this?\n. Ping.\n. \ud83d\udc4d \n. \ud83d\udc4d \n\n@nlu90 or @kramasamy why do we have a separate BUILD process for centos5 than those in scripts/packages?\n. I'm not sure what that is (maybe --verbose_failures would help) but ideally we could have a single build and use bazel to abstract out the OS specific stuff. That would be cleaner than copied & edited BUILD files.\n. Thanks @ajorgensen, looks good!\n. \ud83d\udc4d \n. \ud83d\udc4d \n. The flapping thing is aurora saying your process is failing.\nThe line before the exception at heron-executor.py:359 outputs the process that's trying to be run. Check for that in the logs.\n. \ud83d\udc4d \n. This is great @benley, we'd love to have the bazel/pex stuff pulled out of Heron and put closer to the bazel repo. If you want to submit a PR to remove all of this pex code and instead reference it from a remote source somehow, we'd be glad to accept that.\nThe only benefit we get from having it in the repo currently is that when things go wrong with the pex build, there's sometimes little helpful logging to pinpoint the issue. In that case we manually change tracer.py's should_log to return True. This is an ugly hack and it would be much better if bazels verbose flags caused verbosity to be correctly set within pex. This is something we've been meaning to fix for a while. \n. Just clicking around quickly I see a number of pages that still refer to 0.14.0:\nhttp://lucperkins.github.io/heron/docs/getting-started/\nhttp://lucperkins.github.io/heron/docs/developers/topologies/\n. @kramasamy we should also update the release process to include making this change before tagging the release.\n. Looks good, will merge. Totally unrelated, but I think it's too hard for users to find the mailing list currently. I think it's only hidden in the nav bar. As a result we get tons of questions as github issues and very little mailing list traffic. We should more prominently steer people to the list for help somewhere.\n. \ud83d\udc4d \n. Druid does a good job with this: http://druid.io/community/\nWe could add some similar things to the top of our community page and maybe also clarify how questions and troubleshooting should be over email and only bugs via github issues.\n. Looks good, thanks for doing this. Would you also please verify this branch in twitter's internal CI.\n. \ud83d\udc4d \n. One more think to check is to make sure the docker builds work. Once that's verified I think it's fine to merge.\n. \ud83d\udc4d \n. I thought about that, but heron explorer shows a cleansed view of topology state for end users. This shows a raw dump of what proto objects exist in the state store, so it's more useful for developers inspecting state.\nI have some checkstyle errors to address too.\n. I prefer making methods as closed-scoped as possible and opening scope when needed. So once we write a test we can change it to protected and annotate it @VisibleForTesting.\n. \ud83d\udc4d \n. I don't know if we need to move the packing plan classes into specific schedulers, but we should have a concept of which packing plans are and are not supported (or ideal) for a given scheduler.\n. @ashvina looks like the packing plan unit test is failing in CI.\n. Just left a small naming nit, but changes LGTM if @nlu90 and @maosongfu don't have add'l comments.\n. \ud83d\udc4d for the checkstyle change\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. They run if I explicitly add --experimental_action_listener=tools/java:compile_java but not otherwise\n. I've found that it still doesn't always happen.. Heron has multiple Config classes so if we're going to rename them we should consider the names of all of them:\ncom.twitter.heron.{api,spi.common,common.config}.Config\n. Also I don't think it makes sense for them to all be renamed to HeronConfig. Either they stay as a gerenric Config class with package context, or they get renamed so each implies their scope.\n. @kramasamy (or others), what's the reason for both spi Config and common Config? Can these be consolidated? Looks at the javadocs for the two classes it's not clear the distinction between the two.\n. We already have IntelliJ java styles here:\nhttps://github.com/twitter/heron/blob/master/scripts/resources/idea/codeStyleSettings.xml\nThey're used when creating a heron project here:\nhttps://github.com/twitter/heron/blob/master/scripts/setup-intellij.sh\n. \ud83d\udc4d \n. @kramasamy this PR isn't critical for 0.14.3 so no need for you to block on it.\n. \ud83d\udc4d for the change but please check on the CI failure before shipping.\n. Could you add unit tests for the new classes please.\n. Could you add some unit tests for this class please.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d once CI passes.\n. Thanks @yanxz looks good! Would you please accept the CLA:\nhttp://twitter.github.io/heron/docs/contributors/community#submitting-a-patch\n. @caofangkun could you add comments to the top of this file, or a brief readme that explains how it should be used.\n. @kramasamy I suggest we limit this change to what's needed to manage the python/java processes for now and follow up with the other higher-level info after clarifying our use cases around that more.\n. I was advocating not blocking/coupling this PR with the feature you're talking about. It's best to separate features into different PRs/issue for better tracking.\nWe can put other things in this release if we want to, but let's deal with it separately.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. SchedulerMain.runScheduler() also has overlapping code with SubmitterMain.submitTopology (for object initialization).\nWhat dead code are you referring to in heron-executor.py?\n. Yeah, it looks like some version of this one might be the one we need to checkstyles:\nhttp://checkstyle.sourceforge.net/apidocs/com/puppycrawl/tools/checkstyle/checks/whitespace/EmptyLineSeparatorCheck.html\nhttps://github.com/twitter/heron/blob/master/tools/java/src/com/twitter/bazel/checkstyle/coding_style.xml\nI'm not sure which one is needed for IntelliJ, but we'd set it here:\nhttps://github.com/twitter/heron/blob/master/scripts/resources/idea/codeStyleSettings.xml\n. Thanks @avflor for being patient with my comments. :) The changes made have made things much easier to grok.\nIf the 4s and 3s don't align with bolt and spout counts, maybe a comment about what they are would do the trick. Without looking at the details of the algo it's hard to decipher why some of those numbers are being used in the test assertions.\n. Thanks @avflor! LGTM. :)\n@ajorgensen @maosongfu any further comments?\n. \ud83d\udc4d for code changes, but CI is failing.\n. \ud83d\udc4d \n. LGTM\n@avflor and @ashvina you were also doing some similar refactors lately. Would you please review.\n. Changes LGTM, lets just fix the failing CI.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @objmagic your suggestion is the right one IMO, but since it would break backward compatibility we have to compromise a bit with the proposed solution.\n\ud83d\udc4d \n. @kramasamy I was also wondering about that. Internally we don't have any production jobs on OSS heron, by the looks of the task tracker. If we can break backward compatability we could do the right thing. @taishi8117 do you want to work on a patch to refactor our proto classes to use the pattern that @objmagic showed above?\n. For java we found the \"N files found for checkstyle\" output was misleading when inferring where build time was spent. It turned out that the checkstyle was in fact very fast compared to the compile, but since the checkstyle output log info it appeared (as a red herring) to be the time sink. Before adding parallelism I'd want to see some timing numbers showing how much savings we'd achieve. My suspicion is not much.\n(Personally, I'd rather see the file that produces the style errors upon a failure, which is currently difficult to determine)\n. If the storm code is mostly copied could you add a README.md file below heron/storm/src that explains what storm version we're using and how to diff against the storm repo to pull in future changes?\n. +1 for excluding style checks for /heron/storm and syncing source code without style changes. This will make it easier to diff/patch withe the storm repo.\n. For #2 if we want just logging we should do what you suggest and break it\nout into its own dep. I suspect as the project evolves more bazel targets\nwill get more granular to the package level.\nOn Thursday, August 4, 2016, Taishi Nojima notifications@github.com wrote:\n\nHeron Executor currently uses its own print function do_print()\nhttps://github.com/twitter/heron/blob/master/heron/executor/src/python/heron_executor.py#L41,\ninstead of heron's common logging module (heron/common/src/python/\nutils/log.py). In order to make logging uniform, it's better for the\nexecutor to use this as well, but there are several potential issues:\n1. Since stdout/stderr of the executor is already redirected to a file\n   (heron-executor.stdout) by the scheduler, any logs from the executor should\n   be output to stdout. That case, we can't use the existing StreamHandler\n   https://github.com/twitter/heron/blob/master/heron/common/src/python/utils/log.py#L56,\n   because ugly escape characters from colorlog will also be in the log file.\n   A possible solution is to create another StreamHandler that doesn't use\n   colorlog.\n2. Heron Executor does not use any other common modules (under\n   heron/common/src/python), meaning that importing the whole heron's\n   python common library just for log.py is quite cumbersome (since it\n   includes unnecessary dependencies, like tornado, as well). One possible\n   solution is to create another BUILD rule, such as one just for logging.\nWhat would be the best solution?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/1216, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AABczo_ipvBPZiJS-34Zq1RN-DV671yTks5qcZEfgaJpZM4JcZ_E\n.\n. Resolving, as all changes have been incorporated into master. Woot!\n. @ashvina good observation. I'd love to remove then and that was my initial intent, but the executor needs that info and is written in python. Serializing it means we don't need to change the executor code, so I kept in there for now. Without it we'd need to reimplement that logic in python to reconstruct. I don't love having the logic implemented in two places, but I agree we might want to do that instead before we ship scaling, but in a later patch.\n. Created #1222 \n. Ping.\n. I've removed the instance dist and component ram dist from the serialize proto. @ashvina  and @kramasamy please review. \n. This refactor has been made on the billg/scaling_proto branch (see this commit), so the instance dist has been removed in #1219.\n. Reviewing this request by examining diffs is pretty hard. Could you publish the site to your fork and include a link so @taishi8117 and @lucperkins can review the changes that they expect to see on the site.\n. IIRC we've seen that before when some includes had http:// in them. I don't recall the exact specifics though.\n. \ud83d\udc4d \n. See this line in your logs:\n\n[2016-08-17 11:44:35 -0700] com.twitter.heron.scheduler.SubmitterMain SEVERE: Topology already exists\nKill the topology in aurora and then try to submit.\n. Also, please use the email forums for troubleshooting issues instead of creating github issues.\n. Any other comments on this ticket or can I get a ship it?\n. Will this pass checkstyles?\n. \ud83d\udc4d \n(yes we should add a copyright check for py)\n. Thanks @objmagic for the restart. This failed because I needed to merge in #1282, which I just did. The current run should pass. \n. @avflor put this through a PR on billg/scaling_proto (see https://github.com/billonahill/heron/pull/4) and we did a round of reviews there. To provide a little more context, the key change proposed here (besides the new IRepacking interface) is to the IPacking interface:\n-  void initialize(Config config, Config runtime);\n+  void initialize(Config config, TopologyAPI.Topology topology);\nThe goal is to make the initialize method for IPacking the same as for IRepacking to simplify classes that implement both. By standardizing on Config config, TopologyAPI.Topology topology we can do that but it's of course a breaking change for IPacking impls in the wild. I think that's ok (assuming we're ok with this signature) since we're still evolving the APIs and the refactor to IPacking impls would be straight forward.\nSee discussion in https://github.com/billonahill/heron/pull/4#discussion_r74977716 for more context.\n. Would anyone else like to review before merging? Let me know and I'll hold off.\n/cc @kramasamy @maosongfu @ashvina @nlu90 @objmagic \n. Addresses part of #1292.\n. I think the signature should match what the implementations typically (or contractually) need. In the case of IPacking, every implementation just fishes the Topology from the Runtime and then drops it:\nthis.topology = com.twitter.heron.spi.utils.Runtime.topology(runtime);\nso I think it makes sense to be more explicit with the init signature and pass the Topology itself. Unless we foresee a reason why one might need runtime for something else.\nIIRC during repacking we don't have the runtime available to us. @avflor correct me if I'm wrong. The IScheduler and ILauncher always has it at submission time though.\nThat said, I'm open for other suggestions.\n. +1 @avflor\nI think it's ok for our interfaces to still be evolving at this stage. (I'd love to find time for #953 for this purpose.)\n. If we're not concerned about breaking backward compatibility of external clients (which we should not be at this stage) than we should be able to evolve our interfaces with confidence to get them right. The reflection risk is the only one that we can't catch at compile/test time so we'll need to look for where that might happen, or better yet we should really never, ever be using reflection as it's a tool of the devil (frameworks like Guice achieve the same with compile-time checks).\nI don't fully agree with the Spring DI comparison. That pattern promotes passing the objects depended on into the class that needs them, which in this case is the Topology. That pattern doesn't promote passing what's effectively a giant HashMap of un-typed Objects (i.e. runtime) deep into the code stack and having objects fish out what's important to them.\n. See #1304 for an approach for specifying interface audience and stability.\n. @kramasamy I agree with the intent. That said, none of the existing IPacking impls need it and IRepacking doesn't have access to it so I'm ok with refactoring it out for now.\n. Thanks @avflor!\n. Why 4 backticks and not 3 as seen elsewhere in the md docs?\n. So the issue was the wrong number of indent spaces?\n\ud83d\udc4d \n. Ping. Any comments on this one?\n. A prototype branch for this work can be seen in #1218.\n. +1 this has bothered me as well.\nSemi-related grievance: Another area for potential cleanup is in the instance plan id, which is colon delimited string containing things like global_task_id and component_index. It gets parsed out on the executor to create the instance id. It's fine to use these indexes as part of a descriptive string id, but they should never be expected and parsed out. They should instead become first class entities of the component plan.\n. Fixed split command dup logging.\n@objmagic I see the following areas for improvement:\n- There's too much sleeping and retrying. We should be able to determine when we can proceed by watching something or getting an event, but sleep/retry === brittle/slow.\n- When a test fails there's little to no info re why it failed. For example look at this failed CI where the output file wasn't found. Why did it not appear? What process logs might show us? There's no way to know and if you can't reproduce you're stuck. You can't investigate process logs like you would on a running system.\n- The tests are too black box. They give input and check output but there's no white-box testing (e.g., assert state of processes or state manager along the way). For scaling int tests we'd need that to verify the state of the system with more granularity along the way.\n- The test pattern should be more easily extensible. Currently it's submit/kill process/verify, but you could imagine more variations, like scaling.\n. We use Guice extensively at Twitter too:\nhttps://github.com/google/guice/wiki/Motivation\n. I'd be up for taking this on after the scaling work.\n. \ud83d\udc4d \n. \ud83d\udc4d for merging once CI completes. Thanks @ashvina!\n. Yeah, you need to manually create state/local/packingplans in ZK.\n. Yeah, initTree() is what we want. I can make that change.\nI was struggling to find a way to lazily detect that the node doesn't exist and create it. When Curator throw when the node is not found is just throws Exception (ugg) so a poor indicator of the cause of failure.\n. @kramasamy yes we should be able to do things like check at compile time if a class not in com.twitter.heron is implementing a Private or Unstable interface and emit a warning.\nI don't think we detect at runtime (or even compile time) if a class is calling a class that they shouldn't.\n. Update: @kramasamy it seems annotations can't be inherited which means that if a class extends a superclass that has a given annotation we can detect that at compile time and warn, but if a class implements an interface that has a given annotation we can not detect that.\n. @kramasamy I just added an annotations processor which will emit warnings at compile time if a non-heron class extends a class marked as Private, LimitedPrivate or Unsable. \n$ javac -cp \"bazel-bin/heron/spi/src/java/libheron-spi.jar:bazel-bin/heron/api/src/java/libclassification.jar:bazel-bin/heron/proto/libproto_topology_java.jar\" billg/*.java com/twitter/heron/*.java\nbillg/BillPacking.java:17: warning: billg.BillPacking extends from a class annotated with com.twitter.heron.classification.InterfaceStability.Unstable\npublic class BillPacking extends com.twitter.heron.NullPacking {\n       ^\nbillg/BillPacking.java:17: warning: billg.BillPacking extends from a class annotated with com.twitter.heron.classification.InterfaceAudience.LimitedPrivate\npublic class BillPacking extends com.twitter.heron.NullPacking {\n       ^\n2 warnings\n. @kramasamy this is merged.\n. @ashvina @maosongfu @kramasamy this change will shield us from bugs like #1303 in the future.\n. Favor immutability (#1323) is another best practice.\n. +1 thanks @waitingkuo!\nHave you signed the CLA yet?\nhttp://twitter.github.io/heron/docs/contributors/community/\n. \ud83d\udc4d once CI finishes\n. Committing a change to go with option 2 actually (IScalableScheduler extends IScheduler), since I don't think we'd ever have a non-scheduler class implement add/remove functionality, but I'm already second guessing that choice over option 1 (IScalable). Opinions welcome.\nAlso moved the interface to c.t.h.spi.scheduler and removed existingContainers.\n. @maosongfu and @ashvina all comments addresses, let me know if this looks ok to you.\n. I'm going to merge this. If there are add'l comments we can address in a follow-up.\n. \ud83d\udc4d !!!\n. \ud83d\udc4d \n. @ashvina and @avflor all comments have been addressed. Let me know how things look.\n. @ashvina and @avflor all comments have been addressed. Let me know if you'd like more time to review.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. Ugh, yeah that's the problem. The statemanager config loading is not in the shape I'd like it to be. The issue is that the python state managers take configs in a different format than the java state managers and we have configs for the latter so we need to read them and convert to the former.\nWe can't just read the configs directly, because the java impl has wildcard parsing logic not present in the python impl. For example, for the local statemanager the config is this:\n$ grep heron.statemgr.root.path ~/.heron/conf/local/statemgr.yaml\nheron.statemgr.root.path:                       ${HOME}/.herondata/repository/state/${CLUSTER}\nwhere ${CLUSTER} and ${HOME} are not handled. What we need is a python implementation of https://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/common/Misc.java#L142\nAnyone know if there's anything similar in python?\n. I'm going to take a stab at this. I suspect there are wildcard heron variables that we don't have access to in python like we do in java, but let me see how much we can support.\n. @ashvina no ideally the user should have to. There are different ways to determine the home dir in python, we're using \nhome_dir = os.getenv('HOME')\nSee https://github.com/twitter/heron/blob/master/heron/statemgrs/src/python/configloader.py#L29\nWe could change that approach though. Could you see if this resolves correctly in your environment:\nfrom os.path import expanduser\nhome = expanduser(\"~\")\n. The ${HOME} wildcard token is supported and used extensively by Heron in the java classes, as most files under https://github.com/twitter/heron/blob/master/heron/config/src/yaml/conf show. I don't think we should deviate from that approach just for the python statemanger use case. Yarn must be able to load this path currently for java so we should be able to do the same for python.\n. \ud83d\udc4d \nThis change is included in #1334 fyi.\n. Let's cancel this in favor of #1334 which should merge soon.\n. This was addressed in #1334 so closing.\n. What version of Heron are you running? YarnLauncher was added in 0.14.1.\n. Thanks @ashvina. @avflor let me know if you'd like time to review.\n. java.lang.Long.hashCode was removed in Java 8. \n. Hmm, I still see it in java 8 actually.\n. Yes, it is. It's Long.hashCode(long value) which was introduced in 1.8. We should change it to longValue.hashCode() which is 1.7 compliant.\n. ### Approach 1 - local lock with current packing plan comparison\n1. Client fetches current packing plan from state manager and sends it and proposed changes to scheduler when invoking update.\n2. UpdateTopologyManager receives request and takes out a local lock. If lock can't be obtained, returns a concurrent update exception response.\n3. With the local lock, UpdateTopologyManager compares the current packing plan with what's in the state manager. This is to confirm that the system state hasn't changed since the request was initiated by another agent. If the packing plans don't match, returns a concurrent update exception response.\n4. UpdateTopologyManager updates the topology info in the state manager to invoke the scaling change.\n5. UpdateTopologyManager releases the local lock and returns success.\nApproach 2 - local lock with update request versioning\nThis approach is similar to approach 1, except an atomically incremented request id is used instead of the current packing plan comparison.\n1. Client atomically increments an updateRequestId counter in state manager.\n2. Client submits update request including the updateRequestId.\n3. UpdateTopologyManager receives request and takes out a local lock. If lock can't be obtained, returns a concurrent update exception response.\n4. With the local lock, UpdateTopologyManager compares the updateRequestId with the updateRequestId in the state manager. This is to confirm that the system state hasn't changed since the request was initiated by another agent. If the updateRequestIds don't match, returns a concurrent update exception response.\n5. Proceed as describe in Approach 1.\n. If we don't need to keep track of the last successfully handled updateRequestId that simplifies things, so I was thinking that if we're not handling the current updateRequestId in state manager, we fail. It's more aggressive in that we might fail even when handling the \"next in line\", but it's simple to implement and rationalize. Also it's easy to recover from with another request.\nSince these ids are numerically increasing it's tempting to use them to infer ordering, but I recommend we use them just as atomic optimistic locks on the request/response cycle.\nIf that's the case, it really does make approach 1 and 2 similar, except that 2 requires additional state storage.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. @avflor let me know what you think.\n. \ud83d\udc4d once ci passes.\n. Related discussion in #1422.\n. @wangli1426 in #1422 you mentioned that the paper is implemented in Storm. Could you share a pointer to that code? Is it OSS and as it contributed to Storm?. One last naming nit and then lgtm! Thanks!\n. Also what about the resource usage in com.twitter.heron.packing.Container? Are those required or scheduled or should those resources remain generic?\n. Re com.twitter.heron.packing.Container yes, that class in completely independent. I was just wondering if the naming change from resource to required and scheduled resource was something that that class should also adopt. I'm really not sure if it should or not.\n. One last public -> private comment, then lgtm! Thanks!\n. \ud83d\udc4d once CI completes\n. No I don't think so, because of the threading. The flow is this:\n1. startExecutor starts a process and calls startExecutorMonitor with the process\n2. startExecutorMonitor creates a Runnable to monitor the process and submits it in separate thread and returns.\n3. the new thread monitors the process for failure\n4. upon failure the monitor thread calls startExecutor to start the process and then the monitor thread completes.\n5. a new thread repeats the cycle with step 1.\n. Whoops, I merged this one too soon. @kramasamy et al please let me know if you have comments and I'll put up a patch for them.\n. Ping. Any other comments on this PR?\n. I had to make a lot of changes for tunneling support for scheduler, which I broke out into it's own PR in #1456. Once we ship that I'll merge master into this and this review will get much smaller again.\n. I've merged in the tunneling changes and this PR is small again. Let me know if there are any add'l comments.\n. The checkstyle output is a red herring. What I suspect is happening is the build is taking a while to compile for other reasons, you just see the checkstyle output along the way. To verify you can comment out the bazel listener and extra_action that do the cpp checkstyles:\nhttps://github.com/twitter/heron/blob/master/tools/cpp/BUILD#L5\n. No. We should only do checkstyles on code we write.\n. +1 for not requiring javadocs on unit tests if you're able to add that inclusion. Best to do so in a separate PR please.\n. @wangli1426 this ticket is related to a user manually issuing a parallelism change using the heron update command. Are you referring to auto-scaling functionality, where component parallelism is dynamically adjusted based on current load? I'd be interested in learning the approach used to algorithmically determine that a component should be scaled up or down. This is something we plan to tackle.\n. Thanks @wangli1426. For (1) we currently do not provide guarantees about local state during scaling events. This is something we'd like to tackle though as a general effort to provide stateful durability. It would be useful during scaling but also during routine failures.\nFor (2) we'll certainly check out that paper. Let's move further discussion on that topic to #1389, which tracks the auto-scalding algorithmic work.\n. \ud83d\udc4d once CI passes. Thanks!\n. Thanks @wangli1426!\n. Can you add tests to YarnSchedulerTest for adding and removing containers?\n. Thanks @wangli1426 though, for making the effort to improve our docs!\n. Does https://github.com/twitter/heron/blob/master/heron/statemgrs/src/python/configloader.py#L22 still work without changes? It's used to provide a state manager to the heron cli.\nAlso, please test that Heron UI and Heron Tracker works with this change, and provide config upgrade instructions in the summary. We're going to need to update our internal configs which live outside this repo. \n. \ud83d\udc4d once CI is fixed\n. \ud83d\udc4d \n. Thanks @avflor!\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. This is not completed and can move to a subsequent release if not done by 0.14.4.\n. Looks good, just one nit.\n. \ud83d\udc4d once ci passes\n. I realized that this approach on it's own is not sufficient. The scheduler can be run as a library, so every agent invoking an update command will have it's own instance. Hence there's no shared locking in that scenario. Please hold on reviewing this.\n. @kramasamy yes I'm working on a ZK impl now. The UpdateTopologyManager code will look mostly the same but the Lock will be backed by ZK and fetched via StateManager.\n. This patch has been updated with the inclusion of distributed locks in #1463 and is now ready for final review.\n. Failing test will be fixed once #1472 merges in.\n. Two general observations:\n1. Do you have a concept of default config settings? If so can we have two tests, one for testing defaults are as expected and one for testing non-defaults?\n2. How could we do this in a way where if someone adds a new config but forgets to update this test, it fails? Is there a way to require all config keys to be part of an enum or something like that, where adding one to the config/enum, but not a test verification for it would fail?\n. \ud83d\udc4d \n. The following [painful] \"log and return null\" pattern is prevalent and deeply nested in the code:\nif an error occurs:\n  - log an message\n  - return null\nSo when failures occur deep in the stack it results in a buried error message and a Main class that gets a null and returns a 1 status code. Hence the poor user experience.\nWe should change that pattern to either throw exceptions all the way to Main class, or to return meaningful Response objects with error codes and messages. The Main should then pass this info back to any python wrappers via stderr while returning the correct response code.\n. Here's another example from when an upload to packer failed. In this case there's a log error message from the packer uploader buried in the output and the failure output from the packer command is suppressed. Ideally both of these would be shown at the tail of the output without SubmitterMain stack trace and without the User main failed with status 1. Bailing out... messaging.\n[2016-11-04 21:11:27 +0000] com.twitter.heron.uploader.packer.PackerUploader INFO:  Uploading packer package heron-topology-xxxxxxx_heron-core-oss\n[2016-11-04 21:12:14 +0000] com.twitter.heron.uploader.packer.PackerUploader SEVERE:  Failed to upload package to packer. Cmd: packer add_version --cluster xxxx billg heron-topology-xxxxxxx_heron-core-oss /tmp/tmpbHOAg1/topology.tar.gz --json\n[2016-11-04 21:12:14 +0000] com.twitter.heron.scheduler.SubmitterMain SEVERE:  Failed to upload package.\n[2016-11-04 21:12:14 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Closing the CuratorClient to: xxxxxxxxx:yyyy\n[2016-11-04 21:12:14 +0000] org.apache.zookeeper.ClientCnxn INFO:  EventThread shut down\n[2016-11-04 21:12:14 +0000] org.apache.zookeeper.ZooKeeper INFO:  Session: 0x1058000caf51655a closed\n[2016-11-04 21:12:14 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Closing the tunnel processes\nException in thread \"main\" java.lang.RuntimeException: Failed to submit topology xxxxxxx\n        at com.twitter.heron.scheduler.SubmitterMain.main(SubmitterMain.java:326)\nERROR: Failed to launch topology 'xxxxxxx' because User main failed with status 1. Bailing out...\nTraceback (most recent call last):\n  File \"heron/tools/cli/src/python/submit.py\", line 149, in launch_topologies\n    launch_a_topology(cl_args, tmp_dir, topology_file, defn_file)\n  File \"heron/tools/cli/src/python/submit.py\", line 114, in launch_a_topology\n    java_defines=[]\n  File \"heron/tools/cli/src/python/execute.py\", line 73, in heron_class\n    raise RuntimeError(err_str)\nRuntimeError: User main failed with status 1. Bailing out...\nINFO: Elapsed time: 117.040s.\n. Yes:\nhttps://github.com/apache/curator/blob/master/curator-recipes/src/test/java/org/apache/curator/framework/recipes/locks/TestInterProcessSemaphoreMutex.java\nOur tests verify via mocks that we call the proper methods on the lock. Curator tests verify that the lock impl works.\n. \ud83d\udc4d once tests pass, thanks!\n. The advantage is consistency of depth. The locks have an extra node for <lockName> in the path so without this change all state paths except lock follow this pattern:\n/state/local/pplan/<topologyName>\nwhere <topologyName> is the data file, but locks were initially implemented like this:\n/state/local/locks/<topologyName>/<lockName>\nwhere <lockName> was the file and <topologyName> a dir. This became a special case due to the depth, where /state/local/locks/<topologyName> doesn't exist, causing an exception.\nBy doing this instead, all files are at the same depth in the tree, which makes for more consistent code paths:\n/state/local/locks/<topologyName>__<lockName>\n. \ud83d\udc4d \n. Any comments on this one?\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. Please pose questions like this on the mailing list instead if filing issues.\n. \ud83d\udc4d \n. I've got a few PRs piled up behind this one. Any comments?\n. This makes me so happy, I was just thinking about this broken window! :)\nI'll review soon. Would you please accept the CLA:\nhttp://twitter.github.io/heron/docs/contributors/community/\n. \ud83d\udc4d \n. My suspicion is that that topology is one that @nlu90 was using to test with that has gotten into a bad state.\n. \ud83d\udc4d once CI is fixed.\n. Agreed, we should assume some external state management. Stream manager should just be used to control routing logic.\nQ1.) Regarding moving existing state when increasing parallelism, in the above flow how exactly do you determine which keys k need to move? Would we have to run all current keys in A through the updated hashing algorithm or is there a better way?\nQ2.) Related to Q1 but your example shows how to migrate state from A -> B, but when adding new instances we'd be going from Set_n -> Set_n + Set_m. Would we need to inspect keys on all Set_n to see what needs to move to Set_m? \nQ3.) Re implementation, could this be done with composition instead of inheritance? This would provide more flexibility to the topology author. So instead of extending the operator the bolt could implement StatefulComponent which could cause a configured implementation of a StateStore to be injected by the framework, or something like that.\nAlso we might want to avoid terminology specific to scaling (i.e., elastic) since this functionality wouldn't be limited to just scaling. It might be used when a faulty node gets restarted for example.\n. @wangli1426, @maosongfu is working on preparing a doc to share by the end of the week.\n. All comments addressed and CI is passing. Let me know if anyone has additional comments.\n. \ud83d\udc4d once ci passes\n. \ud83d\udc4d \n. Please don't double post on the mailing list and githib. Let's let the discussion for this happen on the mailing list.\n. @ashvina that's a good point, those are internal classes. We could add updateTopologyContext into IInstance and just use UpdateableInstance for user implementations of Bolt and Spout. \n. @ashvina following your suggestion I added update(PhysicalPlanHelper) to IInstance which will simplify the implementation considerably. Internal classes will use PhysicalPlanHelper similar to how they do today, but user bolts and spouts only need the TopologyContext object.\nThe TODO in the PR will be satisfied after #1516 ships and the update method is made available.\n. Next I've got some more structural changes to make around the interaction between and  PhysicalPlanHelper/TopologyContextImpl but I'll break those into another review.\n. Sorry for all the commits on this one but I realized I can simplify the approach. I've reverted some edits and am setting us up to allow IInstance to take an update(PhysicalPlanHelper) method since this is a core (i.e., internal only) class. The user bolts/spouts would then implement UpdatableInstnace which takes an update(TopologyContext) method.\n. \ud83d\udc4d \n. @objmagic no, there are no consumers of these methods. They're leftover from a previous refactor that used another approach.\n. I've addressed all the TODOs in the diff except for throwing PackingException unstead of returning null, which I'll do separately. I've also simplified methods that are now redundant and the PackingPlanBuilder implementation. Let me know if there are additional comments. This is ready to commit IMO.\nWork to be done in follow-up PRs:\n- Move PackingPlanBuilder into c.t.h.packing.builder \n- Container can become an inner class of PackingPlanBuilder since it's become an internal implementation detail not used by other classes.\n- Move PackingPlanUtil methods that are now only used by PackingPlanBuilder into PackingPlanBuilder.\n- Add PackingPlanBuilderTests to test that code directly. Currently test code coverage is good, but either tests at at a higher level (i.e., the Packing impls) or at a lower lever (i.e., PackingUtilsTest).\n. IntegrationTest_BasicTopologyOneTaskScaleUpAndDown failed. I was able to reproduce, but not yet sure of the fix. Capturing my thoughts here re what's happening. \nThe test topology is Spout -> Identity Bolt -> Aggregator Bolt. Parallelism is S(1), IB(2), A(1) and we scale S up and IB down to produce S(2), IB(1), A(1). This is the sequence of events:\n1. Launch topology.\n2. S emits A,B,A\n3. IB[1] receives and relays A, A. IB[2] does the same for B.\n4. Scaling event occurs, stream managers restarted, S[2] started, IB[2] killed.\n5. Spouts emit 41 more tuples, all processed by IBs and As. \nThe 3 tuples emitted in steps 2-3 go unaccounted for on A.  Of 44 total tuples emitted, only 41 are received.\nStill trying to understand why...\n. The topology is at least once. I've located the issue and verified with @maosongfu. The current integration test spout tracks how many tuples it sends and decrements that count back to zero to determine when it should send terminals, which indicate the job is done. The decrement occurs in both the ack and fail methods of the spout and there is no resend mechanism implemented in fail to re-emit. This opens up two failure scenarios that I've seen:\n1. In the scenario I reported above the stream manager restarts and fails to emit some tuples to the aggregator bolt. The final tuple counts are low because there is no re-emit implemented.\n2. The stream manager successfully relays all tuples, but upon restart it loses it's state and reports a failures back upstream. In this cases all tuples are received, but the fail method is invoked (false negative) on the spout. This only results in a test failure if I comment out the decrement in the fail method, since what's sent and received matches on both ends. By not decrementing the topology will never finish.\nI'm going to fix by doing the following:\n1. Change the spout to re-emit tuples when fail is called.\n2. Change the spout to not decrement the number-of-remaining-items-to-ack count on fail, only on ack.\n3. Change test assertion to verify at least once (as opposed to exactly once), since we could get re-transmits.\n. Updated per my previous comment. We now have the ability to test at least once, which I use for the scaling tests.\n. @moomou could you please provide some more context as to what this PR and the files included. Things like a description for this PR (what's a side container?), a README, comments on the python code, etc would all be helpful.\nAlso better to use kubernetes as the directory name than k8s.\nThe fabric8-zookeeper project looks like it's received only a few commits a while ago and has been somewhat dormant, so it's level of support is a concern. Also it's license isn't specified so I'd be wary of Heron relying on it.. Please submit questions like this on the email list, instead of opening issues for them.\nThe storm code was first in the backtype package and then migrated to apache. Class from both are included to help with backwards compatibility but new feature work is done in heron classes. If you have additional question please ask on the email list.\n. If your spout or bolt needs to read an input file, you'll need to know how to reference it either relatively or absolutely on the container for which you're running. This logic is specific to what works for your application and your deployed environment.\n. This happened when testing scaling on a large topology on aurora. \n. Should fix #1544.\n. I'm inclined to set all of org.apache to INFO actually, but org.apache.zookeeper to WARN. There are other libraries that log a lot more than we need when in verbose mode. For example org.apache.curator does a lot of this:\n15:23:17 [2016-11-09 23:23:17 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: GetDataBuilderImpl-Background - 1 ms  \n15:23:17 [2016-11-09 23:23:17 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: DeleteBuilderImpl-Foreground - 2 ms\nAnd there's also httpUrlConnection which is really noisy:\n15:23:17 [2016-11-09 23:23:17 +0000] sun.net.www.protocol.http.HttpURLConnection FINEST:  ProxySelector Request for http://smf1-euu-07-sr1.prod.twitter.com:31179/deactivate?topologyid=20161109230939_IntegrationTest_BasicTopologyOneTaskScaleUpAndDown_277d8b5d-d053-4741-9986-114cab9d5a3daa69677c-2071-4334-9218-1f23fabef221  \n15:23:17 [2016-11-09 23:23:17 +0000] sun.net.www.protocol.http.HttpURLConnection FINEST:  Proxy used: DIRECT  \n15:23:17 [2016-11-09 23:23:17 +0000] sun.net.www.protocol.http.HttpURLConnection FINE:  sun.net.www.MessageHeader@402bba4f5 pairs: {GET /deactivate?topologyid=20161109230939_IntegrationTest_BasicTopologyOneTaskScaleUpAndDown_277d8b5d-d053-4741-9986-114cab9d5a3daa69677c-2071-4334-9218-1f23fabef221 HTTP/1.1: null}{User-Agent: Java/1.8.0_102}{Host: xxx.yyy.twitter.com:31179}{Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2}{Connection: keep-alive}  \n15:23:17 [2016-11-09 23:23:17 +0000] sun.net.www.protocol.http.HttpURLConnection FINEST:  KeepAlive stream used: http://xxx.yyy.twitter.com:31179/deactivate?topologyid=20161109230939_IntegrationTest_BasicTopologyOneTaskScaleUpAndDown_277d8b5d-d053-4741-9986-114cab9d5a3daa69677c-2071-4334-9218-1f23fabef221  \n15:23:17 [2016-11-09 23:23:17 +0000] sun.net.www.protocol.http.HttpURLConnection FINE:  sun.net.www.MessageHeader@795cd85e4 pairs: {null: HTTP/1.1 200}{Date: Wed, 09 Nov 2016 23:23:17 GMT}{Content-Length: 33}{Content-Type: text/html; charset=ISO-8859-1}\nAlternatively we could just set com.twitter to ALL when verbose is set.\nComments welcome.\n. My main motivation is to reduce the verbosity of the zookeeper logs - when in verbose mode or not - since zookeeper is so log intensive with basic info that I is never used. Setting ZK to WARN is not uncommon. Having INFO appear, even in verbose mode, makes reading the logs harder IMO, especially when reading CI logs.\nFor example, these log lines get generated repeatedly:\n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:zookeeper.version=xxxx, built on 02/20/2014 09:09 GMT  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:host.name=xxxx  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.version=xxxxx  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.vendor=Twitter  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.home=/usr/lib/jvm/java-xxxx/jre  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.class.path=/var/lib/jenkins/.heron/lib/scheduler/heron-aurora-scheduler.jar:/var/lib/jenkins/.heron/lib/scheduler/heron-local-scheduler.jar:/var/lib/jenkins/.heron/lib/scheduler/heron-scheduler.jar:/var/lib/jenkins/.heron/lib/statemgr/heron-zookeeper-statemgr.jar:/var/lib/jenkins/.heron/lib/statemgr/heron-localfs-statemgr.jar:/var/lib/jenkins/.heron/lib/packing/heron-roundrobin-packing.jar:/var/lib/jenkins/.heron/lib/packing/heron-binpacking-packing.jar  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.library.path=/usr/java/packages/lib/xxx:/usr/xxxx:/xxxx:/lib:/usr/lib  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.io.tmpdir=/tmp  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.compiler=<NA>  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:os.name=Linux  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:os.arch=xxxx  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:os.version=xxxx \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:user.name=jenkins  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:user.home=/var/lib/jenkins  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:user.dir=/data/jenkins/workspace/heron-mirror-release_1  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Initiating client connection, connectString=xxxx:2181 sessionTimeout=30000 watcher=org.apache.curator.ConnectionState@50d0686  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ClientCnxn FINE:  zookeeper.disableAutoWatchReset is false  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ClientCnxn INFO:  Opening socket connection to server xxxxxxxxx:2181. Will not attempt to authenticate using SASL (unknown error)  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ClientCnxn INFO:  Socket connection established to xxxxxxxxx:2181, initiating session  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ClientCnxn FINE:  Session establishment request sent on xxxxxxxxx:2181  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ClientCnxnSocket FINEST:  readConnectResult 36 0x[0,0,0,0,0,0,75,30,f,58,0,c,ffffffaf,5e,ffffff87,ffffff82,0,0,0,10,ffffff8a,ffffffe5,ffffffeb,ffffffa6,fffffffd,4f,ffffff82,ffffffdb,a,44,fffffff3,6e,fffffffb,67,34,ffffffe9,]  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ClientCnxnSocket WARNING:  Connected to an old server; r-o mode will be unavailable  \n11:02:36 [2016-11-09 19:02:30 +0000] org.apache.zookeeper.ClientCnxn INFO:  Session establishment complete on server xxxxxxxxx:2181, sessionid = 0xf58000caf5e8782, negotiated timeout = 30000\n@objmagic do you have examples of the ZK log lines that you'd like to still see? Also note that we access ZK through Curator which also logs heavily so we will still have plenty of output for each ZK interaction.\n. @objmagic I removed --verbose for the kill commands (I see I missed one, will update) but for the others for now I think we need to keep it. Because we use the log-and-return-false-upon-failure anti-pattern so extensively, I've seen issues where logging is FINE and we don't see what caused the issue. For example see this recent fix https://github.com/twitter/heron/pull/1543/files#diff-b40887a82a97dd9c6613d65e2b25f4cf.\nOnce we've fully abandoned that pattern in favor of throwing meaningful exceptions, then I think we can remove more --verbose flags with more confidence.\n. Removed verbosity on kill as well as noisy pex logging that was necessary when we first introduced the pex build and needed to troubleshoot frequently, which is no longer the case. We can uncomment if we need move debug info. This is what we won't see lots of anymore:\n10:45:50 pex options: {'use_wheel': False, 'inherit_path': False, 'script': None, 'python': '/usr/bin/python2.7', 'repos': [<pex.fetcher.Fetcher object at 0x7f9d35195790>, <pex.fetcher.Fetcher object at\n 0x7f9d35195950>], 'pex_name': None, 'pypi': False, 'requirement_files': [], 'platform': 'linux-x86_64',\n 'cache_dir': None, 'entry_point': 'heron.shell.src.python.main', 'verbosity': 0, 'pex_root': None, 'zip_safe': True, 'interpreter_cache_dir': '{pex_root}/interpreters', 'ignore_errors': False, 'cache_ttl': \nNone, 'python_shebang': None, 'always_write_cache': False, 'disable_cache': True, 'find_links': 'http://science-binaries.local.twitter.com/home/third_party/python/dist/'}\n10:45:50 pex requirements: ['requests==2.11.1', 'tornado==4.0.2']\n. \ud83d\udc4d \nIn the future we'll want to not mix functional changes and bug fixes with version upgrade patches.\n. The downside is mainly timing. The restart itself is quick, but then each instance has to register and the stream manager has to register with the TMaster. If it could just update it's internal routing table, we could reduce latency due to unnecessary restart cost.\n. +1\nI was thinking the same thing, which is that step 1 isn't really necessary since what matters is the relative allocation factor, not whether it's above or below the ideal. It's conceptually helpful to describe the ideal we're striving for only, but not required when implementing.\n. @ashvina if T3 came after T2 you'd end up with (A B) (A B) (A B) right?\nEither way, yes this algorithm will be optimizing for balance over resource utilization. I think that's acceptable for now until we see how this feature is used. It's more tailored for the case where a single component is being scaled up and down by large amounts, and the balanced approach would free the previously added containers. \nWhen scaling down from T1 to T2 what you show seems better to me that going to  (A A B) (A A B) (B) IMO. I'm inclined to try to achieve balance and if we need to at some point we could implement a repack that isn't biased towards least disruption that just repacks everything as efficiently as possible, to prune wasted space. Kind of like a Full GC that could happen if too many scale up/downs have happened and we're fragmented.\n. My reason for suspecting that a single component will be scaled is that when we see issues in production it's typically a single component that appears to be causing back pressure. As a result I suspect it will be fairly common that a.) the initial topology will be fairly well packed and that b.) users will adjust one at a time. When that happens, it's very likely that new, fairly unbalanced containers will be added, which should be remove upon a scale down.\nRepacking the entire topology is something we should experiment with and possibly iterate towards, but until we have stateful processing, we (i.e. Twitter) want to optimize for least disruption in terms of plan changes, speed of execution and state loss.\n. @avflor what you're describing sounds similar to the scoring algo that I described with the addition of a secondary sort in the event of a scoring tie, by total number of instances ascending. This way you favor draining homogeneous containers first. Sounds reasonable to me.\n+1\n. @avflor no need to specially categorize in 1) as you describe since the homogeneous containers will have a score of 1.0 so they're naturally sort to the top, above heterogeneous.\n@ashvina restarting processes has start up costs in terms of time taken to shutdown/deregister plus time taken to reinitialize, as well as load on other components during service discovery, as well as re-registration with others. As a result, moving and restarting 1000s of process when adding or removing only a few, for example, would incur unnecessary cost.\n. @ashvina only the container plan of the instances removed would be effected if we just removed a few of them. If we instead reshuffled all instances across all containers, then every container plan would be effected.\n@avflor I see what you're saying now. I mis-read the first time. Yes, we should take into account all component changes requested when selecting a container. I'll follow up on #1584 . Actualy, @avflor the algorithm you propose only works in examples like you've shows where you have a balanced plan to start with, which we've found is often not the case after scaling up. Take for example this plan:\n[AABB, AABB, AABB, AABB, AAAA, BBBB]\nIf a request is made to remove 4As and 4Bs the user would expect to remove the last two, but instead we would get this:\n[AABB, AABB, AAAA, BBBB]\nIf they did that again the would get this:\n[AAAA, BBBB]\nIf instead they scaled down 4As and 4Bs in two different scale down requests they would get back to:\n[AABB, AABB, AABB, AABB]\nI think we'd need one more set of tie breakers (see 2):\n1. score based on homogeneity taking all components requested into account\n2. break tie based on homogeneity taking just the component being scaled into account\n3. break tie by count of the component ascending\n4. break tie by count of all components ascending\n5. break tie by container id descending\nI believe this would work for both scenarios that you and I presented.. For some reason I thought you were previously proposing an initial scoring taking into account all components being requested, which I see wasn't your intent.\nThe challenge with the primary scoring being based on number on total instances ascending is that it biases away from removing imbalanced containers. For example if someone makes a request to remove 3As and a B from this:\n[ABC, ABC, ABC, AAAB]\nthey would get this:\n[C, BC, BC, AAAB]\nUnbalance appears as a result of scale up, which is why if we want scale down to effectively back out those changes when scaling down, we would focus on removing imbalance first IMO.. At some point we should consider adopting scale up/down strategies, but not until we see how people actually use the feature. Until then we can provide what we think it a reasonable default.\n@avflor I've updated the algorithm in #1584 per your suggestion. After reading it repeatedly I think I finally grokked all the scoring stages and wrote something that handles the examples we've discussed.. This ticket is blocked on us reaching an agreement for how we should proceed. I suggest we follow option 1 above, because it's unclear to me why tuples should fail in integration tests.\nI would like to hear opinions from others though.. @maosongfu, @kramasamy do you have thoughts re why tuples are being consistently dropped by the integration tests (is this expected?) and if that's something we should fix, or obfuscate by changing to at least once verification?. Relevant classes:\nhttps://github.com/twitter/heron/blob/master/integration-test/src/java/com/twitter/heron/integration_test/core/IntegrationTestSpout.java#L135\nhttps://github.com/twitter/heron/blob/master/integration-test/src/java/com/twitter/heron/integration_test/core/IntegrationTestBolt.java. If a process dies currently and tuples get re-emitted, that would trigger a failure during assertion, since we assert exactly once, not at least once.\nThese tests fail often though, so if we have frequent process deaths in our integration tests, something is really wrong. Hence my hesitation to change to at least once assertion without first identifying the cause of re-transmits.. \ud83d\udc4d \nOnce this is merged, we should tag gh-pages as well actually.\n. \ud83d\udc4d \n. @ashvina the only way that seems like it would happen would be if the same PackingPlanBuilder was used to pack/repack, then build() was called, and then addInstance was called on the builder. The expected usage is that after build() is called, changes can no longer be made to that builder. We need to clarify that in the builder javadocs and throw a PackingException if that happens, instead of a NPE.\n. @ashvina let me know if you can reproduce. I'm puzzled because the first thing addInstance does is initialize the containers, where non-contiguous ids are supported:\nhttps://github.com/twitter/heron/blob/master/heron/packing/src/java/com/twitter/heron/packing/PackingPlanBuilder.java#L260\nThen we never delete containers, we just skip the empty one's when we generate the plan:\nhttps://github.com/twitter/heron/blob/master/heron/packing/src/java/com/twitter/heron/packing/PackingPlanBuilder.java#L181\n. @avflor that looks like it. If that's the case I have a patch already written (for another reason) that would address it. I can submit it after #1565 ships, which it is built upon.\n. The patch in #1572 should address fix this issue, since it doesn't assume a contiguous id space. It sorts containers by id and iterates though each.\n. @ashvina I believe #1572 has addressed this issue since FFDP now places instances using a container scorer which ranks by id, instead of directly by id. Please re-open if you have a test that shows otherwise.. Changes look good, just awaiting two unaddressed comments of mine outstanding. See https://github.com/twitter/heron/pull/1571/files. Any comments on this one? I think this change will make it much easier for us to compose rankings for how instance placement and removal should occur.. @wangli1426 thanks for the comments.\n\nYes, we should improve the scale down logic. In this PR though I'd keep all functionality the same and just changing the implementation to introduce the ranked approach. In a follow up we'll change the scoring logic for scale down, most likely when addressing #1560.\nYes, in the PackingPlanBuilder.pack() method, empty containers get removed from the built PackingPlan. This happens in the buildContainerPlans() method, where you'll see that we continue in the loop if the number of container instances is zero.. I believe we've captured that used case. See how we use the two different\nconstructors to the ContainerIDScorer.\n\nOn Tue, Nov 22, 2016 at 11:02 AM avflor notifications@github.com wrote:\n\n@avflor commented on this pull request.\nIn\nheron/packing/src/java/com/twitter/heron/packing/builder/PackingPlanBuilder.java\nhttps://github.com/twitter/heron/pull/1572#pullrequestreview-9734563:\n\n\n* @throws com.twitter.heron.spi.packing.PackingException if the instance could not be removed\n*/\npublic int removeInstance(Scorer scorer, String componentName) {\nList> scorers = new LinkedList<>();\nscorers.add(scorer);\nreturn removeInstance(scorers, componentName);\n}\n  +\n@SuppressWarnings(\"JavadocMethod\")\n/**\n* Remove an instance from the first container possible ranked by score. If a scoring tie exists,\n* uses the next scorer in the scorers list to break the tie.\n* @return containerId of the container the instance was removed from\n* @throws com.twitter.heron.spi.packing.PackingException if the instance could not be removed\n*/\npublic int removeInstance(List> scorers, String componentName) {\n\n\nIn the RCRR implementation, during scale down we do not always start from\nthe first container. For example if we need to scale down 2 components by 1\ninstance from containers [1...5] and the first component is removed from\ninstance 2 then the removal for the next component will start from\ncontainer 3. This will help with load balancing. This remove instance\nmethod does not capture that. However, it is fine if a good scoring method\nis used when we change the scale down approach as discussed in the other\nthread.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/pull/1572#pullrequestreview-9734563,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AABczvblIJ0Vv-h2nf9dQG0EG4sexTMbks5rAzwmgaJpZM4K26I-\n.\n. Yes, that's why the constructor requires the maxId, so it can loop back to\nthe lowest and continue.\n\nOn Tue, Nov 22, 2016 at 11:18 AM avflor notifications@github.com wrote:\n\n@avflor commented on this pull request.\nIn\nheron/packing/src/java/com/twitter/heron/packing/builder/PackingPlanBuilder.java\nhttps://github.com/twitter/heron/pull/1572:\n\n\n* @throws com.twitter.heron.spi.packing.PackingException if the instance could not be removed\n*/\npublic int removeInstance(Scorer scorer, String componentName) {\nList> scorers = new LinkedList<>();\nscorers.add(scorer);\nreturn removeInstance(scorers, componentName);\n}\n  +\n@SuppressWarnings(\"JavadocMethod\")\n/**\n* Remove an instance from the first container possible ranked by score. If a scoring tie exists,\n* uses the next scorer in the scorers list to break the tie.\n* @return containerId of the container the instance was removed from\n* @throws com.twitter.heron.spi.packing.PackingException if the instance could not be removed\n*/\npublic int removeInstance(List> scorers, String componentName) {\n\n\n@billonahill https://github.com/billonahill I'm not sure if that is the\nsame. In the previous example if we pass as input to the constructor\ncontainer id 3 and containrId 5 then the search will stop at 5, right?\nWehat we ideally want is to start from 3 and use this sequence: 4,5,1,2. Is\nthat behavior captured?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/pull/1572, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AABczkOZ2XD-GVC2Q1_HNvBgW8okdkgVks5rA0AJgaJpZM4K26I-\n.\n. @avflor @wangli1426 any other comments?. \ud83d\udc4d . Any comments on this?. @avflor @ashvina please review when you can.. thanks @avflor for reviewing.. \ud83d\udc4d . @avflor I've updated the algorithm I believe as you've proposed. Please review the unit tests to see if I've captured all the examples we've discussed. . @avflor or @ashvina any comments on this?. Any other comments on the proposed new class?. Thanks @objmagic for the reviews! CI is broken because I now need to refactor a ton of downstream code that is broken due to the proposed changes. I'll make that refactor and resubmit.. Just pushed the large refactor of all the downstream classes. I would recommend reviewing the following 3 classes primarily:\nheron/common/src/java/com/twitter/heron/common/basics/ByteAmount.java\nheron/common/tests/java/com/twitter/heron/common/basics/ByteAmountTest.java\nheron/spi/src/java/com/twitter/heron/spi/packing/Resource.java\n\n@ashvina you might want to check on the yarn classes to check on my work there:\nheron/schedulers/src/java/com/twitter/heron/scheduler/yarn/HeronMasterDriver.java\nheron/schedulers/tests/java/com/twitter/heron/scheduler/yarn/HeronMasterDriverTest.java. Whoops, sorry bout that @fabianmenges!. Serialized protobuf objects are stored in the Heron state manager for the duration of the topology livecycle. Is this change backwards compatible so the new code could continue to read protobuf objects stored with the previous code?. @congwang can you verify the compatibility by deploying heron-tracker with the new version and trying to read data for all of the currently persisted fields? A quick search doesn't reveal anything in the proto3 docs that clarifies ABI compatibility so we should test on our own.\nhttps://developers.google.com/protocol-buffers/docs/overview#introducing-proto3. Maybe I'm confused, is protobuf 3.1.0 not proto3?\nEither way, do we have any protobuf documentation that 3.x is guaranteed to be ABI compat with 2.x? Often times major semver changes are specific not. Without that we should test this change. . There's a lot of duplicate code with only subtle difference in how we run. Seems like it could be refactored to be more extensible, or cli_helper.run could really be made into a class that knows how to run the RuntimeManager class (as opposed to a helper method), which could be extended for customizations that some commands might need. Such an OO approach would be nice than the procedural code that exists now.. +1, better to use these constants than the string version.. It's more strongly typed and has compile time checks built in.. AFAICT Jars is unused and without unit tests and hence should be deleted is what I'm proposing. I don't see a need to keep any of those unused methods around. Most of what it provides is also available in the guava Files class too I believe.. \ud83d\udc4d . @avflor, @ashvina please review when you have a chance. \ud83d\udc4d . This class seems to interchange prefix with contains quit a bit, in both the comments, the method naming and the impl. I don't think this proposed change is safe since contains is also used to match on named, which I suspect are not prefixes. @maosongfu thoughts?\nWe need to have unit test for this class.. \ud83d\udc4d once CI passes. Changes look good, but can we not prefix \"ERROR:\" to every message string? These messages are going to be used as the message to a RuntimeException, which should be ultimately logged as SEVERE, so no need to also put ERROR in the message.. \ud83d\udc4d thanks!\nDoing the same in ByteAmount would be great thanks.. This change seems reasonable (we should really create a way to share common options across main classes) but let's not commit this until there is some functionality backing it.. \ud83d\udc4d\nThanks for the cleanup!. Will review when you push your changes.. \ud83d\udc4d . That's awesome, the table formatter looks good. It seems like a summary table view actually. We could envision a detailed table view that shows the instances on each container. One suggestion would be to add line breaks to the table shown at container boundaries. We'd need to add a container id and instance id column and remove parallelism in that context.. Some views will be practical on the screen for smaller topologies but not larger ones. Larger one's might be piped to a file and grepped or imported into a spreadsheet for analysis for example.\nWhat I would find useful would be to be able to visually see the distribution of instances (by component type) on each container to be get a sense for how different packing algos change the packing density, balance, etc - similar to what we show on the UI. Maybe that's a different view entirely with an ascii-grid of sorts perhaps.\nFor updates the user wants to see what container will be modified or added, so being able to easily see that would be great - even if it involved piping to a grep statement to keep things simple.. In the enlarged containers which instances are added? Can you visually represent that?\nI like the rendering but do you think it would simplify things by condensing states enlarged, reduced and modified all into just modified? Then you could see which instances were added and removed in each of the modified.. Looks good. We should also highlight what's modified in 1 and 2. At the very top we should output some overall info, like total number of containers and max container size.. \ud83d\udc4d . \ud83d\udc4d . Discussed some of this offline. A few more specifics:\n\nThe error occurred when using a custom IPacking impl.\nThe newer IPacking classes (not RoundRobinPacking) use PackingPlanBuilder which prevents empty containers in the build() method. The custom packing impl was not using PackingPlanBuilder.\nThe scheduler should still assert that none of the containers received in the PackingPlan are empty, since users can implement their own packing plans that could be invalid. That's the scope of work for this issue IMO.\n\n. FWIW, here's a script I use to find the missing one:\nsed 's/.*stmgr-//g' logfile.txt\u2002\u2002| sort -n > found.txt\nseq 0 `tail -1 tmp/found.txt` > expected.txt\ndiff expected.txt found.txt. @objmagic yes saving expected output files in a test resources dir would work well.. Please include the relevant help output for review.. This could easily be done using || or && bash operators no?. @objmagic +1 to improving our status codes to allow users to respond to them differently as they wish.. \ud83d\udc4d\ngood one. While we're at it, could we also make it clear what the red font indicates?\nCurrently that's a bit of a mystery.\nOn Thu, Dec 29, 2016 at 12:07 PM cckellogg notifications@github.com wrote:\n\n[image: topologies-with-status]\nhttps://cloud.githubusercontent.com/assets/240473/21553792/562c954a-cdbf-11e6-9d4e-ae85dceb5aab.png\n\nYou can view, comment on, or merge this pull request online at:\nhttps://github.com/twitter/heron/pull/1646\nCommit Summary\n\nAdded a topology status to the list all topologies ui.\n\nFile Changes\n-\nM\nheron/tools/tracker/src/python/handlers/stateshandler.py\n   https://github.com/twitter/heron/pull/1646/files#diff-0\n(4)\n-\nM\nheron/tools/tracker/src/python/handlers/topologieshandler.py\n   https://github.com/twitter/heron/pull/1646/files#diff-1\n(8)\n-\nM\nheron/tools/tracker/src/python/topology.py\n   https://github.com/twitter/heron/pull/1646/files#diff-2\n(20)\n-\nM\nheron/tools/tracker/src/python/tracker.py\n   https://github.com/twitter/heron/pull/1646/files#diff-3\n(15)\n-\nM\nheron/tools/ui/resources/static/js/alltopologies.js\n   https://github.com/twitter/heron/pull/1646/files#diff-4\n(11)\nPatch Links:\n\n\nhttps://github.com/twitter/heron/pull/1646.patch\n\n\nhttps://github.com/twitter/heron/pull/1646.diff\n\n\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/pull/1646, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AABczhCcwBM5CFnshzO8qlDhfgg6PLhCks5rNBL3gaJpZM4LX0Lm\n.\n. @kramasamy the states we currently show align with the documented Heron states that users control via the Heron cli. Since 'ORPHANED' is not one of those, my concern is that it's meaning would also not be understood by a typical user. Somehow we need to convey exactly what is meant by orphaned or red-font. I'm open for UI suggestions for how to do that though, but I think it might need more of an explanation somewhere on the page.. When we fix large site bugs we should be able to easily republish the site\nwithout waiting for the next release.\n\nOn Mon, Jan 2, 2017 at 3:02 AM Mark Li notifications@github.com wrote:\n\nresolved via #1575 https://github.com/twitter/heron/pull/1575 but this\nchange was introduced after 0.14.5 so it was not included in the latest\nwebsite build.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/1652#issuecomment-269958347, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AABczhwHPwaY7oQ91PoLh08xylpx3f_jks5rONlRgaJpZM4LY2Qc\n.\n. \ud83d\udc4d . com/twitter/heron/spi/common/Resource.java is unrelated. That's a heron object to model a container or instance resource (i.e., ram, cpu, disk).. Oh yes, sorry. I forgot about this class. We also have a Resources class, which I was talking about. Too much overloading.... This occurred on the old closed source version of Heron. We should investigate the current code to see if this bug is possible. Expected behavior during a restart is to fail with proper messaging if the shutdown does not succeed to clean state, without proceeding to redeploy.. \ud83d\udc4d . Thanks!. \ud83d\udc4d . \ud83d\udc4d . \ud83d\udc4d . I would recommend json output since @ajorgensen has an immediate use case, and to hold off on on a delimited format until necessary (I'm typically more a fan of TSV that CVS fwiw).. \ud83d\udc4d . Thanks for the reviews. Yes @ashvina the config and runtime thing has also bothered me and I'd like to clean that up too. My main motivation for now is to reduce the amount of config code we have and to make for an easier API to work with. \n\nI've got a follow-up proposal in https://github.com/billonahill/heron/pull/5/files to remove keys.yaml and move into an enum. This is just a straight clean-up refactor, but we could also follow up with a better way to differentiate between cluster config and topology/runtime configs.. @ashvina during an update the physical plan gets removed from the state manager, then the TMaster awaits registration from the existing and new stream managers before updating the physical plan. In this case one or more stream managers never registered. The way to troubleshoot is to identify which one had problems by looking in the tmaster logs, then checking the logs of that stream manager to understand why it failed to register.. Correct. All stream managers are restarted actually as part of update.. Thanks @objmagic for the review. This one got huge indeed. Key classes  (no pun intended) are Key, Config and ClusterConfig. The rest is just the resulting fallout.\n@huijunw not sure why this approach was taken initially.. \ud83d\udc4d \nWould you please rename #1687 to something like \"Fix and re-enable MultiSpoutsMultiTasks test\". There's a flaw in this approach. SubmitterMain initializes configs using the local HERON_HOME (i.e.. /var/lib/jenkins/.heron) using this method when submitting:\nConfig config = Config.expand(Config.newBuilder()\n        .putAll(ClusterConfig.loadConfig(heronHome, configPath, releaseFile, overrideConfigFile))\n        .putAll(commandLineConfigs(cluster, role, environ, dryRun, dryRunFormat, verbose))\n        .putAll(topologyConfigs(topologyPackage, topologyBinaryFile, topologyDefnFile, topology))\n.build());\nThis creates HERON_BIN as ${HERON_HOME}/bin and HERON_SANDBOX_EXECUTOR as ${HERON_HOME}/bin/heron-executor where ${HERON_HOME}=/var/lib/jenkins/.heron which fails on aurora, because ${HERON_HOME} is the absolute path of the heron install. What aurora needs to be set is ./heron-core/bin/heron-executor based ${HERON_SANDBOX_HOME}/bin.\nFor this reason trying to combine concepts like HERON_SANDBOX_BIN and HERON_BIN using smart wildcard substitution underneath for HERON_HOME, doesn't work in this case because the submitter needs both local configs (i.e. ${HERON_HOME}/lib) for submission and remote configs for running aurora tasks.\nI need to think this through more, but suggestions welcome...\n. I've come up with an approach to make the Config object hold on to the un-substituted configs (which I call the rawConfig) as well as the locally-substituted and remote-substituted configs. Consumers can toggle a config object back and forth with the Config.toLocalMode(config) and Config.toRemoteMode(config) methods as needed. Those methods return new cached config instances instead of mutating the one passed.\nAn example can be seen in how AuroraScheduler uses remote for setting the properties in heron.aurora (which is used on the server) but uses local for the heron config dir required to get the local path to heron.aurora used by the aurora CLI call.\nAlso note this PR has some duplicate unit test changes from #1704. Once #1704 ships and I merge this review will get smaller.. @kramasamy the previous one-level expand method was broken. I think it only worked because callers happened to call expand more than once. I've added ClusterConfigTest.assertConfig(..) which now asserts that all tokens are substituted.. @kramasamy all comments addressed. Remaining open issue is that of terminology.. Updated terminology to use LocalMode and ClusterMode. All identified issues have been addressed.. \ud83d\udc4d assuming you've verified that the site can still be properly built on mac and internal/external CI and that the rendering is correct.. Ping. Any comments on this one?. Ping. I went ahead and removed usage of public SystemConfig(String configFile, boolean mustExist) constructor in favor of the Builder approach since it wasn't that big of a change.. PR #1920 removes the config mentioned above.. Fixed in PR #1860.. No idea. I wonder why heron-core includes any schedulers. I'd expect them to be distributed separately to reduce bloat and extra unnecessary deps.. It looks unused and not necessary to me. Also, targets like this in that BUILD file are misnamed IMO, since they catch all configs, not just yaml:\nfilegroup(\n    name = \"conf-aurora-yaml\",\n    srcs = glob([\"conf/aurora/*\"]),\n). I don't know why the packing files are duplicated in heron_client.bzl but this seems incorrect to me.. This is a great feature, but as implemented it seems overly specific. Could we instead extend upon /topologies/executionstate (I don't recall exactly what that returns) or introduce some other endpoint that represents topology state? This seems more extensible.\nI would then expect to be able to make a single request to find the state of the topology, along with a collection of objects for all stream managers, each including their registration state.\nThat way the API would have a few key concepts like physical plan, logical plan and state.. \ud83d\udc4d . Sangjin did something similar for Hadoop. Basically, all user classes are loaded from a user classpath, which includes the user jars/dirs. If classes aren't found, there is a whitelist of system-level classes that will be searched. This list should be as short as possible and it typically includes the framework classes (e.g., Heron API) and some very low level core classes like javax and logging. This is similar to what servlet containers do for user classpath isolation, while providing access to framework classes.\nNote that this class loader logic is reversed from the default classloader logic, which is to first check the parent loader, then check the current loader.\nThis is the trunk version of ApplicationClassLoader:\nhttps://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ApplicationClassLoader.java\nThe default system classes definition:\nhttps://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/resources/org.apache.hadoop.application-classloader.properties. Example usage:\nhttps://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/RunJar.java\nhttps://github.com/apache/hadoop/blob/trunk/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/YarnChild.java\n. Can we hold off on this until after the design review?. Yeah, the other problem with that solution though, is that it's error prone, in that anyone downstream could look at the config or the topology for the number of containers and use that, which would be incorrect. I also suspect there are places where the config is accessible but not the packing plan. I don't love the current solution but I think it's safest and most consistent.. Actually, still investigating the root cause of the behavior I'm seeing. It seems that IUpdtable.update(..) might not be being called at all upon a scaling event.. The issue was that ISpoutDelegate was being used, which was not updated to implement IUpdatable. Fixing in #1749. What you show in 3. is the head of master, where that value is now ByteAmount. You can see in the 0.14.5 tagged version that it was a long at the time of the release: https://github.com/twitter/heron/blob/0.14.5/heron/api/src/java/com/twitter/heron/api/Config.java\nCan you provide a test case that can be used to reproduce what's described in 1? Does the same thing happen when using a build made from the head?. @objmagic can we cherrypick this one into the release too?. @huijunw yes, makes sense. Will update.. updated. +1\nAs discussed I think we should add the following api endpoints:\n/topologies/metadata - existing static topology info from /topologies/executionstate. Possible additional future data like git sha, build date, etc.\n/topologies/runtimestate - state information about the topology. Stream manager registration state and any has_foo states from /topologies/executionstate that are actually in use an necessary.\nWe could add the new API endpoints and then change the UI to use them, then remove /topologies/executionstate.. @objmagic would you please provide a proposal with an overview of the current relevant endpoints, along with the the new proposed endpoints, both with example data. Info, seems like it can also be related to this effort, but it's generically named to not convey it's purpose.. Right, info is a collection of everything.\nFor runtimestate if we structure like this, we can be more extensible to serve additional info later:\nstmgrs {\n  \"stmgr-1\": { \"is_registered\": True }\n  \"stmgr-2\": { \"is_registered\": False }\n}. Would you please provide steps to reproduce.. What is the command you're using in step 4 and what's the classpath? As pointed out above, you probably have a different Heron client binary being used than the version the topology was built against. Typically you would want to also install the heron client locally and submit a topology that way, which will assure you that you're running the client version you expect.. That approach to running a topology (invoking topology main directly) has not been tested AFAIK. The heron client sets a number of environment values and passes parameters to main, so you'd need to mimic the client interaction if you want to run the main directly. I recommend reading the client code at tools/client/src/python to see how you could mimic the client behavior in this way, or just run the topology using the heron client. You could also try using heron Simulator, which is intended to allow a topology to be run in a single jvm in an IDE.\nhttp://twitter.github.io/heron/docs/developers/simulator-mode/. \ud83d\udc4d . \ud83d\udc4d . Note that I commented on the twitter jira about taking steps to assert that the symptom still occurs when the effected topologies are running on OSS Heron, before deciding to ship this. Let's discuss that there.. @huijunw we can't merge this until after a new bazel version is published within Twitter and it's been tested in Twitter CI. This change will require coordination.. If this code is to be committed into the heron project, the package naming should start with com.twitter.heron. Heron should not be a home for code from other projects. If the org.apache.dhalion packing naming is desired, the code should live in a separate repo IMO.. The current approach has limitations, but it also has simplicity and transparency. It's easy for someone to rationalize that one agent invoked back pressure and all the spouts stopped. With this new approach, understanding the system becomes a more complex problem. @congwang can you discuss what metrics, logging, etc that should be (or are being) included to help someone troubleshooting to understand the current state of the system and how it got there? For example why some bolts might be emitting and not others, or visualizations around the reference counting mechanism, or which agents are contributing to the backpressure and in what order.\nAlso, the shortcomings and the features of the new approach are well described above, but could you expand on the specifics of the new algorithms at play, either here or in another doc or state diagram? I think that will be a critical thing to understand both when troubleshooting and when reading the code.. Thanks @congwang, yes we'll certainly want docs under website/content/docs so might as well start there.\nFor the reference counting I understand the reason for the feature, I was questioning how the effect of the feature would be made clear to the user during backpressure. I wanted to make sure we've thought about how to help a user answer these questions (for example) when backpressure is occurring, ideally without having to sift too much through logs:\n\nWhich components are responsible for the back pressure and in what order? Should this be a metric?\nWhich ones have released their call for back pressure, which are remaining?\nAre bolts not emitting because they're under backpressure, or because they're not receiving tuples?\n\nI haven't yet reviewed the code, but wanted to hear your thoughts on these so I'd know what to expect. From past experience understanding backpressure hasn't always been easy with the existing implementation.\nMy question about the bolts not emitting was because with the new approach you would stop upstream bolts from emitting but others would continue to emit, as I understand it. I was using that as an example for something that we might need to clearly point out to our users (or not?).. For documenting the specifics of the algorithm, an README targeted to just Heron developers would suffice, if you're not comfortable writing the public documentation. This is a critical thing to clearly describe to make it possible to best review and maintain the code.. Can you clarify the excepton and what causes it? It's not clear from your description, so based on the code I assumed it was caused when calling cacheMetric.firstKey() when cacheMetric.isEmpty() == true. Putting that check outside the for loop would keep that from happening no?. Could we do this then instead? I'm suggesting for readability. Trying to decipher the reasoning for the null checks make the code harder to understand.\nwhile (!cacheMetric.isEmpty()) {\n  Long firstKey = cacheMetric.firstKey()\n  if (firstKey < now - maxIntervalMilliSecs) {\n    break;\n  }\n  cacheMetric.remove(firstKey);\n}. Thanks @kkdoon! Would you please include a screenshot of the proposed change and update the title of this pr to be more descriptive.. Thanks @kkdoon for the screenshots, this looks awesome and is similar to what we've envisioned for this. I think one of the key things is to be able to easily visualize the containers/instances under backpressure on the physical plan view in the upper right, without having to mouse over anything. If those are somehow highlighted when in backpressure that would be huge. (Being able to link to a viz plot that shows which container is in BP over time would also be a big win, but this is beyond the scope of this PR.)\nAlso, just as an FYI, we've also discussed being able to highlight which containers have registered/not registered with TMaster in that same view during the submit process. This is of course also outside of the scope of this PR, but another use case to keep in mind when developing the highlighting functionality.. I'm not sure about the answer to your question (others might know) but would you mind please posting this to the mailing list instead of filing a ticket for it? That's the best place to get help with questions.. @smirnp I'm glad you rooted out the issue. Would you mind sharing your fix, ideally in the form of a patch?. Yes, make a fork, branch off the master, make the patch, push the branch, create a PR from that branch against origin master.. @objmagic I cache the result of _get_jvm_version so I don't need to keep spinning up new JVMs to check the version repeatedly. The cached value is self.jvm_version.. @kramasamy the bin/java approach is what's used in a number of places in the existing code. I'd prefer for that change to be tackled in a PR of it's own, to keep the changes and scope clean.\nRegarding the version string, yes java -version can output just about anything. That's why I wrote com.twitter.heron.instance.util.JvmVersion which just outputs the version string. This makes parsing much more resilient and reliable.. @objmagic yes. Just got the final approval and merges. Timur has identified other params that we might want to add as well fyi.. I'm not taking a stance on FATAL or not, as I don't have full context, but if we choose to not exit the process we must always take care not to log on error for high-throughput systems. This can quickly fill up disk and cause a cascade of problems. Error counters should be favored over error logging. Or some sort of configurable log throttling/sampling mechanism.. @congwang yeah, this isn't pretty and maybe it could be put into a logging decorator perhaps if we want to do more of this, but java.util.logging doesn't do any of this for us AFAIK.. Thanks @objmagic that was indeed a typo. Fixed.. @kramasamy the point of this branch was just to see if we could get Trident to run on Heron, to understand how it's implemented and to see it work. I have not yet fully achieved all of those goals, but I recommend doing so before deciding to re-implement parts of Trident.. Ping. Any takers?. Sorry for piling on with my commits, but I made a large refactor to set these tests up better for extensibility. With this patch I can now follow up in another PR to make this class abstract and have CustomGroupingTest as the concrete subclass with only the content from below the addComponent method (I didn't want to do this in one PR since the class naming would make reading the diff hard). We can then add another subclass to test emit direct from the spout and another to do the same from a bolt.. Great find. I don't understand how this would make the test flaky. Why wouldn't the tests fail consistently without this fix?. Ping. Any comments?. Good catch @objmagic, fixed.. \ud83d\udc4d looks good!. Can this be part of a scripted (or even automated) release process?. Having the sort be based on submit time is helpful for someone who just submitted a job, but not for others. If we want this, we should consider letting the user change the sort order (submit time asc/desc, alpha, etc).. @ajorgensen you guys also run on EC2, correct? If so could you review this to see if your approach and this approach could coexist in this codebase?. Why not just change that line in the setter to the following? Why do we even convert seconds to strings?\nconf.put(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS, secs);\nIt's not clear to me why breaking the inheritance is the best fix for this issue.\n[Note: This is an example of why it's generally preferable to use custom types (e.g., Duration) over primitives, similar to the refactor we did to use ByteAmount over primitives. By doing that we don't run into issues like these.]. -1\nThis PR seems like we're taking a step backwards with our improvements to types, like ByteAmount, to work around another problem regarding seconds (#1841), which could be improved with better typing (i.e. Duration). With this change we're no longer storing ByteAmount objects, but we're instead storing string representations of integer representations of bytes (or MB, GB?). Even worse, we provide a static helper on c.t.heron.api.Config to take a well typed object like ByteAmount and then swallow it into a string? Instead, why not fix the seconds use a new Duration object and move towards better typing, not away from it.. I've discussed with @nlu90 about this extensively. I still don't like the approach because I think that from a user's perspective this API:\nConfig conf = new o.a.s.Config();\n    com.twitter.heron.api.Config.setComponentRam(conf, \"word\", ByteAmount.fromMegabytes(500));\n    com.twitter.heron.api.Config.setComponentRam(conf, \"exclaim1\", ByteAmount.fromGigabytes(1));\nis a lot more clunky and less intuitive[1]  to a user than this API, which is the easier way to migrate to Heron IMO:\nConfig conf = new o.a.s.Config();\n    conf.setComponentRam(\"word\", ByteAmount.fromMegabytes(500));\n    conf.setComponentRam(\"exclaim1\", ByteAmount.fromGigabytes(1));\n1 - When migrating or writing my job, doing code inspection on the setters on o.a.s.Config is easier than somehow knowing that I need to now look up static setters on an otherwise unknown c.t.h.api.Config class.\nAlso, I think we have way too many config-related classes already and this approach does not help with consolidation. Also I generally don't like static helper methods, especially to do OO things like building a config.\nAlso if the reason for this refactor is to fix the issue that was described in #1841, we should fix our setters to not call toString on numerical values. As a result I'm still unconvinced why breaking inheritance is the best thing to do here.\nThat said I don't want to block this release so I'll state my opinions and step out of the way on this review.. The scp node needs to be available to all other nodes. The client uploads to it and all nodes download from it. If its going down, you will run into problems with this approach.\nPlease use the mailing list for questions instead of opening tickets. Closing this. Please continue the discussion on the list if you have additional questions.. Nothing's been decided about Trident at this point. This was the main feature that was blocking it, and after discussions with @mfu we realized it was easy to implement so I added it. There's a not a lot to supporting direct grouping, since it's very similar to custom grouping, so now that it's been developed we might as well add it IMO, regardless of Trident.. See additional discussion and an alternate approach in PR #1858.. @objmagic we're not changing any user-facing APIs, just the internal SystemConfig getters used by internal heron classes.. In the future please don't reorder unrelated methods in a feature PR, since it makes it hard to read the diff of functional changes.. @huijunw yes, I think it would be good for us to use Instant to represent points in time in a follow up PR.. The red font is also problematic because it's a visual cue without any indicator to what exactly. Nobody knows what red means.. @maosongfu thanks for solving that mystery. Unfortunately users of the UI don't know what the scheme is and if they did, it still wouldn't really be actionable to them.. @kramasamy yeah, a bunch. :) I've got others queued up and will submit in separate PRs, to keep them small.. Fixes #1859.. Follow-ip cleanup in #1878.. Ping for a review.. I broke out the CommunicatorTester from the SlaveTester since we have two use cases:\n\nThe test requires a Slave with Communicators that use a SlaveLooper. This is the case for the bolt/spout tests in this review.\n\nThe test requires only Communicators that use an NIOLooper. This is the case for all tests under heron/instance/tests/java/com/twitter/heron/network which I'll submit ion a follow up review. . Ping. Any takers on this review?. Thanks @objmagic. To make the review easier, it's really just centralizing copied logic in 2 new classes, SlaveTester and CommunicatorTester. The rest is just removing the code from the other classes that can now leverage them.. @objmagic it was being called once by the test and once by the after() method. I have no idea why, probably a bug.. Good call @objmagic, updated.. \ud83d\udc4d . How do you envision this check working? If the process launcher passes an invalid number of args the process fails. How could you know/validate ahead of time how many args are expected?. How would we determine the number of args that aurora will pass?. @objmagic it's easy to disable by using the default constructor. I recommend we create our benchmark topologies that way, but the tutorial topology we reference in the quickstart shouldn't drain the cpu.. Not yet, but I'm trying to reproduce. I hadn't seen this one in a while, so I thought it was ok.... Yeah, I saw that. I was also able to reproduce locally. I think there's a potential race condition with when the physical plan gets updated vs when the stream managers get restarted, but still investigating.. I suspect there are reasons why the SubmitterMain would want the entire path. Should we truncate it lower down in the call stack? Would this work in cases where the defn file isn't in the rood directory where the client is invoked? I'd expect the submitter needs the full path to the file right?. I agree it would be best to handle it centrally, but it should be in the java code of scheduler-core somewhere. The SubmitterMain and the rest of the scheduler-core code might need the full path. Also the contract SubmitterMain provides to it's callers is that of a full path, not the name of a file at some unknown location on the filesystem:\nOption topologyJar = Option.builder(\"f\")\n        .desc(\"Topology jar/pex file path\")\n        .longOpt(\"topology_bin\")\n        .hasArgs()\n        .argName(\"topology binary file\")\n        .required()\n        .build();\nThe python wrapper should be as thin as possible, just passing through the args passed to it in this case. Best to let the scheduler-core core use the local FQ path to determine and set the relative filename that the scheduler impl will find on the remote scheduler. . The submitter takes a number of (valid) local file paths and uses them to build configs to represent what will be the remote settings on the cluster. I recommend we stick with that pattern and do that transformation where we make the remote configs. That's here in SubmitterMain:\n```\n  protected static Config topologyConfigs(\n      String topologyPackage, String topologyBinaryFile, String topologyDefnFile,\n      TopologyAPI.Topology topology) {\n    PackageType packageType = PackageType.getPackageType(topologyBinaryFile);\nreturn Config.newBuilder()\n    .put(Key.TOPOLOGY_ID, topology.getId())\n    .put(Key.TOPOLOGY_NAME, topology.getName())\n    .put(Key.TOPOLOGY_DEFINITION_FILE, topologyDefnFile)\n    .put(Key.TOPOLOGY_PACKAGE_FILE, topologyPackage)\n    .put(Key.TOPOLOGY_BINARY_FILE, topologyBinaryFile)\n    .put(Key.TOPOLOGY_PACKAGE_TYPE, packageType)\n    .build();\n}\n```\nhttps://github.com/twitter/heron/blob/master/heron/scheduler-core/src/java/com/twitter/heron/scheduler/SubmitterMain.java#L81\n\n\nI don't love the inconsistency of mixing remote and local paths in topologyDefnFile, topologyPackage, topologyBinaryFile here as doing so has bit us in the past, but we can at least centralize the fix for this field here. When doing so we should remove all downstream FileUtils.getBaseName calls since we're changing the contract to where the impl already provides that now, which we should also document in Key.TOPOLOGY_BINARY_FILE.. Paired with @maosongfu and @nlu90 regarding the fix for this.\nIf we change TMaster to take a ZK watch on the packingplan node, it can update it internally immediately when it changes and then build a new physical plan to match it. That would negate the need for the scheduler to delete the physical plan and for executor0 to restart the TMaster. It also separates concerns better in that the scheduler controls the lifecycle of the packing plan and the TMaster controls the lifecycle of the physical plan.. I've got a full patch to handle the described logic on branch billg/tmaster_packing. Will break it up into smaller PRs for review.\nhttps://github.com/twitter/heron/compare/billg/tmaster_packing?expand=1. It won't double it since TMaster only runs on 1 node, so it would add 1:1 with the number of topologies we run, which is insignificant.\nWe use a watch on statemanger for the packing plan for a couple of reasons:\n- The executors need the packing plan and doing RPC with them isn't practical due to their lifecycle. Following the same patter for the TMaster is consistent.\n- The initial deployment should be able to proceed without a requirement that the deploying user (where the scheduler-as-a-library might run) maintains a connection to the topology for the entire startup phase. So the client puts some state in statemanager and if that succeeds it launches the aurora containers and it's done. From then on the topology can initialize itself. We don't want the production topology to have a dep on the users client, which scheduler<->tmaster RPC would require.. @srkukarni some schedulers might support different features than others w.r.t. the ability to support profiling, heap/thread dumping, etc. Ideally they would all support these things but we don't require that they all do currently.\nThat's a separate issue though than what I was referring to. I want to be able to step through code in a debugger in my IDE, which means being able to insert JVM args like -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005 into the java command for example.\nHere's how I've manually hacked into the heron_executor.py _get_java_instance_cmd method (in my working branch) to make a java instance start with these args:\nhttps://github.com/twitter/heron/pull/1807/files#diff-2a669e084143b86b5766a49245f8e357\n'-XX:+UseConcMarkSweepGC',\n                       '-XX:ParallelGCThreads=4',\n-                      '-Xloggc:log-files/gc.%s.log' % instance_id]\n+                      '-Xloggc:log-files/gc.%s.log' % instance_id.replace(\"$\", \"\")]\n+      if global_task_id == 3: # Used to enable debugging of a specific instance\n+        instance_cmd =\\\n+            instance_cmd + [\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=5005\"]\n+\n       instance_cmd = instance_cmd + self.instance_jvm_opts.split()\n       if component_name in self.component_jvm_opts:\n         instance_cmd = instance_cmd + self.component_jvm_opts[component_name].split()\nBeing able to pass an argument or probably to set an environment variable when submitting a topology to have this effect would be really helpful when developing/debugging in localmode.. @srkukarni I suspect we could inject the params with those settings, but we'd also need to support injecting them only on the specific instance that we want to debug, probably by instance id. We wouldn't want them to all start with debug options since a.) the user would then need to connect to each one to allow them to start; and b.) we'd have port collisions.\nAlso, this ticket captures adding debug support to the submitter, runtime manager, and the scheduler-as-a-library as well.. To attach a remote debugger (i.e., and IDE) to a java process for debugging, you must start the JVM with specific debug options. Hence the heron shell is not applicable here, we must actually start the instance processes with arguments injected.\nWhen a topology is submitted or updated, a number of java processes get started: SubmitterMain, RuntimeManagerMain and SchedulerMain to deploy/update the topology and then the actual HeronInstances that make up the topology. While developing or debugging locally, it's often convenient to attach a debugger to any of these java processes.\nThat's what this ticket is for, which is making it easy to do so without having to modify source code to inject the arguments. For example to attach a debugger to SchedulerMain running locally currently you need to modify the code in schedulerCommand.schedulerCommand(..). It would be great if instead I could set a local env variable that the Heron CLI picks up to make this happen.. That is correct. This is primarily for heron developers, like us. :). @maosongfu I haven't yet had the need to restart a running instance to add debug params. Instead I typically want to restart my local topology with debug flag enabled. Restarting and instance of a running topology also means you can't debug what happens the first time it's launched as part of a newly submitted topology.\nI suggest we support for sort of passed debug arg or env variable at submit time that can do this.. \ud83d\udc4d once CI passes.. Why do tuple batches need to be grouped by source task id? Since source task id is tracked at the tuple level, couldn't batches contain tuples from a mix of sources?. \ud83d\udc4d . This value needs to change in ResourceCompliantRRPacking:\nstatic final int DEFAULT_CONTAINER_PADDING_PERCENTAGE = 10;. +1. Ping. Any comments?\nThis approach still has some shortcomings which I suggest we iterate upon in follow-ups, but in it's current form it's is a big improvement over what we have now.. @srkukarni I see two previous comments of you, both of which I've addressed. What's the ZK node hierarchy comment?. Why is it a bad thing to fetch the packing plan at initialization before being master? This is the safe thing to do. If we become master and start taking requests without initializing the TMaster we're going to have lots of failed RPCs from the stream managers since absent_stmgrs_ won't be initialized. I'm not seeing the reasoning for why we'd want to do that.\nNote that this is a totally different concern than deleting the physical plan upon update, for which we should assure we're the master somehow.\n. @srkukarni and I discussed on slack and have a plan, which I'll refactor to:\n\nwe can address both of our concerns by fetching the initial packing plan before registering (when we start and it\u2019s null), but then registering the packing plan watcher only once we\u2019re tmaster. That way we can still front-load initialization, while guaranteeing that the packing plan watcher (which takes mutating actions on the topology as master during update) would only happen when it\u2019s master.. Port logic could be related to #1936 where we could potentially do something similar for debug ports.. \ud83d\udc4d . As far as I can tell the tunneling approach (for state manager and scheduler) has worked well. What's the shortcoming you've found?. Couldn't you set the tunnel host to localhost?. I don't think #3 is the right solution here, since it's intent is to specify multiple endpoints in the quorum for high availability and client-based load balancing. It's not to support multiple connection routes to the same cluster.\n\n@srkukarni in your proposed approach how is the case handled where the local connection address to be used is different than the address to be used remotely when on the cluster?\nHistorically handling file system paths differently locally vs remotely has been quite the challenge to do clearly, so it would be good if we could support this cleanly without a local vs. remote thing.\nIIRC the current approach is to fall back to tunneling if a direct connection can't be made and tunnel configs are set. Maybe we could expand upon this to support a configurable, ordered list of connection strategies. So in this case we could configure a.) direct connect remote url (e.g., the path when on the cluster); b.) direct connect localhost url (e.g., kubectl scenario). The default we have today would be a.) direct connect remote url; b.) tunnel approach.. Tunneling is used to connect to both the state manager and the tmaster from the client, so we should continue with consistency on that front. I think we should avoid the ambiguity of what happens locally and remotely when various combinations of tunnel configs, remote uris and local uris are all configured. Instead if we have a clear way to set up the connection strategy where the administrator explicitly lists each connection approach in order, it will be clear what the connection logic both locally and remotely.. Looking closer at the code we have two tunnel types currently: PORT_FORWARD and SOCKS_PROXY. What's needed here is to use an pre-existing port forward setup, instead of creating a new one. Could we add a 3rd tunnelType to represent that? That way an admin could configure that we should fall back on a preexisting tunnel.\nhttps://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/utils/NetworkUtils.java#L488. +1. There are checkstyle errors in CI FYI.. \ud83d\udc4d . This is blocked by #1936.. Updated per your suggestion @srkukarni with some minor tweeks. LMKWYT.. Why DO NOT MERGE in the title? Is this ready for review to be merged?. Yes, I should be able to review today.. Can we also create/copy unit tests?. This looks good. Can you merge this and then add more tests in another PR?. Nice refactor @nlu90. Mixing the test helpers with the other code bugged me too.\nOther changes I'd like for us to consider: 1.) removing the util package and pulling it's current sub-packages up to c.t.h.common. util is non-descriptive and unnecessary here. 2.) Removing misc (which is worse than util IMO) and finding it's current inhabitants better homes.. \ud83d\udc4d . @objmagic could you please provide a description explaining the motivation for the refactor?. @huijunw these are not system or component level configs as defined on that page. They're not set in heron_internal.yaml. These should be defaulted but overridable by the topology.. @srkukarni that dep already exists, so removing it should not be in scope for this issue, but this patch should not make it worse. Opened PR #2061 to remove dep on TypeUtils and #2062 to move ByteAmount into API.. @srkukarni which I'm saying it shouldn't do. :) That probably wasn't clear in my wording above.... Wrapping byte[] makes sense but does that break existing consumers that expect ByteBuffer?\nAlso could you add @Override annotations to these methods while you're here please. . \ud83d\udc4d . @huijunw before going to far down the path of working around this from the Heron side, could you check in with the Aurora team to see where they are towards fixing the root cause. They had previously planned to work on this is Q2.. I commented in #2084 but we should find and fix the root cause, since this is necessary to validate that the site builds and the links are valid.. We build the site in CI so we can verify that a.) the site build still works, and b.) all the site links are valid (via the link checker). We could try to do something smart and only do this if there is a change under website/, but we should have a site check as part of the build. That way someone can't push bad docs.\nWe should only publish the site when we cut a release.. Thanks @lucperkins. Yeah, two ideas come to mine: a second CI or maybe only selectively doing this check if changes under website/ are detected.. \ud83d\udc4d . Can we fix the broken links to the checker doesn't fail, and then enable it in the build? The idea is to catch bad edits before they're merged, so the person publishing the site doesn't have to deal it.. The errors are because these method references are not valid because there is no no-arg method signature:\n```\n/home/travis/build/twitter/heron/heron/api/src/java/com/twitter/heron/api/Config.java:312: error: reference not found\n\n@deprecated use {@link #setTopologyReliabilityMode()} instead.                    ^\n\n\n\n/home/travis/build/twitter/heron/heron/api/src/java/com/twitter/heron/api/Config.java:497: error: reference not found\n\n@deprecated use {@link #setTopologyReliabilityMode()} instead\n```\n\nInstead one of these must be used depending on which one we want to sent people to:\n```\n   * @deprecated use {@link #setTopologyReliabilityMode(TopologyReliabilityMode)} instead.\n   * @deprecated use {@link #setTopologyReliabilityMode(Map, TopologyReliabilityMode)} instead.\n```. One of the challenges with this is that Twitter's CI environment can't access the public internet. This can be done though by deploying a custom proxy rule. The challenge though is that the proxy must point to a valid URI path that's not a file. These files are examples of where we want to proxy:\nhttps://github.com/google/protobuf/archive/v3.3.0.tar.gz\nhttps://github.com/google/glog/archive/v0.3.5.tar.gz\nBut paths like https://github.com/google/protobuf/archive/are invalid and return 404s. @kramasamy do you know of another location where these can be pulled that actually mirrors a directory structure? Even path's like https://github.com/google/protobuf/tree/v3.3.0 can be proxied, but the artifacts don't live below that path anywhere.. @kramasamy we have the ability to upload artifacts to our internal repository and change WORKSPACE to point to them at build time. Could you submit a patch to migrate just one cpp library to use the http approach, maybe glog? We can then verify that patch internally before migrating the rest of them.. Before submitting/reviewing code for this could we discussion the motivation and proposed approach for this on the dev apache list, and share any design docs towards this effort. We should have better communication/collaboration before deciding to create a new core architectural component.. Yes, I will review. Apologies, a snafu in my email filters caused me to miss all github emails for 2 days.... Doing a sed to change to an internal URL as part of the build process is\nfine, we do that elsewhere already. What we don't want to do is have to\nmanually pull new tags. I suspect we can make this work with Mark's\nsuggested approach.\nOn Wed, Jul 26, 2017 at 6:45 PM Karthik Ramasamy notifications@github.com\nwrote:\n\n@kramasamy commented on this pull request.\nIn WORKSPACE\nhttps://github.com/twitter/heron/pull/2104#discussion_r129737919:\n\n@@ -466,3 +466,12 @@ maven_jar(\n   artifact = \"com.yahoo.pulsar:pulsar-client:jar:shaded:1.18\"\n )\n # end Pulsar Client\n+\n+# for pex rules\n+git_repository(\n+    name = \"io_bazel_rules_pex\",\n+    remote = \"https://github.com/streamlio/bazel_rules_pex.git\",\n\nmirror will not be sufficient since the remote repository URLs have to\nchange to internal twitter URLs.\n\u2014\nYou are receiving this because your review was requested.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/pull/2104#discussion_r129737919, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AABcztfnzRnHUuHzzxGTnTGP1uGtAbe7ks5sR-vUgaJpZM4OjLMm\n.\n-- \nSent from Gmail Mobile\n. @kramasamy right, that won't work. Could the pex code be published as a versioned tar? That way it could be downloaded and extracted instead.. This is failing now on master for me locally. I don't know why jenkins CI is not picking it up, but it seems to have started with sha f558fae6d9fa763b47458103477631d4735224b4 from #2103. @nlu90 could you look into that PR to see if that could be the cause?. Nevermind @nlu90 I found it: #2112. I'm concerned something is not right with CI. This test fails locally and on recent unmerged PRs (#2111), but not on other PRs merges since #2103 . We should not have 2 different ways to generate javadocs. Can you change this patch to replace the existing approach with this one?. Can we instead have one javadoc build for everything, and include that in the maven releases?. The website docs[1] state:\nJava 8 is required by Bazel but Heron does not use Java 8 features; Heron topologies can be written in Java 7 or above and all Heron jars are compatible with Java 7.\n\nWould you please update that to say that Heron requires java 8.\n1 - https://twitter.github.io/heron/docs/developers/compiling/compiling/. @aahmed-se have you signed the CLA?. \ud83d\udc4d  once CI passes. Via discussion on the apache mailing list, the code can be migrated to Apache gitbox without the SGA, and non-Apache releases can be still made a that point, so consensus was reached that we should move forward on this.\n@maosongfu we will need to coordinate this move so Twitter's CI can change the location of the git repo. When would be best to do this?. @maosongfu any comments on this? This is a blocker for the Apache migration.. Thanks @maosongfu. Just to clarify, this ticket is to move the code to apache git which can be done in parallel with legal working on the SGA. The latter doesn't block the former.. @maosongfu any update on this? What's blocking the migration of the code from your end?. Done.. When IRepacking was developed we made the decision not to implement it in RoundRobinPacking for a couple of reasons.\nThe first is that RRPacking chooses to take the requested resources as hints and obey the number of containers requested. This has led to unexpected behavior for users who increase the parallelism for a given component to (via changing settings and redeploying) and end up getting fewer resources per component instance. This resulted in hard-to-diagnose issues. The ResourceCompliantRRPacking classes were written to always give the resources the user expects/requests and to adjust the number of containers as needed. This approach seemed more ideal so the plan was to deprecate RRPacking.\nThe second reason is because conceptually there is a disconnect when implementing scaling of an algorithm that ignores requested resources in favor of a number of containers setting. When scaling, the algorithm has to dynamically figure our how many containers should be added/removed to a topology that previously had a fixed number of them. This implies a shift from container-based to resource-based sizing which is not easy to rationalize and is hence prone to misunderstanding or troubleshooting challenges. Or it means we just add/remove containers and the numContainers setting that was the source of truth during deploy becomes not so.\nI would recommend continuing with the plan to deprecate this class in favor of the resource-based ones.. Thanks for the clarification. Fair enough.. Looks like travis is failing for this PR.\nThe approach seems to be to regenerate the packing plan from scratch when doing the repacking. This means that if the right component is scaled, the entire topology set of topology instances will be shuffled. This can be a very disruptive approach. Have you considered other algorithms for deciding where to place new instances, or how to remove them that are less disruptive?\nIf you look at the other IRepacking algos there is a scoring framework utilized which allows the ability to decide if you want to scale for least disruption vs trying to remove containers vs homogeneity, etc. That could be leveraged here for consistency with the other IRepackings. . \ud83d\udc4d . Yes, see PR description. This section appears twice on the same page.\n. Better to have the script return the value than to set it as an environment var as a side effect.\n. There's a lot of XML being generated via a bash script. Could you instead use an XML template file where the script dynamically inserts/updates parts of it to produce the new XML?\n. This is a module file, not a project file. Could you also generate a heron project that includes the modules?\n. I didn't realize you could now just open a project by selecting the top level dir. No need for the ipr file for now then, those appear to be things of the past.\n. @kramasamy my point was not to commit a static XML file but to commit a XML template file that would could then be used dynamically to create the final XML based on what we get from bazel. If doing so would create a complicated mess though, we could punt on it for now.\n. Yes, output now looks prettier. :) \n$ CMAKE=/Applications/CMake.app/Contents/bin/cmake PYTHON2=/usr/bin/python2.7 ./bazel_configure.py\nUsing C compiler          : \"/usr/bin/gcc\"\nUsing C++ compiler        : \"/usr/bin/g++\"\nUsing C preprocessor      : \"/usr/bin/cpp\"\nUsing C++ preprocessor    : \"/usr/bin/cpp\"\nUsing linker              : \"/usr/bin/ld\"\nUsing Automake            : \"/opt/twitter_mde/homebrew/Cellar/automake/1.15/bin/automake\"\nUsing Autoconf            : \"/opt/twitter_mde/homebrew/Cellar/autoconf/2.69/bin/autoconf\"\nUsing Make                : \"/usr/bin/make\"\nUsing CMake               : \"/Applications/CMake.app/Contents/bin/cmake\"\nUsing Python2             : \"/System/Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7\"\nUsing archiver            : \"/usr/bin/libtool\"\nUsing coverage tool       : \"/usr/bin/gcov\"\ndwp                       :  not found, but ok\nUsing nm                  : \"/usr/bin/nm\"\nobjcopy                   :  not found, but ok\nobjdump                   :  not found, but ok\nUsing strip               : \"/usr/bin/strip\"\nWrote the environment exec file:    \"scripts/compile/env_exec.sh\"\n. Got it. Should I revert the commenting out that I did in the meantime? I did that so that the testing commands wouldn't fail.\n. I also wasn't a fan of this. Submitted a change to this in #323. How about we go with just 'Heron Architecture' instead?\n. I think the language is more inclusive if we don't use 'we' and 'us'.\n. heron-cli above should change to heron\n. This isn't the current version. See https://github.com/twitter/heron/tree/master/website. We've added sections about installing nvm and node first that we should keep. Or better yet, make that work in the make file.\n. let's put these in   tags so there's no risk of TODOs being published\n. These lines seem to be the only difference between compile.sh and compile-darwin.sh. Instead we could just have compile.sh with defaults set for these value, which darwin can override. That way we don't need to maintain two duplicate scripts.\n. I don't think the above is actually correct since you need to build the site from master and then push to gh-pages. Either way, let's hold off on adding the gh-pages docs for now, since our approach will be based on the git subtree technique.\n. if this fails, what does the output look like? I've found that I've needed to do some massaging of the csv output to make it easily consumable.\n. can this link be relative?\n. setup probably shouldn't just make the assets and not the pages. either it should create both, or maybe it doesn't make anything, it just sets up the environment?\n. else error out right? doesn't target need to be one of the above?\n. instead of wc to determine errors, could you inspect the status code of linkchecker? If it implements status codes correctly $? will be 0 upon success and 1 upon errors found.\n. pls merge from master, this file has changed.\n. You can omit CONFIG_PLATFORM= above on line 13 now. Bash is loose like that.\n. Since these values will be set on the jvm as -Dfoo=bar, we should use on of the choices with 'property' in it instead of arg. Args are what I'd expect to be passed directly to the main from the right of the classname.\n. There would be an equal sign after the flag:\n--topology-main-jvm-property=hello=world\nThis is the same pattern bazel uses with --jvmopt and pants uses with --jvm-run-jvm-options.\n. Why are these being removed?\n. The FooAsBar class naming is non-standard. Could we instead follow ImplInterface naming like, HttpServiceSchedulerClient?\n. Ugh, it is 2. I need to review the code style files that I pulled from science to see why they were set to 4.\n. for readability best to break the if statement and commands to multiple lines.\n. --extra-heron-classpath and it's intended usage is not clear. We should describe in the usage what is meant by extra, when it should be used and how it will be interpreted.\n. should describe or show what we're checking for to determine validity.\n. Please don't commit more jars. We're in the process of removing them all and using maven_jar instead. Instead add reef to blob/master/3rdparty/java/BUILD like checkstyle and reference jars via maven_jar in ./WORKSPACE.\n. Please build the new target with --experimental_action_listener=tools/java:compile_java to verify correct java styles. We're in the process of cleaning up the existing code style before enabling this by default, some it would be great to commit these new classes already conforming.\n. Sorry to nit, but then is typically on the same line, else's should not be, and indents should be 4:\nif [[ $? == 1 ]]; then\n  cut -sd ';' -f 1,2 linkchecker-out.csv | tr ';' ' ' | \\\n    awk '{ print $2 \" \" $1}' | sort -u >> linkchecker-errors.csv; \n  echo \"linkchecker failed - check linkchecker-errors.csv\"; \nelse\n  echo \"linkchecker passes\"; \nfi\n. All indents should be consistent at 2. You've got a mix of 4 and 8.\n. If this command is supposed to span 2 lines shouldn't there be a backslash at the end?\n. while you're in here could you clean up any previous idea files here so we can re-run on top of an previous project? I think that's just .idea and .iml.\n. intentional?\n. I meant whatever it takes to totally reset the users idea project. I thought that meant deleting .idea/ and .iml, but if the current process achieves the same, we're good.\n. Below, why do we have to dynamically insert static XML into .idea/compiler.xml? Could that not just be committed with the others?\n. If we need to catch and handle a 1 response I think we need to remove the set -e. I'm not sure how else to do that. For commands that should never fail we could use the run some command || die \"Failed to run some command\" approach.\n. +1 for particular_script || true if that lets us keep set -e.\n. Also, what does the +x do in ${HERON_TREE_STATUS+x}?\n. Typically an empty private method is used, since that will inherently prevent any other class from calling it.\n. @kramasamy that answer got only 5 votes. :)\n. This section doesn't apply\n. Let's put all empty sections with TODOs in   so they won't accidentally be published live.\n. +1 for more whole words in names instead of letter abbreviations.\n. If it's never used can we change the method to:\nsafeIncr(String counterName)\n. Casting one vote for an end. :)\n. We should either fix the bug, or refactor the method signature, since as-is it appears broken.\n. +1. for creating a issue and link to it in the code with a TODO\n. Checkstyle will complain about N so might as well change that to incrValue or something.\n. Typical Java convention is to do:\nthis.bolt = bolt;\n. Similar comment applies to other locations in the review:\nthis.inputTuple = inputTuple\n. In general you should log an error or throw an exception but not both. Doing both effects readability when troubleshooting. Same comment applies below.\n. this.myTaskId = myTaskId\n. Let me look into suppressing that rule in checkstyles.\n. Please create an Issue to fix these and link to it in the comments\n. I assume we can't do this with a filter? Would be great if we didn't need to modify the original code, but maybe that's our only option?\n. Good point. It took me a while to figure it out myself. Done.\n. Could we check the uname for ubuntu or darwin explicitly and handle each and throw an error if we find something else?\n. ugh, my bad. I've got --config=darwin in my .bazelrc.\n. Are these flags always required or only during development? If they're always required we should add these to tools/.bazelrc.\n. Let's remove slack from the headers. It will be by invite only to developers who are making contributions.\n. The link text should probably be \"Javadocs\".\n. @kramasamy I think we should avoid having a heron users slack channel. Having an email list provides better archiving and search. This will help users find answers via google, etc. If those discussions happen over chat the same topics will likely keep getting re-hashed.\n. I'm not following you. The link text (i.e. Doc) can be whatever we want, so why can't we change it?\n. I still see slack.\n. @kramasamy if we want to link to javadocs here, the link text should be something like Javadocs. Docs sounds like the heron documentation.\n@lewiskan just notices you're replacing the link the Getting Started page with a link to Javadocs. Why is that?\n. Will this succeed? I thought there were still checkstyles to be cleaned up per #540.\n. That target succeeds but this one produces many checkstyle errors:\nbazel build integration-test/...\nwhich includes the following targets:\n$ bazel query 'kind(\"java_library\", \"integration-test/...\")'\n//integration-test/src/java:local-integration-tests-lib\n//integration-test/src/java:integration-tests-lib\n//integration-test/src/java:common\n//integration-test/src/java:core\nHow come those aren't invoked as part of the integration test build? Are parts dead code?\n. That file also has checkstyles enabled just like tools/bazel.rc:\nhttps://github.com/twitter/heron/blob/master/tools/travis-ci/bazel.rc#L18\nSo if those targets aren't causing checkstyle failures, it implies they're not transitive deps of //integration-test/src/java:local-integration-tests_deploy.jar.\n. None of the src/java paths listed above exist.\n. I see. Let's add an ellipsis to make that clear. For example:\nsrc/java/.../(core, common)\n. there seems to be other targets under integration-test/src/java/... so we should probably build those too.\n. can this be done by a bazel run command?\n. We could also do this on line 17:\nbazel --bazelrc=tools/travis-ci/bazel.rc build integration-test/src/{python,java}/...\n. If we have to jump through a bunch of hoops to do this in bazel then we can skip it. The nice part about doing it in bazel is we abstract ourselves from where bazel puts it's output, but it's not a big deal.\n. For constructors and setters you can use the value, which is the convention. For all other methods you get the hidden field warning.\n. That's what I was hoping to do, but couldn't due to the storm and backtype TopologyContext class and how they use MetricsDelegate. It needs to know both the class that extends IMetric (T) and the type of metric (U), since it takes an IMetric delegate and returns the return type.\npublic class TopologyContext ... {\n  ...\n  public <T extends IMetric<U>, U> T registerMetric(String name,\n                                                    T metric,\n                                                    int timeBucketSizeInSecs) {\n    MetricDelegate<T, U> d = new MetricDelegate<>(metric);\n    delegate.registerMetric(name, d, timeBucketSizeInSecs);\n    return metric;\n  }\n```\npublic class MetricDelegate, U>\n    implements com.twitter.heron.api.metric.IMetric {\n  private T delegate;\npublic MetricDelegate(T delegate) {\n    this.delegate = delegate;\n  }\n@Override\n  public U getValueAndReset() {\n    return delegate.getValueAndReset();\n  }\n}\n``\n. We can't due to things likeMeanReducerwhich takes any kind ofNumberand returns aDouble`:\npublic class MeanReducer implements IReducer<MeanReducerState, Number, Double>\nWe could change that to only take Doubles I suppose. Should we try that route? \n. See my comment above on IMetricsRegister.java\n. See my command above on the other IReducer<T>.\n. If scripts/packages:heron-client-install.sh depends on integration-test/src/java:local-integration-tests_deploy.jar you shouldn't need to build first, you could just run, right?\n. Right, I see that now.\n:+1: \n. No need to link to the heron blog post from the heron docs. Also instead of \"To gain the observability benefits of Heron...\" maybe something like \"To observe Heron runtime metrics...\"\n. s/configure/do/g\n. Sections headings in these docs seem to have words capitalized:\nExporting Topology Metrics from Heron\n. s/there's much more data/much more data is/g\n. Please line break at around 100 here and elsewhere.\n. s/Your scripted dashboard/The scripted dashboard/g\n. s/your/the/g\n. \"an observability URL format\"\n. Should remove he #L32 from the url since that file will change. Better to specify the thing you expect them to find in the linked page.\n. The listed targets should be bullets\n. Yes. Generally speaking I don't have a strong preference one way or the other, but we should be consistent and the rest of the docs seem to use Title Case.\n. I think the listed targets would read better if we used bullets for each.\n. s/on/for/g and period instead of '!'\n. s/on/for/g and period instead of '!'\n. omit 'in github'\n. sed/Once you download a particular version of heron-client-install/and/g\n. omit 'you need to'\n. this doc will drift out of sync as newer verison of the cli are released. Can we instead include all this in the cli usage and refer users to that?\n. cluster, role and env should be each described in their own bullet.\n. s/environ/env/g here and below.\n. s/ould represent/represents/g\n. Instead of saying what it could be, say what it is. \"env represents the topologies environment (e.g., devel, staging, prod). \n. the doc refers to 'heron CLI', 'heron binary' and 'CLI'. Let's pick one of those and be consistent. \n. s/In order to/To/g\n. Instead of describing the args and flags for each of the commands in this doc and copy/pasting the usage info here, could we just describe in more detail the concepts behind the commands themselves, and refer the user to the usage to see specifics. This doc could drift out of sync with the evolution of the client.\n. I'm referring to the Common CLI Args section and the Optional CLI Flags section.\n. s/helps in understanding/describes/g\n. omit: your\n. A complete set of features can be found in the following sections.\n. s/enables a user/provided the ability/g\n. omit?: and other useful links and information, that are explained below.\n. Below is the home page...\n. The following\n. s/This/Below/g here and in other sections\n. I think we could omit the following since it's already clear to the reader: A complete list of information and actions is described below.\n. For 7 and 8 can we give more of an idea of what details? Clicking those actually changed the info highlighted below right?\n. I think there's more capitalization than we need in this list, like Info, Aggregated Metrics, Job, Viz. \n. Let's put all todos in   brackets so they don't inadvertently make it to the site.\n. s/Max/max/g\n. if they're not describe immediately below, we should link to the section.\n. lowercase logs, exceptions and job\n. lower case\n. if they're not describe immediately below, we should link to the section.\n. typo: informaion\n. amongst\n. s/know/find/s\n. what is 'this'? If it's the PID, let's call it that.\n. Switch to active, instructional tense. Instead of 'This runs', say 'To run...'. Same below\n. @saileshmittal yeah, something to guide the reader to learn more than 'click the button for details'. Maybe \"Clicking the foo button toggles the component detail view discussed in the [link to component detail view section]\", or whatever we call that view.\n. part of a topology\n. reducing debugging time considerably\n. typo: follwing\n. This wording is a bit unclear:\nClick on the circles for metrics to color components and instances by these metrics.\n. Can we remove this image of a command line and just make code block here with the command. That will make it easier to update. Same with dump.png, histo.png, jstack.png.\n. I see. It wasn't clear to me that the image shown was the output of the command being displayed on a page. I thought the user was being prompted to run that somewhere.\n. This is a quick way...\n. \"...should check $ERROR_FILE\"\n. This looks like a file copy and not a move from website/javadocs.sh\n. I see that now. Looks good. Mark made changes to this file yesterday so be sure to merge from master.\n. omit:  'to tell what the bolt should do'\nAlso, let's link to that interface in the Javadocs\n. Omit: But\n. Instead of copying and pasting the code, let's link to ExclamationBolt.java. If there are parts of the code or implementation that need commentary, we can include snippets here and discuss.\n. Instead of \"And when you actually develop topology in Java, \" we could say something like \"Instead of extending BaseRichBolt, you can implement IRichBolt directly.\"\n. \"Implementing a Bolt\" maybe?\n. \"Implementing a Spout\" maybe?\n. Omit: \" to tell what the spout should do.\"\n. Same comments as above. Can we just link the example code and only inline snippets requiring add'l explanation?\n. Remove TODO also right?\n. Reads netter to write in a way without colon: \"See ExclamationBolt for a simple bolt example.\"\n. \"Bolts must implement the IBolt interface.\"\nLink should be to javadoc\n. 2 space indents\n. Same comment as above re wording and link to javadoc\n. 2 space indents\n. same comment as above.\n. remove 'special'\n. Bolts\n. these should both also link to the javadocs\n. Spouts\n. these should both link to the javadocs\n. This should link to the javadoc instead of the java file\n. Have you verified that the build works with gulp uninstalled? I though npm required it when building static assets. If it is really not needed we should also remove it from README.md.\ncc/ @lucperkins \n. The doc convention seems to be cap-case:\nConfiguring a Cluster\n. omit: In order\n. \"...the classes required for the launcher, the scheduler,...\"\n. s/goes/go\n. s/It/This file\n. specifies the classes and configuration for the uploader, which uploads...\n. contains parameters that control how Heron behaves.\n. s/This/Tuning these parameters\n. where the run-time system and topology metrics will be routed\n. scribe sink and graphite sink are also supported.\n. which defaults to Round Robin.\n. ...of the heron client.\n. To run Heron topologies in a cluster, you must first deploy the following systems:\n. omit:  out of the box\n. The Heron state manager tracks the state off all deployed topologies.\n. The topology state\n. omit: for\n. omit: such as\n. to a sink\n. s/multiple/the following\n. Going through my notes, early on, the docs failed to build with the following, which is why I added that:\n```\n...\n/Users/billg/ws/git/heron/website\n\u2514\u2500\u252c gulp-aglio@0.0.10\n  \u2514\u2500\u252c aglio@2.2.0\n    \u2514\u2500\u252c protagonist@1.2.6\n      \u2514\u2500\u2500 nan@2.0.9\nmake: gulp: No such file or directory\nmake: *** [setup] Error 1\n```\nAt the fix was to do the following:\n$ npm install gulp\nThe Makefile has evolved since then so maybe that's why this is no longer required, but it's not totally clear to my why.\n. s/stand up/deploy\n. To deploy Heron, the Aurora cluster needs access to the Heron core binary,\n. To instruct Heron to use Aurora scheduler, modify ...\n. The following must be specified for each cluster:\n. instead of: Heron requires a scheduler to run its topologies.\nThe Heron scheduler schedules topology components to run.\n. The Heron uploader distributes the topology jars to the servers that run them.\n. First two sentences could be:\nThe UI provides the ability to find and explore topologies visually.\n. the DAG\n. It allows the ability to...\n. Period after etc.\n. s/instruct/configure\n. omit: This\n. s/You should set this/To configure the aurora launcher, set this to...\n. Generally we can omit things like \"This is used to specify\" or \"This config is used to indicate\" with just Specifies or Indicates. Less wordy.\n. After setting up ZooKeeper and generating...\n. Once the cli is available, topologies can be launched with the Aurora scheduler by...\n. s/needs to specify/specifies\n..., as well as the relationship...\n. Spouts and Bolts discuss how to implement a spouts and bolts, respectively.\n. s/we now start to compose the topology/a topology can be composed\n. methods should use fixed-width code font, here and below.\ns/is the one we composed previously/is the one previously composed\nplease line-break at 100 here and below. makes commenting and diffs easier.\ns/specifying the actual instance number/specifies the number of instances\n. omit (redundant): These two methods are used to specify the components needed in a topology.\n. In addition to the component specification, how to transmit Tuples between the components must also be specified. \n. If these words are CamelCase, it implies code, so we should format fixed-width.\nbased on a given field\nomit: And \n. s/User don't care about the grouping strategy/The grouping strategy is unspecified.\n. Is there a potential performance impact with GlobalGrouping? What if Bolt count is > 1?\n. User-defined\n. missing space between exclaim and bolt.\n. Once the components and grouping are specified, the topology can be built.\n. omit line numbers, which will change: #L39\nMore examples can be found in the examples package. \n. One of two state managers can be used for coordination when deploying locally:\n. The Heron cli is used to deploy using the local scheduler just as would be done using a distributed scheduler.\n. To configure Heron to use local scheduler, specify the following the following in scheduler.yaml:\n. Set to ${JAVA_HOME} to use the value set in the bash environment variable JAVA_HOME.\n. Omit: You should\n. Omit: You should\n. omit: out of the box.\nand below:\ns/You can also run Heron/Heron can also run\ns/stand up/deploy\nomit: you'll need to\n. Can we also say \"Experimental\" here for consistency?\n. See suggested rewording above for running with local scheduler and cli.\n. See wording suggestion above for local scheduler configs\n. omit: you should\n. omit: this\n. Set to ${JAVA_HOME} to use the value set in the bash environment variable JAVA_HOME.\n. s/This config is used to indicate/Indicates\n. s/You can change this file for/Edit this file to set\n. This script and scheduler.yaml must be included with the other cluster configuration files.\n. remove EXT_JARS here too\n. got it. fine as is then.\n. fix: the the\n. I see other docs using &mdash; instead of --. Should use that to be consistent.\n. s/it took/taken\n. Great to list these all out. Could you link each to the section below that discusses it?\n. omit Example if we don't have one.\n. Maybe above this line you can say something about how the following params filter the results. Then this line is \"The cluster the topology is running on\". Similar for environment.\n. the following\n. do we describe anywhere what a stream is versus a downstream bolt?\n. output streams, maybe?\n. a JSON representation of\n. Above we do this:\n```\nParameters\n\nfoo (required) -- ...\n```\n\nLet's be consistent here and below.\n. +1. Let's describe cluster similarly elsewhere. Same for env.\n. s/Each/All\nor make component singular.\n. dictionaries\nfollowing\n. What's the json key for each of these?\n. +1 for reusing this env description elsewhere\n. What is 'SchedulerLocation' in this context? If it's a json field let's fixed-width format it.\n\"...link to the job on the scheduler, for example the Aurora page for the job\"\n. In some cases the description is an action (Returns a JSON object representing...) and in others it's just a noun. Let's pick one and be consistent.\n. Should this be 'Release tag' or is it also the release version like release_version below?\n. Consistent with below?\nWhether the topology has a Topology Master location.\n. I think Viz is a twitter thing. maybe: Metric visualization UI URL for...\n. s/the JSON/a JSON object\n. omit: these are\n. If Tmaster location is a json field we should fixed-width format it.\ns/especially/including\n. Is number ID, or count? Let's change to be more descriptive. Same elsewhere.\n. Returns a JSON map of components of the topology to their respective metrics. To filter components returned use the component parameter discussed below.\n(why is component required if you say it can be omitted to see all? Also can component be repeated?)\n. Elsewhere in this doc you use TMaster (capital M). Let's be consistent.\n. 3 hours of minutely data, as well as cumulative values. If the ...\n. Same comment as above.\n. hours of minutely data, as well as cumulative all-time values. If the\n. period at end\n. same comment as above\n. s/show/include\n. all machines that a topology is currently running on\n. TMaster?\n. if a single instance id given, otherwise returns a multivariate time series.\n. A brief description of what the metric query language is and why you'd use it would be helpful.\n. s/one must be/argument is\n. s/there/included\n. typo: componet1\nAlso these should all use fixed-width font to show you're describing formula syntax\n. Would read more clearly to describe the syntax above with the element names and an example below, instead of mixing those:\nTS(componentName, instanceId, metricName)\nthen describe each field\nthen show an example.\nSame for other sections. As the formulas get more complex it becomes hard to decipher what's part of an example and what's required constant inputs.\n. No line break after Divide Operator. for consistency with other sections\n. End 1., 2. and 3. cases with either a period or a dash but not a mix. Same below\n. s/This page/Compiling\n. omit: these\n. s/To run Heron topologies in a cluster, you need the following/The following must be deployed to run Heron topologies in a cluster\n. Seems like if we're doing to link heron-cli in this sentence we should do so on the 'heron cli' text that comes earlier.\n. omit: But\n. omit: But\n. Other docs seem to use &mdash; instead of ---. Should we be consistent with &mdash;?\n. No links to Roadmap or Support anywhere?\n. In #710 @saileshmittal suggests we instead standardize on --- which is fine by me.\n. That's fine by me. This doc also contains both -- and ---. Is that intentional or should they all be ---.\n. @kramasamy thoughts about this one?\n. I'm not following you. Why should we do this:\n[working with topologies](../../../heron-cli)\ninstead of this:\n[heron cli](../../../heron-cli)\n. For these and other params we should consistently say required or optional like you do above. Or we only say when they're required and not saying anything implies optional.\n. Oh right! Forgot about that.\n. @saileshmittal thoughts on this?\n. This should be a link to the markdown page without the github stuff. [Getting Started](../getting-started).\n. s/Currently it is static/These settings are fixed\n. Heron configurations apply globally to a cluster. It is discouraged...\n. It is not possible to to override heron configurations for a...\n. why do we need 2 copies of the javadocs in multiple folders?\n. Just saw the same change in #798 and commented on it. Let's discuss there.\n. Please provide some human-speak comment that gives an overview of what this script does.\n. s/followings/the following\n. s/Those interfaces/The interfaces listed above\n. s/implementation of all/the implementations of the\n. I think you want to not include this in your PR\n. It's convenient to make these hrefs like http://localhost:8888. Same for below.\n. s/followings/following\n. no space before '.'\n. s/config/configurations\n. s/interface/interfaces\n. Lowercase 'right click'.\nAlso here and elsewhere best to line break these at 80 or 100 to make it easier to provide comments and read in an IDE.\n. As refactored, I think this reads well when just desribing how to tools work without saying what \"you need to do\". For the @objmagic suggestion to be consistent you could remove \"you can\" from the sentence and it reads well.\nAlso capitalize intellij as IntelliJ.\n. These seem redundant to what you say above and can be omitted IMO:\n\nWith a whole topology running in a single process, one could enjoy all free benefits it brings.\nFor example, one can run program in IDE and set breakpoints to examine the states of a topology, or profile the program to optimize it.\n. s/better/easily\n. \"... in a single JVM process, which is useful ...\"\n. Seems like Level.FINE logging should only be done when a verbose flag is passed. If there's general info logs that we want to show, could we instead change those to Level.INFO, instead of logging all of Level.FINE?\n. By # etc are you indicating there's other output not being shown? In that case we typically use ...\n. Instead of saying that it would be helpful to know it, let's note that the working dir is logged which can be useful for troubleshooting.\n. s/would/will\n. Should we call this section Troubleshooting? (cc/ @maosongfu ) Seems like this could grow into it's own page at some point...\n. Link to 'question 3'.\n\nAlso pls limit line lengths to 100 to make it easier to comment and read in an editor.\n. s/your/the\nalso please link to other steps 3 and 4\n. s/my/a\nhere and in the section names below. Best to speak generally about topologies and systems without 'my' and 'your' ownership.\n. compiling.md states minimum versions. let's refer to that to be specific about required gcc and glibc versions.\nFor libunwind, I see that's the output of the investigation below. Can we include this as a requirement somewhere more prominently, like compiling.md? (cc/ @maosongfu @kramasamy) Better for folks to find that out when setting up the system than when troubleshooting why it's broken.\n. omit 'we hope'\n. pls insert line breaks to make this error log more readable when shown on the webpage.\n. I think we can omit 'An issue is encountered:' since above you say your showing how to investigate a failure.\n. omit: 'So'\n. combine with previous paragraph\n. We should also refer users to the published artifacts in maven and include a pom.xml snippet for referencing artifacts that way:\nhttps://search.maven.org/#search|ga|1|heron\nSee related maven tickets:\nhttps://github.com/twitter/heron/issues?utf8=%E2%9C%93&q=is%3Aissue+publish+maven\n. We should recommend that they just add the dep to their pom file. Something like this:\nAlternatively you can add a dependency on heron-storm.jar to your projects pom.xml file:\n<dependency>\n    <groupId>com.twitter.heron</groupId>\n    <artifactId>heron-storm</artifactId>\n    <version>{{% heronVersion %}}</version>\n</dependency>\n. image path should just be /img/heron-ui.png\n. should replace /Users/${USERNAME} with $HOME. Or better yet, ~/ as used below.\n. s/setting up Heron/deploying topologies\n. Can we give this doc a more specific title? We have another called 'Topology Troubleshooting Guide' for example. We should give this one a name that distinguishes it from other troubleshooting guide and makes it's scope clear.\n. This sentence is a bit fragmented. How about:\nIf a component (e.g., TMaster or Stream Manager) has failed during runtime, visit the component's logs in ~/.herondata/topologies/{cluster}/{role}/{TopologyName}/log-files/.\n. Expert might be a bit strong a word. :)\nThe other one is called Troubleshooting Guide in the left nav, but 'Topology Troubleshooting Guide' on the page.\nhttp://twitter.github.io/heron/docs/developers/troubleshooting/\nThis one also seems like a Topology Troubleshooting Guide IMO, but if we want to have two guides, maybe this is Topology Deployment Troubleshooting Guide? @lucperkins do you have a suggestion here?\n. @lucperkins can we reference https://github.com/twitter/heron via params.github.home?\nhttps://github.com/twitter/heron/blob/master/website/config.yaml#L40\n. \"Other interfaces for simulator mode are:\" should be \" Other lifecycle methods to use with simulator mode are:\"\n. ...the main thread.\nWhat do you mean by \"All those interfaces\"? It's not clear what you're referring to here? Do you mean to say \"The Simulator class is thread safe.\"?\n. again, what are \"these interfaces\"? I see methods on a Simulator class discussed above, not interfaces.\n. \"...than the main thread.\"\n. s/your computer/the host\n. s/on your/in a\n. Open the /etc/hosts file...\n. Is rebooting really required for changing /etc/hosts? I typically haven't found that to be the case.\n. Good catch. If I could request one more edit please, we should replace \"for over 2 years\" with \"since 2014\".\n. modify the /etc/hosts file\n. darwin\n. darwin\n. on a Mac OS X\nSame for two other locations below.\n. We should test that the required build artifacts exist. If they don't, let's log output explaining what bazel targets need to be run and exit 1.\n. Can we make this relative to $HOME so it can also be run locally? Also space after %, here and 2 more locations below.\n. pls file an issue for this and add a link to it in the comment.\n. space after %, here and elsewhere\n. pls file an issue for this and add a link to it in the comment.\n. We should define these as constants and reuse here and below, which guarantees that what we check for and what we use stay in sync. e.g.,\nHTTP_SERVER=\"./bazel-bin/integration-test/src/python/http_server/http-server\"\n. Required file $file does not exist.\n. where is HERON_DIST set and instead can the tar be located relatively, like we do with server and runner?\n. Ideally, the runs a bazel build command, which produces heron-core.tar.gz somewhere and then runs this script, which knows how to find it at the that location, without requiring the user to know that location and set it as HERON_DIST.\nCould we set it to a reasonable default and allow the ability to override it, like this:\nHERON_DIST=${HERON_DIST:\u2212some/default/path}\nwhere some/default/path is the location under bazel-*/. that contains the dist?\n. This file is also generated by the build and should be included in required_files.\n. I've got two concerns:\n1. It's not clear who/what sets HERON_DIST and as you point out, you haven't found any code that does, so it's not clear to me why we use it in this script.\n2. This test is misleading, because the user is instructed to run bazel to build the project and then run this script to test, but the test doesn't use what the user just built with bazel. Instead it tests against a pre-installed version of heron in ~/.heron. This behavior is not clearly conveyed to the user and seems misleading, since they're not testing the code they just built.\nLet's discuss more in person if that's easier.\n. This block above seems to be the same as that in gperftools/BUILD. Any way we can share this and not duplicate logic?\n. I'm not that familiar with how this test framework works, but if we emit the following from the spouts:\nnew CustomObject(\"A\", 10)\nnew CustomObject(\"B\", 20)\nand then do an increment bolt, why would we expect to get just [10]?\n. if we're testing serialization are there other types besides String and Integer that we should verify?\n. can we commit this generically as heron-api.pom.template without placeholder tokens for version and include a bash script to generate the pom given a version? That way we don't need to edit, rename and recommit every time we release. Same for the other pom.\n. Can we make this a broader-scoped doc release-process.md that covers this but also:\n- how to update version\n- how to tag\n- test process\n- build process\n- maven process\nSome of those sections can be placeholders.\n. I question whether we need to publish this as a site doc. Maybe this could just be a markdown file under release/, since it's really only for our internal audience for now. We could move it to the site later.\n. does this work as a single command?\n+bazel build --config=${PLATFORM} {heron/...,scripts/packages:tarpkgs,integration-test/src/...}\n. we probably want to also do set -e to fail fast upon internal bash errors\n. So [10] is the number of records we expect? Seems we'd also want to assert the data itself if possible.\nHow is IntegrationTestSpout used here, I only see CustomSpout in use?\n. This is not longer true right?\n. s/Got/Received input tuple/\n. The steps should instead describe how to create the maven artifacts using bazel, as opposed to downloading the artifacts from elsewhere.\n. @kramasamy do we want/need to have your name and email in these poms?\n. please link to an issue in the comment\n. Could we use java java.util.Optional instead to indicate something hasn't been provided?\n. Working with maps of Strings to Maps of Strings to Longs can get confusing and could lead to bugs. We should consider using more strongly typed objects, like a Container that has a Collection of Instances, which each has requiredRam. It would make the code more readable and have better compile time checks. It would also allow Container and Instance to have methods of their own.\n. is allocation used in this method?\n. Please update to use {{% bazelVersion %}} here and below.\n. Please update to use {{% bazelVersion %}} here, above and below.\n. Please update to use {{% bazelVersion %}}.\n. instead of showing full urls, let's link the text here and elsewhere. So 'example' will be a link to the sonatype uri, instead of showing the uri.\n. instead of requiring the user to pass this, could we instead try to auto-detect it using uname?\n. Looks like CI is still failing.\n. Let's add comments here explaining why we need two copies of the javadocs and what each should be used for.\n. There's someone going on with the way that docs generation script is trying to build with bazel here that is making the protobuf gen fail. I suspect we need to change the bazel command that we execute somehow.\n. let's echo this and exit 1 if $1 isn't passed\n. dirname here can be just 'maven'\n. code format for these:\n{artifact-version}.jar\n. code format for jars\n. code format for jar\n. link to uri, not uri\n. link to releases, not uri\n. What is meant by \"Following sonatype.org\"? Please reword to be more clear.\n. ```\n$ docker run -i -t heron-ubuntu14.04 /bin/bash\nroot@45c2337fdea9:/# ls -l /etc/*-release\n-rw-r--r-- 1 root root 105 Feb 16 16:29 /etc/lsb-release\n-rw-r--r-- 1 root root 249 Feb 16 16:43 /etc/os-release\nroot@45c2337fdea9:/# cat /etc/os-release\nNAME=\"Ubuntu\"\nVERSION=\"14.04.4 LTS, Trusty Tahr\"\nID=ubuntu\nID_LIKE=debian\nPRETTY_NAME=\"Ubuntu 14.04.4 LTS\"\nVERSION_ID=\"14.04\"\nHOME_URL=\"http://www.ubuntu.com/\"\nSUPPORT_URL=\"http://help.ubuntu.com/\"\nBUG_REPORT_URL=\"http://bugs.launchpad.net/ubuntu/\"\n$ docker run -i -t heron-centos7 /bin/bash\n[root@ea2d85dbf2af /]# cat /etc/os-release\nNAME=\"CentOS Linux\"\nVERSION=\"7 (Core)\"\nID=\"centos\"\nID_LIKE=\"rhel fedora\"\nVERSION_ID=\"7\"\nPRETTY_NAME=\"CentOS Linux 7 (Core)\"\nANSI_COLOR=\"0;31\"\nCPE_NAME=\"cpe:/o:centos:centos:7\"\nHOME_URL=\"https://www.centos.org/\"\nBUG_REPORT_URL=\"https://bugs.centos.org/\"\nCENTOS_MANTISBT_PROJECT=\"CentOS-7\"\nCENTOS_MANTISBT_PROJECT_VERSION=\"7\"\nREDHAT_SUPPORT_PRODUCT=\"centos\"\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7\"\n```\nOn our internal centos hosts though, /etc/os-release doesn't exist but /etc/centos-release does, so we'd need to also check for that. Distro-detection would be a good thing to put in it's own scripts, since I suspect it might be useful to others as well.\n$ cat /etc/centos-release\nNest Enterprise Linux release 5.11 (Final) (Based on CentOS release 5.11)\n. If it's dead code without an immediate resurrection in view, I'd say we remove it.\n. Thanks. Let's add the issue into the comment in the code, here and elsewhere in this patch.\n. Yeah, looking at the code it seems we've got a lot of the building blocks. Builder pattern makes sense.\n. please keep deps alphabetized in this file\n. will this work when run from both the repo root dir (i.e. scripts/get_all_heron_paths.sh) and from within scripts/.? Same question below in scripts/run_integration_test.sh.\n. These all should all include artifact-name like {artifact-name}-{artifact-version}.jar\n. parens around (see example) and no leading '-'.\n. parens around (see example)\n. parens around (see example)\nperiod before 'note leading...'\nremove trailing paren after keys\n. s/Brief Description/Topology Submit Description/g\n. The following describes in more detail how a topology is submitted.\n. This flow is different for aurora FYI. SubmitterMain calls LaunchRunner which calls AuroraScheduler all on the client. In this way aurora is scheduled as a library on the client and there is no SchedulerMain running on the remote scheduler, as is the case with local scheduler or mesos. @maosongfu would you please confirm what I'm asserting?\n. I think it would be best to describe how schedulers can be invoked as a service or as a library directly, citing which schedulers use either approach. @maosongfu what do you think?\n. how come you removed the localhost link here but keep it below.. it's convenient to be able to click that when getting started. \n. can we instead patch linkchecker to ignore localhost URIs?\n. this seems like it would filter valid links to pages we want to check. Don't we want to filter out links to the tracker and heron ui on localhost?\n. I would argue that we shouldn't change logic in patches that are just style changes. I like the suggestion but think it should be done separately.\n. nit: s/entry/plan/g\n. also at a high level how the algo works.\n. In #3 and #4 Container is capitalized but other occurrences in the doc are now. We should be consistent.\n. omit: also\n. just a nit, but please line break at 100 chars to make commenting and reading in an IDE easier.\n. We should also link to the FirstFitDecreasingPacking javadocs for a detailed explanation of the settings and methods.\n. Is @bazel_tools defined globally somewhere and is this the proper way to reference the pkg targets we need? I see where @io_bazel_rules_pex is defined, but not @bazel_tools.\n. Can these rules get a better home either with the bazel or pex projects? This is fine for testing but we don't want heron to depend on an individual's git project.\nAlso this will fail in Twitters internal CI build since it's a closed environment without access to github, only a maven repo mirror and a pipy mirror. Could this code be distributed by either of those means somehow?\n. Agreed pypi would be awkward, but maven does have support for tar.gz artifacts. See http://stackoverflow.com/questions/32088885/install-tar-gz-file-in-maven-repository\n. if execution state doesn't exist we return true, which doesn't seem like what we want here no?\n. typo\n. please remove this and the 2 other (previously) copy/pasted comments that are erroneous.\n. open question, but do we need 'does'? in these methods (e.g., boolean schedulerLocationExists(..)).\n. Looking closer at the code I think this method would be better named validatePrecommandState or something along those lines. It's basically asserting that the command is ok to be executed based on alignment of client state with zookeeper. If executionState doesn't exist in ZK most commands probably should not be run (active, deactivate, restart). If we want to permit kill to run, then I suggest we change this method to also take the command and use that is an input.\ncc/ @maosongfu for his thoughts as well.\n. +1 to your first paragraph. We could limit this change to say that unless the command is a kill we should require valid system state. Kill should hard-kill everything and clean out all state.\nI don't think the 2nd paragraph suggestion would work, since AFAIK execution state needs to persist across process failures so ephemeral nodes wouldn't work. Those are better suited for state that truly should disappear when the process dies, like tmaster location.\n. yeah, this is a little awkward but we need pex_library since pex_test takes that as a dep, but can't test against a pex_binary. And both pex_library and pex_binary require srcs so we have to declare heron_executor.py in both.\n. I'm not a fan of util methods for things that do statefull operations on the system. IMO utils should be simple static helpers, like string or date helpers.\nIt's also good IMO to keep this as an instance method here to keep the logic together. If a new state method gets added to this class, it should be clear that this method also in this class needs to be updated. If cleanState were in some utility that could be lost.\n. let's explain in a comment why this is being done.\n. Was this print just for testing or do you want to ship this? If the latter it should be do_print and the print line should provide more context around what's being being printed.\n. I don't think we need a bash script just to effectively do this:\nbazel build --config=[config] heron/spi/src/java:heron-spi\nThe experimental_action_listener is registered for each platform so it should always run whenever building a java target.\n. could you add a description of what the EventLooper is and how it should be used? Same for GatewayLooper.\n. This code isn't thread safe. Is it not intended to be invoked by multiple threads? If not it should be documented as such. If so, these objects should be accesses in a thread safe manner.\n. Could you include an example of an expected file format here, to illustrate the parsing logic, as well as a unit test.\n. Could we use a Union type here with the enum discriminator? That would clarify which one is set.\nhttps://developers.google.com/protocol-buffers/docs/techniques#union\n. Or what I just saw that @maosongfu suggested, but either way we should have decent typing for what kind of object we're specifying via an enum or other means.\n. I think LOG.warn would be justified, since code shouldn't be calling delete on something that they don't expect to be set.\n. That's basically what I had started with when I refactored to heron_executor_core.py with just the main in heron-executor.py. At the time I felt it complicated working with the code so I was glad to consolidate back into just heron_executor.py so I think one source file is preferred over removing the dup srcs in the build targets.\n. Agreed, it's not pretty passing 32 args in there. That refactor can be handled separately.\n. See line 170 below. args[5] gets passed to a method that parses it and sets each of these:\nself.instance_distribution = {}\nself.stmgr_ids = []\nself.metricsmgr_ids = []\nself.heron_shell_ids = []\nOnce we add support for the state manager listener, we'd instead register the listener to call that method.\n. Just paired on this. Will change Language to ComponentObjectSpec.\n. let's clarify this comment to describe how what's set below, and it's meaning can be determined by the value of spec.\n. should clarify that it's a class for scheduling recurring tasks.\n. should this be private if only loop should call it?\n. should this method and the ones below be private?\n. Could this be done in the constructor? How come a separate method to set the map? \n. better to use descriptive member names than single letters.\n. The Heron Client is the term used to describe the heron command line client that end users use (i.e. the heron cli). This seems to be a multi-purpose networking client. Can we name this differently to avoid confusion?\n. could we instead set this in common_pex_arguments() so we don't have to set it in two places?\n. on a Mesos cluster\n. Setting up an Apache Mesos Cluster Locally\n. This whole paragraph can be omitted. Assume the docs ship with the version that has support and that people have that version checked out. You can just start the next paragraph with \"After compiling, ...\"\n. \"Mesos only offers a C++ interface, which is not portable across platforms.\"\nThe next sentence is redundant with the first of this section stating that heron needs to know the path, so we can omit. Also best to not talk about what Heron asks for, but to use a less conversational tone. \"Heron requires foo. Configure the foo by doing X\" for example.\n. ... to build and install the Heron client.\n. Instead of speaking about the future, speak as if it's already here and assume that these docs won't go live until after the code release.\n. omit: let's\ns/, Run:/./g\n. please keep line lengths < 100 to make commenting and reading in an ide easier.\n. s/You can look at the log file to see/The log file will show/g\n. s/you will see the error message/an error message will be included/g\ns/E.g. if the Heron can't find Mesos library in the given path, it will complain:/For example, if the Mesos library isn't found in the configured location, the following exception will occur./g\n. In general we want to speak more objectively without reference to 'you' and 'your'.\nAnother way to check that the topology is running is to look at the Mesos management console. If it was launched successfully, two containers will be running.\n. omit: They are containers of topology master and heron container.\nTo view the process logs, click the sandbox on the right side. The sandbox of the heron container is shown below.\n. The log-files directory includes the application and GC log of the processes running in this container.\n. The bolt log of the ExclamationTopology is container_1_exclaim1_1.log.0.\n. Install Heron tools to monitor the topology with the heron-ui, which required the heron-tracker. Configure ... before starting tracker. This configuration sets the location of the state manager root path.\n. Go to the UI to see the topology.\n. To see the metrics, click on the topology.\n. To enter the Mesos Management Console page, click the job button.\n. Do we need TOPOLOGY_ADJUST_NUMBER_CONTAINERS? We could remove it and just state that the contract of this IPacking instance is that it will adjust.\n. This should contain more detailed info to help the user understand what happened (e.g., how many were needed, how many were requested, etc).\n. This should include more info, like increasing from Z to Y for what reason.\n. Typically we don't put model objects in a util package. Could we put the elsewhere and only have static utility helpers under util? A class representing RamRequirement seems like a core object, not a utility.\n. 4 here and below appears to be a magic number. What does it represent and should it be a constant or a config setting with a default?\n. PackingPlan has an inner class Resource, which is a tuple of these 3 objects. Should we make that an outer class and leverage it here?\n. remove empty line\n. extra newline\n. extra newline here and above\n. do these 2 methods need to be protected?\n. This test should assert that a reasonable exception is thrown, instead of returning null\n. extra newline\n. why +1 and *10 and /100 here and elsewhere? could those become variables that implies what they are? same for other tests\n. extra newline here and elsewhere in this class\n. How is testContainerRequestedResources2 different than testContainerRequestedResources? Could we name them as such or include a description in the javadocs?\n. s/to have Mesos installed and running/to install and run Mesos/\ns/make sure you can access/access the/\nAlso please limit line lenghts here and elsewhere to 100 chars to make commenting and reading in an IDE easier.\n. s/You can modify/Modify/\nomit: please\n. s/your Mesos/the Meso install/\n. Mesos only offers a C++ interface,\n. s/to test if everything works right/using the following command/\ns/, add/. Add/\n. The following will be displayed upon a successful submit.\n. s/it's/the topology/\ns/, to check that go the/. To verify check the\n. Start tracker and the UI\n. Go to the UI at http:...\n. For consistency and clarity should change these to numContainers and numAdjustments. noAdjustments sounds like a boolean.\n. what does returning null mean? If one of the components isn't valid, no RamRequirements are returned for any of them? Should clarify the null semantics.\n. should clarify the semantics of what returning null means, which is unclear. It seems to happen if either ramRequirements are null (which appears to be a valid case) as well as if adjustNumberOfContainers if false, which appears to be a severe case.\n. let make this error message as descriptive as possible with things like the values that triggered the exception.\n. should rename index and j to something more descriptive\n. could you either name testInsufficientContainers[1|2|3] more descriptively or clarify in their javadocs what makes them different.\n. It seems this pattern of assertion for cpu/ram/disk is repeated in multiple tests. Could these be broken out into descriptive methods like assertCpu(...) that are reused?\n. The leading _ syntax is one I have not seen. Why do we need it?\n. This method has side-effects in that it modifies the config passed to it, which is not great. I understand the convenience of doing it, but I think it would be better either have the caller do it, or maybe just document this side effect in the javadocs.\n. can you add unit tests for this class please.\n. This is a side effect hidden to the caller. Could you document this effect in the relevant public javadocs.\n. What's the rationale for moving onScheduleAsLibrary from SchedulerUtils to LauncherUtils?\n. adding a new required field will make parsing of any existing protos break. Does StreamSchema get persisted anywhere across topology runs where this would break things or is it only used within a running topology?\n. sounds good.\n. I'm generally not a fan of static methods for these reasons, as well as difficulty overriding. I would recommend using non-static methods and if they need to be visible for unit testing, making them package private and annotating with @VisibleForTesting.\n. Makes sense.\n. I think it would be reasonable to remove it and sometimes get more containers. In that way the container setting would also be considered a hint.\n. If these objects are just created and passed around by a running topology, that's fine to do something non-backward compatible since they just update upon redeploy. If they get persisted while running and accessed by another service, like the heron UI, then they need to be backwards compatible so the service can handle both the old and the new during the transition. I'm not sure if that's the case or not for these objects though.\n. I think it's fine to put these classes directly under c.t.heron.packing if there common objects leveraged by different packing algos.\n. Got it. I thought they were similar except for the constants. The logic behind the math in the assertions is not entirely obvious so if there's something that could be done to help clarify the equation to the reader that would help.\n. yes please.\n. yes please.\n. If this is due to invalid configs, throwing an exception seems preferred over returning null.\n. Why create an inner static class inside another one instead of just making createPackingPlan non-static in the parent class? That would seem more straight forward, and would allow subclassing.\n. Yes, I see that now. Sorry, I misread the first time. Please disregard.\n. If this exception were thrown the reader would have little info about what went wrong. Instead could getRAMInstances throw a more detailed exception instead of returning null to this method, which then throws an exception? Best to throw as soon as an erroneous condition is found, which in this case is within getRAMInstances.\n. Can we replace 7 with totalInstances? The more we clearly define the integers users, the easier it is to understand.\nint totalInstances = spoutParallelism + boltParallelism\nSame below.\n. Instead of 4 and 3 could we use spoutParallelism and boltParallelism here (and elsewhere) respectively. Ideally we define ints with a member variable that states what it represents and then we only use the members, not the ints.\n. This is a bit of a nit, but we divide by 100 for the padding right? In that case whenever we do that could we write this as padding / 100 to make it more clear why the 100 exists?\n. 2 here is numContainers right? if so can we replace it?\n. The 4s and 3s here appear to be spoutParallelism and boltParallelism. Is so can we replace them? Same for below and in testContainerRequestedResourcesTwoContainers.\n. This is marked as @VisibleForTesting but used by LaunchRunner. Instead of a public static instance, could you change this to a static factory method without the annotation?\n. Instead of setting a static mock object here could you instead set the mock object as an instance object in the class that needs this?\n. Can we remove this public static instance entirely and just have classes create an object with the default constructor? That seems better than sharing static instances. And for the unit tests, you could just set a mock instance on the class that needs a LauncherUtils object.\n. Access isn't needed outside of this class and won't be when I add a serde test, since I was going to add it to this class as well.\n. Yes, it's still used in the ContainerPlan and the PackingPlan objects below. I just changed to a resource for each instance that was unique, so I could assert on that.\n. That's the current impl, but we might need to rethink this. For aurora if we remove a container that's not at the end, I'm not sure what the container ids would do and there are places where a continuous set of ids is expected so it feels safer to remove from the end. But it is presumptuous to say the last ones should go.\nWhen doing the packing diff we could pull out the container ids that are now empty and pass those to removeContainers? We'd need to make sure there isn't a mess if we remove containers not at the end though.\n. Yes, this seems reasonable. Aurora would need to ignore the Container info though which is unfortunate, since it can only clone.\n. @ashvina could you make a PR against my branch?\n. I agree that we should strive towards atomicity. The change described will we a considerable refactorting effecting much of the non-scaling functionality (i.e., distribution/update of physical plan, topology, etc during submit) so I'm inclined to ship an initial version of scaling that is weak transitionally but keeps submit as-is and then follow up with a refactoring to harden things from a transactional aspect.\n. Yes. See the changes to the heron_executor.py in this branch. On startup instead of doing:\nself.update_instance_distribution(self.parse_instance_distribution(args[5]))\nwe do:\nexecutor.register_packing_plan_watcher(executor)\nwhich ignores the passed param for both submit/startup and update.\n. Good call, will update. This code path will only happen if someone adds a new command to the parser, but doesn't register a runner, so it should get caught in testing, but with the logging it will help pinpoint where the change was missed.\nAn even better approach would be to somehow have these runners registered as actions with the parser so we don't need this map.\n. Yeah, that makes our invocation a bit more fancy, but we still need to maintain both the commands in the parser and the runners dict. I don't think that's a huge deal, but maybe a yak-shave for another day...\n. I had no idea...\n. Yes. This class is basically a protobuf version of c.t.h.spi.packing.PackingPlan.\n. Yes. See c.t.h.spi.packing.ContainerPlan.\n. This is send from the RuntimeManagerRunner to the Scheduler to invoke the scaling event. \nLook at the rest of scheduler.proto to get a sense for how request/responses are implemented. Each request has it's own type, but all responses use the common SchedulerResponse that includes a status code indicating success.\n. I've gone back and forth on this in the past. In other projects I've implemented both in one class before finding that most clients rely on one and not the other, and sometimes each has different deps. Hence I implemented them separately, which also keeps the class size small and easy to read.\n. Are you referring to the doMain functionality? That is integrated into both the local and zookeeper subclasses of FileSystemStateManager, so it is available in all of our state managers.\n. Good catch. That methods actually not used. Removing it.\n. What we have here is only intended for developers while troubleshooting, not end users per se, so I don't see a need at this point to create a distributable tool. End users would have a higher level abstraction in the heron-explorer. \nThe command to run it currently is already pretty simple:\nlocal:\nbazel run heron/statemgrs/src/java:localfs-statemgr-unshaded -- ExclamationTopology\nzk:\nbazel run heron/statemgrs/src/java:zookeeper-statemgr-unshaded -- ExclamationTopology my.zookeeper.host.com \nWe could make a wrapper bash script I suppose. I don't see the need for a python version, since it would just do the same thing as the java. @kramasamy let me know what you think.\n. typo: debs\n. There are more changes in this class than required for a new initialize signature. Is that because you're refactoring to make things easier when implementing IRepacking or are these functional changes?\n. return Integer.toString(index); is more idiomatic.\n. Can this not be set by the script somehow, maybe in the project file? IntelliJ supports something like that.\n. IMO if a test is failing more than 5 times it's either a bad test or our test framework is too brittle. The fact that we have to retry at all is suspect.\n. The meaning of the return Boolean in ambiguous. Please define it in the javadocs.\nAlso most of the javadocs within this method are previously copy/pasted and invalid and could even be removed. I'd say we remove them all since they don't seem helpful.\n. how come we return false for execution state and topology but not the others?\n. What is meant by \"Set topology def at last\" here?\n. Let's include more context here. Like \"deleteNode called on  but...\"\n. We also now have PackgingPlan in the state manager so please clean that up too.\n. I think this is fine for KILL. It should be a blunt tool that cleans out any possible state it finds of the topology. It shouldn't fail when one part is missing and then leave behind another.\n. We can eliminate a lot of dup lines of with a method like this:\nprivate void logDeleteStatus(Boolean success, String name) {\n  if (result == null || !result) {\n      LOG.warning(\"Failed to clear \" + name);\n  }\n}\n...\nlogDeleteStatus(deleteTMasterLocation(topologyName), \"TMaster location\");\nlogDeleteStatus(...\n. Yeah, I think this are ok as info. They happen rarely and we typically want to know what's going on with this process restarting part.\n. It appears like you're returning null here and above to mask an exceptional condition. Could you instead throw an exception?\n. this seems to be an exceptional case. can we throw an exception instead of returning null?\n. Are you using the boolean as a substitute for an exception here? If so, we should throw an exception if things that are expected to execute fail to do so.\n. Is this an exceptional condition?\n. Ugh. Ah yes, the code is riddled with these unfortunate booleans where each callers in the stack checks for false and logs an error and returns false. It's really a poor man's exception throw IMO.\nMy thought on this is to not propagate this pattern more than we need to. We should implement the boolean return type when the interface demands it, but otherwise we should write new code to expect success and throw an exception on failure. What do you think?\n. My dislike of nulls (one of them) is that they're so ambiguous in meaning. It's much more expressive to throw something like a a ContainerAllocationException, which could be extended with stats codes, reasons, response messages, call stacks, etc. Returning a null basically swallows all contextual information of what happened below, leaving the caller to just log an ambiguous message and move on. \n. If this is a condition that should not ever happen, that's more of a reason IMO to throw an verbose exception than to log and return null and make all other callers above have to check for null. The cleanest implementation in my opinion is to annotate methods with @NotNull and they either do what's expected or they throw. The calling code never needs to check for null and NPEs won't ever occur.\n. If this value might not be found under normal circumstances, could we use Optional instead?\nhttp://www.oracle.com/technetwork/articles/java/java8-optional-2175753.html\n. We can use guava's, which I've already introduced here:\nhttps://github.com/twitter/heron/pull/1218/files#diff-c7b1c2644302d7242028db94c6ea727cR45\n. I tend to think that map like lookup functionality is an ideal place to return an Optional (guava's), regardless of whether we're using other Java 8 features or not.\n. Please append the topologyName to the log message.\n. FWIW, I just added a helper for making test PackingPlans, which I plan to put up for review soon.\nhttps://github.com/twitter/heron/pull/1218/commits/9a83804befd982160b0712e12eeec0f44b57d283#diff-e7d399afac5f08437a091d2116ddadf1R31\n. We want to guard against new locations being added that are not included in this block. Now that we have the enum FileSystemStateManager.StateLocation we could iterate over all enums with a case statement for deleting each. If the case statement misses we could throw an exception, which would let us know that a new one was added but not included.\nIf we do that though, we might want to push this method into the IStateManager interface though. Thoughts?\n. Actually, we could add delete to the enum so it really won't get missed:\npublic void delete(IStateManager stateManager, String topology) {\n      switch(this) {\n        case TMASTER_LOCATION: stateManager.deleteTMasterLocation(topology);\n        case ....\n      }\n    }\nthen we could do things like this:\nfor (StateLocation location : StateLocation.values()) {\n  location.delete(stateManager, topologyName)\n}\n. I've included the change in #1321, which I suspect could merge today or tomorrow. It's up to you, you could either wait and refactor this PR or do it as a followup.\n. I think a best practices doc could be good. We're already starting with some of those in #1311. Generally I think it's good to include identifying info in the log messages, in case this is logged in a shared env or from a suite of unit or integration tests. That said, some discretion is required, since I could also see that practice becoming overkill. \n. The caller of this code always seems to log when isPresent is false so let's not double log here. Instead this method could just return Optional.fromNullable(workerMap.get(workerId)) inside the sync block.\n. can we also use Optional in lookupByEvaluatorId(..)?\n. Not really, since nothing disallows someone from adding a bunch of containers and then removing them. I did add a javadoc though to removeContainers to clarify that containersToRemove must be a subset of existing.\nAdded @VisibleForTesting.\n. That map is initialized in the constructor and never exposed outside the class so making it immutable isn't really necessary. If there was a getter to return it, I would agree.\n. Yeah, I was thinking about that. Updated to copy into an ImmutableSet during initialization.\n. I was also mulling over that inconsistency. We first implemented as this:\nvoid removeContainers(int existingContainerCount, int numContainersToRemove);\nand for the Aurora impl we could do the math to determine the ids last of the containers to be removed. But Yarn needed to know specifically which ones to remove, so we changed it to the current signature. There was also some thinking that we could assert that existingContainers passed was the same as the implementation was already managing, as a type of guard against race conditions of multiple edits.\nAs I think more about it I don't think either of those reasons are totally valid and we should probably remove Set existingContainers from the signature. @ashvina thoughts on that?\n. The intent was to say that this is a Scheduler that's scalable, but I see your point. It seems like we could do one of these two options:\n1. IScalable with the add/remove methods\n2. IScalableScheduler that extends IScheduler and includes the add/remove methods.\nThoughts?\n. Sounds good. I'll remove existingContainers from the signature. And we can look into packing plans ids as a means for transactions.\n. Not all schedulers might support the ability to add/remove containers dynamically, which is why I created an interface for those that do. The 'onUpdate' method delegates to the UpdateTopologyManager with an Optional<ScalableScheduler>. If it is not present instance scaling is supported but not container scaling. If it is present both are.\nWhat if we go with the IScalable approach above for now and then we revisit in the next review where we introduce the UpdateTopologyManager implementation?\n. This will be reached if someone adds a new Enum without updating the case statement. Please throw an IllegalArguementException (?) here. Bonus points for a way to catch such a condition at compile time, which I can't think of atm.\n. My suggestion was to also move the loop into the enum in a deleteAll() method that then delegates to delete().\n. Sounds good to me. Updated.\n. If we have delete state that takes an enum then each implementer will need a switch on the enum, and will need to updated when a new location is introduced. By putting the switch inside the enum as a helper, than only that enum needs to be changes when the a new deleteFoo is added to the interface/enum, so that seems more ideal to me.\nThe idea is to minimize the number of not-always-known places that need to be updated when a new IStateManager method is introduced.\n. Log.warn seems fine here, but I typically like to prepend some text to make it clear that this message is from the metrics manager. \"Received the following response from the metrics manager: ...\"\nSame for metricstimeline.py below.\n. are not available\n. This was for debugging, removed.\n. Yes, this was just for an easy way to manually check a file besides those in the unit tests.\n. There are separate interfaces for each (IPacking and IRepacking) so implementors can implement one or the other. An IRepacking impl should not assume anything about what IPacking impl was used at submit.\n. Typically the launcher (or the TMaster in the case of physical plan) initially writes object and RuntimeManager deletes them on kill. That pattern is somewhat standard.\nWhere we're deviating is with the Scheduler updating PackingPlan and Topology and deleting PhysicalPlan (TMaster will re-create it). I think this is ok for this iteration, but we might want to consider messaging the TMaster to do these operations as a future enhancement.\n. Both good catches. Fixed.\n. Fixed. Let's go with this change and cancel #1348 to keep the relevant updates together.\n. They will differ if they somehow get our of sync, like during a race when multiple agents are trying to update packing at the same time. This is a poor version of checking though. I'd like to keep this in place for now, but follow up with real transnational support.\n. updateComponent will return false under normal circumstances (see the updateComponent javadocs). Because in the protobuf world Bolt and Spout are both Components, but without inheritance, we have to try to update each to see if we have a match. We use true/false so if we get a match on bolt we don't try again on spout.\n. Updating the protobuf Topology object is indeed clunky, due to the protobuf api, but I thing it's safer and easier (and more efficient) than going through a deserialization/serialization process, and the risk of ending up with a different Topology object than we started with. It's fairly large with nested config hanging from all components.\n. Great catch. Updating now to use container ids.\n. Fixed.\n. Resource is still used in the ContainerPlan, but no longer in the PackingPlan.\n. my bad, fixed.\n. fixed\n. Yes. My thought was that if they're not in sync, something is happening that we're unable to reason so unexpected results could occur if we continue. I could be convinced to remove it though, just sharing my thinking when implementing.\n. I've removed the check and filed #1353.\n. good catch, fixed.\n. If I understand correctly, placeInstances adds new containers beyond numContainers. Is that correct? If so can we make the signature and javadocs for placeInstances reflect that, because currently the method description and numContainers field imply that the method places instances within numContainers not beyond.\n. We should add a comment to clarify that this merge approach only works because the ffdAllocation generated above is done by appending new instances into new containers so there is no container intersection in the merge.\n. For a large topology this could be a lot of output on the user console. We could keep it for now, but could you sort the container plans by container id? In #1360 I've added a comparator for unit tests, which we might want to pull into src. (Or we implement Comparable for ContainerPlan and all of it's child members)\n. should we also be asserting an expected container size (ram), here and above?\n. good catch, fixed.\n. What if it has the attribute but the value is None? Should we be doing this?\nif hasattr(self, 'tunnel') and self.tunnel:\nor is that redundant? Not sure if hasattr also covers self.tunnel not None\n. No, it's actually no longer needed. It was only used to be parsed by the packing classes and the executor. Now that it's contents are part of InstanceId, it's no longer needed. There is still a convenient task identifier generated and used by the executor to simplify things like log and pid files and to avoid naming collisions, but that's it. From the executor:\ninstance_id = \"container_%s_%s_%s\" % (str(self.shard), component_name, str(global_task_id))\n. We could use container id 3 here instead of 2 to assert that it's ok to not be consecutive.\n. If we're not already doing so, we should assert proper container ids as well as total container count.\n. If we choose to implement Comparable we must assure that if thisContainer.compareTo(thatContainer) == 0 then thisContainer.equals(thatContainer), which isn't currently the case. It implies implementing Comparable for all child objects and including them in the comparison.\nSee http://docs.oracle.com/javase/8/docs/api/java/lang/Comparable.html#compareTo-T-\n. Cleaner to use PackingTestUtils.testContainerPlan(..) repeatedly to construct an Array<ContainerPlan>.\n. Best to work with ContainerPlan[] here instead of Object[].\n. The output of the assertion classes on failure expect the param ordering to be [expectedValue, foundValue].\n. Please use Optional instead of null. See https://github.com/twitter/heron/issues/1311\n. A given container can have more than one of a given component. Also could we change the method name to something with find or get in it. Something like:\npublic Set<InstanceId> findComponentInstances(String component) {\n. Please don't return nulls to imply multiple scenarios. See #1311\n. Do we need to return null to imply something. If this component should always exist we could throw an exception if it doesn't. If instead the component might not, let's return void. Callers that need to know if the component exists can call another method for that info. Also a container could have more than one of a given component.\n. Shouldn't this throw the exception and never let you proceed?\n. since this is a different condition it should be in another test. You could make a shared doTest method that just takes a map of component changes and each test method calls it with a different map.\n. why does this test remove the first container? is it because container 1 has the 4 spouts being removed and if so, how did we make that the case? We should clarify since it's not clear why that's the case by reading the code.\n. For this test and others we should also assert the correct expected bolt and spout sizes before we scale, to make sure we're starting with the correct assumptions.\n. I feel like we're stretching our use of Lists and Maps in the signature here, by passing a mutable list of containers and a map of containers to instance lists. Instead can we use an object that encapsulated all of that? Like a PackingPlan object or a variant of it? Some container object that is able to effectively mutate a plan? Maybe a Builder that takes a packing plan, allows update methods to be called to change internal state, and then produces a new plan with a build() method?\n. Can we use Pair instead? https://github.com/twitter/heron/blob/master/heron/common/src/java/com/twitter/heron/common/basics/Pair.java#L24\n. Please use Optional instead of null. Or make this method return void if the caller doesn't care either way. See #1311\n. removeEmpyContainers(..)?\n. Instead of iterating over entries it's more idiomatic (and reads easier IMO since it's always obvious what the key and value represents by their naming) to iterate over keys:\nIterator<Integer> containerIds = allocation.keySet().iterator();\nwhile (containerIds.hasNext() {\n  Integer containerId = containerIds.next();\n  if (allocation.get(containerId) == null)\n   allocation.remove(containerId);\n...\nOr even better using for (Integer containerId : allocation.keySet()) if that doesn't cause issues during deletion.\n. An instance id with resources is an InstancePlan. Could we instead take an InstancePlan here and keep a Set of those? That way we don't need to separately track the sum of all resources for each add/remove.\nIf we do that we also don't need to pass a different resource to the remove method than the one that we know is bound to the instance. Currently the two aren't bound to each other, but there seems to be an assumption that the one passed is the same as that of the instance found and removed.\n. When accessing class member variables, best to use self.usedRam here and elsewhere.\n. Instead of string concatenation, better to use String.format(\"%s %d %d\", ...) \n. The first 7 lines here are duplicated in 5 tests. Could you pull those out into an initScalingTests() method or maybe something like this, followed by assertions on the returned plan:\nPackingPlan doScalingTest(Map<String, Integer> componentChanges) {}\n. Could you add back the PackingPlanTest.testContainerSorting from the earlier review as PackingUtilsTest.testContainerSorting?\n. Could you pass a stateManager instead of a runtime here? Better to not bury the stateManager initialization code.\nActually, if PackingPlan is updateable in ZK why is this method needed? Couldn't we just need to do thisL:\nstateManager.setPackingPlan(serializer.toProto(updatedPackingPlan), topologyName)\n. Please add a unit test for this method.\n. Feel free to disregard this comment, as we don't follow this standard in many places.\n. Can we name this after the method it's testing, like testHomogenizedContainerPlan?\n. I'm confused, shouldn't each of these be the same size (i.e. homogeneous)?\n. I see it now. I was searching for the method string and missed it.\n. Good eye. I suspect it was implemented like that in UTM initially because setPackingPlan was setting overwrite to false initially in LocalFileSystemStateManager. I don't think it should be doing a delete there as well.\n. Correct, we shouldn't need to delete first.\n. +1\nHaving some notion of the test method in the name (when possible) helps to clarify the scope of what the test if intended to verify. I find this useful for tests of limited scope, which I guess most should be ideally.\n. Right, but large and smallContainer here are taken from containerArray which is taken form newPlan, which is supposed to be homogeneous right? So I'd expect newPlan to have same size containers, not large and small. Am I misunderstanding the expected behavior?\n. Instead of creating a new Optional you could return the same one removed: instancePlan.\n. to make it clear what this resource is, can we call it something like capacity and clarify it's use in the javadoc for the costructor?\n. Instead of storing each part of the resource passed, can we just store the resource itself?\n. This is an indirect way to test container sorting. Could we instead write this test without a dep on FirstFitDecreasingPacking and just pass a Set of containers to sortOnContainerId and assert the returned ordering? That way we can directly test the utility method without going through the FirstFitDecreasingPacking code, which should be tested in it's own test suite/class.\n. TopologyAPI.Topology topology\n. I see now, thanks for walking me though it. What would you think about renaming getHomogenizedContainerPlan to something like withHomogenousScheduledResources?\n. This seems like a useful method to add directly to ContainerPlan:\npublic PackingPlan.ContainerPlan cloneWithScheduledResource(Resource resource);\nWe did something similar with Resource.cloneWithRam. Then the new constructor on ContainerPlan could be private and we use the clone method instead.\n. You could make the method static and pass totalInstances in as totalExpectedInstances\n. This could be private\n. Instead of using both capacity and maxResource, how about just capacity?\n. Just because nobody else currently needs it besides the class itself. I typically keep things as restrictive as necessary and increase visibility when necessary, or when ready to provide a public, stable API.\n. @kramasamy is this change required topology configs? I see many entries in this file that are not referenced in the codebase so wondering what it's primary function is.\n. No, scheduler.yaml doesn't use this. The current intent is that it can be set in one of two ways:\n1. in the topology main code by calling setUpdateDeactivateWaitDuration(Map<String, Object> conf, int seconds) on Config.\n2. By passing it as a command line arg vai --config-property when calling heron update.\nIf that's the case, it seems I don't need the entires in keys.yaml, defaults.yaml and Context.\n. Instead of mixing both Heron configs and environment variables can we just use configs? Splitting the logic between configs and env variables makes for an inconsistent experience with hidden features. If we use a config param you can still pass it at the command line to override the default:\n$ heron help submit | grep config-property\n  --config-property (key=value; a config key and its value; default: [])\n. Great catch! Instead of maintaining another Set could you just check processToContainer.values().containsKey(container.getId())? That guarantees that we're always in sync with what startExecutor has started, since it updates processToContainer.\n. Thanks for the patch!\nPlease change ERR to ERROR and remove the exclamation point.\nAlso please accept the CLA, see http://twitter.github.io/heron/docs/contributors/community/\n. That's a great observation @windie. If we were running production topologies with this class that added many containers I'd agree that this is a concern. In this case though, this is a local scheduler used for local development which we wouldn't expect to need to add more than max 2-3 containers to ever. For that reason I think think we should favor simplicity and code readability over the performance aspect.\n. This operation works fine in the local filesystem state manager, which is used in the tests, but not the ZK state manager unfortunately.\n. Yeah, I also considered that. Let me give that a shot.\n. I've added update commands to the state manager adaptor. Also changes AuroraScheduler to not update state manager as a side effect of calling createAuroraProperties.\n. Can we keep JavadocStyle in place so if there are javadocs we verify they are valid.\n. That is the intent yes, which is that the setting can be configured at topology submission time in the main method, or overridden at update time from the command line.\n. Some typologies flush internal state to external systems every N seconds. In that case they would set this value to something lager than N. This assures that the topology deactivated (pausing new tuple consumption from spouts) and state is flushed before scaling occurs.\n. Yes, good catch. Updated.\n. @kramasamy Tsar has a use case to set this when topologies are created based on the cache settings so I think it's ok to keep the API. The values wouldn't be hard-coded, they'd be set during topology creation based on the setting from the higher level framework.\n. Since Step 3 is to start Heron Tracker before Step 4 to start the UI, I don't think this comment is needed. It's assumed that the reader is not skipping steps.\n. The first sentence in this section is Heron UI is a user interface that uses Heron Tracker. The text is very clear IMO that the tracker needs to be running.\n. @ashvina implementing callbacks would be great but that's a much more involved feature that we'd need to think through and ideally have a solid use case for before building it. Currently we don't support any network callbacks to other systems so to do so we'd need to design the protocol, the register/deregister callback mechanisms, etc. I suggest we look into that once we have a solid use case for it's use.\n. Does this assure that self.tunnel != None?\n. python vars should be snake_case, not camelCase.\n. space after comma\n. This took me a little while to figure out the NoseFlute naming, I'm a little slow. :) Can we call this MockKazoo or something more boring, but more informative?\n. @ashvina and I spoke offline and agreed to move forward with this approach and consider an event listener approach once we have a concrete use case for it.\n. Can we remove this, or should it not be commented out?\n. This calc gets redone in many places. Can we create utility methods for padding in PackingUtils? Something like this maybe?\npublic static long increaseBy(long value, long percent);\n. best to calculate the pad only once and use that in both the criteria and the logging. Same for below.\n. Assertions in a loop can be tricky to diagnose when they fail. Best to add a failure comment as a first arg that includes the container id, to help upon failure.\n. IMO the code is less verbose and easier to read when using these static imports and not the fully qualified param below.\n. better to set all class member values during initialization, instead of as a side effect from a logic method.\n. No need for this variable, same below.\nreturn new Pair<PackingPlan, Resource>(newPackingPlan, packingPlan.getMaxContainerResources())\n. please clarify in a javadoc what Resource represents.\nActually do we need to return this? It seems like the test should always asserts that the new containers are < the initial maxContainerResource, so could that assertion always be done within doScalingTest?\n. Looks like there are a couple more instances of heron/tracker, as well as heron/cli and heron/ui in the docs. Would you mind please fixes those as well?\n. PackingUtils.increaseBy\n. PackingUtils.increaseBy\n. PackingUtils.increaseBy\n. prefer String.format over concatenation.\n. Seems like we always round doubles. If so, maybe we should just do that as part of the method?\n. Yes, these methods are trivial but we should have a simple unit test.\n. this could be for another PR, but it would be great to have a unit test that loads things up and verifies the default configs, then another one that loads things up with overrides and verifies that too. Such a test would verify that this .set call wasn't overlooked.\n. IMO it improves readability and is generally less typo-prone to have the text template, followed by the dynamic args that follow. It also makes it easier to update text without have so many \" + foo + \"s to work with. But this is subjective I suppose. \n. Fair enough.\n. I could go either way on this. I made it an inner class since it's tightly couple to, and only intended for, NetworkUtils but if we think it should stand on it's own I'm ok with it.\n. Cood catch, fixed. I changed to shutdown() when I was trying to get the CLI to not shutdown and kill the scheduler work before the reactivation finished. Instead I just waited on the future with future.get(timeout).\n. can we iterate over the keySet() here instead? The naming helps it tend to be more descriptive than using entry.getKey() and entry.getValue(). Same for below.\n. please rename 's' to something like configKey or something descriprive\n. This tests the state managers initTree method, which creates all StateLocations and we've added a new one with /locks.\nhttps://github.com/twitter/heron/blob/master/heron/statemgrs/src/java/com/twitter/heron/statemgr/FileSystemStateManager.java#L42\n. If either of these assertions fails, the reason won't be clear to the reader of the logs. Would you please add a string reason for the failure (e.g., an unexpected config found) and output the map/set.\n. Can these key values be static strings referenced from somewhere?\n. please use a more descriptive field name than s.\n. There are enough Map, Set, Config, Builder objects all referring to different expected/found/reef values that reading the test can get tricky to a reader without details of the config code implementation. Would you please add a few comments along the way helping explain things.\n. @windie circling back on this since I realized that HashMap.values() returns the same view into the hashmap when called repeatedly, so the optimization question is moot. Calling remove on the map removes the entry and updates the backing values set.\n. The main use case for locking is the distributed one, where multiple users are trying to make concurrent changes. Since the local file system is only used for compatibility during development by an individual user, do you think it's worth it to invest in a robust local implementation?\n. I've updated the file creation to be atomic. The Files API supports this:\nhttp://docs.oracle.com/javase/8/docs/api/java/nio/file/StandardOpenOption.html#CREATE_NEW\n. I changed this because all other uploaders use the local fs uploader, which just moves it to another directory, so I thought that made sense for yarn as well. Is the yarn workflow different than all the others with respect to it's uploader needs?\n$ grep -r heron.class.uploader heron/config/.\nheron/config/./src/yaml/conf/aurora/uploader.yaml:heron.class.uploader:                            com.twitter.heron.uploader.localfs.LocalFileSystemUploader\nheron/config/./src/yaml/conf/local/uploader.yaml:heron.class.uploader:                            com.twitter.heron.uploader.localfs.LocalFileSystemUploader\nheron/config/./src/yaml/conf/localzk/uploader.yaml:heron.class.uploader:                            com.twitter.heron.uploader.localfs.LocalFileSystemUploader\nheron/config/./src/yaml/conf/marathon/uploader.yaml:heron.class.uploader:                            com.twitter.heron.uploader.localfs.LocalFileSystemUploader\nheron/config/./src/yaml/conf/mesos/uploader.yaml:heron.class.uploader:                            com.twitter.heron.uploader.localfs.LocalFileSystemUploader\nheron/config/./src/yaml/conf/slurm/uploader.yaml:heron.class.uploader:                            com.twitter.heron.uploader.localfs.LocalFileSystemUploader\nheron/config/./src/yaml/conf/yarn/uploader.yaml:heron.class.uploader:                            com.twitter.heron.uploader.NullUploader\n. I've changed back to NullUploader for now, but the proper fix is really to change the yarn impl (or the shared classes it relies on) to not invoke an uploader at all. This is a cleaner approach than calling a class written to do nothing.\n. As a follow-up, we should move ContainerAllocationException into spi and make make IScalable throw it. Same thing for ContainerRemovalException. \n. Instead of logging the exception separately above, can we have an if/then with a different log message in each. That way it's clear what's causing what.\n. sinceThreadName? it's not the thread id, it's the name.\n. Sorry, values is a Collection which is what I was in fact referring to. Calling map.values().contains(foo) repeatedly as we add containers will not have O(# existing containers) * O(# the added containers) complexity, because map.values() does not need to be reconstructed for every value added, which is what I thought you were implying. The same backing collection gets used in the map impl, as new items are inserted so performance is the same as if we manage our own collections of values\n. please Log.SEVERE instead of printing to stdout.\n. My suggestion was instead of doing this:\n- log all the details of the exception\nif it's the main thread\n  - log that we have to die (due to the previously logged exception)\nto the log reader it will be more clear if we consolidate those two:\nif it's the main thread\n  - log all the details of the exception and that we have to die\nelse\n  - log all the details of the exception\n. I wasn't suggesting an extra map, just consistency in variable naming. Above we assign getId() to a variable named mainThreadId:\nmainThreadId = Thread.currentThread().getId();\nbit here we assign getName() to a variable named sinkId (instead of sinkName):\nsinkId = thread.getName();\nthis is a minor nit though and can be disregarded if you think sinkId is more representative bases on it's usage.\n. pls remove.\n. let's camelCase consistently with elsewhere (e.g., getScaleUpResource) by using scaleUpResource and scaleUpResource here and elsewhere in this patch.\n. better to use Math.max(0, cpuDifference) here and elsewhere\n. Seems like this could be a method on Resource itself?\nResource subtract(Resource other);\n. This could also be a method on Resource:\nResource divideBy(Resource other);\n. Typically we want to iterate directly on the keySet here, which is an Iterable<Integer>. There first 3 lines can be replaced with:\nfor (Integer containerId : allocation.keySet()) {\n. What does this.containerId represent and why is it 1 when starting a repack?\n. instead manually resetting values and then calling increaseNumContainers(newValue) could we just call a resetNumContainers(newValue) method?\n. can you explain in comments what's happening here? it seems like we just keep calling getResourceCompliantRRAllocation until it returns something, but nothing changes in the values we pass to it so it's unclear why we should expect a different result each time.\n. There's a lot of duplicate logic in getScale[Up|Down]Resource and getComponentsToScale[Up|Down] that could be replaced by refactoring into generic methods that take an enum that represents whether we're scaling up or down:\n```\n  enum ScalingDirection {\n    UP,\n    DOWN;\nboolean includes(int parallelismChange) {\n  switch (this) {\n    case UP:\n      return parallelismChange > 0;\n    case DOWN:\n      return parallelismChange < 0;\n    default:\n      throw new IllegalArgumentException(\"...\");\n  }\n}\n\n}\n```\nAlso, I think we could name this method more clearly. Maybe something like this maybe?\nResource computeTotalResourceChange(\n  TopologyAPI.Topology topology,\n  Map<String, Integer> componentChanges,\n  Resource defaultInstanceResources,\n  ScalingDirection scalingDirection);\n. better to represent PolicyType as an enum with a method that encapsulated this logic. Or if there's a lot of logic, the enum might map to an impl of a PackingPlacement interface.\n. can we switch on componentName and have an explicit case for \"bolt\" and \"spout\" and fail on default?\n. This pattern is repeated in this class. Could we create a methods like this in this class to simplify this:\npublic static int getValue(Map<String, Object> config, String key, Integer default);\nmight be better to do in another pr.\n. This name on it's own doesn't clearly convey it's meaning. Can we change it to convey that it's the max exceptions in a protobuf? Something like heron.metrics.max.exceptions.per.message.count or something else more clear.\n. What's the scope of this counter? Does this mean that no more exceptions will ever be published, or just for this payload? Same for droppedExceptionsCount. Is that count per message payload, count since the jvm started, or what?\n. since we're logging this and moving on without throwing, we should change this to Level.WARN actually.\n. Ideally try blocks should be as small as possible and this one is pretty large (actually methods in general should ideally be small, but that's another topic). Could you move the logic inside the block into a private method and just delegate to that inside the try?\n. We could change the name to imply the operation (e.g. subtractAbsolute?) or just document that subtract will never go negative. Since it's a mathematical operation between two Resource items it seems fitting to have this as a method on Resource IMO. That's more OO than static helpers. \n. @avflor see my other comment re mathematical operations on the object that does them and OO vs helpers.\n. increaseNumContainers takes a delta to increase by not an absolute, so I was proposing two methods:\nincreaseNumContainers(delta)\nresetNumContainers(absolute)\nit's just about being consistent with our use of encapsulation. When we want to add containers we call a method to increase, which handles the operation. When we want to reset to a value though, we instead manipulate the values directly in multiple places. we'd have better encapsulation and consistency if we did both operations through methods that handles modifying of the underlying values.\n. for (Integer containerId : allocation.keySet()) {\n  if (allocation.get(containerId).isEmpty()) {\n    allocation.remove(containerId);\n  }\n}\n. Ugh, that's right, we need to use the iterator.\n. Sorry, I missed that increaseNumContainers doesn't change numAdjustments and containerId, only numContainers so I was confusing things by lumping that method in with the reset I was suggesting. I'm suggesting a method that resets the following, which happen in multiple places:\nthis.numAdjustments = 0;\nthis.containerId = 1;\n. For consistency please use capital snakcase for enum values: STRICT, FLEXIBLE\n. stat seems like a good approach, provided it works on all distros.\nThere's no need to do all chmods in one concatenated command, so better to break it up to make the code more readable. No need to build up arrays and concated strings, we can just do this:\n```\n- chmod log dir\n\nfor each binary\nif chmod needed\ndo chmod \n```\n. let's make this an int type in argparse and remove the cast\n. can args.component_rammap do this and just return a map?\n. Typically we should log or throw but not both. Could we add the log messaging to the exception message and remove the LOG.warning?\n. Log.error\n. I was wondering if this logic could be encapsulated into the argparse logic via custom actions. I'm not certain it could, and maybe it's not worth the effort, so feel free to disregard.\n. let's remove the newline from the exception message. This complicates log parsing when grepping or using loglens.\n. When there are legit failures retrying 25 times is excessive and it takes over 4 minutes of results checking per test. Even 15 times seems high. If we have to retry anything this many times for simple, small test topologies, something is wrong.\n\n\n\nThat said, I can keep it if our tests are known to be too flaky when set below 25.\n. That's what I thought. Let's reduce it and see if it causes problems.\n. if this is only intended for heron-ui could the method live in heron-ui somewhere instead of in a shared util? Or if it needs to live here, we should put something related to heron-ui in the method name to be explicit. Actually, why is this method only to be used by heron-ui?\n. wherever we specify go_above_dirs could you add a comment showing the dirs that we need to back out of?\n. If the method is in fact general purpose and could be used by anyone that uses zip_save=False, then we don't need to say \"Specifically designed for heron-ui\" and \"return: root location for heron-ui.\" How heron-ui sets zip_save doesn't need to influence how we write this methods docs.\n. if we still need go_above_dirs in this method and/or in get_heron_dir, we should document the value.\n. This target is defined in this file.\n. Yes, there is. See my comment above on this ticket. The code I need to call will exist once #1516 is merged.\n. Renamed.\n. This is something we should look into separately. The same is true for all other calls to bolt in this class (e.g.,  bolt.prepare(..),  bolt.cleanup(..),  bolt.execute(..)). None have exception handling or timeouts.\n. Instead of suppressing this here and elsewhere, can you add a serialVersionId?\n. The design philosophy is that the algorithm author decides the total number of containers and places the instances into the containers of it's choosing. The builder managers the data structures and the resource allocation checks. As a result the algo needs to be aware of the number of containers while the builder just needs to handle placement into the container with the id passed.\nThere is room for improvement in the builder actually in that it shouldn't have to hold a list of containers (and know the size) in addition to the map of containerId -> InstanceId. Really it could just hold a map and manage everything by id, without any sense of total container count. That's an enhancement I can make if we go this route.\nThere is one unfortunate inherent assumption in the code though (both with the current and proposed impl) which is that it during repack, the pre-existing containers can't be easily manipulated because their id space might not be contiguous and the ArrayList impl assumes they are. I'm not sure how to best handle this in the builder approach. Maybe we don't need to and we can assume that existing containers will never be shuffled (for now at least).\n. I haven't yet documented this, but the design intent (which we can enforce) is that the builder is used as follows:\n1. Call the setters to set required settings (e.g., resource settings)\n2. Set the container size\n3. Add/remove instances\n   3a. If more containers are required go to step 2.\n4. Build\nAs such, there are a few reasons why allocation/deallocation happens in add/remove instances.\n- setting container size is required before adding/removing because the system is bounded by container size. (Actually we should support either bounding by container size or containerMaxResources. Currently we require bounding by both.)\n- instances can be added and removed, followed by build without the need to call set container size between the two.\n. I think you're describing a scenario where a container is added and we then catch a ResourceExceedException and nothing has been added so we have an empty container. [We dont't have a check for that case and we should but] What currently happens is that we add a container and a number of instnaces are added to it until it's full and then we get a ResourceExceedException, so we're consistent and the new container is full and it's time to add another.\n. We can't allocate containers in the updateNumContainers because setRequestedContainerPadding might not have been called yet and we need the container padding. Hence we call 'initContainers' before each add/remove and we get our internal data structures consistent.\nMy plan was to document and enforce that steps 1 and 2 in my sequence above are done (in any order) to set the relevant params, before adding/removing instances. If make the decision to change the internal behavior to init containers upon update, we'll need to enforce more restrictions on the user ordering of actions that are not necessary and wouldn't be caught until runtime.\nBecause initContainers() is done first for both add/remove, we'll always be in a consistent state before allocation.\n. I'd make the argument that checking for null before closing a resource if fairly common practice and not in need of an explanation.\n. Yeah, it's painful that these strings are being returned here and we should fix that. I'd like to contain the scope of changes in this PR though, and change all the true/falses in a PR of it's own.\n. That's ok since e is scoped within each except clause so only 1 will be defined without collision.\n. Yes, good call.\n. So you can run a single test like this:\n./bazel-bin/integration-test/src/python/local_test_runner/local-test-runner \\\n  test_template.TestTemplate\ninstead of like this:\n./bazel-bin/integration-test/src/python/local_test_runner/local-test-runner \\\n  integration-test.src.python.local_test_runner.test_template.TestTemplate\n. Could we link to the site at httpp://heronstreaming.io/docs/contributors/community/ or  http://twitter.github.io/heron/docs/contributors/community/ or would that not work?\n. Can we make submitTopology return void and let this exception be thrown higher?. We should include the topology name in the message as well:\nTopology foo already exists.. We no longer raise an exception when the status code != 0, but do we still need to be concerned with python code below execute.heron_class raising an exception due to any other a bug/error condition? Or maybe we just don't need to explicitly catch and handle those since they're no longer part of the expected workflow?. instead of escaping single quotes it's more readable to use double quotes for the python string so you don't need to escape.. For now we should include the URI that failed to upload.\nIn a follow-up we should make uploadPackage also throw and not return null.. This pattern of logging to stdout and then throwing RuntimeException looks odd to the reader so best to clarify why we do both in a comment.\nI suspect we'll want to do similar in other places, like RuntimeManagerMain, so at some point we'll want this logic in a utility method. At that point, we might also want a common super-class above TopologySubmissionException, TopologyUpdateException, TopologyRestartException, etc.. If at the highest level we always want to log errors to stdout and throw a runtime exception, we should make a helper that does both of those. I know it's only two lines, but this will centralize the expected handling into a single place which will make refactoring easier and assure consistent handling.. Yes, that needs to be fixed. It should divide and round. Will update that.. If we add a helper method in this class to do foo != null && !foo.isEmpty() it would improve the readability of this code:\nprivate static boolean isEmpty(String... value) { ... }\nWe could then simplify these conditionals to do:\nif (!isEmpty(accessKey, accessSecret) {\n  // set credentials\n} else if (!isEmpty(awsProfile) {\n  // set credentials\n} else .... please include proxy in the exception message here. please include a helpful error message here, including the bad field value. As implemented we're trying to achieve balance while also removing containers where we can (despite how I named this PR). So I would argue that we would want to go from [BBCD, ABC] to [BCD, ABC] which is a more balanced state. To expand on this example, if we instead had [BBBBBBCD, ABC], we should try to removing as many Bs as possible from the first container to reduce our imbalance before trying to remove the seconds container, which is well balanced.\nAlso as implemented we're trying to prioritize removing containers that have all Bs, which is the scenario we've seen when scaling our typologies. If we had [BBCD, BBB], we'd remove the second container first.. Great point, removed.. I haven't yet found that computation done in the code, but if I do, I'll add it. Otherwise I say we leave it out until it's needed.. always better to attach the actual exception than to include it's message in the string.\nthrow new TopologySubmissionException(\"some message\", e);\nthat gives the catcher more control to decide how to render it.. it looks odd to catch Excetption, log only the message to standard our, and than throw a RuntimeException. Please add comments explaining why we're doing this.. please include the packageURI in the exception message.\nalso in a follow up PR we should create UploaderException instead of returning null.. can we eliminate isSuccessful and just do this in the catch block?. Actually looks like you already throw UploaderException below, so we should be able to remove the null handling.. ugh, getting only this message would be a crappy experience. In a follow-up PR we should fix launchRunner to throw something more meaningful.. This test is odd. We mock the object to throw an exception and then assert that the exception is thrown. Why? It seems to not be testing the actual exception throwing logic of the class. Same is true in 2 more places below.. Should we include packageURI in this class? That way the catcher would have access to it.. why suppress the connection string? Could it have a password in it?. best to not include line breaks in log messages. please put some sort of prefix text before logging the stdout.. can you also include this stderr if the response code  != 0.. above you refactored out the '$> ' log syntax for something more descriptive. we should do the same here too.. instead of logging error messages and returning the return code, could we return some sort of response object that includes he return code and the message?. Instead of requiring each type of command log a message and return a boolean, could they return a response object with it's status and message and let the highest level code handle it (e.g., main.py)? That would guarantee that the response message that the user should see is the last thing to be output, and that logging is done consistently (i.e. stdout vs stderr, displaying stack traces, handling verbose or not, etc) . more context here:\n\"Expected topology package file to be uploaded %s does not exist\"\nSame for massaging in other uploaders below.. more context for the user here (i.e., why are we trying to create this directory?). Maybe in a follow-up, but copyFromLocalFile should throw an exception with meaningful info about why the upload failed which we can wrap with this exception.. Don't cast the caught exception down to a string, wrap it with UploaderException. Also include the dest location:\nthrow new UploaderException(\n  String.format(\"Unable to copy file %s to %s\", source.toString(), filePath),\n  ex);. include AmazonClientException in UploaderException. throw new UploaderException(\"[something helpful here]\" + resourceUrl, e);. For all of these exception messages, assume they get caught at the highest level, up in the python main and rendered to the user without any of the surrounding context that we have when reading the code.\nDo they contain enough context to help the user fully understand what failed and why? So when we say \"Can not connect to tunnel host foo\" is there any other info they might need? Maybe something about who/what/why something is trying to connect to a tunnel host in the first place (i.e. we're doing this for ZK in this case), what username are we using, etc.. Why not send all of this back to main.py and let it render stdout/err based on the response code?\nEither way, we shouldn't have to do this pattern in execute.py AND in all the command impls. It should be done in one place ideally.. 2. Current design requires each command do this at the end:\nif retcode != 0:\n   Log.error('Failed to update topology \\'%s\\'', topology_name)\n   return False\nSo we only have a consistent solution if every command implement cargo-cults just right. Better to codify this into the response time that is required to be returned and let a single agent implement the logging logic. That way it will happen consistently, even if we decide to change some output to go to print instead of Log for instance, or if we decide to render exception or verbose responses differently at some point.\nIt's kind of an extension of the log-and-return-null|false pattern that we're trying to remove in the java code.. please include the invalid value in the exception. Can we replace \"Error undoing deploying\" with something more descriptive? Like what exactly are we trying to undo and why?. We should clarify that if both access_key, secret_key and aws_profile are all included the latter will be ignored. Or probably better to throw an exception if all 3 are specified, just to make sure things are clear. That's consistent with how we throw an exception if just aws_profile and access_key are set for example.. comment. Rarely do we want to just log an object without providing some context as to what it is and/or why we're logging it.. Regarding \"I am testing if the exception will be thrown out,\" the exception will always be throw because you added this to your unit test:\ndoThrow(new TopologySubmissionException(\n        String.format(\"Topology %s already exist\", topology.getName()))).when(submitterMain)\nso you're not in fact testing that the logic within submitterMain will do this on it's own. You're testing logic you've inserted in the unit test.. Could you add a readme to this dir to describe why this patch is necessary, how it's applied, how to regenerate it, etc?. How were you able to determine the long list of required includes here? Could you provide a link in a comment?. Even more reason to clarify this mystery patch in a README. :) The next person to upgrade will thank you.. I think what we want to convey her is that the user must specify either a profile or a key/secret, but not both. Also the wording is off a bit. We should either use supply or provide but not both.. Please include proxy in the message.. I think ByteAmounts should be immutable, so returning a new one makes sense. I'll update the javadoc to reflect that. Yeah, I have mixed feelings about asMegabytes and asGigabytes. They only make sense as convenience methods for callers who are working entirely in MBs or GBs and only adding/subtracting using like units. For example, the yarn scheduler uses asMegabytes to pass configs from one place to another without any math.\nWhen using other math functions though (or mixing units in add/subtract), they can cause problems. For example if you have 10 GB to split among 4 instances, and asGigabytes is used, each would get 2GB or 3GB if we implemented using Math.floor, or Math.ceil respectively.\nBasically if division or increaseBy is used followed by anything but asBytes, there's a potential problem, regardless how we choose to round. I'm not sure how best to address that beyond javadocs.. I debated this one as well. I like the conciseness but not the resulting inconsistency between asBytes, asMB and asGB.. great catch, thanks.. Another thought is we could add an enum to the signature where the caller specifies how the rounding should occur: ceil, floor, round, or strict where we fail if there is a remainder. That way the caller has to acknowledge the potential for rounding and decide how rounding should be handled.. Cool. Since this patch is already large and at risk of drifting out of sync with master, I'd like to commit as-is and address this in a follow-up.. can we return void?. Most exceptions are associated with topologies, but we don't take it as a constructor arg, so I'd remove this pattern here.. Typically Exceptions are part of the API/model objects, not utils. Best to put this in c.t.h.spi.scheduler.. We should also log.fine the entire exception, so we get the stack trace. We might want to prefix the message with something like this, incase you're catching an error that doesn't have a nice message.\n\"Topology {0} submitted successfully\", topology.getName(). e.getMessage() is included in the exception chain when you chain the message, so you would typically not re-included it in a custom message here. Better to replace e.getMessage() with uploaderClass actually.. this catch and throw could be removed. fix javadocs. should explain in comments why you only call undo for certain exceptions. Or better yet, could we always call undo and have that method be written in a way where it's a noop if there is nothing for it to do?. No. Let's leave specific metadata out of this exception until we identify a valid need. pls include packingClass in the message. Better to encapsulate this into a response class than to return tuples. It would take an int response code and an optional error message and error object.. better to return the ex and not stringify it. Leave that to the caller to decide.. Could we simplify this to be:\n__init__(self, status, message=None, detailed_message=None):. NonUserError and UserError feels a bit misleading. Could we call it something like HeronError for errors in the heron classes, or maybe JavaMainError? For < 100 that could be InvokationError maybe?. Why do we need these subtypes? Would Response not suffice on it's own?. We should include the directory in the message. should include the dir name. toString not necessary. omit: happens. hmm, I don't see it. I'm referring to the Log.info above. The creating it now message.. Thanks for the removal of dup code!\nCould you also please change to not abbreviate pkg (e.g., packageType and getPackageType).\nAlso instead of returning strings, better to use an enum, maybe as an inner enum in FileUtils:\nenum PackageType {\n  PEX,\n  JAR,\n  TAR\n}. As implemented the user will always see the stack trace. If the output messaging is sufficient we could only show the stack trace with --verbose (i.e. log.fine). Thoughts?. Best to now assume this is InvokatonError and check for it. Then have a final else that alerts that we have an unexpected response code found. That should never be the case, but it future proofs against people adding an enum value and no updating this method\nAlso why not render msg here?. Why log debug HeronError detailed_msg, but log error for detailed_msg for Invokation error? If there's valid reason please include a comment in the code. This is where it would be better to use a Status.HeronError from Response instead of requiring the caller to know the int value.. Why do we need to have Response.render as well as this logic? Could they somehow be combined/centralized?. If we can't load the uploader class we should probably throw an UploaderException, right?. could you also check that :-1 is in fact a newline?. Config.put takes an object as it's value, so we could put enum values in and add a getter that returns it. We did this with ByteAmount for example:\nhttps://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/common/Config.java#L79\nThen we refactor all consumers to use the enum.\nThe only catch is that if we add the helper to Config, then the enum needs to live somewhere central like in spi. This might not be a bad thing though. It's much better to have a strong set of package types than to pass strings around.... Good catch. That's a bug in my comment, we are doing most homogeneous first. Updated.. spi.common would be good. Just like Commands:\nhttps://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/common/Command.java#L20. Yes for 0 and 100 but invocation error should be defined by the process that returns it, and we should just assert that it's > 0 and < 100 to avoid collisions.. I think it's reasonable to throw an UploaderException when the uploader can't be initialized.. You're right, we can't really assert becuase we any value can be returned and set from the shell command so disregard that part.. I'd say yes for launcher since we already have LauncherExeption. For state manager, no need to create a new one just for this IMO.. include the instance in the error message. include the type in the error message. these first two blocks of comments just echo the code and could probably be omitted. the one above invocation error is useful. does this have the potential to sleep indefinitely?. best to not rely on that and provide a hard limit upon which we give up.. Config supports objects and we should favor type safety where ever possible. Doing so will make code easier to maintain and less prone to bugs than by passing string values around.. Good call. Feel free to submit a patch.. Why is a custom toString necessary? IIRC the enum toString prints nicely.. this.name() does this. This if statement can be replaced with a static factory method on the enum. Something like this:\n```\npublic enum PackageType {\n  PEX,\n  JAR,\n  TAR\npublic static PackageType getType(String filename) {\n    // parse extension\n    return PackageType.valueOf(extension);\n  }\n}\n```\nActually, at that point you no longer even need FileUtils.getPackageType anymore, since callers can just get the type from the enum. We don't want to rely on toString() here since that should only ever really be used for logging. If we need a string better to call .name() on the enum, or create a value field in the enum that is defined as part of the enum (i.e., TAR(\"tar\")).. It says \"Thrown to indicate that an error occurred while creating a packing plan\" which is what PackingException is used for. This is TopologySubmissionException so the javadoc should say that it has to do with submissions.. We can remove ~20 lines of code and just use guava Files class here:\nString extension = Files.getFileExtension(topologyBinaryFile)\nreturn PackageType.valueOf(extension.toUpperCase();\n. better to use `.name()` than toString. Also check on the cases, think both return caps.. Is this RuntimeException or a subclass? If a subclass we should expect that instead.. We should rename topologyBinaryFile to something more generic like filename.. sure. It's better to provide a reasonable timeout here for how long the test should wait. The CI timeouts can be an hour or more. We don't want this test to hang for that long and potentially clog what is a resource-limited CI queue.. 30 seconds? It's hard to tell for sure, since I'm not familiar with the specific of what this test is doing. If you start with 30 seconds and we're still timing out we could increase it, or try to understand why.. This class is really a utility, not a config. We should probably name it `SchedulerConfigUtils`. Also all but the last method can be private.. Are these running elsewhere? I don't see them added under schedulers/tests.. `schedulers-core` should not depend on `schedulers`, but the other way around. The classes we're moving should live in `schedulers-core` so all implemented schedulers could leverage them.. fix error message. fix error message. Yes, by all means don't be shy about reviewing any PR @mycFelix.. You've got the same comment in this block twice. This method has a ton of duplicate code/patterns with `SubmitterMain` and even `SchedulerMain`. We should share that. One suggestion would be for them to extend a common `AbstractMain` or something like that:\nabstract class AbstractMain {\n  // includes a bunch of static long_opt option strings\n  // includes a bunch of static OptionBuilders for shared options\n  AbstractMain(String[] args) {\n    // initializes options, creates the config\n  }\n  final void run() // does the template method (, execute)\n  final void addOption(Option option) { ... } helper to add options\n  abstract initOptions(); // call addOption for all required\n  abstract Config createConfig(CommandLine cmd) // builds config\n  abstract void execute(Config config, CommandLine cmd) // run the thing\n}\nclass RuntimeManagerMain {\n  psfv main(String[] args) {\n    RuntimeManagerMain main = new RuntimeManagerMain(args);\n    main.run();\n  }\n. RuntimeManagementException?. Have the preconditions from this class moved to TMasterUtils?. append \"from state manager\". No need to create `String errMsg`. No need to define `String errMsg` here and elsewhere. I favor using Preconditions.checkState for error states that should never really occur except due to a bug (i.e. dividing by 0 for example). For situations like this one though, this can actually happen if two people are trying to run a command at the same time. In this cass I think it's good to have a better typed exception.. Fair point. Let's keep `TopologyRuntimeManagementException`.. Can we use TMasterException instead of RuntimeException? Also the TM in TMaster should both be in caps.. Sometimes we throw TMasterException and others we throw the exception generated from Proconditions. Should we be consistent and just throw TMasterException? That seems like a better experience.. What about changing the signaure to be more generic:\npublic SubmitDryRunResponse(TopologyAPI.Topology topology, Config config, PackingPlan packingPlan);\n```\nThat would provide more info to the renderer.. This should be a Boolean instead to indicate that it is a dry run.. Better to have --dry_run be a boolean. If it's true --output_format (tsv, json, text, etc) can be set to override the default. We might also need a third flag at some point if we have multiple versions of a format (e.g., summary, detail, etc). . Instead of hard coding 200 here can we have this be a constant somewhere with it's meaning implied in the name?\nRESP_CODE_FOO=100\nRESP_CODE_DRY_RUN=200. These values should not be buried as hidden strings here. They should be python options to a given argument. In the java code we could have a map of renderers keyed by value, where the keys are constants or enums.. We really need to refactor so we can stop copying code between this class and RuntimeManagerMain. Can we do that first, in a separate PR?. Instead or requiring each renderer to support all commands for which dry run is supported, we could be more granular here and have a SubmitDryRunRender interface that knows how to render SubmitDryRunResponse objects. Same for update. This will make for smaller, less coupled, more moduler classes. Common code reuse could be handled via composition or inheritance. . Parallelism mapping isn't needed, it's already done in packingPlan.getComponentCounts(). If we need to sum all resources by component, we should also provide that as a method in PackingPlan. And it should have a unit test.. let's put all the dry run responses and renderers in com.twitter.heron.scheduler.dryrun. Util should almost only ever be used for static utility helper methods, and even then it should be avoided when possible.. Unit tests please for all changes to Resource.. Verbose mode. Increases logging level to show debug messages. We should include a comment here with a reference to the java class where these constants are defined.. shouldn't this be:\nelif status_code == 200:\n  return Status.DryRun\nelse:\n  return Status.HeronError. We could remove this and just append in dry run mode to the end of the existing messages if we're in dry-run mode.. doesn't resp have a response code that we should be returning?. How is the case handled where someone tries to dry-run a command that doesn't support it? How does the user know which command dry run is valid for? For example:\nheron deactivate --dry-run. As written you're defining anything >= 200 returned from the java process to be represented as a dry run by the python process. Why?\nI thought you were adding a new 200 code explicitly for dry run, which is why I suggested changing the logic from this on master:\nelse:\n  return Status.HeronError\nto this:\nelif status_code == 200:\n  return Status.DryRun\nelse:\n  return Status.HeronError\n. I think handling a range >= 100 would be a good way to future proof. If the java code adds 101 for example, the python code should pass through 101 without the need for edits.. let's given a better description to the user for what we mean by dry run. See my previous comment above where this code is duplicated. This abstraction implies that every command/renderer will have to support the same types of views. Some commands might support different views than others so a single String render() method seems more flexible. We can then have multiple implementations.. SubmitDryRunResponse and DryRunResponse are the same. Should we just make DryRunResponse non-abstract?. can you also assert that the topology update never happens?. aren't you trying to assert that the code throws this response without mocking it's behavior to do so?. aren't you trying to assert that the code throws this response without mocking it's behavior to do so?. These should be under tests/resources. The data files should be listed as bazel test resources, which will put them in the classpath of the unit test, so in each test you can just reference them relative to the tests/resources directory.. PackingTestUtils contains test helpers that will create a test packing plan or container plan for you so you don't need to assemble all of this manually. Can you use that instead, or extend it as needed?. best to not suppress star imports. See comment above. We should avoid all this boilerplate setup code in favor of using shared test utility helpers. please add unit tests for the new clone methods. Instead of one implementation that knows how to render both table and raw, you'd have two implementations, one for table and one for raw. You'd then bind each to their respective command line flags.. That's fine to follow your planned approach. Test resources is the more standard way to do this:\nhttps://bazel.build/versions/master/docs/be/java.html#java_test.resources. I don't know why we wrote our own java_tests target, but if we still need that, we can easily add resources to it. It's just a proxy to the bazel java_test method which takes resources.\nIdeally we should write tests to load files using relative paths based on the classpath instead of absolute paths with leaks from the build system included (e.g., main). Those files should live in a resources/ dir instead of being mixed into the java source.\nIf we can't achieve that with the java_test 'resources' attribute, we can do so with the 'data' attribute but we should first try patching our java_tests impl. It's fine to do this test correctly and then open an issue to refactor others to use the proper pattern.. Good catch. I refactored createJob to remove the auroraFilename, which only applies to the CLI impl. I also renamed bindings to auroraProperties and changed the key from a string to an enum to make it's usage clear.\n@objmagic I was in the process of making add'l edits when you gave your ship it. Would you please take a quick look again. I'm done making changes.. Although not the preferred approach I kept that for backward compatibility, but there's no need to do that since the code will all change in lock step. I've made the change to match behavior to docs.. Why would this not work?\nPackingPlan.ContainerPlan containerOnePlan = PackingTestUtils.testContainerPlan(1, 1, 3, 5);\nPackingPlan.ContainerPlan containerTwoPlan = PackingTestUtils.testContainerPlan(2, 4, 2, 6);. The main classes don't leverage the DryRunRender interface. We'd have more flexibility by not binding the DryRunRender impls into the same parent class and instead doing something like this:\nDryRunRender render = null;\nswitch (formatType) {\n  case RAW : render = new SubmitRawDryRunRenderer(resp);\n  case TABLE: render = new SubmitTableDryRunRenderer(resp);\n  default:\n    throw new IllegalArgumentException(\n         String.format(\"Unexpected rendering format: %s\", formatType)); \n}\nreturn render.render();\nIf the two renderers need to share code, they could use inheritance or composition.. don't suppress this.. Resources should be a sibling of java:\nscheduler-core/tests/resources/com/twitter/heron/scheduler/dryrun\nDoing so should make everything in resources be in the classpath when using the resource attribute to java_tests, or propbably the data attribute. See https://maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html\nIn the tests we should just reference the resource by com/twitter/heron/scheduler/dryrun/... which makes the reference isolated from the build system details and the higher-level directory structure.. don't suppress star imports. In Context and Keys you use DRY_RUN_FORMAT_TYPE. Let's change this to be consistent.. 'Enable dry-run mode. Information about the command will print but no action will be taken on the topology'. 'The format of the dry-run output ([raw|table], default=table). Ignored when dry-run mode is not enabled.. Can you tag this with:\n@InterfaceAudience.Private\n@InterfaceStability.Evolving. We no longer need this constant or class right?. Does this compile without the BUILD dep on the classification target?. If you change this signature to use Pair<String, Integer>... instead, you can pass an array or an unbounded number of Pairs, which is more flexible to the caller, since you don't need to populate a List or array first.. Last nit, is that you typically don't want to split the type from the variable on two separate lines.. Instead of using strings, can we use StandardCharsets.UTF-8 for a Locale or StandardCharsets.UTF-8.name()for a string representation? This is consistency with how locales are specified in the rest of the codebase. \nAlso please allow at least a few business hours of time for people to review non-critical patches before merging.. Can these instead be bound directly to the enum values themselves, instead of managed separately?\nhttp://en.cppreference.com/w/cpp/language/enum. While you're at it, could we switch the regex check to allow 1-2 spaces between the leading // and the text? My IDE puts 2 spaces after // and it drives me nuts. I can't find a way to change it.. Why is this needed?. What's strange is that the copyright in the profile is correctly spaced, but when it's inserted it's not.... Let's put null in there instead of \"null\".. This will be refactored out in a follow-up that addresses defaults.. loadClass throws NullPointerException when passed null. The calling classes catch ClassNotFoundException but not NPE. It a slight semantic stretch to throw CNFE in this case, but it seems like a more practical solution than throwing NPE or adding a new exception type like InvalidArgumentException to be caught.. psf should be TITLE_NAMES. Each time the method is invoked we iterate through each config value and do round of pattern substitution on the value, but substituting patterns can in the value can result in more tokens. For example ${HERON_BIN}/heron-executor can be changed to ${HERON_HOME}/bin/heron-executor, so after token substitution we can still have values with tokens. tokensCount counts the fields that still have tokens to know if we need to recurse for another pass.\nI'm curious though about how this worked before without recursion so I'm going to investigate some more. If we do in fact need it, I'll provide more docs about what's happening.. Yeah, I don't love the terminology here of local, remote or sandbox and would like to change it. It's really about how Heron is installed. It's either installed on the system, or bundled in a distribution. I was thinking about \"installed\" and \"bundled\" modes instead, but those aren't great either. Suggestions welcome.. Agreed I don't like local mode. Client is also overloaded and could cause confusion. Let's discuss in the similar comment above.. True says to initialize the Config with default values. See Config.Builder.create(loadDefaults) which this call delegates to.. fixed. fixed. fixed. fixed. Instead of using int, can we change this to ByteAmount and drop '.mb' from the key value (see https://github.com/twitter/heron/blob/master/heron/spi/src/java/com/twitter/heron/spi/common/Key.java#L104)? We could then use ByteAmount everywhere and then call asMegabytes before adding it to the yarn config, which I suspect requires MBs. \nIf this would trigger a large refactor, I'm fine with shipping this as-is and following up with the ByteAmount change in it's own PR.. please change to a class-level psf String.. ALL_CAPS is for class-level psf Strings. These should either be moved to class level or just passed directly to createTempFile without making a one-time use string variable.. Does the caller ever need an InputStream for anything other than creating a Map? If not, let's just return a  Map here.. woot!. While you're here would you please remove public from all these constants (and the class) and remove GROUP_KEY and VERSION_KEY, which are unused. #cruftslayer. We now have a mechanism for defaults and we use it for 2 config keys. @maosongfu do you think that's enough for #970 or should we also be adding defaults for other key values. You comment was initially above keys that now have a default, so maybe we're ok now?. In this and other severe logs (below) can we include the process string in the message.. s/Run/Running/. verifyTopology should really be fixed to return void and throw a better exception that RuntimeException (with a better message) but that's a fix for another PR.... Why does this return the error string and not the response object containing the error string?. Why add the conditional? Does shutil.rmtree(os.path.dirname(cur_file)) fail if cur_file is a directory?. Is there some python magic enforcing this? If not, let's explicitly check the state.. can we also log resp here?. should include a comment re why we don't log here.. do_log and log_context seem like they are intended to be private. If that's the case please move them to the bottom and prepend _ to the method name to make that clear.. Also please provide more context in the error message.. Why is render a static method and not an instance method on Response? The latter seems more natural.. This isinstance of list or Response seems like some python jank. Are there places where lists of Response objects are being passed in? If not, we should remove this.. In this method you're reading stderr, polling if None and then readying stdout, then polling if None. Are their chances for raceconditions here due to state changing on proc between calls? Also are there edge conditions possible when both stderr and stdout are set? I don't know the specifics of the proc API in this sense to say for sure.. Yes the command. This is always good to log in the exception so you don't have to dig through previous logs to find all others commands run, and then guess which one failed.. I follow the desired functionality, just not the need for the if/else. Without it would we not get the desired functionality?. Forcing all to use a list is also clunky. I'd say leave it as is for now.. Not necessarily, no. Just something to give the user more actionable info than what they'd have with a message like \"Unknown status type of value foo\". If they got that message and reported it to us we'd have no idea what that mean and or what triggered it.. Funny I just made the same change on my branch. :) You need a return statement here.. Inserting line breaks in log entries can cause pains when reading the logs. Can we remove the \\n?. Why does this method call require passing a StringBuilder that the caller never uses? Can we simplify the API to not require this here, maybe with another signature?. Why write directly to stderr here instead of LOG like we do below?. we should clarify in the javadocs how this method behaves w.r.t. logging and stderr.. why do we no longer start the thread? This seems unexpected from the callers point of view. If we don't start it we need to clarify in the javadocs how to properly use this method.. can we not inject \\n here?. s/message/messages/. pls clarify that we're combining the two into stderr, which I think is the case.. this comment talks about multiple buffers, which we no longer have below. please update to reflect new behaviour. pls remove \\n. pls include host and port. here and below we're passing an unused StringBuilder. can we refactor to not require that?. This logic seems sound if it works in your tests.\nIn the future, best to avoid making code changes and doing a file rename at the same time, since there's no longer a diff to review.. space after : . can we name builder stdout or output or something more descriptive?. please clarify in the javadocs that this method doesn't actually run anything. We should rename the method to make that very obvious too actually. Maybe createAsyncProcessThread?. space after : . fixed.. It would be better to declare this in the throws clause and not handle it, since it should trigger a test failure.. adding 'of' would mean we should make components singular. Instead though, we can change this to \"can self-adjust in the event that a component lags.\". please elaborate on what should and should not be included in /metadata\n. prepend something contextual log string before outputting a stack trace. please elaborate on what should and should not be included in /runtimestate.  this debug message doesn't seem useful. Why do we still need this endpoint? I thought the plan was to not add it, but instead use /runtimestate to serve this info. Can we remove this?. IIRC we decided that executionstate would be deprecated in favor of /metadata and /runtimestate. Is that correct? If so would you clarify that in the executionstate class, so it's clear to other readers why we have duplicate info returned by multiple endpoints.. I wasn't suggesting removing/refactoring it in this PR, just putting a comment describing that it's slated to be removed. That way if it takes a while to remove people readying the code will understand the duplication.. What's modified and how is it different than NEW and/or REMOVED?. Exactly. The string output is very different depending on the jvm. Try it with a default jvm vs a twitter one for example. Parsing those differences would be more brittle.. Will do.. Good call, updated.. Good catch, fixed.. +1. Please don't change these imports in this review. If we want to do this, we should have a review that is just about that, that people might chime in on. I know there are people with an opinion about this so you don't want potentially hide this change in a review with a title that's not related to it.. also, the nl part of the name isn't informative. I assume it relates to neverlink, but could we instead name based on the functional difference, like something to do with excluding from the artifact? The casual reader won't know what nverlink means.. Why does the jar no longer include backtype? We still want to support this. Also, the only classes we put under storm or backtype are classes that are actually part of that project (some like TopologyBuilder we modify to produce Heron topologies), but we don't add new classes there. We should avoid that.. Could we only use the neverlink version and then add kryo as it's own jar to whatever artifact bundle should also have it?. yeah, we certainly don't want to abandon support for the the backtype api as part of this review.. @kramasamy @objmagic the two versions of storm have 2 different namespaces so they can coexist, providing the deps align. That seems more simple than producing different jars. . I thought we agreed we would not shade heron-api?. No need for all caps, that's what warning is intended to do. Also can you specify in the log message what someone should do differently (i.e. that they should use Kryo and how to configure it). If we have a link to the documentation for how to use Kryo that would be ideal to include in the message.. why would we shade com.twitter classes to the org.apache.storm namespace? I don't think we should do this.. I see. I missed that we were only shading com.google. I thought all of heron-api was affected.. Can you please include these all in one log message? It's typically better to have the entire log entry in one longer warning without line breaks, than to have it split into multiple lines.. Is this class really no longer needed? I thought we supported configs that specify this to be loaded via reflection and that we had to keep it, just in a new location outside of c.t.h.api.. What if instead of removing this completely we move it into contrib at contrib/kryo-serializer/src/java? That way we can easily resurrect it. It won't effect the build there for now.. Please match the directory structure to the package name.. please update the directory structure to match the package structure:\ncontrib/kryo-serializer/src/java/com/twitter/heron/serializer/kryo/KryoSerializer.java. I think this naming is ok as is. It's consistent with how we name scheduler implementations (i.e., c.t.h.scheduler.[aurora|marathon|yarn]). The location of the code could move in or our of the contrib folder without requiring a package renaming.. Can we use generics in the State API for better type safety? (Map<<K extends Serializable>, <V extends Serializable>>). Is this something the user needs to set, or the field that Heron internally sets when inspecting the DAG? (I would expect the latter, in which case can we document that or not expose it as a user setting?). Can we include seconds in the variable and field naming to be explicit.. Is this the implementation class? If so can we use class instead of type to be more explicit?. Is this a pointer to a config file? If so can we include file in the name and clarify in the comments?. +1 for IStatefulComponent interface with common methods. If we later find that bolts and spouts need specific methods we can subclass IStatefulComponent. Fewer interfaces and a shared top-level interface hierarchy will simplify the lives of both heron and user developers. There will be places where we will need to do common operations on stateful components and having to check both stateful bolt and spout will cause code duplication and headaches.. Why does the user need to set topology.stateful? Should using IStatefulComponent and configuring the stateful store be sufficient? Are you thinking they'll want a flag to turn it off? . Map<<K extends Serializable>, <V extends Serializable>> says that we should take any kind of state as long as it is serializable. The difference is how the users will write their code, not how the system deals with serializables. For the spout and bolt in your example, one would be able to work with <String, Integer> while the other would be able to work with <String, String>. This allows the user to write type safe code with compile time checks, while still allowing the system to persist any types of serializables.\nTo do this we would have something like this, where the user knows they can work with <String, Integer> for example:\ninterface IStatefulComponent<String, Integer> {\n  initState(State<String, Integer> state);\n}. Would we need this globally, or per component? Assuming globally, could we give a more descriptive name like topology.component.state.enabled or something like that?. yes, but also if we had K, V users could write this:\n```\n  class MyStatefulBolt implements ITypedStatefulComponent {\n    @Override\n    public void initState(TypedState state) {\n}\n\n}\n``\nand work with the types that it expects, just like they would while working with a typed map likeMap(instead ofMap`. This provides compile time checks in user code for their types and removed the runtime need to cast objects, which leads to more bug-free and easy to maintain code.\nI would need to prototype though to see that we could in fact create an instance of TypedState<String, Integer> at runtime though, give persisted Serializables and an instance of MyStatefulBolt<String, Integer> to derive types from.. Correct. Here's some mock code to show what I mean. Note that the internals use serializable but the user land code can be typed.\n```\npublic class StateExample {\ninterface TypedState {\n    void put(K key, V value);\n    V get(K key);\n    Set keySet();\n  }\ninterface ITypedStatefulComponent {\n    void initState(TypedState state);\n  }\nstatic class MyStateImpl implements TypedState {\n    private Map map = new HashMap<>();\n@Override\npublic void put(Serializable key, Serializable value) {\n  map.put(key, value);\n}\n\n@Override\npublic Serializable get(Serializable key) {\n  return map.get(key);\n}\n\n@Override\npublic Set keySet() {\n  return map.keySet();\n}\n\n}\nstatic class MyStatefulBolt implements ITypedStatefulComponent {\n    @Override\n    public void initState(TypedState state) {\n      for (String key : state.keySet()) {\n        System.out.printf(\"key: %s, value %d\", key, state.get(key));\n      }\n    }\n  }\nstatic TypedState loadStateFromDisk() {\n    Serializable key = \"String\";\n    Serializable value = 2;\nTypedState state = new MyStateImpl();\nstate.put(key, value);\nreturn state;\n\n}\npublic static void main(String[] args) {\n    MyStatefulBolt bolt = new MyStatefulBolt();\n    bolt.initState(loadStateFromDisk());\n  }\n}\n. Can we name this in a way that implies that heron state management is enabled or disabled by this boolean flag? It should be very clear to the user the effect of using IStatefulComponents for example with this set to false. Or what this should be set to for an externally stateful topology that doesn't use IStatefulComponents. As named it sounds like it describes a characteristic of the topology, versus the behavior of the system.. Instead of \"Is this topology stateful?\" can we clarify in the comments what happens when this is set to true/false? For example if I set it to false but use IStatefulComponent, I assume I can still be in-memory stateful, but the state will not persist upon instance failures, scaling, restarts, etc. since the framework will not manage the state transfer/persistence (even if I configure a non-memory state provider below). Would setting this to false with a persistent state provider configured be the equivalent to specifying this to be true while configuring an in-memory HashMap provider?. we're combining the behavior of state management with atleast/atmost/exactly once semantics, so I think more explanation is helpful. For exactly once semantics do we require acking to be enabled and topology.stateful.enabled, for example? We should clarify then what happens w.r.t. atleast/atmost/exactly once when these two settings are true/true, true/false and false/false.\nalso typo: exacltly. Instead of storing an string id and a string[] backupId, could we instead store an ordered array of Checkpoint objects, most recent first? Using an object instead of a string would allow for extensibility to bind more metadata to a checkpoint than just id (e.g., creation time, location).  We could adjust how many checkpoints we keep via config (either time based or absolute count), which becomes the size of the array.. message is stored in state manager right? zk is just one of state managers impls.. Why an inner class? If we make this a top level class it can be used on its own without the collection, like in the RPC for example while working with a specific checkpoint.. If this is an RPC request object should it be StartStatefulCheckpointRequest?. Is this the payload of an RPC request? If so it should follow the same standard and be ResetTopologyStateRequest.. Do the heron client/server model use a protobuf RPC interface or do they just send protobuf object bytes on raw sockets?. Why does the tmaster need to be aware of specific instance state? Why not have the stream manager manage the states of the instances under it's control?. When/how is this message used and how does the stream manager get into the state that is before or after this one (i.e., one where it's not started)?. Why does TMaster register itself with the checkpoint manager instead of the other way around, like the way state manager registers with TMaster? TMaster is already discoverable in StateManager.\nAlso space before {. Why would checkpoint manager need to be told the topology name and id? This info is static and can be passed to checkpoint manager at startup. Same question for RegisterStMgrRequest below.. If these fields are both optional neither of them can be set, which I assume would be invalid. If that's the case can we enforce that one of them is set view oneof? That would assure that every possible field setting is valid.\nhttps://developers.google.com/protocol-buffers/docs/proto#oneof\nWhat's the expected behavior when oldest_checkpoint_preserved is unset and clean_all_checkpoints=false?. cleaned_checkpoint_ids. \"establish the connection\" should be \"register itself\" no?. space before {. Is this the message or is this the state info that's part of another message? Reading above it seems the latter but the description states the former. If it is the former, can we put a verb in this object, to make it more clear what action is being requested?. typo: instace. Same question as above w.r.t. state changes. If this request triggers processing to start, how does the instance get into the state where it's not processing?. I thought checkpointing was non-blocking, but this comment sounds like checkpointing happens serially from one component to the next. Why?\nAlso why does each instance broadcast this message to all instances of the downstream component?. Can we more clearly specify state than an arbitrary array of bytes? What is this byte array and how does the StateStore implementor know how to read the bytes? How for example would I persist the byte array in a key value store in a way where I could inspect individual records?. We need to associate the checkpoint with a packing plan or physical plan so we know it's valid to reuse upon topology restart.. Why is this at the instance level and not the component level? Couldn't \"Once an instance is done checkpointing\" be \"Once a component is done checkpointing\"? Also, if each instance is broadcasting to all of it's downstream instances, that implies that the downstream instances need to be aware of and track the id set of all of its' upstream instances, and which ones have sent the message? Pushing this responsibility onto each instance seems unnecessary. Instead could TMaster message all instances of Component B (via stream managers) that it's time to checkpoint, based on it's knowledge that all Component A instances are complete?. This text description is helpful, but it would still be great to have a sequence diagram of these flows. Visualizing the interactions between the many agents really helps to understand the system and to expose quirks. Here's an example that we created to clearly illustrate the Topology Submit Sequence (which is less complicated than these flow IMO), for example:\nhttps://twitter.github.io/heron/docs/concepts/architecture/. upon startup. all stmgrs connect to. in turn. s/exactly once semantics/stateful topologies/. \"because instances are all busy recovering their state\" assumes that the reader knows the startup sequence of events, which is what we're trying to describe. Could you list the sequence of startup events for stateful topologies? It should be very clear to the reader the order of things like fetching the packing plan, initializing state, registering with stream managers, physical plan generation, etc.. receives. Two questions:\n1. Could deactivate/activate be used for this, or would that overload the activation semantics?\n2. This message would be sent when recovering to a given checkpoint, during which processing is already started, so this message would also be used to reset a started topology to a given checkpoint, correct?. s/exactly once semantics/stateful processing/. omit: sends. I think your comment was about my request for a sequence diagram. What about my comment about how we need to bind a checkpoint to a given packing plan or physical plan to know if it's valid upon startup?. It's  listed in the version 2 spec:\nhttps://developers.google.com/protocol-buffers/docs/reference/proto2-spec\nWhat's the minimum version that supports it? We should consider upgrading, since this was included in 2.x.. Can you add a TODO next to that then to clarify that we don't want to ship with this being a byte array?. Can we change this to IStatefulStorage or maybe ICheckpointStorage and change the package to storage?. HDFSStorage. please use String.format(\" ...\", ...) for improved readability. Favor returning void and throwing a StatefulStorageException instead of using booleans to represent failure conditions, here and elsewhere. See https://github.com/twitter/heron/issues/1311. Why does ckptmgr-java exclude CheckpointManager.java and ckptmgr-unshaded include it?. Can we package the specific storage implementations independently of the API interfaces and core snapshot code? Also elsewhere we split up core/API and impl classes in scheduler-core/schedulers at the top level. Should we follow that pattern here too?. please provide javadocs.. please fix to read from config.. pleae use an Options/CommandLineParser instead of positional fields. See RuntimeManagerMain for examples.. Ugh, we need to upgrade these methods to use ByteAmount instead of ints, but that's for another PR.. This causes a failure where some children are deleted but others are not, so it's not clear how a caller would recover. Could we instead have this return void and throw an IOException that includes the failed path?. 1. We typically don't want to append messages to an exception, since it swallows it the exception chain.\n2. Also we should never throw RuntimeException since it's too generic for a client to derive any real meaning from.\n3. Always include meaningful info in the message to help the user, like the classname.\nInstead we should do something like this (similar for the rest of this class):\nthrow new CheckpointManagerException(classname + \" could not be initiated\", e);. Please follow the same pattern that we use elsewhere where we use an enum for the key that defines the type of the value. See SystemConfigKey and the getters that use it in SystemConfig.. This logic already exists in ConfigReader.loadFile(..).. Please follow the same builder pattern that we use elsewhere. See SystemConfig.newBuilder(..).. s/backendConfig/config/. Could we instead create a CheckpointRequestHandler interface with a single method and have a class for each of these? That approach would be more OO and would allow us to share more of the copied code in these methods in an abstract handler superclass. The interface could look like this:\ninterface CheckpointRequestHandler<T extends CheckpointManagerRequest> {\n  protected void handleRequest(\n    REQID rid,\n    SocketChannel channel,\n    T request);\nThis means the requests types need to be part of an proto enum (or oneof) so we have a common hierarchy.. See other comment about not using RuntimeExcepion and not swallowing exceptions.. Can we instead use a Config class here?. throw new CheckpointException(\"Failed to persist checkpoint with id \" + checkpoint, e);. Prefer String.format(..) for static string template assembly. Here and elsewhere in this class and in LocalFS.. LocalFilesystemStateStorage. Thanks. I missed the java_library and java_binary part the first time through.. Is this necessary? I don't see it accessed. Ideally we should prefer classes that take their customizations in the form of specific constructor args, versus static config objects that classes pick specific things from. This helps with testability and clearly defining the class contract.. omit: for. s/Id/checkpointManagerId/. Can we model this config class more like SystemConfig and SystemConfigKey, where enums are used for the system keys, which also define their type and default values?. Nit: can we call the NIOLooper looper, or slave instead of s?. will this happen often? If so maybe it's debug logging?. Can we make these return void and throw one or more custom exceptions upon failure, instead of using the \"booleans to represent failures\" anti-pattern? See https://github.com/twitter/heron/issues/1311. omit: Initialing... . please be more specific in the log message re just what \"URL\" is. Also why are we logging the user's home dir? Seems better to log the root dir where data will be stored.. please don't throw RuntimeException unless we're in a main method. Can we instead throw something like IStatefulStorageException?. Can we call this getCheckpointFile so it doesn't sound the same as getCheckpointDir?. We should write these with an assumption that they're self explanatory when combined with a bunch of other unrelated configs. Something like heron.stateful.storage.hdfs.root.path.. s/backendConfig/statefulStorageConfig/. Can we break these up so that CheckpointManagerConfig.java contains the Checkpoint Manager configs and StatefulStorageConfig.java contains the storage configs? It will help to separate concerns and to more easily initialize unit tests.. void init(StatefulStorageConfig conf);. this should be part of initialization so we fail to start versus failing later upon first checkpoint.. Local and hdfs storage are basically the same with the exception of how files are read/written. Can we please use a common abstract superclass for consistency and less code duplication? Similar to what we did with FileSystemStateManager. I'm already looking at this class and needing to make the same comments twice due to copied code.. s/hdfsBackend/hdfsStorage. s/localFSBackend/localFileSystemStorage. s/LocalFSStorage/LocalFileSystemStorage for consistency with LocalFileSystemStateManager. s/LocalFSStorageTest/LocalFileSystemStorageTest for consistency with LocalFileSystemStateManager. By keeping this here we're continuing to perpetuate a pattern that we should avoid/remove. If there are no current consumers of this and we don't want consumers, it seems better to not include it than to maintain consistency.. Using an enum of supported key values, that includes their types and default values isn't really heavyweight, it's good practice. It leads to easier to understand code with fewer bug and a better user experience. By following the approach in SystemConfig we can have the following:\n\nType checking of field values. This has caused bugs in the past. Here's one from last week #1841.\nA type-safe way to represent default values that is compile-time verified.\nAbility to confirm at startup if someone has mis-typed a key name, causing some functionality to silently be enabled/disabled.\nA centralized place to manage keys and their defaults\nA way for the consumer to have some way to know the type of a given value, that's not coupled to the specific logic of the value producer.\n\nLet me know if you'd like to discuss the pros/cons more offline.. There's a lot of code in Heron that has evolved in a way that has been loose/sloppy/bug prone/hard to maintain because short cuts were made initially. Before and after open sourcing we spent a lot of time cleaning up tech debt. Search for PRs that include \"refactor\" or \"clean up\" and you see 50-80 closed tickets. It would be great if we could not be subject to that here.. By following an OO approach using a handler interface (versus the procedural approach) we can support the following:\n\nability to share common code in abstract or concrete superclasses (\nability to use the decorator or filter pattern for common handler actions\nmore modular unit tests and test classes\nsmaller classes and methods\neasier to understand code (the RPC function name is the class name)\n\nIf you'd like to not expose the handler implementations to other classes you could use access level modifiers on the classes, or the methods, or better yet, package classes in a way that is specific to their function or preferred visibility.. s/bp/backpressure. s/fetch_bp/fetch_backpressure. s/fetch_bp/fetch_backpressure. Here and elsewhere, let's use backpressure instead of the abbreviated form of bp.. a.) The current approach allows us to share code via composition. Using a handler interface allows that as well as via polymorphism or the decorator pattern, the filter pattern or the template pattern, which can be useful in request handling.\nc.) Note the \"more modular\" part.\nd.) Overtime this class will continue to grow in size as each RPC develops more functionality and bug fixes and as we add more RPC methods. It's generally good practice to favor smaller classes that support a specific purpose when possible, versus combining multiple concerns into one. Even with it's current size it would be easier to understand the codebase if you could browse handler classes named after their RPC call. Hence putting good patterns in place now will help prevent bloat. Also see https://en.wikipedia.org/wiki/God_object\ne) Using the handler pattern won't add coupling to other backend functionality. If you're concerned about mixing RPC classes with other unrelated classes, then we could put the RPC/server related code in it's own subpackage.. Tunneling could be needed for any of the schedulers and they're valid configs so I think it's ok to keep them in the config file. Maybe we set it to False by default though.. +1 for ways to do this without one-off logic to support ECS.. +1 for removing as much of this hard-coding as possible.. Why does this code require a dep on heron-examples.jar?. Why does jvm location need to be hardcoded?. Both PART1 and CMD are only used once. No need to public statics for them. Also there are a bunch of built in assumptions in these commands that should be either config or command line driven. . please remove and replace with valid javadocs, without personal attribution. Same for other classes.. please remove empty lines before } lines in this class. please include empty line between members and methods. why does each container need a different set of port numbers?. remove?. Is String.valueOf( needed?. Does ECS always use docker containers? I thought you could also just run a given AMI without docker, no?. We should either throw an exception here, or log a warning to the logging framework, but we should not do e.printStackTrace().. Why are all these methods public?. The 5000 here and the logic in this method looks duplicative of other logic in this class. Can you consolidate?. Favor StringBuilder over string concat. Or better yet, use String.format(..). please clarify in the javadocs what this method does. Also I suspect StringBuilder might be a better fit here.. return builder.toString();. These methods all need to be implemented and annotated with @Override.. +1 to update all classes to use FileSystem instead of FS.. s/checkpointsBackend/statefulStorage. we should include the classname in this message as well.. omit ': '. Can we not support generic OBJECT? The idea is to only support specific types.. Can this be a Config class of some sort, instead of OBJECT?. I'm not seeing any changes since my initial comment. Also please add test to FileUtilsTest.. s/mockFSDateOutpurStream/mockFSDateOutputStream. Can we name this method better? It can be a bit confusing having a Checkpoint class with a checkpoint() method.. I mentioned this in the other PR, which is that we should store something more structured than just an arbitrary byte array. IIRC we're only storing bytes for now while prototyping. If that's the case can we rename this from saveBytes to something more descriptive of what it will change to, something to do with persistable state maybe?. If the directory doesn't exist that should be an exception correct? If it's never acceptable for this method to not create a dir, let's change to return void and throw upon failure.. Can we add unit tests for these methods? This will keep a change in path naming structure from being introduces inadvertently between releases.. lots of duplication of boilerplate constants and before() logic in each of the stateful storage tests and in CheckpointManagerServerTest. Can we share this code in some test helpers?. ouch. HeronSocketOptions also needs to be updated to use ByteAmount in another pr.. Can we refactor how tests are run to not need this sleep (and all the others)? Tests that require sleep are likely going to be flaky. Also  Hardcoding sleep times in tests has slows down CI, which is already too long (this class spends 20 seconds just sleeping!).. Let's use static imports for all the Assert.assertFoo calls as well to improve readability.. Any chance these request classes can be part of an protobuf enum so we could use a case statement instead of the if-else/instanceof stuff?. Here and elsewhere in this class can we do LOG.log(Level.WARNING, \"Request to dispose checkpoint with id ... failed with error\", e); (or log error) so we don't swallow the stack trace?\nAlso can we add a message to Status.message as well when we fail, here and everywhere else in this class? That could help with debugging.. What happens when an unchecked exception occurs w.r.t. the response sent?. This assertion logic is copied in this class. Please make a generic assertValidRequest method so we can get code reuse.\nHow come the assertion of valid topologyName and topologyId only happens in some methods but not all?\n(I have a broader question which is why is topologyName/topologyId even passed around at all, since they are know at startup time and are static.). How is the implementation of this method different from handleTMasterRegisterRequest? They appear to be duplicates.. Please favor String.format(...) for readability here and elsewhere.. please don't swallow e here and elsewhere where this code is copied. Also please don't copy the logic. Instead make a helper method to do this consistently.\nLOG.warning(\"Failed to close connection from: \" + connection.socket().getRemoteSocketAddress(), e);. please remove res, it's not used.. Can we change this method so it doesn't mutate the state of the checkpoint passed to it and instead have it return a new Checkpoint? Generally it's preferred to favor immutability and to have methods without side effects. It also makes the contract very clear to the caller that it is getting a new checkpoint in return that is different from the one passed. . Is this comment in response to mine about the IStatefulStorage impls to use a common hierarchy? If so I wasn't suggesting that they all need to, but those that are file based should have a common hierarchy since their implementations will be very similar. I'd expect an AbstractFileSystemStatefulStorage super class for the two we have, but not for some KV store, for example.. When a checked exception occurs, you set the status to NOTOK, create a responseBuilder and call sendResponse. What happens when an unchecked exception occurs? Does the status still get properly set (by the superclass?), and does the response get properly sent?. Thrift allows the ability to do something like this (psuedo-code), which is really convenient for things like this. It doesn't seem like protobufs allows this though. It seems to limit the enum values to just constants, which is unfortunate.\nenum RequestTypes {\n  RegisterStMgrRequest,\n  RegisterTMasterRequest\n  ...\n}\n  . That's totally sufficient for the implementors of IStorageInterface. The issue I'm seeing is that one of them throws an unchecked exception due to a bug. In that case that exception seems like it will get thrown all the way up to HeronServer.handleAccept and then continue up to the slave looper Runnable, which would kill the looper thread and leave the server in a zombie state. Somewhere RuntimeException needs to be handled to avoid this possibility.\nIdeally we could do that generically in HeronServer, but since it doesn't know what type of response to return in that situation it can't handle that. If we had a generic interface to handle different requests/responses though, we could decorate that in a single generic handler that catches RuntimeException and returns the appropriate response, as defined by the implementor of the handler interface. Or maybe HeronServer handle it generically if it was extended to have a map of response types, just like it has a map of request types.... s/getBackendConfig/getStatefulStorageConfig. s/backendConfigObject/statefulStorageConfigObject. s/backend/stateful. s/backend/stateful. s/backend/stateful. s/checkpointsBackend/statefulStorage. s/checkpointsBackend/statefulStorage. s/backendStorage/statefulStorage. I thought the checkpoint manager was a long-running daemon that should be up for the duration of the topology? If that's the case, we wouldn't want an exception thrown during a single RPC call to kill it, right?. The methods below are all duplicated in LocalFileSystemStorage.java. Please share them somehow.. ROOT_PATH_KEY is duplicated in LocalFileSystemStorage.java. Please share it somehow.. There are 9 lines of duplicated validation code in two adjacent methods. Can we please share that in a common assertion method?. These can all be local variables in setup() except for TOPOLOGY_NAME and CHECKPOINT_ID. All others are not used, except during initialization.. lots of duplication of boilerplate constants and before() logic in each of the stateful storage tests and in CheckpointManagerServerTest. Can we share this code in some test helpers?. I see. I missed that part.. Why would we not want to standardize on a single directory structure layout for local versus hdfs? I would think we'd want to prevent their structure from diverging, not encourage it. This is similar to what we did with state server.. Whenever we set a status code that is not OK, can we also set Common.Status.message as well to give a brief description of what went wrong. This will help when troubleshooting a failed RPC from the client logs, by removing the need to have to inspect the RPC server logs and remove the need to try to correlate server logs with client logs.. The local filesystem is for testing and would be best to be implemented the same w.r.t. structure as the HDFS version, so people see apples-to-apples of what HDFS does. Having inconsistencies between the two seems really undesirable.. This class still uses backendStorage.. s/localFileSystemBackend/localFileSystemStorage. Can we set an error message here?. Can we set an error message here?. This errorMessage is never set on the response.. This errorMessage is never set on the response.. Do you really think it's the best developer experience for us to implement the two filesystem layouts differently? Based on that logic, we should also implement the local filesystem's IStateStorage class differently than we implement the Zookeeper implementation?\nThe two impls are related in that what a developer sees on the local filesystem while testing and what they expect to see in HDFS. Providing two different approaches for that would be really confusing and unintuitive.. The collector and the simulator both still track how many bytes they've emitted as a long, so I convert to that and do the comparison. I've mainly just been converting the static ByteAmounts that are read from the configs, but not the dynamic ones that get frequently updated at runtime. I'd like to also update the latter but I need to look at that more closely to make sure we can always do that efficiently without too much object creation, versus incrementing a long. I'd like to hold on that for a follow-up PR.. Same comment as above. For now, holding off on updating dynamic byte amounts.. See my response above. Holding off on highly dynamic byte counters for now.. @nlu90 I tried splitting up the code paths and it got pretty redundant and hard to follow. \nTo remove the if/then I'd need to do the emit direct check at a higher level when creating the tuple builder. But the tuple builder is created in both SpoutOutputCollectorImpl.admitSpoutTuple and BoltOutputCollectorImpl.admitBoltTuple after doing other validation. To do the emit direct check separately, I'd need to have 4 more implementations of the same validation (with/without emit direct ids for Bolt/Spout collectors), and then support two similar code paths in AbstractOutputCollector to support emit direct, and non-emit direct. The latter impl would also have the custom grouping checks at 2 levels lower than the emit direct checks.\nI can show you what I mean if it's not clear, but if you have a specific suggestion for how to avoid what I'm describing please let me know.. The object contains multiple checkpoints so we should put an 's' on the method name. Also can it take some identifier of the currently topology like the packing plan id? For example upon restarting a topology we need to know if the previous checkpoints are still valid for the new plan, if it's been changed between deploys.. This method should set a single checkpoint, so StatefulConsistentCheckpoint not StatefulConsistentCheckpoints. It will also need to set them in a way that we can a.) delete a single checkpoint and b.) identify the packing plan that it's bound to.. The deleteStatefulCheckpoint method needs to take an identifier to make it clear which checkpoint is to be deleted. The state server stores multiple checkpoints per topology. setStatefulCheckpoint sets a specific checkpoint, not a collection.. Multiple checkpoints must be persisted and managed by the state server, but they're set one by one, each time a new one is created. Think of it as a Heap, where new ones are pushed to the head and old ones are periodically deleted from the tail. At any time the entire set could be fetched.. We should store the checkpoints in a way that allows us to easily delete (or manually browse when troubleshooting) an entire set by the packing plan hash and checkpoint id, so we should considering including those in the path in the directory tree.\nConsider the use case where the topology gets redeployed with the same parallelism and packing plan hash. We can reuse that state with the new topology. If instead the topology is deployed with different parallelism (i.e., a new packing plan), we can not reload that old state directly. We also might want to easily prune all old checkpoints bound to old packing plans. Hence, being able to delete an entire file system branch would be ideal.. We want to use plural here for consistency, right? statefulcheckpoints. Could we instead attempt to set the location again in ZK if it's lost?. Is it possible that a lost ZK location gets recovered or once it's lost is it lost for good? Is there a scenario where brief network faults would cause this to be lost and the quickly recovered? I suspect it's lost for good, but I don't know for sure.. Yeah good call @nlu90 . I moved TestCommunicator into c.t.h.common.basics to be along side Communicator. I also moved awaitMultiCountMetric into the test that needs it (not yet put up for PR).\nThe remaining inner classes are now more closely coupled with HeronServerTester. They're basically the handlers and the client helper classes. I'm normally not that big a fan of inner classes but in this case I think it makes sense to keep them as inner classes for now since a.) their impl is closely coupled to HeronServerTester and, b.) I don't want to pollute the c.t.h.common.network package space with too many test-related classes.. Do we need this? We mostly refactored out the concept of \"sandbox\" by introducing config value wildcards.. s/backendConfig/statefulStorageConfig. It took all I had to not update that field :) because I didn't want to clutter up the diff. If everything else looks ok I can make that change.. Why do we need classpath and custom classpath? Can we get rid of the later?. Got it. Should we change custom to additional to be consistent with Config.TOPOLOGY_ADDITIONAL_CLASSPATH?. Can Xmx be configurable? Since all state transfers through the checkpoint manager, it could possibly need more memory.. Are these configs for checkpoint manager and the IStatefulService implementation or just one of the other? . I know you're following the existing pattern, but STATEFUL_YAML and Context.statefulFile are not really clear. Can we use something like STATEFUL_CONFIG_YAML and Context.statefulConfigFile?. Yes, since Void is uninstantiable. It's a placeholder to be used as a type where needed, but you can't actually return one, so you instead return null.. The 3 bool methods have identical implementations except for the TopologyConfigVars object checked. Can you have each method instead delegate to an internal helper that centralizes this logic that takes a TopologyConfigVars object?. Why is this method required?. Where does this field get set? Is this controlled by the user? The topology sets acking enabled or not for at least and at most once. How does exactly once get enabled and how does that relate to acking settings?. Yes, good call. Updated.. Yes, good call. Updated.. The user has 3 options: exactly once, at most once and at least once. Can this Config setting take an enum of one of those 3 and do the right thing re acking etc? If we have a simple approach like that, there is no user education required about the various combinations of setting required for the different semantics.. Can IsTopologyStateful also call GetBooleanConfigValue?. This PR is adding support for a boolean for exactly once, which I don't think we should do, so it is relevant. Doing so will create a poor user experience/design where the right combination of isExactlyOnce and acking needs to be set to for people to get what they want, which is exactly once, at most once or at least once, which is how users think about semantics. Can we design this properly and take an enum? What's the reasoning to not do so now? The enum can only have ExactlyOnce for now if you don't want to fix the other parts. If we did that we at least wouldn't be introducing a new config API that should be deprecated upon launch.. Apologies, I missed that Config.setTopologyExactlyOnceEnabled(boolean) was already committed. That should be fixed before we ship. We've discussed the need for better support for how users specify semantics in the past, and the desire to get rid of the acking setting required by the user (acking should be an internal implementation detail used to support at least once). We should do that refactor before exactly once ships so we don't perpetuate a confusing design. But that can be done in another PR. Can we follow up with that after this one so we don't write more code that works with a boolean instead of an enum? The further down that path we go the more painful the refactor will be.. Cool catch, updated.. I would envision we deprecate the setAckingEnabled method for a few releases while updating it's implementation to set the semantics enum to atLeastOnce. Then we can migrate before ultimately removing the acking setter.. Opened issue #1904 to track refactoring out setTopologyExactlyOnceEnabled. Great catch! Could we instead change deactivateTopology to return the new topology, and push the fetch into that method? We can then assert the returned topology in the unit tests.\nAlso how did you realize this as an issue? How did it manifest itself?. Ah right, we'd have to pass in proposedPackingPlan to the deactivateTopology method. That's unfortunate, but I think that's our best option. That way we can at least encapsulate the deactivateTopology logic to return a new valid topology that aligns with the one passed in.. Is there a way to covey that this doesn't actually represent a file or path on the local disk, but the name of the file that will be invoked on the executor?. .desc(\"The remote filename of the heron topology jar/tar/pex file to be run by the executor.\")\n.argName(\"topology binary filename on the cluster\"). Is 2PC 2 Phase Commit? If so can we expand that here and elsewhere in this patch. That abbreviation is not obvious to the code reader.. Can you clarify what happens in the log message (e.g., do we ignore the request or start another one?). If we're able to start and do another one, why is it a warning and how does shared state of this class in_progress_, restore_txid_, etc) not collide?. Can you provide a link to Parsely for reference. Also should we change to README.md to be consistent with other readmes in the repo?. In that case can you include in the log that the restore with id checkpoint_id_in_progress_ will be abandoned? . Why is StMgrMapConstIter needed? I don't see it used anywhere.. please add class-level docs describing what this class does and what it's responsibilities are.. who calls HandleInstanceStateStored? Why does this need to be public?. Why is this redefined here?. How do we know that a checkpoint is valid for a given packing plan? Should we build this into the id or bind it some other way? We will need this to know if a checkpoint can be assigned to a redeployed topology with a different packing plan.. Fetching the packing plan is required for initialization so we can determine the stream manager id set (absent_stmgrs_). We need to be prepared with this info before TMaster registers it's location and starts receiving registration requests from stream managers.. No, we're not actually. What is the deterministic way to know that? We could set a flag during a successful SetTMasterLocationDone call to indicate that we're TMaster since we successfully set our location.. @srkukarni I added a flag to denote if we're master. LMKWYT. s/Restore/restore. Please also replace the 2 other occurrences of Restore in this log message with restore.. I've seen this method in a number of test classes. If you have ideas for how to effectively share test code like this that would be awesome.. Why is 'sandbox' in the new arg names? Can we remove that?. why are these being hard coded here and why use strings to represent integers?. What are the acceptable values for autoHeal and autoHealInterval and can these be Duration?. please add unit tests for the new logic in this class. This logic makes sense, but what doesn't is what currentTime is expected to be after the loop on line 225. Because it gets reset on line 213, after the loop it seems to represent the time after the last tuple was deserialized but before it was handled. This seems really arbitrary. If that's really the intent could you add some comments below to clarify the time math that is occurring after the loop with currentTime - startOfCycle - instanceExecuteBatchTime.toNanos().. Can you clarigy (in comments) what currentTime is expected to reflect when it's used after the loop? It's not clear from reading the code.. In the past we've had issues with credentials being logged when doing this. Can we change this to FINE or remove it. If we keep it, it's also good to add some sort of contextual logging to the message.. Please add unit tests for add/remove containers and the spec builder methods.. Is this path always this way for all kubernetes users? Is default ever something else for example?. Could we reduce the visibility of many of these methods to package private?. success can be removed and this can just be in the if clause. same for below.. can we throw an exception here instead of returning null? same for other occurrences in this class.. variants of this uri building are repeated in this class. could you centralize this logic into a shared method or member? During initialization of this class it seems we could replace set kubernetesURI as \"[kubernetesURI]/api/v1/namespaces/default/pods\" which would help.. instead of returning a boolean to indicate success, can we return void and throw an exception (maybe SchedulerException) upon failure? Same elsewhere. See #1311.. in all of these messages we should include more contextual info to help the log reader understand the issue.. this one for example, should include the uri. The following one should include the uri and probably the json post body.. This same 6 lines of connection initialization logic are repeated in each method. could you put them into a shared helper that takes a uri, returns a connection and throws if it can't for any reason?. for (String appConf : appConfs) {. You can remove the need for allSuccessful by doing this here and returning true at the end:\nif (!deployContainer(appConfs[i])) {\n  return false;\n}\nOr better yet, change this method to return void and throw an exception if one fails. It can have detailed info included about what went wrong.. Why is the proposed approach a better one? With the proposed approach tmaster would register itself as master, which would result in stream managers trying to connect to it. They will fail until tmaster continues it's initialization and opens its server socket, which seems undesirable.. If the intent is to not register the packing plan listener that handles updates (and physical plan deletion) until after we're fully up and running, that seems like a good call. We can differ the packing plan watch initialization until after we've become master, while still fetching the initial packing plan before registering location.. I'm really not as fan of returning null to represent exceptions (see rant in #1311). If we can't get a response code it's an exceptional case, so I think we should throw an exception.. I recommend we put this in c.t.h.common.network since it's generic and not specific to schedulers. Also instead of a bunch of static helpers, could follow an approach more like the java network libraries or HeronClient, where we initialize a HttpJsonClient object with a base uri, and then call one of get, post, put delete on it? That way we can provide better control/contact over the connection lifecycle. It also makes it easier to use in unit tests via mocks.\nWe could throw IOException instead of TopologyRuntimeManagementException.. close parens or open curly don't get their own line in our styles.. Instead of initializing namespace, can we initialize \"%s/api/v1/namespaces/%s/pods as baseUriPath? Every method uses this as their base, so this would reduce duplicate logic.. Looks like kubernetesURI is only used in the constructor now so this can be removed.. can we remove this method since it's implementation is only a one-liner?. General comment about logging and exceptions... Typically we either log an error or throw an exception, but not both. Doing both tends to clutters up the logs with duplicate info. Let's instead just throw here, and include more contextual info in the exception message to clarify which container we couldn't remove. Also we should always chain the exceptions so we can get a good stack trace. So something like this:\nthrow new TopologyRuntimeManagementException(\"Unable to remove container with id \" + container.getId(), ioe);\nComment applies here and elsewhere in the patch.. pls remove empty lines between closing brackets.. Does kubernetes provide the ability to add/remove multiple containers with one call? Doing it individually leaves us in a bad state if we fail midway through. If not we should clearly log that state upon failure (which ones succeeded, which failed or were never attempted).. please include comments about what's happening here with an example.. can we include the relevant version numbers in the error message here?. Can we replace all Object references in the method names of this class with StorageObject to be less generic?. ...a path to the service.... Here and elsewhere can we throw the original exception and not wrap in RuntimeException?. here and in other classes, please remove empty lines from between class members of same type.. it's best to re-initialize mocks in the before() method.. instead of subclassing, could you instead return a mock GcsUploader that has the createGcsController method mocked to return the mock GcsController. better to consistently use mocks than a mix of mocks and sub-classes.. createStorageObject. Can this return a Duration object instead?. can this be private? same for other non-override methods in the other sensor classes.. can this be private?. @Override?. could you elaborate on the javadocs for this class and how to use it.. Can this class leverage ConfigReader to load yaml data from a file path?. for (int value : values) {. no star imports, here and elsewhere.. as discussed, this will need to be a released maven jar not bound to an individual.. I would recommend letting methods declare their exceptions as much as we can. So createGcsController throws IOException and GeneralSecurityException, and the initialize method catches both and wraps in RuntimeException.. Can you fetch the version from the StorageObject? Are there even versions that would be useful for a user to see?. It would be great to change the existing methods in this class to also return void and throw exceptions instead. I know that's scope creep on what you're adding in this PR though, so we could make that change in a follow up PR if you prefer.. can we remove the logging here and add any relevant info to the exception message? Instead of log info, log sever and throw exception, best to just throw the exception with a good message.. Could you create a google_client_version=1.22.0 at the top of this file and reuse that for each of these so we always upgrade in lock step. . I had the same thought but kept it because the section title is Relationship with Storm, so the intent I believe was to clarify the differences. I'm fine with removing the Storm mention here if we think the section would still make sense without being explicit about the distinction.. Here and elsewhere can we not echo the method name in the javadocs, since it's redundant. . here and elsewhere let's not specify @return [type] unless we have something specific to say about what's returned. The return type will be included in the javadocs without specifying @return String.. please include a link to a github issue for this.. here and elsewhere please remove empty returns.. typo: dowp. this seems like a functional change beyond just what's required for the testing, no?. why is this being removed?. If we're going to copy this pattern for other calls, we might want to consider a generic method that takes a string of the heron command and a collection of strings of popen commands. Using strings will likely be easier than arrays. The shared method would deal with converting strings to arrays.. OptsTest sounds like a test that verifies option parsing logic. Can we call this something more representative like SubmitTest or something (which extends a common ClientCommandTest or something like that)? We can then have a bunch of tests to represent how different args to submit result in different popens.. is pass necessary?. I've had this issue too. For some reason checkstyle doesn't always seem to fire on mac and I haven't been able to figure out why.. right, so should this change be reverted?. IMO putting spaces at the end of multi-line strings is a pretty standard thing to do, so I would recommend that.\nWe could even evolve this to read the commands from a text file at some point if we wanted to remove the need for long strings in code.. Could we instead allow the caller to pass the entire list of handlers, so they could either remove or add to the defaults?\ndef run(handlers=utils.default_url_to_handlers). I wouldn't expect someone to remove from the url_to_handlers list, but instead pass a new list, that might include some of those entries.. tuplesToAck gets incremented when a test message is sent, but decremented when any ack is recieved, even if it's not a test message. This seems like a bug, so I changed decrement to only happen when acks for test messages are received.\nI actually don't know why we need to track tuplesToAck, and why we can't just check that size of the pendingMessages map gets to 0, but I'm not ready to make that change yet.. Realized this isn't the right change and reverted it, since there are cases where tuples are emitted with a pre-existing message id, in the case that they're not test messages.. Can this artifact be put in maven? This will break Twitter's internal CI build, which doesn't not have access to the public internet, only an internal maven proxy/cache.. would you please remove the quotes around numbers as json values.. elsewhere in the repo we abbreviate manager with mgr. Please name this dir healthmgr for consistency.. These proto deps can all be replaces with //heron/proto:proto-java. Can we also support a dry run mode that just detects but doesn't take action in the resolvers? This could be a follow-up PR.. Passing a trackerUrl implies that this impl can only work with tracker. Could we instead have an interface which has a tracker-based impl (e.g., a MetricsProvider impl), which could be replaced with a metrics cache manager impl? That impl could be wired into the guice module and swapped out.. This method is called commandLineConfigs but it has nothing in it's impl related to command lines. Can we either change the name to something like initConfig or change it to take CommandLine cmd in it's signature?. Can this method be done via guice?. please provide javadocs for this class. pls clarify what Map> is. If the inner map is a config, why are all values strings?. +1 for enums.\nAlso using Interfaces for constants is a no-no:\nhttps://stackoverflow.com/questions/320588/interfaces-with-static-fields-in-java-for-sharing-constants. These are only used by HealthPolicyConfigReader and it's tests. Better to move them there.. s/long/Duration/g. @Override?. This method is only used by tests. Why is it needed?. This assumes an instance naming convention declared elsewhere which could change. Could we leverage consistent code for this?. This method is never accessed.. This is the topology at the time of submission, not the latest one. The latest one can be gotten from the current physicalPlan.getTopology(), but even that one doesn't contain updates to the config (e.g. parallelism) since creation. If that's needed see UpdateTopologyMaster.. Can we throw an exception that is not RuntimeException? Also, let's include the topologyName in the message.. This is only used in the tests. How is it needed. Also it would be better to make this a utility somewhere that takes a Topology. That way you don't have potential for a race condition.. Please use java.time.Instant and java.time.Duration for startTime and duration.. Is there any retry if one of the getMetricsFromTracker calls fails?. no star imports please, here and elsewhere. Can you clarify in comments what the difference is between draining and clearing?. please append with .bytes to make units explicit. same for disk settings.. s/nbytes/byteAmount/g here and below.. s/ncpus/cpus/g here and below. The packing type is just a Class. It needs to be extensible so users can implement their own so an enum won't work.\npublic static void setTopologyPackingClass(Map<String, Object> conf,\n    Class<? extends IPacking> packingClass);\npublic static void setTopologyRepackingClass(Map<String, Object> conf,\n    Class<? extends IRepacking> repackingClass);. This would cause a problem, since IPacking and IRepacking are in spi, which depends on api, but not the other way around. We could considering moving these interfaces into api. @kramasamy do you have an opinion on this one?. I think the intent (which makes sense to me) is that api is where  topology-level APIs are that topology developers might implement or specify (i.e., IBolt, ISpout, etc). The spi package is where system-level APIs live that would be implemented by administrating teams for a new scheduler or state manager for example.\nI think the packing algorithms could be implemented or at least specified by topology authors at the topology level. Hence it makes sense for those interfaces to move to api, but not others perse.. I think the packing impls should be defaulted at the system level, but I certainly see the need to be able to override them at the topology level. We've recommended to some topology owners to specify a different packing impl to change the way resources are allocated for their specific job, which seems reasonable to me.. why are both instance and task id necessary? Doesn't the former contain the latter?. What is a local spout? Is that a marker on spout instances that are on this container?. why are the client methods virtual?. why's the comment discuss sending message but the method implies getting a checkpoint?. Could you add comments to StartRestore, HandleCheckpointState and HandleInstanceRestoredState clarifying what the responsibilities of each is.. instead of silently ignoring bad requests, would it be useful to the caller to know what happened? should we consider returning a more descriptive status response?. Why does in_progress_ get set to false if it's a non-OK status, but not if it's an invalid checkpoint id that's being ignored?. Can you clarify what is meant by \"we will get back to restore\" here.. Could you include the _checkpoint_id being ignore here.. could you include the _checkpoint_id in this log message. also is this an unexpected condition (i.e., should it be a warning)?. can you clarify the responsibilities of the restorer in comments. What does it do or manage and why/when/by whom is it called?. This code appears to be copied from one of a few other places that contain the same code. Could you put this is a common place and share it?. Is there a cpp library that could be used to test with mock objects, instead of having to write dummy impls for all classes?. this review looks like you're just calculating size of the pool differently, but what causes the pool size logic to change from being based on number of bytes to number of message in the pool impl?. Got it. Looks like your changing the 512 to be configurable. Shouldn't the config-driven value be used here now?. Got it. Could you update the TODO nlu comment above to explain why this is config-driven, but hard coded here.. I could see having multiple packing implementation supported, where power users could specify a non-default based on their needs. Or even write one of their own.. typo: intiate. why is the checkpoint id necessary to start processing?. If the concern is only OOM errors I say we check for that and log a message that it's an OOM without logging the error. If it's anything else, we should log it.. This seems orthogonal to the reliability mode IMO. If exactly once leverages the stateful impl that's an implementation detail. From the users perspective these are different things. The reliability mode should specify guarantees with tuple reliability, while the stateful mode should define whether herons state management is being leveraged and enabled by the user logic in the topology.\nIIRC the reason for requiring the stateful enabled flag was so that users could turn stateful processing of their topology on or off (otherwise we could derive this implicitly by inspecting the topology). If that's the case they might choose to do either while still keeping exactly once enabled.. For this and others we should annotate as deprecated:\n/**\n * Acking is now internally set based on the reliability mode being used.\n *\n * @deprecated use {@link #setReliabilityMode()} instead.  \n */\n@Deprecated\npublic static void setEnableAcking.... Where we add this, we should add a comment that it's only need until TOPOLOGY_ENABLE_ACKING is removed. That way it doesn't linger after TOPOLOGY_ENABLE_ACKING is removed. Same for other occurrences in this pr.. FWIW, I found no occurrences of TOPOLOGY_ENABLE_ACKING or setEnableAcking in twitters codebase. @nlu90 @maosongfu do you know how topologies set acking without using those?. all other heron yaml configs in this directory and elsewhere leverage primitive ints so we should provide a consistent user experience. Actually we don't use quotes around strings or numbers elsewhere, so I think we should remove them here to not be the exception. Also asking users to use quotes where not necessary is cumbersome.. Ideally we could replace the reflection pattern with guice everywhere. The initialize() method is an artifact of the reflection approach. If there were a way to do this with guice that would be awesome, since it would show how this pattern could be replaced elsewhere. If you want to explore/consider that outside this PR that's fine though.. could you consolidate unnecessary string concatenation here pls. Much of this can fit on one line.. could we use the enums as the config keys?. This assumes an instance naming convention declared elsewhere which could change. Are there other places in the java codebase where this logic exists which we could share somehow?. This is the topology at the time of submission, not the latest one, which is what you want right?. The latest one can be gotten from the current physicalPlan.getTopology(), but even that one doesn't contain updates to the config (e.g. parallelism) since creation. If that's needed see UpdateTopologyMaster.. Can we throw an exception that is not RuntimeException? Also, let's include the topologyName in the message.. @can these be package private and @VisibleForTesting? looks like they're only used by testing.. We've been using Instant and Duration yes, but not labdas AFAIK. Seems if we're using the former we should be able to use the latter. :). Let's use Instance and Duration for these.. I thought there were too, but I don't see it being enabled by using either of these. Is there any other way? I see this in one place:\n.put(\"topology.acking\", String.valueOf(true)). The IDE generated code is overly clunky at times. For readability and perf it's nicer to avoid this much string concat for what could be just \"string\" + object + \"string\".. got it.. Our standard for enums is camel case, so PolicyConf. Actually, PolicyConfigKey would be consident with other patterns in the code (e.g., SystemConfigKey).. Can the map be Map<PolicyConf, Object>?. Look at the getTopology method:\nhttps://github.com/twitter/heron/blob/master/heron/scheduler-core/src/java/com/twitter/heron/scheduler/UpdateTopologyManager.java?utf8=%E2%9C%93#L337\nstateManager.getPhysicalPlan(topologyName).getTopology();\nThe physical plan and the topology it holds gets updated during the licecycle of the running topology. stateManager.getTopology(topologyName) does not. It is the immutable topology from the time of submission.. That's what I suspected - only set elsewhere in python. Good to leave it as is here then. Maybe make a comment on both occurrences of it about the other for now.. I recently learned of this myself and I agree with you that it's confusing and error prone. I wonder if there is any value though in keeping the original topology at getTopology. @maosongfu @kramasamy @nlu90 @srkukarni thoughts?. This occurrence should redirect people to the one with the similar constructor, so the first one with the map. The other javadoc error in this class should send them to the second one without it.. Can we create a different exception to use here that's not RuntimeException here and elsewhere in this review? Typically we should avoid throwing generic exceptions like this, except maybe in a main(..) method. Using a more specific exception allows callers to handle them specifically and provides the ability for us to add context-specific features to the custom exception as needed.. Could we name this PolicyConfigKey for consistency with SystemConfigKey?. Could we change to something like InvalidStateException to make it slightly more generic? That way it could be used for any inconsistent or unexpected state.. please add comments describing the controller and what it's general area of responsibility is.. are we able to inject underscores for readability for long, multi-word paths?. Before creating a twitter fork we should look into whether we can proxy this in twitter's env. The fork is not a good solution since it will not automatically get new tags on it, so the build will fail when version updates happen without manual intervention.. +1 for a mirror that picks up updates.. awesome! :). This should not be necessary, since //heron/proto:proto-java should contain all protos. If that's not the case would you please fix that target to make it so.. can you clarify why 1000 is chosen as the cap. Can duration be calculated once in the constructor?. Can these by private and final? If needed by subclasses could we instead do so with a getter? Or if just for initialization they could be part of a constructor. This helps to better define the class contract, and to make that clear in javadocs.. Why not use generics in the interface here so instances can handle any types of state?. why have the Slave create empty state? shouldn't this be injected? there could be previous state to be loaded.. Why are State types being limited to byte[] here? A bolt implementor should be able to define state as anything that satisfies State<? extends Serializable, ? extends Serializable> no?. But the state should always be injected from the state server right? The slave should not assume that the state is empty state until it hears from the state server. It should be told this state by someone else, even if it's empty.. +1 for null. null check?. is it possible that instanceState hasn't yet been set here?. would you please clarify this with comments in the code.. we should apply javac settings consistently for all targets. Can we do this for all?. please add some comments about what this target does and how it does it.. Instead of \"all Heron jars are required to run with Java 8\" how about \"but Heron topologies are required to run with a Java 8 JRE\"\n/cc @lucperkins, @kramasamy for comment. Could this be just Window?. typo: gerts. Best to include the unknown join type in the message.. Would Java's lambda syntactic sugar allow this to instead be written as:\n.mapToKV((word) -> (word, 1)). Could this be just KeyedWindow?. I agree that ports should be Integers. Using Object for flexibility is almost always an anti-pattern. I would suggest using integer and if we find later that there's a requirement for multiple port bound to a single name (which seems like something to avoid IMO), then we would consider adding support for lists of integers.. This enum of port names provides little more than a list of strings. I suggest instead having an enum of ports (not names) and include metadata of each, like whether it's required or not and it's default value. Then enum itself could then have a getter for fetching just the required ones for example.\nOne benefit of the proposed approach is that to add a new port you have to modify the enum and explicitly set whether it's required or not when doing it. As written someone needs to add a port name and know that there's also another place in code to modify to represent required or not.\nAlso enum names should be singular not plural. That way you work with a singular enum (e.g., doSomething(ExecutorPortName name)) or the with all of them (e.g. ExecutorPortName[] all = ExecutorPortName.values()).. Please do not use the value of toString() for any business logic. The toString value should only ever be used to log messages and nothing else. Overriding toString() for logic is an anti-pattern. Doing so violates encapsulation and provides and unclear API to consumers. Also future developers could find that the logging of the object is unclear (e.g., logging just an int for a Port enum might not be desirable) and change it. This would break functionality. I have been bit by this and have spent hours debugging such design flaws. Please don't do this.. If we have an ExecutorPort enum which includes whether it's required or optional, then the caller doesn't need to know which of the two methods to call here. They can just call getPort(ExecutorPort port) and the method can return Integer or throw if a required port isn't found.. ",
    "nwangtw": "user/launch time look ok to me now in heron UI. or maybe i am looking at the wrong place. Is this ticket still valid?. is this still a problem?. is this still a problem?. \"Executable\" means user topology? The build version is topology version or heron API version?\nOr it means the executables in heron itself?. seems like we do have a \"getting started\" page now? is this ticket still needed?. there is a \"status\" column in heron UI now. can this ticket be resolved?. \"state\" is in UI, in the middle of the page.\nResolve for now. Feel free to create a new ticket with more specific requirements if the currently location is not desired.. I believe LOG(FATAL) calls exit() to quit. If the process freezes, it seems the code is not executed. Calling ::exit() doesn't help.\nThe root cause is to solve the freezing issue itself.\n. +1. Thanks for the responses. Never mind about the \"tmerevent can cause\" question. I was wondering about racing condition/deadlock at beginning but it didn't look risky later.. QQ: I am new to Heron/Storm so this question may not make sense. Collection<> interface makes sense in some cases. I am wondering if efficiency a factor here?\nAbout the interface, I am wondering if some kind of hint about the collection size could be helpful to avoid over-sized batch?. SGTM. Thanks.. sorry not yet. will find a time to try it.. seems completed.. @srkukarni \nSo we added config translation for spout and bolt because it was missing last week. When spouts and bolts are created they merge topology configs and component configs.\nsay if topology config has {\nk1: 1\nk2: 2\nke: 3\n}\nand bolt config has {\nk1: 10\nk4: 40\n}\nThe final config for the bolt would be {\nk1: 10\nk2: 2\nke: 3\nk4: 40\n}\nThis looks right as long as all they key/value pairs are expected.\nI missed on case in the doStormTranslation in my previous PR.\nMost part in the function are like this and they are safe:\nif (config.contains(storm_key)) {\n  config.put(heron_key, value)\n}\nHowever in the function, one config is different from others: TOPOLOGY_ACKER_EXECUTORS\nThe code is like:\nif (config.contains(storm.TOPOLOGY_ACKER_EXECUTORS)) {\n  config.put(heron. TopologyReliabilityMode, value1)\n} else {\n  config.put(heron. TopologyReliabilityMode, value2)  // This is dangerous.\n}\nTwo cases:\n- when the bolt config does have \"TOPOLOGY_ACKER_EXECUTORS\" key, the output config would have a heron. TopologyReliabilityMode key with value1. This is ok.\n- when the bolt config DOESN'T have \"TOPOLOGY_ACKER_EXECUTORS\" key, the output config would still have a heron.TOPOLOGY_ACKER_EXECUTORS key, with value2. And when the config is merged with topology config, the heron. TopologyReliabilityMode will overwrite the topology config and this is bad.\nThis PR moves the \"if ... else ...\" block out of the shared translation function to a topology only translation function, so that the config merging is safe.. Sure thing! And thanks for confirming!. I think this should be a simple fix and we are close to cover all edges. So it is not necessary to have the check I think, plus the code would be confusing.\nUnit changes can be scary. I feel the root cause was the config translation. Pretty much all configs have the same names in API layer and core layer, so bugs like this (lost in translation) were hidden. tick_tuple_secs might be the first one that has different keys in different layers so it shows up this time.. fixed in https://github.com/twitter/heron/pull/2592. Got it. Thanks!. Cool. Overall LGTM.\nOne last comment is to add the config in storm compatibility as well to keep them in sync.. Good question. \nI thought that the config as well and the translation should be added to the storm layer. Or maybe it shouldn't since the storm API shouldn't have the new features? I think I am ok either way.. Make sense. Thanks.. Good find~. fixed in https://github.com/twitter/heron/pull/2596. ping for code review. thanks in advance~ \n@kramasamy . just a test @jerrypeng . Please add @kramasamy as reviewer.. @kramasamy Thanks for taking a look.\nYeah. It could be helpful.\nIn fact I have a walkaround without this target in oss heron, but I feel it could be useful for other users as well.. So all other modules are tar balls but APIs are using different language specific packages? Not really a problem for me, but I still feel adding the python api files into the api tar ball makes more sense then keep all api files as is. I think your concern would be that users would have to build all of languages by default.\nAnyway, I am ok to leave the targets as is and drop this change. We can revisit the question if other users have more questions about it.\nThanks again for taking a look!. Thanks~. Yeah. You are right. Need to update a few other files as well.. I see. The target of the PR is to add the encoding so I am open to suggestions on the python version.\n\nit might be better to be consistent across the code base I think. But it may not be a big deal?\nWith explicitly version number we may be able to avoid some unexpected behavior and it could be very hard to debug (I don't know python that well and don't know if it is a good enough reason though).\n\nAnyway, if you think \"#!/usr/bin/env python\" is better, I can make the change.. @srkukarni sure, let me do it.. @srkukarni updated.\nadded \"#!/usr/bin/env python\" for all files that are missing the line, kept the 2.7 version number for all that had specified it and moved the env line to the beginning of every file. Basically version specification should be the same as before.\n. Christmas cleaning!. I am planning to add an this functionality as a new option in heron client to specify this heron home path. It could be useful for all users and it doesn't have side effects.. Probably the last PR for this year~. Dropping this PR in favor of easier installation of older build. Heron tools package is self contained basically so installation is easy.. @kramasamy I considered a few options and a new cli option seems to be the most reliable and straightforward one.\nYou are suggesting leaving all the data in zookeeper for a topology after it is killed or still call clearState() after the flag node is created (I prefer the second one)? this could be an option if it is ok with us. I was assuming that we want a topology to start from a clean state hence trying to avoid calling the function again in submit command. You are right that one major issue is that we need an atomic operation instead of 8 unreliable steps.\nIn fact, if it is ok for us to call clearState() again during submit, we may use topology name node as the flag so we don't need a new one.\n. Create another PR https://github.com/twitter/heron/pull/2689 based on @kramasamy's suggestion.. Discard PR. We are going for a different solution: https://github.com/twitter/heron/pull/2730. Discard because we are going to a different solution in heron kill command: https://github.com/twitter/heron/pull/2730. I am a bit concerned about it and we may need to talk first before merging. @kramasamy \" It is very common for people to ack/fail in another thread.\"\nYeah. I believe it makes sense in these cases. I think we need to treat user functions like addActTuple, addDataTuple and sinking functions like sendOutTuple separately. For high throughput topologies, sinking functions could take a long time and it is not ideal to block user functions/threads.\nI mean this object is basically a queue with input and output operations, and input / output shouldn't block each other.. Personally I am not against adding the protection in the needed functions. If storm assumes that, then keeping the same API sounds more reasonable.\nMy concerns are:\n\neven it is a relax, this is an API change, so it is better to be careful. What exactly is the problem? Users need to call \"OutputCollector.sendTuple\" in multiple threads? Any other functions are needed?\nOutgoingTupleCollection is not an interface layer class and it is shared by both bolt and spout. I can understand the bolt case, but do we need the same protection for spout? Is this the best place to synchronize compare to AbsOutputCollector or BoltOutputCollectorImpl?\nit seems the current bolt process is: 1. check there is room in outQueue, 2. execute bolt logic, 3. sendout. We should be clear if addtuple functions are allowed to be called out of the time window of step 2. If no, then the sendOut* functions are not needed to be synchronized. If yes, I think we have a lot  more to worry about than making these functions to be synchronized.. Thanks for the responses.\n\nRegarding the 3rd concern, I talked with Maosong later and confirmed the queue doesn't have a hard cap. So it should be fine.\nI guess the only concern left is @maosongfu's concern about stateful calculation, or the execution order?. I think Maosong's concern is that this concurrent threads support is not complete.\nHere is my understanding:\nFor stateful (I mean effectively once) topologies, user can't use concurrent threads. It is the same as today and existing topologies should work fine. However today users can't have concurrent threads at all, hence it is less an issue; if we add the concurrent threads support, we have to either handle the synchronization or  block concurrent threads in stateful topologies.\n. Seems not. Maosong's concern hasn't been replied yet.  @jerrypeng @skanjila . I think maosong's concern is still valid. For stateful process, we have to make sure the checkpoint are made after all user threads have done their acks.  Which is why Neng said above \"But I think we need to mention the thread-safe acking to users very carefully, ensures them that EFFECTLY-ONCE might be broken if they don't implement preSave carefully.\"\n. @jerrypeng agreed. The concern is that users CAN use the function in the wrong way. Avoiding it in code would be nice; protecting the case might be helpful and acceptable too; otherwise very clear documentation and education is needed (we should have it anyway though). I think maosong prefers to have one of the first 2 options available.\nMaybe we should have an example code for presave() function if we want to cover stateful process?. This is just an initial PR to demo an issue and a quick solution. It is not intent to be merged as is. I have a few other thoughts but would like to get some more ideas from you guys first. Thanks in advance.. Dropping this PR in favor of the new solution: https://github.com/twitter/heron/pull/2711. lgtm.. dropping this PR for now. Looking for different ways.. 2 might be a bit aggressive. How many lines are generated currently?\nAlso just to confirm: first 2 lines are the most recent ones?. From @skanjila \nI could't add comments to the document, thus am posting my comments to the\nmailing list\nOne more approach could be to do the current measurement as it is, but\ninstead of leaving the quitting decision to the stmgtclient, have\nstmgrclientmgr do the decision. Thus everytime a stmgr client detects\nconnection issues, inform that to stmgrclientmgr which keeps a map of\npeerstmgrid to error count. Thus it is able to decide things like am i\nseeing connection errors from all stmgrs or if only a few of them are\nhaving issues. Then it can take the decisions better.\nReply:\nYeah. That is an option too. In fact it was my first try:\nhttps://github.com/twitter/heron/pull/2693 (just an initiative, not\ncompleted, a count map should be used instead of a single total count)\nIn most cases, I think both solutions should have the same result. A few\nreasons I changed to a tmaster check:\n- with tmaster, there is only one source of truth and tmaster is more\ncritical anyway. If the tmaster link is not healthy, stmgrs won't work\ncorrectly: topology may have created replacement nodes but the disconnected\nnodes could keep going by themselves.\n- it is more straightforward. The logic is the same as the current one. One\nthe other side, if we use an array for all remote stmgrs, we could have a\nsmarter logic (which is good) but it could make stmgrs more complicated and\nless straightforward (bad). I left the stmgr counters there so if in future\nwe decide to add this feature, it should be easy to add. There is a gap\nbetween \"errors from all\" and \"errors from a few\" and this is not a\nsimple/quick question.\n. thanks!. LGTM.\nRelated question: if RoundRobin is not good, should we deprecate it? Like removing it or adding a warning when user uses it? I am wondering if there are some cases that RoundRobin is better than the others.. SGTM. Thanks. Hmm. This looks like a valid failure.\n[2018-02-11 08:27:40 +0000] [ERROR]: Actual result did not match expected result\n[2018-02-11 08:27:40 +0000] [INFO]: Actual result ---------- \n['A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B', 'B', 'B']\n[2018-02-11 08:27:40 +0000] [INFO]: Expected result ---------- \n['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B']\n[2018-02-11 08:27:40 +0000] [ERROR]: Checking result failed for 20180211082544_IntegrationTest_NonGrouping_1159670a-236b-4ae2-935f-68716f5b7824 topology :: Traceback (most recent call last):\n  File \"integration_test/src/python/test_runner/main.py\", line 184, in run_test\n    return results_checker.check_results()\n  File \"integration_test/src/python/test_runner/main.py\", line 103, in check_results\n    return self._compare(expected_results, actual_results)\n  File \"integration_test/src/python/test_runner/main.py\", line 116, in _compare\n    raise failure\nTestFailure: ('Actual result did not match expected result', None). I thought about both and chose this one. I feel it depends on the meaning of \"config\". If it is a verb here, then runtime_config is good enough. No objection to the \"update_runtime_config\" endpoint though. Let's get more idea from other people. I am happy to change it after we make a decision.. sounds good. let me do it.. Just realize I forgot to append the \":runtime\" postfix (described in doc: https://docs.google.com/document/d/1VWVmnGouZMyQriKWwXn25fDeiHKd7nYCIcbAyb2jcak/edit#). Adding it now.. Changed the runtime config validation to verify component name only, not config name.. Thanks for reply! It makes more sense now.\nHowever I am afraid this fix is still not safe. If my understanding is correct, the clear call should clear all tuples immediately and cancel the pending actions. With CopyOnWrite list, the current content in the lists are still being executed afterwards.\n. Good comments are definitely helpful for my concerns. PR is acceptable to me after addressing maosong's comments.. One more comment.. The latest comments will be addressed in the later PR.. LGTM. Let's see if @fred521 has more concerns.. Good question.\nStream manager is more convenient implementation-wise than instance. We thought about the option, but it has its own issues. Instances push tuples to stream manager, so we cant control the input stream. And if we rate limit the output stream, the buffer size would increase and cause back pressure. Therefore, we need a way for stream manager to control the processing speed. (Or is there a way to avoid the issue?) On the other side, the rate limiter in instance slows down processing directly and there is no side effect.\n. @ashvina rate limiting is at component level. Backpressure on the instance itself is ok, but backpressure has sideeffects. Topology will stop all spouts in that case if my understanding is correct. So I am hoping to avoid backpressure triggering/handling. Or we can also handling spout backpressure differently from bolt backpressure?\n@srkukarni Yeah. The scope is correct. We are limiting the same data. The pipeline is like:\nspout -> spout_instance -> gateway -> stmgr -> EXTERNAL.\ncurrently the rate limiter is applied on the connection between spout -> spout_instance which is done via the emit() function and the tuples are pushed one by one. The limiter basically slows down the spout.\nMy understanding is that you are suggesting rate limiting the connection between stmgr and EXTERNAL for the specific spouts. It pauses spouts by putting them into backpressure state. I am trying to avoid the backpressure to avoid confusion and extra handling logic, but maybe this should be fine? I can take another look at this idea.\n. I am looking into stmgr again. Thanks for the suggestion.. @srkukarni got it! Thanks. I think my impression about data pushing between gateway and stmgr might be wrong. Let me check the code again.. stmgr based solution is implemented in: https://github.com/twitter/heron/pull/2783. Dropped in favor of #2783. Two more things to add:\n- convert rate limit to per instance limit\n- add a real unit test (the one added in the previous change was a setup only). My guess is that \"=\" is a valid character in base64, and it might mess up configs potentially because configurations are in config=value format.. I added another PR with a similar change:\nhttps://github.com/apache/incubator-heron/pull/2953. lgtm. but I am curious about the reason to have this PR.. lgtm. is this change backward compatible?. @jerrypeng \nFor 1, yes we will have component default config and per component config. I dont feel this is an overkill because components can be very different. For example, most spouts have very simple logic and may not use much CPU, on the other hand, a bolt for flapmap and filter could use 10x more cpu and memory than the others.\nFor default.yaml, it sounds like a very good idea. It would require some considerations and works though.. @jerrypeng Yeah. that is stage two. https://github.com/twitter/heron/pull/2800\n. dropping to avoid new configs.\nhttps://github.com/twitter/heron/pull/2800 should allow users to do it. We will see if we need another helper function or not in future\n. Sure. It seems the existing cpu configs are floats in java. let me change them all.. unit tests to update. It is more about \"why there was a Math.rounding()\".\nThe rounding logic doesn't seem necessary if we assume cpu # can be less than fractional. Also the logic have bugs that in some cases it can be floored to 0.\nHowever when I was working on this PR, I found out that some parking algorithms rely on the rounding so this is not a quick fix. This PR needs to be revisited when I get more time.\n. config is convenient because the functions/tools/ui are all available. But @ashvina's concern makes sense too.\nI don't know if config is the best option or not so far, but I think we may step back a bit and think about it from tracking point of view. To me it seems to be a feature in heron-tracker/heron-ui. It could b nice to treat the information as a \"property\". And @kramasamy is right that topology.project and topology.team are similar.\nJust my 2 cents.\n. 'f' is definitely not ideal. but 'r' and 'c' are both used. I just feel 'f' is relative better than the other characters in \"runtime-config\". Any suggestion is welcome.\nAlso it seems there are quite a few options now and these characters are not very useful any more. :( I am wondering it might be better to keep abbreviation to only key options.. i believe this is done. I have updated all texts I could find.. After merging, I will do some tests as well in aurora environment.. Overall LGTM. The only concern (not a blocker to me) is about ObjectMapper which needs other feedbacks.. Yeah. It is a good point.\nI was changing the safe ones only in this PR to be quick. For the CHECK_EQ() << \"....\" case I need to do some tests to verify the behavior of the library. Let me see if I can find some time to run the tests.. Just did a test. Seems the checks work with << as expected.\nRun 1:\nint a = 1, b = 2;\nCHECK(a > b) << \"Failing check 'a > b'\";\nThe output is:\nCheck failed: a > b Failing check 'a > b'\nRun 2:\nint a = 1, b = 2;\nCHECK_GT(a, b) << \"Failing check 'gt(a,b)'\";\nThe output is:\nCheck failed: a > b (1 vs. 2) Failing check 'gt(a, b)'\nLet me update the checks with << operator then.\n. And I will see what description I can add now.. Cool. Thx. @Yaliang . Updated descriptions. Thanks for reviewing.\nThis was a super quick PR and incremental improvement. It seems to be more time consuming now. I will leave this PR here for now and come back when I have the time.. Cool. Maybe create an issue for those 3 tests?. @cckellogg shouldn't be. the new data is an extra field in the physical plan json output.\nWe don't use this tool internally. Let me see how to test.. Sorry I haven't got chance to test it yet.. Verified that explore works ok with the change.\n. So quick. Thanks. :). Related PR:\nhttps://github.com/apache/incubator-heron/pull/2893/files. Looks like integration test failed. Need to retrigger travis.. \"(15:41:38) ERROR: build interrupted\". :p . @kramasamy I would think so.. Discussed offline. Update:\nThe fix is valid and helpful in the current implementation. The questions are about the implementation of queue and if we can handle zk events in a simpler way. So we agreed to submit the fix which could improve heron reliability, and then discuss about the zk client implementation in future.\nIn conclusion, we have a green light to merge~\n. And thanks for your help! :)\nOn Tue, Jun 12, 2018 at 5:24 AM, Michael Schmidt notifications@github.com\nwrote:\n\nCool. Knowing that we can get this pulled back into the master branch\nmeans we can work on implementing it. We will stay connected in the\nheronstreaming channel as we move forward. Thanks!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/apache/incubator-heron/issues/2909#issuecomment-396570214,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGDwn74nTy-tqdKv9ocLmainaRvo0auWks5t77LsgaJpZM4UOZKn\n.\n\n\n-- \nRegards,\nNing Wang\n. @kramasamy . Hmm. So Dhalion is activated after this PR for all the schedulers? This could affect many users and it seems to be not really necessary for adding the toggle feature itself.. kk. lgtm.. Hmm. Unit test failure. @kalimfaria Please check.. Not sure the exact cause, but the WARNING above may be related. I would suggest double checking the scheduler config and trying to get more information about the warning.\nAlso, what is the command line to submit this topology? I am curious about the cluster argument.. If my understanding is correct, you are trying to get physical plan right after the topology is submitted?\nPhysical plan is only available after the start up is done (all instances start running and register them to stmgrs). So depends on the size of your topology, it could be ready after a while like 15 seconds to 1 minutes.\nOne thing you can try (or to confirm the theory) is to keep trying to get the physical plan until it is ready.\n. Interesting. This looks tricky.\nCLI does have the zk connection because state data on zk is needed to verify topology in submit and update commands. I am wondering if the UpdateTopologyManager is initialized correctly in your case, or maybe the the zk connection has been closed by the submission handler. Maybe worth checking with extra logs on the opening and closing of zk connection.\n. Hmm. Not sure.\nMaybe you could check in the aurora scheduler, the job has been started successfully and all the processes are running ok?\n. Can you guys help to merge this PR? I dont have the permission yet. Thanks in advance.. \"I could rewrite the client to do raw HTTP requests but I would simply be replicating the work of Influx client.\"\n@Code0x58  made a good point. For a metrics sink, it is quite strange to me that so many dependencies are required. In theory, a sink client should be just a simple wrapper for a few HTTP requests, so it may only need a HTTP lib (and/or thrift requests but still). I am wondering how these dependencies are used and if there is a simpler solution.\n. I see.\nIt is not a hard no, just makes maintenance a bit harder. Personally I am ok (it seems this is the official influxDB client), if they are necessary. . which version of bazel you are using? And your OS info please.\nBazel is not very good at compatibility. I am wondering if you are using an older or newer version.\nMine is 0.14.1 fyi.\nOn Fri, Aug 17, 2018 at 12:56 AM, Clark Yang notifications@github.com\nwrote:\n\nI used command\nbazel build --config=darwin ...\nBut it turned out to be:\nERROR: /Users/clark/IdeaProjects/heron/third_party/cereal/BUILD:5:1:\nReassignment of builtin build function 'package_name' not permitted.\nERROR: package contains errors: third_party/cereal.\nERROR: error loading package 'third_party/cereal': Package\n'third_party/cereal' contains errors.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/apache/incubator-heron/issues/2985, or mute the\nthread\nhttps://github.com/notifications/unsubscribe-auth/AGDwn2v1PB-sCeFJa7sWy5q2TRlMM-h1ks5uRnc6gaJpZM4WBIcz\n.\n\n\n-- \nRegards,\nNing Wang\n. Please upgrade bazel and try again.\nOn Fri, Aug 17, 2018 at 10:34 PM, Clark Yang notifications@github.com\nwrote:\n\n@nwangtw https://github.com/nwangtw I am using bazel 0.5.4 and mac os\n10.13.6.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/apache/incubator-heron/issues/2985#issuecomment-414033824,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AGDwn7Q5dH22NZwufvI8ya2yJZFJToGbks5uR6dfgaJpZM4WBIcz\n.\n\n\n-- \nRegards,\nNing Wang\n. maybe to 0.14.1 to be safe.. Yeah. I remember the doc follows the config automatically, but it needs to be republished.. @kramasamy @srkukarni I saw you guys added the tar.gz file and changed the version from 1.2.2 to 1.2.1 in git log. And from git log there seems to be a patch file before but it is not in this folder any more. Could you please take a look at this PR? And do I need to change 1.2.2 to 1.2.1? \nThanks in advance.. Agreed with @nlu90. Feels strange to merge test/debugging code to master.. this works?. Thanks!. sgtm. thanks. Another pass of refactor is coming. Please review after next update. Thanks in advance.. Here is the design doc for CustomOperator (a more general feature of this PR):\nhttps://docs.google.com/document/d/1XzF0IlfuaaW8Gx3cPx1xLtP-kgCFK0TRNS5aAzuMuMg/edit#. This is a brother PR of https://github.com/apache/incubator-heron/pull/3029. Thanks for pointing out. Cleaned up the \"TODO\"s.\nThe two classes has initState() function but didn't provide implementation for presave(), so I added TODO there. After checking the code, I don't see any code to be added for now.. Thanks for reviewing!\nSure thing to explain a bit more:\nFirstly, in order to support custom operator, we need to pass in the operator object as well as specifying the grouping strategy.\nCurrently, the implementation of Streamlet classes are like:\nbldr.setBolt(getName(), new FlatMapOperator(flatMapFn),\n        getNumPartitions()).shuffleGrouping(parent.getName());\nYou can say the grouping is a function call with the parent name. It is ok for predefined operators because the grouping is likely to be decided, but this is tricky for custom operators (and there might be new operators that allow users to specify grouping strategy in future).\nOne way is to pass an enum, but this is not flexible and there will be a switch(grouping) {....} to maintain. So I check the code and it seems fairly stranghtforward to refactor the grouping strategies into their own classes and here it is~ After this change, user can create any the grouping object and use it in streamlet.\n. Yeah. With the current standard operators, it is not necessary. However,\nFor the new CustomOperator I am working on, it is needed. Streamlet classes are responsible for creating the operator objects and hook up the grouping. However CustomOperatorStreamlet will be shared by all user defined CustomOperators.\nAnother case is that in your proposal to change the streamlet style to be like:\nreduce(...)\n  .windowed(...)\n  .byKey()\n   ... \n  .build();\nthen the byKey() implementation will likely to need the grouping strategies to have their own classes.. > @nwangtw a KeyBy operator will always be a fieldgrouping\nYeah. Very likely.\nWe dont have a grouping function yet currently. Custom grouping might be a separated function or uses keyBy() interface. We will see.\n. Only code move around. No logic change other than a few extra logs.. No real known issue, but the current operators don't handle failures and I feel the try/catch could be necessary. Or it was by design that Streamlet operators couldn't fail?. > can you please provide an example topology that uses the custom operator?\nSure.\n\nAlso, isn't the point of the custom operator to provide some sort of compatibility with the \"Storm\" API? Instead of forcing users to copy their code to the corresponding \"prepare\" and \"execute\" methods in the custom operator, couldn't we just do something like CustomOperator(Bolt). So that the user that just use their existing spout or bolt implementation? and the CustomOperator is just a wrapper for bolts which allows them to be used in streamlet topologies.\n\nNot really in this step/PR.\nThe goal of this step is to provide a way for users to write their own operators when needed. The low level Bolt is supported by previous PRs.\n. It does blur the line a bit to allow users to write their own operators with all functions available in low level API. However these operators still has type safety and can be used the same way as other pre-defined operators. Therefore, from API point of view, I don't think it \"bring the streamlet API back to the bolt and spout API.\" Instead, it brings more powers from the bolt and spout API to streamlet.\nBasically Streamlet is a functional API, and function API is overall nice and elegant. However I believe, a functional API is HARDLY a general purpose API and it is only good for \"nice\" problems. It might enforce uses to change the way of thinking and solving problems, which can be good sometimes (use general/standard operations) and bad in other times (more like hacks). Therefore I think supporting custom operator is important for Heron.\nThat being said, the original feature I needed was to reuse existing Bolts and Spouts which has been completed. This design doc and PR are just an extension from the original feature so it is not as urgent and we still have time to discuss.\n. qq: on which OS you are seeing this error?. Ok. SGTM.\nI am not a linux expert, maybe @nlu90 @huijunw can confirm and approve.. Here is the PR:\nhttps://github.com/apache/incubator-heron/pull/3105. PR Merged. That is the root cause of the exception? Nice find!. I see. Make sense then when you use multi threading. In this case you might be better to synchronized {} to make sure ack() is not called many times at the same time. OutputCollector has a queue in it and I am not sure the queue is thread-safe. In Heron engine, the core assumes single thread environment.\nInternally, we build library and use it directly, instead of the ones in Maven.\nbazel build --compilation_mode=dbg --config=darwin scripts/packages:tarpkgs\n. Thanks for reviewing!\nResolved comments, except the UT for wrong stream id. And I am going to add a UT for the new StrealetShadow class.. - Added stream id check in withStream() function\n- Added unit test for calling withStream() with invalid (unavailable) stream id.\n- Add unit test for StreamletShadow.. Overall LGTM.\nWill merge after fixing the ci errors.. done.\nhttps://github.com/apache/incubator-heron/pull/3155. Hmm. How to resolve an issue?. Resolved. It looks like Aurora cant allocate enough resource for your topology. You\nmay ping your aurora owner for help.\nOn Fri, Nov 23, 2018 at 7:39 PM wanghanlin32 notifications@github.com\nwrote:\n\nI was deployed a Twitter Heron cluster with Aurora and Mesos. The\ncomponents of the cluster as following list:\nScheduler: Aurora scheduler\nState Manager: zookeeper\nUploader: HDFS\nThe instances of Aurora are always pending status after I submitted the\nexample topology named WordCountTopology. The following is a screenshot of\nthe cluster running.\n[image: fireshot capture 1 - mesos - httpheron045050_ _agents]\nhttps://user-images.githubusercontent.com/31752211/48964207-7d12e000-efdd-11e8-88d6-815cac5fe640.png\n[image: fireshot capture 4 - example aurora -\nhttp_heron04_8081_scheduler_lin_devel_wordcounttopology]\nhttps://user-images.githubusercontent.com/31752211/48964208-80a66700-efdd-11e8-8eab-36407a9f3bb4.png\nWhere is the problem? Is the machine's resources in the cluster can not\nmeet the needs the tasks of togology? Thanks for your help.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/apache/incubator-heron/issues/3120, or mute the\nthread\nhttps://github.com/notifications/unsubscribe-auth/AGDwn7bX7XOgFGPs4EZc1osigc23rLacks5uyL9mgaJpZM4YxQaF\n.\n\n\n-- \nRegards,\nNing Wang\n. > Looks like the semantics for T and R go against the ones used in Java function types, say, BiFunction where T is considered the input type and R as the result type.\nYeah. For some reason R is the default generic type in Streamlet API. Like https://github.com/apache/incubator-heron/blob/5ee2490c025c7684c097962cf6d25f54ead73cf4/heron/api/src/java/org/apache/heron/streamlet/impl/streamlets/SupplierStreamlet.java\nI can update all of them if we decide to do so.. Cool. Thanks.\nYeah. Let's wait a little and see if there is any objections before updating all of them.. Cool. Thanks~. > to confirm, MY_OWN_RELEASE_INFO.yaml follows the same format as the CLI release.yaml?\nYes.. This new fix might be preferable.\nhttps://github.com/apache/incubator-heron/pull/3156. discard in favor of https://github.com/apache/incubator-heron/pull/3156. LGTM. @huijunw what do you think?. otherwise lgtm. > there was a contrib dir, which included spouts.\n\nhttps://github.com/apache/incubator-heron/tree/0.15.0/contrib\ni think we may recover the contrib dir, and incubate codes such as spouts from there till they are migrated to other places. thoughts?\n\nCool. Thanks!. I am thinking maybe we should have both spouts and bolts. Both are for reusable components anyway. For example KafkaSpout reads from Kafka and KafkaBolt could be the writer.. @simingweng thx for the input!. image size is 400m bigger. :(  abort. a powerful box? interesting. Just to clarify. the box runs the topology and the bookkeeper is a remote service?. Yeah. The travis folder has a few ci scripts. There differences are:\n- the travis job try to build everything and then run integration tests. It is for continuous integration, not for builds. Also in the scripts there are some travis specific code.\n- The scripts in this folder are general and can be easily used by different CI engines (like the description in README). It is for release only currently but will have some for testing.\nWe will need to migration the integration tests travis job to jenkins. I am thinking of creating a few new scripts for integration tests. Eventually the jenkins/travis folder (if there still is one) will be the config only, no scripts.\n. \"For configs, scripts/ci/config is better than scripts/travis/.\"\nAgreed. \nEventually yeah. jenkins and applatix will be using the same set of scripts with different configs.. We dont use k8s internally so it is hard for me to tell the exact case although it does make sense to me and I feel it shouldn't be too hard to support.\nIs it possible to set the vars in the open() function of each component? or it is different from your case? I am just trying to understand the differences.\n@kramasamy  @jerrypeng you guys have any input or time on this?. I see. Sounds like it needs to be in executor then.. Overall LGTM.\nAny tests done? like verifying physical plan with --dry-run?. > although stmgr/tmaster report uptime every 1 sec, the metricsmgr still report every 1 min? when a stmgr/tmaster is restarted, which still needs 1min for metricsmgr to flush metrics to expose this restarting event.\nYes. The only difference is that when the metric is reported, currently it could be 0, 60, 120, 180 or 0, 60, 60, 180 if the two process timer and reporting timer doesn't sync perfectly. After the PR, the value is more likely to be 0, 60, 118, 179, and so on. Verified after the fix. Cant attach screenshot. :(. Is there any output to share?. I went back and forth about it too. :(  Without the flag the size is like doubled. @mjschmidt @joshfischer1108 how do you think about it?. KK. Thanks.\nIn my test I remember they both work ok, except the size is ~600m vs 1.2G.\nI am fine either way but we dont really use docker image inside Twitter and it is ok if you guys feel it ok. I will approve it later if no objection by the end of day.. Another option is to make it a parameter for the script and the default is without that flag.\nThe only problem I met with that flag is that Apache jenkins hosts don't have the flag enabled. This could be a problem for users' ci environment as well.\n. @windhamwong how do you think about making it a script argument?. Build failure is caused by zookeeper library getting removed. Dmitry is working on the fix.. > @worlvlhole\n\nYes, I think you've got the most part. The KafkaSpout may be operated in 2 different reliability mode, ATMOST_ONCE or ATLEAST_ONCE, (I haven't added EFFECTIVE_ONCE implementation yet, I'm working on it).\nATMOST_ONCE mode\nthe whole topology will not turn the acking mechanism on. so, the KafkaSpout can afford to emit the tuple without any message id, and it also immediately commit the currently-read offset back to Kafka broker, and neither ack() nor fail() callback will be invoked. Therefore, \"in-flight\" tuple will just get lost in case the KafkaSpout instance is blown up or the topology is restarted. That's what ATMOST_ONCE offers.\nATLEAST_ONCE mode\nthe acking mechanism is turned on topology-wise, so the KafkaSpout uses the ack registry to keep tracking all the continuous acknowledgement ranges for each partition, while the failure registry keeps tracking the lowest failed acknowledgement for each partition. When it comes to the time that the Kafka Consumer needs to poll the Kafka cluster for more records (because it's emitted everything it got from the previous poll), then the KafkaSpout reconciles as following for each partition that it is consuming:\n\nif there's any failed tuple, seek back to the lowest corresponding offset\ndiscard all the acknowledgements that it's received but is greater than the lowest failed offset\nclear the lowest failed offset in failure registry\ncommit the offset to be the upper boundary of the first range in the ack registry\n\nSo, it guarantees each tuple emitted by the KafkaSpout must be successfully processed across the whole topology at least once.\nNot Implemented\nWhat is missing in this Kafka Spout implementation now is to handle the EFFECTIVE_ONCE scenario, which should completely rely on the checkpointing mechanism to decide how far it needs to rewind back. I'm working on it right now.\nI know this is quite some information, I'm writing README to explain things in more details, will keep updating the pull request.\n\nThis information might be included in the document?. > Slightly unrelated but, I think the need to set a list of spout requirements is still a big discussing that's needed.\n\n\u2026\n\nYeah. The requirements will evolve. Feel free to add things into the README file.. > Hi, thanks for this PR, happy to finally see a Kafka Spout implementation in Heron. I am planning on using this once it is merged, but I have a question. One major difference between this implementation and the Storm one is that Storm's spout allows emitting to different streams, using the org.apache.storm.kafka.spout.RecordTranslator interface. This implementation is missing this particular functionality, which is quite useful.\n\nIs there a reason for not keeping this functionality in this Kafka Spout implementation? Or is there another way to achieve similar functionality (other than creating a map-function like bolt for this purpose)? It's really useful for sending data from different topics to different downstream bolts.\n\nInteresting. Is topic -> stream 1 to 1? Why not have multiple spout components and each one is responsible for one topic?. > > Interesting. Is topic -> stream 1 to 1? Why not have multiple spout components and each one is responsible for one topic?\n\nTopic -> Stream is 1 -> 1 in our use case, but in Storm's implementation, it can be configured to be many -> 1 as well.\nRunning multiple spouts adds the overhead of running more Heron instances and Kafka consumers. We operate in a resource constrained environment, so that is not always feasible.\n\nGot it. Thanks. many -> 1 does sound useful in resource constrained env. It should be doable but there might be extra logic and config. I am thinking that maybe it can be created as a separated spout and share code with this 1:1 version.\n\n\nvery good question. I actually started with a \"one-record-to-many-tuple\" implementation, then I gave it a deep thought when I was implementing the ATLEAST_ONCE delivery guarantee. Allowing \"one-record-to-many-tuple\" will significantly complicate the algorithm to track acknowledgement, because then we have to keep tracking the mapping relationship between a single Kafka record offset to multiple message IDs.\nAnd then we also face a design choice whether the KafkaSpout itself should decide the uniqueness of a set of Message IDs coming from the same ConsumerRecord, or we should open the choice up to the developer?\nSo, a neater choice is to use multiple KafkaSpout, each dedicated to an output stream.\nBut, I do agree \"one-record-to-many-tuple\" is pretty useful and cost effective in terms of resource consumption. I have no obligation to put it back in, but then it becomes the developer's responsibility to make sure avoid emitting multiple tuples out of one ConsumerRecord ONLY in ATLEAST_ONCE mode, at least for this version of KafkaSpout before we introduce a more complicated ack/fail tracking mechanism.\n\nAgreed that one record -> many tuples is really useful, but it does make tracking offsets a lot harder. However, I don't think that is possible using the current Kafka spout implementation from Storm - it only allows 1 record -> 1 tuple emits, but to any declared stream.\nHaving this ability to choose the stream given a record would be a good start for the Heron Kafka spout, and it's much easier to implement.\n\nAre these streams the same or they are different (like apply different filters/transforms)? If they are the same, what is the difference between it and spout has one output and multiple bolts register to that stream? Different offsets?. LGTM.\n@kramasamy @srkukarni What do you think?. Thanks.\nAnother fix is to replace the link and keep the same version number. I am fine either way.\n@kramasamy Any comment?. > I was having difficulty finding the 3.4.10 version anywhere except the Apache archive site. The Apache site indicates that downloads should be made from a nearby mirror site rather than apache.org. Otherwise I would have taken that approach. If another reliable link can be found I can update the links rather than the version. But I did not fiInd a reliable link.\nI see. Thanks!. Thx.\nIf we change to zookeeper-3.4.13, when zk has a new release, it will be be removed from mirror sites and we will have to update again. I am curious what exactly the apache suggest means.\nMentors do you have any suggestion? @dave2wave @julienledem @ptgoetz \nFYI, Dmitry's PR for the issue is here: https://github.com/apache/incubator-heron/pull/3207\n. Might be safer to stick with a specific version and manually upgrade (just my 2 cents). In this case, archive.apache.org is fine?. Thanks!\nDave has some suggestions in the other similar PR. I am going to merge this one first to fix the build and see if we can improve it more.\n. Hi, @rohanag12, thanks a lot for your contribution!\nThere are some other works going on as well with Bazel 0.21. I believe this is the current state: https://github.com/kramasamy/heron/compare/master...kramasamy:karthik/bazel021\nLet's collaborate and decide what we should do.. Overall LGTM.\nI'd like to have approvals from Bazel experts before merging into master. @kramasamy @joshfischer1108, @skanjila?  Thanks in advance~. Is there any side effect with that change? All things should still work the same way?\nIf no concerns, let's do it~ Thanks! . Is there a way to verify the behavior with unit tests?. Most operators don't have \"failure\" case (for example filter, flatmap). Only ack I remember. \nThe only way to trigger fail() I can think of now is to write a simple bolt that has failure cases and then use the applyOperator() function.\nI was thinking of catching exceptions as failures but it might have some unwanted side effects so it is not decided yet.\n. After the change rat check passed.\n$ java -jar ~/Downloads/apache-rat-0.12.jar . -E .rat-excludes\n.....\n  N     ./tools/travis/toolchain/BUILD\n  AL    ./tools/travis/toolchain/CROSSTOOL\n  AL    ./vagrant/Vagrantfile\n  AL    ./vagrant/init.sh\n***********. LGTM.\nWe don't use dlog internally.  @jerrypeng  do you have time to double check? Thanks!. Thanks @jerrypeng ~. LGTM. Thx~. The translation in StormSubmitter processes topology level configs. Components have their own configs and component level configs are merged later when Spouts and Bolts are created.\nWithout this change, the raw Storm configs for Spouts and Bolts are passed into Heron layer. As the result, TOPOLOGY_TICK_TUPLE_FREQ_MS is missing in Bolt level because it is generated in the translation by Config.setTickTupleFrequency().. Doing all the translation in StormSubmitter.submitTopology() is one option, but it would require iterating all components and replacing their configs(so there will be heron configs in storm objects, which is not ideal). Overall, doing translation in *DeclarerImpl seems to be cleaner to us.. Why 9? Suggest to make a const for it with a name.\nAlso I am wondering if it could be useful to have an else {} block with a log?. I see. Feels a little scary for future maintenance, especially for adding or removing ports. I looked at the other PR briefly but don't have good idea about the typical use cases yet. A few questions I am wondering so far:\n\nis this map necessary? can we limit the jvm debugger to be one at a time? or it is normal to have multiple debuggers in jvm world?\n\ncould it be better to refactor all ports to be a port map instead of creating a map for debugging port and leave others as is?. I am wondering if \"interval\" is more clear than \"timerDuration\". Open to other thoughts.. need to be public?. Formatting looks different.. Sort. why \"Legacy\"? My understanding is that this API should be suggested in some cases. Same for the functional comment. It is not \"External\".. Not sure if this is necessary.\n\n\nStreamletUtils is a class in examples directory. I feel it is reasonable for heron to provide a general function like sleep.\n\nUtils looks like a low level object, correct? Why high level classes should avoid using it? . Make sense. Thanks.. what is this dummy ctor for? Is it better to have mis-usage protection if this is required by kryo?. Is this change necessary?. Suggest to be more explicit in error message, like \"Linking with kryo is needed because useKryoSerializer is used\".. seems not necessary?. Hmm. Do we really want to make Kryo as default?\n\nAlso I don't like this new boolean. If in future we need/have a new serializer, this code could become messy. In fact it already has issues If user calls: builder. useKryoSerializer(false). useKryoSerializer().\nI am wondering if an enum could be cleaner here.. Suggest to have two stages.\nsetSerializer() function set the variable. And have another applySerializer() to be called in build() function.. Not sure if this error message is accurate or not.. true. better to have an example.. What if the id is not found? It would be better to have protective code instead of creating a connection to \"http://x:-1/killexecutor\".. Suggest to move this logic into a function and compare the names in the reversed way:\nStMgr searchContainer(Integer id) {\n  prefix = \"stmgr-\" + id;\n  for (StMgr sm : stateMgrAdaptor.getPhysicalPlan(topologyName).getStmgrsList()) {\n    if (sm.getId().startsWith(prefix)) return sm;\n  }\n  return null;\n}\nIt is more readable and easier to test the function.. suggest to move internal util functions to the beginning.. Please add the port info in the logging message below if it is not there yet:\nLOG.info(\"\\nStarting instance \" + instanceId + \" for topology \" + topologyName\n         + \" and topologyId \" + topologyId + \" for component \" + componentName               + \" and topologyId \" + topologyId + \" for component \" + componentName\n         + \" with taskId \" + taskId + \" and componentIndex \" + componentIndex                + \" with taskId \" + taskId + \" and componentIndex \" + componentIndex\n         + \" and stmgrId \" + streamId + \" and stmgrPort \" + streamPort               + \" and stmgrId \" + streamId + \" and stmgrPort \" + streamPort\n         + \" and metricsManagerPort \" + metricsPort);                + \" and metricsManagerPort \" + metricsPort);\n LOG.info(\"System Config: \" + systemConfig);             LOG.info(\"System Config: \" + systemConfig);. suggest to add comments for the if and else blocks.. QQ: Do we need to delete the old node?. s/then/throw?. real names for the results.. s/delete/considered as success. this is expected?. suggest to move this line to beginning. not a blocker.. suggest to be the same order as build-artifacts.sh. what if remapping doesn't have the key?. - the actual deployed container-id is a ContainerPlan object? the comment is a little confusing.\n\n\nif the mapping is not needed in a specific scheduler, the function should return empty map?\n\nsuggest to have a comment about it.. should this function be VisibleForTesting too? Seems more suitable for unit tests.. suggest to factory out this block into a new function like extractContainerIds() so we can test it.. Not good to match against \"Querying instance statuses: \". I think it is better to match against regex \".[(\\S)]\" and extract the string within [] and then parse it.\n. Do we need to check state&pointer here? Can conn_ be NULL in some cases/states?. suggest to add \"and droptuples_upon_backpressure is set to true\" in log.\nAlso, how many logs are we talking about here?. Do we have any information for the package we can log? Like time window, etc? Sorry if this is a silly question.. same, suggest to add \"and droptuples_upon_backpressure is set to true\" in log.. I am just wondering if this should be a WARNING log. Not a blocker.. Suggest to add a log here like \"droptuples_upon_backpressure is set to true, hence no backpressure mechanism is initialized\". The drop count is for both not connected and backpressure, so the log could be misleading. Just a concern to me, not a real issue. Your call.. Just to confirm. The drop upon backpressure logic is handled in the \"(!IsConnected())\" branch. Is it expected? Do we need it in the \"(IsConnected)\" branch?. I don't know this part of code yet. Just to confirm, the StartBackPressureOnServer() call doesn't affect the HasCausedBackpressure() result, right?. TopologyUtils.getTotalInstance() doesn't work? Maybe worth adding this code to TopologyUtils as a util function?. Please add comment about what the logic below is doing and why. This function is not very straightforward.\nAlso are there unit tests for this class?. I am curious about how the original number of container is evaluated? Is it configured or calculated? If it is configured, maybe better to pass a parameter? If it is calculated based on topology, can we run the same calculation with the new topology config to have a good number of container? I agree with maosong that the ratio may or may not be the best option here.. Hmm. Is it possible to avoid the .sh files here?. revert this change. do we need try/cache here?. Why a new downloader is added? HttpDownloader is not enough in this case? The implementation seems to be the same.\nIf there is a new case, please add comment to explain. My concern is that in future people dont know which one to use or could mis-use them.. space between \"+\" and \"[\". I think refactoring might be better. Easier to read and easier to test.\n. why 2181? I think it is the default port for zk so maybe better to avoid if possible.. kk.. Got it. thx. Please add a comment about the use case. An example could be good enough.. Remove the empty line.. Remove the empty line.. nit: s/succeed/Succeed\nThere are a few of them in this file. nit: s/fail/Fail. nit: bad indent.. heron.aurora might be ok (maybe not ideal though) because aurora file is a config and used by aurora cli.\n.sh file is worse than aurora file. :(  So I am wondering if it is possible to avoid this .sh file and have a config to be used by nomad CLI?\nI don't know nomad enough and I am open to other thoughts/suggestions. Also this is not a blocker if this is not feasible.. got it. thx.. kk.. :( required. Is it possible to be optional?. is \"current packing plan\" accurate?. I am wondering if it is easier to understand this way:\nnewComponentParallelism = currentComponentParallelism;\nfor (e: componentChanges) {\n  newComponentParallelism.put(e.getKey, e.getValue);\n}\nint newNumInstance = TopologyUtils.getTotalInstance(newComponentParallelism);\n. also, I am wondering if we should verify that the value needs to be positive?. Hmm. API is removed? This will affect Twitter internal use case. Is it possible for you to add it back?. I don't see any scheduler passes this flag. Why it is not causing errors?. I see. That could be the case.\nAnother note is that it seems all the other schedulers have statemanager configs explicitly in the following file. It may make sense to have this new config as well, especially since it is a required field.\nheron/config/src/yaml/conf/kubernetes/statemgr.yaml\nheron/config/src/yaml/conf/marathon/statemgr.yaml\nheron/config/src/yaml/conf/yarn/statemgr.yaml\nand so on.. Never mind. I see what you mean. You want to override the config file itself.. @jerrypeng  Cant add you as reviewer. :(  Here is the PR for aurora when you get a minute. I am testing the change internally.\nhttps://github.com/twitter/heron/pull/2630. nit: suggest moving \"[conf/*..yaml\"] to next line, and suggest sorting them.. suggest adding an example here.. suggest adding an example here.. suggest adding an example here.. Maybe a stupid question, can we use if (metaData != null && metaData.get(NomadConstants.NOMAD_TOPOLOGY_NAME) == topologyName) here?\nAlso if we need to break it into two if statements, I think if (metaData != null ) { if (...) } might be better.. 1. Why change to FINE? This error looks serious.\n2. why append strings in two places? One in msg =, another in LOG.log()?. Just to confirm that this indent is expected or not.. sort. suggest to put the standalone command to the local command list, instead of here as an or operation. It is  more complicated than the version command.\nSee below in main() function,\nlocal_commands = ('help', 'version', 'config')\n. It seems cluster config is used only for downloading topology after this change? If this is true, maybe add comment or rename variable. The variable itself gives me an impression that there are a local mode and a cluster mode.. s/2016/2017.\nThis is a new tool? Is there a doc about it?. Users who want to customize the tools. It is needed by multi-version support inside Twitter.. I can also add an option to override heron client data files in cli to use this functionality.. Yeah. It is the number of attempts to connect to a specific endpoint.. In fact, let me think about this line again. The idea for the old and new code are similar and the new code focus on the first time. I may consider reverting this line. Thanks for the question.. The function stops waiting and returns false.\nIn this case, the code works pretty much the same way as before after the waiting. Except exitLoop() set a flag to true so the looper won't do any real work afterwards.\n. It happens to all algorithms in fact, but it may not be a problem because some algorithms requires per component resources specified.\nAnd it is impossible to be handled in RoundRobin. Different container has different set of components. RoundRobin calculates the components with specified Ram sizes first and allocate the memory left to the rest components. As the result, instances in different containers quite often have different memory size. The example in the description is the simplest case.. is this expected?. I think this line might have a bug.\nidx1 is not -1 when the string is not found. Instead, it is -1 + pattern.length(). So you cannot compare it with 0.\nSuggest to add a unit test for the function to make sure it works correctly in both successful and failure cases.. Suggest to have a unit test for this function, not a blocker though.. the function signature is \"String auroraStdoutStr\" and here the parameter is stderr. Maybe the parameter should be renamed.. I think it should be better to test containersToAdd.size() == newAddedContainerId.size().\nThe controller.addContainers() function could return empty set or smaller set, and we need to handle the two cases here.. kk. thx.. UIDs seem to be the same as Java version. Just to confirm if this what we want or not.. consider import the package as\nimport com.twitter.heron.api.{Config => ApiConfig}\nthen use ApiConfig.TopologyReliabilityMode, etc\ninstead of full path.\n. I am wondering if we can use case class here? if it is not expected to be extended.\ncase class KeyValueK, V {\n  override def toString() = ....\n}\nval or var depending on the usage.. same as KeyValue, maybe case class if possible.. suggest to add {}.\ntry {\n  ...\n} catch {\n   ...\n}. is this line necessary?. another candidate to be case class.. maybe keep the Runnable() wrapper?. suggest to revert this change. This pattern seems to be used in some other files as well, so this change seems to be unnecessary.. if a topology has defn and executionState data, it is considered as \"running\", otherwise it is considered as \"not running\". This is the validation logic in RuntimeManager, and i updated the validateSubmit() called right above to follow the same condition in this PR.\nI was thinking of using defn data as the source of truth, but executionState is very important in RuntimeManager validation. So here I am keep both. Another option is to use executionState only as source of truth, if you think it is better that way.\nWhen the topology is \"not running\", it should be safe to call cleanState().\n. The issue here is for a bad stmgr, all connections are failing and it is more likely to be a local problem. with a global max attempt, stmgr doesn't need to wait per client failure count to reach 300. Because network failure can be very slow, bad stmgr tends to restart a lot slower than good stmgr.\nwhen there is only one bad connection, it is more likely to be a remote problem, so we don't  want the stmgr to kill itself too soon. changing 300 to 300/8 makes good stmgrs to quit a lot sooner and this is not what we want.. The PR is a simple demo and I don't really like this solution myself. I just think we need a check based on global view. Any ideas are welcome.. not a blocker. your call.\nnothing wrong. more about consistency to other modules.\nalso in case some users may have to run heron on java7 for any reason.. Sounds good to me.\nThe lines are pretty much copy/paste from rules_scala. In Twitter we need them to be explicit so that we can redirect them. :( I am just testing if it works ok or not. It seems like Travis is happy now. I am going to update the number.. Suggest to add a comment here.. update year. Personally I feel this is a bit overkill.\nAurora does resource check for prod environment first and throw an error if there is no enough resource so try/catch should work although not perfect. It is definitely nice to have a pre-check to have more meaningful responses, but I am not sure it worth more than 100 lines of string parsing code (which is normally error prone and sensitive to aurora output format).\nAnother concern is that we are assuming some aurora configurations in this check and it might be configured differently in other environment or even customized (for example, dedicated cluster, different resource management). As the result the code may not work in other people's setups and we won't be able to reproduce the issue and fix it.\nFurthermore there will be new aurora modes in future and it is hard to maintain this code. For example, the \"tier\" property was \"production\" not too long ago. If it changes in future, or some company is using old aurora, the code doesn't work.\nI would prefer \"keep it simple and stupid\".. I am afraid that this is not totally true. Dedicate cluster doesn't have quota requirements to start a cluster I think.\n\"Aurora requires resource quotas for production non-dedicated jobs. \" in\nhttp://aurora.apache.org/documentation/latest/features/multitenancy/. This is not safe.\nAlthough \"tier = preferred\" is the current config for production jobs,  I would expect that \"production: true\" is still being used in many other companies.\nAlso tier is customizable, so I think there is no safe way to check isProd().. It is not a safe assumption.. the env value is less safe. :(  . I agree with maosong that this is a quite common issue and we should see what we can do. I think a warning could be helpful for users to be aware of this issue. They are free to take the risk but it is our problem that they don't know the risk.\nIdeally we should be able to tell user how many new containers (no need to be core/ram) will be added so user can verify either there is enough quota or there is no quota cap before proceeding.. I mean from aurora point of view.\nFrom aurora doc it seems to me that environments are configurable. For example, inside Twitter we call it \"prod\" and in apache doc it is called \"production\".\nAnother one is that users might config their prod jobs to be preemptible if the SLA is low, which doesn't need quota although it is a \"prod\" job. This requires user to customize the aurora configs, but it is quite easy from heron design.. Thanks!\nFor 1, \"Config runtime\" is for the topology configuration. running flag is not a config.\nFor 2, will do.. I think it might be better to check \"y\" instead of \"n\". don't forget to update \"[Y/n]\" to \"[y/N]\". suggest: \"No console to prompt user. Proceeding.\". looks like you dont need the space at the end of fmt.. maybe rename to something like confirmWithUser(). How do you think?. maybe a warning to be more explicit. I am wondering if it is possible to move this check to RuntimeManagerRunner.java.. This error message is declared and used in a few different places. Is it possible to clean it up?. \"Scheduler failed to add requested containers. Requested X, added Y. ...\". \"Running\" means there are both topology defn and execution state data.\nIf any of them is not available, it means the topology is not \"running\". This could be caused by a failed submit command or a failed kill command.\nLike we discussed before, in this state, kill command won't work. User has to learn how heron stores data in different places and checks/clears them manually, which is not good. In this PR, kill command is allowed with warning messages.. And for all other commands, validation code shouldn't allow them to be executed. So in this line, isRunning flag should always be true. I am adding this assert to make sure the code fails fast if there is any bug. After all, all other commands requires topology to be in normal state.. That is true. Let me think about it. Thanks.. The function addContainers() above the if statement is in AuroraCLIController, the code looks like:\npublic Set addContainers(Integer count) {\n  ....\n  return extractContainerIds(stderr.toString());\n}\nYou should be able to check container added vs count in there before returning I think. Then no changes will be needed in this file.. The config name is not very straghtforward. Suggest to add a comment for it. Also include the values:\ndisabled => no prompt\nprompt => enable prompt.\n. kk. that could be useful.. still not very clear. How about let's do something like this:\n// Prompt user when more containers are required so that user has another chance to double check quota is available. To enable it, change the config from \"disabled\" to \"prompt\".. I think if topology data is not available, it is possible that the topology was not running.  The message \"Scheduler killed topology %s successfully.\"  can be misleading. So I think in this case, \"scheduler has tried to clean up\" is more accurate.\n. sigh.\nMaosong suggested \"exist\" but I feel it is not good either. :(\nThis is a trick case and naming is tricky too.\n. - dataFound\n- topologyFound\n- topologyExist\nor reverse them:\n- topologyNotFound\n- topologyNotExist\nWhich one sounds better?. it is not only about executionState.\nWe got a report later that executionStateExists is available but definition is not. Basically none of them is safe.\nThis flag is from this logic:\nBoolean isTopologyRunning = adaptor.isTopologyRunning(topologyName);\nboolean found = isTopologyRunning != null && isTopologyRunning.equals(Boolean.TRUE);\nThat is why I used isRunning at beginning, which is not ideal for sure.\nAnother option is to skip the validation altogether for kill command and no need to worry about which data is available. The checks and flags here are used to show different message, except envronment check if execution state is available (which isn't critical for this command to me).\n. kk. sgtm. let me make the change.. It seems \"ExecutionState\" is a bit unclear here. One zk node is for \"ExecutionState\", the code is (used as one of the two checks in the validation function):\nExecutionEnvironment.ExecutionState executionState = adaptor.getExecutionState(topologyName);\nI am going to use \"hasExecutionData\" in this function and hasStaleExecutionData in Runner class. Let me know if I missed something.. sounds good. let me do it.. why \"if parsed_args.jvm_remote_debugger_ports\"? typo?. do we need a default value?. two questions:\n1. is modifying the list at the time expected? If not, maybe a more proper fix is to solve issue in the appending code?\n2. will some exit tasks and tasks on wake up being missed with copyOnWrite list?. I would suggest checking == 0, like\nfor (int i = 0; i < s; i ++) {\n      exitTasks.get(i).run();\n      // in case previous task cleared the list\n      if (exitTasks.size() == 0) {\n        break;\n      }\n}           \nAnother qq, no tasks would be added when this function is called? I think onExit() might be safe but executeTasksOnWakeup() is a concern.\n. I think we need to answer two questions beforehand:\n- where the tasks are added?\n- when can tasks be added?. it is more defensive, but confusing mathematically.\nIn fact I dont like either of them because it is not ideal to check the size an array when it can change.\nAnother option is to change the interface:\ntask.run() should return a boolean value indicate the loop should be continued or not. When it returns false, the loop should clear the list and return. Something like:\nfor (int i = 0; i < s; i ++) {\n  if (!exitTasks.get(i).run()) {\n    exitTasks.clear();\n    break;\n  }\n}\nThis way we can avoid the clear() call in tasks.\nHow do you think?\n. It seems that it is possible and allowed to add new tasks when the array is being executed. :(\nbut i think the new tasks added by the tasks before restore should be cleared too. So the clear() call is still correct.. If we want to use this lock, it needs to be an atomicBoolean, or needs to be volatile.\nmy concern is that simple boolean might have code cache and can be inconsistent between threads.. QQ: Can tasks be added by other thread?\nIf they can, then there might be a racing condition.\nthe existing tasks are being cleared, and a new package is received and some tasks might be added. A lock flag is not enough and we have to sync up the two thread.\n. ? why not import them? SerializablePredicate, SerializableConsumer, Sink, Context?. import. use import to avoid full path. Ideally we should support this syntax.\n.map[String] { num: Double =>\n  (num * 10).toString\n}\nNot a blocker for now. We can add this support in the next version.. Clean up the full package path com.twitter.heron.streamlet.impl.streamlets. is it guaranteed that the clear() function is called in the same thread? the concern is that if the clear() function is called in a different thread, it might be called after the if() check and you can still get the wrong index error. Since this object might be used by new code in future so there may not be a guarantee even if there s no problem for now. I think it might be better to have a comment to be clear that this flag is not thread safe and the clear functions are required to be called ONLY in the same thread.\nOr I am wondering would it be safer to make onExit, executeTasksOnWakeup, clearTasksOnWakeup and  clearExitTasks synchronized?. maybe better for the two flags to be initialized in constructor?\nAnother suggest is to rename to clearTasksOnWakeupFlag to avoid variable and function sharing the same name. Maybe not a big deal. Your call.. I was thinking of simply adding synchronized keyword to these functions. I believe Java synchronized blocks in Java are reentrant so it is not a deadlock.. to import. to import. to import. to import. to import. no need for the full path. no need for the full path. no need for the full path. Just want to double check. I think the new build targets are heron.tar.gz and heron-install.sh and I am wondering if these lines need to be updated as well.. I see.\nI think you may consider using alias like:\nimport com.twitter.heron.streamlet.{SerializableTransformer => JavaSerializableTransformer}\nimport com.twitter.heron.streamlet.scala.SerializableTransformer\n. maybe better parameter name. this is expected?. add \"@return\". \"scala.collection.JavaConverters.\" is not necessary.. suggest to use import alias.\nimport com.twitter.heron.streamlet.{Source => JavaSource}. to remove or to implement?. \"override def get()\" to be consistent. \"override def get()\" to be consistent. clean up empty lines. ??. use import alias.. This function name \"toScalaStreamlet\" seems to be a bit misleading. \"fromJavaStreamlet\" might be better?. test data generation as well.. test data generation as well.. \"override def get()\" to be consistent. suggest to rename to fromJavaStreamlet(). The current class is streamlet.scala.impl. StreamletImpl, so I feel fromJavaStreamlet() makes more sense.. Suggest to use alias,\nimport com.twitter.heron.streamlet.{Builder => JavaBuilder}\n...\n    new BuilderImpl(JavaBuilder.newBuilder())\n. Function signature doesn't match comment.. Please use import alias instead of full path. hmm. interesting. another note is that \"asJavaCollectionConverter(...).asJavaCollection\" looks a bit strange. from function name, it seems asJavaCollectionConverter(...) should return a Java collection, hence the following asJavaCollection looks a bit confusing. Any idea why this is needed?. Suggest to use import alias. description seems to be wrong.. sort imports. Ah. Good call.\nI refactored the common validations into a function and call it from all handlers. I must have missed this one. Thanks!. This is not a typo. 'topologyid' is the URL argument. The original error is:\nLOG(ERROR) << \"topologyid not specified in the request\";\nLet me add '' around it to be more readable.. sort. missing incr()?. missing incr()?. missing incr()?. Might be safer to be in this order?\nlooper.exitLoop();\nmetricsMgrClient.stop();\noutMetricsQueues.clear();. so that in case user has a typo, topology can reject the request.. got it. will do.. Yeah. That is correct. This check is just for protection. I am open for other thoughts.\nHere are a few cases,\n- the topology has a config topology.config.1, and user can use command line tool to update the value. in case there is a typo like topolog.config.1, we can reject the config change.\n- This could also be useful when two configs work together and need to be updated at the same time. If one of them has a typo, the update command should be cancelled.\n- Or if the component name is wrong, the issue can be detected instead of waiting for the topology to react.\nLike you said, the limit is that the config has to exist. I feel it is safer this way because if you want to keep the runtime config in future, you have to update the config manually in code. Requiring the config to exist could avoid some mistakes.\nThe check doesn't cover 100% cases and the limit is kinda artificial. If you guys prefer not having the check or any other idea, I am totally happy to consider.. I am going to use UpdateRuntimeConfig for this one.. On the other hand, we can skip the check and let all runtime configs pass through. In that case, in addition to user mistakes, there might be some unused configs and it could be harder to track them.. right. Technically it is not really necessary.\nI left it here to be consistent with the decoding path and easier (just a little) for people to understand. In future we can remove both of them at the same time.. Sure. added.. Thinking about it again. Now I don't think leaving the encoding part here is very helpful. Plus it looks wrong and could cause more confusion. I am removing this replace() call in the new update.\nIt is also easier to tell that the decode() part is for backward compatibility purpose only without the encode() part.. sort. clean up?. remove ths empty line. First of all, what is the impact? This could cause crashing? or bad results?\nThis line is a bit redundant. Just notes, not a blocker.\n(index + taskIds.size()) % taskIds.size()\nor \nif (index) {\n  index = index + taskIds.size();\n}\nshould be enough.\nAnother option might be \nMath.abs(keyExtractor.apply(obj).hashCode()) % taskIds.size();\n. do we need to add the same logic in c++ and python?. yeah. the existing ones, instance.cpu, etc are heron internal configs for all topologies\nthe new ones here, component.default.cpu, etc are for users to set/config for all instances in each topology.\nthe future ones will be for each component.. I am wondering if the change in this file is expected/needed in this PR. it seems unrelated and looks a little strange.. 1. It is a good question. I think both makes sense, in a different way. If you consider the next step, per component resource config, component_default makes more sense. If you consider that all instances are using the config as default value, instance_default makes more sense. I am thinking instance_default might feel better now. Open to other thoughts.\n\nreplacing? existing configs are used as fallback values, unless there is a bug.. I see. So now k8 is considered as a platform? A little unclear but not a blocker to me.. Math.abs(hash) can give negative result? Sounds strange to me.\n\nbitmask is dangerous because it assumes the var is 32 bits. In case someone changed the data type it can cause other problem.. use default arguments directly. suggest to break this code into two steps to be more readable\n1. create musicSource1\n2. musicSource1.filter(...)...... Same as above. Suggest to break the pipe to two steps.. not effecient. Why not just use:\ncase x if (x >=0 && x <= 99) => x / 20 + 1\ncase _ => throw. same as above.. got it. kk then.. suggest to move this above musicSource2. :)  not a blocker. sounds good! Didn't even thinking about two characters. :D. We do want to keep the current format though to be consistent to component_parallelism.\n[comp:]:,[comp:]:,.....\nThe number of component can be infinity, not really 2.\nThis is the argument for scheduler, not the CLI argument, which is in python. The CLI argument is in the format in your example.\n. bad indent. sort. I am wondering if ObjectWriter is available to us, as well as the how to reuse this object.\n. I think the output of toString() might be more readable than asBytes() if it works here.. ah. sorry my bad.\nI couldn't tell the difference between uppercase O and lowercase o. . Thx for the reply. I wasn't too worry about the withDefaultPrettyPrinter() function.\nIt just seems like ObjectWriter and reusing are preferred based on some ObjectMapper document, so I brought the question up. ObjectMapper is kinda magical to me. Hopefully some of us have more experience with it could provide more feedbacks. :). I see. Ok then.. Sounds good to me. thanks~. kk. Thx for the input.\nI am going to approve it and you can decide when to submit.. Quick questions:\n\"Copyright 2018 Twitter. All rights reserved.\"\n\nDo we need to change this to Apache as well?\nHow about the year number?. why this file is here?. why refresh?. maybe \"changed to http://.......\"? I am wondering if apache has a standard page with extra features like a better design, localization, etc.. Yeah. It is an unrelated change and has been merged in a different PR.. kk thx.. kk. component name could be useful for viewing data. Will add.\n\nFirstly, stateful data is organized around components (the schema should be the same for all instances/partitions of the component and could be different between components). It is overall a 3-level tree structure: topology is the root, component is the mid level and instance/partition is the leaf level. We need to store component level data in storage.\nSecondly, the parallelism value is important to detect the plan changes and handle the change correctly. In our first state, we can drop the stateful data when parallelism doesn't match. In future, we should handle repartition based on the old and new parallelisms of the component.\n. They will be used in CheckpointManager. I will add the calls and TODOs.\nWhen a checkpoint is created, instance 0 of each component should be responsible to call this function to save component information to storage.\nDuring a state restore, all instances should load component data first and see if it is possible to recover from the checkpoint (and in future how to recover), then load instance/partition data.\n. Good point!\nThen storage implementation need to make this decision.\nInstance 0 should be responsible for the meta data in distributed storages; and for local storages, all instances need to export the data (and the data needs to be stored in the instance/partition directory). StatefulStorage implementation should know the information and handle the function correctly.. That is one option for distributed storage. Another option is to store the data along with instance 0. This is a question we need to answer when updating stateful storage implementations and adding this feature.. I guess these package names should be reverted?. And these package names?. We have a brief doc internally, but it is not as detailed as this PR. I will clean up the doc and share it here.. The basic idea is that I would like to decouple the API of stateful storage from instances first so that the the implementations are independent of partitioning/repartitioning which we will need to handle later. And partitioning/repartitioning relies on component data such as parallelism. So a basic structure is added here and it will be extended later.. @skanjila here is the doc: https://docs.google.com/document/d/1p443mcdD607Vf9aCnMq5ESVMFrEuneGlceXmtUSvVMQ/edit#heading=h.myl6mycadhfh. what happens if uri and destination are null? the old logic has protection and it seems new code doesn't. I think it might be better to have a check here.. yeah. this info will be passed to ckptmgr, so that it knows if repartition is needed and how to repartition.. We are thinking about calculation change. Not sure how useful it is though. Please feel free to let us know if you feel this is overkill.\nNormally the calculation in bolts won't change much and the version number doesn't need to change. However in case the equation is changed, for example,  the old value is a duration in seconds and it is changed to duration in millisecond, or it is possible the value is a data structure and a new field is added. User can increase the version number and provide a conversion function between the two versions. Then the topology would be able to load the stateful data stored before the update, or skip/reject the data if topology doesn't know how to convert between the two versions.\nIf topology cant tell the difference and just load the data, the state could be polluted without user knowing it. The current design seems to rely on users manually removing the data? it could be risky and hard since the topology needs to be running in order to do it I think?. kk. Let me remove it for now until we have a better idea/support.. chptmgr is stateless and it doesn't have the physical plan, so it doesn't know the parallelism by itself.. But on the other hand, this could be a good idea. If ckptmgr receives the topology structure during initialization, we can save the field from the requests. Let me see how to do it.. Checked the communication between stmgr and ckptmgr, it is a valid option to pass the topology structure information during the registration to simplify the requests.\nWe don't need to make the decision for now about where the structure information should be passed. Therefore I am going to remove this field for now to keep it simple.\nThanks again for the comments!. Suggest: \"Failed to serialize object: \" + object.toString(). could be helpful to have a default value?. suggest to keep it consistent with dispose(), which uses:\nString[] names = new File(topologyCheckpointRoot).list();\nSorting might be cheaper this way as well.. I would suggest to break the logic into two steps:\n1. generate the toDelete list.\n2. for each one in the list, deleteDir().\nThe benefit is that you can write a unit test for step 1 easily to make sure the order is correct.\n. if we do:\nfor (int i = 0; i < children.length - remaining; ++i) {\n}\nwe dont need Comparator, and the sorting might be slightly faster because the list might be likely to be in the same order already.\n. hmm. this seems to change some assumption that physicalplan doesn't change by itself. is this really necessary to get physical plan every time? and how often the function get called?. yeah. the code looks long.. kk. thx.. Agreed.\nAlso, is it possible to have a more general metadata argument like \"SCHEDULE_META\" so that this is for all schedulers? It is not ideal to have AURORA specific configs in heron/spi/common/Key.java to me.\n. Yeah.\nI mean why a tuple instead of a string caused the exception. It seems the exception is triggered in a different place so some explanation could be helpful. :). Ah. I see. That is sneaky. :D  LGTM then. Thx.. it does look strange. From the code it seems this is used to support space in path. And the %%%% can be found in cpp, java and python with code search.. kk.  I was feeling InstanceStateCheckpointPartition is more correct, but it can be renamed later.. Please move this encoding code to its own function and add a unit test for it.. suggest changing the comment to:\nSerializeToString() returns object in binary format which needs to be encoded.. to clean up?. replace darwin with platform. are we going to need these lines?. Give 57, 48 and 55 const variables to be more readable.. Suggest to rename the functions to hexEncode()/hexDecode(), or base16Encode()/base16Decode().\n. Add comment:\n// A simple Hex (Base16) encoder.. Add comment:\n// A simple Hex (Base16) decoder.. check if i+1 is valid.. Suggest refactoring this logic to its own function and use it here.\n// Return decoded value in [0, 15]; return -1 if the character is illegal.\nprivate int decodeHexChar(char c) {\n  const int char_zero = 48;\n  const int char_nine = 57;\n  const int char_A = 65;\n  const int char_F = 70;\nif (c < char_zero) {\n    return -1;\n  } else if (c <= char_nine) {\n    return c - char_zero;\n  } else if (c < char_A) {\n    return -1;\n  } else if (c <= char_F) {\n    return c - char_A + 10;\n  } else {\n    return -1;\n  }\n}\nThe code is easier to read and also have protection.\n. then please add the comment for standalone mode.. maybe add an empty line above to be more readable.  same to other files.. suggest to use have both \"activated\" and \"deactivated\". So that if user has a typo, we can throw an error instead of blindly enable it. Plus the code might be easier to read that way.. suggest to refactor this code into a findInstance() or findFirstInstance() function.. bad indent. a little complicated, please add comment.. where is this function hooked up?. I am a bit concerned that if loopExit() is safe to call here.\n- which thread it is running on and if there might be racing condition.\n- if it is possible that loopExit() might get stuck.. safer way is to use a temp var to make sure the object is erased from the map before deleting it.. Still not idea to have AURORA_METADATA_1, AURORA_METADATA_2, and AURORA_METADATA_3.\nI am thinking maybe in addition to auroraProperties, which is for standard properties with type Map[AuroraField, String], we can add another Map[String, String] extraProperties for these dynamic properties, so that we can avoid the hard coded names.. The new map would be the new 'Map[String, String]'. No need to add to param list I think since the values are generated from existing arguments, but I might be wrong.. nvm. i see it now.. kk. It is ok to leave it there to me. Let's see if @srkukarni has any input.. sort. startsWith() and && might be better?\nAnd please add a space after \"if\" and before \"{\". This function is a bit long. suggest to refactor some blocks into smaller ones.. bad indent. is this expected?. suggest moving this line to a separated PR.. Suggest refactoring this block to a util function ParseResponseToPhysicalPlan(byte[]). local variable might be cleaner.\nphysicalPlan = getCachedPhysicalPlan();\ngetBoltNames(physicalPlan);. same here. local variable might be more readable.. the \"Cached\" feels a bit strange in this PR. It could be helpful to add a comment about when the data is cached and for how long.. GPL is scary. :(\nFrom jquery website:  https://github.com/jquery/jquery/blob/master/LICENSE.txt\nIt seems it is MIT only now?. kk. thx.. getBoltsList() returns logical components or physical instances? I thought it is instances then we need to dedup here.. kk. thx.. \"bazel build --config=darwin heron/...\" seems to be working ok for me locally. Doing another clean rebuild now.. Builds ok.\nINFO: Elapsed time: 1086.264s, Critical Path: 195.24s\nINFO: 2515 processes: 2202 local, 313 worker.\nINFO: Build completed successfully, 4264 total actions. hmm. updatePackingPlan() handles both container and parallelism changes, why we need a new function here?. suggest to follow the convention in this file, put \"then\" in the next line.. add a space between \".git\" and \"]\". Hmm. It seems this new regex doesn't match \"Python 3.6.4 :: Anaconda, Inc.\"?\n. or you just want to make sure the re doesn't match when the version string doesn't end with digits?. Yeah. The version extraction is quite limited to specific format. Maybe we can update the code totally in future.\nI tested with https://regex101.com/ and my local python and it doesn't match. I think \"([\\d.]+\\d)$'\" requires the last character to be digit in order to match.\n\n\n\nre.search('([\\d.]+\\d)$', \"Python 3.6.4\")\n<_sre.SRE_Match object at 0x1047ca030>\nre.search('([\\d.]+\\d)$', \"Python 3.6.4 :: Anaconda, Inc.\")\n. add a new variable and use it, to avoid calculating it twice. it will be easier to maintain.\n\n\n\nto_run =  min(count + int(test_args.max_thread_number), len(test_threads). count should use the to_run value. suggest to add a comment like:\nRun test in batches. should be \"count + i + 1\". i + 1 is not the right value for \"x of N tests\".. First of all, discussion is welcome in this PR (ok to move to mailing list as well). My thoughts are not firm yet but I do believe we need it.\nHere are my thoughts:\nStreamlet is not really the abstraction. My feeling is that Streamlet is good at the DAG layer but not flexible enough in the low level (operators). I would think it is like Scala vs Java(not the same, just some idea). Scala has the nice functional API but it is pretty useless in real life if procedural code is not allowed/supported.\nTwo reasons:\n1. Migration is one major reason. There are quite some existing topologies written in low level API (for heron and storm). Streamlet is only friendly to new users, existing code such as KafkaSpout (it is spout, but same issue) in storm and some ML bolts has to be rewritten to take the readability/maintainability advantages.\n2. Bolt/Spout are more flexible. They can do a lot more than a function provided by Streamlet API (initialization, config, checkpoint, etc). For examples, the stateful processing and component configs, they are not supported currently by Streamlet and if we add the features, it is likely user has to provide extra functions as parameters and the Streamlet API would became more and more complicated. Streamlet API will evolve but supporting Bolt/Spout could give us a lot room to design a clean API.. And with the new feature, it is still true that \"When using the high-level DSL, users don't need to worry about bolt/spout.\". The difference is that \"they can if they need to\".. Cool. Thanks for the idea! @srkukarni \nThat sounds good to me. Let me see what I can do. I think the brother PR for Spout (https://github.com/apache/incubator-heron/pull/3032) is closer to the solution you suggested.\n. genrule is still needed, but exports_files seems to be unnecessary now.. I think pip doesn't reinstall package if it is already installed.. Make sense. Will do.. Yao and I thought about a few options and perform is the best we got.\napply() indeed is better, however it is a special function in Java/Scala so it cant be used. applyOperator() is valid, but maybe the \"Operator\" part sounds a bit redundant?\nI am totally open for a better name. Let's see other thoughts.. Yeah. Grouping is definitely needed.\nIt will do in the next PR. After the grouping refactor PR is merged. https://github.com/apache/incubator-heron/pull/3040\n. Yesh. I think we should enforce all StreamletOperator to be IRichBolt. CustomOperator will be the base class for all user defined operators (non-windowed onces) hence they will be IRichBolt.\nIStreamletBasicOperator is not for CustomOperator. It is for reusing existing user Bolts (the ones extend from IBasicBolt instead of IRichBolt) only. That is why we have only IStreamletBasicOperator but no CustomBasicOperator.\n. Good catch. Will do.. Yeah. It is in another PR already. https://github.com/apache/incubator-heron/pull/3032 (may need refactor though). It is not an addition method users need to implement.\nThis class is for all \"new\" user defined streamlet operators and there is no Bolt concept in it.\nExisting Bolts (or write operators in low level way) is supported but it another different use case.\n```\nclass UserBoltOperator\n    extends UserBolt\n    implements ICustomOperator {\n}\nThen the bolt is converted to a streamlet operator and can be used in Streamlet API.\nsource\n   ....\n  .perform(new UserBoltOperator())\n   ....\n```. maybe put \"__server/\" into a variable and use it in metrics:\nconst sp_string SERVER_SCOPE = \"__server/\";\n....\nduring registration:\nmetrics_manager_client_->register_metric(\n    SERVER_SCOPE | METRIC_DATA_TUPLES_FROM_STMGRS,\n    tuples_from_stmgrs_metrics_);\nNot a blocker though.\n. Please add a comment about why we have to register these metrics one by one instead of registering the scope.. why use Preconditions? require() is not enough?\nWe are hoping to get rid of guava library since it is too big. :(. why \"stage\"? . True. Better to be more explicit. Will do.. Agreed. And there could be some bugs in the API since so far it seems people are testing it but I haven't heard anyone using it in prod yet.\nLet me prepare another PR to at least add the two Scala tests to java. https://github.com/apache/incubator-heron/issues/3101\nAnd we definitely need better IT coverage for Streamlet API.. why equalsIgnoreCase instead of equals?\nThis seems to change the original behavior?. good catch. will do.. True.. ConcurrentHashMap might not be necessary in getValueAndReset(). This function is called every X seconds to sample the current metrics and export by Heron instance, so it is not called from multiple threads.. That is a good question. Some other operators might need it as well.\nAs @jerrypeng said before, the grouping strategy for most (even though I am not sure about its most or some) operations are fixed, like .map doesn't need grouping in general, reduce needs a field grouping or custom grouping. Having a streamlet level setGrouper() could be convenient but it makes it less straightforward and cause confusion (which is why I am hesitate about the Builder API).\nI am totally open to feedbacks and other options. Please feel free to discuss.\n. Yeah. Document is important.\nPlanning to update/add a few new operators. Will update doc afterwards.. Good to know. Thx. Will update.. ConcurrentHashMap  is not needed here I think. This is a get function and it is already synchronized.. ConcurrentHashMap  is not needed here I think. This is a get function and it is already synchronized.. HashMap should be good here I think.. HashMap should be good here I think.. Yeah. A component can generates multiple different streams (such as 3 integer streams: odd, even, all) and the next component needs to specify which one to accept data.\nUI doesn't show this information currently and it could be a useful feature to add. And grouping strategy is another important information on the edges.. Yep.. Yeah. I am not adding it now because no Streamlet operator generates multiple streams now. :(  I will add it when adding the split() function I think.. Agreed. Failfast is desirable for sure.\nValidation is an option question for now. Currently all streamlet operators receive and emit tuples to default stream.\nThe stream ids are in run time information now. So we need to be careful about where the validation should be done and how to apply it.. In fact, I am thinking of adding the split() operator now with this PR so that we can add a proper IT for it. I prefer to break PR down to single functionality in general but in this case it could be useful.. Did a quick search, I think it is pretty close to the split operator in Flink (https://ci.apache.org/projects/flink/flink-docs-release-1.6/dev/stream/operators/).\nIn fact, select() seems to be a better name than \"withStream\". :D I will think about it.\nNeng gives a feedback about the withStream() function as a new user and he made a good point that we need to handle both cases:\n1.\n  stream.map();\n  stream.withStream(\"a\").map();\n  stream.map();   // this should be the default stream\n  stream.withStream(\"a\").map();\n2.\n  steam2 = stream.withString(\"a\");\n  stream2.map();   // this should be the new stream\n  stream.map();   // this should be the default stream\nAnd this is not supported with the current implementation since the this object is muted. I am going to see what is the best way and update this PR.\n. I am thinking in the prepare() function, instead of setting collector direction, it might be better to call super.prepare(...) instead. What do you think?. \"Map>\"  won't really work because right now one Streamlet is mapped to one operator which is a Bolt component. In our case, there is only one component (hence one streamlet object) with multiple streams.\nI had to add a StreamletShadow class to create companion objects of the main Streamlet object in my implementation.. In fact, it should work after the new StreamletShadow class is introduced. Let me think about it.. In fact, still no....\nThe problem is the .setName and .setPartition functions, like:\nnumberStreamlets.split(splitter).setName(\"split\").setPartition(10);\nthe split operator is one single bolt and all entries in Map points to the same bolt. It might be ok to set name and partition like:\nMap splittedStreamlets  = numberStreamlets.split(splitter);\nsplittedStreamlets.get(\"even\").setName(\"split\")\nsplittedStreamlets.get(\"even\").setPartition(partition)\nbut it looks really bad this way.\n. Heron has a check already and it runs during submission time. Not as good as compile time but should be fast enough. :). done. Done.. In that case, the default streamid is used. An error will be thrown during submission because of stream not available.\nThe error happens during submission, let me think what is the best way to cover it in UT.. Suggest to make the same change in SendResponse() and send SendMessage() in this file.. I tried and it doesn't work. Let me try again.\n. will do. thx.. windowCfg doesn't have the function and requires a static cast. :(. \nexamples/src/java/org/apache/heron/examples/streamlet/ImpressionsAndClicksTopology.java:177: error: reference to reduceByKeyAndWindow is ambiguous\n        .reduceByKeyAndWindow(\n        ^\n  both method reduceByKeyAndWindow(SerializableFunction,SerializableFunction,WindowConfig,SerializableBinaryOperator) in Streamlet a\nnd method reduceByKeyAndWindow(SerializableFunction,WindowConfig,T,SerializableBiFunction) in Streamlet match\n  where K#1,V,R,K#2,T are type-variables:\n    K#1 extends Object declared in method reduceByKeyAndWindow(SerializableFunction,SerializableFunction,WindowConfig,SerializableBinar\nyOperator)\n    V extends Object declared in method reduceByKeyAndWindow(SerializableFunction,SerializableFunction,WindowConfig,SerializableBinaryOperator)\n    R extends Object declared in interface Streamlet\n    K#2 extends Object declared in method reduceByKeyAndWindow(SerializableFunction,WindowConfig,T,SerializableBiFunction)\n    T extends Object declared in method reduceByKeyAndWindow(SerializableFunction,WindowConfig,T,SerializableBiFunction). Cleaned up a bit more. Moved the call into Operator classes so we dont need to worry about the details here.\n. Yeah. Thanks for figuring out the cause of the compiling error when the attachWindowConfig() is in the interface.\nLet me update it.. That is what I thought. :D \nBut it turns out that FullSpoutMetrics is a re-implementation (focusing on accuracy and extra information) of SpoutMetrics and they are in parallel instead of parent-child.. There is no inheritance between them.. Yeah. The function simply converts a data into a stream of k-v pair. This is a fairly standard function in other frameworks:\nKeyBy() in Flink: https://ci.apache.org/projects/flink/flink-docs-stable/dev/api_concepts.html,\nGroupByKey() in kafkastreams: https://kafka.apache.org/10/documentation/streams/developer-guide/dsl-api.html\nGroupByKey() in Beam: https://beam.apache.org/releases/javadoc/2.0.0/org/apache/beam/sdk/transforms/GroupByKey.html\ngroupBy() in Summingbird:\ngroupBy() in Storm: http://storm.apache.org/releases/1.0.6/Trident-API-Overview.html\nHowever, the difference might be that in those frameworks, operations on keyed stream cannot be applied on non-keyed stream so keyBy is necessary and currently in streamlet, you \"have to\" provide key extractor to all keyed operation. It is flexible but could be painful to users.\nCurrently a custom field grouping is applied. However I am thinking that maybe it is not needed? The grouping should be applied afterwards.\nI think the use case is to formalize a stream with complicated data structure to make it easier to use in the next steps. It could be convenient especially when there are multiple keyed operations. For example, with the function we can do:\nkeyedStream = source   // Streamlet\n  .keyBy(keyextractor, valueextractor)\nkeyedStream.reduceByKey(reduceFunc1)....\nkeyedStream.reduceByKey(reduceFunc2)....\nkeyedStream.reduceByKeyAndWindow(windowConf, reduceFunc3)....\nCurrently it looks like:\nsource.reduceByKey(keyextractor, valueextractor, reduceFunc1)....\nsource.reduceByKey(keyextractor, valueextractor, reduceFunc2)....\nsource.reduceByKey(keyextractor, valueextractor, windowConf, reduceFunc3)....\n. \"KB * 1024\" might be better.. Ah. Right.\nIt is in examples/ and I didn't catch it.. Yeah. We don't have this data I think.. \"!(other instanceof CPUShare))\" might be safer.. \"share\" or \"value\" might be better than \"cpuShare\" I feel. \"cpuShare\" feels more redundant. Not a hard request though.. Can we move the implementation code into ResourceMeasure and declare CPUShare as:\npublic final class CPUShare extends  ResourceMeasure< Double > {\n}\nAnd ByteAmount can be:\npublic final class ByteAmount extends  ResourceMeasure< Integer > {\n}\nThe util function below might be a lot simpler this way.. \"%.3f\" is more consistent to the toString function in CPUShare. Or maybe wrap it into CPUShare and call the toString() function like the containerRamHint var.. There might be more checks in the function, like CPU and disk and maybe others. I feel validatePackingPlan() makes more sense here.. cool. thx.. null check is still needed.. can this function be implemented in base class?. this change seems to be unnecessary?. this indent change is needed?. Can we add these cases:\n- container cpu is more than needed by instances.\n- container cpu is less than needed by instances.\nalso, other processes in container could need a fixed amount of CPU as well. So if container cpu == needed by instances, it might be better to return false.\n. kk. sgtm.. The code below prints Yes.\n Thread x = null;\n  if (x instanceof Thread) {\n    System.out.println(\"Yes!\");\n  }\n\nSo when other is null, \"if (!(other instanceof ByteAmount))\" returns false, and .value will be called later.. containerCpuHint (configured container cpu, or max (padding + 1 per instance)) is not used. As the result, I think user specify container cpu is ignored. Maybe need a max(containerCpuHint, containerCpu) I feel?. That is true. Assuming user configures:\n- container cpu to be 5\n- 1 cpu per instance\n- 3 instances in container\nThen:\ncpu hint is 5,\ninstance cpu is valid (enough for padding and instances)\nIn the old calculation, container cpu is 5 because user configured it.\nIn the new calculation, container cpu is 4 (1 * 3 + 1 padding).\nWith \"max(containerCpuHint, containerCpu)\", container cpu is the same as before.\nI think it is important to make the result consistent. Some users might rely on setting container cpu to allocate more processing power to their instances instead of configuring per instance cpu (although it is technically correct).\n. But if container cpu is not configured, the getContainerCpuHint() would return padding + 1 * instance number, which is smaller than your new value, unless you update the getContainerCpuHint() function.. The original logic is messy and makes it hard to update. :(. select all text on click. We dont have an instruction page for heorn UI I think currently. Could be useful to have one though.. this is a fix of some issue?. KK.. In status.sh, version string is decided in two steps:\n1. if [ -d .git ];, find the branch name and use it as version string,\n2. otherwise, use the current directory name as version string.\nIn docker script, .git file is not available because tarball doesn't include this hidden file. This directory name is used as version number.. is it possible to avoid platform specific key and value and use a general config here? SPI is an interface for all platforms.. what is this for?. currently, executor uses the list to start instances, I think. If we skip here, where is this function called again to get the list and start the instances?. kk. thx.. should \"&& rm -f /heron/heron-install.sh \\\" be added as well?. Heron API is included in this build which has this shade. I think it is safer to apply the same process.\nI think the shade is used to avoid conflicts if different version of the libraries are used in the user topology.\n. It is the version used in the project. I am ok to update the whole project but here it should be consistent to other BUILD files I think.. seems like bookieJournalHostPath is not used any more here. Is it used in any other places ?. cool. thanks! I will include this change in 0.20.1. this block can be refactored and combined with the \"if (!containerResHint.equals(notSpecified) && usedRes.greaterThan(containerResHint))\" above.. - should be \"components\"\n- maybe better to document why Ram only.. Good question.\nThe default value is not really used (and not really needed) because low level api has its own default values. Let me clean it up.. This is to avoid duplicated code. . ah. you are right. pasted a bit more than needed. good catch.. good catch.. done. move up or down so that padding percentage is close than the other padding configs.. which padding algo is using this config?. 4 might be too small. 10 might be better?. so if the size == maxNum, exception is thrown?. what does it mean?. maybe make it a PackingException?. can we use lessThan instead of compare() < 0?. should ConstraintViolationException be a PackingException?. kk. thx. is there a way to reduce with EMPTY_SOURCE as initial value? might be cleaner that way.. could this happen? should we maybe shrink numContainers to the totalInstances?. same as above. Maybe refactor the check into a function if necessary.. this logic seems to be used in a few places. maybe refactor into a static function?. necessary to be public?. same as #3187, could it be helpful to refactor this logic into a function? since the logic is used in a few places.. same as #3187, is \"public\" necessary here?. the stream() is needed here?. correct in most cases. One exception is that count.incr() and latency().update() are not atomic, so in theory they might be separated into two intervals. This\nThe more straightforward motivation is more like the FullBoltMetrics class has the metric but the others dont. This PR makes them more consistent.. I see. Thx.. Suggest to add more comments about the class like how to use the class.. Suggest to add a blank line above but not a blocker.. Maybe \"Default instance resource, CPU: %f, RAM: %s, DSIK: %s\". Maybe \"Container resource, CPU: %f, RAM: %s, DSIK: %s\". suggest to create a util function in AbstractPacking(). Add comments for this function.\nparallelismMap is used to get component list? Suggest to use the list instead to be more explicit.. suggest to add a blank line above the return. comments. A and B may not be good Spout and Bolt names. yeah. that is right.. sort. break into lines if possible.. Maybe error()?. I am feeling that it is fine to keep the old name, since it is BinPacking anyway. Also if any user was using BinPacking before, renaming would require them to update their config which is not convenient. The new code works very similar to the old logic anyway.\nJust my 2 cents.. Some comments might be helpful here. I can imaging me trying to delete the second \"numContainers = 0;\" since it looks like a bug.. can we avoid this numContainers?. I think more detailed rules could be helpful here.. I think we need a few unit tests for this function.. Maybe a TODO, we can have an interface like PackingPlanEvaluationRule {} and refactor this code with a list of rules.. Could we make this function a static function? I am feeling that we might move this function to a util or some other class in future.. componentsToScaleDown keeps negative counts? It feels a bit intuitive this way to me.. SortingStrategy is a bit strange to be here I feel. Does it make more sense to refactor it out?. Suggest to move out to its own file.\nAlso, why disk is not included?. hmm. So if ramUtil is greater and cpuUtil is less, the function would return 0? Seems strange. Maybe give ram and cpu different priorities. Like only compare cpu utils when the ram utils equal.. And please add unit tests.. I am wondering if there is a better place for this config. @huijunw @nlu90 any thoughts?\nAlso if it will be kept here. I would suggest make the name and string like topology.packing.ffd..... maybe rename to \"validate\"?. tmaster is native code as well.. Suggest to break this change into a different PR.\nAlso I am wondering if it is better to move the evsignal_new() to caller in case multiple event loop objects are created in an application.. so when cmd is a dict and it doesn't have 'env', this branch is called? Maybe it might be cleaner to construct a normalized_cmd object that contains 'cmd' and 'env' and then call popen to execute it?. revert this file.. might include the information that this is a data class and it contains:\n- cmd is a list of strings\n- env is a list of environment variables\n. Is this required by new bazel? If not, maybe better to be in a separated PR I think.. Got it. Thanks.. ",
    "jingwei": "shipit\n. Fixed\n. I think we should leave these conf file here even they are twitter specific. Combined with example/vagrant/aurora, these config files will help people understand how to configure heron on aurora properly.\n. I fixed integration test against our original twitter branch. We will need to fix integration test against the github version\n. We have config-path and config-loader which are not shown here.\n. heron-conf is for scheduler launch configuration. better to separate it from system build configuration.\n. fixed\n. added some comment\n. config-path is more general allowing specific scheduler to interpret accordingly.\n. Consider using constants for \"__system\" and \"__tick\" and introducing HeronConstants.java\n. can callback be null? Could we test callback and then call send with/without callback here?\n. \"value\" may be a better name than \"message\" as it matches generic type V.\n. It is better to throw exception \"Field not found: x\" than return null.\n. Consider renaming this to KeyValueSchemeKafkaMapper since it holds an assumption the tuple is constructed using KeyValueScheme\n. Pleas add brief comment on zkStr and zkPath and give examples such as localhost:2181 and /kafka for clarity.\n. rename to topicPartitionsPath?\n. A few other places are referring to kafka zookeeper configuration paths, should we can consider adding a class KafkaZookeeperConfigs which provides various path accessor methods such as topicsPath, topicPartitionsPath, brokerPath etc.\n. Missing one a generic type here\n. consider moving this class to contrib/kafka9/heron-kafka/src/java/com/twitter/heron/util? It is common enough for other packages to use\n. No need to extend StringScheme?\n. string to bytes?\n. ",
    "prabhuinbarajan": "added pull request https://github.com/twitter/heron/pull/1119\n. git clone \ngit checkout -b prabhu/heron\n./bazel-configure.py\ncd scripts\n./setup-intellij.sh\n. addressed the bazel build and other stylistic issues. also added support for python module in the intellij workspace.  I think i might have messed up the whitespaces with the latest commit so , the diff doesnt appear to be clean. Please advice.\n. sorry, i didnt realize that some of the changes were commited earlier.\nI will fork from master again and include just my changes.\nstarted working on the kubernetes scheduler in my earlier branch. I will\nmove that out as well.\nThanks\nprabhu\nOn Tue, Apr 5, 2016 at 9:59 AM, Karthik Ramasamy notifications@github.com\nwrote:\n\nFiles changes include some of the commits that we did. I would suggest do\nthe following -\ngit clone \ngit checkout -b prabhu/intellj\n\ngit commit -am \ngit push origin prabhu/intellij\nOnce the branch is pushed, please create a pull request. We are verifying\nit and soon you will get som e feedback.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/heron/pull/254#issuecomment-205895059\n. Karthik:\nas I am loading the workspace, i see a few conflicts that otherwise doesnt\nshowup in compile;\n\nfor ex: there are two AuroraLauncher (com.twitter.heron.scheduler.aurora)\none in\nheron-master/heron/scheduler/src/java/com/twitter/heron/scheduler/aurora\nand other in\nheron-master/heron/schedulers\n/src/java/com/twitter/heron/scheduler/aurora/AuroraLauncher.java\none of the AuroraLaunchers refers to packages that dont seem to be in any\nworkspace. Any clues?\nimport com.twitter.heron.scheduler.util.NetworkUtility;\nimport com.twitter.heron.scheduler.util.ShellUtility;\nimport com.twitter.heron.scheduler.util.TopologyUtility;\nOn Tue, Apr 5, 2016 at 10:21 AM, Prabhu Inbarajan \ninbarajan.prabhu@gmail.com wrote:\n\nsorry, i didnt realize that some of the changes were commited earlier.\nI will fork from master again and include just my changes.\nstarted working on the kubernetes scheduler in my earlier branch. I will\nmove that out as well.\nThanks\nprabhu\nOn Tue, Apr 5, 2016 at 9:59 AM, Karthik Ramasamy <notifications@github.com\n\nwrote:\nFiles changes include some of the commits that we did. I would suggest do\nthe following -\ngit clone \ngit checkout -b prabhu/intellj\n\ngit commit -am \ngit push origin prabhu/intellij\nOnce the branch is pushed, please create a pull request. We are verifying\nit and soon you will get som e feedback.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/heron/pull/254#issuecomment-205895059\n. Also do you have slack or some other channel where the communication can be\nreal time?\n\n\nOn Tue, Apr 5, 2016 at 4:26 PM, Prabhu Inbarajan <inbarajan.prabhu@gmail.com\n\nwrote:\nKarthik:\nas I am loading the workspace, i see a few conflicts that otherwise doesnt\nshowup in compile;\nfor ex: there are two AuroraLauncher (com.twitter.heron.scheduler.aurora)\none in\nheron-master/heron/scheduler/src/java/com/twitter/heron/scheduler/aurora\nand other in\nheron-master/heron/schedulers\n/src/java/com/twitter/heron/scheduler/aurora/AuroraLauncher.java\none of the AuroraLaunchers refers to packages that dont seem to be in any\nworkspace. Any clues?\nimport com.twitter.heron.scheduler.util.NetworkUtility;\nimport com.twitter.heron.scheduler.util.ShellUtility;\nimport com.twitter.heron.scheduler.util.TopologyUtility;\nOn Tue, Apr 5, 2016 at 10:21 AM, Prabhu Inbarajan \ninbarajan.prabhu@gmail.com wrote:\n\nsorry, i didnt realize that some of the changes were commited earlier.\nI will fork from master again and include just my changes.\nstarted working on the kubernetes scheduler in my earlier branch. I will\nmove that out as well.\nThanks\nprabhu\nOn Tue, Apr 5, 2016 at 9:59 AM, Karthik Ramasamy \nnotifications@github.com wrote:\n\nFiles changes include some of the commits that we did. I would suggest\ndo the following -\ngit clone \ngit checkout -b prabhu/intellj\n\ngit commit -am \ngit push origin prabhu/intellij\nOnce the branch is pushed, please create a pull request. We are\nverifying it and soon you will get som e feedback.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/heron/pull/254#issuecomment-205895059\n. Bill,\nA lot of this was baselined from the bazel setup intellij reference script which helps setup the intellij workspace for the bazel source code to help get quickly running.  This initially seemed to be of purely utility value .\nI agree with these comments,  and can make changes based on how strongly we feel about them.\n\n\n\n@ajorgensen - will include the cleanup and include third party folder as a module.\n@billonahill  - will research on ipr structure , and get that included.\n@kramasamy  - the verification instructions are quite manual at this point.\ngit clone \ncd scripts\n./setup-intellij.sh\nopen up the heron folder in intellij and it would open two modules (python , and java). \nThanks\nprabhu\n. i like the functional modules. I just dont feel competent enough to comment on the domain specific organization as I am not deeply familiar with a heron codebase. Deferring to the group for advice. \nMy only comment on such an organization is it would be preferable  if the script can discover this. we may both agree that expressly listing each of these (like core, cli etc) may not be completely optimal.\n. one other consideration - intellij does require you to group them under modules that come with specific capabilities . if you look at the IML today , each module has to be declared as java or python or one listed below.\nhttps://www.jetbrains.com/help/idea/2016.1/supported-module-types.html?origin=old_help\n. @kramasamy  , @billonahill  - spent a good part of yesterday and today determining the feasibility of functional modules. it looks promising , but i am running into a few glitches that i am hoping you could help.\ngiven the shared nature of dependencies - it seems the separation is not clear. You mentioned it already. i am just catching up to your comments friday. \ndef heron_core_lib_statemgr_files():\n    return [\n        \"//heron/statemgrs/src/java:heron-localfs-statemgr\",\n        \"//heron/statemgrs/src/java:heron-zookeeper-statemgr\",\n    ]\nand \ndef heron_client_lib_uploader_files():\n    return [\n        \"//heron/uploaders/src/java:heron-null-uploader\",\n        \"//heron/uploaders/src/java:heron-localfs-uploader\",\n        \"//heron/uploaders/src/java:heron-hdfs-uploader\",\n    ], \nHaving said that , the proposal is as follows:\nif we are comfortable with the groups under bazel-bin/heron:\nls bazel-bin/heron:\napi  cli  cli2  common  controller  examples  executor  instance  localmode  metricsmgr  newscheduler  packing  proto  schedulers  shell  spi  statemgrs  stmgr  storm  tmaster  tracker  ui  uploaders\nthen this is pretty much what we currently have under MODULE heron-java/heron. I may have found a way of enabling a single module to be poly-glot and support python , java and C++.\nwith this , there will be a single module  - heron , with multiple content roots (based on bazel-bin/heron) + (3rd party). Please advice.\nThanks\nprabhu\n. thanks, i will proceed down this path.\n. @billonahill  - pushed the latest changes this morning in line with the above proposal. now we have two content roots inside the heron module. one for third party and another for heron itself . Please review.\n\n. i just finished the merge with master.  i dont see the travis build kicking in - anything needs to be done?\nalso @billonahill  - can you tell me what you had to reconfigure so that i can add that as part of the script? itI am curious because no manual steps should be necessary.\n. okay, finally figured out the library setting that's been bugging me for days. this one should be completely hands off. the latest commits include the following:\n1. library settings that include heron-proto and thrift in the module libraries - so we should really have a clean workspace with zero errors\n2. Project SDK inclusion \n3. src folder exclusion if there is a xxBUILD file. if we need to re-enable , this requires to run setup intellij\nsteps to verify - \ncd heron-root\nrm -rf .idea *.iml\ncd scripts\n./setup-intellij.sh\ncd ..\nopen intellij project in idea\nproject -> Make\nthere should be zero compilation errors - see screenshot below\n\n. @billonahill  - i tested this before committing. here is what i see. could you provide a screenshot of your workspace so that I can troubleshoot? @kramasamy  - could we have someone else try so that we get one more data point:\nedit existing file in 3rd party:\n\nadd new file in 3rd party:\n\n\n. @ashvina  - i will submit a separate PR for ignoring compilation errors. \n@billonahill  - i had earlier thought you were trying to add files within 3rdparty. now its clear.\nwhat other directories need to be part of content root (i dont know if it should be everything under heron?)\n. sure...thanks, please await a PR.\n. @maosongfu  , @billonahill  - with respect to this issue - need your advice on choosing between the two options:\noption 1:  organize the workspace in the following way:  in this approach, we define the module at HERON_ROOT, and we define 3 content roots:\n1. Heron root itself\n2. Heron Java (heron/heron)\n3. Heron 3rd party\nthe benefits are that you dont have to whitelist every folder and file under heron root\n\npro's , all files under heron workspace is managed and no out of band process required for top level additions\ncons : the file naming structure is not logical today.. heron root is named heron. heron java is named heron, resulting in a lot of confusion \nOption 2:\nexplicitly include every folder under heron root as a content root. the benefit is a more natural organization, where everything appears to be logically below the heron root . however , in order to do this , every folder needs to be explicitly listed as a content root in the IML., which means addition of a new folder at the top level requires a run of setup intellij. \n\nnot sure if I articulated this well - so please let know if the options are clear . if they are , please advice.\nThanks\nprabhu\n. Bill, sounds good, i will include them\nOn Tue, Apr 19, 2016 at 4:03 PM, Bill Graham notifications@github.com\nwrote:\n\n@prabhuinbarajan https://github.com/prabhuinbarajan in PR #404\nhttps://github.com/twitter/heron/pull/404 I'm adding support for\ncheckstyles and copyright headers, which we should also include in the\nIntelliJ setup.\nFor the styles you can follow the instructions in\nhttps://github.com/twitter/heron/pull/404/files#diff-e3ab259ab2bf07b0feecc1e8eda32bec\nto include tools/java/src/com/twitter/bazel/checkstyle/HeronIDEA.xml in\nthe project and see how that should appear in the generated project files.\nFor the latter we should be able to just include the following files,\nwhich I've included below:\n.idea/copyright/\n\u251c\u2500\u2500 Heron.xml\n\u2514\u2500\u2500 profiles_settings.xml\nHeron.xml:\n\n\n\n\n\n\nprofile_settings.xml:\n\n\n\n\n\n\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/twitter/heron/issues/389#issuecomment-212161468\n. submitted #442 \n. @billonahill  - after some research , i think it would be best to generate a copy of checkstyle file under scripts/resources/idea/codeStyleSettings.xml\n\nit has the same format except that it has wrapper project/component elements. this will avoid extra steps in setting up the checkstyle and comes enabled when setup intellij is run. checkout the attached.\ncodeStyleSettings.txt\nI have added the support for this in my branch, but my pull request seems to be going towards #442 . once that is merged, i should be able to submit this.\nplease checkout my branch\nhttps://github.com/twitter/heron/compare/master...prabhuinbarajan:prabhu/intellij-v2?expand=1\n. @billonahill  - I imported HeronIDEA.xml into scripts/resources/idea/codeStyleSettings.xml once (the format is slightly different and hence cant move). I will delete checkstyle/README.md.\n. done.\n. fixed all pylint comments\n. @objmagic  - all your code review comments + pylint feedback has been implemented.\n@kramasamy  - pls review\n. @objmagic  - .heronrc is not mandatory. it falls back to the current parser.\nadded test case :   test_parser_norcfile  in argparser_unittest.py to assert this.\nplease verify\n. all code review comments and style checks  addressed \n. would the current contents of README.md under heron/cli/src/python/ suffice for documentation?\n. @objmagic  - per our conversation this afternoon, I added an additional test case for configuring  the equivalent of \nheron submit local --verbose  ~/.heron/examples/heron-examples.jar  com.twitter.heron.examples.ExclamationTopology ExclamationTopology\nin heronrc.test as:\nheron:submit:local --verbose  --topology-file-name ~/.heron/examples/heron-examples.jar  --topology-class-name com.twitter.heron.examples.ExclamationTopology --topology-name ExclamationTopology\nalso added a baseline test and a comparison test :\ntest_parser_commandline_norc  (using python's default argparse)\ntest_parser_commandline_positional_withrc (using argparser )\nPlease let me know if this meets your expecations. also added some house keeping code for testability in HeronArgumentParser\n. sounds good to me... one call out on heronrc - when the same argument is provided at multiple levels, this was supposed to clobber the argument list and enforce the precedence. Now we should find another way to support that or have a convention in place where same argument is not supplied at multiple levels .\n. @tnachen -  i agree. the kubectl reference was more an illustration for the reader on what should be accomplished. there were other options that were suggested. But reading the doc again, it seems that point isnt well communicated. i will reword it the design and stories...\nI agree that if there is a native client library for kubernetes, that is what we should use. i will review the spark k8s client library -can you provide some reference to it so that i can lookup?\n. Karthik, thanks for noting that. removed it now.\n. addressed.\n. it would probably just another iteration. the template itself wouldnt have much content , because much of the module and dependencies are generated based on the content in the script. \nthe script would have to be rewritten to leverage xml best practices - at which point, i dont know if shell / bash would be an optimal choice (something like python might be more efficient).  we are talking about a couple of days of effort.\n. Do you mean, rm -rf .idea/ or just the iml? in the following lines, I end up overwriting a copy from scripts/resources/idea/* . However, any residual changes may stay there. let me know your preference\n. nope, that is gone now.\n. I think it is a remnant of bazel intellij generation script. it is gone now. added script/resources/idea/compiler.xml (and cleanedup the setup intellij script)\n. Yes, these are the HERON_RC defaults\n. complete\n. complete\n. complete\n. complete and added a warning message as well\n. complete\n. pylint is complaining that it needs one space around assignment. Please advise.\nI think you are referring to lines 43 - 49 which is addressed.\npylint --rcfile=tools/python/checkstyle.ini heron/cli/tests/python/argparser_unittest.py\n. changed to heronrc.sample\nthe argparser_unittest.py also changed to heronrc.test. \n. line 63 - 70 addressed\n.   addressed as dict_ns = namespace.dict\np.s = dict(namespace) is not working as namespace is not iterable. only namespace.dict is\n. addressed as \nif not  command or not app or not env:\n            Log.warn(\"heronrc config entry %s does not have key parameters (command:app:env) \",\n                     line)\n            continue\n. addressed as \nif not  command or not app or not env:\n            Log.warn(\"heronrc config entry %s does not have key parameters (command:app:env) \",\n                     line)\n            continue\n. addressed as \nif not  command or not app or not env:\n            Log.warn(\"heronrc config entry %s does not have key parameters (command:app:env) \",\n                     line)\n            continue\n. good point, i will combine : HERON_RC_FILE = \"~/.heronrc\"\nHERON_RC = os.path.expanduser(HERON_RC_FILE)\nIn addition, you may want to note a couple of things:\n1. there was supposed to be a system level heron rc (under /etc/)\n2. Karthik mentioned that there may be an implicit hierarchy so that at a system level , there could be heron options, and at a user level , all future additions\n3. the arg parser currently can also be initialized with an RC file , so the one under home is the default.\n. done\n. @objmagic  - HeronRCArgumentParser extends Argparser, and hence the behavior will be defaulting to normal argparser behavior without the file. is that what you mean? it will be transparent to the clients\n. I am wondering if we should also name the class to be more generic , say HeronArgumentParser instead of making it specific to HeronRC. this also provides the fallback behavior in case the RC file is not found. \n. this is the main stub. meant to print it to stdout. info seems okay to me. let me know if you feel strongly\n. this is not a fatal condition. this means a misconfiguration in heronrc file and hence the warning. \n. done\n. done, changed argparser.py to heronparser.py , \nchanged HeronRCArgumentParser to HeronArgumentParser\n. ",
    "windie": "@kramasamy - Looks like @caofangkun is working on this.\n. > Can you sign the CLA?\nSigned.\n. Updated. PTAL.\n. @kramasamy - I just found the package name doesn't match the file path. However, I don't understand the hack totally. @nlu90 could you take a look since you added this file?\n. @kramasamy - Passed locally: Executed 114 out of 114 tests: 114 tests pass. Seems //heron/instance/tests/java:SpoutInstanceTest is flaky. Could you rerun the Travis CI test, please?\n. > @windie - does this cover all the java tests?\nNo. I only updated heron/common, heron/instance, heron/metricsmgr and heron/packing. Should I update this PR for other modules or send a separate one after this one gets merged?\n. @kramasamy - Updated\n. @kramasamy - Updated.\n. Yes, this one works with both Bazel 0.2.3 and 0.3.0.\nI was working on #524 and found homebrew had upgraded to Bazel 0.3.0. So I submitted this PR to unblock #524.\n. @kramasamy - E.g., the following command fails\n$ export CFLAGS=-I /usr/local/include/\n-bash: export: `/usr/local/include/': not a valid identifier\nI found this one when building heron in homebrew environment which sets CFLAGS and other similar environment variables.\nIn addition, right now I could not build heron using homebrew's bazel. Bazel just crashes with a useless error info: ERROR: Process exited with status 1.. Still investigating the cause.\n. @kramasamy - homebrew-core requires Heron built from source. Otherwise, it needs to go into homebrew-cask. However, homebrew-cask only supports downloading a compressed package instead of an install binary like Heron does. It requires Heron releases a bundled tar that contains all install scripts.\nI was thinking to submit a PR to https://github.com/Homebrew/homebrew-binary but it's just deprecated recently :(.\n. @kramasamy - I think you meant Window? Sure. I will take a look.\n. @kramasamy - right now most of codes are from Strom's master branch. I'm not sure if this is the right base. Or should I use some stable version, such as v1.0.2 or v1.1.0? In addition, is there anything I can do to make this one easy to review?\n. @kramasamy sounds great. Will update my PR to base on v1.0.1.\n. @kramasamy - Thanks! My email is **\n. @kramasamy \uff0d Gotcha.\n. The new files are copied from Storm. I created a diff between them to show the differences: https://gist.github.com/windie/28d84a7b00a587cd242ba55681df56dd\n. @kramasamy \uff0d updated. Please take a look.\n. I fixed the failure test in #1263\n. Thanks for reviewing. I will add examples, docs and unit tests at the weekend.\n. Sorry for the delay. Just added examples and unit tests to this PR.\nRight now this PR supports Storm's IWindowedBolt and Watermark but not IStatefulWindowedBolt.\n- IWindowedBolt example: com.twitter.heron.examples.SlidingWindowTopology.\n- Watermark example: com.twitter.heron.examples.SlidingTupleTsTopology.\n. > [2016-10-07 20:19:04 +0800] com.twitter.heron.instance.HeronInstance ERROR:  Exception caught in thread: SlaveThread with id: 13 \n\njava.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Number\n\n@mycFelix fixed this error in my PR. You can just run com.twitter.heron.examples.SlidingWindowTopology and com.twitter.heron.examples.SlidingTupleTsTopology in this PR.\n. What's the convention for codes copied from Storm? Most of codes are changed because of the style errors and different log codes, it's impossible to apply any patches from storm repo directly.\n. How about just excluding style checking for //heron/storm? \n. Updated.\n. What's the contract for conf? I saw most of places don't check the existence. E.g.,  https://github.com/twitter/heron/blob/master/heron/common/src/java/com/twitter/heron/common/config/SystemConfig.java#L468\n. I'm not sure if this is the correct way. \nRight now Heron doesn't have the Grouping class, so I created the method to get all source streams. It's used by WindowedBoltExecutor.\nIn Storm, WindowedBoltExecutor uses getThisSources (The above method commented out)\n. @billonahill The time complexity of my current approach is just O(# existing containers) + O(# the added containers). But if using processToContainer.values().contains(container.getId()), the time complexity will become O(# existing containers) * O(# the added containers). \n. @billonahill Updated.\n. @billonahill values is not a set. Did you mean keySet?\n. Sorry that I was not clear. I meant the contains method is O(# existing containers).\n. > since we're logging this and moving on without throwing, we should change this to Level.WARN actually.\nFixed.\n. This old test actually didn't test the behavior. sleep 1 && echo ... will be split to sleep, 1, &&, echo, ..., and the code will launch the process sleep with the other strings as its arguments. Then sleep will fail at once because of invalid arguments. Since redirectErrorStream didn't work before, the error info will go into stderr. So p.getInputStream().available() will always be 0.\n. ",
    "bclicn": "Could you also release installers for CentOS6? Since most of our servers are running on CentOS 6.5.\nThank you!\n. ",
    "osgigeek": "One minor comment + don't see any unit-tests but otherwise LGTM.\n. @ashvina adding my input but open to hearing what others think. \nI was also pondering the same however for multiple resources it felt like the construct would lead to compromising code readability. I lean towards code readability over the convenience of AutoCloseable resources.\n. @maosongfu thanks for considering the logging improvements. Added one comment, not necessarily related to this change just for clarification. But this looks good.\n. Looks good\n. Looks good \n. LGTM\n. Suggest adding it at the same level as \"Heron Architecture\" under \"Heron Concepts\" and maybe naming the page \"Heron Design Principles\". Consider adding \n- Observability\n- Failover - explaining no single point of failure\n- Recoverability - recovery of state of bolts\n. Would it make sense to keep the command enum value separated from the endpoint? \n. Should this be in a finally block? If any of the calls fail, does the runtimeManager need to be closed?\n. A minor thing to note. Consider using the logger api\njava\nlog(Level level, String msg, Object[] params)\nthe advantage of using that API is you wont incur the overhead of string concatenations unless log level is enabled. Here you still do all the string cat work even if log levels are at ERROR.\nSo this will be \njava\nlog(Level.INFO, \"Topology: {0} to be {1}ed\", new Object[]{topologyName, command});\n. I noted this in the Runtime.schedulerShutdown which I believe calls Shutdown.\njava\ncom.twitter.heron.spi.utils.Shutdown\nIn the Shutdown class there are 2 objects and a boolean used (a) boolean (b) Lock and (c) Condition to await shutdown, would it be appropriate to just use a single object say a CountDownLatch instead? I am looking at\nMethod\njava\npublic void terminate() {\n    lock.lock();\n    terminated = true;\n    terminateCondition.signal();\n    lock.unlock();\n  }\nWith a CountdownLatch\n``` java\nprivate final CountDownLatch countDownToTermination = new CountDownLatch(1);\npublic void await() {\n    try {\n        countDownToTermination.await();\n    } catch (InterruptedException e) {\n      LOG.info(\"Process received interruption, terminating...\");\n    }\n  }\npublic void terminate() {\n   countDownToTermination.countDown();\n }\n```\nI am sure I am missing something so thought should ask.\n. Please consider using \njava\nLOG.log(Level.FINE, \"Reading config file {0}\", fileName);\nThis prevents any unnecessary string concatenation when the logger levels are higher than FINE\n. I know this is not your change but thought should point out...the fin needs to be closed.\n. @kramasamy its in an interface, by default all methods are public.\n. @maosongfu the javadoc for IUploader's method\njava\nObject uploadPackage();\ncalls out that it returns a URI, if that is the case then the return type should be URI or . \nConfig should ideally expect a URI so that it is clear as to what config value is expected to be. The issue with String is it leaves it vague as to what the format of the data in the string should be, IMO it is better to be explicit rather than leave it to the user to figure out.\nAlso, pardon my ignorance for asking what you mean when you say packer, it returns a triple. What is a triple? Did you mean tuple?\n. can we avoid all this duplication by just creating a private method which does these steps?\n. Please consider use Log.log(Level.INFO, \"Package URI: {0}\", resourceUrl), this avoids the string concatenation when the level is higher than INFO.\n. Or just have isProduction() return a java.lang.Boolean and call .toString() on the returned value here.\n. Rather than TopologyUtils why not name this TopologyLock. Its primary function seems to be to acquire and release lock.\n. Pardon my ignorance, but thinking out aloud how about having the submit API on the TopologyAPI.Topology it could also have lock and unlock methods. It might keep the functionality nicely encapsulated.\n. So I was not thinking of encapsulating it within the submit. My thought was having lock and unlock within the Topology as separate methods. I don't know if the lock unlock need to be public but ideally they are protected. Here is what I was thinking\n``` java\npublic final class TopologyAPI.Topology {\npublic .. submit(){\n     lock();\n     try{\n         .... //do the work\n     }finally{\n        unlock();\n     }\n   }\n//Maybe what you identified as kill could be destroy?\n   public .. destroy(){\n      lock();\n      try{\n        .... //do the work\n      }finally{\n         unlock();\n      }\n     unlock();\n   }\n//Not sure this needs to be public, ideally not\n   protected boolean lock(){\n    //lock the topology\n   }\n//Not sure this needs to be public, ideally not\n   protected boolean unlock(){\n    //unlock the topology\n  }\n}\n``\n. Fair enough, lets not undertake a huge refactor.\n. Please remove System.out\n. Wont this fail if the value is a String? I imagine you want to convert the value to Boolean.\n. One thing to note here that the users of this class cannot invokerestartTopologyand then callkillTopologyon the same instance as theconnectionisdisconnectedin the finally block ofrequestSchedulerService` is that as designed?\n. Please follow the same convention as the log statement above\nLOG.info(\"Scheduler is listening on location: {} \", schedulerLocation.toString())\n. Just out of curiosity, is the reason for delegating to AuroraUtils because the utils class already exists? It would appear all that functionality should be encapsulated in the here.\n. The nomenclature of utils is best suited for classes like for example IOUtils from apache commons. IOUtils allows all sorts of utility functions for I/O operations. \nThinking out aloud here are some of my thoughts...\nIn this case Aurora Commands should be ideally be invoked on an object which is more like a controller, it should represent an object which represents the target domain i.e. in this case Aurora scheduler. So thinking out aloud (and please disregard if I am not making sense here) like you said we should have the separation of decoupling AuroraScheduler and the commands. \nHow about having AuroraScheduler call into an object AuroraController or AuroraJobController which has APIs which indicate commands like createJob or create, re-startJob or restart and killJob or kill respectively. Alternate name could be AuroraCommandDispatcher.\n. @maosongfu nice!! I like this, it also prevents the need to pass the HttpConnection object around. Its nicely encapsulated \ud83d\udc4d \n. Thx @maosongfu ... sorry hope it does not feel like nitpick just thought it would be good to be consistent.\n. @maosongfu looks good. \ud83d\udc4d \n. Could you please use \njava\nLOG.log(Level.INFO, \"Preparing evaluator for running {}\", containerId);\n. Is -1 passes in via a default somewhere? Should this check for > 0?\n. Consider using  java.nio.charset.StandardCharsets.UTF_8\n. Is there a risk heron-core.tar.gz might change? If so maybe we should consider externalizing it?\n. Why do we need this method if we have a public constructor? Was the idea to create a singleton?\n. Same here\njava\nLOG.log(Level.INFO, \"Started {}\", containerId);\n. Use logger format as above\n. Is the intention that this thread will wait forever for the countdown? Is so do you mind adding a note here to call that out?\n. If configureTopology is not called packing will be null and this will throw NPE.\n. See note on log formats above.\n. Just out of curiosity. Is the HeronSchedulerLauncher an inner class by restriction from REEF? If not is it necessary to have this as an inner class? \n. Same question here as the HeronSchedulerLauncher on the use of inner class\n. Should this be thread-safe (I see the other event handler HeronWorkerBuilder synchronized)? If the event handler is called by multiple threads does it need to extract the topology repeatedly? Is the assumption that it will be called exactly once?\n. This is synchronizing on the same lock as the HeronWorkerBuilder. Is there a chance of just too much lock contention if we use the same lock?\n. Are these RequiredParameter instances expected to be used outside the scope of this class? Should these constants be marked private?\n. See note on Logger formatting as above\n. See note on logger formatting.\n. Do we need these to be public?\n. But this is not a singleton, you will need to make the constructor private if you want to create a singleton or else anyone can create an instance.\n. I am not sure @kramasamy or @maosongfu might be best suited to say. \n. Ok sounds good. Thx for explaining, might be a good idea to add the same as code notes.\n. Thx for explaining.\n. Should we consider placing these in an interface which can be implemented by all classes which need access to these constants rather than having all classes place a reference to HeronDriverConfiguration?\n. Hi @ashvina this is not for readability. If the log levels are set to Error each of these log statements will incur the overhead of concatenation before the message is sent down to the logger only to realize that the levels are set to Error and this message does not need to be logged. You can imagine the amount of overhead we will have if we keep concatenating during runtime only to discard those strings.\nAs regards other areas where this format is used, I have not been a part of the codebase since its inception so I cannot comment on other areas which use this format. I can however point to improvements in changes I see as a reviewer. I leave it to you as a coder to decide if you want to take the improvement or disregard it.\n. @ashvina the volume I was talking about is the cumulative overhead of all such log statements (like you said the practice is noted in other places) where such concatenation is done across the codebase. All those concatenations add up, so the more places we apply this diligence the better we are that is all.\n. Please realize that this is not the way to create a singleton. A public constructor will allow creating new instances, each time the public constructor is called this instance variable will be reset to the new instance. To create a singleton we need to make the constructor private and create a single instance in the static getInstance call.\n. Do you mind following the logging convention as above in the RunningJobHandler\n. Do you mind following the logging convention as above in the RunningJobHandler\n. Sounds good, I hope REEF provides APIs to access the instance.\n. \ud83d\udc4d \nIn addition we should try using \nLOG.log(Level.SEVERE, \"Unable to load the defaults class\", e)\n. For future reference, just realized these are all formatting changes. We should try to use \nLOG.log(Level.INFO, \"Incarnating ourselves as {0} with task id {1}\", new String[]{physicalPlanHelper.getMyComponent(), physicalPlanHelper.getMyTaskId())\n. What is the expectation here for logging this message? It becomes very difficult to know if there was a real issue with a stream which cannot be closed or failed on housekeeping tasks to cleanup resources. Preferably we should just use something like IOUtils.closeQuietly(os)\n. We need some defensive coding here before assuming that the connection is a HTTPURLConnection\n. @nlu90 if you refactor the methods it would be good to keep the connection creation (preferably using a connection pool) within the class which is doing all the additional work of setting Request-type, etc. That might also mean not having the flexibility to use static methods.\n. ",
    "huijunw": "estimated change will involve both http server(tmaster/metricscache) and client(tracker/dhalionIngester). @billonahill 'travis ci max time has been extended' how do you extend it? in what config? I was trying to extend it in my forked repo\n:: all right. i found the answer. rich man... karthik said twitter/heron paid travis ci so that ci does not kill it when compiling time is longer than 50min. but in my forked repo, my account does not pay ci, as a result, i cannot compile heron in my forked repo.. @kramasamy reported in the slack # general this afternoon.\nkarthikz [3:48 PM] \nin the travis there is a http unittest failure\n[3:48] \nis this a flaky test?\n[3:48] \nhttps://travis-ci.org/twitter/heron/builds/248556434?utm_source=github_status&utm_medium=notification\nThis issue still exists? @objmagic @congwang . made an issue to separate order_unittest and switch_unittest in #1810 . origin 1861 link\nhttps://github.com/twitter/heron/issues/1681. updated. tested.\nFor the null value in the map, there are 4 options:\n1. put null there. exception thrown and test failed.\n    Neither protobuf nor heron definition allows null value config.\n2. put \"\" there.\n3. put \"null\". \n    The string semantic is inconsistent within heron during parsing process.\n4. ignore that config entry.\n    The null value basically means the entry is not a meaningful config. A warning is logged.\nThanks for @maosongfu suggestion.. lgtm. I am curious why we used keys.yaml and defaults.yaml instead of enum at the very beginning.. \ud83d\udc4d . looks good to me. \ud83d\udc4d . what were these 2 files originally for?. thanks @objmagic . This feature sits along with /stats,/exceptions,/exceptionsummary but is not metrics/exceptions stats related. It will remain in TMaster when we eventually move tmetrics-collector to MetricsCache?. The URL looks good to me. @billonahill may have better ideas on path naming though.. At present, the metric timestamp works as follow. The source (eg. spout/bolt/stmgr) sends the metric with no timestamp to the metricsMgr. The metricMgr creates metric msg with local time and send to tmaster-collector where the timestamp is dropped. Tmaster-collector marks the metric with tmaster local time.\nIdeally, IMO, the metric timestamp is supposed to be tagged by the source where the metric is generated.\nOne possible cause to the over 60k backpressure time is: several backpressure metric msgs belonging to several minutes may arrive at the tmaster-colector in one 60s so that they count to more than 60k.. what is the difference between the existing \"/topologies/info\" and the new \"/topologies/metadata\" ?. what c++ version does Heron require?. @congwang actually, the motivation is a jira ticket assigned to me month ago ..\nIdeally, the backpressure is supposed to auto-heal. @kramasamy pls see the above comment. updated to use '--config-property' . This feature will not merge. Instead, it will be moved to Dhalion as one of the detectors/resolvers.. This PR is moved to #2230 . standard library +1. link to https://github.com/twitter/heron/issues/1769. i have question on point 2.\nis it possible the guess of 'upstream of two stmgr' wrong? what happens if the guess wrong?\nif FindBusiestTaskOnStmgr guess wrong, then GetUpstreamInstances returns different taskid, then the backpressure routes to a different connection?. The for loop changes the treemap. After entering the if block, the treemap may change to empty.. updated. is this pr related to #1501 and #1502 ?. We do not have plan to merge this PR. Close it.. log said:\n```\nPASS: //heron/common/tests/cpp/network:order_unittest\nPASS: //heron/common/tests/cpp/metrics:multi-count-metric_unittest\nPASS: //heron/common/tests/cpp/metrics:multi-mean-metric_unittest\nPASS: //heron/executor/tests/python:executor_unittest\nPASS: //heron/statemgrs/tests/cpp:zk-statemgr_unittest\nPASS: //heron/stmgr/tests/cpp/util:xor-manager_unittest\n_[1,444 / 1,446] Still waiting for 2 jobs to complete:\n      Running (standalone):\n        Testing //heron/common/tests/java:HeronServerTest, 24 s\n        Testing //heron/common/tests/cpp/network:switchunittest, 18 s\nPASS: //heron/common/tests/java:HeronServerTest\n__[1,445 / 1,446] Still waiting for 1 job to complete:\n      Running (standalone):\n        Testing //heron/common/tests/cpp/network:switch_unittest, 59 s\nTIMEOUT: //heron/common/tests/cpp/network:switch_unittest (see /var/lib/jenkins/.cache/bazel/_bazel_jenkins/8d6f21acdfe04ba486cfe7f3c8fbb82d/heron-mirror-release_5/bazel-out/local-opt/testlogs/heron/common/tests/cpp/network/switch_unittest/test.log)\n_From Testing //heron/common/tests/cpp/network:switchunittest:\n==================== Test output for //heron/common/tests/cpp/network:switch_unittest:\n[==========] Running 3 tests from 1 test case.\n[----------] Global test environment set-up.\n[----------] 3 tests from NetworkTest\n[ RUN      ] NetworkTest.test_switch_1\nheron/common/tests/cpp/network/switch_unittest.cpp:80: Failure\nFailed\n Aborted at 1487293808 (unix time) try \"date -d @1487293808\" if you are using GNU date \nPC: @     0x7fba011fc3c8 GI_epoll_wait\n SIGTERM (@0x4eb00005c9e) received by PID 23746 (TID 0x7fba02452710) from PID 23710; stack trace: \n    @     0x7fba01abdca0 (unknown)\n    @     0x7fba011fc3c8 __GI_epoll_wait\n    @           0x42900f epoll_dispatch\n    @           0x4204e6 event_base_loop\n    @           0x40bc9e terminate_server()\n    @           0x40c0a3 start_test()\n    @           0x4e9a13 testing::internal::HandleExceptionsInMethodIfSupported<>()\n    @           0x4dcd7d testing::Test::Run()\n    @           0x4dce14 testing::TestInfo::Run()\n    @           0x4dcf15 testing::TestCase::Run()\n    @           0x4dd18d testing::internal::UnitTestImpl::RunAllTests()\n    @           0x4dd46e testing::UnitTest::Run()\n    @           0x408a00 main\n    @     0x7fba011449f4 __libc_start_main\n    @           0x40aa41 (unknown)\n================================================================================\n____Elapsed time: 198.503s, Critical Path: 194.67s\n//heron/common/tests/cpp/basics:fileutils_unittest                       PASSED in 0.0s\n//heron/common/tests/cpp/basics:rid_unittest                             PASSED in 0.0s\n//heron/common/tests/cpp/basics:strutils_unittest                        PASSED in 0.0s\n//heron/common/tests/cpp/basics:utils_unittest                           PASSED in 0.0s\n``\nAfter investigation, foundif (server_->Start() != 0) GTEST_FAIL();` failed due to port conflict.\nIn all Heron tests, the server ports are hard coded. fix in #1847. @nlu90 i think classloader to separate user space and heron space may be an alternative to solve the package conflict.. @kramasamy Test was run hundred times; all passed\n@objmagic IntegrationTest_OneSpoutBoltMultiTasks is different. More time needed.. i guess @nlu90 means https://github.com/twitter/heron/blob/master/heron/common/src/cpp/config/BUILD#L22. lets keep what it is. got it\nthe log said \nFailed to fetch results after 15 attempts. Thanks @objmagic . no idea why.\n@kramasamy may know the reason.. @srkukarni oss migration is done. you may submit the refactoring. This PR has been refactored. Ping @srkukarni @nlu90 @kramasamy for review. do we plan to use Instant to represent an instantaneous point on the time-line in future refactoring?. do we have a switch to turn this feature on or off ?. This PR has been refactored. Ping @srkukarni @nlu90 @kramasamy for review. related #1512, #1513, #1514, #1515. This integration test will not merge. It will be moved to Dhalion following #1770.. A new PR will replace this PR soon. Close this PR now.. Will this PR be included in the 0.14.8?. how are these metrics collected? I did not find them in the metrics_sinks.yaml/tmaster-metrics-type. @nlu90 added. pls review again. why not let scheduler and tmaster talk directly rather than use packing plan in statemgr in between as communication signal ?\nin 3a, the executor, besides stmgr, watches the statemgr, which doubles the connection count on zookeeper for each container ?. I would suggest a guaranteed minimum padding 2G whatever percentage is set.. related to #1844. @ashvina any update on this pr?. Thank you @ashvina . @ashvina could you merge master that has ci fix\n@billonahill could you review and give shipit. @kramasamy what do you think on the above reply from @ashvina ? could you give a shipit. talked with @kramasamy on slack and we agreed:\nWe can merge it and evolve from it\nThank @ashvina for the great work on this PR.\n@ashvina let's move to PART-2 of #1952. tested on local ubuntu. tested on local machine. Any updates on this PR ? We saw stmgr out of memory on 0.15.1 version again. If the stmgr memory profiling is enabled, it should be easier to debug stmgr out of memory issue.\nWhat about adding two handlers to the stmgr server to control the profiling start & end:\nHandleStartHeapProfilingMessage: HeapProfilerStart()\nHandleStopHeapProfilingMessage: HeapProfilerStop()\nIn this way, the profiling is not enabled by default and there is no impact on performance.\nBesides, this profiling tool cannot tell which stmgr part code calls new_string(), new_string() occupied more than 60% of total stmgr memory if I remember correctly. @objmagic @srkukarni  thoughts?. global_protobuf_pool_acquire() is called with these types:\nStartBackPressureMessage\nStopBackPressureMessage\nTupleStreamMessage2\nHeronTupleSet2\nglobal_protobuf_pool_release() is called with these types:\nHeronTupleSet2\nStrMgrHelloResponse\nStartBackPressureMessage\nStopBackPressureMessage\nHeronTupleSet\nTupleStreamMessage2\nI guess global_protobuf_pool_release() continues pushing objects into the vectors of some types but these cached objects are not re-used .?. As @maosongfu suggested, what about triggering back-pressure if heap reaches a certain size ? @nlu90 . The present heron c++ code always assumes memory allocation success. The process crashes when 'bad allocation' exception appears.. I did not find the CachedByteSize() in protobuf.\nThere is SpaceUsed() which is noticeably slower than ByteSize(). thanks @objmagic . moved to #2035. moved to #2035. It is twitter internal repo sync issue rather than a bug in github repo.. shipit after ci pass. It seems flaky. I ran ci again and it passed.. There is a spout implementation in heron/contrib/kafka-spout/. Why do we put spouts in different directories?. One note for the pending unit test refactor #1852,\nThere is dependency cycle to be solved if we keep the paradigm 'putting all initialization in constructor':\n- StartStmgrServer() depends on metrics_manager_client_\n- metrics_manager_client_ needs stmgr_port_ after StartStmgrServer()\n. There is a handler in heron-shell, which can make heron-executor suicide. \nhttps://github.com/twitter/heron/blob/master/heron/shell/src/python/main.py#L37\nTmaster may send request to this heron-shell handler. Zombie container scenario is hard to re-produce. I would suggest enable the proposed implementation and wait for the zombie container to appear to verify.. All right @billonahill . we confirmed with aurora team that aurora does not restart the zombie container. aurora team told us, if a new container is scheduled, the old zombie is already marked as task lost in aurora state, thus aurora does restart the task lost old zombie if it dies. @srkukarni . The aurora team rejected the heron assumption 'there are no multiple same containers at one time point'. And the customers requested to raise priority of this issue. We plan to provide a PR to implement the above proposed solution. @srkukarni @objmagic @maosongfu thoughts?. Yes, in the first post. @srkukarni \nThe basic idea is:\nwhen Tmaster receives registration from the two same stmgrs, Tmaster sends http request to heron-shell endpoint '/killexectuor' of both containers. The heron-shell kill its parent heron-executor. This approach is non aurora specific.. @kramasamy i think the new stmgr has 0 connection count.\nthe new stmgr tries to register tmaster to obtain pplan but fails due to old zombie stmgr holding the tmaster connection. so the new stmgr does not have pplan and has 0 connection to the other containers. \n. @kramasamy all right. will do. what is the motivation for api server ? is there a design doc ?. @billonahill  any comments or shipit?. @ashvina any updates to Bill's comments?. @billonahill Your comments have been resolved. Could you give shipit?. @ashvina @billonahill Thank you for the great job on this PR. Let's move to the next part.. @ashvina Thanks for the effort on this PR. Let's move to the next part. agree @srkukarni . Throwing an error lets user know the tuple is too large.. fixed in #2226 . do we plan to refactor the present src code to be compatible with both python 2 and python 3?. Cool. Looks like a long journey. :). what is the motivation to separate python from the current directory structure?. ping @ashvina @maosongfu for review. thanks @maosongfu . @ashvina I merged this PR to test. If we find any issues, we may create new PRs to update it.. There are two configs, one from APi and the other from Spi. Does this issue target at APi.config or SPI.config, or both?\nFor annotations, does annotation apply to c++ and python?. If the performance is a concern, I would suggest the old method: put it in mempool and run a garage collection against mempool to remove large tuples every 1 min.. this temp pr is deprecated. close it.. The heron-instance main() only accepts the heron_internals.yaml. The override.yaml is not passed to it.\nhttps://github.com/twitter/heron/blob/master/heron/instance/src/java/com/twitter/heron/instance/HeronInstance.java#L149\nTo let heron-instance pick up the --config-property, heron-instance should read override.yaml at least.. agree with @srkukarni \nre-use heron/heron/downloaders/ code. thanks @maosongfu . With #2300, we have two sets of examples (heron-examples.jar and storm-compatibility-examples). Does this PR run the examples for both heron and storm-compatibility?. cool performance result @nlu90 . looks good to me. fix ci pls. The web page is now available. This issue is resolved. @athuras . This PR reminds me of heronpy. What is the relation between heronpy and heron-api/python?. @ashvina i have updated the pr according to your comments.\nDo not worry about the metricscache part. Mark already reviewed that part.\nOnce you find comfortable with dhalion part, we can merge.. This PR will be divided into small PRs.. This PR is replaced by #2401 and #2406, so close it.. I have a question, what is our policy to update bazel version? Do we follow bazel version closely for every version?. @srkukarni thank you for the clarification.\nneeds @nlu90 comment. A metric unique location is three item tuple , but the Dhalion.MetricsProvider interface uses  to locate metrics. Tmaster/metricscache  returns a list of metrics including all instances, which is reasonable.\nMetricscacheprovider not only provides backpressure metrics but also may provide metrics for other sensors. It is a general metrics provider and is supposed to return the exact query result rather than optimize for backpressure sensor.\n@ashvina . relates to https://github.com/twitter/heron/pull/2005. our ci is dead. @objmagic is trying to fix it. we plan to test this pr after we fix the ci.. our ci pass. any thoughts to fix this @srkukarni . here is the compiling error\n[error] xBolt.scala: class xBolt needs to be abstract, since method prepare in trait IBolt of type (x$1: java.util.Map[String,Object], x$2: org.apache.storm.task.TopologyContext, x$3: org.apache.storm.task.OutputCollector)Unit is not defined\n[error] (Note that java.util.Map[String,Object] does not match java.util.Map[_, _]: their type parameters differ)\n[error] class xBolt extends BaseRichBolt {\n[error]       ^\n[error] xBolt.scala: method prepare overrides nothing.\n[error] Note: the super classes of class xBolt contain the following, non final members named prepare:\n[error] def prepare(x$1: java.util.Map[String,Object],x$2: org.apache.storm.task.TopologyContext,x$3: org.apache.storm.task.OutputCollector): Unit\n[error]   override def prepare(conf: JMap[_, _], context: TopologyContext, collector: OutputCollector) {\n[error]                ^\n[error] two errors found. good to know that Heron does not use kryo. @maosongfu i would prefer setting the config in tool main. it is hard to say the default is false rather than true. for auroa, it is false. for local, it is true. both aurora and local are often seen scenarios.. @maosongfu updated with error/corner case handling . tested pass in aurora in datacenter.. after careful examine, i found this pr is not necessary.. if a topology uses RoundRobinPacking packing alg, what re-packing alg should this topology use?. @ashvina This PR is trying to map from logical-container-id to aurora-container-id. However Aurora does not accept metadata. \nAurora gives container id on its own implementation. We do not know what container id Aurora will give before Aurora actually does container allocation. \nThus this PR does a container-id remapping to align logical-container-id to the real aurora-container-id after Aurora does container allocation.\nThe update to the spi has no harm to the other schedulers. Besides, it may lead to further scheduler improvement.. talked with @ashvina , and @nwangtw @maosongfu \nwill update according to @ashvina suggestion:\n\nChange SPI to :\n// @return Set of added containers with real ids and resources from scheduler\nSet<PackingPlan.ContainerPlan> addContainers(Set<PackingPlan.ContainerPlan> containersToAdd);\nAlso, add the returned containers to current packing plan and persist that in ZK.\n. 1. The present resource based packing algorithms requires changes in user topology code. Meanwhile, it is not prod verified yet so a lot of testing and bug-fixes will be involved. The migration will not be trivial and it can take 2+ quarters.\nThe users would like to use heron update now but RRpacking does not support it yet and it is a blocker since people can not repack with FirstFitPackingAlgorithm.  The implementation in this PR and #2602 handle the mentioned end up getting fewer resources per component instance issue.\nMake RRrepacking extends IRepack can add an option for user; it will not hurt the migration to the new packing algorithm in future. . Thanks @billonahill , could you also review #2602, which is related to this PR.. @billonahill  the unit test has been fixed.\nI would suggest have this PR merged to make update work at present. Then, we may evolve the algorithm to be less disruptive.. After talking with @ashvina , the following ideas will achieve the backpressure detector only.\n\nmaking skew-ratio high and disparity-ratio = 1 will always pass skew-ratio and disparity ration check in the current slow instance diagnoser.\nThe command is heron submit --config-property WaitQueueDisparityDetector.disparityRatio=999999 --config-property ProcessingRateSkewDetector.skewRatio=1 xxxxx \nOur customer verified the above command. Thus close this PR.. At present, if cleaning zk fails, we suggest user :\ndo \n   `heron kill xxx`\nuntil success\nwhich is similar as this pr. This pr suggests:\ndo\n  `heron submit xxx`\nuntil success\nThis pr does not solve the root of cause 'cleaning zk fails'.\nI prefer heron kill xx until success because it does what it is named.. The line count depends. As far as I saw, they vary from 40 to 90.\nThe first 2 lines are shows what the exception is (the first line) and where the exception is thrown (the second line). I would prefer keeping the RR alg since it is tested in production and simple enough as a reference for new developers.. yes. unit test updated. Close this PR. Let's find a better way to do this. replaced by #2735. as long as heron-aurora-scheduler.jar is available in class path for healthmgr, it is fine to me.. two questions:\n1. why is zookeeper not single thread ? libzookeeper_mt vs libzookeeper_st\n2. if multi thread zk client is expected, how to solve the race condition? blind guess: delete piper after zookeeper_close() https://github.com/apache/incubator-heron/blob/0.17.8/heron/common/src/cpp/zookeeper/zkclient.cpp#L146. replace https://github.com/apache/incubator-heron/issues/2765. For the thread 0x7f616cc62700:\nIt called the GetCompletionWatcher(), when a getting-zk-node operation is done. In the watcher, ZkActionCb-> ExecuteInEventLoop-> enqueue-> notify_one-> __lll_lock_wait(), stuck.\nFor the thread main 0x7f616ecae780:\nA zk session expired, GlobalWatchEventHandler was called -> ~ZKClient() -> first delete piper, then zookeeper_close() -> join thread, stuck\nOur theory is: two events(session_expire and get_zk_node) happened, and each was handled in a thread. The main thread handled session_expire, while the other thread handled get_zk_node_done. The session_expire watcher in main wait for the other thread to join, while the other thread wait for a lock which was deleted in piper_ by  ~ZKClient() in main thread.\nIf multi thread is intended, proposed solution is:\nreorder the delete_piper and close_zk_client\ndelete piper_;\n  zookeeper_close(zk_handle_);\nhttps://github.com/apache/incubator-heron/blob/0.17.8/heron/common/src/cpp/zookeeper/zkclient.cpp#L146\n. @srkukarni why is zookeeper not single thread ? libzookeeper_mt vs libzookeeper_st\nIf I remember correctly, the stmgr should be single thread and the zkclient should be in the same main thread. What we observed is that zk client is multi thread. I would like to know if this is intended, and shall we make zk client share the same single main thread. @srkukarni another thought: for the pcqueue, https://github.com/apache/incubator-heron/blob/0.17.8/heron/common/src/cpp/threads/pcqueue.h#L36, when the pcqueue is destructed, the mutex and cond_ behavior is undefined. but pcqueue.enqueue is called after ~PCQueue(). what do you think?. Hi @avflor \nThe healthmgr failed and quited in the first round policy execution, then restarted. There were no last 5 policy execution.\nThe issue is because previousCheckpoint is not initialized (null) in the first round policy execution. https://github.com/Microsoft/Dhalion/blob/0.2.1/src/main/java/com/microsoft/dhalion/policy/PoliciesExecutor.java#L137\n```\n[2018-06-27 23:15:11 +0000] [INFO] org.apache.heron.healthmgr.HealthManager: Logging setup done.\n[2018-06-27 23:15:11 +0000] [INFO] org.apache.heron.healthmgr.HealthManager: Static Heron config loaded successfully \n[2018-06-27 23:15:11 +0000] [INFO] org.apache.heron.healthmgr.HealthManager: Initializing health manager\n[2018-06-27 23:15:13 +0000] [INFO] org.apache.heron.statemgr.zookeeper.curator.CuratorStateManager: Starting Curator client connecting to: xxxxxxx_zk\n[2018-06-27 23:15:13 +0000] [INFO] org.apache.curator.framework.imps.CuratorFrameworkImpl: Starting\n[2018-06-27 23:15:13 +0000] [INFO] org.apache.curator.framework.state.ConnectionStateManager: State change: CONNECTED\n[2018-06-27 23:15:13 +0000] [INFO] org.apache.heron.statemgr.zookeeper.curator.CuratorStateManager: Directory tree initialized.\n[2018-06-27 23:15:14 +0000] [INFO] org.apache.heron.statemgr.zookeeper.curator.CuratorStateManager: Starting Curator client connecting to: xxxxxxx_zk \n[2018-06-27 23:15:14 +0000] [INFO] org.apache.curator.framework.imps.CuratorFrameworkImpl: Starting\n[2018-06-27 23:15:14 +0000] [INFO] org.apache.curator.framework.state.ConnectionStateManager: State change: CONNECTED\n[2018-06-27 23:15:14 +0000] [INFO] org.apache.heron.statemgr.zookeeper.curator.CuratorStateManager: Directory tree initialized.\n[2018-06-27 23:15:14 +0000] [INFO] org.apache.heron.healthmgr.HealthPolicyConfig: Health Policy Configuration:{health.policy.class=org.apache.heron.healthmgr.policy.AutoRestartBackpressureContainerPolicy, health.policy.interval.ms=120000, BackPressureDetector.noiseFilterMillis=20}\n[2018-06-27 23:15:14 +0000] [INFO] org.apache.heron.healthmgr.HealthManager: Initializing auto-restart-backpressure-container with class org.apache.heron.healthmgr.policy.AutoRestartBackpressureContainerPolicy\n[2018-06-27 23:15:14 +0000] [INFO] org.apache.heron.healthmgr.sensors.MetricsCacheMetricsProvider: Metrics will be provided by MetricsCache at http://yyyyyyyyyy/stats\n[2018-06-27 23:15:14 +0000] [INFO] org.apache.heron.healthmgr.HealthManager: Starting Health Manager\n[2018-06-27 23:15:14 +0000] [INFO] org.apache.heron.healthmgr.HealthManager: Starting Health Manager metric posting thread\n[2018-06-27 23:15:14 +0000] [INFO] org.apache.heron.common.network.HeronClient: Connecting to endpoint: /127.0.0.1:31671\n[2018-06-27 23:15:14 +0000] [INFO] org.apache.heron.healthmgr.HealthManagerMetrics: Connected to Metrics Manager. Ready to send register request\n[2018-06-27 23:15:14 +0000] [INFO] com.microsoft.dhalion.policy.PoliciesExecutor: Executing Policy: AutoRestartBackpressureContainerPolicy, checkpoint: 2018-06-27T23:15:14.462Z\n[2018-06-27 23:15:14 +0000] [INFO] org.apache.heron.healthmgr.common.TopologyProvider: Fetching topology from state manager: zzzzzzzzzz\n[2018-06-27 23:15:14 +0000] [INFO] org.apache.heron.healthmgr.HealthManagerMetrics: We registered ourselves to the Metrics Manager\n[2018-06-27 23:15:15 +0000] [INFO] org.apache.heron.common.network.SocketChannelHelper: Forcing to flush data to socket with best effort.\n[2018-06-27 23:15:15 +0000] [INFO] org.apache.heron.common.network.HeronClient: To stop the HeronClient.\n[2018-06-27 23:15:15 +0000] [INFO] org.apache.heron.healthmgr.HealthManagerMetrics: SimpleMetricsManagerClient exits\n[2018-06-27 23:15:15 +0000] [STDERR] stderr: Exception in thread \"main\" \n[2018-06-27 23:15:15 +0000] [STDERR] stderr: java.util.concurrent.ExecutionException: java.lang.NullPointerException\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.util.concurrent.FutureTask.report(FutureTask.java:122)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.util.concurrent.FutureTask.get(FutureTask.java:192)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at org.apache.heron.healthmgr.HealthManager.main(HealthManager.java:241)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr: Caused by: java.lang.NullPointerException\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.time.Instant.compareTo(Instant.java:1255)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.time.Instant.isBefore(Instant.java:1285)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at com.microsoft.dhalion.policy.PoliciesExecutor.lambda$null$0(PoliciesExecutor.java:83)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at com.microsoft.dhalion.policy.PoliciesExecutor.lambda$start$2(PoliciesExecutor.java:84)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n[2018-06-27 23:15:15 +0000] [STDERR] stderr:    at java.lang.Thread.run(Thread.java:748)\n[2018-06-27 23:15:26 +0000] [INFO] org.apache.heron.healthmgr.HealthManager: Logging setup done.\n[2018-06-27 23:15:26 +0000] [INFO] org.apache.heron.healthmgr.HealthManager: Static Heron config loaded successfully \n[2018-06-27 23:15:26 +0000] [INFO] org.apache.heron.healthmgr.HealthManager: Initializing health manager\n[2018-06-27 23:15:27 +0000] [INFO] org.apache.heron.statemgr.zookeeper.curator.CuratorStateManager: Starting Curator client connecting to: xxxxxxx_zk\n[2018-06-27 23:15:27 +0000] [INFO] org.apache.curator.framework.imps.CuratorFrameworkImpl: Starting\n[2018-06-27 23:15:27 +0000] [INFO] org.apache.curator.framework.state.ConnectionStateManager: State change: CONNECTED\n[2018-06-27 23:15:27 +0000] [INFO] org.apache.heron.statemgr.zookeeper.curator.CuratorStateManager: Directory tree initialized.\n[2018-06-27 23:15:28 +0000] [INFO] org.apache.heron.statemgr.zookeeper.curator.CuratorStateManager: Starting Curator client connecting to: xxxxxxx_zk\n[2018-06-27 23:15:28 +0000] [INFO] org.apache.curator.framework.imps.CuratorFrameworkImpl: Starting\n[2018-06-27 23:15:28 +0000] [INFO] org.apache.curator.framework.state.ConnectionStateManager: State change: CONNECTED\n[2018-06-27 23:15:28 +0000] [INFO] org.apache.heron.statemgr.zookeeper.curator.CuratorStateManager: Directory tree initialized.\n[2018-06-27 23:15:28 +0000] [INFO] org.apache.heron.healthmgr.HealthPolicyConfig: Health Policy Configuration:{health.policy.class=org.apache.heron.healthmgr.policy.AutoRestartBackpressureContainerPolicy, health.policy.interval.ms=120000, BackPressureDetector.noiseFilterMillis=20}\n[2018-06-27 23:15:28 +0000] [INFO] org.apache.heron.healthmgr.HealthManager: Initializing auto-restart-backpressure-container with class org.apache.heron.healthmgr.policy.AutoRestartBackpressureContainerPolicy\n[2018-06-27 23:15:28 +0000] [INFO] org.apache.heron.healthmgr.sensors.MetricsCacheMetricsProvider: Metrics will be provided by MetricsCache at http://yyyyyyyyyy/stats\n[2018-06-27 23:15:28 +0000] [INFO] org.apache.heron.healthmgr.HealthManager: Starting Health Manager\n[2018-06-27 23:15:29 +0000] [INFO] org.apache.heron.healthmgr.HealthManager: Starting Health Manager metric posting thread\n[2018-06-27 23:15:29 +0000] [INFO] org.apache.heron.common.network.HeronClient: Connecting to endpoint: /127.0.0.1:31671\n[2018-06-27 23:15:29 +0000] [INFO] org.apache.heron.healthmgr.HealthManagerMetrics: Connected to Metrics Manager. Ready to send register request\n[2018-06-27 23:15:29 +0000] [INFO] com.microsoft.dhalion.policy.PoliciesExecutor: Executing Policy: AutoRestartBackpressureContainerPolicy, checkpoint: 2018-06-27T23:15:29.036Z\n[2018-06-27 23:15:29 +0000] [INFO] org.apache.heron.healthmgr.common.TopologyProvider: Fetching topology from state manager: zzzzzzzzzz\n[2018-06-27 23:15:29 +0000] [INFO] org.apache.heron.healthmgr.HealthManagerMetrics: We registered ourselves to the Metrics Manager\n[2018-06-27 23:15:30 +0000] [INFO] org.apache.heron.common.network.SocketChannelHelper: Forcing to flush data to socket with best effort.\n[2018-06-27 23:15:30 +0000] [INFO] org.apache.heron.common.network.HeronClient: To stop the HeronClient.\n[2018-06-27 23:15:30 +0000] [INFO] org.apache.heron.healthmgr.HealthManagerMetrics: SimpleMetricsManagerClient exits\n[2018-06-27 23:15:30 +0000] [STDERR] stderr: Exception in thread \"main\" \n[2018-06-27 23:15:30 +0000] [STDERR] stderr: java.util.concurrent.ExecutionException: java.lang.NullPointerException\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.util.concurrent.FutureTask.report(FutureTask.java:122)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.util.concurrent.FutureTask.get(FutureTask.java:192)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at org.apache.heron.healthmgr.HealthManager.main(HealthManager.java:241)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr: Caused by: java.lang.NullPointerException\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.time.Instant.compareTo(Instant.java:1255)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.time.Instant.isBefore(Instant.java:1285)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at com.microsoft.dhalion.policy.PoliciesExecutor.lambda$null$0(PoliciesExecutor.java:83)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:174)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at com.microsoft.dhalion.policy.PoliciesExecutor.lambda$start$2(PoliciesExecutor.java:84)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n[2018-06-27 23:15:30 +0000] [STDERR] stderr:    at java.lang.Thread.run(Thread.java:748)  \n```. @avflor thank you !. @avflor Is there ETA for the new Dhalion version? cc @maosongfu @nwangtw @nlu90 . Hi @avflor just check in, how is the progress, and what is the eta? Thank you.. @ashvina cool. Thank you !. ship after ci pass. to confirm, MY_OWN_RELEASE_INFO.yaml follows the same format as the CLI release.yaml? . related\n3158\n3159\n3165\n3166\n. related\n3158\n3159\n3165\n3166\n. related\n3158\n3159\n3165\n3166\n. related\n3158\n3159\n3165\n3166\n. there was a contrib dir, which included spouts.\nhttps://github.com/apache/incubator-heron/tree/0.15.0/contrib\ni think we may recover the contrib dir, and incubate codes such as spouts from there till they are migrated to other places. thoughts?. Agree the the ultimate goal is to unite CIs in one place. This may make the CI scripts more organized and easy to maintain. Besides scripts/travis/, scripts/applatix/ should be migrated to scripts/ci/ as well. For configs, scripts/ci/config is better than scripts/travis/.\nTo confirm, scripts/ci/ will handle all CIs including travis, jenkins, applatix; and no more scripts/travis/ or scripts/applatix/ ?. although stmgr/tmaster report uptime every 1 sec, the metricsmgr still report every 1 min? when a stmgr/tmaster is restarted, which still needs 1min for metricsmgr to flush metrics to expose this restarting event.. ",
    "lewiskan": "I'm a fan of excluding 'we' and 'us' as well - this was just continuing to follow the Bazel community as a guide: http://bazel.io/contributing.html (note nav bar 'community' is also following Bazel), thanks.\n. @billonahill would you like me to take this over, checking in updates and assigning issue to me?\nusing linkchecker, i see current master links at 2863 and errors at 276:\nThat's it. 2863 links in 161 URLs checked. 0 warnings found. 276 errors found.\n. @billonahill @kramasamy any additional comments on this updated PR, removing 'we/us' language and addressing 'heron architecture' comment?  thanks!\n. Thanks, @billonahill !  I had problems with the new build make, so enclosing   didn't appear to work in the md, but testing this now it works, thanks for the pointer!  The committers/contributors list we discussed Sunday, potentially adding company affiliation if possible, to encourage a growing OSS community, hopefully we revisit a list.  I'll add v0.0.1 going forward, thanks.\n. @billonahill @kramasamy - I can take a shot at this, 'pip install linkchecker' worked, then add a check when pushing to gh-pages for publication.\n. Yes, we are on the same page - as a test and in order to keep the site private, I re-created the heron structure and used 'hugo build' to generate the static site, pushing to branch 'gh-pages' following the github-pages-blog outline.  Here's the sample:\nhttp://lewiskan.github.io/hackprojectorg/website/public/\n(redirected from 'hackproject.org')\nlinkchecker appears to allow checking static pages via:\nChecking local HTML files on Unix:\n  linkchecker ../bla.html subdir/blubber.html\n(mac osx, unix only however).  This avoids needing to host the site locally using 'hugo server --watch', seeing if there is an easy way to get github to force this check prior to pushing to gh-pages specifically, any suggestions greatly appreciated!\n. Adding the following MakeFile publish section is close:\npublish:\n    @hugo\n    @pip install linkchecker\n    @linkchecker public/index.html -F csv\n    @cut -sd ';' -f 1,2 linkchecker-out.csv | tr ';' ' ' |awk '{ print $2 \" \" $1}' | sort -u\n. Awesome!  We were careful not to publish anything hinting at a Heron open source release so I removed any reference to Heron.  The Twitter link should be discovered soon, similar to the Apache Heron proposal and tweeted, but we are closer to launch.\nMakeFile added publish:\npublish:\n    @hugo\n    @pip install linkchecker\n    @linkchecker public/index.html \nNOTE: this works on Mac but unsure on Windows/Linux.  Command result that I'm investingating: \nmake: *** [publish] Error 1\nREADME instructions:\nTo publish website using github pages (requires python pip)\nbash\n$ make publish\nIf linkchecker errors are 0, then publish to branch gh-pages\nbash\n$ git checkout -b gh-pages\n$ git push origin gh-pages\nSubmit PR to main branch gh-pages (not master)\n. Closing PR to updated based on above comments (remove gh-pages push for now, automate linkchecker further so no manual check required)\n. got it - i usually close and reopen to prevent merges taking place, but i'll keep open going forward, thanks.\n. @billonahill - per last comment, since make terminates if error is reached unless suppressed (which sets error code to \"success\"), the only workaround i could find is to run linkchecker twice.  first run, error code is suppressed, error output is formatted.  second runs without error code suppressed -> on fail, linkchecker fails.  on success, linkchecker removes formatted error output (which is empty) and continues.  not ideal, but any other suggestions would be helpful!  thanks.\n. @billonahill cool, linkchecker.sh and MakeFile modifications added for review.\n. @billonahill , incorporated comments and pushed yesterday, any additional comments on this?  thanks!\n. Looks like current build (and 3 current PRs for review) failed travis-ci, but nothing changed in the code besides a merge from master.  \nhttps://travis-ci.com/twitter/heron/builds/24273204\nINFO: Building...\n[0 / 4] Expanding template heron/uploaders/tests/java/s3-uploader_unittest\n[0 / 18] Expanding template heron/api/src/java/api-unshaded\n[0 / 38] Creating source manifest for //scripts/compile:gen_errcodes [for host]\n[7 / 45] Creating runfiles tree bazel-out/host/bin/scripts/compile/gen_errcodes.runfiles [for host]\nERROR: Process exited with status 1.\nINFO: Elapsed time: 7.771s, Critical Path: 0.21s.\nThe command \"scripts/travis/build.sh\" exited with 1.\nDone. Your build exited with 1.\n. @billonahill git looks to render the script a bit jenky and different than sublime.  i modified removing trailing whitespace and styled to 2 indent (changed from your previous comment of 4 indent) and added backslash so git renders correctly.  hopefully this looks a-ok, thanks.\n. Hi @maosongfu - @kramasamy  merged this fix into master.  The compilation using bazel 0.2.0-homebrew failed on my machine after running:\n$ bazel build --config=darwin heron/...\n(note no options)\nwith error described in issue.  Following the error suggestion in 3 files fixed the compilation issue, and all travis-ci tests pass.  This issue should be closed, thanks!\n. Commands look incorrect, where are they located?  In \"http://localhost:1313/heron/docs/developers/compiling/compiling/\"  I only see the correct:\n$bazel build --config=darwin heron/...\nand building all components:\n$ bazel build --config=darwin scripts/packages:binpkgs\n$ bazel build --config=darwin scripts/packages:tarpkgs\nand building specific components:\n$ bazel build --config=darwin heron/tracker/src/python:heron-tracker\ntravis-ci modification could still check these commands if not already.\n. @kramasamy @billonahill local Darwin and AWS Ubuntu failing on \"bazel build\" command, has command changed or is this a known issue? thanks!\n```\n$bazel build --config=darwin heron/...\nERROR: /Users/lewiskan/OpenSource/heron/WORKSPACE:12:1: maven_jar rule //external:commons-beanutils's name field must be a legal workspace name.\nERROR: /Users/lewiskan/OpenSource/heron/WORKSPACE:17:1: maven_jar rule //external:commons-cli's name field must be a legal workspace name.\nERROR: /Users/lewiskan/OpenSource/heron/WORKSPACE:22:1: maven_jar rule //external:commons-collections's name field must be a legal workspace name.\nERROR: /Users/lewiskan/OpenSource/heron/WORKSPACE:27:1: maven_jar rule //external:commons-lang's name field must be a legal workspace name.\nERROR: /Users/lewiskan/OpenSource/heron/WORKSPACE:32:1: maven_jar rule //external:commons-logging's name field must be a legal workspace name.\nERROR: /Users/lewiskan/OpenSource/heron/WORKSPACE:42:1: maven_jar rule //external:protobuf-java's name field must be a legal workspace name.\nERROR: Error evaluating WORKSPACE file.\nERROR: package contains errors: heron/statemgrs/conf.\nERROR: no such package 'external': Package 'external' contains errors.\nINFO: Elapsed time: 0.171s\n```\n(aws Ubuntu)\n```\n$ bazel build --config=ubuntu scripts/packages:binpkgs  --verbose_failures  --genrule_strategy=standalone --ignore_unsupported_sandboxing --sandbox_debug --spawn_strategy=standalone\nERROR: /home/ubuntu/heron/WORKSPACE:12:1: maven_jar rule //external:commons-beanutils's name field must be a legal workspace name.\nERROR: /home/ubuntu/heron/WORKSPACE:17:1: maven_jar rule //external:commons-cli's name field must be a legal workspace name.\nERROR: /home/ubuntu/heron/WORKSPACE:22:1: maven_jar rule //external:commons-collections's name field must be a legal workspace name.\nERROR: /home/ubuntu/heron/WORKSPACE:27:1: maven_jar rule //external:commons-lang's name field must be a legal workspace name.\nERROR: /home/ubuntu/heron/WORKSPACE:32:1: maven_jar rule //external:commons-logging's name field must be a legal workspace name.\nERROR: /home/ubuntu/heron/WORKSPACE:42:1: maven_jar rule //external:protobuf-java's name field must be a legal workspace name.\nERROR: Error evaluating WORKSPACE file.\nERROR: no such package '@bazel_tools//tools/cpp': error loading package 'external': Could not load //external package.\nINFO: Elapsed time: 0.279s\n```\n. This is a Bazel > 0.1.2 issue as I was using Bazel 0.2.2 and 0.2.0.\nNote: Bazel 0.1.5 now rejects maven jars with dots or dashes in their names\nhttps://github.com/bazelbuild/bazel/issues/874\n. @billonahill yes, ubuntu host to test bazel sandbox errors from #108 and the recent heron-user thread:\nkarthik cited: https://groups.google.com/forum/#!topic/bazel-discuss/kHvq5faKiTw\nand supan found the following command to work (turning off sandbox):\nbazel build --config=ubuntu scripts/packages:binpkgs  --verbose_failures  --genrule_strategy=standalone --ignore_unsupported_sandboxing --sandbox_debug --spawn_strategy=standalone\nwe need to update Ubuntu host directions to avoid the sandbox issue and i'm testing this now on aws.\n. @kramasamy @billonahill Supan's Ubuntu commands compile successfully on aws (confirming sandbox issue, finally!).\nHowever, in addition to simple apt-get installs, we needed to manually install:\n1) libtool-2.4.6 , since \"sudo apt-get install libtool\" installs a lower than required v2.4.2, (unchecked by bazel_configure.py)\nafter multiple gperftools errors, and following http://gperftools.googlecode.com/svn/trunk/INSTALL \n2) libunwind-1.1\n3) gperftools-2.5\nafter these 3 installations via .tar, the following commands work:\nbazel build --config=ubuntu heron/...  --verbose_failures \\  \n--genrule_strategy=standalone --ignore_unsupported_sandboxing \\\n--sandbox_debug --spawn_strategy=standalone\nbazel build --config=ubuntu scripts/packages:binpkgs  --verbose_failures \\  \n--genrule_strategy=standalone --ignore_unsupported_sandboxing \\\n--sandbox_debug --spawn_strategy=standalone\nI can add the above commands to Ubuntu compile web directions but want to confirm that we should include the libtool, libunwind and gperftools instructions.  Attached is a .txt file outlining all required steps, thanks.\naws-ubuntu-heron.txt\n. thanks @kramasamy and @supunkamburugamuve , attached is .txt of all commands for aws Ubuntu 14.04, namely:\n- installing libtool-2.4.6 (NOTE: 'sudo apt-get install libtool' installs 2.4.2)\n```\ncd /home/ubuntu\nhttps://www.gnu.org/software/libtool/\nwget http://ftpmirror.gnu.org/libtool/libtool-2.4.6.tar.gz\ntar -xvf libtool-2.4.6.tar.gz && cd libtool-2.4.6\n./configure\nsudo make\nsudo make install\n```\n- error message if libunwind and gperftools is not installed\n``\nconfig.status: executing libtool commands\nconfigure: error: in/tmp/gperftools.Q9gsI/gperftools-2.4':\nconfigure: error: No frame pointers and no libunwind. The compilation will fail\naws-ubuntu-heron.txt\nSee `config.log' for more details\nERROR: /home/ubuntu/heron/3rdparty/gperftools/BUILD:11:1: Executing genrule //3rdparty/gperftools:gperftools-srcs failed: bash failed: error executing command\n  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/49c4744f9ca584202f9a40692e0a51e0/heron && \\\n  exec env - \\\n    PATH=/home/ubuntu/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; export INSTALL_DIR=$(pwd)/bazel-out/local_linux-fastbuild/genfiles/3rdparty/gperftools\nexport TMP_DIR=$(mktemp -d -t gperftools.XXXXX)\nmkdir -p $TMP_DIR\ncp -R 3rdparty/gperftools/gperftools-2.4.tar.gz $TMP_DIR\ncd $TMP_DIR\ntar xfz gperftools-2.4.tar.gz\ncd gperftools-2.4\n./configure --prefix=$INSTALL_DIR --enable-shared=no\nmake install\nrm -rf $TMP_DIR'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: bash failed: error executing command\n  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/49c4744f9ca584202f9a40692e0a51e0/heron && \\\n  exec env - \\\n    PATH=/home/ubuntu/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; export INSTALL_DIR=$(pwd)/bazel-out/local_linux-fastbuild/genfiles/3rdparty/gperftools\nexport TMP_DIR=$(mktemp -d -t gperftools.XXXXX)\nmkdir -p $TMP_DIR\ncp -R 3rdparty/gperftools/gperftools-2.4.tar.gz $TMP_DIR\ncd $TMP_DIR\ntar xfz gperftools-2.4.tar.gz\ncd gperftools-2.4\n./configure --prefix=$INSTALL_DIR --enable-shared=no\nmake install\nrm -rf $TMP_DIR'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nINFO: Elapsed time: 25.187s, Critical Path: 14.94s.\n```\naws-ubuntu-heron.txt\n. heron-api, heron-storm 0.14.0 released.  oss.sonatype.org jira issue raised for initial sync per: \nhttp://central.sonatype.org/pages/releasing-the-deployment.html\nstaged/relesaed repos, all checks pass:\nhttps://oss.sonatype.org/content/repositories/comtwitter-1779\nhttps://oss.sonatype.org/content/repositories/comtwitter-1780\nissue can be resolved.\n. @kramasamy success, tracker UI launch with /bin/bash automatically. (upgraded to heron v.0.13.5)\nstep 1: check and save local docker-machine ip address\n$ docker-machine ip default\nfor example: 192.168.99.100\nstep 2: launch image with /bin/bash , specify port address with -p\ndocker run -it -p 8888:8888 -p 8889:8889 puffin:u14.04h0.13.5 /bin/bash\nthis maps tracker port :8888-> {your-docker-ip}:8888 and ui :8889 -> {your-docker-ip}:8889\nstep 3: within container, submit heron example (following getting-started guide)\nheron submit local ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology\nstep 4: check running topology UI on local machine\nhttp://{your-docker-ip}:8889\nexample: http://192.168.99.100:8889/\ncurrently heron-tracker and heron-ui run in background and output to command line. this can be cleaned up to output to file if desired. \n. @kramasamy @billonahill docker image upgraded to heron v.0.13.7, launches UI automatically. docker hub repos \"heron\" \"heronstream\" \"heronstreaming\" reserved. \nstep 1: launch docker terminal, check and save local docker-machine ip address\n$ docker-machine ip default\nfor example: 192.168.99.100\nstep 2: pull and launch docker image with /bin/bash , specify port address with -p\ndocker run -it -p 8888:8888 -p 8889:8889 puffin:u14.04h0.13.7 /bin/bash\nthis maps tracker port :8888-> {your-docker-ip}:8888 and ui :8889 -> {your-docker-ip}:8889\nstep 3: within container, submit heron example (following getting-started guide)\nheron submit local ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology\nstep 4: check running topology UI on local machine\nhttp://{your-docker-ip}:8889\nexample: http://192.168.99.100:8889/\ncurrently heron-tracker and heron-ui run in background and output to command line. this can be cleaned up to output to file if desired.\n. public docker v0.14.0 published:\nhttps://hub.docker.com/r/heronstreaming/heron/tags/\nusage:\ndocker run -it -p 8888:8888 -p 8889:8889 heronstreaming/heron:0.14.0-ubuntu /bin/bash\nsee above directions, UI available at: http://{your-docker-ip}:8889 \n. @tysonnorris I'll check in an updated version now that project is open sourced and Dockerfile can pull directly from public release, thanks.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. \ud83d\udc4d \n. :+1\n. :+1\n. \ud83d\udc4d \n. Twitter is a founding member of Cloud Native Computing Foundation ( https://cncf.io/ ).  Any experiences with CNCF, or pros/cons compared to Apache? Heron could become the 3rd project next to Kubernetes and Prometheus.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. per @kramasamy this is solved by using --user flag and original is correct.\n. @kramasamy yes, will investigate further how to publish source with Bazel.\n. @billonahill @kramasamy Bazel forum was helpful, testing source artifacts now to publish on 0.14.1 update.  Next step is to automate as process currently manual.  Maven has an automatic deploy option, Bazel does not as far as I know.\n. @billonahill great, I'll document current process for consistency and update going froward, thanks for the guidance.\n. @billonahill agree on all points, updated current-process.md.  TODOs include 1) create bash script for POM versioning 2) complete remaining release-process and 3) refer to BUILD process and rm downloading artifacts.\n. @kramasamy @billonahill name and email changed to Heron Users and 'heron-users@googlegroups.com'\n. @billonahill uri examples modified, automated POM template versioning with script.  issue #935 to add \"heron-spi\" POM included, build process needs update from download to build process including heron-spi.\n. @billonahill updates do not address \"code style for jar\" comment, but added a note that code format should be generated by Bazel build process.  please lmk how to update and i'll complete, thanks.\n. @kramasamy building standalone \"heron-spi\" with bazel build --config=darwin heron/spi/src/java:heron-spi  generates \"libheron-spi.jar\" located in bazel-out/local_darwin-fastbuild/bin/heron/spi/src/java \nIs \"libheron-spi.jar\" correct jar to publish, if so, do we rename to \"heron-spi\" to match documentation, or do we need a new BUILD genrule similar to \"heron-api\" \ngenrule(\n    name = \"heron-api\",\n    srcs = [\":api-unshaded_deploy.jar\"],\n    outs = [\"heron-api.jar\"],\n    cmd  = \"cp $< $@\",\n)\n. @ajorgensen I see the PR https://github.com/twitter/heron/pull/937/files with genrule added, great, I'll wait for the merge then complete Maven Central publish, thanks.\n. cool, my thoughts exactly!  thanks for generating the PR, i'll wait for merge before uploading to Maven.\n. @kramasamy : website 0.2.3 PR updated and conflicts resolved.  Note the previous commit used {{% bazelVersion %}} in bash commands that fail to parse correctly.  These have been corrected to 0.2.3 to resolve.\n. @kramasamy please let me know if the new update modifying /tmp/bazel.sh in place is what you had in mind, thanks.\n. @kramasamy yes, the image name is 'heronstreaming/heron:0.14.0-ubuntu' and available on docker hub. however this PR was requested for the Dockerfile and I did not want to confuse namespace with locally built image instructions. Please let me know if I should modify or document further (we can add to release process w script to generate VERSION similar to maven as needed, but this was requested yesterday)\n. @tysonnorris Thanks for the guidance - I haven't build Dockerfiles for production use.\n- the 'getting-started' docker image requirements are to start heron-tracker and heron-ui automatically for a getting-started heron use case, rather than lightweight production.\n- Docker conventions indicate one running process per container.  However, for the getting-started use case it's easiest to have all running processes in a single container.  The workaround of using .bashrc within one container is not ideal so your suggestion to use an init system is great, it would be helpful if you could please submit or comment an example, thanks.\n- on the build comment, we removed the Bazel install for a smaller image and instead download install scripts from release.  I believe this already results in a smaller image, but if not, please let me know what you are suggesting, thanks.\n. @kramasamy - I need to incorporate @tysonnorris enhancement to use docker-compose rather than simple bashrc. I'll find time to work on this over the long weekend, thanks.\n. @pulasthi thanks for writing this excellent step-by-step guide!  @kramasamy asked me to review.  I'm able to follow all steps, however, upon submitting to devcluster the job stalls at pending - Insufficient: RAM in http://192.168.33.7:8081/scheduler/heronuser/devel/ExclamationTopology.  Was it necessary for you to increase Vagrant memory to run successfully, if so we should add directions?  It may be my machine, thanks!\n. @pulashti thanks for confirmation, I have an older Mac so that must be the reason. @kramasamy - this PR looks good to me. We can later edit with memory directions for older machines and add explicit java_home and heron install directions, but overall the post is very helpful and can be merged as-is.\n. +1\n. @chris-pardy @kramasamy Thanks for heads up, the 'heron-spi.jar' artifact uploaded to Maven Central is generated by the Heron release script.  We will need to check the release process to include the IUploader file.  I'm discussing with Karthik to resolve, thanks.\n. Hi @leonardgithub - thanks for pointing this out, and you're right SNAPSHOT should be replaced by 0.14.5.  It usually is updated in the MVN pom file, but this must have been missed.  We will upload a new .jar soon, thanks for catching this.. done, new version uploaded for 0.14.5, thanks.. TODO's removed, PR updated resolving merge conflicts, thanks.\n. This assumed building from master or modified local then creating a new (local) branch gh-pages with -b, then push, but I agree, if we are using subtree then lets remove for now.\n. Output is a a dump (earlier I had your -F csv with piped massaging).  Intended use was a check on final error, then continuing to push to gh-pages manually.  \nIt feels like more automation is required, so it makes sense to close the PR and keep working at it.\n. @kramasamy - good point, sorry but I assumed the standard install path.  I'll rewrite using regex since Anaconda version is included in version string, similar to Bill's, thanks.\n. sounds good - i was ignoring the status code so we format the linechecker-out.csv as discussed, but i can simply use the status code and keep/remove formatted linechecker-errors.csv on result.\n. @billonahill make quits once it hits an error...i avoided this using -linkchecker ... but make simply sets $? to 0 (success).  taking this out causes make to halt, even when calling it with \"make linkchecker --keep-going\".  Ideas on this?  Command line yields $? = 1 as expected, but make halting seems to be a problem.  thanks!\n. @billonahill no problem, updated with consistent format, thanks.\n. @kramasamy using the \"sudo apt-get installer\" for libtool installs v2.4.2 but we state a requirement >= v2.4.6.  This requires manual install as @supunkamburugamuve outlines here.\n. @kramasamy @supunkamburugamuve should we add a comment that flags are required on Ubuntu only due to Bazel Sandbox issue?  This should go away for later versions of Bazel if we are allowed to upgrade from 0.1.2.\n. @kramasamy, pls comment to @billonahill as only \"build:ubuntu --ignore_unsupported_sandboxing\" was added to tools/.bazelrc and changes are as requested.  flags are always required until Bazel fixes their sandbox issue.\n. We've created a new \"heronusers.slack.com\" specifically for this issue, keeping slack in headers but preserving \"heronstreaming\" for devs, per @kramasamy \n. yup, that was my original thought too but \"javadoc\" is the original misspelled \"javadoc.md\" file referenced throughout the current site for legacy reasons.\n. done - removed slack\n. I see, I thought you were commenting on the path.  The text Docs seems standard (e.g., bazel.io), but changed to Javadocs per your comment.\n. --genrule_strategy=standalone \n--sandbox_debug \n--spawn_strategy=standalone\nwe discussed last night and removed --ignore_unsupported_sandboxing and left the others.  I'm not sure exactly which commands are needed for all build commands.  lmk if I should spinup an aws ubuntu to check, or if we should add these to bazel.rc as well.\n. @kramasamy @billonahill done, added to tools/bazel.rc and updated compile guide \n. final aws Ubuntu command checks are successful.\n. @billonahill @kramasamy  - slack is removed from site navbar, exists in .yaml only and not visible.\n. @billonahill yes, per @kramasamy, replaced getting started with docs/javadocs, following bazel.io\n. I recall having a problem when I first used {{% bazelVersion %}} that doesn't parse within the bash syntax.  Trying again just now it fails, parses as {@{@HUGOSHORTCODE-2@}@}.  I'll look into it further, but for tomorrow's update this hopefully suffices so users don't install Bazel 0.1.2?\n. @billonahill example of format?  i'm not sure how to proceed on this one, these are artifact jars generated by the Bazel release, thanks.\n. @billonahill check and exit added to .sh\n. 'maven-central' changed to 'maven', including .sh script and release-process.md docs\n. backticks added throughout.\n. link to uri updated.\n. modified three backticks to one throughout.\n. Empty line after headers added.\n. done.\n. Button syntax and list format added.\n. Re-wrote this per comments to be more thorough and added individual steps.\n. Revised with more descriptive directions to improve readability.\n. @lucperkins updated step #s and trailing header line, thanks for the catch.\n. @lucperkins @kramasamy periods added for consistency.  problem sentence modified to \"After running the above install script with --user flag, the heron-api artifacts are installed into the ~/.heronapi directory.\"\n. @billonahill thanks, maven format syntax modified to {artifact-name}-{artifact-version}\n. @billonahill done\n. done\n. done\n. Great point, done.  I searched other website/script locations requiring export and found it in Dockerfile, will update in future PR.\n. ",
    "supunkamburugamuve": "I also had to install the above tools in order to get the build working.\n. Yes, that seems like a good idea. Will create separate guides for Mac OS and Ubuntu. Also I think we need to refactor the existing compiling guide to suite the new format. Will work on that too.. \n. Separated the mac and linux guides. The current Linux guide is for 14.04, I'll try to get a 12.04 to see weather we need separate instructions. Do we need to test on Cent OS?\n. Thanks Ashvin. Fixed it.\n. The guide has sudo apt-get install libtool and apt-get install libunwind8 libunwind-setjmp0-dev at the top. Added the manual installation steps at the bottom as a precaution only if they encounter errors with the apt installed versions. In my Ubuntu installation this was not an issue, but @lewiskan pointed that this can be an issue in AWS (which I couldn't access). I'll test without gperftool manual installation.\n. Tested without gperftool manual installation and everything works great. Should we remove the last two manual installations of libtool and libunwind? \n. Thanks @kramasamy will try this out without the modifications.\n. @kramasamy, I think we can close this PR as this issue is solved with the mentioned changes.\n. Fixed the java styles. I was using the bazel build --config=ubuntu scripts/packages:binpkgs. Does it enforce the style checks?\n. After syncing with the master, the following error happens.\n0: [libprotobuf ERROR google/protobuf/message_lite.cc:123] Can't parse message of type \"heron.proto.api.Topology\" because it is missing required fields: id, name, state\n0: F0513 14:43:24.454993 11468 stmgr-main.cpp:70] Corrupt topology defn file\n0: * Check failure stack trace: *\n0:     @           0x52672a  google::LogMessage::Fail()\n0:     @           0x52865f  google::LogMessage::SendToLog()\n0:     @           0x52630f  google::LogMessage::Flush()\n0:     @           0x528f9e  google::LogMessageFatal::~LogMessageFatal()\n0:     @           0x40d3af  main\n0:     @     0x7f3a045d2ec5  __libc_start_main\n0:     @           0x40cdff  (unknown)\n0:     @              (nil)  (unknown)\n0: WARNING: Logging before InitGoogleLogging() is written to STDERR\n. Yes, it worked, I'll focus on the unit tests and Torque scheduler. Before torque I think I need to get the CentOs 6 build working.\n. Thanks Karthik, added some unit tests and scheduler is in a working state.\n. @kramasamy will add documentation. \n. @maosongfu Thanks for the feedback. Did most of the improvements suggested. Need to test with the localzk. \n. @maosongfu thanks, moved the code to new methods and tested with zk. \n. @maosongfu thanks. Changed the code accordingly.\n. @kramasamy and @maosongfu in what type of circumstances the executor can fail? \n. The concept of workers is absent in heron. In storm a worker can host multiple tasks. In Heron each task is executed as a separate process. AFAIK You can set the number of stream managers. This will determine number of containers you run. In these containers you'll spawn the individual processes for each task. \n. Thanks @billonahill, the answer in the link worked. \n. @vivilife are you running the build as root? If that is the case, it will be worth running it under a normal user and see.\n. Thanks @lucperkins. Fixed the documentation.\n. @kramasamy sure will add the documentation. \n. Thanks @objmagic. @kramasamy added the documentation.\n. Added a test for configuration.\n. @chgl Thanks for the pull request. Could you update the sample.yaml with the new configurations as well?. @kramasamy changes are looking great. thanks @chgl . Working on the test error. Yes, it is ready.. I'll revert these changes from this PR as they are not relevant. These three lines gives an error when creating the Ubuntu Intellij files.\n. In HPC systems, the home directory is a shared one. Usually these have a single large disk shared across all the machines with NFS mounts. The home directory of the user is shared across all the nodes. Some HPC systems have local hard disks attached to them but they are usually mounted on different locations.\nI guess the issue is weather Heron can work in a shared directory across all the nodes. If not, we can create a separate folder for each node and copy the files.\n. Will check this and update the guide. \n. Changed the documentation\n. Will have a look in to this..\n. The API provided is same as local fs. NFS mounts the remote directory in the local file system. \n. This one is following the style in the Local Scheduler which has the same pattern where the keys are defined in a file as well.\n. it is a blocking call, but it won't block and will terminate immediately after submitting the job.\n. I missed that one in my current branch. Will follow the new style.\n. Is SchedulerUtils the right place to move this code?\n. @kramasamy what is the preferred way to put the default value of a configuration when we remove the defaults.yaml? \n. We are expecting HPC users to run streaming jobs as short jobs in a shared infrastructure. These systems are reliable so lets assume if the job fails, user will run it again. Our system admin is working on getting some dependencies done on a real cluster. Once it is up I'll test more into the requirements.\n. For now lets assume this functionality is not needed in this Context. If a job fails due to other errors (these are rare)  in a HPC environment, we usually kill the job and submit a new one. Right now I don't see a good way to implement this. Need to come up with a design on how to implement this as this can be little tricky.\n. the two methods are different in few places.. I tried to do it.. but it seemed little messy.. so I created a new method for now..\n. Thanks @ashvina. It should be options. Changed the code to reflect that. \n. ",
    "congwang": "For C++ code, we don't use thread except in unit tests. So not sure how useful this check is for these unit tests.. @kramasamy Any update on this?. Did you test it on Mac? On linux, it is much better:\n$ ./test-hash \n0 :: 6\n1 :: 5\n2 :: 0\n3 :: 3\n4 :: 6\n5 :: 9\n6 :: 1\n7 :: 2\n8 :: 6\n9 :: 4\nGcc implements std::hash using Murmur hash.\n. @kramasamy We can always add more in the future if we need. ;)\n. Looks good. \ud83d\udc4d\n. I think you need to handle the OOM exception from unpackMessage() instead of checking the bytes, it needs to be handled anyway regardless to size, in theory OOM could happen even for allocating 1 byte.\nAlso, why 131072? The max IPv4 packet size is 65536 (needs GRO).\n. Looks good to me.\n. This should be addressed by https://github.com/twitter/heron/pull/1541\n. @ajorgensen How much does the following one-line fix help?\n```\ndiff --git a/heron/common/src/cpp/network/mempool.h b/heron/common/src/cpp/network/mempool.h\nindex a8467df..875e0ac 100644\n--- a/heron/common/src/cpp/network/mempool.h\n+++ b/heron/common/src/cpp/network/mempool.h\n@@ -75,6 +75,7 @@ class MemPool {\n     }\n     B* t = pool.back();\n     pool.pop_back();\n+    pool.shrink_to_fit();\n     return static_cast(t);\n   }\n```\nI still can't reproduce it locally with AckingTopology.\n. Just in case, make sure you didn't change the default config for tuple cache:\n```\nThe frequency in ms to drain the tuple cache in stream manager\nheron.streammgr.cache.drain.frequency.ms: 10\nThe sized based threshold in MB for draining the tuple cache\nheron.streammgr.cache.drain.size.mb: 100\n```\n. The stream manager will send back pressure to the spout in this case.\n. @ajorgensen I am still trying figure out how to build your topology. I guess we need the following fix:\n```\ndiff --git a/heron/common/src/cpp/network/connection.cpp b/heron/common/src/cpp/network/connection.cpp\nindex c03ea8d..90cfbf3 100644\n--- a/heron/common/src/cpp/network/connection.cpp\n+++ b/heron/common/src/cpp/network/connection.cpp\n@@ -240,6 +240,8 @@ void Connection::handleDataWritten() {\nsp_int32 Connection::readFromEndPoint(sp_int32 fd) {\n   sp_int32 bytesRead = 0;\n+  if (mUnderBackPressure)\n+    return 0;\n   while (1) {\n     sp_int32 read_status = mIncomingPacket->Read(fd);\n     if (read_status == 0) {\n```. @ajorgensen I am trying it, definitely not sure yet, when back pressure happens, spout should already stop sending data, only the pending data is still being transmitted. At least it could help us to rule out the case.. I don't have a chance to test it in data center yet.. \ud83d\udc4d . Yeah, this one can be closed as a DUP. Right, the original back-pressure-related memory issue is addressed by https://github.com/twitter/heron/pull/1785.\nAbout this PR itself, the mempool does not contribute to memory pressure, we will hit the hard limit anyway no matter we have mempool or not, the problem with mempool is that it may hit the soft limit eventually since this memory never shrinks.. @billonahill I think the API and ABI compatibility is a minimum requirement for protobuf as a library. I didn't touch the new incompatible API under proto3 (unlike PR #1217 ).. @billonahill I am confused, we don't use proto3 even with this PR, so why should I verify proto3 compatibility? I explicitly specify proto2 in the code actually.. @billonahill protobuf-3.x has proto2 which is compatible with protobuf-2.x, and also proto3 which is an incompatible API. We only use proto2, so I don't think there is any incompatible issue.. This failure is caused by the sleep(3):\nhttps://github.com/twitter/heron/blob/557c9804af0c23952b3df7e29715f01e338884f8/heron/stmgr/tests/cpp/server/stmgr_unittest.cpp#L1524\nFor some reason, within 3 seconds, we still don't get a new physical plan, thus fail.. @kramasamy Waiting for @billonahill's feedback.. Looks good. What's the difference between single-quote and double-quote in Bazel?. \ud83d\udc4d\nThanks for update!. stdlib/strtol_l.c is from glibc, not heron,  so this is really strange. And many symbols are missing in the stack trace. What's the version of glibc on those hosts?. Yeah, it makes sense now. Stmgr fails here:\nvoid StMgr::CheckTMasterLocation(EventLoop::Status) {\n  if (!tmaster_client_) {\n    LOG(FATAL) << \"Could not fetch the TMaster location in time. Exiting. \" << std::endl;\n  }\n}\n. @ajorgensen Can you share the log of that stmgr?. Nice analysis! If I understand correctly, the following should fix it?\n```diff\ndiff --git a/heron/common/src/cpp/zookeeper/zkclient.cpp b/heron/common/src/cpp/zookeeper/zkclient.cpp\nindex a833f2e..0481273 100644\n--- a/heron/common/src/cpp/zookeeper/zkclient.cpp\n+++ b/heron/common/src/cpp/zookeeper/zkclient.cpp\n@@ -198,6 +198,8 @@ void ZKClient::CreateNode(const std::string& node, const std::string& _value, b\n                           VCallback cb) {\n   sp_int32 flags = 0;\n   if (_is_ephimeral) {\n+    zoo_adelete(zk_handle, _node.c_str(), -1, VoidCompletionWatcher,\n+                CreateCallback(this, &ZKClient::ZkActionCb, std::move(cb)));\n     flags |= ZOO_EPHEMERAL;\n   }\n   LOG(INFO) << \"Creating zknode \" << _node << std::endl;\n```. @huijunw The benefits are their difference, that is the order, it speeds up the fast path, search, from O(logN) to O(1) (average case). You probably want to read their documents for details:http://www.cplusplus.com/reference/unordered_set/unordered_set/. @avflor By number of tuples, does it count all kinds of tuples? Ack tuples, fail tuples too?. @objmagic IntegrationTest_MultiSpoutsMultiTasks fails.. There is a new CI failure:\n[ RUN      ] StMgr.test_tmaster_restart_on_new_address\nsrc/central_freelist.cc:333] tcmalloc: allocation failed 24576 \nUnknown exception caught in global handler. @avflor \nAnswer your four questions together:\n1) I use active_instances_, so these instances should be alive. But yeah, I should cleanup the metrics map in HandleConnectionClose() too, will add it.\n2) You probably want to look at PR #1397, where I added it.\n3) tuples_size() should return the number of tuples, not bytes.\n4) This is exactly why I asked you if this is what you want. If you just want to count the number of tuples sent to each instance, this PR already does so. If you want the number of tuples pending in the queue, it is harder, because conceptually it does not fit well: tuples are packed into packets before putting into the queue, there are no longer tuples in concept at the lower level, so harder to count the number of tuples leaving the queue. One solution would be packing \"number of tuples\" into packets too, but again, it needs more work.\n. This is caused by my back pressure code although I didn't touch HandleConnectionClose(), it is somewhere else.. The C++ part looks good to me.. @ashvina I must miss something here... When you add them up, the max should be N*60k in a 60 second interval, where N is the number of instances you add. No?. What's the status of this issue? I saw it twice this week:\n20170321183034_IntegrationTest_OneSpoutBoltMultiTasks_98f78c1a-21eb-42f1-8116-b0b65f47c1e8. The last time I saw this it was a version mismatch, I changed the version in maven file and recompile the topology code.. @huijunw The motivation is not clear. Our current back pressure algorithm inside stmgr is kinda auto-healing too: it throttles the spouts in hope to get recovered from back pressure situation (and unthrottles them after recover). I guess you are trying to handle some unrecoverable situation like network partition? But I think in this situation Aurora (or other scheduler) should be able to detect and do its own auto-restart. So, why do we have to do it inside Heron?\nAlso, it needs to mention that our back pressure algorithm is improved a lot: https://github.com/twitter/heron/commits/cwang/backpressure, it should do better than before.. @maosongfu Yeah, there are certainly many unrecoverable cases, but I am still wondering how restart could help in such situations. Auto-restart sounds like saying \"we are not lucky this time, let's try it again\", it can never guarantee we could totally solve the problem. IOW, this does not look like a real solution, it looks more like a workaround. For me, I think the users should think about how to adjust their logic in such situations rather than just restarting, which means for Heron we probably need to provide some API(s) to users in back pressure situation and let them to decide what to do in their logic.. @maosongfu We all want to help customers to solve their problem, no doubt, but auto-restart is not solving it. They want auto-heal, you give auto-restart, auto-restart can't guarantee a healing.. @avflor It helps only because you are luck enough after a restart, no other reasons. We don't want our solution to depend on luck, do we?. @kramasamy You do need a lot more information to decide where is a good place to reschedule the container if you still prefer this way, and I doubt you could obtain enough information from Aurora, instead of relying on the container scheduler, why not do it by ourselves? IOW, why not consider adding more resources for slow containers to speed them up? This is a good direction, we already have scale up/down in our code base (thanks for Bill).\nMy back pressure code could improve the current algorithm, but can't solve all of the situations, for example if the resources are naturally the bottleneck back pressure will consistently happen no matter how we improve the algorithm (unless we drop).. Looks good. Thanks!. @billonahill The new one is not any harder to understand, we still throttle things, just that we only throttle relevant things in the path, in a smart way. The reference counting might be slightly harder to understand, but think about the cases where paths overlap, it is natural to have.\nThe logging is extended to include task id rather than just stmgr id, to reflect this change. But if you feel we need more logging in some places for trouble shooting purpose, I am very happy to add (actually I removed some debugging logs before sending this PR).\nThe metrics should be the same as before, instance_metric_map_ should include both bolts and spouts. People could figure out if a bolt is under back pressure with this metric.\nI don't understand your question about bolts emitting tuples, this totally depends on their code, right?\nI am not sure about the doc, perhaps need to add a page under website/content/docs/ ?. @billonahill I will open a separate issue on github for the doc, since my English writing skill is not good at all, I expect someone else could write it.\nWe do have metrics for back pressure initiater, METRIC_TIME_SPENT_BACK_PRESSURE_INIT and METRIC_TIME_SPENT_BACK_PRESSURE_AGGR too. They serve the purpose you ask for here. Unfortunately, we don't have a way to show the path.\nOf course we should not throttle downstream ones, otherwise no one will consume the packets, deadlock!! ;-). @billonahill I believe I already clarify the high-level overview of the new algorithm in the description of this PR. If anything not clear, please point it out, I am happy to add more. BTW, I don't think we should cover any code details in description.. @huijunw It is not a guess, the topology is in pplan, it is 100% accurate. FindBusiestTaskOnStmgr() is the best effort we can do to find out which one to blame, but we can't predicate the future, if we blame a wrong one with this, the next time when BP is triggered again we could catch up.. @huijunw Don't waste your time on this... People do love crash here.... Looks good.. I am a bit surprised glog doesn't provide an API for ratelimit logging.. @objmagic Oops, I mean java.util.logging... My point is it is ugly to expose rate limit details to it caller.. @billonahill Agreed. My Java knowledge is almost none, so I don't know if it is hard to just extend java.util.logging.Logger.. This is still ugly. The memory leaks are leaks, adding a limit is just adding a cap to these leaks, doesn't resolve them at all. How would you expect users to figure out how many messages they actually need?\nThe real solution here is probably to explore the new protobuf arena API to see how we can integrate it with mempool. But that requires protobuf 3.0 at least, fortunately I already have this here: https://github.com/twitter/heron/tree/cwang/protobuf30. I think you can break this loop and skip the following loop once find it.\n. Removed.\n. This needs to be removed.\n. This should be named as \"out_tasks_\".\n. No need 'inline' explicitly, inline is default here.\n. Should be removed.\n. No 'inline'\n. Use ::pipe().\n. No need to check the value, we only have one reader and one writer.\n. 'sent' is no longer needed either.\n. This patch already exists for 2.5.0, I just regenerate it for 3.1.0. The reasons why we need it are:\n1) The C++11 version is faster, it saves an if check on super hot path.\n2) We compile our stream manager with C++11, so it is safe to just remove the macro\n3) I don't see any configure parameter to enable it, so we have to patch the source directly.\nNot sure if it is really worth a README, since we don't change the patch often. It is generated manually actually.. Basically based on the bazel build errors, if some of them is missing, bazel complains. I don't know any other better way to determine this.. Will do it in a separate PR.. It's hard to define a time here, b/c we never know if the message is lost or just delayed. I think relying on CI is fine, otherwise still have the same failure.. Can you give a reasonable timeout? As in the current code, 3-second was supposed to be reasonable, but it is not... So I don't know what is a reasonable one.. Makes sense, it is safer to use a longer timeout.. If you want to allow multiple interfaces, please rename it to INTERFACES (singular vs plural). And I am not sure how much sense it makes to support multiple interfaces here, could you explain?. Ditto, interfaces_.. So where do you split the list of interfaces?. Why this unix sock addr?. I don't understand the logic here, mind to explain?. I think it is cleaner to just use:\nproto::stmgr::StartBackPressureMessage* message = nullptr;\nmessage = acquire(message);\n. People should read the definition of acquire() and then figure out the type deduction. So I don't think it is a problem.. Yeah, it makes sense to add a counter for dropped packets in another PR.\nThere is a real issue in our DC, the cause of the corruption is unknown yet.. I am not aware of any C++ syntax for this, we may achieve it by a wrapper class or something like that, but I doubt it is worth the effort to improve it. Does it hurt readability to you?. There is always some way to do it. Again, why do you think it is worth our effort to do it? For me, we only have one case so far, no need to bother a complex way to make it unnecessarily elegant. If you believe it is worth, you are welcome to do it in a separate PR.. Will add it.. It is just a copy of PROCESS_METRICS_FREQUENCY. I understand you may suggest to expose it in config file, but we already have too many config's.... Because we don't need it any more, as I already describe in this PR, we move the code out of hot path. Good point. It does not seem to be standard even for C++.. iter points to an element which is a pointer, so iter is the pointer, delete iter means deleting the contents pointed by that pointer, it is fine.. We can always add it back if we need it in the future, so I don't worry about it for now.. Because it comes from std::allocator but returns to mempool so never goes back to std::allocator again.. It is unused, I don't know why an unused parameter contributes to generality. Mind to be more specific?\nAgain, we can always add it back if we need it in the future, it is pretty easy to do with git. . Good point. Never know this API before.... This is not my code, it shows up as diff context... Also, the 100M there might mean 100 million rather than 100 megabytes.. Yeah, we can do it in a separate PR, because there are multiple places using the same.. We have to, because the stmgr unit test cases don't want to read from config, they set these values by themselves.. 1) StMgr() doesn't accept NetworkOptions as parameter, max_packet_size is passed in NetworkOptions for those who accept it.\n2) stmgr unit tests want 2 different values in different tests, so you have to provide at least 2 different configs if you really want to read from config. Not to mention other config's.. Done: https://github.com/twitter/heron/issues/1758. Yeah, or change it to 'megabytes'.. If you want to interpret literally, yeah.. We could do that but need to move 'total_time_msecs_ = 0' down.. Please read the description of this PR: \"The only thing needs to note is that template parameter pack must be the last so we have to reorder the arguments for callback1.\". Use std::to_string().. Use 'auto'. Will add it.. Hmm, actually we already have a log for stmgr-id:\nLOG(INFO) << \"Got a register stmgr request from \" << stmgr_id << std::endl;\nSo I don't think we need to log it again.. @objmagic Why not turn on -B for your grep? ;). to_string(). @maosongfu It is right before this log, I don't know why it is hard for you to find it... At least I don't rely on one-line log for debugging, I read its context too.. No need c_str() here, operator<< works well with string. No need newline at the end.. What makes you think a += operator is clearing it?? I am totally confused.... Oops, replied too fast. Good point, we need to clear it after a backpressure is triggered.. Sure.. If instance_stats_[_stmgr_id] does not exist that means we don't have any traffic to that stmgr, therefore impossible to trigger a back pressure.. @huijunw That should be handled in HandleConnectionClose().. @huijunw I already noticed the test case failure, will fix it.. ",
    "tysonnorris": "Hi - Is there a Dockerfile available to match the built image in docker hub? I need to use a local build, and would rather not stumble through recreating one that might already exist.\nThanks\nTyson\n. Hi @joestein - is there a new PR coming for this? We are looking for a mesos native (framework) scheduler to test out on (non-aurora) mesos clusters. \nedit: oh I see above some mention of work in progress at https://github.com/elodina/heron - if you want additional help, I can take a look (need an invite).\n. I had the same problem running docker/build-artifacts.sh centos7 master ~/centos\nI took a stab at fixing it - see the PR\n. Hi @aShevc - in this case (where submit can become a recover), who would invoke the heron submit recover? Couldn't that submitter could end up clobbering a running topology that did not need recovering? \nI'm just starting to look at MesosScheduler, and was expecting more of the previous design (\"had a high level Mesos scheduler for launching an arbitrary tasks\". I think the reason the other launchers do not have this design is that they depend on other frameworks that expose APIs for allowing some additional semantics around creating, monitoring, and recovering mesos(or slurm, etc) tasks. \nDo you have more info on how you plan to support starting the scheduler under supervision or running heron submit on a different mesos scheduler? \nThanks!\n. @aShevc Thanks for your response - can you elaborate on \"the way to go here was actually using Marathon, feeding it the \u201cheron submit\u201d command\"? On the surface it looks like the submit command returns immediately, so I'm not following what you expect marathon to do with it? Maybe this is what you are getting at missing in the Mesos scheduler docs.\n. Sure, CLA accepted!\n. Would it be possible to instead:\n- build the image for running tracker or ui as part of the regular build process? e.g. a Dockerfile that moves the /dist files into place from the build, I think a separate image (from compiler image) will be better to get a smaller image, otherwise it could be created from a docker commit.\n- avoid using .bashrc to launch daemons, rather launch them manually with separate docker run commands? Or else, use an init system if there is some reason the daemons need to run in the same container. This way conventions like logging to stdout etc can be managed more easily.\nI can submit examples if that is helpful (to this or a separate PR).\n. What I would typically do for the case of starting heron-tracker and heron-ui is use docker-compose. This only requires creating a docker-compose.yml file, and using \"docker-compose up\" to start instead of \"docker run\". docker-compose effectively gives you a simple way to cause multiple docker containers to start/stop using a single command, which allows you to have a convenient local system (e.g. start multiple procs with a single command), as well as still use the \"one process per container convention\". It also keeps things a bit simpler than using an init system, but it can be complicated IF there are \"startup order\" issues, but I don't think that applies here.\nI'll submit a separate PR for this to avoid any confusion. \n. OK, created this PR to provide a different approach to dist build and docker-compose example local env: #971 \n. @ajorgensen good point; I can do the other dockerfiles too, but maybe in a separate PR\n. @ajorgensen I cleaned this up a bit, mostly so that the inter-container networking is tidier and the readme is more correct.\n. Yes, I was trying to make this consistent-ish with the other scripts there. If there is a more specific script style I can adjust it.\n. ",
    "amontalenti": "Some notes on this:\n- We have two projects, pystorm and streamparse. The former is an implementation of basic APIs for bolts/spouts and implementation based on Storm's multi-lang protocol. The latter is the actual framework for managing \"projects\" with multiple topologies that are written using the pystorm API.\n- As of our very-recent 3.0.0+ release, topologies themselves are specified using a Python Topology DSL, based on thriftpy to serialize the topology into a Thrift Topology struct. We serialize that by embedding a copy of storm.thrift inside streamparse, as shown in this code.\n- You can read some docs on the way the Topology DSL works here.\n. Hey! One of the streamparse original creators here. This is looking really interesting so far! I added a comment on #574 about recent happenings in pystorm & streamparse. But I agree with the approach you've taken of building your own pyheron interop layer and then just matching the duck-typed API that streamparse's Topology DSL and that pystorm components implement.\n. ",
    "caniszczyk": "There are a handful of choices for open source foundations for Heron. Last year, we discussed potentially the Apache Software Foundation as a choice and sketched out an initial proposal here: https://github.com/heronproject/apache-proposal\n. Looks like this is happening at the ASF so this issue can be closed IMHO: https://wiki.apache.org/incubator/HeronProposal. I'm sure @kramasamy can add more in this list, but it's just a start, it's more important we have a location to adopters to be listed :)\n. woooo! but I can't access that google doc cc: @kramasamy \n. ",
    "innerzeal": "+1 to apache.\n. ",
    "bradbiddle": "The ASF proposal looks pretty compelling to me, FWIW, but some other options that may not be on your radar:\n- Joint Development Foundation  provides an easy and inexpensive structure for projects to launch under an independent non-profit framework (disclosure: I'm on the Board)\n- Launching as an independent non-profit entity is simpler and cheaper than most people think, particularly if you use an association management firm to handle the details, and gives you lots of control of your own destiny. \n--Brad\n. ",
    "paralin": "Interested in doing this, will see if I can get it working\n. I plan to work on this in the coming weeks, not quite to the stream processing part of my project yet, but when I get there, I'll likely build a Kubernetes integration for Heron if nobody else has yet.\nWith Kubernetes you have a few tools that could make Heron work nicely. It seems you have pluggable schedulers in Heron which is ideal for Kubernetes, really all you'll need to do is write some code to interface with the Kubernetes API. In every Kubernetes container on default you get a file with a token used to authenticate against the API at 10.0.0.1. From there it's fairly easy to implement your own scheduler logic for pods.\n. ",
    "moomou": "Hello. I am interested in this. Please let me if there's anything that needs help. \n. @kramasamy How would we like to do the initial meeting? I imagine a video group chat would be really hard given everyone's schedule.\nWould listing tasks in bullet points here be more achievable? Haven't jumped in yet, btw.\n. Will do this tonight. \n. Updated with the suggested changes. \nI clicked on the CLA link and submitted. \nLet me know if I missed anything.\n. Yep.\n. I will start to look into this. \n. I opened a PR here: https://github.com/twitter/heron/pull/1525\n@prabhuinbarajan  @kramasamy Please provide feedback on if that is inline with what we are imagining. \nOne thing that is ostensibly missing is the management of persistent volume. I am familiar with Google cloud and AWS provisioning, but not sure what should the default k8s templates assume.\n. Sorry about the confusion, I opened this with the idea of gathering feedback, but I suppose it wasn't clear what I was asking for.\nBasically, \n1) Is the location where I am putting these files good? I am proposing (with this PR) to make heron/config/src/yaml/conf/k8s/ for all k8s related things sans source code (ie scripts, configs, etc)\n2) Is the choice of zookeeper image acceptable (https://github.com/fabric8io/fabric8-zookeeper-docker)? \nIf there are no major objections, I consider this PR to be basically done. Just need to integrate it when a kubernetes controller is ready.\n. ",
    "waitingkuo": "Will the discussion or development be transparent? Sorry I'm not familiar with Heron. I was a Storm user and I'm currently trying to port other frameworks to kubernetes now. Quite interested in your approach, would be a good inspiration when I port other frameworks. Got the chance to audit or contribute?\n. In this docker-compose file, to share .herondata among multiple containers, that directory is mounted as a docker volume. So it's hard create the directory when installing. One quick hack to the docker-compose is wrap creating dir & running heron-track as one command when running heron-track container:\ntracker:\n    image: ${HERON_IMAGE}\n    command: bash -c \"mkdir -p /root/.herondata/repository/state/local/topologies; heron-tracker\"\n    ports:\n      - \"8888:8888\"\n    hostname: tracker\n    networks:\n      - heron\n    volumes:\n      - \"herondata:/root/.herondata\"\nThis works but I believe we need to find a better way to do so.\n. @kramasamy - Just checked the code. \ntopologies = filter(     \n    lambda f: os.path.isfile(os.path.join(topologies_path, f)), os.listdir(topologies_path))\nIt intends to collect the topologies every few seconds. How about modifying as\ntopologies = []\nif os.path.isdir(topologies_path):\n    topologies = filter(     \n        lambda f: os.path.isfile(os.path.join(topologies_path, f)), os.listdir(topologies_path))\nIf the path isn't existed, just returns a empty array.\nCould you give me a hint that how I build this tool via Bazel without building all the codes.\n. docker-compose works normally after adding this\n. @kramasamy  sent, #1312 \n. @objmagic @billonahill just signed\n. It's copied from Dockerfile.dist.ubuntu14.04. I'll remove the last line and build again to see whether it works or not\n. /build-artifacts.sh use Docker.ubuntu16.04 to compile and output artifacts.\n/build-docker.sh use Docker.dist.ubuntu16.04 and build-artifacts.sh's output (artifacts) to build docker heron images.\n. Duplicated one is deleted\n. ",
    "nicknezis": "Any updates?. Awesome thanks!\nOn Fri, Jun 16, 2017, 10:17 PM Karthik Ramasamy notifications@github.com\nwrote:\n\n@nicknezis https://github.com/nicknezis - initial version of heron in\nk8s is already in 0.14.9. We are adding documentation and tutorial in the\nupcoming version. Please check #1946\nhttps://github.com/twitter/heron/pull/1946\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/617#issuecomment-309186876, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AALGqnWuGxxKT4sv226TP4PBXrrkSmbxks5sEzdMgaJpZM4IawyH\n.\n. The issue I was fixed was resolved by another pull request. Closing this pull request.. Is this only needed if you install in a non standard location? Aren't brew binaries installed in /usr/local/bin which should already be on the PATH. What is in your /etc/paths file?. \n",
    "srkukarni": "This has been subsumed by https://github.com/twitter/heron/pull/1970\n. Is this Mac or Linux?\nOn Tue, Jun 13, 2017 at 6:07 AM, jrpspam notifications@github.com wrote:\n\nmy topo gets in this state within matter of hours and is emitting only 10\ncharacter strings. I seem to permanently lose connection to stream manager\nuntil restarting the topo. Any ideas would be great. I'm trying to evaluate\nHeron and do not see any reason for the failures\nSEVERE] com.twitter.heron.common.network.IncomingPacket: channel.read\nreturned negative -1\n[2017-06-12 21:14:03 +0000] [SEVERE] com.twitter.heron.common.network.SocketChannelHelper:\nSomething bad happened while reading from channel: /127.0.0.1:60947\n[2017-06-12 21:14:03 +0000] [INFO] com.twitter.heron.common.network.HeronClient:\nHandling Error. Cleaning states in HeronClient.\n[2017-06-12 21:14:03 +0000] [INFO] com.twitter.heron.common.network.HeronClient:\nSuccessfully closed the channel: java.nio.channels.SocketChannel[closed]\n[2017-06-12 21:14:03 +0000] [SEVERE] com.twitter.heron.network.StreamManagerClient:\nDisconnected from Stream Manager.\n[2017-06-12 21:14:03 +0000] [INFO] com.twitter.heron.network.StreamManagerClient:\nClean the old PhysicalPlanHelper in StreamManagerClient.\n[2017-06-12 21:14:03 +0000] [WARNING] com.twitter.heron.network.StreamManagerClient:\nError connecting to Stream Manager with status: CONNECT_ERROR, Retrying...\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/932#issuecomment-308109604, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACabuuvYqvWd19JgGIgSDVIb6S7p_Vgaks5sDomSgaJpZM4I3inv\n.\n. What scheduler are you using? Does it happen to all topologies or just some\nselected ones?\n\nOn Tue, Jun 13, 2017 at 12:42 PM, jrpspam notifications@github.com wrote:\n\nthis is a Ubuntu m4.xlarge EC2 AWS instance. I think I am having trouble\nunderstanding how to manage/config mem usage though not 100% clear where\nto look\nOn 6/13/2017 11:08 AM, Sanjeev Kulkarni wrote:\n\nIs this Mac or Linux?\nOn Tue, Jun 13, 2017 at 6:07 AM, jrpspam notifications@github.com\nwrote:\n\nmy topo gets in this state within matter of hours and is emitting\nonly 10\ncharacter strings. I seem to permanently lose connection to stream\nmanager\nuntil restarting the topo. Any ideas would be great. I'm trying to\nevaluate\nHeron and do not see any reason for the failures\nSEVERE] com.twitter.heron.common.network.IncomingPacket: channel.read\nreturned negative -1\n[2017-06-12 21:14:03 +0000] [SEVERE]\ncom.twitter.heron.common.network.SocketChannelHelper:\nSomething bad happened while reading from channel: /127.0.0.1:60947\n[2017-06-12 21:14:03 +0000] [INFO]\ncom.twitter.heron.common.network.HeronClient:\nHandling Error. Cleaning states in HeronClient.\n[2017-06-12 21:14:03 +0000] [INFO]\ncom.twitter.heron.common.network.HeronClient:\nSuccessfully closed the channel: java.nio.channels.\nSocketChannel[closed]\n[2017-06-12 21:14:03 +0000] [SEVERE]\ncom.twitter.heron.network.StreamManagerClient:\nDisconnected from Stream Manager.\n[2017-06-12 21:14:03 +0000] [INFO]\ncom.twitter.heron.network.StreamManagerClient:\nClean the old PhysicalPlanHelper in StreamManagerClient.\n[2017-06-12 21:14:03 +0000] [WARNING]\ncom.twitter.heron.network.StreamManagerClient:\nError connecting to Stream Manager with status: CONNECT_ERROR,\nRetrying...\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/932#issuecomment-308109604,\nor mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/\nACabuuvYqvWd19JgGIgSDVIb6S7p_Vgaks5sDomSgaJpZM4I3inv\n.\n\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/932#issuecomment-308147514,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/\nAbpRuyGtpGkZWXHDFe9Sq0pus_BweELGks5sDqXggaJpZM4I3inv.\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/932#issuecomment-308226440, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACabuiuQcGRTDT9ZzEzmywXPkQfaBHJGks5sDuYrgaJpZM4I3inv\n.\n. I think whats happening is that when you manually tried to clean things up, things weren;t completely cleaned. Suggest doing a kill once again, make sure that no heron processes are running(tmaster/stmgrs/instance) and then cleaning up ~/.herondata/ and restarting. Is this pr still valid?. @rohanag12 This is slated for the 0.15.3 release. . BTW any users of Heron can use proto2 or proto3. This ticket is just for the internal implementation of certain Heron components and does not have any ramifications from user perspective.. This pr breaks so many abstractions all across the board(too many to list). Please do not merge it. Thanks!. @maosongfu Regardless of whether it fixes the bug that we saw, this itself is a memory leak that needs to be fixed.. I've removed IStatefulSpout/Bolt interfaces as requested. Also made State typed.\nPlease take a look again. Thanks!. My refactoring is ready, but unfortunately things are being blocked by oss\nmigration :)\n\nOn Tue, Jun 13, 2017 at 2:17 PM, Huijun Wu notifications@github.com wrote:\n\n@huijunw commented on this pull request.\nIn heron/common/src/cpp/metrics/metricsmgr-client.cpp\nhttps://github.com/twitter/heron/pull/1852#discussion_r121799281:\n\n@@ -63,6 +63,12 @@ void MetricsMgrClient::HandleClose(NetworkErrorCode) {\n\nvoid MetricsMgrClient::ReConnect() { Start(); }\n+void MetricsMgrClient::SetPublisherPort(const sp_int32 port) {\n+  if (0 == port && 0 != _port) {\nWe saw this test failure a lot and we suffer from it. Do you have any ETA\nwhen your refactor could be merged?\nHow about I fix this test failure first and then later align to your\nchanges when your refactoring is available? Will create an issue for\ntracking this\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/pull/1852#discussion_r121799281, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACabugj5gWXA6uwrB-YmbSBOC-BvAOVMks5sDvxsgaJpZM4NXynT\n.\n. @nlu90 local file system state manager does not support watches, so thats probably not the right method to reproduce this. Try localzk instead.. @jrcrawfo please review. @objmagic This is just the C++ api. Mostly interfaces. Integration tests will come after the cpp-instance is merged and examples are created.. +1\nOn Wed, May 24, 2017 at 7:31 PM John Crawford notifications@github.com\nwrote:\nI really like the idea of including the the core binaries in the image,\nbut the Dockerfile you used to put the libraries in there should be made\navailable to the user and built into the build process. Otherwise, it just\nseems very disconnected and when a new release is cut, there's no process\nfor getting that new release built into a new docker image to be ran.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/pull/1894#issuecomment-303812041, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACabuk02xMPEanDFCaWyY-19yj4HfNNuks5r9Hd3gaJpZM4Nkp9s\n.\n. Solved by #2089 . @maosongfu updated pr.. The submitter main actually does nothing with this argument. All the packing of the defn/bin happens inside the cli before calling the submitter. The submitter merely copies this as the arg to the executor.. Disagree with that. Infact this looks like a bug to me. \nSchedulers should not be doing this truncating business. It should be done at a base layer(which is cli). Otherwise we will have the same situation like the one we have today where some schedulers handle it properly and others dont. For instance, aurora handles it fine(does a basename, and thats pretty much the only thing that it does). The others(including LocalScheduler) do not do that. Thus for instance if you submit a tar topology on localscheduler, it probably does not work.\nIs there a strong reason why schedulers would want the absolute filename?. I believe that comment needs to be changed. And there should be no reason why the scheduler needs fq path. Please let me know why would the scheduler would need it. . Ultimately the scheduler abstraction should be as focused as possible. It needs to get bare minimum things a) topology contents and b) parameters affecting scheduler like number of containers and container description(ram/cpu, etc). Ultimately the scheduler's main job is to just act as the interface to the actual scheduler implementation to fire off the containers and start the executor on each container. The rest should be handled outside.. Great that we agree that we need to do this at a central place :)\nOnly debate now is where. . So here is the interesting part. The cli already does a basename conversion while preparing the topology.tar.gz. See https://github.com/twitter/heron/blob/master/heron/tools/common/src/python/utils/config.py#L61\nMoreoever the submitter main at any point does not require fq path of either the defn file/topology binary. How about consolidating all of that within cli and cleaning all basename stuff in all schedulers?. Also note that none of the config files are really available to the scheduler. The cli bundles all of them inside the topology.tar.gz.. @billonahill I thought we already can get both jstack/jmap information from heron-shell. Could you please explicitly specify the capability that you need?. This is very inconsistent behaviour. Why do we have things for one scheduler and not for other?. But don;'t we already have ways to add more parameters to the jvm?\nTOPOLOGY_WORKER_CHILDOPTS and TOPOLOGY_COMPONENT_JVMOPTS. You are right that we don't have any way to pass instance specific command line params, but by defn the instance id/task id not something known a priori and thus this cannot be part of api per se.\nHaving said that isn't the purpose of heron-shell to do these kind of profiler/debugger attachments?\nI also miss what you mean by 'adding debug support to the submitter, runtime manager, and the scheduler-as-a-library'. A little more specific use would help me understand. Thanks! . There are two kinds of people.\n1) Heron Users, that is topology writers. For them the current TOPOLOGY_COMPONENT_JVMOPTS approach should work right?\n2) Heron Developers, that is people working to enhance Heron. Adding any kind of debugger for components like SubmitterMain, RuntimeManagerMain/SchedulerMain are aimed for this group. \nAm I to understand that this is ticket is mainly aimed at the second set? . We do not track src task id at tuple level but at batch level for performance reasons. . @objmagic We cannot remove that since a) that's topology defn file and b) scheduler currently needs to read topology_defn. \nFixed style error.. Made the changes wrt arg description based on feedback. Please take a look again. Thanks!. Please look at my prev comment about the zk node hierarchy. . Sorry, looks like i hadn't submitted my comments(damn github).\nAnyways, I really want the invariant to be maintained by the logic of the code rather than the is_master_ thing. I just checked the code, and it looks like we do not start serving until much later. Thus, I believe the following approach is better\n1. Upon start, make sure you are the master(just like the current logic).(Btw this avoids the is_master_ thingie).\n2. Upon becoming master, check for packing plan and make sure that it exists. \n3. Then continue with checking for physical plan and becoming master.\nThis way I believe you also reduce the size of this commit substantially . There are two steps here\n1. Becoming the master(when we write the /tmasters zk node).\n2. Serving as master(when we start the TMasterServer.\nIMO, 1 should be the first step in the logic of tmaster. 2 can follow anytime. So as soon as you become tmaster, you can start the fetching packing plan and only after that you can start serving requests.\nRight now we do not do 2 until much later(after physical plan fetch). So we will not get any requests from stmgrs. . So, what's the issue with stmgrs trying to connect and retrying? . The main issue is that we need to copy our keys to a machine that's facing the data center which may not be always feasible.\nThe specific difficulty is Heron's integration with kubernetics/dcos. Currently we need our keys on one of the cluster machines. In Kubernetic's case we could easily do a port forwarding using kubectl and thus no longer need to tunnel.. @billonahill tunnelling to localhost does not solve the problem. Because when we establish a tunnel to zk port from localhost, we still refer the zk as localhost:2181, but thats not what topology components inside containers can connect to. \n@maosongfu disabling tunneling is not the issue here. The issue is how the submitter is able to reach the zk  vs how the containers are able to.. 1. There will be a single zookeeper. So there is no data syncing issue at all. We are just talking about the 'hostname' of how different processes can reach the same zk\n2. That's essentially tunneling which needs to be setup and not always available. This is one of the main reason for this ticket\n3. This is a good observation. The zk cpp document says that it should work. Let me circle back with my findings. We tested 3 and it doesn't work.\nCurrently zk client(and curator framework) does not work even if one host in the ensemble is not reachable. There are various tickets to solve it(chiefly https://issues.apache.org/jira/browse/ZOOKEEPER-1576) but are not really short term.\nAn easy fix would be to add a CLI_CONNECTION_STRING variable in the statemgr.yaml thats default initialized to the normal connection string and have the scheduler-core pick that up.. @billonahill The only time we need to use the CLI_CONN_STRING is while manipulating using cli. Thus when the SubmittedMain starts up and loads config, thats when we use CLI_CONN_STRING as CONN_STRING. This change thus is explicitly done in SubmitterMain \n. We introduce a new optional variable called (and name could be changed) heron.statemgr.cliconnection.string in statemgr.yaml\nIn SubmitterMain we check for this variable and if it exists, override the config[CONNETION_STRING] to that before initing statemgr. . IIRC Tunnelling is only used currently when we need to talk to tmaster(for commands like activate/deactivate/restart/update).\nHere I'm talking about when we need to reach zk(like during submit/kill). Thus IMO tunnelling and this are orthogonal. @billonahill and I had a chat about this and here is what we are thinking of.\nCurrently heron supports two types of tunneling mechanism(socks, ssh port forwarding), but this isn't explicitly specified anywhere in the config and the bindings are somewhat hardcoded. We will clarify this by doing the following\n1. Just like what exists, heron.statemgr.is.tunnel.needed will be the decider switch to see if we need any kind of tunneling at all.\n2. We introduce a new variable called heron.statemgr.tunnel.type that defines the kinds of tunneling options. We will support SOCKS, SSH_PORT_FORWARD, LOCAL_PORT_FORWARD as accepted values\n3. We introduce a new variable called heron.statemgr.tunnel.port which is needed for the local port forwarding case.\n. Except for lack of unittestsm looks good. Added unittests. What are the performance implications of doing this? Do we want to do this only when DEBUG is set?. @objmagic One easy check that you can do is run the exclamation topology both with and without this change and see what kind of impact you see.. Just a note that in cases when there is heavy traffic, this will not stop the mempool from growing beyond a certain size. It will only ensure that once the traffic passes, the stmgr comes back to some limits rather than remain at the large size.. setrlimit might be one way to go here.. The only reason stmgr uses large amounts of heap is when there is a mismatch of producer and consumer rates. Spout based backpressure is the only mechanism that we have right now to limit this, however there will still be cases when even after back pressure has been applied on the spouts, there are other bolts that are consuming the remaining buffer in the channel and producing more data after processing. However this should be a temporary phenomenon and should go away quickly. The change that @nlu90 did makes sure that when we dont need that much memory, we give it away.\nIMO if we really see stmgr using Gigs of ram, we need to investigate what other problems besides this exist if any.. @nlu90 done. GetCachedSize returns the previously calculated GetByteSize. Thus in order for GetCachedSize to be valid, some one should have called GetByteSize and the message should not have been altered since then.\nI am seeing that no one is calling GetBytesSize on HeronTupleSet. This means that GetCachedSize is likely incorrect.\nHere is a proposal\n1. In Stmgr-Server where stmgr first gets anything(either TupleStreamMessage from stmgr or HeronTupleSet from instance) we need to call GetByteSize(). \n2. Rest other places GetCachedSize should return the right values.\nCan we see whats the perf impact of these changes?. The semantics of clear makes sense. This change however was not intended for memory saving but with client semantics in mind. When I acquire a protobuf I expect it to start using it as if I got a clean data structure. . @billonahill This pr makes it worse by adding more dep from common. . @billonahill Unfortunately, there is no unittest for stmgr-server.cpp(which this change targets). I'm planning to augment the stmgr_unittest.cpp which tests the stmgr.cpp in the future pr when stmgr TODO items are filled up.. Unittests are on their way for this pr, but I wanted to put this out so that folks can start reviewing the functionality.. Added the first set of unit tests. More coming soon.. Have added more tests. @billonahill I have updated the description of the pr and given specific pointer wrt original design doc.. @billonahill @huijunw Addressed your comments. Please take a look again.\nThanks!. In principle I like this change, however I think you should add a heron internals config variable to control this parameter?. @objmagic could you please elaborate what these cir dep are?. There might be ways to avoid this. One way is to actually send this parameter as part of the init method. Or have an explicit InitBuffer pool method inside main of stmgr/tmaster. . @objmagic Sorry, looks like our comments crossed each other. I see that you have done it. Let me take a look at this. Thanks!. @congwang unless I'm mistaken, afaik, we are not talking about memory leak here. Its just about restricting the size of mempool from growing and remaining high for a long time.. @objmagic Thats not true. If you notice, acquire does a Clear before giving the proto structure to the client, so Clear is called.\nYou can do one expt. Remove the Clear from acquire to release. . Could you elaborate more on how/who the Tmaster would send the 'killcontainer' message? Is it to the stmgrs in X and Y. Or to the scheduler somehow?. Are we sure that if executor dies in a zombie container, internal aurora handlers within that container won't restart it?. @huijunw Could you tell whats the proposed solution? Is this the one thats mentioned in the first post(TMaster killing container). Also could you add more details about the mechanics of how this will be done in a general manner(aka non aurora specific). Thanks!. Looking forward to the pr!. @huijunw The proposed change\n1) in tmaster cleanup httpclient and asyncdns\n2) Add more logging\n3) What are the changes in the LocalScheduler?. Fixes #1904 . Relates to #1512 #1513 #1514 #1515 . @huijunw cereal is the equivalent of kryo in java. Its a fast and efficient serialization library that serializes general data structures in c++.. https://github.com/twitter/heron/pull/2429 supersedes this.. Looks good to me. On testing on local mac and a linux based local mode in gcloud, i found 7-8% perf regression due to this change. Hence closing this pr.. @nlu90 Just looked a little closely at the reconnect interval. Its currently set at 1 second. Which basically means that if we fail for 30 seconds we kill the stmgr. I believe this is too aggressive. Maybe increase the max reconnect attempts from 30 to 300?. looks good!. @kramasamy \nThe config file looks like stuff in verbose mode. When submitting regularly, I really don't look at the config file.\nI agree that we need to add some more comments in Uploading to and scheduling in. I can add those in later prs.\nThe Elapsed time comparison to bazel is slightly is not appropriate imo. bazel is a compilation tool which values how much time it spent while compiling. We are just submitting the job, submission time is something that's not really a highlight.. Related to #2146. https://github.com/twitter/heron/issues/2146 will track moving away from CheckpointManagerConfig to the generic Config class. . @kramasamy changed. @yohan233 restarted build. One broad comment. This change seems to be making a single connectors and putting spouts/sinks in there. Instead would a second level differentiation help? I.e. connectors/spouts, connections/sinks? . @cckellogg makes sense. @cckellogg Is it even possible to do this without heron.tar.gz being present at the local machine? . @objmagic Note that this change does not force python3. Rather it moves away from using apis that are python2 only to apis that are existing in both python2 and python3. This way we can run Heron on both python2(which is what Twitter's environment is) as well as python3.. @nlu90 It passes now. I didnt see the old gflags package being removed?. If I look at the code, there are already parameters controlling the size of HeronTupleSet(which is actually just a collection of HeronDataTuple). The issue arises when one HeronDataTuple is so large that it blows away all these limits.\nIMO the way to solve this would be some sort of setting inside the instance that controls what is the largest size of HeronDataTuple allowed. And throw an error to the user if the user's emit is larger than that. Thoughts?. The question is whether this should be made configurable? And if so, how. Might be something worthwhile in the API.. Yes. One step at a time :)\nFirst we need to make sure that heron compiles both in python2 and python3. Currently the sticking points for this are a) bazel needs to be upgraded to 0.5.4 and b) pex needs to be upgraded in place. One issue with this pr is that it is making system components(like executor) depend on heronpy which is not what we want to do. I don't think thats a good idea. One issue that we need to address in this pr is to make sure that heron/common does not depend on heronpy.. Better to have a discussion over hangouts so that we can see what exactly is happening. One potential reason is that our zk library is too old and contains many bugs which have since been solved. Here is the change to update that to latest stable version\nhttps://github.com/twitter/heron/pull/2245. This is basically API configs if I understand correctly.. looks good!. Thats interesting obs. So ByteSize is probably more related to wire format size and SpaceUsed is related to actual in memory repr. In that case, shouldn't you use SpaceUsed? But isn;t that very slow?. Could you also share the thruput figures? Particularly in exclamation or other topologies that are used for Heron benchmarking?. This is a really bad change. Essentially the system is throwing things out without informing the upper layers. Imagine effectively once topologies encountering this.. I don't understand this. Why are we duplicating functionality in python for something that already exists in java?. @objmagic I understand that. However why are we duplicating functionlity that already exists in Java. Shouldn't the pathway be\n1. If jar is regular file, continue as before\n2. If jar is uri of some sort, invoke a downloader(analogous to the uploader). I believe the downloader functionality already exists in the java spi side of things.. @maosongfu Can you tell me who wakes up the gateway thread once the slave processes the assignment message? Currently the logic is as follows\n1. We start reading from stmgr if either we have capacity to absorb messages or if helper is null(that is assignment is not done). This ensures that we read the assignment message\n2. Once we read the assignment message we promptly stopReading from stmgr(which is fine)\n3. But after assignment message is processed by the slave, I would have expected us to start reading stmgr immediately. Rather what happens is that nobody informs the gateway thread about this. After around 5 seconds the slave thread wakes up(not sure who is doing that) and then it realizes that it now has capacity to absorb and thus starts reading from stmgr.\nMy feeling is that this change wont be necessary if we figure out why slave thread is not communicating back to the gateway thread after processing the assignment message.. @maosongfu is there a way to increase the ci timeout?. Thanks @maosongfu That worked.. @maosongfu Fixed most of your feedback. replied about session windows.\nWRT doBuild stuff, this is kind of evolving. My thinking is that since it is not part of the main interface, we should let it evolve as we write some real world spouts, before refactoring.. @billonahill Component naming is tied to the stream that the component is consuming and the operator. Users can set the stream names via setName(). Agree that we want to make the component names as obvious as possible.\nwrt testing, yes, there are a bunch of integration tests coming right after this.. We still need to do memory management of the register request anyways. That way the memory overhead is the same.\nMaybe I worded it incorrectly when I said simplification. Its probably clearer, that when you say getInstanceInfo, you are just collecting instance info and storing them over at the tmaster client to be used later.. I have restarted the build. This is a very interesting link between codes(java examples to python cli). Are we sure about this dep?. Closing this one. @maosongfu sounds like a plan!. @nlu90 Thanks for the graphs!\nDo we have longer duration graphs(like day long or weekend long)?. This mostly looks good to me. Adding @maosongfu and @nlu90 for their thoughts. @jerrypeng It did it two times. I don't see the same failures in other prs. Are all the unittests working in your working env?. @jerrypeng I figured out the issue wrt failures. This has been fixed in the move to the buffer events pr. https://github.com/twitter/heron/pull/2297. After examining the fix to the build file of the heron instance, it seems to be a valid way of fixing the inclusion issue. I'll try to summarize below\nHeronPython Instance depends on heronpy because it implements the api functionality. In the current version, we include heronpy.api inside the instance itself. This means that when the instance starts up, it can instantiate its heronpy references. But, all subsequent heronpy references will now be directed at this instantiation. Which means that if there is a dsl topology that refers heronpy.dsl or heronpy.connectors, they won't be found because the instantiated heronpy embedded inside the instance, only contains api.\nThis change includes the heronpy.api/dsl/connectors inside the instance thus fixing that issue.. I believe the rebase was done incorrectly. It should be instance_server and not server_. Hi Naveen,\nIt is certainly possible to integrate CoAP protocol inside Heron. Heron Spouts/Sources are general enough that one can connect to any data source/data type/protocol.\nAre you already looking at some CoAP libraries to integrate with? It should be a fairly straightforward thing to construct a Heron source from that library.\n. I'm assuming that you are running the CoAP server in UDP setting? In that\ncase something like UDPConnector\nhttps://github.com/eclipse/californium/blob/master/element-connector/src/main/java/org/eclipse/californium/elements/UDPConnector.java\ncould\nwork.\n\nOn Tue, Sep 19, 2017 at 12:28 PM, naveen marri notifications@github.com\nwrote:\n\nI'm looking at californium.\nCan you suggest some good implementations? My CoAP server is running on\nNodeJS and I want to build a CoAP listener at heron side and pass it on\nspout.\nCan you point me to some references for this if available?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/2323#issuecomment-330648182, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACabujNjdBNGSH3_TOJl6ndQGGQALuV-ks5skBXngaJpZM4PcsOs\n.\n. You can put this uri information(assuming its a string), in the constructor\nof the spout. For example take a look at\nhttps://github.com/twitter/heron/blob/master/examples/src/java/com/twitter/heron/examples/TaskHookTopology.java\nHere the AckingTestWordSpout is constructed without arguments. You can pass\nthe uri information here and then this will be carried over to the runtime.\n\nOn Sat, Sep 23, 2017 at 6:28 PM, naveen marri notifications@github.com\nwrote:\n\nHi,\nI'm able to pull data from coap server by observing the URI.\nHowever I'm not able to get how to pass the URI configuration to the\nISpout class.\nCan you please help me on this or share some example how it is done?\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/2323#issuecomment-331680364, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACabuoG0i_SssIyqPWRTHf0EPUcBAnPgks5slbBOgaJpZM4PcsOs\n.\n. @aahmed-se has ideas about how this can be debugged.. One of the things we found was that the behaviour is normal prior to pex upgrade.\nSo we are looking at what went wrong in the pex upgrade. The fix has been found. pr coming soon. https://github.com/twitter/heron/pull/2333. This has been fixed. This has been fixed. We might need some changes in heronpy as well to make it python3 compatible. Currently it works in python2.7.\nBut want to do this in smaller prs.. @huijunw The aim is to provide the dsl func in the same jar as regular api.. looks good. if users use collector.emit(new Values(..)), then you would see default.\nIn cases when they use collector.emit(streamName, new Values(...)) like the underlying spouts/bolts in dsl, you would see the actual stream name.. @huijunw Its ok to introduce non breaking changes. i.e. there is a officially supported version(currently 0.5.4). As long as things work in that officially supported version, making any new changes to support newer version should be ok.\nWhat do you think?. @huijunw I believe this has since been resolved?. You need to modify aurora file as well. @huijunw comments?. As far as I'm aware, we haven't tested installing heron in windows.. @nlu90 WRT Logging issue, we have a work around where TopologyContext will export a logging object that the topology writers can use.. Looks good. Maybe make analogous change in python as well?. This looks to be working in the current verison.. @jrcrawfo What is the symptoms of the bug? Does it exist in master as well?. @cckellogg Might be related to the same issue that you are looking into. +1 for this. The scribe stuff looks good. I'll let @cckellogg comment on the prometheus stuff. Am not able to reproduce this. Infact I'm getting a different error when I try running this.\njava.lang.RuntimeException: java.io.NotSerializableException: com.twitter.heron.examples.streamlet.WireRequests$WireRequest\n. I believe this was due to a version mismatch between heron installed version and the topology.. +1. LOG(FATAL) is supposed to kill the process.\nOne thing that we could do is maybe add an explicit ::exit() after FATAL?. @nwangtw Now that the port mapping pr has been submitted, this pr is simplified and ready for another review. Thanks!. @jerrypeng . wrt mapToKV equivalent, do we need that in KVStreamlet? All operations on Streamlet can be done on KVStreamlet. Can you give an example when this might be needed. wrt Streamlet.reduceByWindow returning KVStreamlet, imo its cleaner to explicitly recognize that the return streamlet is a KVStreamlet rather than dance around that with a Tuple2. @lucperkins What do you think?. WRT windowing operator, that might be true. Maybe you can explore that in a seperate pr?. @jerrypeng Should we instead add a reduceByWindow operator to KVStreamlet?. @lucperkins One of the intentions of this pr was to have all functionalities replicated in each other(KVStreamlet and Streamlet). Thus KVStreamlet has log, etc operation just like Streamlet. Thats why I mentioned adding reduceByWindow to it as well. @lucperkins @jerrypeng I have added a mapToStreamlet operation. @jerrypeng . +1. @jerrypeng . @jerrypeng That makes sense. I've re-implemented it as one version. @lucperkins I'm modeling the naming on native java functions which have essentially the same concept. For example look at https://docs.oracle.com/javase/tutorial/collections/streams/reduction.html.\n. @lucperkins I've added the reduceByWindow general function as you requested. @jerrypeng Added the unittests that you requested.. This is actually not needed.. I know what happened.\nThese errors are coming from the merge pr 2398. This pr was put long ago, when travis used to check only compilation in heron/...\nAfter this pr was put up, we made the change for travis to check other parts of the code base as well(including examples/...). But we never rebased this pr before merging. Thats the reason for the break.\nA pr for the examples should fix it. And this travis is automatically going to enforce it for all future prs.. This has since been fixed. @huijunw Can you check if this covers for aurora as well. It should be removed. @jerrypeng . @lucperkins This change affects only the complex source. Thus the simple sources are still just having a simple interface. The reason why I propose this change is that most of the real-world spouts (kafka/pulsar) actually prefer to emit a list at a time rather than one element. Thus it naturally fits the use case. Also having the ability to emit multiple means that sources get less complicated:- they dont need to have to buffer data logic in them, etc.. @nwangtw I did think about some kind of a hint. The issue is that the impl cannot really make smart choice regarding this hint value. There is nothing that the base api exposes any info about this.\nWe could in the future think about enhancing the base api to support this, but that probably requires some extra thought.. It is not included in the instance jar either. This snippet of code seems to be a leftover from prev where kryo was part of instance/api.. @jerrypeng . @jerrypeng . I didn't understand the purpose of this change. Why is it better?. unrelated to this change and its been failing for all prs now.. Sorry, but I'm having a hard time understanding the rationale behind this. Could you please detail more?. @erenavsarogullari Can you pl resolve the merge conflict?. I saw a merge conflict a little earlier. Also it seems I cannot see the travis pass link. Does anyone know what happened?. We want to avoid dependencies with any layer. Thats why the new resource class has setRamInMB and setRamInGB helper methods. But thats outside this pr scope. But still would rather use the setInMB and GB alternatives which are as clear. @huijunw Can you please comment why this was not found necessary?. 2559 is merged in. Could you please rebase?. @nwangtw This flag makes sense even in ATLEAST ONCE because the tuples will time out and will retry later. @nwangtw 1. Does Storm have an equivalent call?. Unless there is an equivalent storm api, there is no point adding this flag in the storm layer. As you mentioned storm layer is strictly for emulating the storm api, not adding func to it.. @billonahill Does the pr look good to you? If so can you pl approve? Thanks!. One effect of this change is that it forces python2.7. One of the significant efforts that we have made is for us to be compatible with python3. This pr is at odds with that effort. IMO there are two issues here that possibly should be separated\n1. Python version\n2. encoding.\nCan we fix just the encoding here and put the other one for discussion?. +100 on this one. Thanks a ton @nwangtw!. @jerrypeng . @dancollins34 I believe the reason why travis is failing is checkstyle issues.\nIf you are building on mac, please use \nbazel build --config=darwin heron/...\nthis command should also throw the above check style issues. Not sure what the build failure was. Have kicked the build again. . Is there a perf implication?. IMO adding new apis is not the right way to do this. Storm's acking mechanism is thread safe and so should we. This will not break any existing clients since it actually relaxes constraints. The only issue is performance. And even there I'm not sure if this is a huge hit esp since the contention between the two threads is very little.. This change shouldn't affect stateful semantics.. Heron effectively once assumes that tuples are processed in the order they are received. Thus (leaving aside this pr), in today's code, if tuples are processed out of bounds, the contract of effectively once does not apply. This pr does not change that invariant.. @maosongfu I think the concern is misplaced. Lets take two cases\n1. Users use only a single thread to do process/ack. In this case, both the current version and the new pr work the same.\n2. Users use multiple threads. Then in the current version, even if they acked in the right order, the system does not ensure effectively once, since the system could do checkpointing before tuple_A is acked for example. This doesn't change with the new pr either.\nThus in both cases, the current semantics still hold good. \nHeron's chandy lamport algorithm implementation requires that users take up next tuple only after processing the old tuple. If users deviate from that, effectively once cannot be ensured. This is true both in the current version and the new pr.. Two points\n1. This is not related to this pr. This concurrency can happen today as well.\n2. More importantly, if there are user threads doing execution, its not possible to even know when its good to do the checkpoint. Do we have to wait for all pending tuples to be acked(this may not happen based on user logic). \n. Users can(and do) have concurrent threads today. SummingBird for instance has internal threads that do all kind of processing. The only thing that they have to do is to do their acks/fails in the same thread as process thread. Thus in the present scenario imagine this situation\n1. User Bolt has two 'worker' threads that actually do some work and use some kind of queue to put their acks/fails in the main process thread.\n2. tupleA, tupleB, ckptTuple, tupleC is the order as seen by the system\n3. say tupleA is handled by threadA and tupleB is handled by threadB.\n4. process thread receives ckptTuple. It calls preSave, followed by state collection. If the user's dont carefully program their preSave, we wont have guarantee that users will record tupleA and tupleB's execution state. \n5. When things go bad and state is rewinded, ckpt is restored at the bolt level  and system continues from tupleC.\nNote that in this case, because users are doing out of band/out of order processing, tupleA and tupleB are not acked/failed before ckpt is handled. Thus it is left to the preSave functionality for them to save their state properly.\nNow whether the acks/fails are done using some kind of sync between user thread and worker threads or in the worker threads themselves has no bearing upon this state of affairs. Thus this pr does not in any way alter the way stateful topologies are handled.\nIt would be great if you can provide an example of why a stateful topology would break with this pr.\nThanks!. @nlu90 can you please take a look as well. To start off, just Java. Then once the api stabilizes, we can have other\nlanguages as well.\n\nOn Mon, Feb 5, 2018 at 1:32 PM, naveen marri notifications@github.com\nwrote:\n\nHi Karthik,\nI'm interested in the issue, Are you looking at both java and python\nimplementations?\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/2698#issuecomment-363228399, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACabuiL14ge_f5ZWFT_9Usx4FgLJnAODks5tR3OKgaJpZM4RzNm2\n.\n. One meta comment. Why is rate limiting being done at the spout level and not stmgr level\n1. libevent already has a leaky bucket abstraction layer that will allow you to do this pretty easily\n2. this will then automatically apply to other languages.. I want to move a little bit higher level and understand what goals we are trying to achieve here. I was under the impression that we want to rate limit the sources(aka the spouts). Thus each spout(or combined together) will have a certain rate that will be allowed into the system and the rest will be backpressure. Is my understanding correct? If this is correct, then applying this at stmgr is not only convenient(because stmgr knows which are spout connections and can enforce the rate limits on only those connections) but also language agnostic. @nwangtw My suggestion is to rate limit between gateway and stmgr. And this is different from backpressure. Backpressure is a state that the stmgr gets into and at that time it stops reading from spouts. On the other hand, rate limiting is simply reading at a certain rate from the connection of spout instance inside the stmgr. This should be something that you do during connection setup. This doesnt require any kind of handshaking, etc between stmgr/spout instance. And because stmgr uses libevent as the read interface, its literally just a matter of calling one function on the connection. Checkout http://www.wangafu.net/~nickm/libevent-book/Ref6a_advanced_bufferevents.html and in particular the  rate limiting section.. retest this please. @jerrypeng I'm not sure if this is the right approach to take. You are breaking storm compatibility as well as breaking abstractions. Agree with @congwang . can we add more description here as to whats the key, value means.. if instance_stats_[stmgr_id] does not exist?. where are you clearing this? Shouldn;t this be cleared on a regular basis?. Makes sense. I will add it and update. The State as defined here is a very generic interface for spouts/bolts to keep their internal state. Any type of common operation will probably come in higher layers(like DSL). Spouts/bolts should hold on to state variable to store their data. This is very consistent with OutputCollector in ISpout/IBolt. Done. IStatefulSpout/IStatefulBolt are mimicking IRichSpout/IRichBolt. They are essentially a shorthand version for any kind of stateful component.\nIRich/IBasic stuff are essentially more developer friendly versions of the basic ISpout stuff. Thus in the future, depending on requirements, we might have IBasicStatefulBolt/Spout as well. They make it easy for developers to quickly start writing bolts/spouts.. These are all user facing apis. Users are expected to set this(we of course take some default stuff). We don't want to restrict State to a particular type of key/value. Thus say KafkaSpout can do state.put(\"offset\", 111);\nwhile some other type of bolt can do\nstate.put(120, \"Sanjeev\");\nWe should take any object as long as it is serializable.. Not really.\nOne could run a stateful topology in non stateful mode. i.e. I could write a IStatefulSpout/Bolt but may prefer to turn on and off the stateful config for maybe different use-cases or experimentation.. done. done. This is a topology specific config. Thus it should always be topology.\nAll per component variables are prefixed by topology.component.. Changed to TOPOLOGY_STATEFUL_ENABLED. Added more comments to clarify. Thats a good point. I've separated out Stateful processing and exactly once semantics.. Added more comments to explain this. Let me know if it makes more sense now :). Added comments.. This is just the oldest checkpoint that needs to be preserved. All older ones can be cleaned. added. done. added. Explained earlier. This is the marker message. Added comments to explain. Made changes. changed. Done. We do follow that terminology where all request/responses are labelled as such.\nAnything else that is not a request/response are just best effort messages that go from one component to another. For some, a response is not expected. For others it cannot be a request because in Heron c++ client/server communication, only clients can send a request and a server cannot.\n. Same as above. the latter(i.e. use protobuf object bytes). This is purely for information sake only. Mostly used for printing/debugging. The idea is to use ckptmgr as some sort of service by the tmaster. TMaster upon startup would connect to it(which is also running in the same container) and register itself. This is for the sake of consistency wrt all other register requests. The original idea was sanity checking. Current phase only treats it as arbitrary bytes. The aim is to get the flow mechanism correct, before expanding this. distributed checkpointing is non-blocking. However the mechanism of how they are triggered are serialized. This is part of the Lamport's algorithm. To be part of a consistent checkpoint, an upstreamer needs to checkpoint first and then send these markers to all potential downstream components. Downstream components can checkpoint only after they received all their upstream components.. oneof is not supported by the protobuf version that we are using. Rather than do this in the design doc(which is seperate entity) I would rather include some sort of flow in the file itself. This is done. Please take a look. Added more in the header section. Done style fix. done. changed. fixed. Fixed this. Fixed. Couple of clarifications:-\n1. Just to be clear in Heron parlance component means one logical component(like spout/bolt). Instance is one running process of it. Thus there are 5 instances of 'wordSpout' component. In this case, it is instance level.\n2. Chandy Lamport's algorithm necessitates that downstream instances are aware of all possible upstreamers. In practice this is performed at the stmgr so the real instance code is not aware of it. We cannot involve tmaster here because then a) it is not decentralized and b) only local state inside stmgr is sufficient  . 1. activate/deactivate are meant for totally different things and imo not applicable.\n2. yup. In future we could add some commands like that to the cli itself. fixed. fixed. reworded. fixed. removed. I've done most of the changes that you requested. Working on unittests. Meanwhile can you please take a look again?. Not sure I like the idea. All throughout Heron code, we have specific handlers for specific requests. Also, this allows the request/response semantics to be within the ckptmgr server and not leak to actual backend implementations. Again this is consistent with how Instance/MetricsManager/StMgr handle config. I would suggest to keep this consistent across different services and change all services in one go if we do this change.. i don't think its a good idea making this as complicated as SystemConfig. It looks very heavy weight for this purpose. Again my same comments as above. We really do not want to make things this modular (and complicated) at this stage. Start with something simple and see how it evolves. Not very often. Would prefer to be in info mode for better visibility. Done. a) Not sure what prevents us from sharing code with the current approach?\nb) same as above\nc) we still have all the unittests for each of these components\nd) I dont believe that the current implementations are huge by any means\ne) I especially want to decouple RPC function from the semantics of the backend functionality. My meta comment about this request is that it makes the whole thing too complicated. I always favor simple(and working) solutions first before going to one thats very modular. I still stand by that argument.. The changes are on the ckptmgr side of things. I don't want to change in the FileUtils layer yet because all other calls there are all returning status as return value(as opposed to throwing exceptions).. I'll rename the saveBytes, but this description should go in the proto file actually when we do make changes to a more structured form. done. WRT refactor of IStatefulStorage, we really cannot predict what kind of storage mechanism will be used. The first two might look similar but lets say if the next one is cassandra, then no longer we deal with files/dirs but with keyspaces.. done. done. done. done. Fixed. Not sure we want to pollute the proto structures with enums. The fact that we are doing enum is just a limitation of our java Client interface. Probably a cleanup needed there in a separate change. Not sure what you mean by response sent? You mean at the client side?. No longer swallowing the exception. This check only happens for the register requests. And the log message etc are all different and the checks are very minor. Thats why it's not worth refactoring them.. done. The error messages are different. removed. done. The IStorageInterface declares that only StatefulStorage exceptions are thrown. Isn't that not sufficient?. If the exception runs all the way to slaveLooper runnable, that would actually kill the ckptmanager. Thus its kind of a fail fast approach. . All backendConfig have been changed to statefulStorageConfig. Just stmgr/metricsmgr/tmaster any 'bugs' will result in the daemon process being killed and restarted by the executor. So in that way its pretty consistent with the rest of the system. They have different values and are specific to their own implementation. Not sure how we could share those. . Again, I want to give maximum flexibility for these state managers to diverge in the future(which they almost certainly will).. There could be several reasons. Local File System and HDFS differ in how they handle files/directories. Thus there might a chance that we might take a different route in a production hdfs environment minimizing directories and just going plain files. While LocalFS being just for production, ease of looking up is more important. done. Fixed. fixed. fixed. fixed. fixed. The proto structure thats stored in the statemgr is the actual list. Not just the last one.. All manipulation of the proto structure happens inside the relevant servers. This class purely is for interfacing with the statemgr to store/retrieve/delete. According to zk semantics, this situation should never happen. If it happens it certainly is a zk client/server bug and we are already in a bad state. better to die and restart fresh. name it stateful_config_file maybe?. Added. Changed everywhere. Corrected. This will be required in stmgrs to see whether marker propagation needs to be done. This field gets set via Config.java in the api. This is controlled by the user.\nhttps://github.com/twitter/heron/blob/master/heron/api/src/java/com/twitter/heron/api/Config.java#L202\nexplains the relation. Done. I feel that we kind of made that choice some time back. IMO it belongs to a different discussion and not really releveant to this pr.. Sorry, missed that. Fixed. This pr is not adding any new user facing config variables. It is just the implementation part of the already existing api(ACKING ENABLED, EXT1_ONCE, etc) that already exists in the Config.java which is the main user api. As such this api just addresses the implementation part.\nThere might be a new pr needed to clean up the user facing api(which is really is Config.java). But that is not this pr. And that has wider implications. If/once we do that, the underlying implementation like this one, can also change at that point. We could certainly clean that up in a later pr. However one good thing is that all of these methods are abstracted out by the config helpers. So when we make that change, its just a matter of making the change in this class rather than in any other component. . BTW I also feel that the other change has wider implications. It would need a careful migration policy since existing topologies will have to be recompiled.\nOne good thing however is that heron api itself is not widely used, but the wrapper apis(Storm, etc) are. So even when we change it later, it would still have very little impact on other users.. Makes sense. Done. Made it generic so that even multi threaded processes can use the interface.. The Install methods are only in the control plane. Not worth using pool imo.. Not sure what the memory leak here is. Ideally would have had more comments.\nI want to keep this change as logic less as possible so maybe we can find the leak and fix it in a separate pr. . When metrics mgr client is created, you are going to supply it a port to connect to. Supplying 0 should not be an option at all. Thus I dont understand why this method is required at all.. IMO the most cleanest solution would be to not create metrics mgr client until you are ready to start it in the first place. That would avoid all this unnecessary code. Thoughts?. @huijunw If you can hold on for a week or so, I have other refactoring coming in stmgr that hopefully should alleviate some of this. Suggestions please?. Could you please add some doc about sample heron submit/kill commands on kubernetics. My feeling is that this pr breaks abstractions and should be avoided unless its breaking something in production.. Why Packing Plan is fetched ahead of TMaster set? One invariant is that we should be confirmed as Tmaster before doing anything. Are we sure we are the tmaster here?. A better approach to prevent that is a) still maintain the invariant that you first become tmaster, and only after fetching packing plan, do you start your listening server. That way stmgrs wont reach you. Why mention Storm at all here?. Can we say more along the lines of\nHeron is a general purpose streaming engine with performance, low latency, isolation, reliability in mind. It also provides API compatibilty with Storm.. Please remove the streamparse sentence since that is no longer supported. \nAlso please change the \"upgrade from Storm to Heron\" to something along the lines written in architecture.md. please remove streamparse related stuff. remove streamparse. ditto. ditto. please remove upgrade along the lines stated above. Migrate please . Migrate. migrate . is the link right? pyheron does not exist anymore. same here. same here. shouldnt 0.14.9 be changed?. done. done. done. fixed. CachedByteSize?. Added more comments. This is debug anyways. I've reworded it. Changed. Fixed. As part of my cleanup, I'm going to change the TupleSet2 protobufs to remove the 2 part in future changes.. Done. This will become clearer in the following prs.\nBut essentially the intuition is that when a stmgr notices that its connection to another stmgr is gone, it should initiate certain actions. Like for instance, if it was running normally, it should ask the tmaster to start a recover topololgy from last sane checkpoint.. A stmgr services a bunch of tasks, some spouts and others bolts. local spouts are the former. This check is basically checking if this task is a local spout and only send the initiate checkpoint message to them. I will add a comment here.. good catch. Fixed.. For unit-testing of StatefulRestorer class, we need to define dummy ckptmgr client(and others) that override certain methods.. Updated comments in .h(as well as the pr description). Added. Fixed. Actually the aim is to keep the caller caller(the stmgr class) as simple as possible. Even if we did return some status code, it's not clear what stmgr would do(other than logging). But taking all those actions in this class, we can isolate all decision making to this central place wrt recovery. Included. Updated. If we cannot get the state from the checkpoint manager, then we cannot proceed with the restore. Thus its better to invoke the callback with a failure code and reset ourselves by making is_progress = false. (Most likely there will be another attempt by tmaster possibly using a different checkpoint id).\nInvalid ids could occur for many reasons. What if one stmgr is in the process of recovering and fails, and tmaster initiates another recovery at a different ckpt id (or different restore_txid). In that case we abandon the the old restore state machine and go to the new one, but the old messages will still be delivered to us, which we need to ignore.. Done. This is part of a bigger cleanup that touches other tests in the directory. Will send that as part of a cleanup in another pr. Sailesh attempted this some time back and only did partial things(mostly in the networking world). The thing is that there is some extensive re jiggering needed to make our classes mock friendly. That's why we are using virtual methods for unit tests for now.. My feeling is that its not necessary since its on control path. So topologies not doing exactly once can start processing right away as soon as assignment are propagated to all stmgrs/instances. However for exactly once semantics, we have to wait until all of the stmgrs/instances are restored to a certain globally consistent checkpoint before proceeding. Once that determination is done by tmaster, this is the message that it sends to the stmgr to start the actual processing. Not necessary. We issue these messages to the tmaster as soon as we notice any instance failure. The same applies for any stmgr client connection failures. Mostly for correctness checking purposes. Instances will check that they have recovered to this checkpoint id before starting. They should die if this checkpoint id does not match theirs.. Fixed. Thats an interesting thought! It seems in general there might be lack of consensus/clarity on this one. So I've removed STATEFUL from the list of the modes. We'll add it back later.. Added comments. I remember @maosongfu @kramasamy mentioning that there are some native heron topologies inside Twitter that might be using these features.. Added. Fixed. Changed the description. Fixed the comment. Can you actually have\nheron::config::TopologyConfigVars::EXACTLY_ONCE\ninstead of a \"EXACTLY_ONCE\". should this be called cleanallstatefulcheckpoints?. stateful_controller?. stateful_controller?. Can I do it in the next pr? My feeling is that this name change will make this change confusing.. fixed. changed. renamed. For what its worth, its better to just piggy back on the regular serialization that ppl use for tuples instead of having multiple serialization interfaces.. Agree with Neng, null might be more appropriate. We could. Do you know where is validation done?. Why do we need it in non verbose mode?. Again the same reason as above. Why do we need it in non verbose mode?. The thing is that when launching we need to clean the directory. I've updated the pr to ensure that we clean the working directory while setting it up. This way, new topology launches will be ok.. why is the SuppressWarnings needed?. It's much easier to expand ~ than using ${HOME}.\nBTW the other uses of ${HOME} used in files like statemgr.yaml is really not used by the code. Its being overriden manually. See https://github.com/twitter/heron/blob/master/heron/statemgrs/src/java/com/twitter/heron/statemgr/localfs/LocalFileSystemStateManager.java#L245\nI will fix this as well in another pr. Note that parallelism is optional and will inherit from the prev stage.. pythonconnectors-py. Here and other places. Can you specifically set client = null. an assert that client is null. also eventloop should not necessary here. Its already passed to us in the constructor. shouldnt this be options_?. Could you please elaborate why this portion of code moved?. this breaks one fundamental invariant in tmaster design. No operations before verifying that we are the master. . I'm deleting the interface function from streamlet, but letting all other derivates implement calculating inputs in their own way. Thus the calculate_inputs method here is the local one defined above in the same file. One of the fundamental assumptions in tmaster when it is processing requests is that it is already established itself as the master. This is to prevent multiple tmasters trampling on each other. Thats the reason, we do StartServer only after TMaster Establishment is complete.\nThis change starts the TMasterServer which is now ready to accept connections, even before it is established as master. Thus all sorts of bad issues can happen. Is this correct?. pex loader should go back to common. That way tools will not depend on instance. ?. ?. I believe Java already has a Duration?. Could it be possible not to use getSourceGlobalStreamId(). One of the reasons that we did not implement this in heron was that we felt it exposed too low level details. Could we instead get the SourceComponent and SourceStreamId as individual parameters?. Could you please add java docs as well as descriptions for what these members stand for. What does S stand for?. ByteSizeLong?. It might also be a more scalable way of doing things because now you have prometheus server connecting to each metrics manager to get the stats so the load is more evenly distributed. . can we have heron_java_api_proto_files instead. can we print iter->first here as well. why not make it a an actual instance of register request rather than a pointer? That way the cleaning logic is greatly simplified. Done. Which one are you referring to? Streamlet or StreamletImpl?. done. Done. Because BuilderImpl needs access to doBuild which is only in StreamletImpl. JoinStreamlet for example requires KVStreamletImpl rather than KVStreamlet. Thats the reason for using. Do you see any issue?. As discussed, Count might be overkill for now. So leaving it as it is for now.. Could you illustrate what the window function would do?. Done. Renamed StreamletImpl to BaseStreamlet.\nDid same for KV. Done. Fixed. Added. Changed. Not clear how. I tried some variations. Will keep you posted, but if you have any ideas that would be great. can heronpy dep come first before the common?. pl remove. Check out the transform method. The main interface is the newSource(renamed from newStreamlet) that supplies an already created streamlet. How to write source streamlet, is probably another set of documentation/pr. Fixed. renamed to toSink. fixed by adding count. added check. fixed. This is not an error. Added comments.. fixed. It supports both size/time based operations. We now pass both the time and count information. refactored. in the executor, the ports are called master_port and tmaster_controller_port. They are labelled that way for both the tmaster container, as well as the regular container.\nWhat we are doing is that using the tmaster_controller_port to be the local data port.. I prefer not to change any 'names' in this pr because that will pollute this pr. I will send out a follow up pr to fix the names. Deliberately kept it simple for now. We can add SessionWindows later. Want to keep the first cut simple by adding simple types first. We can add SessionWindows later. Fixed. fixed. I belive we could do a simplification here.\nCan we rename SetStmgrRegisterRequest to SetInstanceInfo, where the tmaster client copies the instances into local variable. And then during send register, just copies them back into the register request and sends it.. I'm just following java.util.function terminology as defined in https://docs.oracle.com/javase/8/docs/api/java/util/function/BiFunction.html. I avoided that because that would expose the TopologyBuilder stuff which is really an implementation detail. 1. We need to Clean instances if it already exists\n2. for (auto iter = instances.begin(); iter != _instances.end(); ++iter) {\n    auto instance = new proto::system::Instance();\n    instance->CopyFrom(*iter);\n    instances.push_back(instance)\n   }. Please rebase with master. I would reword this as\nthey need to ensure that any state changes caused by execution of the tuple is captured in the heron state variable thats passed at startup time. This way Heron can keep track of state of the processing logic and rewind state when failures occur.. Leftover debugging changes?. This should be 0.16.0. this is not supported in dsl. Might be removed from python dsl as well. createSupplierStreamlets are now not public. Please use Builder.newSource. Can we talk about how Resources can be specified?. 0.16.0. Can we include Type safety . can we name them explicitly. like setTickTupleFreqencySec and Msec. Ok. How about the new api then. Make it explicitly say Ms?. Just __execute-time-ns should do the trick. Since the stream name could be different. done. Would we rather have SerializableFunction<? super R, ? extends Iterable<? extends T>> flatMapFn\n. This is not true. @jerrypeng can you add some more color here?. updated. The toSink is just for abstracting out complex sinks(aka terminator bolts) that might need a setup/cleanup.Not really related to stateful processing.. You could move this line to preSave. That way the topoliogy can be used both in effectively once as well as atleast/atmost once mode. Why have this method (and the execute method below)? . Is this a breaking change? @maosongfu?. Whats this new file?. same as above comment. Need to include JoinOperatortest?. why streamletapi? why not just streamlet?. fixed. fixed. Can you add one for cpp as well?. If we are introducing this dep, we need to add a shading rule for this.. This should be INFO. this is not necessary. We technically need to create a new message id for every new message. Maybe some explanation about that and why we are doing the way we are doing in this example. Can you add the following line as well\nTechnically each new message that we emit should be annottated with a unique message id. This will allow the spout to keep track of which messages should be acked back to the producer/retried when appropriate ack/fail happens. However for the sake of simplicitity we are tagging all tuples in this example to have the same message id.. why are we adding this dep?. Does Files.write do an open/write/close? Isnt that inefficient to do it per element. I believe a better example is a Copy example where in you read from one File Source and copy it to another File Sink. actually the better way to do this would be to add edison backports and just do apt-get install.. It's ok for now. When we will have say a twitter connector handy, we could think of converting this example to reading twitter and writing to a file. Do you want to instead join on the adid/userid combo?. this reduce will not needed once you change the key in the join. We need a better example for repartition.\n. might be better to pipe this to the log operator. Currently the translation occurs at the end of the step, in StormSubmitter. Any reason  not to do it there? That way there will be just one centralized place of doing it. I think one of attempts of these examples is to show that one can use just the high level api without relying on anything from the low level api. . Not needed. Reverted. Changed. This is primarily for Kryo. Made it non public. Can you add a little more color to what this port is? . What is this?. Can this be done better?. Can we pl not do this. I specifically want no dep with anything heron other than api.. Not really, but I've added some checks just for safety. Expanded comment and also log every so often. Addressed by adding bytes dropped. Added. Sounds good. Added. Redid the logic. I think as long as we are counting drops, we should be ok. Nopes. can you comment on what's going on here?. why the check comp->has_config?. same as above. all new code should use std::string. sp_string is legacy. HandleUpdateRuntimeConfigRequest. tmaster_->UpdateRuntimeConfigTopology. If I read the code correctly, if a particular component doest not have a comp specific configuration initially, I wont be able to add something specific there. Why this limitation?. hashCode can return negative numbers. % according to spec can be negative if dividend is negative(it really is implementation specific, but looks like atleast some versions of jvm return negative).. The issue is that if index is negative, list.get(index) fails.. @nwangtw Good catch!\nIt looks like python already does this but c++ doesn't. I've added that logic as well.. is there any documentation that describes whats this pr aims to do? Without it its very hard to understand whats going on. exactly what does this data version do?. does this need to be passed to cjptmgr? Doesn't ckptmgr know this info. IMO this really pollutes the abstraction. You have now introduced some kind of versioning in the system at such a low level to begin with without any kind of higher level api/rationale. . my comment was more that when you change the parallelism, wouldnt the ckptmgr be restarted also and thus it too knows the topology structure?. Is this just instance or partition or instance and partition? Its unclear, especially since it does not have anything to do with partition right now. My suggesstion would be to leave the name of InstanceStateCheckpoint for now and rename later?. Adding IRichBolt interface to the streamlet really mixes two things that stand for different things. Here is one potential compromise. Leave the streamlet interface the same, and maybe have something that inherits from streamlet but which has more interfaces like this? Would that leave pure streamlet pure while at the same time help the migration, etc?. \n",
    "vidit-bhatia": "hi @joestein with the recent announcement from Adobe and Microsoft collaboration to support Azure. We have an ongoing project in discussion that will use Heron . I was wondering if you have already started working on this or if you have an ETA\n. HI @kramasamy  signed the CLA.\nVidit\n. @kramasamy @lucperkins looks good committing now\n. @kramasamy done\n. ",
    "msbarry": "Awesome, thanks @lucperkins!\n. Since gulp is a local dependency and npm run build uses the local installation, we don't need it installed globally.  I verified this works.  In the README.md, it just lists gulp as a tool \"that the documentation was built with\" which seems fine to me.\n. Ahh yeah it's because now instead of calling gulp globally, the makefile calls gulp via an npm run script which references the local copy of gulp that npm install puts in node_modules.\n. @kramasamy any thoughts on these homepage changes?\n. @saileshmittal looks like your header ends up being at the top of the screen, but behind the fixed nav bar.  You can add this CSS hack to this page to adjust for this:\ncss\na[name] {\n  padding-top: 75px;\n  margin-top: -75px;\n}\n. ",
    "bmhatfield": "@billonahill thanks for all the feedback. I've addressed most of it, with one lingering question inline.\n. Not sure what the deal with the travis failure - it seems totally unrelated to my changes?\n[2016-05-11 19:45:26 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /home/travis/.heron/heron-0.13.4-18-gb9c0fd2/client/release.yaml  \nException in thread \"main\" mapping values are not allowed here\n in 'reader', line 8, column 38:\n     ... .build.git.commit.message : Docs: Add Graphite to TOC\n                                         ^\n    at org.yaml.snakeyaml.scanner.ScannerImpl.fetchValue(ScannerImpl.java:871)\n    at org.yaml.snakeyaml.scanner.ScannerImpl.fetchMoreTokens(ScannerImpl.java:360)\n    at org.yaml.snakeyaml.scanner.ScannerImpl.checkToken(ScannerImpl.java:226)\n    at org.yaml.snakeyaml.parser.ParserImpl$ParseBlockMappingKey.produce(ParserImpl.java:558)\n    at org.yaml.snakeyaml.parser.ParserImpl.peekEvent(ParserImpl.java:158)\n    at org.yaml.snakeyaml.parser.ParserImpl.checkEvent(ParserImpl.java:143)\n    at org.yaml.snakeyaml.composer.Composer.composeMappingNode(Composer.java:224)\n    at org.yaml.snakeyaml.composer.Composer.composeNode(Composer.java:155)\n    at org.yaml.snakeyaml.composer.Composer.composeDocument(Composer.java:122)\n    at org.yaml.snakeyaml.composer.Composer.getSingleNode(Composer.java:105)\n    at org.yaml.snakeyaml.constructor.BaseConstructor.getSingleData(BaseConstructor.java:120)\n    at org.yaml.snakeyaml.Yaml.loadFromReader(Yaml.java:450)\n    at org.yaml.snakeyaml.Yaml.load(Yaml.java:381)\n    at com.twitter.heron.common.config.ConfigReader.loadFile(ConfigReader.java:78)\n    at com.twitter.heron.spi.common.ClusterConfig.loadReleaseConfig(ClusterConfig.java:138)\n    at com.twitter.heron.spi.common.ClusterConfig.loadConfig(ClusterConfig.java:169)\n    at com.twitter.heron.scheduler.SubmitterMain.defaultConfigs(SubmitterMain.java:94)\n    at com.twitter.heron.scheduler.SubmitterMain.main(SubmitterMain.java:307)\nERROR: Failed to launch topology 'IntegrationTest_LocalReadWriteTopology' because User main failed with status 1. Bailing out...\n. Oh my goodness, it seems like it's blowing up because I have a commit message with a :?\n. Approved. I was a little concerned about user input here, but it looks like it should work:\n[bhatfield heron@master]% message='`echo oops`'\n[bhatfield heron@master]% echo \"THING \\\"${message}\\\"\"\nTHING \"`echo oops`\"\n. Done.\n. I only capitalized proper nouns - is this a request for me to switch all the headings to be Title Cased?\n. ",
    "DarinJ": "@joestein I've go an interest in this and the kafka spouts, happy to take a look and run it through my test infra if I can get access to your repo.\n. Joe,\nStarted looking at this, tried running it and noticed the executor kept crashing due to some double quotes. \nRun command is pasted below for reference.\nRegistered executor on ip-172-31-2-167.eu-central-1.compute.internal\nStarting task task:container_1_2a30610b-168f-402c-add8-73a39be9c6fe:1464924583725:1336\nForked command at 6469\nsh -c 'tar -xvf test17-ubuntu-tag-0-4204049127527475993 && rm test17-ubuntu-tag-0-4204049127527475993 heron-core-release.tar.gz && ./heron-core/bin/heron-executor 1 \"test17\" \"test17af4487e3-bf2a-44a1-aec4-93b42d5c55d1\" \"test17.defn\" \"1:word:2:0:exclaim1:1:0\" \"172.31.11.31:2181\" \"/heron\" \"./heron-core/bin/heron-tmaster\" \"./heron-core/bin/heron-stmgr\" \"./heron-core/lib/metricsmgr/*\" \"\"LVhYOitIZWFwRHVtcE9uT3V0T2ZNZW1vcnlFcnJvcg&equals;&equals;\"\" \"heron-examples.jar\" \"31004\" \"31000\" \"31005\" \"./heron-conf/heron_internals.yaml\" \"exclaim1:536870912,word:536870912\" \"\"\"\" \"jar\" \"heron-examples.jar\" \"/usr/lib/jvm/java-8-oracle\" \"31003\" \"./heron-core/bin/heron-shell\" \"31002\" \"mesos\" \"ubuntu\" \"default\" \"./heron-core/lib/instance/*\" \"./heron-conf/metrics_sinks.yaml\" \"./heron-core/lib/scheduler/*:./heron-core/lib/packing/*:./heron-core/lib/statemgr/*\" \"31001\"'\nCommand exited with status 2 (pid: 6469)\nShutting down\n. Figured that bit out, see inline comments.  I'm able to now run the exclamation topology.\n. @kramasamy yes they appear, and I'm seeing tuples pass through.  I can also confirm the I can kill the topology and it is torn down properly in mesos.  The main thing I'm looking at now is the HdfsUploader, but that'll be another day.\n. @kramasamy was able to get working with a http or shared disk, using a patch described above (some arguments were being double escaped with quotes and broke the executor command line on at least Ubuntu + mesos 28.1). \nI also had issues with the hdfsuploader, but that may be a user error and I didn't have time to investigate.  It seemed curl was being called to download the artifacts stored in hdfs.  I've got some cycles next week and could check it out again.  If someone's succeeded with the hdfsuploader I would love to hear about it.\n. Here was the issue.  s/\\\"\\\"/gc and the heron-executor ran.\n. This is why it was double escaped.\n. ",
    "sijie": "I saw the contrib dir is removed. what is the principle to manage the spouts?\n. @kramasamy I am okay with that. @khurrumnasimm feel free to work on it.. @wangli1426 it stands for 'open source software'.. @khurrumnasimm feel free to send out the pull request once you have a change ready for review. I would help review it.. @huijunwu \nAs @maosongfu pointed out, this issue is for using bookkeeper for storing topology jars.\nI have a pull request for use bookkeeper as the statefulstorage. it is coming out soon.. @cckellogg I will try to standardize the naming.. pushed new changes:\n\nadd dependencies (bump a few. if zk is a concern, I can take a look at how to shade it)\nadd tests\nfix some errorprone error after bumping guava version\nstandardize name to \"dlog\" because the URI is starting with \"distributedlog\". @kramasamy @cckellogg moved the heron/dlog to 'heron/io/dlog`. @kramasamy @cckellogg : \n\nI pushed a few new changes:\n\nusing a shaded dlog jar. it simplifies the dependency management.\nadd a config for the number of replicas used for storing topology jars\nadd the dlog-uploader in the apiserver. @cckellogg @kramasamy what is the next step for this?. @nlu90 will try to improve it in future pull requests.. @nlu90 the method is a void method with parameters. I tried to mock static. but I never successfully mock it.\n\nPowerMockito.mockStatic(Extractor.class);\n    PowerMockito.doNothing().when(Extractor.class, \"extract\", any(InputStream.class), any(Path.class));\nbut the stack trace shows that the reflection was happening, but the actual method was still called.\n1) testDownload(com.twitter.heron.downloader.DLDownloaderTest)\njava.io.IOException: Stream closed\n        at java.io.BufferedInputStream.getInIfOpen(BufferedInputStream.java:159)\n        at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:265)\n        at org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.init(GzipCompressorInputStream.java:154)\n        at org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.<init>(GzipCompressorInputStream.java:137)\n        at org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream.<init>(GzipCompressorInputStream.java:102)\n        at com.twitter.heron.downloader.Extractor.extract(Extractor.java:34)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.powermock.reflect.internal.WhiteboxImpl.performMethodInvocation(WhiteboxImpl.java:1873)\n        at org.powermock.reflect.internal.WhiteboxImpl.doInvokeMethod(WhiteboxImpl.java:773)\n        at org.powermock.reflect.internal.WhiteboxImpl.invokeMethod(WhiteboxImpl.java:753)\n        at org.powermock.reflect.Whitebox.invokeMethod(Whitebox.java:466)\n        at org.powermock.api.mockito.internal.expectation.PowerMockitoStubberImpl.when(PowerMockitoStubberImpl.java:106)\n        at com.twitter.heron.downloader.DLDownloaderTest.testDownload(DLDownloaderTest.java:90)\nany ideas? I've followed the instructions from PowerMockito - https://github.com/powermock/powermock/wiki/Mockito#mocking-static-method. @nlu90 \nyes. I added @PrepareForTest(Extractor.class) and the test case.\nI tried bunch of options\n\nspy & mockStatic\ndoNothing().when(...)\n\nIt didn't work out.\nHere is one of my diff\n```\ndiff --git a/heron/downloaders/tests/java/com/twitter/heron/downloader/DLDownloaderTest.java b/heron/downloaders/tests/java/com/twitter/heron/downloader/DLDownloaderTest.java\nindex cffccd414..d5ba1028b 100644\n--- a/heron/downloaders/tests/java/com/twitter/heron/downloader/DLDownloaderTest.java\n+++ b/heron/downloaders/tests/java/com/twitter/heron/downloader/DLDownloaderTest.java\n@@ -27,6 +27,9 @@ import org.apache.distributedlog.api.namespace.Namespace;\n import org.apache.distributedlog.api.namespace.NamespaceBuilder;\n import org.apache.distributedlog.exceptions.EndOfStreamException;\n import org.junit.Test;\n+import org.mockito.stubbing.Answer;\n+import org.powermock.api.mockito.PowerMockito;\n+import org.powermock.core.classloader.annotations.PrepareForTest;\nimport com.twitter.heron.dlog.DLInputStream;\n@@ -41,6 +44,7 @@ import static org.mockito.Mockito.times;\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.when;\n+@PrepareForTest(Extractor.class)\n public class DLDownloaderTest {\n@Test\n@@ -82,9 +86,10 @@ public class DLDownloaderTest {\n     when(nsBuilder.uri(any(URI.class))).thenReturn(nsBuilder);\n     when(nsBuilder.build()).thenReturn(ns);\n\nExtractor extractor = mock(Extractor.class);\nPowerMockito.mockStatic(Extractor.class);\n\nPowerMockito.doNothing().when(Extractor.class, \"extract\", any(InputStream.class), any(Path.class));\n\n\nDLDownloader downloader = new DLDownloader(() -> nsBuilder, extractor);\n\n\nDLDownloader downloader = new DLDownloader(() -> nsBuilder);\n     downloader.download(uri, path);\nURI parentUri = URI.create(\"distributedlog://127.0.0.1/test/distributedlog\");\n@@ -92,7 +97,8 @@ public class DLDownloaderTest {\n verify(nsBuilder, times(1)).conf(eq(CONF));\n verify(nsBuilder, times(1)).uri(parentUri);\n\n\nverify(extractor, times(1)).extract(any(InputStream.class), eq(path));\n\nPowerMockito.verifyStatic(times(1));\nExtractor.extract(any(InputStream.class), eq(path));\n``. The exception was thrown when executingPowerMockito.doNothing().when(Extractor.class, \"extract\", any(InputStream.class), any(Path.class));. The stack trace showed that it is using reflection to load the class. but it turned out the mock wasn't actually setup even aftermockStatic` was called.\n\nI check all the examples in heron. It doesn't have a case that mocks a static void method with parameters. I am not sure if that is the reason. if you have any suggestions on how to address that, that would be great!. discussed with @nlu90 offline. he suggested adding runwith(PowerMockitoRunner). The tests pass now. I pushed the new changes and am waiting for travis-ci to complete the testing.\nthanks so much for the help from @nlu90 !. there was a checkstyle warning. retrigger the travis-ci.. The statefulstorage change is in this commit https://github.com/twitter/heron/pull/2287/commits/aecd679a5ea8a2e2668c72b0e2785dd0c3993988 for review.. /cc @srkukarni @kramasamy for reviews. merge with latest master.. @srkukarni this should be ready for review now. addressed the two checkstyle problems. waiting for CI to complete.. Created apache/bookkeeper#681 for providing a yaml file to run bookkeeper using StatefulSets.. @cckellogg ideally yes. I can add one here. . @cckellogg added pod affinity. @mjschmidt what kind of errors do you encounter?. do you mean daemonset doesn't work for you and statefulset works for you?. @mjschmidt do you any logs about the crash loop? and how many nodes do you use? I am wondering if this is related to the number of nodes you are using. because daemonset start one pod each node.. Okay. I will try to reproduce myself and see if I can find the problem.. /cc @cckellogg . @kramasamy we increased directory memory size. so this already addressed the issue for now. I will work on the change to improve it. /cc @cckellogg . I think dlog 0.5.0 doesn't support /. in order to support /, we have to bump the dlog version to the latest version.. we don't support 'dlog://' in distributedlog now. if we want to support that, it requires a change on distributedlog itself.. I will change that.. nice caught. fixed . @cckellogg I can remove direct dependency. however the dlog/bk library depends on guava. I can try to shade them in dlog library. does that work for you?. I changed it from static method to a class method of a singleton for unit testing. so I can easily mock Extractor for unit testing.. can't we just use \"{ \" + key + \" : \" + value + \" }\"?. do we really need String.valueOf or toString? if you just \"{ key: \" + key + \" window: \" + window + \" }\", it is better than explicitly call toString, because it handles null field, while window.toString would throw NPE if window is null.. SerializableFunction is a java function, not a scala function. which I think you can't really use all those scala syntactic sugar. if we are generating a scala oriented api, it would be better to think about taking scala function, rather than using java functions.. yeah I suggested changing it to Apache Heron.. I am a bit confused here, I see JQuery is listed at all 3 sections, Apache 2.0, MIT, GPL. Is that expected?\nGPL is a problem. GPL is in category-x : https://www.apache.org/legal/resolved.html#category-x. @nlu90 for dual licensed, just pick the one that is compatible with ALv2. so MIT is good enough.. ",
    "khurrumnasimm": "Is there anyone active working on this? If no one, I am interested in contributing.. ",
    "wangli1426": "May I ask what does OSS mean?\nOn 6 December 2016 at 04:08, Sijie Guo notifications@github.com wrote:\n\n@kramasamy https://github.com/kramasamy I am okay with that.\n@khurrumnasimm https://github.com/khurrumnasimm feel free to work on it.\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/645#issuecomment-264962401, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AFBD9I9lrYiT1PvPF3K1b_XDj2mZr03eks5rFG9FgaJpZM4IdUqm\n.\n. The paper I refer to in #1422 calculates the optimal parallelism of the operators with two steps.\n\nThe first phase reasons about the minimal parallelism each operator requires to avoid being the performance bottleneck of the topology. The judgement is straightforward and based on a few metrics, such as input rate and operator processing rate.\nThe second phase is a little complicated. The algorithm leverages the queuing theory to estimate the tuple processing latency given the current parallelism of the operators. If the estimated latency is higher than the user-defined constraint, the algorithm increases the parallelism of some operators such that the desirable latency can be guaranteed with minimal parallelism increase.\nIMO, we can start with the first phase, which is sufficient enough in practice. What do you think? \n. @billonahill The code is available here. You can also find a simple World Count Example.\nThe code is for validating our ideas of achieving elasticity in streaming systems. And I am reorganizing the code recently. We are open to discussing the possibility of implementing some of its elastic features in Heron and are willing to contribute. Any feedback is appreciated.. Hi @billonahill,\nThis feature is very interesting. Actually I have done something similar thing on Apache Storm, to adjust the parallelism of a bolt according to its instantaneous workload. If you are interested, we may have discussion for more details. \nThanks.\n. Hi @billonahill,\nYes, I was referring to the auto-scaling functionalities, which consists of two parts: (1) the mechanism achieving scaling operators and (2) the algorithm to determine the optimal parallelism of the operators.\nFor (1), to support scaling of stateful operators, we model the operator state as key-value pairs. And the scaling is done by re-partitioning the operator state. I will send you our paper about run-time operator scaling if you are interested. I am wondering how you plan to deal with operator state when scaling up or down. Will you store the operator state to a persistent store before scaling? Will you consider live scaling without deactivating the current topology? \nFor (2), we have a paper that utilizes the Queueing Theory to determine the optimal parallelism of each operator given the user-defined tuple processing latency constraint. The first step of the algorithm is to reason about the minimal parallelism of each operator to avoid being the performance bottleneck. This is can be easily computed by $\\lambda/u$, where $\\lambda$ and $u$ is the arrival rate and the operator currently processing rate respectively. \nI have implemented the auto-scaling functionality based on Storm. And I am willing to make contributions to Heron on this feature.\nThanks.\nLi\n. @avflor You are welcome. \n. Hi @billonahill @kramasamy @avflor,\nI proposed a new operator for live scaling in #1499, please review. \nThanks.\n. @billonahill Your comments are addressed and the CLA is accepted. Thanks.\n. OK, I missed that sentence. Thanks.\n\nOn Sep 27, 2016, at 12:59 PM, Bill Graham notifications@github.com wrote:\n@billonahill commented on this pull request.\nIn website/content/docs/getting-started.md https://github.com/twitter/heron/pull/1435:\n\n@@ -178,6 +178,8 @@ You can open Heron UI in your browser at [http://localhost:8889](http://localhos\n and see something like this upon successful submission of the topology:\n \n\n+Note: heron-tracker (in Step 4) should be running at the same time so that Heron UI can work properly. \nThe first sentence section of this section is Heron UI is a user interface that uses Heron Tracker. The text is very clear IMO that the tracker needs to be running.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/pull/1435, or mute the thread https://github.com/notifications/unsubscribe-auth/AFBD9OcXbnsZG_LhN4F3tFDN7cSeXAA8ks5quKLNgaJpZM4KHNlM.\n. Hi @avflor ,\n\nThanks for your response.\nActually, my proposal refers to both 1) and 2). The second one makes it possible for the system to migrate a subset of state among Bolt Instances. And the first makes sure the tuples are sent to the right Bolt Instances after state migration.\nFor 1), you are right. Re-routing can be done in the StreamManager by modifying the key to tasks mapping. \nFor 2), as data flows among HeronInstances are via stream mananger, it is reasonable to migrate the state by stream manager. \n. @avflor \nThanks for your comments.\n\nI think it is not that simple to do state migration through the stream manager. I believe we need a central external unit to do the state migration as we will avoid many coordination problems between stream managers. \n\nI agree with you. Stream manager alone is not sufficient. As stated in my proposal, we need a centralized scaling delegate to coordinate the state migration and the rerouting. For instance, before a key is migrated from a HeronInstance to another, we need to make sure all the tuples sent to the HeronInstance has been completed processed. \n\nThere are multiple things that need to be synchronized: we need to make sure tuples are not routed to the original bolts while migrating the state of the bolts, we need to know when all state migrations have been completed so that processing is resumed, we need to handle failures (e.g, a new bolt failed while receiving state).\n\nYes, to migrate a set of key, say k from HeronInstance A to HeronInstance B, we need the following steps.\n1) stop sending tuples with keys in k to A;\n2) make sure all tuples with keys in k sent to A has been processed;\n3) migrate  the involved state from A to B;\n4) resume sending tuples with keys in key to B.\n\nMoreover, I'm not sure how we will make the heron instance understand that it is not receiving regular data tuples through the stream manager but some other bolt's state.\n\nWe can add additional fields to HeronTuples.HeronDataTuple to indicate whether a tuple is a regular tuple or control tuple, and process the tuples properly at BoltInstance.java:216 like this:\njava\n      for (HeronTuples.HeronDataTuple dataTuple : tuples.getData().getTuplesList()) {\n        if(dataTuple.type == ControlTuple) {\n             //do the control work such as state migration, state merging.\n        } else\n            handleDataTuple(dataTuple, topologyContext, stream);\n      }\n. @billonahill \nThanks for the comments.\n\nQ1.) Regarding moving existing state when increasing parallelism, in the above flow how exactly do you determine which keys k need to move? Would we have to run all current keys in A through the updated hashing algorithm or is there a better way?\nQ2.) Related to Q1 but your example shows how to migrate state from A -> B, but when adding new instances we'd be going from Set_n -> Set_n + Set_m. Would we need to inspect keys on all Set_n to see what needs to move to Set_m? \n\nThis is a good question. A naive method is to change the hash function when new task is created. This method however results in a large number of state migration. For instance, if we increase the number of tasks from n to n+m, then for each task approximately (n+m-1)/(n+m) of the keys will be migrated to other tasks, which results in expensive state migration overhead and significant scaling delay. Another method is to maintain a key-to-task mapping in the routing table. This allows us to reassign a few keys to the new tasks when scaling. However, the maintaining overhead could be large and the lookup to the mapping will cause a large number of cache misses when there are many distinct keys. My idea is to use a two-tire routing table. We logically partition the key space of an operator into many, e.g., 512, mini-partitions, called shards. This can be done by a global hash function. And we maintain shard-to-task mapping and monitor the workload on each shard. When a new task is created, we can reassign some shards from existing tasks to the new one based on the workload of the shards. This problem is similar to Multi-way partition problem which divides a set of n integers into a given number k of subsets, minimizing the difference between the smallest and the largest subset sums. The difference is that we also want to minimize the shard reassignments. This problem can be solved by using a heuristic algorithm similar to First-Fit-Decreasing. The benefit of the two-tire routing is that it gives us great flexibility to minimize the shard reassignments while does not bring much additional overhead. The shard reassignments enabled by the two-tire routing not only supports operator scaling, but also can be used for inter-task load balancing when the workload is skewed among the tasks. \n\nQ3.) Re implementation, could this be done with composition instead of inheritance? This would provide more flexibility to the topology author. So instead of extending the operator the bolt could implement StatefulComponent which could cause a configured implementation of a StateStore to be injected by the framework, or something like that.\n\nThe ElasticStatefulOperator also exposes some methods to the system for state management, like this:\n``` java\npublic abstract class ElasticStatefulOperator implements Serializable {\nprivate KeyValueState state = new KeyValueState();\n...\n\n// to extract a subset of state for migration\n   KeyValueState getState(StateFilter filter){...};\n// to merge the new state migrated to the current instance\n   void MergeState(KeyValueState newState){...};\n}\n```\nIf this is done with composition instead of inheritance, I am afraid the system cannot get access to the StatefulComponent member in their class.\n\nAlso we might want to avoid terminology specific to scaling (i.e., elastic) since this functionality wouldn't be limited to just scaling. It might be used when a faulty node gets restarted for example.\n\nI agree. A better name might to be StatefulOperator, StatefulBolt or StatefulKeyValueOperator. We can choose a proper name after we have reached an agreement on the design.\nThanks.\n. @billonahill @maosongfu @avflor \nAny further feedback on this proposal?\n. @maosongfu Got it. Thank you!\n. @billonahill @maosongfu @avflor \nAny further comment?\n. @maosongfu Thanks for the update. I look forward to the doc.\n. @maosongfu @billonahill @avflor @kramasamy Any update? I am more than eager to make contribution to new features regarding to operator state management and operator scaling. \n. @billonahill @maosongfu glad to know that. Look forward to the doc.\n. @maosongfu May I ask if the doc is ready to share? :). @kramasamy First, thanks for your interest to my code. As a spout is typically used to receive input stream from external system / source for the topology, it is usually not computationally intensive. Of cause, we can also implement elastic spout in a similar way, if needed.\n. @kramasamy any comments?. I agree. IMO, least disruption in terms of plan changes is important, even if the underlying engine supports elastic operations, e.g. operator scaling, task migration. That's because plan changes always come with cost.\n. @ashvina Heron currently does not support restoring operator state (you referred to as in-memory state) after restarting a heron instance. But I believe it will be achieved in the near future. \nThere are two ways to scale an operator. The first way is to kill all the instances of the operator and then restart a desirable number of instances. To guarantee operator state consistency, before an instance is called, the associated state should be written to a persistent storage. When new instances start, the state is restored from the persistent storage. Additional efforts are needed to decide how to re-partition the state to the instances. \nThe second way is to redistribute the state from the instances to delete among the existing instances, or migration partial state from existing instances to the instances to create. By control the routing rules in the stream manager and the process of state migration carefully, live migration can be achieved without restarting instances. Compared with the first method, this method significantly reduces the interruption to the data processing, but comes with high implementation complex. So I believe we can implement the first method first.\n. Is there any plan to support HeronInstance migration among different containers? With this feature, we can easily design algorithm to remove the under-utilized containers.\nFor instance, consider we have two containers: (A A B B)  ( B B A). After B is scaled-down to only one instance, the containers look like: (A A B) ( A ). If HeronInstance migration is supported, we can move A from the second container to the first one and release the second container.\n. I agree with @ashvina . It seems that we mixed different objectives, say load balance and minimization of # of containers, together. \nIMO, if we consider the problem we discuss as a Bin Packing problem, minimization of the # of containers used implicitly covers the load balance objective. Because you can remove more containers, only if the containers are well balanced. The difference is that in our scenario there is already a packing plan, so we also want to minimize the instance movement cost (Removing an instance has no cost, but moving an instance does) when doing the bin packing. By proposing a algorithm to solve the bin packing problem with the consideration on minimization the instance movement overhead, we can achieve both load balance, container removal and operator scaling-up/down, simultaneously.\n. Or, alternatively, we can remove the ack() statement in the ConsumerBolt to avoid misleading the users.\n. Since how to use ack is demonstrated in AckingTopology, I believe it's better to remove the ack() statement in the ConsumerBolt.\n. @maosongfu I agree. So I closed the RP for the first approach.\n. Sorry I did noticed that there is an AckingTopology demonstrating how to enable ack. So now I believe a better way here is to remove the ack() statement in ConsumerBolt to avoid confusing the user. I will do it in another RP.\n. @billonahill I have reviewed this PR. It is smart to use a scorer here. Great work! I have two comments:\n1)  Since instance insertion is done by searching the containers from the smallest container id to the largest one until a container with enough resource is found, you may want to remove the instances of the scaling-down component from the containers in a reverse order. In this way, the container with the largest id is more likely to be empty eventually, making it possible to release this empty container in order to improve the resource utilization.\n2) If a container becomes empty after a component scaled down, will the container be released?\nThanks.. LGTM. After Step 3, as heron-tracker is still running in the command line, the user might kill it by control + c, so as to type the command to launch Heron Ui as suggested in Step 4. In such case, the heron-ui will not work properly and the users will be confused, because from the instructions alone they don't know heron-tracker should be running along with heron-ui. \nThis is what exactly happened to me, when I tried to run Heron local mode step by step following the instructions. So I propose this PR, in hope that the misunderstanding will not happen on other users. \n. a typo: starte -> start. Is a reverse scorer for instance removal better? IMO, inserting and removing instances among the containers in reverse direct will result in better resource utilization.. ",
    "huijunwu": "\nAnother example, of a scheduler is\ncontrib/scheduler/yarn-1.0.1/src/java/com/twitter/scheduler/yarn/...\n\nI saw yarn in heron/heron/schedulers/src/java/com/twitter/heron/scheduler/yarn/.\nwhat scheduler should be in contrib dir? and what scheduler shoud not?. @chatterjeesubarna i guess, your first submit created some metadata in zookeeper, which your second submit conflited with. i suggest try to submit with a different name\nheron submit aurora/root/devel --config-path ~/.heron/conf/ ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopologyDifferent1 --verbose\n@maosongfu to confirm. @Laxman-SM N+1 for Aurora. There is no AM nor Scheduler containers. Only Tmaster container exists.. @objmagic #1720 passed all the CI and integration tests. Anyway, I will check it again. @pankajroark make sense. will do. beg for review. #2207 replaced this pr.. Any updates on this PR?. if (!cacheMetric.isEmpty()) {\n   for ( ;; /*this may throw exception*/ ) {\n      // _remove_ changes map\n   }\n}\ni do not think 'puting the forloop inside ifblock' can handle the exception throw from forloop.\nthe loop changes the collection and the exception may be thrown from inside ifblock.. link to #1735 . @billonahill \nThe time when 'termination tuple' arrives at the IdentityBolt is different everytime. If a 'termination tuple' arrives between two normal tuple, the count is one more than expected and the acking tuple is one more than expected. \nUsually, the 'termination tuple' is after all the normal tuples, so there is no issue. \nThere are 6 spouts in the test. If one of them runs fast and sends out 'termination tuple' before the other spouts sending out all normal tuples, the wrong count appears.. see #1847. @srkukarni Is there a way to prevent stmgr heap from growing beyond a certain size?. For http semantics, kill-executor updates/writes the container state rather than query/read/GET. POST is more proper.. During compiling process:\nERROR: /home/ubuntu/heron/third_party/python/pylint/BUILD:7:1: null failed: _pex failed: error executing command bazel-out/host/bin/third_party/pex/_pex --disable-cache --entry-point third_party.python.pylint.main bazel-out/host/bin/third_party/python/pylint/pylint.pex ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n**** Failed to install pylint-1.5.5. stdout:\n**** Failed to install pylint-1.5.5. stderr:\nTraceback (most recent call last):\n  File \"\", line 2, in \nImportError: No module named setuptools\nTraceback (most recent call last):\n  File \"/home/ubuntu/.cache/bazel/_bazel_ubuntu/49c4744f9ca584202f9a40692e0a51e0/execroot/heron/bazel-out/host/bin/third_party/pex/_pex.runfiles/main/third_party/pex/_pex.py\", line 211, in \n    sys.exit(main())\n  File \"/home/ubuntu/.cache/bazel/_bazel_ubuntu/49c4744f9ca584202f9a40692e0a51e0/execroot/heron/bazel-out/host/bin/third_party/pex/_pex.runfiles/main/third_party/pex/_pex.py\", line 162, in main\n    resolver_options_builder, interpreter=interpreter)\n  File \"/home/ubuntu/.cache/bazel/_bazel_ubuntu/49c4744f9ca584202f9a40692e0a51e0/execroot/heron/bazel-out/host/bin/third_party/pex/_pex.runfiles/main/third_party/pex/pex/bin/pex.py\", line 490, in build_pex\n    resolveds = resolver.resolve(resolvables)\n  File \"/home/ubuntu/.cache/bazel/_bazel_ubuntu/49c4744f9ca584202f9a40692e0a51e0/execroot/heron/bazel-out/host/bin/third_party/pex/_pex.runfiles/main/third_party/pex/pex/resolver.py\", line 200, in resolve\n    dist = self.build(package, resolvable.options)\n  File \"/home/ubuntu/.cache/bazel/_bazel_ubuntu/49c4744f9ca584202f9a40692e0a51e0/execroot/heron/bazel-out/host/bin/third_party/pex/_pex.runfiles/main/third_party/pex/pex/resolver.py\", line 168, in build\n    raise Untranslateable('Package %s is not translateable by %s' % (package, translator))\npex.resolver.Untranslateable: Package SourcePackage(u'https://pypi.python.org/packages/d3/8b/a82129243571dba8f37e8b488265984d86ddd881faf82745ede600627871/pylint-1.5.5.tar.gz#md5=208a6365f81ba072293539ae51614d09') is not translateable by ChainedTranslator(EggTranslator, SourceTranslator)\nINFO: Elapsed time: 11.423s, Critical Path: 9.39s\nsudo apt-get install python-setuptools solved it.\n. @objmagic see the previous comment, GetCachedSize was suggested @srkukarni . These packing.algorithm configs are set in yaml as well. What happens if config in yaml conflicts with setting in topology writter's code?\nDoes this PR violate 'Neither system- nor component-level configurations can be overridden by topology developers.' in document https://twitter.github.io/heron/docs/operators/configuration/config-intro/. notes from slack channnel\n\nexisting - library mode:\ncurrently the mode of deployment is called library mode, where we don't have any services running - but a lot of configuration is in the client side. this mode is advantageous since there is no need to run any services.\nthis pr - services mode:\nthe api server adds another mode of deployment. in this mode, there will not be any configuration at the client side - api server manages everything, include submission/kill/activate/deactivate/ etc. heron cli will use the rest api to even submit jobs. note that we will support both the modes\nfuture - tracker mode:\nheron tracker in python duplicates state manager code in python as well. if we can make api-server support the heron tracker api in addition to other api, we can run the api server - both in tracker mode and api server mode. \ninitially current tracker and api server will be in separate directory until the good amount of testing is done. once it is done, we can think about cutting over to use api server just as tracker in library mode.\nps:\nnone of the existing deployment will change\n. Assume [stmgr A -> out queue(destination stmgr B)] -> [in queue -> stmgr B]. The out queue(destination stmgr B) size in stmgr A may indidate the processing ability of stmgr B. A simple idea is sending out tuples according to out queue(destination stmgr B) size.\n. is bookkeeper a candidate for heron/heron/statefulstorages/src/java/com/twitter/heron/statefulstorage/. replaced by #2886. close this pr. does this pr replace #2301?. the motivation looks good to me.\nanother curious question, what version does heronpy use?. some issues in centos\nERROR: /home/ubuntu/heron/scripts/centos/BUILD:316:1: no such package 'heron/cli2/src/python': BUILD file not found on package path and referenced by '//scripts/centos:hcli2'\nERROR: /home/ubuntu/heron/scripts/centos/BUILD:336:1: no such package 'heron/cli2/src/python': BUILD file not found on package path and referenced by '//scripts/centos:hmesos-scheduler-config'\n. My previous description included the two results. tracker result has 1 instance. tmaster result has 5 instances.\n\nBesides the previous 2 directions from healthmgr perspective, another direction is to unify tmaster/tracker metric query interface, so that there should be only one metric query interface. Then only one metric query client is necessary and this query client can be re-used. Since Dhalion.MetricsProvider is not backpressure specific, the MetricscacheMetricsprovider should not be backpressure specific. \nI think the backpressure Sensor is backpressure specific, where the bakcpressure specific fix should go @ashvina \nBesides, I do not think  'to guess instance-id from metric-name' is proper. when you query by  to tmaster, it means you want all instances. The result should include all instances. If the metric-value is empty, the result explicitly says the value is empty rather than hide the value implying the empty value. This Tmaster behavior exists for long time even before I started to work on this project .. \nThis fix is not for all the sensors. It is for backpressure sensor because backpressure sensor assumes query by  returns one instance instead of all instances. If the sensor just wants to query only one instance, it should pass instance-id in the query request. Thus I would suggest adding  query interface for Dhalion.MetricsPRovider besides  interface, which enables its query capability.. +1. very good proposal. @kramasamy  any further comments?. @nlu90  #2821 is big, this PR is small. so i think we may merge the big then refactor this small.. Talked with @ashvina yesterday.\nBesides reporting the invocation counts, we may also report the results (output of sensor/detector/diagnoser/resolver) in state-cache, which will be added in another PR.. gitbox commits(when the PRs are merged) are to commits@ already.\nI think this Issue is: disable the PR/Issues comments email notification ?. @jerrypeng \n1. The current Downloader hard-code the registry map, which is not extendable.\n2. The Downloader should work in a similar way as other code in the heron, such as Uploader which adopts the yaml config for extension.\n. @jerrypeng updated to be compatible with downloader <topology-package-uri> <extract-destination>. could you review it again. not all schedulers. only local and yarn due to the yaml config.\n. put it on hold. lets find better way for aurora meta. updated according to Karthik suggestion 'Let us use one parameter and comma separated. heron.scheduler.properties is a good one.'. ship after ci pass. ship after ci pass. this pr does not touch schedulers. it works for both aurora and non-aurora. could you pls copy pr description into comment in the code so that readers can easily understand why we have this metric. will revert later after test. merge this pr for integration threads fix. the 3 integration failure fix will be in the next pr . #3162 . #3175. looks like protobuf 3.4.1 is not available in pypi. close this pr. after long run, the mean metric becomes obtuse. \nfor example, numerator_=10*1000, denominator_=1000, the mean is 10. \nthen, 20 value pulse 100 times only lead to 0.9 increase on the mean. \nnumerator_=10*1000+20*100, denominator_=1000+100, the mean is 10.9. \nis it the intention?. so it ignores the metric type and caches only the latest one metric. interface State is already a map. If a hashmap member placed here, it leads to nested map.. This code appears only once.\nThis PR is the prerequisite of the other PRs. If this PR is merged, this code will not appear in the other PRs.. The latch solves the race condition.\nThe server thread updates server_port, then latch is released, then the client thread can read server_port.. This function is run as thread with no return value.\nI tried reference, but compiler complained. I suggest pointer.. If the input _port is zero, which is not a actual port in use, it is not a valid input.\nIf the existing port_ in MetricsMgrClient is not zero, it should not be set to another value during MetricsMgrClient lifetime.. This PR depends on #1847. This piece code appears only once.\nIf #1847 is merged, this code will not be highlighted here.. Copied from #1847:\nThe compiler complained on reference here. Similar to MetricsMgrClient::SetPublisherPort\nThe stmgr_port_ should be set once if it was originally zero and keeps same during its lifetime.. This printout is added for future diagnosis. \nThe timeout issue was not reproduced. I suspect the timeout issue may be due to this timer action. \nIt does not print a lot since it is supposed to be called only once.. The compiler complained on the reference. The compiler complained on the reference. The metrics mgr client contructor does initiates the connection until the client Start(). This method is to supply the actual stmgr port before metrics mgr client Start(). agree. it will touch more code in the stmgr. will do. done. If stmgr A lost connection to stmgr B and tried 28 times on Monday, and re-connected successfully at the 29th attempt. Four days later on Friday, stmgr A lost connection to stmgr B again and tried to re-connect 2 times and quit.\nI suggest resetting reconnect_attempts_to 0 after A resumed connection to B on Monday. stmgr A can try another 30 attempts on Friday.. agree.. What is the reason for 'No operations before verifying that we are the master'?. is it local_data_port?. what is this patch file for?. @cckellogg no more concerns, LGTM. indent could be improved for better reading, or put ')' in a new line.. i would suggest either,\nkeep the -verbose now till the code is stable\nor,\nalign the -verbose with 'heron submit xxx -verbose': If user supplies verbose in cli, executor appends '-verbose'.. it looks like that component and instance are independent here.\nshall we enforce/verify that the instance should be of the above component?\nbesides, component and instance are flat in the table/context. is there a place to track the component-instance relation?. the new addresses may be put in the below if block. shall we limit the data to a recent time window?. why not put the two helper methods in MeasurementsTable. it can be put inside the below if. sgtm. sgtm. not necessary. will remove it\nactually, it was from: grep -r 'TODO:- is this a good thing?' heron. updated. updated. updated to keep the original format. updated. updated to guice. done. updated. updated. 1. The original mode includes disabled, cluster, standalone; rather than 2 dimensions.\n2. updated the other config files. updated. updated. updated. standalone is not suggested, thus we do not put in example config. updated. added activated\nlog warning for error config. updated according to @kramasamy suggestion. if i understand correctly, the new 'Map[AuroraField, String]' will be added in the method param list https://github.com/apache/incubator-heron/blob/0.17.8/heron/schedulers/src/java/com/twitter/heron/scheduler/aurora/AuroraController.java#L26. Thoughts to update the interface? @nlu90 @maosongfu . after discussion, i am going to update the method to boolean createJob(Map<AuroraField, String> auroraProperties, Map<String, String> extra);. IStatefulSpout I means interface ?. intended. the healthmgr updates runtime config by pulling new pplan at the begin of each policy loop round.. restricted by parent interface. added. no benefit. but align with dhalion style which uses Collection.Empty some where. changed to list. updated name.\nexposed spout + bolt, this method is replaced by adding spout and bolt return.. since multiple policy logs are mixed in the log loop. this helps to find when each policy begins.. updated. added. updated. updated. added comment. logical compiennt. the new repacking plan triggers the executor to launch the actual instnaces. if there is no container meta for this shard, keep shell only for health-check. health-check in shell returns ok then aurora returns. thus shell is mandatory to run in container. ",
    "alienxt": "Extracting Bazel installation...\nwhy the installer got stuck?\n. ",
    "Ishiihara": "@kramasamy I am interested in looking at this. Can you assign this issue to me?\n. @billonahill @maosongfu I am interested in contributing to this, can you assign this to me\uff1f\n. @kramasamy I just accept the invitation. \n. ",
    "skanjila": "@kramasamy, @Ishiihara I am interested in contributing to Heron and want to work on this issue, is this still an issue that needs attention.  If so I can go through steps to get on heron dev list and get started.  Also is there a JIRA associated with this where I can start looking at all the associated issues?  Thanks. Here's the work that I am planning:\n1) I have run a java to scala tool (http://javatoscala.com/) on the streamlet code to get an initial scala set of traits\n2) I have created a design doc and sent that out for review and am incorporating the feedback from the doc\nHere are the tasks needing to be completed:\n1) Will create a bazel build file in this directory to build this code\n2) Will have scala code invoke java implementations \n3) Will send a PR with this effort. can we add a method that allows one to collect state associated with a streamlet's state, also I would like to return a meaningful data structure from these methods, maybe a map[String,Int] that corresponds with a status and a meaningful error message. Is this annotation needed?. can you add some comments here as to what this class actually does?. should this also have the @SerialVersionUID annotation or not or is that only for classes. I see a pecularity in the java code that we should discuss, all the implemented Sink classes extend the StreamletOperator and all the classes like StreamletCloneTopology have inner classes that implement Sink, take a look at this and just make sure we're  kind of following the same trend, also we may want to have some different types of Sinks readily available for testing, take a look at the ComplexSink,ConumerSink and LogSink for reference, am wondering if we need to have these extend sink in the scala world and if not we need to figure out the representation of StreamletOperator in the scala world. I am assuming this class can be used across the Streamlet API , is that assumption correct, where do you see this class being used?. I see that you have a singleton object and a class, do we need both?. I think we need to make this object be general purpose for all of the scala traits to use, for the Builder stuff lets get this code review moving before I work more on that because I will add more to this function for the Builder stuff. It seems we are overriding the java Sink and plugging in our sink, however what happens if the sink is sufficiently complicated inside those methods, do we need to account for that. what does this imply when we pass the javaStreamlet as an arg and have it listed with a colon, the notation is a bit confusing here, can you add some comments. Yes I was saying I would like this checked in so that I can add those functions that you are describing. why was this change needed. can this become an perf issue if the number of clones are high?. I removed this class as its no longer needed. Done. Done. sourceFn stands for source function, what better name were you thinking about?. It is actually, I noticed that I was getting compilation errors if I didnt include this even though I had the appropriate import statement. I will leave it as is, this is a nice to have that I can put in once we make deeper changes, right now lets focus on getting Builder operational. See my comment above. Removed. Done, see newest checkin. Done as well, see newest checkin. Done. Will rename in a later commit. Yes that is indeed the intention. I will dig into that issue on the next commit. Changed in latest commit. Removed. Changed for both methods to leverage java builder. Changed. Changed. I believe this just takes an input collection and converts it to a Java collection, again I will dig into this as well on the next commit to simplify this, here's the issue I created to track the import alias issue and this: https://github.com/twitter/heron/issues/2772. why is this file needed?. should we use a later version of commons_lang. we should add comments around why things were put in . we should move the versions to be environment vars that can be read in. ",
    "alanngai": "ok.  I was just going by whatever the checkstyle rules was configured.  Should I change the rules?  There are also some fairly stringent rules regarding line length and exception catching scope.  Should I change those as well?\n\nOn May 23, 2016, at 5:05 PM, Sailesh Mittal notifications@github.com wrote:\n@alanngai https://github.com/alanngai We use 2 space indent. Could you change your setting to take that into account? It will decrease the diff length quite a bit.\nAlso, please try to create multiple PRs for different modules, so it will be easier to review.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub https://github.com/twitter/heron/pull/735#issuecomment-221131269\n. Will do.  Regarding #5, deleting parameters will alter the the api.  For example, in heron/cli/src/python/activate.py:46\ndef run(command, parser, cl_args, unknown_args):\n\u2018parser\u2019 and \u2018unknown_args\u2019 are unused, but if I remove them, I\u2019d change the function definition, and potentially break places that uses this function.  I see 3 options\n1. refactor all usages of this function and change the way they call this.  This option is dangerous because I don\u2019t know all the areas that use this, and because of duck typing, I may not be able to find all usages even with the help of and IDE\n2. change the checkstyle rules to ignore unused function params.  don\u2019t know how to do this yet, I assume there\u2019s a way\n3. redefine the function to use underscores in place of the unused params (see below).  looks pretty ugly to me, but it\u2019s an option\n   def run(command, , cl_args, ):\n   If you could point me to where the checkstyle config is that would be much appreciated.\n\nThanks,\nAlan\n\nOn May 23, 2016, at 5:16 PM, Sailesh Mittal notifications@github.com wrote:\nYes, please go ahead and change the rules.\nIndentation to 2 spaces\nIgnore imports. This is because of the way bazel and pex work. The imports have to have the scope starting from root directory of project.\nIgnore Exception rules. Most python code is lenient on exceptions.\nLength should be 100 chars per line. We need to fix them if they are violated.\nUnused params should be deleted.\nPlease create issues for the parts of codes that are not straightforward.\nIf you know how to enable these checks based on modules (cli or tracker or ui or common), we should enable it selectively for those that have been taken care of. Otherwise we'll enable once everything is compliant.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub https://github.com/twitter/heron/pull/735#issuecomment-221132864\n. Hey Sailesh,\n\nI\u2019m trying to find where the rules are configured, but it\u2019s not obvious.  I\u2019ll keep digging, but can you provide pointers?  I know it\u2019s a twitter python library, but I can\u2019t find information on how it\u2019s supposed to be configured\nAlan\n\nOn May 23, 2016, at 6:37 PM, Sailesh Mittal notifications@github.com wrote:\nI would recommend option 2 (relax that check) and creating an issue.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub https://github.com/twitter/heron/pull/735#issuecomment-221143661\n. created separate PR with full changes here: https://github.com/twitter/heron/pull/780 \n. continued fixes on https://github.com/twitter/heron/pull/849\n. The following are still issues.  I'll go ahead and open up a separate ticket\n\nheron/ui/src/python/handler/api/topology.py: \nW:231, 0: FIXME: is there a purpose to saving the return value of this? (fixme)\nW:234, 0: FIXME: estate not defined (fixme)\nE:240,13: Undefined variable 'estate' (undefined-variable)\nW:232, 4: Unused variable 'scheduler_location' (unused-variable)\n. build is still busted.  The build log indicates a problem executing a test topology, but doesn't say exactly what failed or why (see below).  The only thing I can think of is the addition of checkstyle.ini inside of tools/python that I added, and maybe that needs to be packaged in somehow.  I'm not familiar with how the build and test system works to do this.  Will need some help on this.  For reference, part of this PR was to modify the tools/java/.../PythonCheckstyle.java:84 where I added an additional command line parameter to read the checkstyle.ini file\n// Create and run the command\n  List<String> commandBuilder = new ArrayList<>();\n  commandBuilder.add(pylintFile);\n  commandBuilder.add(\"--rcfile=\" + PYLINT_RCFILE); // tools/python/checkstyle.ini\n  commandBuilder.addAll(sourceFiles);\n  runLinter(commandBuilder);\nthis seems to work fine locally when I run the linter, but maybe it breaks the build somehow?\nbuild output\nHeron is now installed!\nMake sure you have \"/home/travis/bin\" in your path. \nSee http://heronstreaming.io/docs/getting-started.html for how to use Heron.\nheron.build.version : 0.14.0-1083-g85bfa2b\nheron.build.time : Thu Jun  2 22:41:52 UTC 2016\nheron.build.timestamp : 1464907312000\nheron.build.host : testing-gce-e46a86b8-e324-4d1f-b559-f2aca088a128\nheron.build.user : travis\nheron.build.git.revision : 85bfa2bf893768f7e508ab45c2fd1188bd76f826\nheron.build.git.status : Modified\n+python integration-test/src/python/local_test_runner/main.py\nINFO:root:Submitting topology\nINFO:root:Submitting topology: \nINFO:root:[u'/home/travis/bin/heron', 'submit', '--verbose', '--', u'local', u'/home/travis/build/twitter/heron/bazel-genfiles/integration-test/src/java/local-integration-tests.jar', u'com.twitter.heron.local_integration_test.topology.local_readwrite.LocalReadWriteTopology', u'IntegrationTest_LocalReadWriteTopology', u'/home/travis/.herondata/topologies/local/travis/IntegrationTest_LocalReadWriteTopology/testing.txt', u'/home/travis/.herondata/topologies/local/travis/IntegrationTest_LocalReadWriteTopology/testing2.txt', '12']\n{'config_property': [], 'topology-file-name': '/home/travis/build/twitter/heron/bazel-genfiles/integration-test/src/java/local-integration-tests.jar', 'override_config_file': '/tmp/tmpnKCfnQ/override.yaml', 'config_path': '/home/travis/.heron/conf/local', 'subcommand': 'submit', 'deploy_deactivated': False, 'environ': 'default', 'cluster': 'local', 'topology_main_jvm_property': [], 'role': 'travis', 'verbose': 'True', 'topology-class-name': 'com.twitter.heron.local_integration_test.topology.local_readwrite.LocalReadWriteTopology'}\nERROR: Unable to execute topology main class\nINFO: Elapsed time: 0.001s.\nINFO:root:Submitted topology\nINFO:root:Successfully submitted KILL_TMASTER topology\nNo output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.\nThe build has been terminated\n. apologies, will work on it later this week and/or weekend.  my plate is rather full this week\n\nOn Jun 15, 2016, at 4:11 PM, Neng Lu notifications@github.com wrote:\n@alanngai https://github.com/alanngai any updates?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub https://github.com/twitter/heron/pull/849#issuecomment-226346655, or mute the thread https://github.com/notifications/unsubscribe/ABk-epnzu0HKGi4OWSEtmlpzXZ7jzhQ_ks5qMIavgaJpZM4IsEM3.\n. @nlu90, @kramasamy looks like everything passed finally.  ready to be merged\n. @nlu90, can you give me the steps to reproduce, as well as instructions on how to set things up so that I can see the page you' referring to?  The error is not obvious, so It's difficult for me to fix without having a working reference\n. @kramasamy looks like the documentation on (http://twitter.github.io/heron/docs/developers/compiling/mac/) is dated.  bazel build is broken, and from the git log it looks like it's been upgraded to 0.2.3.  The documentation still has instructions to install 0.1.2.  Filed ticket #978\n. @nlu90, @kramasamy: i was able to build and run heron-tracker and heron-ui, but I was not able to launch a topology.  In fact, building heron per instructions (bazel build --config=darwin heron/...) fails for me with the following error\n\nERROR: /Users/alan/work/cloudmon/dev-master/heron-1/config/BUILD:5:1: Executing genrule //config:config-srcs failed: bash failed: error executing command \n  (cd /private/var/tmp/_bazel_alan/db3f53e50a1fc88e2445c3b62d2e0efd/heron-1 && \\\n  exec env - \\\n    PATH='/Applications/Xcode.app/Contents/Developer/usr/bin:~/dev/bin:/Users/alan/dev/activator:/Users/alan/dev/play:/Users/alan/dev/scala/bin:/Users/alan/dev/sbt/bin:/usr/local/mysql/bin:/Users/alan/dev/go/bin:/Users/alan/dev/apache-maven/bin:/Users/alan/dev/spark/bin:/Users/alan/dev/flink/bin:/Users/alan/dev/apache-ant/bin:/Users/alan/work/cloudmon/dev-master/Tools/DevTools:/Users/alan/dev/ec2-api-tools/bin:/Users/alan/bin:/Users/alan/.rvm/gems/ruby-1.9.3-p194/bin:/Users/alan/.rvm/gems/ruby-1.9.3-p194@global/bin:/Users/alan/.rvm/rubies/ruby-1.9.3-p194/bin:/Users/alan/.rvm/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/Users/alan/.rvm/bin' \\\n    TMPDIR=/var/folders/fq/fzcyqkcx7rgbycg4kr8z3m180000gn/T/ \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; export WORKSPACE_ROOT=$(pwd)\nexport INSTALL_DIR=$(pwd)/bazel-out/local-fastbuild/genfiles/config\nexport TMP_DIR=$(mktemp -d -t config.XXXXX)\nmkdir -p $TMP_DIR\ncp -R config/m4/acx_pthread.m4 config/m4/gxx_stdio_filebuf.m4 config/autogen.sh config/configure.ac config/heron.def $TMP_DIR\ncd $WORKSPACE_ROOT\ncd $TMP_DIR\ntouch Makefile.am\nmkdir -p m4\nmv *.m4 m4\n./autogen.sh\n./configure --enable-shared=no\ncp heron-config.h $INSTALL_DIR\nrm -rf $TMP_DIR'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 2.\nPreparing the heron build system...please wait\nFound GNU Autoconf version 2.69\nFound GNU Automake version 1.15\nFound GNU Libtool version 2.4.4\nWarning: Several files expected of projects that conform to the GNU\ncoding standards were not found.  The files were automatically added\nfor you since you do not have a 'foreign' declaration specified.\nConsidered adding 'foreign' to AM_INIT_AUTOMAKE in configure.ac\nor to AUTOMAKE_OPTIONS in your top-level Makefile.am file.\nAutomatically preparing build ... Warning: autoreconf failed\nAttempting to run the preparation steps individually\nPreparing build ... ERROR: aclocal failed\nTarget //scripts/packages:binpkgs failed to build\nERROR: /Users/alan/work/cloudmon/dev-master/heron-1/heron/common/src/cpp/errors/BUILD:32:1 Executing genrule //config:config-srcs failed: bash failed: error executing command \n  (cd /private/var/tmp/_bazel_alan/db3f53e50a1fc88e2445c3b62d2e0efd/heron-1 && \\\n  exec env - \\\n    PATH='/Applications/Xcode.app/Contents/Developer/usr/bin:~/dev/bin:/Users/alan/dev/activator:/Users/alan/dev/play:/Users/alan/dev/scala/bin:/Users/alan/dev/sbt/bin:/usr/local/mysql/bin:/Users/alan/dev/go/bin:/Users/alan/dev/apache-maven/bin:/Users/alan/dev/spark/bin:/Users/alan/dev/flink/bin:/Users/alan/dev/apache-ant/bin:/Users/alan/work/cloudmon/dev-master/Tools/DevTools:/Users/alan/dev/ec2-api-tools/bin:/Users/alan/bin:/Users/alan/.rvm/gems/ruby-1.9.3-p194/bin:/Users/alan/.rvm/gems/ruby-1.9.3-p194@global/bin:/Users/alan/.rvm/rubies/ruby-1.9.3-p194/bin:/Users/alan/.rvm/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/Users/alan/.rvm/bin' \\\n    TMPDIR=/var/folders/fq/fzcyqkcx7rgbycg4kr8z3m180000gn/T/ \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; export WORKSPACE_ROOT=$(pwd)\nexport INSTALL_DIR=$(pwd)/bazel-out/local-fastbuild/genfiles/config\nexport TMP_DIR=$(mktemp -d -t config.XXXXX)\nmkdir -p $TMP_DIR\ncp -R config/m4/acx_pthread.m4 config/m4/gxx_stdio_filebuf.m4 config/autogen.sh config/configure.ac config/heron.def $TMP_DIR\ncd $WORKSPACE_ROOT\ncd $TMP_DIR\ntouch Makefile.am\nmkdir -p m4\nmv *.m4 m4\n./autogen.sh\n./configure --enable-shared=no\ncp heron-config.h $INSTALL_DIR\nrm -rf $TMP_DIR'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 2.\n. yes, the error I posted was after the merge.  I merged again just now, same problem.  I even cloned a fresh copy of the project just to be sure, but ran into the same problem\nalan@Leung-Sheung-Wing-Chun ~/work/cloudmon/dev-master/heron (master) $ bazel build --config=darwin heron/...\nWarning: ignoring _JAVA_OPTIONS in environment.\n.\nSLF4J: The following set of substitute loggers may have been accessed\nSLF4J: during the initialization phase. Logging calls during this\nSLF4J: phase were not honored. However, subsequent logging calls to these\nSLF4J: loggers will work as normally expected.\nSLF4J: See also http://www.slf4j.org/codes.html#substituteLogger\nSLF4J: org.eclipse.aether.internal.impl.DefaultRepositorySystem\nINFO: Found 333 targets...\nERROR: /Users/alan/work/cloudmon/dev-master/heron/config/BUILD:5:1: Executing genrule //config:config-srcs failed: bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 2.\nPreparing the heron build system...please wait\nFound GNU Autoconf version 2.69\nFound GNU Automake version 1.15\nFound GNU Libtool version 2.4.4\nWarning: Several files expected of projects that conform to the GNU\ncoding standards were not found.  The files were automatically added\nfor you since you do not have a 'foreign' declaration specified.\nConsidered adding 'foreign' to AM_INIT_AUTOMAKE in configure.ac\nor to AUTOMAKE_OPTIONS in your top-level Makefile.am file.\nAutomatically preparing build ... Warning: autoreconf failed\nAttempting to run the preparation steps individually\nPreparing build ... ERROR: aclocal failed\nINFO: Elapsed time: 13.354s, Critical Path: 6.44s\n. @nlu90 : reinstalled 0.2.3 via the link you gave me.  Still no luck.  Here's the output of ./bazel_configure.py\n```\nalan@Leung-Sheung-Wing-Chun ~/work/cloudmon/dev-master/heron (master)\u2191 $ ./bazel_configure.py \nPlatform Darwin\nUsing C compiler          : /Applications/Xcode.app/Contents/Developer/usr/bin/gcc (4.2.1)\nUsing C++ compiler        : /Applications/Xcode.app/Contents/Developer/usr/bin/gcc (4.2.1)\nUsing C preprocessor      : /usr/bin/cpp (7.3.0)\nUsing C++ preprocessor    : /usr/bin/cpp (7.3.0)\nUsing linker              : /Applications/Xcode.app/Contents/Developer/usr/bin/ld\nUsing Automake            : /usr/local/Cellar/automake/1.15/bin/automake (1.15)\nUsing Autoconf            : /usr/local/Cellar/autoconf/2.69/bin/autoconf (2.69)\nUsing Make                : /Applications/Xcode.app/Contents/Developer/usr/bin/make (3.81)\nUsing CMake               : /usr/local/Cellar/cmake/3.1.0/bin/cmake (3.1.0)\nUsing Python2             : /System/Library/Frameworks/Python.framework/Versions/2.7/bin/python2.7 (2.7.10)\nUsing archiver            : /usr/bin/libtool\nUsing coverage tool       : /usr/bin/gcov\ndwp                       : not found, but ok\nUsing nm                  : /usr/bin/nm\nobjcopy                   : not found, but ok\nobjdump                   : not found, but ok\nUsing strip               : /usr/bin/strip\nWrote the environment exec file scripts/compile/env_exec.sh\nhere's the output of 'bazel build --config=darwin --verbose_failures heron/...'\n./install-sh -c -d '/private/var/tmp/_bazel_alan/848753e5ba8b8f098ab31862e547561a/heron/bazel-out/local-fastbuild/genfiles/third_party/jansson/lib/pkgconfig'\n /usr/bin/install -c -m 644 jansson.pc '/private/var/tmp/_bazel_alan/848753e5ba8b8f098ab31862e547561a/heron/bazel-out/local-fastbuild/genfiles/third_party/jansson/lib/pkgconfig'\nERROR: /Users/alan/work/cloudmon/dev-master/heron/config/BUILD:5:1: Executing genrule //config:config-srcs failed: bash failed: error executing command \n  (cd /private/var/tmp/_bazel_alan/848753e5ba8b8f098ab31862e547561a/heron && \\\n  exec env - \\\n    PATH='/Applications/Xcode.app/Contents/Developer/usr/bin:~/dev/bin:/Users/alan/dev/activator:/Users/alan/dev/play:/Users/alan/dev/scala/bin:/Users/alan/dev/sbt/bin:/usr/local/mysql/bin:/Users/alan/dev/go/bin:/Users/alan/dev/apache-maven/bin:/Users/alan/dev/spark/bin:/Users/alan/dev/flink/bin:/Users/alan/dev/apache-ant/bin:/Users/alan/work/cloudmon/dev-master/Tools/DevTools:/Users/alan/dev/ec2-api-tools/bin:/Users/alan/bin:/Users/alan/.rvm/gems/ruby-1.9.3-p194/bin:/Users/alan/.rvm/gems/ruby-1.9.3-p194@global/bin:/Users/alan/.rvm/rubies/ruby-1.9.3-p194/bin:/Users/alan/.rvm/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/Users/alan/.rvm/bin' \\\n    TMPDIR=/var/folders/fq/fzcyqkcx7rgbycg4kr8z3m180000gn/T/ \\\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; export WORKSPACE_ROOT=$(pwd)\nexport INSTALL_DIR=$(pwd)/bazel-out/local-fastbuild/genfiles/config\nexport TMP_DIR=$(mktemp -d -t config.XXXXX)\nmkdir -p $TMP_DIR\ncp -R config/m4/acx_pthread.m4 config/m4/gxx_stdio_filebuf.m4 config/autogen.sh config/configure.ac config/heron.def $TMP_DIR\ncd $WORKSPACE_ROOT\ncd $TMP_DIR\ntouch Makefile.am\nmkdir -p m4\nmv *.m4 m4\n./autogen.sh\n./configure --enable-shared=no\ncp heron-config.h $INSTALL_DIR\nrm -rf $TMP_DIR'): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 2.\nPreparing the heron build system...please wait\nFound GNU Autoconf version 2.69\nFound GNU Automake version 1.15\nFound GNU Libtool version 2.4.4\nWarning: Several files expected of projects that conform to the GNU\ncoding standards were not found.  The files were automatically added\nfor you since you do not have a 'foreign' declaration specified.\nConsidered adding 'foreign' to AM_INIT_AUTOMAKE in configure.ac\nor to AUTOMAKE_OPTIONS in your top-level Makefile.am file.\nAutomatically preparing build ... Warning: autoreconf failed\nAttempting to run the preparation steps individually\nPreparing build ... ERROR: aclocal failed\nINFO: Elapsed time: 72.917s, Critical Path: 66.74s\n```\n. I'm running OSX El Capitan 10.11.5\n. @kramasamy that output was from a freshly cloned master.  It's probably some env diff on my machine.  Can you copy and paste the output of your bazel_configure.py?  Maybe there's a difference there that's breaking things on my machine\n. hmm, that didn't help.  All my versions are at least as the same or newer than yours.  I guess I'm going to have to try this on a different laptop or debug the makefiles.  I'll have to do it later in the week after I'm back from traveling\n. @kramasamy or @nlu90, would you mind checking to see if you get this warning message in your build log somewhere?  For me it happens right before the failure\nWarning: Several files expected of projects that conform to the GNU\ncoding standards were not found.  The files were automatically added\nfor you since you do not have a 'foreign' declaration specified.\nConsidered adding 'foreign' to AM_INIT_AUTOMAKE in configure.ac\nor to AUTOMAKE_OPTIONS in your top-level Makefile.am file.\nAutomatically preparing build ... Warning: autoreconf failed\n. better yet, if you could email me your entire build log to alan@opsclarity.com\n. interesting.  @nlu90 it looks like my bazel conf is different from yours.  You're using clang whereas I'm using gcc.  But @kramasamy is using gcc and that did not appear to be a problem for him.  The version numbers for all of my stuff is at least as up to date as yours.  I did find one difference in your build log that's different from mine.  Here's yours\n-- @nlu90 ---------------------------------------\nFound GNU Autoconf version 2.69\nFound GNU Automake version 1.14.1\nWarning:  libtoolize does not appear to be available.  This means that\nthe automatic build preparation via autoreconf will probably not work.\nPreparing the build by running each step individually, however, should\nwork and will be done automatically for you if autoreconf fails.\nFortunately, glibtoolize was found which means that your system may simply\nhave a non-standard or incomplete GNU Autotools install.  If you have\nsufficient system access, it may be possible to quell this warning by\nrunning:\nsudo ln -s /opt/twitter/bin/glibtoolize /opt/twitter/bin/libtoolize\nFound GNU Libtool version 2.4.2\nWarning: Several files expected of projects that conform to the GNU\ncoding standards were not found.  The files were automatically added\nfor you since you do not have a 'foreign' declaration specified.\nConsidered adding 'foreign' to AM_INIT_AUTOMAKE in configure.ac\nor to AUTOMAKE_OPTIONS in your top-level Makefile.am file.\nAutomatically preparing build ... done\n-- @alanngai ---------------------------------------\nFound GNU Autoconf version 2.69\nFound GNU Automake version 1.15\nFound GNU Libtool version 2.4.4\nWarning: Several files expected of projects that conform to the GNU\ncoding standards were not found.  The files were automatically added\nfor you since you do not have a 'foreign' declaration specified.\nConsidered adding 'foreign' to AM_INIT_AUTOMAKE in configure.ac\nor to AUTOMAKE_OPTIONS in your top-level Makefile.am file.\nAutomatically preparing build ... Warning: autoreconf failed\n\nIt could be that my new versions of automake or libtool is breaking things.  I suspect it might be automake.  Brew doesn't seem to allow me to revert to a prev version however.   This should probably be fixed, because if automake versioning is the problem, I probably won't be the only one with this problem.  Will need to investigate further when I get back to the office later this week.\nIn the meantime, @kramasamy, would you mind sending me your build log as well, so I can cross reference with @nlu90 and mine?  Thanks\n. @kramasamy, would you mind emailing me your build log?  Thanks\n. @kramasamy, @nlu90: never mind.  It looks like i was missing lib tool on the new laptop and the build proceeded successfully.  I should have a path forward\n. @kramasamy, @nlu90 \nmerged latest changes from upstream.  Once I got the build to work, I didn't have any issues with the UI.  Topo submission, activation, deactivation, and kill seemed to work fine.\n\n\n. @objmagic, which configs are you referring to?  Also, if it's an addition, can we merge this pr first and handle the additional changes in a separate ticket?  I'll be happy to do it if you point me to which configs you mean.\n. @objmagic, changed checkstyle.ini indent to 2 spaces\n. @objmagic, so you're not asking for additional changes?\n. @objmagic, I see, but the two links in your comment point to the same file?  \nAlso, if there are to be additional checkstyle changes, I'd recommend that we handle it in a separate ticket and just merge this PR in first.  I've already had to fix the same styling issues on the same files several times because of having to merge from upstream over the course of a month and a half.  It was @kramasamy's recommendation when I started work on this to just go with the default pylint settings with  some indent rule tweaks, as a first pass\n. created #1053 to track @objmagic's suggestion\n. @kramasamy \n1. I'll take a look at fixing or reverting heron.py later today \n2. wrt python checkstyle errors, there are 2 classes of errors\n   - explicitly ignored checkstyle errors\n   - errors that I don't have enough context to fix (labelled with FIXME), one of which I've filed an issue for (#855)\nin other words, i can tackle (1) but not (2)\n. fixed with latest commit\n. @nlu90: fixed and merge latest from master\n. oops, i was looking at the redundant run.run_helper.  Apologies\n. ",
    "aaronshan": "I also find this error when I run\ndocker/build-artifacts.sh centos7 master ~/centos\n@kramasamy Have you resolve it?\n. @tysonnorris  Thank u very much. use the latest code, I have resolved this problem.\n. I also get this error in centos7 and I resolved it by installing libunwind. I think this problem should due to lacking native c++ dependency. @dsyu @AMirFirouzi @Udit93 \n. @kramasamy @maosongfu , I also get this error in centos6, when I try use my aforementioned method to resolve it, but I failed. If I want use heron in centos6, should I do?\n. @jipinxieshen If you use centos7, you nedd install libunwind, If you use centos6, you need install libunwind, upgrade your gcc and glibc version.\n. @maosongfu \nI upload the log:\nlog-files.tar.gz\nheron-executor.stderr.tar.gz\nheron-executor.stdout.tar.gz\n. @maosongfu Where can I find the stream mgr's cmd?\n. @maosongfu thanks. I have resolved this problem. For centos6, It need to upgrade gcc and glibc verion. Except this, it need to install libunwind.\nIf cannot find libunwind, It will report error like this:\n./heron-core/bin/heron-stmgr: error while loading shared libraries: libunwind.so.8: cannot open shared object file: No such file or directory\nIf use default gcc and glibc, It will report error like this:\n``\n./heron-core/bin/heron-stmgr: /usr/lib64/libstdc++.so.6: versionGLIBCXX_3.4.18' not found (required by ./heron-core/bin/heron-stmgr)\n./heron-core/bin/heron-stmgr: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.14' not found (required by ./heron-core/bin/heron-stmgr)\n./heron-core/bin/heron-stmgr: /usr/lib64/libstdc++.so.6: version `CXXABI_1.3.5' not found (required by ./heron-core/bin/heron-stmgr)\n./heron-core/bin/heron-stmgr: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.15' not found (required by ./heron-core/bin/heron-stmgr)\n./heron-core/bin/heron-stmgr: /usr/lib64/libstdc++.so.6: version `GLIBCXX_3.4.19' not found (required by ./heron-core/bin/heron-stmgr)\n./heron-core/bin/heron-stmgr: /lib64/libc.so.6: version `GLIBC_2.14' not found (required by ./heron-core/bin/heron-stmgr)\n```\nAfter resolve above all problems, Heron will work in centos6(even if use install-file that complied on centos7).\n. @lohithsamaga you can run this code, paste your output\ncd /home/ubu002/.herondata/topologies/local/ubu002/ETopo03\n./heron-core/bin/heron-stmgr ETopo03 ETopo031ec80dd9-fae2-43a4-9065-741780825723 ETopo03.defn LOCALMODE /home/ubu002/.herondata/repository/state/local stmgr-1 container_1_word_2,container_1_exclaim1_1 35484 39327 37004 ./heron-conf/heron_internals.yaml\n. @maosongfu  @qiuyij I get same error when I use aurora. on local env, I can find detail error info from log-files directory. but I can't find it on aurora env. where can I find the log-files directory?\n. @wking1986  thanks.\n@maosongfu \nI find task run failed on mesos.\n\nI get stderr log on sandbox:\n\nlog cotent:\nI0609 11:19:34.714751 41904 fetcher.cpp:414] Fetcher Info: {\"cache_directory\":\"\\/tmp\\/mesos\\/fetch\\/slaves\\/56dd9481-d4b1-4133-a258-51d5a538c46d-S0\\/root\",\"items\":[{\"action\":\"BYPASS_CACHE\",\"uri\":{\"executable\":true,\"extract\":true,\"value\":\"\\/usr\\/bin\\/thermos_executor\"}}],\"sandbox_directory\":\"\\/tmp\\/mesos\\/slaves\\/56dd9481-d4b1-4133-a258-51d5a538c46d-S0\\/frameworks\\/dc22c117-1cd9-43fa-bb2c-bee1f5e7500d-0000\\/executors\\/thermos-1465442074668-datadev-devel-ExclamationTopology-1-8c0bb83f-301d-47f9-9e46-43c07f5c13bc\\/runs\\/44dc2a93-f7e6-4892-8bbc-668c3b845e17\",\"user\":\"root\"}\nI0609 11:19:34.716109 41904 fetcher.cpp:369] Fetching URI '/usr/bin/thermos_executor'\nI0609 11:19:34.716125 41904 fetcher.cpp:243] Fetching directly into the sandbox directory\nI0609 11:19:34.716142 41904 fetcher.cpp:180] Fetching URI '/usr/bin/thermos_executor'\nI0609 11:19:34.716159 41904 fetcher.cpp:160] Copying resource with command:cp '/usr/bin/thermos_executor' '/tmp/mesos/slaves/56dd9481-d4b1-4133-a258-51d5a538c46d-S0/frameworks/dc22c117-1cd9-43fa-bb2c-bee1f5e7500d-0000/executors/thermos-1465442074668-datadev-devel-ExclamationTopology-1-8c0bb83f-301d-47f9-9e46-43c07f5c13bc/runs/44dc2a93-f7e6-4892-8bbc-668c3b845e17/thermos_executor'\nI0609 11:19:34.754954 41904 fetcher.cpp:446] Fetched '/usr/bin/thermos_executor' to '/tmp/mesos/slaves/56dd9481-d4b1-4133-a258-51d5a538c46d-S0/frameworks/dc22c117-1cd9-43fa-bb2c-bee1f5e7500d-0000/executors/thermos-1465442074668-datadev-devel-ExclamationTopology-1-8c0bb83f-301d-47f9-9e46-43c07f5c13bc/runs/44dc2a93-f7e6-4892-8bbc-668c3b845e17/thermos_executor'\ntwitter.common.app debug: Initializing: twitter.common.log (Logging subsystem.)\nWriting log files to disk in /tmp/mesos/slaves/56dd9481-d4b1-4133-a258-51d5a538c46d-S0/frameworks/dc22c117-1cd9-43fa-bb2c-bee1f5e7500d-0000/executors/thermos-1465442074668-datadev-devel-ExclamationTopology-1-8c0bb83f-301d-47f9-9e46-43c07f5c13bc/runs/44dc2a93-f7e6-4892-8bbc-668c3b845e17\nI0609 11:19:35.444795 41901 exec.cpp:134] Version: 0.25.0\nI0609 11:19:35.452504 41913 exec.cpp:208] Executor registered on slave 56dd9481-d4b1-4133-a258-51d5a538c46d-S0\nWriting log files to disk in /tmp/mesos/slaves/56dd9481-d4b1-4133-a258-51d5a538c46d-S0/frameworks/dc22c117-1cd9-43fa-bb2c-bee1f5e7500d-0000/executors/thermos-1465442074668-datadev-devel-ExclamationTopology-1-8c0bb83f-301d-47f9-9e46-43c07f5c13bc/runs/44dc2a93-f7e6-4892-8bbc-668c3b845e17\nERROR] Regular plan unhealthy!\ntwitter.common.app debug: Shutting application down.\ntwitter.common.app debug: Running exit function for twitter.common.log (Logging subsystem.)\ntwitter.common.app debug: Finishing up module teardown.\ntwitter.common.app debug:   Active thread: <_MainThread(MainThread, started 139986815493888)>\ntwitter.common.app debug:   Active thread (daemon): <Thread(Thread-6, started daemon 139986237478656)>\ntwitter.common.app debug:   Active thread (daemon): <Thread(Thread-7, started daemon 139986216498944)>\ntwitter.common.app debug:   Active thread (daemon): <TaskResourceMonitor(TaskResourceMonitor[1465442074668-datadev-devel-ExclamationTopology-1-8c0bb83f-301d-47f9-9e46-43c07f5c13bc] [TID=41953], started daemon 139986125973248)>\ntwitter.common.app debug:   Active thread (daemon): <WaitThread(Thread-12, started daemon 139986226988800)>\ntwitter.common.app debug:   Active thread (daemon): <Thread(Thread-8, started daemon 139986136463104)>\ntwitter.common.app debug:   Active thread (daemon): <WaitThread(Thread-15, started daemon 139986094503680)>\ntwitter.common.app debug:   Active thread (daemon): <_DummyThread(Dummy-2, started daemon 139986480895744)>\ntwitter.common.app debug:   Active thread (daemon): <WaitThread(Thread-14, started daemon 139986081892096)>\ntwitter.common.app debug: Exiting cleanly.\nHow can I solve the problem \"ERROR] Regular plan unhealthy!\"  thank u ~\n. @maosongfu \nenter sandbox folder:\n\nand then enter .logs folder:\n\nin fetch_heron_system folder, I can get info from stderr file:\n\n```\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n0 37.3M    0 16383    0     0  2674k      0  0:00:14 --:--:--  0:00:14 2674k\n100 37.3M  100 37.3M    0     0   826M      0 --:--:-- --:--:-- --:--:--  956M\ntar: ./release.yaml: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/bin/heron-executor: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/bin/heron-shell: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/bin/heron-stmgr: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/bin/heron-tmaster: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/bin: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/lib/scheduler/heron-scheduler.jar: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/lib/scheduler/heron-local-scheduler.jar: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/lib/scheduler/heron-slurm-scheduler.jar: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/lib/scheduler: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/lib/packing/heron-roundrobin-packing.jar: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/lib/packing: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/lib/metricsmgr/heron-metricsmgr.jar: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/lib/metricsmgr: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/lib/statemgr/heron-localfs-statemgr.jar: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/lib/statemgr/heron-zookeeper-statemgr.jar: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/lib/statemgr: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/lib/instance/heron-instance.jar: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/lib/instance: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core/lib: implausibly old time stamp 1970-01-01 08:00:00\ntar: ./heron-core: implausibly old time stamp 1970-01-01 08:00:00\ntar: .: implausibly old time stamp 1970-01-01 08:00:00\n```\nthis error I report at #845 \nand in fetch_user_package folder, I can get info from stderr file:\n\ncurl: (6) Couldn't resolve host 'hdfs:'\nI think is problem maybe caused by heron.aurora file config error, my heron.aurora file like this:\n```\n\"\"\"\nLaunch the topology as a single aurora job with multiple instances.\nThe heron-executor is responsible for starting a tmaster (container 0)\nand regular stmgr/metricsmgr/instances (container index > 0).\n\"\"\"\nheron_core_release_uri = '{{CORE_PACKAGE_URI}}'\nheron_topology_jar_uri = '{{TOPOLOGY_PACKAGE_URI}}'\ncore_release_file = \"heron-core.tar.gz\"\ntopology_package_file = \"topology.tar.gz\"\n--- processes ---\nfetch_heron_system = Process(\nname = 'fetch_heron_system',\ncmdline = 'curl %s -o %s && tar zxf %s' % (heron_core_release_uri, core_release_file, core_release_file)\n)\nfetch_heron_system = Process(\n  name = 'fetch_heron_system',\n  cmdline = 'hadoop fs -get  hdfs:///tmp/heron/topologies/aurora/heron-core.tar.gz  . && tar zxf %s' % ( core_release_file)\n)\nfetch_user_package = Process(\nname = 'fetch_user_package',\ncmdline = 'curl %s -o %s && tar zxf %s' % (heron_topology_jar_uri, topology_package_file, topology_package_file)\n)\nfetch_user_package = Process(\n  name = 'fetch_user_package',\n  cmdline = 'hadoop fs -get  %s  .  && tar zxf %s' % (heron_topology_jar_uri, topology_package_file)\n)\n```\n. @maosongfu \nI revise the heron.aurora file, and now it can working.I start two mesos slave, and I find that the one run task ok and the other one run task still fail.\n\nand when I click hostname:\n\nand launch_heron_executor's stdout file and stderr file are empty.\nI run these command step by step:\nhadoop fs -get  hdfs:///tmp/heron/topologies/main/heron-core.tar.gz  . && tar zxf heron-core.tar.gz\nhadoop fs -get hdfs:///tmp/heron/topologies/main/ExclamationTopology-ruifeng.shan-tag-0--5954092425683288689  topology.tar.gz && tar zxf topology.tar.gz\n./heron-core/bin/heron-executor 1 ExclamationTopology ExclamationTopology603f5dd1-da30-46ac-8e6b-01650fd35cfe ExclamationTopology.defn 1:word:2:0:exclaim1:1:0 l-hdps1.data.cn5:2181,l-hdps2.data.cn5:2181,l-hdps3.data.cn5:2181 /heron ./heron-core/bin/heron-tmaster ./heron-core/bin/heron-stmgr \"./heron-core/lib/metricsmgr/*\" \"LVhYOitIZWFwRHVtcE9uT3V0T2ZNZW1vcnlFcnJvcg&equals;&equals;\" \"heron-examples.jar\" 31749 31148 31006 ./heron-conf/heron_internals.yaml exclaim1:536870912,word:536870912 \"\" jar heron-examples.jar /home/q/java8/jdk1.8.0_91 31985 ./heron-core/bin/heron-shell 31984 main ruifeng.shan devel \"./heron-core/lib/instance/*\" ./heron-conf/metrics_sinks.yaml \"./heron-core/lib/scheduler/*:./heron-core/lib/packing/*:./heron-core/lib/statemgr/*\" \"31347\"\nand output is also empty. but heron-executor.stderr info is :\nTraceback (most recent call last):\n  File \"/home/ruifeng.shan/heron-core/bin/heron-executor/.bootstrap/_pex/pex.py\", line 319, in execute\n  File \"/home/ruifeng.shan/heron-core/bin/heron-executor/.bootstrap/_pex/pex.py\", line 254, in _wrap_coverage\n  File \"/home/ruifeng.shan/heron-core/bin/heron-executor/.bootstrap/_pex/pex.py\", line 286, in _wrap_profiling\n  File \"/home/ruifeng.shan/heron-core/bin/heron-executor/.bootstrap/_pex/pex.py\", line 362, in _execute\n  File \"/home/ruifeng.shan/heron-core/bin/heron-executor/.bootstrap/_pex/pex.py\", line 420, in execute_entry\n  File \"/home/ruifeng.shan/heron-core/bin/heron-executor/.bootstrap/_pex/pex.py\", line 425, in execute_module\n  File \"/usr/local/lib/python2.7/runpy.py\", line 180, in run_module\n    fname, loader, pkg_name)\n  File \"/usr/local/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/home/ruifeng.shan/heron-core/bin/heron-executor/heron/executor/src/python/heron-executor.py\", line 450, in <module>\n  File \"/home/ruifeng.shan/heron-core/bin/heron-executor/heron/executor/src/python/heron-executor.py\", line 417, in main\n  File \"/home/ruifeng.shan/heron-core/bin/heron-executor/heron/executor/src/python/heron-executor.py\", line 398, in launch\n  File \"/home/ruifeng.shan/heron-core/bin/heron-executor/heron/executor/src/python/heron-executor.py\", line 362, in do_run_and_wait\n  File \"/home/ruifeng.shan/heron-core/bin/heron-executor/heron/executor/src/python/heron-executor.py\", line 352, in run_process\n  File \"/usr/local/lib/python2.7/subprocess.py\", line 710, in __init__\n    errread, errwrite)\n  File \"/usr/local/lib/python2.7/subprocess.py\", line 1335, in _execute_child\n    raise child_exception\nOSError: [Errno 2] No such file or directory\nheron-executor.stdout content:\n2016-06-09 15:29:41: Set up process group; executor becomes leader\n2016-06-09 15:29:41: Register the SIGTERM signal handler\n2016-06-09 15:29:41: Register the atexit clean up\n2016-06-09 15:29:41: Logging pid 40559 to file heron-executor-1.pid\n2016-06-09 15:29:41: Running process as mkdir -p log-files\n2016-06-09 15:29:41: Running process as chmod a+rx . && chmod a+x log-files && chmod +x ./heron-core/bin/heron-tmaster && chmod +x ./heron-core/bin/heron-stmgr && chmod +x ./heron-core/bin/heron-shell\nword 536870912 512 64 128\nexclaim1 536870912 512 64 128\n2016-06-09 15:29:41: Running heron-shell-1 process as ./heron-core/bin/heron-shell --port=31782 --log_file_prefix=log-files/heron-shell.log\n2016-06-09 15:29:41: Logging pid 40569 to file heron-shell-1.pid\n2016-06-09 15:29:41: Running container_1_word_2 process as /home/q/java8/jdk1.8.0_91/bin/java -Xmx320M -Xms320M -Xmn160M -XX:MaxPermSize=128M -XX:PermSize=128M -XX:ReservedCodeCacheSize=64M -XX:+CMSScavengeBeforeRemark -XX:TargetSurvivorRatio=90 -XX:+PrintCommandLineFlags -verbosegc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCCause -XX:+PrintPromotionFailure -XX:+PrintTenuringDistribution -XX:+PrintHeapAtGC -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:ParallelGCThreads=4 -Xloggc:log-files/gc.container_1_word_2.log -XX:+HeapDumpOnOutOfMemoryError -Djava.net.preferIPv4Stack=true -cp ./heron-core/lib/instance/*:heron-examples.jar com.twitter.heron.instance.HeronInstance ExclamationTopology ExclamationTopology603f5dd1-da30-46ac-8e6b-01650fd35cfe container_1_word_2 word 2 0 stmgr-1 31719 31300 ./heron-conf/heron_internals.yaml\n2016-06-09 15:29:41: Executor terminated; exiting all process in executor.\nand the other machine's heron-executor.stdout content:\n2016-06-09 17:36:29: Set up process group; executor becomes leader\n2016-06-09 17:36:29: Register the SIGTERM signal handler\n2016-06-09 17:36:29: Register the atexit clean up\n2016-06-09 17:36:29: Logging pid 7100 to file heron-executor-0.pid\n2016-06-09 17:36:29: Running process as mkdir -p log-files\n2016-06-09 17:36:29: Running process as chmod a+rx . && chmod a+x log-files && chmod +x ./heron-core/bin/heron-tmaster && chmod +x ./heron-core/bin/heron-stmgr && chmod +x ./heron-core/bin/heron-shell\n2016-06-09 17:36:29: Running heron-shell-0 process as ./heron-core/bin/heron-shell --port=31101 --log_file_prefix=log-files/heron-shell.log\n2016-06-09 17:36:29: Logging pid 7110 to file heron-shell-0.pid\n2016-06-09 17:36:29: Running metricsmgr-0 process as /home/q/java8/jdk1.8.0_91/bin/java -Xmx1024M -XX:+PrintCommandLineFlags -verbosegc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCCause -XX:+PrintPromotionFailure -XX:+PrintTenuringDistribution -XX:+PrintHeapAtGC -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:+PrintCommandLineFlags -Xloggc:log-files/gc.metricsmgr.log -Djava.net.preferIPv4Stack=true -cp ./heron-core/lib/metricsmgr/* com.twitter.heron.metricsmgr.MetricsManager metricsmgr-0 31132 ExclamationTopology ExclamationTopology603f5dd1-da30-46ac-8e6b-01650fd35cfe ./heron-conf/heron_internals.yaml ./heron-conf/metrics_sinks.yaml\n2016-06-09 17:36:29: Logging pid 7111 to file metricsmgr-0.pid\n2016-06-09 17:36:29: Running heron-tmaster process as ./heron-core/bin/heron-tmaster 31481 31107 31866 ExclamationTopology ExclamationTopology603f5dd1-da30-46ac-8e6b-01650fd35cfe l-hdps1.data.cn5:2181,l-hdps2.data.cn5:2181,l-hdps3.data.cn5:2181 /heron stmgr-1 ./heron-conf/heron_internals.yaml ./heron-conf/metrics_sinks.yaml 31132\n2016-06-09 17:36:29: Logging pid 7112 to file heron-tmaster.pid\n. @kartik894 \nAs I known, when u submit topology, you can set env(prod | devel | test | staging).\n```\n$ heron help submit\nusage: heron submit [options] cluster/[role]/[env] topology-file-name topology-class-name [topology-args]\nRequired arguments:\n  cluster/[role]/[env]  Cluster, role, and environment to run topology\n  topology-file-name    Topology jar/tar/zip file\n  topology-class-name   Topology class name\nOptional arguments:\n  --config-path (a string; path to cluster config; default: \"/home/q/heron/heron-0.14.0/heron/conf\")\n  --config-property (key=value; a config key and its value; default: [])\n  --deploy-deactivated (a boolean; default: \"false\")\n  --topology-main-jvm-property (property=value; JVM system property for executing topology main; default: [])\n  --verbose (a boolean; default: \"false\")\n```\n. @maosongfu  thank u very much~ Heron on Aurora is run ok!!\n. @maosongfu\nthe problem caused by no directory \"/home/q/java8/jdk1.8.0_91\". I forgot to configure it on the machine.\ud83d\ude02\ud83d\ude02\ud83d\ude02. \n. @maosongfu \nwhen I sumbit a new topology\nheron submit main/ruifeng.shan/devel /home/q/ruifeng.shan/heron-learn-1.0-SNAPSHOT-shaded.jar com.qunar.data.WordCountTopology WordCountTopology\nand it still waiting:\n[2016-06-10 01:50:54 +0000] com.twitter.heron.scheduler.aurora.AuroraLauncher INFO:  Launching topology in aurora\n[2016-06-10 01:50:54 +0000] com.twitter.heron.spi.common.ShellUtils INFO:  $> [aurora, job, create, --wait-until, RUNNING, --bind, TOPOLOGY_NAME=WordCountTopology, --bind, SANDBOX_SYSTEM_YAML=./heron-conf/heron_internals.yaml, --bind, COMPONENT_RAMMAP=sentence-spout:1073741824,count-bolt:1073741824,report-bolt:1073741824,split-bolt:1073741824, --bind, SANDBOX_METRICS_YAML=./heron-conf/metrics_sinks.yaml, --bind, INSTANCE_JVM_OPTS_IN_BASE64=\"\", --bind, ROLE=ruifeng.shan, --bind, ENVIRON=devel, --bind, SANDBOX_SCHEDULER_CLASSPATH=./heron-core/lib/scheduler/*:./heron-core/lib/packing/*:./heron-core/lib/statemgr/*, --bind, SANDBOX_INSTANCE_CLASSPATH=./heron-core/lib/instance/*, --bind, ISPRODUCTION=false, --bind, TOPOLOGY_CLASSPATH=heron-learn-1.0-SNAPSHOT-shaded.jar, --bind, CLUSTER=main, --bind, SANDBOX_EXECUTOR_BINARY=./heron-core/bin/heron-executor, --bind, STATEMGR_CONNECTION_STRING=l-hdps1.data.cn5:2181,l-hdps2.data.cn5:2181,l-hdps3.data.cn5:2181, --bind, COMPONENT_JVM_OPTS_IN_BASE64=\"\", --bind, TOPOLOGY_ID=WordCountTopology1117b603-69c3-4096-b005-789fa81ea727, --bind, TOPOLOGY_PACKAGE_URI=hdfs:///tmp/heron/topologies/main/WordCountTopology-ruifeng.shan-tag-0--3163552258663319321, --bind, SANDBOX_STMGR_BINARY=./heron-core/bin/heron-stmgr, --bind, CORE_PACKAGE_URI=file:///home/q/heron/heron-0.14.0/heron/dist/heron-core.tar.gz, --bind, SANDBOX_METRICSMGR_CLASSPATH=./heron-core/lib/metricsmgr/*, --bind, TOPOLOGY_PACKAGE_TYPE=jar, --bind, RAM_PER_CONTAINER=5368709120, --bind, SANDBOX_TMASTER_BINARY=./heron-core/bin/heron-tmaster, --bind, TOPOLOGY_DEFINITION_FILE=WordCountTopology.defn, --bind, INSTANCE_DISTRIBUTION=1:count-bolt:2:0:report-bolt:3:0:split-bolt:4:0:sentence-spout:1:0, --bind, NUM_CONTAINERS=2, --bind, CPUS_PER_CONTAINER=5.0, --bind, TOPOLOGY_JAR_FILE=heron-learn-1.0-SNAPSHOT-shaded.jar, --bind, SANDBOX_SHELL_BINARY=./heron-core/bin/heron-shell, --bind, DISK_PER_CONTAINER=17179869184, --bind, STATEMGR_ROOT_PATH=/heron, --bind, HERON_SANDBOX_JAVA_HOME=/home/q/java8/jdk1.8.0_91, main/ruifeng.shan/devel/WordCountTopology, /home/q/heron/heron-0.14.0/heron/conf/main/heron.aurora, --verbose]\n[2016-06-10 01:51:04 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\n[2016-06-10 01:51:14 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\n[2016-06-10 01:51:24 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\n[2016-06-10 01:51:34 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\n[2016-06-10 01:51:44 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\n[2016-06-10 01:51:54 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\n[2016-06-10 01:52:04 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\n[2016-06-10 01:52:14 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\n[2016-06-10 01:52:24 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\n[2016-06-10 01:52:34 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\n[2016-06-10 01:52:44 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\n[2016-06-10 01:52:54 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\n[2016-06-10 01:53:04 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\n[2016-06-10 01:53:14 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\n[2016-06-10 01:53:24 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\n[2016-06-10 01:53:34 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\n[2016-06-10 01:53:44 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x15515dbd90f005e after 0ms\nI find aurora show \"PENDING : Insufficient: disk\"\n\nand mesos resources:\n\nIf I kill ExclamationTopology and re-submit ExclamationTopology, ExclamationTopology will work.\n. @maosongfu \nthank u. I config the containers disk, cpu,ram with a smaller value, and now it run ok! and do you know that how can I increase my aurora resource config?\n. ",
    "taishi8117": "Not loading tools/travis-ci/bazel.rc for javadoc.sh might be the root cause for this issue, without which Bazel executes the build in a sandbox by default --- if that's the case, #968 should resolve this.\n. Should we merge this?\n. @caofangkun We've figured out that Bazel made some internal changes with handling Python scripts, which seem to have caused the travis failure. The following diff should fix this issue.\n```\ndiff --git a/3rdparty/pex/_pex.py b/3rdparty/pex/_pex.py\nindex 14e1ffe..a0c4f3b 100644\n--- a/3rdparty/pex/_pex.py\n+++ b/3rdparty/pex/_pex.py\n@@ -20,7 +20,7 @@ if not zipfile.is_zipfile(sys.argv[0]):\n     sys.modules.pop('twitter.common.python', None)\n root = os.path.join(\n\n\nos.sep.join(file.split(os.sep)[:-6]), '3rdparty/pex/_pex.runfiles/3rdparty')\nos.sep.join(file.split(os.sep)[:-6]), 'pex/_pex.runfiles/main/3rdparty')\n     sys.path.insert(0, os.path.join(root, 'pex'))\n     sys.path.insert(0, os.path.join(root, 'setuptools'))\n     setuptools_py =  os.path.join(root, 'setuptools')\n```\n\nAdditionally, the following modifications may be necessary.\n```\ndiff --git a/scripts/packages/BUILD b/scripts/packages/BUILD\nindex a990e75..4779d77 100644\n--- a/scripts/packages/BUILD\n+++ b/scripts/packages/BUILD\n@@ -1,7 +1,6 @@\n package(default_visibility = [\"//visibility:public\"])\n-load(\"/tools/build_defs/pkg/pkg\", \"pkg_tar\")\n-load(\"/tools/build_defs/pkg/pkg\", \"pkg_deb\")\n+load(\"@bazel_tools//tools/build_defs/pkg:pkg.bzl\", \"pkg_tar\", \"pkg_deb\")\n```\ndiff --git a/tools/java/BUILD b/tools/java/BUILD\nindex 3a099b8..74a39b4 100644\n--- a/tools/java/BUILD\n+++ b/tools/java/BUILD\n@@ -9,7 +9,7 @@ java_toolchain(\n     target_version = \"7\",\n     bootclasspath = [\"@bazel_tools//tools/jdk:bootclasspath\"],\n     extclasspath = [\"@bazel_tools//tools/jdk:extdir\"],\n-    javac = [\"@bazel_tools//tools/jdk:javac\"],\n+    javac = [\"@bazel_tools//third_party/java/jdk/langtools:javac_jar\"],\n     javabuilder = [\"@bazel_tools//tools/jdk:JavaBuilder_deploy.jar\"],\n     singlejar = [\"@bazel_tools//tools/jdk:SingleJar_deploy.jar\"],\n     genclass = [\"@bazel_tools//tools/jdk:GenClass_deploy.jar\"],\n. Bazel team is working on this issue -- they might remove these warnings as well.\nbazelbuild/bazel#1286\n. For unit tests failure, it seems because of Bazel's change in java_test behavior since 0.2.1.\nbazelbuild/bazel#1017\n. @caofangkun Since Bazel's internal test structure changed, the pull from #938 (already merged to master) and the following changes are also necessary, which should resolve the unit test failures in Travis.\n```\ndiff --git a/heron/common/tests/java/com/twitter/heron/common/config/Constants.java b/heron/common/tests/java/com/twitter/heron/common/config/Constants.java\nindex bd13b68..db1ed1e 100644\n--- a/heron/common/tests/java/com/twitter/heron/common/config/Constants.java\n+++ b/heron/common/tests/java/com/twitter/heron/common/config/Constants.java\n@@ -24,7 +24,7 @@ public final class Constants {\n   public static final String LAUNCHER_CLASS_KEY = \"heron.launcher.class\";\npublic static final String TEST_DATA_PATH =\n-      \"/heron/common/tests/java/com/twitter/heron/common/config/testdata\";\n+      \"/main/heron/common/tests/java/com/twitter/heron/common/config/testdata\";\nprivate Constants() {\n   }\n```\n```\ndiff --git a/heron/instance/tests/java/com/twitter/heron/resource/Constants.java b/heron/instance/tests/java/com/twitter/heron/resource/Constants.java\nindex cd8b8d1..dadef82 100644\n--- a/heron/instance/tests/java/com/twitter/heron/resource/Constants.java\n+++ b/heron/instance/tests/java/com/twitter/heron/resource/Constants.java\n@@ -41,7 +41,7 @@ public final class Constants {\n   // For bazel, we use the env var to get the path of heron internals config file\n   public static final String BUILD_TEST_SRCDIR = \"TEST_SRCDIR\";\n   public static final String BUILD_TEST_HERON_INTERNALS_CONFIG_PATH =\n-      \"/heron/config/src/yaml/conf/test/test_heron_internals.yaml\";\n+      \"/main/heron/config/src/yaml/conf/test/test_heron_internals.yaml\";\nprivate Constants() {\n   }\n```\n```\ndiff --git a/heron/spi/tests/java/com/twitter/heron/spi/common/TestConstants.java b/heron/spi/tests/java/com/twitter/heron/spi/common/TestConstants.java\nindex f968363..cc2cd59 100644\n--- a/heron/spi/tests/java/com/twitter/heron/spi/common/TestConstants.java\n+++ b/heron/spi/tests/java/com/twitter/heron/spi/common/TestConstants.java\n@@ -16,7 +16,7 @@ package com.twitter.heron.spi.common;\nfinal class TestConstants {\n   public static final String TEST_DATA_PATH =\n-      \"/heron/spi/tests/java/com/twitter/heron/spi/common/testdata\";\n+      \"/main/heron/spi/tests/java/com/twitter/heron/spi/common/testdata\";\nprivate TestConstants() {\n   }\n```\n```\ndiff --git a/heron/uploaders/tests/java/com/twitter/heron/uploader/localfs/LocalFileSystemConstantsTest.java b/heron/uploaders/tests/java/com/twitter/heron/uploader/localfs/LocalFileSystemConstantsTest.java\nindex d206f33..deb96cd 100644\n--- a/heron/uploaders/tests/java/com/twitter/heron/uploader/localfs/LocalFileSystemConstantsTest.java\n+++ b/heron/uploaders/tests/java/com/twitter/heron/uploader/localfs/LocalFileSystemConstantsTest.java\n@@ -16,7 +16,7 @@ package com.twitter.heron.uploader.localfs;\nfinal class LocalFileSystemConstantsTest {\n   public static final String TEST_DATA_PATH =\n-      \"/heron/uploaders/tests/java/com/twitter/heron/uploader/localfs/testdata\";\n+      \"/main/heron/uploaders/tests/java/com/twitter/heron/uploader/localfs/testdata\";\nprivate LocalFileSystemConstantsTest() {\n   }\n``\n. @kramasamy @caofangkun For thenot a valid file name (cc, h, cpp, cu, cuh)` warnings, the following diff should resolve it.\ndiff --git a/tools/java/src/com/twitter/bazel/checkstyle/CppCheckstyle.java b/tools/java/src/com/twitter/bazel/checkstyle/CppCheckstyle.java\nindex dd5d758..d1b6f87 100644\n--- a/tools/java/src/com/twitter/bazel/checkstyle/CppCheckstyle.java\n+++ b/tools/java/src/com/twitter/bazel/checkstyle/CppCheckstyle.java\n@@ -134,6 +134,7 @@ public final class CppCheckstyle {\n                     Predicates.not(Predicates.containsPattern(\"third_party/\")),\n                     Predicates.not(Predicates.containsPattern(\"config/heron-config.h\")),\n                     Predicates.not(Predicates.containsPattern(\".*pb.h$\")),\n+                    Predicates.not(Predicates.containsPattern(\".*cc_wrapper.sh$\")),\n                     Predicates.not(Predicates.containsPattern(\".*pb.cc$\"))\n             )\n     );\n. Looking at the Travis failure message, I just realized that a similar error existed even from cc5da14, although Travis CI was successful for some reasons.\nERROR: /home/travis/build/twitter/heron/heron/proto/BUILD:109:1: error executing shell command: 'set -e\nrm -rf bazel-out/local-fastbuild/genfiles/heron/proto/proto_stmgr_java_src.srcjar.srcs\nmkdir bazel-out/local-fastbuild/genfiles/heron/proto/proto_stmgr_java_src.srcjar.srcs\nbazel-out/local-f...' failed: namespace-sandbox failed: error executing command\n[Update]\nIt seems that Travis started failing after merging #943 (as javadoc failure check is enabled by set -e in heron/website/scripts/javadocs.sh). Related to the issue #766.\n. @caofangkun Thanks for fixing the conflicts. The Javadoc error might be resolved by merging #968 (already merged to master) --- can you try that? (and enable set -e again)\n. Looks great!\n. @billonahill Just made a commit about referencing from maven\n. @lucperkins Thanks for your grammar suggestion!\n. @billonahill It seems working fine.\n. @billonahill Thanks for pointing out. It seems to have trivial conflicts with master branch now, so i'll close this, make modification and reopen the pull request\n. @billonahill fixed.\n. @maosongfu The sample error output is already written.\n. @billonahill Fixed.\n. @billonahill This is because the newer versions of Bazel require that for java_test fields, name needs to match with Java class name that is tested.\nThis PR should not conflict with the current master (built with bazel 0.1.2), and we are working on other necessary changes that might conflict with the current master in #869 \n. @lucperkins @billonahill Thanks for your comments!\n. I will do so too\n. As this PR becomes pretty large, 441a1a9 will be the last push for this PR, unless there is any issue. Please review the newly added codes. \n. @kramasamy @billonahill I added an enum to specify language, for which I added Java and Python for now, but I think it's easily extensible by adding new languages to the enum later.\n. @objmagic I considered that but to be safe with the backward compatibility, changing a type for the existing field id (5) might not be a good idea.\n. @billonahill Yes, I will work on the patch to refactor them today.\n. We decided to leave this PR backward compatible for now, and we can later refactor them when we move to protobuf v3.0 and might need to break the backward compatibility anyways (#1183).\n. Protobuf team states that it's not generally recommended to upgrade from proto2 to proto3, so if we do this, we would risk breaking the backward compatibility. \nIf we are fine with that, it's also better to refactor other parts of the existing protobuf to be more semantically generic (if necessary, like #1181) at the same time as this upgrade.\n. @objmagic Do you mean that I should use Log = log.Log, instead of from heron.common.src.python.utils.log import Log?\n. @objmagic oh okay, that makes sense.\n. @amontalenti Thanks for your comment! As you suggested, we are making the Heron Python topology as compatible with streamparse API (v3.0) as possible. It would also be great if you could take a look at our codes and give us some suggestions.\n. API (#1236) is separated from instance, will close this PR.\n. @kramasamy They should have been fixed in https://github.com/twitter/heron/pull/1197/commits/4ab7e4f586e05cb8925e7c9cd2f00d8b005df11c and https://github.com/twitter/heron/pull/1197/commits/e0ecb77124984d9f5c908c8536a574c7c31a75f2.\n. \ud83d\udc4d \n. @objmagic yes, it should.\n. @objmagic I'll do that once I'm done with integration tests for pyheron.\n. #1231 describes how to write topologies in python\n. This PR depends on #1237\n. Will remove pyheron dir, as it's unnecessary\n. Integration test's top level dir should be renamed to integration_test, instead of integration-test, because python doesn't like -. I will do it once other related PRs are all merged.\n. Also, we need to create a PyPI account.\n. We will close this PR, as @kramasamy will push the script integrated with Bazel.\n. \ud83d\udc4d \n. \ud83d\udc4d \n. heron-ui is still not working\n. @billonahill it seems to have passed. I think we don't have a Python check style for copyright strings now, which we should probably do?\n. We should investigate whether this issue still exists after the protobuf performance is improved.\n. maybe we can add an option to heron-cli (or heronrc) so custom env var can be set on distributed mode too?\n. @objmagic I'll take a look at it\n. @kramasamy @objmagic I made some comments, but other than that, this PR looks fine to me.\n. Looks good to me!\n. Looks great!\n. it looks good to me.\n. I've also updated the pyheron-library in the pyheron-starter repo, packaged using this commit, if anyone wants to try testing.\n. I might be wrong but I remember this happened occasionally for Java instances too when Metrics Manager was not yet initialized when the instance sends a request for the first time.\n. @billonahill It seems that ${HERON_DIST} is resolved internally at runtime, so no need to set this environment variable --- 'file://${HERON_DIST}/heron-core.tar.gz' is provided to heron submit command as a configuration argument \"heron.package.core.uri\", and com.twitter.heron.spi.common.Misc resolves it at runtime.\nIf the tar file is located at another location, it can be specified by modifying ${HERON_DIST} part. But I believe with default installation of heron-core, this script would work fine. \n. @billonahill I don't know if that's really necessary --- despite this format, HERON_DIST doesn't seem to be a shell environment variable (I didn't find any code that sets this env var, and also ${HERON_DIST} needs to be surrounded by a single quote in the script), meaning that if some users want to override, they need to fully replace the -pi option with their customized location. Also, heron-core.tar.gz is by default produced under the ~/.heron/dist directory when heron is installed, rather than when the bazel build --config=PLATFORM integration-test/src/... command was run. So users neither need to know how to find that location nor set it as HERON_DIST.\nBut I agree that this script should run bazel build --config=PLATFORM integration-test/src/..., rather than requiring users to do so; I will work on that.\n. @billonahill This is because spouts used for integration tests (com.twitter.heron.integration_test.core.IntegrationTestSpout) executes 10 times as default.\n. @billonahill Yes, I will add other types, also make sure serialization/deserialization works fine in the intermediate bolts too.\n. @billonahill I just pushed a commit that asserts the data itself in bolts.\nAlso, CustomSpout is wrapped by IntegrationTestSpout when TestTopologyBuilder creates a topology.\n. s/linux/darwin/\n. @billonahill Do you think it would be better to provide an Aurora specific description as well, or just to note that this flow is an example of the local scheduler?\n. I opened another PR (#1150) for just constants.py, which have all of the constants for default heron options (system config and topology config)\n. @billonahill Yes, this code is not intended to be thread safe, as it is not for Wakeable Looper. I will add documentation for that.\n. abc is a module that contains abstractmethod decorator.\n. @maosongfu @billonahill Ok, will do that.\n. @kramasamy I think in order to insert UUID, we will need to modify the contents of PEX file quite a lot (like bootstrapping and entrypoints etc), so might not be the best idea. \n. @billonahill Just discussed on this, but this name is consistent with Java implementation, and I think it's pretty clear that heron-cli and Heron Client are different from the code structure. \n. @billonahill I agree that it is quite possible that adding a new required field to config would break the existing topology, as the .defn file (containing this Config and StreamSchema) for already running topologies would not have this required field. I will consider making ConfigValueType optional.\n. @billonahill Also, I added a required field to Component in PR #1149, which might have not been a great idea for the same reason. Do you think it's better to make it optional as well? \n. @billonahill I think these objects are created when a topology is submitted and other services (tracker/ui) reference them, so it's safe to be backward compatible.  (Also, for a similar reason, required/optioanl fields were removed in protobuf 3.0) \nI just pushed a version where ConfigValueType is optional --- with this, it shouldn't affect the existing codes, and when we add a multi-lang support later, we can possibly make it backward compatible by 1) check the existence of ConfigValueType, and if it exists, handle according to that type, 2) if not existent, handle as if the value is Java serialized.\n. @kramasamy @objmagic I will open another PR for this, after this is merged.\n. @nlu90 Sorry for this, but #1180 fixed this problem.\n. FYI, deploy_deactivated() is defined here\n. FYI, write() is defined here\n. @kramasamy I will add them.\n. @kramasamy yes, it will be packaged into heron core package (refer to https://github.com/twitter/heron/pull/1201/files#diff-3e725585fbf57518c1e7209b1b18e401R13), but we still need a parameter (we can just call it by HERON_PYTHON_INSTANCE_BINARY) for heron executor to indicate its location, just like so for TMaster binary.\n. I agree with using logging uniformly -- I'll fix then\n. @kramasamy The only reason why they are here is because equivalent ones in Java are under heron/common. \n. I thought it might be better to indicate in the name that this is an interface.\n. @kramasamy I just thought if we are going to add double-thread heron instance later, it's better to distinguish between them.\n. Waiting 30 secs for retry is too long, as most retry happens because posting result was slightly later than trying to fetch the result.\n. Travis machine uses Python 2.7.3, in which asyncore module doesn't have connecting attribute.\n. btw, why do we need this? Isn't this the same as the get_heron_dir()?\n. maybe change the variable name to half_ack_bolt?\n. We should change the default logging level back to INFO\n. It seems that the name of the egg file should follow the original naming convention (pyheron-{version number}-py2.7.egg instead of just pyheron.egg), so that it can be recognized as a valid package.\n. ",
    "jmcomets": "Done.\n. ",
    "frewsxcv": "\ncan you please fill up the CLA agreement at\n\nDone\n. ",
    "nickmerwin": "@kramasamy done! Thanks.\n. ",
    "aShevc": "@maosongfu\nThink it would be fair enough for me to reply on this. Yes, MesosLauncher is pretty much based off Heron\u2019s basic LocalLauncher, therefore it launches SchedulerMain process within itself. It has a parameter heron.scheduler.background to regulate whether to launch it in a default LocalLauncher fashion in a background, or to launch in a foreground. The big picture idea is that the users who use Mesos on their clusters will most probably want to use some high level Mesos scheduler to have Heron-Mesos scheduler itself under some sort of Mesos supervision. If we go back from where we started, Heron Mesos scheduler, that was initially provided from the Twitter side, had a high level Mesos scheduler for launching an arbitrary tasks, and it was used by MesosLauncher to submit topologies to. This didn\u2019t look easy to setup and launch comparing to other Heron schedulers, so we gave up this approach in favor of preserving the simplicity of launching topologies via heron submit command. And yes, after launching HTTP server and Tmaster, it launches other containers on slaves.\n. @maosongfu \nSure, on the fault tolerance side, there are two obvious updates that yet need to be done. First, yes, Tmaster should be definitely started as a job on a Mesos slave, like the other Heron containers. That way it would be simply supervised with the Heron Mesos scheduler. Second is a bit trickier and involves slight changes to Heron\u2019s SubmitterMain, so would be nice to hear your thoughts on this. Currently Heron is not allowing to \u201cpick up\u201d the failed topology with the same heron submit command obviously. Like, if we would put the heron submit under the Mesos supervision as is, SubmitterMain will yell at us at this point: https://github.com/twitter/heron/blob/master/heron/scheduler-core/src/java/com/twitter/heron/scheduler/SubmitterMain.java#L440. What would be nice to do here is the following. Let's say, Heron would acknowledge through state manager that there is a Scheduler registered for the topology, and the topology is found in a running state. So if when it pings the scheduler\u2019s address and it responds as being unreachable, it still launches the topology through the specified Launcher, but it also passes some config parameter to let know the Launcher that it should do a recover, and not start everything from scratch. Generally speaking, this solution will allow, should one want to start the scheduler under supervision, to set heron submit under any high level Mesos scheduler and get a failover tolerant design.\nAlso, sure, the submitting machine should have Mesos master and Zookeeper reachable. Mesos slaves are not required to be able to reach the Heron Mesos scheduler back, however commands like  heron kill should be performed from the machines that can reach it. The mentioned updates hopefully to ship soon.\n. @tysonnorris, sure, your concern on not introducing high-level Mesos scheduler from our side is well understood. However, the willingness to somehow comply with the existing submitting semantics is not the only reason on moving away from introducing high-level Mesos scheduler from our side, it\u2019s rather kind of a side effect. So, consider the case that would take place should we introduce a high level scheduler here, as the initial implementation suggests. The question is what would happen when this high level scheduler itself would fail? Should there be another Mesos scheduler layer to recover this scheduler? I think you should get the point by now. So obviously, at some point you should have a Mesos scheduler which features highly availability mode, therefore can run multiple scheduler copies on the cluster and having only one leader which actually is registered with Mesos. It goes without saying that leader election algorithm should be implemented here. There are also some suggestions on implementing such a feature in the Mesos docs just to get a full picture: http://mesos.apache.org/documentation/latest/high-availability-framework-guide/. So, in short, if we would use the initial implementation we were given, we wouldn\u2019t succeed in creating failover ready solution because high availability mode wasn\u2019t implemented there. Generally, of course, we could implement it in the Heron-Mesos, but, first, this is partly a reinventing the wheel, as there are proven solutions for this kind of problems, we could rely on, for example, Marathon and Singularity. Another thing is that it\u2019s not only obviously a significant effort in terms of resources, but it\u2019s also a part which in case of some implementation issues can make a real mess. I remember Marathon have had a bug some time ago, where the ZK data was in some rare cases messed up badly during the leadership election, and this caused harm not only to Marathon, but Mesos cluster itself as well. So, for us the way to go here was actually using Marathon, feeding it the \u201cheron submit\u201d command. As you may notice, we also use Zookeeper as a data storage for the scheduler, so when it is recovered, it just can pick up where things were left off. The only thing, probably worth fixing in the approach is that we should maybe be more explicit in pointing out to long-running processes schedulers like Marathon and Singularity in the Mesos scheduler docs.\nAnother concern you pointed out about Heron possibly trying to recover a running job was actually addressed in one of my previous comments. In short, yes, the logic on recovering is not yet implemented, the thing to note here is that Mesos launcher, if asked to recover, will try to do a healthcheck on the scheduler HTTP server, and will take actions on recovering only if the healthcheck fails.\n. @tysonnorris we actually made heron submit launch a Mesos scheduler's HTTP server in it's foreground, so in Mesos case, heron submit would be a long-running command one may use for injecting this to Marathon, Singularity, etc. And, I agree this is also a part to point out in the docs\n. ",
    "mhausenblas": "This LGTM from my end @kramasamy and we'd be willing to invest further cycles to make this happen. Now hoping @joestein finds some cycles to resolve the conflicts and I'd be working with him (we had a quick sync today) to turn this also into a DC/OS package.\n. Yes, see https://mesosphere.github.io/marathon/docs/generated/api.html#v2_groups_delete\n. ",
    "samek": "So mesos scheduler is not supported anymore ? \n. @objmagic Ok I see. \nThanks, for the answer. \n. @nlu90 I've probably changed it in the process of trying it to work. Anyway same error with localzk as name and rootpath changed to /heron :(\n. sure\ncat ~/.herontools/release.yaml \nheron.build.version : 0.14.0\nheron.build.time : Tue May 24 22:44:01 PDT 2016\nheron.build.timestamp : 1464155053000\nheron.build.host : tw-mbp-kramasamy\nheron.build.user : kramasamy\nheron.build.git.revision : be87b09f348e0ed05f45503340a2245a4ef68a35\nheron.build.git.status : Clean\n. I'll give it a go. \nWhat should the values of nodes be ? null ?\n. It would definitely help me :) \nThanks \n. @kramasamy it's more of a IDE/Project setup thing.. since If I run same topology with (simulator or normal) heron submit It works like it should.\nIn InteliJ as a previous maven (storm) project I only get operations from spout and first bolt after it nothing more. \nBut you cant develop it like that. ( \n. @nlu90 \nI'm writing a new topology and as of now It's like this:\n```\nTopologyBuilder builder = new TopologyBuilder();\nSpoutConfig config = new SpoutConfig(consumerTopic, bootstrapKafkaServers, \"spoutId\");\nconfig.scheme = new KeyValueSchemeAsMultiScheme(new ByteArrayKeyValueScheme());\n//Get messages from kafka//\nbuilder.setSpout(\"spout\", new KafkaSpout(config), 1);\n//Create multiple streams\nbuilder.setBolt(\"splitter\",new kafkaSplitterBolt(),1).shuffleGrouping(\"spout\");\n//bind bolts to correct stream//\nbuilder.setBolt(\"articleRollingWindow\", new ArticleRollingWindowBolt(),1).shuffleGrouping(\"splitter\",\"articleStream\");\nbuilder.setBolt(\"SectionRollingWindow\", new SectionRollingWindow(),1).shuffleGrouping(\"splitter\", \"sectionStream\");\nConfig conf = new Config();\nconf.setDebug(true);\nconf.setNumStmgrs(1);\nconf.setContainerCpuRequested(0.2f);\nconf.setContainerRamRequested(1024L * 1024 * 512);\nconf.setContainerDiskRequested(1024L * 1024 * 1024);\nsimulator.submitTopology(\"Testtopology\", conf, builder.createTopology());\n```\nso I've got a kafka spout --> splitter bolt (parses json, creates multiple streams), then I've got 2 bolts each consuming each own stream generated from splitter bolt. \nThing is that If I run it in InteliJ as debug or just run, I don't see any output from bolts which should consume streams from splitter (I've added output in the execute) nor I cannot put breakpoints in them. \nBut If I leave the simulator mode on, compile the topology (mvn package) then run heron submit local I can see all the outputs from all the bolts. \nSo It's basically both: I can't see output from bolts beyond the splitter bolt and I guess I need a guide on how to setup the InteliJ development for topologies :( \n. Thanks for the detailed explanation. I hope someone else is going to find it useful as well. \n. ",
    "mrmiywj": "Please use English.\nThe main component is Java, with C++ and Python.\n. ",
    "zuyu": "Java for Worker.\n. @kramasamy Yes, I did, but got stuck in Step 8 due to the build error above.\n. @billonahill Changing Trace.should_log does not print out any new error info, but enabling --verbose_failures does (see below).\nINFO: From PexPython heron/ui/src/python/heron-ui.pex:\nTraceback (most recent call last):\n  File \"bazel-out/local_darwin-fastbuild/bin/3rdparty/pex/_pex\", line 105, in <module>\n    Main()\n  File \"bazel-out/local_darwin-fastbuild/bin/3rdparty/pex/_pex\", line 84, in Main\n    'Cannot exec() %r: file not found.' % main_filename\nAssertionError: Cannot exec() '/private/var/tmp/_bazel_zzhang/3964dd02d192385870c24d7a89344f18/heron/bazel-out/local_darwin-fastbuild/bin/3rdparty/pex/_pex.runfiles/3rdparty/pex/_pex.py': file not found.\nERROR: /Users/zzhang/workspace/heron/heron/ui/src/python/BUILD:18:1: null failed: _pex failed: error executing command\n  (cd /private/var/tmp/_bazel_zzhang/3964dd02d192385870c24d7a89344f18/heron && \\\n  exec env - \\\n  bazel-out/local_darwin-fastbuild/bin/3rdparty/pex/_pex --not-zip-safe --entry-point heron.ui.src.python.main bazel-out/local_darwin-fastbuild/bin/heron/ui/src/python/heron-ui.pex bazel-out/local_darwin-fastbuild/bin/heron/ui/src/python/heron-ui.pex.manifest): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: _pex failed: error executing command\n  (cd /private/var/tmp/_bazel_zzhang/3964dd02d192385870c24d7a89344f18/heron && \\\n  exec env - \\\n  bazel-out/local_darwin-fastbuild/bin/3rdparty/pex/_pex --not-zip-safe --entry-point heron.ui.src.python.main bazel-out/local_darwin-fastbuild/bin/heron/ui/src/python/heron-ui.pex bazel-out/local_darwin-fastbuild/bin/heron/ui/src/python/heron-ui.pex.manifest): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\n. I have no build errors in another Mac laptop, although the origin one still has the same build issue.\n. Thanks @objmagic @maosongfu.\n. @kramasamy We use FarmHash for strings.\n. Yes, but limited.\nhttps://github.com/google/farmhash/blob/master/src/farmhash.h#L124\n. Btw, the example shows a string-based hash result.\n. ",
    "cybernet": "#811 failed on building with bazel 0.2.3, therefore, i'll close\nhttps://travis-ci.org/twitter/heron/builds/133044996\n. i don't have the expertise to do it\nsorry \n. ",
    "ghost": "You probably need rebuild it on centos (there is a building guide) then try deploying to rh.\n. ",
    "longdafeng": "Rebuild the source code is no problem.\nBut another question,  must Heron run on JDK8?\n. @kramasamy \uff0c thanks\nneed one more modify\nmaven_jar(\n  name = \"thrift\",\n  artifact = \"org.apache.thrift:libthrift:0.5.0-1\",\n  #artifact = \"org.apache.thrift:libthrift:0.5.0\",\n  server = \"default\",\n)\n. From the scheduler log, it didn't started Stream manager\n[2016-05-30 12:38:24 +0800] com.twitter.heron.scheduler.local.LocalScheduler INFO:  Executor command line: [./heron-core/bin/heron-executor, 0, ExclamationTopology, ExclamationTopology5661f7ee-3270-47eb-9372-4007c65a1712, ExclamationTopology.defn, 1:word:2:0:exclaim1:1:0, LOCALMODE, /home/admin/.herondata/repository/state/local, ./heron-core/bin/heron-tmaster, ./heron-core/bin/heron-stmgr, ./heron-core/lib/metricsmgr/*, \"LVhYOitIZWFwRHVtcE9uT3V0T2ZNZW1vcnlFcnJvcg&equals;&equals;\", heron-examples.jar, 59215, 45101, 53051, ./heron-conf/heron_internals.yaml, exclaim1:536870912,word:536870912, \"\", jar, /home/admin/.heron/examples/heron-examples.jar, /opt/taobao/java, 37275, ./heron-core/bin/heron-shell, 34822, local, admin, default, ./heron-core/lib/instance/*, ./heron-conf/metrics_sinks.yaml, ./heron-core/lib/scheduler/*:./heron-core/lib/packing/*:./heron-core/lib/statemgr/*, 36020]\n[2016-05-30 12:38:24 +0800] com.twitter.heron.spi.common.ShellUtils INFO:  $> [./heron-core/bin/heron-executor, 0, ExclamationTopology, ExclamationTopology5661f7ee-3270-47eb-9372-4007c65a1712, ExclamationTopology.defn, 1:word:2:0:exclaim1:1:0, LOCALMODE, /home/admin/.herondata/repository/state/local, ./heron-core/bin/heron-tmaster, ./heron-core/bin/heron-stmgr, ./heron-core/lib/metricsmgr/*, \"LVhYOitIZWFwRHVtcE9uT3V0T2ZNZW1vcnlFcnJvcg&equals;&equals;\", heron-examples.jar, 59215, 45101, 53051, ./heron-conf/heron_internals.yaml, exclaim1:536870912,word:536870912, \"\", jar, /home/admin/.heron/examples/heron-examples.jar, /opt/taobao/java, 37275, ./heron-core/bin/heron-shell, 34822, local, admin, default, ./heron-core/lib/instance/*, ./heron-conf/metrics_sinks.yaml, ./heron-core/lib/scheduler/*:./heron-core/lib/packing/*:./heron-core/lib/statemgr/*, 36020]\n[2016-05-30 12:38:24 +0800] com.twitter.heron.scheduler.local.LocalScheduler INFO:  Started the executor for container: 0\n[2016-05-30 12:38:24 +0800] com.twitter.heron.scheduler.local.LocalScheduler INFO:  Starting a new executor for container: 1\n[2016-05-30 12:38:24 +0800] com.twitter.heron.scheduler.local.LocalScheduler INFO:  Executor command line: [./heron-core/bin/heron-executor, 1, ExclamationTopology, ExclamationTopology5661f7ee-3270-47eb-9372-4007c65a1712, ExclamationTopology.defn, 1:word:2:0:exclaim1:1:0, LOCALMODE, /home/admin/.herondata/repository/state/local, ./heron-core/bin/heron-tmaster, ./heron-core/bin/heron-stmgr, ./heron-core/lib/metricsmgr/*, \"LVhYOitIZWFwRHVtcE9uT3V0T2ZNZW1vcnlFcnJvcg&equals;&equals;\", heron-examples.jar, 53098, 60037, 35245, ./heron-conf/heron_internals.yaml, exclaim1:536870912,word:536870912, \"\", jar, /home/admin/.heron/examples/heron-examples.jar, /opt/taobao/java, 51229, ./heron-core/bin/heron-shell, 39894, local, admin, default, ./heron-core/lib/instance/*, ./heron-conf/metrics_sinks.yaml, ./heron-core/lib/scheduler/*:./heron-core/lib/packing/*:./heron-core/lib/statemgr/*, 42429]\n[2016-05-30 12:38:24 +0800] com.twitter.heron.scheduler.local.LocalScheduler INFO:  Waiting for container 0 to finish.\n[2016-05-30 12:38:24 +0800] com.twitter.heron.spi.common.ShellUtils INFO:  $> [./heron-core/bin/heron-executor, 1, ExclamationTopology, ExclamationTopology5661f7ee-3270-47eb-9372-4007c65a1712, ExclamationTopology.defn, 1:word:2:0:exclaim1:1:0, LOCALMODE, /home/admin/.herondata/repository/state/local, ./heron-core/bin/heron-tmaster, ./heron-core/bin/heron-stmgr, ./heron-core/lib/metricsmgr/*, \"LVhYOitIZWFwRHVtcE9uT3V0T2ZNZW1vcnlFcnJvcg&equals;&equals;\", heron-examples.jar, 53098, 60037, 35245, ./heron-conf/heron_internals.yaml, exclaim1:536870912,word:536870912, \"\", jar, /home/admin/.heron/examples/heron-examples.jar, /opt/taobao/java, 51229, ./heron-core/bin/heron-shell, 39894, local, admin, default, ./heron-core/lib/instance/*, ./heron-conf/metrics_sinks.yaml, ./heron-core/lib/scheduler/*:./heron-core/lib/packing/*:./heron-core/lib/statemgr/*, 42429]\nthen how to start stream manager\n. Thanks @kramasamy  @maosongfu \nI have uploaded the log  in the first comment\nI found some log in ~/.herondata/topologies/{cluster}/{role}/{topologyName}/ heron-executor.stdout\n2016-05-30 12:56:38: Running stmgr-1 process as ./heron-core/bin/heron-stmgr ExclamationTopology ExclamationTopology5661f7ee-3270-47eb-9372-4007c65a1712 ExclamationTopology.defn LOCALMODE /home/admin/.herondata/repository/state/local stmgr-1 container_1_word_2,container_1_exclaim1_1 38133 33156 46759 ./heron-conf/heron_internals.yaml\n2016-05-30 12:56:38: Logging pid 184367 to file stmgr-1.pid\n2016-05-30 12:56:38: stmgr-1 exited with status 32512\nheron-executor.stdout.zip\nlog-files.zip\n. heron-executor.stderr file is empty\n. fix the problem\nlibunwind install directory should be set in the shared library path\n. ",
    "serejja": "Hi @nlu90, I've just signed the CLA, thanks!\n. ",
    "jolestar": "same problem, compile with bazel:0.2.3\nheron/WORKSPACE:15:1: maven_server rule //external:twitter-maven's name field must be a legal workspace name.\n. ",
    "daxwang": "bazel 0.1.2 not support http_proxy. so i update it  to the lastest version.0.2.3, but  error when complie heron with  bazel 0.2.3\nERROR: WORKSPACE:15:1: maven_server rule //external:twitter-maven's name field must be a legal workspace name.\nso please fix it  immediately, thx\n. hi, checkout this branch and compiler error: \nheron/tools/java/BUILD:5:1: //tools/java:heron_java_toolchain: missing value for mandatory attribute 'bootclasspath' in 'java_toolchain' rule.\nheron/tools/java/BUILD:5:1: //tools/java:heron_java_toolchain: missing value for mandatory attribute 'extclasspath' in 'java_toolchain' rule\nheron/tools/java/BUILD:5:1: //tools/java:heron_java_toolchain: missing value for mandatory attribute 'javac' in 'java_toolchain' rule\nwhat's wrong?\n. ",
    "amirfirouzi": "i have the same issue here, \n(using kubuntu 16.04)\n\ncom.twitter.heron.common.basics.FileUtils SEVERE:  Failed to read from file\njava.nio.file.NoSuchFileException: .../.herondata/repository/state/local/pplans/ExclamationTopology\n\nit seems that no physical plan can be generated and then chain of errors appear because of that.\nany ideas?\n. hi guys, i just started to deploy heron over YARN and got some issues and answers her will absolutely help!\njust a quick question, am i supposed to install heron binaries(client and tools) over all nodes in yarn cluster(master and slaves) or installing over master will suffice? i figured because YarnLauncher is uploading heron-core and we are submitting topologies to master, installing heron over slaves won't be necessary, is that right?\nthanks. I've set up a multi-node hadoop(v.2.7.2) cluster to test heron(v0.14.5) over Yarn Cluster. i've done exactly whats in Yarn Cluster documentation section, and also read answers in this issue and it helped me to fix some problems like needed custom jar files(updated jars), yarn & mapreduce classpathes in yarn-site & mapred-site(to fix avro error) and in submit command and all of them are done. here are what i've done and some of my key configs:\n\nYarn cluster is healthy(hadoop apps run successfully), installed and configured ZK in master node of cluster, installed heron-client and heron-tools binaries in master, configured statemanager to use CuratorStateManager and connection.string to localhost:2181 and modified heron_tracker.yaml to use zk and Scheduler and Launcher and Uploader are set as the doc suggested.\n\nafter fixing some configs, now when i submit topology to heron, it seems to be submitted successfully and no errors are logged, and in yarn's UI it marks the submitted application as SUCCEEDED, but it's strange that the running of each topology lasts for couple of seconds and when i see the Yarn's users Logs, there are only one container in one of the slaves! and in that container the logging stops after \"INFO: Launching Heron scheduler\". here is the log generated for slave1's driver.stderr  of my cluster:\n\nOpenJDK 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0\nOpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0\nDec 28, 2016 12:36:54 PM org.apache.reef.runtime.common.REEFLauncher main\nINFO: Entering REEFLauncher.main().\nDec 28, 2016 12:36:54 PM org.apache.reef.util.REEFVersion logVersion\nINFO: REEF Version: 0.14.0\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/local/hadoop/hadoop_data/tmp/nm-local-dir/usercache/hduser/appcache/application_1482914874252_0003/filecache/10/reef-job-3258437929217330603.jar/global/heron-zookeeper-statemgr.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.JDK14LoggerFactory]\nDec 28, 2016 12:36:55 PM org.apache.hadoop.yarn.client.RMProxy createRMProxy\nINFO: Connecting to ResourceManager at hadoopmaster/192.168.100.100:8050\nDec 28, 2016 12:36:55 PM org.apache.hadoop.yarn.client.RMProxy createRMProxy\nINFO: Connecting to ResourceManager at hadoopmaster/192.168.100.100:8030\nDec 28, 2016 12:36:55 PM org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl serviceInit\nINFO: Upper bound of the thread pool size is 500\nDec 28, 2016 12:36:55 PM org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy \nINFO: yarn.client.max-cached-nodemanagers-proxies : 0\nDec 28, 2016 12:36:56 PM com.twitter.heron.scheduler.yarn.HeronReefUtils extractPackageInSandbox\nINFO: Extracting package: reef/global/topology.tar.gz at: .\ntar: ExclamationTopology.defn: time stamp 2016-12-28 12:36:59 is 2.692387499 s in the future\ntar: ./heron-conf/override.yaml: time stamp 2016-12-28 12:36:59 is 2.692153692 s in the future\nDec 28, 2016 12:36:56 PM com.twitter.heron.scheduler.yarn.HeronReefUtils extractPackageInSandbox\nINFO: Extracting package: reef/global/heron-core.tar.gz at: .\nDec 28, 2016 12:36:56 PM com.twitter.heron.scheduler.yarn.HeronMasterDriver$HeronSchedulerLauncher launchScheduler\nINFO: Launching Heron scheduler\n\nhere is my submit command:\nheron submit yarn ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology --extra-launch-classpath /home/hduser/libs/heron-yarn-classpath-jars/*:/usr/local/hadoop/etc/hadoop:/usr/local/hadoop/share/hadoop/common/lib/*:/usr/local/hadoop/share/hadoop/common/*:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/*:/usr/local/hadoop/share/hadoop/hdfs/*:/usr/local/hadoop/share/hadoop/yarn/lib/*:/usr/local/hadoop/share/hadoop/yarn/*:/usr/local/hadoop/share/hadoop/mapreduce/lib/*:/usr/local/hadoop/share/hadoop/mapreduce/*\nhere is last part of returned output in shell:\n\n[2016-12-28 12:37:01 +0330] org.apache.reef.util.REEFVersion INFO:  REEF Version: 0.14.0\n[2016-12-28 12:37:01 +0330] com.twitter.heron.scheduler.yarn.ReefClientSideHandlers INFO:  Initializing REEF client handlers for Heron, topology: ExclamationTopology\n[2016-12-28 12:37:01 +0330] org.apache.hadoop.yarn.client.RMProxy INFO:  Connecting to ResourceManager at hadoopmaster/192.168.100.100:8050\n[2016-12-28 12:37:10 +0330] org.apache.reef.runtime.common.files.JobJarMaker WARNING:  Failed to delete [/tmp/reef-job-2395324188246634929]\n[2016-12-28 12:37:11 +0330] org.apache.reef.runtime.yarn.client.YarnSubmissionHelper INFO:  Submitting REEF Application to YARN. ID: application_1482914874252_0003\n[2016-12-28 12:37:11 +0330] org.apache.hadoop.yarn.client.api.impl.YarnClientImpl INFO:  Submitted application application_1482914874252_0003\n[2016-12-28 12:37:15 +0330] com.twitter.heron.scheduler.yarn.ReefClientSideHandlers INFO:  Topology ExclamationTopology is running, jobId ExclamationTopology.\n[2016-12-28 12:37:15 +0330] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Closing the CuratorClient to: 127.0.0.1:2181\n16/12/28 12:37:15 INFO imps.CuratorFrameworkImpl: backgroundOperationsLoop exiting\n16/12/28 12:37:15 INFO zookeeper.ZooKeeper: Session: 0x159449b7dd90004 closed\n[2016-12-28 12:37:15 +0330] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Closing the tunnel processes\n16/12/28 12:37:15 INFO zookeeper.ClientCnxn: EventThread shut down\n[2016-12-28 12:37:17 +0330] org.apache.reef.runtime.common.client.defaults.DefaultCompletedJobHandler INFO:  Job Completed: CompletedJob{'ExclamationTopology'} \n\n\nand it stops here! it's strange, why it doesn't continue running? and why only one container is allocated? it seems that Application is submitted successfully but before HeronScheduler is run to schedule containers, somehow it stops running. but why Yarn thinks it is succeeded? and how to make it keep running?\nAny help how to detect the problem and solve it? it's driving me crazy.\nThanks\n. Dear @ashvina,\nthanks for the response, I've looked at all slaves for this log file, there is none! there is just one userlogs folder in the container local dir and it contains only std.err with this content and as i said before it stops logs at Starting Heron Launcher(so i think it can't start Scheduler):\n\nOpenJDK 64-Bit Server VM warning: ignoring option PermSize=128m; support was removed in 8.0\nOpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=128m; support was removed in 8.0\nDec 31, 2016 11:50:22 AM org.apache.reef.runtime.common.REEFLauncher main\nINFO: Entering REEFLauncher.main().\nDec 31, 2016 11:50:22 AM org.apache.reef.util.REEFVersion logVersion\nINFO: REEF Version: 0.14.0\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/local/hadoop/hadoop_data/tmp/nm-local-dir/usercache/hduser/appcache/application_1483172140480_0001/filecache/10/reef-job-6303074797688388694.jar/global/heron-zookeeper-statemgr.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.JDK14LoggerFactory]\nDec 31, 2016 11:50:23 AM org.apache.hadoop.yarn.client.RMProxy createRMProxy\nINFO: Connecting to ResourceManager at hadoopmaster/192.168.100.100:8050\nDec 31, 2016 11:50:23 AM org.apache.hadoop.yarn.client.RMProxy createRMProxy\nINFO: Connecting to ResourceManager at hadoopmaster/192.168.100.100:8030\nDec 31, 2016 11:50:23 AM org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl serviceInit\nINFO: Upper bound of the thread pool size is 500\nDec 31, 2016 11:50:23 AM org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy \nINFO: yarn.client.max-cached-nodemanagers-proxies : 0\nDec 31, 2016 11:50:23 AM com.twitter.heron.scheduler.yarn.HeronReefUtils extractPackageInSandbox\nINFO: Extracting package: reef/global/topology.tar.gz at: .\nDec 31, 2016 11:50:24 AM com.twitter.heron.scheduler.yarn.HeronReefUtils extractPackageInSandbox\nINFO: Extracting package: reef/global/heron-core.tar.gz at: .\nDec 31, 2016 11:50:24 AM com.twitter.heron.scheduler.yarn.HeronMasterDriver$HeronSchedulerLauncher launchScheduler\nINFO: Launching Heron scheduler\n\nand about Resources, i have 1 master and 3 slaves (each with 3GB RAM and 1 cores of CPU), isn't that sufficient? . i tested to find out if it's deployable on local Yarn(1 machine) and it failed just like the cluster, after reading Yarn logs found out that it was killing containers because of this option in yarn-site.xml:\nyarn.nodemanager.vmem-pmem-ratio (the ratio of virtual memory used based on physical memory-in this case 1GB for physical mem). so i increased the ratio and also set the yarn.scheduler.minimum(and maximum)-allocation-mb(and vcores) and also yarn.nodemanager.resource.memory-mb & \nyarn.nodemanager.resource.cpu-vcores for node manager and after these settings topologies keeps running. \nit's worth mentioning that you should check topology required resource and available resources and min/max allocatable resource as you've set in Yarn, requested resource (cpu and ram) must be less than yarn.scheduler.maximum-allocation-mb & yarn.scheduler.maximum-allocation-vcores and of course less than available resources in the cluster over all:\nrequested mem for topology< SUM_OF(yarn.nodemanager.resource.memory-mb) (for all slave node)\nrequested cpu for topology< SUM_OF(yarn.nodemanager.resource.cpu-vcores) (for all slave node)\nso check and rebuild your topology before submitting to Yarn cluster. just one question after running topologies, is it possible to track the logs and metrics using heron-ui? cuz i ran the heron-tracker and ui on the master and it doesn't show anything in the ui(topology plans and metrics), i also set the heron-tracker.yaml to use ZK.\nand where are heron logs being saved? because now i've tested on the cluster and it succeed on Yarn but stops running and i want to check heron logs to find out whats wrong? \n@ashvina even if i use ZK for state management logs will be saved in slaves local directory? but where? i don't see any and i've looked everywhere!\nthanks. actually i found the source of the problem, it was the path to input file, i was using relative paths and when submitting to cluster it couldn't find the file.\nwhat is the correct way to get input files? we should specify it in the input arguments when submitting topologies?\n. ",
    "yuniversed": "@maosongfu , neither of those files exist within my topologies log-files directory.\nI do see this on container_1_word_2.log:\n\n[2016-05-31 07:40:36 -0600] com.twitter.heron.common.network.HeronClient SEVERE:  Failed to FinishConnect to endpoint: /127.0.0.1:57743 \njava.net.ConnectException: Connection refused\n        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)\n        at com.twitter.heron.common.network.HeronClient.handleConnect(HeronClient.java:244)\n        at com.twitter.heron.common.basics.NIOLooper.handleSelectedKeys(NIOLooper.java:115)\n        at com.twitter.heron.common.basics.NIOLooper.access$000(NIOLooper.java:32)\n        at com.twitter.heron.common.basics.NIOLooper$1.run(NIOLooper.java:45)\n        at com.twitter.heron.common.basics.WakeableLooper.executeTasksOnWakeup(WakeableLooper.java:142)\n        at com.twitter.heron.common.basics.WakeableLooper.runOnce(WakeableLooper.java:74)\n        at com.twitter.heron.common.basics.WakeableLooper.loop(WakeableLooper.java:64)\n        at com.twitter.heron.instance.Gateway.run(Gateway.java:155)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n[2016-05-31 07:40:36 -0600] com.twitter.heron.network.StreamManagerClient INFO:  Stop writing due to not yet connected to Stream Manager.\n[2016-05-31 07:40:36 -0600] com.twitter.heron.network.StreamManagerClient INFO:  Stop reading due to not yet connected to Stream Manager.\n[2016-05-31 07:40:36 -0600] com.twitter.heron.network.StreamManagerClient WARNING:  Error connecting to Stream Manager with status: CONNECT_ERROR, Retrying...\n. \n",
    "Udit93": "I am getting the same error on ubuntu 14.04. \nudit@udit:~$ heron activate local ExclamationTopology\n[2016-05-31 23:16:20 +0530] com.twitter.heron.common.basics.FileUtils SEVERE:  Failed to read from file.  \njava.nio.file.NoSuchFileException: /home/udit/.herondata/repository/state/local/pplans/ExclamationTopology\nThe pplans folder in above directory tree is empty.\nUnder the directory ~/.herondata/topologies/{cluster}/{role}/{topologyName}/log-files/ I notice the files \n\ncontainer_1_exclaim1_1.log.0\ncontainer_1_word_2.log.0\ngiving same error as pointed out by dsyu.\n\n[2016-05-31 14:52:14 +0530] com.twitter.heron.common.network.HeronClient SEVERE:  Failed to FinishConnect to endpoint: /127.0.0.1:50171 \njava.net.ConnectException: Connection refused\n    at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)\n    at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:740)\n    at com.twitter.heron.common.network.HeronClient.handleConnect(HeronClient.java:244)\n    at com.twitter.heron.common.basics.NIOLooper.handleSelectedKeys(NIOLooper.java:115)\n    at com.twitter.heron.common.basics.NIOLooper.access$000(NIOLooper.java:32)\n    at com.twitter.heron.common.basics.NIOLooper$1.run(NIOLooper.java:45)\n    at com.twitter.heron.common.basics.WakeableLooper.executeTasksOnWakeup(WakeableLooper.java:142)\n    at com.twitter.heron.common.basics.WakeableLooper.runOnce(WakeableLooper.java:74)\n    at com.twitter.heron.common.basics.WakeableLooper.loop(WakeableLooper.java:64)\n    at com.twitter.heron.instance.Gateway.run(Gateway.java:155)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nFile \"heron-executor.stdout\" reports the following error - \n2016-05-31 17:36:57: Running container_1_word_2 process as /usr/lib/jvm/default-java/bin/java -Xmx320M -Xms320M -Xmn160M -XX:MaxPermSize=128M -XX:PermSize=128M -XX:ReservedCodeCacheSize=64M -XX:+CMSScavengeBeforeRemark -XX:TargetSurvivorRatio=90 -XX:+PrintCommandLineFlags -verbosegc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCCause -XX:+PrintPromotionFailure -XX:+PrintTenuringDistribution -XX:+PrintHeapAtGC -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:ParallelGCThreads=4 -Xloggc:log-files/gc.container_1_word_2.log -XX:+HeapDumpOnOutOfMemoryError -Djava.net.preferIPv4Stack=true -cp ./heron-core/lib/instance/*:heron-examples.jar com.twitter.heron.instance.HeronInstance ExclamationTopology ExclamationTopology4dd1e087-a0cf-4ea5-9e87-4217c412c646 container_1_word_2 word 2 0 stmgr-1 58491 50621 ./heron-conf/heron_internals.yaml\nand \n2016-05-31 17:36:57: Running container_1_exclaim1_1 process as /usr/lib/jvm/default-java/bin/java -Xmx320M -Xms320M -Xmn160M -XX:MaxPermSize=128M -XX:PermSize=128M -XX:ReservedCodeCacheSize=64M -XX:+CMSScavengeBeforeRemark -XX:TargetSurvivorRatio=90 -XX:+PrintCommandLineFlags -verbosegc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCCause -XX:+PrintPromotionFailure -XX:+PrintTenuringDistribution -XX:+PrintHeapAtGC -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:ParallelGCThreads=4 -Xloggc:log-files/gc.container_1_exclaim1_1.log -XX:+HeapDumpOnOutOfMemoryError -Djava.net.preferIPv4Stack=true -cp ./heron-core/lib/instance/*:heron-examples.jar com.twitter.heron.instance.HeronInstance ExclamationTopology ExclamationTopology4dd1e087-a0cf-4ea5-9e87-4217c412c646 container_1_exclaim1_1 exclaim1 1 0 stmgr-1 58491 50621 ./heron-conf/heron_internals.yaml\nThe error being HeapDumpOnOutOfMemoryError. Is it possible that this error is caused because zookeeper was not setup for State Manager?\n. I installed the missing dependencies on my ubuntu 14.04\nHowever when I run command heron kill --verbose local ExclamationTopology , It fails giving error:\n[2016-06-13 22:19:46 +0530] sun.net.www.protocol.http.HttpURLConnection FINEST:  ProxySelector Request for http://udit:48269/kill\n[2016-06-13 22:19:46 +0530] com.twitter.heron.spi.common.NetworkUtils SEVERE:  Failed to close OutputStream:\njava.net.ConnectException: Connection refused \nSomeone please help.\n. @maosongfu The output I receive is 127.0.0.1\n. ",
    "HongzhuLiu": "I also get this error in centos7! Hope can be solved\n. ",
    "XciD": "I've got this error by running the stmgr-1 directly : \n./heron-core/bin/heron-stmgr: error while loading shared libraries: libunwind.so.8: cannot open shared object file: No such file or directory\nResolved by installing libunwind\n. ",
    "deveshagrawal": "Faced exact same error while using ubuntu 16 while following the getting started guide.\nThere are some dependencies on linux:\nFollowing library files fixed it for me:\nsudo apt-get install git build-essential automake cmake libtool zip libunwind-setjmp0-dev zlib1g-dev unzip pkg-config -y\n. ",
    "lilalinda": "With Ubuntu 14.04.4 server, I faced the same issue of the missing file \n/home/USER/.herondata/repository/state/local/pplans/ExclamationTopology\nand this also resulted in an empty diagram or image for Logical Topology in the heron-ui web view as well as empty metrics and containers and such.\nThese minimal dependencies solved it for me:\n$ sudo apt-get install unzip libunwind-setjmp0\nand the heron-ui topology view now contains these diagrams (circled):\n\nBut still no log files\nheron-executor.stderr or heron-executor.stdout\nin\n.herondata/topologies/local/USER/ExclamationTopology/log-files/ \n. @maosongfu Yes, you are right that the log files are indeed a level higher than expected.  Now I declare success with the local getting-started example. Thanks!\n. @kramasamy Sorry for my silence -- I was sidetracked with other stuff.  Now back to Heron without vagrant and docker; quite an adventure!  I do have a small cluster of one master and two slave nodes all running Ubuntu 14.04.\nI tried your suggestion moving the setting above from scheduler.yaml to client.yaml and also adjusted to URI to include the scheme file such that it now says:\nheron.package.core.uri:                      \"file:///home/hadoopuser/.heron/dist/heron-core.tar.gz\"\nHowever, I now do get an error on the slave nodes that they cannot find the file, which is not surprising, as I had not installed Heron on them because I naively assumed that the necessary files would be shared through the Hadoop file system.  \nNow my specific question: How should this be set in a non-vagrant case and what is the minimal installation of Heron on the slaves?  I am reading the vagrant/init.sh file with the argument slave and it looks like build_heron() only gets executed on the master node, which corroborates my assumption above.  I must admit that I am not very familiar with docker yet, so when I look at install_docker() I am just reading that it deals with installing mesos-slave and starting the service...  Any thoughts or help?\n. @kramasamy Thanks for the pointer to Supun's blog post -- he did pretty much exactly what I tried (including using HDFS for distributing the Heron core and topology files to the workers).  Likewise, I discovered the same couple of missing libraries (libunwind, libcurl3/4). Unfortunately, I am still running into a runtime issue, which I will post about in a separate thread, namely #1027 \n. @kramasamy If I leave out role and environment (i.e., /hadoopuser/test) in the cluster specification, it tells me:\nhadoopuser@nimbus:~$ heron submit --verbose aurora ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology      ERROR: Argument cluster/[role]/[env] is not correct: role required but not provided\nBut I do see in my config files (using the Aurora ones from the release with minimal changes): \n```\nhadoopuser@nimbus:~$ more .heron/conf/aurora/client.yaml\nlocation of the core package\nheron.package.core.uri:                      \"file:///vagrant/.herondata/dist/heron-core-release.tar.gz\"\nheron.package.core.uri:                      \"hdfs:///heron/topologies/heron-core.tar.gz\"\nWhether role/env is required to submit a topology. Default value is False.\nheron.config.is.role.required:               True\nheron.config.is.env.required:                True\n```\nNow turning these off (by commenting them out) sets the role for the Aurora call to hadoopuser and the environment to default but then Aurora balks during the submit attempt with:\nError loading configuration: Environment should be one of \"prod\", \"devel\", \"test\" or staging<number>!  Got default\nso I guess it is safer to keep using test or one of the other expected values.\nAlso, I like using the 1 master and 2 slaves setup for now as I have seen spurious connection attempts from the slave nodes to 127.0.0.1:2181 (Zookeeper) even though I configured the Zookeeper IP at all recommended places to 10.9.9.10.  If the Aurora job happens to get deployed on the master node, I would miss such errors because the connection would succeed.\n. @kramasamy I am trying to package the log files and think I have a useful bundle now -- will share in a minute.  (Are you guys on Pacific Time?  I am currently working from Germany...)\nYes, I have on all 3 nodes:\nhadoopuser@nimbus:~$ dpkg -l | grep libunwind\nii  libunwind-setjmp0                   1.1-2.2ubuntu3                      amd64        libunwind-based non local goto - runtime\nii  libunwind-setjmp0-dev               1.1-2.2ubuntu3                      amd64        libunwind-based non local goto - development\nii  libunwind8                          1.1-2.2ubuntu3                      amd64        library to determine the call-chain of a program - runtime\nii  libunwind8-dev                      1.1-2.2ubuntu3                      amd64        library to determine the call-chain of a program - development\n. @maosongfu and @kramasamy Here are the log files.  I created them on the 2nd slave node changing into the directory\n/var/lib/mesos/slaves/<HASH>/frameworks/<HASH>/executors/thermos-<AURORA-TASK>/runs/latest/\nand then:\n$ tar -cvzf ~/test-logs.tgz --exclude=*.tar.gz --exclude=*.jar --exclude=*.pex --exclude=.pex --exclude=heron-core .\nFinally, I had to compress it again with 7z to .zip format for github to allow this file type to be uploaded.\n(Let me know if there is a better place to grab the logs.)\ntest-logs.zip\n. @maosongfu Re your 3 questions:\n1. No, I did not run the \"heron kill... \" command at that time.  I ran it a couple minutes later when the subsequently launched tasks also failed.  The logs you are referring to are the ones from the first task incarnation.\n2. @wuye9036 commented on this, I believe.\n3. I did not check any Aurora specific logs but you were right: it seems to be a RAM issue!\nI followed your guidance and re-built Heron from the latest source code (git 4cbe5ae96b72f) and it generated a Java Heap out-of-memory error!  The two slave nodes are each configured with 4GB of RAM, and Mesos advertised about 2.7GB available on each.  When I ran the ExclamationTopology out of the box (the code requests 512MB for each of the 2 containers: word and exclaim, so I though it should be around 1GB), and looked at the --verbose output of \"heron submit --verbose ...\" I saw that Aurora blew it up to 3GB RAM_PER_CONTAINER!\nNext, I tried lowering the RAM requested in the source code to 1MB (failed immediately upon \"heron submit\" asking for a minimum of 192MB) then to 200MB so that it would have a chance to run on my slave nodes.  Then I encountered the Java Heap Space error, which would explain how it failed at different points in the Google Protobuf code (because the out-of-memory gets hit at different times from run to run).\nFinally, I bit the bullet and powered everything down (well, just the 2 slaves) and increased the VMs to have 16GB rather than 4GB.  Now things seemed to somewhat work with the code recompiled at the original level of requesting 512MB per container.  I even got some log lines with the bolt counting some emits!  However, the topology still failed and I am running out of time now to debug this further.  I fear I won't get to it until Monday...\nThanks for all your help so far and have a great weekend.  I'll be looking into this further on Monday morning German time.  Best!\n. Happy Monday, everyone.  Back to the office and my Heron cluster.  I updated from git this morning to use the latest sources and am now running `\nStill some issues but more details in the logs.  It looks like the TMaster thread consistently dies.  But I have no clue, why.  Maybe you guys could take a look at the logs?  See below for a copy-and-paste and the attached ZIPs.\n@ajorgensen I enabled core dumps but it looks like none got generated.  Well, since the Java Runtime Exceptions are now thrown, maybe this bypasses the core dumping in Ubuntu?  Here some details after my last run:\nhadoopuser@supervisor1:~$ ulimit -a\ncore file size          (blocks, -c) unlimited\ndata seg size           (kbytes, -d) unlimited\nscheduling priority             (-e) 0\nfile size               (blocks, -f) unlimited\npending signals                 (-i) 64103\nmax locked memory       (kbytes, -l) 64\nmax memory size         (kbytes, -m) unlimited\nopen files                      (-n) 1024\npipe size            (512 bytes, -p) 8\nPOSIX message queues     (bytes, -q) 819200\nreal-time priority              (-r) 0\nstack size              (kbytes, -s) 8192\ncpu time               (seconds, -t) unlimited\nmax user processes              (-u) 64103\nvirtual memory          (kbytes, -v) unlimited\nfile locks                      (-x) unlimited\nhadoopuser@supervisor1:~$ cat /proc/sys/kernel/core_pattern\n|/usr/share/apport/apport %p %s %c %P\nhadoopuser@supervisor1:~$ ls -lat /usr/share/apport/apport*\n-rwxr-xr-x 1 root root 18241 May 18 11:44 /usr/share/apport/apport\n-rwxr-xr-x 1 root root  1269 May 18 11:44 /usr/share/apport/apport-checkreports\n-rwxr-xr-x 1 root root  3441 May 18 11:44 /usr/share/apport/apportcheckresume\n-rw-r--r-- 1 root root  2366 May 18 11:44 /usr/share/apport/apport.jar\nhadoopuser@supervisor1:~$ date\nMon Jul 11 11:27:59 CEST 2016\nsupervisor1 shows this interesting output in log-files/metricsmgr-0.log.0:\n```\n[...]\n[2016-07-11 11:24:13 +0200] com.twitter.heron.metricsmgr.MetricsManagerServer INFO:  We received a new TMasterLocation. Register it into SingletonRegistry\n[2016-07-11 11:24:13 +0200] com.twitter.heron.metricsmgr.MetricsManagerServer INFO:  Current TMaster location: topology_name: \"ExclamationTopology\"\ntopology_id: \"ExclamationTopology7a7981c0-c0d4-4d10-bd11-c5a2cecf7d70\"\nhost: \"supervisor1\"\ncontroller_port: 31945\nmaster_port: 31118\nstats_port: 31204\n[2016-07-11 11:24:18 +0200] com.twitter.heron.metricsmgr.sink.tmaster.TMasterSink INFO:  Update current TMasterLocation to: topology_name: \"ExclamationTopology\"\ntopology_id: \"ExclamationTopology7a7981c0-c0d4-4d10-bd11-c5a2cecf7d70\"\nhost: \"supervisor1\"\ncontroller_port: 31945\nmaster_port: 31118\nstats_port: 31204\n[2016-07-11 11:24:18 +0200] com.twitter.heron.metricsmgr.sink.tmaster.TMasterSink SEVERE:  Starting TMasterClient for the 1 time.\n[2016-07-11 11:24:18 +0200] com.twitter.heron.common.network.HeronClient INFO:  Connecting to endpoint: supervisor1/127.0.1.1:31118\n[2016-07-11 11:24:18 +0200] com.twitter.heron.metricsmgr.sink.tmaster.TMasterClient INFO:  Connected to TMaster. Ready to send metrics\n[2016-07-11 11:24:20 +0200] com.twitter.heron.common.network.IncomingPacket SEVERE:  channel.read returned negative -1\n[2016-07-11 11:24:20 +0200] com.twitter.heron.common.network.SocketChannelHelper SEVERE:  Something bad happened while reading from channel: /127.0.0.1:45008\n[2016-07-11 11:24:20 +0200] com.twitter.heron.common.network.HeronServer INFO:  Handling error from channel: /127.0.0.1:45008\n[2016-07-11 11:24:20 +0200] com.twitter.heron.common.network.HeronServer INFO:  Removing all interest on channel: /127.0.0.1:45008\n[2016-07-11 11:24:20 +0200] com.twitter.heron.metricsmgr.MetricsManagerServer SEVERE:  Got a connection close from remote socket address: /127.0.0.1:45008\n[2016-07-11 11:24:20 +0200] com.twitter.heron.metricsmgr.MetricsManagerServer SEVERE:  Un-register publish from hostname: supervisor1, component_name: tmaster, port: 31,118, instance_id: 0, instance_index: -1\n[2016-07-11 11:24:20 +0200] com.twitter.heron.common.network.IncomingPacket SEVERE:  channel.read returned negative -1\n[2016-07-11 11:24:20 +0200] com.twitter.heron.common.network.SocketChannelHelper SEVERE:  Something bad happened while reading from channel: supervisor1/127.0.1.1:31118\n[2016-07-11 11:24:20 +0200] com.twitter.heron.common.network.HeronClient INFO:  Handling Error. Cleaning states in HeronClient.\n[2016-07-11 11:24:20 +0200] com.twitter.heron.common.network.HeronClient INFO:  Successfully closed the channel: java.nio.channels.SocketChannel[closed]\n[2016-07-11 11:24:20 +0200] com.twitter.heron.metricsmgr.sink.tmaster.TMasterClient SEVERE:  Disconnected from TMaster.\n[2016-07-11 11:24:20 +0200] com.twitter.heron.metricsmgr.sink.tmaster.TMasterSink SEVERE:  TMasterClient dies in thread: Thread[Thread-1,5,main] \njava.lang.RuntimeException: Errors happened due to write or read failure from TMaster.\n    at com.twitter.heron.metricsmgr.sink.tmaster.TMasterClient.onError(TMasterClient.java:58)\n    at com.twitter.heron.common.network.HeronClient.handleError(HeronClient.java:349)\n    at com.twitter.heron.common.network.SocketChannelHelper.read(SocketChannelHelper.java:146)\n    at com.twitter.heron.common.network.HeronClient.handleRead(HeronClient.java:162)\n    at com.twitter.heron.common.basics.NIOLooper.handleSelectedKeys(NIOLooper.java:111)\n    at com.twitter.heron.common.basics.NIOLooper.access$000(NIOLooper.java:32)\n    at com.twitter.heron.common.basics.NIOLooper$1.run(NIOLooper.java:45)\n    at com.twitter.heron.common.basics.WakeableLooper.executeTasksOnWakeup(WakeableLooper.java:142)\n    at com.twitter.heron.common.basics.WakeableLooper.runOnce(WakeableLooper.java:74)\n    at com.twitter.heron.common.basics.WakeableLooper.loop(WakeableLooper.java:64)\n    at com.twitter.heron.metricsmgr.sink.tmaster.TMasterClient.run(TMasterClient.java:117)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\n```\nWhen I cd into the directory where the log files etc reside, namely /var/lib/mesos/slaves/<HASH>/frameworks/<HASH>/executors/thermos-<AURORA-TASK>/runs/latest, I see this interesting bit in the file __main.log__, which is not visible in the Aurora browsing (port 1338):\n$ head -n 9 __main__.log\nLog file created at: 2016/07/11 11:24:03\nRunning on machine: supervisor1\n[DIWEF]mmdd hh:mm:ss.uuuuuu pid file:line] msg\nCommand line: /var/lib/mesos/slaves/a7058b40-59c4-4742-aa30-bd69cadb5ad1-S8/frameworks/1e37e686-4cc7-465d-bc0c-a2b6e1f286f4-0000/executors/thermos-hadoopuser-test-ExclamationTopology-0-941a7b9b-a46a-46e4-8581-61dd877659d8/runs/fe9a376b-f502-4714-9dea-eac9994a8087/thermos_executor.pex --announcer-ensemble 127.0.0.1:2181\nI0711 11:24:03.502679 9436 executor_base.py:45] Executor [None]: registered() called with:\nI0711 11:24:03.503627 9436 executor_base.py:45] Executor [None]:    ExecutorInfo:  executor_id {\n  value: \"thermos-hadoopuser-test-ExclamationTopology-0-941a7b9b-a46a-46e4-8581-61dd877659d8\"\n}\nresources {\nHere, the configuration with --announcer-ensemble 127.0.0.1:2181 is what worries me:  The Zookeeper instance is NOT running on localhost but on 10.9.9.10:2181, which is how I configured it in all possible places.  But could this be the culprit?  The Aurora processes fetch_* run through without a hitch but then the launch_heron_executor fails.  Could the mistaken reference to --announcer-ensemble cause the error later on???\nAttached are the logs from the above cited run and generated with:\n$ tar -cvzf ~/supervisor1-logs.tgz --exclude=*.tar.gz --exclude=*.jar --exclude=*.pex --exclude=.pex --exclude=heron-core .\n$ cd\n$ zip supervisor1-logs.zip supervisor1-logs.tgz\nsupervisor1-logs.zip\nI am also looking at supervisor2 and the same issues turn up there.\nFinally, I am trying to read and understand the code at heron/executor/src/python/heron-executor.py and specifically how the class HeronExecutor gets initialized but I don't see an immediate place where the announcer ensemble gets conveyed.  Maybe it is some hidden Aurora configuration?  Which would be weird because I was able to run the Aurora hello_world example from their Getting Started tutorial issued from nimbus (master) to the supervisors (slaves) without a problem...\n. Finally, I got it to run!  It was indeed the 127.0.0.1:2181 culprit printed in __main__.log!\nI wasn't able to backtrace where this setting came from but poking around I finally found the file /etc/default/aurora-scheduler on nimbus and changed the occurrences of 127.0.0.1 in the appropriate places such that:\n$ grep \"2181\" /etc/default/aurora-scheduler\nZK_ENDPOINTS=\"10.9.9.10:2181\"\nTHERMOS_EXECUTOR_FLAGS=\"--announcer-ensemble 10.9.9.10:2181\"\nNow I will roll back to the Heron 0.14.0 release and see if it works as well.  Closing this issue as it is solved.\n. ",
    "ababushkin": "~~Anyone have a solution for this issue on Mac OS?~~\nThe solution was pretty simple, I just checked the Heron Troubleshooting Guide http://twitter.github.io/heron/docs/getting-started-troubleshooting/\nI needed to add a new host entry to /etc/hosts:\npython -c \"import socket; print socket.gethostname()\"\nI then added the output of that command to 127.0.0.1 in /etc/hosts. ",
    "storysj": "It would be nice to also have this note visible in the Quick Start guide: http://twitter.github.io/heron/docs/getting-started/\n. ",
    "smxysu3": "@kramasamy - thanks ! \nbecause I did  not install  \"setuptools\"\n. ",
    "dnk8n": "Hi all. Promising project, excited to get started. However, I have found quite a few dead links around the Heron project...\nThe link above (http://twitter.github.io/heron/docs/developers/compiling/linux/#building-on-ubuntu-14.04:34fe9f2d9cc3cd7e319ca75e7da08332) is dead.\nAlso, https://apache.github.io/incubator-heron/docs/developers/python/topologies/, the following do not work:\n- https://github.com/twitter/heron/tree/master/heron/examples/src/python\n- Almost all links under the 'Example topologies' section.\nTrying to work out how I build a PEX file. Can anyone point me in the right direction?. ",
    "bretlowery": "Checking my path, install locations, etc. \n. I uninstalled and reinstalled and it worked. I think I missed a PATH export the first time around.\n. ",
    "mlinge": "My Ubuntu version is 16.04 LTS, Its version >=14.04.\nI was using the \"./xxx.sh --user\" installed, did not use the compiled code, it's installed successfully\u3002\nexecute \"Heron help submit\" command failed\n. ",
    "avitorovic": "I get the same error when running 'heron version', although I am using Ubuntu 14.04 (I upgraded from Ubuntu 12.04). \n. My OS is 32-bit, and that caused trouble. It seems I will have to install from the source. \n. A related comment: It would also be nice to have a way to programmatically kill a topology. In Storm, I could do it with backtype.storm.generated.Nimbus.Client.killTopology(topologyName). This is useful when data sources send some kind of EOF signal, after which we might decide to kill the topology.\n. ",
    "pradeep0605": "I am getting the same error :\n$> heron\nFailed to execute PEX file, missing compatible dependencies for:\npsutil\npyyaml\nWas there any fix for this ? I think I did heron-client-install-0.14.4-ubuntu.sh (without the --user) once, and after running this command, I am unable to run heron at all. Can someone please let me know what can be the problem here ? I am using ubutnu 14.04.\nThanks\nPradeep\n. ",
    "phynman": "@billonahill  download the artifact and upload it to your local maven repo.\nI am working on centos7 , how to put it to local maven repo ?  There is no maven repo when I build in my machine .\n. Thanks , I got it  . I have successfully builded  it .\n. @kramasamy  Thanks ,  I  know it \n. ",
    "goldenhoo": "@phynman , How to  fix libthrift-0.5.0-1.jar?. There is no libthrift-0.5.0-1.jar  ( org.apache.thrift:libthrift:jar:0.5.0-1 )  in the website \"http://maven.twttr.com \".\nHas any website  it?. Thanks in advance. \n  My question:\n    goldens-MacBook-Air:heron golden$ bazel build --config=darwin heron/...\nERROR: /Users/golden/heron/heron/metricsmgr/src/thrift/BUILD:5:1: no such package '@org_apache_thrift_libthrift//jar': Failed to fetch Maven dependency: Could not transfer artifact org.apache.thrift:libthrift:jar:0.5.0-1 from/to org_apache_thrift_libthrift (http://maven.twttr.com): Connect to maven.twttr.com:80 timed out and referenced by '//heron/metricsmgr/src/thrift:thrift_scribe_java'.\nERROR: Analysis of target '//heron/metricsmgr/src/thrift:thrift-java' failed; build aborted.\nINFO: Elapsed time: 13.361s. ",
    "danielschonfeld": "No problem! Thank you for all the work you guys have and are doing.\nJust so I'm clear is IScheduler and ILauncher the absolute minimum required\nat the moment for a custom scheduler?\nOn Sunday, May 29, 2016, Karthik Ramasamy notifications@github.com wrote:\n\n@danielschonfeld https://github.com/danielschonfeld - thanks for\npointing out. We are in the process of updating it.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/833#issuecomment-222373058, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe/AAf5ZiZBJyVO8PJSiosfgDa_QvDcI54kks5qGdCrgaJpZM4IpUym\n.\n. \n",
    "jieliu3": "@longdafeng i put libunwind into LD_LIBRARY_PATH ,then resubmit \uff0cproblem also is flying ..\n@maosongfu  \"2. Document clearly the dependencies (native system libraries) c++ executable relies on into the getting-started guide\"  \uff0ci can't find this update in http://twitter.github.io/heron/docs/getting-started/\n. it'ok when switch to centos 7 (glibc 2.17) @maosongfu \n. ",
    "thedrow": "Yes, sorry about that.\n. Any reason why the build is still on Ubuntu precise?\n. Seems like bazel has some known problems with CCache. See https://github.com/bazelbuild/bazel/issues?utf8=%E2%9C%93&q=is%3Aissue+is%3Aopen+ccache\nHow do we proceed?\n. ",
    "Jonathan-Wei": "No body help me?\n. @kramasamy \nI execute automake command.\nThe command line display \uff1a\nautomake:error:'configure.ac' is required\nAnd I link automake to /usr/bin/automake\nBut the problem still exists\n. My system is Mac OS X Yosemite 10.10.5\n. @billonahill \nI add some print to debugging in real_progream_path and which method!\npath is /opt/local/bin\nexe_file is /opt/local/bin/cmake\ncandidate is /opt/local/bin/cmake\nisfile : False ,access enough: False :\nis_exc method return is False\nisfile : False ,access enough: False :\ncandidate is /opt/local/bin/cmake\nisfile : False ,access enough: False :\nis_exc method return is False\nisfile : False ,access enough: False :\n```\ndef which(program):\n  def is_exe(fpath):\n    print(\"isfile : %s ,access enough: %s :\" % (os.path.isfile(fpath) , os.access(fpath, os.X_OK)))\n    return os.path.isfile(fpath) and os.access(fpath, os.X_OK)\ndef ext_candidates(fpath):\n    yield fpath\n    for ext in os.environ.get(\"PATHEXT\", \"\").split(os.pathsep):\n      yield fpath + ext\nfpath, fname = os.path.split(program)\n  print(\"fpath %s fname %s\" %(fpath,fname))\n  if fpath:\n    if is_exe(program):\n      return program\n  else:\n    for path in os.environ[\"PATH\"].split(os.pathsep):\n      print(\"path is %s \" % path)\n      exe_file = os.path.join(path, program)\n      print(\"exe_file is %s\" % exe_file)\n      for candidate in ext_candidates(exe_file):\n        print(\"candidate is %s \" % candidate)\n        print(\"is_exc method return is %s\" % is_exe(candidate))\n        if is_exe(candidate):\n          return candidate\nreturn None\n```\nI execute os.path.isfile and os.access in python command return true,but why here return false\n\u279c  heron git:(master) \u2717 python\nPython 2.7.10 (default, Jul 14 2015, 19:46:27)\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport os\nos.path.isfile('/usr/local/bin/automake')\nTrue\nos.access('/usr/local/bin/automake',os.X_OK)\nTrue\n\n\n\n\u279c  heron git:(master) \u2717 ls -al /usr/local/bin/automake\nlrwxr-xr-x  1 jonathanwei  admin  36  5 31 01:16 /usr/local/bin/automake -> ../Cellar/automake/1.15/bin/automake\n\u279c  heron git:(master) \u2717 cd /usr/local/Cellar/automake/1.15/bin\n\u279c  bin git:(master) \u2717 ls -al\ntotal 1152\ndrwxr-xr-x   6 jonathanwei  admin     204  5 31 01:16 .\ndrwxr-xr-x  10 jonathanwei  admin     340  5 31 01:16 ..\n-r-xr-xr-x   2 jonathanwei  admin   36873  5 31 01:16 aclocal\n-r-xr-xr-x   2 jonathanwei  admin   36873  5 31 01:16 aclocal-1.15\n-r-xr-xr-x   2 jonathanwei  admin  253233  5 31 01:16 automake\n-r-xr-xr-x   2 jonathanwei  admin  253233  5 31 01:16 automake-1.15\nThanks!\n. @billonahill\nOh,I'm Sorry!\nI re-check it again! \nThanks!\n. Execute bazel build --config=darwin heron/... --verbose_failures command :\nINFO: From Building heron/spi/tests/java/network-utils_unittest.jar (1 files):\nheron/spi/tests/java/com/twitter/heron/spi/common/NetworkUtilsTest.java:41: \u9519\u8bef: \u65e0\u6cd5\u8bbf\u95eeAbstractCommonPowerMockRunner\n@RunWith(PowerMockRunner.class)\n                        ^\n  \u627e\u4e0d\u5230org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner\u7684\u7c7b\u6587\u4ef6\nheron/spi/tests/java/com/twitter/heron/spi/common/NetworkUtilsTest.java:216: \u9519\u8bef: \u65e0\u6cd5\u8bbf\u95eeMemberModifier\n    PowerMockito.spy(NetworkUtils.class);\n                ^\n  \u627e\u4e0d\u5230org.powermock.api.support.membermodification.MemberModifier\u7684\u7c7b\u6587\u4ef6\n2 \u4e2a\u9519\u8bef\nBazelJavaBuilder threw exception: java compilation returned status ERROR\nERROR: /Users/jonathanwei/summary/GITRepository/heron/heron/spi/tests/java/BUILD:47:1: Java compilation in rule '//heron/spi/tests/java:network-utils_unittest' failed: java failed: error executing command\n  (cd /private/var/tmp/_bazel_jonathanwei/f0e0d1fb1f89f02c31d3facf1d735668/heron && \\\n  exec env - \\\n  external/local-jdk/bin/java -Xbootclasspath/p:external/bazel_tools/third_party/java/jdk/langtools/javac.jar -client -jar external/bazel_tools/tools/jdk/JavaBuilder_deploy.jar @bazel-out/local_darwin-fastbuild/bin/heron/spi/tests/java/network-utils_unittest.jar-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1: java failed: error executing command\n  (cd /private/var/tmp/_bazel_jonathanwei/f0e0d1fb1f89f02c31d3facf1d735668/heron && \\\n  exec env - \\\n  external/local-jdk/bin/java -Xbootclasspath/p:external/bazel_tools/third_party/java/jdk/langtools/javac.jar -client -jar external/bazel_tools/tools/jdk/JavaBuilder_deploy.jar @bazel-out/local_darwin-fastbuild/bin/heron/spi/tests/java/network-utils_unittest.jar-2.params): com.google.devtools.build.lib.shell.BadExitStatusException: Process exited with status 1.\nINFO: Elapsed time: 4.737s, Critical Path: 4.08s.\n. 3Q!@zhangzhonglai @billonahill \nBuild success!\n. @kramasamy \nIt does not support yarn?\nhttps://github.com/twitter/heron/pull/419  not merge\uff1f\n. I try it!\n3Q guys!\n. @kramasamy \nOh! It's great!\nThank you!\n. ",
    "insomniacslk": "@billonahill just accepted the CLA\n. and also removed a line that I forgot around\n. The tests seem to be passing now, I guess it was an issue unrelated to my PR\n. you're really doing +1 on all the notes from @nlu90? :D\n. However good point, I'm replacing the calls as @nlu90 suggests\n. ",
    "zhangzhonglai": "@Jonathan-Wei \nI got the same problem.\nFollowing change can fix this issue.\ndiff --git a/3rdparty/java/BUILD b/3rdparty/java/BUILD\nindex a3a3688..e3250f4 100644\n--- a/3rdparty/java/BUILD\n+++ b/3rdparty/java/BUILD\n@@ -99,8 +99,10 @@ java_library(\n     srcs = [ \"Empty.java\" ],\n     exports = [\n         \"@powermock-api-mockito//jar\",\n+       \"@powermock-api-support//jar\",\n         \"@powermock-core//jar\",\n         \"@powermock-module-junit4//jar\",\n+       \"@powermock-module-junit4-common//jar\",\n     ],\n     deps = [\n         \"@powermock-api-mockito//jar\",\n. @billonahill \nYes. I'll try to do this.But I've never done this before. If there is something wrong, please tell me.\n. @ashvina Actually there is only the first container's log-files exist. Other log-files generated from tmaster and stmgr are not existing in the working dir.\nlog-files.tar.gz\nThe working directory list of the first container is following:\ntotal 51396\n-rw-r--r-- 1 yarn yarn     2226 Jul  1 12:34 AckingTopology.defn\n-rw-r--r-- 1 yarn yarn       68 Jul  1 12:35 container_tokens\n-rwx------ 1 yarn yarn      641 Jul  1 12:35 default_container_executor_session.sh\n-rwx------ 1 yarn yarn      695 Jul  1 12:35 default_container_executor.sh\n-rw-r--r-- 1 yarn yarn 50412533 Jul  1 12:35 global.jar\ndrwxr-xr-x 2 yarn yarn     4096 Jan  1  1970 heron-conf\ndrwxr-xr-x 4 yarn yarn       26 Jan  1  1970 heron-core\n-rwxr-xr-x 1 yarn yarn  2187258 Jan  1  1970 heron-examples.jar\n-rwx------ 1 yarn yarn     3683 Jul  1 12:35 launch_container.sh\ndrwxr-xr-x 2 yarn yarn       96 Jul  1 12:35 log-files\nlrwxrwxrwx 1 yarn yarn      116 Jul  1 12:35 reef -> /data/9/yarn/nm/usercache/yarn/appcache/application_1466049056624_1614/filecache/10/reef-job-7516731119712048753.jar\n-r-xr-xr-x 1 yarn yarn      272 Jan  1  1970 release.yaml\ndrwx--x--- 2 yarn yarn        6 Jul  1 12:35 tmp\nAnd next other contains' working directory:\ntmaster\ntotal 2168\n-rw-r--r-- 1 yarn yarn    2226 Jul  1 12:34 AckingTopology.defn\n-rw-r--r-- 1 yarn yarn      97 Jul  1 12:35 container_tokens\n-rwx------ 1 yarn yarn     641 Jul  1 12:35 default_container_executor_session.sh\n-rwx------ 1 yarn yarn     695 Jul  1 12:35 default_container_executor.sh\ndrwxr-xr-x 2 yarn yarn    4096 Jan  1  1970 heron-conf\ndrwxr-xr-x 4 yarn yarn      26 Jan  1  1970 heron-core\n-rwxr-xr-x 1 yarn yarn 2187258 Jan  1  1970 heron-examples.jar\n-rwx------ 1 yarn yarn    4093 Jul  1 12:35 launch_container.sh\n-rw-r--r-- 1 yarn yarn       6 Jul  1 12:35 PID.txt\ndrwxr-xr-x 2 yarn yarn      31 Jul  1 12:35 reef\n-r-xr-xr-x 1 yarn yarn     272 Jan  1  1970 release.yaml\ndrwx--x--- 2 yarn yarn       6 Jul  1 12:35 tmp\nstmgr\ntotal 2168\n-rw-r--r-- 1 yarn yarn    2226 Jul  1 12:34 AckingTopology.defn\n-rw-r--r-- 1 yarn yarn      97 Jul  1 12:35 container_tokens\n-rwx------ 1 yarn yarn     645 Jul  1 12:35 default_container_executor_session.sh\n-rwx------ 1 yarn yarn     699 Jul  1 12:35 default_container_executor.sh\ndrwxr-xr-x 2 yarn yarn    4096 Jan  1  1970 heron-conf\ndrwxr-xr-x 4 yarn yarn      26 Jan  1  1970 heron-core\n-rwxr-xr-x 1 yarn yarn 2187258 Jan  1  1970 heron-examples.jar\n-rwx------ 1 yarn yarn    4093 Jul  1 12:35 launch_container.sh\n-rw-r--r-- 1 yarn yarn       6 Jul  1 12:35 PID.txt\ndrwxr-xr-x 2 yarn yarn      31 Jul  1 12:35 reef\n-r-xr-xr-x 1 yarn yarn     272 Jan  1  1970 release.yaml\ndrwx--x--- 2 yarn yarn       6 Jul  1 12:35 tmp\nYou can see there's no log-files in their working directory. But the pb is actually running. \nMy yarn cluster's configuration is almost default of CDH. What kind yarn information do you want?\n. @ashvina I redirect the output and error stream of ProcessBuilder to tmp file. There is a Permission denied error in the tmp file.\nIt's my fault. I'm using the user yarn to submit the heron topology which hasn't a home directory. This means the $HOME env point to /home. Then heron use the pex to execute the heron-executor command. It will generate the .pex into $HOME/.pex which is permission denied.\nFor now, there is no way to know what happened between pb.start() and the heron-executor started. This will make people confused if some system dependent error were occurred at that time.\nFinally, Thank you for taking time to focus on my issue. I will change the title and close this issue.\n. @ashvina I find that my yarn user has a home directory which point to /var/lib/hadoop-yarn. So the problem is: the pex get an incorrect $HOME env for yarn user.\n. @mycFelix Hi, I think there is a bit of difference between my problem with yours. My problem maybe encountered before the point of yours. Following is the error:\nTraceback (most recent call last):\n  File \".bootstrap/_pex/pex.py\", line 322, in execute\n  File \".bootstrap/_pex/pex.py\", line 78, in _activate\n  File \".bootstrap/_pex/environment.py\", line 132, in activate\n  File \".bootstrap/_pex/environment.py\", line 176, in _activate\n  File \".bootstrap/_pex/environment.py\", line 121, in update_candidate_distributions\n  File \".bootstrap/_pex/environment.py\", line 107, in load_internal_cache\n  File \".bootstrap/_pex/environment.py\", line 95, in write_zipped_internal_cache\n  File \".bootstrap/_pex/util.py\", line 176, in cache_distribution\n  File \".bootstrap/_pex/common.py\", line 124, in safe_open\n  File \".bootstrap/_pex/common.py\", line 112, in safe_mkdir\n  File \"/opt/rh/python27/root/usr/lib64/python2.7/os.py\", line 150, in makedirs\n    makedirs(head, mode)\n  File \"/opt/rh/python27/root/usr/lib64/python2.7/os.py\", line 150, in makedirs\n    makedirs(head, mode)\n  File \"/opt/rh/python27/root/usr/lib64/python2.7/os.py\", line 150, in makedirs\n    makedirs(head, mode)\n  File \"/opt/rh/python27/root/usr/lib64/python2.7/os.py\", line 157, in makedirs\n    mkdir(name, mode)\nOSError: [Errno 13] Permission denied: '/home/.pex/install'\nMy problem occurred when the pex is installing. So the error message will not exist in heron_executor.stdout. Because the heron_executor command can't run and the heron_executor.stdout file is not exist in path.\nIn your case, your problem occurred after the pex installed. Can you find the .pex directory in the yarn working directory?\nI guess this problem maybe a system-dependent issue or config-dependent issue with python or pex. But I have no time to deal with the problem. \nSorry, I have no idea of the problem for now.\n. @kramasamy Done.\n. @maosongfu Actually, this log is for people who use YARN scheduler, quickly and easily to locate the working directory. For now, only the yarn working directory is difficult to find in executor machine. So I think this code can move to another class HeronExecutorTask which is only for yarn scheduler.\nOn the other hand, is there a way to easily adjust the log level. If not, then the log level is set to Level.INFO will be better.\n. ",
    "aspin": "Are there any plans to do this in the near future? Alternatively, got any thoughts on doing HTTP posts without this? (mostly on a final bolt, synchronous requests would be undesirable). ",
    "qudongfang": "@billonahill  I have accepted the CLA.\n. ",
    "jiandongjia": "@nlu90  yes,thanks, I added the --verbose parameter and found that it was a question of permission.\n. @maosongfu   I had the same problem, but my   Aurora PENDING didn't have any tips.\n[2016-06-10 12:09:21 +0800] com.twitter.heron.scheduler.aurora.AuroraLauncher INFO:  Launching topology in aurora  \n[2016-06-10 12:09:21 +0800] com.twitter.heron.spi.common.ShellUtils INFO:  $> [aurora, job, create, --wait-until, RUNNING, --bind, SANDBOX_STMGR_BINARY=./heron-core/bin/heron-stmgr, --bind, COMPONENT_JVM_OPTS_IN_BASE64=\"\", --bind, TOPOLOGY_NAME=ExclamationTopology, --bind, ENVIRON=devel, --bind, ROLE=root, --bind, STATEMGR_ROOT_PATH=/heron, --bind, TOPOLOGY_DEFINITION_FILE=ExclamationTopology.defn, --bind, TOPOLOGY_ID=ExclamationTopology24ef552e-69d1-48ae-ade2-cb9cc932f47e, --bind, SANDBOX_SHELL_BINARY=./heron-core/bin/heron-shell, --bind, TOPOLOGY_PACKAGE_URI=/heron/topologies/main/ExclamationTopology-root-tag-0--7553500226791833473, --bind, STATEMGR_CONNECTION_STRING=192.168.1.108:2181, --bind, HERON_SANDBOX_JAVA_HOME=/usr/src/jdk1.7.0_79, --bind, TOPOLOGY_PACKAGE_TYPE=jar, --bind, DISK_PER_CONTAINER=1073741824, --bind, SANDBOX_SYSTEM_YAML=./heron-conf/heron_internals.yaml, --bind, NUM_CONTAINERS=2, --bind, TOPOLOGY_CLASSPATH=heron-examples.jar, --bind, SANDBOX_TMASTER_BINARY=./heron-core/bin/heron-tmaster, --bind, RAM_PER_CONTAINER=2147483648, --bind, SANDBOX_METRICS_YAML=./heron-conf/metrics_sinks.yaml, --bind, INSTANCE_JVM_OPTS_IN_BASE64=\"LVhYOitIZWFwRHVtcE9uT3V0T2ZNZW1vcnlFcnJvcg&equals;&equals;\", --bind, COMPONENT_RAMMAP=exclaim1:536870912,word:536870912, --bind, CORE_PACKAGE_URI=file:///usr/local/heron/dist/heron-core.tar.gz, --bind, SANDBOX_METRICSMGR_CLASSPATH=./heron-core/lib/metricsmgr/*, --bind, ISPRODUCTION=false, --bind, SANDBOX_EXECUTOR_BINARY=./heron-core/bin/heron-executor, --bind, CLUSTER=main, --bind, CPUS_PER_CONTAINER=1.0, --bind, SANDBOX_SCHEDULER_CLASSPATH=./heron-core/lib/scheduler/*:./heron-core/lib/packing/*:./heron-core/lib/statemgr/*, --bind, INSTANCE_DISTRIBUTION=1:word:2:0:exclaim1:1:0, --bind, SANDBOX_INSTANCE_CLASSPATH=./heron-core/lib/instance/*, --bind, TOPOLOGY_JAR_FILE=heron-examples.jar, main/root/devel/ExclamationTopology, /usr/local/heron/conf/main/heron.aurora, --verbose]  \n[2016-06-10 12:09:31 +0800] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x1553877b4970008 after 1ms  \n[2016-06-10 12:09:41 +0800] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x1553877b4970008 after 0ms  \n[2016-06-10 12:09:51 +0800] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x1553877b4970008 after 1ms  \n[2016-06-10 12:10:01 +0800] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x1553877b4970008 after 1ms  \n[2016-06-10 12:10:11 +0800] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x1553877b4970008 after 0ms  \n[2016-06-10 12:10:21 +0800] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x1553877b4970008 after 0ms  \n[2016-06-10 12:10:31 +0800] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x1553877b4970008 after 1ms  \n[2016-06-10 12:10:41 +0800] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x1553877b4970008 after 0ms\n. @nlu90 thank you,I have no problem with my users, but why I use sudo other users will visit my  home directory\n. I modified get_all_heron_paths.sh file, add a parameter --config = centos, now works correctly.\n\n. ",
    "qiuyij": "@billonahill @maosongfu Agreed that some reorganization of the web pages is needed. Though I think it's helpful to temporarily put this up.\n. @billonahill Cool. I'll move things around.\n. @maosongfu \n. Perhaps these may help #834 #822 \nMore guides on troubleshooting will be published soon #877 \n. Thanks. Resolved in new commit.\n. I did a hacky thing now this should be resolved.\n. The other one is actually also just Troubleshooting Guide. It's a good idea to distinguish the two. As I remember from our team meeting two weeks ago, you @billonahill are the expert on naming things! Suggestions?\n. ",
    "wking1986": "Get it !!\n\nI modify env with \"devel\" and sucess submit to aurora\n\n@maosongfu    Thank you for your help\n. @maosongfu ,  I can see Topology  in aurora  , but not find in Heron-ui  , Please Why\uff1f\n\n. OK, I try again\u00a0,  Thank you very much!! \n. @maosongfu \uff0cI have modfied heron_tracker.yaml\uff0cand heron-ui can show topology\nBut topology is not activate\uff0cthen I execute cmd\uff1a\nheron activate --verbose aurora/root/devel ExclamationTopology\n\nI find zk path \uff1a/heron/pplans  reliably hava not TopologyName(ExclamationTopology) ,But other zk dir hava ExclamationTopology (eg: /heron/topologies/ExclamationTopology)\nWhy \"/heron/pplans\" hava no  ExclamationTopology?  which yaml config have problem\uff1f\nstatemgr.yaml like this:\n\n. @maosongfu @qiuyij  If Topology sumbit to aurora\uff0cCan I  figure out reasons failed to start process from:~/.herondata/topologies/{cluster}/{role}/{topologyName}/ heron-executor.stdout  ? \n. @aaronshan If Topology sumbit to aurora\uff0cyou can find out in mesos/slaves/........./latest/sandbox/heron-executor.stdout\n. @maosongfu @nlu90 @qiuyij  Thanks for your help\uff0cHeron on Aurora is running!!\n. @maosongfu Great!! Very much looking forward to Heron on Mesos \n. \nheron version is \u2018master\u2019\ngit clone date is : 2016.7.27 \n. ",
    "kartik894": "Hi,\nI am getting the following error:\nError loading configuration: Could not find job aurora/root/default/ExclamationTopology\nCandidates are:\n  aurora/root/devel/ExclamationTopology\n@wking1986 Where should I exactly change the environment?\n. I am using HDFS uploader for the aurora cluster. I am getting the following error upon submitting the topology:\nCaused by: java.lang.IllegalArgumentException: Invalid path string \"/hdfs:///heron/topologies/foo\" caused by empty node name specified @7\nThese are my config files:\nscheduler.yaml\n```\nscheduler class for distributing the topology for execution\nheron.class.scheduler: com.twitter.heron.scheduler.aurora.AuroraScheduler\nlauncher class for submitting and launching the topology\nheron.class.launcher: com.twitter.heron.scheduler.aurora.AuroraLauncher\nlocation of the core package\nheron.package.core.uri: hdfs:///tmp/.heron/dist/heron-core.tar.gz\nlocation of java - pick it up from shell environment\nheron.directory.sandbox.java.home: /usr/lib/jvm/java-8-oracle\nInvoke the IScheduler as a library directly\nheron.scheduler.is.service: False\n```\nstatemgr.yaml\n```\nlocal state manager class for managing state in a persistent fashion\nheron.class.state.manager: com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager\nlocal state manager connection string\nheron.statemgr.connection.string:  \"masternode:2181\"\npath of the root address to store the state in a local file system\nheron.statemgr.root.path: \"hdfs:///heron\"\ncreate the zookeeper nodes, if they do not exist\nheron.statemgr.zookeeper.is.initialize.tree: True\ntimeout in ms to wait before considering zookeeper session is dead\nheron.statemgr.zookeeper.session.timeout.ms: 30000\ntimeout in ms to wait before considering zookeeper connection is dead\nheron.statemgr.zookeeper.connection.timeout.ms: 30000\ntimeout in ms to wait before considering zookeeper connection is dead\nheron.statemgr.zookeeper.retry.count: 10\nduration of time to wait until the next retry\nheron.statemgr.zookeeper.retry.interval.ms: 10000\n```\nuploader.yaml\n```\nuploader class for transferring the topology jar/tar files to storage\nheron.class.uploader: com.twitter.heron.uploader.hdfs.HdfsUploader\nDirectory of config files for hadoop client to read from\nheron.uploader.hdfs.config.directory: /usr/local/hadoop/etc/hadoop\nname of the directory to upload topologies for HDFS uploader\nheron.uploader.hdfs.topologies.directory.uri: hdfs:///heron/topologies/${CLUSTER}\n```\nclient.yaml\n```\nlocation of the core package\nheron.package.core.uri:                      \"hdfs:///tmp/.heron/dist/heron-core.tar.gz\"\nWhether role/env is required to submit a topology. Default value is False.\nheron.config.is.role.required:               False\nheron.config.is.env.required:               False\n```\nIs there anything wrong in the config files?\n. @maosongfu Thanks! Its running now.\n. Check /etc/aurora/clusters.json file . Change name of cluster to 'aurora'\n. Still getting the same error after swapping the command line arguments.\n. Even after adding --verbose, getting the same output. \nERROR: Argument cluster/[role]/[env] is not correct: role required but not provided\n. Thanks @nlu90 . Its working now\n. My bad. There were a few missing dependencies in the executor nodes. Works fine now.\n. @nlu90 \nHi, point 2 was the problem. Fixed it.\n. Ah, okay!\n. @ideal-hp \nAll issues related to submitting topologies in Heron have been addressed in #883 . If you scroll down to the last, I have posted a sample configuration that I used for Aurora and HDFS(for config files). \n. ",
    "mhajibaba": "@maosongfu I have the same problem with error message in #883, but i get the following messages:\n```\n[2016-07-02 16:27:05 +0430] com.twitter.heron.spi.common.ShellUtils INFO:  \n[2016-07-02 16:27:05 +0430] com.twitter.heron.spi.common.ShellUtils INFO:  DEBUG] Command=(['job', 'create', '--wait-until', 'RUNNING', '--bind', 'TOPOLOGY_NAME=ExclamationTopology', '--bind', 'SANDBOX_SYSTEM_YAML=./heron-conf/heron_internals.yaml', '--bind', 'COMPONENT_RAMMAP=exclaim1:536870912,word:536870912', '--bind', 'SANDBOX_METRICS_YAML=./heron-conf/metrics_sinks.yaml', '--bind', 'INSTANCE_JVM_OPTS_IN_BASE64=\"LVhYOitIZWFwRHVtcE9uT3V0T2ZNZW1vcnlFcnJvcg&equals&equals\"', '--bind', 'ROLE=root', '--bind', 'ENVIRON=devel', '--bind', 'SANDBOX_SCHEDULER_CLASSPATH=./heron-core/lib/scheduler/:./heron-core/lib/packing/:./heron-core/lib/statemgr/', '--bind', 'SANDBOX_INSTANCE_CLASSPATH=./heron-core/lib/instance/', '--bind', 'ISPRODUCTION=false', '--bind', 'TOPOLOGY_CLASSPATH=heron-examples.jar', '--bind', 'CLUSTER=aurora', '--bind', 'SANDBOX_EXECUTOR_BINARY=./heron-core/bin/heron-executor', '--bind', 'STATEMGR_CONNECTION_STRING=192.168.11.231:2181,192.168.11.232:2181,192.168.11.233:2181', '--bind', 'COMPONENT_JVM_OPTS_IN_BASE64=\"\"', '--bind', 'TOPOLOGY_ID=ExclamationTopologyc2f53ad0-76be-4e83-8c63-2134faede687', '--bind', 'TOPOLOGY_PACKAGE_URI=file:///root/.herondata/repository/topologies/aurora/root/ExclamationTopology/ExclamationTopology-root-tag-0--3706733491519378097', '--bind', 'SANDBOX_STMGR_BINARY=./heron-core/bin/heron-stmgr', '--bind', 'CORE_PACKAGE_URI=file:///root/.heron/dist/heron-core.tar.gz', '--bind', 'SANDBOX_METRICSMGR_CLASSPATH=./heron-core/lib/metricsmgr/*', '--bind', 'TOPOLOGY_PACKAGE_TYPE=jar', '--bind', 'RAM_PER_CONTAINER=2147483648', '--bind', 'SANDBOX_TMASTER_BINARY=./heron-core/bin/heron-tmaster', '--bind', 'TOPOLOGY_DEFINITION_FILE=ExclamationTopology.defn', '--bind', 'INSTANCE_DISTRIBUTION=1:word:2:0:exclaim1:1:0', '--bind', 'NUM_CONTAINERS=2', '--bind', 'CPUS_PER_CONTAINER=1.0', '--bind', 'TOPOLOGY_JAR_FILE=heron-examples.jar', '--bind', 'SANDBOX_SHELL_BINARY=./heron-core/bin/heron-shell', '--bind', 'DISK_PER_CONTAINER=1073741824', '--bind', 'STATEMGR_ROOT_PATH=/heron', '--bind', 'HERON_SANDBOX_JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64', 'aurora/root/devel/ExclamationTopology', '/root/.heron/conf/aurora/heron.aurora', '--verbose'])\nDEBUG] Config: ['\"\"\"\\n', 'Launch the topology as a single aurora job with multiple instances.\\n', 'The heron-executor is responsible for starting a tmaster (container 0)\\n', 'and regular stmgr/metricsmgr/instances (container index > 0).\\n', '\"\"\"\\n', '\\n', \"heron_core_release_uri = '{{CORE_PACKAGE_URI}}'\\n\", \"heron_topology_jar_uri = '{{TOPOLOGY_PACKAGE_URI}}'\\n\", 'core_release_file = \"heron-core.tar.gz\"\\n', 'topology_package_file = \"topology.tar.gz\"\\n', '\\n', '# --- processes ---\\n', 'fetch_heron_system = Process(\\n', \"  name = 'fetch_heron_system',\\n\", \"  cmdline = 'curl %s -o %s && tar zxf %s' % (heron_core_release_uri, core_release_file, core_release_file)\\n\", ')\\n', '\\n', 'fetch_user_package = Process(\\n', \"  name = 'fetch_user_package',\\n\", \"  cmdline = 'curl %s -o %s && tar zxf %s' % (heron_topology_jar_uri, topology_package_file, topology_package_file)\\n\", ')\\n', '\\n', 'command_to_start_executor = \\'{{SANDBOX_EXECUTOR_BINARY}} {{mesos.instance}} {{TOPOLOGY_NAME}} {{TOPOLOGY_ID}} {{TOPOLOGY_DEFINITION_FILE}} {{INSTANCE_DISTRIBUTION}} {{STATEMGR_CONNECTION_STRING}} {{STATEMGR_ROOT_PATH}} {{SANDBOX_TMASTER_BINARY}} {{SANDBOX_STMGR_BINARY}} \"{{SANDBOX_METRICSMGR_CLASSPATH}}\" {{INSTANCE_JVM_OPTS_IN_BASE64}} \"{{TOPOLOGY_CLASSPATH}}\" {{thermos.ports[port1]}} {{thermos.ports[port2]}} {{thermos.ports[port3]}} {{SANDBOX_SYSTEM_YAML}} {{COMPONENT_RAMMAP}} {{COMPONENT_JVM_OPTS_IN_BASE64}} {{TOPOLOGY_PACKAGE_TYPE}} {{TOPOLOGY_JAR_FILE}} {{HERON_SANDBOX_JAVA_HOME}} {{thermos.ports[http]}} {{SANDBOX_SHELL_BINARY}} {{thermos.ports[port4]}} {{CLUSTER}} {{ROLE}} {{ENVIRON}} \"{{SANDBOX_INSTANCE_CLASSPATH}}\" {{SANDBOX_METRICS_YAML}} \"{{SANDBOX_SCHEDULER_CLASSPATH}}\" \"{{thermos.ports[scheduler]}}\"\\'\\n', '\\n', 'launch_heron_executor = Process(\\n', \"  name = 'launch_heron_executor',\\n\", '  cmdline = command_to_start_executor,\\n', '  max_failures = 1\\n', ')\\n', '\\n', 'discover_profiler_port = Process(\\n', \"  name = 'discover_profiler_port',\\n\", \"  cmdline = 'echo {{thermos.ports[yourkit]}} > yourkit.port'\\n\", ')\\n', '\\n', '# --- tasks ---\\n', 'heron_task = SequentialTask(\\n', \"  name = 'setup_and_run',\\n\", '  processes = [fetch_heron_system, fetch_user_package, launch_heron_executor, discover_profiler_port],\\n', \"  resources = Resources(cpu = '{{CPUS_PER_CONTAINER}}', ram = '{{RAM_PER_CONTAINER}}', disk = '{{DISK_PER_CONTAINER}}')\\n\", ')\\n', '\\n', '# -- jobs ---\\n', 'jobs = [\\n', '  Job(\\n', \"    name = '{{TOPOLOGY_NAME}}',\\n\", \"    cluster = '{{CLUSTER}}',\\n\", \"    role = '{{ROLE}}',\\n\", \"    environment = '{{ENVIRON}}',\\n\", '    service = True,\\n', '    task = heron_task,\\n', \"    instances = '{{NUM_CONTAINERS}}',\\n\", \"    announce = Announcer(primary_port = 'http')\\n\", '  )\\n', ']\\n']\nUnknown cluster: aurora\n[2016-07-02 16:27:05 +0430] com.twitter.heron.spi.utils.SchedulerUtils SEVERE:  Failed to invoke IScheduler as library\n[2016-07-02 16:27:05 +0430] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x255aabf8eaa0028, packet:: clientPath:null serverPath:null finished:false header:: 19,2  replyHeader:: 19,4294967667,0  request:: '/heron/executionstate/ExclamationTopology,-1  response:: null\n[2016-07-02 16:27:05 +0430] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: DeleteBuilderImpl-Foreground - 9 ms\n[2016-07-02 16:27:05 +0430] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Deleted node for path: /heron/executionstate/ExclamationTopology\n[2016-07-02 16:27:05 +0430] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x255aabf8eaa0028, packet:: clientPath:null serverPath:null finished:false header:: 20,2  replyHeader:: 20,4294967668,0  request:: '/heron/topologies/ExclamationTopology,-1  response:: null\n[2016-07-02 16:27:05 +0430] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: DeleteBuilderImpl-Foreground - 7 ms\n[2016-07-02 16:27:05 +0430] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Deleted node for path: /heron/topologies/ExclamationTopology\n[2016-07-02 16:27:05 +0430] com.twitter.heron.scheduler.LaunchRunner SEVERE:  Failed to launch topology\n[2016-07-02 16:27:05 +0430] com.twitter.heron.scheduler.SubmitterMain SEVERE:  Failed to launch topology. Attempting to roll back upload.\n[2016-07-02 16:27:05 +0430] com.twitter.heron.uploader.localfs.LocalFileSystemUploader INFO:  Clean uploaded jar\n[2016-07-02 16:27:05 +0430] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Closing the CuratorClient to: 192.168.11.231:2181,192.168.11.232:2181,192.168.11.233:2181\n[2016-07-02 16:27:05 +0430] org.apache.curator.framework.imps.CuratorFrameworkImpl FINE:  Closing\n[2016-07-02 16:27:05 +0430] org.apache.curator.CuratorZookeeperClient FINE:  Closing\n[2016-07-02 16:27:05 +0430] org.apache.curator.ConnectionState FINE:  Closing\n[2016-07-02 16:27:05 +0430] org.apache.zookeeper.ZooKeeper FINE:  Closing session: 0x255aabf8eaa0028\n[2016-07-02 16:27:05 +0430] org.apache.zookeeper.ClientCnxn FINE:  Closing client for session: 0x255aabf8eaa0028\n[2016-07-02 16:27:05 +0430] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x255aabf8eaa0028, packet:: clientPath:null serverPath:null finished:false header:: 21,-11  replyHeader:: 21,4294967669,0  request:: null response:: null\n[2016-07-02 16:27:05 +0430] org.apache.zookeeper.ClientCnxn FINE:  Disconnecting client for session: 0x255aabf8eaa0028\n[2016-07-02 16:27:05 +0430] org.apache.zookeeper.ClientCnxn INFO:  EventThread shut down\n[2016-07-02 16:27:05 +0430] org.apache.zookeeper.ZooKeeper INFO:  Session: 0x255aabf8eaa0028 closed\n[2016-07-02 16:27:05 +0430] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Closing the tunnel processes\nException in thread \"main\" java.lang.RuntimeException: Failed to submit topology ExclamationTopology\n    at com.twitter.heron.scheduler.SubmitterMain.main(SubmitterMain.java:319)\nERROR: Failed to launch topology 'ExclamationTopology' because User main failed with status 1. Bailing out...\nINFO: Elapsed time: 3.951s.\n```\nI changed the env role and ..., but issue didn't solved.\n. @kartik894 Thanks a lot! It resolved.\n. I found the problem!\nFirst need to add hadoop in $PATH.\nexport PATH=$PATH:$HADOOP_HOME/bin\nThen correct HDFS uploader configuration file with a well-formed URL such as follow: \nheron.uploader.hdfs.topologies.directory.uri: hdfs://192.168.11.xx:9000/heron/topologies/${CLUSTER}/${TOPOLOGY}\nor\nheron.uploader.hdfs.topologies.directory.uri: hdfs:///heron/topologies/${CLUSTER}/${TOPOLOGY} with 3 slashes!\nBut i get a new error:\ncurl: (1) Protocol hdfs not supported or disabled in libcurl\n. @caofangkun Thanks a lot! I've configured HDFS and can run heron to use HDFS.\nbut i use an static command in aurora file and need to change the hdfs:/// URI to http:///webhdfs/v1/ URI!\nHow can i do it?\n@kramasamy Thanks. I changed the heron.aurora file before like follow:\nfetch_user_package = Process(\n  name = 'fetch_user_package',\n  cmdline = '/opt/hadoop-2.7.0/bin/hadoop fs -get  %s  ./%s  && tar zxf %s' % (heron_topology_jar_uri, topology_package_file, topology_package_file)\n)\nbut it need to install hadoop client on all worker machines that is not good. so i need to use curl command. \n. @maosongfu What is the proper configuration! It can not be heron.uploader.hdfs.topologies.directory.uri.\nHow i tell uploader to return my desired URI?\nThe main reason that i'm going to use HDFS is to use heron on real distributed mode and to upload my program (jar file) in a single location not on all machines. Is HDFS the solution? If true where i find the proper configuration?\n. @maosongfu  your link about guarantees message processing work just on Storm not heron!!!\n. ",
    "harbby": "thanks! I deployed in ubuntu 16.04 and centos7 successful, but centos6 submit a job error occurred.\n. @kartik894 Thank you!\n. ",
    "chatterjeesubarna": "Hello, I am new to Heron. I am submitting a topology as root.\nInitially, I did \"heron submit aurora/ubuntu/devel --config-path ~/.heron/conf/ ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology --verbose\"\nThen I got the error \"Failed to initialize sandbox: Could not create sandbox because user does not exist: ubuntu\" \nSo I modified and did this:\nheron submit aurora/root/devel --config-path ~/.heron/conf/ ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology --verbose\"\nI am getting this error:\nE0329 14:34:14.479283 32970 runner.py:299] Regular plan unhealthy!\nCan someone help? Thanks a lot!. Hello,\nThank you a lot. Yes, it was solved and I could run the job. Probably it was some topology running already and so mesos couldn't schedule another one!\nNow, I can see the following on my terminal:\n\"INFO: Topology 'ExclamationTopology' launched successfully\"\nMy heron ui shows: \n\"{\"status\": \"success\", \"executiontime\": 5.316734313964844e-05, \"message\": \"\", \"version\": \"0.14.5\", \"result\": {\"aurora\": {\"root\": {\"devel\": [\"ExclamationTopology\"]}}}}\"\nJust that, I cannot see the topology on heron tracker. My heron-tracker.yaml looks like this:\nstatemgrs:\n\u00a0 -\n\u00a0\u00a0\u00a0 type: \"zookeeper\"\n\u00a0\u00a0\u00a0 name: \"localzk\"\n\u00a0\u00a0\u00a0 hostport: \"heron01:2181\"\n\u00a0\u00a0\u00a0 rootpath: \"/heron\"\n\u00a0\u00a0\u00a0 tunnelhost: \"localhost\"\nCan you kindly help? Thanks a lot again!\u00a0Thanking you,\nSubarna Chatterjee\nPost-Doctoral ResearcherInria, Rennes\nWebsite:\u00a0http://chatterjeesubarna.wix.com/subarna \nOn Wednesday, 29 March 2017, 19:33, bed debug <notifications@github.com> wrote:\n\n@chatterjeesubarna i guess, your first submit created some metadata in zookeeper, which your second submit conflited with. i suggest try to submit with a different name\nheron submit aurora/root/devel --config-path ~/.heron/conf/ ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopologyDifferent1 --verbose\n@maosongfu to confirm\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.  \n. ",
    "bjmota": "Hello! I have a problem developing a Heron Cluster, when I submit the ExclamationTopoly......\n`\nb1@master_1:~$ heron submit aurora/b1/devel  --config-path ~/.heron/conf/ ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology\n[2017-05-25 09:18:08 +0000] [INFO]: Using config file under /home/b1/.heron/conf/aurora\n[2017-05-25 09:18:08 +0000] [INFO]: Launching topology: 'ExclamationTopology'\n[2017-05-25 09:18:09 -0700] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Starting Curator client connecting to: 192.168.57.163:2181  \n[2017-05-25 09:18:09 -0700] [INFO] org.apache.curator.framework.imps.CuratorFrameworkImpl: Starting  \n[2017-05-25 09:18:09 -0700] [INFO] org.apache.curator.framework.state.ConnectionStateManager: State change: CONNECTED  \n[2017-05-25 09:18:09 -0700] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Directory tree initialized.  \n[2017-05-25 09:18:09 -0700] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Checking existence of path: /heron/topologies/ExclamationTopology  \n[2017-05-25 09:18:09 -0700] [INFO] com.twitter.heron.spi.utils.ShellUtils: Running synced process:hadoop --config /usr/lib/hadoop-2.8.0/etc/hadoop fs -test -e /heron/topologies/aurora''\n[2017-05-25 09:18:09 -0700] [INFO] com.twitter.heron.spi.utils.ShellUtils: Process output (stdout+stderr):\n[2017-05-25 09:18:13 -0700] [INFO] com.twitter.heron.uploader.hdfs.HdfsUploader: Target topology file already exists at '/heron/topologies/aurora/ExclamationTopology-b1-tag-0--7108568726115264257.tar.gz'. Overwriting it now\n[2017-05-25 09:18:13 -0700] [INFO] com.twitter.heron.uploader.hdfs.HdfsUploader: Uploading topology package at '/tmp/tmpvMxs3m/topology.tar.gz' to target HDFS at '/heron/topologies/aurora/ExclamationTopology-b1-tag-0--7108568726115264257.tar.gz'\n[2017-05-25 09:18:13 -0700] [INFO] com.twitter.heron.spi.utils.ShellUtils: Running synced process: hadoop --config /usr/lib/hadoop-2.8.0/etc/hadoop fs -copyFromLocal -f /tmp/tmpvMxs3m/topology.tar.gz /heron/topologies/aurora/ExclamationTopology-b1-tag-0--7108568726115264257.tar.gz''  \n[2017-05-25 09:18:13 -0700] [INFO] com.twitter.heron.spi.utils.ShellUtils: Process output (stdout+stderr):  \n[2017-05-25 09:18:17 -0700] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Created node for path: /heron/topologies/ExclamationTopology  \n[2017-05-25 09:18:17 -0700] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Created node for path: /heron/packingplans/ExclamationTopology  \n[2017-05-25 09:18:18 -0700] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Created node for path: /heron/executionstate/ExclamationTopology  \n[2017-05-25 09:18:18 -0700] [INFO] com.twitter.heron.scheduler.aurora.AuroraLauncher: Launching topology in aurora  \n[2017-05-25 09:18:18 -0700] [INFO] com.twitter.heron.scheduler.utils.SchedulerUtils: Updating scheduled-resource in packing plan: ExclamationTopology  \n[2017-05-25 09:18:18 -0700] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Deleted node for path: /heron/packingplans/ExclamationTopology  \n[2017-05-25 09:18:18 -0700] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Created node for path: /heron/packingplans/ExclamationTopology  \n[2017-05-25 09:18:18 -0700] [INFO] com.twitter.heron.spi.utils.ShellUtils: Running synced process:aurora job create --wait-until RUNNING --bind STMGR_BINARY=./heron-core/bin/heron-stmgr --bind RAM_PER_CONTAINER=11811160064 --bind TOPOLOGY_PACKAGE_TYPE=jar --bind SHELL_BINARY=./heron-core/bin/heron-shell --bind TMASTER_BINARY=./heron-core/bin/heron-tmaster --bind STATEMGR_ROOT_PATH=/heron --bind TOPOLOGY_PACKAGE_URI=/heron/topologies/aurora/ExclamationTopology-b1-tag-0--7108568726115264257.tar.gz --bind JAVA_HOME=/usr/lib/jvm/java-8-oracle --bind CLUSTER=aurora --bind TOPOLOGY_BINARY_FILE=heron-examples.jar --bind SYSTEM_YAML=./heron-conf/heron_internals.yaml --bind EXECUTOR_BINARY=./heron-core/bin/heron-executor --bind CPUS_PER_CONTAINER=5.0 --bind IS_PRODUCTION=false --bind PYTHON_INSTANCE_BINARY=./heron-core/bin/heron-python-instance --bind METRICS_YAML=./heron-conf/metrics_sinks.yaml --bind CORE_PACKAGE_URI=/heron/dist/heron-core.tar.gz --bind TOPOLOGY_CLASSPATH=heron-examples.jar --bind TOPOLOGY_ID=ExclamationTopologyf7fa5898-9d12-4e3d-917c-e24be0fd9ef6 --bind ROLE=b1 --bind COMPONENT_JVM_OPTS_IN_BASE64=\"\" --bind TOPOLOGY_NAME=ExclamationTopology --bind STATEMGR_CONNECTION_STRING=192.168.57.163:2181 --bind INSTANCE_CLASSPATH=./heron-core/lib/instance/ --bind DISK_PER_CONTAINER=5368709120 --bind COMPONENT_RAMMAP=exclaim1:3221225472,word:3221225472 --bind METRICSMGR_CLASSPATH=./heron-core/lib/metricsmgr/ --bind ENVIRON=devel --bind SCHEDULER_CLASSPATH=./heron-core/lib/scheduler/:./heron-core/lib/packing/:./heron-core/lib/statemgr/ --bind TOPOLOGY_DEFINITION_FILE=ExclamationTopology.defn --bind NUM_CONTAINERS=3 --bind INSTANCE_JVM_OPTS_IN_BASE64=\"LVhYOitIZWFwRHVtcE9uT3V0T2ZNZW1vcnlFcnJvcg&equals&equals\" aurora/b1/devel/ExclamationTopology /home/b1/.heron/conf/aurora/heron.aurora''\n[2017-05-25 09:18:18 -0700] [INFO] com.twitter.heron.spi.utils.ShellUtils: Process output (stdout+stderr):\nError loading configuration: Unknown cluster: aurora\n[2017-05-25 09:18:20 -0700] [SEVERE] com.twitter.heron.scheduler.aurora.AuroraCLIController: Failed to run process. Command=[aurora, job, create, --wait-until, RUNNING, --bind, STMGR_BINARY=./heron-core/bin/heron-stmgr, --bind, RAM_PER_CONTAINER=11811160064, --bind, TOPOLOGY_PACKAGE_TYPE=jar, --bind, SHELL_BINARY=./heron-core/bin/heron-shell, --bind, TMASTER_BINARY=./heron-core/bin/heron-tmaster, --bind, STATEMGR_ROOT_PATH=/heron, --bind, TOPOLOGY_PACKAGE_URI=/heron/topologies/aurora/ExclamationTopology-b1-tag-0--7108568726115264257.tar.gz, --bind, JAVA_HOME=/usr/lib/jvm/java-8-oracle, --bind, CLUSTER=aurora, --bind, TOPOLOGY_BINARY_FILE=heron-examples.jar, --bind, SYSTEM_YAML=./heron-conf/heron_internals.yaml, --bind, EXECUTOR_BINARY=./heron-core/bin/heron-executor, --bind, CPUS_PER_CONTAINER=5.0, --bind, IS_PRODUCTION=false, --bind, PYTHON_INSTANCE_BINARY=./heron-core/bin/heron-python-instance, --bind, METRICS_YAML=./heron-conf/metrics_sinks.yaml, --bind, CORE_PACKAGE_URI=/heron/dist/heron-core.tar.gz, --bind, TOPOLOGY_CLASSPATH=heron-examples.jar, --bind, TOPOLOGY_ID=ExclamationTopologyf7fa5898-9d12-4e3d-917c-e24be0fd9ef6, --bind, ROLE=b1, --bind, COMPONENT_JVM_OPTS_IN_BASE64=\"\", --bind, TOPOLOGY_NAME=ExclamationTopology, --bind, STATEMGR_CONNECTION_STRING=192.168.57.163:2181, --bind, INSTANCE_CLASSPATH=./heron-core/lib/instance/, --bind, DISK_PER_CONTAINER=5368709120, --bind, COMPONENT_RAMMAP=exclaim1:3221225472,word:3221225472, --bind, METRICSMGR_CLASSPATH=./heron-core/lib/metricsmgr/, --bind, ENVIRON=devel, --bind, SCHEDULER_CLASSPATH=./heron-core/lib/scheduler/:./heron-core/lib/packing/:./heron-core/lib/statemgr/, --bind, TOPOLOGY_DEFINITION_FILE=ExclamationTopology.defn, --bind, NUM_CONTAINERS=3, --bind, INSTANCE_JVM_OPTS_IN_BASE64=\"LVhYOitIZWFwRHVtcE9uT3V0T2ZNZW1vcnlFcnJvcg&equals&equals\", aurora/b1/devel/ExclamationTopology, /home/b1/.heron/conf/aurora/heron.aurora], STDOUT=, STDERR=Error loading configuration: Unknown cluster: aurora\n[2017-05-25 09:18:20 -0700] [SEVERE] com.twitter.heron.scheduler.utils.LauncherUtils: Failed to invoke IScheduler as library\n[2017-05-25 09:18:20 -0700] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Deleted node for path: /heron/executionstate/ExclamationTopology\n[2017-05-25 09:18:20 -0700] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Deleted node for path: /heron/packingplans/ExclamationTopology\n[2017-05-25 09:18:20 -0700] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Deleted node for path: /heron/topologies/ExclamationTopology\n[2017-05-25 09:18:20 -0700] [INFO] com.twitter.heron.spi.utils.ShellUtils: Running synced process: ``hadoop --config /usr/lib/hadoop-2.8.0/etc/hadoop fs -rm /heron/topologies/aurora/ExclamationTopology-b1-tag-0--7108568726115264257.tar.gz''\n[2017-05-25 09:18:20 -0700] [INFO] com.twitter.heron.spi.utils.ShellUtils: Process output (stdout+stderr):\nDeleted /heron/topologies/aurora/ExclamationTopology-b1-tag-0--7108568726115264257.tar.gz\n[2017-05-25 09:18:23 -0700] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Closing the CuratorClient to: 192.168.57.163:2181\n[2017-05-25 09:18:23 -0700] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Closing the tunnel processes\n[2017-05-25 09:18:23 +0000] [ERROR]: Failed to launch topology 'ExclamationTopology'\n[2017-05-25 09:18:23 +0000] [ERROR]: Failed to launch topology 'ExclamationTopology'\n[2017-05-25 09:18:23 +0000] [INFO]: Elapsed time: 15.872s.\nAlso in Aurora I only see the Example in thehttp://192.168.57.163:8081/scheduler....\n```\nHelp me, Please!. ",
    "javi1104": "Note that Tracker does not create any zookeeper nodes itself. It is a readonly service. If you launch the Tracker without creating /topologies node under the rootpath, Tracker will fail with a NoNodeError.\nhttp://twitter.github.io/heron/docs/operators/heron-tracker/\n. ",
    "kiril-me": "Added meaningful log message.\nhttps://github.com/twitter/heron/pull/1141\n. @kramasamy - Signed.\n. ",
    "cranst0n": "@billonahill Signed!\n. ",
    "vivilife": "I build the docker image using docker/Dockerfile.centos7 without the last line. Then I run the container, the same problem still exist. But after I copy the configuration params for centos from tools/bazel.rc to ~/.bazelrc and build with command \" bazel build  heron/... \", build succedd.\nI think there is some problem with my bazel,  so I bypass it with some trick\n. ",
    "jrpspam": "my topo gets in this state within matter of hours and is emitting only 10 character strings. I seem to permanently lose connection to stream manager until restarting the topo. Any ideas would be great. I'm trying to evaluate Heron and do not see any reason for the failures\nSEVERE] com.twitter.heron.common.network.IncomingPacket: channel.read returned negative -1\n[2017-06-12 21:14:03 +0000] [SEVERE] com.twitter.heron.common.network.SocketChannelHelper: Something bad happened while reading from channel: /127.0.0.1:60947\n[2017-06-12 21:14:03 +0000] [INFO] com.twitter.heron.common.network.HeronClient: Handling Error. Cleaning states in HeronClient.\n[2017-06-12 21:14:03 +0000] [INFO] com.twitter.heron.common.network.HeronClient: Successfully closed the channel: java.nio.channels.SocketChannel[closed]\n[2017-06-12 21:14:03 +0000] [SEVERE] com.twitter.heron.network.StreamManagerClient: Disconnected from Stream Manager.\n[2017-06-12 21:14:03 +0000] [INFO] com.twitter.heron.network.StreamManagerClient: Clean the old PhysicalPlanHelper in StreamManagerClient.\n[2017-06-12 21:14:03 +0000] [WARNING] com.twitter.heron.network.StreamManagerClient: Error connecting to Stream Manager with status: CONNECT_ERROR, Retrying... \n. this is a Ubuntu m4.xlarge EC2 AWS instance. I think I am having trouble \nunderstanding how to manage/config mem usage though not 100% clear where \nto look\nOn 6/13/2017 11:08 AM, Sanjeev Kulkarni wrote:\n\nIs this Mac or Linux?\nOn Tue, Jun 13, 2017 at 6:07 AM, jrpspam notifications@github.com wrote:\n\nmy topo gets in this state within matter of hours and is emitting \nonly 10\ncharacter strings. I seem to permanently lose connection to stream \nmanager\nuntil restarting the topo. Any ideas would be great. I'm trying to \nevaluate\nHeron and do not see any reason for the failures\nSEVERE] com.twitter.heron.common.network.IncomingPacket: channel.read\nreturned negative -1\n[2017-06-12 21:14:03 +0000] [SEVERE] \ncom.twitter.heron.common.network.SocketChannelHelper:\nSomething bad happened while reading from channel: /127.0.0.1:60947\n[2017-06-12 21:14:03 +0000] [INFO] \ncom.twitter.heron.common.network.HeronClient:\nHandling Error. Cleaning states in HeronClient.\n[2017-06-12 21:14:03 +0000] [INFO] \ncom.twitter.heron.common.network.HeronClient:\nSuccessfully closed the channel: java.nio.channels.SocketChannel[closed]\n[2017-06-12 21:14:03 +0000] [SEVERE] \ncom.twitter.heron.network.StreamManagerClient:\nDisconnected from Stream Manager.\n[2017-06-12 21:14:03 +0000] [INFO] \ncom.twitter.heron.network.StreamManagerClient:\nClean the old PhysicalPlanHelper in StreamManagerClient.\n[2017-06-12 21:14:03 +0000] [WARNING] \ncom.twitter.heron.network.StreamManagerClient:\nError connecting to Stream Manager with status: CONNECT_ERROR, \nRetrying...\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/932#issuecomment-308109604, \nor mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACabuuvYqvWd19JgGIgSDVIb6S7p_Vgaks5sDomSgaJpZM4I3inv\n.\n\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub \nhttps://github.com/twitter/heron/issues/932#issuecomment-308147514, \nor mute the thread \nhttps://github.com/notifications/unsubscribe-auth/AbpRuyGtpGkZWXHDFe9Sq0pus_BweELGks5sDqXggaJpZM4I3inv.\n\n\n. whatever default scheduler there is. This is first real topo so likely a \nsimple not complex issue. From stmgr logs I see these logs:\nheron-localfilestatemgr.cpp:324] Error reading from \n/home/ubuntu/.herondata/repository/state/local/tmasters/CnamTopology \nwith errno\n2\nI0613 19:49:35.300354  9542 stmgr.cpp:202] Creating StmgrServer\nI0613 19:49:35.311589  9542 stmgr.cpp:236] Creating tuple cache\nE0613 19:49:35.311666  9542 metricsmgr-client.cpp:53] Could not connect \nto metrics mgr. Will Retry in 1 second\nI0613 19:49:35.311677  9542 stmgr.cpp:286] TMaster Location Fetch failed \nwith status 2000\nE0613 19:49:36.309404  9542 fileutils.cpp:139] Unable to get file \nmodified time for \n/home/ubuntu/.herondata/repository/state/local/tmasters/CnamTopology: No suc\nh file or directory [2]\nF0613 19:49:38.311533  9542 stmgr.cpp:296] Topology name/id mismatch \nbetween stmgr and TMaster We expected CnamTopology : \nCnamTopologycf1671fe-bee7-4953-bc5f-e8\nfb1fa188bd but tmaster had CnamTopology : \nCnamTopology82bd9dce-0ac5-48b6-91d3-92602c4a6256\nAlso I can say I got into state where I could not deactivate nor kill \nthe topo so manually deleted topo and repo folders under ~/.herondata \nand rebooted the box -- but something is clearly mis-configured\nOn 6/13/2017 5:11 PM, Sanjeev Kulkarni wrote:\n\nWhat scheduler are you using? Does it happen to all topologies or just \nsome\nselected ones?\nOn Tue, Jun 13, 2017 at 12:42 PM, jrpspam notifications@github.com \nwrote:\n\nthis is a Ubuntu m4.xlarge EC2 AWS instance. I think I am having trouble\nunderstanding how to manage/config mem usage though not 100% clear where\nto look\nOn 6/13/2017 11:08 AM, Sanjeev Kulkarni wrote:\n\nIs this Mac or Linux?\nOn Tue, Jun 13, 2017 at 6:07 AM, jrpspam notifications@github.com\nwrote:\n\nmy topo gets in this state within matter of hours and is emitting\nonly 10\ncharacter strings. I seem to permanently lose connection to stream\nmanager\nuntil restarting the topo. Any ideas would be great. I'm trying to\nevaluate\nHeron and do not see any reason for the failures\nSEVERE] com.twitter.heron.common.network.IncomingPacket: \nchannel.read\nreturned negative -1\n[2017-06-12 21:14:03 +0000] [SEVERE]\ncom.twitter.heron.common.network.SocketChannelHelper:\nSomething bad happened while reading from channel: /127.0.0.1:60947\n[2017-06-12 21:14:03 +0000] [INFO]\ncom.twitter.heron.common.network.HeronClient:\nHandling Error. Cleaning states in HeronClient.\n[2017-06-12 21:14:03 +0000] [INFO]\ncom.twitter.heron.common.network.HeronClient:\nSuccessfully closed the channel: java.nio.channels.\nSocketChannel[closed]\n[2017-06-12 21:14:03 +0000] [SEVERE]\ncom.twitter.heron.network.StreamManagerClient:\nDisconnected from Stream Manager.\n[2017-06-12 21:14:03 +0000] [INFO]\ncom.twitter.heron.network.StreamManagerClient:\nClean the old PhysicalPlanHelper in StreamManagerClient.\n[2017-06-12 21:14:03 +0000] [WARNING]\ncom.twitter.heron.network.StreamManagerClient:\nError connecting to Stream Manager with status: CONNECT_ERROR,\nRetrying...\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/932#issuecomment-308109604,\nor mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/\nACabuuvYqvWd19JgGIgSDVIb6S7p_Vgaks5sDomSgaJpZM4I3inv\n.\n\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/932#issuecomment-308147514,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/\nAbpRuyGtpGkZWXHDFe9Sq0pus_BweELGks5sDqXggaJpZM4I3inv.\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/932#issuecomment-308226440, \nor mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACabuiuQcGRTDT9ZzEzmywXPkQfaBHJGks5sDuYrgaJpZM4I3inv\n.\n\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub \nhttps://github.com/twitter/heron/issues/932#issuecomment-308249693, \nor mute the thread \nhttps://github.com/notifications/unsubscribe-auth/AbpRu-bbV1ISQ7kW1ijjIJUGulCRHA-kks5sDvrsgaJpZM4I3inv.\n\n\n. Thanks for the info. I see much output under ps aux | grep heron \nincluding bin/heron-stmgr running\nstmgr logs with oddities: >>\nheron-localfilestatemgr.cpp:324] Error reading from \n/home/ubuntu/.herondata/repository/state/local/tmasters/CnamTopology \nwith errno 2\nI0613 19:49:35.300354  9542 stmgr.cpp:202] Creating StmgrServer\nI0613 19:49:35.311589  9542 stmgr.cpp:236] Creating tuple cache\nE0613 19:49:35.311666  9542 metricsmgr-client.cpp:53] Could not connect \nto metrics mgr. Will Retry in 1 second\nI0613 19:49:35.311677  9542 stmgr.cpp:286] TMaster Location Fetch failed \nwith status 2000\nE0613 19:49:36.309404  9542 fileutils.cpp:139] Unable to get file \nmodified time for \n/home/ubuntu/.herondata/repository/state/local/tmasters/CnamTopology: No \nsuch file or directory [2]\nF0613 19:49:38.311533  9542 stmgr.cpp:296] Topology name/id mismatch \nbetween stmgr and TMaster We expected CnamTopology : \nCnamTopologycf1671fe-bee7-4953-bc5f-e8\nfb1fa188bd but tmaster had CnamTopology : \nCnamTopology82bd9dce-0ac5-48b6-91d3-92602c4a6256\nAlso I can say I got into state where I could not deactivate nor kill \nthe topo (mem issues I believe) so manually deleted topo and repo \nfolders under ~/.herondata and rebooted the box -- not sure if that was \ncorrect, but something is clearly mis-configured\nOn 6/13/2017 10:44 AM, Andrew Jorgensen wrote:\n\n@jrpspam https://github.com/jrpspam This looks like there might be \nan issue with the connection between the stream manager and the \ncomponent instance. When this happens can you look at the process list \n(|ps aux | grep heron|) and confirm that the stmgr process is still \nrunning? You can also run |netstat -tulpn | grep | to \nmake sure there is still a process listening on that port you see in \nthe logs.\nYou can also look at the stmgr logs directly (should be in the same \nfolder) to make sure there wasnt an exception or something.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub \nhttps://github.com/twitter/heron/issues/932#issuecomment-308139605, \nor mute the thread \nhttps://github.com/notifications/unsubscribe-auth/AbpRu7fwC1UuIVG5ktzh38Opdv_FMXNOks5sDqBDgaJpZM4I3inv.\n\n\n. Thanks for the info Andrew.  I see much output under ps aux | grep heron \nincluding bin/heron-stmgr running\nstmgr logs with oddities: >>\nheron-localfilestatemgr.cpp:324] Error reading from \n/home/ubuntu/.herondata/repository/state/local/tmasters/CnamTopology \nwith errno 2\nI0613 19:49:35.300354  9542 stmgr.cpp:202] Creating StmgrServer\nI0613 19:49:35.311589  9542 stmgr.cpp:236] Creating tuple cache\nE0613 19:49:35.311666  9542 metricsmgr-client.cpp:53] Could not connect \nto metrics mgr. Will Retry in 1 second\nI0613 19:49:35.311677  9542 stmgr.cpp:286] TMaster Location Fetch failed \nwith status 2000\nE0613 19:49:36.309404  9542 fileutils.cpp:139] Unable to get file \nmodified time for \n/home/ubuntu/.herondata/repository/state/local/tmasters/CnamTopology: No \nsuch file or directory [2]\nF0613 19:49:38.311533  9542 stmgr.cpp:296] Topology name/id mismatch \nbetween stmgr and TMaster We expected CnamTopology : \nCnamTopologycf1671fe-bee7-4953-bc5f-e8\nfb1fa188bd but tmaster had CnamTopology : \nCnamTopology82bd9dce-0ac5-48b6-91d3-92602c4a6256\nAlso I can say I got into state where I could not deactivate nor kill \nthe topo (mem issues I believe) so manually deleted topo and repo \nfolders under ~/.herondata and rebooted the box -- not sure if that was \ncorrect, but something is clearly mis-configured\nOn 6/13/2017 10:44 AM, Andrew Jorgensen wrote:\n\n@jrpspam https://github.com/jrpspam This looks like there might be \nan issue with the connection between the stream manager and the \ncomponent instance. When this happens can you look at the process list \n(|ps aux | grep heron|) and confirm that the stmgr process is still \nrunning? You can also run |netstat -tulpn | grep | to \nmake sure there is still a process listening on that port you see in \nthe logs.\nYou can also look at the stmgr logs directly (should be in the same \nfolder) to make sure there wasnt an exception or something.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub \nhttps://github.com/twitter/heron/issues/932#issuecomment-308139605, \nor mute the thread \nhttps://github.com/notifications/unsubscribe-auth/AbpRu7fwC1UuIVG5ktzh38Opdv_FMXNOks5sDqBDgaJpZM4I3inv.\n\n\n. whatever default scheduler there is. This is first real topo so likely a \nsimple not complex issue. From stmgr logs I see these logs:\nheron-localfilestatemgr.cpp:324] Error reading from \n/home/ubuntu/.herondata/repository/state/local/tmasters/CnamTopology \nwith errno\n2\nI0613 19:49:35.300354  9542 stmgr.cpp:202] Creating StmgrServer\nI0613 19:49:35.311589  9542 stmgr.cpp:236] Creating tuple cache\nE0613 19:49:35.311666  9542 metricsmgr-client.cpp:53] Could not connect \nto metrics mgr. Will Retry in 1 second\nI0613 19:49:35.311677  9542 stmgr.cpp:286] TMaster Location Fetch failed \nwith status 2000\nE0613 19:49:36.309404  9542 fileutils.cpp:139] Unable to get file \nmodified time for \n/home/ubuntu/.herondata/repository/state/local/tmasters/CnamTopology: No suc\nh file or directory [2]\nF0613 19:49:38.311533  9542 stmgr.cpp:296] Topology name/id mismatch \nbetween stmgr and TMaster We expected CnamTopology : \nCnamTopologycf1671fe-bee7-4953-bc5f-e8\nfb1fa188bd but tmaster had CnamTopology : \nCnamTopology82bd9dce-0ac5-48b6-91d3-92602c4a6256\nAlso I can say I got into state where I could not deactivate nor kill \nthe topo so manually deleted topo and repo folders under ~/.herondata \nand rebooted the box -- but something is clearly mis-configured\nOn 6/13/2017 5:11 PM, Sanjeev Kulkarni wrote:\n\nWhat scheduler are you using? Does it happen to all topologies or just \nsome\nselected ones?\nOn Tue, Jun 13, 2017 at 12:42 PM, jrpspam notifications@github.com \nwrote:\n\nthis is a Ubuntu m4.xlarge EC2 AWS instance. I think I am having trouble\nunderstanding how to manage/config mem usage though not 100% clear where\nto look\nOn 6/13/2017 11:08 AM, Sanjeev Kulkarni wrote:\n\nIs this Mac or Linux?\nOn Tue, Jun 13, 2017 at 6:07 AM, jrpspam notifications@github.com\nwrote:\n\nmy topo gets in this state within matter of hours and is emitting\nonly 10\ncharacter strings. I seem to permanently lose connection to stream\nmanager\nuntil restarting the topo. Any ideas would be great. I'm trying to\nevaluate\nHeron and do not see any reason for the failures\nSEVERE] com.twitter.heron.common.network.IncomingPacket: \nchannel.read\nreturned negative -1\n[2017-06-12 21:14:03 +0000] [SEVERE]\ncom.twitter.heron.common.network.SocketChannelHelper:\nSomething bad happened while reading from channel: /127.0.0.1:60947\n[2017-06-12 21:14:03 +0000] [INFO]\ncom.twitter.heron.common.network.HeronClient:\nHandling Error. Cleaning states in HeronClient.\n[2017-06-12 21:14:03 +0000] [INFO]\ncom.twitter.heron.common.network.HeronClient:\nSuccessfully closed the channel: java.nio.channels.\nSocketChannel[closed]\n[2017-06-12 21:14:03 +0000] [SEVERE]\ncom.twitter.heron.network.StreamManagerClient:\nDisconnected from Stream Manager.\n[2017-06-12 21:14:03 +0000] [INFO]\ncom.twitter.heron.network.StreamManagerClient:\nClean the old PhysicalPlanHelper in StreamManagerClient.\n[2017-06-12 21:14:03 +0000] [WARNING]\ncom.twitter.heron.network.StreamManagerClient:\nError connecting to Stream Manager with status: CONNECT_ERROR,\nRetrying...\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/932#issuecomment-308109604,\nor mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/\nACabuuvYqvWd19JgGIgSDVIb6S7p_Vgaks5sDomSgaJpZM4I3inv\n.\n\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/932#issuecomment-308147514,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/\nAbpRuyGtpGkZWXHDFe9Sq0pus_BweELGks5sDqXggaJpZM4I3inv.\n\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/932#issuecomment-308226440, \nor mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/ACabuiuQcGRTDT9ZzEzmywXPkQfaBHJGks5sDuYrgaJpZM4I3inv\n.\n\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub \nhttps://github.com/twitter/heron/issues/932#issuecomment-308249693, \nor mute the thread \nhttps://github.com/notifications/unsubscribe-auth/AbpRu-bbV1ISQ7kW1ijjIJUGulCRHA-kks5sDvrsgaJpZM4I3inv.\n\n\n. Thanks. Will do that\nOn 6/13/2017 7:29 PM, Sanjeev Kulkarni wrote:\n\nI think whats happening is that when you manually tried to clean \nthings up, things weren;t completely cleaned. Suggest doing a kill \nonce again, make sure that no heron processes are \nrunning(tmaster/stmgrs/instance) and then cleaning up ~/.herondata/ \nand restarting\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub \nhttps://github.com/twitter/heron/issues/932#issuecomment-308277314, \nor mute the thread \nhttps://github.com/notifications/unsubscribe-auth/AbpRuxPUX-5XsO7NvvvjOb_P00I2CZfDks5sDxtygaJpZM4I3inv.\n\n\n. Thanks, sorry to be dense but the whole \"plugin\" concept is not clear. If I want to create a simple heron project w/ spout/bolts and I want to be able to use 'heron submit' on the jar produced and I have specified the heron dependency as noted above in my POM, what mvn package/install build options do I need? Where does plugin fit in this picture? Is there not some basic example of how to take a simple heron example -- ExclamationTopology -- and build/package/install it based on the java source and be able to deploy it via 'heron submit'? I do not see what mvn command is relevent in this simple scenario. Thanks very much for any clues.\n. Okay,, found example of how to do it. Building jar w/ dependencies takes a lot of pom.xml to get working. Absolutely nothing obvious about how to do it. Might be very nice if the heron examples came w/ an actual working pom.xml that one could use as a starting point. I had to add entire build section to pom as below:\n\n4.0.0\ncom.neustar\ncnam\njar\n1.0-SNAPSHOT\ncnam\nhttp://maven.apache.org\n\n\njunit\njunit\n3.8.1\ntest\n\n<!-- https://mvnrepository.com/artifact/com.twitter.heron/heron-storm -->\n<dependency>\n      <groupId>com.twitter.heron</groupId>\n      <artifactId>heron-storm</artifactId>\n      <version>0.14.6</version>\n</dependency>\n\n\n<build>\n<finalName>${project.artifactId}</finalName>\n<plugins>\n  <plugin>\n    <artifactId>maven-compiler-plugin</artifactId>\n    <version>3.0</version>\n    <configuration>\n      <source>1.6</source>\n      <target>1.6</target>\n    </configuration>\n  </plugin>\n  <plugin>\n    <groupId>org.codehaus.mojo</groupId>\n    <artifactId>exec-maven-plugin</artifactId>\n    <version>1.2.1</version>\n    <configuration>\n      <mainClass>com.twitter.storm.Topology</mainClass>\n      <classpathScope>compile</classpathScope>\n    </configuration>\n  </plugin>\n  <plugin>\n    <artifactId>maven-assembly-plugin</artifactId>\n    <executions>\n      <execution>\n        <id>make-assembly</id>\n        <phase>package</phase>\n        <goals>\n          <goal>single</goal>\n        </goals>\n      </execution>\n    </executions>\n    <configuration>\n      <descriptorRefs>\n        <descriptorRef>jar-with-dependencies</descriptorRef>\n      </descriptorRefs>\n      <archive>\n        <manifest>\n          <mainClass>biz.neustar.cnamheron.CnamTopology</mainClass>\n        </manifest>\n      </archive>\n    </configuration>\n  </plugin>\n</plugins>\n\n\n\n. ",
    "avflor": "Andrew, I think the enhanced round robin algorithm is a reasonable approach under the assumption that the containers have to be homogeneous, which is a constraint imposed by Aurora. BTW, I'm not able to find the code for the enhanced round robin algorithm. It seems to me that currently the getRoundRobinAllocation() method does not perform any sorting.\nIn this issue, we try to address the packing problem optimizing for the resources used (currently memory) using binpacking algorithms. However, the input of the algorithm would be slightly different from what is currently used. More specifically, we will ask for the maximum container size in terms of ram, cpu and disk and the maximum number of containers. The algorithm will decide how many containers to use. Apart from homogeneous containers, this version of the algorithm will also work with heterogeneous containers that can be handled by schedulers such as Yarn. In this way, we can further improve the resource utilization for such environments. Finally, the binpacking algorithm can be extended in the future to handle multiple dimensions (e.g., cpu and disk requirements per component) when these are supported by Heron.  Do you know if there is any plan to support different cpu configurations per component? I would think that this would be useful for users with cpu-bound topologies. In this case, we could use a multi-dimensional binpacking algorithm that would take both memory and cpu into account.\nI think Bill's suggestion to implement multiple packing algorithms and observe the resource usage at the tuning phase is a very good idea since it is not clear to me how different algorithms behave for various topologies and user configurations. \n. @billonahill The number of containers is provided as input at the RR algo but the other two algos can change the number of containers. Moreover, the way they add and remove instances from containers is specific to each packing algo. The Requested componentram, disk and cpu methods apply to the RR algos. We have different parameters for the binpacking algos (e.g., TOPOLOGY_CONTAINER_MAX_CPU_HINT). We could make all the algos use the same parameters.\n. @ashvina @billonahill I think the logic for creating the new packing plan should be left to each individual algo. The builder should hold and modify the data structures needed to produce the packing plan but should not incorporate the logic of generating a new packing plan.\n. @objmagic Done. Thanks!\n. I see there are some checkstyle errors which I don't get when building locally and running the tests. Is there any way to run chcekstyle alone and figure out what is wrong?\n. @maosongfu Thanks! I tried this command and it still works locally. I'm working on Ubuntu but we also tried on a mac and we don't get any checkstyle errors there as well.\n. @kramasamy I fixed the checkstyle errors. After bazel clean I was able to see them and I checked in the new code. I will write some extra documentation. I have some comments about the algo in the code but I can expand.\n@objmagic Thanks for noticing that. There has been some weird configuration issue and apparently GitHub linked my email with an old GitHub account (avrilia). I will fix this and from now on only avflor will be used.\n. I made some updates to address some comments and also changed some configuration parameters. I will check it in tomorrow. \n. @ajorgensen Let me know if you have other comments and I will address them tomorrow before I push the current version.\n. @ajorgensen @kramasamy @maosongfu I submitted a revised version of the binpacking algorithm. I noticed when I run in local mode that the number of stream managers is picked based on the topology config. However, the binpacking algorithm decides on its own how many containers to use. The number of stream managers launched should be based on the result of packing algorithm. I enhanced the SchedulerMain.java code to take the value from the PackedPlan. I made the necessary change to the junit tests as well. I also changed the build files so that the Heron client can use the binpacking algorithm. Finally, I made the padding configurable and changed the binpacking algorithm to accommodate it. Please let me know if you have any further comments.\n. @kramasamy Karthik, what do you think about me submitting the documentation PR as a separate one next week?  I need some more time to look into the tools and the setup. In the meantime, I have comments on how to use the algorithm in the FirstFitDecreasing.java class. \n. First fit decreasing documentation: #1098 \n. @ajorgensen This is a good suggestion and this one of the configuration changes I'm currently making in the ffdpacking algorithm. \n. @kramasamy It is complete but just for the binpacking algo. I will adapt round robin as we discussed yesterday and then I'll close this.\n. @kramasamy @billonahill I think I have addressed all your comments. Please let me know how the doc looks like now.\n. @kramasamy This version of the round robin packing algorithm will by default return a null packing if the number of containers specified by the user is not sufficient to satisfy the resource requirements of the components. Optionally the user can configure the algorithm (using topologyConfig) to automatically increase the number of containers if the user-specified number is not sufficient so that a packing plan is created. Is this an acceptable behavior? Let me know if you agree with the defaults.\n. @kramasamy It might affect existing jobs if the number of containers provided as input is not sufficient to satisfy the memory requirements of each instance. The current round robin algorithm will reduce the memory requirements of each instance to fit into a smaller number of containers. This algorithm will fail.  If I make the autoadjustment as default then this algo will not affect current jobs. They will not fail but they may use more containers than previously. This is because now the memory requirements of each instance are guaranteed to be satisfied. However with homogeneous containers you may end up seeing more resources being used than the current round robin. \nI think if you want to guarantee the memory resources per instance, this algorithm is better since it provides the guarantee and can automatically figure out the number of containers. However, if you are fine not providing this guarantee but use fewer homogeneous containers, then you should use the existing algo. In case of YARN, I would recommend the enhanced algo since it can support heterogeneous containers and thus will not waste resources.\nThis algorithm supports configurable padding that is added on top of the container_ram defined in the topology. Thus your users, might see slightly bigger containers as it is not assumed that they take padding into account when defining the container size. This is because the padding value is dependent on the number of instances packed in a container and thus is applied after packing.\nLet me know if you want me to change the default for auto-adjustment.\n. @kramasamy I have already coded it as a separate algorithm under the round robin package so that it does not affect existing jobs. We can try it and see. I was thinking whether it makes sense to have the packing algo being configurable for each topology. E.g a topology where minimizing resources is important can use binpacking, another one round robin etc. What do you think?\n. @kramasamy Cool I'll try that. I thought packing algo is a global config.\n. @billonahill @ajorgensen Thanks for the detailed feedback. Some parts of the code I copied from the current round robin algo and I'm not sure If I should change them or keep the same for consistency. I will mark individual changes so that you can clarify what is best to do.\n. @billonahill @kramasamy @ajorgensen I think I addressed all the comments. I moved the Resource class out of the PackingPlan and this change affected multiple files. Please let me know if anything else is needed.\n. @billonahill @kramasamy @ajorgensen BTW, is there any style formatter that applies the twitter guidelines like additional newlines? Checkstyle does not give me any errors on newlines and intellij does not fix the issue when reformatting the code. What do you usually use for that? Is there an intellij plugin for this?\n. @kramasamy Yeah, it's not there. Maybe we should add it. It will make the process much easier.\n. @billonahill I think I made all the changes. Regarding the constants in the tests:\nA number 4 in an equation doesn't necessarily mean 4 spouts. It can reflect 2 spouts and 2 bolts in one container. So replacing the number with spoutParalellim (or BoltParallelsim) can be misleading. I made the change for the tests that this makes sense. I kept the constants for the tests that are more complex. \n. @billonahill I added some comments to clarify the numbers in the assertions. Hope it works now :)\n. @kramasamy Fixed\n. @nlu90 @billonahill The change looks good to me. The only difference is that in the original code Aurora takes only memory into account when computing the max resources whereas Mesos considers all the resources. I think it is more correct to use all resources as done in this PR.\n. I also agree with Bill that the runtime is not needed. So far it has been used in packing just to extract the topology. In my first repacking algo implementation I have not used it at all. I think we should not include it in the signature unless there is evidence that a packing or repacking algorithm might need it.  I don't see this happening right away. Can we remove it for now and include it if there is need in the future or this is too messy?\n. @kramasamy Any feedback on this PR? Should we merge?\n. How about hash(x) = x mod K where K is the number of buckets and x is an integer?\n. @billonahill Looks good to me. Maybe we should add a comment at ContainerDelta to clarify that the code assumes that the size of an existing container cannot change from currentPlan to proposedPlan. \nIf the scheduler provides this functionality we would need to change this code since it might return wrong results.\n. @billonahill I'd like some more time to review this one. Please don't merge yet\n. @billonahill It looks good to me :)\n. @billonahill I prefer the second approach since it allows us to do some ordering in the update requests if needed. So the update topology manager can detect if a particular request is out of order (the current updateRequestId is much greater than the previous one it processed). I'm not sure if this is useful though. Just thinking. \n. @billonahill I have addressed all the comments. Let me know if anything else is needed.\n. @billonahill I reviewed the code a few hours ago. It looks good. Thanks for catching the bug with the globaltaskIndex!\n. @billonahill I addressed all the comments except for the packing builder object. I need to think more about it.\n. @billonahill I addressed all the comments. We have some ideas about the packing builder. Will submit that later.\n. @billonahill Thanks for the comments. I think we can merge now. \n. @kramasamy Sure, I can definitely read the paper an provide feedback. \n@wangli1426 Thanks for pointing us to the paper. \n. @billonahill Can you please take a look at this? I added the padding constraints and also fixed the container size problem for Aurora scheduler\n. @billonahill Comments addressed.\n. @billonahill Comments addressed\n. @billonahill Can you please take a look at this? It is some refactoring for the RR repack\n. @billonahill Can you please take a look at this PR? It contains the repacking implementation of RR.\n. @billonahill I think I addressed all your comments. Can you please take a look?\n. @billonahill @kramasamy Should we merge this one?\n. @wangli1426 I think there are two things to consider:\n1) Re-routing of keys in case we add/remove instances and\n2) State migration of Bolts to newly started bolts.\nI think your proposal refers to the 2nd one, right? I believe the 1st one we can be implemented in the grouping methods with a smart hash function and not in user code. If your proposal refers to the 2nd method, then do you assume that the state migration will be done by the stream manager? I see that you mention this component but I'm not sure If I understood correctly.\n. @wangli1426 I think it is not that simple to do state migration through the stream manager. I believe we need a central external unit to do the state migration as we will avoid many coordination problems between stream managers. There are multiple things that need to be synchronized: we need to make sure tuples are not routed to the original bolts while migrating the state of the bolts, we need to know when all state migrations have been completed so that processing is resumed, we need to handle failures (e.g, a new bolt failed while receiving state). I think a central external component that performs the coordination, failure handling etc might be a better solution that introducing this functionality to the stream manager. Moreover, I'm not sure how we will make the heron instance understand that it is not receiving regular data tuples through the stream manager but some other bolt's state.  But apart from that I think a central state migration is a more robust solution. @billonahill @kramasamy @maosongfu may have more input on this.\n. Will review that tomorrow\n. @billonahill The changes look good to me.\n. @billonahill This is an interesting idea. I think the algorithm can be simplified to work as follows:\nBefore you do the scaling down (step 2 in this case) compute for each container the following: number of instances of component you will scale/total number of instances in a container. So in your example the ratios will be [2/3, 2/3,2/2] as you also said in step 2 of your algorithm. Then sort the containers in decreasing order of these ratios and remove the component from the first container, update the first container's ratio and adjust its position in the ordered list if needed. Then remove again from the first container and so on until you remove all the components you want. This version avoids step 1 of your algo and also lets you decide at any point in time which container should be targeted next. Thoughts?\n. +1\n. I thought more about this based on @ashvina comments and I think that essentially @billonahill also wants to remove the containers that consist of same components whenever that possible. However, the solution he suggests is through the imbalance metric. The problem with this solution is that it might lose opportunities for removing containers because it attempts to remove components from the most loaded instances. I think we can still achieve what Bill wants (removing from newly added containers) without using the imbalance metric. The overall idea is that during scale down, we want to remove as many containers as possible (essentially from all the sets of possible solutions we want the one that minimizes the minimum container load). I think we can write an integer program that does that for not only single component scale downs but also for multiple component scale down in one command. Another approach is to use a heuristic that operates at a single component at a time. Let's take an example:\nWe have 9 containers with 2 As and 2 Bs (A A B B), 1 container (A,B), 1 container with (A A A A A) and 1 container (A A A) each as a result of a scale up operation. The user wants to scale down A 10 times. The heuristic woks as follows:\nFirst pick the containers that contain As only (2 containers in this example) and sort them based on increasing number of instances . Start removing from the first container in the list (A A A), moving to the next one (A A A A A) until both containers are removed or we do not need to remove more instances. Note that in this example, we still need to remove two instances of A.\nNow we look at the containers that contain As and other components as well. We sort them in increasing order of load (total number of instances per container) and break ties by sorting them in increasing number of As. So the first container in the list will be (A B). We remove A from this container (result is container (B)) and then remove A from the next container (A A B B) to get container (A B B).\nI think this heuristic will have the expected behavior that Bill mentioned in his original example (in cases where users want to scale up/down one component at a time) and at the same time will attempt to remove as many containers as possible. In Ashvin's example above, the container (AB) will be removed resulting in the original packing plan. \nAny thoughts? We can also create an integer program and solve it thorugh some optimization framework if we want to be more accurate/fancy.\n. @billonahill Yes, the basic differences are the following:\n1) We categorize the containers based on their type (containers that contain only the component we want to scale down and containers that contain a mix)\n2) The heuristic sorts the containers of each category in increasing load (total number of instances) and not decreasing.\n3) Ties are broken by sorting the containers in increasing order of number of instances of the component we want to remove.\n4) We operate on the category of homogeneous containers first and next on the category with the mix.\nUsing this heuristic we remove from least loaded containers thus increasing the chances of removing containers overall, but at the same time achieve the behavior we want for the scenarios you described using container categorization.\n. @billonahill Note that I don't use the ratio as the score. The scoring metric now is load (total number of instances in a container). The ratio score will result in removing from overloaded containers which is not correct since you will lose opportunities for container removal. By container categorization + different scoring function you avoid these problems while still getting the behavior you want in the scenario you described. Let me give an example:\nLet's take the case where we have the containers (A A B B ) (A A B B ) (A A B B ) and the user wants tor remove 2As and 2 Bs. You heuristic (ration as scoring function) after removal of 2 As will result in:\n(A B B ) (A B B) (A A B B) and after removal of B will result in (A B ) (A B) ( A A B B). We lost an opportunity here to remove one container.\nThe heuristic I propose will produce ( A A B B) (A A B B).\nAt the same time, it will work as you want in the example you gave in the beginning of this thread due to container categorization.\n. @billonahill I think we need a call to clarify all this :). My algorithm works at one component at a time as yours (not multiple ones). If a user has submitted a single request to remove 4As and 4 Bs, then the algorithm will work on the first A, second A, third A, fourth A, first B, second B, third B and fourth B in that order. Let's see the algo now:\n1st A removal: We categorize the containers into 2 groups. \nGroup that contains only As: [A,A,A,A]\nGroup that contains a mix of A with other stuff: [[AABB, AABB, AABB, AABB]\nWe start with first group and remove 1 A.\nThe result would be  [[AABB, AABB, AABB, AABB, AAA,BBBB].\n2nd A removal. Creating again two groups:\nGroup that contains only As (homogeneous) : [A,A,A]\nGroup that contains a mix of A with other stuff (heterogeneous): [[AABB, AABB, AABB, AABB]\n We remove 1 A from container in first group and the result is:\n [[AABB, AABB, AABB, AABB, AA, BBBB]\nit is easy to see that 3rd and 4rth removal will result in removing one container resulting in:\n [AABB, AABB, AABB, AABB, BBBB]\nThen we get to 1st B. We have the following groups:\nGroup that contains only Bs: [B,B,B,B]\nGroup that contains a mix of B with other stuff: [[AABB, AABB, AABB, AABB]\nWe remove B from group 1 resulting in:\n[AABB, AABB, AABB, AABB, BBB]\nAs previosly after the fourth removal we will have [AABB, AABB, AABB, AABB].\nNote that if you had multiple containers to pick in a group you pick the one with the least number of instances. In this example we had only one container every time in the homogeneous group so that didn't come across.\n. @billionahill. The result of scale up (which is homogeneous containers) will be undone with both your and my algorithm since they both remove the homogeneous containers (yours because of the imbalance metric and mine because of categorization). In this scenario both algos will produce the old state (state before scale upoperation). The problem comes when you operate on mixed containers. If your goal is for mixed containers to remove imbalance then that's ok, but if your goal is to remove as much containers as possible during scale down then imbalance metric will not do that. IMO the latter provides better user experience than the former because users ultimately scale down to release resources.. The example you give me is a use case where your heuristic will remove a container and my will not. However, note that these are heuristics. There are cases that these will not work. The observation here is that optimizing for removing imbalance doesn't necessarily mean that you remove containers. In fact it can prevent container removal.  If you want to optimize for removing containers then a heuristic that removes from the least loaded container will result in the general case  in more removals that a heuristic that optimizes for imbalance. If we agree on these observations and you still think that removing imbalance is the first goal here then it's ok. But I want to make sure you understand my point that these are two different and conflicting objectives and that removing imbalance results in lost opportunities for removing containers in the general case. . @ashvina We need to agree on the objective function first which means that we need to agree that optimizing for imbalance and optimizing for container removal is a different thing. When we agree on the objective function then we can determine the best algo fr our use cases.. @ashvina @billonahill I think there might be a problem with the placeFFDInsatce method in binpacking. \nThere is a for loop assuming contiguous container Ids:\nhttps://github.com/twitter/heron/blob/master/heron/packing/src/java/com/twitter/heron/packing/binpacking/FirstFitDecreasingPacking.java#L305\nI will investigate tomorrow.\n. Great! The basic idea is that the containers should still be ordered for the FFD heuristic to work but we should not assume the container Ids are contiguous.\n. @billonahill Will take a look later today. @billionahill LGTM. @billonahill This LGTM.. @billonahill Thanks Bill. The code LGTM.. @billonahill LGTM. @congwang Just the number of data tuples (no ack tuples). What is fail tuples? If they have failed would they be in the queue?. That would be great! We might need these metrics in the future.. @congwang The code looks fine to me. I have some questions.\n1) It seems that every time a new tuple for a particular instance arrives, the counter that corresponds to that instance is increased. Then every time the updatestrmmetrics is called the number of tuples for this instance is set to the value of the counter. Is it possible that the tuples do not exist any more in the queue at that point and thus the value of the metric is incorrect?\n2)Does the queue size metric show the total bytes corresponding to the data tuples in the queue or it also includes other data/metadata?. Two more comments:\n1) The counter is updated based on each tuple's size. Can we also have the a similar metric for number of tuples?\n2) it seems that the instance_info_[task_id] keeps increasing as we receive more tuples. Is that right? Since it represents the queue size at a particular time, it should also decrease as the tuples get processed by the corresponding instances.. @maosongfu This looks good to me.. @congwang I don't think it should be more than 60 sec since only one instance of a given component would be the source of backpressure at a time. So during a 1-minute interval, the sum should not be more than 60 sec. @maosongfu may want to comment.. I agree with @congwang that restarting the container does not necessarily mean that backpressure will disappear. However, restart can be helpful in some cases (e.g., when backpressure is caused by a slow container/machine).  In the context of Healthmanager, restarting a container is one resolution mechanism that we have but there are others as well depending on the cause of backpressure.. @kramasamy We will submit  a PR with the health manager APIs tomorrow for review. Once we agree on the APIs, we will start checking in the code for the policies.. Why do they have different APIs? I thought the metricscache would have the same api as metricstracker.. Looks good to me... Hi @huijunw .The FFD repacking algo will only work if you have done the initial packing with the FFD algorithm. Is that the case here?. The idea was that RoundRobin packing should be replaced by resource compliant round robin packing and that the scaling capability would work only for topologies that have updated to the resource compliant algo.. Hi @huijunw. The healthmgr logs the checkpoint values each time the policy is executed (see https://github.com/Microsoft/Dhalion/blob/0.2.1/src/main/java/com/microsoft/dhalion/policy/PoliciesExecutor.java#L77) Can you please post the log lines for the last 5 policy executions? . Hi @huijunw We are making some changes to the Dhalion APIs that should be able to fix this problem. We will release a new Dhalion version soon.. Hi @huijunw . We have done most of the changes in the APIs but need to do some more testing. I should be able to give a concrete estimate next week. Does this work for you?. ",
    "Laxman-SM": "HI kramasamy,\npresently heron topology on AURORA will have n+3 containers ? 1 for AM, 1 for TMasterand 1 for Scheduler and N worker nodes. this logic also applied for Aurora ?\n. also how to declare custom variable for host.\ndeclare -a hosts=(ec2-xx-xxx-xxx-xxx.compute-1.amazonaws.com ec2-xx.xxx.xx.xx.compute-1.amazonaws.com ec2-xx-xxx-xx-xx.compute-1.amazonaws.com)\nhow to replace custom variable on scp -i ~/.ssh/id_rsa user@xx.xxx.xx.xx:%s x.xxx.xx.xx place.. ",
    "merttgokalp": "@kramasamy \nThis code snippet works for Storm to deploy topologies.\nTopologyBuilder builder = new TopologyBuilder();\n    builder.setSpout(\"word\", new TestWordSpout(), 1);\n    builder.setBolt(\"exclaim1\", new ExclamationBolt(), 1)\n        .shuffleGrouping(\"word\");\n    Config conf = new Config();\n    config.setMaxTaskParallelism(3);\n    config.setMessageTimeoutSecs(10);\n    config.setNumWorkers(2);\n    System.setProperty(\"storm.jar\", \"/Users/merttgokalp/Desktop/heron-examples.jar\");\n    StormSubmitter.submitTopology(\"myTopology\", conf, builder.createTopology());\nThe problem is that HeronSubmitter.java file only reads the command line parameters to deploy a topology. Therefore, I can not deploy a topology programmatically at run-time using Java.\nif (heronCmdOptions.get(\"cmdline.topologydefn.tmpdirectory\") != null) {\n      submitTopologyToFile(fTopology, heronCmdOptions);\n    } else {\n      throw new RuntimeException(\"topology definition temp directory not specified\");\n    }\n. ",
    "leonardgithub": "I also get the \"topology definition temp directory not specified\" error programmatically, any solution?. @billonahill It is OK now by using \nhttps://github.com/twitter/heron/blob/0.14.5/heron/examples/src/java/com/twitter/heron/examples/ExclamationTopology.java.. After a deep thought, I think no need to change. PR closed now\n. IDE: Intellij Idea\nsteps: \n1. create an new java maven project\n2. add the below into pom.xml\n<dependency>\n            <groupId>com.twitter.heron</groupId>\n            <artifactId>heron-storm</artifactId>\n            <version>0.14.5</version>\n        </dependency>\n        <dependency>\n            <groupId>com.twitter.heron</groupId>\n            <artifactId>heron-api</artifactId>\n            <version>0.14.5</version>\n        </dependency>\n        <dependency>\n            <groupId>com.twitter.heron</groupId>\n            <artifactId>heron-kafka</artifactId>\n            <version>0.14.5</version>\n        </dependency>\n        <dependency>\n            <groupId>com.twitter.heron</groupId>\n            <artifactId>heron-spi</artifactId>\n            <version>0.14.5</version>\n        </dependency>\n\ncopy the example folder into the project\nrun the main method of MultiSpoutExclamationTopology\n\"topology definition temp directory not specified\" is shown. @billonahill Thank you for your answers.\nin step 4, I just right click the MultiSpoutExclamationTopology.java at Intellij Idea, then run the main method.\n\nThe heron info at my local environment is:\n$ heron version\nheron.build.version : '0.14.5'\nheron.build.time : Fri Nov 18 09:38:42 PST 2016\nheron.build.timestamp : 1479490745000\nheron.build.host : tw-mbp-kramasamy\nheron.build.user : kramasamy\nheron.build.git.revision : 89fbce68156464621aaf00646921de0770cd2b59\nheron.build.git.status : Clean\n$ echo $CLASSPATH\n/usr/java/default/lib/dt.jar:/usr/java/default/lib/tools.jar\n. @billonahill I will try, thanks for your help. @objmagic HI, could you tell me how to use mailling list, I find a mvn repository problem just now.\nsee http://mvnrepository.com/artifact/com.twitter.heron/heron-kafka/0.14.5, could not find artifact com.twitter.heron:heron-api:jar:SNAPSHOT. HI @kramasamy \nhttp://central.maven.org/maven2/com/twitter/heron/heron-kafka/0.14.5/heron-kafka-0.14.5.pom\n<dependency>\n<groupId>com.twitter.heron</groupId>\n<artifactId>heron-api</artifactId>\n<version>SNAPSHOT</version>\n<scope>compile</scope>\n</dependency>\nShould it be 0.14.5?. @lewiskan Thank you very much. @lewiskan I use the aliyun as a maven repository mirror, but the heron-api and heron-storm are still SNAPSHOT in http://maven.aliyun.com/nexus/service/local/repositories/central/content/com/twitter/heron/heron-kafka/0.14.5/heron-kafka-0.14.5.pom, any sulotion?. ",
    "wuye9036": "For single binary module and source only C++ executable,\nWe could use freopen() for C styled standard IO stream,\nAnd replace rdbuf for C++ styled standard IO stream.\n. In Linux, it means SIGUSR1 as same as POSIX standard.\nhttps://github.com/torvalds/linux/blob/master/include/uapi/asm-generic/signal.h\nAurora treats this SIGNAL as same as KILL.\nhttps://github.com/apache/aurora/blob/b24619b28c4dbb35188871bacd0091a9e01218e3/src/main/python/apache/thermos/runner/thermos_runner.py#L159\nCould we get the log from slave.cpp of aurora? I think it is helpful.\n. ",
    "pulasthi": "@lewiskan I did not have to make any changes in memory. The memory and Java heap values are the once i got by default. My machine has 16GB of RAM so i might have higher memory values for the vagrant as well. The memory allocated by default to vagrant in my machine is 2GB and java heap space is 512MB in my vagrant VM that i used for this. Can you check your values and let me know. if needed i will add some direction on how to increase the memory values.\n. @sharmaarun is there a more complete error log that we can take look at. Can you try adding --verbose to get a more detailed output of the error.\n. ",
    "sharmaarun": "Hi @pulasthi, I am also getting the same message \"Insufficient: RAM\". Can you help me solve this problem ? I have already increased the RAM for vagrant to 6G and also set the quota for aurora as :\naurora_admin set_quota devcluster vagrant 2 6G 4G\nStill when submitting the topology, it says insufficient : RAM. When I try to activate the topology, it says \nFailed to fetch data from path: /heronroot/pplans/ExclamationTopology\n. @maosongfu Before, it was only 3 GB memory given to vagrant box. But I changed it to 6GB.\n@pulasthi here is the --verbose output when I submit the topology :\n```\nheron submit devcluster/vagrant/devel --config-path ~/.heron/conf/ ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology1 --verbose\n{'config_property': [], 'topology-file-name': '/home/vagrant/.heron/examples/heron-examples.jar', 'override_config_file': '/tmp/tmphxZ7Vl/override.yaml', 'config_path': '/home/vagrant/.heron/conf/devcluster', 'subcommand': 'submit', 'deploy_deactivated': False, 'environ': 'devel', 'cluster': 'devcluster', 'topology_main_jvm_property': [], 'role': 'vagrant', 'verbose': 'True', 'topology-class-name': 'com.twitter.heron.examples.ExclamationTopology'}\n$> /usr/lib/jvm/java-8-openjdk-amd64/bin/java -client -Xmx1g -Dheron.options=cmdline.topologydefn.tmpdirectory=/tmp/tmp7u9K6p,cmdline.topology.initial.state=RUNNING -cp /home/vagrant/.heron/lib/3rdparty/:/home/vagrant/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology1\nINFO: Launching topology 'ExclamationTopology1'\n$> /usr/lib/jvm/java-8-openjdk-amd64/bin/java -client -Xmx1g -Dheron.options=cmdline.topologydefn.tmpdirectory=/tmp/tmp7u9K6p,cmdline.topology.initial.state=RUNNING -cp /home/vagrant/.heron/lib/scheduler/:/home/vagrant/.heron/lib/uploader/:/home/vagrant/.heron/lib/statemgr/:/home/vagrant/.heron/lib/packing/ com.twitter.heron.scheduler.SubmitterMain --cluster devcluster --role vagrant --environment devel --heron_home /home/vagrant/.heron --config_path /home/vagrant/.heron/conf/devcluster --override_config_file /tmp/tmphxZ7Vl/override.yaml --release_file /home/vagrant/.heron/release.yaml --topology_package /tmp/tmp7u9K6p/topology.tar.gz --topology_defn /tmp/tmp7u9K6p/ExclamationTopology1.defn --topology_jar /home/vagrant/.heron/examples/heron-examples.jar --verbose\n[2016-07-06 11:42:30 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config stream\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config stream\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Config file /home/vagrant/.heron/conf/devcluster/cluster.yaml does not exist\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /home/vagrant/.heron/conf/devcluster/client.yaml\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /home/vagrant/.heron/conf/devcluster/client.yaml\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /home/vagrant/.heron/conf/devcluster/packing.yaml\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /home/vagrant/.heron/conf/devcluster/packing.yaml\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /home/vagrant/.heron/conf/devcluster/scheduler.yaml\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /home/vagrant/.heron/conf/devcluster/scheduler.yaml\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /home/vagrant/.heron/conf/devcluster/statemgr.yaml\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /home/vagrant/.heron/conf/devcluster/statemgr.yaml\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /home/vagrant/.heron/conf/devcluster/uploader.yaml\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /home/vagrant/.heron/conf/devcluster/uploader.yaml\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /home/vagrant/.heron/release.yaml\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /home/vagrant/.heron/release.yaml\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /tmp/tmphxZ7Vl/override.yaml\n[2016-07-06 11:42:31 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /tmp/tmphxZ7Vl/override.yaml\n[2016-07-06 11:42:31 +0000] com.twitter.heron.scheduler.SubmitterMain FINE:  Static config loaded successfully \n[2016-07-06 11:42:31 +0000] com.twitter.heron.scheduler.SubmitterMain FINE:  (\"heron.binaries.sandbox.executor\", ./heron-core/bin/heron-executor)\n(\"heron.binaries.sandbox.shell\", ./heron-core/bin/heron-shell)\n(\"heron.binaries.sandbox.stmgr\", ./heron-core/bin/heron-stmgr)\n(\"heron.binaries.sandbox.tmaster\", ./heron-core/bin/heron-tmaster)\n(\"heron.build.git.revision\", be87b09f348e0ed05f45503340a2245a4ef68a35)\n(\"heron.build.git.status\", Modified)\n(\"heron.build.host\", tw-mbp-kramasamy)\n(\"heron.build.time\", Tue May 24 22:52:11 PDT 2016)\n(\"heron.build.timestamp\", 1464155548000)\n(\"heron.build.user\", kramasamy)\n(\"heron.build.version\", 0.14.0)\n(\"heron.class.launcher\", com.twitter.heron.scheduler.aurora.AuroraLauncher)\n(\"heron.class.packing.algorithm\", com.twitter.heron.packing.roundrobin.RoundRobinPacking)\n(\"heron.class.scheduler\", com.twitter.heron.scheduler.aurora.AuroraScheduler)\n(\"heron.class.state.manager\", com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager)\n(\"heron.class.uploader\", com.twitter.heron.uploader.localfs.LocalFileSystemUploader)\n(\"heron.classpath.instance\", /home/vagrant/.heron/lib/instance/)\n(\"heron.classpath.metrics.manager\", /home/vagrant/.heron/lib/metricsmgr/)\n(\"heron.classpath.packing\", /home/vagrant/.heron/lib/packing/)\n(\"heron.classpath.sandbox.instance\", ./heron-core/lib/instance/)\n(\"heron.classpath.sandbox.metrics.manager\", ./heron-core/lib/metricsmgr/)\n(\"heron.classpath.sandbox.packing\", ./heron-core/lib/packing/)\n(\"heron.classpath.sandbox.scheduler\", ./heron-core/lib/scheduler/)\n(\"heron.classpath.sandbox.statemgr\", ./heron-core/lib/statemgr/)\n(\"heron.classpath.sandbox.uploader\", ./heron-core/lib/uploader/)\n(\"heron.classpath.scheduler\", /home/vagrant/.heron/lib/scheduler/)\n(\"heron.classpath.statemgr\", /home/vagrant/.heron/lib/statemgr/)\n(\"heron.classpath.uploader\", /home/vagrant/.heron/lib/uploader/*)\n(\"heron.config.cluster\", devcluster)\n(\"heron.config.environ\", devel)\n(\"heron.config.file.client.yaml\", /home/vagrant/.heron/conf/devcluster/client.yaml)\n(\"heron.config.file.cluster.yaml\", /home/vagrant/.heron/conf/devcluster/cluster.yaml)\n(\"heron.config.file.defaults.yaml\", /home/vagrant/.heron/conf/devcluster/defaults.yaml)\n(\"heron.config.file.metrics.yaml\", /home/vagrant/.heron/conf/devcluster/metrics_sinks.yaml)\n(\"heron.config.file.packing.yaml\", /home/vagrant/.heron/conf/devcluster/packing.yaml)\n(\"heron.config.file.scheduler.yaml\", /home/vagrant/.heron/conf/devcluster/scheduler.yaml)\n(\"heron.config.file.statemgr.yaml\", /home/vagrant/.heron/conf/devcluster/statemgr.yaml)\n(\"heron.config.file.system.yaml\", /home/vagrant/.heron/conf/devcluster/heron_internals.yaml)\n(\"heron.config.file.uploader.yaml\", /home/vagrant/.heron/conf/devcluster/uploader.yaml)\n(\"heron.config.is.env.required\", true)\n(\"heron.config.is.role.required\", true)\n(\"heron.config.role\", vagrant)\n(\"heron.config.sandbox.file.cluster.yaml\", ./heron-conf/cluster.yaml)\n(\"heron.config.sandbox.file.defaults.yaml\", ./heron-conf/defaults.yaml)\n(\"heron.config.sandbox.file.metrics.yaml\", ./heron-conf/metrics_sinks.yaml)\n(\"heron.config.sandbox.file.packing.yaml\", ./heron-conf/packing.yaml)\n(\"heron.config.sandbox.file.scheduler.yaml\", ./heron-conf/scheduler.yaml)\n(\"heron.config.sandbox.file.statemgr.yaml\", ./heron-conf/statemgr.yaml)\n(\"heron.config.sandbox.file.system.yaml\", ./heron-conf/heron_internals.yaml)\n(\"heron.config.sandbox.file.uploader.yaml\", ./heron-conf/uploader.yaml)\n(\"heron.config.verbose\", true)\n(\"heron.directory.bin\", /home/vagrant/.heron/bin)\n(\"heron.directory.conf\", /home/vagrant/.heron/conf/devcluster)\n(\"heron.directory.dist\", /home/vagrant/.heron/dist)\n(\"heron.directory.etc\", /home/vagrant/.heron/etc)\n(\"heron.directory.home\", /home/vagrant/.heron)\n(\"heron.directory.java.home\", /usr/lib/jvm/java-8-openjdk-amd64)\n(\"heron.directory.lib\", /home/vagrant/.heron/lib)\n(\"heron.directory.sandbox.bin\", ./heron-core/bin)\n(\"heron.directory.sandbox.conf\", ./heron-conf)\n(\"heron.directory.sandbox.home\", ./heron-core)\n(\"heron.directory.sandbox.java.home\", /usr/lib/jvm/java-1.8.0-openjdk-amd64)\n(\"heron.directory.sandbox.lib\", ./heron-core/lib)\n(\"heron.jars.sandbox.scheduler\", ./heron-core/lib/scheduler/heron-scheduler.jar)\n(\"heron.jars.scheduler\", /home/vagrant/.heron/lib/scheduler/heron-scheduler.jar)\n(\"heron.package.core.uri\", file:///vagrant/.herondata/dist/heron-core-release.tar.gz)\n(\"heron.resources.instance.cpu\", 1.0)\n(\"heron.resources.instance.disk\", 1073741824)\n(\"heron.resources.instance.ram\", 1073741824)\n(\"heron.resources.stmgr.ram\", 1073741824)\n(\"heron.scheduler.is.service\", false)\n(\"heron.statemgr.connection.string\", 127.0.0.1:2181)\n(\"heron.statemgr.root.path\", /heronroot)\n(\"heron.statemgr.zookeeper.connection.timeout.ms\", 30000)\n(\"heron.statemgr.zookeeper.is.initialize.tree\", true)\n(\"heron.statemgr.zookeeper.retry.count\", 10)\n(\"heron.statemgr.zookeeper.retry.interval.ms\", 10000)\n(\"heron.statemgr.zookeeper.session.timeout.ms\", 30000)\n(\"heron.topology.definition.file\", /tmp/tmp7u9K6p/ExclamationTopology1.defn)\n(\"heron.topology.id\", ExclamationTopology1308cd17d-cd49-4ee5-a356-8a9acff652a8)\n(\"heron.topology.jar.file\", /home/vagrant/.heron/examples/heron-examples.jar)\n(\"heron.topology.name\", ExclamationTopology1)\n(\"heron.topology.package.file\", /tmp/tmp7u9K6p/topology.tar.gz)\n(\"heron.topology.package.type\", jar)\n(\"heron.uploader.localfs.file.system.directory\", /vagrant/.herondata/repository/topologies/devcluster/vagrant/ExclamationTopology1)\n[2016-07-06 11:42:31 +0000] com.twitter.heron.statemgr.FileSystemStateManager FINE:  File system state manager root address: /heronroot\n[2016-07-06 11:42:32 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Starting client to: 127.0.0.1:2181\n[2016-07-06 11:42:32 +0000] org.apache.curator.framework.imps.CuratorFrameworkImpl INFO:  Starting\n[2016-07-06 11:42:32 +0000] org.apache.curator.CuratorZookeeperClient FINE:  Starting\n[2016-07-06 11:42:32 +0000] org.apache.curator.ConnectionState FINE:  Starting\n[2016-07-06 11:42:32 +0000] org.apache.curator.ConnectionState FINE:  reset\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:host.name=aurora.local\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.version=1.8.0_91\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.vendor=Oracle Corporation\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.class.path=/home/vagrant/.heron/lib/scheduler/heron-slurm-scheduler.jar:/home/vagrant/.heron/lib/scheduler/heron-scheduler.jar:/home/vagrant/.heron/lib/scheduler/heron-aurora-scheduler.jar:/home/vagrant/.heron/lib/scheduler/heron-roundrobin-packing.jar:/home/vagrant/.heron/lib/scheduler/heron-local-scheduler.jar:/home/vagrant/.heron/lib/uploader/heron-hdfs-uploader.jar:/home/vagrant/.heron/lib/uploader/heron-null-uploader.jar:/home/vagrant/.heron/lib/uploader/heron-localfs-uploader.jar:/home/vagrant/.heron/lib/uploader/heron-s3-uploader.jar:/home/vagrant/.heron/lib/statemgr/heron-localfs-statemgr.jar:/home/vagrant/.heron/lib/statemgr/heron-zookeeper-statemgr.jar:/home/vagrant/.heron/lib/packing/heron-roundrobin-packing.jar\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.io.tmpdir=/tmp\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.compiler=\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:os.name=Linux\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:os.arch=amd64\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:os.version=3.13.0-88-generic\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:user.name=vagrant\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:user.home=/home/vagrant\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:user.dir=/home/vagrant\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ZooKeeper INFO:  Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=30000 watcher=org.apache.curator.ConnectionState@17211155\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  zookeeper.disableAutoWatchReset is false\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn INFO:  Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn INFO:  Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Session establishment request sent on 127.0.0.1/127.0.0.1:2181\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxnSocket FINEST:  readConnectResult 37 0x[0,0,0,0,0,0,75,30,1,55,ffffffc0,1,35,15,0,c,0,0,0,10,ffffff86,ffffffad,60,49,ffffff9a,47,fffffff2,3c,66,62,11,fffffff8,ffffffdf,4,ffffffdd,ffffffb1,0,]\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn INFO:  Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x155c0013515000c, negotiated timeout = 30000\n[2016-07-06 11:42:32 +0000] org.apache.curator.framework.state.ConnectionStateManager INFO:  State change: CONNECTED\n[2016-07-06 11:42:32 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Topologies directory: /heronroot/topologies\n[2016-07-06 11:42:32 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Tmaster location directory: /heronroot/tmasters\n[2016-07-06 11:42:32 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Physical plan directory: /heronroot/pplans\n[2016-07-06 11:42:32 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: connection-state-parent-process - 49 ms\n[2016-07-06 11:42:32 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Execution state directory: /heronroot/executionstate\n[2016-07-06 11:42:32 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Scheduler location directory: /heronroot/schedulers\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 1,3  replyHeader:: 1,682,0  request:: '/heronroot,F  response:: s{74,74,1467513024815,1467513024815,0,5,0,0,4,5,83} \n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 2,3  replyHeader:: 2,682,0  request:: '/heronroot/topologies,F  response:: s{75,75,1467513074362,1467513074362,0,20,0,0,4,0,680} \n[2016-07-06 11:42:32 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 53 ms\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 3,3  replyHeader:: 3,682,-101  request:: '/heronroot/topologies/foo,F  response::  \n[2016-07-06 11:42:32 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 20 ms\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 4,3  replyHeader:: 4,682,0  request:: '/heronroot,F  response:: s{74,74,1467513024815,1467513024815,0,5,0,0,4,5,83} \n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 5,3  replyHeader:: 5,682,0  request:: '/heronroot/tmasters,F  response:: s{80,80,1467513214249,1467513214249,0,0,0,0,0,0,80} \n[2016-07-06 11:42:32 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 15 ms\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 6,3  replyHeader:: 6,682,-101  request:: '/heronroot/tmasters/foo,F  response::  \n[2016-07-06 11:42:32 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 3 ms\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 7,3  replyHeader:: 7,682,0  request:: '/heronroot,F  response:: s{74,74,1467513024815,1467513024815,0,5,0,0,4,5,83} \n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 8,3  replyHeader:: 8,682,0  request:: '/heronroot/pplans,F  response:: s{81,81,1467513214306,1467513214306,0,0,0,0,0,0,81} \n[2016-07-06 11:42:32 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 17 ms\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 9,3  replyHeader:: 9,682,-101  request:: '/heronroot/pplans/foo,F  response::  \n[2016-07-06 11:42:32 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 5 ms\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 10,3  replyHeader:: 10,682,0  request:: '/heronroot,F  response:: s{74,74,1467513024815,1467513024815,0,5,0,0,4,5,83} \n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 11,3  replyHeader:: 11,682,0  request:: '/heronroot/executionstate,F  response:: s{82,82,1467513214377,1467513214377,0,20,0,0,0,0,679} \n[2016-07-06 11:42:32 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 19 ms\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 12,3  replyHeader:: 12,682,-101  request:: '/heronroot/executionstate/foo,F  response::  \n[2016-07-06 11:42:32 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 2 ms\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 13,3  replyHeader:: 13,682,0  request:: '/heronroot,F  response:: s{74,74,1467513024815,1467513024815,0,5,0,0,4,5,83} \n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 14,3  replyHeader:: 14,682,0  request:: '/heronroot/schedulers,F  response:: s{83,83,1467513214432,1467513214432,0,0,0,0,0,0,83} \n[2016-07-06 11:42:32 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 15 ms\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 15,3  replyHeader:: 15,682,-101  request:: '/heronroot/schedulers/foo,F  response::  \n[2016-07-06 11:42:32 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 9 ms\n[2016-07-06 11:42:32 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Directory tree initialized.\n[2016-07-06 11:42:32 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Checking existence of path: /heronroot/topologies/ExclamationTopology1\n[2016-07-06 11:42:32 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 16,3  replyHeader:: 16,682,-101  request:: '/heronroot/topologies/ExclamationTopology1,F  response::  \n[2016-07-06 11:42:32 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 8 ms\n[2016-07-06 11:42:32 +0000] com.twitter.heron.scheduler.SubmitterMain FINE:  Topology ExclamationTopology1 to be submitted\n[2016-07-06 11:42:32 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config stream\n[2016-07-06 11:42:32 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config\n[2016-07-06 11:42:32 +0000] com.twitter.heron.uploader.localfs.LocalFileSystemUploader FINE:  The working directory does not exist; creating it.\n[2016-07-06 11:42:32 +0000] com.twitter.heron.uploader.localfs.LocalFileSystemUploader FINE:  Copying topology /tmp/tmp7u9K6p/topology.tar.gz package to target working directory /vagrant/.herondata/repository/topologies/devcluster/vagrant/ExclamationTopology1/ExclamationTopology1-vagrant-tag-0--1691573636521729062\n[2016-07-06 11:42:33 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 17,1  replyHeader:: 17,683,0  request:: '/heronroot/topologies/ExclamationTopology1,#a384578636c616d6174696f6e546f706f6c6f67793133303863643137642d636434392d346565352d613335362d38613961636666363532613812144578636c616d6174696f6e546f706f6c6f6779311a4ea2da4776f72641a25a23a1e746f706f6c6f67792e636f6d706f6e656e742e706172616c6c656c69736d12131121dafa764656661756c74124776f726412aa8a4776f72641012248a31a86578636c61696d311a25a23a1e746f706f6c6f67792e636f6d706f6e656e742e706172616c6c656c69736d121311213afa764656661756c74124776f726410128132ffffff8f8a3ea19746f706f6c6f67792e636f6d706f6e656e742e72616d6d61701221776f72643a3533363837303931322c6578636c61696d313a353336383730393132a75a18746f706f6c6f67792e6175746f2e7461736b2e686f6f6b731a59ffffffacffffffed0573720146a6176612e7574696c2e4c696e6b65644c697374c29535d4a60ffffff882230078707740001740266261636b747970652e73746f726d2e686f6f6b732e495461736b486f6f6b44656c656761746578a5ba28746f706f6c6f67792e736b69702e6d697373696e672e6b72796f2e726567697374726174696f6e731a2fffffffacffffffed0573720116a6176612e6c616e672e426f6f6c65616effffffcd2072ffffff80ffffffd5ffffff9cfffffffaffffffee2015a0576616c756578700a25a17746f706f6c6f67792e636f6e7461696e65722e6469736b12a31303733373431383234a28a20746f706f6c6f67792e656e61626c652e6d6573736167652e74696d656f75747312474727565a5ea1d746f706f6c6f67792e73657269616c697a65722e636c6173736e616d65123d6261636b747970652e73746f726d2e73657269616c697a6174696f6e2e4865726f6e506c75676761626c6553657269616c697a657244656c6567617465a16ae746f706f6c6f67792e646562756712474727565a20a1a746f706f6c6f67792e6d61782e73706f75742e70656e64696e671223130a4fa15746f706f6c6f67792e6b72796f2e666163746f72791a36ffffffacffffffed057402f6261636b747970652e73746f726d2e73657269616c697a6174696f6e2e44656661756c744b72796f466163746f7279a1da16746f706f6c6f67792e636f6e7461696e65722e637075123312e30a5ba28746f706f6c6f67792e66616c6c2e6261636b2e6f6e2e6a6176612e73657269616c697a6174696f6e1a2fffffffacffffffed0573720116a6176612e6c616e672e426f6f6c65616effffffcd2072ffffff80ffffffd5ffffff9cfffffffaffffffee2015a0576616c756578700a25ad746f706f6c6f67792e6e616d6512144578636c616d6174696f6e546f706f6c6f677931a23a1e746f706f6c6f67792e636f6d706f6e656e742e706172616c6c656c69736d12131a62a2473746f726d636f6d7061742e746f706f6c6f67792e6175746f2e7461736b2e686f6f6b731a3affffffacffffffed0573720136a6176612e7574696c2e41727261794c69737478ffffff81ffffffd21dffffff99ffffffc761ffffff9d301490473697a6578700000774000078a14af746f706f6c6f67792e73746d67727312131a3ca19746f706f6c6f67792e776f726b65722e6368696c646f707473121f2d58583a2b4865617044756d704f6e4f75744f664d656d6f72794572726f72a23a1d746f706f6c6f67792e6d6573736167652e74696d656f75742e736563731223330a18af746f706f6c6f67792e61636b696e6712566616c7365,v{s{31,s{'world,'anyone}}},0  response:: '/heronroot/topologies/ExclamationTopology1 \n[2016-07-06 11:42:33 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: CreateBuilderImpl-Foreground - 23 ms\n[2016-07-06 11:42:33 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Created node for path: /heronroot/topologies/ExclamationTopology1\n[2016-07-06 11:42:33 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515000c, packet:: clientPath:null serverPath:null finished:false header:: 18,1  replyHeader:: 18,684,0  request:: '/heronroot/executionstate/ExclamationTopology1,#a144578636c616d6174696f6e546f706f6c6f67793112384578636c616d6174696f6e546f706f6c6f67793133303863643137642d636434392d346565352d613335362d38613961636666363532613818ffffffa9ffffffddfffffff3ffffffbb522776616772616e742aa646576636c7573746572325646576656c3a776616772616e744215a96b72616d6173616d791201a6302e31342e30,v{s{31,s{'world,'anyone}}},0  response:: '/heronroot/executionstate/ExclamationTopology1 \n[2016-07-06 11:42:33 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: CreateBuilderImpl-Foreground - 42 ms\n[2016-07-06 11:42:33 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Created node for path: /heronroot/executionstate/ExclamationTopology1\n[2016-07-06 11:42:33 +0000] com.twitter.heron.scheduler.aurora.AuroraLauncher INFO:  Launching topology in aurora\n[2016-07-06 11:42:33 +0000] com.twitter.heron.spi.common.ShellUtils INFO:  $> [aurora, job, create, --wait-until, RUNNING, --bind, TOPOLOGY_NAME=ExclamationTopology1, --bind, SANDBOX_SYSTEM_YAML=./heron-conf/heron_internals.yaml, --bind, COMPONENT_RAMMAP=exclaim1:536870912,word:536870912, --bind, SANDBOX_METRICS_YAML=./heron-conf/metrics_sinks.yaml, --bind, INSTANCE_JVM_OPTS_IN_BASE64=\"LVhYOitIZWFwRHVtcE9uT3V0T2ZNZW1vcnlFcnJvcg&equals&equals\", --bind, ROLE=vagrant, --bind, ENVIRON=devel, --bind, SANDBOX_SCHEDULER_CLASSPATH=./heron-core/lib/scheduler/:./heron-core/lib/packing/:./heron-core/lib/statemgr/, --bind, SANDBOX_INSTANCE_CLASSPATH=./heron-core/lib/instance/, --bind, ISPRODUCTION=false, --bind, TOPOLOGY_CLASSPATH=heron-examples.jar, --bind, CLUSTER=devcluster, --bind, SANDBOX_EXECUTOR_BINARY=./heron-core/bin/heron-executor, --bind, STATEMGR_CONNECTION_STRING=127.0.0.1:2181, --bind, COMPONENT_JVM_OPTS_IN_BASE64=\"\", --bind, TOPOLOGY_ID=ExclamationTopology1308cd17d-cd49-4ee5-a356-8a9acff652a8, --bind, TOPOLOGY_PACKAGE_URI=file:///vagrant/.herondata/repository/topologies/devcluster/vagrant/ExclamationTopology1/ExclamationTopology1-vagrant-tag-0--1691573636521729062, --bind, SANDBOX_STMGR_BINARY=./heron-core/bin/heron-stmgr, --bind, CORE_PACKAGE_URI=file:///vagrant/.herondata/dist/heron-core-release.tar.gz, --bind, SANDBOX_METRICSMGR_CLASSPATH=./heron-core/lib/metricsmgr/*, --bind, TOPOLOGY_PACKAGE_TYPE=jar, --bind, RAM_PER_CONTAINER=2147483648, --bind, SANDBOX_TMASTER_BINARY=./heron-core/bin/heron-tmaster, --bind, TOPOLOGY_DEFINITION_FILE=ExclamationTopology1.defn, --bind, INSTANCE_DISTRIBUTION=1:word:2:0:exclaim1:1:0, --bind, NUM_CONTAINERS=2, --bind, CPUS_PER_CONTAINER=1.0, --bind, TOPOLOGY_JAR_FILE=heron-examples.jar, --bind, SANDBOX_SHELL_BINARY=./heron-core/bin/heron-shell, --bind, DISK_PER_CONTAINER=1073741824, --bind, STATEMGR_ROOT_PATH=/heronroot, --bind, HERON_SANDBOX_JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64, devcluster/vagrant/devel/ExclamationTopology1, /home/vagrant/.heron/conf/devcluster/heron.aurora, --verbose]\n[2016-07-06 11:42:43 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 2ms\n[2016-07-06 11:42:53 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 2ms\n[2016-07-06 11:43:03 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 8ms\n[2016-07-06 11:43:13 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 3ms\n[2016-07-06 11:43:23 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:43:33 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 9ms\n[2016-07-06 11:43:43 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 2ms\n[2016-07-06 11:43:53 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:44:03 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:44:13 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 6ms\n[2016-07-06 11:44:23 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 2ms\n[2016-07-06 11:44:33 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:44:43 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:44:53 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:45:03 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:45:13 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:45:23 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:45:33 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 2ms\n[2016-07-06 11:45:43 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 3ms\n[2016-07-06 11:45:53 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:46:03 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 6ms\n[2016-07-06 11:46:13 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:46:23 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:46:33 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 4ms\n[2016-07-06 11:46:43 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 2ms\n[2016-07-06 11:46:53 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:47:03 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:47:13 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 3ms\n[2016-07-06 11:47:23 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:47:33 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:47:43 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:47:53 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:48:03 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 1ms\n[2016-07-06 11:48:13 +0000] org.apache.zookeeper.ClientCnxn FINE:  Got ping response for sessionid: 0x155c0013515000c after 2ms\n```\nAnd below is the output when I try to activate the topology:\n```\nheron activate devcluster/vagrant/devel ExclamationTopology1 --verbose\n{'config_property': [], 'topology-name': 'ExclamationTopology1', 'verbose': 'True', 'override_config_file': '/tmp/tmpyd8n4G/override.yaml', 'config_path': '/home/vagrant/.heron/conf/devcluster', 'subcommand': 'activate', 'cluster': 'devcluster', 'role': 'vagrant', 'environ': 'devel'}\n$> /usr/lib/jvm/java-8-openjdk-amd64/bin/java -client -Xmx1g -Dheron.options= -cp /home/vagrant/.heron/lib/scheduler/:/home/vagrant/.heron/lib/statemgr/ com.twitter.heron.scheduler.RuntimeManagerMain --cluster devcluster --role vagrant --environment devel --heron_home /home/vagrant/.heron --config_path /home/vagrant/.heron/conf/devcluster --override_config_file /tmp/tmpyd8n4G/override.yaml --release_file /home/vagrant/.heron/release.yaml --topology_name ExclamationTopology1 --command activate --verbose\n[2016-07-06 22:40:29 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config stream\n[2016-07-06 22:40:29 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config\n[2016-07-06 22:40:29 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config stream\n[2016-07-06 22:40:29 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config\n[2016-07-06 22:40:30 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Config file /home/vagrant/.heron/conf/devcluster/cluster.yaml does not exist\n[2016-07-06 22:40:30 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /home/vagrant/.heron/conf/devcluster/client.yaml\n[2016-07-06 22:40:30 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /home/vagrant/.heron/conf/devcluster/client.yaml\n[2016-07-06 22:40:30 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /home/vagrant/.heron/conf/devcluster/packing.yaml\n[2016-07-06 22:40:30 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /home/vagrant/.heron/conf/devcluster/packing.yaml\n[2016-07-06 22:40:30 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /home/vagrant/.heron/conf/devcluster/scheduler.yaml\n[2016-07-06 22:40:30 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /home/vagrant/.heron/conf/devcluster/scheduler.yaml\n[2016-07-06 22:40:30 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /home/vagrant/.heron/conf/devcluster/statemgr.yaml\n[2016-07-06 22:40:30 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /home/vagrant/.heron/conf/devcluster/statemgr.yaml\n[2016-07-06 22:40:30 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /home/vagrant/.heron/conf/devcluster/uploader.yaml\n[2016-07-06 22:40:30 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /home/vagrant/.heron/conf/devcluster/uploader.yaml\n[2016-07-06 22:40:30 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /home/vagrant/.heron/release.yaml\n[2016-07-06 22:40:30 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /home/vagrant/.heron/release.yaml\n[2016-07-06 22:40:30 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /tmp/tmpyd8n4G/override.yaml\n[2016-07-06 22:40:30 +0000] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /tmp/tmpyd8n4G/override.yaml\n[2016-07-06 22:40:30 +0000] com.twitter.heron.scheduler.RuntimeManagerMain FINE:  Static config loaded successfully \n[2016-07-06 22:40:30 +0000] com.twitter.heron.scheduler.RuntimeManagerMain FINE:  (\"heron.build.git.revision\", be87b09f348e0ed05f45503340a2245a4ef68a35)\n(\"heron.build.git.status\", Modified)\n(\"heron.build.host\", tw-mbp-kramasamy)\n(\"heron.build.time\", Tue May 24 22:52:11 PDT 2016)\n(\"heron.build.timestamp\", 1464155548000)\n(\"heron.build.user\", kramasamy)\n(\"heron.build.version\", 0.14.0)\n(\"heron.class.launcher\", com.twitter.heron.scheduler.aurora.AuroraLauncher)\n(\"heron.class.packing.algorithm\", com.twitter.heron.packing.roundrobin.RoundRobinPacking)\n(\"heron.class.scheduler\", com.twitter.heron.scheduler.aurora.AuroraScheduler)\n(\"heron.class.state.manager\", com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager)\n(\"heron.class.uploader\", com.twitter.heron.uploader.localfs.LocalFileSystemUploader)\n(\"heron.classpath.instance\", /home/vagrant/.heron/lib/instance/)\n(\"heron.classpath.metrics.manager\", /home/vagrant/.heron/lib/metricsmgr/)\n(\"heron.classpath.packing\", /home/vagrant/.heron/lib/packing/)\n(\"heron.classpath.scheduler\", /home/vagrant/.heron/lib/scheduler/)\n(\"heron.classpath.statemgr\", /home/vagrant/.heron/lib/statemgr/)\n(\"heron.classpath.uploader\", /home/vagrant/.heron/lib/uploader/)\n(\"heron.config.cluster\", devcluster)\n(\"heron.config.environ\", devel)\n(\"heron.config.file.client.yaml\", /home/vagrant/.heron/conf/devcluster/client.yaml)\n(\"heron.config.file.cluster.yaml\", /home/vagrant/.heron/conf/devcluster/cluster.yaml)\n(\"heron.config.file.defaults.yaml\", /home/vagrant/.heron/conf/devcluster/defaults.yaml)\n(\"heron.config.file.metrics.yaml\", /home/vagrant/.heron/conf/devcluster/metrics_sinks.yaml)\n(\"heron.config.file.packing.yaml\", /home/vagrant/.heron/conf/devcluster/packing.yaml)\n(\"heron.config.file.scheduler.yaml\", /home/vagrant/.heron/conf/devcluster/scheduler.yaml)\n(\"heron.config.file.statemgr.yaml\", /home/vagrant/.heron/conf/devcluster/statemgr.yaml)\n(\"heron.config.file.system.yaml\", /home/vagrant/.heron/conf/devcluster/heron_internals.yaml)\n(\"heron.config.file.uploader.yaml\", /home/vagrant/.heron/conf/devcluster/uploader.yaml)\n(\"heron.config.is.env.required\", true)\n(\"heron.config.is.role.required\", true)\n(\"heron.config.role\", vagrant)\n(\"heron.config.sandbox.file.cluster.yaml\", ./heron-conf/cluster.yaml)\n(\"heron.config.sandbox.file.defaults.yaml\", ./heron-conf/defaults.yaml)\n(\"heron.config.sandbox.file.metrics.yaml\", ./heron-conf/metrics_sinks.yaml)\n(\"heron.config.sandbox.file.packing.yaml\", ./heron-conf/packing.yaml)\n(\"heron.config.sandbox.file.scheduler.yaml\", ./heron-conf/scheduler.yaml)\n(\"heron.config.sandbox.file.statemgr.yaml\", ./heron-conf/statemgr.yaml)\n(\"heron.config.sandbox.file.system.yaml\", ./heron-conf/heron_internals.yaml)\n(\"heron.config.sandbox.file.uploader.yaml\", ./heron-conf/uploader.yaml)\n(\"heron.config.verbose\", true)\n(\"heron.directory.bin\", /home/vagrant/.heron/bin)\n(\"heron.directory.conf\", /home/vagrant/.heron/conf/devcluster)\n(\"heron.directory.dist\", /home/vagrant/.heron/dist)\n(\"heron.directory.etc\", /home/vagrant/.heron/etc)\n(\"heron.directory.home\", /home/vagrant/.heron)\n(\"heron.directory.java.home\", /usr/lib/jvm/java-8-openjdk-amd64)\n(\"heron.directory.lib\", /home/vagrant/.heron/lib)\n(\"heron.directory.sandbox.bin\", ./heron-core/bin)\n(\"heron.directory.sandbox.conf\", ./heron-conf)\n(\"heron.directory.sandbox.home\", ./heron-core)\n(\"heron.directory.sandbox.java.home\", /usr/lib/jvm/java-1.8.0-openjdk-amd64)\n(\"heron.directory.sandbox.lib\", ./heron-core/lib)\n(\"heron.jars.scheduler\", /home/vagrant/.heron/lib/scheduler/heron-scheduler.jar)\n(\"heron.package.core.uri\", file:///vagrant/.herondata/dist/heron-core-release.tar.gz)\n(\"heron.resources.instance.cpu\", 1.0)\n(\"heron.resources.instance.disk\", 1073741824)\n(\"heron.resources.instance.ram\", 1073741824)\n(\"heron.resources.stmgr.ram\", 1073741824)\n(\"heron.scheduler.is.service\", false)\n(\"heron.statemgr.connection.string\", 127.0.0.1:2181)\n(\"heron.statemgr.root.path\", /heronroot)\n(\"heron.statemgr.zookeeper.connection.timeout.ms\", 30000)\n(\"heron.statemgr.zookeeper.is.initialize.tree\", true)\n(\"heron.statemgr.zookeeper.retry.count\", 10)\n(\"heron.statemgr.zookeeper.retry.interval.ms\", 10000)\n(\"heron.statemgr.zookeeper.session.timeout.ms\", 30000)\n(\"heron.topology.container.id\", -1)\n(\"heron.topology.name\", ExclamationTopology1)\n(\"heron.uploader.localfs.file.system.directory\", /vagrant/.herondata/repository/topologies/devcluster/vagrant/ExclamationTopology1)\n[2016-07-06 22:40:30 +0000] com.twitter.heron.statemgr.FileSystemStateManager FINE:  File system state manager root address: /heronroot\n[2016-07-06 22:40:30 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Starting client to: 127.0.0.1:2181\n[2016-07-06 22:40:30 +0000] org.apache.curator.framework.imps.CuratorFrameworkImpl INFO:  Starting\n[2016-07-06 22:40:30 +0000] org.apache.curator.CuratorZookeeperClient FINE:  Starting\n[2016-07-06 22:40:30 +0000] org.apache.curator.ConnectionState FINE:  Starting\n[2016-07-06 22:40:30 +0000] org.apache.curator.ConnectionState FINE:  reset\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:host.name=aurora.local\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.version=1.8.0_91\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.vendor=Oracle Corporation\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.home=/usr/lib/jvm/java-8-openjdk-amd64/jre\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.class.path=/home/vagrant/.heron/lib/scheduler/heron-slurm-scheduler.jar:/home/vagrant/.heron/lib/scheduler/heron-scheduler.jar:/home/vagrant/.heron/lib/scheduler/heron-aurora-scheduler.jar:/home/vagrant/.heron/lib/scheduler/heron-roundrobin-packing.jar:/home/vagrant/.heron/lib/scheduler/heron-local-scheduler.jar:/home/vagrant/.heron/lib/statemgr/heron-localfs-statemgr.jar:/home/vagrant/.heron/lib/statemgr/heron-zookeeper-statemgr.jar\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib/x86_64-linux-gnu/jni:/lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/usr/lib/jni:/lib:/usr/lib\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.io.tmpdir=/tmp\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.compiler=\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:os.name=Linux\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:os.arch=amd64\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:os.version=3.13.0-88-generic\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:user.name=vagrant\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:user.home=/home/vagrant\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Client environment:user.dir=/home/vagrant\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ZooKeeper INFO:  Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=30000 watcher=org.apache.curator.ConnectionState@23d2a7e8\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ClientCnxn FINE:  zookeeper.disableAutoWatchReset is false\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ClientCnxn INFO:  Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ClientCnxn INFO:  Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ClientCnxn FINE:  Session establishment request sent on 127.0.0.1/127.0.0.1:2181\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ClientCnxnSocket FINEST:  readConnectResult 37 0x[0,0,0,0,0,0,75,30,1,55,ffffffc0,1,35,15,0,2f,0,0,0,10,ffffffd6,ffffffae,ffffffaa,48,fffffff8,6f,fffffffc,ffffff93,18,7e,ffffff92,39,7f,ffffff96,59,ffffff87,0,]\n[2016-07-06 22:40:30 +0000] org.apache.zookeeper.ClientCnxn INFO:  Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x155c0013515002f, negotiated timeout = 30000\n[2016-07-06 22:40:30 +0000] org.apache.curator.framework.state.ConnectionStateManager INFO:  State change: CONNECTED\n[2016-07-06 22:40:31 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Topologies directory: /heronroot/topologies\n[2016-07-06 22:40:31 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: connection-state-parent-process - 53 ms\n[2016-07-06 22:40:31 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Tmaster location directory: /heronroot/tmasters\n[2016-07-06 22:40:31 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Physical plan directory: /heronroot/pplans\n[2016-07-06 22:40:31 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Execution state directory: /heronroot/executionstate\n[2016-07-06 22:40:31 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Scheduler location directory: /heronroot/schedulers\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 1,3  replyHeader:: 1,776,0  request:: '/heronroot,F  response:: s{74,74,1467513024815,1467513024815,0,5,0,0,4,5,83} \n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 2,3  replyHeader:: 2,776,0  request:: '/heronroot/topologies,F  response:: s{75,75,1467513074362,1467513074362,0,21,0,0,4,1,683} \n[2016-07-06 22:40:31 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 54 ms\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 3,3  replyHeader:: 3,776,-101  request:: '/heronroot/topologies/foo,F  response::  \n[2016-07-06 22:40:31 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 27 ms\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 4,3  replyHeader:: 4,776,0  request:: '/heronroot,F  response:: s{74,74,1467513024815,1467513024815,0,5,0,0,4,5,83} \n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 5,3  replyHeader:: 5,776,0  request:: '/heronroot/tmasters,F  response:: s{80,80,1467513214249,1467513214249,0,0,0,0,0,0,80} \n[2016-07-06 22:40:31 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 20 ms\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 6,3  replyHeader:: 6,776,-101  request:: '/heronroot/tmasters/foo,F  response::  \n[2016-07-06 22:40:31 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 4 ms\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 7,3  replyHeader:: 7,776,0  request:: '/heronroot,F  response:: s{74,74,1467513024815,1467513024815,0,5,0,0,4,5,83} \n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 8,3  replyHeader:: 8,776,0  request:: '/heronroot/pplans,F  response:: s{81,81,1467513214306,1467513214306,0,0,0,0,0,0,81} \n[2016-07-06 22:40:31 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 29 ms\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 9,3  replyHeader:: 9,776,-101  request:: '/heronroot/pplans/foo,F  response::  \n[2016-07-06 22:40:31 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 6 ms\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 10,3  replyHeader:: 10,776,0  request:: '/heronroot,F  response:: s{74,74,1467513024815,1467513024815,0,5,0,0,4,5,83} \n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 11,3  replyHeader:: 11,776,0  request:: '/heronroot/executionstate,F  response:: s{82,82,1467513214377,1467513214377,0,21,0,0,0,1,684} \n[2016-07-06 22:40:31 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 14 ms\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 12,3  replyHeader:: 12,776,-101  request:: '/heronroot/executionstate/foo,F  response::  \n[2016-07-06 22:40:31 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 17 ms\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 13,3  replyHeader:: 13,776,0  request:: '/heronroot,F  response:: s{74,74,1467513024815,1467513024815,0,5,0,0,4,5,83} \n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 14,3  replyHeader:: 14,776,0  request:: '/heronroot/schedulers,F  response:: s{83,83,1467513214432,1467513214432,0,0,0,0,0,0,83} \n[2016-07-06 22:40:31 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 5 ms\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 15,3  replyHeader:: 15,776,-101  request:: '/heronroot/schedulers/foo,F  response::  \n[2016-07-06 22:40:31 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 3 ms\n[2016-07-06 22:40:31 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Directory tree initialized.\n[2016-07-06 22:40:31 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Checking existence of path: /heronroot/topologies/ExclamationTopology1\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 16,3  replyHeader:: 16,776,0  request:: '/heronroot/topologies/ExclamationTopology1,F  response:: s{683,683,1467805353192,1467805353192,0,0,0,0,1278,0,683} \n[2016-07-06 22:40:31 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 3 ms\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:/heronroot/executionstate/ExclamationTopology1 serverPath:/heronroot/executionstate/ExclamationTopology1 finished:false header:: 17,4  replyHeader:: 17,776,0  request:: '/heronroot/executionstate/ExclamationTopology1,F  response:: #a144578636c616d6174696f6e546f706f6c6f67793112384578636c616d6174696f6e546f706f6c6f67793133303863643137642d636434392d346565352d613335362d38613961636666363532613818ffffffa9ffffffddfffffff3ffffffbb522776616772616e742aa646576636c7573746572325646576656c3a776616772616e744215a96b72616d6173616d791201a6302e31342e30,s{684,684,1467805353232,1467805353232,0,0,0,0,146,0,684} \n[2016-07-06 22:40:31 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: GetDataBuilderImpl-Background - 29 ms\n[2016-07-06 22:40:31 +0000] com.twitter.heron.scheduler.RuntimeManagerMain FINE:  Topology: ExclamationTopology1 to be ACTIVATEed\n[2016-07-06 22:40:31 +0000] com.twitter.heron.scheduler.client.SchedulerClientFactory FINE:  Creating scheduler client\n[2016-07-06 22:40:31 +0000] com.twitter.heron.scheduler.client.SchedulerClientFactory FINE:  Invoke scheduler as a library\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:/heronroot/pplans/ExclamationTopology1 serverPath:/heronroot/pplans/ExclamationTopology1 finished:false header:: 18,4  replyHeader:: 18,776,-101  request:: '/heronroot/pplans/ExclamationTopology1,F  response::  \n[2016-07-06 22:40:31 +0000] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: GetDataBuilderImpl-Background - 19 ms\n[2016-07-06 22:40:31 +0000] com.twitter.heron.spi.statemgr.SchedulerStateManagerAdaptor SEVERE:  Exception processing future\njava.util.concurrent.ExecutionException: java.lang.RuntimeException: Failed to fetch data from path: /heronroot/pplans/ExclamationTopology1\n    at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)\n    at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:272)\n    at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:96)\n    at com.twitter.heron.spi.statemgr.SchedulerStateManagerAdaptor.awaitResult(SchedulerStateManagerAdaptor.java:71)\n    at com.twitter.heron.spi.statemgr.SchedulerStateManagerAdaptor.awaitResult(SchedulerStateManagerAdaptor.java:63)\n    at com.twitter.heron.spi.statemgr.SchedulerStateManagerAdaptor.getPhysicalPlan(SchedulerStateManagerAdaptor.java:207)\n    at com.twitter.heron.spi.utils.TMasterUtils.getRuntimeTopologyState(TMasterUtils.java:100)\n    at com.twitter.heron.spi.utils.TMasterUtils.transitionTopologyState(TMasterUtils.java:115)\n    at com.twitter.heron.scheduler.RuntimeManagerRunner.activateTopologyHandler(RuntimeManagerRunner.java:78)\n    at com.twitter.heron.scheduler.RuntimeManagerRunner.call(RuntimeManagerRunner.java:56)\n    at com.twitter.heron.scheduler.RuntimeManagerMain.callRuntimeManagerRunner(RuntimeManagerMain.java:369)\n    at com.twitter.heron.scheduler.RuntimeManagerMain.manageTopology(RuntimeManagerMain.java:326)\n    at com.twitter.heron.scheduler.RuntimeManagerMain.main(RuntimeManagerMain.java:251)\nCaused by: java.lang.RuntimeException: Failed to fetch data from path: /heronroot/pplans/ExclamationTopology1\n    at com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager$1.processResult(CuratorStateManager.java:236)\n    at org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:743)\n    at org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:520)\n    at org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:254)\n    at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:564)\n    at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)\n[2016-07-06 22:40:31 +0000] com.twitter.heron.spi.utils.TMasterUtils SEVERE:  Failed to get physical plan for topology ExclamationTopology1\n[2016-07-06 22:40:31 +0000] com.twitter.heron.spi.utils.TMasterUtils SEVERE:  Topology still not initialized.\n[2016-07-06 22:40:31 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Closing the CuratorClient to: 127.0.0.1:2181\n[2016-07-06 22:40:31 +0000] org.apache.curator.framework.imps.CuratorFrameworkImpl FINE:  Closing\n[2016-07-06 22:40:31 +0000] org.apache.curator.CuratorZookeeperClient FINE:  Closing\n[2016-07-06 22:40:31 +0000] org.apache.curator.ConnectionState FINE:  Closing\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ZooKeeper FINE:  Closing session: 0x155c0013515002f\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Closing client for session: 0x155c0013515002f\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x155c0013515002f, packet:: clientPath:null serverPath:null finished:false header:: 19,-11  replyHeader:: 19,777,0  request:: null response:: null\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  An exception was thrown while closing send thread for session 0x155c0013515002f : Unable to read additional data from server sessionid 0x155c0013515002f, likely server has closed socket\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn FINE:  Disconnecting client for session: 0x155c0013515002f\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ClientCnxn INFO:  EventThread shut down\n[2016-07-06 22:40:31 +0000] org.apache.zookeeper.ZooKeeper INFO:  Session: 0x155c0013515002f closed\n[2016-07-06 22:40:31 +0000] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Closing the tunnel processes\nException in thread \"main\" java.lang.RuntimeException: Failed to ACTIVATE topology ExclamationTopology1\n    at com.twitter.heron.scheduler.RuntimeManagerMain.main(RuntimeManagerMain.java:255)\nError: User main failed with status 1. Bailing out...\nERROR: Failed to activate topology 'ExclamationTopology1'\nINFO: Elapsed time: 3.330s.\n```\nI am not sure though if it is happening because of insufficient RAM message in the aurora scheduler UI or some permission error.\n. ",
    "mycFelix": "@zhangzhonglai hi, I encountered the same problem about the pex get an incorrect $HOME env for yarn user on Yarn mode. \nHave you found a way to solve it or a correct configuration to make it right?\n. @zhangzhonglai - Hi, thank you for your reply and helpful detail informations. I will review my configuration again and do more tests.\n. Thank you @kramasamy. And Thank @ashvina AGAIN for committing the YARN mode code.\nOur Cluster has 8 node with CentOS 7 System and Hadoop-2.7.1 .\nThe scheduler.yaml is following the default config:\n```\nscheduler class for distributing the topology for execution\nheron.class.scheduler:                       com.twitter.heron.scheduler.yarn.YarnScheduler\nlauncher class for submitting and launching the topology\nheron.class.launcher:                        com.twitter.heron.scheduler.yarn.YarnLauncher\nworking directory for the topologies\nheron.scheduler.local.working.directory:     ${HOME}/.herondata/topologies/${CLUSTER}/${ROLE}/${TOPOLOGY}\nlocation of java - pick it up from shell environment\nheron.directory.sandbox.java.home:           ${JAVA_HOME}\n```\nThe statemgr.yaml is also default conifg:\n```\nlocal state manager class for managing state in a persistent fashion\nheron.class.state.manager:        com.twitter.heron.statemgr.localfs.LocalFileSystemStateManager\nlocal state manager connection string\nheron.statemgr.connection.string: LOCALMODE\npath of the root address to store the state in a local file system\nheron.statemgr.root.path:         ${HOME}/.herondata/repository/state/${CLUSTER}\n```\nMy topology is very simple WordCount. the core-site.xml,hdfs-site.xml,yarn-site.xml was located on the resouce directory,and was packaged in the jar.\n```\n.\n|_core-site.xml\n|_hdfs-site.xml\n|_heron\n| |_wordcount\n| | |_bolts\n| | | |_WordCounter.class\n| | | |_WordNormalizer.class\n| | |_spouts\n| | | |_WordReader.class\n| | |_Topologies\n| | | |_TopologyMain.class\n|_yarn-site.xml\n```\nWhen i submit a topology using command:\nheron submit yarn ~/heron-jars/heron-study.jar heron.wordcount.Topologies.TopologyMain Getting-Started-Topologie\nAll the logs would like to be(removing ip details):\n```\n[2016-07-14 14:07:42 +0800] com.twitter.heron.scheduler.yarn.YarnLauncher Info:  Initializing topology: Getting-Started-Topologie, core: /home/miaoyichen/.heron/dist/heron-core.tar.gz\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/home/miaoyichen/.heron/lib/scheduler/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/home/miaoyichen/.heron/lib/statemgr/heron-zookeeper-statemgr.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n2016-07-14 14:07:43,269 DEBUG org.apache.hadoop.security.authentication.util.KerberosName  main Kerberos krb5 configuration not found, setting default realm to empty\n[2016-07-14 14:07:43 +0800] org.apache.hadoop.util.NativeCodeLoader Warn:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2016-07-14 14:07:43,275 DEBUG org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback  main Falling back to shell based\n2016-07-14 14:07:43,668 DEBUG org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory  main Both short-circuit local reads and UNIX domain socket are disabled.\n2016-07-14 14:07:43,673 DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil getSaslPropertiesResolver main DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection\nPowered by\n     __    _\n    /    / /  / /  / /  /\n   /     _/ /  /  /  /  /  /\n  /  /\\  \\     /  / /  / /  _/\n /  /  \\  \\   /  /  /  /  /  /\n//    _\\ /_/ /___/ /__/\n2016-07-14 14:07:43,688 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main Using SLF4J as the default logging framework\n2016-07-14 14:07:43,694 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main java.nio.Buffer.address: available\n2016-07-14 14:07:43,694 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main sun.misc.Unsafe.theUnsafe: available\n2016-07-14 14:07:43,694 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main sun.misc.Unsafe.copyMemory: available\n2016-07-14 14:07:43,695 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main java.nio.Bits.unaligned: true\n2016-07-14 14:07:43,697 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main UID: 1028\n2016-07-14 14:07:43,698 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main Java version: 7\n2016-07-14 14:07:43,698 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main -Dio.netty.noUnsafe: false\n2016-07-14 14:07:43,698 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main sun.misc.Unsafe: available\n2016-07-14 14:07:43,698 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main -Dio.netty.noJavassist: false\n2016-07-14 14:07:43,756 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main Javassist: available\n2016-07-14 14:07:43,756 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main -Dio.netty.tmpdir: /tmp (java.io.tmpdir)\n2016-07-14 14:07:43,757 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main -Dio.netty.bitMode: 64 (sun.arch.data.model)\n2016-07-14 14:07:43,757 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main -Dio.netty.noPreferDirect: false\n2016-07-14 14:07:43,769 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main -Dio.netty.eventLoopThreads: 64\n2016-07-14 14:07:43,772 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main -Dio.netty.noKeySetOptimization: false\n2016-07-14 14:07:43,773 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main -Dio.netty.selectorAutoRebuildThreshold: 512\n2016-07-14 14:07:43,790 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main -Dio.netty.initialSeedUniquifier: 0x8fa128a55ac653e6 (took 0 ms)\n2016-07-14 14:07:43,805 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main -Dio.netty.allocator.type: unpooled\n2016-07-14 14:07:43,805 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main -Dio.netty.threadLocalDirectBufferSize: 65536\n2016-07-14 14:07:43,807 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main Loopback interface: lo (lo, 0:0:0:0:0:0:0:1%1)\n2016-07-14 14:07:43,807 DEBUG io.netty.util.internal.logging.Slf4JLogger debug main /proc/sys/net/core/somaxconn: 32768\n[2016-07-14 14:07:43 +0800] org.apache.reef.util.REEFVersion Info:  REEF Version: 0.14.0\n[2016-07-14 14:07:43 +0800] com.twitter.heron.scheduler.yarn.ReefClientSideHandlers Info:  Initializing REEF client handlers for Heron, topology: Getting-Started-Topologie\n[2016-07-14 14:07:43 +0800] org.apache.hadoop.yarn.client.RMProxy Info:  Connecting to ResourceManager at xx.xx.xx.xx:10832\n[2016-07-14 14:07:46 +0800] org.apache.reef.runtime.common.files.JobJarMaker Warn:  Failed to delete [/tmp/reef-job-3818779904304115363]\n2016-07-14 14:07:46,305 DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient send Thread-6 SASL client skipping handshake in unsecured configuration for addr = /10.209.19.142, datanodeId = DatanodeInfoWithStorage[10.209.19.142:10510,DS-3b18e335-e51a-40a0-b703-f9adfef5f894,DISK]\n2016-07-14 14:07:46,940 DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil getSaslPropertiesResolver main DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection\n[2016-07-14 14:07:46 +0800] org.apache.reef.runtime.yarn.client.YarnSubmissionHelper Info:  Submitting REEF Application to YARN. ID: application_1468466724671_0002\n[2016-07-14 14:07:46 +0800] org.apache.hadoop.yarn.client.api.impl.YarnClientImpl Info:  Submitted application application_1468466724671_0002\n2016-07-14 14:07:49,901 DEBUG io.netty.util.internal.logging.Slf4JLogger debug org.apache.reef.wake.remote.transport.netty.NettyMessagingTransportServerWorker-pool-4-thread-1 Generated: io.netty.util.internal.matchers.io.netty.buffer.ByteBufMatcher\n2016-07-14 14:07:49,906 DEBUG io.netty.util.internal.logging.Slf4JLogger debug org.apache.reef.wake.remote.transport.netty.NettyMessagingTransportServerWorker-pool-4-thread-1 Generated: io.netty.util.internal.matchers.byte[]Matcher\n2016-07-14 14:07:49,997 DEBUG io.netty.util.internal.logging.Slf4JLogger debug org.apache.reef.wake.remote.transport.netty.NettyMessagingTransportServerWorker-pool-4-thread-1 -Dio.netty.leakDetectionLevel: simple\n2016-07-14 14:07:50,003 DEBUG io.netty.util.internal.logging.Slf4JLogger debug org.apache.reef.wake.remote.transport.netty.NettyMessagingTransportServerWorker-pool-4-thread-1 -Dio.netty.recycler.maxCapacity.default: 262144\n2016-07-14 14:07:50,005 DEBUG io.netty.util.internal.logging.Slf4JLogger debug org.apache.reef.wake.remote.transport.netty.NettyMessagingTransportServerWorker-pool-4-thread-1 java.nio.ByteBuffer.cleaner(): available\n[2016-07-14 14:07:50 +0800] com.twitter.heron.scheduler.yarn.ReefClientSideHandlers Info:  Topology Getting-Started-Topologie is running, jobId Getting-Started-Topologie.\n```\nAnd What important is It looks like the shell was not terminated,still waiting for some signals.\nAnd The LOCAL repository/state/ would be:\n```\n[miaoyichen@CDM1C10-209019142 yarn]$ pwd\n/home/miaoyichen/.herondata/repository/state/yarn\n[miaoyichen@CDM1C10-209019142 yarn]$ ll ./*\n./executionstate:\ntotal 4\n-rw-r--r-- 1 miaoyichen miaoyichen 158 July  14 17:39 Getting-Started-Topologie\n./pplans:\ntotal 0\n./schedulers:\ntotal 0\n./tmasters:\ntotal 0\n./topologies:\ntotal 4\n-rw-r--r-- 1 miaoyichen miaoyichen 1304 July  14 17:39 Getting-Started-Topologie\n```\n@ashvina if you need more detail informations, please let me know. It will make a great deal for me to get your help. \n. Hi @ashvina .Thank you for your guide.\nI've done what you told me. The line about ERROR log Error: YarnConfiguration.YARN_APPLICATION_CLASSPATH is empty. was disappeared. But the topo has not started yet\u2026\u2026\u2026\u2026\nHere's the new driver.stderr log\nJuly 15, 2016 9:25:49 \u4e0a\u5348 org.apache.reef.runtime.common.REEFLauncher main\nInfo: Entering REEFLauncher.main().\nJuly 15, 2016 9:25:49 \u4e0a\u5348 org.apache.reef.util.REEFVersion logVersion\nInfo: REEF Version: 0.14.0\nJuly 15, 2016 9:25:50 \u4e0a\u5348 org.apache.hadoop.yarn.client.RMProxy createRMProxy\nInfo: Connecting to ResourceManager at xx.xx.xx.xx:10832\nJuly 15, 2016 9:25:50 \u4e0a\u5348 org.apache.hadoop.yarn.client.RMProxy createRMProxy\nInfo: Connecting to ResourceManager at xx.xx.xx.xx:8030\nJuly 15, 2016 9:25:50 \u4e0a\u5348 org.apache.hadoop.yarn.client.api.async.impl.NMClientAsyncImpl serviceInit\nInfo: Upper bound of the thread pool size is 500\nJuly 15, 2016 9:25:50 \u4e0a\u5348 org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy <init>\nInfo: yarn.client.max-cached-nodemanagers-proxies : 0\nJuly 15, 2016 9:25:51 \u4e0a\u5348 com.twitter.heron.scheduler.yarn.HeronReefUtils extractPackageInSandbox\nInfo: Extracting package: reef/global/topology.tar.gz at: .\ntar: ./heron-conf/metrics_sinks.yaml\uff1a 1970-01-01 08:00:00\ntar: ./heron-conf/client.yaml\uff1a 1970-01-01 08:00:00\ntar: ./heron-conf/heron_internals.yaml\uff1a 1970-01-01 08:00:00\ntar: ./heron-conf/release.yaml\uff1a 1970-01-01 08:00:00\nJuly 15, 2016 9:25:51 \u4e0a\u5348 com.twitter.heron.spi.utils.ShellUtils runSyncProcess\nError:  STDERR: \nJuly 15, 2016 9:25:51 \u4e0a\u5348 com.twitter.heron.scheduler.yarn.HeronReefUtils extractPackageInSandbox\nInfo: Extracting package: reef/global/heron-core.tar.gz at: .\ntar: ./release.yaml\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/bin/heron-executor\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/bin/heron-shell\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/bin/heron-stmgr\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/bin/heron-tmaster\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/bin\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/lib/scheduler/heron-scheduler.jar\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/lib/scheduler/heron-local-scheduler.jar\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/lib/scheduler/heron-slurm-scheduler.jar\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/lib/scheduler\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/lib/packing/heron-roundrobin-packing.jar\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/lib/packing\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/lib/metricsmgr/heron-metricsmgr.jar\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/lib/metricsmgr\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/lib/statemgr/heron-localfs-statemgr.jar\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/lib/statemgr/heron-zookeeper-statemgr.jar\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/lib/statemgr\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/lib/instance/heron-instance.jar\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/lib/instance\uff1a 1970-01-01 08:00:00\ntar: ./heron-core/lib\uff1a 1970-01-01 08:00:00\ntar: ./heron-core\uff1a 1970-01-01 08:00:00\ntar: .\uff1a 1970-01-01 08:00:00\nJuly 15, 2016 9:25:51 \u4e0a\u5348 com.twitter.heron.spi.utils.ShellUtils runSyncProcess\nError:  STDERR: \nJuly 15, 2016 9:25:51 \u4e0a\u5348 com.twitter.heron.scheduler.yarn.HeronMasterDriver$HeronSchedulerLauncher launchScheduler\nInfo: Launching Heron scheduler\nThe /home/miaoyichen/.herondata/repository/state/yarn directory's situation is still same with last time.\n. hi @ashvina .\ni finally know where the mistake is . There are some limits of authority problems on our cluster. So i  was failed when i choose LOCAL MODE on statemgr.yaml. \ni turned it to ZK MODE. IT WORKS!!!!!!!!!!\nThank you so much AGAIN!!!\nAnd Thank you @kramasamy \n. @ashvina - sure. i'm happy to do that\n. @kramasamy - thank you for your reply. I've started heron-tracker and heron-ui and i can see the Logical plan and Physical plan of the topology. I'm wondering if there has a way to check or change the status of topology from Heron UI, such as activated or deactivate, at less check. It would be more convenient than executing some shell commands. \n. @kramasamy - looking forward that feature. Thank you and please close my issue.\n. so.... I need to execute 'chmod 777 /home/.pex'? \nIs there any other way?\n. @ashvina - thank you for your reply. I didn't notice #1010 issue. It seems we get the same problem on incorrect $HOME env.  I will change admin user accounts and try it again. I hope this problem could be fixed in the next version. I will close this issue. Thank you AGAIN.\n. @ashvina - I installed heron on admin user account and tried it again. And I got the same error when I ran topo on Yarn mode.\nThe $HERON_HOME is located at /usr/local/heron and my hdfs user $HOME is /home/hdfs.\nmy shell command is following:\n[hdfs ~]$ heron submit yarn /usr/local/heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology\nThe error log is following:\n2016-07-19 14:49:25: Running heron-shell-1 process as ./heron-core/bin/heron-shell --port=48529 --log_file_prefix=log-files/heron-shell.log\n2016-07-19 14:49:25: Logging pid 30749 to file heron-shell-1.pid\n2016-07-19 14:49:25: heron-shell-1 exited with status 256\n2016-07-19 14:49:25: heron-shell-1 stdout:\n2016-07-19 14:49:25: heron-shell-1 stderr: Traceback (most recent call last):\n  File \"/data9/yarn/local/usercache/hdfs/appcache/application_1468545858776_0023/container_1468545858776_0023_01_000003/heron-core/bin/heron-shell/.bootstrap/_pex/pex.py\", line 313, in execute\n  File \"/data9/yarn/local/usercache/hdfs/appcache/application_1468545858776_0023/container_1468545858776_0023_01_000003/heron-core/bin/heron-shell/.bootstrap/_pex/pex.py\", line 78, in _activate\n  File \"/data9/yarn/local/usercache/hdfs/appcache/application_1468545858776_0023/container_1468545858776_0023_01_000003/heron-core/bin/heron-shell/.bootstrap/_pex/environment.py\", line 132, in activate\n  File \"/data9/yarn/local/usercache/hdfs/appcache/application_1468545858776_0023/container_1468545858776_0023_01_000003/heron-core/bin/heron-shell/.bootstrap/_pex/environment.py\", line 176, in _activate\n  File \"/data9/yarn/local/usercache/hdfs/appcache/application_1468545858776_0023/container_1468545858776_0023_01_000003/heron-core/bin/heron-shell/.bootstrap/_pex/environment.py\", line 121, in update_candidate_distributions\n  File \"/data9/yarn/local/usercache/hdfs/appcache/application_1468545858776_0023/container_1468545858776_0023_01_000003/heron-core/bin/heron-shell/.bootstrap/_pex/environment.py\", line 107, in load_internal_cache\n  File \"/data9/yarn/local/usercache/hdfs/appcache/application_1468545858776_0023/container_1468545858776_0023_01_000003/heron-core/bin/heron-shell/.bootstrap/_pex/environment.py\", line 95, in write_zipped_internal_cache\n  File \"/data9/yarn/local/usercache/hdfs/appcache/application_1468545858776_0023/container_1468545858776_0023_01_000003/heron-core/bin/heron-shell/.bootstrap/_pex/util.py\", line 174, in cache_distribution\n  File \"/data9/yarn/local/usercache/hdfs/appcache/application_1468545858776_0023/container_1468545858776_0023_01_000003/heron-core/bin/heron-shell/.bootstrap/_pex/common.py\", line 115, in safe_open\n  File \"/data9/yarn/local/usercache/hdfs/appcache/application_1468545858776_0023/container_1468545858776_0023_01_000003/heron-core/bin/heron-shell/.bootstrap/_pex/common.py\", line 103, in safe_mkdir\n  File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs\n    makedirs(head, mode)\n  File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs\n    makedirs(head, mode)\n  File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs\n    makedirs(head, mode)\n  File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs\n    makedirs(head, mode)\n  File \"/usr/lib64/python2.7/os.py\", line 157, in makedirs\n    mkdir(name, mode)\nOSError: [Errno 13] Permission denied: '/home/.pex'\n. @maosongfu - hi\nI installed Heron command is following:\nsudo ./home/hdfs/heron-install/heron-client-install-0.14.1-centos.sh\n[hdfs ~]$ which heron\n/usr/local/bin/heron\nAnd the $HERON_HOME is : \n[hdfs ~]$ ll /usr/local/heron/\ntotal 28\ndrwxr-xr-x 2 root root 4096 Jan   1 1970 bin\ndrwxr-xr-x 6 root root 4096 Jan   1 1970 conf\ndrwxr-xr-x 2 root root 4096 Jan   1 1970 dist\ndrwxr-xr-x 2 root root 4096 Jan  19 14:32 etc\ndrwxr-xr-x 2 root root 4096 Jan   1 1970 examples\ndrwxr-xr-x 7 root root 4096 Jan   1 1970 lib\n-rwxr-xr-x 1 root root  280 Jan   1 1970 release.yaml\nIs this right...?\n. I tried,but failed. That's why i write an issue.\nI installed Heron using following command on hdfs or miaoyichen user accont:\n./home/hdfs/heron-install/heron-client-install-0.14.1-centos.sh --user\nI got an error about '/home/.pex' when the topo tried to start heron-shell.\n@ashvina told me that it was a problem about the non admin user accounts yesterday. But I've followed the guide,but failed too.\n. @maosongfu - more details\nThere are part of logs after i submited a topo:\n```\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\n[2016-07-19 15:38:43 +0800] org.apache.hadoop.util.NativeCodeLoader Warn:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nPowered by\n     __    _\n    /    / /  / /  / /  /\n   /     _/ /  /  /  /  /  /\n  /  /\\  \\     /  / /  / /  _/\n /  /  \\  \\   /  /  /  /  /  /\n//    _\\ /_/ /___/ /__/\n[2016-07-19 15:38:44 +0800] org.apache.reef.util.REEFVersion Info:  REEF Version: 0.14.0\n[2016-07-19 15:38:44 +0800] com.twitter.heron.scheduler.yarn.ReefClientSideHandlers Info:  Initializing REEF client handlers for Heron, topology: ExclamationTopology\n[2016-07-19 15:38:44 +0800] org.apache.hadoop.yarn.client.RMProxy Info:  Connecting to ResourceManager at CDM1C09-209019061.wdds.com/10.209.19.61:10832\n[2016-07-19 15:38:46 +0800] org.apache.reef.runtime.common.files.JobJarMaker Warn:  Failed to delete [/tmp/reef-job-8347081899682646607]\n[2016-07-19 15:38:47 +0800] org.apache.reef.runtime.yarn.client.YarnSubmissionHelper Info:  Submitting REEF Application to YARN. ID: application_1468545858776_0024\n[2016-07-19 15:38:47 +0800] org.apache.hadoop.yarn.client.api.impl.YarnClientImpl Info:  Submitted application application_1468545858776_0024\n[2016-07-19 15:38:50 +0800] com.twitter.heron.scheduler.yarn.ReefClientSideHandlers Info:  Topology ExclamationTopology is running, jobId ExclamationTopology.\n[2016-07-19 15:38:50 +0800] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager Info:  Closing the CuratorClient to: CDM1C09-209019064.wdds.com:10218\n2016-07-19 15:38:50,328 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x155dd32e71da634 closed\n2016-07-19 15:38:50,328 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down\n[2016-07-19 15:38:50 +0800] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager Info:  Closing the tunnel processes\n```\nThe last log I got is Closing the tunnel processes . The program seems to have hung up and wait for some signal to close, even all the executor has been killed.\n```\nFile \"/usr/lib64/python2.7/os.py\", line 150, in makedirs\n    makedirs(head, mode)\n  File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs\n    makedirs(head, mode)\n  File \"/usr/lib64/python2.7/os.py\", line 157, in makedirs\n    mkdir(name, mode)\nOSError: [Errno 13] Permission denied: '/home/.pex'\n2016-07-19 15:56:05: heron-shell-1 exited too many times\n2016-07-19 15:56:05: Executor terminated; exiting all process in executor.\n``\n. @ashvina Thanks you very much. I'll do some other tests on YARN mode.\n. Hi @ashvina , I can run mapreduce or spark job on our Yarn cluster usinghdfsormiaoyichen` user. \nI tried it again using hdfs user. The topo was running correctly and metrics info was showed on UI. I do have the log-files,one of container's log is following:\n[hdfs@CDM1C10-209019142 log-files]$ tail container_1_exclaim1_1.log.0\n[2016-07-20 10:43:08 +0800] stdout STDOUT:  Bolt processed 123700000 tuples in 88031 ms\n[2016-07-20 10:43:08 +0800] stdout STDOUT:  Bolt processed 123800000 tuples in 88093 ms\n[2016-07-20 10:43:08 +0800] stdout STDOUT:  Bolt processed 123900000 tuples in 88168 ms\n[2016-07-20 10:43:08 +0800] stdout STDOUT:  Bolt processed 124000000 tuples in 88245 ms\n[2016-07-20 10:43:08 +0800] stdout STDOUT:  Bolt processed 124100000 tuples in 88318 ms\n[2016-07-20 10:43:08 +0800] stdout STDOUT:  Bolt processed 124200000 tuples in 88395 ms\n[2016-07-20 10:43:08 +0800] stdout STDOUT:  Bolt processed 124300000 tuples in 88471 ms\n[2016-07-20 10:43:08 +0800] stdout STDOUT:  Bolt processed 124400000 tuples in 88547 ms\n[2016-07-20 10:43:08 +0800] stdout STDOUT:  Bolt processed 124500000 tuples in 88622 ms\n[2016-07-20 10:43:08 +0800] stdout STDOUT:  Bolt processed 124600000 tuples in 88685 ms\nBut I failed to open every instance's log&job page,and all executor terminated after running a while,maybe tens of minutes, cause the heron-shell didn't  started. \n. Thank @ashvina and @maosongfu AGAIN!\nThe situation is I installed Heron using hdfs user which is not admin user account following  @maosongfu 's guide. hdfs user's $HOME and $HERON_HOME is following:\n[hdfs@CDM1C10-209019142 ~]$ echo $HOME\n/home/hdfs\n[hdfs@CDM1C10-209019142 ~]$ echo $HERON_HOME\n/home/hdfs/.heron\nBut It seems what Heron-shell finds is /home/.pex,following heron-executor.stdout logs:\npex/common.py\", line 103, in safe_mkdir\n  File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs\n    makedirs(head, mode)\n  File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs\n    makedirs(head, mode)\n  File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs\n    makedirs(head, mode)\n  File \"/usr/lib64/python2.7/os.py\", line 150, in makedirs\n    makedirs(head, mode)\n  File \"/usr/lib64/python2.7/os.py\", line 157, in makedirs\n    mkdir(name, mode)\nOSError: [Errno 13] Permission denied: '/home/.pex'\nSo the Heron-shell failed to start. \nI think that maybe $HOME/.pex,means /home/hdfs/.pex,  is the right path should be found by Heron-shell,not /home/.pex.\n. @maosongfu @ashvina  - one more basic question.\nDo i need to install heron on each node of our cluster using same user account,or just one node to be a client?\n. Sounds great!!! \nLooking forward it~~~\n. GREAT!!!\n. @kramasamy - hi~ I'm using storm core api not trident api. \n- About the first question,NoClassDefFoundError. Maybe the logs show more details. Please notice this part:\nat storm.kafka.trident.GlobalPartitionInformation.getOrderedPartitions(GlobalPartitionInformation.java:54)\n    at storm.kafka.KafkaUtils.calculatePartitionsForTask(KafkaUtils.java:214)\n    at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:80)\n    at storm.kafka.ZkCoordinator.getMyManagedPartitions(ZkCoordinator.java:69)\n    at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:135)\nThe storm.kafka.KafkaUtils class invoked calculatePartitionsForTask method. And\nthe GlobalPartitionInformation class belongs to the package storm.kafka.trident,which aims to find  kafka partition informations.The ISpoutPartition interface,belongs to the package storm.trident.spout, was invoked by GlobalPartitionInformation.getOrderedPartitions method.\npublic class GlobalPartitionInformation implements Iterable<Partition>, Serializable {\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\npublic List<Partition> getOrderedPartitions() {\n        List<Partition> partitions = new LinkedList<Partition>();\n        for (Map.Entry<Integer, Broker> partition : partitionMap.entrySet()) {\n            partitions.add(new Partition(partition.getValue(), partition.getKey()));\n        }\n        return partitions;\n    }\n```\npackage storm.kafka;\nimport com.google.common.base.Objects;\nimport storm.trident.spout.ISpoutPartition;\npublic class Partition implements ISpoutPartition {\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\n```\nThe storm-kafka-0.10.0 jar is included in my assembly.xml,storm-core isn't. \n<include>org.apache.storm:storm-kafka</include>\n- About the second question. it will cause null exceptions when creates CuratorFramework object in ZkState and DynamicPartitionConnections class if the configurations about the zookeeper were not set. The following source code\uff1a\n```\npackage storm.kafka;\npublic class ZkState {\n    public static final Logger LOG = LoggerFactory.getLogger(ZkState.class);\n    CuratorFramework _curator;\nprivate CuratorFramework newCurator(Map stateConf) throws Exception {\n    Integer port = (Integer) stateConf.get(Config.TRANSACTIONAL_ZOOKEEPER_PORT);\n    String serverPorts = \"\";\n    for (String server : (List<String>) stateConf.get(Config.TRANSACTIONAL_ZOOKEEPER_SERVERS)) {\n        serverPorts = serverPorts + server + \":\" + port + \",\";\n    }\n\n    /*\n     must set,or it will cause null exceptions in heron\n     */\n    return CuratorFrameworkFactory.newClient(serverPorts,\n            Utils.getInt(stateConf.get(Config.STORM_ZOOKEEPER_SESSION_TIMEOUT)),\n            15000,\n            new RetryNTimes(Utils.getInt(stateConf.get(Config.STORM_ZOOKEEPER_RETRY_TIMES)),\n                    Utils.getInt(stateConf.get(Config.STORM_ZOOKEEPER_RETRY_INTERVAL))));\n}\n\n```\n```\npackage storm.kafka;\npublic class DynamicBrokersReader {\n    public static final Logger LOG = LoggerFactory.getLogger(DynamicBrokersReader.class);\n    private CuratorFramework _curator;\n    private String _zkPath;\n    private String _topic;\npublic DynamicBrokersReader(Map conf, String zkStr, String zkPath, String topic) {\n    Preconditions.checkNotNull(conf, \"conf cannot be null\");\n    /*\n     must set,or it will cause null exceptions in heron\n     */\n    this.validateConfig(conf);\n    Preconditions.checkNotNull(zkStr, \"zkString cannot be null\");\n    Preconditions.checkNotNull(zkPath, \"zkPath cannot be null\");\n    Preconditions.checkNotNull(topic, \"topic cannot be null\");\n    this._zkPath = zkPath;\n    this._topic = topic;\n\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\nprivate void validateConfig(Map conf) {\n        Preconditions.checkNotNull(conf.get(\"storm.zookeeper.session.timeout\"), \"%s cannot be null\", new Object[]{\"storm.zookeeper.session.timeout\"});\n        Preconditions.checkNotNull(conf.get(\"storm.zookeeper.connection.timeout\"), \"%s cannot be null\", new Object[]{\"storm.zookeeper.connection.timeout\"});\n        Preconditions.checkNotNull(conf.get(\"storm.zookeeper.retry.times\"), \"%s cannot be null\", new Object[]{\"storm.zookeeper.retry.times\"});\n        Preconditions.checkNotNull(conf.get(\"storm.zookeeper.retry.interval\"), \"%s cannot be null\", new Object[]{\"storm.zookeeper.retry.interval\"});\n    }\n```\n. @maosongfu - Thank you for taking your time.\nLike what you said,it is more an issue of storm kafka-spout. Maybe I'm not the only person who met those problems.\nI think it should be considered carefully before any decision is made,cause one of the best advantages of heron for most developers is upgrading from storm unpainful.\nIf there has something the community has to do, heron-kafka-spout probably is the best option. \nHowever,it is my personal view.There might be a lot of kafka-spout users and their voice should be considered too.\n. @kramasamy - Thank you. \nI would like to but I'm afraid that i couldn't handle it.\nI'm very happy to be the first one to test this function.\n. @objmagic - Yes. I would like to. It is early in the morning in my country, around 04:00am. The pom is in my company's computer. I will upload it as soon as I get work.\n. @objmagic - Hi, sorry for your waiting.\nThere would be more details might help you to reproduce problem on heron-study, and the pom.xml could be found on pom. Please let me know if there is anything I could help.\nThank you for your invitation.It's my honor. My e-mail address:**** \n. @nlu90 -- Hi\uff0cIt means heron-kafka-spout has been patched in heron-api.jar or heron-storm.jar?\n. @kramasamy @nlu90 -- Thank you guys.\nI run test code. It seems we need one more function Utils.getBoolean() ,the error logs are following:\njava.lang.NoSuchMethodError: org.apache.storm.utils.Utils.getBoolean(Ljava/lang/Object;Z)Z\n    at org.apache.storm.kafka.DynamicBrokersReader.<init>(DynamicBrokersReader.java:59)\n    at org.apache.storm.kafka.trident.ZkBrokerReader.<init>(ZkBrokerReader.java:43)\n    at org.apache.storm.kafka.KafkaUtils.makeBrokerReader(KafkaUtils.java:58)\n    at org.apache.storm.kafka.KafkaSpout.open(KafkaSpout.java:77)\n    at org.apache.storm.topology.IRichSpoutDelegate.open(IRichSpoutDelegate.java:47)\n    at com.twitter.heron.simulator.instance.SpoutInstance.start(SpoutInstance.java:124)\n    at com.twitter.heron.simulator.executors.InstanceExecutor.startInstance(InstanceExecutor.java:212)\n    at com.twitter.heron.simulator.executors.InstanceExecutor.run(InstanceExecutor.java:181)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n    at java.lang.Thread.run(Thread.java:745)\nAnd the storm-zookeeper configurations must be set.\n. @nlu90 - It's my pleasure to do any test on it. if you want, we can contact each other using email address, in case there has more problems. My email address: ****\n. @kramasamy - Thank you for your invitation and @nlu90 gives me a lots of help.\n@nlu90 - I run the test code using new jars. It works when I try to connect kafka,but fails to read data. My code is following:\n```\nBrokerHosts hosts = new ZkHosts(zkConnString, zkRoot);\nSpoutConfig spoutConfig = new SpoutConfig(hosts, topicName, \"/kafka/\" + topicName, \"felix\");\nspoutConfig.scheme = new PrintScheme();\n\u2026\u2026\u2026\u2026\u2026\u2026\u2026\u2026\nimport org.apache.storm.spout.MultiScheme;\nimport org.apache.storm.tuple.Fields;\nimport java.nio.charset.StandardCharsets;\nimport java.util.ArrayList;\nimport java.util.List;\npublic class PrintScheme implements MultiScheme {\n    @Override\n    public Iterable> deserialize(byte[] bytes) {\n        String message = new String(bytes, StandardCharsets.UTF_8);\n        System.out.println(\"kafka=======>\" + message);\n        List> result = new ArrayList<>();\n```\nThe error is following:\njava.lang.NoSuchMethodError: org.apache.storm.spout.MultiScheme.deserialize(Ljava/nio/ByteBuffer;)Ljava/lang/Iterable;\n    at org.apache.storm.kafka.KafkaUtils.generateTuples(KafkaUtils.java:235)\n    at org.apache.storm.kafka.PartitionManager.next(PartitionManager.java:161)\n    at org.apache.storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:135)\n    at org.apache.storm.topology.IRichSpoutDelegate.nextTuple(IRichSpoutDelegate.java:67)\n    at com.twitter.heron.simulator.instance.SpoutInstance.produceTuple(SpoutInstance.java:263)\nAnd the storm-kafka KafkaConfig.scheme seems need org.apache.storm.spout.MultiScheme. The error seems due to the interface difference.\n. @lukess - Option 2,new heron-kafka  will be the possible solution of this issue. @nlu90 is working on it. And heron-kafka-spout will be coming soon. \n. @khushboo13 - hi, those are test jars. @nlu90 has new PR(#1299 & #1317) for heron-kafka-spout that support for storm-core-api. And we will have new jars to make it works soon. \n. @windie - Hi, so great to add Window Bolt support! \nI checkouted PR and installed heron-storm on my local env. Then I run test-topology named SlidingWindowTopology given by Apache-Storm. The only thing I have changed in SlidingWindowTopology.java is following\n```\n Config conf = new Config();\n        conf.setDebug(true);\n//        if (args != null && args.length > 0) {\n//            conf.setNumWorkers(1);\n//            StormSubmitter.submitTopologyWithProgressBar(args[0], conf, builder.createTopology());\n//        } else {\n//            LocalCluster cluster = new LocalCluster();\n//            cluster.submitTopology(\"test\", conf, builder.createTopology());\n//            Utils.sleep(40000);\n//            cluster.killTopology(\"test\");\n//            cluster.shutdown();\n//        }\nStormSubmitter.submitTopology(\"test\", conf, builder.createTopology());\n```\nI failed to run test-topology. Would you please take a look? And The error log is following:\n[2016-10-07 20:19:04 +0800] com.twitter.heron.instance.HeronInstance ERROR:  Exception caught in thread: SlaveThread with id: 13 \njava.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Number\n    at org.apache.storm.topology.WindowedBoltExecutor.initWindowManager(WindowedBoltExecutor.java:153)\n    at org.apache.storm.topology.WindowedBoltExecutor.prepare(WindowedBoltExecutor.java:267)\n    at org.apache.storm.topology.IRichBoltDelegate.prepare(IRichBoltDelegate.java:48)\n    at com.twitter.heron.instance.bolt.BoltInstance.start(BoltInstance.java:110)\n    at com.twitter.heron.instance.Slave.startInstance(Slave.java:173)\n    at com.twitter.heron.instance.Slave.handleNewAssignment(Slave.java:159)\n    at com.twitter.heron.instance.Slave.access$200(Slave.java:40)\n    at com.twitter.heron.instance.Slave$1.run(Slave.java:85)\n    at com.twitter.heron.common.basics.WakeableLooper.executeTasksOnWakeup(WakeableLooper.java:142)\n    at com.twitter.heron.common.basics.WakeableLooper.runOnce(WakeableLooper.java:74)\n    at com.twitter.heron.common.basics.WakeableLooper.loop(WakeableLooper.java:64)\n    at com.twitter.heron.instance.Slave.run(Slave.java:169)\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n    at java.lang.Thread.run(Thread.java:745)\nI have a minor comment though. I think it will be great if we can add unit tests,example topology and doc for this.\n. @windie - hi, any update on this?\n. @windie - Great! I'll check it ASAP. BTW, free to ping me at SLACK group anytime.\n. The bug mentioned before is fixed. \ud83d\udc4d\n. Hi,guys. \nBTW, how do we deal with //contrib folder if it was copied from Apache-storm.\n. Great!!!\n. @khushboo13 - Hi, #1317 has been merged and Heron 0.14.3 version was released.\nI have no idea about whether kafka-spout has already been pushed into maven. But kafka-spout source code is in the heron/contrib folder. \nI package and install the jar to my local maven lib by myself, fortunately it works, which means we can use kafka-spout in Heron.\n```\n\ncd $heron-path/contrib/kafka-spout/\nmvn clean package\n```\n\nI hope that will help you.\n. @khushboo13 - Hi, let's make it work.\n- You need to install heron-api-jar to your local maven lib\n```\n\n$path/heron-api-install-0.14.3-darwin.sh --user --maven\n``\n- You need to package the newkafka-spout` jar.\n\n```\n\ncd $heron-path/contrib/kafka-spout/\nmvn clean package\n``\n- You can install the newkafka-spout` jar to your local maven lib.\n\n```\n\nmvn install:install-file -q -Dfile=\"${path}/kafka-spout.jar\" -DgroupId=\"org.apache.storm\" \\\n    -DartifactId=\"kafka-spout\" -Dversion=\"SNAPSHOT\" -Dpackaging=\"jar\"\n``\n- Then you can usekafka-spout` in your pom.xml\n\n<dependency>\n            <groupId>com.twitter.heron</groupId>\n             <artifactId>heron-api</artifactId>\n            <version>SNAPSHOT</version>\n            <scope>compile</scope>\n      </dependency>\n      <dependency>\n            <groupId>com.twitter.heron</groupId>\n            <artifactId>heron-storm</artifactId>\n            <version>SNAPSHOT</version>\n            <scope>compile</scope>\n        </dependency>\n     <dependency>\n            <groupId>org.apache.storm</groupId>\n            <artifactId>kafka-spout</artifactId>\n            <version>SNAPSHOT</version>\n      </dependency>\nI hope that will help you. \n. @khushboo13 \nHi, would you like to import  org.apache.storm instead of  backtype.storm and try it again? \n. @khuhboo13 -  You dont need to change your pom.xml just the code you show me.  Heron has been included org.apache.storm\n. @khushboo13\nI see.\nYou are using backtype.storm.topology.TopologyBuilder in your code which required backtype.storm.topology.IRichSpout in setSpout method. \nKafkaSpout comes from org.apache.storm.kafka which implements org.apache.storm.topology.IRichSpout. \nI think you may get the same error if you change it from heron-kafka to storm-kafka\n. @kramasamy thanks for mentioning it,I've updated it.\n. @nlu90 - Should we remove the local prefix in pom.xml?\n. Great!\n. Would you like to show your Heron version and more configurations?\n. @HosiYuki  - Hi~~~\nYarnLauncher was added in 0.14.1 as @billonahill said. \nUser should copy hadoop-lib jars to specified path in 0.14.2 following the instructions on http://twitter.github.io/heron/docs/operators/deployment/schedulers/yarn/ . \nI highly recommend you use 0.14.3 version. PR #1245 added extra-launch-classpath arg,which means we dont need to copy hadoop-lib jars any more to submit topo on Yarn mode after 0.14.3 version released.\nSubmitting command will be:\nheron submit yarn ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology --extra-launch-classpath <extra-classpath-value>\nNo matter which version of Heron you use, there is something you should pay attention to if you want to submit topo to Yarn(Hadoop 2.7.x).\nFor localfs-statemgr\n- The common-cli jar\u2019s version should be greater than or equal to 1.3.1. \nFor zookeeper-statemgr\n- The common-cli jar\u2019s version should be greater than or equal to 1.3.1. \n- The curator-framework jar's version should be greater than or equal to 2.9.0\n- The curator-client jar's version should be greater than or equal to 2.9.0\nSince perhaps the most popular Hadoop version is 2.7.x , which uses common-cli-1.2.1,curator-framework-2.7.1 and curator-client-2.7.1. \nThe YARN scheduler doc should be updated for the latest version later.\n. @ashvina - I'm afraid my english problem...... but I'd like to try\n. @HosiYuki - Hi\nAbout first question\nAs @kramasamy said, the situation '*' is fixed by #1373 . And the submitting command will be:\nheron submit yarn ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology --extra-launch-classpath <extra-classpath-value>\nAnd the key point is the extra-classpath-value.  It should be the hadoop-lib-jar path if you want to submit topology to YARN. If you are familiar with the hadoop working environment, you can put all jars in an  independent directory such as '$hadoop-lib-jars' , and the submitting command will be:\nheron submit yarn ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology --extra-launch-classpath $hadoop-lib-jars\nAnd there is an easier way to use --extra-launch-classpath arg by following steps.\n-  Finding your hadoop classpath by following command.\n```\n\nhadoop classpath\n```\n- Making sure every single path is existed\n\nThen the example submitting command in my working environment will be:\nheron submit yarn ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology \\\n--extra-launch-classpath /home/hdfs/heron-yarn-classpath-jars/*:${HADOOP_DIR_HOME}/etc/hadoop:${HADOOP_DIR_HOME}/share/hadoop/common/lib/*:${HADOOP_DIR_HOME}/share/hadoop/common/*:${HADOOP_DIR_HOME}/share/hadoop/hdfs:${HADOOP_DIR_HOME}/share/hadoop/hdfs/lib/*:${HADOOP_DIR_HOME}/share/hadoop/hdfs/*:${HADOOP_DIR_HOME}/share/hadoop/yarn/lib/*:${HADOOP_DIR_HOME}/share/hadoop/yarn/*:${HADOOP_DIR_HOME}/share/hadoop/mapreduce/lib/*:${HADOOP_DIR_HOME}/share/hadoop/mapreduce/*\nAbout second question\nPlease notice the command I gave above. I downloaded the jars I need and put them to the /home/hdfs/heron-yarn-classpath-jars/ directory and invoked them first. \n```\n\nll /home/hdfs/heron-yarn-classpath-jars/*\n-rw-r--r-- 1 hdfs hdfs  52988 Jun  14 2015 /home/hdfs/heron-yarn-classpath-jars/commons-cli-1.3.1.jar\n-rw-r--r-- 1 hdfs hdfs  71909 Sep  13 14:06 /home/hdfs/heron-yarn-classpath-jars/curator-client-2.9.0.jar\n-rw-r--r-- 1 hdfs hdfs 192090 Sep  12 16:57 /home/hdfs/heron-yarn-classpath-jars/curator-framework-2.9.1.jar\n```\n\nAbove all, I think it should work!  Let me know if you have any other question.\n. @HosiYuki - Would you like to check the YARN logs, following the instructions on here ,Log File location section.\n. @silence-liu - Would you please to do more checks? \n1. I think the first step you need to check is whether your topology is running well on YARN. Pls check your YARN-scheduler-webstie to confirm your applicationId's status.\n2. If your topology is running well on YARN, then we should focus on the driver.stderr and evaluator.stderr to make sure there is no error while running.  You can follow the instructions on http://twitter.github.io/heron/docs/operators/deployment/schedulers/yarn/ section Log File location \n3. If the step 1 and 2 are both fine. I think what you need to do is to check your .herontools/conf/heron_tracker.yaml configs following the instructions on http://twitter.github.io/heron/docs/operators/heron-tracker/ to make sure the statemgrs is set up right.\n. @ashvina - Shall we add the yarn.application.classpath config into the doc?  I'd like to do that\n. closed #1373\n. It works!    \ud83d\udc4d\n. @kramasamy - Thank you for your response.\nThis issue comes from our working scenario. Our team has to figure it out in case Kafka data or topology is wrong.\nKafka-spout commits current offset automatic to the zookeeper. Our main work is for making every topology which consumes data from Kafka saves today-start-offset on the zookeeper automatic too. What we need to do is identifying daily-change and save the offset. Then every topology which uses Kafka-spout is able to be traced back to today-start-offset by following steps.\n- deactive topology\n- change kafka-offset data on zookeeper using zookeeper-cli-command\n- rebalence/restart topology\n. @kramasamy - We met the problem you mentioned too, which brought us a lot of trouble. Indeed, the functionality of today-start-offset is with the spouts. \nCurrently What kafka-spout stores is following:\nMap<Object, Object> data = (Map<Object, Object>) ImmutableMap.builder()\n          .put(\"topology\", ImmutableMap.of(\"id\", topologyInstanceId,\n              \"name\", stormConf.get(Config.TOPOLOGY_NAME)))\n          .put(\"offset\", lastCompletedOffset)\n          .put(\"partition\", partition.partition)\n          .put(\"broker\", ImmutableMap.of(\"host\", partition.host.host,\n              \"port\", partition.host.port))\n          .put(\"topic\", partition.topic).build();\nTopology commits offset to ZK. What we need to do is adding today-start-offset key-value to the map data. We do not increase the number of write times. What we increased is the number of bytes of map data.\nTopology will never read the key today-start-offset  from the ZK on its own. When something goes wrong, we copy the today-start-offset value to the offset key by zk-cli-command and restart the topology. Spouts will read the offset value from ZK and restart.\nThe today-start-offset  just gives our user a chance to change their topology and re-calculated the statistical indicators.\n. @yunfanfighting - Hi, currently Heron supports darwin, centos and ubuntu.\n. @yunfanfighting - Here is Compiling Heron Guide telling us how to compile Heron and which OS Heron supports currently. If you want to know something about the code organization, you could click Heron code organization . And There is Contributing New Environments telling us how to compile Heron with other OS.\n. Closed #1401\n. @ashvina - I've modified as your suggestions. Thank you for your time\n. @kramasamy - Maybe it is helpful for the 0.14.2 version users if they want to submit topology to YARN. @ashvina hopes users know they have second option doing it. Do you think we don't need to introduce this?\n. @yunfan123 - Hi, it takes me around 30-40 mins to compile the entire codebase using docker. My working environment is MacBook Pro Mid 2012 .\n. @yunfan123  - Released Page is addressed on Getting Started Guide. \n. @ashvina - Thank you so much for your suggestions. I linked this PR with issue number and tried to reform the commits. I'll send another PR for updating yarn/scheduler.yaml and the doc after this PR is merged. Please let me know if there's anything else needed.\n. @ashvina - I added YarnContext class, updated the  conf/yarn/scheduler file and the doc. Could you please look at this change again? Thank you very much for taking your time.\n. @nlu90 - Shall we do the same change for org.apache.storm.serialization.SerializationFactory in Heron at line since the namespace was changed after Storm 1.0.x released.\n. \ud83d\udc4d\n. @silence-liu as @ashvina mentioned,please make sure the right config. And would you like to share your Heron version? if you are using 0.14.3, you can follow the guide https://github.com/twitter/heron/blob/master/website/content/docs/operators/deployment/schedulers/yarn.md . The website will be updated soon.\n. You are welcome\nPlease close the issue if it was solved\n. @silence-liu I think there might something wrong with your curator-framework lib version. Could you please check your class path and make sure the lib's version is right and try it again? The doc https://github.com/twitter/heron/blob/master/website/content/docs/operators/deployment/schedulers/yarn.md, section Configuring the Heron client classpath,\n might help you. \n. I think it will be much more efficient than the previous implementation.\n. @kramasamy - Sorry for the delay.  It runs beautiful. \n. Oops! CI tests are failed......\nIs there anything we could do to make the gateway_looper_unittest passed? . @kramasamy @maosongfu @billonahill - I'll send PR for this. . merged #1607 and closed. @billonahill - I'm not sure whether my understanding is correct. First the c.t.h.spi.common.Jars is unused in Heron project and it should be removed or deleted. Second, there are some utility helpers we may want to reserve, such as getJars ,getClassPath... which should be live in utils package. I'm not sure if creating a new Utils class named JarsUtils which contains getJars and getClassPath static method is what we want.. Understood, thanks!.  @billonahill - I send PR for this. Please review when you have a chance. \ud83d\udc4d . merged #1613 and closed. Try to do the same in ByteAmount. @billonahill Thanks for your suggestion and the PR was updated. . @billonahill - I'll send another PR for ByteAmount later. Thanks for reviewing!. Updated\n\nmove classes into schedulers-core\nrefactor SchedulerConfig class\nadd test_classes\n\n@billonahill Please take a look again when you have a chance. . \ud83d\udc4d\ud83c\udffb. @billonahill - I've addressed all comments. Thanks for reviewing!. @billonahill @objmagic - Please let me know after the website release. I'll check the page again.. @objmagic - It's great! Thank you~. \ud83d\udc4d. @ashvina @objmagic I've addressed all comments. Would you please take a look again?. @objmagic - Thanks for reviewing. I'm agree with you. At first, I think putting ByteAmountUnit inside of ByteAmount is a better approach and using enum to instead static MB and GB inside of ByteAmount seems nice too. But It might trigger a large refactor in ByteAmount. So, I decided to put ByteAmountUnit outside of ByteAmount temporarily.. Considering @billonahill 's comment,  creating ByteAmountUnit enum class might not be a  proper way.\n@ashvina @billonahill - How about we make 2048 into 2048 * 1024 here\uff1f. i was wondering do we need to modify README.md to mention users that current release only support core api,Trident api is not included.\n. Thank you so much for your reviewing. I will change it.\n. Planning to move this method to the YarnContext. I'll update it~\n. ~~I was wondering if we should check streamId as what storm has done here. I'm afraid it will cause another problem at line. What do you think?~~\nmy bad, delete comment\n. I was wondering if the declare statement of result variable could be in the synchronized  block, or it might be \nOptional<HeronWorker>  result = findLargestFittingWorker(evaluator, workersAwaitingAllocation, true);\n. @objmagic - hi, Is there anything I could do? The method name which @billonahill mentioned is better . However, I don't think adding an new enum class is a good idea. Perhaps we could add new static constants in FileUtils instead of handling string variable explicitly, for example\nprivate static final String PEX = \"pex\";\n . @objmagic - Cause String pkgType is used for Config's value. Just in my opion, this scene is applicable to the principle of Occam's razor -- If not necessary, do not increase the entity . Hi, @billonahill . I'm reading this part of code. I was wondering if we could use Preconditions.checkArgument or Preconditions.checkState provided by Google guava instead of assertTrue method?  . Understood, I'll update it.. My bad, forgot doing this. . Good Point!  Thank you for your advice. I'll try to move them into schedulers-core. my carelessness. using Preconditions.checkState?. Sorry, I didn't notice this PR has been requested a reviewer.. I'm not quite sure if Strings.repeat would help this part of code(56-61). \nStringBuilder metaFormatterBuilder = new StringBuilder();\n        metaFormatterBuilder.append(Strings.repeat(String.format(\"| %s \", \"%%%ds\"),title.size()));\n        metaFormatterBuilder.append(\"|\"); . I noticed that the difference between componentsResource and componentsParallelism method is the return type and those methods were invoked several times by subclass separately. How about we combine two methods into one method named componentsInfo and create a ComponentInfoBean so that we can  change the return type to Map<String, ComponentInfoBean>. I'm not sure if we already have such a bean which contains enough component's informations we might need, such as Resource, Parallelism or something else. It might be more convenient for subclass processing.. I was wondering if we could use List<String> and Joiner here instead of StringBuilder and addRow method as following code.\nList<String> contentList = new ArrayList();\ncontentList.add(Strings.repeat(\"=\", titleRow.length()));\ncontentList.add(titleRow);\ncontentList.add(Strings.repeat(\"-\", titleRow.length()));\nfor (List<String> row: rows) {\n     contentList.add(String.format(rowFormatter, (Object[]) row.toArray(new String[row.size()])));\n}\ncontentList.add(Strings.repeat(\"=\", titleRow.length()));\nreturn Joiner.on(\"\\n\").join(contentList)\nI'm not sure which one is better.. got it. tiny comment. a typo?. A tiny comment. I was wondering if the titleNames variable could be a static and immutable List instead of being create every times when the function invoked since it looks like the variable will never be changed in every single table.. @billonahill - Thanks for reviewing. It will be great if we change driver.memory.mb from int to ByteAmount. I would like to send another PR for this. BTW, dropping '.mb' might be not good for users cause they need to know the unit of this parameter.. Agree. will do. Good point~. @billonahill - It seems we don't have ByteAmountUnit class currently. Do you think it is a good idea if we add a new class in c.t.h.common.basics in this PR? . @objmagic - Understood. Thank you for your suggestion. . To be honest, it make me confused too. I didn't notice that.... @objmagic - cast it to int. @objmagic - Sure~. ",
    "atibon": "thx,  I fix it. This is a network problem.\n. Finally I fix it. That's because the JAVA_HOME of submitting server is not the same to the worker server.\n. @caofangkun I've fixed it.\n. @objmagic For example, Emit Count for 10 mins  is 0. \nAnd sometimes all of the metrics value is 0. \n. @maosongfu Thx. I will try. \n. @objmagic Maybe network problem. BTW, Must I collect all of metrics.json.* under hundreds of sandbox directories if I wanner get the correct data? \n. ",
    "gustavopinto": "PS: Sorry for the mistake closing the issue.\n. Hi @kramasamy, \nthanks once again for answering our research inquiries. We were able to collect 35 responses and we drafted a research paper with the results. The paper was submitted and accepted for the 14th International Conference on Open Source Systems (http://oss2018.org/). You can find the paper here. Hope you enjoy reading the paper!\nThanks again,\nGustavo. ",
    "severun": "@nlu90 Is there an alternative way to implement c++ calls in a bolt? Or does this mean Heron is no longer a viable framework for us? We can't really port all our code to java just to support the framework .\n. Is there any update on this yet? I noticed that the milestone has been updated/rescheduled quite often but cannot find any progress description regarding this feature request.\n. ",
    "benley": "Cool, I'm glad to hear that :-)\nI don't know if there's currently a way to hook into bazel's verbosity options from within skylark, but lacking that it ought to be easy enough to add a debug mode toggle to the pex rules.\n. I would love to get these rules using an up-to-date upstream version of PEX without any modifications, but that is currently blocked on at least one bug: https://github.com/pantsbuild/pex/pull/277\n. Relatedly: is there still a need to support python2.4 in the Heron codebase?  I'm now running into some cruft that could just go away if backwards compatibility all the way to 2.4 isn't required.\n. I'll try to keep my fork in sync with the update!  Haven't had time to do much work on this in the last week but I'll get back to it soon.\n. I've got my copy of pex upgraded now.  I want to do a little more testing before diving in with heron; stay tuned.\n. I think you might be missing some updates to pex/bin/pex.py; here's what that diff looks like from my repo: https://github.com/benley/bazel_rules_pex/pull/6/files#diff-da6b827279fc47e8b1603fe3af0f4a0e\nThe changes are pretty minor, so maybe you deliberately didn't include them?\n. Aside from that, \ud83d\udc4d \n. Sorry for disappearing; I haven't had time to work on this in the past week, but I intend to get back to it fairly soon.\n. Not much news, but I think I know of a reasonably sane way of publishing a usable source archive via pypi for the pex rules, which should remove the main obstacle.  I'll try to make progress on this issue this week.\n. Hi there, sorry I have gone silent for so long.  I changed jobs and have not had much time to work on bazel-related things recently.\nIt would be cool to get the pex rules hosted under the bazelbuild project, but I don't think they will be interested because it more or less duplicates the functionality of https://github.com/google/subpar, along with bazel's built-in Python rules.\nI did work out a way of hosting skylark rules on pypi, though! https://pypi.python.org/pypi/bazel-rules-pex. indeed, thanks for the mention :)\n. I believe @bazel_tools is defined internally by Bazel; it replaces the former implicit merging of //tools into the user workspace.\n. Better home: yes, I hope so.  I haven't directly requested a github project from bazelbuild, but moving it out of my personal account does seem appropriate before merging this change.\nInternal CI: it would work equally well to get these rules from a .zip or .tgz archive using a http_archive rule, or even from a path on disk using local_repository if that turns out to be easier.  In theory we could publish a pypi project containing the bazel rules, though it would be a bit awkward.  I'm not very familiar with Maven - can it be used to deliver arbitrary files?  If so, I imagine that could be made to work as well.\nIf none of that is practical, we could just vendor things back into this repo rather than fetching it externally as part of the build.\n. I'm doing all my development and testing on 0.3.0 so far. @bazel_tools does work there.\n. ",
    "pradeepchemical1": "Below code i tested locally and got the same error with AWS sdk 1.10.66 and run successfully with 1.11.x.\nBelow code i pulled from S3uploader under tags 0.14.0.\nAWSCredentials credentials = new BasicAWSCredentials(\"\", \"\");\n        AmazonS3 s3Client = new AmazonS3Client(credentials);\n        try {\n```\n         // Backup any existing files incase we need to undo this action\n    final String topologyName = \"ExclamationTopology\";\n    final String topologyPackageLocation = \"topology.tar.gz\";\n    String pathPrefix=\"\";\n    String bucket=\"heron-data\";\n    File packageFileHandler = new File(topologyPackageLocation);\n\n    // The path the packaged topology will be uploaded to\n   String remoteFilePath = \"ExclamationTopology/topology.tar.gz\";//generateS3Path(pathPrefix,topologyName,packageFileHandler.getName());\n\n    // Generate the location of the backup file incase we need to revert the deploy\n\n\n    if (s3Client.doesObjectExist(bucket, remoteFilePath)) {\n        System.out.println(\"EXIST**\");\n      //s3Client.copyObject(bucket, remoteFilePath, bucket, previousVersionFilePath);\n    }\n    else {\n        System.out.println(\"*NOT EXIST*\");\n    }\n\n } catch (AmazonServiceException ase) {\n    System.out.println(\"Caught an AmazonServiceException, which \" +\n            \"means your request made it \" +\n            \"to Amazon S3, but was rejected with an error response\" +\n            \" for some reason.\");\n    System.out.println(\"Error Message:    \" + ase.getMessage());\n    System.out.println(\"HTTP Status Code: \" + ase.getStatusCode());\n    System.out.println(\"AWS Error Code:   \" + ase.getErrorCode());\n    System.out.println(\"Error Type:       \" + ase.getErrorType());\n    System.out.println(\"Request ID:       \" + ase.getRequestId());\n} catch (AmazonClientException ace) {\n    System.out.println(\"Caught an AmazonClientException, which \" +\n            \"means the client encountered \" +\n            \"an internal error while trying to \" +\n            \"communicate with S3, \" +\n            \"such as not being able to access the network.\");\n    System.out.println(\"Error Message: \" + ace.getMessage());\n}\n\n```\n. Below is exception trace also.\nException in thread \"main\" com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: 8C2A9D868737EB62), S3 Extended Request ID: MwaqxlNVSIHFE11P4V9QSH+LoQFRRIvn8bM2GVoDFTOdSZB4lcNyEatODuRAGU/ZgYopuElKsWA=\n    at com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1389)\n    at com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:902)\n    at com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:607)\n    at com.amazonaws.http.AmazonHttpClient.doExecute(AmazonHttpClient.java:376)\n    at com.amazonaws.http.AmazonHttpClient.executeWithTimer(AmazonHttpClient.java:338)\n    at com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:287)\n    at com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3654)\n    at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:964)\n    at com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:940)\n    at com.amazonaws.services.s3.AmazonS3Client.doesObjectExist(AmazonS3Client.java:999)\n    at com.twitter.heron.uploader.s3.S3Uploader.uploadPackage(S3Uploader.java:111)\n    at com.twitter.heron.scheduler.SubmitterMain.uploadPackage(SubmitterMain.java:457)\n    at com.twitter.heron.scheduler.SubmitterMain.submitTopology(SubmitterMain.java:400)\n    at com.twitter.heron.scheduler.SubmitterMain.main(SubmitterMain.java:315)\nERROR: Failed to launch topology 'ExclamationTopology' because User main failed with status 1. Bailing out...\nINFO: Elapsed time: 20.657s.\n. I did try to investigate a lot. But no luck :-(. My endpoint is \"heron-data.s3-website.ap-south-1.amazonaws.com\". \nI had enabled logging at S3, below were logs, As you could see it was not giving me any root cause. It is just saying InvalidRequest. So i had left with only option of raising issue.\nIf you want to reproduce this error, I can share my S3 information, as i had created only to test this. Shall i send details on email \"andrew@andrewjorgensen.com\" ?\n63dc215b38837d827b3f150a4aef955cd731d18a8ca6b03ac7c2852c84e96e2f heron-data [05/Jul/2016:16:50:37 +0000] 106.51.130.237 - 48D29BDF020B7270 REST.HEAD.OBJECT ExclamationTopology/topology.tar.gz \"HEAD /ExclamationTopology/topology.tar.gz HTTP/1.1\" 400 InvalidRequest 324 - 4 - \"-\" \"aws-sdk-java/1.10.66 Mac_OS_X/10.10.5 Java_HotSpot(TM)_64-Bit_Server_VM/25.60-b23/1.8.0_60\" -\n. Sure Andrew. Let me know your results.\nAlso if you could tell me S3 reason that is working for you. I can also try at my side too.\n. ",
    "sirinath": "Great. Also I opened this: https://github.com/bazelbuild/bazel/issues/1497\n. Apparently Bazel has Windows support according to: https://github.com/bazelbuild/bazel/issues/1497. I guess there is nothing stopping you from doing a release?\n. ",
    "yanxz": "@objmagic pulled the change and reran the script, it works now! Thanks for such quick response, it's really awesome!\n. Hey I'm trying to play with Mesos scheduler and I'm very interested in running Heron using Mesos Native Scheduler. I can wrap up my practices into this documentation if you guys are fine with it.\n. Thanks for the detailed suggestions @billonahill @kramasamy, addressed all the comments. @billonahill deleted the contents about compiling Heron as this doc will be released with the Mesos Scheduler. User can use the binaries then.\n. @billonahill sorry forgot the char number issue in my last commit. Addressed all the comments.\n. @billonahill I've accepted the CLA.\n. I didn't actually launch the mesos cluster :), which I don't think it's worth mentioning.\n. ",
    "kennylbj": "you're right, read operation may fail write's if it read -1 or something.\nthank you and please close my issue.\n. i think the second isvalid() check could be removed and others work well.\n. It will continue if the first isvalid return false or jump to the second check if it return true which was checkd before. and that is duplicate. \nAny oher check valid op are useful.\n. I got it. Thank you!\n. I've implemented a method to check the thread safety whenever adding event to timer queue. \nhttps://github.com/kennylbj/heron/blob/master/heron/common/src/java/com/twitter/heron/common/basics/WakeableLooper.java\nMaybe get some overhead but gain more safety.\nShould i pull a request for that implementation or just remain the current design?\n. What you say is very reasonable! By the way PriorityQueue seems hard to implement cancel timer.\n. ",
    "cliffyg": "@kramasamy I've signed the form\n. ",
    "joelanford": "@ajorgensen - Did this (or another similar PR) get merged? I think I'm running into this problem in 0.14.5.. ",
    "evenluo": "Signed\n. ",
    "ZacharyThomas": "So is it safe to say that the storm kafka spout isn't backwards compatible currently? Is there any alternative? \n. ",
    "lukess": "What will be the possible solution of this issue?\na hack like https://github.com/elodina/heron/tree/kafka_spout or a new heron-kafka?\n. @mycFelix thanks!\n. @kramasamy just accepted. thanks for merging.\n. ah~ I followed the https://github.com/lukess/heron/blob/master/website/content/docs/operators/heron-ui.md\nI think I should follow\nhttp://twitter.github.io/heron/docs/developers/compiling/mac/\nand\nhttp://twitter.github.io/heron/docs/contributors/testing/\n. Just realized I should not use marathon way to define the mesos-slave. Changed resources in mesos-slave to workaround.\n. @kramasamy @objmagic \nIn the multiple NICs env socket.gethostbyname(socket.gethostname()) would return virtual NIC address such as vpn, virualbox, docker...\nTherefore, I added '0.0.0.0' as Tornado default bind for IPv4, and It's not a good solution too.\n. @kramasamy you are right 0.0.0.0 is not a good log. Make it clearer,\n$ heron-ui, currently it prints Listening at http://{ip address from one of the interfaces}:8889, but it actually LISTEN 0.0.0.0/:::8889\n. @kramasamy honestly I don't know right now. I will test it later. My assumption is from https://github.com/tornadoweb/tornado/blob/7ec71d46a83eade88f6d588f8f568f9759676c89/tornado/netutil.py#L111.\n. @kramasamy sure,\n127.0.0.1           localhost\n255.255.255.255 broadcasthost\n::1             localhost\nMay I know LISTEN when you run heron-ui too?\n$ lsof -i -P | grep LISTEN\n. ok~ it did listen to 0.0.0.0 and ::\nI think the problem right now is if want to force address to socket.gethostbyname(socket.gethostname()) by default which always send Address into Tornado bind.\n. or add a warning log, and print 0.0.0.0 as well.\n. my bad! I did not read create_parsers() carefully. The default arguments have been set up. Should I add tracker address in this pull request?\n. ",
    "khushboo13": "@mycFelix -  Hi, I am getting the same issue you mentioned.Can you please help me in resolving this.\nI tried using the jars @nlu90 updated.\n[2016-09-01 17:52:38 +0530] com.twitter.heron.instance.HeronInstance SEVERE:  Exception caught in thread: SlaveThread with id: 12\njava.lang.NoClassDefFoundError: storm/trident/spout/ISpoutPartition\n        at java.lang.ClassLoader.defineClass1(Native Method)\n        at java.lang.ClassLoader.defineClass(ClassLoader.java:760)\n        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n        at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n        at java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        at storm.kafka.trident.GlobalPartitionInformation.getOrderedPartitions(GlobalPartitionInformation.java:54)\n        at storm.kafka.KafkaUtils.calculatePartitionsForTask(KafkaUtils.java:215)\n        at storm.kafka.ZkCoordinator.refresh(ZkCoordinator.java:80)\n        at storm.kafka.ZkCoordinator.getMyManagedPartitions(ZkCoordinator.java:69)\n        at storm.kafka.KafkaSpout.nextTuple(KafkaSpout.java:135)\n        at backtype.storm.topology.IRichSpoutDelegate.nextTuple(IRichSpoutDelegate.java:67)\n        at com.twitter.heron.instance.spout.SpoutInstance.produceTuple(SpoutInstance.java:271)\n        at com.twitter.heron.instance.spout.SpoutInstance.access$100(SpoutInstance.java:42)\n        at com.twitter.heron.instance.spout.SpoutInstance$1.run(SpoutInstance.java:176)\n        at com.twitter.heron.common.basics.WakeableLooper.executeTasksOnWakeup(WakeableLooper.java:142)\n        at com.twitter.heron.common.basics.WakeableLooper.runOnce(WakeableLooper.java:74)\n        at com.twitter.heron.common.basics.WakeableLooper.loop(WakeableLooper.java:64)\n        at com.twitter.heron.instance.Slave.run(Slave.java:169)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.ClassNotFoundException: storm.trident.spout.ISpoutPartition\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        ... 28 more\n. @kramasamy By when can I expect the new jar.\n. @nlu90 You will be updating the storm-kafka to work with heron or adding a new heron-kafka.\n. @nlu90 I cannot see any new releases for storm-kafka in maven.Latest version it is showing is 1.0.2.\nIs it not published yet?\n. @kramasamy - Hi, is the new jar published?\n. @mycFelix - Hi, Thanks for the response.I upgraded my version and compiled the new jar.But still I am not able to make it work with heron.\nCan you please post the updated code with pom.xml?\n. Hi,\nI followed these steps previously, in the code I am getting some errors because Kafka spout is using BaseRichSpout which is not supported in the heron topology builder.\nPlease find the code I am using\npackage rtm.rtm;\nimport java.util.Arrays;\nimport org.apache.storm.kafka.BrokerHosts;\nimport org.apache.storm.kafka.KafkaSpout;\nimport org.apache.storm.kafka.SpoutConfig;\nimport org.apache.storm.kafka.ZkHosts;\nimport com.twitter.heron.api.Config;\nimport backtype.storm.StormSubmitter;\nimport backtype.storm.spout.MultiScheme;\nimport backtype.storm.spout.SchemeAsMultiScheme;\nimport backtype.storm.topology.TopologyBuilder;\npublic class Spout \n{\n    public static void main( String[] args ) throws backtype.storm.generated.AlreadyAliveException, backtype.storm.generated.InvalidTopologyException\n    {\n        String zkConnString = \"ip\";\n        String topicName = \"kafkastorm\";\n        String zkRoot = \"/brokers\";\n```\n    BrokerHosts hosts = new ZkHosts(zkConnString, zkRoot);\n    SpoutConfig spoutConfig = new SpoutConfig(hosts, topicName, \"/kafka/\" + topicName, \"felix\");\n    //spoutConfig.scheme = new MultiScheme();\n    spoutConfig.scheme = new SchemeAsMultiScheme(new StringScheme());\n    spoutConfig.zkServers = Arrays.asList(\"ip\");\n    spoutConfig.zkPort = 2181;\nKafkaSpout kafkaSpout = new KafkaSpout(spoutConfig);\n\nTopologyBuilder builder = new TopologyBuilder();\nbuilder.setSpout(\"kafka\",  kafkaSpout);\nbuilder.setBolt(\"reciever\", new BoltTest(),3).shuffleGrouping(\"kafka\");\nConfig conf = new Config();\n// must set,or it will cause null exceptions\nconf.put(\"storm.zookeeper.session.timeout\", 20000);\nconf.put(\"storm.zookeeper.connection.timeout\", 15000);\nconf.put(\"storm.zookeeper.retry.times\", 5);\nconf.put(\"storm.zookeeper.retry.interval\", 1000);\n\ntry {\n    StormSubmitter.submitTopology(\"Kafka-Base-Test\", conf, builder.createTopology());\n} catch (Exception e) {\n    // TODO Auto-generated catch block\n    e.printStackTrace();\n}\n\n}\n```\n}\n. @mycFelix - I am using it with the heron, if I change to apache.storm then my other topologies will not work\n. @mycFelix - I don't need to change pom.xml, the problem I have is that in same topology builder I am having several other spouts(like RabbitMQ spout) which will not work with apache.storm.\n. ",
    "Yitian-Zhang": "I encountered the same question: #3103.  Do you know how to fix it? Any help is grateful. . Thanks for your help, and  I have already used --verbose parameter for displaying the output. So it means I can only get the scheduler log by using the parameter, and I can't get a scheduler log file just like running on local mode, is it right?\n. Thanks a lot.. Does anyone know how to solve this problem? Or what is the cause of the problem?\n. @nwangtw  Thanks for your commit. Let's me introduce my question in detail. First, I created a CustomScheduler that have been deployed on Heron. In the CustomScheduler, besides the main thread that heron runs, I created a new thread after created job by Heron. The new created Thread is responsible for running my programs. This problem is happening in the thread I created.\nSecond, I can submit topologies and activate them normally using the CustomScheduler.  So that means my CustomScheduler is deployed correctly and the main thread is right.\nFor your curious, my submit commands is:\nheron submit aurora/yitian/devel --config-path ~/.heron/conf ~/aurora-topolgoies/heron-with-dependencies.jar zyt.custom.topology.aurora.SentenceWordCountTopology SentenceWordCountTopology --deploy-deactivated --verbose\nAs for this problem happened in the new thread that I created as above mentioned. In this new thread, I wanted to update a topology when it was running by using UpdateTopologyManager with a new PackingPlan. I want to make the effect of this method just like using the update command to update the topology. So I attempted to create ISchedulerClient and new runtime Config to update the topology by invoking ISchedulerClient.updateTopology function. Then this problem happened. \nWhat's more, I found the Exception in Zookeeper log as follows. \n2018-07-09 17:23:06,960 [myid:] - INFO  [main:QuorumPeerConfig@134] - Reading configuration from: /home/yitian/zookeeper/zookeeper-3.4.10/bin/../conf/zoo.cfg\n2018-07-09 17:23:06,968 [myid:] - INFO  [main:QuorumPeer$QuorumServer@167] - Resolved hostname: heron01 to address: heron01/218.195.228.24\n2018-07-09 17:23:06,969 [myid:] - ERROR [main:QuorumPeerConfig@345] - Invalid configuration, only one server specified (ignoring)\n2018-07-09 17:23:06,970 [myid:] - INFO  [main:DatadirCleanupManager@78] - autopurge.snapRetainCount set to 3\n2018-07-09 17:23:06,970 [myid:] - INFO  [main:DatadirCleanupManager@79] - autopurge.purgeInterval set to 0\n2018-07-09 17:23:06,970 [myid:] - INFO  [main:DatadirCleanupManager@101] - Purge task is not scheduled.\n2018-07-09 17:23:06,970 [myid:] - WARN  [main:QuorumPeerMain@113] - Either no config or no quorum defined in config, running  in standalone mode\n2018-07-09 17:23:06,977 [myid:] - INFO  [main:QuorumPeerConfig@134] - Reading configuration from: /home/yitian/zookeeper/zookeeper-3.4.10/bin/../conf/zoo.cfg\n2018-07-09 17:23:06,978 [myid:] - INFO  [main:QuorumPeer$QuorumServer@167] - Resolved hostname: heron01 to address: heron01/218.195.228.24\n2018-07-09 17:23:06,978 [myid:] - ERROR [main:QuorumPeerConfig@345] - Invalid configuration, only one server specified (ignoring)\n2018-07-09 17:23:06,978 [myid:] - INFO  [main:ZooKeeperServerMain@96] - Starting server\n2018-07-09 17:23:06,982 [myid:] - INFO  [main:Environment@100] - Server environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT\n2018-07-09 17:23:06,982 [myid:] - INFO  [main:Environment@100] - Server environment:host.name=heron01\n2018-07-09 17:23:06,982 [myid:] - INFO  [main:Environment@100] - Server environment:java.version=1.8.0_151\n2018-07-09 17:23:06,982 [myid:] - INFO  [main:Environment@100] - Server environment:java.vendor=Oracle Corporation\n2018-07-09 17:23:06,982 [myid:] - INFO  [main:Environment@100] - Server environment:java.home=/usr/java/jdk1.8.0_151/jre\n2018-07-09 17:23:06,983 [myid:] - INFO  [main:Environment@100] - Server environment:java.class.path=/home/yitian/zookeeper/zookeeper-3.4.10/bin/../build/classes:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../build/lib/*.jar:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../lib/slf4j-log4j12-1.6.1.jar:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../lib/slf4j-api-1.6.1.jar:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../lib/netty-3.10.5.Final.jar:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../lib/log4j-1.2.16.jar:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../lib/jline-0.9.94.jar:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../zookeeper-3.4.10.jar:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../src/java/lib/*.jar:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../conf:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../build/classes:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../build/lib/*.jar:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../lib/slf4j-log4j12-1.6.1.jar:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../lib/slf4j-api-1.6.1.jar:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../lib/netty-3.10.5.Final.jar:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../lib/log4j-1.2.16.jar:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../lib/jline-0.9.94.jar:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../zookeeper-3.4.10.jar:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../src/java/lib/*.jar:/home/yitian/zookeeper/zookeeper-3.4.10/bin/../conf:.:/usr/java/jdk1.8.0_151/lib:/usr/java/jdk1.8.0_151/jre/lib\n2018-07-09 17:23:06,983 [myid:] - INFO  [main:Environment@100] - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib\n2018-07-09 17:23:06,983 [myid:] - INFO  [main:Environment@100] - Server environment:java.io.tmpdir=/tmp\n2018-07-09 17:23:06,984 [myid:] - INFO  [main:Environment@100] - Server environment:java.compiler=<NA>\n2018-07-09 17:23:06,984 [myid:] - INFO  [main:Environment@100] - Server environment:os.name=Linux\n2018-07-09 17:23:06,984 [myid:] - INFO  [main:Environment@100] - Server environment:os.arch=amd64\n2018-07-09 17:23:06,984 [myid:] - INFO  [main:Environment@100] - Server environment:os.version=4.13.0-43-generic\n2018-07-09 17:23:06,984 [myid:] - INFO  [main:Environment@100] - Server environment:user.name=yitian\n2018-07-09 17:23:06,984 [myid:] - INFO  [main:Environment@100] - Server environment:user.home=/home/yitian\n2018-07-09 17:23:06,984 [myid:] - INFO  [main:Environment@100] - Server environment:user.dir=/home/yitian\n2018-07-09 17:23:06,988 [myid:] - INFO  [main:ZooKeeperServer@829] - tickTime set to 10000\n2018-07-09 17:23:06,988 [myid:] - INFO  [main:ZooKeeperServer@838] - minSessionTimeout set to -1\n2018-07-09 17:23:06,988 [myid:] - INFO  [main:ZooKeeperServer@847] - maxSessionTimeout set to -1\n2018-07-09 17:23:06,993 [myid:] - INFO  [main:NIOServerCnxnFactory@89] - binding to port 0.0.0.0/0.0.0.0:2181\n2018-07-09 17:23:07,406 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.24:37094\n2018-07-09 17:23:07,423 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@935] - Client attempting to renew session 0x1647ccaffd300ec at /218.195.228.24:37094\n2018-07-09 17:23:07,426 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@687] - Established session 0x1647ccaffd300ec with negotiated timeout 20000 for client /218.195.228.24:37094\n2018-07-09 17:23:07,434 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647ccaffd300ec type:create cxid:0x35 zxid:0x400000556 txntype:-1 reqpath:n/a Error Path:/aurora/scheduler/member_0000000043 Error:KeeperErrorCode = NodeExists for /aurora/scheduler/member_0000000043\n2018-07-09 17:23:07,435 [myid:] - INFO  [SyncThread:0:FileTxnLog@203] - Creating new log file: log.400000556\n2018-07-09 17:23:07,596 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.24:37096\n2018-07-09 17:23:07,597 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@935] - Client attempting to renew session 0x1647e45ec500007 at /218.195.228.24:37096\n2018-07-09 17:23:07,599 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@687] - Established session 0x1647e45ec500007 with negotiated timeout 30000 for client /218.195.228.24:37096\n2018-07-09 17:23:10,330 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.24:37098\n2018-07-09 17:23:10,332 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.24:37100\n2018-07-09 17:23:10,333 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@896] - Connection request from old client /218.195.228.24:37098; will be dropped if server is in r-o mode\n2018-07-09 17:23:10,333 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@935] - Client attempting to renew session 0x1647e0f2f340001 at /218.195.228.24:37098\n2018-07-09 17:23:10,335 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@687] - Established session 0x1647e0f2f340001 with negotiated timeout 20000 for client /218.195.228.24:37098\n2018-07-09 17:23:10,336 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@896] - Connection request from old client /218.195.228.24:37100; will be dropped if server is in r-o mode\n2018-07-09 17:23:10,337 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@935] - Client attempting to renew session 0x1647e0f2f340003 at /218.195.228.24:37100\n2018-07-09 17:23:10,338 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@687] - Established session 0x1647e0f2f340003 with negotiated timeout 20000 for client /218.195.228.24:37100\n2018-07-09 17:23:10,339 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.24:37102\n2018-07-09 17:23:10,341 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.24:37104\n2018-07-09 17:23:10,342 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@896] - Connection request from old client /218.195.228.24:37102; will be dropped if server is in r-o mode\n2018-07-09 17:23:10,342 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@935] - Client attempting to renew session 0x1647e0f2f340002 at /218.195.228.24:37102\n2018-07-09 17:23:10,344 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@687] - Established session 0x1647e0f2f340002 with negotiated timeout 20000 for client /218.195.228.24:37102\n2018-07-09 17:23:10,345 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@896] - Connection request from old client /218.195.228.24:37104; will be dropped if server is in r-o mode\n2018-07-09 17:23:10,345 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@935] - Client attempting to renew session 0x1647e0f2f340000 at /218.195.228.24:37104\n2018-07-09 17:23:10,346 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@687] - Established session 0x1647e0f2f340000 with negotiated timeout 20000 for client /218.195.228.24:37104\n2018-07-09 17:23:10,347 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.24:37106\n2018-07-09 17:23:10,349 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@896] - Connection request from old client /218.195.228.24:37106; will be dropped if server is in r-o mode\n2018-07-09 17:23:10,349 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@935] - Client attempting to renew session 0x1647ccaffd300ed at /218.195.228.24:37106\n2018-07-09 17:23:10,350 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@687] - Established session 0x1647ccaffd300ed with negotiated timeout 20000 for client /218.195.228.24:37106\n2018-07-09 17:23:10,351 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.24:37108\n2018-07-09 17:23:10,352 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.24:37110\n2018-07-09 17:23:10,353 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@896] - Connection request from old client /218.195.228.24:37108; will be dropped if server is in r-o mode\n2018-07-09 17:23:10,353 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@935] - Client attempting to renew session 0x1647ccaffd300ee at /218.195.228.24:37108\n2018-07-09 17:23:10,355 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@687] - Established session 0x1647ccaffd300ee with negotiated timeout 20000 for client /218.195.228.24:37108\n2018-07-09 17:23:10,356 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@896] - Connection request from old client /218.195.228.24:37110; will be dropped if server is in r-o mode\n2018-07-09 17:23:10,357 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@935] - Client attempting to renew session 0x1647ccaffd300ef at /218.195.228.24:37110\n2018-07-09 17:23:10,358 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@687] - Established session 0x1647ccaffd300ef with negotiated timeout 20000 for client /218.195.228.24:37110\n2018-07-09 17:23:18,175 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.24:37112\n2018-07-09 17:23:18,177 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.24:37112\n2018-07-09 17:23:18,178 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af0000 with negotiated timeout 30000 for client /218.195.228.24:37112\n2018-07-09 17:23:23,438 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.24:37120\n2018-07-09 17:23:23,439 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.24:37120\n2018-07-09 17:23:23,444 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af0001 with negotiated timeout 20000 for client /218.195.228.24:37120\n2018-07-09 17:23:23,450 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x1647e5a08af0001\n2018-07-09 17:23:23,460 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1044] - Closed socket connection for client /218.195.228.24:37120 which had sessionid 0x1647e5a08af0001\n2018-07-09 17:23:26,544 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.43:60796\n2018-07-09 17:23:26,545 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.43:60796\n2018-07-09 17:23:26,547 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af0002 with negotiated timeout 20000 for client /218.195.228.43:60796\n2018-07-09 17:23:26,736 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.28:59652\n2018-07-09 17:23:26,738 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.28:59652\n2018-07-09 17:23:26,740 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af0003 with negotiated timeout 20000 for client /218.195.228.28:59652\n2018-07-09 17:23:28,223 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.19:39240\n2018-07-09 17:23:28,225 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.19:39240\n2018-07-09 17:23:28,229 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af0004 with negotiated timeout 20000 for client /218.195.228.19:39240\n2018-07-09 17:23:30,001 [myid:] - INFO  [SessionTracker:ZooKeeperServer@358] - Expiring session 0x1647e565a5b0003, timeout of 20000ms exceeded\n2018-07-09 17:23:30,002 [myid:] - INFO  [SessionTracker:ZooKeeperServer@358] - Expiring session 0x1647e565a5b0004, timeout of 20000ms exceeded\n2018-07-09 17:23:30,002 [myid:] - INFO  [SessionTracker:ZooKeeperServer@358] - Expiring session 0x1647e565a5b0002, timeout of 20000ms exceeded\n2018-07-09 17:23:30,003 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x1647e565a5b0003\n2018-07-09 17:23:30,003 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x1647e565a5b0004\n2018-07-09 17:23:30,004 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x1647e565a5b0002\n2018-07-09 17:23:31,043 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x1647e5a08af0000\n2018-07-09 17:23:31,045 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@368] - caught end of stream exception\nEndOfStreamException: Unable to read additional data from client sessionid 0x1647e5a08af0000, likely client has closed socket\n    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:239)\n    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203)\n    at java.lang.Thread.run(Thread.java:748)\n2018-07-09 17:23:31,047 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1044] - Closed socket connection for client /218.195.228.24:37112 which had sessionid 0x1647e5a08af0000\n2018-07-09 17:23:40,000 [myid:] - INFO  [SessionTracker:ZooKeeperServer@358] - Expiring session 0x1647e565a5b0009, timeout of 30000ms exceeded\n2018-07-09 17:23:40,001 [myid:] - INFO  [SessionTracker:ZooKeeperServer@358] - Expiring session 0x1647e565a5b000b, timeout of 30000ms exceeded\n2018-07-09 17:23:40,001 [myid:] - INFO  [SessionTracker:ZooKeeperServer@358] - Expiring session 0x1647e565a5b0006, timeout of 30000ms exceeded\n2018-07-09 17:23:40,002 [myid:] - INFO  [SessionTracker:ZooKeeperServer@358] - Expiring session 0x1647e565a5b0007, timeout of 30000ms exceeded\n2018-07-09 17:23:40,002 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x1647e565a5b0009\n2018-07-09 17:23:40,003 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x1647e565a5b000b\n2018-07-09 17:23:40,003 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x1647e565a5b0006\n2018-07-09 17:23:40,004 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x1647e565a5b0007\n2018-07-09 17:23:45,568 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.43:60808\n2018-07-09 17:23:45,570 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@368] - caught end of stream exception\nEndOfStreamException: Unable to read additional data from client sessionid 0x0, likely client has closed socket\n    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:239)\n    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203)\n    at java.lang.Thread.run(Thread.java:748)\n2018-07-09 17:23:45,571 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1044] - Closed socket connection for client /218.195.228.43:60808 (no session established for client)\n2018-07-09 17:23:45,783 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.43:60810\n2018-07-09 17:23:45,784 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.43:60810\n2018-07-09 17:23:45,789 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af0005 with negotiated timeout 20000 for client /218.195.228.43:60810\n2018-07-09 17:23:46,711 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.43:60812\n2018-07-09 17:23:46,712 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@896] - Connection request from old client /218.195.228.43:60812; will be dropped if server is in r-o mode\n2018-07-09 17:23:46,712 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.43:60812\n2018-07-09 17:23:46,714 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af0006 with negotiated timeout 30000 for client /218.195.228.43:60812\n2018-07-09 17:23:58,931 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.19:39252\n2018-07-09 17:23:58,932 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@368] - caught end of stream exception\nEndOfStreamException: Unable to read additional data from client sessionid 0x0, likely client has closed socket\n    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:239)\n    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203)\n    at java.lang.Thread.run(Thread.java:748)\n2018-07-09 17:23:58,934 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1044] - Closed socket connection for client /218.195.228.19:39252 (no session established for client)\n2018-07-09 17:23:58,944 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.19:39254\n2018-07-09 17:23:58,946 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.19:39254\n2018-07-09 17:23:58,950 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af0007 with negotiated timeout 20000 for client /218.195.228.19:39254\n2018-07-09 17:23:59,012 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.19:39258\n2018-07-09 17:23:59,013 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@896] - Connection request from old client /218.195.228.19:39258; will be dropped if server is in r-o mode\n2018-07-09 17:23:59,013 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.19:39258\n2018-07-09 17:23:59,017 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af0008 with negotiated timeout 30000 for client /218.195.228.19:39258\n2018-07-09 17:24:00,095 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.19:39262\n2018-07-09 17:24:00,098 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.19:39262\n2018-07-09 17:24:00,102 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af0009 with negotiated timeout 30000 for client /218.195.228.19:39262\n2018-07-09 17:24:12,444 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.28:59666\n2018-07-09 17:24:12,446 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@368] - caught end of stream exception\nEndOfStreamException: Unable to read additional data from client sessionid 0x0, likely client has closed socket\n    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:239)\n    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203)\n    at java.lang.Thread.run(Thread.java:748)\n2018-07-09 17:24:12,448 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1044] - Closed socket connection for client /218.195.228.28:59666 (no session established for client)\n2018-07-09 17:24:12,449 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.28:59668\n2018-07-09 17:24:12,450 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.28:59668\n2018-07-09 17:24:12,455 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af000a with negotiated timeout 20000 for client /218.195.228.28:59668\n2018-07-09 17:24:12,840 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.28:59670\n2018-07-09 17:24:12,845 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@896] - Connection request from old client /218.195.228.28:59670; will be dropped if server is in r-o mode\n2018-07-09 17:24:12,846 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.28:59670\n2018-07-09 17:24:12,854 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af000b with negotiated timeout 30000 for client /218.195.228.28:59670\n2018-07-09 17:24:42,664 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.24:37232\n2018-07-09 17:24:42,665 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.24:37232\n2018-07-09 17:24:42,668 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af000c with negotiated timeout 30000 for client /218.195.228.24:37232\n2018-07-09 17:24:42,807 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@368] - caught end of stream exception\nEndOfStreamException: Unable to read additional data from client sessionid 0x1647e5a08af0008, likely client has closed socket\n    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:239)\n    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203)\n    at java.lang.Thread.run(Thread.java:748)\n2018-07-09 17:24:42,808 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1044] - Closed socket connection for client /218.195.228.19:39258 which had sessionid 0x1647e5a08af0008\n2018-07-09 17:24:42,832 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@368] - caught end of stream exception\nEndOfStreamException: Unable to read additional data from client sessionid 0x1647e5a08af000b, likely client has closed socket\n    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:239)\n    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203)\n    at java.lang.Thread.run(Thread.java:748)\n2018-07-09 17:24:42,833 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1044] - Closed socket connection for client /218.195.228.28:59670 which had sessionid 0x1647e5a08af000b\n2018-07-09 17:24:42,837 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000c type:create cxid:0x21 zxid:0x40000057d txntype:-1 reqpath:n/a Error Path:/heron/locks/AuroraMonitorSentenceWordCountTopology__updateTopology/locks Error:KeeperErrorCode = NoNode for /heron/locks/AuroraMonitorSentenceWordCountTopology__updateTopology/locks\n2018-07-09 17:24:42,842 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@368] - caught end of stream exception\nEndOfStreamException: Unable to read additional data from client sessionid 0x1647e5a08af0006, likely client has closed socket\n    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:239)\n    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203)\n    at java.lang.Thread.run(Thread.java:748)\n2018-07-09 17:24:42,843 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1044] - Closed socket connection for client /218.195.228.43:60812 which had sessionid 0x1647e5a08af0006\n2018-07-09 17:24:42,873 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.28:59706\n2018-07-09 17:24:42,873 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@896] - Connection request from old client /218.195.228.28:59706; will be dropped if server is in r-o mode\n2018-07-09 17:24:42,873 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.28:59706\n2018-07-09 17:24:42,875 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af000d with negotiated timeout 30000 for client /218.195.228.28:59706\n2018-07-09 17:24:42,914 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.43:60848\n2018-07-09 17:24:42,916 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@896] - Connection request from old client /218.195.228.43:60848; will be dropped if server is in r-o mode\n2018-07-09 17:24:42,916 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.43:60848\n2018-07-09 17:24:42,922 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af000e with negotiated timeout 30000 for client /218.195.228.43:60848\n2018-07-09 17:24:42,968 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000c type:create cxid:0x2a zxid:0x400000583 txntype:-1 reqpath:n/a Error Path:/heron/locks/AuroraMonitorSentenceWordCountTopology__updateTopology/leases Error:KeeperErrorCode = NoNode for /heron/locks/AuroraMonitorSentenceWordCountTopology__updateTopology/leases\n2018-07-09 17:24:43,002 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x1647e5a08af000c\n2018-07-09 17:24:43,003 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1044] - Closed socket connection for client /218.195.228.24:37232 which had sessionid 0x1647e5a08af000c\n2018-07-09 17:24:52,838 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.19:39274\n2018-07-09 17:24:52,840 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@896] - Connection request from old client /218.195.228.19:39274; will be dropped if server is in r-o mode\n2018-07-09 17:24:52,840 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.19:39274\n2018-07-09 17:24:52,845 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af000f with negotiated timeout 30000 for client /218.195.228.19:39274\n2018-07-09 17:24:52,851 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b4329e6 zxid:0x40000058a txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:24:53,869 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b4329e8 zxid:0x40000058b txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:24:54,874 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b4329ea zxid:0x40000058c txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:24:55,880 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b4329ec zxid:0x40000058d txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:24:56,888 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b4329ee zxid:0x40000058e txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:24:57,894 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b4329f0 zxid:0x40000058f txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:24:58,897 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b4329f2 zxid:0x400000590 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:24:59,901 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b4329f4 zxid:0x400000591 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:00,906 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b4329f6 zxid:0x400000592 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:01,912 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b4329f8 zxid:0x400000593 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:02,921 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b4329fa zxid:0x400000594 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:03,925 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b4329fc zxid:0x400000595 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:04,934 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b4329fe zxid:0x400000596 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:05,937 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b432a00 zxid:0x400000597 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:06,942 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b432a02 zxid:0x400000598 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:07,955 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b432a04 zxid:0x400000599 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:08,961 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b432a06 zxid:0x40000059a txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:09,969 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b432a08 zxid:0x40000059b txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:10,000 [myid:] - INFO  [SessionTracker:ZooKeeperServer@358] - Expiring session 0x1647e5a08af000b, timeout of 30000ms exceeded\n2018-07-09 17:25:10,001 [myid:] - INFO  [SessionTracker:ZooKeeperServer@358] - Expiring session 0x1647e5a08af0006, timeout of 30000ms exceeded\n2018-07-09 17:25:10,002 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x1647e5a08af000b\n2018-07-09 17:25:10,002 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x1647e5a08af0006\n2018-07-09 17:25:10,977 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b432a0a zxid:0x40000059e txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:11,983 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b432a0c zxid:0x40000059f txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:12,986 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b432a0e zxid:0x4000005a0 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:13,995 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b432a10 zxid:0x4000005a1 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:15,003 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b432a12 zxid:0x4000005a2 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:16,006 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b432a14 zxid:0x4000005a3 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:17,010 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b432a16 zxid:0x4000005a4 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:18,019 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b432a18 zxid:0x4000005a5 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:19,023 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x1647e5a08af000f type:create cxid:0x5b432a1a zxid:0x4000005a6 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-09 17:25:20,001 [myid:] - INFO  [SessionTracker:ZooKeeperServer@358] - Expiring session 0x1647e5a08af0008, timeout of 30000ms exceeded\n2018-07-09 17:25:20,003 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x1647e5a08af0008\n2018-07-09 17:25:20,008 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@368] - caught end of stream exception\nEndOfStreamException: Unable to read additional data from client sessionid 0x1647e5a08af000f, likely client has closed socket\n    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:239)\n    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203)\n    at java.lang.Thread.run(Thread.java:748)\n2018-07-09 17:25:20,011 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1044] - Closed socket connection for client /218.195.228.19:39274 which had sessionid 0x1647e5a08af000f\n2018-07-09 17:25:30,036 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.19:39292\n2018-07-09 17:25:30,039 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@896] - Connection request from old client /218.195.228.19:39292; will be dropped if server is in r-o mode\n2018-07-09 17:25:30,039 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.19:39292\n2018-07-09 17:25:30,041 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x1647e5a08af0010 with negotiated timeout 30000 for client /218.195.228.19:39292\n2018-07-09 17:25:50,000 [myid:] - INFO  [SessionTracker:ZooKeeperServer@358] - Expiring session 0x1647e5a08af000f, timeout of 30000ms exceeded\n2018-07-09 17:25:50,000 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x1647e5a08af000f\nAnd there are my codes :\n```\npublic void doSchedule(PackingPlan packingPlan) {\n        String stateMgrClass = Context.stateManagerClass(this.config); // get state manager instance\n        IStateManager stateMgr = null;\n        try {\n            stateMgr = ReflectionUtils.newInstance(stateMgrClass);\n            FileUtils.writeToFile(filename, \"Create IStateManager object success...\");\n        } catch (ClassNotFoundException | InstantiationException | IllegalAccessException e) {\n            e.printStackTrace();\n        }\n    try {\n        stateMgr.initialize(this.config);\n        SchedulerStateManagerAdaptor stateManagerAdaptor = new SchedulerStateManagerAdaptor(stateMgr, 5000);\n\n        // Then created the new packingplan. It is omitted here.\n        PackingPlans.PackingPlan currentPackingPlan = serializer.toProto(packingPlan);\n        PackingPlans.PackingPlan proposedPackingPlan = serializer.toProto(newPackingPlan);\n\n        // build updatetopologyrequest object to update topogolgy\n        Scheduler.UpdateTopologyRequest updateTopologyRequest =\n                Scheduler.UpdateTopologyRequest.newBuilder()\n                        .setCurrentPackingPlan(currentPackingPlan)\n                        .setProposedPackingPlan(proposedPackingPlan)\n                        .build();\n\n        // create runtime config using statemanageradaptor, this adaptor not included topology information\n        // just add topologyname and adaptor to build schedulerCLient object\n        Config primaryRuntime = LauncherUtils.getInstance().createPrimaryRuntime(topology);\n        Config newRuntime = Config.newBuilder()\n                .putAll(primaryRuntime)\n                .put(Key.TOPOLOGY_NAME, Context.topologyName(config))\n                .put(Key.SCHEDULER_STATE_MANAGER_ADAPTOR, stateManagerAdaptor)\n                .build();\n\n        // Create a ISchedulerClient basing on the config\n        ISchedulerClient schedulerClient = getSchedulerClient(newRuntime);\n\n        // In fact, I can't get the physicalplan right here by testing. I don't know why.\n        // TopologyAPI.Topology topology3 = stateManagerAdaptor.getPhysicalPlan(topologyName).getTopology();\n\n        if (!schedulerClient.updateTopology(updateTopologyRequest)) {\n            throw new TopologyRuntimeManagementException(String.format(\n                    \"Failed to update \" + topology.getName() + \" with Scheduler, updateTopologyRequest=\"\n                            + updateTopologyRequest));\n        }\n\n    } finally {\n        // close zookeeper client connnection\n        SysUtils.closeIgnoringExceptions(stateMgr);\n    }\n}\n\nAnd there is the output using --verbose:\n[2018-07-06 11:08:56 +0000] [DEBUG]: Input Command Line Args: {'config_property': [], 'dry_run_format': 'colored_table', 'topology-file-name': '/home/yitian/aurora-topolgoies/heron-java-streamlet-api-example-latest-jar-with-dependencies.jar', 'cluster/[role]/[env]': 'aurora/yitian/devel', 'dry_run': False, 'config_path': '/home/yitian/.heron/conf', 'subcommand': 'submit', 'deploy_deactivated': 'True', 'topology_main_jvm_property': [], 'extra_launch_classpath': '', 'service_url': '', 'topology-class-name': 'zyt.custom.topology.aurora.AuroraMonitorSentenceWordCountTopology', 'verbose': 'True'}\n[2018-07-06 11:08:56 +0000] [INFO]: Using cluster definition in /home/yitian/.heron/conf/aurora\n[2018-07-06 11:08:56 +0000] [DEBUG]: Processed Command Line Args: {'config_property': [], 'topology-file-name': '/home/yitian/aurora-topolgoies/heron-java-streamlet-api-example-latest-jar-with-dependencies.jar', 'verbose': 'True', 'subcommand': 'submit', 'deploy_deactivated': 'True', 'submit_user': 'yitian', 'cluster': 'aurora', 'extra_launch_classpath': '', 'deploy_mode': 'direct', 'role': 'yitian', 'dry_run_format': 'colored_table', 'override_config_file': '/tmp/tmpRpLWgs/override.yaml', 'dry_run': False, 'config_path': '/home/yitian/.heron/conf/aurora', 'topology_main_jvm_property': [], 'environ': 'devel', 'service_url': '', 'topology-class-name': 'zyt.custom.topology.aurora.AuroraMonitorSentenceWordCountTopology'}\n[2018-07-06 11:08:56 +0000] [DEBUG]: Submit Args {'config_property': [], 'topology-file-name': '/home/yitian/aurora-topolgoies/heron-java-streamlet-api-example-latest-jar-with-dependencies.jar', 'verbose': 'True', 'subcommand': 'submit', 'deploy_deactivated': 'True', 'submit_user': 'yitian', 'cluster': 'aurora', 'extra_launch_classpath': '', 'deploy_mode': 'direct', 'role': 'yitian', 'dry_run_format': 'colored_table', 'override_config_file': '/tmp/tmpRpLWgs/override.yaml', 'dry_run': False, 'config_path': '/home/yitian/.heron/conf/aurora', 'topology_main_jvm_property': [], 'environ': 'devel', 'service_url': '', 'topology-class-name': 'zyt.custom.topology.aurora.AuroraMonitorSentenceWordCountTopology'}\n[2018-07-06 11:08:56 +0000] [DEBUG]: Invoking class using command: /usr/java/jdk1.8.0_151/bin/java -client -Xmx1g -cp /home/yitian/aurora-topolgoies/heron-java-streamlet-api-example-latest-jar-with-dependencies.jar:/home/yitian/.heron/lib/third_party/* zyt.custom.topology.aurora.AuroraMonitorSentenceWordCountTopology AuroraMonitorSentenceWordCountTopology''\n[2018-07-06 11:08:56 +0000] [DEBUG]: Heron options: {cmdline.topologydefn.tmpdirectory=/tmp/tmpwOlWfZ,cmdline.topology.initial.state=PAUSED}\n[2018-07-06 11:08:56 +0000] [INFO]: Launching topology: 'AuroraMonitorSentenceWordCountTopology'\n[2018-07-06 11:08:56 +0000] [DEBUG]: Invoking class using command:/usr/java/jdk1.8.0_151/bin/java -client -Xmx1g -cp :/home/yitian/.heron/lib/scheduler/:/home/yitian/.heron/lib/uploader/:/home/yitian/.heron/lib/statemgr/:/home/yitian/.heron/lib/packing/ com.twitter.heron.scheduler.SubmitterMain --cluster aurora --role yitian --environment devel --submit_user yitian --heron_home /home/yitian/.heron --config_path /home/yitian/.heron/conf/aurora --override_config_file /tmp/tmpRpLWgs/override.yaml --release_file /home/yitian/.heron/release.yaml --topology_package /tmp/tmpwOlWfZ/topology.tar.gz --topology_defn /tmp/tmpwOlWfZ/AuroraMonitorSentenceWordCountTopology.defn --topology_bin heron-java-streamlet-api-example-latest-jar-with-dependencies.jar --verbose''\n[2018-07-06 11:08:56 +0000] [DEBUG]: Heron options: {cmdline.topologydefn.tmpdirectory=/tmp/tmpwOlWfZ,cmdline.topology.initial.state=PAUSED}\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Config file /home/yitian/.heron/conf/aurora/cluster.yaml does not exist\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Reading config file /home/yitian/.heron/conf/aurora/client.yaml\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Successfully read config file /home/yitian/.heron/conf/aurora/client.yaml\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Config file /home/yitian/.heron/conf/aurora/healthmgr.yaml does not exist\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Reading config file /home/yitian/.heron/conf/aurora/packing.yaml\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Successfully read config file /home/yitian/.heron/conf/aurora/packing.yaml\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Reading config file /home/yitian/.heron/conf/aurora/scheduler.yaml\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Successfully read config file /home/yitian/.heron/conf/aurora/scheduler.yaml\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Reading config file /home/yitian/.heron/conf/aurora/statemgr.yaml\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Successfully read config file /home/yitian/.heron/conf/aurora/statemgr.yaml\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Reading config file /home/yitian/.heron/conf/aurora/uploader.yaml\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Successfully read config file /home/yitian/.heron/conf/aurora/uploader.yaml\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Reading config file /home/yitian/.heron/conf/aurora/stateful.yaml\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Successfully read config file /home/yitian/.heron/conf/aurora/stateful.yaml\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Reading config file /home/yitian/.heron/release.yaml\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Successfully read config file /home/yitian/.heron/release.yaml\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Reading config file /tmp/tmpRpLWgs/override.yaml\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.common.config.ConfigReader: Successfully read config file /tmp/tmpRpLWgs/override.yaml\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.scheduler.SubmitterMain: Static config loaded successfully\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.scheduler.SubmitterMain: (\"heron.binaries.cpp.instance\", /home/yitian/.heron/bin/heron-cpp-instance)\n(\"heron.binaries.downloader\", /home/yitian/.heron/bin/heron-downloader)\n(\"heron.binaries.executor\", /home/yitian/.heron/bin/heron-executor)\n(\"heron.binaries.python.instance\", /home/yitian/.heron/bin/heron-python-instance)\n(\"heron.binaries.shell\", /home/yitian/.heron/bin/heron-shell)\n(\"heron.binaries.stmgr\", /home/yitian/.heron/bin/heron-stmgr)\n(\"heron.binaries.tmaster\", /home/yitian/.heron/bin/heron-tmaster)\n(\"heron.build.git.revision\", 874feb31b1ad9df6ea4a51d58b573750468ad28d)\n(\"heron.build.git.status\", Clean)\n(\"heron.build.host\", ci-server-01)\n(\"heron.build.time\", Sat Nov 18 01:07:07 UTC 2017)\n(\"heron.build.timestamp\", 1510967227000)\n(\"heron.build.user\", release-agent1)\n(\"heron.build.version\", 0.17.1)\n(\"heron.ckptmgr.network.options.maximum.packetsize.bytes\", 10485760)\n(\"heron.ckptmgr.network.options.socket.receive.buffer.size.bytes\", 655360)\n(\"heron.ckptmgr.network.options.socket.send.buffer.size.bytes\", 655360)\n(\"heron.ckptmgr.network.read.batch.size.bytes\", 32768)\n(\"heron.ckptmgr.network.read.batch.time.ms\", 16)\n(\"heron.ckptmgr.network.write.batch.size.bytes\", 32768)\n(\"heron.ckptmgr.network.write.batch.time.ms\", 16)\n(\"heron.class.launcher\", zyt.custom.my.scheduler.aurora.AuroraHotEdgeLauncher)\n(\"heron.class.packing.algorithm\", com.twitter.heron.packing.roundrobin.RoundRobinPacking)\n(\"heron.class.repacking.algorithm\", com.twitter.heron.packing.binpacking.FirstFitDecreasingPacking)\n(\"heron.class.scheduler\", zyt.custom.my.scheduler.aurora.AuroraCustomScheduler)\n(\"heron.class.scheduler.aurora.controller.cli\", true)\n(\"heron.class.state.manager\", com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager)\n(\"heron.class.uploader\", com.twitter.heron.uploader.hdfs.HdfsUploader)\n(\"heron.classpath.ckptmgr\", /home/yitian/.heron/lib/ckptmgr/)\n(\"heron.classpath.healthmgr\", /home/yitian/.heron/lib/healthmgr/)\n(\"heron.classpath.instance\", /home/yitian/.heron/lib/instance/)\n(\"heron.classpath.metrics.manager\", /home/yitian/.heron/lib/metricsmgr/)\n(\"heron.classpath.metricscache.manager\", /home/yitian/.heron/lib/metricscachemgr/)\n(\"heron.classpath.packing\", /home/yitian/.heron/lib/packing/)\n(\"heron.classpath.scheduler\", /home/yitian/.heron/lib/scheduler/)\n(\"heron.classpath.statefulstorage\", /home/yitian/.heron/lib/statefulstorage/)\n(\"heron.classpath.statemgr\", /home/yitian/.heron/lib/statemgr/)\n(\"heron.classpath.uploader\", /home/yitian/.heron/lib/uploader/)\n(\"heron.config.cluster\", aurora)\n(\"heron.config.dry_run\", false)\n(\"heron.config.dry_run_format_type\", TABLE)\n(\"heron.config.environ\", devel)\n(\"heron.config.file.client.yaml\", /home/yitian/.heron/conf/aurora/client.yaml)\n(\"heron.config.file.cluster.yaml\", /home/yitian/.heron/conf/aurora/cluster.yaml)\n(\"heron.config.file.healthmgr.yaml\", /home/yitian/.heron/conf/aurora/healthmgr.yaml)\n(\"heron.config.file.metrics.yaml\", /home/yitian/.heron/conf/aurora/metrics_sinks.yaml)\n(\"heron.config.file.override.yaml\", /home/yitian/.heron/conf/aurora/override.yaml)\n(\"heron.config.file.packing.yaml\", /home/yitian/.heron/conf/aurora/packing.yaml)\n(\"heron.config.file.scheduler.yaml\", /home/yitian/.heron/conf/aurora/scheduler.yaml)\n(\"heron.config.file.stateful.yaml\", /home/yitian/.heron/conf/aurora/stateful.yaml)\n(\"heron.config.file.statemgr.yaml\", /home/yitian/.heron/conf/aurora/statemgr.yaml)\n(\"heron.config.file.system.yaml\", /home/yitian/.heron/conf/aurora/heron_internals.yaml)\n(\"heron.config.file.uploader.yaml\", /home/yitian/.heron/conf/aurora/uploader.yaml)\n(\"heron.config.is.env.required\", true)\n(\"heron.config.is.role.required\", true)\n(\"heron.config.role\", yitian)\n(\"heron.config.submit_user\", yitian)\n(\"heron.config.verbose\", true)\n(\"heron.directory.bin\", /home/yitian/.heron/bin)\n(\"heron.directory.cluster.conf\", ./heron-conf)\n(\"heron.directory.cluster.home\", ./heron-core)\n(\"heron.directory.conf\", /home/yitian/.heron/conf/aurora)\n(\"heron.directory.dist\", /home/yitian/.heron/dist)\n(\"heron.directory.etc\", /home/yitian/.heron/etc)\n(\"heron.directory.home\", /home/yitian/.heron)\n(\"heron.directory.java.home\", /usr/java/jdk1.8.0_151)\n(\"heron.directory.lib\", /home/yitian/.heron/lib)\n(\"heron.directory.sandbox.java.home\", /usr/lib/jvm/java-1.8.0-openjdk-amd64)\n(\"heron.jars.scheduler\", /home/yitian/.heron/lib/scheduler/heron-scheduler.jar)\n(\"heron.package.core.uri\", /heron/dist/heron-core.tar.gz)\n(\"heron.resources.instance.cpu\", 1.0)\n(\"heron.resources.instance.disk\", ByteAmount{1 GB (1073741824 bytes)})\n(\"heron.resources.instance.ram\", ByteAmount{1 GB (1073741824 bytes)})\n(\"heron.resources.stmgr.ram\", ByteAmount{1 GB (1073741824 bytes)})\n(\"heron.scheduler.is.service\", false)\n(\"heron.scheduler.is.tunnel.needed\", false)\n(\"heron.scheduler.job.kill.retry.interval.ms\", 2000)\n(\"heron.scheduler.job.max.kill.attempts\", 5)\n(\"heron.scheduler.tunnel.connection.retry.count\", 2)\n(\"heron.scheduler.tunnel.connection.timeout.ms\", 1000)\n(\"heron.scheduler.tunnel.host\", my.tunnel.host)\n(\"heron.scheduler.tunnel.retry.interval.ms\", 1000)\n(\"heron.scheduler.tunnel.verify.count\", 10)\n(\"heron.statefulstorage.classname\", com.twitter.heron.statefulstorage.hdfs.HDFSStorage)\n(\"heron.statefulstorage.config\", {heron.statefulstorage.classpath=$(hadoop --config /path/to/configs classpath), heron.statefulstorage.hdfs.root.path=/user/heron/checkpoints})\n(\"heron.statemgr.connection.string\", heron01:2181)\n(\"heron.statemgr.is.tunnel.needed\", false)\n(\"heron.statemgr.root.path\", /heron)\n(\"heron.statemgr.tunnel.connection.retry.count\", 2)\n(\"heron.statemgr.tunnel.connection.timeout.ms\", 30000)\n(\"heron.statemgr.tunnel.host\", my.tunnel.host)\n(\"heron.statemgr.tunnel.retry.interval.ms\", 1000)\n(\"heron.statemgr.tunnel.verify.count\", 10)\n(\"heron.statemgr.zookeeper.is.initialize.tree\", true)\n(\"heron.topology.binary.file\", heron-java-streamlet-api-example-latest-jar-with-dependencies.jar)\n(\"heron.topology.definition.file\", /tmp/tmpwOlWfZ/AuroraMonitorSentenceWordCountTopology.defn)\n(\"heron.topology.id\", AuroraMonitorSentenceWordCountTopology85c19772-4cd4-4a5c-8cf3-d6306f792067)\n(\"heron.topology.name\", AuroraMonitorSentenceWordCountTopology)\n(\"heron.topology.package.file\", /tmp/tmpwOlWfZ/topology.tar.gz)\n(\"heron.topology.package.type\", JAR)\n(\"heron.uploader.hdfs.config.directory\", /home/yitian/hadoop/hadoop-2.7.4/etc/hadoop)\n(\"heron.uploader.hdfs.topologies.directory.uri\", /heron/topologies/aurora)\n[2018-07-06 11:08:56 +0800] [FINE] com.twitter.heron.statemgr.FileSystemStateManager: File system state manager root address: /heron\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/home/yitian/.heron/lib/uploader/heron-dlog-uploader.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/home/yitian/.heron/lib/statemgr/heron-zookeeper-statemgr.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.JDK14LoggerFactory]\n[2018-07-06 11:08:57 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Starting Curator client connecting to: heron01:2181\n[2018-07-06 11:08:57 +0800] [INFO] org.apache.curator.framework.imps.CuratorFrameworkImpl: Starting\n[2018-07-06 11:08:57 +0800] [FINE] org.apache.curator.CuratorZookeeperClient: Starting\n[2018-07-06 11:08:57 +0800] [FINE] org.apache.curator.ConnectionState: Starting\n[2018-07-06 11:08:57 +0800] [FINE] org.apache.curator.ConnectionState: reset\n[2018-07-06 11:08:57 +0800] [INFO] org.apache.curator.framework.state.ConnectionStateManager: State change: CONNECTED\n[2018-07-06 11:08:57 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: TMaster location directory: /heron/tmasters\n[2018-07-06 11:08:57 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: MetricsCache location directory: /heron/metricscaches\n[2018-07-06 11:08:57 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Topologies directory: /heron/topologies\n[2018-07-06 11:08:57 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Packing plan directory: /heron/packingplans\n[2018-07-06 11:08:57 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Physical plan directory: /heron/pplans\n[2018-07-06 11:08:57 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Execution state directory: /heron/executionstate\n[2018-07-06 11:08:57 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Scheduler location directory: /heron/schedulers\n[2018-07-06 11:08:57 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Stateful checkpoints directory: /heron/statefulcheckpoints\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: connection-state-parent-process - 5 ms\n[2018-07-06 11:08:57 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Distributed locks directory: /heron/locks\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 7 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 1 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 0 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 0 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 0 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 0 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 0 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 0 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 0 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 1 ms\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:08:57 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Directory tree initialized.\n[2018-07-06 11:08:57 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Checking existence of path: /heron/topologies/AuroraMonitorSentenceWordCountTopology\n[2018-07-06 11:08:57 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:08:57 +0800] [FINE] com.twitter.heron.scheduler.SubmitterMain: Topology AuroraMonitorSentenceWordCountTopology to be submitted\n[2018-07-06 11:08:57 +0800] [FINE] com.twitter.heron.spi.utils.ShellUtils: Running synced process: hadoop --config /home/yitian/hadoop/hadoop-2.7.4/etc/hadoop fs -test -e /heron/topologies/aurora''  \n[2018-07-06 11:08:57 +0800] [FINE] com.twitter.heron.spi.utils.ShellUtils: Process output (stdout+stderr):  \n[2018-07-06 11:08:58 +0800] [INFO] com.twitter.heron.uploader.hdfs.HdfsUploader: Target topology file already exists at '/heron/topologies/aurora/AuroraMonitorSentenceWordCountTopology-yitian-tag-0-1395922594708948980.tar.gz'. Overwriting it now  \n[2018-07-06 11:08:58 +0800] [INFO] com.twitter.heron.uploader.hdfs.HdfsUploader: Uploading topology package at '/tmp/tmpwOlWfZ/topology.tar.gz' to target HDFS at '/heron/topologies/aurora/AuroraMonitorSentenceWordCountTopology-yitian-tag-0-1395922594708948980.tar.gz'  \n[2018-07-06 11:08:58 +0800] [FINE] com.twitter.heron.spi.utils.ShellUtils: Running synced process:hadoop --config /home/yitian/hadoop/hadoop-2.7.4/etc/hadoop fs -copyFromLocal -f /tmp/tmpwOlWfZ/topology.tar.gz /heron/topologies/aurora/AuroraMonitorSentenceWordCountTopology-yitian-tag-0-1395922594708948980.tar.gz''\n[2018-07-06 11:08:58 +0800] [FINE] com.twitter.heron.spi.utils.ShellUtils: Process output (stdout+stderr):\n[2018-07-06 11:09:01 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: CreateBuilderImpl-Foreground - 5 ms\n[2018-07-06 11:09:01 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Created node for path: /heron/topologies/AuroraMonitorSentenceWordCountTopology\n[2018-07-06 11:09:01 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: CreateBuilderImpl-Foreground - 10 ms\n[2018-07-06 11:09:01 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Created node for path: /heron/packingplans/AuroraMonitorSentenceWordCountTopology\n[2018-07-06 11:09:01 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: CreateBuilderImpl-Foreground - 2 ms\n[2018-07-06 11:09:01 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Created node for path: /heron/executionstate/AuroraMonitorSentenceWordCountTopology\n[2018-07-06 11:09:01 +0800] [INFO] zyt.custom.my.scheduler.aurora.AuroraLauncher: Launching WordCountTopology in aurora\n[2018-07-06 11:09:01 +0800] [INFO] com.twitter.heron.scheduler.utils.SchedulerUtils: Updating scheduled-resource in packing plan: AuroraMonitorSentenceWordCountTopology\n[2018-07-06 11:09:01 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: GetDataBuilderImpl-Background - 2 ms\n[2018-07-06 11:09:01 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: DeleteBuilderImpl-Foreground - 2 ms\n[2018-07-06 11:09:01 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Deleted node for path: /heron/packingplans/AuroraMonitorSentenceWordCountTopology\n[2018-07-06 11:09:01 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: CreateBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:09:01 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Created node for path: /heron/packingplans/AuroraMonitorSentenceWordCountTopology\n[2018-07-06 11:09:01 +0800] [FINE] com.twitter.heron.spi.utils.ShellUtils: Running synced process: ``aurora job create --wait-until RUNNING --bind NUM_CONTAINERS=3 --bind EXECUTOR_BINARY=./heron-core/bin/heron-executor --bind METRICSCACHEMGR_CLASSPATH=./heron-core/lib/metricscachemgr/ --bind CPP_INSTANCE_BINARY=./heron-core/bin/heron-cpp-instance --bind METRICSMGR_CLASSPATH=./heron-core/lib/metricsmgr/ --bind SHELL_BINARY=./heron-core/bin/heron-shell --bind OVERRIDE_YAML=./heron-conf/override.yaml --bind TOPOLOGY_PACKAGE_TYPE=jar --bind CORE_PACKAGE_URI=/heron/dist/heron-core.tar.gz --bind CPUS_PER_CONTAINER=2.0 --bind INSTANCE_CLASSPATH=./heron-core/lib/instance/ --bind TMASTER_BINARY=./heron-core/bin/heron-tmaster --bind IS_STATEFUL_ENABLED=false --bind RAM_PER_CONTAINER=3758096384 --bind TOPOLOGY_PACKAGE_URI=/heron/topologies/aurora/AuroraMonitorSentenceWordCountTopology-yitian-tag-0-1395922594708948980.tar.gz --bind STATEMGR_ROOT_PATH=/heron --bind JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64 --bind INSTANCE_JVM_OPTS_IN_BASE64=\"\" --bind TOPOLOGY_BINARY_FILE=heron-java-streamlet-api-example-latest-jar-with-dependencies.jar --bind METRICS_YAML=./heron-conf/metrics_sinks.yaml --bind STATEFUL_CONFIG_YAML=./heron-conf/stateful.yaml --bind TOPOLOGY_DEFINITION_FILE=AuroraMonitorSentenceWordCountTopology.defn --bind TOPOLOGY_NAME=AuroraMonitorSentenceWordCountTopology --bind SYSTEM_YAML=./heron-conf/heron_internals.yaml --bind CKPTMGR_CLASSPATH=./heron-core/lib/ckptmgr/:./heron-core/lib/statefulstorage/: --bind COMPONENT_RAMMAP=split:536870912,count:536870912,spout:536870912 --bind COMPONENT_JVM_OPTS_IN_BASE64=\"\" --bind PYTHON_INSTANCE_BINARY=./heron-core/bin/heron-python-instance --bind ROLE=yitian --bind ENVIRON=devel --bind TOPOLOGY_ID=AuroraMonitorSentenceWordCountTopology85c19772-4cd4-4a5c-8cf3-d6306f792067 --bind STATEMGR_CONNECTION_STRING=heron01:2181 --bind STMGR_BINARY=./heron-core/bin/heron-stmgr --bind TOPOLOGY_CLASSPATH=heron-java-streamlet-api-example-latest-jar-with-dependencies.jar --bind HEALTHMGR_CLASSPATH=./heron-core/lib/healthmgr/ --bind HEALTHMGR_MODE=disabled --bind TIER=preemptible --bind SCHEDULER_CLASSPATH=./heron-core/lib/scheduler/:./heron-core/lib/packing/:./heron-core/lib/statemgr/ --bind DISK_PER_CONTAINER=3221225472 --bind CLUSTER=aurora aurora/yitian/devel/AuroraMonitorSentenceWordCountTopology /home/yitian/.heron/conf/aurora/heron.aurora --verbose''\n[2018-07-06 11:09:01 +0800] [FINE] com.twitter.heron.spi.utils.ShellUtils: Process output (stdout+stderr):\nDEBUG] Command=(['job', 'create', '--wait-until', 'RUNNING', '--bind', 'NUM_CONTAINERS=3', '--bind', 'EXECUTOR_BINARY=./heron-core/bin/heron-executor', '--bind', 'METRICSCACHEMGR_CLASSPATH=./heron-core/lib/metricscachemgr/', '--bind', 'CPP_INSTANCE_BINARY=./heron-core/bin/heron-cpp-instance', '--bind', 'METRICSMGR_CLASSPATH=./heron-core/lib/metricsmgr/', '--bind', 'SHELL_BINARY=./heron-core/bin/heron-shell', '--bind', 'OVERRIDE_YAML=./heron-conf/override.yaml', '--bind', 'TOPOLOGY_PACKAGE_TYPE=jar', '--bind', 'CORE_PACKAGE_URI=/heron/dist/heron-core.tar.gz', '--bind', 'CPUS_PER_CONTAINER=2.0', '--bind', 'INSTANCE_CLASSPATH=./heron-core/lib/instance/', '--bind', 'TMASTER_BINARY=./heron-core/bin/heron-tmaster', '--bind', 'IS_STATEFUL_ENABLED=false', '--bind', 'RAM_PER_CONTAINER=3758096384', '--bind', 'TOPOLOGY_PACKAGE_URI=/heron/topologies/aurora/AuroraMonitorSentenceWordCountTopology-yitian-tag-0-1395922594708948980.tar.gz', '--bind', 'STATEMGR_ROOT_PATH=/heron', '--bind', 'JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64', '--bind', 'INSTANCE_JVM_OPTS_IN_BASE64=\"\"', '--bind', 'TOPOLOGY_BINARY_FILE=heron-java-streamlet-api-example-latest-jar-with-dependencies.jar', '--bind', 'METRICS_YAML=./heron-conf/metrics_sinks.yaml', '--bind', 'STATEFUL_CONFIG_YAML=./heron-conf/stateful.yaml', '--bind', 'TOPOLOGY_DEFINITION_FILE=AuroraMonitorSentenceWordCountTopology.defn', '--bind', 'TOPOLOGY_NAME=AuroraMonitorSentenceWordCountTopology', '--bind', 'SYSTEM_YAML=./heron-conf/heron_internals.yaml', '--bind', 'CKPTMGR_CLASSPATH=./heron-core/lib/ckptmgr/:./heron-core/lib/statefulstorage/:', '--bind', 'COMPONENT_RAMMAP=split:536870912,count:536870912,spout:536870912', '--bind', 'COMPONENT_JVM_OPTS_IN_BASE64=\"\"', '--bind', 'PYTHON_INSTANCE_BINARY=./heron-core/bin/heron-python-instance', '--bind', 'ROLE=yitian', '--bind', 'ENVIRON=devel', '--bind', 'TOPOLOGY_ID=AuroraMonitorSentenceWordCountTopology85c19772-4cd4-4a5c-8cf3-d6306f792067', '--bind', 'STATEMGR_CONNECTION_STRING=heron01:2181', '--bind', 'STMGR_BINARY=./heron-core/bin/heron-stmgr', '--bind', 'TOPOLOGY_CLASSPATH=heron-java-streamlet-api-example-latest-jar-with-dependencies.jar', '--bind', 'HEALTHMGR_CLASSPATH=./heron-core/lib/healthmgr/', '--bind', 'HEALTHMGR_MODE=disabled', '--bind', 'TIER=preemptible', '--bind', 'SCHEDULER_CLASSPATH=./heron-core/lib/scheduler/:./heron-core/lib/packing/:./heron-core/lib/statemgr/', '--bind', 'DISK_PER_CONTAINER=3221225472', '--bind', 'CLUSTER=aurora', 'aurora/yitian/devel/AuroraMonitorSentenceWordCountTopology', '/home/yitian/.heron/conf/aurora/heron.aurora', '--verbose'])\nDEBUG] Config: ['\"\"\"\\n', 'Launch the topology as a single aurora job with multiple instances.\\n', 'The heron-executor is responsible for starting a tmaster (container 0)\\n', 'and regular stmgr/metricsmgr/instances (container index > 0).\\n', '\"\"\"\\n', '\\n', \"heron_core_release_uri = '{{CORE_PACKAGE_URI}}'\\n\", \"heron_topology_jar_uri = '{{TOPOLOGY_PACKAGE_URI}}'\\n\", 'core_release_file = \"heron-core.tar.gz\"\\n', 'topology_package_file = \"topology.tar.gz\"\\n', '\\n', '# --- processes ---\\n', '#fetch_heron_system = Process(\\n', \"#  name = 'fetch_heron_system',\\n\", \"#  cmdline = 'curl %s -o %s && tar zxf %s' % (heron_core_release_uri, core_release_file, core_release_file)\\n\", '#)\\n', '\\n', '#fetch_user_package = Process(\\n', \"#  name = 'fetch_user_package',\\n\", \"#  cmdline = 'curl %s -o %s && tar zxf %s' % (heron_topology_jar_uri, topology_package_file, topology_package_file)\\n\", '#)\\n', '\\n', 'fetch_heron_system = Process(\\n', \"   name = 'fetch_heron_system',\\n\", \"   cmdline = '/home/yitian/hadoop/hadoop-2.7.4/bin/hdfs dfs -get %s %s && tar zxf %s' % (heron_core_release_uri, core_release_file, core_release_file)\\n\", ')\\n', 'fetch_user_package = Process(\\n', \"   name = 'fetch_user_package',\\n\", \"   cmdline = '/home/yitian/hadoop/hadoop-2.7.4/bin/hdfs dfs -get %s %s && tar zxf %s' % (heron_topology_jar_uri, topology_package_file, topology_package_file)\\n\", ')\\n', '\\n', 'command_to_start_executor = \\\\n', \"  '{{EXECUTOR_BINARY}}' \\\\n\", \"  ' --shard={{mesos.instance}}' \\\\n\", \"  ' --topology-name={{TOPOLOGY_NAME}}' \\\\n\", \"  ' --topology-id={{TOPOLOGY_ID}}' \\\\n\", \"  ' --topology-defn-file={{TOPOLOGY_DEFINITION_FILE}}' \\\\n\", \"  ' --state-manager-connection={{STATEMGR_CONNECTION_STRING}}' \\\\n\", \"  ' --state-manager-root={{STATEMGR_ROOT_PATH}}' \\\\n\", \"  ' --tmaster-binary={{TMASTER_BINARY}}' \\\\n\", \"  ' --stmgr-binary={{STMGR_BINARY}}' \\\\n\", '  \\' --metrics-manager-classpath=\"{{METRICSMGR_CLASSPATH}}\"\\' \\\\n', \"  ' --instance-jvm-opts={{INSTANCE_JVM_OPTS_IN_BASE64}}' \\\\n\", '  \\' --classpath=\"{{TOPOLOGY_CLASSPATH}}\"\\' \\\\n', \"  ' --master-port={{thermos.ports[port1]}}' \\\\n\", \"  ' --tmaster-controller-port={{thermos.ports[port2]}}' \\\\n\", \"  ' --tmaster-stats-port={{thermos.ports[port3]}}' \\\\n\", \"  ' --heron-internals-config-file={{SYSTEM_YAML}}' \\\\n\", \"  ' --override-config-file={{OVERRIDE_YAML}} ' \\\\n\", \"  ' --component-ram-map={{COMPONENT_RAMMAP}}' \\\\n\", \"  ' --component-jvm-opts={{COMPONENT_JVM_OPTS_IN_BASE64}}' \\\\n\", \"  ' --pkg-type={{TOPOLOGY_PACKAGE_TYPE}}' \\\\n\", \"  ' --topology-binary-file={{TOPOLOGY_BINARY_FILE}}' \\\\n\", \"  ' --heron-java-home={{JAVA_HOME}}' \\\\n\", \"  ' --shell-port={{thermos.ports[http]}}' \\\\n\", \"  ' --heron-shell-binary={{SHELL_BINARY}}' \\\\n\", \"  ' --metrics-manager-port={{thermos.ports[port4]}}' \\\\n\", \"  ' --cluster={{CLUSTER}}' \\\\n\", \"  ' --role={{ROLE}}' \\\\n\", \"  ' --environment={{ENVIRON}}' \\\\n\", '  \\' --instance-classpath=\"{{INSTANCE_CLASSPATH}}\"\\' \\\\n', \"  ' --metrics-sinks-config-file={{METRICS_YAML}}' \\\\n\", '  \\' --scheduler-classpath=\"{{SCHEDULER_CLASSPATH}}\"\\' \\\\n', '  \\' --scheduler-port=\"{{thermos.ports[scheduler]}}\"\\' \\\\n', \"  ' --python-instance-binary={{PYTHON_INSTANCE_BINARY}}' \\\\n\", \"  ' --cpp-instance-binary={{CPP_INSTANCE_BINARY}}' \\\\n\", \"  ' --metricscache-manager-classpath={{METRICSCACHEMGR_CLASSPATH}}' \\\\n\", \"  ' --metricscache-manager-master-port={{thermos.ports[metricscachemgr_masterport]}}' \\\\n\", \"  ' --metricscache-manager-stats-port={{thermos.ports[metricscachemgr_statsport]}}' \\\\n\", \"  ' --is-stateful={{IS_STATEFUL_ENABLED}}' \\\\n\", '  \\' --checkpoint-manager-classpath=\"{{CKPTMGR_CLASSPATH}}\"\\' \\\\n', \"  ' --checkpoint-manager-port={{thermos.ports[ckptmgr_port]}}' \\\\n\", \"  ' --stateful-config-file={{STATEFUL_CONFIG_YAML}}' \\\\n\", \"  ' --health-manager-mode={{HEALTHMGR_MODE}}' \\\\n\", \"  ' --health-manager-classpath={{HEALTHMGR_CLASSPATH}}'\\n\", '\\n', 'launch_heron_executor = Process(\\n', \"  name = 'launch_heron_executor',\\n\", '  cmdline = command_to_start_executor,\\n', '  max_failures = 1\\n', ')\\n', '\\n', 'discover_profiler_port = Process(\\n', \"  name = 'discover_profiler_port',\\n\", \"  cmdline = 'echo {{thermos.ports[yourkit]}} > yourkit.port'\\n\", ')\\n', '\\n', '# --- tasks ---\\n', 'heron_task = SequentialTask(\\n', \"  name = 'setup_and_run',\\n\", '  processes = [fetch_heron_system, fetch_user_package, launch_heron_executor, discover_profiler_port],\\n', \"  resources = Resources(cpu = '{{CPUS_PER_CONTAINER}}', ram = '{{RAM_PER_CONTAINER}}', disk = '{{DISK_PER_CONTAINER}}')\\n\", ')\\n', '\\n', '# -- jobs ---\\n', 'jobs = [\\n', '  Job(\\n', \"    name = '{{TOPOLOGY_NAME}}',\\n\", \"    cluster = '{{CLUSTER}}',\\n\", \"    role = '{{ROLE}}',\\n\", \"    environment = '{{ENVIRON}}',\\n\", '    service = True,\\n', '    task = heron_task,\\n', \"    instances = '{{NUM_CONTAINERS}}',\\n\", \"    announce = Announcer(primary_port = 'http')\\n\", '  )\\n', ']\\n']\nDEBUG] Getting tier configurations\nDEBUG] Using auth module: \n INFO] Creating job AuroraMonitorSentenceWordCountTopology\nDEBUG] Full configuration: JobConfiguration(instanceCount=3, cronSchedule=None, cronCollisionPolicy=0, key=JobKey(environment=u'devel', role=u'yitian', name=u'AuroraMonitorSentenceWordCountTopology'), taskConfig=TaskConfig(isService=True, contactEmail=None, taskLinks={}, tier=u'preemptible', mesosFetcherUris=None, executorConfig=ExecutorConfig(data='{\"environment\": \"devel\", \"health_check_config\": {\"health_checker\": {\"http\": {\"expected_response_code\": 0, \"endpoint\": \"/health\", \"expected_response\": \"ok\"}}, \"min_consecutive_successes\": 1, \"initial_interval_secs\": 15.0, \"max_consecutive_failures\": 0, \"timeout_secs\": 1.0, \"interval_secs\": 10.0}, \"name\": \"AuroraMonitorSentenceWordCountTopology\", \"service\": true, \"max_task_failures\": 1, \"cron_collision_policy\": \"KILL_EXISTING\", \"enable_hooks\": false, \"cluster\": \"aurora\", \"task\": {\"processes\": [{\"daemon\": false, \"name\": \"fetch_heron_system\", \"ephemeral\": false, \"max_failures\": 1, \"min_duration\": 5, \"cmdline\": \"/home/yitian/hadoop/hadoop-2.7.4/bin/hdfs dfs -get /heron/dist/heron-core.tar.gz heron-core.tar.gz && tar zxf heron-core.tar.gz\", \"final\": false}, {\"daemon\": false, \"name\": \"fetch_user_package\", \"ephemeral\": false, \"max_failures\": 1, \"min_duration\": 5, \"cmdline\": \"/home/yitian/hadoop/hadoop-2.7.4/bin/hdfs dfs -get /heron/topologies/aurora/AuroraMonitorSentenceWordCountTopology-yitian-tag-0-1395922594708948980.tar.gz topology.tar.gz && tar zxf topology.tar.gz\", \"final\": false}, {\"daemon\": false, \"name\": \"launch_heron_executor\", \"ephemeral\": false, \"max_failures\": 1, \"min_duration\": 5, \"cmdline\": \"./heron-core/bin/heron-executor --shard={{mesos.instance}} --topology-name=AuroraMonitorSentenceWordCountTopology --topology-id=AuroraMonitorSentenceWordCountTopology85c19772-4cd4-4a5c-8cf3-d6306f792067 --topology-defn-file=AuroraMonitorSentenceWordCountTopology.defn --state-manager-connection=heron01:2181 --state-manager-root=/heron --tmaster-binary=./heron-core/bin/heron-tmaster --stmgr-binary=./heron-core/bin/heron-stmgr --metrics-manager-classpath=\\\"./heron-core/lib/metricsmgr/\\\" --instance-jvm-opts=\\\"\\\" --classpath=\\\"heron-java-streamlet-api-example-latest-jar-with-dependencies.jar\\\" --master-port={{thermos.ports[port1]}} --tmaster-controller-port={{thermos.ports[port2]}} --tmaster-stats-port={{thermos.ports[port3]}} --heron-internals-config-file=./heron-conf/heron_internals.yaml --override-config-file=./heron-conf/override.yaml  --component-ram-map=split:536870912,count:536870912,spout:536870912 --component-jvm-opts=\\\"\\\" --pkg-type=jar --topology-binary-file=heron-java-streamlet-api-example-latest-jar-with-dependencies.jar --heron-java-home=/usr/lib/jvm/java-1.8.0-openjdk-amd64 --shell-port={{thermos.ports[http]}} --heron-shell-binary=./heron-core/bin/heron-shell --metrics-manager-port={{thermos.ports[port4]}} --cluster=aurora --role=yitian --environment=devel --instance-classpath=\\\"./heron-core/lib/instance/\\\" --metrics-sinks-config-file=./heron-conf/metrics_sinks.yaml --scheduler-classpath=\\\"./heron-core/lib/scheduler/:./heron-core/lib/packing/:./heron-core/lib/statemgr/\\\" --scheduler-port=\\\"{{thermos.ports[scheduler]}}\\\" --python-instance-binary=./heron-core/bin/heron-python-instance --cpp-instance-binary=./heron-core/bin/heron-cpp-instance --metricscache-manager-classpath=./heron-core/lib/metricscachemgr/ --metricscache-manager-master-port={{thermos.ports[metricscachemgr_masterport]}} --metricscache-manager-stats-port={{thermos.ports[metricscachemgr_statsport]}} --is-stateful=false --checkpoint-manager-classpath=\\\"./heron-core/lib/ckptmgr/:./heron-core/lib/statefulstorage/:\\\" --checkpoint-manager-port={{thermos.ports[ckptmgr_port]}} --stateful-config-file=./heron-conf/stateful.yaml --health-manager-mode=disabled --health-manager-classpath=./heron-core/lib/healthmgr/*\", \"final\": false}, {\"daemon\": false, \"name\": \"discover_profiler_port\", \"ephemeral\": false, \"max_failures\": 1, \"min_duration\": 5, \"cmdline\": \"echo {{thermos.ports[yourkit]}} > yourkit.port\", \"final\": false}], \"name\": \"setup_and_run\", \"finalization_wait\": 30, \"max_failures\": 1, \"max_concurrency\": 0, \"resources\": {\"gpu\": 0, \"disk\": 3221225472, \"ram\": 3758096384, \"cpu\": 2.0}, \"constraints\": [{\"order\": [\"fetch_heron_system\", \"fetch_user_package\", \"launch_heron_executor\", \"discover_profiler_port\"]}]}, \"production\": false, \"role\": \"yitian\", \"tier\": \"preemptible\", \"announce\": {\"primary_port\": \"http\", \"portmap\": {\"aurora\": \"http\"}}, \"lifecycle\": {\"http\": {\"graceful_shutdown_endpoint\": \"/quitquitquit\", \"port\": \"health\", \"shutdown_endpoint\": \"/abortabortabort\"}}, \"priority\": 0}', name='AuroraExecutor'), requestedPorts=set([u'port4', u'http', u'metricscachemgr_masterport', u'yourkit', u'metricscachemgr_statsport', u'scheduler', u'ckptmgr_port', u'port2', u'port3', u'port1']), maxTaskFailures=1, priority=0, ramMb=3584, job=JobKey(environment=u'devel', role=u'yitian', name=u'AuroraMonitorSentenceWordCountTopology'), production=False, diskMb=3072, resources=frozenset([]), owner=Identity(user='yitian'), container=Container(docker=None, mesos=MesosContainer(image=None, volumes=None)), metadata=frozenset([]), numCpus=2.0, constraints=set([])), owner=Identity(user='yitian'))\nDEBUG] Querying instance statuses: None\nDEBUG] Response from scheduler: OK (message: )\nDEBUG] Querying instance statuses: None\nDEBUG] Response from scheduler: OK (message: )\nDEBUG] Querying instance statuses: None\nDEBUG] Response from scheduler: OK (message: )\n INFO] Checking status of aurora/yitian/devel/AuroraMonitorSentenceWordCountTopology\nDEBUG] Command terminated successfully\nJob create succeeded: job url=http://218.195.228.24:8081/scheduler/yitian/devel/AuroraMonitorSentenceWordCountTopology\n[2018-07-06 11:09:09 +0800] [INFO] com.twitter.heron.scheduler.utils.SchedulerUtils: Setting Scheduler locations: topology_name: \"AuroraMonitorSentenceWordCountTopology\"\nhttp_endpoint: \"scheduler_as_lib_no_endpoint\"\n[2018-07-06 11:09:10 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: CreateBuilderImpl-Foreground - 1 ms\n[2018-07-06 11:09:10 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Created node for path: /heron/schedulers/AuroraMonitorSentenceWordCountTopology\n[2018-07-06 11:09:10 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Closing the CuratorClient to: heron01:2181\n[2018-07-06 11:09:10 +0800] [FINE] org.apache.curator.framework.imps.CuratorFrameworkImpl: Closing\n[2018-07-06 11:09:10 +0800] [FINE] org.apache.curator.CuratorZookeeperClient: Closing\n[2018-07-06 11:09:10 +0800] [FINE] org.apache.curator.ConnectionState: Closing\n[2018-07-06 11:09:10 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Closing the tunnel processes\n[2018-07-06 11:09:10 +0800] [FINE] com.twitter.heron.scheduler.SubmitterMain: Topology AuroraMonitorSentenceWordCountTopology submitted successfully\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: ObjectName = JMImplementation:type=MBeanServerDelegate\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: name = JMImplementation:type=MBeanServerDelegate\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: Send create notification of object JMImplementation:type=MBeanServerDelegate\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered JMImplementation:type=MBeanServerDelegate\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=ClassLoading\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.ClassLoadingImpl\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.lang:type=ClassLoading\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: name = java.lang:type=ClassLoading\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.lang:type=ClassLoading\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.lang:type=ClassLoading\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=Compilation\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.CompilationImpl\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.lang:type=Compilation\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: name = java.lang:type=Compilation\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.lang:type=Compilation\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.lang:type=Compilation\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=Memory\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.MemoryImpl\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.lang:type=Memory\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: name = java.lang:type=Memory\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.lang:type=Memory\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.lang:type=Memory\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=GarbageCollector,name=PS Scavenge\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.GarbageCollectorImpl\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.lang:type=GarbageCollector,name=PS Scavenge\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: name = java.lang:type=GarbageCollector,name=PS Scavenge\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.lang:name=PS Scavenge,type=GarbageCollector\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.lang:type=GarbageCollector,name=PS Scavenge\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=GarbageCollector,name=PS MarkSweep\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.GarbageCollectorImpl\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.lang:type=GarbageCollector,name=PS MarkSweep\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: name = java.lang:type=GarbageCollector,name=PS MarkSweep\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.lang:name=PS MarkSweep,type=GarbageCollector\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.lang:type=GarbageCollector,name=PS MarkSweep\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=MemoryManager,name=CodeCacheManager\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.MemoryManagerImpl\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.lang:type=MemoryManager,name=CodeCacheManager\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: name = java.lang:type=MemoryManager,name=CodeCacheManager\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.lang:name=CodeCacheManager,type=MemoryManager\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.lang:type=MemoryManager,name=CodeCacheManager\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=MemoryManager,name=Metaspace Manager\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.MemoryManagerImpl\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.lang:type=MemoryManager,name=Metaspace Manager\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: name = java.lang:type=MemoryManager,name=Metaspace Manager\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.lang:name=Metaspace Manager,type=MemoryManager\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.lang:type=MemoryManager,name=Metaspace Manager\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=GarbageCollector,name=PS Scavenge\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=GarbageCollector,name=PS MarkSweep\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=MemoryPool,name=Code Cache\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.MemoryPoolImpl\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.lang:type=MemoryPool,name=Code Cache\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: name = java.lang:type=MemoryPool,name=Code Cache\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.lang:name=Code Cache,type=MemoryPool\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.lang:type=MemoryPool,name=Code Cache\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=MemoryPool,name=Metaspace\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.MemoryPoolImpl\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.lang:type=MemoryPool,name=Metaspace\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: name = java.lang:type=MemoryPool,name=Metaspace\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.lang:name=Metaspace,type=MemoryPool\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.lang:type=MemoryPool,name=Metaspace\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=MemoryPool,name=Compressed Class Space\n[2018-07-06 11:09:20 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.MemoryPoolImpl\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.lang:type=MemoryPool,name=Compressed Class Space\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: name = java.lang:type=MemoryPool,name=Compressed Class Space\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.lang:name=Compressed Class Space,type=MemoryPool\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.lang:type=MemoryPool,name=Compressed Class Space\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=MemoryPool,name=PS Eden Space\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.MemoryPoolImpl\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.lang:type=MemoryPool,name=PS Eden Space\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: name = java.lang:type=MemoryPool,name=PS Eden Space\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.lang:name=PS Eden Space,type=MemoryPool\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.lang:type=MemoryPool,name=PS Eden Space\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=MemoryPool,name=PS Survivor Space\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.MemoryPoolImpl\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.lang:type=MemoryPool,name=PS Survivor Space\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: name = java.lang:type=MemoryPool,name=PS Survivor Space\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.lang:name=PS Survivor Space,type=MemoryPool\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.lang:type=MemoryPool,name=PS Survivor Space\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=MemoryPool,name=PS Old Gen\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.MemoryPoolImpl\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.lang:type=MemoryPool,name=PS Old Gen\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: name = java.lang:type=MemoryPool,name=PS Old Gen\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.lang:name=PS Old Gen,type=MemoryPool\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.lang:type=MemoryPool,name=PS Old Gen\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=OperatingSystem\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.OperatingSystemImpl\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.lang:type=OperatingSystem\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: name = java.lang:type=OperatingSystem\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.lang:type=OperatingSystem\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.lang:type=OperatingSystem\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=Runtime\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.RuntimeImpl\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.lang:type=Runtime\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: name = java.lang:type=Runtime\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.lang:type=Runtime\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.lang:type=Runtime\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=Threading\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.ThreadImpl\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.lang:type=Threading\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: name = java.lang:type=Threading\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.lang:type=Threading\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.lang:type=Threading\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver:  name = java.util.logging:type=Logging\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.ManagementFactoryHelper$PlatformLoggingImpl\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.util.logging:type=Logging\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: name = java.util.logging:type=Logging\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.util.logging:type=Logging\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.util.logging:type=Logging\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver:  name = java.nio:type=BufferPool,name=direct\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.ManagementFactoryHelper$1\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.nio:type=BufferPool,name=direct\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: name = java.nio:type=BufferPool,name=direct\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.nio:name=direct,type=BufferPool\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.nio:type=BufferPool,name=direct\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver:  name = java.nio:type=BufferPool,name=mapped\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.ManagementFactoryHelper$1\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: ObjectName = java.nio:type=BufferPool,name=mapped\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: name = java.nio:type=BufferPool,name=mapped\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: Send create notification of object java.nio:name=mapped,type=BufferPool\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered java.nio:type=BufferPool,name=mapped\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=GarbageCollector,name=PS Scavenge\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=GarbageCollector,name=PS MarkSweep\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=OperatingSystem\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver:  name = java.lang:type=OperatingSystem\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver:  name = com.sun.management:type=HotSpotDiagnostic\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.misc: Building MBeanInfo for sun.management.HotSpotDiagnostic\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: ObjectName = com.sun.management:type=HotSpotDiagnostic\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: name = com.sun.management:type=HotSpotDiagnostic\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: Send create notification of object com.sun.management:type=HotSpotDiagnostic\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered com.sun.management:type=HotSpotDiagnostic\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: ObjectName = com.sun.management:type=DiagnosticCommand\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: name = com.sun.management:type=DiagnosticCommand\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: Send create notification of object com.sun.management:type=DiagnosticCommand\n[2018-07-06 11:09:21 +0800] [FINER] javax.management.mbeanserver: JMX.mbean.registered com.sun.management:type=DiagnosticCommand\n[2018-07-06 11:10:19 +0800] [FINE] com.twitter.heron.statemgr.FileSystemStateManager: File system state manager root address: /heron\n[2018-07-06 11:10:19 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Starting Curator client connecting to: heron01:2181\n[2018-07-06 11:10:19 +0800] [INFO] org.apache.curator.framework.imps.CuratorFrameworkImpl: Starting\n[2018-07-06 11:10:19 +0800] [FINE] org.apache.curator.CuratorZookeeperClient: Starting\n[2018-07-06 11:10:19 +0800] [FINE] org.apache.curator.ConnectionState: Starting\n[2018-07-06 11:10:19 +0800] [FINE] org.apache.curator.ConnectionState: reset\n[2018-07-06 11:10:19 +0800] [INFO] org.apache.curator.framework.state.ConnectionStateManager: State change: CONNECTED\n[2018-07-06 11:10:19 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: TMaster location directory: /heron/tmasters\n[2018-07-06 11:10:19 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: MetricsCache location directory: /heron/metricscaches\n[2018-07-06 11:10:19 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Topologies directory: /heron/topologies\n[2018-07-06 11:10:19 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Packing plan directory: /heron/packingplans\n[2018-07-06 11:10:19 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Physical plan directory: /heron/pplans\n[2018-07-06 11:10:19 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Execution state directory: /heron/executionstate\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: connection-state-parent-process - 2 ms\n[2018-07-06 11:10:19 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Scheduler location directory: /heron/schedulers\n[2018-07-06 11:10:19 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Stateful checkpoints directory: /heron/statefulcheckpoints\n[2018-07-06 11:10:19 +0800] [FINE] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Distributed locks directory: /heron/locks\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 6 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 1 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 6 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 1 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 2 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 1 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 1 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 2 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 1 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 1 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground-CreateParents - 1 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: ExistsBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:10:19 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Directory tree initialized.\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: GetDataBuilderImpl-Background - 2 ms\n[2018-07-06 11:10:19 +0800] [FINEST] com.twitter.heron.packing.builder.PackingPlanBuilder: Added to container 1 instance {componentName: split, taskId: 1, componentIndex: 0}\n[2018-07-06 11:10:19 +0800] [FINEST] com.twitter.heron.packing.builder.PackingPlanBuilder: Added to container 1 instance {componentName: split, taskId: 2, componentIndex: 1}\n[2018-07-06 11:10:19 +0800] [FINEST] com.twitter.heron.packing.builder.PackingPlanBuilder: Added to container 1 instance {componentName: spout, taskId: 3, componentIndex: 0}\n[2018-07-06 11:10:19 +0800] [FINEST] com.twitter.heron.packing.builder.PackingPlanBuilder: Added to container 1 instance {componentName: count, taskId: 4, componentIndex: 0}\n[2018-07-06 11:10:19 +0800] [FINEST] com.twitter.heron.packing.builder.PackingPlanBuilder: Added to container 2 instance {componentName: count, taskId: 5, componentIndex: 1}\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: GetDataBuilderImpl-Background - 0 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: DeleteBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:10:19 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Deleted node for path: /heron/topologies/AuroraMonitorSentenceWordCountTopology\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: CreateBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:10:19 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Created node for path: /heron/topologies/AuroraMonitorSentenceWordCountTopology\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: GetDataBuilderImpl-Background - 0 ms\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: DeleteBuilderImpl-Foreground - 0 ms\n[2018-07-06 11:10:19 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Deleted node for path: /heron/packingplans/AuroraMonitorSentenceWordCountTopology\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: CreateBuilderImpl-Foreground - 1 ms\n[2018-07-06 11:10:19 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Created node for path: /heron/packingplans/AuroraMonitorSentenceWordCountTopology\n[2018-07-06 11:10:19 +0800] [FINE] com.twitter.heron.scheduler.client.SchedulerClientFactory: Creating scheduler client\n[2018-07-06 11:10:19 +0800] [FINE] com.twitter.heron.scheduler.client.SchedulerClientFactory: Invoke scheduler as a library\n[2018-07-06 11:10:19 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: GetDataBuilderImpl-Background - 1 ms\n[2018-07-06 11:10:19 +0800] [WARNING] com.twitter.heron.spi.statemgr.SchedulerStateManagerAdaptor: Exception processing future: java.lang.RuntimeException: Failed to fetch data from path: /heron/pplans/AuroraMonitorSentenceWordCountTopology\n[2018-07-06 11:10:19 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Closing the CuratorClient to: heron01:2181\n[2018-07-06 11:10:19 +0800] [FINE] org.apache.curator.framework.imps.CuratorFrameworkImpl: Closing\n[2018-07-06 11:10:19 +0800] [FINE] org.apache.curator.CuratorZookeeperClient: Closing\n[2018-07-06 11:10:19 +0800] [FINE] org.apache.curator.ConnectionState: Closing\n[2018-07-06 11:10:19 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Closing the tunnel processes\nException in thread \"Thread-4\" java.lang.NullPointerException\n    at zyt.custom.my.scheduler.aurora.AuroraSchedulerController.triggerSchedule(AuroraSchedulerController.java:189)\n    at zyt.custom.my.scheduler.aurora.AuroraSchedulerThread.run(AuroraSchedulerThread.java:36)\n[2018-07-06 11:10:20 +0000] [INFO]: Starting write file : /home/yitian/logs/aurora-scheduler/aurora-scheduler.txt : Starting [NO. 01 times] WordNode load searching...\n... \n. I have updated the question in detail. Does anyone have any ideas?. @nwangtw Thanks a lot. Your understanding is corrent. I do want to get physical plan after the topology is submitted. I tried to get the physical plan after successfully submitting the topology for 3 minutes. At that time, the instances was running normally and registered in stmgrs(zookeeper). **But running updateTopology function still shows the same error as before.** I extracted the specific error message as shown below.\n[2018-07-10 16:52:00 +0800] [WARNING] com.twitter.heron.spi.statemgr.SchedulerStateManagerAdaptor: Exception processing future: java.lang.RuntimeException: Failed to fetch data from path: /heron/pplans/AuroraMonitorSentenceWordCountTopology\n[2018-07-10 16:52:00 +0800] [FINEST] org.apache.curator.utils.DefaultTracerDriver: Trace: DeleteBuilderImpl-Foreground - 7 ms\n[2018-07-10 16:52:00 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Closing the CuratorClient to: heron01:2181\n[2018-07-10 16:52:00 +0800] [FINE] org.apache.curator.framework.imps.CuratorFrameworkImpl: Closing\n[2018-07-10 16:52:00 +0800] [FINE] org.apache.curator.CuratorZookeeperClient: Closing\n[2018-07-10 16:52:00 +0800] [FINE] org.apache.curator.ConnectionState: Closing\n[2018-07-10 16:52:00 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Closing the tunnel processes\nException in thread \"Thread-4\" java.lang.NullPointerException\n    at com.twitter.heron.scheduler.UpdateTopologyManager.getTopology(UpdateTopologyManager.java:338)\n    at com.twitter.heron.scheduler.UpdateTopologyManager.updateTopology(UpdateTopologyManager.java:147)\n    at com.twitter.heron.scheduler.UpdateTopologyManager.updateTopology(UpdateTopologyManager.java:112)\n    at zyt.custom.my.scheduler.aurora.AuroraCustomScheduler.onUpdate(AuroraCustomScheduler.java:196)\n    at com.twitter.heron.scheduler.client.LibrarySchedulerClient.updateTopology(LibrarySchedulerClient.java:70)\n    at zyt.custom.my.scheduler.aurora.AuroraSchedulerController.triggerSchedule(AuroraSchedulerController.java:154)\n    at zyt.custom.my.scheduler.aurora.AuroraSchedulerThread.run(AuroraSchedulerThread.java:45)\nIn Zookeeper, there will also be an Exception that the client disconnects after the connection is established. Is the failure to get physical plan because of  the disconnection of the zookeeper? What's more, Heron cluster uses zookeeper to run normally, but the client connection of this zookeeper failed. I cannot figure it out what's going on about it.\nThe EndOfStreamException of Zookeeper is as follows. And even if I use the default AuroraScheduler, Zookeeper still has the EndOfStreamException. Does this mean that there is a problem with zookeeper\uff1f For more information on Zookeeper EndOfStreamException, there is the link: #2955\n2018-07-10 16:52:10,133 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x164828618e9002c with negotiated timeout 30000 for client /218.195.228.28:44428\n2018-07-10 16:52:10,137 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473bc zxid:0x1685 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:11,139 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473be zxid:0x1686 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:12,143 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473c0 zxid:0x1687 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:13,209 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473c2 zxid:0x1688 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:14,220 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473c4 zxid:0x1689 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:15,241 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473c6 zxid:0x168a txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:16,248 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473c8 zxid:0x168b txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:17,280 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473ca zxid:0x168c txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:18,304 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473cc zxid:0x168d txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:19,344 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473ce zxid:0x168e txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:20,347 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473d0 zxid:0x168f txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:21,353 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473d2 zxid:0x1690 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:22,361 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473d4 zxid:0x1691 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:23,368 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473d6 zxid:0x1692 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:24,372 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473d8 zxid:0x1693 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:25,001 [myid:] - INFO  [SessionTracker:ZooKeeperServer@358] - Expiring session 0x164828618e90022, timeout of 30000ms exceeded\n2018-07-10 16:52:25,001 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x164828618e90022\n2018-07-10 16:52:25,375 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473da zxid:0x1695 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:26,378 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473dc zxid:0x1696 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:27,379 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473de zxid:0x1697 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:28,386 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473e0 zxid:0x1698 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:29,393 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473e2 zxid:0x1699 txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:30,000 [myid:] - INFO  [SessionTracker:ZooKeeperServer@358] - Expiring session 0x164828618e90027, timeout of 30000ms exceeded\n2018-07-10 16:52:30,001 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x164828618e90027\n2018-07-10 16:52:30,395 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473e4 zxid:0x169b txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:31,401 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473e6 zxid:0x169c txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:32,409 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473e8 zxid:0x169d txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:33,413 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473ea zxid:0x169e txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:34,413 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@648] - Got user-level KeeperException when processing sessionid:0x164828618e9002c type:create cxid:0x5b4473ec zxid:0x169f txntype:-1 reqpath:n/a Error Path:/heron/tmasters/AuroraMonitorSentenceWordCountTopology Error:KeeperErrorCode = NodeExists for /heron/tmasters/AuroraMonitorSentenceWordCountTopology\n2018-07-10 16:52:35,001 [myid:] - INFO  [SessionTracker:ZooKeeperServer@358] - Expiring session 0x164828618e90025, timeout of 30000ms exceeded\n2018-07-10 16:52:35,001 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x164828618e90025\n2018-07-10 16:52:35,007 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@368] - caught end of stream exception\nEndOfStreamException: Unable to read additional data from client sessionid 0x164828618e9002c, likely client has closed socket\n    at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:239)\n    at org.apache.zookeeper.server.NIOServerCnxnFactory.run(NIOServerCnxnFactory.java:203)\n    at java.lang.Thread.run(Thread.java:748)\n2018-07-10 16:52:35,008 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1044] - Closed socket connection for client /218.195.228.28:44428 which had sessionid 0x164828618e9002c\n2018-07-10 16:52:45,072 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@192] - Accepted socket connection from /218.195.228.28:44444\n2018-07-10 16:52:45,074 [myid:] - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@896] - Connection request from old client /218.195.228.28:44444; will be dropped if server is in r-o mode\n2018-07-10 16:52:45,074 [myid:] - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:ZooKeeperServer@942] - Client attempting to establish new session at /218.195.228.28:44444\n2018-07-10 16:52:45,078 [myid:] - INFO  [SyncThread:0:ZooKeeperServer@687] - Established session 0x164828618e9002d with negotiated timeout 30000 for client /218.195.228.28:44444\n2018-07-10 16:53:05,000 [myid:] - INFO  [SessionTracker:ZooKeeperServer@358] - Expiring session 0x164828618e9002c, timeout of 30000ms exceeded\n2018-07-10 16:53:05,000 [myid:] - INFO  [ProcessThread(sid:0 cport:2181)::PrepRequestProcessor@486] - Processed session termination for sessionid: 0x164828618e9002c\n``. @nwangtw Thanks for your reply. In order to verify whether it is caused by the abnormality of zookeeper, I tested the same code in a single-nodelocalenvironment withthe same zookeeper version`. The results show that in the local single-node environment, the physical plan data can be successfully obtained and the topology was updated successfully. But I don't know why it doesn't work in the Aurora+Mesos cluster environment.\nWhat's more, by testing, I found that using the following code, the data can be successfully obtained in the single-node environment with zookeeper, but in the cluster environment, the Could not getNodeData Exception occurs.\nThe code: I can get the SchedulerStateManagerAdaptorinstance to obtain physical plan from zookeeper in single-node. But It does not work in Aurora cluster environment.\nSchedulerStateManagerAdaptor stateManagerAdaptor = Runtime.schedulerStateManagerAdaptor(this.runtime);\nPhysicalPlans.PhysicalPlan physicalPlan = stateManagerAdaptor.getPhysicalPlan(topologyName);\nThe Exception:\n[2018-07-11 02:15:37 +0800] [WARNING] com.twitter.heron.spi.statemgr.SchedulerStateManagerAdaptor: Exception processing future: java.lang.RuntimeException: Could not getNodeData  \nException in thread \"rescheduler\" java.lang.NullPointerException\n    at zyt.custom.my.scheduler.aurora.AuroraHotEdgeSchedulerWithTxtLog.getPhysicalPlanInfo(AuroraHotEdgeSchedulerWithTxtLog.java:384)\n    at zyt.custom.my.scheduler.aurora.AuroraHotEdgeSchedulerWithTxtLog.triggerSchedule(AuroraHotEdgeSchedulerWithTxtLog.java:296)\n    at zyt.custom.my.scheduler.aurora.AuroraHotEdgeSchedulerWithTxtLog.access$400(AuroraHotEdgeSchedulerWithTxtLog.java:65)\n    at zyt.custom.my.scheduler.aurora.AuroraHotEdgeSchedulerWithTxtLog$2.run(AuroraHotEdgeSchedulerWithTxtLog.java:257)\n    at java.lang.Thread.run(Thread.java:748)\nSo I confused that what is the difference between getting data from zookeeper in a single node or in the cluster environment?\n. @nwangtw The Aurora scheduler shows that the job runs without problems, and the heron-ui also shows that the topology can run successfully.\n\n\nIn addition, as for the data in the zookeeper can be successfully obtained in the single-node environment, but failed in the cluster environment.  Is it possible that because the node running tmaster cannot get the data from the node running zookeeper (if they are running on different nodes) in a cluster environment? If it is, do you have any ideas about the cause of this problem? Thanks.\n. Do you have any suggestions on how to solve this problem? And I don't seem to be sure what the real cause of the problem is. @nwangtw Thanks so much.\n. @kramasamy Yes, this exception occurred both in my custom scheduler and  Aurora Scheduler.. This problem is caused by conflicts in some classes. And it has been solved by deleting theheron-api:[HERON_VERSION].jar from the pom.xml. But the heron-spi:[HERON_VERSION].jar is necessary to create a custom scheduler.. ",
    "yunfan123": "Any update?Any examples of usages is very helpful.\n. @mycFelix , Why? Major component is written in java. It should can run in any platform. Which component can't be compiled in other platform. \n. Can I run all of heron components in docker? So I can run the heron in other linux platform.\n. @kramasamy , all of left tasks is checkstyle_cpp. What task is this task do? \nI'm in China, but I'm sure my compile env can access maven.twttr.com and http://central.maven.org/maven2/ normally.\nIs there any jars from other maven central?\n. If checkstyle cpp just to ensures the coding style. \nCan I totally disable this action?\n. Is any release compiled heron docker images? So I can try heron without compile it...\n. @kramasamy   Ubuntu.\n. ",
    "rohanag12": "Any updates on this? Some documentation would be really helpful.. Thanks @kramasamy. Is there any way to provide custom Marathon properties (like job constraints) to the Scheduler?. Any updates on this?\nProtobuf 3 support would be nice to have. We have a few protobufs that are written in the proto 3 syntax which we would like to use with Heron.. Fixed by #3209 . Fixed by #3209 . Fixed by #3209 . Fixed by #3209 . Hi, thanks for this PR, happy to finally see a Kafka Spout implementation in Heron. I am planning on using this once it is merged, but I have a question. One major difference between this implementation and the Storm one is that Storm's spout allows emitting to different streams, using the org.apache.storm.kafka.spout.RecordTranslator interface. This implementation is missing this particular functionality, which is quite useful.\nIs there a reason for not keeping this functionality in this Kafka Spout implementation? Or is there another way to achieve similar functionality (other than creating a map-function like bolt for this purpose)? It's really useful for sending data from different topics to different downstream bolts.. > Interesting. Is topic -> stream 1 to 1? Why not have multiple spout components and each one is responsible for one topic?\nTopic -> Stream is 1 -> 1 in our use case, but in Storm's implementation, it can be configured to be many -> 1 as well.\nRunning multiple spouts adds the overhead of running more Heron instances and Kafka consumers. We operate in a resource constrained environment, so that is not always feasible.\n\nvery good question. I actually started with a \"one-record-to-many-tuple\" implementation, then I gave it a deep thought when I was implementing the ATLEAST_ONCE delivery guarantee. Allowing \"one-record-to-many-tuple\" will significantly complicate the algorithm to track acknowledgement, because then we have to keep tracking the mapping relationship between a single Kafka record offset to multiple message IDs.\nAnd then we also face a design choice whether the KafkaSpout itself should decide the uniqueness of a set of Message IDs coming from the same ConsumerRecord, or we should open the choice up to the developer?\nSo, a neater choice is to use multiple KafkaSpout, each dedicated to an output stream.\nBut, I do agree \"one-record-to-many-tuple\" is pretty useful and cost effective in terms of resource consumption. I have no obligation to put it back in, but then it becomes the developer's responsibility to make sure avoid emitting multiple tuples out of one ConsumerRecord ONLY in ATLEAST_ONCE mode, at least for this version of KafkaSpout before we introduce a more complicated ack/fail tracking mechanism.\n\nAgreed that one record -> many tuples is really useful, but it does make tracking offsets a lot harder. However, I don't think that is possible using the current Kafka spout implementation from Storm - it only allows 1 record -> 1 tuple emits, but to any declared stream.\nHaving this ability to choose the stream given a record would be a good start for the Heron Kafka spout, and it's much easier to implement.. @nwangtw The Bazel upgrade was taken up by @skanjila on #3166, but was missing a few changes. I am collaborating with @skanjila on Heron slack, but we should move the discussion to GitHub.. @nwangtw Reverted whitespace changes.. ",
    "akozich": "I faced the same issue. I'm running:\n- heron 0.17.4\n- mesos 1.4.1\n- CentOS 7.4\nThis exception \"Operation not permitted\" is mapped from underlying EPERM system error code. This code means that current process is session master and by POSIX standard is not permitter to change process group.\nI added some debug logging just before executing 'setpgrp' to print parent processes with corresponding process id, process group id, session id, parent process id and here's the result:\nPID     PGID    SESS    PPID    ARGS\n31845   31845   31845   31835   python2.7 ./heron-core/bin/heron-executor --topology-name=WordCountTopology ...\n31835   31800   31800   31800   mesos-executor --launcher_dir=/usr/libexec/mesos\n31800   31800   31800   6110    /usr/libexec/mesos/mesos-containerizer launch\n6110    6110    6110    1       /usr/sbin/mesos-slave --master=zk://zookeeper:2181/mesos --log_dir=/var/log/mesos --containerizers=mesos --hostname_lookup=false --work_dir=/var/lib/mesos\nAs you can see heron-executor is started by mesos-executor in a new session and is a master of that session.\nBefore executing 'setpgrp' current process should be checked against being session master. If it is a master 'setpgrp' should not be called.. ",
    "zgong": "Signed.\n. ",
    "xcf1992": "Thanks for your reply.\nSince JDK14 is too old to be compatible with logback I use. But do you think it is a good to remove slf4j-jdk14-1.7.7.jar from the /usr/local/heron/lib/third_party folder.\nWould the remove cause any issue in actual running?\n. Ok, I will keep using backtype.storm, but is there performance different between backtype and heron.api.\nBesides, which specific file you want to take a look at?\n. I think this these files files may be the most important:\nheron-executor.stdout.txt\ncontainer_1_g_6.log.0.txt\nheron-stmgr-stmgr-1.log.INFO.20160810-160954.14282.txt\nCause I have to remove any info about the service or company, let me know if any other file may also neededed.\nThanks\n. Thanks a lot. I will try 0.14.2 to see if this get fixed.\n. ```\nINFO: Using config file under ./heron-conf/re\nDEBUG: {'config_property': [], 'topology-file-name': './target/re-jar-with-dependencies.jar', 'override_config_file': '/var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmpbmzEXd/override.yaml', 'config_path': './heron-conf/re', 'subcommand': 'submit', 'deploy_deactivated': 'True', 'cluster': 're', 'topology_main_jvm_property': [], 'role': 'ubuntu', 'environ': 'devel', 'topology-class-name': 'com.h.re.reTopology', 'verbose': 'True'}\nDEBUG: $> /Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/bin/java -client -Xmx1g -Dheron.options=cmdline.topologydefn.tmpdirectory=/var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmprDYeAv,cmdline.topology.initial.state=PAUSED -cp /usr/local/heron/lib/third_party/:./target/re-jar-with-dependencies.jar com.h.re.reTopology reTopology\n13:47:21,958 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback.groovy]\n13:47:21,958 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback-test.xml]\n13:47:21,958 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback.xml]\n13:47:21,959 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Setting up default configuration.\n13:47:22,076 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - About to instantiate appender of type [com.h.do.logback.Logbackdo2Appender]\n13:47:22,203 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - Naming appender as [do]\n13:47:22,227 |-ERROR in ch.qos.logback.core.joran.spi.Interpreter@14:18 - no applicable action for [encoder], current ElementPath  is [[configuration][appender][encoder]]\n13:47:22,227 |-ERROR in ch.qos.logback.core.joran.spi.Interpreter@15:22 - no applicable action for [pattern], current ElementPath  is [[configuration][appender][encoder][pattern]]\n13:47:22,272 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - About to instantiate appender of type [ch.qos.logback.core.ConsoleAppender]\n13:47:22,272 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - Naming appender as [CONSOLE]\n13:47:22,276 |-INFO in ch.qos.logback.core.joran.action.NestedComplexPropertyIA - Assuming default type [ch.qos.logback.classic.encoder.PatternLayoutEncoder] for [encoder] property\n13:47:22,283 |-INFO in ch.qos.logback.classic.joran.action.RootLoggerAction - Setting level of ROOT logger to INFO\n13:47:22,283 |-INFO in ch.qos.logback.core.joran.action.AppenderRefAction - Attaching appender named [do] to Logger[ROOT]\n13:47:22,283 |-INFO in ch.qos.logback.core.joran.action.AppenderRefAction - Attaching appender named [CONSOLE] to Logger[ROOT]\n13:47:22,283 |-INFO in ch.qos.logback.classic.joran.action.ConfigurationAction - End of configuration.\n13:47:22,284 |-INFO in ch.qos.logback.classic.joran.JoranConfigurator@51cdd8a - Registering current configuration as safe fallback point\n[INFO] 2016-08-15 13:47:22.458 |- in com.h.re.reTopology => Topology submitted\nINFO: Launching topology 'reTopology'\nDEBUG: $> /Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/bin/java -client -Xmx1g -Dheron.options=cmdline.topologydefn.tmpdirectory=/var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmprDYeAv,cmdline.topology.initial.state=PAUSED -cp /usr/local/heron/lib/scheduler/:/usr/local/heron/lib/uploader/:/usr/local/heron/lib/statemgr/:/usr/local/heron/lib/packing/ com.twitter.heron.scheduler.SubmitterMain --cluster re --role ubuntu --environment devel --heron_home /usr/local/heron --config_path ./heron-conf/re --override_config_file /var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmpbmzEXd/override.yaml --release_file /usr/local/heron/release.yaml --topology_package /var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmprDYeAv/topology.tar.gz --topology_defn /var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmprDYeAv/reTopology.defn --topology_jar ./target/re-jar-with-dependencies.jar --verbose\n[2016-08-15 13:47:24 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config stream\n[2016-08-15 13:47:24 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config\n[2016-08-15 13:47:24 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config stream\n[2016-08-15 13:47:24 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config\n[2016-08-15 13:47:24 -0700] sun.awt.multiscreen.SunDisplayChanger FINEST:  notifyListeners\n[2016-08-15 13:47:24 -0700] sun.awt.multiscreen.SunDisplayChanger FINER:  Adding listener: javax.swing.RepaintManager$DisplayChangedHandler@47c62251\n[2016-08-15 13:47:25 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Config file ./heron-conf/re/cluster.yaml does not exist\n[2016-08-15 13:47:25 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file ./heron-conf/re/client.yaml\n[2016-08-15 13:47:25 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file ./heron-conf/re/client.yaml\n[2016-08-15 13:47:25 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file ./heron-conf/re/packing.yaml\n[2016-08-15 13:47:25 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file ./heron-conf/re/packing.yaml\n[2016-08-15 13:47:25 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file ./heron-conf/re/scheduler.yaml\n[2016-08-15 13:47:25 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file ./heron-conf/re/scheduler.yaml\n[2016-08-15 13:47:25 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file ./heron-conf/re/statemgr.yaml\n[2016-08-15 13:47:25 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file ./heron-conf/re/statemgr.yaml\n[2016-08-15 13:47:25 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file ./heron-conf/re/uploader.yaml\n[2016-08-15 13:47:25 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file ./heron-conf/re/uploader.yaml\n[2016-08-15 13:47:25 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /usr/local/heron/release.yaml\n[2016-08-15 13:47:25 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /usr/local/heron/release.yaml\n[2016-08-15 13:47:25 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmpbmzEXd/override.yaml\n[2016-08-15 13:47:25 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmpbmzEXd/override.yaml\n[2016-08-15 13:47:25 -0700] com.twitter.heron.scheduler.SubmitterMain FINE:  Static config loaded successfully\n[2016-08-15 13:47:25 -0700] com.twitter.heron.scheduler.SubmitterMain FINE:  (\"heron.binaries.sandbox.executor\", ./heron-core/bin/heron-executor)\n(\"heron.binaries.sandbox.shell\", ./heron-core/bin/heron-shell)\n(\"heron.binaries.sandbox.stmgr\", ./heron-core/bin/heron-stmgr)\n(\"heron.binaries.sandbox.tmaster\", ./heron-core/bin/heron-tmaster)\n(\"heron.build.git.revision\", d26a5d932d9562773291863469e586c4f6defa87)\n(\"heron.build.git.status\", Clean)\n(\"heron.build.host\", tw-mbp-kramasamy)\n(\"heron.build.time\", Wed Aug 3 18:16:25 PDT 2016)\n(\"heron.build.timestamp\", 1470273420000)\n(\"heron.build.user\", kramasamy)\n(\"heron.build.version\", 0.14.2)\n(\"heron.class.launcher\", com.twitter.heron.scheduler.aurora.AuroraLauncher)\n(\"heron.class.packing.algorithm\", com.twitter.heron.packing.roundrobin.RoundRobinPacking)\n(\"heron.class.scheduler\", com.twitter.heron.scheduler.aurora.AuroraScheduler)\n(\"heron.class.state.manager\", com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager)\n(\"heron.class.uploader\", com.twitter.heron.uploader.hdfs.HdfsUploader)\n(\"heron.classpath.instance\", /usr/local/heron/lib/instance/)\n(\"heron.classpath.metrics.manager\", /usr/local/heron/lib/metricsmgr/)\n(\"heron.classpath.packing\", /usr/local/heron/lib/packing/)\n(\"heron.classpath.sandbox.instance\", ./heron-core/lib/instance/)\n(\"heron.classpath.sandbox.metrics.manager\", ./heron-core/lib/metricsmgr/)\n(\"heron.classpath.sandbox.packing\", ./heron-core/lib/packing/)\n(\"heron.classpath.sandbox.scheduler\", ./heron-core/lib/scheduler/)\n(\"heron.classpath.sandbox.statemgr\", ./heron-core/lib/statemgr/)\n(\"heron.classpath.sandbox.uploader\", ./heron-core/lib/uploader/)\n(\"heron.classpath.scheduler\", /usr/local/heron/lib/scheduler/)\n(\"heron.classpath.statemgr\", /usr/local/heron/lib/statemgr/)\n(\"heron.classpath.uploader\", /usr/local/heron/lib/uploader/*)\n(\"heron.config.cluster\", re)\n(\"heron.config.environ\", devel)\n(\"heron.config.file.client.yaml\", ./heron-conf/re/client.yaml)\n(\"heron.config.file.cluster.yaml\", ./heron-conf/re/cluster.yaml)\n(\"heron.config.file.defaults.yaml\", ./heron-conf/re/defaults.yaml)\n(\"heron.config.file.metrics.yaml\", ./heron-conf/re/metrics_sinks.yaml)\n(\"heron.config.file.packing.yaml\", ./heron-conf/re/packing.yaml)\n(\"heron.config.file.scheduler.yaml\", ./heron-conf/re/scheduler.yaml)\n(\"heron.config.file.statemgr.yaml\", ./heron-conf/re/statemgr.yaml)\n(\"heron.config.file.system.yaml\", ./heron-conf/re/heron_internals.yaml)\n(\"heron.config.file.uploader.yaml\", ./heron-conf/re/uploader.yaml)\n(\"heron.config.role\", ubuntu)\n(\"heron.config.sandbox.file.cluster.yaml\", ./heron-conf/cluster.yaml)\n(\"heron.config.sandbox.file.defaults.yaml\", ./heron-conf/defaults.yaml)\n(\"heron.config.sandbox.file.metrics.yaml\", ./heron-conf/metrics_sinks.yaml)\n(\"heron.config.sandbox.file.override.yaml\", ./heron-conf/override.yaml)\n(\"heron.config.sandbox.file.packing.yaml\", ./heron-conf/packing.yaml)\n(\"heron.config.sandbox.file.scheduler.yaml\", ./heron-conf/scheduler.yaml)\n(\"heron.config.sandbox.file.statemgr.yaml\", ./heron-conf/statemgr.yaml)\n(\"heron.config.sandbox.file.system.yaml\", ./heron-conf/heron_internals.yaml)\n(\"heron.config.sandbox.file.uploader.yaml\", ./heron-conf/uploader.yaml)\n(\"heron.config.verbose\", true)\n(\"heron.directory.bin\", /usr/local/heron/bin)\n(\"heron.directory.conf\", ./heron-conf/re)\n(\"heron.directory.dist\", /usr/local/heron/dist)\n(\"heron.directory.etc\", /usr/local/heron/etc)\n(\"heron.directory.home\", /usr/local/heron)\n(\"heron.directory.java.home\", /Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home)\n(\"heron.directory.lib\", /usr/local/heron/lib)\n(\"heron.directory.sandbox.bin\", ./heron-core/bin)\n(\"heron.directory.sandbox.conf\", ./heron-conf)\n(\"heron.directory.sandbox.home\", ./heron-core)\n(\"heron.directory.sandbox.java.home\", /usr/lib/jvm/java-8-openjdk-amd64)\n(\"heron.directory.sandbox.lib\", ./heron-core/lib)\n(\"heron.jars.sandbox.scheduler\", ./heron-core/lib/scheduler/heron-scheduler.jar)\n(\"heron.jars.scheduler\", /usr/local/heron/lib/scheduler/heron-scheduler.jar)\n(\"heron.package.core.uri\", hdfs://corehadoop/heron/dist/heron-core-0.14.2-ubuntu.tar.gz)\n(\"heron.package.topology.uri\", /heron/topologies/aurora)\n(\"heron.resources.instance.cpu\", 1.0)\n(\"heron.resources.instance.disk\", 1073741824)\n(\"heron.resources.instance.ram\", 1073741824)\n(\"heron.resources.stmgr.ram\", 1073741824)\n(\"heron.scheduler.is.service\", false)\n(\"heron.statemgr.connection.string\", ec2:2181)\n(\"heron.statemgr.root.path\", /heron)\n(\"heron.statemgr.zookeeper.connection.timeout.ms\", 30000)\n(\"heron.statemgr.zookeeper.is.initialize.tree\", true)\n(\"heron.statemgr.zookeeper.retry.count\", 10)\n(\"heron.statemgr.zookeeper.retry.interval.ms\", 10000)\n(\"heron.statemgr.zookeeper.session.timeout.ms\", 30000)\n(\"heron.topology.definition.file\", /var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmprDYeAv/reTopology.defn)\n(\"heron.topology.id\", reTopology1b1258fb-2ae5-4b4a-a637-d4cc6df83a46)\n(\"heron.topology.jar.file\", ./target/re-jar-with-dependencies.jar)\n(\"heron.topology.name\", reTopology)\n(\"heron.topology.package.file\", /var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmprDYeAv/topology.tar.gz)\n(\"heron.topology.package.type\", jar)\n(\"heron.uploader.hdfs.config.directory\", /Users/cx/conf)\n(\"heron.uploader.hdfs.topologies.directory.uri\", /heron/topologies/aurora)\n[2016-08-15 13:47:25 -0700] com.twitter.heron.statemgr.FileSystemStateManager FINE:  File system state manager root address: /heron\n[2016-08-15 13:47:25 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Starting client to: ec2:2181\n[2016-08-15 13:47:25 -0700] org.apache.curator.framework.imps.CuratorFrameworkImpl INFO:  Starting\n[2016-08-15 13:47:25 -0700] org.apache.curator.CuratorZookeeperClient FINE:  Starting\n[2016-08-15 13:47:25 -0700] org.apache.curator.ConnectionState FINE:  Starting\n[2016-08-15 13:47:25 -0700] org.apache.curator.ConnectionState FINE:  reset\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:host.name=localhost\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.version=1.8.0_60\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.vendor=Oracle Corporation\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.class.path=/usr/local/heron/lib/scheduler/heron-aurora-scheduler.jar:/usr/local/heron/lib/scheduler/heron-binpacking-packing.jar:/usr/local/heron/lib/scheduler/heron-local-scheduler.jar:/usr/local/heron/lib/scheduler/heron-mesos-scheduler.jar:/usr/local/heron/lib/scheduler/heron-roundrobin-packing.jar:/usr/local/heron/lib/scheduler/heron-scheduler.jar:/usr/local/heron/lib/scheduler/heron-slurm-scheduler.jar:/usr/local/heron/lib/scheduler/heron-yarn-scheduler.jar:/usr/local/heron/lib/uploader/heron-hdfs-uploader.jar:/usr/local/heron/lib/uploader/heron-localfs-uploader.jar:/usr/local/heron/lib/uploader/heron-null-uploader.jar:/usr/local/heron/lib/uploader/heron-s3-uploader.jar:/usr/local/heron/lib/uploader/heron-scp-uploader.jar:/usr/local/heron/lib/statemgr/heron-localfs-statemgr.jar:/usr/local/heron/lib/statemgr/heron-zookeeper-statemgr.jar:/usr/local/heron/lib/packing/heron-binpacking-packing.jar:/usr/local/heron/lib/packing/heron-roundrobin-packing.jar\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.library.path=/Users/cx/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.io.tmpdir=/var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.compiler=\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:os.name=Mac OS X\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:os.arch=x86_64\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:os.version=10.11.6\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:user.name=cx\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:user.home=/Users/cx\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:user.dir=/Users/cx/Work/re\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ZooKeeper INFO:  Initiating client connection, connectString=ec2:2181 sessionTimeout=30000 watcher=org.apache.curator.ConnectionState@7ba18f1b\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  zookeeper.disableAutoWatchReset is false\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn INFO:  Opening socket connection to server ec2.server.h.com/10.80.1.136:2181. Will not attempt to authenticate using SASL (unknown error)\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn INFO:  Socket connection established to ec2.server.h.com/10.80.1.136:2181, initiating session\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Session establishment request sent on ec2.server.h.com/10.80.1.136:2181\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxnSocket FINEST:  readConnectResult 37 0x[0,0,0,0,0,0,75,30,1,56,6c,56,5f,7a,0,63,0,0,0,10,ffffffea,1f,ffffff98,73,2d,79,ffffffd4,ffffffe9,ffffffd3,b,ffffff82,ffffffa5,ffffff8b,5e,7,ffffffca,0,]\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn INFO:  Session establishment complete on server ec2.server.h.com/10.80.1.136:2181, sessionid = 0x1566c565f7a0063, negotiated timeout = 30000\n[2016-08-15 13:47:25 -0700] org.apache.curator.framework.state.ConnectionStateManager INFO:  State change: CONNECTED\n[2016-08-15 13:47:25 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Topologies directory: /heron/topologies\n[2016-08-15 13:47:25 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Tmaster location directory: /heron/tmasters\n[2016-08-15 13:47:25 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: connection-state-parent-process - 2 ms\n[2016-08-15 13:47:25 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Physical plan directory: /heron/pplans\n[2016-08-15 13:47:25 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Execution state directory: /heron/executionstate\n[2016-08-15 13:47:25 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Scheduler location directory: /heron/schedulers\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 1,3  replyHeader:: 1,455,0  request:: '/heron,F  response:: s{268,268,1470958614584,1470958614584,0,5,0,0,0,5,273}\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 2,3  replyHeader:: 2,455,0  request:: '/heron/topologies,F  response:: s{269,269,1470958614667,1470958614667,0,14,0,0,0,0,408}\n[2016-08-15 13:47:25 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 40 ms\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 3,3  replyHeader:: 3,455,-101  request:: '/heron/topologies/foo,F  response::\n[2016-08-15 13:47:25 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 16 ms\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 4,3  replyHeader:: 4,455,0  request:: '/heron,F  response:: s{268,268,1470958614584,1470958614584,0,5,0,0,0,5,273}\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 5,3  replyHeader:: 5,455,0  request:: '/heron/tmasters,F  response:: s{270,270,1470958614754,1470958614754,0,0,0,0,0,0,270}\n[2016-08-15 13:47:25 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 33 ms\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 6,3  replyHeader:: 6,455,-101  request:: '/heron/tmasters/foo,F  response::\n[2016-08-15 13:47:25 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 14 ms\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 7,3  replyHeader:: 7,455,0  request:: '/heron,F  response:: s{268,268,1470958614584,1470958614584,0,5,0,0,0,5,273}\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 8,3  replyHeader:: 8,455,0  request:: '/heron/pplans,F  response:: s{271,271,1470958614825,1470958614825,0,0,0,0,0,0,271}\n[2016-08-15 13:47:25 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 28 ms\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 9,3  replyHeader:: 9,455,-101  request:: '/heron/pplans/foo,F  response::\n[2016-08-15 13:47:25 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 16 ms\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 10,3  replyHeader:: 10,455,0  request:: '/heron,F  response:: s{268,268,1470958614584,1470958614584,0,5,0,0,0,5,273}\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 11,3  replyHeader:: 11,455,0  request:: '/heron/executionstate,F  response:: s{272,272,1470958614893,1470958614893,0,14,0,0,0,0,407}\n[2016-08-15 13:47:25 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 29 ms\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 12,3  replyHeader:: 12,455,-101  request:: '/heron/executionstate/foo,F  response::\n[2016-08-15 13:47:25 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 14 ms\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 13,3  replyHeader:: 13,455,0  request:: '/heron,F  response:: s{268,268,1470958614584,1470958614584,0,5,0,0,0,5,273}\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 14,3  replyHeader:: 14,455,0  request:: '/heron/schedulers,F  response:: s{273,273,1470958614980,1470958614980,0,0,0,0,0,0,273}\n[2016-08-15 13:47:25 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 28 ms\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 15,3  replyHeader:: 15,455,-101  request:: '/heron/schedulers/foo,F  response::\n[2016-08-15 13:47:25 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 13 ms\n[2016-08-15 13:47:25 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Directory tree initialized.\n[2016-08-15 13:47:25 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Checking existence of path: /heron/topologies/reTopology\n[2016-08-15 13:47:25 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 16,3  replyHeader:: 16,455,-101  request:: '/heron/topologies/reTopology,F  response::\n[2016-08-15 13:47:25 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 14 ms\n[2016-08-15 13:47:25 -0700] com.twitter.heron.scheduler.SubmitterMain FINE:  Topology reTopology to be submitted\n[2016-08-15 13:47:25 -0700] com.twitter.heron.spi.utils.ShellUtils FINE:  Process command: $ [hadoop, --config, /Users/cx/conf, fs, -test, -e, /heron/topologies/aurora]\n[2016-08-15 13:47:27 -0700] com.twitter.heron.spi.utils.ShellUtils FINE:\nSTDOUT:\n 2016-08-15 13:47:26,409 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[2016-08-15 13:47:27 -0700] com.twitter.heron.uploader.hdfs.HdfsUploader INFO:  Target topology file /heron/topologies/aurora/reTopology-ubuntu-tag-0-7513928017329669682.tar.gz exists, overwriting...\n[2016-08-15 13:47:27 -0700] com.twitter.heron.uploader.hdfs.HdfsUploader INFO:  Uploading topology /var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmprDYeAv/topology.tar.gz package to target hdfs /heron/topologies/aurora/reTopology-ubuntu-tag-0-7513928017329669682.tar.gz\n[2016-08-15 13:47:27 -0700] com.twitter.heron.spi.utils.ShellUtils FINE:  Process command: $ [hadoop, --config, /Users/cx/conf, fs, -copyFromLocal, -f, /var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmprDYeAv/topology.tar.gz, /heron/topologies/aurora/reTopology-ubuntu-tag-0-7513928017329669682.tar.gz]\n[2016-08-15 13:47:30 -0700] com.twitter.heron.spi.utils.ShellUtils FINE:\nSTDOUT:\n 2016-08-15 13:47:28,509 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n[2016-08-15 13:47:30 -0700] com.twitter.heron.packing.roundrobin.RoundRobinPacking SEVERE:  Require at least 192MB ram. Given on -219 MB\n[2016-08-15 13:47:30 -0700] com.twitter.heron.scheduler.LaunchRunner SEVERE:  Failed to pack a valid PackingPlan. Check the config.\n[2016-08-15 13:47:30 -0700] com.twitter.heron.scheduler.SubmitterMain SEVERE:  Failed to launch topology. Attempting to roll back upload.\n[2016-08-15 13:47:30 -0700] com.twitter.heron.spi.utils.ShellUtils FINE:  Process command: $ [hadoop, --config, /Users/cx/conf, fs, -rm, /heron/topologies/aurora/reTopology-ubuntu-tag-0-7513928017329669682.tar.gz]\n[2016-08-15 13:47:31 -0700] com.twitter.heron.spi.utils.ShellUtils FINE:\nSTDOUT:\n 2016-08-15 13:47:30,994 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2016-08-15 13:47:31,453 INFO  [main] fs.TrashPolicyDefault (TrashPolicyDefault.java:initialize(92)) - Namenode trash configuration: Deletion interval = 360 minutes, Emptier interval = 0 minutes.\nMoved: 'hdfs://corehadoop/heron/topologies/aurora/reTopology-ubuntu-tag-0-7513928017329669682.tar.gz' to trash at: hdfs://corehadoop/user/cx/.Trash/Current\n[2016-08-15 13:47:31 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Closing the CuratorClient to: ec2:2181\n[2016-08-15 13:47:31 -0700] org.apache.curator.framework.imps.CuratorFrameworkImpl FINE:  Closing\n[2016-08-15 13:47:31 -0700] org.apache.curator.CuratorZookeeperClient FINE:  Closing\n[2016-08-15 13:47:31 -0700] org.apache.curator.ConnectionState FINE:  Closing\n[2016-08-15 13:47:31 -0700] org.apache.zookeeper.ZooKeeper FINE:  Closing session: 0x1566c565f7a0063\n[2016-08-15 13:47:31 -0700] org.apache.zookeeper.ClientCnxn FINE:  Closing client for session: 0x1566c565f7a0063\n[2016-08-15 13:47:31 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x1566c565f7a0063, packet:: clientPath:null serverPath:null finished:false header:: 17,-11  replyHeader:: 17,456,0  request:: null response:: null\n[2016-08-15 13:47:31 -0700] org.apache.zookeeper.ClientCnxn FINE:  Disconnecting client for session: 0x1566c565f7a0063\n[2016-08-15 13:47:31 -0700] org.apache.zookeeper.ClientCnxn FINE:  An exception was thrown while closing send thread for session 0x1566c565f7a0063 : Unable to read additional data from server sessionid 0x1566c565f7a0063, likely server has closed socket\n[2016-08-15 13:47:31 -0700] org.apache.zookeeper.ZooKeeper INFO:  Session: 0x1566c565f7a0063 closed\n[2016-08-15 13:47:31 -0700] org.apache.zookeeper.ClientCnxn INFO:  EventThread shut down\n[2016-08-15 13:47:31 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Closing the tunnel processes\nException in thread \"main\" java.lang.RuntimeException: Failed to submit topology reTopology\n    at com.twitter.heron.scheduler.SubmitterMain.main(SubmitterMain.java:319)\nERROR: Failed to launch topology 'reTopology' because User main failed with status 1. Bailing out...\nTraceback (most recent call last):\n  File \"heron/cli/src/python/submit.py\", line 145, in launch_topologies\n    launch_a_topology(cl_args, tmp_dir, topology_file, defn_file)\n  File \"heron/cli/src/python/submit.py\", line 110, in launch_a_topology\n    java_defines=[]\n  File \"heron/cli/src/python/execute.py\", line 68, in heron_class\n    raise RuntimeError(err_str)\nRuntimeError: User main failed with status 1. Bailing out...\nINFO: Elapsed time: 10.135s.\n```\n. @objmagic \nHeron version is 0.14.2\nthe command I used is:\nheron submit re/ubuntu/devel --config-path ./heron-conf/ ./target/re-jar-with-dependencies.jar com.h.re.ReTopology ReTopology --deploy-deactivated --verbose\n. Thanks @maosongfu,\nbut I am still a little bit confused. Please correct me if I am wrong.\n I think the setContainerRamRequested is setting the RAM that will be assigned to the container that will be shared by all the heron instances running on this container.\nThe RAM available on the machine is 2GB. So I changed to \nconfig.setContainerRamRequested(2048 * MEGABYTE);\nHowever, I still get message below, which is similar but with different numbers.\ncom.twitter.heron.packing.roundrobin.RoundRobinPacking SEVERE:  Require at least 192MB ram. Given on 0 MB\nCould you tell me a little more about the meaning of \"Require at least 192MB ram. Given on 0 MB\"\nAnd does this message mean 2GB is not enough for my topology, since I run 1 spout and  6 bolts on one container.\nThanks.\n. \u203aheron submit re/ubuntu/devel --config-path ./heron-conf/ ./target/re-jar-with-dependencies.jar com.h.re.reTopology reTopology --deploy-deactivated --verbose\nINFO: Using config file under ./heron-conf/re\nDEBUG: {'config_property': [], 'topology-file-name': './target/re-jar-with-dependencies.jar', 'override_config_file': '/var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmp4UQozM/override.yaml', 'config_path': './heron-conf/re', 'subcommand': 'submit', 'deploy_deactivated': 'True', 'cluster': 're', 'topology_main_jvm_property': [], 'role': 'ubuntu', 'environ': 'devel', 'topology-class-name': 'com.h.re.reTopology', 'verbose': 'True'}\nDEBUG: $> /Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/bin/java -client -Xmx1g -Dheron.options=cmdline.topologydefn.tmpdirectory=/var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmpMQzjqq,cmdline.topology.initial.state=PAUSED -cp /usr/local/heron/lib/third_party/:./target/re-jar-with-dependencies.jar com.h.re.reTopology reTopology\n11:44:31,899 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback.groovy]\n11:44:31,899 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback-test.xml]\n11:44:31,899 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Could NOT find resource [logback.xml]\n11:44:31,900 |-INFO in ch.qos.logback.classic.LoggerContext[default] - Setting up default configuration.\n11:44:32,018 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - About to instantiate appender of type [com.h.doppler.logback.LogbackDoppler2Appender]\n11:44:32,115 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - Naming appender as [DOPPLER]\n11:44:32,143 |-ERROR in ch.qos.logback.core.joran.spi.Interpreter@14:18 - no applicable action for [encoder], current ElementPath  is [[configuration][appender][encoder]]\n11:44:32,143 |-ERROR in ch.qos.logback.core.joran.spi.Interpreter@15:22 - no applicable action for [pattern], current ElementPath  is [[configuration][appender][encoder][pattern]]\n11:44:32,190 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - About to instantiate appender of type [ch.qos.logback.core.ConsoleAppender]\n11:44:32,190 |-INFO in ch.qos.logback.core.joran.action.AppenderAction - Naming appender as [CONSOLE]\n11:44:32,192 |-INFO in ch.qos.logback.core.joran.action.NestedComplexPropertyIA - Assuming default type [ch.qos.logback.classic.encoder.PatternLayoutEncoder] for [encoder] property\n11:44:32,197 |-INFO in ch.qos.logback.classic.joran.action.RootLoggerAction - Setting level of ROOT logger to INFO\n11:44:32,197 |-INFO in ch.qos.logback.core.joran.action.AppenderRefAction - Attaching appender named [DOPPLER] to Logger[ROOT]\n11:44:32,197 |-INFO in ch.qos.logback.core.joran.action.AppenderRefAction - Attaching appender named [CONSOLE] to Logger[ROOT]\n11:44:32,197 |-INFO in ch.qos.logback.classic.joran.action.ConfigurationAction - End of configuration.\n11:44:32,198 |-INFO in ch.qos.logback.classic.joran.JoranConfigurator@51cdd8a - Registering current configuration as safe fallback point\n[INFO] 2016-08-17 11:44:32.358 |- in com.h.re.reTopology => Topology submitted\nINFO: Launching topology 'reTopology'\nDEBUG: $> /Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/bin/java -client -Xmx1g -Dheron.options=cmdline.topologydefn.tmpdirectory=/var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmpMQzjqq,cmdline.topology.initial.state=PAUSED -cp /usr/local/heron/lib/scheduler/:/usr/local/heron/lib/uploader/:/usr/local/heron/lib/statemgr/:/usr/local/heron/lib/packing/* com.twitter.heron.scheduler.SubmitterMain --cluster re --role ubuntu --environment devel --heron_home /usr/local/heron --config_path ./heron-conf/re --override_config_file /var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmp4UQozM/override.yaml --release_file /usr/local/heron/release.yaml --topology_package /var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmpMQzjqq/topology.tar.gz --topology_defn /var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmpMQzjqq/reTopology.defn --topology_jar ./target/re-jar-with-dependencies.jar --verbose\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config stream\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config stream\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config\n[2016-08-17 11:44:34 -0700] sun.awt.multiscreen.SunDisplayChanger FINEST:  notifyListeners\n[2016-08-17 11:44:34 -0700] sun.awt.multiscreen.SunDisplayChanger FINER:  Adding listener: javax.swing.RepaintManager$DisplayChangedHandler@47c62251\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Config file ./heron-conf/re/cluster.yaml does not exist\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file ./heron-conf/re/client.yaml\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file ./heron-conf/re/client.yaml\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file ./heron-conf/re/packing.yaml\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file ./heron-conf/re/packing.yaml\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file ./heron-conf/re/scheduler.yaml\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file ./heron-conf/re/scheduler.yaml\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file ./heron-conf/re/statemgr.yaml\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file ./heron-conf/re/statemgr.yaml\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file ./heron-conf/re/uploader.yaml\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file ./heron-conf/re/uploader.yaml\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /usr/local/heron/release.yaml\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /usr/local/heron/release.yaml\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Reading config file /var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmp4UQozM/override.yaml\n[2016-08-17 11:44:34 -0700] com.twitter.heron.common.config.ConfigReader FINE:  Successfully read config file /var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmp4UQozM/override.yaml\n[2016-08-17 11:44:34 -0700] com.twitter.heron.scheduler.SubmitterMain FINE:  Static config loaded successfully\n[2016-08-17 11:44:34 -0700] com.twitter.heron.scheduler.SubmitterMain FINE:  (\"heron.binaries.sandbox.executor\", ./heron-core/bin/heron-executor)\n(\"heron.binaries.sandbox.shell\", ./heron-core/bin/heron-shell)\n(\"heron.binaries.sandbox.stmgr\", ./heron-core/bin/heron-stmgr)\n(\"heron.binaries.sandbox.tmaster\", ./heron-core/bin/heron-tmaster)\n(\"heron.build.git.revision\", d26a5d932d9562773291863469e586c4f6defa87)\n(\"heron.build.git.status\", Clean)\n(\"heron.build.host\", tw-mbp-kramasamy)\n(\"heron.build.time\", Wed Aug 3 18:16:25 PDT 2016)\n(\"heron.build.timestamp\", 1470273420000)\n(\"heron.build.user\", kramasamy)\n(\"heron.build.version\", 0.14.2)\n(\"heron.class.launcher\", com.twitter.heron.scheduler.aurora.AuroraLauncher)\n(\"heron.class.packing.algorithm\", com.twitter.heron.packing.roundrobin.RoundRobinPacking)\n(\"heron.class.scheduler\", com.twitter.heron.scheduler.aurora.AuroraScheduler)\n(\"heron.class.state.manager\", com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager)\n(\"heron.class.uploader\", com.twitter.heron.uploader.hdfs.HdfsUploader)\n(\"heron.classpath.instance\", /usr/local/heron/lib/instance/)\n(\"heron.classpath.metrics.manager\", /usr/local/heron/lib/metricsmgr/)\n(\"heron.classpath.packing\", /usr/local/heron/lib/packing/)\n(\"heron.classpath.sandbox.instance\", ./heron-core/lib/instance/)\n(\"heron.classpath.sandbox.metrics.manager\", ./heron-core/lib/metricsmgr/)\n(\"heron.classpath.sandbox.packing\", ./heron-core/lib/packing/)\n(\"heron.classpath.sandbox.scheduler\", ./heron-core/lib/scheduler/)\n(\"heron.classpath.sandbox.statemgr\", ./heron-core/lib/statemgr/)\n(\"heron.classpath.sandbox.uploader\", ./heron-core/lib/uploader/)\n(\"heron.classpath.scheduler\", /usr/local/heron/lib/scheduler/)\n(\"heron.classpath.statemgr\", /usr/local/heron/lib/statemgr/)\n(\"heron.classpath.uploader\", /usr/local/heron/lib/uploader/)\n(\"heron.config.cluster\", re)\n(\"heron.config.environ\", devel)\n(\"heron.config.file.client.yaml\", ./heron-conf/re/client.yaml)\n(\"heron.config.file.cluster.yaml\", ./heron-conf/re/cluster.yaml)\n(\"heron.config.file.defaults.yaml\", ./heron-conf/re/defaults.yaml)\n(\"heron.config.file.metrics.yaml\", ./heron-conf/re/metrics_sinks.yaml)\n(\"heron.config.file.packing.yaml\", ./heron-conf/re/packing.yaml)\n(\"heron.config.file.scheduler.yaml\", ./heron-conf/re/scheduler.yaml)\n(\"heron.config.file.statemgr.yaml\", ./heron-conf/re/statemgr.yaml)\n(\"heron.config.file.system.yaml\", ./heron-conf/re/heron_internals.yaml)\n(\"heron.config.file.uploader.yaml\", ./heron-conf/re/uploader.yaml)\n(\"heron.config.role\", ubuntu)\n(\"heron.config.sandbox.file.cluster.yaml\", ./heron-conf/cluster.yaml)\n(\"heron.config.sandbox.file.defaults.yaml\", ./heron-conf/defaults.yaml)\n(\"heron.config.sandbox.file.metrics.yaml\", ./heron-conf/metrics_sinks.yaml)\n(\"heron.config.sandbox.file.override.yaml\", ./heron-conf/override.yaml)\n(\"heron.config.sandbox.file.packing.yaml\", ./heron-conf/packing.yaml)\n(\"heron.config.sandbox.file.scheduler.yaml\", ./heron-conf/scheduler.yaml)\n(\"heron.config.sandbox.file.statemgr.yaml\", ./heron-conf/statemgr.yaml)\n(\"heron.config.sandbox.file.system.yaml\", ./heron-conf/heron_internals.yaml)\n(\"heron.config.sandbox.file.uploader.yaml\", ./heron-conf/uploader.yaml)\n(\"heron.config.verbose\", true)\n(\"heron.directory.bin\", /usr/local/heron/bin)\n(\"heron.directory.conf\", ./heron-conf/re)\n(\"heron.directory.dist\", /usr/local/heron/dist)\n(\"heron.directory.etc\", /usr/local/heron/etc)\n(\"heron.directory.home\", /usr/local/heron)\n(\"heron.directory.java.home\", /Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home)\n(\"heron.directory.lib\", /usr/local/heron/lib)\n(\"heron.directory.sandbox.bin\", ./heron-core/bin)\n(\"heron.directory.sandbox.conf\", ./heron-conf)\n(\"heron.directory.sandbox.home\", ./heron-core)\n(\"heron.directory.sandbox.java.home\", /usr/lib/jvm/java-8-openjdk-amd64)\n(\"heron.directory.sandbox.lib\", ./heron-core/lib)\n(\"heron.jars.sandbox.scheduler\", ./heron-core/lib/scheduler/heron-scheduler.jar)\n(\"heron.jars.scheduler\", /usr/local/heron/lib/scheduler/heron-scheduler.jar)\n(\"heron.package.core.uri\", hdfs://corehadoop/heron/dist/heron-core-0.14.2-ubuntu.tar.gz)\n(\"heron.package.topology.uri\", /heron/topologies/aurora)\n(\"heron.resources.instance.cpu\", 1.0)\n(\"heron.resources.instance.disk\", 1073741824)\n(\"heron.resources.instance.ram\", 1073741824)\n(\"heron.resources.stmgr.ram\", 1073741824)\n(\"heron.scheduler.is.service\", false)\n(\"heron.statemgr.connection.string\", eremesosmaster001:2181)\n(\"heron.statemgr.root.path\", /heron)\n(\"heron.statemgr.zookeeper.connection.timeout.ms\", 30000)\n(\"heron.statemgr.zookeeper.is.initialize.tree\", true)\n(\"heron.statemgr.zookeeper.retry.count\", 10)\n(\"heron.statemgr.zookeeper.retry.interval.ms\", 10000)\n(\"heron.statemgr.zookeeper.session.timeout.ms\", 30000)\n(\"heron.topology.definition.file\", /var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmpMQzjqq/reTopology.defn)\n(\"heron.topology.id\", reTopology0e4d652c-5a80-46aa-b1e2-dd32ae93033c)\n(\"heron.topology.jar.file\", ./target/re-jar-with-dependencies.jar)\n(\"heron.topology.name\", reTopology)\n(\"heron.topology.package.file\", /var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/tmpMQzjqq/topology.tar.gz)\n(\"heron.topology.package.type\", jar)\n(\"heron.uploader.hdfs.config.directory\", ./hdfs-conf)\n(\"heron.uploader.hdfs.topologies.directory.uri\", /heron/topologies/aurora)\n[2016-08-17 11:44:34 -0700] com.twitter.heron.statemgr.FileSystemStateManager FINE:  File system state manager root address: /heron\n[2016-08-17 11:44:34 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Starting client to: eremesosmaster001:2181\n[2016-08-17 11:44:34 -0700] org.apache.curator.framework.imps.CuratorFrameworkImpl INFO:  Starting\n[2016-08-17 11:44:34 -0700] org.apache.curator.CuratorZookeeperClient FINE:  Starting\n[2016-08-17 11:44:34 -0700] org.apache.curator.ConnectionState FINE:  Starting\n[2016-08-17 11:44:34 -0700] org.apache.curator.ConnectionState FINE:  reset\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:host.name=localhost\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.version=1.8.0_60\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.vendor=Oracle Corporation\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.class.path=/usr/local/heron/lib/scheduler/heron-aurora-scheduler.jar:/usr/local/heron/lib/scheduler/heron-binpacking-packing.jar:/usr/local/heron/lib/scheduler/heron-local-scheduler.jar:/usr/local/heron/lib/scheduler/heron-mesos-scheduler.jar:/usr/local/heron/lib/scheduler/heron-roundrobin-packing.jar:/usr/local/heron/lib/scheduler/heron-scheduler.jar:/usr/local/heron/lib/scheduler/heron-slurm-scheduler.jar:/usr/local/heron/lib/scheduler/heron-yarn-scheduler.jar:/usr/local/heron/lib/uploader/heron-hdfs-uploader.jar:/usr/local/heron/lib/uploader/heron-localfs-uploader.jar:/usr/local/heron/lib/uploader/heron-null-uploader.jar:/usr/local/heron/lib/uploader/heron-s3-uploader.jar:/usr/local/heron/lib/uploader/heron-scp-uploader.jar:/usr/local/heron/lib/statemgr/heron-localfs-statemgr.jar:/usr/local/heron/lib/statemgr/heron-zookeeper-statemgr.jar:/usr/local/heron/lib/packing/heron-binpacking-packing.jar:/usr/local/heron/lib/packing/heron-roundrobin-packing.jar\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.library.path=/Users/cx/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.io.tmpdir=/var/folders/tm/x0mryh7s14xc0qhljg6yk84m1jfz0t/T/\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:java.compiler=\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:os.name=Mac OS X\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:os.arch=x86_64\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:os.version=10.11.6\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:user.name=cx\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:user.home=/Users/cx\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ZooKeeper INFO:  Client environment:user.dir=/Users/cx/Work/re\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ZooKeeper INFO:  Initiating client connection, connectString=eremesosmaster001:2181 sessionTimeout=30000 watcher=org.apache.curator.ConnectionState@7ba18f1b\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ClientCnxn FINE:  zookeeper.disableAutoWatchReset is false\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ClientCnxn INFO:  Opening socket connection to server eremesosmaster001.server.h.com/10.128.35.212:2181. Will not attempt to authenticate using SASL (unknown error)\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ClientCnxn INFO:  Socket connection established to eremesosmaster001.server.h.com/10.128.35.212:2181, initiating session\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ClientCnxn FINE:  Session establishment request sent on eremesosmaster001.server.h.com/10.128.35.212:2181\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ClientCnxnSocket FINEST:  readConnectResult 37 0x[0,0,0,0,0,0,75,30,1,56,ffffff90,ffffffb3,ffffffa7,ffffffd6,0,78,0,0,0,10,39,21,ffffff96,6f,12,2,ffffffff,5,12,fffffffe,ffffff9a,70,ffffffa5,ffffff8e,ffffff8a,ffffffda,0,]\n[2016-08-17 11:44:34 -0700] org.apache.zookeeper.ClientCnxn INFO:  Session establishment complete on server eremesosmaster001.server.h.com/10.128.35.212:2181, sessionid = 0x15690b3a7d60078, negotiated timeout = 30000\n[2016-08-17 11:44:34 -0700] org.apache.curator.framework.state.ConnectionStateManager INFO:  State change: CONNECTED\n[2016-08-17 11:44:34 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Topologies directory: /heron/topologies\n[2016-08-17 11:44:34 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Tmaster location directory: /heron/tmasters\n[2016-08-17 11:44:34 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: connection-state-parent-process - 1 ms\n[2016-08-17 11:44:34 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Physical plan directory: /heron/pplans\n[2016-08-17 11:44:34 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Execution state directory: /heron/executionstate\n[2016-08-17 11:44:34 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Scheduler location directory: /heron/schedulers\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 1,3  replyHeader:: 1,304,0  request:: '/heron,F  response:: s{274,274,1471385095681,1471385095681,0,5,0,0,0,5,279}\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 2,3  replyHeader:: 2,304,0  request:: '/heron/topologies,F  response:: s{275,275,1471385095688,1471385095688,0,1,0,0,0,1,280}\n[2016-08-17 11:44:35 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 15 ms\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 3,3  replyHeader:: 3,304,-101  request:: '/heron/topologies/foo,F  response::\n[2016-08-17 11:44:35 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 3 ms\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 4,3  replyHeader:: 4,304,0  request:: '/heron,F  response:: s{274,274,1471385095681,1471385095681,0,5,0,0,0,5,279}\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 5,3  replyHeader:: 5,304,0  request:: '/heron/tmasters,F  response:: s{276,276,1471385095700,1471385095700,0,0,0,0,0,0,276}\n[2016-08-17 11:44:35 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 6 ms\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 6,3  replyHeader:: 6,304,-101  request:: '/heron/tmasters/foo,F  response::\n[2016-08-17 11:44:35 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 3 ms\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 7,3  replyHeader:: 7,304,0  request:: '/heron,F  response:: s{274,274,1471385095681,1471385095681,0,5,0,0,0,5,279}\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 8,3  replyHeader:: 8,304,0  request:: '/heron/pplans,F  response:: s{277,277,1471385095710,1471385095710,0,0,0,0,0,0,277}\n[2016-08-17 11:44:35 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 5 ms\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 9,3  replyHeader:: 9,304,-101  request:: '/heron/pplans/foo,F  response::\n[2016-08-17 11:44:35 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 2 ms\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 10,3  replyHeader:: 10,304,0  request:: '/heron,F  response:: s{274,274,1471385095681,1471385095681,0,5,0,0,0,5,279}\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 11,3  replyHeader:: 11,304,0  request:: '/heron/executionstate,F  response:: s{278,278,1471385095719,1471385095719,0,1,0,0,0,1,281}\n[2016-08-17 11:44:35 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 5 ms\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 12,3  replyHeader:: 12,304,-101  request:: '/heron/executionstate/foo,F  response::\n[2016-08-17 11:44:35 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 2 ms\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 13,3  replyHeader:: 13,304,0  request:: '/heron,F  response:: s{274,274,1471385095681,1471385095681,0,5,0,0,0,5,279}\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 14,3  replyHeader:: 14,304,0  request:: '/heron/schedulers,F  response:: s{279,279,1471385095728,1471385095728,0,0,0,0,0,0,279}\n[2016-08-17 11:44:35 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground-CreateParents - 5 ms\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 15,3  replyHeader:: 15,304,-101  request:: '/heron/schedulers/foo,F  response::\n[2016-08-17 11:44:35 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 2 ms\n[2016-08-17 11:44:35 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Directory tree initialized.\n[2016-08-17 11:44:35 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Checking existence of path: /heron/topologies/reTopology\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 16,3  replyHeader:: 16,304,0  request:: '/heron/topologies/reTopology,F  response:: s{280,280,1471385099944,1471385099944,0,0,0,0,2187,0,280}\n[2016-08-17 11:44:35 -0700] org.apache.curator.utils.DefaultTracerDriver FINEST:  Trace: ExistsBuilderImpl-Foreground - 2 ms\n[2016-08-17 11:44:35 -0700] com.twitter.heron.scheduler.SubmitterMain SEVERE:  Topology already exists\n[2016-08-17 11:44:35 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Closing the CuratorClient to: eremesosmaster001:2181\n[2016-08-17 11:44:35 -0700] org.apache.curator.framework.imps.CuratorFrameworkImpl FINE:  Closing\n[2016-08-17 11:44:35 -0700] org.apache.curator.CuratorZookeeperClient FINE:  Closing\n[2016-08-17 11:44:35 -0700] org.apache.curator.ConnectionState FINE:  Closing\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ZooKeeper FINE:  Closing session: 0x15690b3a7d60078\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Closing client for session: 0x15690b3a7d60078\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Reading reply sessionid:0x15690b3a7d60078, packet:: clientPath:null serverPath:null finished:false header:: 17,-11  replyHeader:: 17,305,0  request:: null response:: null\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINE:  Disconnecting client for session: 0x15690b3a7d60078\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ZooKeeper INFO:  Session: 0x15690b3a7d60078 closed\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn INFO:  EventThread shut down\n[2016-08-17 11:44:35 -0700] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Closing the tunnel processes\nException in thread \"main\" java.lang.RuntimeException: Failed to submit topology reTopology\n    at com.twitter.heron.scheduler.SubmitterMain.main(SubmitterMain.java:319)\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxnSocketNIO FINEST:  Doing client selector close\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxnSocketNIO FINEST:  Closed client selector\n[2016-08-17 11:44:35 -0700] org.apache.zookeeper.ClientCnxn FINEST:  SendThread exitedloop.\nERROR: Failed to launch topology 'reTopology' because User main failed with status 1. Bailing out...\nTraceback (most recent call last):\n  File \"heron/cli/src/python/submit.py\", line 145, in launch_topologies\n    launch_a_topology(cl_args, tmp_dir, topology_file, defn_file)\n  File \"heron/cli/src/python/submit.py\", line 110, in launch_a_topology\n    java_defines=[]\n  File \"heron/cli/src/python/execute.py\", line 68, in heron_class\n    raise RuntimeError(err_str)\nRuntimeError: User main failed with status 1. Bailing out...\nINFO: Elapsed time: 6.896s.\n. ",
    "kylozw": "When I built packages on ubuntu, the following happened:\nERROR: missing input file '//scripts/compile:env_exec.sh'.\nERROR: /usr/local/heron/heron/schedulers/tests/java/BUILD:132:1: Creating runfiles tree bazel-out/local-fastbuild/bin/heron/schedulers/tests/java/MesosFrameworkTest.runfiles failed: build-runfiles failed: error executing command /root/.cache/bazel/_bazel_root/ea5f5d1d1e31949bd99eb97a82759a2c/execroot/heron/_bin/build-runfiles bazel-out/local-fastbuild/bin/heron/schedulers/tests/java/MesosFrameworkTest.runfiles_manifest ... (remaining 1 argument(s) skipped): com.google.devtools.build.lib.shell.AbnormalTerminationException: Process terminated by signal 15.\nERROR: /usr/local/heron/third_party/yaml-cpp/BUILD:11:1: //third_party/yaml-cpp:yaml-cpp-srcs: missing input file '//scripts/compile:env_exec.sh'.\nERROR: /usr/local/heron/third_party/yaml-cpp/BUILD:11:1 1 input file(s) do not exist.\n. ",
    "yannisxu": "signed cla\n. ",
    "pankajroark": "Command to run the snippet I posted:\nclang++ -std=c++11 -stdlib=libc++ -Weverything main.cpp; ./a.out\n. Is it a possibility to special case integers to hash to themselves?\nOn Wednesday, October 5, 2016, Maosong Fu notifications@github.com wrote:\n\nHere is a benchmark report for popular hash functions:\nhttps://lonewolfer.wordpress.com/2015/01/05/benchmarking-hash-functions/\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/1308#issuecomment-251818476, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AAojhtzehfUWpS3OKckBzOIc9LGZTy-Tks5qxCUcgaJpZM4Ju3kQ\n.\n. @maosongfu  They are guarded by the same lock. If wait is executing then execution cannot reach notify  and vice versa.. @maosongfu It was my rusty understanding of how wait works. Wait actually gives up the monitor and waits for another thread to enter the same monitor and call notify. So you're right and that's why the test is failing. Let me change the implementation.. Proposed a different way of optimizing, changed the title to reflect that.. @maosongfu I changed the description and added more information. Let me know if any more info is required. Thanks. Sounds like there is no limit on how many containers are killed in a period of time, this can result in uncontrolled data loss. Is this a stop-gap implementation? Will this be replaced/enhanced soon after?. \n",
    "hsc": "Oops. Looks like it is related to:\nhttps://github.com/twitter/heron/issues/1292\n. ",
    "HosiYuki": "@billonahill @mycFelix @kramasamy \nThank you for your replies.\nMy heron version was 0.14.0 originally. I have installed 0.14.2 recently, and the error disappeared.\nBut new errors come up:\n\ncheng@node18-10:~$ heron submit yarn .heron/examples/heron-examples.jar com.twitter.heron.examples.AckingTopology AckingTopology\nINFO: Using config file under /home/cheng/.heron/conf/yarn\nINFO: Launching topology 'AckingTopology'\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/home/cheng/.heron/lib/scheduler/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/home/cheng/.heron/lib/statemgr/heron-zookeeper-statemgr.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n[2016-09-14 10:25:17 +0800] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Starting client to: 10.107.18.210:1210\n2016-09-14 10:25:17,194 INFO  [main] imps.CuratorFrameworkImpl (CuratorFrameworkImpl.java:start(224)) - Starting\n2016-09-14 10:25:17,204 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT\n2016-09-14 10:25:17,205 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:host.name=node18-10.pdl.net\n2016-09-14 10:25:17,205 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.version=1.8.0_11\n2016-09-14 10:25:17,206 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.vendor=Oracle Corporation\n2016-09-14 10:25:17,206 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.home=/home/cheng/jdk1.8.0_11/jre\n2016-09-14 10:25:17,206 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.class.path=\n2016-09-14 10:25:17,206 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib\n2016-09-14 10:25:17,206 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.io.tmpdir=/tmp\n2016-09-14 10:25:17,206 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:java.compiler=\n2016-09-14 10:25:17,206 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:os.name=Linux\n2016-09-14 10:25:17,206 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:os.arch=amd64\n2016-09-14 10:25:17,206 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:os.version=3.13.0-24-generic\n2016-09-14 10:25:17,206 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:user.name=cheng\n2016-09-14 10:25:17,207 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:user.home=/home/cheng\n2016-09-14 10:25:17,207 INFO  [main] zookeeper.ZooKeeper (Environment.java:logEnv(100)) - Client environment:user.dir=/home/cheng\n2016-09-14 10:25:17,208 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:(438)) - Initiating client connection, connectString=10.107.18.210:1210 sessionTimeout=30000 watcher=org.apache.curator.ConnectionState@4d826d77\n2016-09-14 10:25:17,224 INFO  [main-SendThread(10.107.18.210:1210)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(975)) - Opening socket connection to server 10.107.18.210/10.107.18.210:1210. Will not attempt to authenticate using SASL (unknown error)\n2016-09-14 10:25:17,232 INFO  [main-SendThread(10.107.18.210:1210)] zookeeper.ClientCnxn (ClientCnxn.java:primeConnection(852)) - Socket connection established to 10.107.18.210/10.107.18.210:1210, initiating session\n2016-09-14 10:25:17,259 INFO  [main-SendThread(10.107.18.210:1210)] zookeeper.ClientCnxn (ClientCnxn.java:onConnected(1235)) - Session establishment complete on server 10.107.18.210/10.107.18.210:1210, sessionid = 0x1572287bedc0016, negotiated timeout = 30000\n2016-09-14 10:25:17,267 INFO  [main-EventThread] state.ConnectionStateManager (ConnectionStateManager.java:postState(228)) - State change: CONNECTED\n[2016-09-14 10:25:17 +0800] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Topologies directory: /heron/topologies\n[2016-09-14 10:25:17 +0800] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Tmaster location directory: /heron/tmasters\n[2016-09-14 10:25:17 +0800] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Physical plan directory: /heron/pplans\n[2016-09-14 10:25:17 +0800] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Execution state directory: /heron/executionstate\n[2016-09-14 10:25:17 +0800] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Scheduler location directory: /heron/schedulers\n[2016-09-14 10:25:17 +0800] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Closing the CuratorClient to: 10.107.18.210:1210\n2016-09-14 10:25:17,291 INFO  [main] zookeeper.ZooKeeper (ZooKeeper.java:close(684)) - Session: 0x1572287bedc0016 closed\n2016-09-14 10:25:17,291 INFO  [main-EventThread] zookeeper.ClientCnxn (ClientCnxn.java:run(512)) - EventThread shut down\n[2016-09-14 10:25:17 +0800] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Closing the tunnel processes\nException in thread \"main\" java.lang.NoSuchMethodError: org.apache.curator.framework.CuratorFramework.createContainers(Ljava/lang/String;)V\n        at com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager.initTree(CuratorStateManager.java:130)\n        at com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager.initialize(CuratorStateManager.java:94)\n        at com.twitter.heron.scheduler.SubmitterMain.submitTopology(SubmitterMain.java:380)\n        at com.twitter.heron.scheduler.SubmitterMain.main(SubmitterMain.java:315)\nERROR: Failed to launch topology 'AckingTopology' because User main failed with status 1. Bailing out...\nTraceback (most recent call last):\n  File \"/home/cheng/bin/heron/heron/cli/src/python/submit.py\", line 145, in launch_topologies\n    launch_a_topology(cl_args, tmp_dir, topology_file, defn_file)\n  File \"/home/cheng/bin/heron/heron/cli/src/python/submit.py\", line 110, in launch_a_topology\n    java_defines=[]\n  File \"/home/cheng/bin/heron/heron/cli/src/python/execute.py\", line 68, in heron_class\n    raise RuntimeError(err_str)\nRuntimeError: User main failed with status 1. Bailing out...\nINFO: Elapsed time: 0.861s.\n\nThe statemgr.yaml is : \n\nlocal state manager class for managing state in a persistent fashion\nheron.class.state.manager: com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager\nlocal state manager connection string\nheron.statemgr.connection.string:  \"10.107.18.210:1210\"\npath of the root address to store the state in a local file system\nheron.statemgr.root.path: \"/heron\"\ncreate the zookeeper nodes, if they do not exist\nheron.statemgr.zookeeper.is.initialize.tree: True\ntimeout in ms to wait before considering zookeeper session is dead\nheron.statemgr.zookeeper.session.timeout.ms: 30000 \ntimeout in ms to wait before considering zookeeper connection is dead\nheron.statemgr.zookeeper.connection.timeout.ms: 30000 \ntimeout in ms to wait before considering zookeeper connection is dead\nheron.statemgr.zookeeper.retry.count: 10 \nduration of time to wait until the next retry\nheron.statemgr.zookeeper.retry.interval.ms: 10000 \n\nAnd the other config files is default. Zookeeper runs in mode standalone.\n. @mycFelix Thank you for your enthusiastic help\uff01And I\u2019ve learned a lot.\nBut I still have some doubts.\n1\u3001What should the extra-launch-classpath be set to? hadoop classpath or something else? Besides, the extra-launch-classpath arg couldn't handle strings with '*', as your another issue said. How can I deal with the situation with the current 0.14.3 version?\n2\u3001How to update the common-cli and curator jars to higher version? Will it be ok that download higher version jars and replace the lower ones? Or should I update them through other ways?\nThanks again for your patient help!\n. @kramasamy \nThank you for your attention!\nHis answer helps lot. But there are still problems to operate normally.\n. @mycFelix Thank you for your detailed guidance\uff01\nNow, I can submit topo to yarn cluster and find out the  topo through hadoop http and heron ui. But the topo doesn't seem to execute correctly. According to heron ui, there is  no container for the topo.\nOn the Heron node, there is processes named SubmitterMain. And REEFLauncher on the AM node. Except these, there seems to be no process related to heron topo.\nPart of the output as follows:\n\ncom.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Created node for path: /heron/executionstate/ExclamationTopology\n[2016-09-22 08:16:56 +0800] org.apache.hadoop.util.NativeCodeLoader WARNING:  Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nPowered by\n     __    _\n    /    / /  / /  / /  _/\n   /     _/ /  /  /  /  /  /\n  /  /\\  \\     /  / /  / /  _/\n /  /  \\  \\   /  /  /  /__  /  /\n/*/    *\\ /_/ // //\n[2016-09-22 08:16:57 +0800] org.apache.reef.util.REEFVersion INFO:  REEF Version: 0.14.0\n[2016-09-22 08:16:57 +0800] com.twitter.heron.scheduler.yarn.ReefClientSideHandlers INFO:  Initializing REEF client handlers for Heron, topology: ExclamationTopology\n[2016-09-22 08:16:57 +0800] org.apache.hadoop.yarn.client.RMProxy INFO:  Connecting to ResourceManager at node18-10.pdl.net/10.107.18.210:8032\n[2016-09-22 08:17:00 +0800] org.apache.reef.runtime.common.files.JobJarMaker WARNING:  Failed to delete [/tmp/reef-job-8823727808098291604]\n[2016-09-22 08:17:02 +0800] org.apache.reef.runtime.yarn.client.YarnSubmissionHelper INFO:  Submitting REEF Application to YARN. ID: application1474441925257_0003\n[2016-09-22 08:17:02 +0800] org.apache.hadoop.yarn.client.api.impl.YarnClientImpl INFO:  Submitted application application_1474441925257_0003\n[2016-09-22 08:17:05 +0800] com.twitter.heron.scheduler.yarn.ReefClientSideHandlers INFO:  Topology ExclamationTopology is running, jobId ExclamationTopology.\n[2016-09-22 08:17:05 +0800] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Closing the CuratorClient to: 10.107.18.210:1210\n16/09/22 08:17:05 INFO imps.CuratorFrameworkImpl: backgroundOperationsLoop exiting\n16/09/22 08:17:05 INFO zookeeper.ZooKeeper: Session: 0x1573c238041001b closed\n16/09/22 08:17:05 INFO zookeeper.ClientCnxn: EventThread shut down\n[2016-09-22 08:17:05 +0800] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager INFO:  Closing the tunnel processes\n. @mycFelix @ashvina \nI find the drive.stderr and evaluator.err, but don't find the log-files.\nPart of drive.stderr is as follows:\nException: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread \"IPC Client (1997548433) connection to node18-10.pdl.net/10.107.18.210:8030 from cheng\"\n16/09/22 09:05:42 WARN nio.NioEventLoop: Unexpected exception in the selector loop.\njava.lang.OutOfMemoryError: Java heap space\n16/09/22 09:05:42 WARN concurrent.SingleThreadEventExecutor: Unexpected exception from an event executor:\njava.lang.OutOfMemoryError: Java heap space\n        at org.apache.log4j.Category.forcedLog(Category.java:391)\n        at org.apache.log4j.Category.log(Category.java:856)\n        at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:478)\n        at io.netty.util.internal.logging.Slf4JLogger.warn(Slf4JLogger.java:151)\n        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:367)\n        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)\n        at java.lang.Thread.run(Thread.java:745)\nException: java.lang.RuntimeException thrown from the UncaughtExceptionHandler in thread \"server-timer\"\n\nAnd part of evaluator.err is as follows:\n\nINFO: Entering REEFLauncher.main().\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/avro/io/DatumReader\n        at java.lang.Class.getDeclaredConstructors0(Native Method)\n        at java.lang.Class.privateGetDeclaredConstructors(Class.java:2658)\n        at java.lang.Class.getDeclaredConstructors(Class.java:2007)\n        at org.apache.reef.tang.util.ReflectionUtilities.getNamedParameterTargetOrNull(ReflectionUtilities.java:311)\n        at org.apache.reef.tang.implementation.java.ClassHierarchyImpl.buildPathToNode(ClassHierarchyImpl.java:207)\n        at org.apache.reef.tang.implementation.java.ClassHierarchyImpl.registerClass(ClassHierarchyImpl.java:387)\n        at org.apache.reef.tang.implementation.java.ClassHierarchyImpl.register(ClassHierarchyImpl.java:331)\n        at org.apache.reef.tang.implementation.java.ClassHierarchyImpl.getNode(ClassHierarchyImpl.java:257)\n        at org.apache.reef.tang.implementation.java.InjectorImpl.parseDefaultImplementation(InjectorImpl.java:375)\n        at org.apache.reef.tang.implementation.java.InjectorImpl.buildInjectionPlan(InjectorImpl.java:449)\n        at org.apache.reef.tang.implementation.java.InjectorImpl.filterCandidateConstructors(InjectorImpl.java:193)\n        at org.apache.reef.tang.implementation.java.InjectorImpl.buildClassNodeInjectionPlan(InjectorImpl.java:277)\n        at org.apache.reef.tang.implementation.java.InjectorImpl.buildInjectionPlan(InjectorImpl.java:452)\n        at org.apache.reef.tang.implementation.java.InjectorImpl.getInjectionPlan(InjectorImpl.java:472)\n        at org.apache.reef.tang.implementation.java.InjectorImpl.getInstance(InjectorImpl.java:514)\n        at org.apache.reef.tang.implementation.java.InjectorImpl.getInstance(InjectorImpl.java:533)\n        at org.apache.reef.runtime.common.REEFLauncher.getREEFLauncher(REEFLauncher.java:106)\n        at org.apache.reef.runtime.common.REEFLauncher.main(REEFLauncher.java:167)\nCaused by: java.lang.ClassNotFoundException: org.apache.avro.io.DatumReader\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:372)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:361)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:360)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n        ... 18 more\n\nTopologies are deployable in local mode. And I use zookeeper as statemgr. The statemgr.yaml is as follows:\n\nlocal state manager class for managing state in a persistent fashion\nheron.class.state.manager: com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager\nlocal state manager connection string\nheron.statemgr.connection.string: \"10.107.18.210:1210\"\npath of the root address to store the state in a local file system\nheron.statemgr.root.path: \"/heron\"\ncreate the zookeeper nodes, if they do not exist\nheron.statemgr.zookeeper.is.initialize.tree: True\ntimeout in ms to wait before considering zookeeper session is dead\nheron.statemgr.zookeeper.session.timeout.ms: 30000 \ntimeout in ms to wait before considering zookeeper connection is dead\nheron.statemgr.zookeeper.connection.timeout.ms: 30000 \ntimeout in ms to wait before considering zookeeper connection is dead\nheron.statemgr.zookeeper.retry.count: 10 \nduration of time to wait until the next retry\nheron.statemgr.zookeeper.retry.interval.ms: 10000\n. @ashvina \nThe avro-1.7.4.jar is included. I don't know why org/apache/avro/io/DatumReadercan;t be found.\nI changed the avro jar to higher version, but didn't work. \n. @ashvina - Thanks a lot. I modified the yarn site config as you said, and the error was corrected.\n\nHowever, I met news problems.\nThe following is info from heron-ExclamationTopology-scheduler.log.0 in log-files :\n```\nSep 26, 2016 2:37:59 PM com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager createNode\nINFO: Created node for path: /heron/schedulers/ExclamationTopology\nSep 26, 2016 2:37:59 PM com.twitter.heron.scheduler.SchedulerMain runScheduler\nINFO: Waiting for termination...\nSep 26, 2016 2:38:00 PM com.twitter.heron.scheduler.yarn.HeronMasterDriver submitHeronExecutorTask\nINFO: Submitting evaluator task for id: 1\nSep 26, 2016 2:38:00 PM com.twitter.heron.scheduler.yarn.HeronMasterDriver$HeronWorkerStartHandler onNext\nINFO: Task, id:1, has started.\nSep 26, 2016 2:38:05 PM org.apache.reef.runtime.common.driver.evaluator.EvaluatorManager onEvaluatorException\nWARNING: Failed evaluator: container_1474871539042_0001_01_000002\norg.apache.reef.exception.EvaluatorException: Evaluator [container_1474871539042_0001_01_000002] is assumed to be in state [RUNNING]. But the resource manager reports it to be in state [FAILED]. This means that the Evaluator failed but wasn't able to send an error message back to the driver. Task [0] was running when the Evaluator crashed.\n        at org.apache.reef.runtime.common.driver.evaluator.EvaluatorManager.onResourceStatusMessage(EvaluatorManager.java:589)\n        at org.apache.reef.runtime.common.driver.resourcemanager.ResourceStatusHandler.onNext(ResourceStatusHandler.java:63)\n        at org.apache.reef.runtime.common.driver.resourcemanager.ResourceStatusHandler.onNext(ResourceStatusHandler.java:36)\n        at org.apache.reef.runtime.yarn.driver.REEFEventHandlers.onResourceStatus(REEFEventHandlers.java:91)\n        at org.apache.reef.runtime.yarn.driver.YarnContainerManager.onContainerStatus(YarnContainerManager.java:391)\n        at org.apache.reef.runtime.yarn.driver.YarnContainerManager.onContainersCompleted(YarnContainerManager.java:128)\n        at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$CallbackHandlerThread.run(AMRMClientAsyncImpl.java:300)\nSep 26, 2016 2:38:05 PM com.twitter.heron.scheduler.yarn.HeronMasterDriver$FailedContainerHandler onNext\nWARNING: Container:container_1474871539042_0001_01_000002 failed\nSep 26, 2016 2:38:05 PM com.twitter.heron.scheduler.yarn.HeronMasterDriver$FailedContainerHandler onNext\nINFO: Trying to relaunch executor 0 running on failed container container_1474871539042_0001_01_000002\nSep 26, 2016 2:38:05 PM com.twitter.heron.scheduler.yarn.HeronMasterDriver allocateContainer\nINFO: Requesting container for executor, id: 0, mem: 1,024, cpu: 1\nSep 26, 2016 2:38:06 PM org.apache.reef.wake.impl.ThreadPoolStage close\nWARNING: Executor did not terminate in 1000ms.\nSep 26, 2016 2:38:06 PM org.apache.reef.wake.impl.ThreadPoolStage close\nWARNING: Executor dropped 0 tasks.\nSep 26, 2016 2:38:06 PM com.twitter.heron.scheduler.yarn.HeronMasterDriver$FailedContainerHandler onNext\nSEVERE: Failed to relaunch failed container: 0\ncom.twitter.heron.scheduler.yarn.HeronMasterDriver$ContainerAllocationException: Interrupted while waiting for container\n        at com.twitter.heron.scheduler.yarn.HeronMasterDriver.launchContainerForExecutor(HeronMasterDriver.java:233)\n        at com.twitter.heron.scheduler.yarn.HeronMasterDriver.access$1900(HeronMasterDriver.java:74)\n        at com.twitter.heron.scheduler.yarn.HeronMasterDriver$FailedContainerHandler.onNext(HeronMasterDriver.java:467)\n        at com.twitter.heron.scheduler.yarn.HeronMasterDriver$FailedContainerHandler.onNext(HeronMasterDriver.java:451)\n        at org.apache.reef.runtime.common.driver.evaluator.IdlenessCallbackEventHandler.onNext(IdlenessCallbackEventHandler.java:46)\n        at org.apache.reef.runtime.common.utils.BroadCastEventHandler.onNext(BroadCastEventHandler.java:40)\n        at org.apache.reef.util.ExceptionHandlingEventHandler.onNext(ExceptionHandlingEventHandler.java:46)\n        at org.apache.reef.runtime.common.utils.DispatchingEStage$1.onNext(DispatchingEStage.java:68)\n        at org.apache.reef.runtime.common.utils.DispatchingEStage$1.onNext(DispatchingEStage.java:65)\n        at org.apache.reef.wake.impl.ThreadPoolStage$1.run(ThreadPoolStage.java:182)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.InterruptedException\n        at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:191)\n        at com.twitter.heron.scheduler.yarn.HeronMasterDriver.launchContainerForExecutor(HeronMasterDriver.java:231)\n        ... 14 more\n```\nBesides, there are too many HeronInstance processes on each node.\n. @ashvina \nI use the command heron kill yarn ExclamationTopology, and the info shows INFO: Successfully kill topology 'ExclamationTopology'.\nBut the following processes still exist on each node.\ncheng@node18-15:~$ jps\n6976 HeronInstance\n8064 HeronInstance\n9091 HeronInstance\n6983 MetricsManager\n9671 HeronInstance\n8583 MetricsManager\n8073 MetricsManager\n11019 HeronInstance\n9104 HeronInstance\n9685 MetricsManager\n11032 HeronInstance\n5336 DataNode\n7321 HeronInstance\n6557 HeronInstance\n9119 MetricsManager\n6180 HeronInstance\n7782 HeronInstance\n11046 MetricsManager\n5482 NodeManager\n6955 HeronInstance\n8557 HeronInstance\n7342 HeronInstance\n6578 HeronInstance\n7795 HeronInstance\n8053 HeronInstance\n7350 MetricsManager\n6200 HeronInstance\n6585 MetricsManager\n9658 HeronInstance\n8570 HeronInstance\n7805 MetricsManager\n12350 Jps\n6207 MetricsManager\ncheng     9076     1  0 10:27 ?        00:00:00 python2.7 ./heron-core/bin/heron-executor 1 ExclamationTopology ExclamationTopology87f09956-f369-42b7-9f26-3b41f7faac72 ExclamationTopology.d\ncheng     9090  9076  0 10:27 ?        00:00:00 python2.7 ./heron-core/bin/heron-shell --port=47231 --log_file_prefix=log-files/heron-shell.log\ncheng     9091  9076  0 10:27 ?        00:00:06 /home/cheng/jdk1.8.0_11/bin/java -Xmx320M -Xms320M -Xmn160M -XX:MaxPermSize=128M -XX:PermSize=128M -XX:ReservedCodeCacheSize=64M -XX:+CMSScav\ncheng     9101  9076  0 10:27 ?        00:00:02 ./heron-core/bin/heron-stmgr ExclamationTopology ExclamationTopology87f09956-f369-42b7-9f26-3b41f7faac72 ExclamationTopology.defn 10.107.18.2\ncheng     9104  9076  0 10:27 ?        00:00:06 /home/cheng/jdk1.8.0_11/bin/java -Xmx320M -Xms320M -Xmn160M -XX:MaxPermSize=128M -XX:PermSize=128M -XX:ReservedCodeCacheSize=64M -XX:+CMSScav\ncheng     9119  9076  0 10:27 ?        00:00:03 /home/cheng/jdk1.8.0_11/bin/java -Xmx1024M -XX:+PrintCommandLineFlags -verbosegc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateS\ncheng     9598     1  0 10:28 ?        00:00:00 python2.7 ./heron-core/bin/heron-executor 1 ExclamationTopology ExclamationTopology87f09956-f369-42b7-9f26-3b41f7faac72 ExclamationTopology.d\ncheng     9639  9598  0 10:28 ?        00:00:00 python2.7 ./heron-core/bin/heron-shell --port=38657 --log_file_prefix=log-files/heron-shell.log\ncheng     9658  9598  0 10:28 ?        00:00:05 /home/cheng/jdk1.8.0_11/bin/java -Xmx320M -Xms320M -Xmn160M -XX:MaxPermSize=128M -XX:PermSize=128M -XX:ReservedCodeCacheSize=64M -XX:+CMSScav\ncheng     9665  9598  0 10:28 ?        00:00:02 ./heron-core/bin/heron-stmgr ExclamationTopology ExclamationTopology87f09956-f369-42b7-9f26-3b41f7faac72 ExclamationTopology.defn 10.107.18.2\ncheng     9671  9598  0 10:28 ?        00:00:05 /home/cheng/jdk1.8.0_11/bin/java -Xmx320M -Xms320M -Xmn160M -XX:MaxPermSize=128M -XX:PermSize=128M -XX:ReservedCodeCacheSize=64M -XX:+CMSScav\ncheng     9685  9598  0 10:28 ?        00:00:03 /home/cheng/jdk1.8.0_11/bin/java -Xmx1024M -XX:+PrintCommandLineFlags -verbosegc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCDateS\nroot     10349     2  0 10:31 ?        00:00:00 [kworker/u16:2]\ncheng    11006     1  0 10:32 ?        00:00:00 python2.7 ./heron-core/bin/heron-executor 1 ExclamationTopology ExclamationTopology87f09956-f369-42b7-9f26-3b41f7faac72 ExclamationTopology.d\ncheng    11018 11006  0 10:32 ?        00:00:00 python2.7 ./heron-core/bin/heron-shell --port=56344 --log_file_prefix=log-files/heron-shell.log\nPart of hadoop namenode logs: \n2016-09-27 10:35:59,297 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/2016_09_27_10_24_13_reef-job-1/reef-evaluator-3160321392980106428.jar is closed by DFSClient_NONMAPREDUCE_-1142593035_1\n2016-09-27 10:36:10,321 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741968_1144{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-69aed421-e042-4aa6-84e3-098ac974d44b:NORMAL:10.107.18.215:50010|RBW], ReplicaUC[[DISK]DS-9cc7cded-f003-44c7-be87-f685cd8ad39d:NORMAL:10.107.18.212:50010|RBW]]} for /tmp/2016_09_27_10_24_13_reef-job-1/reef-evaluator-245277802915514085.jar\n2016-09-27 10:36:10,343 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.107.18.212:50010 is added to blk_1073741968_1144{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-69aed421-e042-4aa6-84e3-098ac974d44b:NORMAL:10.107.18.215:50010|RBW], ReplicaUC[[DISK]DS-9cc7cded-f003-44c7-be87-f685cd8ad39d:NORMAL:10.107.18.212:50010|RBW]]} size 0\n2016-09-27 10:36:10,345 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.107.18.215:50010 is added to blk_1073741968_1144{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-69aed421-e042-4aa6-84e3-098ac974d44b:NORMAL:10.107.18.215:50010|RBW], ReplicaUC[[DISK]DS-9cc7cded-f003-44c7-be87-f685cd8ad39d:NORMAL:10.107.18.212:50010|RBW]]} size 0\nPart of yarn resource manager logs:\n2016-09-27 10:24:51,545 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application attempt appattempt_1474942797178_0001_000001 released container container_1474942797178_0001_01_000010 on node: host: node18-13.pdl.net:38504 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: RELEASED\n2016-09-27 10:24:51,666 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1474942797178_0001_01_000008 Container Transitioned from ACQUIRED to RUNNING\n2016-09-27 10:24:52,174 ERROR org.apache.hadoop.yarn.server.webapp.ContainerBlock: Failed to read the container container_1474942797178_0001_01_000002.\njava.lang.reflect.UndeclaredThrowableException\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1672)\n    at org.apache.hadoop.yarn.server.webapp.ContainerBlock.render(ContainerBlock.java:77)\n    at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)\n    at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)\n    at org.apache.hadoop.yarn.webapp.View.render(View.java:235)\n    at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)\n    at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)\n    at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:845)\n    at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:56)\n    at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)\n    at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)\n    at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.container(RmController.java:62)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:483)\n    at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:153)\n    at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)\n    at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)\n    at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)\n    at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)\n    at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)\n    at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)\n    at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)\n    at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:142)\n    at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)\n    at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)\n    at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)\n    at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)\n    at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n    at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n    at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:595)\n    at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:291)\n    at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:554)\n    at org.apache.hadoop.yarn.server.security.http.RMAuthenticationFilter.doFilter(RMAuthenticationFilter.java:82)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n    at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1243)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n    at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n    at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)\n    at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n    at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n    at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)\n    at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n    at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n    at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)\n    at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)\n    at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n    at org.mortbay.jetty.Server.handle(Server.java:326)\n    at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n    at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n    at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n    at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n    at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n    at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\n    at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\nCaused by: org.apache.hadoop.yarn.exceptions.ContainerNotFoundException: Container with id 'container_1474942797178_0001_01_000002' doesn't exist in RM.\n    at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getContainerReport(ClientRMService.java:464)\n    at org.apache.hadoop.yarn.server.webapp.ContainerBlock$1.run(ContainerBlock.java:81)\n    at org.apache.hadoop.yarn.server.webapp.ContainerBlock$1.run(ContainerBlock.java:78)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:422)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n    ... 58 more\n2016-09-27 10:24:52,667 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1474942797178_0001_01_000011 Container Transitioned from NEW to ALLOCATED\n. ",
    "silence-liu": "\nI submit the topology on heron, the log have no error,but i can't see it on heron-ui and submit command not result success ? what reason?\n. @mycFelix @ashvina \nThank you for your suggestion. this problem is solved! because the core-site.xml not in classpath.\n. @mycFelix @ashvina  \nThanks for help~~ \n. I used curator-client\u3001curator-framework\u3001curator-recipes version 2.10.0 or 2.11.0 and commons-cli version 1.3.1 .but it not sunmit successed, they have same error message. the heron version is 0.14.3\n\nSystem version \uff1a ubuntu 14.04\n. ",
    "yesimsure": "@amirfirouzi Since it has been a long time, you may work it out. However, hope it can helps others. To find  in the second and forth term of Log File Location here, check the value of ${yarn.nodemanager.local-dirs} in yarn-site.xml. The default value is ${hadoop.tmp.dir}/nm-local-dir. And the default value of ${hadoop.tmp.dir} is /tmp/hadoop-${user.name}. I found my Topo logs here. . @silence-liu @amirfirouzi \nFollow the steps provided by @mycFelix , I can see my topology on heron-ui after step 3. It IS a problem of heron-tracker configuration for me. Thanks a lot.\n\n\n\nI think the first step you need to check is whether your topology is running well on YARN. Pls check your YARN-scheduler-webstie to confirm your applicationId's status. \n\n\nIf your topology is running well on YARN, then we should focus on the driver.stderr and evaluator.stderr to make sure there is no error while running. You can follow the instructions on http://twitter.github.io/heron/docs/operators/deployment/schedulers/yarn/ section Log File location\n\n\nIf the step 1 and 2 are both fine. I think what you need to do is to check your .herontools/conf/heron_tracker.yaml configs following the instructions on http://twitter.github.io/heron/docs/operators/heron-tracker/ to make sure the statemgrs is set up right.\n. @ashvina @mycFelix \nExcuse me. I'm not very sure whether my topology submission is successful. \nFollow the instruction here, I deploy heron on a single node using yarn as scheduler. I'm using single node zookeeper and hadoop as well.\n\n\n\nI can see my topology on heron-ui and yarn-ui, and the result of jps looks like this:\n28608 REEFLauncher\n28354 SubmitterMain\n2243 MetricsManager\n28421 REEFLauncher\n872 SecondaryNameNode\n618 DataNode\n4235 ResourceManager\n28685 HeronInstance\n4398 NodeManager\n4242 Jps\n4786 QuorumPeerMain\n28727 MetricsManager\n28696 HeronInstance\n476 NameNode\nBut the submission stuck here:\n[2017-11-02 22:07:17 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Created node for path: /heron/topologies/WordCountTopology  \n[2017-11-02 22:07:17 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Created node for path: /heron/packingplans/WordCountTopology  \n[2017-11-02 22:07:17 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Created node for path: /heron/executionstate/WordCountTopology  \n[2017-11-02 22:07:17 +0800] [WARNING] org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable  \n[2017-11-02 22:07:18 +0800] [INFO] org.apache.reef.util.REEFVersion: REEF Version: 0.14.0  \n[2017-11-02 22:07:18 +0800] [INFO] com.twitter.heron.scheduler.yarn.ReefClientSideHandlers: Initializing REEF client handlers for Heron, topology: WordCountTopology  \n[2017-11-02 22:07:18 +0800] [INFO] org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at /127.0.0.1:8032  \n[2017-11-02 22:07:24 +0800] [WARNING] org.apache.reef.runtime.common.files.JobJarMaker: Failed to delete [/tmp/reef-job-5871854936332514150]  \n[2017-11-02 22:07:25 +0800] [INFO] org.apache.reef.runtime.yarn.client.YarnSubmissionHelper: Submitting REEF Application to YARN. ID: application_1509597897282_0004  \n[2017-11-02 22:07:25 +0800] [INFO] org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1509597897282_0004  \n[2017-11-02 22:07:27 +0800] [INFO] com.twitter.heron.scheduler.yarn.ReefClientSideHandlers: Topology WordCountTopology is running, jobId WordCountTopology.  \n[2017-11-02 22:07:27 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Closing the CuratorClient to: 127.0.0.1:2181  \n17/11/02 22:07:27 INFO imps.CuratorFrameworkImpl: backgroundOperationsLoop exiting\n17/11/02 22:07:27 INFO zookeeper.ZooKeeper: Session: 0x15f7b0c6f680028 closed\n[2017-11-02 22:07:27 +0800] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Closing the tunnel processes  \n17/11/02 22:07:27 INFO zookeeper.ClientCnxn: EventThread shut down\nI'm confused about this.\nAnd I can kill the topology from another terminal successfully. About 1 hours later, there is only 1 MetricsManager left. The topology\u2019s overview on heron-ui turns to red and no more data(all metrics are 0) on its detailed page.\nIs there anything wrong? Grateful for any help. \n. @kramasamy Thanks for your reply. http://maven.twttr.com/  is blocked in China. Bazel can't access it in terminal. I can access http://maven.twttr.com/ and download libthrift.jar in my browser some way. I would think I've put the jar in the right path in my local maven repository. \nls ~/.m2/repository/org/apache/thrift/libthrift/0.5.0-1\nlibthrift-0.5.0-1.jar libthrift-0.5.0-1.pom libthrift-0.5.0-1-sources.jar\nIs there any way to let bazel find the maven dependency in my local maven repository? Or it just retrieves jars from the remote maven_server?. ",
    "jomsdev": "I have just signed it.\nThanks.\n. I did it yesterday cloning from the master branch.\nEDIT:\nIf you are asking about the \n\nNo handlers could be found for logger \"root\"\n\nthen the answer is yes. It was after install heron 0.14.2 using the Ubuntu .sh files downloaded from https://github.com/twitter/heron/releases\nI am using OS X and tried to installed it on two fresh virtual machines.\n. @objmagic - Only to be sure that it was the 0.14.02 version I repeated it. Same problem \n\nNo handlers could be found for logger \"root\"\n\nI installed a fresh ubuntu version and then installed java using the apt-get default-jre default-jdk. That's all.\n. I could install it compiling it from source. However, using the script that you provide (0.14.02) I couldn't. It was using a fresh Ubuntu. \nI would close it and take it into account only if anyone else complains about the same problem.\n. I just tried it and it is working now, we can close the issue.\n. Hello kramasamy,\nThank you for your prompt reply, I opened an issue at the same time that this pull request explaining the troubles. #1355 \n. ",
    "Au80": "@kramasamy I run bazel build --config=centos --verbose_failures heron/... , but nothing more shows.\n[rjfh@au heron-0.14.2]$ bazel build --config=centos --verbose_failures heron/...\nINFO: Found 385 targets...\nERROR: Process exited with status 1.\nINFO: Elapsed time: 1.359s, Critical Path: 0.25s\n. @kramasamy I have checked the software versions which are needed by building Heron, versions match requirements.\nBazel = 0.3.0. \nJava 8 \nAutoconf >= 2.6.3\nAutomake >= 1.11.1\nGNU Make >= 3.81\nGNU Libtool >= 2.4.6\ngcc/g++ >= 4.8.1 (Linux platforms)\nCMake >= 2.6.4\nPython >= 2.7 (not including Python 3.x)\nPerl >= 5.8.8\nBefore I rebuild Heron, I have ran the command \nsudo yum install gcc gcc-c++ kernel-devel wget unzip zlib-devel zip git automake cmake patch libtool -y\nand \nyum -y update\nand \nyum groupinstall \"Base\" \"Development Libraries\" \"Development Tools\".\nHow can I check the docker/Dockerfile.centos7 file to sure softwares that I have installed in my machine are available  . \n. @kramasamy - I have sured my Docker is well installed.\n[rjfh@au heron-0.14.2]$ docker run hello-world\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\nTo generate this message, Docker took the following steps:\n1. The Docker client contacted the Docker daemon.\n2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n3. The Docker daemon created a new container from that image which runs the\n   executable that produces the output you are currently reading.\n4. The Docker daemon streamed that output to the Docker client, which sent it\n   to your terminal.\n   To try something more ambitious, you can run an Ubuntu container with:\n   $ docker run -it ubuntu bash\n   Share images, automate workflows, and more with a free Docker Hub account:\n   https://hub.docker.com\n   For more examples and ideas, visit:\n   https://docs.docker.com/engine/userguide/\nI followed documentation in https://github.com/twitter/heron/tree/master/docker, I run the command\n./docker/build-artifacts.sh centos 7.2 /home/rjfh/downloads/heron-0.14.2.tar.gz /home/rjfh/downloads/output/heron-0.14.2\n[rjfh@au heron-0.14.2]$ pwd\n/home/rjfh/downloads/heron-0.14.2\n[rjfh@au heron-0.14.2]$ ./docker/build-artifacts.sh centos 7.2 /home/rjfh/downloads/heron-0.14.2.tar.gz /home/rjfh/downloads/output/heron-0.14.2\nCreating output directory /home/rjfh/downloads/output/heron-0.14.2\nfatal: Not a git repository (or any of the parent directories): .git\nfatal: Not a git repository (or any of the parent directories): .git\nfatal: Not a git repository (or any of the parent directories): .git\nThe Dockerfiler /home/rjfh/.heron-compile/Dockerfile.centos does not exist\nI alse list the files in directory /home/rjfh/.heron-compile/\n[rjfh@au heron-0.14.2]$ ll /home/rjfh/.heron-compile/\ntotal 52\n-rwxrwxr-x. 1 rjfh rjfh 3160 Sep 11 12:43 build-artifacts.sh\n-rwxrwxr-x. 1 rjfh rjfh 1167 Sep 11 12:43 build-docker.sh\n-rwxrwxr-x. 1 rjfh rjfh  223 Sep 11 12:43 cleanup-dockers.sh\n-rwxrwxr-x. 1 rjfh rjfh 1365 Sep 11 12:43 compile-docker.sh\n-rwxrwxr-x. 1 rjfh rjfh 1502 Sep 11 12:43 compile-platform.sh\n-rw-rw-r--. 1 rjfh rjfh  711 Sep 11 12:43 docker-compose.yml\n-rw-rw-r--. 1 rjfh rjfh 1379 Sep 11 12:43 Dockerfile.centos7\n-rw-rw-r--. 1 rjfh rjfh  827 Sep 11 12:43 Dockerfile.dist.ubuntu14.04\n-rw-rw-r--. 1 rjfh rjfh 1140 Sep 11 12:43 Dockerfile.ubuntu14.04\n-rw-rw-r--. 1 rjfh rjfh 1140 Sep 11 12:43 Dockerfile.ubuntu15.10\n-rw-rw-r--. 1 rjfh rjfh 1291 Sep 11 12:43 Readme.md\n-rwxrwxr-x. 1 rjfh rjfh 1102 Sep 11 12:43 start-docker.sh\n-rwxrwxr-x. 1 rjfh rjfh   71 Sep 11 12:43 stop-docker.sh\nThe Dockerfiler /home/rjfh/.heron-compile/Dockerfile.centos does not exist, but Dockerfile.centos7 exists. What is wrong?\n. @kramasamy - Forgive my stupid mistake. Thank you for your valuable time.  Thank you for your constant help and patience.  Unfortunately, I guess I am still failed.\nINFO: From Executing extra_action //tools/java:checkstyle_java on //third_party/java:jarjar:\nStarting audit...\nAudit done.\nINFO: From Executing extra_action //tools/python:checkstyle_python on //heron/shell/src/python:heron-shell:\nSep 11, 2016 9:28:42 AM com.twitter.bazel.checkstyle.PythonCheckstyle main\nINFO: 2 python files found by checkstyle\nINFO: From Executing extra_action //tools/python:checkstyle_python on //heron/executor/src/python:heron-executor:\nSep 11, 2016 9:28:44 AM com.twitter.bazel.checkstyle.PythonCheckstyle main\nINFO: 8 python files found by checkstyle\nINFO: From Executing extra_action //tools/python:checkstyle_python on //heron/explorer/src/python:heron-explorer:\nSep 11, 2016 9:28:43 AM com.twitter.bazel.checkstyle.PythonCheckstyle main\nINFO: 45 python files found by checkstyle\nINFO: From Executing extra_action //tools/python:checkstyle_python on //heron/ui/src/python:heron-ui:\nSep 11, 2016 9:28:45 AM com.twitter.bazel.checkstyle.PythonCheckstyle main\nINFO: 45 python files found by checkstyle\nINFO: From Executing extra_action //tools/python:checkstyle_python on //heron/tracker/src/python:heron-tracker:\nSep 11, 2016 9:28:42 AM com.twitter.bazel.checkstyle.PythonCheckstyle main\nINFO: 70 python files found by checkstyle\n[1,195 / 1,200] Still waiting for 1 job to complete:\n[1,195 / 1,200] Still waiting for 1 job to complete:\n      Running (standalone):\n        PexPython heron/cli/src/python/heron.pex, 6323 s\n^CCleaning up scratch dir\n. @kramasamy - In the log, PexPython heron/cli/src/python/heron.pex, 6323 s, I think something has gone wrong, so I press ^C.\n. @kramasamy - I am sure I can access internet, but I can't know which site is restricted because of some reasons.\n[1,195 / 1,200] Still waiting for 1 job to complete: \nRunning (standalone): PexPython heron/cli/src/python/heron.pex, 6323 s\nThe building is always waiting for [1,195 / 1,200]  job. This may cost hours, but job will not compelete.\n. ",
    "mtunique": "java version \"1.8.0_101\"\nJava(TM) SE Runtime Environment (build 1.8.0_101-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode)\nOS: macOS 10.12 (16A320)\n. ubuntu is also failed.\n@kramasamy \njava version \"1.8.0_101\"\nJava(TM) SE Runtime Environment (build 1.8.0_101-b13)\nJava HotSpot(TM) 64-Bit Server VM (build 25.101-b13, mixed mode)\nLinux 4.4.0-36-generic #55~14.04.1-Ubuntu SMP Fri Aug 12 11:49:30 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\n. @kramasamy \nI run ./scripts/setup-intellij.sh.\n. I change   (JavaCheckstyle.java)\njava\n  private static String[] getSourceFiles(String extraActionFile,\n                                         Predicate<String> predicate) {\nto \njava\nprivate static String[] getSourceFiles(String extraActionFile,\n                                         Predicate<CharSequence> predicate) {\nand \njava\n  private static void checkStyle(String[] files, String config) throws IOException {\n    if (files.length == 0) {\n      LOG.fine(\"No java files found by checkstyle\");\n      return ;\n    }\nto\njava\n  private static void checkStyle(String[] files, String config) throws IOException {\n    if (files.length == 0) {\n      LOG.fine(\"No java files found by checkstyle\");\n      return;\n    }\n```.\nIt is passed on mac.\n. @kramasamy ok\n. @kramasamy accepted\n. Because of the same reason, TMasterSinkTest.testTMasterClientService is also failed.\n. ",
    "tobecontinued": "@kramasamy , I have done it.\n. @maosongfu , It's not my first time send PR, Should I sign CLA again?\n. ",
    "chris-pardy": "@kramasamy - much like the other uploaders this leaves all the questions of \"how\" to run the topology to the schedulers. Assuming you have a base image with the Heron Core libs and then you use this uploader the result will be a docker image with all the code needed to run a topology container. In the case of something like Aurora we're starting multiple jobs that use this container each of which run a part of the topology.\n. @kramasamy the Scheduler is passed the URL that the uploader outputs in the Runtime Config, since we're sending the Docker tag as the URI here (technically it's not a URI but...) the Scheduler can export it. In the case of the Aurora scheduler you have:\n`...\nString heronCoreReleasePkgURI = Context.corePackageUri(config);\nString topologyPkgURI = Runtime.topologyPackageUri(runtime).toString();\nauroraProperties.put(\"CORE_PACKAGE_URI\", heronCoreReleasePkgURI);\nauroraProperties.put(\"TOPOLOGY_PACKAGE_URI\", topologyPkgURI);\n...`\nSo the heron.aurora file could download the image by doing something like docker pull {{TOPOLOGY_PACKAGE_URI}}. Aurora will handle the templating.\nAnd re: Slack chris.pardy@gmail.com \n. @kramasamy I've added a documentation page and a sample heron.docker.aurora file. Please let me know if there's anything else needed.\n. @ajorgensen the docker image is just used to distribute the topology binaries and yaml config. The log files are written into the working directory which is the Aurora Sandbox.\n. @ajorgensen no, the Aurora Scheduler with a correctly configured heron.aurora file (per the example given) will work. I'm not sure what the details of this are but effectively Aurora will take care of getting the docker image from the registry and inflating it, it will then configure the sandbox such that it's reachable from the executable that are in the docker image. Basically using this serves 2 purposes.\n1) It makes it so that fetching the topology tar doesn't rely on something like 'curl' or 'hdfs' being installed on the mesos slaves. Generally speaking making the slaves stupid enough to just use docker is the advice I've gotten for Aurora.\n2) It allows multiple instances of the job on the same machine to leverage the same docker image download preventing it from repeating downloads.\nAs part of 2 if you were to build a docker image with all the docker core libraries and set it as the base image in the uploader config Mesos/Aurora is smart enough to cache that base image on the executors which means that if you're running a number of different topologies they'll each only download the bits that make them different. I haven't explicitly configured things for this use case as I'm expecting that each user will end up wanting  +  + .\n. @kramasamy I'm not sure what exactly is required by the new Mesos/Aurora Docker support. However I'm guessing it would be the addition of a \"CMD\" directive at the end of the Dockerfile basically saying what to run. Because the heron core binaries have to be compiled to the target system (Ubuntu, CentOs, etc.) you can't install those as part of creating this image without tying yourself to a specific distribution so you would need to rely on base image that was specified having them installed already. Additionally at the point when the uploader is invoked you don't know most of the information you need to actually run the topology (ie. Ram Map) so in practice what would be required is a CMD directive that looks a lot like the command_to_start_executor in the heron.aurora file, which is to say a lot of variables that can be specified. The scheduler would then need to specify things like the path to the Core binaries to the docker command so that they can be turned into environment variables.\nI like the idea of getting this Docker container to be executable but I think it's beyond the scope of this PR. Assuming that the base image on which this container is built includes the core binaries it's possible to take the output of this uploader and run it by specifying the executor binary to the docker run command.\n. Is this good to be merged or does it need additional changes?\n. It doesn't nessesarily make sense to build a base image since people are\nlikely going to want to build off task or organization specific base images.\nI believe the problem you're seeing is caused by permissions in the docker\ncontainer, can you run docker and do an ls -l on the directory where the\ntopology files are written and see if they're readable by everyone?\nOn Thu, Oct 6, 2016, 17:51 Karthik Ramasamy notifications@github.com\nwrote:\n\n@kramasamy commented on this pull request.\nIn heron/uploaders/src/java/com/twitter/heron/uploader/docker/sample.yaml\nhttps://github.com/twitter/heron/pull/1419:\n\n@@ -0,0 +1,11 @@\n+# uploader class for transferring the topology jar/tar files to storage\n+heron.class.uploader:         com.twitter.heron.uploader.docker.DockerUploader\n+\n+#base image\n+heron.uploader.docker.base:                      ubuntu:latest\n\ngreat! if you and @chris-pardy https://github.com/chris-pardy can let\nus what will be needed, we can get this done.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/pull/1419, or mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AFZplhGvse1gW8u3wOpcMNBTvIzd0O62ks5qxW1HgaJpZM4KFHVZ\n.\n. @ajorgensen Doing a build of Heron via Docker seems to hang with 177 tasks left. So I'm not able to do a complete verification yet.\nThe Base image needs to be something with both python and java installed, I'm not sure if there are standard images out on github you can use but this may be a good use case for building a base image.\n. @kramasamy I think the next step at this point is to build a base docker image that can be uploaded to docker hub for people to use. I don't have time to do that right now but I may be able to get to it in the next week.\n. @kramasamy I obviously don't have time to finish this now, but please feel free to appropriate what you will in the future!. @kramasamy @billonahill I believe I've addressed all the PR comments. The CI check seems to be failing because I'm missing JavaDoc on a Unit Test that I modified. I'm guessing this is the result of a mis-configured checkstyle rule. Would you prefer if I edited the rule to ignore tests or added JavaDoc to the test?\n. @kramasamy I think you're right that the 2 configs is overkill, I was mirroring the structure of the code (conf dir + file) but I think it makes more sense just to have one config. I've updated the code + added some documentation, I've also changed the conf files to call out that the line is commented out and what it will do more clearly.\n. @kramasamy I just filled out the CLA. Is there a way I can get this PR to  build again?\n. @kramasamy not sure what happened to the CI build but it doesn't look related to my PR?\n. Yep\n\nOn Sun, Oct 9, 2016, 14:22 Karthik Ramasamy notifications@github.com\nwrote:\n\n@chris-pardy https://github.com/chris-pardy - is this ready to go?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/pull/1420#issuecomment-252503049, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AFZplrnZKn6rptdKl25OTDJO6oSFWeN-ks5qyTDygaJpZM4KFYgP\n.\n. @billonahill this should be completely backwards compatible unless someone was specifically calling the host or port properties. There was never anything in the config loader that mandated host:port formatting, that was a requirement in the statemanagerfactory. Additionally there should be no config file changes required statemanagerfactory now just correctly handles the case of a string of comma separated values. The only case where behavior would change from before this that I can come up with is if you had a zookeeper statemanager and you were relying on it failing to get configured by passing a set of comma seperated host:ports that's a incredibly weird behavior so I don't think it needs to be called out.\n. @kramasamy @billonahill Looking at the CI it looks like my unittests are failing. Running locally I'm seeing the following failure:\n\nexec ${PAGER:-/usr/bin/less} \"$0\" || exit 1\n============================= test session starts ==============================\nplatform darwin -- Python 2.7.11 -- py-1.4.27 -- pytest-2.6.4\nERROR: file not found: heron/statemgrs/tests/python/zkstatemanager_unittest.py\n===============================  in 0.00 seconds ===============================\nThe cause seems to be the script that is generated to run the unit tests as part of the Pex Test target which is:\nPYTHONDONTWRITEBYTECODE=1 /private/var/tmp/_bazel_cpardy/9b99ba7514b33ef96b48f910b2c02c50/execroot/heron/bazel-out/local-fastbuild/bin/heron/statemgrs/tests/python/statemanager_unittest.pex /private/var/tmp/_bazel_cpardy/9b99ba7514b33ef96b48f910b2c02c50/execroot/heron/heron/statemgrs/tests/python/configloader_unittest.py heron/statemgrs/tests/python/zkstatemanager_unittest.py heron/statemgrs/tests/python/statemanagerfactory_unittest.py\nyou can see that the existing tests (configloader_unittest.py) has generated a path in the bazel build directory while my added unit tests have not. I'm not sure if I can get the logs on the CI server but I'm guessing there's a similar problem. \nIf I copy and paste the code above into my terminal and run it the result is passing tests, I also made the tests fail and verified that they do actually run. I dug through the pex rule and it doesn't seem to be doing anything that would treat my new tests differently. I'm guessing at this point that the problem is something with bazel itself. Any thoughts?\n. Is there a way to check if the user didn't supply the role? I don't like the idea of checking for default.\n. That's how we're using it but it's not necessary, a scheduler could do that work, or configure the scheduled task to download the core. I can change the documentation to call out that way of leveraging the base image if you think it's worthwhile.\n. @billonahill I didn't know the --config-property was a thing, thanks. I'll drop the environment config, I actually much prefer the commandline option.\n. I wanted to put something in this file to indicate that it could be set to a different, fixed directory. The Aurora Context file already does the fall back if the tag is not defined. I thought that having it here but commented out made sense since it would easy to simply uncomment the line and change the directory to make use of it.\n. This assumes that the docker image only contains the topology, so we fetch core from a remote url first. The container = ... bit tells Aurora to use the Docker image that the uploader built for the service code. If you wanted to also include the core files you would need to first build a docker image with those libraries. That's going to be an exercise left up to the reader.\n. The second parameter to the config.getStringValue method is the default value, I'm using the same values here that used to be in AuroraScheduler.\n. The containder = Docker is only supported in newer versions of Aurora. The heron_topology_image_uri is a python variable that's defined through the binding interpolation near the top of the file so just using it raw here is fine. We're using a patched version of Aurora with different config files so it's harder for me to test this change. Thanks.\n. Did it work when you changed the package to python:2.7?\n. I'm personally using a Docker base image with the binaries, but writing them into the sandbox should be fine. I'm going to verify this tonight.\n. @ajorgensen when you do go down that route you'll find that the heron executor tries to run chmod +x on a number of the base files. That won't work if they're not owned by the user who's running the executor. That's definitely not going to be the case for a Docker Image deployed to Aurora. It's a simple change to the executor to treat those as non-terminal failures.\nI can put together a Docker file for some base images easily enough, however that's not going to change this Uploader at all. I'm working on figuring out why it's not working for you but the problem I'm currently having is that I can't get the docker build to complete locally, it just hangs with 177 tasks remaining.\n. ",
    "vbhavsar": "That was super fast. Thank you for the wonderful support @maosongfu @kramasamy!\n. ",
    "tnachen": "Part of the work we are doing for spark on k8s is a Java client that integrates with k8s, that I think Heron can use as well instead of shelling to kubectl.\n. ",
    "reconditesea": "@kramasamy Sure thing, done.\n. @nlu90 Is it an option to update checkstyle to also accept Amazon License?\n. ",
    "antoninoxs": "Sorry i am a newbie\nIl 21/Ott/2016 20:21, \"Bill Graham\" notifications@github.com ha scritto:\n\nPlease don't double post on the mailing list and githib. Let's let the\ndiscussion for this happen on the mailing list.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/1508#issuecomment-255437060, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AEbajlR6pkbHHSABnlxHgVjLdEPPyz3Qks5q2QKtgaJpZM4KddaS\n.\n. \n",
    "fabianmenges": "@billonahill Thanks for your feedback. I'm a Twitter employee, I probably don't need to accept the contributors CLA.. @billonahill fixed, thx.. I don't think that failed integration check can be caused by my changes..... Thanks. TypeUtils.getInteger already does that. However I should probably catch and re-throw that to add context.. Flat map of metrics (testFlatMetrics)  vs. a map of map of metrics (testGroupedMetrics), check out the unit tests. . Not sure if changing your metrics names over time is a great design either ;) but I will fix that.. Just checked some other Sinks and they tend to throw a RuntimeException if they can't start, I'll make that change as well.. I would be just repeating this comment,\nhttps://github.com/tc-dc/heron/blob/ed0e25cb71dd882c159ef4bbcf1fe634a891667c/heron/metricsmgr/src/java/com/twitter/heron/metricsmgr/sink/WebSink.java#L74. Sadly we cannot. Apparently you can not annotate a definition if the variable is not also assigned.\ne.g. \n@SuppressWarnings(\"unchecked\")\nMap<String, Double> sourceCache;\nObject sourceObj = metricsCache.getIfPresent(source);\nif (sourceObj instanceof Map) {\n    sourceCache =  (Map<String, Double>) sourceObj;\n...\nbreaks with warning: [unchecked] unchecked cast and\n@SuppressWarnings(\"unchecked\")\nsourceCache =  (Map<String, Double>) sourceObj;\nis not legal syntax. Yes, it does. Its the standard .putAll method from the Map interface. \npublic void putAll(Map<? extends K,? extends V> m)\nCopies all of the mappings from the specified map to this map. These mappings will replace any mappings that this map had for any of the keys currently in the specified map.. I'm not sure what you mean with ignoring the metric type. \nLets say we have two records record_1 and record_2 with this yaml representation:\nrecord_1:\n  source: source_1\n  metrics: \n    metric_1: 1\n    metric_2: nop\n    metric_3: 3\nrecord_2:\n  source: source_1\n  metrics:\n    metric_1: 2\n    metric_4: 4\nAnd they are getting send to the WebSink in order.\nThe WebSink will publish these metrics on a REST endpoint returning json. \nAfter \"record_1\" is processed the endpoint will return\n\"source_1\": {\n    \"metric_1\": 1.0\n    \"metric_3\": 3.0\n}\nAs you can see only numeric metrics are published, \"metric_2\" is omitted.\nAfter \"record_2\" is processed the endpoint will return:\n\"source_1\": {\n  \"metric_1\": 2.0,\n  \"metric_3\": 3.0,\n  \"metric_4\": 5.0,\n}\n\"metric_1\" was updated, \"metric_3\" was not changed, \"metric_4\" was added by the 2nd record.\nThe old behavior, before this change, would have returned:\n\"source_1\": {\n  \"metric_1\": 2.0,\n  \"metric_4\": 5.0,\n}\nThe point of this metrics sink is, that an external process queries periodically (once a min) the \nREST endpoint to collect the metrics. If the two records would get processed right after one another,\nthere would be the chance that the metrics collector would never see \"metric_3\".\nThis unit test is verifying the behavior described:\nhttps://github.com/tc-dc/heron/blob/ed0e25cb71dd882c159ef4bbcf1fe634a891667c/heron/metricsmgr/tests/java/com/twitter/heron/metricsmgr/sink/WebSinkTest.java#L165. ",
    "dwy189": "Why shall we Replace Charset.forName(\"UTF-8\") to StandardCharsets.UTF_8? . ",
    "anty": "I follow the instructions on Building on Linux Platforms\n, but can't make success. \n\nbazel build --config=centos --verbose_failures heron/...\nINFO: Found 434 targets...\nERROR: Process exited with status 1.\nINFO: Elapsed time: 3.080s, Critical Path: 2.92s. Sometimes, it will fail with following error\nthird_party/python/pylint:pylint [action 'PexPython third_party/python/pylint/pylint.pex [for host]']\n(cd /root/.cache/bazel/_bazel_root/6f84414296ba6593093f156bd47eea3c/execroot/heron-0.14.5 && \\\n  exec env - \\\n  bazel-out/host/bin/third_party/pex/_pex --disable-cache --entry-point third_party.python.pylint.main bazel-out/host/bin/third_party/python/pylint/pylint.pex bazel-out/host/bin/third_party/python/pylint/pylint.pex.manifest)\nERROR: Process exited with status 1.. Hi objmagic, this is output for ./bazel_configure.py\nPlatform Linux\nUsing C compiler          :     /data/anty/testbed/gcc-build-5.1.0/bin/gcc (5.1.0)\nUsing C++ compiler        :     /data/anty/testbed/gcc-build-5.1.0/bin/g++ (5.1.0)\nUsing C preprocessor      :     /data/anty/testbed/gcc-build-5.1.0/bin/cpp (5.1.0)\nUsing C++ preprocessor    :     /data/anty/testbed/gcc-build-5.1.0/bin/cpp (5.1.0)\nUsing linker              :     /usr/bin/ld\nUsing Automake            :     /usr/bin/automake (1.11.1)\nUsing Autoconf            :     /usr/local/bin/autoconf (2.69)\nUsing Make                :     /usr/bin/make (3.81)\nUsing CMake               :     /usr/local/bin/cmake (2.8.12.2)\nUsing Python2             :     /data/anty/testbed/python/bin/python2.7 (2.7.12)\nUsing archiver            :     /usr/bin/ar\nUsing coverage tool       :     /data/anty/testbed/gcc-build-5.1.0/bin/gcov\ndwp                       :     not found, but ok\nUsing nm                  :     /usr/bin/nm\nUsing objcopy             :     /usr/bin/objcopy\nUsing objdump             :     /usr/bin/objdump\nUsing strip               :     /usr/bin/strip\nWrote the environment exec file scripts/compile/env_exec.sh\n. Following is error output after add -s option to bazel\n//third_party/java:yarn [action 'Building third_party/java/libyarn.jar (1 source file)']\n(cd /root/.cache/bazel/_bazel_root/6f84414296ba6593093f156bd47eea3c/execroot/heron-0.14.5 && \\\n  exec env - \\\n    LC_CTYPE=en_US.UTF-8 \\\n  external/local_jdk/bin/java -Xbootclasspath/p:external/bazel_tools/third_party/java/jdk/langtools/javac.jar -jar external/bazel_tools/tools/jdk/JavaBuilder_deploy.jar @bazel-out/local-fastbuild/bin/third_party/java/libyarn.jar-2.params)\n //third_party/python/pylint:pylint [action 'PexPython third_party/python/pylint/pylint.pex [for host]']\n(cd /root/.cache/bazel/_bazel_root/6f84414296ba6593093f156bd47eea3c/execroot/heron-0.14.5 && \\\n  exec env - \\\n  bazel-out/host/bin/third_party/pex/_pex --disable-cache --entry-point third_party.python.pylint.main bazel-out/host/bin/third_party/python/pylint/pylint.pex bazel-out/host/bin/third_party/python/pylint/pylint.pex.manifest)\nERROR: Process exited with status 1.\nINFO: Elapsed time: 0.220s, Critical Path: 0.12s\n. I try to compile heron on Centos7, can't make any success anywhere.\n1. this is the output of ./bazel_configure.py\nPlatform Linux\nUsing C compiler          :     /usr/bin/gcc (4.8.5)\nUsing C++ compiler        :     /usr/bin/g++ (4.8.5)\nUsing C preprocessor      :     /usr/bin/cpp (4.8.5)\nUsing C++ preprocessor    :     /usr/bin/cpp (4.8.5)\nUsing linker              :     /usr/bin/ld.bfd\nUsing Automake            :     /usr/bin/automake (1.13.4)\nUsing Autoconf            :     /usr/bin/autoconf (2.69)\nUsing Make                :     /usr/bin/make (3.82)\nUsing CMake               :     /usr/local/bin/cmake (3.3.2)\nUsing Python2             :     /usr/bin/python2.7 (2.7.5)\nUsing archiver            :     /usr/bin/ar\nUsing coverage tool       :     /usr/bin/gcov\nUsing dwp                 :     /usr/bin/dwp\nUsing nm                  :     /usr/bin/nm\nUsing objcopy             :     /usr/bin/objcopy\nUsing objdump             :     /usr/bin/objdump\nUsing strip               :     /usr/bin/strip\nWrote the environment exec file scripts/compile/env_exec.sh. 2. There are error output when compile heron(0.14.5)\n\n```\n             from src/logging_unittest.cc:33:\n\nsrc/logging_unittest.cc: In function 'void TestLogging(bool)':\nsrc/glog/logging.h:917:30: warning: typedef 'INVALID_REQUESTED_LOG_SEVERITY' locally defined but not used [-Wunused-local-typedefs]\n                              INVALID_REQUESTED_LOG_SEVERITY);           \\\n                              ^\nsrc/glog/logging.h:912:73: note: in definition of macro 'GOOGLE_GLOG_COMPILE_ASSERT'\n   typedef google::glog_internal_namespace_::CompileAssert<(bool(expr))> msg[bool(expr) ? 1 : -1]\n                                                                         ^\nsrc/logging_unittest.cc:254:5: note: in expansion of macro 'LOG_EVERY_N'\n     LOG_EVERY_N(ERROR, 4) << \"Log every 4, iteration \" << COUNTER << endl;\n     ^\nIn file included from /usr/include/c++/4.8.2/ext/hash_set:60:0,\n                 from src/glog/stl_logging.h:73,\n                 from src/stl_logging_unittest.cc:53:\n/usr/include/c++/4.8.2/backward/backward_warning.h:32:2: warning: #warning This file includes at least one deprecated or antiquated header which may be removed without further notice at a future date. Please use a non-deprecated interface with equivalent functionality instead. For a listing of replacement headers and interfaces, consult the file backward_warning.h. To disable this warning use -Wno-deprecated. [-Wcpp]\n #warning \\\n  ^\nIn file included from src/utilities.h:73:0,\n                 from src/googletest.h:38,\n                 from src/stl_logging_unittest.cc:54:\nsrc/base/mutex.h:137:0: warning: \"_XOPEN_SOURCE\" redefined [enabled by default]\n #   define _XOPEN_SOURCE 500  // may be needed to get the rwlock calls\n ^\nIn file included from /usr/include/c++/4.8.2/x86_64-redhat-linux/bits/os_defines.h:39:0,\n                 from /usr/include/c++/4.8.2/x86_64-redhat-linux/bits/c++config.h:2097,\n                 from /usr/include/c++/4.8.2/iostream:38,\n                 from src/stl_logging_unittest.cc:34:\n/usr/include/features.h:170:0: note: this is the location of the previous definition\n # define _XOPEN_SOURCE 700\n ^\nERROR: Process exited with status 1.\nINFO: Elapsed time: 197.198s, Critical Path: 192.43s\n```. ",
    "chgl": "Oops, didn't see the sample.yaml. Updated now. @objmagic I signed the CLA before with my GitHub username, just to clarify: do I need to resign for every PR (with the URL)?\nI'm not sure how I feel about destination, seems confusing in the context of copying files/scp.. ",
    "jrcrawfo": "Hey guys, I did a bit more work over the weekend primarily around getting this to truly work on a remote DC/OS cluster. I'm having trouble with the deactivate/activate commands, as the Tmaster is behind a firewall and it seems like that command has to talk directly with Tmaster (I think?). I got a docker image for the tools package built so I got the Tracker API and the Heron UI up and running on DC/OS correctly. I'm working on bundling that into a legit DC/OS universe package where DCOS users can just do dcos package install heron. So, things I have outstanding here are:\n - Fix remaining failing test\n - Make changes that were suggested on the reviews above (small code changes, verify the 'HOST' variable)\n - Add my Dockerfiles for the executor to the project\n - Add Dockerfiles used for API tracker and Heron UI deployment\n - Figure out where to put DC/OS package information\n - Add a configuration in the marathon scheduler for Auth token (most clusters use JWT-based authentication)\n - Figure out how to get around activate/deactivate command issues behind a firewall\n - Add some documentation for getting marathon scheduler running against a dc/os scheduler\nThis is what I think I have left to do. Any other suggestions or things I'm missing?. @kramasamy on vacation til the 28th but I plan on hopping on this the following week pretty hard, as we also want to get this into testing on our cluster. Trying to convert our storm topologies over :). Sure, it's **. @kramasamy not quite...i've done a lot of work with the tracker and UI to get that to work on DC/OS properly and maintain backwards compatibility but i've still got to do some cleanup with the container Dockerfiles and still make some changes as requested above. Been super busy since I got back to work a couple weeks ago but I should be able to spend time on it soon.. @kramasamy still a work in progress. i've been working on getting it running in DC/OS via a command line arg (dcos package install heron-tracker). I ran the exclamation topology via Marathon for close to a month without issue so i'm feeling good about the marathon deployment itself via the new Docker image. I'm going to try and finish the remaining items in this PR by the end of the week next week and figure out where to put the dcos package info as well. @kramasamy think i'm ready for another round of reviews. Currently working on docs for Marathon and DC/OS deployments to add as well.. Ok, I'll add in the comments you made above. I don't think it's very useful without some documentation so I'll add that in tonight or in the morning as well. I'll detail the marathon deployment (which the DC/OS implementation is based on), but it might be more appropriate to do the DC/OS docs along with wherever the actual dcos install heron command will live once i'm finished with that. Does that sound fine?. @kramasamy -- Tested via my Marathon test cluster and everything functions as it should \ud83d\udc4d . Done. @kramasamy @nlu90 Added the test and fixed the new line. In general, I don't think we should do this unless the Docker build process for adding those core binaries into the image are made available. Otherwise, it's difficult to test any changes to a new build since the Dockerfile is not available. I really like the idea of including the the core binaries in the image, but the Dockerfile you used to put the libraries in there should be made available to the user and built into the build process. Otherwise, it just seems very disconnected and when a new release is cut, there's no process for getting that new release built into a new docker image to be ran.. Other than the Docker tagging comment above, which is something to be done outside of this anyway, this looks good \ud83d\udc4d  . @nlu90 @kramasamy ready for a second review. \ud83d\udc4d  looks good to me. Still a work in progress as there is a bug in the update function -- not quite sure of the source of the bug, as it's not related to the scheduler. This bug is preventing me from testing the update functionality, however.. @srkukarni i can look against master later, but here's the exception i was getting:\n```\ncom.twitter.heron.spi.packing.PackingException: Could not initialize containers using existing packing plan\n        at com.twitter.heron.packing.builder.PackingPlanBuilder.initContainers(PackingPlanBuilder.java:259)\n        at com.twitter.heron.packing.builder.PackingPlanBuilder.addInstance(PackingPlanBuilder.java:153)\n        at com.twitter.heron.packing.builder.PackingPlanBuilder.addInstance(PackingPlanBuilder.java:141)\n        at com.twitter.heron.packing.binpacking.FirstFitDecreasingPacking.placeFFDInstance(FirstFitDecreasingPacking.java:312)\n        at com.twitter.heron.packing.binpacking.FirstFitDecreasingPacking.assignInstancesToContainers(FirstFitDecreasingPacking.java:265)\n        at com.twitter.heron.packing.binpacking.FirstFitDecreasingPacking.getFFDAllocation(FirstFitDecreasingPacking.java:246)\n        at com.twitter.heron.packing.binpacking.FirstFitDecreasingPacking.repack(FirstFitDecreasingPacking.java:180)\n        at com.twitter.heron.scheduler.RuntimeManagerRunner.buildNewPackingPlan(RuntimeManagerRunner.java:303)\n        at com.twitter.heron.scheduler.RuntimeManagerRunner.updateTopologyHandler(RuntimeManagerRunner.java:182)\n        at com.twitter.heron.scheduler.RuntimeManagerRunner.call(RuntimeManagerRunner.java:81)\n        at com.twitter.heron.scheduler.RuntimeManagerMain.callRuntimeManagerRunner(RuntimeManagerMain.java:448)\n        at com.twitter.heron.scheduler.RuntimeManagerMain.manageTopology(RuntimeManagerMain.java:396)\n        at com.twitter.heron.scheduler.RuntimeManagerMain.main(RuntimeManagerMain.java:317)\nCaused by: com.twitter.heron.packing.ResourceExceededException: Insufficient container resources to add instancePlan {component-name: exclaim1, task-id: 4, component-index: 3, instance-resource: {cpu: 1.000000, ram: ByteAmount{256 MB (268435456 bytes)}, disk: ByteAmount{1 GB (1073741824 bytes)}}} to container {containerId=2, instances=[{component-name: exclaim1, task-id: 2, component-index: 1, instance-resource: {cpu: 1.000000, ram: ByteAmount{256 MB (268435456 bytes)}, disk: ByteAmount{1 GB (1073741824 bytes)}}}], capacity={cpu: 1.000000, ram: ByteAmount{3 GB (2952790016 bytes)}, disk: ByteAmount{3 GB (3221225472 bytes)}}, paddingPercentage=10}[2017-10-18 09:49:32 -0400] [ERROR]: Could not initialize containers using existing packing plan\n    at com.twitter.heron.packing.builder.PackingPlanBuilder.getContainers(PackingPlanBuilder.java:392)\n\n[2017-10-18 09:49:32 -0400] [ERROR]: Failed to update topology: exclamationtopology\n[2017-10-18 09:49:32 -0400] [DEBUG]: Elapsed time: 8.798s.\n```\nWas using the heron.class.repacking.algorithm:  com.twitter.heron.packing.binpacking.FirstFitDecreasingPacking repacking class. @cckellogg i personally wouldn\u2019t recommend using a stateful set, as it\u2019s beta in the newest version (1.8 not available on google compute engine yet either). Of course, you could fairly easily swap this out in the near future, since k8s is moving so fast anyway.\nI'm also not sure you'd be able to get away with making one rest call, as every heron instance (pod) has a different command line argument. Keep in mind the specs that work for kubectl commands don't necessarily translate to API commands as well. kubectl does a lot of work itself making multiple API calls to get things deployed even if it is just one command using kubectl. #1. Yes, we should do that. I\u2019ll make that change. \n2. The reason for using HOST is because it is commonly passed in by the scheduler to be an actual resolvable hostname in a Docker environment (where hostname otherwise is the container id). So, it\u2019s best to leave this as is, as there is also a helper function in the python executor code to do the same logic. . I need to go and verify this -- not sure if there's another way to get the actual hostname but i'll see what i can find. Yes, that makes sense. I think it would also be a good idea to actually include the Dockerfile as well. Could I put that file in the /docker directory and name it Dockerfile.executor maybe?. portName isn't important for it to run within Marathon, but setting the HOST_PORT to 0 allows the host to randomly choose a port on the machine to use. So it does need to be in there.. Makes sense. Will make this change.. As far as I can find, you can't natively \"introspect\" the host IP from within a Docker container. The HOST environment variable is set by Marathon automatically as an environment variable for use cases like this where we care about the HOST IP. So, for other environments outside of DC/OS and Marathon, it will be a requirement (if the executor is running inside a Docker container) to set the HOST environment variable when launched, otherwise the hostname will default to the container ID (which was the trouble I ran into originally).. BRIDGE networking here I think needs to be a requirement, so I'm just going to make that a constant variable instead of being configurable. This seems to be similar to what we had to do with the Marathon Docker environment. You can basically run the executor on any image built with the Dockerfile in the docker/Dockerfile.dist.ubuntu<version> image. The issue here is that we don't currently have a common Docker repo for this to live in yet. I'm thinking we need to have a heron/executor image with tagged versions that is created on new version releases. Otherwise, we're going to keep running into cases like this where people are building their own images, which is an extra step to deployment and a bit of a pain. Could also bundle in the core binaries on the release as well to eliminate the need to pull them when the Docker image is deployed.. If your Docker image contains a pre-installed version of the Java 8 JRE, you should be able to reference $JAVA_HOME here. You should also be able to set that from within the scheduler configuration as well, via the heron.directory.sandbox.java.home option. No need for this line since you won't be grabbing os.environ['HOST'] after this. That paths to the core binaries will need to be more dynamic than this and will need to feed out of the heron.package.core.uri configuration option in the scheduler library right?. Can you please explain what the process is here for creating this docker.yml file just so I understand what's going on? What is it used for and what is it composed of -- just so I can understand a little better.. Yes, I can try to add a test here where i'm looking for the specific response code. Nope, will reset that. With the tag, the only comment i have is to be sure we have tags corresponding to versions number. So, ideally, you'd be referencing something like streamlio/heron:0.14.8. Done. In the controller. Done. Merged master and took care of this. This is not yet supported in kubernetes. I couldn\u2019t find it immediately in the docs last night, but it seems like based on my reading there\u2019s essentially no limit until the node runs out of space. They are planning to add this very soon though from what I read. Good point. Should probably convert the namespace into a configuration for the scheduler. Unfortunately not. I will make a clear log to cover this.. Done. Yeah, since I'll need to re-do the tests here, I'd prefer to do a separate PR here.. @nlu90 agreed, that seems like the best place for it. No need to extract the TAR file anymore? Assuming you've tested this?.",
    "thomas4g": "@objmagic okay, thanks. I noticed after filing this issue that the compilation instructions for Heron (which I hadn't looked at before since I was just installing with the script) mention it can only be compiled on CentOS 7. I'm on CentOS 6. Could that be the source of the issue? . @kramasamy @objmagic OK, thanks very much. . @kramasamy All the other containers are Debian based, so the apt-get install is fine and doesn't need to be changed.. ",
    "dmarchand": "Confirmed libs are now available.. Works for me, I'll make that change. Stat appears to work on everything major, but I'll do some additional digging and testing on some VMs to be sure.\n. In that case argparse is supposed to dump the list of required arguments: https://docs.python.org/3/library/argparse.html#usage\nIt's probably not the best possible help message and we should likely take some time at some point to document the individual arguments, but this will at least clue the developer in as to which argument they added doesn't belong.\n. ",
    "kkdoon": "Yes, This PR solves https://github.com/twitter/heron/issues/1501. However, the issue mentioned in https://github.com/twitter/heron/issues/1502 is not tackled by this PR. So if there is any instance in back-pressure then the UI will indicate that instance. However, if any container is in BP due to network issues, then the UI will not display that. . No problem @billonahill. Updated the title. Please refer to the attached image below for more details:\n\n\nOverall Heron UI page for any topology:\nThis image shows all the instances that are under back-pressure. User can hover over the instance for more details.\n\n\n\nBack Pressure Metric:\nThis image shows inclusion of back pressure metric in Heron UI.\n\n\n\nMax Back Pressure Metric:\nThis image shows that max back pressure data is plotted, across all instances in topology.\n\n\n\n. Hi @kramasamy ,\n\nPoint 1 is implemented, i.e. all instances in back pressure are shown in the physical plan, once the bp metric is clicked. Also, the components in logical plan/DAG are highlighted.\nPoint 2 is not implemented. I was wondering which metric to use to find out which container is under back pressure. I can see that metric \"__server/__time_spent_back_pressure_initiated\" as well as \"__server/__time_spent_back_pressure_aggr\" is available from stream manager instances. Could you direct me to relevant API or metric? I can try to add this feature as well.. I intentionally named it BP in the heron/tools/ui/resources/static/js/plan-stats.js file as this name is too long and it gets cut while rendering, as shown below:\n\n\nInstead of displaying Maximum Back Pressure it only displays Back Pressure. So I changed it to BP.. @objmagic ok I will try to adjust the CSS and try again with 'Back Pressure' label.. ",
    "smirnp": "I have found the issue of the problem. \nPlease check the contents of the org\\apache\\storm\\spout\\RawMultiScheme.class \nStorm's class is working, Heron one is not. So I have reimpelemented it in my topology. \nPlease do not forget to fix it in future releases\nThanks!. Sure! I should do a fork, patch it and make a pull request, right?. ",
    "ttim": "Good job, thanks!. @srkukarni @objmagic protobuf documentation says about this (https://developers.google.com/protocol-buffers/docs/cpptutorial, Optimization Tips):\n\nReuse message objects when possible. Messages try to keep around any memory they allocate for reuse, even when they are cleared. Thus, if you are handling many messages with the same type and similar structure in succession, it is a good idea to reuse the same message object each time to take load off the memory allocator. However, objects can become bloated over time, especially if your messages vary in \"shape\" or if you occasionally construct a message that is much larger than usual. You should monitor the sizes of your message objects by calling the SpaceUsed method and delete them once they get too big.\n\nSo even if you do Clear you still have some kind of memory leak (and therefore assumption @objmagic made still make sense).\nHaving limited number of objects in pool can fix this issue thou: they will be rotated over the time so most likely each message eventually will be deleted. Having big enough pool at the same time almost guarantee all messages will only become bigger and memory will keep increasing.. I recommend to make name worse (with __shaded__ for example if possible) to make it impossible to pass the review in user code with accidental usages.. Unclear for me why do we need two of them.. Do we really need this? Comment explaining that will be very useful.. nit: I prefer to separate this kind of things and keep review as small as possible.. nit: spaces, here and couple of times beyond.. Do we depend on simulator to implement LocalCluster in heron-storm library? Or, if we removed LocalCluster, I recommend do not depend on simulator from heron-storm. I guess publishing of separate testing artifact would be useful for that.. same as above for shading. . In general I can see a point - heron-storm artifacts aims to provide storm api, and therefore heron inside is impl detail. But I guess it's better to expose dependency on heron-api, for different config related things for example.. Is GlobalMetrics exposed in storm 1.0 api?. @objmagic __shaded__ in the beginning will be enough I believe.. @objmagic you can do that separately, merge and do this review on top of it. I'm not strictly against that (maybe too much action needed) but I will prefer to do things this way.. @billonahill one of the reasons is - storm api < 1.0 uses kryo 2.x, while >= 1.0 uses kryo 3.x. \nI guess it's not a big dial to migrate to new storm api if you want to use heron and use old storm thou.. Nobody used this before, because everyone uses heron-storm lib and therefore HeronPluggableSerializerDelegate.\nWe should dive into it again when people will start to use heron-api directly (if this will happen), but for now I guess it's totally fine to remove this class.. @billonahill I see your point, but this class will be easy to resurrect because it's just simplified version of HeronPluggableSerializerDelegate (and actually it's not really useful because doesn't allow user to customize kryo serializer).\nThat's said I'm also ok to keep this class in contrib.. Do we need this?. Is it in contrib? If yes - it's unclear.. heron-api doesn't depend on kryo, right?. @billonahill It's unclear from the name of the class that this is contrib. How do you package this class then? In which artifact? This name suggests this class is a part of somehow main API, which is not. \nAnyway I don't expect you to change the way you package contrib in this PR, but I guess it should be somehow different from what you have right now.  . I guess you can do something like \ntry {\n  LOG.log(\"...\")\n} finally {\n  Runtime.getRuntime().halt(1);\n}\nThis both will log and most likely will halt the process.. ",
    "Detoo": "Thanks, I have just signed it. ",
    "pingzh": "Thanks for replying.\nhmm, 0.14.7 was just released. I have uninstalled the old one and reinstalled 0.14.7. It still does not work :( and got the same error.\nbtw, there is no \nheron-tools-install-0.14.7-darwin.sh``` in the download page.. @objmagic thanks. I have installedheron-tools-install-0.14.7-darwin.sh` and here is the output:\n~/Desktop\n$  heron submit local ~/.heron/examples/heron-examples.jar com.twitter.heron.examples.ExclamationTopology ExclamationTopology --deploy-deactivated --verbose\nusage: heron submit [options] cluster/[role]/[env] topology-file-name topology-class-name [topology-args]\nheron submit: error: unrecognized arguments: ExclamationTopology\n~/Desktop\n$. @objmagic yep\n$ heron version\nheron.build.version : '0.14.7'\nheron.build.time : Thu Apr 27 14:50:04 PDT 2017\nheron.build.timestamp : 1493329804000\nheron.build.host : tw-172-24-220-90.office.twttr.net\nheron.build.user : mobile\nheron.build.git.revision : 052dd4d25d3c64a73d8751d346a4dad7ccdab2b2\nheron.build.git.status : Modified. @objmagic, still, thanks for your help.. ",
    "aahmed-se": "closing as is duplicate of https://github.com/twitter/heron/issues/1769.  * Can we generate javadocs for the entire codebase?\nI plan to do that in the future.\n\nHow are javadocs currently being generated for the website?\nCurrently a release script is run separately which generates googles docs outside the build process.\n\nAlso when would you expect to trigger this? As part of CI and the website build, or just some other time?\nAs part of CI process , the website script should be able to gather the artifacts and publish them as needed.. @billonahill @lucperkins \nI have update the review without the java version tag and enabled it for the three public projects that are published to maven , the website script generates a unified java doc for everything that is not needed for maven publications , we will keep it for now and decide on it's future later.\n. @billonahill @kramasamy I have addressed the comments. @billonahill we are trying to avoid building a single doc archive to upload to maven , since it will have more classes then required for the individual jars , for the website it's fine to have a unified doc collection and we do that already.. Current formulae published , will figure out an automation strategy later,\nhttps://github.com/streamlio/homebrew-formulae. @billonahill update the docs in the repo. @billonahill updated with you doc comment , awaiting for approval. @billonahill not sure , I think I agreed to something when I joined the project.. @billonahill I have signed it now. @billonahill Hey bill can you merge the commit , I don't have merge rights.. We are trying to segregate the external client python package from the internal python code and adopting a flat directory structure so import statements become more straightforward.. We want to test topologies written using heron api objects , because that is what we are encouraging more users to do.\n. generally that's not a thing distributed systems do, I guess one would write a script to do it, but ideally log collectors should be deployed in the cluster. This is not a unit this requires zookeeper instances running and providing ssh access , this should be removed from unit test folder or be rewritten with mocks. @nlu90 can you clarify what you mean, are you are converting it to real unit test case and it's not reporting errors as expected ?\n. the issue seems to bazel not running the tests at all, the issue seems to be happening since the bazel upgrade. example \n./docker/scripts/ci-docker.sh build ubuntu14.04 nightly 0.15.3 bazel-bin/scripts/packages/. the ci docker only needs the location to the bin location which can be be bazel build folder, my opinion is to only keep that one. yes and that's won't work because the base image is ubuntu\nOn Wed, Sep 27, 2017 at 11:06 AM, cckellogg notifications@github.com\nwrote:\n\nIf we build locally on the laptop those binaries will be for osx and not\nubuntu or centos right?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/pull/2350#issuecomment-332607423, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AHo1fUlSaIRmsvYDGzbUnxda3PG2ToDTks5smo63gaJpZM4Pjb-r\n.\n\n\n-- \n-Ali\n. there is no real relation, heronpy was moved out to separate client api from internal code, we are doing somewhat the same here, dsl is now ready to be exposed as part of the  api jar that is published on maven.. for reference\nhttps://github.com/google/protobuf/issues/539. this is resolved in the interim by\nhttps://github.com/twitter/heron/pull/2397. Will cancel this after consultation with @kramasamy . Twitter copyright  will be removed once move to asf is complete, this is an interim solution , as files are intermixed with copied portions and modifications.. This can be closed I didn't get a chance to a proper PR , we are still making different binaries for centos and ubuntu we will keep that for now.\n. @kramasamy @srkukarni \nWe tested this is multi node virtual machine env , there is problem in detection of localhost execution whereas if we want to run the installer on a node and still make it part of the cluster, things don't work as expected it needs to be resolved before we merge the pr.. No not at this time , I couldn't get it to work yet.. @cckellogg refactored to method.  Existing topology with this config  fails\ncom.twitter.heron.api.Config.setContainerCpuRequested(conf, 0.5f);\nHere is the error message from logs \n[2018-03-13 21:24:41 +0000] [ERROR]: This instance requires containers \nwith at least true cpu cores. The current max containersize is 0.5 cores. this is standalone mode on a local machine, not sure what is the configuration for it. We can close this issue, resolution has been discussed offline.. the homebrew-formulae is fixed please run brew update.. Hi @SioKCronin \nI tried the steps below on a fresh brew install and it works fine.\nbrew tap streamlio/homebrew-formulae\nbrew update\nbrew install heron\n. this seems to be a problem with ResourceCompliantRRPacking , instances with ram config between  192 - 256 MB generate 0xmx for the instance plan , the problem seems to be with PackingUtils.java\npublic static Resource computeTotalResourceChange(TopologyAPI.Topology topology,\n                                                    Map<String, Integer> componentChanges,\n                                                    Resource defaultInstanceResources,\n                                                    ScalingDirection scalingDirection) {\n    double cpu = 0;\n    ByteAmount ram = ByteAmount.ZERO;\n    ByteAmount disk = ByteAmount.ZERO;\n    Map<String, ByteAmount> ramMap = TopologyUtils.getComponentRamMapConfig(topology);\n    Map<String, Integer> componentsToScale = PackingUtils.getComponentsToScale(\n        componentChanges, scalingDirection);\n    for (String component : componentsToScale.keySet()) {\n      int parallelismChange = Math.abs(componentChanges.get(component));\n      cpu += parallelismChange * defaultInstanceResources.getCpu();\n      disk = disk.plus(defaultInstanceResources.getDisk().multiply(parallelismChange));\n      if (ramMap.containsKey(component)) {\n        ram = ram.plus(ramMap.get(component).multiply(parallelismChange));\n      } else {\n        ram = ram.plus(defaultInstanceResources.getRam().multiply(parallelismChange));\n      }\n    }\n    return new Resource(cpu, ram, disk);\n  }. it does they are located here, the location depends on you heron installation.\nls -la $(brew --prefix heron)/libexec/examples/. What is jobs 25 parameter doing , is it controlling parallelism. The goal would be do that eventually but not part of this commit.. What kind of comments specifically it's not my code but from google themselves.. done. Will have to parameterize it to make it shareable , will do that.. Will change that and take a look at the existing pom scripts.. why are we switching to cmake ?. You are deleting this function. fixed. done. done. done. ",
    "geoffret": "Just to add a little more detail, when the topology is launched and activated, polling the topology info through the Heron Tracker interface results in:\n```bash\nBefore update\n$ curl \"localhost:8888/topologies/info?cluster=local&environ=default&topology=ExclamationTopology\"  | python -m json.tool | grep status\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  5206  100  5206    0     0   521k      0 --:--:-- --:--:-- --:--:-- 5083k\n            \"status\": \"Running\",\n    \"status\": \"success\",\nAfter update\n$ curl \"localhost:8888/topologies/info?cluster=local&environ=default&topology=ExclamationTopology\"  | python -m json.tool | grep status\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  4439  100  4439    0     0  1039k      0 --:--:-- --:--:-- --:--:-- 4334k\n            \"status\": \"Paused\",\n```\nI'm not 100% on this, but this makes me think that the state is correct at the TMaster, but whatever is getting the TopologyAPI.Topology is the culprit. \nFurthermore, if I start the topology without the --deploy-deactivated flag, then the scaling is able to work as expected while the topology is activated. However, if the topology was launched in this manner, then it is unable to update if the topology is updated. It's as if the state from the UpdateTopologyManager point of view is stuck at the initial state. \nPerhaps whatever gets the SchedulerStateManagerAdaptor is not getting updated/functioning as expected? I have to leave this for now due to time constraints for other projects but hope that this at least helps point out a good direction. . Just one last note, as far as I can tell, I also observe the above symptoms on an Aurora cluster with Zookeeper state manager. . Just to follow up with this, I pulled the changes from the above pull request and it seems to work on my end. I'll use this fix for now. Thanks! . Okay, sounds good! \nI had to made changes to the UpdateTopologyManagerTest to allow everything to build since the method signature changed, but didn't change any logic there since I'm not familiar with Mockito.. ",
    "bismoy2013": "@objmagic I think this issue can be marked as closed , I can see you've merged it already #1995 .. ",
    "yohan233": "@billonahill just accepted. Seems to have failed due to not being able to reach maven.. is there a way to retry?\n. ",
    "ithjz": "Has the problem been solved?. ",
    "jerrypeng": "My thoughts:\nFor every config we need to have single source of truth of what type the config is so as to be consistent across the whole project.  I also don't think we should manually convert the configs to strings.  If we need to do so across languages for compatibility, we can always create a layer of converting certain configs to strings and converting them back to the correct types where appropriate. Perhaps we can use annotations like in Storm to specify what types configs are and also validate config types accordingly e.g.\n@isString\nPublic static String SOME.HERON.CONFIG. Correct I am talking about API.Config.  For C++ and python we could have similar approach ( though not sure what at this moment). We just need to be clear what is the type of the config that is being passed around in the code so all developers are on the same page. @srkukarni Thanks for the review.  I have addressed your comment. @srkukarni I have address your comments. @srkukarni thanks for the review.  I think I have addressed your last round of coments. @srkukarni and @kramasamy can you guys take a look?  Thanks!. @kramasamy thanks for the review! I think I have addressed your comments. @maosongfu Sorry my initial explanation of this bug might not be correct.  Let me first talk about what I have observed.  I was testing the windowing function that was recently merged into master.  I have a topology that has one instance of a spout and one instance of a bolt.  The spout emits a tuple every second.  However, for the first several tuples the bolt would receive them all at once and not in the time interval that the spout has sent them.  And this only happens at the beginning of a topology. \nI think I have found the reason.  The doWait function for the NIOLooper:\nhttps://github.com/twitter/heron/blob/master/heron/common/src/java/com/twitter/heron/common/basics/NIOLooper.java#L53\nis blocking for around 4 seconds before it timeout causing readStreamMessageIfNeeded:\nhttps://github.com/twitter/heron/blob/master/heron/instance/src/java/com/twitter/heron/network/StreamManagerClient.java#L104\nnot to be called for that duration thus tuples are not read during that time.  Not sure why the selector there is blocking until it times out.  \nThough the code in this patch still fixes the issue or at least side steps it.\nStill need to investigate why the selector is blocking for that long. @maosongfu Thanks for looking into this.  Your patch does fix the issue I was seeing.  Your solution is definitely more elegant so I am +1 for this patch. I will also close the PR I submitted to solve this issue. @srkukarni @kramasamy Can you look.  Thanks!. Test failed due to being too long.  Though all the java tests passed. @maosongfu thank you!\n. I am mostly +1 on this PR.  The only concern I have is the flexibility for users to define a \"init\" function that gets called once before an operator starts processing tuples and a \"close\" or \"cleanup\" function that gets called when the topology is terminated.  I know flink has this functionality at least for maps and flapmaps and I found it useful when implementing the yahoo streaming benchmarks.  Perhaps we should at least have an \"init\" function that allows users to initialize certain things e.g. external clients or connections prior to actually processing any tuples.  Also state for operators like map, flapmap, filter, etc.  Users might also want to use state with those operations. @srkukarni and @kramasamy can you take a look. test failed seems to be unrelated, can someone perhaps kick the tests again\n. @kramasamy the tests have been fixed and travis passed. some unrelated tests failed. Can someone kick travis again?. Weird travis error:\nERROR: (09-11 21:16:36.307) /home/travis/build/twitter/heron/third_party/java/BUILD:200:1: no such package '@google_http_client//jar': Failed to fetch Maven dependency: Could not transfer artifact com.google.http-client:google-http-client:jar:1.22.0 from/to google_http_client (http://central.maven.org/maven2/): Connection reset and referenced by '//third_party/java:google-api-services-storage'.. @huijunw thanks for the reviewing my PR!  I have added back to simulator code in the examples that originally contain it. @nlu90 yes that will remain the same.  In heron-client.tar.gz, heron examples will be at:\n./examples/heron-examples.jar. +1 @srkukarni thanks for the fix!. @srkukarni . @aahmed-se really? how should we build heron images?. @srkukarni @maosongfu @nlu90 Can you review this?  Thanks!. @nlu90 we could definitely discuss about making acking thread safe, but i think that might be part of a larger discussion and research as it may be performance impacting.  That discussion may involve architectural changes to heron and could be time consuming.  While I am definitely willing to take part in that discussion, this PR addresses the bug immediately.. @nlu90 thanks for taking the time to review my PR!  I have addressed your comments.. @srkukarni thanks for your review.  I have addressed your comments. just squashed commits into one.  Are we ready to merge?. @kramasamy @srkukarni please review. @srkukarni @nlu90 @kramasamy please review this PR.  Thanks!\n. @maosongfu thanks for your review.  I have addressed your comments. @srkukarni thanks for the review! I have addressed your comments. @srkukarni can you review. @maosongfu I added TOPOLOGY_ENVIRONMENT to make heron more compatible with Storm since Storm has this feature. \nWe can use TOPOLOGY_WORKER_CHILDOPTS to set these environment properties but it might not be an elegant way of doing it.  We would essentially have to put all the user defined environment properties on the java command line that starts the heron instance in a format:  -Denv.variable=foo .  If the user has many environment properties, this could cause the java command line to start the heron instance to be very large.  I don't know if that is a good idea. Doing a System.setProperty in the start of the heron instance seems like a more eloquent approach. @maosongfu sounds good!  I have added the description.  Thanks for reviewing!. @cckellogg can you take a look?. @srkukarni @maosongfu can you take a look at this. Thanks!. @objmagic I have not, but this change should not break it.  Feel free to work on changing Python Heron instance to accept arguments in this fashion if you want to.. @srkukarni can you look at this. @srkukarni @nwangtw thanks for the review.  I have addressed your comments. We have the mapToKv operator for Streamlet -> KVStreamlet but what about vice versa?  . @srkukarni does it make sense for Streamlet.reduceByWindow to return a KVStreamlet ?\nShouldn't it return another Streamlet? Something like Streamlet> ?. @srkukarni I am also noticing that the API might not be sufficient for processing windows in a more generic fashion.  What if I wanted to more generic processing of windows other than just a reduceByWindow?  For example the sliding average?  Perhaps we need the concept of a WindowedStreamlet?  So there is another set of operators that can operate on WindowedStreamlets?  Operators that consume windows of data instead of single tuples.. @srkukarni in regards an operator to transform KVStreamlet -> Streamlet.  A user might want to do a reduceByWindow instead of a reduceByWindowAndKey.  I think we should allow users to be able to convert if they wanted to.. @srkukarni everything else LGTM. @srkukarni imo we should add a KVStreamlet -> Streamlet operator.  The reduceByWindow is just one example, I can also image a user would want convert KVStreamlet ->Streamlet and then print then log result or use other operators that operates on Streamlets and not KVStreamlets. @srkukarni I think the existing:\n@Override\n   public KVStreamlet, V>\n       reduceByKeyAndWindow(WindowConfig windowCfg, SerializableBinaryOperator reduceFn) \nshould be called by the more generic version with the identity parameter.  The identity parameter can just be passed as null or something.  This way you don't have two implementations of the same thing essentially. @srkukarni it would be nice if you can also write some unit tests to test out the logic of the ReduceByKeyAndWindowOperator and ReduceByWindowOperator. @srkukarni . @maosongfu i'm looking into it. @maosongfu I have submitted a PR for this issue:\nhttps://github.com/twitter/heron/pull/2511. @maosongfu @srkukarni . @ashvina I saw the OOME immediately after starting the AckingTopology.  Perhaps, we can just increase the resource request for the AckingTopology and leave the rest the same as before.  Would you prefer that?. @cckellogg @srkukarni Can you take a look. @ashvina The reason for this PR is to move away of passing in ports for executors like positional arguments which make it very difficult to read and hard to pass in optional ports.  Also in a lot of places in the various schedulers, there exists a list of port names and then another list of ports.  Why not just have one object that is a map that has the info for both and gives you the flexibility of having optional arguments.  I don't think this solution is that complex.  Can you share your thoughts on why this solution is too complex?. @billonahill @ashvina thanks for the code review and suggestions.  I have refactored the code.  Can you guys have another look.  Thanks!. @ashvina thanks for your review.  I have addressed your comments. @cckellogg . @nwangtw to address your questions\nWhat are the typical use cases of this timer event? Is it for users or for internal cases?\nMentioned in the description of the PR, the use cases are:\n1 . Users don't have to use additional threads in their topology to use a timers.\n2. Collectors are not thread safe so use other threads to emit tuples will cause race conditions. Using timer events part of the slave event loop with prevent that since its the same thread.\n3. Tick tuples are cumbersome to use to trigger events and you can only have tick tuples emitted at one frequency.\n4. Can also use in spout while tick tuples are only supported for bolts\nThis could be used by the user writing the topology or by a bolt we implement for the user such as the windowed bolt.\nTimer event is for topology? Will it be potentially in Spout/Bolt level?\nThis could be used in a spout or bolt\nIs it possible that we may need an per-timer on/off feature in future?\nI don't see this as a valid use case\nIs there a limit of how many timers can we have and how long/short can it be? Suggest to check the duration to be at least positive.\nHow many timers or the duration of timers should be up to the user.  The user should already understand the obvious performance consequences if there are too many timers or the timer frequency is too often.  We can check that the duration is positive though.  I will add that.\ntickTupleFreqMs becomes hidden. It might be better to be more explicit (like sharing TimerEvent code but having a standalone PrepareTickTupleTimer function).\nWe can, but to be honest, this feature more or less deprecates tick tuples. Tick tuples are not very easy to use for triggering timer events.  This feature makes it much easier and more natural.\nI will move initializing tick tuples to its own function\nI am not familiar with this looper object and . Is it possible that tmerevent can cause\nNot sure what your question is\nneeds unit tests.\nWe can add some unit tests\n. @nwangtw thanks for the review.  I have addressed your comments. @nwangtw I have addressed your comments. @cckellogg can you look at this. @cckellogg thanks for the review.  I have addressed your comments. @srkukarni . @srkukarni @maosongfu @nwangtw . @srkukarni @nwangtw @cckellogg thanks for your reviews!  I have addressed your comments.. @nwangtw thanks for the additional review!  I have addressed your comments. @dadgar heron can submit to an existing nomad scheduler, which is this PR focuses.  This PR allows users to submit heron jobs to existing nomad clusters.  In another PR, after this one was merged, we introduced a standalone cluster/distributed mode for heron using Nomad as the scheduler.  Before this mode was introduced, users would have to had setup Aurora, YARN, Kubernetes, etc. to run a Heron job using many nodes if they didn't already have a cluster of such available to them.  This was a high barrier for entry to users since many of these schedulers are not easy to setup, if not impossible, if you do not have sudo access.  Since Nomad is so simple to install and start up, we decided to use Nomad as the de facto scheduler/cluster manager for Heron standalone cluster mode.  The code to for setting up a standalone cluster is mainly scripts to start a nomad cluster and copying and templating files.  Like I said before, if a user already has Nomad cluster, they can absolutely use their existing deployment to run Heron jobs.  @dadgar let me know if you have any other questions.. @dadgar here is some instructions on how to start a standalone cluster via nomad:\nhttps://twitter.github.io/heron/docs/operators/deployment/schedulers/standalone/\nWe still have to write some docs on how to deploy to existing nomad cluster. @dadgar yup!\n@lucperkins if you have some time let's write some docs on how to deploy to an existing nomad cluster and also a blog to announce nomad support and standalone cluster mode.. +1. @cckellogg thanks for the review! I have addressed your comments. @nwangtw thanks for the review!  I have addressed your comments. +1\n. @cckellogg ya it will since the FileResource should resolve ${HOME}/.herondata/repository/topologies/ to be ~/.herondata/repository/topologies/ which I believe is what is resolved for local cluster.  How was the all working before?  What changed to cause the break?. nvm I think this is ok +1. @nwangtw do you need to add STATEMGR_YAML to:\nhttps://github.com/twitter/heron/blob/master/heron/schedulers/src/java/com/twitter/heron/scheduler/aurora/AuroraScheduler.java#L211\nas well?. +1. @dancollins34 currently, there is not way to specify a custom trigger or eviction policy, however we can easily add that it to the API as few changes will be needed.  Allowing users to specify custom triggering and eviction policies is definitely a good feature to have! Feel free to add the change yourself  and make a PR. You can also create a an issue and myself or another heron developer can pick work on it when we have time.  Most of the code for the windowing is located here:\nhttps://github.com/twitter/heron/blob/master/heron/api/src/java/com/twitter/heron/api/bolt/WindowedBoltExecutor.java\nI will also look at the code you have posted for your custom eviction and trigger polices and get back to you with me comments. Generally looks good, but I would like to bring a of point for discussion:\nEven when using custom triggers and evictors, should users still be be required to use the existing API (e.g.  setTopologyBoltsWindowLengthDurationMs(long value) in WindowingConfigs) to set window lengths and sliding intervals?  or should such parameters be passed directly into the custom trigger and evictors objects e.g. passing these parameter via their respective constructors?  I can see pros and cons in each approach. @dancollins34 thanks for your work!  Generally looks good.  I just have couple of comments for you.. @dancollins34 thanks for replying to my comments.  I had a question regarding the example you provided above for a custom trigger.  The TriggerHandler is never set but you call the handler in the method track: \"this.handler.onTrigger();\" How is that going to work? or am I missing something?. @dancollins34 thanks for the clarification.  I am +1 on this PR. @kramasamy we can once the last comment is addressed. +1\n@kramasamy we can merge. LGTM +1\n@lucperkins Thanks for fixing these things!. @srkukarni I don't think so. @nwangtw To address your concerns:\n\n\nYes there are use cases when users will emit and ack tuples from different threads.  For example, there are use cases when a user will put tuples on a queue and ack them later.\n\n\nWe do need this synchronization for spouts as well.  A user can very well want to call emit from different threads.\n\n\nI am not sure I understand you question.  Are you saying that because users can emit from a different thread, that it might overflow the outQueue?  The current check doesn't prevent a user from overflowing the queue to begin with right?  Even if I am not emitting from a different thread, I can still emit as many times as I want which can potentially overflow the outqueue.. some numbers:\n\n\nAcking Topology\n| Emit Count (10min) | Latency (ms)\n-- | -- | --\nVanilla | 112342934 | 7001.68\nWith Change | 104313910 | 6328.94\n. For ExclamationTopology:\n| Emit Count (10min) | 10min avg tuples/sec\n-- | -- | --\nVanilla | 587,239,062 | 978,732\nWith Change | 642,861,133 | 1,071,435\n. @nwangtw @maosongfu I have refactored the code to include synchronization with saving the state.  Can you please take a look. Thanks!. @nlu90 just to be clear, with out this patch its not even possible to guarantee at-most once.  An NPE will very likely to be thrown if you emit, ack, or fail from different threads or even if you emit, ack, or fail from the same thread but not the bolt/spout instance thread you will run into trouble. @bornej here is an example of a bolt that acks and emits from a different thread\n```\npublic static class ExclamationBolt extends BaseRichBolt {\n    private static final long serialVersionUID = -2267338658317778214L;\n    private OutputCollector collector;\n    private long nItems;\n    private long startTime;\n    private LinkedBlockingQueue queue = new LinkedBlockingQueue<>();\n@Override\n@SuppressWarnings(\"rawtypes\")\npublic void prepare(Map conf, TopologyContext context, OutputCollector acollector) {\n  collector = acollector;\n  nItems = 0;\n  startTime = System.currentTimeMillis();\n\n  Thread thread1 = new Thread(new Runnable() {\n    private long pItems;\n\n    @Override\n    public void run() {\n      System.out.println(\"start thread 1 ....\");\n      while (true) {\n        Tuple tuple = null;\n        try {\n          tuple = queue.take();\n        } catch (InterruptedException e) {\n          e.printStackTrace();\n        }\n        collector.ack(tuple);\n\n        ++pItems;\n\n        if (pItems % 10000 == 0) {\n          long latency = System.currentTimeMillis() - startTime;\n          System.out.println(\"Bolt - 1 processed \" + pItems + \" tuples in \" + latency + \" ms\");\n        }\n      }\n    }\n  });\n\n  thread1.setName(\"thread-1\");\n  thread1.start();\n}\n\n@Override\npublic void execute(Tuple tuple) {\n  try {\n    queue.put(tuple);\n  } catch (InterruptedException e) {\n    e.printStackTrace();\n  }\n\n  ++nItems;\n  if (nItems % 10000 == 0) {\n    long latency = System.currentTimeMillis() - startTime;\n    System.out.println(\"Bolt - 0 processed \" + nItems + \" tuples in \" + latency + \" ms\");\n  }\n}\n\n}\n```. @maosongfu can you approve? or do you have anymore concerns?. @nwangtw I agree that effectively-once semantics might not be honored if users decide process tuples asynchronously, but it is possible if the user uses the presave function as a trigger to create synchrony among user threads.  I think it is matter of education and documentation to educate users on this, but I think this PR is good go.. perhaps instead of a parameter we can have a more builder style API.  Something like\n.timeWindow(Time.Duration(5, Time.sec)) //tumbling window\n.sum(); . @erenavsarogullari that would be great thanks!. @erenavsarogullari fyi you can also use heron's API server as a file server for the HttpUploader to upload topology package/jars to.  If you can briefly mention that in the docs.  Thanks!\nYou can run the API Server as such:\n${HOME}/.heron/bin/heron-apiserver \\\n--cluster standalone \\\n--base-template standalone \\\n-D heron.statemgr.connection.string= \\\n-D heron.nomad.scheduler.uri= \\\n-D heron.class.uploader=com.twitter.heron.uploader.http.HttpUploader \\\n--verbose \n. @kramasamy you can run the api server by itself and not necessarily in nomad.  If a user already has a nomad cluster, we can assume they will use some sort of service discovery mechanism (Consul) to  find api server anyways. @kramasamy I see that can be a problem. You can also pin the api server to run only on a set of nodes or not run in Nomad as well.  Not sure what else we can do about that.  The user also don't have to use the API server for distributing core.  They can also just upload to S3 and expose a HTTP link. @kramasamy thanks for the review.  Do you have any more concerns?. @aahmed-se . @fred521 Yup I experimented with using StringUtils.isNotBlank() but it will return true if the input is null for some reason even though the javadoc of it states it should return false.  Didn't want to spend too much time investigating why for such a trivial thing. @erenavsarogullari thanks for doing this. Generally looks good!  Can you please also mention that the http server the topology package/jars are uploaded to need to return a URI upon upload so that heron will know where to download it in the future.  Example in the API server:\nhttps://github.com/twitter/heron/blob/master/heron/tools/apiserver/src/java/com/twitter/heron/apiserver/resources/FileResource.java#L75. @maosongfu @nwangtw This exception doesn't arise from any concurrency situations.  Like you said there is only one thread. The problem is just one task in list to run actually clears the list.  So in the previous for loop when we try to get the next task to run e.g. task i + 1 there will be an index out of bounds exception thrown since their is nothing in the list anymore.  When iterating a CopyOnWriteArrayList, a separate copy is made for the read and thus the clearing of the list will not effect us. @maosongfu @nwangtw I have made the fix simpler and with fewer changes.  Can you please take another look?  I also address @nwangtw concern about no executing tasks after clear happens. @nwangtw I have pushed another commit that adds a locking mechanism for adding new tasks and timers to the wakeable loop. During the state restore, we can lock the looper so that no new tasks can be added and after the state restore is completed we can unlock.  What do you think?. @nwangtw @maosongfu I looked through code more thoroughly to find who is add adding or removing tasks from wakeable loop for the slave thread. Only methods that handle state operation i.e. com.twitter.heron.instance.Slave.handleControlMessage will potentially add tasks as a task exisiting in the queue. There is no concurrency problems since only tasks are add upon initializing the slave thread or by tasks run by the slave looper so everything is in sync.  Thus, we don't really need locks per se but just make sure we handle the situation where tasks add other tasks and tasks clearing the list correctly . @nwangtw I am not sure about changing the Runnable to a custom interface to support the return of a boolean to determine whether we should clear to list.  Seems like a heavy change since a lot things use the wakeable not just the slave thread. @nwangtw I add a boolean that gets set when tasks are cleared to break out of the loop.  What do think about that mechanism?. @maosongfu @nwangtw can you look at this PR again? thanks!\n. @nwangtw thanks for reviewing the PR again.  I have addressed your comments.  Do you have any further concerns?. @maosongfu @nwangtw I have added the comments requested.  Thanks for the review!. +1. LGTM thanks @lucperkins . @nwangtw thanks for looking into this for me!  My only question is why are we replacing \"=\" with something else to begin with.  Its already base64 encoded and in quotes.. LGTM just some minor comments. @nwangtw @maosongfu Is this PR good to go?\n. @nwangtw in the future are we planning on having the non-default versions of all the variables?  If that is the case I am not sure that is a good idea.  We will have something like TOPOLOGY_COMPONENT_DEFAULT_CPU as well as TOPOLOGY_COMPONENT_CPU.  Might be an overkill.  Can we just use one variable e.g. TOPOLOGY_COMPONENT_CPU and just set a default for it and it will be the default value for all components in a topology for that resource.  If the user want to overwrite they can do it on a per component basis.  This is how it was done in Storm. \nAlso in Storm we used file i.e. default.yaml to hold all the default values for variables.  On packaging storm, the defaults.yaml will be placed inside storm-core.jar. The defaults.yaml file will be read on startup for default values.  This make it easier to find and change defaults compared to hardcoding default values in code. . @nwangtw yes components can have different resource requirements, I am just suggesting we use one variable for each resource and just pre set a default value for that variable and allow the user to overwrite it in their topology main if they want.\nfor example \nTOPOLOGY_COMPONENT_CPU is set to default of 10 the \"word\" component\nif the user doesn't overwrite it like:\nSpoutDeclarer spout = builder.setSpout(\"word\", new TestWordSpout(), 10).setCPULoad(20);\nthan the \"word\" component will has a CPU req of 10\n. On a side note, I think the API for resource specification will be more user friendly if users can specify resource requirements directly on the Spout/bolt object like:\nSpoutDeclarer spout = builder.setSpout(\"word\", new TestWordSpout(), 10).setCPULoad(20);\ninstead of separately declaring it:\nconf.setComponentRam(\"word\", 20);\nPerhaps we can have both options. @kramasamy . The configs don\u2019t make sense for storm though\nOn Sat, Mar 31, 2018 at 3:07 PM Karthik Ramasamy notifications@github.com\nwrote:\n\nOk. Can we have the two configs at heron also available in storm namespace\n- users in storm namespace should not use heron namespace.\nImplementing a stateful interface is fine.\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub\nhttps://github.com/apache/incubator-heron/pull/2832#issuecomment-377726795,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ADcir_1wH0rnHB6-QxeT9GpGH2ENYscTks5tj_4NgaJpZM4TCoIB\n.\n. @kramasamy this is kinda of a hack to begin with.  Storm doesn't have exactly once state.  If users want exactly once state and eloquence of the API they should use the Heron API to begin with.  This just unblocks users that want to use the heron state api with the storm api. @cckellogg @kramasamy can you guys take a look? Thanks!. @kramasamy @cckellogg . @kramasamy . So the arguments are changed for the downloader right?  Won't this break existing usages of the downloader.  Heron standalone/nomad calls the downloader to download topology packages.  Can we also support calling the downloader with the existing parameters as well? . I feel the changes have made the downloader a lot more complicated.  Now we have to potentially pass in a cluster mode, heron home path, config path because we have to read the downloader.yaml from the conf directory.  However, I think the most common use case for the downloader is \n\ndownloader <topology-package-uri> <extract-destination>. @huijunwu my concern is that it will break existing code that assume the format for the downloader is \ndownloader <topology-package-uri> <extract-destination>\nI am completely fine with making the downloader extensible but we should maintain backwards compatibility for now and until we get a chance to change the existing code that uses the new downloader format. @lucperkins @kramasamy please review. @mjschmidt so one of the design choices we made for the streamlet API was to move away from the concept of spouts/bolts towards the concepts of sources and sinks to better support state and effectively once processing provided chandy/lamport style state snapshotting.  Acking was more natural spout and bolt APIs where managed state by heron isn't available while not so much for the functional style APIs used in Streamlets Acking also doesn't gel well with the state implementation since state snapshotting is provided by chandy/lamport global snapshotting. Perhaps @srkukarni can also chime in.. @mjschmidt so just to be clear.  The streamlet API currently does not have a way for users to explicitly fail a tuple but it will automatically ack tuples for you but there is no call back to ack at the source.  If you want, I am open to the idea of having a source that have ack and fail callbacks. @nwangtw the change looks fine to me. However, I don't quite get the use case?  Why is change needed for the streamlets.  Can you elaborate some more? Thanks. @nwangtw thanks for the explanation. I not sure there will be such a use case.  The concept of of these types of grouping is really for the spout/bolt APIs.  The grouping for a DSL is really coupled with type of the operator.  At a DSL level operators like something \"keyBy\" is really the only operators that are needed.  I am also not aware of any other DSLs that allow users to have custom groupings.  I think that breaks the DSL abstraction.  \nI also don't seem any harm with merging this PR.  We can still merge it and see in the future.. @nwangtw a KeyBy operator will always be a fieldgrouping. +1. @nwangtw was there an issue that was fixed by adding the try catches? . @nwangtw can you please provide an example topology that uses the custom operator?  Also, isn't the point of the custom operator to provide some sort of compatibility with the \"Storm\" API?  Instead of forcing users to copy their code to the corresponding \"prepare\" and \"execute\" methods in the custom operator, couldn't we just do something like CustomOperator(Bolt)?  So that the user that just use their existing spout or bolt implementation? and the CustomOperator is just a wrapper for bolts which allows them to be used in streamlet topologies.. @nwangtw If that is the case. I don't think that is appropriate then.  Streamlet aims to break away from the spout and bolt API.  With this change we are kind of bring the streamlet API back to the bolt and spout API. . @srkukarni I understand your concern, but there are already places in the Heron Java API that use proto buf objects e.g. TopologyContext:\nMap getThisSources(). will remove.  This had to do with state implementation along side windowing, but we will not pulling in that piece yet. will add. sure. will fix. @kramasamy I discussed with @srkukarni and he recommended we just use some hard coded values since these values for the queue are going to be transient anyways. The values used are default values chosen from the heron_internals.yaml file. @kramasamy sure I can add some comments. can we use the Class Count for the parameter instead of an int?\n. can we change this line to something like:\n.reduceByKeyAndWindow(TumblingWindow.of(Count.of(10)), (x, y) -> x + y). Can we also have a windowing operator e.g.\nsource\n.window(TumblingWindows.of(Duration.ofSeconds(2)))\n.flatMap(s -> Arrays.asList(s.split(\" \"))). Can we reduce \nStreamlet source = Streamlet.createStreamlet(() -> \"Mary had a little lamb\");\nbuilder.addSource(source);\nto something like:\nStreamlet source = builder.newStreamlet(() -> \"Mary had a little lamb\")\nso we can just:\nbuilder.newStreamlet(() -> \"Mary had a little lamb\")\n.flatMap((word) -> Arrays.asList(word.split(\"\\s+\")))\n.... Do we really need a simulator in Heron?  A LocalCluster or simulator is useful in Storm because running a single topology requires you start up a cluster on a single node (Nimbus, zookeeper, Supervisor, etc.) which can be a lot of work.  This is not the case in Heron.  It is super easy to run a topology in local mode.  Thus the usefulness for a simulator is diminished. . gotcha, but is it easier to debug if everything is just threads in a single process? A jstack might be confusing as there are many threads executing at the same time.  People often say it was hard to debug storm workers because there are many instances/threads of different components running in each worker process. but perhaps this discussion about the usefulness of the simulator is not in the scope of this pull request.  I can just add back the code for the simulator and we can discuss the usefulness of having a simulator in heron somewhere else. sure. I intentionally left this comment in here so it is easier to debug the integration tests.  Tuples received is already printed but tuples emitted was missing. How will a user implement a custom source?  The current builder.newStreamlet on allows a supplier function to be passed in, but what if the user wants more functionality such as use the \"open\" function to initialize some parameters?. Similar comment for operators?  Users might won't to use a \"prepare\" method to initialize things only once in the beginining. don't think these comments are relevant anymore. \"word\" should probably be renamed to \"sentence\". @nlu90 not all types of windows need the timer.  Only processing time based windows that use the TimerTriggerPolicy need a timer thus tick tuples other window types do not need it. \n Its not eloquent to determine what kind of window it is on the client side. The existing python windowing implementation also sets the tick tuple frequency in the initialize/prepare method. . The point of reset is to clear some internal state of the trigger.  Please look at the CountTriggerPolicy.  The TimeTriggerPolicy does not have any internally held state to clear since its just a timer. +1 thanks for pointing that out. but that might break backwards compatibility right?. What does that really buy us though?. Ya we can change it to that. Lets keep this same as it is for Storm. will fix. +1. you are right, it redundant. I can remove. @maosongfu The change is need because we need a way to store buffered tuples in windows that haven't been triggered yet.  So that when a checkpoint a happens, the state of all untriggered windows, which is essentially just buffered tuples, will be persisted. This is to maintain the effectively once guarantee which windowing components.. @maosongfu the hashmap state is partitioned into USER_STATE and WINDOWING_INTERNAL_STATE and the user will only get state.get(USER_STATE). Thus even if the user uses the same key it won't matter since its part of a different namespace as you might think of it.. We could have another class e.g. WindowState that contain these objects but what are the benefits of that?  It just seems like a wrapper class that will only be used in the WindowManager. IWindowedBolt cannot extend IBolt since the execute API for IBolt is execute(Tuple tuple) but for windowed bolts in needs to be execute(TupleWindow window). I have added a rule to shade this dep. com.twitter.heron.common.basics.ByteAmount is from there. Can you update the java doc to reflect the change in method signature?. Can you update the java doc to reflect the change in method signature?. shouldn't a null check be conducted here? if identity is null, it should be set to to the first tuple or something. shouldn't a null check be conducted here? if identity is null, it should be set to to the first tuple or something. This is done to be generic so that some options in the future like ports for remote debugging of the jvm can be a list of integers as well:\nhttps://github.com/twitter/heron/pull/2484. Currently it contains all the defined port types, but this is not going to be the case in the future.  For example, the ports for remote debugging as part of https://github.com/twitter/heron/pull/2484 is optional and with not be part of the required.  And currently we pass in all the ports that any possible executor will need which is also unnecessary.  For instance only container-0 need the ports related to the tmaster.  That cleanup can be done in a follow up PR.. That is what the existing implementation implies.  SysUtils.getFreePort will return a -1 if it cannot find a free port.  We can do the check here or when we call SysUtils.getFreePort in the respective schedulers.  I am ok with either way. Sorry i don't quite follow how I can use an iterator or lambda here.  I am iterating portsToUse and ExecutorPortNames.values() at the same time.  . I think its more nature to override toString.  If I was to print ExecutorPortNames, I can just do \nSystem.out.println(ExecutorPortNames.MASTER_PORT) instead of something like System.out.println(ExecutorPortNames.MASTER_PORT.getName()). We can, but I am not sure it is the place to check for something like that since currently it will work fine even if there are additional ports but not if there are not enough ports.. This method is just used for getting ports that are not required thus I don't throw an exception if I don't find the port in the map unlike getRequired. This method is just so that in the scheduler code I don't need to:\nExecutorPorts executorPorts = new ExecutorPorts();\nexecutorPorts.setRequiredPorts(portsToUse);\ninstead I can just :\nExecutorPorts.withRequiredPorts(portsToUse)\n. its simpler because if I didn't I would have to have a class variable that is a map which will hold the contents of the portname to port mappings.  I would also need to write wrappers such as \"put\" to expose the those methods to users or make the variable public.  It the same reason why heron Config extends HashMap as well. remote debugging ports will only be need if the user sets a flag in there topology.  This will be an optional port .  The scheduler will decide how to get these ports and pass them to SchedulerUtils.  . This is because ports are pass in as positional arguments.  There are 9 ports prior to the debugging ports which could be more than one port.  \nThis PR should address the positional arguments issue:\nhttps://github.com/twitter/heron/pull/2516. @nwangtw \nYour first question:\nThe reason why we pass multiple debugger ports is because the heron_executor.py can potentially starts up multiple heron instances based on the physical plan thus we need multiple debugger port assignments to be passed to it.\nSecond question:\nThat is essentially what #2516 does.  Puts all ports in a map\n. nope I will change it back. I will change the name. sure\n. a user can attach a remote debugger to the port. currently remote debugging is only implemented for localscheduler and kubernetes scheduler.  . I am not sure this is a good idea.  For some of the schedulers, e.g. Marathon Scheduler, the ports are actually strings, e.g. $PORT1.  Thus, its probably simpler if we just keep everything as a String.. In the current implementation, a exception will be thrown if the scheduler doesn't set all of the required ports.  The method \"executorCommandArgs\" calls the \"getPort\" method in \"ExecutorPort\" to get all the ports.  If the port is required and missing a runtime exception will be thrown:\nhttps://github.com/twitter/heron/pull/2516/files#diff-2f8f616b79cedacafb019678d260770fR81 . will update. I will delete the executorCommand method.  It was my intention to pass the port map where the value is a string since some schedulers specify their ports as a string such as the Marathon Scheduler.. Please see my other comment concerning the current implementation will already throw an exception if a required port is missing.. If a new port required port is added here and retrieved in the \"executorCommandArgs\" method then an exception will be thrown.. I will replace getNumOfRequiredPorts with getRequiredPorts as it is a more generic method. I have added this. I add this because specifying an override via commandline for the config \"heron.statemgr.connection.string\" does not get propagated into the heron_executor.  Thus, if I want to specify the location of zookeeper via command line, I cannot.. So I added this because the heron_downloader couldn't handle URIs like: \"file:///foo/bar\".  This is the same implementation as the HttpDownloader, but I am not sure if we should point URIs with a prefix of \"file\" to the the http downloader or just have a new file downloader.  The reason to why we need to support copy of files by the heron_downloader is because there could be instances where the user is distributing JARs via some sort of network storage and we need to copy and extract the jars from the network storage location into the local heron directory structure. will add. will revert. though this is really done once on the start of the topology. We need the heron_nomad.sh script to start heron_executors in nomad.  Not sure where else would be more appropriate to put this since the script needs to be available on where you install the heron cli.  The heron.aurora script is placed in the aurora config folder.. will change back. will change back. The .sh file is actually read by the scheduler and submitted to nomad as part of a job description in json format.  I can inline the contents of the .sh script in the scheduler code, but that might not be as clear to the user as just having a file with comments in it. sure thing. 2181 is the default port number for zookeeper.  If I was to run whole setup (scheduler, topology, and zookeeper) on a single node, this is the default I would use.  The cli and all executors will be able to contact the local zookeeper on 127.0.0.1:2181. will fix. will fix. will fix. will fix. will fix. Yes if there is some other nomad job running that does not have this user defined meta data set e.g. API server. I can move the function to FileHelper but the existing copy only copies files already on disk. it is added in SchedulerUtils that every scheduler calls except aurora I believe.\nhttps://github.com/twitter/heron/pull/2601/files#diff-2f8f616b79cedacafb019678d260770fR265. will add. will add. will add. will do. We cannot do \"metaData.get(NomadConstants.NOMAD_TOPOLOGY_NAME) == topologyName)\" because first in java to compare strings you have to your .equals instead of \"==\".  \"==\" will check if its the same instance rather if it has to same value.  If you use .equals we need to make sure that its not null or NPE will be thrown.. To answer you first question:\nThe job spec can be quite long that is why I would rather print it explicitly if the user specified --verbose via the command line.\nTo answer your second question:\nThe reason why I append if so that the RuntimeException thrown only contains the error message and not the whole job spec but I would log the error message and the job spec if verbose is set. I will fix the incorrect indent. will do. There isn't currently.  Will right one in the future, but the you can always type \"heron-admin help\" and it will give you a detailed explanation of all commands and subcommads. nit.  Please remove extra line  . Place remove extra line. If a custom trigger or evictor, validate should not be called.  Arguments would be directly passed into the custom evictor and trigger when they are intialized. I think we should remove this from the interface.  Having this interface is confusing to users since WindowManager implements the TriggerHandler interface.  WindowManager is also not a interface that users can implement thus it shouldn't be part of an interface that users will have to implement. I think having \"setTriggerHandler\" should suffice.  For build in triggers (e.g. WatermarkCountTriggerPolicy, WatermarkTimeTriggerPolicy), we can just pass in the WindowManager via their constructor. . Not sure if this interface is necessary, if we remove WindowManager from the TriggerPolicy interface . Looking more closely at the code I am confused how the a custom trigger policy will work.  All the existing trigger policies require the WindowManager to actually trigger windows. How will a user implementing a custom TriggerPolicy do this.  They have no reference to the WindowManager.  Shouldn't the windowManager be passed into the custom trigger implicitly?  As of right now, trigger polices can't trigger windows without the WindowManager.  That is why I suggested removing the setWindowManager interface, since trigger policies need the window manager regardless of what kind of trigger policy it is.  . Not sure if we need these checks.  I think it should be up to the user to make sure to pass in the correct objects he or she needs for a custom trigger.  Having to pass these booleans into the constructor also makes the interface confusing to some reading to code.  If you really want these checks in place, I would suggest using setters to set these.  Then it is clear to the reader of the code what is being set.. I see. lets remove them to keep the interface as simple and clean as possible. This changes the expected behavior of the install and will break a lot of things.  For example, the nomad scheduler expected that heron core to be at:\nfile://${HERON_DIST}/heron-core.tar.gz\nyou unTARing it here and removing the archive will break that. this will Aurora too. For nomad/standalone, the user will use heron-install.sh to install on a bootstrap machine.  Then the .heron directory will be copied to all the machines in the standalone cluster as is. It will break Aurora as well:\nheron_core_release_uri = '{{CORE_PACKAGE_URI}}'\nheron_topology_jar_uri = '{{TOPOLOGY_PACKAGE_URI}}'\ncore_release_file = \"heron-core.tar.gz\"\ntopology_package_file = \"topology.tar.gz\"\nfetch_heron_system = Process(\n  name = 'fetch_heron_system',\n  cmdline = 'curl %s -o %s && tar zxf %s' % (heron_core_release_uri, core_release_file, core_release_file)\n). I don't understand how adding just a config will work?  Both in Nomad and Aurora start scripts it expects a archive and will execute \"tar zxf\" on it to untar it. We will have to change that as welll. gotcha, we can probably use the same method of creating a symbolic link in heron-nomad.sh. we are not changing it in the file.  The later config is commented an serves just as a reminder that user can do that as well. I can make that change. installing -> running. y -> by. sure. done\n. @fred521 thanks for point that out I have refactored the code. @maosongfu The code snippet of the task that clears the tasksOnWakeup list:\nhttps://github.com/twitter/heron/blob/master/heron/instance/src/java/com/twitter/heron/instance/Slave.java#L113\nHas to do with restoring state.. Though is checking tasksOnWakeup.size() <= i more defensive programming?  With this check an index out of bounds exception cannot happen while for some reason or another not all the tasks are cleared you can get into a situation where i > list size causing index out of bounds exception. This won't happen during the state restore since I just add the locks which will prevent new tasks from being add during the state restore.  And its only during the state restore that we clear the tasks. I have checked all code paths.  Ya I can confirm clear() is always called from the same thread.  I don't know how we can make nExit, executeTasksOnWakeup, clearTasksOnWakeup and clearExitTasks synchronized.  This would give rise to deadlock situations since tasks run in executeTasksOnWakeup() can in turn call clear().. I see what you are saying, but should we add them? It gives a developer the sense that these operations are thread safe now.  Should we even allow that in the architecture.  I think that is a bigger discussion. I can't think of a use case for that now.  It also might not be wise to allow developers to call these from potentially other threads since heron's instance architecture is clearly defined to be 2 thread based, gateway and slave. . I have renamed. do we need .replace(\"=\", \"\\&equals\")) here?  All the \"=\" would already be replaced by the previous replace statement right?. can we add comments on why we need to replace \"=\" for future reference because of parsing in Aurora . gotcha can we just add some comments at the places this is done for future reference.  Thanks!. or maybe a TODO for the future. why are we doing this?. how can index but less than 0?. Math.abs(hash). This can also give you negative numbers when hash is bigger than 2^31, and the best way would be to use a shift mask (key.hashCode() & 0x7fffffff) % M, where M is the table size.. no this is for API server. Thanks for adding these checks.  We need to add these checks all around in Heron.  We currently lack any sort of organized config validation. I created a config validation system using annotations in storm back in the day:\nhttps://github.com/apache/storm/blob/master/storm-client/src/jvm/org/apache/storm/Config.java\nPerhaps worthwhile to think if its worth while to have in Heron.  Though in Storm all configs are centralized which is not the case for Heron. Ya I don't know if we should mix the two APIs. Kind of breaks the abstraction.. Unless there is a strong user demand to help them transition from storm api -> streamlets. ",
    "naveenkumarmarri": "I'm looking at californium.\nCan you suggest some good implementations? My CoAP server is running on NodeJS and I want to build a CoAP listener at heron side and pass it on spout.\nCan you point me to some references for this if available?. Hi,\nI'm able to pull data from coap server by observing the URI.\nHowever I'm not able to get how to pass the URI configuration to the ISpout class.\nCan you please help me on this or share some example how it is done?. Hi Karthik,\nI'm interested in this issue,I'll work on it. Hi @kramasamy , @srkukarni ,\nthe parameters these aggregators can take is just windowing strategy right? or am I missing something?\n. ",
    "dancollins34": "@nlu90 I'd be willing to take a look at this, do you have any idea on the broad strokes of what would need to be implemented, so I'd know where to start?. Thank you very much @jerrypeng . I'll take a look, see if I can allow custom policies without making many changes, and submit a pr. And thank you for taking a look.  Happy new year!. Closing, resolved by making a pr for custom trigger policies. As an aside, the travis build appears to be failing due to style issues.  I can make those changes, but I'm unsure if I'm allowed to sign the CLA, and therefore contribute, given my current terms of employment.  I'll find that out tomorrow.. Thank you for the advice. I've removed the style issues.. I don\u2019t think there\u2019s any way you could prescribe the universe of all possible windowing parameters in that way. I think it makes a lot more sense to make the extent of the requirements to be \u201csubclass triggerPolicy/evictionPolicy\u201d instead of trying to restrict the subset of what each of those could be.\nTo your comment about validate, I think it might make sense to make this a 3 condition if statement. I.E. if eviction and trigger policy are defined in the config, use them. If neither eviction nor trigger policy are defined, run validate and run the code currently in the master branch, and if only one of them is defined, raise an exception.\nI\u2019ll fix the blank lines, make this above change and try to write some example code tonight.. So, I've noticed something else about TriggerPolicies that I had missed on my first go round.  There is no way for the API user to actually instantiate an instance of a custom TriggerPolicy to put it into the WindowConfig object because they would first need to have a reference to the WindowManager to act as a TriggerHandler.  I'm writing a commit attempting to address this now.\n  . Built on my machine that\u2019s weird. I\u2019ll fix it tonight.. Looks like it just timed out. Re running build. I see the failures now. Attempting to address them now.. It looks like this one failed on a c++ test? I\u2019m not sure how that\u2019s possible I haven\u2019t modified the c++ code in any way. . Example usage\n```java\npublic class RandomTriggerPolicy extends AbstractBaseTriggerPolicy {\n    Float triggerChance;\n    Random sampler;\npublic RandomTriggerPolicy(Float triggerChance){\n    super(false, false, false);\n\n    if(triggerChance > 1.0 || triggerChance < 0.0){\n        throw new IllegalArgumentException(\"Cannot have a likelihood outside of 0 to 1 range\");\n    }\n\n    this.triggerChance = triggerChance;\n    this.sampler = new Random();\n}\n\n@Override\npublic void track(Event<T> event) {\n    if (started && !event.isWatermark()) {\n        if (sampler.nextDouble() < triggerChance){\n            this.handler.onTrigger();\n        }\n    }\n}\n\n@Override\npublic void reset() {\n    // NOOP\n}\n\n@Override\npublic void shutdown() {\n    // NOOP\n}\n\n@Override\npublic Boolean getState() {\n    return false;\n}\n\n@Override\npublic void restoreState(Boolean state) {\n    // NOOP\n}\n\n}\npublic class AllEvictionPolicy implements EvictionPolicy {\n    private EvictionContext context;\n@Override\npublic Action evict(Event<T> event) {\n    return Action.PROCESS;\n}\n\n@Override\npublic void track(Event<T> event) {\n    // NOOP\n}\n\n@Override\npublic void setContext(EvictionContext context) {\n    this.context = context;\n}\n\n@Override\npublic EvictionContext getContext() {\n    return context;\n}\n\n@Override\npublic void reset() {\n    // NOOP\n}\n\n@Override\npublic Boolean getState() {\n    return true;\n}\n\n@Override\npublic void restoreState(Boolean state) {\n    // NOOP\n}\n\n}\npublic class Main{\n    public static void main(String[] args){\n        Builder builder = Builder.newBuilder();\n    builder.newSource(() -> 1).reduceByKeyAndWindow(\n        value -> value,\n        value -> value,\n        WindowConfig.CustomWindow(new RandomTriggerPolicy<>(.5f), new AllEvictionPolicy<>()),\n        (x, y) -> x + y\n    ).log();\n}\n\n}\n```. Will address @kramasamy comments and add to /examples following resolution of concerns raised by @jerrypeng. Yes it is- in the windowedBoltExecutor. Take a look at those changes.. @jerrypeng. Don\u2019t know who to ask to look at this. @kramasamy ? Should this be isolated or should I just include it in the bigger simulator update?. Reverted the changes and made a new pr which puts an interface under heronSubmitter and TopologySimulator\n. ",
    "athuras": "ty. ",
    "HeartSaVioR": "Some questions on this change:\n\nCould you explain how dual license works and why it is needed? I think the source files Twitter created don't need to have ASF license header, and adding Twitter copyright to the source files which are copied from Apache Storm doesn't feel right. Yes I'm not a lawyer, but I'm wondering it as a PMC member of Apache Storm.\nWhy Heron would want to keep Twitter copyright even after the project went to the ASF incubator? Moving the codebase should be followed up also.. \n",
    "erenavsarogullari": "Hi @skanjila ,\nFirstly, thanks for sharing the plan.\nI am also interested in Heron Scala API support so we can work in parallel by splitting to subtasks.\nIn addition to your list, i think the following changes as the part of roadmap:\n1- Bazel Integration for Heron-Scala\n2- 3 level Bazel BUILD files:\n/heron/examples/src/scala\n/heronheron/api/src/scala\n/heronheron/api/tests/scala\n3- Scala Traits:\nI think you have already started by converting from Java Interface.\n4- Scala Implementations:\nJava interface and Scala trait function signatures will be different. I think java specific logic (eg: SerializableFunction, SerializablePredicate..) need to be wrapped by hiding from Scala users and expecting pure scala functions so these logic can be covered in Scala Impls. These conversions can be implicit or explicit in the light of the final decision.\n5- UT Coverages with ScalaTest\n6- Scala Examples (under examples module)\n7- Documentation for end users\nThe Heron Streamlet API for Java - https://twitter.github.io/heron/docs/developers/java/streamlet-api/\n8- Load Tests\nThis can be useful to verify if any perf overhead exist or not)\nAlso, i think we don' t have Jira support to track issues so this kind of list can be documented (via proposal doc or another Scala tasks doc) and addressed step by step by Github-issues ;) . Thanks @lucperkins and @srkukarni for the review ;). Hi @kramasamy, @srkukarni and @nwangtw,\nSorry, yesterday, i was offline due to timezone diff. Probably, my patch had rebase problem by amending commit message. Many thanks for the following this and sorry again for inconvenience.. Build looks broken due to connection problem. Looks irrelevant.\nJust wondering: currently, i do not have permission to trigger travis. is it possible to get permission?. @lucperkins It will be perfect if possible. Many thanks ;). Hi @lucperkins,\nIn addition this PR, i can work on abstraction of stageNames.add(getName()) logic in the next PR if there is no concern.\nThanks in advance ;). Thanks @lucperkins. Will be addressing offered abstraction via separated PR ;). cc @lucperkins @srkukarni . Hi @lucperkins,\nAll comments have been addressed and this PR is ready to be re-reviewed.\nAlso, it can be merged after #2559.\nThanks. @srkukarni Sure, having a look ;) . cc @lucperkins @srkukarni . cc @kramasamy @skanjila @sijie @nwangtw. Review comments have been addressed with last commit ;). cc @kramasamy @skanjila @sijie @nwangtw. cc @kramasamy @jerrypeng . Hi @jerrypeng \nThanks to create this.\nI was thinking to address HttpUploader documentation so please assign to me and will address it ;). @jerrypeng Sure, Will mention this as well. Thanks. @jerrypeng This issue has been addressed via PR #2750 so can be closed. Thanks. cc @kramasamy @skanjila @sijie @nwangtw. cc @kramasamy @skanjila @sijie @nwangtw. cc @kramasamy @srkukarni @jerrypeng. @kramasamy @nwangtw Thanks again for the reviews.\n- Last comment has been addressed.\n- Merge conflict has been fixed.\nSo PR is ready for final review / merge.. cc @kramasamy @jerrypeng. Thanks @kramasamy @jerrypeng for review this.\nLast comment is addressed ;) It is ready for final review/merge.. cc @kramasamy @skanjila @sijie @nwangtw. @nwangtw Thanks for the review again. All comments have been addressed via last patch so it is ready for final review / merge ;). @nwangtw Thanks for the review again. Last comments have been addressed via last patch so it is ready for final review / merge ;). @nwangtw Firstly, thanks for the review. All comments have been addressed so it is ready for re-review.. @nwangtw Firstly, thanks for review. \nAll feedbacks have been addressed so PR is ready to re-review / merge.. @kramasamy @nwangtw Thanks for the reviews again. I think we can merge and will address with incoming PR ;). @kramasamy Thanks for following this. Yes, it is ready for review.\nAlso, Scala Streamlet Integration Test Execution Time looks 17s as follows:\n// Integration Pipeline Summary\nheron build integration_test    0:01:25\nheron install   0:04:52\nheron tests install 0:00:19\nheron integration_test local    0:04:10\nheron integration_test http-server initialization   0:00:00\nheron integration_test scala    0:00:17\nheron integration_test java 0:13:11\nheron integration_test python   0:03:25. +1 for Scala/Java Streamlet API parts via following verification steps:\n- API Files has been checked and related files are under org.apache.*\n- Local clean build passed\n- All UTs passed\n- Examples are under org.apache.*\n Also following examples have been run locally successfully by verifying their behaviours on UI as well\n```\n// Scala Streamlet\nScalaIntegerProcessingTopology\nScalaWindowedWordCountTopology\nScalaClassicalMusicTopology\nScalaRepartitionTopology\nScalaTransformsAndCloneTopology\n// Java Streamlet\nFilesystemSinkTopology\nFormattedOutputTopology\nIntegerProcessingTopology\nRepartitionTopology\n``. @sreev , Following changes needs to beorg.apache.*` as well on your branch. Also i tested these changes locally by running integration-tests.\n1- File: /heron/integration_test/src/scala/BUILD\n[Row 12] srcs = glob([\"com/twitter/heron/integration_test/**/*.scala\"]),\n[Row 20] main_class = \"com.twitter.heron.integration_test.topology.scala_streamlet_with_filter_and_transform.ScalaStreamletWithFilterAndTransform\"\n2- File: /scripts/packages/BUILD\n[Row 352] strip_prefix = '/integration_test/src/scala/com/twitter/heron/integration_test/topology/'\n3- File: scripts/run_integration_test.sh\n[Row 11] SCALA_TESTS_DIR=\"integration_test/src/scala/com/twitter/heron/integration_test/topology\"\n4- File: integration_test/src/scala/org/apache/heron/integration_test/common/ScalaIntegrationTestBase.scala\n//Please add the following lines\nimport org.apache.heron.streamlet.scala.Builder\nimport org.apache.heron.streamlet.scala.impl.BuilderImpl\n5- File: \nintegration_test/src/scala/org/apache/heron/integration_test/topology/scala_streamlet_with_filter_and_transform/ScalaStreamletWithFilterAndTransform.scala\n//Please remove the following line\nimport org.apache.heron.integration_test.common.AbstractTestTopology\n//Please add the following lines\nimport org.apache.heron.integration_test.common.{\n  AbstractTestTopology,\n  ScalaIntegrationTestBase\n}\nimport org.apache.heron.streamlet.scala.{Builder, SerializableTransformer}. @sreev You' re welcome. Travis is Green now ;). cc @kramasamy @nwangtw . Thanks @kramasamy. Not urgent, just notification ;). @kramasamy @nwangtw \nThis PR is ready to review / merge. Thanks in advance ;). Hi @Code0x58,\nFirstly, thanks to having a look this.\nIt looks like no matching toolchains found from latest travis log:\n(18:43:50) ERROR: While resolving toolchains for target //heron/api/src/scala:api-scala: no matching toolchains found for types @io_bazel_rules_scala//scala:toolchain_type\n(18:43:51) ERROR: Analysis of target '//heron/api/src/scala:api-scala' failed; build aborted: no matching toolchains found for types @io_bazel_rules_scala//scala:toolchain_type\nFrom bazel doc, we can try setting default toolchain if no concern.\n```\nWORKSPACE\nregister default scala toolchain\nload(\"@io_bazel_rules_scala//scala:toolchains.bzl\", \"scala_register_toolchains\")\nscala_register_toolchains()\nRef: https://github.com/bazelbuild/rules_scala/blob/master/README.md. Overall is ok for me. \n**IT** coverage will be useful so i will be addressing through #3106. @nwangtw @huijunwu \nThanks for reviews. It is ready to be merged.. Thanks again @nwangtw for all your effort.\nOverall looks ok for me and also, split function logic looks as common pattern with Flink.. cc @nwangtw @nlu90 . Hi Luc. I was also thinking about this changes. Thanks for it. Adding streamlet name to stagesNames logic looks missed for `ConsumerStreamlet`. Other Streamlets look ok. I think this logic can also be part of the abstracted `setDefaultNameIfNone` method after streamlet name is uniqueness verification.. I think definition of Streamlet/StagePrefixNameType as enum can be useful by supporting type-safety and all types can sit in single place.. Sure, this has just been addressed with last commit.. UT coverages have been addressed for two use cases viatestSetNameWithInvalidValuesandtestSetNumPartitionsWithInvalidValueand alsorequire``` is set as private to avoid opening to public.. Thanks @lucperkins for quick feedback. Sure, i will be removing enum changes from this PR due to addressed by other PR. \nIn addition to 3 test cases, this change (abstraction of stageNames.add(getName());) can also be part of this PR, if there is no concern ;). Firstly, thanks for review.\nSure, what about StreamletNamePrefixType by emphasizing type? In general, most of the enums have type suffix (e.g: JoinType). Addressed and ready for re-review ;). General BUILD Comment:\n*.scala files are not referenced here. Bazel supports Scala via \nhttps://github.com/bazelbuild/rules_scala \nso i think we need to use scala_library to compile scala files by referencing\ncom/twitter/heron/streamlet/scala/**/*.scala as src.. General Comment:\nCurrently, All files look living under heron/api/src/scala. \n- Related *.scala files should be under package: heron/api/src/scala/com/twitter/heron/streamlet/scala  (except BUILD file)\ne.g: Currently, Java Streamlet API lives under heron/api/src/java/com/twitter/heron/streamlet. In the light of Scala User perspective and in my opinion, All exposed Scala Traits should be pure scala. This means Scala traits should accept Scala Functions instead of FunctionalInterface here:\ndef newSource[R](supplier: () => R): Streamlet[R]\ninstead of \ndef newSource[R](supplier: SerializableSupplier[R]): Streamlet[R]. General Serializable-FunctionalInterfaces Comment\nI think all Serializable Functional Interfaces(SerializableFunction, SerializableConsumer etc...) should be hidden from Scala Users. This means Scala API should just accept pure Scala functions instead of them. In the light of this, i think we do not need to have these Serializable interfaces' s Scala versions. These can be wrapped at Scala StreamletImpl level by transforming incoming Scala Function to related SerializableFunctionalInterface. In the light of my previous comment, as a sample for map function, i think this should be as follows.\ndef map[T](mapFn: R => _ <: T): Streamlet[T]\ninstead of \ndef map[T](mapFn: SerializableFunction[R, _ <: T]): Streamlet[T]\nSame comment for the following functions.. Addressed.. Hi @nwangtw,\nThanks for working on this.\nFor long-term maintenance perspective, i think the following constants can help us when Scala version is upgraded with just small change (if makes sense)\nscala_version = \"2.11\"\nscala_point_version = scala_version +\".11\". Apache Http Client test dependency requires classifier flag with tests value. To achieve this, http_jar support has been used.. @kramasamy -  Thanks for review this. Currently, Bazel looks having an open PR to support classifier functionality. I have tried with maven-jar as well. Also, classifier is supported by Maven and Gradle. \nhttps://github.com/johnynek/bazel-deps/issues/69. Firstly, thanks for review.\nHigh level Flow is as follows:\nScala Streamlet API => [Java Streamlet API => Topology API]\nComplexSink, ConsumerSink and LogSink are internal implementation used as adapter between Java Streamlet and Topology API. Currently, User defined Java Streamlet Sink is converted to their internal implementation as follows:\nscala\ntoSink(Sink<R> sink) => ComplexSink\nconsume(SerializableConsumer<R> consumer) => ConsumerSink\nlog() => LogSink\nAbove conversion parts are already abstracted at Java tier. Scala tier just needs to convert passed User defined Scala Sink logic to its java version and pass to Java StreamletImpl' s toSink function so we will not need to have Scala impl of ComplexSink, ConsumerSink and LogSink.\nFor Unit Test Coverage: This change-set just aims to surface Scala version of Sink Trait and its UT coverage is covered with ListBufferSink. When Scala Streamlet implementation tier is developed as next step, UT coverages will already being added for each function(e.g: toSink, map, flatMap) and different Sink implementation(FileSink, MapSink etc...) can be covered as /examples.. It is required for class/implementation level because they keep the state. On the other hand, each interface can have multiple implementation and each of them should have its own @SerialVersionUID. Builder, Streamlet, Source, Sink Traits are the main interfaces for Scala API and ScalaToJavaConverter is common container for these traits.. Builder functions accept both SerializableSupplier and Source. You can use existing toSerializableSupplier for SerializableSupplier. For Source, toJavaSource function needs to be created as same as with existing toJavaSink function.. Sink exposes the following functions(setup, put and cleanup) as public and other logics which are covered internally are managed and exposed to low level Topology API by these functions. (e.g: com.twitter.heron.streamlet.impl.sinks.ComplexSink just calls its setup and put functions. This just provides wrapper.. Yes, this is Scala Companion object and object parts covers static functions. For example, createSupplierStreamlet function is static in Java StreamletImpl and used by its UTs.. After related user defined function is applied(e.g: map), it will return Java Streamlet object type and this needs to be transformed to Scala Streamletobject type again before exposing the user so in order to achieve this, toScalaStreamlet function accept Java Streamlet Interface and transforms it to Scala Streamlet Trait type.. Sure, will be addressing.. @nwangtw Firstly, thanks for the review.\nUpdated by using multi import. Just an exceptional point here: Scala API supports own Sink Trait via com.twitter.heron.streamlet.scala package so toJavaSink function needs to have Java Sink package definition as follows due to using for same named Interface & Trait in different packages:\n```\n//ScalaToJavaConverter.scala\nimport com.twitter.heron.streamlet.scala.Sink\ndef toJavaSinkT: com.twitter.heron.streamlet.Sink[T]\n``. Addressed.. @nwangtw Thanks for the review. Scala API has ownSerializabLeTransformerandSinkTraits. They need to be transformed to their Java versions before passing to Java API. This class covers this transformation functions and same named classes living in different packages can not use import keyword in same class. So, Scala versions useimportkeyword and java versions usepackage name` directly. Same comment is for others, too.\n. @nwangtw Thanks for the comment. Yep, alias make sense and addressed with new commit.. Scala Functions support contravariant(parameter type) and covariance(return type) type parameters as default as follows :\nFrom scala.Function1 source code:\ntrait Function1[-T1, +R] extends scala.AnyRef {\n this : scala.Function1[T1, R] =>\n  def apply(v1 : T1) : R\n  ...\n}. Currently, there is no limitation about the number of clones and number of partitions. I think limitation can be useful as long-term by getting team-agreement first and putting Java tier.. Scala Builder needs to pass Java Builder instance via constructor as follows. \nobject Builder {\n  def newBuilder(): Builder =\n    new BuilderImpl(com.twitter.heron.streamlet.Builder.newBuilder())\n}. Currently, Java Source is accepted and return type Java Streamlet. However, this is Scala Builder Trait so Java Source and Streamlet should not be return types so following import needs to be removed.\nimport com.twitter.heron.streamlet.{Source, Streamlet}. This is Scala Streamlet Trait and return types are itself(Scala Streamlet), not Java one so Streamlet needs to be removed.  . BuilderImpl needs to get Java Builder via constructor as follows.\nBuilderImpl(builder: com.twitter.heron.streamlet.Builder). BuilderImpl level, we need to call Java Builder functions. Currently, Java BuilderImpl creates a source list (List<StreamletImpl<?>> sources) and keeps Sources, Transformations and Sink to build topology so this is why important and for example, this function needs to be as follows:\noverride def newSource[R](supplier: () => R): Streamlet[R] = {\n    val serializableSupplier = toSerializableSupplier[R](supplier)\n    val newJavaStreamlet = builder.newSource(serializableSupplier) //Java Builder\n    StreamletImpl.toScalaStreamlet[R](newJavaStreamlet)\n  }. Yep, this function transform Java Streamlet to Scala Streamlet version so why its naming convention is toScalaStreamlet. Also, it is same convention what Scala currently has (i.e: toList, toMap etc..) . @nwangtw Thanks again for the review. Addressed.. Kryo needs explicit defined no-arg constructor for Serialization so this has been added for it. @jerrypeng Yes, exactly. Thanks for review ;). @ajorgensen Thanks for the PR.\nWhat about org.apache.commons.lang3.StringUtils.isNotBlank() by supporting not-null, not-empty, also not-whitespace (\"  \")?\nIn addition to this, adding UT Cases can be useful for negative-cases (customRegion = null / \"\" / \"  \"). final modifiers look not required for 3 vals.. Needs to be set as Spout_Streamlet_1 even if not breaking UT.. @nwangtw Thanks for working on this PR.\nIntegration Test is also useful for end2end coverage. \nCurrently, the following streamlet ITs exist: \n- Python: word_count_streamlet \n- Scala: ScalaStreamletWithFilterAndTransform and ScalaStreamletWithMapAndFlatMapAndFilterAndClone\n- Java Streamlet API does not have any IT yet.. Yes, require can be used by moving from StreamletImpl to StreamletUtils as the current patch. @nwangtw Thanks for following this.\nI have developed Scala Integration Tests. If you want, i can also address Java ITs in the light of #3101. \nAlso, please let me know to collaborate on other Streamlet Improvements ;). Thanks again @nwangtw for working on this PR.\nnull check can be useful for spout parameter due to public API\nStreamletUtils.require(spout != null, \"spout must not be null.\");. Number of partitions is already set at createSpoutStreamlet#SpoutStreamlet level so this looks redundant. \nAlso, same for following source functions:\nnewSource(SerializableSupplier<R> supplier)\nnewSource(Source<R> generator). null checks can be useful for operator and grouper parameters due to public API. @nwangtw Thanks for working on this PR.\nStreamlet API Grouping Documentation can be useful for end user as follows: \n- how they can apply, \n- grouping types(shuffle, global, fields...), \n- enabling multi groupings for single operator\nRef: https://apache.github.io/incubator-heron/docs/developers/java/topologies/. Do we plan to support other Streamlet operators for user defined grouping (as same with applyOperator)?\nAlso, an alternative way for passing grouping as function parameter, is it applicable to accept grouping at Streamlet level? (e.g: same level with setName, setNumPartitions etc). - HashMap accepts null K,V pairs so ConcurrentHashMap can be preferred to avoid null access (due to Public API)\n- On the other hand, streamId also needs to be robust for blank values(streamId = \" \"). Test coverage can be useful for the following cases:\n- if streamId is set duplicate (test_stream2 == test_stream3)\n- if streamId is set Bolt level again as different from Spout's streamId. may parentStreamId be more meaningful?. I think streamId offers to be distinguished streams, right? \nAlso, wondering is there UI impact (if user would like to see streamId)?. Exactly. Vertices show Streamlet name and Edges can show both streamId and grouping strategy. . Also wondering the case: if Bolt points invalid streamId (not matched with parentStreamId of Spout/Bolt) so could validation be required? (to cover this case either fail-fast or warning message due to redundant Bolt)?\n. Also IT coverage will be useful to verify end2end so will be addressing in the light of #3106. Sure, IT coverage is too important due to aim end2end. I think it should be part of the code review checklist for incoming PRs ;) \nOn the other hand, do you have template for split function signature? . Yep, i was having a look for Flink implementation as well. It supports child and parent Stream hierharchy to support select functions through split function.\nI agree that split function naming convention make sense and following can be as an alternative: \n```\nStreamlet numberStreamlets = ...\n// user can apply his custom split logic\nMap> splittedStreamlets = numberStreamlets.split(new SerializableSplitter())\nsplittedStreamlets.get(\"even\")\nsplittedStreamlets.get(\"odd\")\n```\nIf we have proposal document by covering the all potential use-cases, that will be useful as well ;). @nwangtw yep, i agree and submitted new patch. Could UnsupportedOperationException be more convenient? Also, same for Row: 111. Could this validation be moved to StreamletImpl where the splitFns is set as first place?\nStreamletUtils.require(!splitFns.isEmpty(), \"splitFns can not be empty\");\nStreamletUtils.require(splitFns.keySet().stream().allMatch(stream -> StringUtils.isNotBlank(stream)), \"Stream Id can not be blank\");. Wondering what is the behaviour if transformation is set without selecting streamId after split function as follows?\nsupplierStreamlet\n      .split(Map(\n        \"position\" -> { num: Double => num > 0 },\n        \"negative\" -> { num: Double => num < 0 }\n      ))\n      .setName(\"Split_Streamlet_1\")\n      .setNumPartitions(5)\n      .map() // without selecting streamId\nAlso, UT coverage can be useful for this case. This is already checked upper level. I have just created for this: #3117. ",
    "mjschmidt": "https://github.com/twitter/heron/pull/2513\nSame pull request. Use one of these. . #2513 has passed all checks, please use this pull request. Thanks.\nhttps://github.com/twitter/heron/pull/2513\n. I added the js files from the links they were being called from, added the files names and replaced the cloudera links in:\napplication.html\nconfig.html\nexception.html\ntopologies.html\ntopology.html. YAY it worked (:. Hey this is a duplication of effort. Nick and I mad the same exact changes. So use either, right now it looks like nick's build is finished and mine is still building due to a type I realized I needed to fix. So either will work. Maybe now we should merge in Nick's now.. @cckellogg . Okay, so what about the code changes made in this pull request? Anyway to get them into the current heron container?. @kramasamy. This works. I don't know what was going on with my docker. Sorry guys. Can close. Is this issue done then?. It looks like stateful bookkeeper did work when I brought up a new gcp cluster. Yes. The daemonset would get up and running, but upon trying to launch a topology the bookeepers go into a crash loop.. Just 3 nodes total. was just trying to get a quick and dirty heron cluster\nup for a conference. Unfortunately no logs.\nOn Fri, Nov 24, 2017 at 3:27 PM, Sijie Guo notifications@github.com wrote:\n\n@mjschmidt https://github.com/mjschmidt do you any logs about the crash\nloop? and how many nodes do you use? I am wondering if this is related to\nthe number of nodes you are using. because daemonset start one pod each\nnode.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/twitter/heron/issues/2586#issuecomment-346892452, or mute\nthe thread\nhttps://github.com/notifications/unsubscribe-auth/AGPeIjPx08Zat0WdurfsJadyxABweO1Cks5s5yadgaJpZM4QqFY0\n.\n. Chrome issue again -_-... yikes.. https://github.com/twitter/heron/blob/12425366a9ade8501675c1e29c2ea34cb36044cb/heron/tools/ui/resources/static/css/main.css\n\nIts this file, my contract doesn't currently technically allow me to make changes to this project yet.. Remember all. Always use at least a semi updated version of chrome -_- my bad all. What are these changes for? What does using emptyDir mean?\n. @kramasamy @sijie That is awesome! I love that! Great work guys!. @kramasamy what does the empty dir bookkeeper do?. Also fails when I try to use the api jar. Goal is to easily spin up a dev environment where people can test heron locally quickly using docker without doing a build on their machine. This seems like a bug, I just tried it on the open internet as well.. @kramasamy \n0.17.2 (MAYBE .1 i forget)  does have the client installed  because I can run the heron command from inside the cluster and it works, java home is set etc. There is just something going on where it cannot access the Exclaimation Topology and I don't know enough about the container to know what that is. The new container you are right does not have the heron client in it, and ideally it would be super nice if the heron client worked in the debian container, but I was referring to the older container that was based off ubuntu.\nQuick recap because I have found the way I say things can be confusing:\n1) Heron client is in 17.2 (17.1?) and works but I can't launch the topology locally by either bashing into the container or following the docker documentation found here:\nhttps://github.com/twitter/heron/tree/master/docker\n2) Yes I would love it if the new container (17.3) which is you guys' debian build could also have the heron client in it and that would work too\nThanks heron team you guys rock!. I am close to getting a dev container running, having issues with step 6 on the docker page\n\"Step 6 \u2014 Add the bazelrc configuration file for Bazel and the compile.sh script (from the docker folder) that compiles Heron\"\nhttps://twitter.github.io/heron/docs/developers/compiling/docker/\nThen I can pass y'all a base image I push to container yard that you guys could use to push a dev container under heron name if you wanted. where can I find the basil stuff from step 6?. Not the way I was hoping to do this docker file but what ever, you can use this as template for providing a container in which people can do local dev work with heron 0.17.3\nI don't mind maintaining it.\nFROM centos:centos7\n# Dockerfile author / maintainer\nMAINTAINER mjschmidt mjschmidt\n# This is passed to the heron build command via the --config flag\nENV TARGET_PLATFORM centos\nENV bazelVersion 0.9.0\nRUN yum -y upgrade\nRUN yum -y update\nRUN yum -y install \\\nautomake \\\ncurl \\\ncmake \\\nopenssl-devel \\\nfile \\\ngcc \\\ngcc-c++ \\\ngit \\\nkernel-devel \\\nlibtool \\\nmake \\\npatch \\\npython \\\npython-devel \\\npython-setuptools \\\nzip \\\nunzip \\\nvim \\\nwget \\\nwhich\n`RUN yum -y install java-1.8.0-openjdk java-1.8.0-openjdk-devel`\nENV JAVA_HOME /usr/lib/jvm/java-1.8.0\n\nRun git clone https://github.com/twitter/heron.git\nRUN curl -LO http://github.com/twitter/heron/releases/download/0.17.3/heron-install-0.17.3-centos.sh\nRUN chmod +x heron-install-0.17.3-centos.sh\nRUN ./heron-install-0.17.3-centos.sh\n#@TODO I want to get heron completely compiled from github at some point... unfortunately dockering inside of docker is hard and the container wants to do docker things\n#RUN heron-install-0.17.3-centos.sh\n#RUN heron/docker/scripts/build-artifacts.sh centos7 0.17.3 ~/heron-release\n#RUN ~/heron-release/heron-install-0.17.3-centos7.sh\n#ADD bazelrc /root/.bazelrc\n#ADD scripts/compile-platform.sh /compile-platform.sh\n``\n. orchapod/herondev could be used for heron as of 0.17.3 let me know if you'd like me to tag according to heron version installed, and how I should handle other changes that might need container changes. (0.17.3.x? based on version of container I am running?). @kramasamy I think this is solved for the 17.4 or 17.5 release right?\nThis can also be found at orchapod/herondev. I think this is solved now? I checked the container and it looks good. . This can be closed as resuloved :). Can we linn the kubernetes bug to this? . Link*. @jerrypeng  So lets say I as a Heron analytic developer want to guarantee that all of the data sent from one container is received in the next? In the topology model this was done through ack and fail. It is a capability that will be required for us to move to the Streamlet API/Heron. If we put a pull request in and have it as a feature that is turned off by default would you guys be open to a merge request? If not how can we get similar functionality from Streamlet?. Cool. Knowing that we can get this pulled back into the master branch means we can work on implementing it. We will stay connected in the heronstreaming channel as we move forward. Thanks!. I can live with either. . ",
    "biebiep": "Probably requires a small code change in:\ncom\\twitter\\heron\\scheduler\\marathon\\MarathonScheduler.java\nOn:\n```\nprotected String getTopologyConf(PackingPlan packing) {\nconfig = Config.newBuilder()\n    .putAll(config)\n    .put(Key.TOPOLOGY_BINARY_FILE,\n        Context.topologyBinaryFile(config))\n    .build();\n\nObjectMapper mapper = new ObjectMapper();\n\n// TODO (nlu): use heterogeneous resources\n// Align resources to maximal requested resource\nPackingPlan updatedPackingPlan = packing.cloneWithHomogeneousScheduledResource();\nSchedulerUtils.persistUpdatedPackingPlan(Runtime.topologyName(runtime),\n    updatedPackingPlan, Runtime.schedulerStateManagerAdaptor(runtime));\n\nResource containerResource = updatedPackingPlan.getContainers()\n    .iterator().next().getRequiredResource();\n\n// Create app conf list for each container\nArrayNode instances = mapper.createArrayNode();\nfor (int i = 0; i < Runtime.numContainers(runtime); i++) {\n  ObjectNode instance = mapper.createObjectNode();\n\n  instance.put(MarathonConstants.ID, \"/\"+Runtime.topologyName(runtime)+\"/\"+Integer.toString(i)); //Absolute paths need to go.\n\n^ right there and also\n      instance.put(MarathonConstants.COMMAND, getExecutorCommand(i));\n      instance.put(MarathonConstants.CPU, containerResource.getCpu());\n      instance.set(MarathonConstants.CONTAINER, getContainer(mapper));\n      instance.put(MarathonConstants.MEMORY, containerResource.getRam().asMegabytes());\n      instance.put(MarathonConstants.DISK, containerResource.getDisk().asMegabytes());\n      instance.put(MarathonConstants.INSTANCES, 1);\n      instance.set(MarathonConstants.LABELS, getLabels(mapper));\n      instance.set(MarathonConstants.FETCH, getFetchList(mapper));\n  instances.add(instance);\n}\n\n// Create marathon group for a topology\nObjectNode appConf = mapper.createObjectNode();\nappConf.put(MarathonConstants.ID, \"/\"+Runtime.topologyName(runtime));\n\n^ about here\n    appConf.set(MarathonConstants.APPS, instances);\nreturn appConf.toString();\n\n}\n```\nWill edit and run tests tomorrow to see what happens. Will open PR if it passes.. ",
    "dadgar": "@jerrypeng I am curious why not have heron submit a job to a running Nomad cluster?. @jerrypeng Ah thanks for the great answer! I saw the PR and then looked at the docs so I thought this was implementing the standalone mode! So there are just missing docs for running against an existing Nomad cluster.\nThat is awesome that you all chose to run the standalone cluster mode on Nomad!. ",
    "CLAassistant": " All committers have signed the CLA..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.Ali Ahmed seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please add the email address used for this commit to your account.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  All committers have signed the CLA..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.Chris seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please add the email address used for this commit to your account.You have signed the CLA already but the status is still pending? Let us recheck it..  All committers have signed the CLA..  All committers have signed the CLA..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.Ali Ahmed seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please add the email address used for this commit to your account.You have signed the CLA already but the status is still pending? Let us recheck it..  All committers have signed the CLA..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.Ali Ahmed seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please add the email address used for this commit to your account.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.Ali Ahmed seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please add the email address used for this commit to your account.You have signed the CLA already but the status is still pending? Let us recheck it..  All committers have signed the CLA..  All committers have signed the CLA..  All committers have signed the CLA..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.Ali Ahmed seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please add the email address used for this commit to your account.You have signed the CLA already but the status is still pending? Let us recheck it..  All committers have signed the CLA..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  All committers have signed the CLA..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.Ali Ahmed seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please add the email address used for this commit to your account.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.Ali Ahmed seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please add the email address used for this commit to your account.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you all sign our Contributor License Agreement before we can accept your contribution.1 out of 2 committers have signed the CLA.:white_check_mark: aahmed-se:x: Ali AhmedAli Ahmed seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please add the email address used for this commit to your account.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.Maosong Fu seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please add the email address used for this commit to your account.You have signed the CLA already but the status is still pending? Let us recheck it..  All committers have signed the CLA..  All committers have signed the CLA..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  All committers have signed the CLA..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  All committers have signed the CLA..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.You have signed the CLA already but the status is still pending? Let us recheck it..  Thank you for your submission, we really appreciate it. Like many open source projects, we ask that you sign our Contributor License Agreement before we can accept your contribution.Da Cheng seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please add the email address used for this commit to your account.You have signed the CLA already but the status is still pending? Let us recheck it.. ",
    "Code0x58": "It looks like the changes were merged elsewhere, or the is practically a null change with a PR that doesn't match the change, so I am guessing this is a candidate for closing.. It looks good to me. The last failure looks like it may have been transient, but I am not certain.\nThere is now protobuf 3.6 (only 3.6.0 on PyPI, but the .1 patch had no python changes) . It has been moved to its own GitHub organisation, among other changes.. That log file is a couple of hours off the event you reported, and there are a few things that look suspicious to me (inc.[SEVERE] log messages). I'd look around lines which have the topologies in them if they look like they are serving the request.. Ah, I was thrown off by the +0000 which made it look like it was all in UTC\u2020, so thought you got logs for another instance of the event. I think this is the relevent section of the log which is littered with [WARNING] and [SEVERE]:\n```\n[2018-06-14 10:17:44 +0000] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Starting Curator client connecting to: heron-zookeeper:2181\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.curator.framework.imps.CuratorFrameworkImpl: Starting\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=heron-zookeeper:2181 sessionTimeout=30000 watcher=org.apache.curator.ConnectionState@18db52f6\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.zookeeper.ClientCnxn: Opening socket connection to server heron-zookeeper.heron.svc.cluster.local/100.71.255.123:2181. Will not attempt to authenticate using SASL (unknown error)\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.zookeeper.ClientCnxn: Socket connection established to heron-zookeeper.heron.svc.cluster.local/100.71.255.123:2181, initiating session\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.zookeeper.ClientCnxn: Session establishment complete on server heron-zookeeper.heron.svc.cluster.local/100.71.255.123:2181, sessionid = 0x163fdb642fc0014, negotiated timeout = 30000\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.curator.framework.state.ConnectionStateManager: State change: CONNECTED\n[2018-06-14 10:17:44 +0000] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Directory tree initialized.\n[2018-06-14 10:17:44 +0000] [INFO] com.twitter.heron.uploader.dlog.DLUploader: Initializing distributedlog namespace for uploading topologies : distributedlog://heron-zookeeper:2181/distributedlog\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.api.namespace.NamespaceBuilder: No feature provider is set. All features are disabled now.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.impl.BKNamespaceDriver: Created shared zooKeeper client builder dlzk:distributedlog://heron-zookeeper:2181/distributedlog:factory_writer_shared: zkServers = heron-zookeeper:2181, numRetries = 3, sessionTimeout = 30000, retryBackoff = 5000, maxRetryBackoff = 30000, zkAclId = null.\n[2018-06-14 10:17:44 +0000] [WARNING] org.apache.distributedlog.impl.BKNamespaceDriver: Could not use Netty Epoll event loop for bookie server: \njava.lang.NoClassDefFoundError: Could not initialize class dlshade.io.netty.channel.epoll.EpollEventLoopGroup\n    at org.apache.distributedlog.impl.BKNamespaceDriver.getDefaultEventLoopGroup(BKNamespaceDriver.java:259)\n    at org.apache.distributedlog.impl.BKNamespaceDriver.initializeBookKeeperClients(BKNamespaceDriver.java:270)\n    at org.apache.distributedlog.impl.BKNamespaceDriver.initialize(BKNamespaceDriver.java:208)\n    at org.apache.distributedlog.api.namespace.NamespaceBuilder.build(NamespaceBuilder.java:238)\n    at com.twitter.heron.uploader.dlog.DLUploader.initializeNamespace(DLUploader.java:137)\n    at com.twitter.heron.uploader.dlog.DLUploader.initialize(DLUploader.java:94)\n    at com.twitter.heron.scheduler.SubmitterMain.submitTopology(SubmitterMain.java:420)\n    at com.twitter.heron.apiserver.actions.SubmitTopologyAction.execute(SubmitTopologyAction.java:33)\n    at com.twitter.heron.apiserver.resources.TopologyResource.submit(TopologyResource.java:223)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81)\n    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:144)\n    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:161)\n    at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:160)\n    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:99)\n    at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:389)\n    at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:347)\n    at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:102)\n    at org.glassfish.jersey.server.ServerRuntime$2.run(ServerRuntime.java:326)\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:315)\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:297)\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:267)\n    at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:317)\n    at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:305)\n    at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:1154)\n    at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:473)\n    at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:427)\n    at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:388)\n    at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:341)\n    at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:228)\n    at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:841)\n    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:535)\n    at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:188)\n    at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1253)\n    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:168)\n    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\n    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:166)\n    at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1155)\n    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\n    at org.eclipse.jetty.server.Server.handle(Server.java:564)\n    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:317)\n    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n    at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:279)\n    at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:110)\n    at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:124)\n    at org.eclipse.jetty.util.thread.Invocable.invokePreferred(Invocable.java:128)\n    at org.eclipse.jetty.util.thread.Invocable$InvocableExecutor.invoke(Invocable.java:222)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:294)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:199)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:673)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:591)\n    at java.lang.Thread.run(Thread.java:748)\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.impl.BKNamespaceDriver: Created shared client builder bk:distributedlog://heron-zookeeper:2181/distributedlog:factory_writer_shared : zkServers = heron-zookeeper:2181, ledgersPath = /bookkeeper/ledgers, numIOThreads = 4\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.impl.metadata.BKDLConfig: Propagate BKDLConfig to DLConfig : encodeRegionID = false, firstLogSegmentSequenceNumber = 1, createStreamIfNotExists = true, isFederated = false.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.impl.BKNamespaceDriver: Initialized BK namespace driver: clientId = heron-uploader, regionId = 0, federated = false.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.logsegment.LogSegmentMetadataCache: Log segment cache is enabled = true\n[2018-06-14 10:17:44 +0000] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Checking existence of path: /heron/topologies/xxxxxxxxxx\n[2018-06-14 10:17:44 +0000] [INFO] com.twitter.heron.packing.roundrobin.ResourceCompliantRRPacking: Initalizing ResourceCompliantRRPacking. CPU default: 1.000000, RAM default: ByteAmount{1 GB (1073741824 bytes)}, DISK default: ByteAmount{1 GB (1073741824 bytes)}.\n[2018-06-14 10:17:44 +0000] [INFO] com.twitter.heron.packing.roundrobin.ResourceCompliantRRPacking: ResourceCompliantRRPacking newPackingPlanBuilder. CPU max: 4.000000, RAMmaxMax: ByteAmount{4 GB (4724464026 bytes)}, DISK max: ByteAmount{4 GB (4724464026 bytes)}, Padding percentage: 10.\n[2018-06-14 10:17:44 +0000] [INFO] com.twitter.heron.packing.roundrobin.ResourceCompliantRRPacking: ResourceCompliantRRPacking newPackingPlanBuilder. CPU max: 4.000000, RAMmaxMax: ByteAmount{4 GB (4724464026 bytes)}, DISK max: ByteAmount{4 GB (4724464026 bytes)}, Padding percentage: 10.\n[2018-06-14 10:17:44 +0000] [INFO] com.twitter.heron.packing.roundrobin.ResourceCompliantRRPacking: ResourceCompliantRRPacking newPackingPlanBuilder. CPU max: 4.000000, RAMmaxMax: ByteAmount{4 GB (4724464026 bytes)}, DISK max: ByteAmount{4 GB (4724464026 bytes)}, Padding percentage: 10.\n[2018-06-14 10:17:44 +0000] [INFO] com.twitter.heron.packing.roundrobin.ResourceCompliantRRPacking: ResourceCompliantRRPacking newPackingPlanBuilder. CPU max: 4.000000, RAMmaxMax: ByteAmount{4 GB (4724464026 bytes)}, DISK max: ByteAmount{4 GB (4724464026 bytes)}, Padding percentage: 10.\n[2018-06-14 10:17:44 +0000] [INFO] com.twitter.heron.packing.roundrobin.ResourceCompliantRRPacking: ResourceCompliantRRPacking newPackingPlanBuilder. CPU max: 4.000000, RAMmaxMax: ByteAmount{4 GB (4724464026 bytes)}, DISK max: ByteAmount{4 GB (4724464026 bytes)}, Padding percentage: 10.\n[2018-06-14 10:17:44 +0000] [WARNING] com.twitter.heron.scheduler.SubmitterMain: The packing plan (generated by com.twitter.heron.packing.roundrobin.ResourceCompliantRRPacking) calls for a different number of containers (5) than what was explicitly set in the topology configs (1). Overriding the configs to specify 5 containers. When using com.twitter.heron.packing.roundrobin.ResourceCompliantRRPacking do not explicitly call config.setNumStmgrs(..) or config.setNumWorkers(..).\n[2018-06-14 10:17:44 +0000] [INFO] dlshade.org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=heron-zookeeper:2181 sessionTimeout=30000 watcher=org.apache.bookkeeper.zookeeper.ZooKeeperWatcherBase@19094b97\n[2018-06-14 10:17:44 +0000] [INFO] dlshade.org.apache.zookeeper.ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n[2018-06-14 10:17:44 +0000] [INFO] dlshade.org.apache.zookeeper.ClientCnxn: Opening socket connection to server heron-zookeeper/100.71.255.123:2181. Will not attempt to authenticate using SASL (unknown error)\n[2018-06-14 10:17:44 +0000] [INFO] dlshade.org.apache.zookeeper.ClientCnxn: Socket connection established, initiating session, client: /100.105.146.3:49994, server: heron-zookeeper/100.71.255.123:2181\n[2018-06-14 10:17:44 +0000] [INFO] dlshade.org.apache.zookeeper.ClientCnxn: Session establishment complete on server heron-zookeeper/100.71.255.123:2181, sessionid = 0x163fdb642fc0015, negotiated timeout = 30000\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.bookkeeper.zookeeper.ZooKeeperWatcherBase: ZooKeeper client is connected now.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.zk.LimitedPermitManager: EnablePermits = true, Epoch = 1.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.zk.LimitedPermitManager: EnablePermits = true, Epoch = 1.\n[2018-06-14 10:17:44 +0000] [INFO] com.twitter.heron.uploader.dlog.DLUploader: Uploading topology package at '/tmp/xxxxxxxxxx5459940652240403157/topology.tar.gz' to target DL at 'distributedlog://heron-zookeeper:2181/distributedlog/xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz'\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.bk.SimpleLedgerAllocator: Ledger allocator for /distributedlog/xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz//allocation moved version from -1 to 0.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.BKLogReadHandler: Initiating Recovery For xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz: : []\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.BKLogReadHandler: Initiating Recovery For xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz: : []\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.bk.SimpleLedgerAllocator: Ledger allocator /distributedlog/xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz//allocation moved to phase ALLOCATING : version = 0.\n[2018-06-14 10:17:44 +0000] [INFO] dlshade.org.apache.zookeeper.ZooKeeper: Initiating client connection, connectString=heron-zookeeper:2181 sessionTimeout=30000 watcher=org.apache.bookkeeper.zookeeper.ZooKeeperWatcherBase@404da4b\n[2018-06-14 10:17:44 +0000] [INFO] dlshade.org.apache.zookeeper.ClientCnxnSocket: jute.maxbuffer value is 4194304 Bytes\n[2018-06-14 10:17:44 +0000] [INFO] dlshade.org.apache.zookeeper.ClientCnxn: Opening socket connection to server heron-zookeeper/100.71.255.123:2181. Will not attempt to authenticate using SASL (unknown error)\n[2018-06-14 10:17:44 +0000] [INFO] dlshade.org.apache.zookeeper.ClientCnxn: Socket connection established, initiating session, client: /100.105.146.3:49996, server: heron-zookeeper/100.71.255.123:2181\n[2018-06-14 10:17:44 +0000] [INFO] dlshade.org.apache.zookeeper.ClientCnxn: Session establishment complete on server heron-zookeeper/100.71.255.123:2181, sessionid = 0x163fdb642fc0016, negotiated timeout = 30000\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.bookkeeper.zookeeper.ZooKeeperWatcherBase: ZooKeeper client is connected now.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Initialize rackaware ensemble placement policy @  @ /heron/tools : org.apache.distributedlog.net.DNSResolverForRacks.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Not weighted\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.bookkeeper.client.BookKeeper: Weighted ledger placement is not enabled\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.bookkeeper.net.NetworkTopologyImpl: Adding a new node: /default-region/default-rack/100.126.73.199:3181\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Initialize rackaware ensemble placement policy @  @ /heron/tools : org.apache.distributedlog.net.DNSResolverForRacks.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Not weighted\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.BookKeeperClient: BookKeeper Client created bk:distributedlog://heron-zookeeper:2181/distributedlog:factory_writer_shared with its own ZK Client : ledgersPath = /bookkeeper/ledgers, numRetries = 3, sessionTimeout = 30000, backoff = 5000, maxBackoff = 30000, dnsResolver = \n[2018-06-14 10:17:44 +0000] [INFO] org.apache.bookkeeper.net.NetworkTopologyImpl: Adding a new node: /default-region/default-rack/100.126.73.199:3181\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.bk.SimpleLedgerAllocator: Ledger allocator for /distributedlog/xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz//allocation moved version from 0 to 1.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.bk.SimpleLedgerAllocator: Ledger allocator /distributedlog/xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz//allocation moved to phase ALLOCATED : version = 1.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.bk.SimpleLedgerAllocator: Ledger allocator /distributedlog/xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz//allocation moved to phase HANDING_OVER : version = 1.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.BKLogReadHandler: No max ledger sequence number found while creating log segment 1 for xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz:.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.bk.SimpleLedgerAllocator: Ledger allocator /distributedlog/xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz//allocation moved to phase HANDED_OVER : version = 1.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.bk.SimpleLedgerAllocator: Ledger allocator for /distributedlog/xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz//allocation moved version from 1 to 2.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.bk.SimpleLedgerAllocator: Ledger allocator /distributedlog/xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz//allocation moved to phase ALLOCATING : version = 2.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.logsegment.PerStreamLogSegmentCache: xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz added log segment (inprogress_000000000000000001 : [LogSegmentId:4, firstTxId:131072, lastTxId:-999, version:VERSION_V5_SEQUENCE_ID, completionTime:0, recordCount:0, regionId:0, status:0, logSegmentSequenceNumber:1, lastEntryId:-1, lastSlotId:-1, inprogress:true, minActiveDLSN:DLSN{logSegmentSequenceNo=1, entryId=0, slotId=0}, startSequenceId:-1]) to cache.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.BKLogReadHandler: Deleting log segments older than -7729412157735137 for xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz: : []\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.bk.SimpleLedgerAllocator: Ledger allocator for /distributedlog/xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz//allocation moved version from 2 to 3.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.distributedlog.bk.SimpleLedgerAllocator: Ledger allocator /distributedlog/xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz//allocation moved to phase ALLOCATED : version = 3.\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.bookkeeper.proto.PerChannelBookieClient: Successfully connected to bookie: [id: 0xe89c134d, L:/100.105.146.3:42366 - R:100.126.73.199/100.126.73.199:3181]\n[2018-06-14 10:17:44 +0000] [INFO] org.apache.bookkeeper.proto.PerChannelBookieClient: Successfully connected to bookie: 100.126.73.199:3181\n[2018-06-14 10:17:45 +0000] [INFO] org.apache.bookkeeper.proto.PerChannelBookieClient: connection [id: 0xe89c134d, L:/100.105.146.3:42366 - R:100.126.73.199/100.126.73.199:3181] authenticated as BookKeeperPrincipal{ANONYMOUS}\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 101): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 102): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 103): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 104): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 105): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 106): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 107): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 108): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 109): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 110): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 111): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 112): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 113): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 114): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 115): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 116): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 117): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 118): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 119): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 120): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 121): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 122): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 123): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 124): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 125): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 126): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 127): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 128): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.PendingAddOp: Failed to write entry (4, 129): Bookie operation timeout\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicyImpl: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.RegionAwareEnsemblePlacementPolicy: Failed to choose a bookie from /default-region/default-rack : excluded [], fallback to choose bookie randomly from the cluster.\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Could not get additional bookie to remake ensemble, closing ledger: 4\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Closing ledger 4 due to error -6\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E101\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E102\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 102: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E103\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 103: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E104\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 104: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E105\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 105: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E106\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E107\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 106: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E108\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 107: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E109\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E110\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 108: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 109: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E111\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 110: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E112\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 111: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E113\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 112: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E114\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 113: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E115\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 114: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 115: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E116\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E117\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 116: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E118\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 117: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E119\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 118: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E120\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 119: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E121\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 120: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E122\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 121: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E123\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 122: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E124\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E125\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 123: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 124: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E126\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 125: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E127\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E128\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 126: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E129\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 127: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E130\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 128: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E131\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 129: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E132\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 130: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E133\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 131: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E134\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 132: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E135\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E136\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 133: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E137\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E138\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 134: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 135: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 136: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 137: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E139\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 138: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E140\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 139: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E141\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 140: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E142\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 141: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E143\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 142: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E144\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 143: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E145\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 144: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E146\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 145: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E147\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 146: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E148\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 147: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E149\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 148: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E150\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 149: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E151\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 150: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E152\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E153\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 151: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E154\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E155\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 152: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E156\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 153: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E157\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.PendingAddOp: Write of ledger entry to quorum failed: L4 E158\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 154: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 155: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [SEVERE] com.twitter.heron.uploader.dlog.DLUploader: Encountered exceptions on uploading the package 'xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz' \norg.apache.distributedlog.exceptions.WriteException: Write rejected because stream xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz has encountered an error : writer has been closed due to error.\n    at org.apache.distributedlog.BKAsyncLogWriter.doGetLogSegmentWriter(BKAsyncLogWriter.java:217)\n    at org.apache.distributedlog.BKAsyncLogWriter.getLogSegmentWriter(BKAsyncLogWriter.java:207)\n    at org.apache.distributedlog.BKAsyncLogWriter.getLogSegmentWriterForEndOfStream(BKAsyncLogWriter.java:236)\n    at org.apache.distributedlog.BKAsyncLogWriter.markEndOfStream(BKAsyncLogWriter.java:474)\n    at org.apache.distributedlog.AppendOnlyStreamWriter.markEndOfStream(AppendOnlyStreamWriter.java:79)\n    at com.twitter.heron.dlog.DLOutputStream.close(DLOutputStream.java:64)\n    at com.twitter.heron.uploader.dlog.DLUploader.doUploadPackage(DLUploader.java:189)\n    at com.twitter.heron.uploader.dlog.DLUploader.uploadPackage(DLUploader.java:154)\n    at com.twitter.heron.scheduler.SubmitterMain.uploadPackage(SubmitterMain.java:542)\n    at com.twitter.heron.scheduler.SubmitterMain.submitTopology(SubmitterMain.java:445)\n    at com.twitter.heron.apiserver.actions.SubmitTopologyAction.execute(SubmitTopologyAction.java:33)\n    at com.twitter.heron.apiserver.resources.TopologyResource.submit(TopologyResource.java:223)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81)\n    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:144)\n    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:161)\n    at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:160)\n    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:99)\n    at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:389)\n    at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:347)\n    at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:102)\n    at org.glassfish.jersey.server.ServerRuntime$2.run(ServerRuntime.java:326)\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:315)\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:297)\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:267)\n    at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:317)\n    at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:305)\n    at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:1154)\n    at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:473)\n    at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:427)\n    at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:388)\n    at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:341)\n    at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:228)\n    at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:841)\n    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:535)\n    at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:188)\n    at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1253)\n    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:168)\n    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\n    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:166)\n    at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1155)\n    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\n    at org.eclipse.jetty.server.Server.handle(Server.java:564)\n    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:317)\n    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n    at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:279)\n    at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:110)\n    at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:124)\n    at org.eclipse.jetty.util.thread.Invocable.invokePreferred(Invocable.java:128)\n    at org.eclipse.jetty.util.thread.Invocable$InvocableExecutor.invoke(Invocable.java:222)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:294)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:199)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:673)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:591)\n    at java.lang.Thread.run(Thread.java:748)\n[2018-06-14 10:17:55 +0000] [INFO] org.apache.distributedlog.BookKeeperClient: BookKeeper Client closed bk:distributedlog://heron-zookeeper:2181/distributedlog:factory_writer_shared\n[2018-06-14 10:17:55 +0000] [INFO] org.apache.bookkeeper.proto.PerChannelBookieClient: Closing the per channel bookie client for 100.126.73.199:3181\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 156: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 157: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.distributedlog.BKLogSegmentWriter: Log segment xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz::inprogress_000000000000000001 entryId 158: Tried to set transmit result to (-6) but is already (-6)\n[2018-06-14 10:17:55 +0000] [INFO] org.apache.bookkeeper.proto.PerChannelBookieClient: Disconnected from bookie channel [id: 0xe89c134d, L:/100.105.146.3:42366 ! R:100.126.73.199/100.126.73.199:3181]\n[2018-06-14 10:17:55 +0000] [SEVERE] org.apache.bookkeeper.client.LedgerHandle: Error update ledger metadata for ledger 4 : -19\n[2018-06-14 10:17:55 +0000] [WARNING] org.apache.bookkeeper.client.LedgerHandle: Close failed: BookKeeper client is closed\n[2018-06-14 10:17:55 +0000] [INFO] org.apache.distributedlog.ZooKeeperClient: Close zookeeper client bk:distributedlog://heron-zookeeper:2181/distributedlog:factory_writer_shared:zk.\n[2018-06-14 10:17:55 +0000] [INFO] org.apache.distributedlog.ZooKeeperClient: Closing zookeeper client bk:distributedlog://heron-zookeeper:2181/distributedlog:factory_writer_shared:zk.\n[2018-06-14 10:17:55 +0000] [INFO] dlshade.org.apache.zookeeper.ClientCnxn: EventThread shut down for session: 0x163fdb642fc0016\n[2018-06-14 10:17:55 +0000] [INFO] dlshade.org.apache.zookeeper.ZooKeeper: Session: 0x163fdb642fc0016 closed\n[2018-06-14 10:17:55 +0000] [INFO] org.apache.distributedlog.ZooKeeperClient: Closed zookeeper client bk:distributedlog://heron-zookeeper:2181/distributedlog:factory_writer_shared:zk.\n[2018-06-14 10:17:55 +0000] [INFO] org.apache.distributedlog.ZooKeeperClient: Close zookeeper client dlzk:distributedlog://heron-zookeeper:2181/distributedlog:factory_writer_shared.\n[2018-06-14 10:17:55 +0000] [INFO] org.apache.distributedlog.ZooKeeperClient: Closing zookeeper client dlzk:distributedlog://heron-zookeeper:2181/distributedlog:factory_writer_shared.\n[2018-06-14 10:17:55 +0000] [INFO] dlshade.org.apache.zookeeper.ClientCnxn: EventThread shut down for session: 0x163fdb642fc0015\n[2018-06-14 10:17:55 +0000] [INFO] dlshade.org.apache.zookeeper.ZooKeeper: Session: 0x163fdb642fc0015 closed\n[2018-06-14 10:17:55 +0000] [INFO] org.apache.distributedlog.ZooKeeperClient: Closed zookeeper client dlzk:distributedlog://heron-zookeeper:2181/distributedlog:factory_writer_shared.\n[2018-06-14 10:17:55 +0000] [INFO] org.apache.distributedlog.impl.BKNamespaceDriver: Release external resources used by channel factory.\n[2018-06-14 10:17:55 +0000] [INFO] org.apache.distributedlog.impl.BKNamespaceDriver: Stopped request timer\n[2018-06-14 10:17:55 +0000] [INFO] org.apache.distributedlog.BKDistributedLogNamespace: Executor Service Stopped.\n[2018-06-14 10:17:55 +0000] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Closing the CuratorClient to: heron-zookeeper:2181\n[2018-06-14 10:17:55 +0000] [INFO] org.apache.zookeeper.ClientCnxn: EventThread shut down\n[2018-06-14 10:17:55 +0000] [INFO] org.apache.zookeeper.ZooKeeper: Session: 0x163fdb642fc0014 closed\n[2018-06-14 10:17:55 +0000] [INFO] com.twitter.heron.statemgr.zookeeper.curator.CuratorStateManager: Closing the tunnel processes\n[2018-06-14 10:17:55 +0000] [SEVERE] com.twitter.heron.apiserver.resources.TopologyResource: error submitting topology xxxxxxxxxx \ncom.twitter.heron.spi.uploader.UploaderException: Encountered exceptions on uploading the package 'xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz'\n    at com.twitter.heron.uploader.dlog.DLUploader.uploadPackage(DLUploader.java:160)\n    at com.twitter.heron.scheduler.SubmitterMain.uploadPackage(SubmitterMain.java:542)\n    at com.twitter.heron.scheduler.SubmitterMain.submitTopology(SubmitterMain.java:445)\n    at com.twitter.heron.apiserver.actions.SubmitTopologyAction.execute(SubmitTopologyAction.java:33)\n    at com.twitter.heron.apiserver.resources.TopologyResource.submit(TopologyResource.java:223)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81)\n    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:144)\n    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:161)\n    at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:160)\n    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:99)\n    at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:389)\n    at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:347)\n    at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:102)\n    at org.glassfish.jersey.server.ServerRuntime$2.run(ServerRuntime.java:326)\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:315)\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:297)\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:267)\n    at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:317)\n    at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:305)\n    at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:1154)\n    at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:473)\n    at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:427)\n    at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:388)\n    at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:341)\n    at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:228)\n    at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:841)\n    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:535)\n    at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:188)\n    at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1253)\n    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:168)\n    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\n    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:166)\n    at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1155)\n    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\n    at org.eclipse.jetty.server.Server.handle(Server.java:564)\n    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:317)\n    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n    at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:279)\n    at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:110)\n    at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:124)\n    at org.eclipse.jetty.util.thread.Invocable.invokePreferred(Invocable.java:128)\n    at org.eclipse.jetty.util.thread.Invocable$InvocableExecutor.invoke(Invocable.java:222)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:294)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:199)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:673)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:591)\n    at java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.distributedlog.exceptions.WriteException: Write rejected because stream xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz has encountered an error : writer has been closed due to error.\n    at org.apache.distributedlog.BKAsyncLogWriter.doGetLogSegmentWriter(BKAsyncLogWriter.java:217)\n    at org.apache.distributedlog.BKAsyncLogWriter.getLogSegmentWriter(BKAsyncLogWriter.java:207)\n    at org.apache.distributedlog.BKAsyncLogWriter.getLogSegmentWriterForEndOfStream(BKAsyncLogWriter.java:236)\n    at org.apache.distributedlog.BKAsyncLogWriter.markEndOfStream(BKAsyncLogWriter.java:474)\n    at org.apache.distributedlog.AppendOnlyStreamWriter.markEndOfStream(AppendOnlyStreamWriter.java:79)\n    at com.twitter.heron.dlog.DLOutputStream.close(DLOutputStream.java:64)\n    at com.twitter.heron.uploader.dlog.DLUploader.doUploadPackage(DLUploader.java:189)\n    at com.twitter.heron.uploader.dlog.DLUploader.uploadPackage(DLUploader.java:154)\n    ... 53 more\n```\nI can't say I happen to know how any of this works at the moment, I am just interested in the issue as I will soon be deploying PEXs to Kubernetes. Does this work with Java toplogies? I think it would be a config issue too, but haven't got to that point myself to know better of the process or documentation. Hopefully someone else will know better and be able to give you more help, or I might encounter the same things and work it out but that would probably be a week or two away. If you do work it out I'd be interested in hearing what the issue is.\n\u2020 out of curiosity, which time zones were the different applications running in, the +XXXX should be meaningful so I think one or both aren't behaving as they should. While making and starting to test a heronpy PEX, I found that the documentation and example repos don't have valid imports. Were you able to execute your PEXs locally without getting import errors? I don't know if that could be a contributing factor. There is PR #2928 to get the documentation a bit more up to date.. Thanks! I'll probably have questions at some point.\nKubernetes moves pretty fast so I wouldn't be surprised if it is the case. That said, it looks like the chart was made with Helm 2.7.2 (vs. latest which is 2.9.1). I don't have practical experience with it yet but imagine I'll be there in 2-3 weeks so will have a look over it.\nIt looks like the the files are here and the chart is generated here.. I found I had to up the minikube resources, and wait for everything to come up before heron was deployed properly, so issues with time+lacking resources on my laptop/network were a problem to start with.\nI also got the two issues:\n 1. Failed to get physical plan for topology ExclamationTopology - this is mentioned in the troubleshooting guide but that wasn't helpful in this case - it only helps if there was an executor failure (it doesn't mention ~/.herondata/ is on the worker). I think the issue happens when the workers (topology pods) aren't up yet either because of start times/resources/executor failures. With a bit of poking around, it looks like this happens when trying to activate the topology while it is in the UNKOWN status. I guess this issue is less apparent on beefier setups.\n The documentation on troubleshooting could do with being updated, as well as docs for using Kubernetes/minikue and what to look out for with resource issues. It would be nice if the issue if the CLI gave more feedback if there are resource issues, and if it waited around for executors to come up.\n\nCaused by: org.apache.distributedlog.exceptions.WriteException: Write rejected because stream xxxxxxxxxx-cristobal-tag-0-7116362347360204918.tar.gz has encountered an error : writer has been closed due to error comes up for me with the larger PEX too. I think the topology is supposed to be uploaded to ZooKeeper by BookKeeper, but it is dying due to the ZooKeeper client session timing out (which I saw as a warning in BookKeeper's logs), which explains why smaller ones are more likely to succeed. I suspect increasing the tick duration will help, especially when using minikube when your machine is going to be under a lot of load.\nThis would be something to document, or try avoiding by tweaking the deploy files to include an extended tick. I havent tested this, but am pretty sure that is the case.\n\nIt feels like the moral of the story is \"have a beefy AF setup so you are less likely to encounter issues\", time for a Dell Precision?\n. I think it was something to do with the resources requested by heron being something quite large (1CPU?) but there was way to get the client to request lower resources, there was some talk around the time of this in the general Heron slack channel which may be helpful. I've since fallen off the Heron trail. @kramasamy yep, I think it's good to go now. Once this does work locally, it'll need support on the schedulers, which I think is to update the executor image to include Python3.6.\nI will look into making .pex files pin to more generic interpreters (python3 vs. python3.6), otherwise it will be a pain where you have to rebuild your PEXs every time the executor image's python 3 minor version increases - even if it is infrequent.. As an update, I was waiting for changes around ./docker/ to happen before I go changing things in there for the executors. I'll probably use the first official Apache release as my trigger to get back on this.. This is failing for the same reason as master now, but should be good to merge. I pulled in #2937 so the tests pass, so this should probably be merged after it.. It seems like the scala rules have changed a bit since the last pinned commit. Hopefully the SSL cert issue will be fixed with a renewal very soon as I don't feel I have the time to look into this more at the moment vs. waiting for Lightbend to fix the critical issue.. Thanks! It looks like that does the trick, I'll have a quick check in tomorrow morning.. Yep, I think this was merged as a part of #2928 though but not showing up because of the FF merge. Yep, this is in. The build failure on master is down to #2925 now. Out of curiosity, is there a pull based alternative to this which doesn't require changes? I was inclined to look as this introduces a lot of dependencies. Here's a post from May 2017 that made me think it might be an option:\n\nKapacitor & Pull\nWith this release, we\u2019ve integrated Prometheus\u2019 service discovery and scraping code into Kapacitor. That means that any service discovery target that works with Prometheus will work with Kapacitor. Combined with a TICK script, you can use Kapacitor to monitor Prometheus scrape targets, write data into InfluxDB, and perform filtering, transformation and other tasks. With Kapacitor\u2019s user defined functions (UDFs) it becomes trivial to pull in advanced anomaly detection and custom logic.. \n",
    "joshfischer1108": "I should also mention that most of the this code is based off the Apache Storm Flux implementation.  There has been some refactoring done, but still largely based off Storm's Flux.. @kramasamy Will do.. @kramasamy I think we should be good for a first pass.  I ran into some of the unknown issues last night while trying to move more examples from the Flux repo.  For instance, Heron's version of Storm does not include the package below\nimport org.apache.storm.state.*\nSo any of bolts or spouts that contain this, won't work.  I'm sure you are aware, just making sure to let it be known.   I have more examples I would like to create, but I will do them over the next few weeks.  Once this is merged, I will work on writing some docs in the static markdown files on how to run this examples and how to create your own ECO topologies.\nIf you are ok with this, then let's merge this branch.. @kramasamy Will do!. @kramasamy the yaml files to assemble the topologies are inheron/examples/src/resources/*yaml.    \nIf you want to move them, now would be the time, but then you will also have to modify \ntools/rules/heron_examples.bzl. @kramasamy Looks good to me.. @lucperkins or @kramasamy Can either of you look at this error that occurred during the build?  I think I've seen it before when building out the docs.  error: Server does not allow request for unadvertised object 87d417e53308ef794b53734bc09f86f01d1a0ace.  Everything built fine on my local.  No rush, when ever you have time.   I see now that the hash is a git commit.  Do I need to do something extra to get this build to run?  I'm assuming it's something that has to do with my gh-pages branch that I am committing to for building the docs.\n. @kramasamy All requested changes have been made.. @lucperkins I cannot get these pages to build locally on my machine, that is why I had to push them to the gh-pages branch.  I spent quite a bit of time trying to get this to work.   I will try again.  If not I will mention you and ask for assistance. Ok with you?  Also the build broke again. :-(\n. @kramasamy @lucperkins Please pull this branch down and verify the docs make sense and they work as expected.  I think we are good to go here.  But lets talk before you merge.. @kramasamy All whitespace removed.. @kramasamy I've noticed my build broke a few times over the past week or so because of something outside of my code.  20180208042137_IntegrationTest_StatefulWindowingTest_ab0c695b-5236-47c6-9083-1c768c2275ed\n[2018-02-08 04:35:26 +0000] [ERROR]: FAILURE: 1/24 tests failed:\n[2018-02-08 04:35:26 +0000] [ERROR]:   - [54s]   : 20180208042137_IntegrationTest_OneBoltMultiTasks_e3887c7c-d59c-4131-ae87-f8e4b4ebd7fa\nHave you seen this test fail before?  I think this is the one that keeps failing randomly.  Ahh I see the issue now, master is failing.  I'll wait till this is fixed.\n. @kramasamy This wont work.  I pulled in changes from master.  The master branch build is broke.  So my build will break regardless\n. @kramasamy We are good to go\n. Looks fine to me.. \n. that is what I see when I hit the docs from the heronstreaming.io link\n. @kramasamy I've verified that all Heron topologies launch as expected.  Nice work.. +1 to merge @kramasamy \n. @kramasamy  Once master build is fixed I will merge in those changes. @nwangtw Didn't realize I had the permissions to restart build.  I don't think I did when Heron was in Twitter repo.   Just restarted it.  Thanks for bringing this up.. @nwangtw Can you look at this CI error please?  I'm not sure where this is coming from.. Just kicked it off again \ud83d\ude10 . +1 @jerrypeng .  Can't wait to try it out.\n. Test comment. @cckellogg Yes we should and glad you mentioned them.  It totally slipped my mind.  Just pushed them up.. Also found a file at heron/deploy/kubernetes/helm/templates/NOTES.txt that needed some changes.  Not sure if that's used to dump to the console or not, so I updated the values to be sure.  Also, integration tests have failed on me three times in a row.  Are there still some race conditions that could be causing this?\n. To verify these changes execute\nbash \n$ ./docker/scripts/build-artifacts.sh ubuntu14.04 0.19.0 ~/local-heron-release/\n$ ./docker/scripts/build-docker.sh ubuntu14.04 0.19.0 ~/local-heron-release/. This build failed because of an integration test.  I don't see a way to kick off the build anymore.  Seems the button was removed via permission changing\n. ~looking over, I found in error in my docker config.~\nStep 1/19 : FROM ubuntu:14.04\n ---> 578c3e61a98c\nStep 2/19 : RUN apt-get -y update && apt-get -y install     python     unzip     software-properties-common     supervisor     curl &&     apt-get clean all\n ---> Using cache\n ---> b7292a142da0\nStep 3/19 : RUN add-apt-repository ppa:openjdk-r/ppa && apt-get -y update &&     apt-get -y install openjdk-8-jdk && apt-get clean all\n ---> Using cache\n ---> dbdc059747ff\nStep 4/19 : ENV JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64\n ---> Using cache\n ---> 3632db9a55cb\nStep 5/19 : RUN update-ca-certificates -f\n ---> Using cache\n ---> a8870c88f73e\nStep 6/19 : ADD artifacts /heron\n ---> Using cache\n ---> 8c3fb38e66ee\nStep 7/19 : WORKDIR /heron\n ---> Using cache\n ---> 01f1def45d52\nStep 8/19 : RUN /heron/heron-client-install.sh && /heron/heron-tools-install.sh\n ---> Running in a341c5438a1a\n/bin/sh: 1: /heron/heron-client-install.sh: not found\nThe command '/bin/sh -c /heron/heron-client-install.sh && /heron/heron-tools-install.sh' returned a non-zero code: 127\n````\nI'll look into this more when I can.\n** Update **\nAfter looking at this, this ended up being a fluke.  This was an old build.  This issue has been resolved.. @kramasamy I went ahead and updated the other  docker images as well.  I think we are good to go.  Please look over my changes.. @kramasamy After doing further investigation.  This PR does fix the broken docker references to the install scripts, but it does not fix the other os specific issues that need to be addressed.  It's up to you if you decide you want to merge.. And of course the upside is that we can move away from Bazel version 0.14.1.. We have some work to do on the docs. I apologize for the missing details.  Try this\nbrew tap streamlio/homebrew-formulae\nbrew install heron\n``. \ud83d\udc4d . This is an issue we are aware of and ~are working~ have a plan to fix.  After the typical set up that is listed in the README.  you should be able to run make servein thewebsitedirectory that will serve the pages locally for you..  To see the site you would then visithttp://localhost:1313/incubator-heron/in your browser.. If using the flag--squashdoesn't cause any harm I don't see anything wrong with it.. We are building with Bazel0.14.x.  You could consider that as experimental too.. updated my comment for clarity.. ~I would prefer to keep the--squashflag.  600m is better than 1.2G~ I guess I don't mind.  Whatever you think is best Ning.\n. \ud83d\udc4d . :shipit: . I will back this change out.  I had to do some tricky stuff to get the pages to build on my local.  I meant to revert this, it slipped my mind.  Good catch @kramasamy .  I don't think the script would run without the -H under appended to both commands on my mac.  \n. @kramasamy Yes, just a typo error. I will fix . This will only print if someone submits a topology with a--props` flag pointing to a  file.  If you want I can take this out.\n. @kramasamy Ok I will enter some high level logging for each one of the steps.  I will do this after master comes back to a passing state.\n. @nwangtw \noh yeah..  Will fix.\n. No it is not.. I will remove.. If we did that here I think it would make sense to do everywhere in the project to be consistent.  . ",
    "sreev": "prefixing 'sudo' make build successful for both python 2 and python 3.\nclosing the issue.\nwill update the documentation.. @kramasamy recollected that env_exec.sh file is removed.  commenting out the check for it in the script file. i ran into the python3 incompatability error.\n$ ./scripts/setup-intellij.sh\nchanging to /Users/svaddi/SreeVaddi/source/github/sreev/heron/scripts/..\nINFO: Analysed 690 targets (14 packages loaded).\nINFO: Found 690 targets...\nERROR: /Users/svaddi/SreeVaddi/source/github/sreev/heron/tools/rules/pex/BUILD:48:1: Bootstrapping pex //tools/rules/pex:pex_wrapper failed (Exit 100)\nERROR: The executable /var/folders/40/_cbf9y0936x52h1fk8tlvrlm0000gn/T/pex.XXXXX.LhCeznXS/venv/bin/python is not functioning\nERROR: It thinks sys.prefix is '/private/var/tmp/_bazel_svaddi/d87af79cadec1a9b05b7625062a16274/execroot/main' (should be '/var/folders/40/_cbf9y0936x52h1fk8tlvrlm0000gn/T/pex.XXXXX.LhCeznXS/venv')\nERROR: virtualenv is not compatible with this system or executable\ndyld: Library not loaded: @rpath/libpython3.6m.dylib\n  Referenced from: /var/folders/40/_cbf9y0936x52h1fk8tlvrlm0000gn/T/pex.XXXXX.LhCeznXS/venv/bin/python\n  Reason: image not found\nINFO: Elapsed time: 3.915s, Critical Path: 1.84s\nFAILED: Build did NOT complete successfully\nWARNING!!! - bazel build failed - intellij setup may not be consistent\nPath is /Users/svaddi/SreeVaddi/source/github/sreev/heron\nGenerating IDEA project...\nfind: contrib: No such file or directory\nDone. IDEA module file: heron.iml\nCould not locate IntelliJ IDEA. Manually open /Users/svaddi/SreeVaddi/source/github/sreev/heron as a project in IDEA.\n. Commenting out the check for file 'env_exec.sh' and using 'sudo', makes the setup successfully.\nProject opened in intellij successfully.\nWill update the documentation, later.. \"-XX:+UseConcMarkSweepGC\" is removed in JDK 9, a humble FYI!. Troubleshoot:\n\n$ cat $(bazel info output_base)/external/local_config_xcode/BUILD\npackage(default_visibility = ['//visibility:public'])\nxcode_config(name = 'host_xcodes')\nError: Running xcodebuild -version failed, return code 72, stderr: xcrun: error: unable to find utility \"xcodebuild\", not a developer tool or in PATH , stdout:\n. Fix:\n$ bazel clean --expunge\n$ sudo xcode-select -s /Applications/Xcode.app/Contents/Developer\n$ sudo xcodebuild -license \n$ bazel clean --expunge\n\n\nBuild, again:\n\n$ ./bazel_configure.py\n$ bazel build --config=darwin heron/...\n. Add to Documentation, before closing the issue.. > (06:17:20) ERROR: /home/travis/build/apache/incubator-heron/heron/statemgrs/src/java/BUILD:82:1: Executing extra_action //tools/java:checkstyle_java on //heron/statemgrs/src/java:zookeeper-statemgr-unshaded failed (Exit 2): bash failed: error executing command \n  (cd /home/travis/.cache/bazel/_bazel_travis/be6dac4936703c7eedcb4f5cf38cdd65/execroot/org_apache_heron && \\\n  exec env - \\\n    PATH=/usr/bin:/opt/pyenv/libexec:/opt/pyenv/plugins/python-build/bin:/usr/lib/jvm/java-8-oracle/bin:/home/travis/bin:/home/travis/.local/bin:/opt/pyenv/shims:/home/travis/.phpenv/shims:/home/travis/perl5/perlbrew/bin:/home/travis/.nvm/versions/node/v8.9.1/bin:/home/travis/.kiex/elixirs/elixir-1.4.5/bin:/home/travis/.kiex/bin:/home/travis/.rvm/gems/ruby-2.4.1/bin:/home/travis/.rvm/gems/ruby-2.4.1@global/bin:/home/travis/.rvm/rubies/ruby-2.4.1/bin:/home/travis/gopath/bin:/home/travis/.gimme/versions/go1.7.4.linux.amd64/bin:/usr/local/phantomjs/bin:/usr/local/phantomjs:/usr/local/neo4j-3.2.7/bin:/usr/local/maven-3.5.2/bin:/usr/local/cmake-3.9.2/bin:/usr/local/clang-5.0.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/travis/.rvm/bin:/home/travis/.phpenv/bin:/opt/pyenv/bin:/home/travis/.yarn/bin \\\n  /bin/bash -c 'bazel-out/host/bin/tools/java/src/org/apache/bazel/checkstyle/checkstyle_java --extra_action_file bazel-out/k8-fastbuild/extra_actions/tools/java/checkstyle_java/heron/statemgrs/src/java/91e041116f510e1abfa9263b60052f0b.xa --heron_checkstyle_config_file tools/java/src/org/apache/bazel/checkstyle/heron_coding_style.xml --apache_checkstyle_config_file tools/java/src/org/apache/bazel/checkstyle/apache_coding_style.xml')\nStarting audit...\n[ERROR] /home/travis/.cache/bazel/_bazel_travis/be6dac4936703c7eedcb4f5cf38cdd65/execroot/org_apache_heron/heron/statemgrs/src/java/org/apache/heron/statemgr/zookeeper/ZkWatcherCallback.java:20: Wrong order for 'org.apache.heron.spi.statemgr.WatchCallback' import. Order should be: java, javax, scala, com, net, org, org.apache. Each group should be separated by a single blank line. Static imports at the bottom. [ImportOrder]\n[ERROR] /home/travis/.cache/bazel/_bazel_travis/be6dac4936703c7eedcb4f5cf38cdd65/execroot/org_apache_heron/heron/statemgrs/src/java/org/apache/heron/statemgr/zookeeper/curator/CuratorStateManager.java:41: Wrong order for 'org.apache.heron.api.generated.TopologyAPI' import. Order should be: java, javax, scala, com, net, org, org.apache. Each group should be separated by a single blank line. Static imports at the bottom. [ImportOrder]\nAudit done.\nCheckstyle ends with 2 errors.. > (08:08:08) ERROR: /home/travis/build/apache/incubator-heron/heron/uploaders/tests/java/BUILD:58:1: Executing extra_action //tools/java:checkstyle_java on //heron/uploaders/tests/java:DlogUploaderTest failed (Exit 1): bash failed: error executing command \n  (cd /home/travis/.cache/bazel/_bazel_travis/be6dac4936703c7eedcb4f5cf38cdd65/execroot/org_apache_heron && \\\n  exec env - \\\n    PATH=/usr/bin:/opt/pyenv/libexec:/opt/pyenv/plugins/python-build/bin:/usr/lib/jvm/java-8-oracle/bin:/home/travis/bin:/home/travis/.local/bin:/opt/pyenv/shims:/home/travis/.phpenv/shims:/home/travis/perl5/perlbrew/bin:/home/travis/.nvm/versions/node/v8.9.1/bin:/home/travis/.kiex/elixirs/elixir-1.4.5/bin:/home/travis/.kiex/bin:/home/travis/.rvm/gems/ruby-2.4.1/bin:/home/travis/.rvm/gems/ruby-2.4.1@global/bin:/home/travis/.rvm/rubies/ruby-2.4.1/bin:/home/travis/gopath/bin:/home/travis/.gimme/versions/go1.7.4.linux.amd64/bin:/usr/local/phantomjs/bin:/usr/local/phantomjs:/usr/local/neo4j-3.2.7/bin:/usr/local/maven-3.5.2/bin:/usr/local/cmake-3.9.2/bin:/usr/local/clang-5.0.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/travis/.rvm/bin:/home/travis/.phpenv/bin:/opt/pyenv/bin:/home/travis/.yarn/bin \\\n  /bin/bash -c 'bazel-out/host/bin/tools/java/src/org/apache/bazel/checkstyle/checkstyle_java --extra_action_file bazel-out/k8-fastbuild/extra_actions/tools/java/checkstyle_java/heron/uploaders/tests/java/eff0462a84339f65c80ebc1ef599c9b6.xa --heron_checkstyle_config_file tools/java/src/org/apache/bazel/checkstyle/heron_coding_style.xml --apache_checkstyle_config_file tools/java/src/org/apache/bazel/checkstyle/apache_coding_style.xml')\nStarting audit...\n[ERROR] /home/travis/.cache/bazel/_bazel_travis/be6dac4936703c7eedcb4f5cf38cdd65/execroot/org_apache_heron/heron/uploaders/tests/java/org/apache/heron/uploader/dlog/DlogUploaderTest.java:25: Wrong order for 'org.junit.Before' import. Order should be: java, javax, scala, com, net, org, org.apache. Each group should be separated by a single blank line. Static imports at the bottom. [ImportOrder]\nAudit done.\nCheckstyle ends with 1 errors.\n. > [ERROR] /home/travis/.cache/bazel/_bazel_travis/be6dac4936703c7eedcb4f5cf38cdd65/execroot/org_apache_heron/heron/uploaders/tests/java/org/apache/heron/uploader/dlog/DlogUploaderTest.java:28: Wrong order for 'org.junit.Before' import. Order should be: java, javax, scala, com, net, org, org.apache. Each group should be separated by a single blank line. Static imports at the bottom. [ImportOrder]\n[ERROR] /home/travis/.cache/bazel/_bazel_travis/be6dac4936703c7eedcb4f5cf38cdd65/execroot/org_apache_heron/heron/uploaders/tests/java/org/apache/heron/uploader/dlog/DlogUploaderTest.java:36: Using the '.' form of import should be avoided - org.mockito.Mockito.. [AvoidStarImport]\nAudit done.\nCheckstyle ends with 2 errors.\n(08:38:29) INFO: Elapsed time: 1205.889s, Critical Path: 425.19s\n(08:38:29) FAILED: Build did NOT complete successfully. > Making install in doc\nmake[1]: Entering directory /home/travis/.cache/bazel/_bazel_travis/be6dac4936703c7eedcb4f5cf38cdd65/execroot/org_apache_heron/tmp244_1e818feb61dcc793/libunwind.oQ5wx/doc'\nlatex2man -t ./libunwind.trans unw_get_accessors.tex unw_get_accessors.man\nmake[1]: Leaving directory/home/travis/.cache/bazel/_bazel_travis/be6dac4936703c7eedcb4f5cf38cdd65/execroot/org_apache_heron/tmp244_1e818feb61dcc793/libunwind.oQ5wx/doc'\nptrace/_UPT_get_dyn_info_list_addr.c: In function 'get_list_addr':\nptrace/_UPT_get_dyn_info_list_addr.c:78:3: warning: #warning Implement get_list_addr(), please. [-Wcpp]\n  warning Implement get_list_addr(), please.\n   ^\ncoredump/_UPT_get_dyn_info_list_addr.c: In function 'get_list_addr':\ncoredump/_UPT_get_dyn_info_list_addr.c:81:3: warning: #warning Implement get_list_addr(), please. [-Wcpp]\n  warning Implement get_list_addr(), please.\n   ^\n/bin/bash: latex2man: command not found\nmake[1]:  [unw_get_accessors.man] Error 127\nmake:  [install-recursive] Error 1\n(18:33:55) INFO: Elapsed time: 48.979s, Critical Path: 20.68s\n(18:33:55) FAILED: Build did NOT complete successfully\n(22:13:51) ERROR: /home/travis/build/apache/incubator-heron/heron/schedulers/src/java/BUILD:164:1: Executing extra_action //tools/java:checkstyle_java on //heron/schedulers/src/java:yarn-scheduler-unshaded failed (Exit 5): bash failed: error executing command \n  (cd /home/travis/.cache/bazel/_bazel_travis/be6dac4936703c7eedcb4f5cf38cdd65/execroot/org_apache_heron && \\\n  exec env - \\\n    PATH=/usr/bin:/opt/pyenv/libexec:/opt/pyenv/plugins/python-build/bin:/usr/lib/jvm/java-8-oracle/bin:/home/travis/bin:/home/travis/.local/bin:/opt/pyenv/shims:/home/travis/.phpenv/shims:/home/travis/perl5/perlbrew/bin:/home/travis/.nvm/versions/node/v8.9.1/bin:/home/travis/.kiex/elixirs/elixir-1.4.5/bin:/home/travis/.kiex/bin:/home/travis/.rvm/gems/ruby-2.4.1/bin:/home/travis/.rvm/gems/ruby-2.4.1@global/bin:/home/travis/.rvm/rubies/ruby-2.4.1/bin:/home/travis/gopath/bin:/home/travis/.gimme/versions/go1.7.4.linux.amd64/bin:/usr/local/phantomjs/bin:/usr/local/phantomjs:/usr/local/neo4j-3.2.7/bin:/usr/local/maven-3.5.2/bin:/usr/local/cmake-3.9.2/bin:/usr/local/clang-5.0.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/travis/.rvm/bin:/home/travis/.phpenv/bin:/opt/pyenv/bin:/home/travis/.yarn/bin \\\n  /bin/bash -c 'bazel-out/host/bin/tools/java/src/org/apache/bazel/checkstyle/checkstyle_java --extra_action_file bazel-out/k8-fastbuild/extra_actions/tools/java/checkstyle_java/heron/schedulers/src/java/cf5cd75c3c2d02b81710a6837bdac016.xa --heron_checkstyle_config_file tools/java/src/org/apache/bazel/checkstyle/heron_coding_style.xml --apache_checkstyle_config_file tools/java/src/org/apache/bazel/checkstyle/apache_coding_style.xml')\nStarting audit...\n[ERROR] /home/travis/.cache/bazel/_bazel_travis/be6dac4936703c7eedcb4f5cf38cdd65/execroot/org_apache_heron/heron/schedulers/src/java/org/apache/heron/scheduler/yarn/HeronDriverConfiguration.java:24: Wrong order for 'org.apache.heron.scheduler.yarn.HeronConfigurationOptions.Cluster' import. Order should be: java, javax, scala, com, net, org, org.apache. Each group should be separated by a single blank line. Static imports at the bottom. [ImportOrder]\n[ERROR] /home/travis/.cache/bazel/_bazel_travis/be6dac4936703c7eedcb4f5cf38cdd65/execroot/org_apache_heron/heron/schedulers/src/java/org/apache/heron/scheduler/yarn/HeronExecutorTask.java:33: Wrong order for 'org.apache.heron.api.exception.InvalidTopologyException' import. Order should be: java, javax, scala, com, net, org, org.apache. Each group should be separated by a single blank line. Static imports at the bottom. [ImportOrder]\n[ERROR] /home/travis/.cache/bazel/_bazel_travis/be6dac4936703c7eedcb4f5cf38cdd65/execroot/org_apache_heron/heron/schedulers/src/java/org/apache/heron/scheduler/yarn/HeronMasterDriver.java:56: Wrong order for 'org.apache.heron.api.exception.InvalidTopologyException' import. Order should be: java, javax, scala, com, net, org, org.apache. Each group should be separated by a single blank line. Static imports at the bottom. [ImportOrder]\n[ERROR] /home/travis/.cache/bazel/_bazel_travis/be6dac4936703c7eedcb4f5cf38cdd65/execroot/org_apache_heron/heron/schedulers/src/java/org/apache/heron/scheduler/yarn/HeronTaskConfiguration.java:23: Wrong order for 'org.apache.heron.scheduler.yarn.HeronConfigurationOptions.Cluster' import. Order should be: java, javax, scala, com, net, org, org.apache. Each group should be separated by a single blank line. Static imports at the bottom. [ImportOrder]\n[ERROR] /home/travis/.cache/bazel/_bazel_travis/be6dac4936703c7eedcb4f5cf38cdd65/execroot/org_apache_heron/heron/schedulers/src/java/org/apache/heron/scheduler/yarn/YarnLauncher.java:37: Wrong order for 'org.apache.heron.scheduler.yarn.HeronMasterDriver.ContainerAllocationHandler' import. Order should be: java, javax, scala, com, net, org, org.apache. Each group should be separated by a single blank line. Static imports at the bottom. [ImportOrder]\nAudit done.\nCheckstyle ends with 5 errors.\n(22:13:51) INFO: Elapsed time: 1304.852s, Critical Path: 361.65s\n(22:13:51) FAILED: Build did NOT complete successfully\n. redoing the rename from the beginning.. i am closing this PR, now and will reopen a brand new, later, soon.\n. this change went into these three commits along with other changes.\n\nhttps://github.com/apache/incubator-heron/pull/2840/commits/33dcdcc83bd686a44d01bc24f71d8841e1cbbca9\nhttps://github.com/apache/incubator-heron/pull/2840/commits/c39289fe7abf43f38e0993aac8db3a677cf2e88e\nhttps://github.com/apache/incubator-heron/pull/2840/commits/dca2e07c8d553a16384a657711117fc5ee34e751\nreduces build time by 5.31 s. But reverted back to install.sh script, for cross OS compatibility.\nWill visit this another time.\nNo changes.. @erenavsarogullari and @kramasamy , would you please look in to the travis error.\n\n[2018-04-08 04:38:59 +0000] [INFO]: Using cluster definition in /home/travis/.heron/conf/local\nError: Could not find or load main class org.apache.heron.integration_test.topology.scala_streamlet_with_filter_and_transform.ScalaStreamletWithFilterAndTransform\n[2018-04-08 04:38:59 +0000] [ERROR]: Failed to create topology definition file when executing class 'org.apache.heron.integration_test.topology.scala_streamlet_with_filter_and_transform.ScalaStreamletWithFilterAndTransform' of file '/home/travis/.herontests/lib/scala-integration-tests.jar'\n[2018-04-08 04:38:59 +0000] [ERROR]: Unable to submit the topology\n[2018-04-08 04:38:59 +0000] [ERROR]: Failed to submit 20180408043859_IntegrationTest_ScalaStreamletWithFilterAndTransform_c0e1aa5e-481f-4645-b7ab-9519fb34f497 topology :: Traceback (most recent call last):\n  File \"integration_test/src/python/test_runner/main.py\", line 166, in run_test\n    params.release_package_uri, args)\n  File \"integration_test/src/python/test_runner/main.py\", line 251, in submit_topology\n    raise status.TestFailure(\"Unable to submit the topology\")\nTestFailure: ('Unable to submit the topology', None)\n[2018-04-08 04:38:59 +0000] [ERROR]: FAILURE: 1/1 tests failed:\n[2018-04-08 04:38:59 +0000] [ERROR]:   - [0s]    : 20180408043859_IntegrationTest_ScalaStreamletWithFilterAndTransform_c0e1aa5e-481f-4645-b7ab-9519fb34f497. Thank you for looking into it and for detailed fixes, @erenavsarogullari .\nI have incorporated all those changes.. @ashvina \ndhalion:\n1. checkin metrics folder in microsoft/dhalion repo.\n2. update the version tag from 0.2.1 to 0.2.2.\n3. publish in maven repo.\n\nheron:\n1. update version tag in file WORKSPACE.\n2. test locally.\n3. create another pull request and coordinate so it merged into master.\nI am blocked until then.. Thank you for merging, @nlu90.. please pay attention to this file when merging and discard this version of this file.\nheron/healthmgr/src/java/org/apache/heron/healthmgr/HealthManagerMetrics.java. 'asf-site' branch should exists in apache.org/incubator-heron.. @kramasamy, removed the merge conflict file.. i will create another pull request with more cleaner commit.\n. On macOS:\n$ bazel build --config=darwin heron/...\nERROR: /Users/svaddi/SreeVaddi/source/github/sreev/incubator-heron/heron/api/src/java/BUILD:8:1: every rule of type java_doc implicitly depends upon the target '@local_jdk//:jdk-default', but this target could not be found because of: no such target '@local_jdk//:jdk-default': target 'jdk-default' not declared in package '' (did you mean 'jre-default'?) defined by /private/var/tmp/_bazel_svaddi/ad590ad4eafb86a9912ced0bf3547c8f/external/local_jdk/BUILD.bazel\nERROR: Analysis of target '//heron/api/src/java:heron-api-javadoc' failed; build aborted: Analysis failed\nINFO: Elapsed time: 11.656s\nINFO: 0 processes.\nFAILED: Build did NOT complete successfully (291 packages loaded). attached the patch:\nissue-3045.patch.txt\n. attached the patch:\nissue-3046.patch.txt\n. attached the patch:\nissue-3047.patch.txt\n. ```\n$ brew uninstall --force bazel\nUninstalling bazel... (48 files, 402.5MB)\n$ brew tap bazelbuild/tap\nUpdating Homebrew...\n==> Auto-updated Homebrew!\nUpdated 2 taps (homebrew/core and homebrew/cask).\n==> Tapping bazelbuild/tap\nCloning into '/usr/local/Homebrew/Library/Taps/bazelbuild/homebrew-tap'...\nremote: Enumerating objects: 9, done.\nremote: Counting objects: 100% (9/9), done.\nremote: Compressing objects: 100% (9/9), done.\nremote: Total 9 (delta 0), reused 4 (delta 0), pack-reused 0\nUnpacking objects: 100% (9/9), done.\nTapped 2 formulae (38 files, 47.0KB).\n$ brew tap-pin bazelbuild/tap\n==> Pinned bazelbuild/tap\n$ brew install bazel\nTo restore the stashed changes to /usr/local/Homebrew run:\n  'cd /usr/local/Homebrew && git stash pop'\n==> Downloading https://homebrew.bintray.com/bottles/bazel-0.18.1.mojave.bottle.tar.gz\n################################################################## 100.0%\n==> Pouring bazel-0.18.1.mojave.bottle.tar.gz\n==> Caveats\nBash completion has been installed to:\n  /usr/local/etc/bash_completion.d\nzsh completions have been installed to:\n  /usr/local/share/zsh/site-functions\n==> Summary\n\ud83c\udf7a  /usr/local/Cellar/bazel/0.18.1: 12 files, 117.4MB\n$ bazel version\nWARNING: Processed legacy workspace file incubator-heron/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\nExtracting Bazel installation...\nStarting local Bazel server and connecting to it...\nBuild label: 0.18.1-homebrew\nBuild target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Nov 2 11:16:42 2018 (1541157402)\nBuild timestamp: 1541157402\nBuild timestamp as int: 1541157402\n$ brew upgrade bazel\nError: bazel 0.18.1 already installed\n$ bazel clean --expunge\nWARNING: Processed legacy workspace file /Users/svaddi/SreeVaddi/source/github/sreev/incubator-heron/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\nINFO: Starting clean.\n$ ./bazel_configure.py\nPlatform Darwin\nUsing C compiler          : /usr/bin/clang (10.0.0)\nUsing C++ compiler        : /usr/bin/clang++ (10.0.0)\nUsing C preprocessor      : /usr/bin/cpp (10.0.0)\nUsing C++ preprocessor    : /usr/bin/cpp (10.0.0)\nUsing linker              : /usr/bin/ld\nUsing JDK                 : /Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home\nUsing Automake            : /usr/local/Cellar/automake/1.16.1_1/bin/automake (1.16.1)\nUsing Autoconf            : /usr/local/Cellar/autoconf/2.69/bin/autoconf (2.69)\npackage(default_visibility = [\"//visibility:public\"])\nUsing Make                : /usr/bin/make (3.81)\nUsing Python              : /usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/bin/python2.7 (2.7.15)\nCopyright (C) 2016 The Android Open Source Project\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\nJavadoc rule.\ndef _impl(ctx):\n  zip_output = ctx.outputs.zip\n  transitive_jar_set = depset()\n  source_jars = depset()\n  for l in ctx.attr.libs:\n    source_jars += l.java.source_jars\n    transitive_jar_set += l.java.transitive_deps\n  transitive_jar_paths = [j.path for j in transitive_jar_set]\n  dir = ctx.outputs.zip.path + \".dir\"\n  source = ctx.outputs.zip.path + \".source\"\n  external_docs = [\"http://docs.oracle.com/javase/8/docs/api\"] + ctx.attr.external_docs\n  cmd = [\n      \"rm -rf %s\" % source,\n      \"mkdir %s\" % source,\n      \" && \".join([\"unzip -qud %s %s\" % (source, j.path) for j in source_jars]),\n      \"rm -rf %s\" % dir,\n      \"mkdir %s\" % dir,\n      \" \".join([\n        ctx.file._javadoc.path,\n        \"-Xdoclint:-missing\",\n        \"-protected\",\n        \"-encoding UTF-8\",\n        \"-charset UTF-8\",\n        \"-notimestamp\",\nCopyright (C) 2016 The Android Open Source Project\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\nJavadoc rule.\ndef _impl(ctx):\n  zip_output = ctx.outputs.zip\n  transitive_jar_set = depset()\n  source_jars = depset()\n  for l in ctx.attr.libs:\n    source_jars += l.java.source_jars\n    transitive_jar_set += l.java.transitive_deps\n  transitive_jar_paths = [j.path for j in transitive_jar_set]\n  dir = ctx.outputs.zip.path + \".dir\"\n  source = ctx.outputs.zip.path + \".source\"\n  external_docs = [\"http://docs.oracle.com/javase/8/docs/api\"] + ctx.attr.external_docs\n  cmd = [\n      \"rm -rf %s\" % source,\n      \"mkdir %s\" % source,\n      \" && \".join([\"unzip -qud %s %s\" % (source, j.path) for j in source_jars]),\n      \"rm -rf %s\" % dir,\n      \"mkdir %s\" % dir,\n      \" \".join([\n        ctx.file._javadoc.path,\n        \"-Xdoclint:-missing\",\n        \"-protected\",\n        \"-encoding UTF-8\",\n        \"-charset UTF-8\",\n        \"-notimestamp\",\nUsing Libtool             : /usr/local/Cellar/libtool/2.4.6_1/bin/glibtool (2.4.6)\nCopyright (C) 2016 The Android Open Source Project\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\nJavadoc rule.\ndef _impl(ctx):\n  zip_output = ctx.outputs.zip\n  transitive_jar_set = depset()\n  source_jars = depset()\n  for l in ctx.attr.libs:\n    source_jars += l.java.source_jars\n    transitive_jar_set += l.java.transitive_deps\n  transitive_jar_paths = [j.path for j in transitive_jar_set]\n  dir = ctx.outputs.zip.path + \".dir\"\n  source = ctx.outputs.zip.path + \".source\"\n  external_docs = [\"http://docs.oracle.com/javase/8/docs/api\"] + ctx.attr.external_docs\n  cmd = [\n      \"rm -rf %s\" % source,\n      \"mkdir %s\" % source,\n      \" && \".join([\"unzip -qud %s %s\" % (source, j.path) for j in source_jars]),\n      \"rm -rf %s\" % dir,\n      \"mkdir %s\" % dir,\n      \" \".join([\n        ctx.file._javadoc.path,\n        \"-Xdoclint:-missing\",\n\"tools/rules/javadoc.bzl\" [noeol] 74L, 2593C\n        ctx.file._javadoc.path,\n        \"-Xdoclint:-missing\",\n        \"-protected\",\n        \"-encoding UTF-8\",\n        \"-charset UTF-8\",\n        \"-notimestamp\",\n        \"-quiet\",\n        \"-windowtitle '%s'\" % ctx.attr.title,\n        \" \".join(['-link %s' % url for url in external_docs]),\n        \"-sourcepath %s\" % source,\n        \"-subpackages \",\n        \":\".join(ctx.attr.pkgs),\n        \" -classpath \",\n        \":\".join(transitive_jar_paths),\n        \"-d %s\" % dir]),\n    \"find %s -exec touch -t 198001010000 '{}' ';'\" % dir,\n    \"(cd %s && zip -qr ../%s *)\" % (dir, ctx.outputs.zip.basename),\n  ]\n  ctx.action(\n      inputs = list(transitive_jar_set) + list(source_jars) + ctx.files._jdk,\n      outputs = [zip_output],\n      command = \" && \".join(cmd))\njava_doc = rule(\n    attrs = {\n        \"libs\": attr.label_list(allow_files = False),\n        \"pkgs\": attr.string_list(),\n        \"title\": attr.string(),\n        \"external_docs\": attr.string_list(),\n        \"_javadoc\": attr.label(\n            default = Label(\"@local_jdk//:bin/javadoc\"),\n            single_file = True,\n            allow_files = True,\n        ),\n        \"_jdk\": attr.label(\n            default = Label(\"@local_jdk//:jdk-default\"),\n        \"_javadoc\": attr.label(\nUsing archiver            : /usr/bin/ar\nUsing coverage tool       : /usr/bin/gcov\ndwp                       : not found, but ok\nUsing nm                  : /usr/bin/nm\nobjcopy                   : not found, but ok\nUsing objdump             : /usr/bin/objdump\nUsing strip               : /usr/bin/strip\n$ bazel build --config=darwin heron/...\nWARNING: Processed legacy workspace file /Users/svaddi/SreeVaddi/source/github/sreev/incubator-heron/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\nStarting local Bazel server and connecting to it...\nINFO: SHA256 (https://pypi.python.org/packages/c9/1d/bd19e691fd4cfe908c76c429fe6e4436c9e83583c4414b54f6c85471954a/wheel-0.29.0.tar.gz) = 1ebb8ad7e26b448e9caa4773d2357849bf80ff9e313964bcaf79cbf0201a1648\nINFO: SHA256 (https://pypi.python.org/packages/d9/03/155b3e67fe35fe5b6f4227a8d9e96a14fda828b18199800d161bcefc1359/requests-2.12.3.tar.gz) = de5d266953875e9647e37ef7bfe6ef1a46ff8ddfe61b5b3652edf7ea717ee2b2\nINFO: SHA256 (https://pypi.python.org/packages/3a/1d/cd41cd3765b78a4353bbf27d18b099f7afbcd13e7f2dc9520f304ec8981c/pex-1.2.15.tar.gz) = 0147d19123340677b9793b00ec86fe65b6697db3ec99afb796da2300ae5fec14\nINFO: SHA256 (https://pypi.python.org/packages/68/13/1bfbfbd86560e61fa9803d241084fff41a775bf56ee8b3ad72fc9e550dad/setuptools-31.0.0.tar.gz) = 0818cc0de692c3a5c83ca83aa7ec7ba6bc206f278735f1e0267b8d0e095cfe7a\nINFO: SHA256 (https://github.com/jbeder/yaml-cpp/archive/yaml-cpp-0.6.2.tar.gz) = e4d8560e163c3d875fd5d9e5542b5fd5bec810febdcba61481fe5fc4e6b1fd05\nINFO: SHA256 (https://pypi.python.org/packages/53/67/9620edf7803ab867b175e4fd23c7b8bd8eba11cb761514dcd2e726ef07da/py-1.4.34-py2.py3-none-any.whl) = 2ccb79b01769d99115aa600d7eed99f524bf752bba8f041dc1c184853514655a\nINFO: SHA256 (https://pypi.python.org/packages/fd/3e/d326a05d083481746a769fc051ae8d25f574ef140ad4fe7f809a2b63c0f0/pytest-3.1.3-py2.py3-none-any.whl) = 2a4f483468954621fcc8f74784f3b42531e5b5008d49fc609b37bc4dbc6dead1\nERROR: /Users/svaddi/SreeVaddi/source/github/sreev/incubator-heron/heron/spi/src/java/BUILD:6:1: every rule of type java_doc implicitly depends upon the target '@local_jdk//:jdk-default', but this target could not be found because of: no such target '@local_jdk//:jdk-default': target 'jdk-default' not declared in package '' (did you mean 'jre-default'?) defined by /private/var/tmp/_bazel_svaddi/ad590ad4eafb86a9912ced0bf3547c8f/external/local_jdk/BUILD.bazel\nERROR: Analysis of target '//heron/spi/src/java:heron-spi-javadoc' failed; build aborted: Analysis failed\nINFO: Elapsed time: 47.900s\nINFO: 0 processes.\nFAILED: Build did NOT complete successfully (186 packages loaded)\n$ vi tools/rules/javadoc.bzl\nreplace:\ndefault = Label(\"@local_jdk//:jdk-default\"),\nwith:\ndefault = Label(\"@local_jdk//:bin/javadoc\"),\n& save\n$ bazel clean --expunge\nWARNING: Processed legacy workspace file /Users/svaddi/SreeVaddi/source/github/sreev/incubator-heron/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\nINFO: Starting clean.\n$ ./bazel_configure.py\nPlatform Darwin\nUsing C compiler          : /usr/bin/clang (10.0.0)\nUsing C++ compiler        : /usr/bin/clang++ (10.0.0)\nUsing C preprocessor      : /usr/bin/cpp (10.0.0)\nUsing C++ preprocessor    : /usr/bin/cpp (10.0.0)\nUsing linker              : /usr/bin/ld\nUsing JDK                 : /Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home\nUsing Automake            : /usr/local/Cellar/automake/1.16.1_1/bin/automake (1.16.1)\nUsing Autoconf            : /usr/local/Cellar/autoconf/2.69/bin/autoconf (2.69)\nUsing Make                : /usr/bin/make (3.81)\nUsing Python              : /usr/local/Cellar/python@2/2.7.15_1/Frameworks/Python.framework/Versions/2.7/bin/python2.7 (2.7.15)\nUsing Libtool             : /usr/local/Cellar/libtool/2.4.6_1/bin/glibtool (2.4.6)\nUsing archiver            : /usr/bin/ar\nUsing coverage tool       : /usr/bin/gcov\ndwp                       : not found, but ok\nUsing nm                  : /usr/bin/nm\nobjcopy                   : not found, but ok\nUsing objdump             : /usr/bin/objdump\nUsing strip               : /usr/bin/strip\n$ bazel build --config=darwin heron/...\n...\n...\n...\nNov 18, 2018 11:50:54 PM org.apache.bazel.checkstyle.PythonCheckstyle main\nINFO: 98 python files found by checkstyle\nINFO: Elapsed time: 954.644s, Critical Path: 233.01s\nINFO: 2252 processes: 1921 local, 331 worker.\nINFO: Build completed successfully, 3926 total actions\n. build successful on macOS Mojave Version 10.14, java version \"1.8.0_181\".\n.\nadded new file %WORKSPACE%/.bazelrc\nwith one line:\nimport tools/bazel.rc\n$ bazel build --config=darwin heron/...\nWARNING: Duplicate rc file: /Users/svaddi/SreeVaddi/source/github/sreev/incubator-heron/tools/bazel.rc is read multiple times, most recently imported from /Users/svaddi/SreeVaddi/source/github/sreev/incubator-heron/.bazelrc\nWARNING: Processed legacy workspace file /Users/svaddi/SreeVaddi/source/github/sreev/incubator-heron/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.\nStarting local Bazel server and connecting to it...\n...\n...\n...\nINFO: From Executing extra_action //tools/python:checkstyle_python on //heron/tools/tracker/tests/python:query_operator_unittest_runner:\nNov 19, 2018 12:27:50 AM org.apache.bazel.checkstyle.PythonCheckstyle main\nINFO: 85 python files found by checkstyle\nINFO: Elapsed time: 946.638s, Critical Path: 179.46s\nINFO: 2252 processes: 1921 local, 331 worker.\nINFO: Build completed successfully, 3926 total actions\n```. i want to keep the any other changes (including copyright that you suggested) separate from package renaming.\nalso, i am waiting to hear back from Mentors, if a long single would be convenient or the current way.. it came in from merge conflict.\nCONFLICT (rename/delete): heron/healthmgr/src/java/com/twitter/heron/healthmgr/HealthManagerMetrics.java deleted in f444372de2749c8a6953ff98a5178550ec026cd8 and renamed to heron/healthmgr/src/java/org/apache/heron/healthmgr/HealthManagerMetrics.java in HEAD. Version HEAD of heron/healthmgr/src/java/org/apache/heron/healthmgr/HealthManagerMetrics.java left in tree.\nDeleted merge conflict for 'heron/healthmgr/src/java/org/apache/heron/healthmgr/HealthManagerMetrics.java':\n  {local}: created file\n  {remote}: deleted\nUse (c)reated or (d)eleted file, or (a)bort? d\n. this file many not be needed with that content as .htaccess is enough. but using it as a place holder.. ",
    "wxl24life": "@kramasamy sorry, 404 error found when opening your link?. @kramasamy Thanks. I have tested it. After removing the headingif block, ./scripts/setup-intellij.sh can run smoothly. I think for history reason, as was noted in PR #1429 , that headingif block was added to check the existence of env_exec.sh. But for now, the env_exec.sh file is not needed anymore. I will close this thread and send a new PR;-). ",
    "bornej": "Hi, \nwhile investigating the possibility of using multi-threaded bolt/spouts, I stumbled upon this PR.\nI don't know if this is the appropriate place to ask but I'm stuck with some questions...\nFrom what I understand, in each Heron instance there is only one slave thread doing bolt/spout computation (and a gateway thread used for communications with the stream manager). But after reading this discussion I'm a bit confused\n@jerrypeng \n\nWe have some use cases in which users will want to emit and ack or fail in different threads.\n\n@srkukarni \n\nUsers can(and do) have concurrent threads today. \n... imagine this situation\n1. User Bolt has two 'worker' threads that actually do some work and use some kind of queue to put \ntheir acks/fails in the main process thread.\n\nIs it currently possible for a user to create a multi-threaded Bolt/spout? (Without altering HeronInstance implementation-> FixedThreadPool size) \nIf so, could someone help me figure out how to achieve that?\nI'm very new to Heron so I hope you will forgive me if my question seems stupid.\nI understand that Heron philosophy is to use \"single\"-threaded Instances (bolt/spouts) and why this design choice was made but I'm wondering if some computation tasks could benefit from multi-threading...\nThanks for your help and your work.\nBest Regards.\nJonathan.\n. Thank you very much,\nIs there any example of multi-threaded heron bolt/spout/topology out there?\n. @jerrypeng Thank you so much!\nI will do experiments with heron multithreading in the next weeks. I hope to have some results to share soon.\n . ",
    "asudhindra": "@kramasamy Done. Please review when you get the chance. Thanks!. ",
    "spmason": "I just tried following these docs.  Heron doesn't seem to be available on homebrew by default - do you have to tap https://github.com/streamlio/homebrew-formulae first, as hinted to in https://github.com/twitter/heron/issues/2128#issuecomment-322059440? Or should it be available in the main homebrew repository at some point?. ",
    "Yaliang": "Just a general question: is there a test to test configuration settings?. Thanks @ajorgensen ! \nMerged! . Just pass you a document for glog: http://rpg.ifi.uzh.ch/docs/glog.html. ",
    "bjonnh": "Building thrift 0.11\u00a0and using the thrift binary produced in the \"compiler\" directory solved part of the issue. But the build fails later with /gen-java/org/apache/scribe/LogEntry.java:17: error: package org.apache.thrift.scheme does not exist  and thousands of errors like that.\nI'll continue investigating.. As I don't need Scribe, I'll just disable that (if I can find how to do that)\nEdit: Yes it builds, I removed all references to Scribe and thrift in the code and it builds perfectly and tests pass.. I propose a more extreme (but probably more viable long-term) solution in #2788 . Feel free to close that one if #2788 proposition gets accepted.. @kramasamy  I tried single-job and multi-job to rule that out. No change (just really really hard to find where the error was with multiple jobs). \nAlso I am compiling on x86, it is just that the thrift executable seg fault\u2026\nThanks for the PR!. Still fails. Again, I am on x86\u2026\n /bin/mkdir -p '/root/.cache/bazel/_bazel_root/b8d5998435f0e9226488fd32fcb079f8/execroot/main/bazel-out/k8-fastbuild/genfiles/third_party/zookeeper/bin'\n  /bin/bash ./libtool   --mode=install /usr/bin/install -c cli_st cli_mt load_gen '/root/.cache/bazel/_bazel_root/b8d5998435f0e9226488fd32fcb079f8/execroot/main/bazel-out/k8-fastbuild/genfiles/third_party/zookeeper/bin'\nlibtool: install: /usr/bin/install -c cli_st /root/.cache/bazel/_bazel_root/b8d5998435f0e9226488fd32fcb079f8/execroot/main/bazel-out/k8-fastbuild/genfiles/third_party/zookeeper/bin/cli_st\nlibtool: install: /usr/bin/install -c cli_mt /root/.cache/bazel/_bazel_root/b8d5998435f0e9226488fd32fcb079f8/execroot/main/bazel-out/k8-fastbuild/genfiles/third_party/zookeeper/bin/cli_mt\nlibtool: install: /usr/bin/install -c load_gen /root/.cache/bazel/_bazel_root/b8d5998435f0e9226488fd32fcb079f8/execroot/main/bazel-out/k8-fastbuild/genfiles/third_party/zookeeper/bin/load_gen\n /bin/mkdir -p '/root/.cache/bazel/_bazel_root/b8d5998435f0e9226488fd32fcb079f8/execroot/main/bazel-out/k8-fastbuild/genfiles/third_party/zookeeper/include/zookeeper'\n /usr/bin/install -c -m 644 include/zookeeper.h include/zookeeper_version.h include/zookeeper_log.h include/proto.h include/recordio.h generated/zookeeper.jute.h '/root/.cache/bazel/_bazel_root/b8d5998435f0e9226488fd32fcb079f8/execroot/main/bazel-out/k8-fastbuild/genfiles/third_party/zookeeper/include/zookeeper'\nmake[1]: Leaving directory '/tmp/zookeeper.Uy8RA/zookeeper-3.4.10/src/c'\nar: u' modifier ignored sinceD' is the default (see U')\nar:u' modifier ignored since D' is the default (seeU')\nar: u' modifier ignored sinceD' is the default (see U')\nar:u' modifier ignored since D' is the default (seeU')\nar: u' modifier ignored sinceD' is the default (see `U')\nERROR: /root/heron/heron/metricsmgr/src/thrift/BUILD:3:1: error executing shell command: 'set -e\nrm -rf bazel-out/k8-fastbuild/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs\nmkdir bazel-out/k8-fastbuild/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src...' failed (Segmentation fault): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\n/bin/bash: line 3: 26022 Segmentation fault      third_party/thrift/thrift-linux-x86_64.exe -r --gen java -o bazel-out/k8-fastbuild/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs heron/metricsmgr/src/thrift/scribe.thrift\nINFO: Elapsed time: 46.282s, Critical Path: 40.93s\nFAILED: Build did NOT complete successfully\nthe problem is \" third_party/thrift/thrift-linux-x86_64.exe\" that segfaults on my machine (even when running it manually).. Intel(R) Xeon(R) CPU           X5675  @ 3.07GHz 20 cores * 2\nDebian Testing - in Xen\nI've never had anything segfault before on that VM nor on its clone.\nif that may help, gdb trace: \n(gdb) run\nProgram received signal SIGSEGV, Segmentation fault.\n0xffffffffff600400 in ?? ()\n(gdb) where\n0  0xffffffffff600400 in ?? ()\n1  0x00000000005df87d in ?? ()\n2  0x00000000004178a3 in ?? ()\n3  0x00000000005a8440 in ?? ()\n4  0x00000000004001b9 in ?? ()\n5  0x00007fffffffebc8 in ?? ()\n6  0x0000000000000000 in ?? ()\nI tried in an ubuntu docker (to have a different glibc) same problem.\nMaybe it is an issue of that static build with the kernel I am using. \nI think I'll just comment out the build of scribe for now. But we may want to keep that as a reference somewhere if anybody else struggle with that.\nThanks.. Tried with latest master:\nbazel build --config=ubuntu heron/...\nINFO: Analysed 574 targets (288 packages loaded).\nINFO: Found 574 targets...\nERROR: /root/heron/heron/heron/metricsmgr/src/thrift/BUILD:3:1: error executing shell command: 'set -e\nrm -rf bazel-out/k8-fastbuild/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs\nmkdir bazel-out/k8-fastbuild/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src...' failed (Segmentation fault): bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)\n/bin/bash: line 3:  5879 Segmentation fault      third_party/thrift/thrift-linux-x86_64.exe -r --gen java -o bazel-out/k8-fastbuild/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs heron/metricsmgr/src/thrift/scribe.thrift\nINFO: Elapsed time: 44.657s, Critical Path: 11.12s\nFAILED: Build did NOT complete successfully\nI'm going to change the OS on that machine anyway, will see if it still fails.. ",
    "bash-horatio": "Encounter the same error in my Debian Machine. Unfortunately, I found that it is difficult to comment out all the dependencies and references to scribe. A conditional compilation of scribe will be preferred in the future.\nHeron: 0.17.8\nOS: Debian Testing (upgraded recently)\nCPU: Intel(R) Core(TM) i5-2520M CPU @ 2.50GHz. ",
    "SioKCronin": "The error I get when I follow these instructions is: Error: No available formula with the name \"heron\". ",
    "jcoyne": "You're right.  Should that be added to the getting started guide?. ",
    "tomncooper": "Duplicate of #2874 . Can you clarify if you are using the Docker containers defined in the Heron repo (docker/compile) or you are constructing your own?. I would recommend using the dockerfiles in the docker/compile directory they are used routinely for compilation so should work. Can you give the instructions in the docker/Readme.md a try?. Have you checked out one of the release tags or are you building from the head of the master branch? Try checking out the latest tag, 0.17.8, and compiling from there?. @nwangtw Hope those changes address your comments. Thanks for looking at this for me.. I agree the added dependencies are excessive. I could rewrite the client to do raw HTTP requests but I would simply be replicating the work of Influx client.\nTo cut down on dependencies, could I package the influx sink on maven central and have it as a single external dependency? Or is that overkill? \nThe Prometheus/Kapacitor option is a great solution (wish I had found it before I wrote the Influx sink). However, it does require Kapacitor (a stream processing engine) to be setup, so we have a stream processing engine to monitor a stream processing engine (\"who watches the watchmen\", \"it's turtles all the way down\", etc etc).\nI think this highlights an issue with the centralised metrics sink design in Heron. In Storm the sink is part of the topology and is included in the deployed fat JAR, so is just pulled in as needed by the topology developers. I am wondering if there may be a way to do an \"on demand\" version of sinks for Heron? Probably not as they are part of the metrics manager.\n. Well most of the dependencies come from the http lib that the influxDB java client is using (com_squareup_okhttp3). It looks excessive as I have to list every sub dependency in the WORKSPACE file. Is there a way for bazel to resolve the java dependencies itself? Then I would just need to include the InfluxDB client.\nIf adding these deps really is a hard no, then I can rewrite the sink to just do HTTP requests on the InfluxDB line protocol using the Apache Common client we are already including.. Yeah I did wonder about all those clean commands!. ",
    "comes5": "The error I am mentioning is in the log file I posted:\nERROR: /incubator-heron/heron/tools/admin/src/python/BUILD:22:1: PexPython heron/tools/admin/src/python/heron-admin.pex failed (Exit 1)\n**** Failed to install netifaces-0.10.6 (caused by: NonZeroExit(\"received exit code 1 during execution of['/usr/bin/python2.7', '-', 'bdist_wheel', '--dist-dir=/tmp/tmpwn7Syj']while trying to execute['/usr/bin/python2.7', '-', 'bdist_wheel', '--dist-dir=/tmp/tmpwn7Syj']\",)\n):\nThe problem is that I want to use Dhalion to implement new policies for Heron so I need to modify the source code and doing some tests with it.. Just constructing it on my own using the ubuntu default container. \nHere is my dockerfile:\nDockerfile.txt\n. Ok tried to compile using the instructions in /docker/Readme.md but there seem to be another error:\nINFO: Starting clean (this may take a while). Consider using --async if the clean takes more than several minutes.\nCreating packages\nINFO: Analysed target //scripts/packages:tarpkgs (240 packages loaded).\nINFO: Found 1 target...\nERROR: /scratch/heron/metricsmgr/src/thrift/BUILD:3:1: error executing shell command: 'set -e\nrm -rf bazel-out/k8-opt/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs\nmkdir bazel-out/k8-opt/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs...' failed (Segmentation fault): bash failed: error executing command \n  (cd /root/.cache/bazel/_bazel_root/c481f31d0aff7f9fd86654fb84c9a629/execroot/org_apache_heron && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  /bin/bash -c 'set -e\nrm -rf bazel-out/k8-opt/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs\nmkdir bazel-out/k8-opt/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs\nthird_party/thrift/thrift-linux-x86_64.exe -r --gen java -o bazel-out/k8-opt/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs heron/metricsmgr/src/thrift/scribe.thrift\njar cMf bazel-out/k8-opt/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar -C bazel-out/k8-opt/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs .\nrm -rf bazel-out/k8-opt/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs'): bash failed: error executing command \n  (cd /root/.cache/bazel/_bazel_root/c481f31d0aff7f9fd86654fb84c9a629/execroot/org_apache_heron && \\\n  exec env - \\\n    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \\\n  /bin/bash -c 'set -e\nrm -rf bazel-out/k8-opt/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs\nmkdir bazel-out/k8-opt/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs\nthird_party/thrift/thrift-linux-x86_64.exe -r --gen java -o bazel-out/k8-opt/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs heron/metricsmgr/src/thrift/scribe.thrift\njar cMf bazel-out/k8-opt/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar -C bazel-out/k8-opt/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs .\nrm -rf bazel-out/k8-opt/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs')\n/bin/bash: line 3:  2632 Segmentation fault      (core dumped) third_party/thrift/thrift-linux-x86_64.exe -r --gen java -o bazel-out/k8-opt/genfiles/heron/metricsmgr/src/thrift/thrift_scribe_java_src.srcjar.srcs heron/metricsmgr/src/thrift/scribe.thrift\nTarget //scripts/packages:tarpkgs failed to build\nINFO: Elapsed time: 168.796s, Critical Path: 115.46s\nFAILED: Build did NOT complete successfully\nCleaning up scratch dir. So, I am trying with virtualbox now, following the heron documentation with ubuntu 16.04. So far no errors from thrift and scribe, but the build fails after a long time building:\nERROR: Process exited with status 1: Process exited with status 1\nfatal: Not a git repository (or any of the parent directories): .git\nFailed to run command to check head: git rev-parse --abbrev-ref HEAD\nINFO: Elapsed time: 1525.185s, Critical Path: 69.10s\nINFO: 1914 processes: 1603 local, 311 worker.\nFAILED: Build did NOT complete successfully. Solved, Bazel couldn't find the .git directory which I had removed. One question though: do I have to build everything if I modify only one component and I want to test it?. ",
    "westurner": "\n. ",
    "sebastienpattyn93": "Hi @cckellogg I'm using the yaml from this [turotial] (https://apache.github.io/incubator-heron/docs/operators/deployment/schedulers/kubernetes/#general-kubernetes-clusters)\n```yaml\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\nhttp://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n\nHeron API server deployment\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: heron-apiserver\n  name: heron-apiserver\n  namespace: default\n\napiVersion: rbac.authorization.k8s.io/v1beta1\nkind: ClusterRoleBinding\nmetadata:\n  name: heron-apiserver\n  labels:\n    app: heron-apiserver\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: heron-apiserver\n  namespace: default\n\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: heron-apiserver\n  labels:\n    app: heron-apiserver\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: heron-apiserver\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: heron-apiserver\n    spec:\n      serviceAccountName: heron-apiserver\n      terminationGracePeriodSeconds: 0\n      tolerations:\n        - key: \"node.kubernetes.io/not-ready\"\n          operator: \"Equal\"\n          effect: \"NoExecute\"\n          tolerationSeconds: 10\n        - key: \"node.alpha.kubernetes.io/notReady\"\n          operator: \"Equal\"\n          effect: \"NoExecute\"\n          tolerationSeconds: 10\n        - key: \"node.alpha.kubernetes.io/unreachable\"\n          operator: \"Equal\"\n          effect: \"NoExecute\"\n          tolerationSeconds: 10\n      containers:\n        - name: heron-apiserver\n          image: heron/heron:latest\n          command: [\"sh\", \"-c\"]\n          args:\n            - >-\n              heron-apiserver\n              --base-template kubernetes\n              --cluster kubernetes\n              -D heron.statemgr.connection.string=zookeeper:2181\n              -D heron.kubernetes.scheduler.uri=http://localhost:8001\n              -D heron.executor.docker.image=heron/heron:latest\n              -D heron.class.uploader=org.apache.heron.uploader.dlog.DLUploader\n              -D heron.uploader.dlog.topologies.namespace.uri=distributedlog://zookeeper:2181/distributedlog\n        - name: kubectl-proxy\n          image: heron/kubectl:latest\n          command: [\"sh\", \"-c\"]\n          args:\n            - >\n              kubectl proxy -p 8001\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: heron-apiserver\nspec:\n  selector:\n    app: heron-apiserver\n  ports:\n    - protocol: TCP\n      port: 9000\n      targetPort: 9000\n  type: NodePort\n```\nand this is my response when I curl the api server:\n{\n  \"heron.build.git.revision\" : \"ddbb98bbf173fb082c6fd575caaa35205abe34df\",\n  \"heron.build.git.status\" : \"Clean\",\n  \"heron.build.host\" : \"ci-server-01\",\n  \"heron.build.time\" : \"Sat Mar 31 09:27:19 UTC 2018\",\n  \"heron.build.timestamp\" : \"1522488439000\",\n  \"heron.build.user\" : \"release-agent\",\n  \"heron.build.version\" : \"0.17.8\"\n}. @cckellogg any updates on this?. ",
    "cristobalcl": "Last week I posted an issue very similar to this one: #2919\nAs this issue included an error message in the CLI side like this Failed to instantiate uploader class 'org.apache.heron.uploader.dlog.DLUploader' and I don't, I thought at that moment they should be different problems. But now I'm thinking they are related and maybe we are using different CLI versions, because the errors in the server side are similar. I posted there a full server log.\nAny progress on this? Did you get over somehow?. The lapse in the timestamps of the logs is because I was using computers in different time zones (deploying from my local computer into the cluster). I was watching the logs in real-time and I can say those logs are related to the issue. Anyway, it's possible that I was missing another important logs of other system that is needed to debug, if so I could replicate the problem.\nIn the other hand, I'm looking for an alternative way of deploying the PEX. I was trying to use the S3 uploader, but I can not understand how it works. I could open another issue for this, if you wish, but anyway...\nFirstly, I have no idea how to configure the CLI. I have tried multiple combinations of parameters and configuration files, but the only thing I have get to work is modifying the uploader.yaml for the local config and running heron submit local \u2026. And doing so the PEX is uploaded to S3 succesfully... But nothing more. Ok, the PEX is in the S3 bucket, now what? How does Heron download the PEX and submit this and run the topology?\nSorry if this is obvious, but I have seen no example anywhere about using the S3 uploader. And again, I don't understand how to use the Heron CLI with the cluster/[role]/[env], configurations files, --config-path and --config-property arguments,...\nWhat I usually do with Kubernetes is:\nbash\nheron config heron set service_url http://\u2026/heron-apiserver:9000\nheron submit heron \"dist/xxxxxxxxxxx.pex\" --verbose - [\u2026]\nAnd this worked like a charm. But I have no idea which uploader.yaml I need to edit, or how to set up this to use the S3 uploader. And, again, I don't know how the server will launch the topology, or if I need to config anything in the Helm chart of Heron.\nThank you very much!. Yeah, it seems that the documentation is pretty outdated. I've just checked your PR and effectively that fixed the problems with the documented imports that we faced at the beginning.\nMy fear is that the Helm chart I'm using to install Heron in Kubernetes is also outdated.\nFeel free to ask for anything you think I could help!. I have edited the issue because I tested again with a small PEX and it works (though I have a new problem after that). With big PEX still does not work.\nAlso I have reproduce the issue locally using Minikube.\nThe problem that appeared when the small PEX was successfully submitted was that the topology does not launch because of an insufficient resources error in the launched pod: 0/5 nodes are available: 1 PodToleratesNodeTaints, 5 Insufficient cpu. Of course, I added a new node and the problem persisted. This same problem appeared in Minikube just the same (only with Heron deployed).. I made this to reproduce the issue: https://github.com/cristobalcl/heron-issue-2919\nAlso, I tested more times and I can reproduce the issue with small PEX... :(. ",
    "sprasad09": "Getting the same error :( \n[2018-08-20 22:55:33 -0700] [INFO]: Launching topology: 'WindowedWordCount'\n[2018-08-20 22:55:33 -0700] [INFO]: {'config_property': [], 'topology-file-name': '/Users/sprasad/.heron/examples/heron-streamlet-examples.jar', 'verbose': False, 'subcommand': 'submit', 'deploy_deactivated': False, 'submit_user': 'sprasad', 'cluster': 'kubernetes', 'extra_launch_classpath': '', 'deploy_mode': 'server', 'role': 'sprasad', 'dry_run_format': 'colored_table', 'dry_run': False, 'config_path': '/usr/local/heron/conf', 'topology_main_jvm_property': [], 'environ': 'default', 'service_url': 'http://localhost:8001/api/v1/proxy/namespaces/default/services/heron-apiserver:9000', 'topology-class-name': 'com.twitter.heron.examples.streamlet.WindowedWordCountTopology'}\n[2018-08-20 22:55:33 -0700] [ERROR]: Failed to instantiate uploader class 'org.apache.heron.uploader.dlog.DLUploader'\n[2018-08-20 22:55:33 -0700] [ERROR]: Failed to launch topology 'WindowedWordCount'. ",
    "xiaoyao1991": "I had similar issues trying to run it on minikube locally. Turns out that the docker image on docker hub is outdated. The artifact in the docker image have com.twitter.... as namespaces instead of org.apache..... \nWhen following the instructions on https://apache.github.io/incubator-heron/docs/operators/deployment/schedulers/kubernetes/#minikube:\n1. Before running $ kubectl create -f https://raw.githubusercontent.com/twitter/heron/master/deploy/kubernetes/minikube/apiserver.yaml\n2. Locally modify heron/deploy/kubernetes/minikube/apiserver.yaml \n3. Change -D heron.class.uploader=org.apache.heron.uploader.dlog.DLUploader to -D heron.class.uploader=com.twitter.heron.uploader.dlog.DLUploader. \n4. Run $ kubectl create -f <path_to_your_source>/heron/deploy/kubernetes/minikube/apiserver.yaml. \n5. Then follow through the rest of the instructions should work. . I also happened to find that the service_url on the doc does not work for my minikube setting on mac. I ended up using http://localhost:8001/api/v1/namespaces/default/services/heron-apiserver/proxy as my service_url. @nwangtw just close it. \ud83d\udc4d . Looks like the semantics for T and R go against the ones used in Java function types, say, BiFunction where T is considered the input type and R as the result type. . \ud83d\udc4d . \ud83d\udc4d . Can we use Math.max and Math.min instead? . :+1:. :+1:. \ud83d\udc4d . \ud83d\udc4d . \ud83d\udc4d . \ud83d\udc4d . \ud83d\udc4d . \ud83d\udc4d . \ud83d\udc4d . LGTM. \ud83d\udc4d . @nwangtw Yes. Verified by running modified word_count topologies with all 3 packing algorithms in dry-run mode. . \ud83d\udc4d . \ud83d\udc4d . @nwangtw that test is excluded because it was flaky when I run it in my local environment. However, every test is flaky when running in travis. The previous two runs indicate failures at different places. Not very consistent.  . \ud83d\udc4d . \ud83d\udc4d . \ud83d\udc4d . unused import . Since it's no longer an Impl class, could we move the abstract method to the interface itself and get rid of this class? . applyWindowConfig and attachWindowConfig seem redundant to each other. We could call windowCfg.attachWIndowConfig(bolt) here directly right? . Hmm, looks like the compiler can't tell the difference between an interface and lambda expressions at compile time. We can combine WindowConfig and WindowConfigImpl still as one abstract class to remove redundancy.  . > windowCfg doesn't have the function and requires a static cast. :(\nhmm, if we combine WindowConfig and WindowConfigImpl into one class as an abstract class, then windowCfg.attachWindowConfig(bolt) should work.  . \ud83d\udc4d addressed. missing import. It doesn't seem necessary to separate RamShare and DiskShare as RAM and disk are both measured in ByteAmount. CPU is measured in share of time used. \nPreviously, we didn't take CPU resource constraints into consideration when composing packing plan. Now that we do -- namely we need to collect instance cpu resource mapping,  and the logic of that is very similar to that of instance RAM resource mapping (previously as getInstancesRAMMapInContainer()). But due to the difference in the type of measurement(ByteAmount vs double), we might end up with a lot of code duplicates. Hence, I abstract out CPUShare so that it looks very similar to what we have in ByteAmount, and thus we can have one calculateInstancesResourceMapInContainer() that works for both CPU and RAM (potentially disk if needed) instance resource mapping collection. . Good question. The ByteAmount had these methods and I preserved these to guarantee minimum changes. I guess they were there just for readability? . The semantics for factor is percentage in ByteAmount and don't allow decimal spaces. The original method signatures in ByteAmount use int too.  . \ud83d\udc4d Updated. . Looks like instanceof automatically does null check, so it's redundant. The IDE suggests the type declaration here is unnecessary. I figure just to clean it up. \ud83d\udc4d Added more tests to cover the mentioned scenarios. It is actually used down in calculateInstancesResourceMapInContainer as the containerResHint. It is used to validate the all instances + padding in roundRobinAllocation does not exceed the containerResHint. It's just not used anymore in packingInternal, similar to how we treated containerRamHint before the change. hmm. I see. Then I think we don't even need to take the max out of 2. ContainerCpuHint is always gonna be greater no matter what. . Yeh you are right, I just figured that situation :) Updated. Yes, this is exactly where the problem is. Previously, it is wrongfully using the old plan's component parallelism to calculate the new number of containers, but that's always going to return the same number of containers as the old plan.... Why is this 100MB by default? This would almost certainly fail during submission because it's lower than min RAM. . This indirection seems unnecessary. PackingException is a RuntimeException. . yeh, flatMap takes stream only as input. It's public in the test scope so it should be fine I think. I made it public because other test files involve comparing double too. . Yes, this is possible. The way RCRR works now is run in an while(true) loop and add containers one by one. Without this check here, the code could stuck in an infinite loop if the allocated container size cannot even fit 1 instance. nice catch. this is specific to RCRR, I don't think it should be abstracted to the parent class. But it makes sense to abstract it in the RCRR? . That actually sounds perfect to me. I prefer not to have too many packing classes too. . bazel query 'kind(\"cc_test rule\", ...)' basically returns all test targets that are cpp. tmaster tests are included as well. Sure. I named it test because I first wanted to use Predicate for this, but it's fine. . Refactored so that now SortingStrategy becomes a comparator itself. . ",
    "zhangcheng": "Indeed this help, thanks @xiaoyao1991 for the tips. ",
    "placeacall": "wow, works like a charm\n(venv) D:\\Users\\PycharmProjects\\Heron\\test>pip install heronpy\nCollecting heronpy\n  Downloading https://files.pythonhosted.org/packages/cf/74/4707816aa9a5994b4b22c6d3707513d077728e21a5bc77a27cacfdaaf49b/heronpy-0.17.8-py2-none-any.whl (117kB)\n    100% |################################| 122kB 358kB/s\nCollecting protobuf==3.4.0 (from heronpy)\n  Downloading https://files.pythonhosted.org/packages/b7/24/e7514e574b025bd86eed3bff69bf342abacd77e680a0b0c61f89a3197de3/protobuf-3.4.0-py2.py3-none-any.whl (375kB)\n    100% |################################| 378kB 2.6MB/s\nCollecting six>=1.9 (from protobuf==3.4.0->heronpy)\n  Using cached https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\nRequirement already satisfied: setuptools in d:\\users\\speak\\pycharmprojects\\heron\\test\\venv\\lib\\site-packages (from protobuf==3.4.0->heronpy) (39.2.0)\nInstalling collected packages: six, protobuf, heronpy\nSuccessfully installed heronpy-0.17.8 protobuf-3.4.0 six-1.11.0\nThank you @kramasamy \n. ",
    "sautran": "I am using the same exact issue when using minikube. it failed to upload. in the bookie server log, there ar e a lot of errors. It has error in the worker container as well as the following error\njava.lang.NoClassDefFoundError: Could not initialize class dlshade.io.netty.channel.epoll.EpollEventLoopGroup\nI don't use helm. I follow this steps https://apache.github.io/incubator-heron/docs/operators/deployment/schedulers/kubernetes/\nI created 2 tickets, #3170  and #3173 \n. @Code0x58 , @cristobalcl , any progress on this?  it seems nobody get this fixed.\n. @Code0x58 , for me, I checked the bookkeeper log, it is full of bookkeeper IOException, I described here  #3173\n. After I saw timeout warning,  I migrate to a powerful box, the issue is gone. . i am running minikube on mac. Look like heron packaged with old distributedlog core jar (0.5). in this release, it package io.netty with dlshade which will cause issue when loading native library. . Here is the log in the api server\n[2019-01-25 18:30:18 +0000] [INFO] org.apache.bookkeeper.zookeeper.ZooKeeperWatcherBase: ZooKeeper client is connected now.\n[2019-01-25 18:30:18 +0000] [WARNING] org.apache.distributedlog.impl.BKNamespaceDriver: Could not use Netty Epoll event loop for bookie server: \njava.lang.UnsatisfiedLinkError: failed to load the required native library\n    at dlshade.io.netty.channel.epoll.Epoll.ensureAvailability(Epoll.java:78)\n    at dlshade.io.netty.channel.epoll.EpollEventLoopGroup.(EpollEventLoopGroup.java:38)\n    at org.apache.distributedlog.impl.BKNamespaceDriver.getDefaultEventLoopGroup(BKNamespaceDriver.java:259)\n    at org.apache.distributedlog.impl.BKNamespaceDriver.initializeBookKeeperClients(BKNamespaceDriver.java:270)\n    at org.apache.distributedlog.impl.BKNamespaceDriver.initialize(BKNamespaceDriver.java:208)\n    at org.apache.distributedlog.api.namespace.NamespaceBuilder.build(NamespaceBuilder.java:238)\n    at com.twitter.heron.uploader.dlog.DLUploader.initializeNamespace(DLUploader.java:137)\n    at com.twitter.heron.uploader.dlog.DLUploader.initialize(DLUploader.java:94)\n    at com.twitter.heron.scheduler.SubmitterMain.submitTopology(SubmitterMain.java:420)\n    at com.twitter.heron.apiserver.actions.SubmitTopologyAction.execute(SubmitTopologyAction.java:33)\n    at com.twitter.heron.apiserver.resources.TopologyResource.submit(TopologyResource.java:223)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:498)\n    at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81)\n    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:144)\n    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:161)\n    at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$ResponseOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:160)\n    at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:99)\n    at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:389)\n    at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:347)\n    at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:102)\n    at org.glassfish.jersey.server.ServerRuntime$2.run(ServerRuntime.java:326)\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271)\n    at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267)\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:315)\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:297)\n    at org.glassfish.jersey.internal.Errors.process(Errors.java:267)\n    at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:317)\n    at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:305)\n    at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:1154)\n    at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:473)\n    at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:427)\n    at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:388)\n    at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:341)\n    at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:228)\n    at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:841)\n    at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:535)\n    at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:188)\n    at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1253)\n    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:168)\n    at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\n    at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:166)\n    at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1155)\n    at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\n    at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\n    at org.eclipse.jetty.server.Server.handle(Server.java:564)\n    at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:317)\n    at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)\n    at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:279)\n    at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:110)\n    at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:124)\n    at org.eclipse.jetty.util.thread.Invocable.invokePreferred(Invocable.java:128)\n    at org.eclipse.jetty.util.thread.Invocable$InvocableExecutor.invoke(Invocable.java:222)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:294)\n    at org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:126)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:673)\n    at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:591)\n    at java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NoClassDefFoundError: io/netty/channel/epoll/NativeStaticallyReferencedJniMethods\n    at java.lang.ClassLoader$NativeLibrary.load(Native Method)\n    at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1941)\n    at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1824)\n    at java.lang.Runtime.load0(Runtime.java:809)\n    at java.lang.System.load(System.java:1086)\n    at dlshade.io.netty.util.internal.NativeLibraryUtil.loadLibrary(NativeLibraryUtil.java:36)\n    at dlshade.io.netty.util.internal.NativeLibraryLoader.loadLibrary(NativeLibraryLoader.java:278)\n    at dlshade.io.netty.util.internal.NativeLibraryLoader.load(NativeLibraryLoader.java:231)\n    at dlshade.io.netty.channel.epoll.Native.loadNativeLibrary(Native.java:191)\n    at dlshade.io.netty.channel.epoll.Native.(Native.java:61)\n    at dlshade.io.netty.channel.epoll.Epoll.(Epoll.java:33)\n    ... 59 more\nCaused by: java.lang.ClassNotFoundException: io.netty.channel.epoll.NativeStaticallyReferencedJniMethods\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:335)\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n    ... 70 more\n. adding   -D io.netty.packagePrefix=dlshade. didn't solve the problem\n     heron-apiserver\n              --base-template kubernetes\n              --cluster kubernetes\n              -D io.netty.packagePrefix=dlshade.\n              -D heron.statemgr.connection.string=zookeeper:2181. Look like after I move to a powerful box. the issue is gones\n. I have this powerful box having minikube hosting all the bookeeper, heron-api etc. services. I deploy the topology from my desktop to this box. I am talking about shell environment variable not java properties\n. in the AppsV1beta1Controller.java, can we support more user defined env variable?\n    // setup the environment variables for the container\n    final V1EnvVar envVarHost = new V1EnvVar();\n    envVarHost.name(KubernetesConstants.ENV_HOST)\n        .valueFrom(new V1EnvVarSource()\n            .fieldRef(new V1ObjectFieldSelector()\n                .fieldPath(KubernetesConstants.POD_IP)));\nfinal V1EnvVar envVarPodName = new V1EnvVar();\nenvVarPodName.name(KubernetesConstants.ENV_POD_NAME)\n    .valueFrom(new V1EnvVarSource()\n        .fieldRef(new V1ObjectFieldSelector()\n            .fieldPath(KubernetesConstants.POD_NAME)));\ncontainer.setEnv(Arrays.asList(envVarHost, envVarPodName));\n\n. @nwangtw, Hi, Wang, Are you able to put this feature in soon?. @nwangtw ,if I understand correctly,  open() is in jvm level. need to set it in the shell. outside of jvm.\nBear with me if i am wrong. @nwangtw, in fact, this is not only for container also for standard deployment. I also have a aurora/mesos cluster. There is no configuration that we can set environment variables in the executors. \nI worked around by change heron.aurora file add some shell scripts in it.. ",
    "yaoliclshlmch": "Yes. It is weird this part is missing. Let me fix it.. @huijunwu \n\"The IdentityBolt ABSpout IntegrationTopologyTestBolt IntegrationTopologyTestSpout\nlook similar to classes in inregation_test directory. is there a way to reuse code\"\nIt is a little bit difficult, because they inherit from different class/interface and have different processing logic.. Because checkpoint rollback is triggered by intended instance failure at a given component when the second checkpoint control tuple arrives. The failed instances will be restarted then and an additional checkpoint will be made afterwards which will result in unexpected instance state info.\nWhat happens to the instances which can trigger checkpoint rollback:\n1. instance started\n2. successfully make checkpoint 1\n3. throw exception to trigger instance failure when making checkpoint 2\n4. restart this instance and rollback to checkpoint 1\n5. successfully make checkpoint 3\n6. throw exception to trigger instance failure when making checkpoint 4\n......\nthe expected results are the instance states in checkpoint 1, so only those info at the http server is expected.\nAt the instance level, checkpoints will be made periodically and cannot be stopped or otherwise controlled.. won't delete it.. @nwangtw @nlu90 @huijunwu . @nwangtw @nlu90 @huijunwu . @nwangtw @nlu90 @huijunwu Please ignore the ci failure and merge this PR when you think it is ready.  I will fix the integration test failures in the next PR.. @nwangtw @nlu90 @huijunwu . @huijunw @nlu90 @nwangtw . Yes, I tried the WordCount topology, and the \"__bytes_\" and \"__tuples_\" work well. There are no ack and fail in this topology, but the corresponding metrics should be fine.\n. ",
    "antiguru": "See #2950 for the corresponding pull request.. ",
    "Glorfischi": "Sorry for the delay.\nI have a fix ready for one of the feedback, but I'll wait for a response before I push it.  See the comment on the unRegister* functions. \nAs for the ss.loopExit() call. I think this should be fine, but I hoped someone with more experience with libevent could give the ok.. ",
    "kalimfaria": "Instead of changing the old metric to a reduced type, now I've added an additional metric that measures average serialization/deserialization time per tuple.. ",
    "Clarkkkkk": "@nwangtw I am using bazel 0.5.4 and mac os 10.13.6.. @nwangtw Thank you for your help. I think the doc should be modified since it still recommend bazel 0.5.4.. ",
    "simingweng": "@nwangtw @jerrypeng . If you want to use Apache Storm KafkaSpout with Heron, you can use\n<dependency>\n            <groupId>org.apache.storm</groupId>\n            <artifactId>storm-kafka-client</artifactId>\n            <version>1.2.2</version>\n        </dependency>\nBut, there's a source file from Apache Storm is missing in the Heron Storm compatibility library which is required by the dependency above. You will need to manually copy org.apache.storm.utils.Time class from Apache Storm source tree into your project. We was using Heron 0.17.8 with storm-kafka-client 1.2.2 successfully, but recently rewrote a new one for Heron due to performance issue we found in storm-kafka-client.. @nwangtw definitely make sense to have both Spout and Bolt, Apache Storm has the same thing.. The Travis CI failed due to the following test failure:\nIt seems to be due to the actual JSON output just had different order in the JSON array compared to the expected output, which, I'm pretty sure, has nothing to do the maven test I added for the Kafka Spout. Can someone familiar with the integration test code base help with this IntegrationTest_SlidingTimeWindow1 test failure?\n\n[2019-02-27 18:05:29 +0000] \u001b[31m[ERROR]\u001b[0m: Actual result did not match expected result\u001b[0m\n[2019-02-27 18:05:29 +0000] \u001b[32m[INFO]\u001b[0m: Actual result ---------- \n['{\"1\":[{\"tuplesInWindow\":[\"1\"]},{\"newTuples\":[\"1\"]},{\"expiredTuples\":[]}]}', '{\"2\":[{\"tuplesInWindow\":[\"2\",\"3\"]},{\"newTuples\":[\"2\",\"3\"]},{\"expiredTuples\":[\"1\"]}]}', '{\"3\":[{\"tuplesInWindow\":[\"4\"]},{\"newTuples\":[\"4\"]},{\"expiredTuples\":[\"2\",\"3\"]}]}', '{\"4\":[{\"tuplesInWindow\":[\"5\"]},{\"newTuples\":[\"5\"]},{\"expiredTuples\":[\"4\"]}]}', '{\"5\":[{\"tuplesInWindow\":[\"6\"]},{\"newTuples\":[\"6\"]},{\"expiredTuples\":[\"5\"]}]}', '{\"6\":[{\"tuplesInWindow\":[\"7\"]},{\"newTuples\":[\"7\"]},{\"expiredTuples\":[\"6\"]}]}', '{\"7\":[{\"tuplesInWindow\":[\"8\"]},{\"newTuples\":[\"8\"]},{\"expiredTuples\":[\"7\"]}]}', '{\"8\":[{\"tuplesInWindow\":[\"9\"]},{\"newTuples\":[\"9\"]},{\"expiredTuples\":[\"8\"]}]}', '{\"9\":[{\"tuplesInWindow\":[\"10\"]},{\"newTuples\":[\"10\"]},{\"expiredTuples\":[\"9\"]}]}']\u001b[0m\n[2019-02-27 18:05:29 +0000] \u001b[32m[INFO]\u001b[0m: Expected result ---------- \n['{\"1\":[{\"tuplesInWindow\":[\"1\"]},{\"newTuples\":[\"1\"]},{\"expiredTuples\":[]}]}', '{\"10\":[{\"tuplesInWindow\":[\"10\"]},{\"newTuples\":[\"10\"]},{\"expiredTuples\":[\"9\"]}]}', '{\"2\":[{\"tuplesInWindow\":[\"2\"]},{\"newTuples\":[\"2\"]},{\"expiredTuples\":[\"1\"]}]}', '{\"3\":[{\"tuplesInWindow\":[\"3\"]},{\"newTuples\":[\"3\"]},{\"expiredTuples\":[\"2\"]}]}', '{\"4\":[{\"tuplesInWindow\":[\"4\"]},{\"newTuples\":[\"4\"]},{\"expiredTuples\":[\"3\"]}]}', '{\"5\":[{\"tuplesInWindow\":[\"5\"]},{\"newTuples\":[\"5\"]},{\"expiredTuples\":[\"4\"]}]}', '{\"6\":[{\"tuplesInWindow\":[\"6\"]},{\"newTuples\":[\"6\"]},{\"expiredTuples\":[\"5\"]}]}', '{\"7\":[{\"tuplesInWindow\":[\"7\"]},{\"newTuples\":[\"7\"]},{\"expiredTuples\":[\"6\"]}]}', '{\"8\":[{\"tuplesInWindow\":[\"8\"]},{\"newTuples\":[\"8\"]},{\"expiredTuples\":[\"7\"]}]}', '{\"9\":[{\"tuplesInWindow\":[\"9\"]},{\"newTuples\":[\"9\"]},{\"expiredTuples\":[\"8\"]}]}']\u001b[0m\n[2019-02-27 18:05:29 +0000] \u001b[31m[ERROR]\u001b[0m: Checking result failed for 20190227175427_IntegrationTest_SlidingTimeWindow1_e8519bcd-2a19-4971-b5ef-6bcb1dbf0b6e topology :: Traceback (most recent call last):\n  File \"integration_test/src/python/test_runner/main.py\", line 204, in run_test\n    return results_checker.check_results()\n  File \"integration_test/src/python/test_runner/main.py\", line 123, in check_results\n    return self._compare(expected_results, actual_results)\n  File \"integration_test/src/python/test_runner/main.py\", line 136, in _compare\n    raise failure\nTestFailure: ('Actual result did not match expected result', None)\n\u001b[0m. @worlvlhole \nYes, I think you've got the most part. The KafkaSpout may be operated in 2 different reliability mode, ATMOST_ONCE or ATLEAST_ONCE, (I haven't added EFFECTIVE_ONCE implementation yet, I'm working on it).\n\nATMOST_ONCE mode\nthe whole topology will not turn the acking mechanism on. so, the KafkaSpout can afford to emit the tuple without any message id, and it also immediately commit the currently-read offset back to Kafka broker, and neither ack() nor fail() callback will be invoked. Therefore, \"in-flight\" tuple will just get lost in case the KafkaSpout instance is blown up or the topology is restarted. That's what ATMOST_ONCE offers.\nATLEAST_ONCE mode\nthe acking mechanism is turned on topology-wise, so the KafkaSpout uses the ack registry to keep tracking all the continuous acknowledgement ranges for each partition, while the failure registry keeps tracking the lowest failed acknowledgement for each partition. When it comes to the time that the Kafka Consumer needs to poll the Kafka cluster for more records (because it's emitted everything it got from the previous poll), then the KafkaSpout reconciles as following for each partition that it is consuming:\n\nif there's any failed tuple, seek back to the lowest corresponding offset\ndiscard all the acknowledgements that it's received but is greater than the lowest failed offset\nclear the lowest failed offset in failure registry\ncommit the offset to be the upper boundary of the first range in the ack registry\n\nSo, it guarantees each tuple emitted by the KafkaSpout must be successfully processed across the whole topology at least once.\nNot Implemented\nWhat is missing in this Kafka Spout implementation now is to handle the EFFECTIVE_ONCE scenario, which should completely rely on the checkpointing mechanism to decide how far it needs to rewind back. I'm working on it right now.\nI know this is quite some information, I'm writing README to explain things in more details, will keep updating the pull request.. > Hi, thanks for this PR, happy to finally see a Kafka Spout implementation in Heron. I am planning on using this once it is merged, but I have a question. One major difference between this implementation and the Storm one is that Storm's spout allows emitting to different streams, using the org.apache.storm.kafka.spout.RecordTranslator interface. This implementation is missing this particular functionality, which is quite useful.\n\nIs there a reason for not keeping this functionality in this Kafka Spout implementation? Or is there another way to achieve similar functionality (other than creating a map-function like bolt for this purpose)? It's really useful for sending data from different topics to different downstream bolts.\n\nvery good question. I actually started with a \"one-record-to-many-tuple\" implementation, then I gave it a deep thought when I was implementing the ATLEAST_ONCE delivery guarantee. Allowing \"one-record-to-many-tuple\" will significantly complicate the algorithm to track acknowledgement, because then we have to keep tracking the mapping relationship between a single Kafka record offset to multiple message IDs.\nAnd then we also face a design choice whether the KafkaSpout itself should decide the uniqueness of a set of Message IDs coming from the same ConsumerRecord, or we should open the choice up to the developer?\nSo, a neater choice is to use multiple KafkaSpout, each dedicated to an output stream.\nBut, I do agree \"one-record-to-many-tuple\" is pretty useful and cost effective in terms of resource consumption. I have no obligation to put it back in, but then it becomes the developer's responsibility to make sure avoid emitting multiple tuples out of one ConsumerRecord ONLY in ATLEAST_ONCE mode, at least for this version of KafkaSpout before we introduce a more complicated ack/fail tracking mechanism.. I think @rohanag12\u2019s use case is that he has multiple topics and the record from each topic will only be translated into one tuple, and emit to one stream. Basically, it is \u201ctopic1\u201d -> \u201cstream 1\u201d, \u201ctopic2\u201d -> \u201cstream 2\u201d, etc. So, he would like to use one KafkaSpout, subscribing to multiple topics, and have the ConsumerRecordTransformer declare multiple output streams, and route the translated tuple to its destined stream base on the topic of the received record. \nThis pattern can save user one extra \u201cdistributor\u201d bolt connected to the KafkaSpout. Basically, the routing logic becomes embedded in the KafkaSpout itself. But again, the trade-off of this flexibility is that developer needs to be careful not to emit multiple tuple out of one single record in ATLEAST_ONCE mode.\nSo, as long as one record is only translated into one emitted tuple, then the current KafkaSpout implementation shall work in ATLEAST_ONCE mode. . @sijie @nwangtw . we need to upgrade from apachedistributedlog:distributedlog:0.5.0 to apache/bookkeeper:4.7.3. The latest apache/bookkeeper image can not be used due to https://github.com/apache/bookkeeper/issues/1660. @nwangtw please assign this to me cuz' I've started working on it, actually, almost done the final testing. There will be some other minor implication to Heron after the upgrade, for example, the Kubernetes deployment manifest needs to be updated, because the startup mechanism is quite different now in apache/bookkeeper, the default dlog namespace is no longer created by default.\nI added a initContainer to the heron deployment yaml, to manually create the namespace that Heron expects, also, the distributedlog-core maven dependencies needs to align with 4.7.3 as well.. @nwangtw . @nwangtw . @nwangtw . @nwangtw . ",
    "worlvlhole": "Same issue here using heron helm chart deploying in GKE.. I also agree this would be a great feature.. @simingweng  Im having trouble following what you are doing when there is a failed tuple. Not due to your code being confusing, i am just missing something and would like an explanation.  Are you replaying all tuples from the most recent failed offset?\nFrom what I have read in the code, i have come to believe this is your process, is this correct?\n1. Add offset to failure registry.\n2. Check if there is a failure.\n3. Seek to failure offset.\n4. Remove previous acks \n5. Continue emitting..\nThe code looks good. I plan on using this shortly and will hopefully have more feedback after i get into it. \nThanks!\n. @simingweng  Thank you so much for the in depth explanation!. Slightly unrelated but, I think the need to set a list of spout\nrequirements is still a big discussing that's needed.\nOn Mon, Mar 18, 2019, 4:45 PM Rohan Agarwal notifications@github.com\nwrote:\n\nHi, thanks for this PR, happy to finally see a Kafka Spout implementation\nin Heron. I am planning on using this once it is merged, but I have a\nquestion. One major difference between this implementation and the Storm\none is that Storm's spout allows emitting to different streams, using the\norg.apache.storm.kafka.spout.RecordTranslator interface. This\nimplementation is missing this particular functionality, which is quite\nuseful.\nIs there a reason for not keeping this functionality in this Kafka Spout\nimplementation? Or is there another way to achieve similar functionality\n(other than creating a map-function like bolt for this purpose)? It's\nreally useful for sending data from different topics to different\ndownstream bolts.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\nhttps://github.com/apache/incubator-heron/pull/3198#issuecomment-474093610,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AHNon-PwB3e0EsUXtX27qZuH_MQUo8nmks5vX_rlgaJpZM4bQ7Tz\n.\n. \n",
    "thinker0": "LGTM. @nwangtw \nI created a Bolt with MoultiThread (finagle-http) async and ack all at once.\nI think NPE is happening often.\nheron-0.18.7 I can not create a maven package file, so I can not test it in my project.\nCan you tell me how to package it so I can test it?. @nwangtw fixed. aurora-0.20 on mesos-1.5.1\ncheck aurora and mesos version \ncurrent aurora-0.21 on mesos-1.7. When I use Aurora, sometimes it is often the case that the Instance is not healthy.\n. Nice +++++1. ",
    "sleepy-brook": "@nlu90 does it require trident?\nIf so, AFAIK heron doesn't support trident.. ",
    "windhamwong": "I currently have the Kafka spout code working, however, it falls after some time randomly (around 2 - 7 days). It shows initialized but nothing received from the Kafka broker. There is no sign or error showing its falling.\nOnly thing I can notice is, the Kafka Spout reinitialize when it says Restoring instance after some time.\n[2018-10-30 04:06:41 +0000] [INFO] st_heron_instance.py: Restoring instance state to checkpoint 1540822768709483564-1540872269\n[2018-10-30 04:06:41 +0000] [INFO] st_heron_instance.py: Instance restore state deserialized\n[2018-10-30 04:06:42 +0000] [INFO] st_heron_instance.py: Received start stateful processing for 1540822768709483564-1540872269\n[2018-10-30 04:06:42 +0000] [INFO] st_heron_instance.py: Starting bolt/spout instance now...\n[2018-10-30 04:06:42 +0000] [INFO] base_instance.py: [+] Initializing InputKafkaLogSpout...\nI also tried to use 3 or 1 Kafka Spouts as parallel Spout, but still not fixing the issue.\nHowever, one thing I can confirm is that, 3 parallel spouts setup doesn't duplicate message emit.. My Kafka Spout is more like a standard Lafka client in Python and emitting messages in Heron. It appears stop emitting after some time (random time period) and now just have a new update.\nKafka reports that the client is up-to-date to the offset, but just not emitting in Heron. Might be an issue in Heron. For reference only\ndef next_tuple(self):\n    msg = self.CONFLUENT_CONSUMER.poll(0.1)\n    if msg is None:\n    return\n\n    if msg.error():\n    if msg.error().code() == KafkaError._PARTITION_EOF:\n        return\n    else:\n        self.log(\"[*] [ERROR] \" + msg.error())\n        return\n\n    self.emit([msg.value()])\n\n. Debian 9. The AWK command should read '{}' as the output format.. Maybe we can add an option to the script that allows using this flag or not?. Let me check again and make an update to this. Updated the script with [-s|--squash] option.\nRephasing the default command help message. ",
    "bdparrish": "Awesome, I saw that but I didn't know if it was the same library or a new flavor.  Thanks @joshfischer1108 . ",
    "orchapod": "make serve indeed does work. Is there a way to get those web files that are being used in make serve process and put them to my private Nginx web server? I tried looking at website/public but it's empty.. ",
    "jmark99": "I was having difficulty finding the 3.4.10 version anywhere except the Apache archive site. The Apache site indicates that downloads should be made from a nearby mirror site rather than apache.org. Otherwise I would have taken that approach. If another reliable link can be found I can update the links rather than the version. But I did not find a reliable link.. @nwangtw that sounds good? I\u2019ll update the link to archive.apache.org and remove the current links. Would you prefer I revert back to 3.4.10 or stay with the most recent release?. @nwangtw I updated WORKSPACE to use archive.apache.org as the url for the zookeeper archive. I reverted back to the 3.4.10 version for now to keep the change to a minimum at this tmie. The version can be updated when desired without having to modify the url.. I believe I will have to find a way to test via integration tests in order to verify programmatically. Even with integration tests I'm not sure how to test tuple failures. I'm open to ideas. \nWhile running my heron-examples there are situation where tuples fail and are re-emitted successfully. I'm just not sure how to force that behaviour in testing.. ",
    "dnrusakov": "@jmark99 @nwangtw When the new version released (for example, zookeeper-3.4.14), the current zookeeper-3.4.13 will go to archive (as it happened with zookeeper-3.4.10). The archive is not a part of any mirror, so we will have the same problem again. Since they intentionally didn't add archive to any mirror, i assume it's safe to directly use the following link: http://archive.apache.org/dist/zookeeper/zookeeper-3.4.10/zookeeper-3.4.10.tar.gz. @dave2wave \nYes, ZK is available in Maven Central. However, it doesn't have C++ sources on board...\nSo, when I tried to replace\nnew_http_archive(\n    name = \"org_apache_zookeeper\",\n    urls = [\n      \"http://archive.apache.org/dist/zookeeper/zookeeper-3.4.10/zookeeper-3.4.10.tar.gz\",\n    ],\n    strip_prefix = \"zookeeper-3.4.10\",\n    build_file = \"third_party/zookeeper/zookeeper.BUILD\",\n    sha256 = \"7f7f5414e044ac11fee2a1e0bc225469f51fb0cdf821e67df762a43098223f27\",\n)\nwith\nmaven_jar(\n  name = \"org_apache_zookeeper\",\n  artifact = \"org.apache.zookeeper:zookeeper:3.4.10\",\n)\nit failed to compile:\nERROR: /Users/drusakov/workspace/incubator-heron/heron/common/src/cpp/zookeeper/BUILD:3:1: no such package '@org_apache_zookeeper//': BUILD file not found on package path and referenced by '//heron/common/src/cpp/zookeeper:zookeeper-cxx'.\nERROR: Analysis of target '//heron/tmaster/src/cpp:tmaster-cxx' failed; build aborted.\nAny ideas?. Another PR with the same fix has been already merged: https://github.com/apache/incubator-heron/pull/3205 So, i'm closing this PR.. ",
    "dave2wave": "I think it depends on whether or not you ask for a specific version or latest from Maven Central.\nSent from my iPhone\n\nOn Mar 5, 2019, at 1:46 PM, Ning Wang notifications@github.com wrote:\nThx.\nIf we change to zookeeper-3.4.13, when zk has a new release, it will be be removed from mirror sites and we will have to update again. I am curious what exactly the apache suggest means.\nMentors do you have any suggestion? @dave2wave @julienledem @ptgoetz\nFYI, Dmitry's PR for the issue is here: #3207\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub, or mute the thread.\n. ZK is available from Maven Central - https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper\n\nWhy not use the maven_jar directive to retrieve the code? You are doing that to retrieve Apache Curator.. Or better than archive.apache.org use http://central.maven.org/maven2/org/apache/zookeeper/zookeeper/3.4.13/. ",
    "ananthgs": "It does expect zookeeper to be running on one of the ec2 instances. As suggested I will put up a document explaining the offline set ups on Amazon ec2 and the following steps on the scheduler.. This docker image contains ububtu, java, python, heron-clinet, heron-tools and aws s3 cli. I agree with @ jrcrawfo if there is a plan to have a Docker repo we can leverage it.\n. Done set default to false. done, Passing it as argument. added gethost function . Done . Removed the hard coding. Created a template.yaml file which will be used instead.. Done removed hard coded ports. Done.. Done. . removed IDE generated lines.. Done. Changed to using getFreePorts(). Done ... Removed the String.valueOf . The Default ECS Cluster Formation uses the ecs tasks where in we can pass the docker image and an entry point to start each of the containers. Not sure on the AMIs .. . Done. Done.. I agree. Will modify. Added the get joblink feature to return the ecs tasks. The AWS ECS task can be triggered via a docker style compose command.  This Task is then run on the AWS Cluster in a Container instance. The overall approach is explained  here : \nhttps://docs.google.com/document/d/1ecbCuA46cIKPfY0SP0F1dcRlei4DIPz3pZ6ZSZ5zZgc/edit?usp=sharing. \nPlease feel free to comment on the approach.\n  . ",
    "fred521": "private static final String CONFIRMED_YES = \"y\";\n,\nreturn CONFIRMED_YES.equalsIgnoreCase(userInput);. I'd prefer to use the name pattern like \"hasConfirmedWithUser(int newContainerCount )\", but it is up to you. try to avoid if else or if if, maybe single statement is a little bit clear. But I am not sure the entire logic, I could be wrong.\nBoolean hasNotConfirmedWithUser = \"prompt\".equalsIgnoreCase(Context.updatePrompt(config)) && !hasConfirmedWithUser(containersToAdd.size());\n  if(hasNotConfirmedWithUser) LOG.warning(\"Scheduler updated topology canceled.\");. return null sometimes will cause an additional null check logic when using this method.\nIn the effective java design pattern, it should always avoid returning null.\nOr you can return an empty object or initial the object at the very beginning.\nag, I don't know the whole logic, just some notes.. Duplicate code found here, better to have a method to reduce the code\ngetResponseByFile(String filePath)\n```\n  public Response downloadFile(final @PathParam(\"file\") String file) {\n    Config config = createConfig();\n    String uploadDir = config.getStringValue(FILE_SYSTEM_DIRECTORY);\n    String filePath = uploadDir + \"/\" + file;\n    return getResponseByFile(filePath);\n  }\npublic Response downloadHeronCore() {\n    String corePath = getHeronCorePackagePath();\n    return getResponseByFile(corePath);\n  }\nprivate Response getResponseByFile(String filePath){\nFile file = new File(filePath);\nif (!file.exists()) {\n  //Debug log if it is necessary\n  return Response.status(Response.Status.NOT_FOUND).build();\n}\nString mimeType = new MimetypesFileTypeMap().getContentType(file);\nResponse.ResponseBuilder rb = Response.ok(file, mimeType);\nrb.header(\"content-disposition\", \"attachment; filename = \"\n        + file.getName());\nreturn rb.build();\n\n}\n```. Curious why you not using java-> java.util.concurrent.atomic.AtomicLong package, it is thread safe with better performance.\nsynchronized with the method which makes the code single thread only even there are multiple threads running.\n. ",
    "julienledem": "ah right. maybe it doesn't fail when version is empty. This code needs to be hardened a little bit to verify what version matched is and whether it makes sense. Having a unit test with known version strings would help. Not sure why the version needs to be trailing. . I instrumented the code and it seems to match:\nfirst_line='Python 3.6.4 :: Anaconda, Inc.'\nmatch('3.6.4','>=2.7.0')\n. "
}
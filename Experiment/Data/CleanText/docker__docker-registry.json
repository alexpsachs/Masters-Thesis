{
    "mhrivnak": "I am willing to do this work. Should we have any discussion before I begin?\nOne simple question is what the namespace should be. I suggest \"docker.registry\" as a natural, predictable namespace. from docker.registry.storage import Storage for example. Or if you want to keep this distinct from the \"docker\" namespace, you could choose \"docker_registry\", as in from docker_registry.storage import Storage. That's a bit more cumbersome to type though. :)\nI am very interested in doing this because I would also like to add (as a separate effort of course) the use of python entry points for loading 3rd-party storage backends. That isn't possible without making the registry a python package.\n. I'll run through a few benefits just to help support the RFE:\n- The install process already steers you toward making a virtualenv in which to install dependencies, so it is a natural next step to install the registry the same way.\n- A huge benefit of having the registry on the python path is that other projects can use it, import it, and extend it. For example, \"from docker.registry.storage import Storage\", then make a subclass.\n- Another benefit is that it makes packaging for an OS easier.\n- Manipulating sys.path is risky, and it could cause OS distributions to be hesitant about including docker-registry. However, making an rpm/deb/whatever from a standard python package is very straight-forward and a common/well-understood process.\n. docker-py does indeed already have the docker namespace, so I'm going with docker_registry.\nI've already done most of the work of moving things around, and all the tests pass. Now I just need to hand-test running it, and update things like build.sh and Dockerfile. So I should be able to submit a PR in the near future, and hopefully it won't require too much back-and-forth in the review process. :)\nSo @wking , I am hopeful that the restructure will be done very soon. I think it won't be difficult to rebase, as I've left the structure within each module alone. Everything in lib looks the same, everything in storage looks the same, etc. Just the import statements are different.\n. +1, would be very useful at $WORK. Storing the path with authentication state is a good idea. Any thoughts on how this could work in a scenario without authentication? I suppose if this feature was only available when authentication is in use, that could be acceptable. And it would encourage people to use authentication!\n. Great feature. I actually just went scouring through the existing code looking for this, and I'm very happy to see it in a PR. At $WORK, we may end up creating a new storage plugin for our purposes, and this feature is a must-have.\n. Looks like the travis build failed because it couldn't connect to github. Can someone with access restart that build? There should be a button on the build page: http://stackoverflow.com/questions/17606874/trigger-a-travis-ci-rebuild-without-pushing-a-commit\nUPDATE: looks like someone did restart it\n. I made the changes requested by @wking and am working on replacing the run.sh with a python console-script. The latter is a little more complex than one might expect, but I have something working now that just needs a little polish.\n. I replaced run.sh with a console script, so this is ready for another review.\n. 1. Whatever the directory holding the code is named will be what people import, and it will be on the python path (as our root module), sharing the global python namespace. As this branch currently stands, you can import docker_registry. \"registry\" would be a very generic name to be in the global namespace. It's a best practice for the python package name to be roughly equivalent to the name used by the root module, and I don't think \"registry\" would make a good package name. Perhaps the nail in that coffin is that there is already a package called \"registry\" on pypi, and it's a Windows registry API.\nAs for \"-\" vs. \"_\", that's a quirk of python package naming. \"-\" is illegal in a variable name, so we can't use that in the root module name. But PEP8 says of package names: \"the use of underscores is discouraged.\" The generally accepted way to reconcile this discrepancy is to name a package \"a-b\" and name its root module \"a_b\". For example, if you go on pypi and search for \"django\", you'll see a long list of packages named \"django-x\" whose module is named \"django_x\".\nIt might be more clear to put this is usability terms; you pip install <package-name>, and import <root_module_name>.\nSo my strong advice from a pure python packaging perspective is to go with the names that are in this branch, but I will quite happily change the branch to use any variation you prefer. Or if it would be helpful to continue this discussion, I'm happy to answer any additional questions, or elaborate on the above. \n2. The docker-registry console script I created does exactly what run.sh did. The only change is that the new console script adds help/usage text. As for its location, (I apologize if this is old news to you) a benefit of having a standard python package is that you can write a function to be your console script without actually having to make something like bin/docker-registry. When you install the python package, such as with pip, python will auto-generate a skeleton file in the \"right place\" for your OS that calls the specified function. So in my branch, you won't find a docker-registry executable, but you will find an entry point in setup.py defining where the function is that gets called by the generated executable.\nGiven that I ported the run.sh script to fit in with the python package, do you think it would be reasonable to save behavioral changes for another PR?\n. Thanks for catching the package name! I have fixed that, and I improved the doc block for \"run_gunicorn\".\nI don't feel strongly about where the run_gunicorn function lives.\n. Did you consider the \"entry points\" feature of python? Maybe it's too late to consider a different approach, but that feature is designed for plugin/extension loading. It would alleviate the need for special import handling, and it has the advantage of keeping third-party code out of your namespace.\nEntry points will also avoid any possible packaging conflicts over what package owns docker_registry/__init__.py and docker_registry/extensions/__init__.py. And by \"packaging\", I'm thinking rpm and deb packaging. It's a problem that could be worked around, but it would mean that extension packages would have to do extra work beyond the usual setup.py build and setup.py install.\nJust a thought. I'm happy to elaborate if that would be helpful.\n. pkg_resources is not in the standard library, so you probably need to add it to the requirements file (I don't see it listed there now). It is distributed with setuptools.\n. I'm making some assumptions here about your release process. I'm happy to undo this or change it to a different version if that's preferable.\n. I wasn't real sure what to put here or for the author_email, so just let me know if you prefer something different.\n. I think 302 would be more broadly useful. I'm thinking especially about a backend that generates signed URLs that expire, which is something S3 can actually do. 301 means that future requests should use the content_redirect_url instead of the url to this view. However, I think most use cases would benefit from having future requests for this resource still come to this URL.\nGenerally speaking, if we want to\n- change the redirect's destination URL from time to time (in order to do load balancing, signed URLs, or any other purpose)\n- authorize access\n- log access\nthen 302 is the way to go.\n. perhaps 4 instead of 7?\n. You might consider list(pkg_resources.iter_entry_points('docker_registry.extensions')) instead of doing a list comprehension without item transformation.\n. I'm really splitting hairs, so don't think there's anything wrong with what you did. It just seems that what you're trying to do is take what iter_entry_points() returns and make it a list. If that's all you want, then just casting it as a list is the simple and direct thing to do.\nHowever, if you want to do item transformation, then a list comprehension would be useful. You could, for example, return [ep.module_name for ep in pkg_resources.iter_entry_points('docker_registry.extensions')] to return a list of the module names instead of a list of EntryPoint objects.\nBut like I said, nothing about what you did is incorrect. I'm just butting in with a minor drive-by unsolicited suggestion. ;)\n. ",
    "wking": "On Fri, Mar 21, 2014 at 12:45:31PM -0700, Michael Hrivnak wrote:\n\nOne simple question is what the namespace should be. I suggest\n\"docker.registry\" as a natural, predictable namespace. from\ndocker.registry.storage import Storage for example. Or if you want\nto keep this distinct from the \"docker\" namespace, you could choose\n\"docker_registry\", as in from docker_registry.storage import\nStorage.\n\nI'm not a maintainer, but my personal preference is for\ndocker_registry, to force a clear distinction between this server-side\nstuff and the the client-side docker package 1.  We also avoid the\nnamespace package shenanigans needed to graft the registry code onto\nan existing (or not?) docker-py install [2,3].\n. On Fri, Mar 21, 2014 at 12:45:31PM -0700, Michael Hrivnak wrote:\n\nI am willing to do this work. Should we have any discussion before I\nbegin?\n\nAlso, as I pointed out earlier 1, this restructuring would conflict\nwith my outstanding #247.  A restructuring PR in parallel with #247\nwould be possible, but a bit awkward, and I'd like to minimize the\noverlap period while both are cooking.  I'm happy if that's by merging\n247 before starting on this, or by doing this restructuring and\nlanding it quickly, after which I can rebase #247.  I just don't want\nboth a restructuring and #247 to drag on through multiple future\niterations before one of them lands.\n. Now that #297 is landed, future releases should get pushed to PyPI and\nwe can close this issue.\n. On Fri, Feb 28, 2014 at 05:45:10PM -0800, stoopsj wrote:\n\nI can avoid pulling all the previous attempts by removing tags other\nthan the one corresponding to the id in\n$storage_path/repositories/library/customubuntu/tag_12.04 from\n_index_images.\n\nIn #247 I've wired delete_tag into a restored DELETE\n/v1/repositories/<path:repository>/ endpoint (with a8db686), so it\nshould be easy to remove the earlier tags before pushing your new\nimage.  That will keep clients from pulling the old images.  There's\nno refcounting for the images themselves though (yet), so they'll\nstill be stored in the registry itself.  It looks like delete_tag\nneeds to be adjusted to clear old entries out of index_images too, it\ncurrently just removes the tag{tag_name} file (Storage.tag_path) and\nnot the tag{tag_name}_json file (Storage.repository_tag_json_path) or\nthe entries in _index_images (Storage.index_images_path).\n. On Sat, Mar 01, 2014 at 08:08:39AM -0800, W. Trevor King wrote:\n\nIt looks like delete_tag needs to be adjusted to clear old entries\nout of index_images too, it currently just removes the\ntag{tag_name} file (Storage.tag_path) and not the\ntag{tag_name}_json file (Storage.repository_tag_json_path) or the\nentries in _index_images (Storage.index_images_path).\n\nI've addressed repository_tag_json_path and repository_json_path\nremoval in abbb8dd (tags: Extend delete_tag to remove additional\nmetadata files, 2014-03-01) as part of the repository-deletion cleanup\nthat leads off #247.\n. On Sat, Mar 01, 2014 at 08:08:39AM -0800, W. Trevor King wrote:\n\nIt looks like delete_tag needs to be adjusted to clear old entries\nout of index_images too, it currently just removes the\ntag{tag_name} file (Storage.tag_path) and not the\ntag{tag_name}_json file (Storage.repository_tag_json_path) or the\nentries in _index_images (Storage.index_images_path).\n\nAfter looking into the pull logic more closely, it doesn't look like\nold id/checksum pairs in Storage.index_images_path are a problem.  The\nproblem seems to be the presence of 'Tag' entries, which throws off\nthe server-side pull logic.  I've posted a fix an a more detailed\nanalysis in #266.\nBetween #266 and #247 I think the client-side of issue is handled on\nthe ${storage}/repositories/ front.  The only thing remaining is to\nremove unreferenced images from ${storage}/images/, but that's just a\nregistry-side optimization.\n. On Sat, Mar 01, 2014 at 10:48:41AM -0800, W. Trevor King wrote:\n\nBetween #266 and #247 I think the client-side of issue is handled on\nthe ${storage}/repositories/ front.\n\nThese have both landed now.\n\nThe only thing remaining is to remove unreferenced images from\n${storage}/images/, but that's just a registry-side optimization.\n\nI think this registry-side cleanup is the last bit of image deletion\nthat isn't done yet.\n. On Fri, May 09, 2014 at 08:29:00AM -0700, Jake Goulding wrote:\n\n\nthere are multiple entries for tags\n\n@stoopsj thanks so much for posting this! We were suffering from\npulling far too much, and this seems to have been the cause. Here is\n[the script][script] that I ran to clean up all of our repositories\n_index_images in one fell swoop!\n\nI don't think you need any Tag entries in _index_images.  See #266\nfor the 5-liner I used to clean up my own registry.\n. The rename from DELETE /v1/repositories/<namespace>/<path:repository>/ (which matches the spec) to DELETE /v1/repositories/<namespace>/<path:repository>/tags (not in the spec) happened in 1026a87 (renaming DELETE call on a repos, 2013-06-26), but the motivation behind the rename is unclear.  I'm also not sure why a delete_repository function is in the registry.tags module (which already has the expected delete_tag method).\n. With #266, #268, and #247 landed, I think this issue can be closed in favor of #7 (which I think is only waiting on refcounted image removal).\n. It looks like docker-registry has had a search stub since 82cfd0b (add index routes, 2013-06-14), but I'm not sure how to search my local registry, because docker search doesn't take an -index HOST:PORT argument.  It seems like this client searches should hit the index directly instead of going through the local docker daemon, but that's not how Docker works at the moment.  Without this, it seems like a docker-registry implementation of /v1/search would be pretty useless.  Maybe users are supposed to wget/curl their local registry directly?\n. I'm trying to figure out how difficult this would be, and it seems like it may be easier to implement the indexer as a stand-alone project, that interacts with this registry using the usual API.  Am I missing some reason for why the index and registry should be tied together?\n. On Fri, Mar 07, 2014 at 10:39:40AM -0800, Victor Vieux wrote:\n\n@b83s I guess this works only when you use the local flavor. \nIf we do this, it needs to be baked in the registry to work with all\nthe flavors (S3, Glance, ...)\n\nThere's a backend-agnostic search implementation (with a local SQLite\ndatabase, but an extensible framework) in #247.  If you want a full\nlist, just skip the query term:\n$ curl -XGET http://registry.example.net/v1/search\n  {\n    \"num_results\": 27,\n    \"query\": \"\",\n    \"results\": [\n        {\n            \"description\": \"\",\n            \"name\": \"wking/portage\"\n        },\n        {\n            \"description\": \"\",\n            \"name\": \"wking/nginx-proxy\"\n        },\n        {\n            \"description\": \"\",\n            \"name\": \"wking/redis\"\n        },\n    \u2026\n  }\nWe don't currently store descriptions anywhere in docker-registry,\nbut with #247 listing is easy.\n. On Fri, Mar 07, 2014 at 02:39:21PM -0800, W. Trevor King wrote:\n\nThere's a backend-agnostic search implementation (with a local\nSQLite database, but an extensible framework) in #247.\n\nNow that #247 has landed, I think this can be closed.\n. See also: dotcloud/docker#2700.\n. I think this was fixed with aae74db0 (change Content-Type & expand bytes ranges support, 2014-04-09, #315).\n. At least (as of 6ca389b, Bumped version to 0.6.8, 2014-04-16, which actually landed a few commits after the 0.6.8 tag), I'm getting:\n$ wget -S --header 'Authorization: Token signature=123abc,repository=\"wking/gentoo\",access=read' http://my-registry.net:5000/v1/images/188e0b82abcea4c7c29a299a5da66a3659ebbdf5bac9398673a642fb8466951b/layer\n\u2026\n  Content-Type: application/octet-stream\n  X-Docker-Registry-Version: 0.6.8\n  X-Docker-Registry-Config: dev\n\u2026\n. Was this fixed by #141?\n. I think this has been supported since the standalone option was added in 5f43748 (Fake Index module can now be loaded according to a configuration directive named standalone, 2013-06-24).\n. This is a side-effect of your packaging (#1).  If you setup a registry logger, lower level logging (e.g. from registry.app) will propagate up the chain and be filtered by your registry-level settings.\n. On Fri, Aug 15, 2014 at 04:55:50PM -0700, Olivier Gambier wrote:\n\nstill an issue now that we repackaged everything?\n\nLooks like no.  Actually, since basicConfig configures the root logger\n1 and that landed in bb1cbd5b (Fixed various security problems and\nencoding problems between the Index and Registry, 2013-04-25), I'd be\nsurprised if this was even an issue when this was opened on\n2014-12-10.  Sorry for the noise.\n. Instead of periodically checking for orphans, I'd expect it would be\neasier to refcount images and remove them when their reference count\ndrops to zero.  You'd need some temporary code to fill in the initial\nrefcounts for folks with existing storage instances that haven't been\nrefcounting, but you could remove that migration code after a suitable\ntransition period.\n. On Thu, Mar 27, 2014 at 10:02:47AM -0700, Sam Alba wrote:\n\nI agree refcounting is better.\n\nI think this overlaps with #7.  I think we should consolidate into a\nsingle \u201cremove refcounted images\u201d issue, and don't mind if it's this\none or #7.\n. Hmm, it's possible that the correct target path is config/config.yml, which is what the Dockerfile uses.\n. The correct target path is config/config.yml.  With 94237d3, I've updated the docs accordingly.\n. On Wed, Feb 12, 2014 at 01:56:19PM -0800, Sam Alba wrote:\n\nI know this is WIP but here are some early feedback:\n\nThanks :)\n\n\nI don't understand why you need to modify several API endpoint\n   for implementing search, can it be just implementing the\n   \"/search\" in the Index stub class?\n\n\nI needed to add repository created / updated / deleted signals, and\nthe existing delete_repository function was in the wrong place for\nthat.  The earlier commits in this series just set the stage for the\nrepository deletion cleanup.\n\n\nI'd use sqlalchemy for storing the search index.\n\n\nOk.  Any pointers on setting that up in the tox tests?\n\n\nThe search must be optional like some other features on the\n   registry. I should be able to use the registry as is, the search\n   would be just like it does today: nothing.\n\n\nOk.  I'll postpone registry.search.index creation until we know if the\nuser has enabled indexing.\n\nLooks like a big chunk, I think it'll need some discussions /\nimprovements / changes / testing before thinking about merging back\nto master.\n\nNo problem.  I'm working out some issues Travis has now.  I'll report\nback once I've addressed the SQLAlchemy and enable/disable config.\n. On Wed, Feb 12, 2014 at 02:10:26PM -0800, W. Trevor King wrote:\n\nNo problem.  I'm working out some issues Travis has now.\n\nAs of 976af52, the only py27 errors are those listed in #248, and I\njust had a successful local run that didn't hit them.\n. On Wed, Feb 12, 2014 at 02:10:26PM -0800, W. Trevor King wrote:\n\nI'll report back once I've addressed the SQLAlchemy and\nenable/disable config.\n\nThis should all be setup now with 8add1e2 (search: Use SQLAlchemy as a\nsearch-index backend, 2014-02-12).  If I get lucky and miss the #248\nerrors, all the py27 tests pass :).  However, I haven't added new\ntests to explicitly exercise the new search-index code, and there are\nstill some pep8 indentation issues I don't understand yet :p.\n. On Wed, Feb 12, 2014 at 07:56:33PM -0800, W. Trevor King wrote:\n\nWed, Feb 12, 2014 at 02:10:26PM -0800, W. Trevor King:\n\nI'll report back once I've addressed the SQLAlchemy and\nenable/disable config.\n\nThis should all be setup now with 8add1e2 (search: Use SQLAlchemy as a\nsearch-index backend, 2014-02-12).\n\nI'm currently hitting problems in local testing where each Gunicorn\nworker tries to initialize the database, and they collide:\nOperationalError: (OperationalError) table repository already exists\u2026\nI'm not sure how to fix that cleanly, because run.sh currently sets\n--debug.  That's mostly a no-op (benoitc/gunicorn#701), but it does\ndisable --preload (with gunicorn v18.0).  If I remove --debug from\nour run.sh a --preload would work, but it would make future\n--reloads impossible.  Thoughts?\n. The current series (b32a4b3) passes all the tox tests except for:\n/home/travis/build/dotcloud/docker-registry/registry/index.py:110:80: E501 line too long (116 > 79 characters)\n  /home/travis/build/dotcloud/docker-registry/registry/index.py:111:80: E501 line too long (112 > 79 characters)\nfor the two URLs in the delete_repository docstring.  There's an\noutstanding pep8 issue to except URLs in docstrings from the\nline-length limitation (jcrocholl/pep8#224).  I think the URLs are\nuseful enough that we should ignore pep8 in this particular case.\n. On Mon, Feb 17, 2014 at 07:37:13AM -0800, Lee Trout wrote:\n\nI'd like to suggest abstracting this out in to \"backends\" so that it\nis pluggable and shipping the current work as the default SQLAlchemy\nbackend.\n\nGood idea.  Done in 9906bda (index: Factor out the SQLAlchemy Index\ninto flexible backends, 2014-02-17).  The config interface is slightly\ndifferent than your proposal, since I tried to mirror the storage\nbackend implementation (although I'm not wild about having these\nbackends in lib/ and thus the global namespace.  I'd prefer\nregistry.storage and registry.index to avoid messing with the global\npackage namespace).\n\nAnd if I wanted to use a custom Xapian backend from my own lib I\ncould -e SEARCH_BACKEND foo.backends.xapian -e SEARCH_INDEX\n/var/docker/registry-index.\n\nDone in 6c9616d (index: Expand load() to handle arbitrary module\nnames, 2014-02-17), but the custom backends have to call their\nimplementation class 'Index' and handle config-loading internally.\n. On Mon, Feb 17, 2014 at 12:45:13PM -0800, W. Trevor King wrote:\n\nI tried to mirror the storage backend implementation (although I'm\nnot wild about having these backends in lib/ and thus the global\nnamespace.  I'd prefer registry.storage and registry.index to avoid\nmessing with the global package namespace).\n\nFrom #1, it looks like lib/ may be deprecated anyway.  Should I move\nlib/index/ under registry/search?  Something like:\nregistry/\n  -- search/\n      |-- __index__.py-- sqlalchemy.py\nwhere registry/search/index.py would include both my current\nregistry/search.py and lib/index/init.py.\n. Is there anything I can do to help this along?\n. On Tue, Feb 25, 2014 at 03:26:48PM -0800, Victor Vieux wrote:\n\nmaybe you could start by fixing travis:\nhttps://travis-ci.org/dotcloud/docker-registry/builds/19065311\n\nI explain my personal position on those long-line errors here 1, but\nI'm happy to make any changes that folks want to that docstring or the\npep8 configs.  I just need to know what those preferences are ;).\n. On Tue, Feb 25, 2014 at 04:11:02PM -0800, Sam Alba wrote:\n\n@wking Tests have to pass. I agree with you to make an exception\nhere. You can actually force flake8 to ignore those lines. But\nanyway, tests have to pass.\n\nThere are at least two ways I can think of to get them to pass:\n- Ignore all E501 errors, which lets me have my docstring the way I\n  like it, but means we won't catch other long lines that should be\n  shortened.\n- Add '# nopep8' tags to those lines, which makes those lines even\n  longer, but means we still catch other long lines.\nFrom your \u201cthose lines\u201d phrasing I'm expecting you prefer the latter,\nand I've gone that way with 2335f63 (registry/index.py: Add '# nopep8'\ntags to long URL lines, 2014-02-25).  Now Travis is happy 1.\n\nThe PR is fairly big (and contains some stuff unrelated ;-),\n\nI think it's all related, as I tried to explain earlier 2.  If you\nsee something that can be cut out into a separate PR, let me know and\nI'll refactor accordingly.\n\nso I need some more time to do another round of review.\n\nI understand that reviewing this is not something you'll dash off in\nhalf an hour, so I try and wait at least a week between bumps ;).\nBesides the long-line issue which we've just resolved, there are two\nother policy decisions where I could use some maintainer input:\n- Is '--preload' acceptable for database initialization 3?\n- Is lib/ the prefered location for the search index backends (or\n  should they be under registry/search/) 4?\nI'm happy to do whatever you like on either of those, but I'm not sure\nwhat your preferences are.\n. On Sun, Mar 02, 2014 at 09:39:05AM -0800, Sam Alba wrote:\n\n\nRe-submit a PR with changes to tags (delete, lists, etc...)\n\n\nDone with #268, which grabs the non-search commits from this PR.  The\nonly change is a tweak to the commit message from 40fb153 to ac29c36.\n\n\nRe-submit a PR later for search\n\n\nI'll rebase this one on top of #268 after that lands.\n. On Sun, Mar 02, 2014 at 10:16:25AM -0800, W. Trevor King wrote:\n\nI'll rebase this one on top of #268 after that lands.\n\nI just pushed a rebased version of this PR.\n. On Mon, Mar 03, 2014 at 07:54:49AM -0800, W. Trevor King wrote:\n\nI just pushed a rebased version of this PR.\n\nIs there any further restructuring I can do to help this along?\n. On Thu, Mar 27, 2014 at 06:30:24AM -0700, Joffrey F wrote:\n\nLGTM!\n\nIs there anything else I can do to get this landed?\n. On Mon, Apr 07, 2014 at 09:27:16AM -0700, Sam Alba wrote:\n\nI'll review today. On my todo list.\n\nNo worries.  I'm not in a particular hurry, just making sure it\ndoesn't fall off the radar entirely ;).\n. On Mon, Apr 07, 2014 at 12:15:08PM -0700, Sam Alba wrote:\n\nHowever how to you plan to index a non-indexed registry?\n\nI didn't plan to index non-indexed registrys, because the search\nendpoint is part of the index API 1.  Why would folks be searching a\nnon-indexed registry?\n. On Mon, Apr 07, 2014 at 12:23:16PM -0700, Sam Alba wrote:\n\nLet's say I have my registry hosted somewhere and I want to enable\nyour search feature. I already have 1000 images pushed to it, how I\nindex them?\n\nWhen the index DB is initialized, SQLAlchemyIndex._generate_index uses\n_walk_storage to traverse the existing repositories and index them.\n\nAlso I spotted a possible issue, do you handle pagination? What\nhappens if my search query matches 100K images, I guess they are all\nreturned in results with the current implementation.\n\nYup.  I don't see any limit-to-#-results settings in the search API\n1.\n. I just pushed a rebased a3b51f4, dropping the all from\nregistry/search.py 1.  To make the next round of review easier, here\nare the outstanding issues from today's review (as I see it) and my\nresponses:\n- Sam doesn't like my run.sh tweaks 2, but I need them [3,4].\n- Sam thinks my absolute_import is inconsistent 5, and my ideal\n  solution would be to add absolute_import everywhere 6.\nI think the linked comments express my positions clearly (but let me\nknow if you want me to clarify anything).  Let me know what you want\nme to do to make this mergable.  I'm happy to sacrifice my principles\nto get this landed, but I'd like clear targets before heading over to\nthe dark side ;).\n. On Tue, Apr 08, 2014 at 05:24:37AM -0700, Joffrey F wrote:\n\n\nJust renaming sqlalchemy.py to something like db.py,\n  models.py or data.py\u2026\n\n\nI can do that, but it might make things confusing if someone wants to\nadd an in-tree indexer based on the standard library's sqlite3 module\n(lib.indexer.db2?).  Maybe it can be a temporary workaround until we\nland a global absolute_import PR?\n. On Tue, Apr 08, 2014 at 08:37:36AM -0700, Sam Alba wrote:\n\nWhat do you guys think? I am ok to merge.\n@wking One last rebase would be welcome :-)\n\nWhat am I rebasing?  sqlalchemy \u2192 db and dropping absolute_import?\n. On Mon, Apr 14, 2014 at 05:46:59PM -0700, Sam Alba wrote:\n\n\nWhat am I rebasing?  sqlalchemy \u2192 db and dropping absolute_import?\n\nYes, if you agree with this.\n\nRebased and pushed.\n. On Tue, Apr 15, 2014 at 10:44:19AM -0700, Sam Alba wrote:\n\nWeird, I still cannot merge...\n@wking Did you correctly rebased on top of master?\n\nI had been rebasing on the initial commit from which my branch was\nbased.  I just rebased on master now, and had to resolve a trivial\nconflict in README.md (this commit and #308 both added sections at the\nsame place).\n. On Tue, Apr 15, 2014 at 08:31:39PM -0700, Sam Alba wrote:\n\nHmm, could you check why Travis is complaining?\n\nAh, it's because without absolute_import,\nimport index\nis importing registry.index instead of lib.index.  We can either\nrestore the absolute_import or use something like:\nimport lib.index as index\nI've rebased and pushed the latter, but let me know if you'd prefer\nthe absolute_import approach for that module.\n. On Tue, Jul 15, 2014 at 06:40:12PM -0700, dbason wrote:\n\nI've tried running the docker-registry image with sqlalchemy enabled\nbut it's trying to pull down an sqlachemy image and there are none\nin the docker registry.  So I guess the options are to build my own\nsqlaclchemy image or put it into the docker-registry image.\n\nSQLAlchemy has to be inside the docker-registry image.  Here's where I\ninstall SQLAlchemy (and the other docker-registry dependencies) in my\nown Dockerfile template 1.  I expect the canonical solution to this\n(for folks using the official Dockerfile), would be to add an\n\u2018sqlalchemy\u2019 entry to \u2018extras_require\u2019 2, and install with 3:\nrun pip install file:///docker-registry#egg=docker-registry[bugsnag,sqlalchemy]\n. On Tue, Jul 15, 2014 at 07:14:58PM -0700, W. Trevor King wrote:\n\nI expect the canonical solution to this (for folks using the\nofficial Dockerfile), would be to add an \u2018sqlalchemy\u2019 entry to\n\u2018extras_require\u2019 \u2026\n\nScratch that, sqlalchemy is listed in the base dependencies (currently\nrequirements/main.txt).  So I don't see what the problem is ;).\n. On Tue, May 20, 2014 at 07:20:38AM -0700, Mangled Deutz wrote:\n\nHaven't seen this for a while.\n\nProbably because we don't look for it anymore: a1f3ed62 (Disabling\nbroken tests temporarily, 2014-03-11).\n. On Tue, Feb 18, 2014 at 04:40:18PM -0800, jamie brim wrote:\n\nwhen only the registry is hosted on AWS and docker is not, docker is\nable to pull from the registry, but at very slow speeds (20k/s).\n\nI don't know much about Flask, but it would be nice to use sendfile to\nstream image layers if the kernel supports it.  Unfortunately, native\nsendfile support only landed in Python 3.3 1.  It looks like there\nis some Flask-side support though 2, so maybe it wouldn't be too\ndifficult to do.  The current storage.local.LocalStorage.stream_read\nreads the file into local memory before passing it on to Flask, which\nseems inefficient.\n. On Wed, Feb 19, 2014 at 10:26:00AM -0800, jamie brim wrote:\n\ni haven't tried the registry container, how would i configure it?\nthere doesn't seem to be a default config.yml bundled with it.\n\nIt should use config_sample.yml because of this Dockerfile line 1.\nYou can use environment variable to tune the config to match your\nlocal setup.\n. While this patch brings us closer to the API specs, I'm not entirely sold on the specs themselves.  It seems like tags should be an index-side attribute and the registry should just focus on images (layers, metadata, and ancestry).\n. The Travis failures look like #248, maybe that should be re-opened?\n. On Sun, Mar 02, 2014 at 08:54:39AM -0800, Sam Alba wrote:\n\nWhich other data gets removed by doing this whitelisting?\n\nI didn't find anything else besides 'Tag' in my registry.  You can\ncheck locally by using a 'diff' in the for loop (instead of\noverwriting the original with the filtered index).  Theoretically,\nthere could be any number of entries getting removed, since folks can\ncurrently upload whatever they want here.\n. This addresses #45 by fixing the delete_repository URL 1, filling in tag-deleted signals, and triggering a new repository-deleted signal.  It also addresses #7 for client interactions 2, although without the refcounted image removal we're still potentially wasting space on the registry itself.\n. On Sun, Mar 02, 2014 at 02:37:52PM -0800, Sam Alba wrote:\n\nDELETE /v1/repositories/foo/bar/tags is currently used internally.\n\nThe endpoint is used internally, or the logic attached to it?  I\ncouldn't find someone calling DELETE on that URL internally (except\nfor the tests I fixed).  If some other code uses that endpoint, I'd\nsuggest the:\ndef delete_repository(\u2026):\n      # logic here\n@app.route('/v1/repositories/path:repository/', methods=['DELETE'])\n  @toolkit.parse_repository_name\n  @toolkit.requires_auth\n  def _delete_repository(namespace, repository):\n      # minimal Flask wrapper here\nlike I do in 764a2ef and 62e7cb0.  No sense in going through a socket\nwhen you can call the Python directly.\n\nWhen the Registry is configured with \"standalone: False\", it relies\non an external entity to act as an \"Index\" and all routes inside the\nmodule \"index.py\" are then not used.\n\nAh, that makes sense.\n\nWe leave the DELETE /v1/repositories/foo/bar/ code in the tags.py\nmodule, which means that the query will work if the index.py module\nis used or not (standalone True/False).\n\nDone in 0a4eeec (tags: Restore delete_repository URL to the spec\nvalue, 2014-02-12).\n. On Sun, Mar 02, 2014 at 06:02:44PM -0800, Sam Alba wrote:\n\ncould you add the old route for deleting tags so we allow both (so\nwe ensure backward compatibility)?\n\nIs the plan to split off the /tags handler once\nregistry.tags.delete_repository grows support for removing\nunreferenced images?  Or are /tags users comfortable with the full\nrepository deletion?  It's hard to imagine someone wanting to remove\nall the tags for a repository but not cleanup unreferenced images,\nbut I can imagine folks being surprised when a /tags URL removes more\nthan just tags.\n. On Wed, Jun 04, 2014 at 11:58:47AM -0700, Sam Alba wrote:\n\nredis.exceptions.ConnectionError: Error 111 connecting registry-docker.azva.dotcloud.net:2755. Connection refused.\n\u2026\nredis/client.py:705 get\ndocker_registry/core/lru.py:80 wrapper\n\u2026\n\nIs it more complicated than:\ntry:\n    content = redis_conn.get(key)\nexcept redis.exceptions.ConnectionError as e:\n    logger.warning(e)\n    content = None\n?  I'm not sure if the Redis client will handle reconnects internally,\nor if you'd need extra handling to get that to work.\n. On Mon, Mar 03, 2014 at 12:39:46PM -0800, Greg Weber wrote:\n\n\ndocument mounting a configuration file\n\n\nThe rest of the README uses unindented code fences for example\ncommands:\ndocker run -p 5000:5000 -v /home/user/registry:/registry -e DOCKER_REGISTRY_CONFIG=/registry/config.yml\nYou also have two spaces before your -e.\nI've been using the stock config file and environment variables\nmyself, but it seems easier to me to skip DOCKER_REGISTRY_CONFIG and\njust mount the file to the default location:\n\u2026 -v /home/user/registry/config.yml:/registry/config/config.yml \u2026\n. On Mon, Mar 03, 2014 at 01:33:00PM -0800, Greg Weber wrote:\n\nok, I will lelt you commit it with proper syntax that is faster than\ntalking about it\n\nI'm not the maintainer, so I'd just be making another PR.  I'll punt\nback to you ;).\n. On Wed, Mar 12, 2014 at 01:15:59AM -0700, lcheng61 wrote:\n\nFile \"/home/lcheng/Project/docker-registry/registry/tags.py\", line 74\n    for tag_name, tag_content\n      ^\nSyntaxError: invalid syntax\n\nThat's part of a dict comprehension (PEP 274 1), and requires Python\n2.7 or 3.x.  Were you using Python 2.6?\n. On Sun, Mar 23, 2014 at 01:41:54PM -0700, Michael Hrivnak wrote:\n\n\nrearranging code into a proper python package, per #1\n\n\nI'd prefer relative imports here.  For example, in\ndocker_registry/init.py:\nfrom .lib import config\nto the current (as of 555dbb2):\nfrom docker_registry.lib import config\nImporting just the final module (without the associated namespacing)\nwill also allow for shorter lines.  For example, in\ntest/test_images.py,\nimages.cfg._config['nginx_x_accel_redirect'] = accel_prefix\ninstead of the current:\ndocker_registry.images.cfg._config['nginx_x_accel_redirect'] \\\n      = accel_prefix\nYou can also avoid some wrapped lines in your setup.py by storing\nos.path.dirname(os.path.abspath(file)) in a local variable:\n_this_dir = os.path.dirname(os.path.abspath(file))\n  desc_path = os.path.join(_this_dir, 'README.md')\n  \u2026\n  req_path = os.path.join(_this_dir, 'requirements.txt')\nwhich reads more easily to me.\nI think that this would be a good time to move registry.app.VERSION to\nthe more conventional docker_registry.version, and also shift the\nconfig import to a less central location (wsgi.py?), although those\ncan certainly happen in subsequent commits in this PR.\n. On Tue, Apr 15, 2014 at 10:01:25PM -0700, Michael Hrivnak wrote:\n\n\n\u2026 The generally accepted way to reconcile this discrepancy is to\n   name a package \"a-b\" and name its root module \"a_b\"\u2026 So my strong\n   advice from a pure python packaging perspective is to go with the\n   names that are in this branch\u2026\n\n\nOutsider vote for package name 'docker-registry' and root module\n'docker_registry'.  It looks like the current tip of this PR (bf3360a)\nis using 'docker_registry' for both.\n\n\nThe docker-registry console script I created\u2026\n\n\nI'd prefer if docker_registry.run_gunicorn was moved to\ndocker_registry.gunicorn.run.  This would avoid cluttering\ndocker_registry/init.py, which is complicated enough with it's\nexisting import-to-trigger-side-effects behaviour.  As a side benefit,\nDESCRIPTION could be moved to docker_registry.gunicorn.doc.\nAnother minor quibble is the lack of a summary line 1 for\nrun_gunicorn.doc.\n. On Tue, Apr 22, 2014 at 10:34:26AM -0700, Joffrey F wrote:\n\nMerged #297.\n\nThis maybe should have been rebased after #247 landed.  Do we want a\nnew PR to shift lib/index?\n. On Tue, Apr 22, 2014 at 10:45:28AM -0700, Joffrey F wrote:\n\nI've rebased it before merging, see\nhttps://github.com/dotcloud/docker-registry/commit/9baaddaf2a836f047e785c78ffa27fa35d036a2a\n. If you spot any inconsistency or see something I missed don't\nhesitate to let me know / call me out on it, but I think we're good.\n\nAh, I'd missed that.  9baaddaf2 looks good to me.\n. On Wed, Apr 02, 2014 at 08:05:56AM -0700, hufman wrote:\n\nIt does, but it provides a simpler implementation that does not\nrequire a database backend.\n\nWhich is what #247 had before @samalba's 1:\n\nI'd use sqlalchemy for storing the search index\n\nAlthough my original implementation (e.g. 976af52, search: Stub out a\npreliminary search index, 2014-02-12) still used signals to manage\nentries in an in-storage index, instead of using a directory listing\nfor each /v1/search request.  I like this approach better for\nsearching names, but I like my old index-file approach better for\nsearching both names and descriptions.  We don't store descriptions at\nthe moment, but I think we should, and I think an indexing\nimplementation should not have to change radically when we do.\nWalking the filesystem to extract descriptions is not going to scale\nup well.\nHowever, with the current #247 code (c9e438c, index: Expand load() to\nhandle arbitrary module names, 2014-02-17), it's easy to drop in your\nown indexing module that works however you like ;).\n. On Mon, Apr 14, 2014 at 06:13:34PM -0700, Sridhar Ratnakumar wrote:\n\nthe 'tags' file is created as an extra. the original tag_* files\ncontinue to be read/updated as before.\n\nI'm not a maintainer, but this sounds reasonable to me.  Since you're\nrecalculating this file after every tag change, I'd rather\nregistry.tags._get_tags just served that file from the disk (instead\nof recalculating on the fly).\n\n\nthat's tricky to roll-out on Registries with existing dataset.\n\nthere is the rename of '_index_images' to 'images. to support\nexisting data set, i suppose i can change this to make 'images' a\ncopy of '_index_images.\n\nThis rename sounds good to me.  I wouldn't keep a copy (because it's\nnot very DRY).  Ideally, we'd version the storage backend, and have\ninitialization check that version and perform any required migrations\n(e.g. \u201cyou're using storage layout 1.0, but I'm docker-registry 1.2.3\nand I expect storage layout 1.2.  I'll run the\nupgrade_storage_from_1_0_to_1_1 and upgrade_storage_from_1_1_to_1_2\nfunctions before I start serving.\u201d).  If we wanted to add this\nversioning now, we could interpret the lack of a $ROOT/version file as\n\u201cstorage layout 1.0\u201d.\n. On Wed, Apr 09, 2014 at 12:58:57PM -0700, Marcus Ramberg wrote:\n\n172.17.42.1 - - [09/Apr/2014:19:08:23] \"GET /v1/repositories/library/hello/tags HTTP/1.1\" 200 2 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:08:23,986 INFO: 172.17.42.1 - - [09/Apr/2014:19:08:23] \"GET /v1/repositories/library/hello/tags HTTP/1.1\" 200 2 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:08:24] \"GET /v1/images/242172258e6f899e7198d467184fb8765e979f6c3ab7b33ee8d76fb1f76ee139/ancestry HTTP/1.1\" 404 28 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:08:24,058 INFO: 172.17.42.1 - - [09/Apr/2014:19:08:24] \"GET /v1/images/242172258e6f899e7198d467184fb8765e979f6c3ab7b33ee8d76fb1f76ee139/ancestry HTTP/1.1\" 404 28 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n\nWhat do you get from:\n$ curl -XGET http://registry.nordaaker.com/v1/repositories/library/hello/tags\nIt should be an object with tag keys and layer-id values.\n. On Sun, Apr 20, 2014 at 01:28:21AM -0700, Marcus Ramberg wrote:\n\nI just get {} \n- does that mean I haven't pushed any tags? \n\nThat's what it sounds like, which is surprising.  Here's the end of a\nre-push for me (with Debian's 0.9.1~dfsg1-2, showing the tags getting\npushed:\n# docker.io push registry.example.net:5000/wking/gentoo\n  The push refers to a repository registry.example.net:5000/wking/gentoo\n  Sending image list\n  Pushing repository registry.example.net:5000/wking/gentoo (3 tags)\n  Image 188e0b82abce already pushed, skipping\n  Pushing tag for rev [188e0b82abce] on {http://registry.example.net:5000/v1/repositories/wking/gentoo/tags/20140206}\n  Image 0b36c39e768a already pushed, skipping\n  Pushing tag for rev [0b36c39e768a] on {http://registry.example.net:5000/v1/repositories/wking/gentoo/tags/20140410}\n  Pushing tag for rev [0b36c39e768a] on {http://registry.example.net:5000/v1/repositories/wking/gentoo/tags/latest}\nDocker logs:\n2014/04/21 12:02:14 POST /v1.10/images/registry.example.net:5000/wking/gentoo/push\n  [/var/lib/docker|d8456732] +job push(registry.example.net:5000/wking/gentoo)\n  [/var/lib/docker|d8456732] -job push(registry.example.net:5000/wking/gentoo) = OK (0)\nRegistry logs:\n\"10.1.1.1 - - [21/Apr/2014:19:02:16] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n  2014-04-21 19:02:16,781 INFO: \"10.1.1.1 - - [21/Apr/2014:19:02:16] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n  2014-04-21 19:02:16,783 DEBUG: check_session: Session is empty\n  \"10.1.1.1 - - [21/Apr/2014:19:02:17] \"PUT /v1/repositories/wking/gentoo/ HTTP/1.1\" 200 2 \"-\" \"docker/0.9.1 go/go1.2.1 git-commit/3600720 kernel/3.13.0-1-amd64 os/linux arch/amd64\"\n  2014-04-21 19:02:17,018 INFO: \"10.1.1.1 - - [21/Apr/2014:19:02:17] \"PUT /v1/repositories/wking/gentoo/ HTTP/1.1\" 200 2 \"-\" \"docker/0.9.1 go/go1.2.1 git-commit/3600720 kernel/3.13.0-1-amd64 os/linux arch/amd64\"\n  2014-04-21 19:02:17,020 DEBUG: check_session: Session is empty\n  \"10.1.1.1 - - [21/Apr/2014:19:02:17] \"GET /v1/images/188e0b82abcea4c7c29a299a5da66a3659ebbdf5bac9398673a642fb8466951b/json HTTP/1.1\" 200 619 \"-\" \"docker/0.9.1 go/go1.2.1 git-commit/3600720 kernel/3.13.0-1-amd64 os/linux arch/amd64\"\n  2014-04-21 19:02:17,023 INFO: \"10.1.1.1 - - [21/Apr/2014:19:02:17] \"GET /v1/images/188e0b82abcea4c7c29a299a5da66a3659ebbdf5bac9398673a642fb8466951b/json HTTP/1.1\" 200 619 \"-\" \"docker/0.9.1 go/go1.2.1 git-commit/3600720 kernel/3.13.0-1-amd64 os/linux arch/amd64\"\n  2014-04-21 19:02:17,025 DEBUG: check_session: Session is empty\n  2014-04-21 19:02:17,025 DEBUG: [put_tag] namespace=wking; repository=gentoo; tag=20140206\n  \"10.1.1.1 - - [21/Apr/2014:19:02:17] \"PUT /v1/repositories/wking/gentoo/tags/20140206 HTTP/1.1\" 200 4 \"-\" \"docker/0.9.1 go/go1.2.1 git-commit/3600720 kernel/3.13.0-1-amd64 os/linux arch/amd64\"\n  2014-04-21 19:02:17,027 INFO: \"10.1.1.1 - - [21/Apr/2014:19:02:17] \"PUT /v1/repositories/wking/gentoo/tags/20140206 HTTP/1.1\" 200 4 \"-\" \"docker/0.9.1 go/go1.2.1 git-commit/3600720 kernel/3.13.0-1-amd64 os/linux arch/amd64\"\n  2014-04-21 19:02:17,029 DEBUG: check_session: Session is empty\n  \"10.1.1.1 - - [21/Apr/2014:19:02:17] \"GET /v1/images/0b36c39e768a1d0ba47d2481620f631bbb5727d48fbcc66c1bc4947271d1ceb7/json HTTP/1.1\" 200 618 \"-\" \"docker/0.9.1 go/go1.2.1 git-commit/3600720 kernel/3.13.0-1-amd64 os/linux arch/amd64\"\n  2014-04-21 19:02:17,032 INFO: \"10.1.1.1 - - [21/Apr/2014:19:02:17] \"GET /v1/images/0b36c39e768a1d0ba47d2481620f631bbb5727d48fbcc66c1bc4947271d1ceb7/json HTTP/1.1\" 200 618 \"-\" \"docker/0.9.1 go/go1.2.1 git-commit/3600720 kernel/3.13.0-1-amd64 os/linux arch/amd64\"\n  2014-04-21 19:02:17,033 DEBUG: check_session: Session is empty\n  2014-04-21 19:02:17,034 DEBUG: [put_tag] namespace=wking; repository=gentoo; tag=20140410\n  \"10.1.1.1 - - [21/Apr/2014:19:02:17] \"PUT /v1/repositories/wking/gentoo/tags/20140410 HTTP/1.1\" 200 4 \"-\" \"docker/0.9.1 go/go1.2.1 git-commit/3600720 kernel/3.13.0-1-amd64 os/linux arch/amd64\"\n  2014-04-21 19:02:17,035 INFO: \"10.1.1.1 - - [21/Apr/2014:19:02:17] \"PUT /v1/repositories/wking/gentoo/tags/20140410 HTTP/1.1\" 200 4 \"-\" \"docker/0.9.1 go/go1.2.1 git-commit/3600720 kernel/3.13.0-1-amd64 os/linux arch/amd64\"\n  2014-04-21 19:02:17,037 DEBUG: check_session: Session is empty\n  2014-04-21 19:02:17,037 DEBUG: [put_tag] namespace=wking; repository=gentoo; tag=latest\n  \"10.1.1.1 - - [21/Apr/2014:19:02:17] \"PUT /v1/repositories/wking/gentoo/tags/latest HTTP/1.1\" 200 4 \"-\" \"docker/0.9.1 go/go1.2.1 git-commit/3600720 kernel/3.13.0-1-amd64 os/linux arch/amd64\"\n  2014-04-21 19:02:17,039 INFO: \"10.1.1.1 - - [21/Apr/2014:19:02:17] \"PUT /v1/repositories/wking/gentoo/tags/latest HTTP/1.1\" 200 4 \"-\" \"docker/0.9.1 go/go1.2.1 git-commit/3600720 kernel/3.13.0-1-amd64 os/linux arch/amd64\"\n  2014-04-21 19:02:17,040 DEBUG: check_session: Session is empty\n  \"10.1.1.1 - - [21/Apr/2014:19:02:17] \"PUT /v1/repositories/wking/gentoo/images HTTP/1.1\" 204 - \"-\" \"docker/0.9.1 go/go1.2.1 git-commit/3600720 kernel/3.13.0-1-amd64 os/linux arch/amd64\"\n  2014-04-21 19:02:17,275 INFO: \"10.1.1.1 - - [21/Apr/2014:19:02:17] \"PUT /v1/repositories/wking/gentoo/images HTTP/1.1\" 204 - \"-\" \"docker/0.9.1 go/go1.2.1 git-commit/3600720 kernel/3.13.0-1-amd64 os/linux arch/amd64\"\nI don't see any PUT /v1/repositories/hello/tags/\u2026 calls in your\nposted logs, so that might be something to look into.\n. On Wed, Apr 23, 2014 at 09:24:47AM -0700, Joffrey F wrote:\n\n\nDecode utf-8 json metadata before saving it to storage\n\n\nDecoding looks good if we can assume UTF-8.  Maybe we can teach the\ndocker client to send this information in in a request header if it\ndoesn't already?  Maybe assuming UTF-8 is fine?\nI'd prefer if the UTF-8 encoding was moved as far down the stack as\npossible.  Can we move it to the area of the json.dumps call in\ntoolkit.response?\n. On Thu, May 08, 2014 at 01:59:07AM -0700, Mangled Deutz wrote:\n\nDo you mean we should handle only unicode strings internally (eg:\ndecode early from flask)?\n\nYes.  The earier you decode them, the more likely you are to know the\ncorrect encoding (based on headers in the HTTP request or the API\nrequiring UTF-8).  Once you're deeper into our code, it's easier if\nyou don't have to think \u201cwas this decoded already?\u201d\n\nIf so, and if understand correctly, it means that stores must be\nable to handle unicode as well (which I think they don't right now -\nat least file localstorage).\n\nThat sounds right.  I'd just encode JSON as UTF-8 when writing it to\nstorage and decode it from UTF-8 before decoding the JSON:\nDict with unicode \u2194 JSON serialization \u2194 UTF-8 encoding \u2194 bytes \u2194 storage\nYou'll have to do this for Python 3 anyway, because the JSON handlers\nwill only process Unicode:\n$ python3 -c 'import json; json.loads(b\"{\\\"hi\\\": 5}\")'\n  Traceback (most recent call last):\n    File \"\", line 1, in \n    File \"/usr/lib64/python3.4/json/init.py\", line 312, in loads\n      s.class.name))\n  TypeError: the JSON object must be str, not 'bytes'\n  $ python3 -c 'import json; print(type(json.dumps({\"hi\": 5})))'\n  \n. On Thu, May 08, 2014 at 09:02:22AM -0700, Mangled Deutz wrote:\n\nWe are unlikely to be compatible with python3 anytime soon IMO\n(gevent for one), but making the effort is good anyhow\n\nRight, but when you're thinking about \u201chow should I handle\nbytes/Unicode?\u201d, it's best not to be re-inventing the wheel ;).\n\nSo, I'll go with:\n- assume utf8 on the input, and try to decode ASAP at the client\n  request handling level\n- manipulate ONLY unicode objects everywhere else in registry core\n  code\n\nBoth good.\n\n\nencode before passing data to storage drivers - eg: pass bytes\n  to them so they don't need to mess with encoding\n\n\nWe don't want encoding code duplicated in each driver, but I'd rather\navoid lots of:\nstore.put_content(path, json.dumps(data).encode('UTF-8'))\nand:\njson.loads(unicode(store.get_content(path), 'UTF-8'))\nAs far as I remember, our stored data is either JSON or binary, so I'd\nbe happy with an inherited Storage.write_json(path, data) and\nStorage.read_json(path) that handled the JSON/Unicode\nencoding/decoding internally.\n. On Thu, May 08, 2014 at 09:15:19AM -0700, W. Trevor King wrote:\n\nAs far as I remember, our stored data is either JSON or binary, so\nI'd be happy with an inherited Storage.write_json(path, data) and\nStorage.read_json(path) that handled the JSON/Unicode\nencoding/decoding internally.\n\nFor consistency our existing put_content and get_content, I suppose\nthe methods should be put_json and get_json.\n. On Thu, May 08, 2014 at 09:25:44AM -0700, Mangled Deutz wrote:\n\nNow, I have mixed feelings about providing an inherited/helper\nmethod write_json:\n- that do (somewhat) prevent duck-typing (at least people who want\n  to not inherit will have to handle unicode on their own\n- people will have to remember to call that method anyway in their\n  driver - so, it's just a shortcut to data.encode('UTF-8')...\n\nI don't understand these concerns.  Maybe we're talking past each\nother.  I want (in docker_registry/storage/init.py):\nclass Storage(object):\n      \u2026\n      def get_json(self, path):\n          return json.loads(unicode(self.get_content(path=path), 'UTF-8'))\ndef put_json(self, path, data):\n      return self.put_content(path=path, content=json.dumps(data).encode('UTF-8'))\nand then storage consumers can replace their existing:\nstore.put_content(path, json.dumps(data))\ncalls with:\nstore.put_json(path=path, data=data)\nand replace their existing:\njson.loads(store.get_content(path))\ncalls with:\nstore.get_json(path=path)\nThere's no funny typing or shortcut-remembering there, just two new\nhelper methods to centralize encoding/decoding JSON \u2194 storage.\n. On Thu, May 08, 2014 at 10:13:27AM -0700, Mangled Deutz wrote:\n\nYes, sorry for the misunderstanding - storage drivers are going to\nbe extracted from the core soon and become standalone. My concerns\nhence are about making the \"API\" the smaller, less error prone and\nthe more straightforward for third-parties who are going to write\nand maintain said drivers.\n\nSure.  Storage driver implementations (subclasses of Storage) will\nonly need to implement the binary get_content and put_content, so it\ndoesn't grow the API.  It's just a wrapper layer between the actual\nstorage implementation (e.g. local.LocalStorage, etc.) and the\nconsumer code in the docker-registry core.\n. On Fri, Apr 25, 2014 at 09:03:28AM -0700, Shreyas Karnik wrote:\n\n\nimporting logging.handler because only importing logging breaks sending email when there is a faliure\n\n\nOops :p.  Clearly we don't get very many email-inducing errors ;).\n. On Tue, Apr 29, 2014 at 06:26:18AM -0700, Joel Wurtz wrote:\n\nit may be good also to stop execution of the following code when a\nresponse is returned in it but i do not know if it's something\npossible within signals\n\nIt is possible with blinker 1, but not with out current\ndocker-registry code (which doesn't check the return value of .send).\n. On Wed, Apr 30, 2014 at 08:58:36AM -0700, Tony Becker wrote:\n\nDoes it seem like a given base image should be marked with the\nprocessor architecture that it is meant for?  And that this should\nbe a search/filter field?\n\nThe only searchable metadata is currently the repository name and\ndescription 1.  The registry's index implementation currently only\nrecords the name (and not the description) 2, so I'd look at adding\nsupport for a description field first.  Once we've got that working,\nit shouldn't be too hard to add additional fields if we need more\nstructure later.  I think the sticking point for me is that there is\nno index API that I can find for uploading descriptions (or other\nmetadata).\n. On Thu, May 08, 2014 at 09:54:02AM -0700, Patrick Hemmer wrote:\n\nIf this were completely up to me, I would do away with\nsetup-configs.sh, and make config templates that use environment\nvariables (config_s3.yml, config_gcs.yml, config_local.yml,\netc).\n\n+1 for dropping the REPLACEME stuff handled by setup-config.sh in\nfavor of _env:ENV_VAR_NAME[:DEFAULT].  I expect the setup-config.sh\nscript just predates the built-in environment variable replacement.\n. On Thu, May 08, 2014 at 10:26:17AM -0700, Patrick Hemmer wrote:\n\nOne question though. Do we want to have separate config files for\neach storage engine and let the user set DOCKER_REGISTRY_CONFIG,\nor default to a single config and let the user set\nSETTINGS_FLAVOR?\n\nI'm -1 on SETTINGS_FLAVOR.  I can't think of any functionality that\ncouldn't be turned on/off with more specific environment variables\n(e.g. STORAGE to select the storage backend).  Grouping that into\nSETTINGS_FLAVOR seems like unnecessary complication that I would\noffload to the launching code, not bake into the config framework.\nOf course, showing something like our current SETTINGS_FLAVOR groups\nwould be useful in the docs.  I just don't think it should be part of\nthe config syntax itself.\n. On Fri, May 09, 2014 at 06:04:59AM -0700, Patrick Hemmer wrote:\n\n\nClean up configuration management.\n\n\nWhy did you drop the 'exec' in your Dockerfile CMD?  I don't see a\nreason to keep the shell process around.\n\nThis is a PR for #347\nDON'T MERGE YET\nThis adds a default env.yml file which supports all options through environment variables.\nYou can use this to launch a container and set any parameter without having to inject your own config file.\nExample usage:\ndocker run \\\n  -e SETTINGS_FLAVOR=s3 \\\n  -e AWS_REGION=us-west-1 \\\n  -e AWS_BUCKET=my-docker-bucket \\\n  -e AWS_KEY='AKIASDHAF48HSKDAF' \\\n  -e AWS_SECRET='SAGDKFJSHDKFJHAVUSKDJFSHKLDFLSDF' \\\n  -e SECRET_KEY='foo'  \\\n  -e SEARCH_BACKEND=sqlalchemy\n  registry\nYou can also launch without any settings to get the dev 'flavor'\nwhich will launch local storage with sqlalchemy enabled.\n\nI still think it would be better to just have a STORAGE variable\ninstead of SETTINGS_FLAVOR blocks 1.  If STORAGE=s3, it doesn't\nmatter if there are also a bunch of swift or glance configs set,\nyou're not going to use them.  Having a single block would also avoid\nfragmentation like AWS_BUCKET and GCS_BUCKET variables for the same\nboto_bucket setting.  It also avoids the whole glance-swift business,\njust set:\nSTORAGE=glance GLANCE_STORAGE_ALTERNATE=swift\n\nI also wasn't sure how to go on defaults. This bit should probably\nbe determined and cleaned up. For example in the new env.yml,\ns3_encrypt defaults to true if not specified. I did this as this\nis the value set in config_s3.yml. But I'm not sure if we want to\nleave this unset and let the code handle it.  But then on some\nthings like config_path, I think a default makes sense. So which\nparameters get defaults and which don't?\n\nI'd add defaults everywhere we think we have a reasonable suggestion\nfor development setups.  Then I'd add notes to the README explaining\nhow folks might want to tune for higher loads.\n\nI personally think it would be best if all the vars for a storage\nengine were prefixed with that engine name. So engine: swift would\nbe $SWIFT_AUTH_URL, not $OS_AUTH_URL.\n\n+1.  I expect this will shake up existing configs a bit, so it's\nbetter to do a full cleanup all at once.\n. On Fri, May 09, 2014 at 09:23:15AM -0700, Patrick Hemmer wrote:\n\nI think that would work as long as the app doesn't do anything if a\nconfig param is set and not used. Like if storage: s3 and\nelliptics_addr_family is set.  I wasn't sure if this is the case,\nso I kept everything separate.\n\n$ git grep -c elliptics_\n  README.md:20\n  docker_registry/storage/ellipticsbackend.py:1\nso at least for the elliptics backend, you don't have to worry about\naccidentally using those settings unless you're using that backend.\n\nThe other argument for keeping them separate is to have different\ndefaults for the different storage engines. Like for local storage,\nstorage_path defaults to /tmp/registry. But for S3, it defaults\nto /registry.\n\nI think that small gain is not worth the added layer of config-syntax\ncomplexity.  I'd be happy to make the default STORAGE_PATH\n/docker-registry in all cases, and leave it up to folks who don't like\nthat to override as they see fit.  In cases where a globally-workable\ndefault can not be found (maybe an empty set?), a note in the README\nsaying \u201cIf you're using storage backend $a, make sure to override\nenvironment variable $b with a suiltable value (e.g. $c).  The default\n$d doesn't work because\u2026\u201d should be sufficient.\n. On Fri, May 09, 2014 at 10:00:35AM -0700, Mangled Deutz wrote:\n\nGetting rid of setup-configs.sh\n\nYou can probably do this without much trouble, and you can certainly\nland that now and put off a more thorough cleanup until later.\n\nand harmonizing the use of env vars is one thing - introducing\nbreaking changes is another.\n\nBut I expect you won't be able to \u201charmonize the use of env vars\u201d\nwithout requiring users to adapt to the new variables.  I think the\nblind-user count is still low enough that cutting 0.7.0 with release\nnotes that describe suggested environment variable mappings:\nAWS_ACCESS_KEY_ID \u2192 S3_ACCESS_KEY\n  AWS_SECRET_KEY \u2192 S3_SECRET_KEY\n  \u2026\nshould cover almost all users, without needing code in the registry to\nsupport both config flavors while emitting deprecation warnings.\n\nI would definitely be glad to merge the first ASAP, but killing\nSETTINGS_FLAVOR and changing the way storages are selected will\nprobably have to wait.\n\nI don't see a particular rush in either case, but breaking user-facing\nsettings is best done before the user base grows too large, and\nincludes too many people who will blindly upgrade without reading\nrelease notes.  I'd be happy to write up a patch killing\nSETTINGS_FLAVOR (based on this PR) if you'd like to kick it around.  I\ndon't expect it to be too intrusive.\n. On Fri, May 09, 2014 at 10:46:57AM -0700, Mangled Deutz wrote:\n\n@wking that certainly makes a lot of sense, and we definitely agree here.\n[snip good stuff]\n\nThat all sounds good to me.  I just think it helps to have explicit\ntimelines etc. for the non-sexy bits before kicking them down the\nroad.  Then feelings of guilt will help them actually get done ;).\nI'm fine if this PR explicitly punts on environment variable renaming\nand SETTINGS_FLAVOR removal.  In that case, it looks pretty solid as\nis ;).\n. On Fri, May 09, 2014 at 01:51:18PM -0700, Patrick Hemmer wrote:\n\nAre we settled upon leaving env.yml as is? If so I'll update the README.\n\nI see no reason to add a new config when we can just replace any\nexisting REPLACEME markers with the analagous _env:\u2026.  That should\nmake it easy to ensure we're not breaking existing users.\nAfter a commit like that, I think your current env.yml is mostly a\nstreamlined config_sample.yml, so it should replace the existing\nconfig_sample.yml (but probably keep the comments), with a commit\nmessage that explains the improvements (e.g. \u201cpulls in glance-swift,\netc. for a single, unified config script\u201d).  Then you can remove the\nstorage-specific config files as you integrate the in\nconfig_sample.yml, and leave symlinks while we wait for folks to\nmigrate away from the old filenames.\n. On Wed, May 14, 2014 at 04:11:31PM -0700, Patrick Hemmer wrote:\n\ncache/cache_lru (whats the difference).\n\nIt looks like cache_lru is used to cache files for the storage drivers\n(see local.LocalStorage.get_content), while cache (without _lru) is\nused as a job-queue for image diffs.  The code overlaps almost\ncompletely though, so I'm in favor of consolidating these (but\nprobably not in this PR).\n. On Thu, May 15, 2014 at 06:39:04PM -0700, Mengxuan Xia wrote:\n\nSuppose I use DELETE\n/v1/repositories/(namespace)/(repository)/tags/(tag*) and the image\nnow has no tag, will the image gets deleted also? I use tag to\ntimestamp builds, and want to throw old builds away from time to\ntime.\n\nAs far as clients are concerned, yes.  As far as registry disk space,\nno.  See #7, which I think addresses this exact issue.\n. On Fri, May 16, 2014 at 07:33:44AM -0700, Mengxuan Xia wrote:\n\nHow to save disk space by removing them? Is there an API or\ndocumentation?\n\nNeither yet.  Patches welcome ;).\n. On Fri, May 16, 2014 at 04:00:59PM -0700, Sam Alba wrote:\n\nI think it's just to avoid keeping a sh child process...\n\nYup, see c170cc5fe (Use exec to eliminate extra shell processes,\n2014-03-01).\n\nMaybe docker should do that directly.\n\nYou can do that with ENTRYPOINT, but I prefer 'CMD exec \u2026' when\n/bin/sh exists because it makes it easier to launch an interactive\nsession when you want to poke things before launching the registry.\n. On Tue, May 20, 2014 at 05:50:41AM -0700, Mangled Deutz wrote:\n\nafter we merge all our recent work, what about we clean-up the\n(useless?) branches?\n\n+1\n. On Tue, May 20, 2014 at 09:09:00AM -0700, Mangled Deutz wrote:\n\nThe reason it crashes is:\nhttps://github.com/dotcloud/docker-registry/blob/master/docker_registry/toolkit.py#L28\nQuestions:\n- should we accept UA that don't contain docker/XXX ?\n- if yes, then what would be the proper behavior? assume the latest\n  docker version?\n\nI missed reviewing the troublesome code in #300, but I think parsing\nAPI versions out of the UA string is the wrong approach here.  Isn't\nthis what the endpoint URLs (/v1/\u2026) were designed for?\n. On Tue, May 20, 2014 at 10:17:47AM -0700, Mangled Deutz wrote:\n\nWell, I guess this will get kind of philosophical :-) (disclaimer: I\nsort of agree not bumping the version in that case).\n\nIf we want to version the API, it makes sense to have version checks\nin one place (e.g. in the URL, or in an explicit X-Docker-Version\nheader, or in the PUT payload, \u2026).  I'm happy with either the URL\napproach (always explicit) or the X-Docker-Version approach\n(defaulting to the current version).  If we make some small change to\nthe API (such as not storing layer tarsums), and want to add\nper-feature headers or registry-side options to enable/disable this\nbehaviour, that's fine.  If we can auto-detect new features (e.g. if\nX-Docker-Checksum-Payload exists, it must be using the new SHA-256\nsemantics), that's fine too.\nBut regardless of how we version the API, conflating the API version\nwith the user-agent version just seems confusing.\n. On Tue, May 20, 2014 at 10:19:21AM -0700, Mangled Deutz wrote:\n\nCurrently, bugsnag is a hard requirement to the package - although,\nit's import is try catched (like it could be optional).\n\nMaybe shift requirements.txt to requirements/core.txt and have the\noptional stuff like bugsnag and stand-alone backends in\nrequirements/extra.txt?\n. Sounds reasonable and looks good to me, although I think the three\ncommits should be squashed together.\nLonger term, I think it would be better to have clients set\nContent-Encoding to trigger decompression, rather than attempting a\nblind decompression.  It would be nice if we could avoid the temporary\nfile we use to make the uploaded contents seekable.  As a side\nbenefit, we could store the uploaded Content-Encoding and Content-Type\nand echo them when serving layers, instead of using the uninfomative\napplication/octet-stream and ignoring Content-Encoding entirely.\n. You could be less invasive with:\nimport xtarfile as tarfile\nThat would allow you to drop the other tarfile \u2192 xtarfile changes.\n. Maybe some squashing would help clarify things too.  I don't see a\nneed for:\ndocker_registry/lib/xtarfile.py\n  \u2192 docker_registry/vendor/xtarfile.py\n  \u2192 monkeypatched tarfile\nAnd would rather skip straight to the monkeypatched version ;).  Not\nthat that's a big deal, but the series would be easier to review if it\nwas boiled down to the changes that stick.\n. On Fri, Jun 06, 2014 at 07:41:42AM -0700, Vincent Batts wrote:\n\nBut that would require exporting all of tarfile in xtarfile. I don't\nthink we're game for that.\n\nYou already do ;).  With e505885 (xattrs: monkey patch, instead of\nvendor, 2014-06-02), you revert a lot of the tarfile \u2192 xtarfile\nchanges and have things like:\nfrom .lib import xtarfile\ntarfile = xtarfile.tarfile\nI'd prefer:\nfrom .lib.xtarfile import tarfile\nBut both approaches are getting the whole (monkeypatched) tarfile\nmodule into your consuming namespace as \u2018tarfile\u2019.\n. On Wed, May 21, 2014 at 11:57:49AM -0700, Matthew Miller wrote:\n\nCurrently, the Docker trusted builds process\u2026\n\nThe index and trusted-build code is (I think) closed source.  This\nrepository is the registry, with some stub index code for stand-alone\ninstances.  In any case, I don't think we have any code here that\nhandles the index-side repository descriptions.\n. On Wed, May 21, 2014 at 12:11:18PM -0700, Matthew Miller wrote:\n\nYeah, I was just looking at the code -- sorry for putting this in\nthe wrong place. Is there a better place to report this?\n\nPrevious index.docker.io requests have been referred to\nsupport-index@docker.com [1,2,3].\n. On Wed, May 21, 2014 at 02:16:20PM -0700, Vincent Batts wrote:\n\nthis is to enhance the tarsum algorithm\n\nSince #300, registry-side tarsums have been on their way out.  Why\nwould we want to improve them?\n. On Wed, May 21, 2014 at 02:27:46PM -0700, Vincent Batts wrote:\n\nRegardless, having the TarSum be consistent for the golang and\npython implementation is important.\n\nI'd just drop all the tarsum handling from docker-registry to make it\nconsistent ;).  However, until we can drop support for < 0.10, I\nsuppose patching the local handling makes sense.\n\nBesides, that checksum is only for in flight. Once a layer is pulled\ndown by docker, that tar is discarded. TarSum is the best way to\nhave fixed-time consistent checksum of the layer. not the\nin-flight checksum that docker-registry opted for with #300\n\nWho cares about consistent checksums?  If you trust the registry, you\ndon't need them.  If you don't trust the registry, you'll want\nsomething more serious (like my detached signature proposal [1,2]).\n. On Wed, May 21, 2014 at 02:37:51PM -0700, W. Trevor King wrote:\n\nWho cares about consistent checksums?  If you trust the registry, you\ndon't need them.  If you don't trust the registry, you'll want\nsomething more serious (like my detached signature proposal [1,2]).\n\nWhich does all the tarsum calculations locally using a hypothetical\n'docker fingerprint', so it needs no registry support at all.  You'd\njust be storing opaque detached signature files on the registry.\n. On Wed, May 21, 2014 at 02:40:20PM -0700, Vincent Batts wrote:\n\nI saw that part of the conversation, and part of what a detached\nsignature would be signing would include the TarSum ... so you'll\nstill need a way to validate that sum.\n\nRight, but that would just be client-side code, so you wouldn't need\nthe registry-side code in this PR.\n. On Fri, May 23, 2014 at 11:42:33AM -0700, James Bardin wrote:\n\nWhen pushing new image an exiting tag to a repo (S3 in this case),\ne.g. :latest, the repo index still knows about the previous images\nwith this tag.\n\nDoes your registry have #266 (released in 0.6.6)?  If it does, did you\nrun the jq cleanup suggested in the #266 message?\n. On Thu, May 29, 2014 at 09:12:34AM -0700, Sam Alba wrote:\n\nCould you also remove s3_bucket? boto_bucket replaced it at some point.\n\nSkimming through 'git log -p -S s3_bucket', it seems to have been gone\nsince b49325d (Refactor to a parent boto class and s3/gcs subclasses,\n2013-10-24) in #107.\n. On Thu, May 29, 2014 at 08:02:16AM -0700, Mangled Deutz wrote:\n\n\nPackaging clean-up\n\n\nInstead of:\nrequirements-style.txt\n  requirements-test.txt\n  requirements.txt\nI prefer the reduced clutter of:\nrequirements/style.txt\n  requirements/test.txt\n  requirements/default.txt\nI like the bit with extras_require.  Hadn't seen that before.\n\n\nMore packaging cleanup + version and package metadata is now\n  centralized in lib/init.py\n\n\nYou shift this to server/init.py later, so you should probably\nsquash that commit into this one.  Personally, I'd prefer to have this\nmetadata in docker_registry/init.py, which should allow you to\navoid the execfile wonkiness in 7b7d0219 (Fix namespace collision,\n2014-05-23).\n\n\nMore cleanup\n\n\nI don't understand the \u201c(pip)\u201d parentheticals.  Is that suggesting\npip as the installer for the subsequent packages?\nThere's also some unneccessary churn in requirements.txt.  If you want\nto churn that file, I'd suggest just alphabetizing the whole thing ;).\n\n\nFix workflow test\n\n\nThis adds colorama and assorted stuff that's (mostly?) removed again\nby c943b412 (Remove debug, 2014-05-29).  Maybe drop both commits?  Or\nsquash them together?\nYou also comment out some cookie stuff, that you should probably just\nremove entirely.  It's in the version control history if folks want to\nresurrect it.\n\n\nFix namespace collision\n\n\nWhat does the namespace breakage look like?  Can't all this metadata\nlive in a boring docker_registry/init.py?\n. On Thu, May 29, 2014 at 11:03:50AM -0700, Mangled Deutz wrote:\n\nUltimately, we may even get rid of them and use a docker-python-dev\npackage to express tooling for all docker python development if we\ndecide we need coherence.\n\nI like explicit requirement files more than splitting the dependencies\nout into a separate virtual package.  You'll still need the\nrequirements listed somewhere, and it seems simplest to list them in\nthe package that's doing the imports and using the APIs ;).\n\n\nYou shift this to server/init.py late. Personally, I'd prefer\nto have this metadata in docker_registry/init.py\n\nWell, that's not possible - that's the caveat of python namespaces:\nyou can't have anything package specific in the init file of a\nnamespaced folder.\n\nWith PEP 420 and Python 3.3, namespace packages move into the standard\nlibrary 1, but they will still lack an init.py 2.  Why do we\nneed a namespace package, though?\n. On Thu, May 29, 2014 at 11:44:35AM -0700, W. Trevor King wrote:\n\nThu, May 29, 2014 at 11:03:50AM -0700, Mangled Deutz:\n\n\nYou shift this to server/init.py late. Personally, I'd prefer\nto have this metadata in docker_registry/init.py\n\nWell, that's not possible - that's the caveat of python namespaces:\nyou can't have anything package specific in the init file of a\nnamespaced folder.\n\nWith PEP 420 and Python 3.3, namespace packages move into the standard\nlibrary [1], but they will still lack an init.py [2].  Why do we\nneed a namespace package, though?\n\nI see that namespaces landed with a181c13 (Move away content from\ninit to let namespace, 2014-05-12, #353), but I can't find notes\nexplaining the motivation for using namespaces.  We should be able to\nplugin storage backends etc. even if they don't live under a single\nnamespace (see, for example, a8bd4a4, index: Expand load() to handle\narbitrary module names, 2014-02-17, #247).\n. On Thu, May 29, 2014 at 02:01:15PM -0700, Mangled Deutz wrote:\n\n\nWhy do we need a namespace package, though?\n\nThe PR you link to actually asked for comment :-)\n\nIt was a big PR, and I was too busy to look it over ;).  Apologies for\nthe late review.\n\nEither way, there are a number of advantages in using namespaces,\nespecially not multiplying different global names between different\npackages (-core, -registry),\n\nMeh :p.  This doesn't seem to be worth any implementation\nhoop-jumping.\n\nor ability to list all available drivers by enumerating\ndocker_registry.drivers, etc.\n\nIs this actually useful?  Users will have to know which drivers are\navailable beforehand (so they can configure the one they want).\nHaving a Python oneliner to list installed driver packages (in case\nyou forgot after installing them?) doesn't sound useful enough to me\nfor the associated namespace-module hoop jumping.  I'm happy to be\nwrong here, I just don't see the utility yet.\nI'm still not sure why this blows up and requires the execfile hack\nfrom 7b7d021 (Fix namespace collision, 2014-05-23).  Is it a quirk of\nthe setuptools implementation?  Are you seeing:\n'error: package directory '\u2026' does not exist'\nas in 1?  How can I reproduce the error you were working around\nlocally?\n. On Thu, May 29, 2014 at 03:05:51PM -0700, Mangled Deutz wrote:\n\nAnyhow, we are entirely OT here (*), and I'm on a GMT timezone :-)\n\nHeh, works for me.\n. On Thu, May 29, 2014 at 03:05:51PM -0700, Mangled Deutz wrote:\n\n\n[@wking doesn't like namespaces, at all, he thinks they are all\nugly, and smelly, and they are bad and... :-)]\n\nWell I do like namespaces, and do wish they were less  \"hoopy\" :-)\n\nAnd I like PEP 420, and was able to get a docker_registry.meta import\nworking without hoops using Python 3.3 ;).  I just don't like\nsetuptools or other outside-the-stdlib packaging.  Packaging is too\nimportant to trust to something as unstable and poorly spec-ed as an\nexternal package :p.\n. This is much nicer, thanks :).\nOn Fri, May 30, 2014 at 08:37:18AM -0700, Mangled Deutz wrote:\n\n\nClearer contribute documentation + authors\n\n\nCan we just drop the Python dependencies from CONTRIBUTE.md and list\nthem in requirements/test.txt and maybe\nrequirements/test-optional.txt?  I personally don't have a problem\njust dropping them all into requirements/test.txt and having\nCONTRIBUTE.md suggest:\n$ pip install -r requirements/test.txt\nwith requirements/test.txt holding:\n-r main.txt\n  nose>=1.3\n  coverage>=3.7\n  mock>=1.0\n  hacking>=0.8\nHacking itself depends on a specific version of flake8 1, so I\nwouldn't specify our own dependency.\nI'm not sure what our minimal versions actually are.  For folks who\nlike getting locked into explicit versions, I'd add\nrequirements/frozen.txt with the output of 'pip freeze'.  Only\nspecifying explicit versions in the other requirement files just makes\nit less clear which APIs we're actually using.  I'm fine pushing this\nrelaxation off to a subsequent PR though.\n\n\nFix #368\n\n\ndocker_registry.server.env.defined is a global variable not used\noutside the module, so I'd rather call it _SETTINGS (or at least\n_DEFINED).\n\n\nEntirely trivial cleanup\n\n\nThere's still some non-trivial stuff going on here.  How does the\ndocker-registry-core shift into setup.py ease tox?  Why don't we keep\nthe flake8 config?  docker_registry/lib/init.py was empty, so why\nthe UTF-8 declaration?\n\n\nCleaner, simplified requirements expressions\n\n\nDo you expect folks to want the style requirements without the test\nrequirements (test without style makes sense to me)?  If not, maybe:\n-r main.txt\n  hacking>=0.8\nin style.txt?  You'd have to restore the flake8 requirement to\ntest.txt, with a sufficiently flexible version that we don't get\nbitten the next time hacking bumps their excessively constrained\ndependency.\n\n\nFix #377 && Fix #376\n\n\nI prefer the nestable $(\u2026) to \u2026 for command substitution.  But PWD\nis a POSIX-specified environment variable 2, so I'd rather have:\npip install --download-cache=~/.pip-cache \"file://${PWD}#egg=docker-registry[bugsnag]\" || exit 1\n\n\nBring back workflow test\n\n\n\u201c# XXX revert\u201d ?  Does this have some magic effect?  If it's aimed at\npeople, a bit more detail would be nice ;).\n\n\nSlightly enhanced driver interface + more verbose exception\n\n\nI expect the dummy Base.init is to consume **kwargs that are\ngetting passed down a super() stack.  If we expect drivers to be\ndeclaring these particular variables (path and config), notes in the\nBase docstring explaining what they are for would be nice.  Otherwise\neach driver has to check how their particular super-class interprets\nthem.\n. On Fri, May 30, 2014 at 09:59:27AM -0700, Mangled Deutz wrote:\n\nthe req files are used by automated, isolated testing environments\n(tox and travis)\n\nThere's no reason you can't have loosely-versioned reqirement files\nfor humans and explicit frozen files for automation.  With just frozen\nfiles you give no guidance to folks using unpinned installs.  Will\nnose 1.0.0 work?  I dunno.  Maybe?  It's better to just say: \u201cwe don't\nuse anything that's been added to nose since 1.0.0\u201d (or wherever) with\nnose>=1.0.0.\n\n\nThere's still some non-trivial stuff going on here.\n\nSorry if some slipped through - it was not always easy to split the\ndifferent modifications into coherent commits.\n\nUnderstood.  And it doesn't have to be perfect to land, otherwise we'd\nall just be polishing forever ;).  So feel free to say, \u201cthat's ok, I\ndon't care\u201d when I point out some suitably trivial issue.\n\n[explainations of minor stuff]\n\nThese make sense to me :).  In an ideal world I'd like them written up\nin the commit message, but maybe that's over the \u201ctoo trivial\u201d\nthreshold.\n\n\n\u201c# XXX revert\u201d ?  Does this have some magic effect?\n\nYes, it makes differential anisotropic Higgs boson follow the\nabelian group underneath the unicorn that runs the bike inside the\nregistry :-)\n\n:D.\n. On Fri, May 30, 2014 at 10:25:36AM -0700, Joffrey F wrote:\n\nThis is an index issue.\n\nWith a registry URL?  I thought the registry API 1 was all this\npackage, and the closed-source stuff was just the index stuff (which\nnow seems to be here 2).\n. On Fri, May 30, 2014 at 10:34:30AM -0700, Joffrey F wrote:\n\nNo such change has been made to\nhttps://github.com/dotcloud/docker-registry/blob/master/docker_registry/tags.py\n, so I am 100% sure that he's talking about the similar index route.\n\nAh, I didn't realize that the closed-source code handled registry-side\nendpoints the same way this package has stubs for index-side endpoints\n(in standalone mode).\n. On Fri, May 30, 2014 at 10:50:14AM -0700, Joffrey F wrote:\n\n\nthe closed-source code handled registry-side endpoints\n\nThat's not actually the case =3 just so happens that we have\nrepositories and tags resources on both sides, and we're consistent\nwith our REST, so some similitudes happen with routes and stuff.\n\nRegistry-side endpoint [1,2]:\nGET /v1/repositories/(namespace)/(repository)/tags\nI can't find docs for an index-side endpoint for tags, but this came\nup before in discussion for 268 3.  I think a separate\nregistry.docker.io host would help make this index/registry\ndistinction more obvious ;).\n. On Fri, May 30, 2014 at 01:35:53PM -0700, getitlive wrote:\n\nWas definitely a mistake in my code, misunderstood the distinction\nbetween index and registry. Calling the same endpoint (from the\nregistry this time) works as expected and returns full IDs.\n\nI just can't find any docs for the fact that there is an index-side\nversion of this endpoint.  On the other hand, if I was writing the API\nspecs, I'd have tags exist only on the index, and the registry would\njust be unlabeled layer storage (with ancestry and other stuff\nextractable from the layer metadata, but no knowledge of anything\nabove the layer level).  However, the current specs seem to relegate\ntags to the registry, and the closed-source implementation seems to\ntake an awkward, undocumented position between between the\nregistry-side-tags of the current specs and index-side-tags of my\nideal specs ;).\n. On Mon, Jun 02, 2014 at 01:58:43AM -0700, wrote:\n\nWe need a (preferably lightweight) way to issue bugfixes releases\n(say 0.7.1, 0.7.2).\nEither we create a 0.7 branch from the tagged 0.7.0 (then later\nanother 0.8 one, etc), or we create a stable branch of the\ntagged release, \u2026\n\nI prefer . branches like 0.7.  That allows you to create\n0.8 off master at the beginning of the feature freeze, while still\nmaintaining 0.7 for additional point releases until you cut 0.8 (or\nuntil interest/funding for 0.7 support dries up ;).  That said, I\nthink you only need a formal procedure to maintain old releases once\nyou hit 1.0.  Early adopters shouldn't have problems upgrading to 0.8.\n. On Fri, Aug 01, 2014 at 02:57:58AM -0700, Olivier Gambier wrote:\n\nIt's somewhat working. Kind of a PITA to track fixes on two\nbranches, but I guess that's what happens when you grow up :-)\n\nI don't think you need to do that.  For example, consider the \u201cRemove\nnon-sensical blindly copy-pasted comment\u201d commit that landed in master\nas 4c83f82 and in 0.7 as 45edb13.  Instead of cherry-picking like\nthat, the Git project just merges up from the maintenance branch\n(\u2018maint\u2019 for Git, \u20180.7\u2019 for us):\n$ git clone git://git.kernel.org/pub/scm/git/git.git\n  $ cd git\n  $ git glog --simplify-by-decoration origin/master origin/maint\n-   aa544bf (HEAD, origin/master, origin/HEAD, master) Sync with 2.0.4\n  |\\\n  | * 32f5660 (tag: v2.0.4, origin/maint) Git 2.0.4\n- | 49f1cb9 (tag: v2.1.0-rc0) Git 2.1.0-rc0\n  |/\n- 740c281 (tag: v2.0.3) Git 2.0.3\n  \u2026\nWhen you merge the maintenance branch, you'll automatically get its\nbugfixes.  Any merge conflicts will be conflicts you'd have to resolve\nanyway during your cherry-pick (except for conflicts in the version\nnumber and such, which should be easy to resolve).\nOh, and it looks like the \u2018next\u2019 branch (from #414) is still hanging\naround.  Maybe that should be removed?\n. On Mon, Jun 02, 2014 at 02:20:34AM -0700, Mangled Deutz wrote:\n\n\nFix #401\n\n\nInstead of extracting cfg.get('storage_path') and passing it\nseparately to storage class, can't we just pass the config instance\nand leave path-extraction (where appropriate) up to the class itself?\nI can certainly imagine storage class implementations that don't need\na base path.\n. Mangled Deutz wrote:\n\nstrong coupling between an opaque config object (registry side) and driver is IMO bad, and I want to break/reduce it.\n\nAgreed.  It would be nice to have a namespace for all storage-side config options, and then something like:\nkwargs = {k[len('config_'):]: cfg.get(k, None) for k in cfg.keys() if k.startswith('config_')}\nengine.fetch(kind)(**kwargs)\nWith a ConfigParser-based config file format, the kwarg extraction would just be:\nkwargs = config['storage']\nin Python 3.2 and:\nkwargs = {k: config.get('storage', k) for k in config.options('storage')}\nin older Pythons.\nThe specifics of a driver's config are completely orthogonal to the rest of the registry core, so anything other than opaque config forwarding seems overcomplicated.\n\nIndeed, most if not all drivers have a concept of base path (or prefix), which is why this one should really be different from the other driver-specific configs.\n\nMaybe all existing drivers do, but I could imagine a storage backend based on a database that doesn't need a base path or prefix.  In any case, this is still guessing about the semantics of a hypothetical (or at least external) driver's config, and I don't see any gains by doing that in the core code.\n. On Tue, Jun 03, 2014 at 02:14:20AM -0700, Mangled Deutz wrote:\n\n\nIn any case, this is still guessing about the semantics of a\nhypothetical (or at least external) driver's config, and I don't\nsee any gains by doing that in the core code.\n\nThe gain is that whenever / if we change the configuration format\nfor storage_path, we don't need to patch ALL drivers, but just the\nregistry (mapping between the config key and the arg) - which is\nexactly what happens here - I don't want to touch the driver itself.\n\nWhy have a mapping between the config key and the arg that's more than\n\u201cstrip off a prefix that only the registry side knows about, leaving\nthe tail that only the storage side knows about\u201d.  I agree that it\nwould be a terrible idea to translate a STORAGE_PATH config to a\n\u2018prefix\u2019 argument.\n\nAnyhow, the future may be like what we did for Hipache - use uris\nas a configuration provider for most \"common\" variables (see\nhttps://github.com/dotcloud/hipache/tree/master/lib/drivers).\n\nI'm fine with that too, since it's just another way to make the\nstorage config completely opaque to the core code.\n\nStill need to review the drivers and see if that make sense - but\nthis is getting OT - more on that later!\n\nFair enough :p.  And sorry for drilling down on these design decisions\nafter they land.  You're too fast for me to notice them while they're\nin flight ;).\n. On Mon, Jun 02, 2014 at 01:00:31PM -0700, discordianfish wrote:\n\n2014-06-02_16:08:49.97379 HTTP code 500 while uploading metadata: invalid character '<' looking for beginning of value\n\nSimilar errors have been reported in #298 and #320.  There might be\nuseful stuff in them.  I haven't been following any of this, but\nremembered archiving messages from those issues as they were\ndiscussed ;).\n. On Tue, Jun 03, 2014 at 08:38:44AM -0700, Pierre Chaussalet wrote:\n\nThe only (simple) ways to avoid such errors would be to :\n 1/ Add a wait time before accessing data (and hope that this wait\n    time is long enough for all cases)\n 2/ Implement a retry function until the data is available\n\nUgh :p.  Is there no way to get a pushed notification for the sync, so\nwe don't have to poll.  Do we hold the client open while the storage\nbackend polls?  Can't we just return \u201c202 Accepted\u201d and leave it to\nthe client to poll 1?  When are we doing this sort of \u201cwrite and\nthen read back again\u201d.  Is that in the handling of a single request?\n. On Tue, Jun 03, 2014 at 09:59:38AM -0700, Mangled Deutz wrote:\n\n\nWhen are we doing this sort of \u201cwrite and then read back again\u201d\n\nThat's the one easy answer :-) -> PUT checksum\n\nIn PUT /v1/images/<image_id>/checksum, it looks like we're checking\nfor the existence of the uploaded checksum in storage, and then\nremoving the \u201cuploading\u201d image_mark_path if the checksum matches.  I\nguess the client is calling this after uploading the image, but before\nserver-side checksum processing is complete?  Or after that's\ncompleted, but before Amazon has synced the s3 storage?  In either\ncase (distinguishing between them is likely not useful), it's probably\nbetter to return a 202 and cache the uploaded checksum.  After the\nlocal checksum calculation completes, we can compare the calculated\nchecksum with the uploaded checksum to consider rejecting the image.\nNo need to involve the client again (although they can use GET\n/v1/private_images/<image_id>/json to see the checksums we've\ncalculated locally).\nI don't understand our current handling of missmatched checksums.  It\nlooks like we currently return a 400 and then wait for another\nchecksum upload attempt.  Wouldn't it make more sense to trust the\nclient's uploaded checksum, assume a missmatch meant a corrupted\nupload, and drop the image?\nAlso, I can't find PUT /v1/images/<image_id>/checksum in the docs.\nIt looks like it landed in dotcloud/docker@8ca7b064 (Refactor\nchecksum, 2013-07-17), but without supporting documentation, so I'm\nnot clear on why we're not just setting the checksum in a header with\nthe initial image upload.\n. On Tue, Jun 03, 2014 at 03:41:03PM -0700, Pierre Chaussalet wrote:\n\nEven if the checksum were calculated on the layer upload request, to\nensure this security level we need to store then read the checksum\nfile, so the consistency problem due to S3 us-standard behavior\nwould be the same.\n\nI think they are calculated on the layer upload request.  See the\nstuff like:\nh, sum_hndlr = checksums.simple_checksum_handler(json_data)\n  \u2026\n  sr.add_handler(sum_hndlr)\nin docker_registry.images.put_image_layer.  So we're calculating\nchecksums while we stream the layer out to storage.  If we had the\nchecksum header(s) in the initial PUT /v1/images/<image_id>/layer we\ncould compare the submitted and computed checksum immediately without\nneeding to read anything back from storage or get a separate\nchecksum-submission request from the client.\n\nBTW, I agree that returning a 400 when a checksum is wrong is weird\nas the checksum is not send in the same request as the image layer,\nbut I can't see any HTTP status more appropriate.\n\n400 (Bad Request) is described as 1:\nThe request could not be understood by the server due to malformed\n  syntax.\nHaving a bad checksum is not a \u201cmalformed syntax\u201d error.  I think 403\n(Forbidden) is closer 2:\nThe server understood the request, but is refusing to fulfill\n  it. Authorization will not help and the request SHOULD NOT be\n  repeated. If the request method was not HEAD and the server wishes\n  to make public why the request has not been fulfilled, it SHOULD\n  describe the reason for the refusal in the entity.\n. On Tue, Jun 03, 2014 at 02:18:21PM -0700, W. Trevor King wrote:\n\nAlso, I can't find PUT /v1/images/<image_id>/checksum in the docs.\n\nI still haven't found docs for that call, but I found a related\nreference to checksum pushes in the index API 1:\n1. docker contacts the index to give checksums for upload images\nAnd then later in the list of detailed requests:\n1. (Docker -> Index) PUT /v1/repositories/foo/bar/images\npushes id/checksum lists.  That endpoint is spec-ed, and appropriately\npart of the index API.  Why we need a separate endpoint for pushing\nchecksums to the registry side of things (when it can calculate those\nchecksums on upload) is beyond me ;).\n. On Sat, Jun 07, 2014 at 12:05:03AM -0700, Mangled Deutz wrote:\n\n@wking IIRC this was about performance.\n\nI thought it was \u201csometimes we write something to S3 and then try to\nread it back before it's propagated throughout S3\u201d.  I think a\npossible solution there cleaning up the registry API to remove calls\nthat require reading so soon after writing.  In the \u2018PUT checksum\u2019\ncase, I think the call is completely unnecessary, so I'd just drop\nthat whole endpoint.  Is that not a viable solution for some reason?\n. On Sat, Jun 07, 2014 at 10:12:45AM -0700, Mangled Deutz wrote:\n\nWhich endpoint / request doesn't matter, and we need a generic\nsolution to handle S3 behavior.\n\nI think cleaning up the API would reduce the cross-section here, but I\nagree that's it's not a rock-solid fix.  If that reduced cross-section\nisn't good enough, I agree that you'll need to patch the S3 backend\nsomehow.\n. On Wed, Jun 04, 2014 at 08:27:01AM -0700, Sam Alba wrote:\n\nI think we should have that completely optional. The registry should\nwork without it.\n\nCan do.  However, the only use cases I can think of where you'd want to\nturn it off are:\n- If you have external tags pointing to images, which would not be\n  accounted for in the registry's internal refcounting.\n- If you wanted to archive all the images you'd ever seen, but not the\n  tags that referenced them.\nNeither is particularly convincing to me, but there's no accounting\nfor taste ;).\n\nOn a large dataset, if a refcount is not updated and the refcount\nindex gets corrupted, what are the options to rebuild it?\n\nAs in \u201cdocker-registry crashes while/before writing the references\nfiles\u201d?  I see two cases there:\n- Crashed during a write.  When restarted, this can be detected by\n  invalid JSON in the references file, and we'd have to walk the whole\n  repositories/tags tree to see if any of those files depended on our\n  corrupted image.  Slow, but that's ok for recovering from such an\n  unlikely situation.\n- Crashed before descending to a particular image in the ancestry\n  stack.  I think this is also unlikely, and it would be marked by the\n  complete lack of a references file.  Same recovery as for \u201ccrashed\n  during a write\u201d.\nNeither case is currently handled in this PR, but I can add the code\nto do so.  Are there other cases I should consider?\n\nI think it's suitable only for small dataset by design.\n\nWhy is that?  Is the problem the long list of references for\ncommonly-used base images.  For example, I imagine index.docker.io has\nquite a number of references to the various \u2018ubuntu\u2019 images.\nIn any case, how you implement refcounting is a storage-side decision.\nSo long as the add_references and and remove_references endpoints are\nthere (even if they're null-ops), the core code doesn't have to care\nabout how you want to handle this.\n. On Wed, Jun 04, 2014 at 10:07:53AM -0700, Sam Alba wrote:\n\n\n\nCrashed during a write.  When restarted, this can be detected by\n  invalid JSON in the references file, and we'd have to walk the whole\n  repositories/tags tree to see if any of those files depended on our\n  corrupted image.  Slow, but that's ok for recovering from such an\n  unlikely situation.\n\n\nTo rephrase what you explain, the operation is not atomic. Walking\nthe whole dataset means it's not suitable for big dataset.\n\nTo make things atomic, I'd suggest initially staging images (and their\nrefcounts) in a tmp/ directory and moving them into their true\nlocations after they were full (assuming your backing store supports\natomic moves).  I'd do this for all of the registry's file\nimage/repository alterations, not just refcounting.  What happens\ncurrently if you crash with a half-written image ID in a tag file?\nEven with atomic file-writes, you'd need some way to distinguish\nbetween provisional refcounts (i.e. claimed by a tag that we're in the\nprocess of writing) that were still provisional because they were\nstill in progress, and those that were still provisional because we\ncrashed while writing them (so they were aborted).  In that case, I\nthink the logic would be:\ndef put_tag(namespace, repository, tag):\n      \u2026 extract data from request \u2026\n            \u2026 write marker for tag-put-in-progress \u2026\n      store.add_provisional_references(\u2026)\n      \u2026 put all repository-side tag content \u2026\n      store.add_references(\n          image_id=data, namespace=namespace, repository=repository, tag=tag)\n      \u2026 remove tag-put-in-progress marker \u2026\n      return toolkit.response()\nWhen touching the references files, we'd check for the provisional\nreferences file.  If we found the provisional referencing tags, we'd\nadd them to the permanent list.  If we found an active\ntag-put-in-progress marker, we'd leave them in the provisional\nreferences list, but treat them as true references (to avoid garbage\ncollecting images while the put was in progress).  If we found an\ninactive tag-put-in-progress marker, we'd start a background job\ncleaning up the ancestry chain of the failed provisional reference.\nFor local storage, the tag-put-in-progress marker storage could be\nimplemented with flock.  For other backends, I'm not sure what the\nbest approach would be.\nHow does that sound?\nHaving a transactional storage backend would make this much easier ;).\n. After cleaning up some silly typos, the unit tests are passing again\n:p.\nWe still need to reach a consensus on where this should live (I can\npull it out into a refcounting class (CautiousRefcounter?) that\nsubclasses Base and is in turn subclassed by the in-tree storage\nbackends.  That way other storage backends have a clean slate by\nsubclassing Base, or can use my implementation by subclassing\nCautiousRefcounter, or can skip ref-counting entirely by subclassing a\nNoopRefcounter.  How does that sound?\n. On Wed, Jun 25, 2014 at 08:57:25AM -0700, Olivier Gambier wrote:\n\n\nor it should be made optional\n\n\nOptional shouldn't be too hard, especially now that option parsing is\na bit better.  Any feedback on this structure 1, or should I stay\ncloser to what I already have in this branch?\n. On Wed, Jun 25, 2014 at 11:26:14AM -0700, W. Trevor King wrote:\n\nWed, Jun 25, 2014 at 08:57:25AM -0700, Olivier Gambier:\n\n\nor it should be made optional\n\n\nOptional shouldn't be too hard, especially now that option parsing is\na bit better.  Any feedback on this structure [1], or should I stay\ncloser to what I already have in this branch?\n[1]: https://github.com/dotcloud/docker-registry/pull/409#issuecomment-45178032\n\nPing.\n. On Tue, Aug 12, 2014 at 09:28:21AM -0700, W. Trevor King wrote:\n\nWed, Jun 25, 2014 at 11:26:14AM -0700, W. Trevor King:\n\nWed, Jun 25, 2014 at 08:57:25AM -0700, Olivier Gambier:\n\n\nor it should be made optional\n\n\nOptional shouldn't be too hard, especially now that option parsing is\na bit better.  Any feedback on this structure [1], or should I stay\ncloser to what I already have in this branch?\n[1]: https://github.com/dotcloud/docker-registry/pull/409#issuecomment-45178032\n\nPing.\n\nI just saw #533, so I thought I'd ping this again.  I'm still looking\nfor feedback on my proposed structure for swapable refcounting\nalgorithms\u2026\n. On Tue, Aug 12, 2014 at 09:28:21AM -0700, W. Trevor King wrote:\n\nWed, Jun 25, 2014 at 11:26:14AM -0700, W. Trevor King:\n\nWed, Jun 25, 2014 at 08:57:25AM -0700, Olivier Gambier:\n\n\nor it should be made optional\n\n\nOptional shouldn't be too hard, especially now that option parsing is\na bit better.  Any feedback on this structure [1], or should I stay\ncloser to what I already have in this branch?\n[1]: https://github.com/dotcloud/docker-registry/pull/409#issuecomment-45178032\n\nPing.\n\nI just saw #533, so I thought I'd ping this again.  I'm still looking\nfor feedback on my proposed structure for swapable refcounting\nalgorithms\u2026\n. On Tue, Sep 16, 2014 at 01:08:00PM -0700, Andy Goldstein wrote:\n\nwould you like me to try to convert this to an extension?\n\nSure.  If we're not touching Base, what's your plan for injecting this\ninto the Storage class structure?  Personally, I think the current\nstorage API is a confusing mix of registry-facing APIs and\nbackend-facing helpers that leads to wonky behavior like whatever was\nfixed by #553 (see also my earlier comments [1,2].  I've been working\ntowards what I'd like the storage APIs to look like 3, but it's\nvery WIP (and I'll wait until #570 lands before working on it much\nmore).  If we get there, it the registry-side API would be a nice\nplace to hang some logging 4 where it's independent of any\nparticular storage backend.\nI think this sort of thing would be much cleaner once we had the\nfilesystem stuff abstracted out of the registry-side storage API.\n. On Tue, Sep 16, 2014 at 01:48:00PM -0700, Andy Goldstein wrote:\n\nI wasn't actually going to try to modify Storage at all.\n\nAh, I suppose you could tie it to the signals instead.  Where would\nyou store tag references (I do it with db05eef8)?\n. On Tue, Sep 16, 2014 at 01:59:20PM -0700, Andy Goldstein wrote:\n\nIf we use your existing code in this PR, just like you've been doing\nit via image_references_path.\n\nWhich I added to Base in 3603066.  If you're not touching Base, where\ndo you put it?\n. On Tue, Sep 16, 2014 at 02:08:49PM -0700, Andy Goldstein wrote:\n\nI'd put image_references_path in the extension code, and use signals\nto react to tags being created and deleted.\n\nI think it's going to be hard to convince me to like anything that's\ngoing to end up binding to the current Base API ;).  Since fixing that\nAPI is a long shot at the moment, feel free to do whatever it takes to\nget this PR translated into something @dmp42 will accept sooner.  If\nit ends up keeping a lot of filesystem-backend assumptions, I can\nalways cut those strings and pretty things up later if/when we get a\ncleaner registry-side storage API.\n. On Wed, Sep 17, 2014 at 12:56:01PM -0700, Andy Goldstein wrote:\n\n@wking do you think we need to worry about file locking and/or\nmutexes when updating image references?\n\nHere are my previous thoughts for these issues [1,2].\n. On Thu, Oct 09, 2014 at 02:52:35PM -0700, dbason wrote:\n\nSorry if this has been covered off (I'm having problems following\nthe various discussions on deletion of images).  Will the code as is\nclean up images on an s3 compatible back end (I ask because most of\nthe scripts/cleanups I have seen in other threads look to be local\nfile system only)?\n\nI tweaked Base a bit, but I don't think I added any new\nstorage-implementation-facing methods, so I expect it will.  This\nparticular implementation is unlikely to land though, so you might\nwant to checkout #606 instead.\n. On Wed, Oct 29, 2014 at 10:53:28AM -0700, Olivier Gambier wrote:\n\n@wking @ncdc do you guys intend to work on this for v1?\n\nNot me, and @ncdc has #606.\n. On Wed, Jun 04, 2014 at 08:45:31AM -0700, Sean Fitts wrote:\n\nI saw this brought up in another issue which was closed without much\nin the way of closure.\n\nFor reference, the other issue is probably #168.\n. On Wed, Jun 04, 2014 at 10:03:07AM -0700, Sam Alba wrote:\n\nIt's weird to have to introspect, can we default signer to None in\nthe base class instead?\n\nI don't know which base class you had in mind, but having:\nif self._config.cloudfront:\n        self.signer = Cloudfront(\n            \u2026\n        ).sign\n            else:\n        self.signer = None\nin Storage.makeConnection makes the most sense to me.\n. On Wed, Jun 04, 2014 at 04:20:10PM -0700, Mangled Deutz wrote:\n\nMerged #414.\n\nI'm too slow, but I'd expect it would be easier to just use strings in\ndocker_registry/server/env.py :p.\n. On Thu, Jun 05, 2014 at 01:27:57PM -0700, Brian Hicks wrote:\n\n2014/06/05 15:14:12 Received HTTP code 400 while uploading layer: {\"error\":\"Bad Request\"}\n\n\u2018git grep\u2019 is only turning up 400 in association with better error\nmessages, except for toolkit.api_error, which takes it's own message.\nGrepping for api_error turns up no calls that would set such a generic\nerror.  Grepping for \u2018Bad Request\u2019 turns up nothing at all.  I'm\nstumped ;).\n. On Thu, Jun 05, 2014 at 06:24:45PM -0700, Joffrey F wrote:\n\n\nWrap redis calls in try..except blocks in lru module\ntry..except around redis_conn uses\n\n\nThe errors seem to have useful information like 1:\nError 111 connecting registry-docker.azva.dotcloud.net:2755.\nso I'd prefer:\nexcept redis.exceptions.ConnectionError as e:\n      logging.warning('Redis connection error: {0}'.format(e))\nIf you want an LRU prefix, I think it's better to use a logging\nformatter 2 with the '%(name)s' placeholder 3.  That will give\ndocker_registry.core.lru because we initialize with 4:\nlogger = logging.getLogger(name)\n. On Thu, Jun 05, 2014 at 10:07:44PM -0700, Mangled Deutz wrote:\n\nNow, I don't see an easy way out of this...\n\nIn our case, we know what got pushed, so we can keep a \u2018dirty\u2019 flag on\neach cached key, set the flag if we fail a push, refuse to use the\ncache if that flag is set, and clear the flag once we get the\nun-cached response from storage.\n. On Fri, Jun 06, 2014 at 11:56:56PM -0700, Mangled Deutz wrote:\n\nthat won't work - unless you only use one registry (in which case\nhaving a distributed datastore is not that meaningful...).\n\nHow does this work now?  Do we lock files before writing them?  Do we\njust hope that we don't have simultaneous, conflicting writes?\n- Registry A gets a PUT request that will cause it to update file X.\n- Registry A reads file X from cache (or storage).\n- Registry B geta a PUT request that will cause it to update file X.\n- Registry B reads file X from cache (or storage).\n- Registry A alters it's in-memory version of X and writes it to cache\n  and storage.\n- Registry B alters it's in-memory version of X and writes it to cache\n  and storage, clobbering registry A's changes.\nShort of storage-side file locking (and a way to cleanup the locks if\na registry dies with the lock held), I don't see how we can avoid racy\ndata loss.  If we're comfortable with racy data loss, having a more\nrobust Redis cluster managed by Sentinel 1 and registry-local dirty\nflags seems like the best path forward.  It only increases the\n\u2018parallel write\u2019 cross section to include Redis outage times.  Given\nthe uptime on the previous Redis instance, that seems like a minor\nincrease.\n. On Sat, Jun 07, 2014 at 11:22:24AM -0700, Joffrey F wrote:\n\nThanks guys, I'm looking into it.\n\n@shin- tracked this down to an overly aggressive pull/push timeout,\nand it should be fixed with dotcloud/docker#6260 (which is in master\nnow).\n. On Sun, Jun 08, 2014 at 08:55:40AM -0700, Nick Stinemates wrote:\n\nWhen will this fix be pushed? It's happening on the public registry\nas of this morning.\n\nIt's a docker-daemon-side issue, not a registry-side issue.  So just\ndowngrade your local docker-daemon to something before 0.12.0.  Likely\nanything after 0.12.0 will have the fix too.\n. On Sun, Jun 08, 2014 at 10:13:34AM -0700, Nick Stinemates wrote:\n\n@wking - thanks for fixing that issue!\n\n@shin- tracked it down, and @vieux wrote the patch.  I'm just passing\ninformation around ;).\n. On Wed, Jun 11, 2014 at 06:57:45AM -0700, Jay Atkinson wrote:\n\nError:\n2014/06/11 08:55:50 Invalid repository name (SM_release), only [a-z0-9-_.] are allowed\n\nThat's definately a docker-side issue, not a registry issue.  See\ndotcloud/docker@a55a0d370 (([a-z0-9_]{4,30})/([a-zA-Z0-9-_.]+),\n2013-06-03) and dotcloud/docker@5867f9e7 (Modify repository name regex\nto match index, 2013-09-19).\n. On Fri, Jun 13, 2014 at 10:15:54AM -0700, Mangled Deutz wrote:\n\nParts of the code expect an unset value to default to False, and\nobviously False is not None.\n\nIn situations like this, I think it's usually best to use:\nif myvar:\nor:\nif not myvar:\ninstead of the stricter:\nif myvar == True:\nor:\nif myvar == False:\nObviously, that's if the \u201cunset\u201d None should be defaulting to False,\nwhich is also usually the case for me.\n. On Fri, Jun 13, 2014 at 10:21:57AM -0700, Mangled Deutz wrote:\n\nMerged #424.\n\nAh, my #423 comment was too slow :p.\n. On Mon, Jun 16, 2014 at 12:44:15PM -0700, David Woodruff wrote:\n\nI have a need to host a private docker registry instance on a server\nwhere the registry will be hosted on a non-root path. For example, I\nneed to host it on localhost:5000/my-path instead of just\nlocalhost:5000/.\n\nI can't think of any reason to need to do that that doesn't already\ninvolve a reverse-proxy, and I can't think of reverse-proxy software\nthat doesn't make that sort of path-manipulation easy.  Can you give\nmore details about why you need the extra path and about what sort of\nnetwork setup you plan to use?\n. On Mon, Jun 16, 2014 at 01:48:53PM -0700, David Woodruff wrote:\n\nThis problem manifests itself in any application that sends back any\nlinks to the client. If your API never does that, then there's no\nproblem using a reverse proxy.\n\nI don't know of any Docker endpoints that return URLs or 3xx\nredirects, so I think you'll be fine rewriting the incoming URLs.\n. On Tue, Jun 17, 2014 at 04:35:51AM -0700, petarmaric wrote:\n\nThe solution is to properly convert a string into a boolean, as per\nhttp://stackoverflow.com/questions/715417/converting-from-a-string-to-boolean-in-python\n\nOr converting to a subclass of ConfigParser (a subclass so we can add\nenvironment-variable expansion) 1, after which we could use getboolean\n2.\n. I think hard-coding this setting is fine for a quick fix, but I'd\nprefer to a more generic solution like getboolean 1.  We took a\nsimilar approach (casting to strings at config-access time) in\n4c8cfc51 (Fix for execv() typage error (only strings), 2014-06-02).\n. On Mon, Sep 08, 2014 at 02:40:14PM -0700, Olivier Gambier wrote:\n\n@shin- @wking you guys want to take a look?\n\nI'm not familiar with any of the s3 stuff.\n. I seem to remember Python-2.6 support being somewhere in the\ndocker-registry plan (although I don't care about that myself).\nAnyhow, I have no problem with keeping the explicit formatting\nindexes.\nOther than that, the more tests the better ;).  If Travis likes them,\nI'm happy.\n. On Fri, Jul 11, 2014 at 07:48:25AM -0700, Olivier Gambier wrote:\n\nHow do other python application projects resolve this?\n\nAs I mentioned earlier 1, I solve this (unfortunately, in a\nclosed-source package), by having ranged requirements (depending on\nthe APIs I'm using) in most requirements/*.txt files.  To freeze out a\nparticular set of exact versions, I run:\n$ pip freeze > requirements/frozen.txt\nand commit that as well.  Devs are recommended to run from the looser\nrequirements:\n$ pip install -r requirements/main.txt\nand to re-freeze them (to automatically catch upgrades) before testing\nfor a new release.  Users and production deploys are encouraged to run:\n$ pip install -r requirements/frozen.txt\nto get the exact versions that have been tested before the release was\ncut.\n. On Fri, Jul 11, 2014 at 09:11:27AM -0700, W. Trevor King wrote:\n\n$ pip freeze > requirements/frozen.txt\n\nAnother benefit to this approach is that you'll also get exact\nversions for any indirect dependencies.  Of course, you'll need to run\nit from a virtualenv or other clean system to avoid accidentally\nfreezing system packages you've installed for other projects.\n. On Mon, Jul 14, 2014 at 10:54:18AM -0700, Olivier Gambier wrote:\n\nThe question though is more what to put/use inside setup.py to\naccommodate both package maintainers and us.\n\nPackage managers shouldn't be using setup.py to install dependencies\n(via setuptools or some such), since they should be installing the\ndependencies using their package manager.  I think and dependencies\nlisted in setup.py should be as flexible as possible.  Folks who are\ninterested in installing their own non-dev docker-registry should run\nsomething like:\n$ pip install -r requirements/frozen.txt\n  $ python setup.py install\nThat way they get the exact tested requirements, but package managers\njust using setup.py for installation don't accidentally pull in a\nbunch of slightly-different versions.\n. On Tue, Jul 15, 2014 at 07:13:01AM -0700, Olivier Gambier wrote:\n\n\nIf you can't have softer dependencies in setup.py\n\nIndeed, I'm not keen on that. We already had problems in the past\nwith minor revisions breaking on us, and going that way (eg: soft\ndeps) is a PITA for us (people already have a hard time stating\nclearly what registry version they are using...) - and we are just\nnot mature enough for non-fixed deps, sorry.\n\nSo the problem is that some users are going to install using just\nsetup.py, and you want the default to be frozen deps.  Since none of\nthe requirements files exist locally for the pip-using public, perhaps\nyou could have sliding dependencies by default in setup.py, and then a\nseparate extras_require with frozen (and frozen-bugsnag)\ndependencies.  Then you can suggest folks run:\n$ pip install docker-registry[frozen]\nor\n$ pip install docker-registry[frozen-bugsnag]\nwhile devs and packagers who are comfortable with the increased risk\nof sliding dependencies could use the requirements files and sliding\ndefault setup.py dependencies.\nThe \u201cour users have trouble reporting the version their using, but\nwill get better as our project matures\u201d argument you seem to be making\ndoesn't make much sense to me ;).  One way to make this easier for\neverybody might be to report versions of all dependencies in /_ping\n(or add a registry-specific /_debug?).  Then just ask folks to cURL\nthat when reporting bugs.\n. On Wed, Jul 16, 2014 at 06:35:55AM -0700, Olivier Gambier wrote:\n\n\"our user have trouble reporting the version they are using, but WE\nwill get better at providing them with simple to use debug/reporting\ntools giving us the information we need as our project matures -\nright now, we don't, and debugging the registry is admittedly voodoo\nto the general public\".\n\n473 should solve the version-reporting issue (as I understand it).\nNow just ask folks to submit the output of:\n$ curl -XGET 'http://your-registry:5000/_versions'\nalong with their bug report.  If they're running a production config\nand don't know how to set the VERSIONS environment variable to enable\nthat endpoint, then shame on them ;).\n. On Mon, Jul 21, 2014 at 11:42:11AM -0700, Troy Cornwall wrote:\n\n2) run the gunicorn command yourself, with something along the lines of:\n\nI just submitted #483 for these sorts of arbitrary command-line\ntweaks.  There's no need to restrict users to Gunicorn options that\nhave explicit config settings.\n. On Mon, Jul 21, 2014 at 11:42:11AM -0700, Troy Cornwall wrote:\n\n2) run the gunicorn command yourself, with something along the lines of:\ngunicorn --access-logfile /var/logs/gunicorn.access.log --error-logfile /var/log/gunicorn.error.log --debug --max-requests 100-k gevent --graceful-timeout 3600 -t 3600 -w 4 -b 0.0.0.0:5000 docker_registry.wsgi:application\n\nNote that if you're using the search-index (search_backend) you'll\nwant --preload too 1.\n. On Mon, Jul 14, 2014 at 01:51:14PM -0700, Matt Hughes wrote:\n\nRight now, I'm going down the path of pip installing everything but\nmy container needs to be built offline, so that means preloading all\ndependencies which is kind of a pain.\n\nWhy is it a pain?  Can't you just run:\n$ pip install -r requirements/main.txt\n. On Tue, Jul 15, 2014 at 07:02:01AM -0700, Olivier Gambier wrote:\n\njust installing the (main) requirements will (likely) fail and is\nnot recommended.\n\nWhat doesn't get pulled in?  \u2018pip install -r requirements/main.txt\u2019\nshould recurse through their dependencies.  Looking at setup.py, it\nlooks like some additional dependencies are injected dynamically for\nPython 2 (backports.lzma>=0.0.2) and <=2.6 (argparse>=1.2.1,\nimportlib>=1.0.3) as well as the tox-protecting\ndocker-registry-core>=1,<2.  How about:\n$ cat requirements/main-for-tox.txt\n  blinker==1.3\n  \u2026\n  sqlalchemy==0.9.4\n  $ cat requirements/main.txt\n  -r main-for-tox.txt\n  docker-registry-core>=1,<2\n  $ cat requirements/python2.7.txt\n  -r main.txt\n  backports.lzma>=0.0.2\n  $ cat requirements/python2.6.txt\n  -r python2.7.txt\n  argparse>=1.2.1\n  importlib>=1.0.3\nThen folks on Python 2.7 could:\n$ pip install -r requirements/python2.7.txt\n  $ pip install .\nFolks on Python 2.6 could:\n$ pip install -r requirements/python2.6.txt\n  $ pip install .\nFolks running production deploys of either could run the parallel,\nfrozen chain:\n$ pip install -r requirements/python2.7-frozen.txt\n  $ pip install .\nAssorted optional stuff like bugsnag could be dropped into\nrequirements/extra.txt, which folks could cherry-pick from if they\ndidn't want all the extras.\n. On Wed, Jul 16, 2014 at 06:46:38AM -0700, Olivier Gambier wrote:\n\n\nWhat doesn't get pulled in?  Looking at setup.py, it looks like\nsome additional dependencies are injected dynamically\n\nThen you got your answer, right?\n\nI think so.  Thanks for checking :).\n\n\nFolks on Python 2.6 could:\n\n$ pip install -r requirements/python2.6.txt\n  $ pip install .\nI don't want it.\nThis makes it uselessly complex\u2026\n\nI agree it's not super-elegant.  However, static dependencies that\nvary over our range of supported systems seems like a complex problem\nto me, so I'm not surprised that supporting it requires a slightly\ncomplex solution.  We could always wire those requirements files into\nextras_require and have folks do things like 1:\n$ pip install docker-registry[frozen]\nand have that automatically pull in the\nrequirements/python2.6-frozen.txt dependencies when run from Python\n2.6 (etc., etc. for other packages).\n\nthe general public should get the most simple solution possible...\n... which is, and will stay pip install docker-registry (no matter\nwhat python version they use).\n\nThe problem is that it's impossible to turn off frozen dependencies if\ntheir the default.  But I guess folks needing static, flexible\ndependencies can always patch setup.py.\n. On Wed, Jul 23, 2014 at 03:51:51AM -0700, Olivier Gambier wrote:\n\nNow, do we need a separated conf var to enable this?\n\nIf it was me, I'd drop the config completely, have this always\nenabled, and just tweak my Nginx config to mask it from host-external\nrequests ;).\n\nMy point being, could we \"centralize\" things, and when (say) a given\n\"debug\" variable is true, activate all debug endpoints / logging /\netc, instead of (possibly) ending up with different vars for\ndifferent infos?\n\nFine with me.  It's easy to swap in whatever variable name you like,\njust tell me what you want ;).\n. On Fri, Jul 25, 2014 at 07:12:33AM -0700, Olivier Gambier wrote:\n\n\nFine with me.  It's easy to swap in whatever variable name you\nlike, just tell me what you want ;).\n\nLet's have a \"debug\" key.\n\nIf it's just for enabling endpoints, how about debug_endpoints?\nWith text like:\ndebug_endpoints: boolean, enables additional endpoints for\n  accessing debugging information (current just /_versions, but we\n  may add more later). This will leak information about your system\n  (that's the point ;), so you probably don't want it enabled for\n  production systems.  If you do have it enabled for a production\n  system, you should have a reverse proxy in front of your registry\n  that masks the endpoints from external users.\n. Thanks for the ping :).\nOn Fri, Jul 25, 2014 at 09:16:19AM -0700, Olivier Gambier wrote:\n\nI would still prefer \"debug\" - true, for now it's only about\nenabling endpoints, but ultimately, that's for putting the registry\nin \"give me all the infos\" mode, whatever it does (endpoints,\nextended log info, whatever).\n\nWe already have \u2018loglevel\u2019 for setting the log level.  If we convert\n\u2018loglevel\u2019 into a generic \u2018debug\u2019, I think it would just be confusing\n(e.g. \u2018debug: critical\u2019 means \u201cvery little debugging\u201d).  So let's\nleave \u2018loglevel\u2019 alone.\nThen \u2018debug\u2019 will be \u201cenable debugging endpoints and whatever\u2026\u201d.\nUnless we have some idea of what that whatever will be, I'd rather\navoid an overly generic name here.\n. On Tue, Aug 19, 2014 at 11:18:20AM -0700, Olivier Gambier wrote:\n\nOk. Can we then call this \"debug_versions\", then?\n\nDone and pushed.\n. On Fri, Jul 18, 2014 at 08:54:52AM -0700, David Woodruff wrote:\n\nSo I'm not familiar with SQLAlchemy, If I wanted to configure a\ndatabase at  on port , how would I represent that in a\nSQLAlchemy string? Also, how does your registry know the credentials\nfor my database? Is there some way to pass those in?\n\nThe original search-index docs pointed folks to the SQLAlchemy docs\nfor this.  Subsequent docker-registry commits removed those details,\nbut I've got a PR open to restore them in #472.  Basically, we just\npass the configured sqlalchemy_index_database value through to\ncreate_engine 1, and SQLAlchemy has good docs for what that accepts.\n. On Fri, Jul 18, 2014 at 10:03:40AM -0700, David Woodruff wrote:\n\nOne more question about this: If my database dies for some reason,\nwhat is the recommended way for repopulating this search index?\nObviously restoring from backups is one option, although we'll\nexperience some data loss. Is there a way to have the registry go\nlook in S3 and repopulate the search index?\n\nIf you enable the search-index and docker-registry connects to an\nempty database, it will automatically populate the database from\nscratch.  So no worries if your database crashes, so long as your\nregistry isn't so huge that repopulating the database won't take ages\n;).\n. On Fri, Jul 18, 2014 at 04:46:59PM -0700, fs-build-system wrote:\n\nSo I need to get that psycopg2 module inside the container in order\nto be able to use a postgres database on the registry. Is there any\nway I can do that?\n\nIt's probably easiest to write your own follow-up Dockerfile based on\nthe Stackbrew image 1.\n$ cat Dockerfile\n  FROM registry\n  RUN apt-get install python-psycopg2\n  $ build -t my/registry .\n  $ docker run my/registry \u2026\n. On Mon, Jul 21, 2014 at 06:17:15AM -0700, Anton Tyurin wrote:\n\nI'm pretty sure that it would be useful to give an ability to a\ndriver implementation to return some statistic about its health\nstatus. It's quite cool to organize monitoring system.\n\nIt's hard to know what statistics folks would want to gather.  For\nexample, if your using the local-storage driver, do you just want disk\nusage percentage?  What about disk throughput speeds?\nS.M.A.R.T. feedback?  Anyhow, I'd recommend folks reach around the\nregistry and just ask the backend directly for whatever information\nthey'd like to monitor.  That said, I have no problem if folks want to\ntake a stab at useful information, and have docker-registry expose it\non some private endpoint.  Just have the registry-side code catch\nNotImplementedError to support drivers that don't want to bother.\n. On Mon, Jul 21, 2014 at 10:09:18AM -0700, Anton Tyurin wrote:\n\nFor example in Yandex we use Elliptics driver and Elliptics has its\nown monitoring system and it would be great to use this information\nin the status checking withoug going to storage nodes.\n\nCan't you just write an independent Elliptics-status-checker, and have\nthe sysadmin use that to query Elliptics directly?  I don't see the\nbenefit to piping the information through docker-registry.\n. On Mon, Jul 21, 2014 at 10:42:58AM -0700, Olivier Gambier wrote:\n\nI see a benefit in simplifying/centralizing things for the user\u2026\n\nIt's your (light) maintenance burden.  Knock yourself out ;).\n. On Mon, Jul 21, 2014 at 11:17:34AM -0700, Anton Tyurin wrote:\n\nLet's go step by step. The first point is that the current '_ping'\nis absolutely useless as it doesn't check anything except\ngunicorn/nginx.\n\nAnd the registry code itself (e.g. Flask).  That's about all the\nregistry-side status I think there is.  Say you have a handful of\nregistry servers, backed by another handful of storage servers.  You\ndon't want to be pinging the registry servers to ask about the health\nof the storage servers.  I expect you'd want one manager to make sure\nthe Flask apps were up and working, and another manager to make sure\nthe storage servers were up and working.\n. On Mon, Jul 21, 2014 at 01:22:09PM -0700, Anton Tyurin wrote:\n\nPlease, don't let me down!\n\nDon't worry, I'm not a maintainer ;).  I'm just pushing to keep the\ndocker-registry code base focused on what it does best.\n\nIt's hardly possible to know more then the mainteiner about a driver\nneeds.\n\nThe sysadmin may have specific needs as well.  Anyhow, I have no\nproblem with the driver maintainer also publishing a\nstorage-engine-status-checker, I'd just keep that independent of\ndocker-registry (if I were in charge, which I'm not).\n\nIt goes without saying that this handle will check Flask code (it's\nimmutable during one installation, yeah? So what the purpose to\ncheck it?).\n\nThe Flask app may have crashed ;).  I was just pointing out that a\nsuccessful ping response means everything is working up through the\nPython code in this repository.  I agree that it's not checking about\nthe health of the local disk or any networked systems behind the\ncode in this repository (and I'm fine with that, since I see the\nbacking storage as an independent service).\n. On Mon, Jul 21, 2014 at 01:35:59PM -0700, Anton Tyurin wrote:\n\nAlso we have guys who are responsible for keeping an eye on storage.\n\nWho presumably want status reporting regardless of whether or not\ndocker-registry is running on top of their storage.\n\nAnd guys who maintain registry should think and imagine this storage\nas blackbox. So they need to have this handler.\n\nErr.  I agree with the first sentence, but my conclusion is \u201cSo they\nshouldn't care what the storage status is.  Their storage maintainers\nwill notice if the storage breaks, and fix them without any\nregistry-maintainer prompting\u201d.\n. On Mon, Jul 21, 2014 at 01:02:43PM -0700, Olivier Gambier wrote:\n\nTravis doesn't like you today :-)\n\nAh, thanks.  Potential fix squashed and pushed.\n. On Wed, Jul 23, 2014 at 02:37:45AM -0700, Olivier Gambier wrote:\n\nGithub is not happy :/\n\nc7af1ae (Updated eny.py to include default logging to console / docker\nlogs, 2014-07-21, #481) touched the same part of\ndocker_registry/server/env.py as my patch.  I've rebased my patch and\npushed.\n. On Tue, Jul 22, 2014 at 09:48:12AM -0700, Gabriel Monroy wrote:\n\nThis PR resolves the problem by moving logging.basicConfig prior\nto importing modules that assume logging is already setup.\n\nThis looks good to me.  I usually keep my logging config in the main\npackage's namespace (e.g. some_project/init.py), so it gets setup\nbefore any of the subpackages.  However, I don't think we can do this\nand use setuptools' namespace stuff (or PEP 420's namespace stuff).\nSo baring a shift away from namespace packages (not likely 1) or the\naddition of a core namespace:\ndocker_registry\n  |-- init.py  (with the setuptools namespace magic)\n  -- core\n      |-- __init__.py  (with the logging setup, version declaration, etc.)\n      |-- app.py\n      |-- drivers\n      |-- \u2026\n      |-- images.py\n      \u2026\nI don't see a cleaner way to do this than what's already in this PR.\n. On Wed, Jul 23, 2014 at 08:37:08AM -0700, Matthew Fisher wrote:\n\n\nfeat(tags): add registry-to-registry transfers\n\n\nI'm still digesting this as well, but:\nsrc_image = flask.request.form.get('src')\nisn't used for DELETE, so you should be setting it inside the \u2018if\nflask.request.method == 'POST'\u2019 block.\nAlso, are you merging a pull request from yourself in 44fe2dae8?  And\nsourcing the pull request from a deis branch instead of a branch of\nyour own fork?  To me, it makes more sense if you just pushed 3cf76a48\nto your own repository-import branch and dropped 44fe2dae8.\n. On Thu, Aug 07, 2014 at 11:44:24AM -0700, Matthew Fisher wrote:\n\nWe're actively developing these features for this branch and using\nit in Deis \u2026 This is something we had to integrate with our platform to\nget this feature in, so this is more of a posing question of \"does\nupstream want this sort of feature?\"\n\nI'm not a maintainer, so it's not my call.  I like the idea here, but\nI haven't looked through the implementation yet.  How will Deis handle\nit if a rebased version of this branch lands here?  Will you just\nrebase any affected Deis branches?\n. Why do you need configurable listeners?  It sounds like something the\nexisting signals framework already handles 1.\n. On Mon, Sep 08, 2014 at 03:24:15PM -0700, Clayton Coleman wrote:\n\nAre you asking why the listeners are configurable (so folks can\ninject customer listeners via registry conf) or why there is a\ncustom listener class instead of just directly hooking up to\nsignals?\n\nI was asking the latter, but feel free to answer the former too ;).\nI'm just having trouble imagining what this is for.\n. On Mon, Sep 08, 2014 at 03:34:04PM -0700, Clayton Coleman wrote:\n\nThe concrete example is so we can define a listener that makes rest\ncalls when things happen in the registry as simple glue (tag\ncreated, make a API call to foo to notify).\n\nI'd just bind to the signal than, since I don't see much need for glue\naround:\nimport docker_registry.lib.signals\ndef push_api_notification(sender, namespace, repository, tag, value):\n      \u2026your push code here\u2026\ndocker_registry.lib.signals.tag_created.connect(push_api_notification)\nIf you just need a way to load this from a stock docker-registry, I\nthink all you need is an 'import_modules' config that imports your\nmodule with the connection call:\nimport_modules:\n    - openshift.docker_registry.hooks\n    - mycustom.package\n    \u2026\n. On Wed, Jul 30, 2014 at 01:36:47PM -0700, Matthew Fisher wrote:\n\nYou can only delete tags and repositories, but not individual\nimages.\n\nTo remove unused images alongside the tags, see #409, which is\ncurrently blocked waiting for architectural feedback, but works for me\nlocally.\n. On Fri, Aug 01, 2014 at 02:23:15AM -0700, Olivier Gambier wrote:\n\n\nrun the registry on 2.6 and verify that basic pull / push\n  operations work, plus search enabled works\n\n\nCan't we just write those into the test suite?  I haven't checked,\nmaybe we already do, and you just don't trust the tests ;).\n. On Thu, Aug 07, 2014 at 08:37:25AM -0700, Olivier Gambier wrote:\n\n\nNew relic support\n\n\nThis commit uses env.source('NEW_RELIC_INI') twice.  You just use the\nalready-extracted newini when calling newrelic.agent.initialize().\nAlso, I'd prefer _new_relic_ini as the variable name.  It's private\n(hence the underscore) and lowercase (for PEP 8).  Otherwise I see no\nreason to diverge from the environment variable name.\nI'd also prefer a trailing comma in setup.py's extras_require.  Then\nyou don't have to touch the newrelic line when you add another extra\n(like you had to touch the bugsnag line here).\n\n\nLoose dev requirements\n\n\nYay :).  Although it looks like you're still not trusting these\npackages to use semantic versioning ;).  But I'll take what I can get\n:p.\n\n\nPython compat\n\n\nWhy move the \u2018ver\u2019 assignment here?  And I imagine these are for\nPython 2.6, but saying that explicitly in the commit message would be\nnice.\nOtherwise these look good to me.\n. On Sat, Aug 09, 2014 at 01:13:38AM -0700, Olivier Gambier wrote:\n\nYou got your semver zen, mister :-)\n\n6a580a2e (Amending previous commits, 2014-08-09) looks good to me.\nAny chance I can get it split and squashed into the original commits?\n;).  It will make for easier archaeology later and easier reviewing\nfor @shin- now ;).\n. On Fri, Aug 08, 2014 at 07:58:10AM -0700, Romain Vrignaud wrote:\n\nI'm currently running docker-registry (0.7.3) with\ndocker-registry-driver-swift (0.1.0).\n\nI don't think the driver should matter.\n\nWhen starting a new docker-registry with an emtpy database, search\nqueries are returned empty.  If I push a new image, it's correctly\nindexed.  Is there any way to force re-indexing of all present\nimages ?  In issue #475, @wking mentioned that docker-registry\nshould automatically repopulate the index. Am I missing something ?\n\nHere's how it's supposed to work:\n1. docker_registry.lib.index.SQLAlchemyIndex.init() \u2192\n   SQLAlchemyIndex._setup_database/\n2. SQLAlchemyIndex._setup_database tries to get the version from the\n   \u2018version\u2019 table (\u2018version = session.query(\u2026).first()[0]\u2019).\n3. The version table doesn't exist (because the DB is empty), so the\n   session.query raises a sqlalchemy.exc.OperationalError.\n4. We catch the error and fallback to \u2018version = None\u2019\n5. \u2018if version\u2019 is false, so we call SQLAlchemyIndex._generate_index()\n6. SQLAlchemyIndex._generate_index iterates through\n   Index._walk_storage() and adds the returned repositories.\nYou can add some debug logging to figure out where that's going\nastray.  The only dependencies are store.list_directory and\nstore.repositories.  Perhaps docker-registry-driver-swift tweaks the\nstorage layout for repositories?\nIn any case, I think docker-registry-core's docker_registry.core.Base\nUI is too tightly bound to the filesystem structure used by\ndocker_registry.drivers.file.Storage, and I'd be happier if there was\na more generic UI (e.g. Storage.repositories() which iterated through\navailable repository names).  Storage drivers that used a backing\nfilesystem could inherit from docker_registry.drivers.file.Storage if\nthey wanted to pick up the current filesystem layout, and just replace\nthe disk-access methods with their own implementation.\n. On Mon, Aug 11, 2014 at 08:37:25AM -0700, chewmanfoo wrote:\n\n2014-08-11 15:32:47,735 ERROR: Exception on /v1/repositories/cem_base_centos/ [PUT]\nTraceback (most recent call last):\n  File \"/\u2026/flask/app.py\", line 1817, in wsgi_app\n    response = self.full_dispatch_request()\n  \u2026\n  File \"/\u2026/docker_registry/index.py\", line 102, in put_repository\n    update_index_images(namespace, repository, flask.request.data)\n  File \"/\u2026/docker_registry/index.py\", line 83, in update_index_images\n    value=json.loads(data.decode('utf8')))\n  \u2026\n  File \"/\u2026/docker_registry/lib/index/db.py\", line 92, in _handle_repository_created\n    session.commit()\n  \u2026\n  File \"/\u2026/sqlalchemy/engine/default.py\", line 435, in do_execute\n    cursor.execute(statement, parameters)\nIntegrityError: (IntegrityError) UNIQUE constraint failed: repository.name u'INSERT INTO repository (name, description) VALUES (?, ?)' ('library/cem_base_centos', '')\n\nIt looks like we're raising the repository_created signal, but the\nrepository already exists?  Looking through update_index_images(), the\nonly storage access seems to be for\npath = store.index_images_path(namespace, repository)\nI'm not sure how that path does not exist on your storage backend,\nbut the associated entry does exist in your index database.  Perhaps\nit's another \u201ceventual consistency\u201d thing (like #408 and #418)?\n. On Mon, Aug 11, 2014 at 10:50:12AM -0700, chewmanfoo wrote:\n\nJust to be clear - this registry has no persistent storage - it's\njust for test.  When I kill the container, I assume the images the\nregistry contains disappear, as well as any internal database data.\nAm I wrong about that?  I never commit any changes to this container\n\nThat's right without the volume options (where your registry works\n1).  When you mount the storage path from the host, the host is\npreserving your images.  You're probably not preserving the index\ndatabase in either case, because you're not tweaking\nSQLALCHEMY_INDEX_DATABASE or volume-mounting something to back\n/tmp/docker-registry.db.\nOn Mon, Aug 11, 2014 at 10:54:16AM -0700, Olivier Gambier wrote:\n\n@wking we are speaking filesystem storage here (not S3) - so,\nconsistency issue seems unlikely.\n\nOk.  Seeing a log of database and storage actions would be helpful.\nNeither seems to be instrumented with logging calls at the moment, but\nadding that logging would probably be a good idea.\n. On Wed, Aug 20, 2014 at 03:08:25PM -0700, Olivier Gambier wrote:\n\n\nJust to be clear - this registry has no persistent storage - it's just for test.\n\nWell, you told it to have persistent storage, right?\n\nsudo docker run -d -e STORAGE_PATH=/tmp/docker-store -v /tmp/docker-store /login/sg218049/docker-store\n\nOr am I missing something?\n@wking so, does that change the scenario / explain why we fail?\n\nSo we're persisting the usual repository storage, but not the index\ndatabase (which is /tmp/docker-registry.db by default 1).  However,\nthe problem is due to a repository that is in the index database, but\nnot the present in the registry.  I don't see how persisting the\nrepository storage and forcing a index database rebuild would cause\nthat.\n. On Mon, Sep 08, 2014 at 01:30:56PM -0700, Olivier Gambier wrote:\n\n@wking I reproduced.\n1. start fresh\n2. DOCKER_REGISTRY_CONFIG=config_sample.yml SETTINGS_FLAVOR=test nosetests so you get some content\n3. rm the index: rm /tmp/docker-registry.db\n4. start the registry and crash: DOCKER_REGISTRY_CONFIG=config_sample.yml SETTINGS_FLAVOR=test docker-registry\n\nHmm, maybe I'm doing something wrong, but I don't see an error with:\n$ docker pull registry:0.8.1\n  $ docker run --rm -i -t registry:0.8.1 /bin/bash\n  # cd docker-registry\n  # pip install -r requirements/test.txt\n  # DOCKER_REGISTRY_CONFIG=config_sample.yml SETTINGS_FLAVOR=test nosetests\n  # rm /tmp/docker-registry.db\n  # DOCKER_REGISTRY_CONFIG=config_sample.yml SETTINGS_FLAVOR=test docker-registry\n. On Mon, Oct 27, 2014 at 04:01:31PM -0700, Olivier Gambier wrote:\n\nI don't quite understand why there is a race-condition (and it looks\nlike there is) in the first place...\n\nI expect the race is just a bunch of Gunicorn threads all trying to\ncreate the database at the same time.  Here's how the initial index\ngeneration is supposed to work 1, and here's using --preload to\navoid the race [2,3,4].  Personally, I'd just drop --preload into my\nGUNICORN_OPTS [5,6].\nLet me know if that fixes your problem.  If it does, we should patch\nthe search-engine notes in the README to mention it.\n. On Tue, Oct 28, 2014 at 10:33:38AM -0700, Olivier Gambier wrote:\n\nwhy does preloading prevent the race condition?\n\nBecause it means the master process loads a single SQLAlchemyIndex\ninstance, and that instance sets up the database.  After the database\nis setup, the master process starts forking workers.  The race happens\nwhen the workers are all trying to setup the database simultaneously,\nand the initialization is not designed for parallel execution.  If you\nwanted something like that, you'd have to shard\ndocker_registry.lib.index.Index._walk_storage to keep the parallel\nworkers from just treading in each-other's footsteps.\n. On Tue, Oct 28, 2014 at 12:28:00PM -0700, Olivier Gambier wrote:\n\nDo you say that our current implementation is not safe to be used in\na context where there are multiple workers? Or is it \"just\" for the\ndatabase creation?\n\nI expect it's just the database creation, but my personal deploy\ndoesn't do a lot of simultaneous writes to the search index, so it's\npossible that SQLAlchemy has broken locking that I haven't hit yet.\n\nIs there no mechanism in sqlalchemy to prevent such race condition?\n\nIt's hard to lock on database creation, since you'd need an\nout-of-band lock.  Still, I see no point in bandaiding over the\ndatabase/table creation if you're just going to have all the workers\nwalk the storage hand in hand to populate the database (with frequent\ncontention for indexing the repositories that they hit?).  So you'd\nwant to lock all but one worker while the unlocked fellow initialized\nthe database and walked the storage.  That's basically what --preload\ndoes, so why bother rolling our own solution?\n. On Tue, Aug 12, 2014 at 12:18:06PM -0700, Olivier Gambier wrote:\n\n@wking I assume this is LGTM from you?\n\nI'd still want:\n- A squash 1, \n- An explanation for \u2018-w 1\u2019 \u2192 \u2018-w 10\u2019\n- An explanation for \u2018--max-requests 100\u2019\n- Uniformity between the README suggestion and the\n  docker_registry/wsgi.py comment (or just dropping the wsgi.py\n  comment).\n- Dropping the trailing whitespace cleanups in the README (or\n  factoring them out into a separate commit).\n. On Tue, Aug 12, 2014 at 12:25:07PM -0700, W. Trevor King wrote:\n\nTue, Aug 12, 2014 at 12:18:06PM -0700, Olivier Gambier:\n\n@wking I assume this is LGTM from you?\n\nI'd still want:\n\u2026\n\nA link to http://docs.gunicorn.org/en/19.0/news.html#core in the\ncommit message to motivate the --debug removal would be nice too.\n. On Tue, Aug 12, 2014 at 12:31:46PM -0700, Kevin Littlejohn wrote:\n\nthe update to -w and --max-requests just make the dock match the\nactual use a bit more closely\n\nSounds good to me.  Just say that in your commit message so folks\ndon't have to ask ;).\n\nI can fix all the above, but newbie question - do I submit this as a\nnew PR with a single commit, or can I cleanly adjust what's been\npushed to be a single commit (and if so, how)?\n\nWhen there's already more than one commit, I usually do something\nlike:\n$ git rebase -i origin/master\nand convert all the commits but the first from \u2018pick\u2019 to \u2018squash\u2019.\nThen edit the commit message appropriately.\nIf you have just made one commit and realize you goofed, you can:\n$ git commit --amend \u2026\nto adjust the most recent commit.  Once you've fixed your local\nbranch, just force-push to the source of the pull-request:\n$ git push -f silarsis master\nassuming \u2018silarsis\u2019 is the name for your public GitHub repository.\n. On Tue, Aug 12, 2014 at 12:36:43PM -0700, W. Trevor King wrote:\n\n$ git rebase -i origin/master\n\nIt looks like this went well :).  bbca252f still has some issues,\nthough:\n- Multi-line commit messages should have a single-line summary,\n  followed by a blank line, followed by the commit message body.\n  Otherwise you get things like:\n$ git log -1 --oneline bbca252f\n\n\nUpgraded gunicorn to 19.1 to gain timezones in logs * Altered docker-registry logging to include timezone * Altered documentation to more closely reflect implementation (-w and --max-requests flags as per use in run.py) * Removed unecessary documentation from wsgi.py\n\nIf you feel the need for a bullet-point list, you may want to make a\n  separate commit for each bullet point.  In this case, I'd probably\n  just have a single commit with a message like:\nUpgrade gunicorn to 19.1\n\n\nGunicorn added timezone logs in vX.X.X 1\nAltered docker-registry logging to include timezone\nAltered documentation to more closely reflect implementation (-w\n    and --max-requests flags as per use in run.py)\nDropped --debug flag which is deprecated in Gunicorn 0.19 [2].\nRemoved unecessary documentation from wsgi.py (we already\n    suggest how to run Gunicorn in the README).\n\nMention GUNICORN_OPTS in run.py's docstring, which should have\n    happened in 2008330 (docker_registry.run: Restore GUNICORN_OPTS\n    functionality, 2014-07-18).\n1: http://docs.gunicorn.org/en/\u2026/news.html\n[2]: http://docs.gunicorn.org/en/19.0/news.html#core\n- You bump \u2018-w 1\u2019 \u2192 \u2018-w 10\u2019, but the default in run.py is 4 1.  I'd\n  stick with 4.\n. On Tue, Aug 12, 2014 at 01:12:37PM -0700, Kevin Littlejohn wrote:\n\n\n\nOk, believe that's done now - let me know if there's any other\nalterations needed.\n\nAh, I see you made some of those changes in 498491e ;).\n. On Tue, Aug 12, 2014 at 01:39:00PM -0700, Kevin Littlejohn wrote:\n\nI actually set the workers to 10 based on observation of running\n0.7.3 out of the box - but it appears that's been changed between\n0.7.3 and now, so my bad :) I've adjusted in my commit.\n\nIndeed, there seems to be some churn here.  The current code base has\nsuggestions for:\n- 9 1, set in 4963f286 (Fix #368, 2014-05-30)\n- 4 2, which dates back to a shift from 2 in 5a2664da (Generate\n  secret_key randomly in the Dockerfile, 2013-08-23).  The commit\n  message for 5a2664da does not motivate the change.\nI'd stick with 4.\n. On Tue, Aug 12, 2014 at 01:54:35PM -0700, W. Trevor King wrote:\n\n\n9 [1], set in 4963f286 (Fix #368, 2014-05-30)\n\n\nThis dates back to the initial Upstart script in 50017d4a (Upstart\nfile for Ubuntu and others, 2013-07-02), which does not explain the\nchoice of 9 over 4.\n. Ok, f0703e6 looks good to me :).\n. On Fri, Aug 29, 2014 at 11:16:30AM -0700, Olivier Gambier wrote:\n\nClosed #521.\n\nThere are still a few dotcloud references in the master branch:\n$ git describe --tags\n  0.8.0-42-g1bf6773\n  $ git grep -i dotcloud\n  .gitignore:.dotcloud\n  config/config_sample.yml:# See also: https://github.com/dotcloud/openstack-docker\n  config/config_sample.yml:    index_endpoint: https://indexstaging-docker.dotcloud.com\n  contrib/golang_impl/Dockerfile:run git clone https://github.com/dotcloud/docker-registry.git /docker-registry.git\n  contrib/golang_impl/README.md:    $ git clone https://github.com/dotcloud/docker-registry.git docker-registry.git\n  contrib/nginx.conf:  proxy_set_header Authorization  \"\"; # see https://github.com/dotcloud/docker-registry/issues/170\n  contrib/nginx.conf:  # required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\n  contrib/nginx_1-3-9.conf:  proxy_set_header Authorization  \"\"; # see https://github.com/dotcloud/docker-registry/issues/170\n  contrib/nginx_1-3-9.conf:  # required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\n  docker_registry/server/init.py:url = 'https://github.com/dotcloud/docker-registry'\n  docker_registry/server/init.py:download = 'https://github.com/dotcloud/docker-registry/archive/master.zip'\n  scripts/bandwidth_parser.py:    with open('/home/dotcloud/environment.json') as f:\n  scripts/bandwidth_parser.py:            'host': env['DOTCLOUD_REDIS_REDIS_HOST'],\n  scripts/bandwidth_parser.py:            'port': int(env['DOTCLOUD_REDIS_REDIS_PORT']),\n  scripts/bandwidth_parser.py:            'password': env['DOTCLOUD_REDIS_REDIS_PASSWORD'],\n  tests/workflow.py:`INDEX_ENDPOINT=https://indexstaging-docker.dotcloud.com \\\n  tests/workflow.py:        'https://registrystaging-docker.dotcloud.com')\n  tests/workflow.py:        'https://indexstaging-docker.dotcloud.com')\nDo we want to update those too?\n. On Wed, Aug 13, 2014 at 10:30:01PM -0700, qxo wrote:\n\nadd feature to cleanup docker registry storage:remove history images\n\nFor built-in cleanup (where you don't need to run a script by hand),\nsee #409.\n. On Fri, Aug 15, 2014 at 06:22:31PM -0700, Olivier Gambier wrote:\n\n\nDocumentation update\n\n\nIf you need extra requirements, like bugsnag, or new-relic specify them:\nShould have the comma moved:\nIf you need extra requirements, like bugsnag or new-relic, specify them:\nOther than that, this all looks reasonable to me.\n. On Fri, Aug 29, 2014 at 12:47:04AM -0700, Samuel Huang wrote:\n\nImportError: No module named MySQLdb\nI then went to this\nlink\nto try resolving the problem, but it looks like MySQL configuration\nwas not found.\n\nI'm not sure what you mean by \u201cMySQL configuration was not found\u201d, but\nthe main answer in the SO post explains that the problem is due to the\nlack of a MySQLdb package in the container.  The docker-registry\nrequirements only list sqlalchemy 1.  The SQLite driver is\nbuilt-into Python 2, but if you want to use MySQL you'll need to\ninstall python-mysqldb 3 on Debian-based systems like the\ncontainer's Ubuntu 4.\n. On Tue, Sep 16, 2014 at 10:43:32AM -0700, Samuel Huang wrote:\n\nI got it to work when I installed python-mysqldb to my Ubuntu machine.\n\nI'd thought that would be all you needed.\n\nI also ran this SQL on a separate MySQL container:\nsql\nCREATE DATABASE containers_db;\nCREATE TABLE containers_db.version(id INT PRIMARY KEY);\nCREATE TABLE containers_db.repository(id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(95) NOT NULL UNIQUE, description VARCHAR(100));\n\nDo you really need that?  SQLAlchemy should be creating the database\nand tables automatically if they're missing.\n. On Tue, Sep 16, 2014 at 12:07:52PM -0700, Samuel Huang wrote:\n\nApparently I encountered SQLAlchemy exceptions in the 0.8.0\nrelease. Was that functionality available during that time?\n\nAh, maybe you were bit by #555, which should land in the next release.\n. On Fri, Sep 19, 2014 at 09:56:21AM -0700, Raymond Barbiero wrote:\n\nIn this case, why would we ever check the local db? Upstream\nprovides us all available (mirrorable) repositories, descriptions,\nstar count, and boolean flags for 'official' or 'automated'.\n\nBut what if upstream is down?  I think the usual mirror approach is to\ntry and expire the local cache (ideally via ETag or some intelligent\ncache-control mechanism), and if it's expired, re-fetch from upstream.\nIf it's not expired (or if upstream is not available for some reason),\nserve the cached object.\nYou'd need additional logic if you wanted the mirror to be a primary\nstore for some objects, so it knew to never go upstream for those.\n. On Tue, Sep 02, 2014 at 09:43:09AM -0700, Joffrey F wrote:\n\n```\n\nJSON.parse('true')\ntrue\n```\n\nLooks like valid JSON to me =/\n\nJust because Python's JSON implementation can handle it doesn't mean\nit's valid JSON ;).  I think the root node of valid JSON should be\neither an object or a list, but the spec isn't particuarly clear 1.\n. On Tue, Sep 02, 2014 at 09:52:58AM -0700, W. Trevor King wrote:\n\nOn Tue, Sep 02, 2014 at 09:43:09AM -0700, Joffrey F wrote:\n\n```\n\nJSON.parse('true')\ntrue\n```\n\nLooks like valid JSON to me =/\n\nJust because Python's JSON implementation can handle it doesn't mean\nit's valid JSON ;).  I think the root node of valid JSON should be\neither an object or a list, but the spec isn't particuarly clear 1.\n\nAh, ECMAScript 5.1 is more precise, and it does allow a bare 'true'\n[1,2].\n. On Tue, Sep 09, 2014 at 05:54:03PM -0700, Olivier Gambier wrote:\n\nI'm changing _ping semantic for next release (see #563 and\nspecifically:\nhttps://github.com/docker/docker-registry/commit/0570610d9f1ef7acfed5e00d34b5fa608bd9edcf).\nPing will output {} in production usage, and a bunch of additional infos as an object if DEBUG=true \n\nHmm, changing semantics without an API version bump seems tricky.\nWill clients have to support both the old syntax and the new syntax?\nAdmittedly, the old syntax wasn't very interesting here ;).  The specs\nsuggest an empty response 1, maybe we can update those at the same\ntime to explain the expected empty-object response.  It would be nice\nto say that the implementation was free to add arbitrary fields to the\nobject, which clients were free to ignore.\n. On Tue, Sep 02, 2014 at 09:41:41AM -0700, Joffrey F wrote:\n\nFixes #486 \n\nHow does this fix 486?  If it does fix it, I'm surprised that\n\u2018store.list_directory(tag_path)\u2019 is still listing tags.  I still think\nthis sort of tag handling should be built into a generic Storage\nclass, since there's currently no filesystem-agnostic way I can think\nof to list tags.\n. On Thu, Sep 04, 2014 at 02:32:46AM -0700, Huayi Zhang wrote:\n\n\nMySQLdb will raise ProgrammingError if table doesn't\n   exists. Use engine.has_table to check whether table exists for\n   compatible\n\n\nUsing has_table is a good idea.  Do we need to use tablename\nthough?  I'd expect:\nif self._engine.has_table(Version):\nwould work as well, and leave the double-underscore extraction to\nSQLAlchemy.  I haven't tested this yet.\n\n\nVARCHAR requires a length on dialect mysql. String length limit\n   is according to Docker Hub's\n   limit\n\n\nIt looks like the limits there are 30 (for repo names) and 100 (for\ndesciptions).  Why did you go with 31 and 127?\nVery minor, but I also prefer using keyword arguments when possible,\nso folks who aren't familiar with the called API have explicit hints\nfor what arguments mean:\ndescription = sqlalchemy.Column(sqlalchemy.String(length=127))\nI just skipped the type_ keyword because of the ugly trailing\nunderscore ;) 1.\n. On Thu, Sep 04, 2014 at 10:21:12AM -0700, W. Trevor King wrote:\n\nThu, Sep 04, 2014 at 02:32:46AM -0700, Huayi Zhang:\n\n\nVARCHAR requires a length on dialect mysql. String length limit\n   is according to Docker Hub's\n   limit\n\n\nIt looks like the limits there are 30 (for repo names) and 100 (for\ndesciptions).  Why did you go with 31 and 127?\n\nAlso related: docker/docker#7609, which is shooting for <=64 character\nrepo names.\n. I just tested:\nself._engine.has_table(Version)\nand it doesn't work, so I'm fine with Version.tablename, although I'd prefer:\nself._engine.has_table(table_name=Version.tablename)\nOn Thu, Sep 04, 2014 at 09:08:22PM -0700, Huayi Zhang wrote:\n\nSo the name's limit should be 30 + 1 + 64\n\nWorks for me.\n\n./docker_registry/app.py:19:1: H302  import only modules.'from flask.ext.cors import CORS' does not import a module\n\nI'd leave this to a separate PR, ideally one that used a module import:\nimport flask.ext.cors\nsomewhere.use(flask.ext.cors.CORS)\nBut either way (fixing the CORS import or ignoring H302), it shouldn't\nbe bundled into this commit.\nI also don't see the point to having separate variables for the\nlengths.  Why not:\nname = sqlalchemy.Column(\n      sqlalchemy.String(length=30 + 1 + 64),  # namespace / respository\n      nullable=False, unique=True)\n  description = sqlalchemy.Column(\n      sqlalchemy.String(length=100))\n. On Thu, Sep 04, 2014 at 11:06:57PM -0700, Huayi Zhang wrote:\n\ntable_name isn't a keyword argument\u2026. It only works fine as a positional argument.                                                                                  \n\nIt doesn't have a default, but you can still pass it in via a keyword.\nFor example:\nimport sqlalchemy\nimport sqlalchemy.ext.declarative\nBase = sqlalchemy.ext.declarative.declarative_base()\nclass Version (Base):\n    __tablename__ = 'version'\n    id = sqlalchemy.Column(sqlalchemy.Integer, primary_key=True)\nengine = sqlalchemy.create_engine('sqlite:////tmp/testing.db')\nengine.has_table(table_name=Version.__tablename__)  # False\n. On Thu, Sep 04, 2014 at 11:06:57PM -0700, Huayi Zhang wrote:\n\ntable_name isn't a keyword argument\u2026. It only works fine as a positional argument.\n\nI doesn't have a default, but you can still pass it in via a keyword.\nFor example:\nimport sqlalchemy\n  import sqlalchemy.ext.declarative\n  Base = sqlalchemy.ext.declarative.declarative_base()\n  class Version (Base):\n      tablename = 'version'\n      id = sqlalchemy.Column(sqlalchemy.Integer, primary_key=True)\n  engine = sqlalchemy.create_engine('sqlite:////tmp/testing.db')\n  engine.has_table(table_name=Version.tablename)  # False\n. On Sun, Sep 07, 2014 at 12:51:32AM -0700, Huayi Zhang wrote:\n\n@wking duplicate comment?\n\nGitHub didn't seem to be posting emailed comments, so I repeated it\nwith the web UI.  Perhaps they just had a blockage in their mail queue\nthat they've since cleaned out.\n. Anyhow, 0b839378 looks good to me.  Thanks :).\n. On Mon, Sep 08, 2014 at 10:13:16AM -0700, Olivier Gambier wrote:\n\nMixed feelings here. Running a S3 backed registry without caching is\ngenerally a bad idea enough.\n\nI think warning is the appropriate level for \u201cwe think this is a\nproblem, but can continue anyway\u201d, which is the right state for folks\nusing a remote backend.  I think being slightly alarmist for folks\nusing local backends is ok, since it's not an error, but maybe:\nlogger.warn(\"\u2026 disabled (we suggest you enable this if your registry uses a remote backend).\")\n. On Tue, Sep 09, 2014 at 04:25:01PM -0700, Olivier Gambier wrote:\n\nIf not (like in standalone = True), there is no simple way to do\nthat for now (eg: there is no API endpoint that allows you to send a\ndescription).\n\nCan you give odds on such an API endpoint existing in the future?  Or\non the likelyhood of y'all accepting a patch that added such and\nendpoint to this repository, even if that endpoint wasn't part of the\nclosed-source index's API?\n. On Thu, Sep 11, 2014 at 10:17:43AM -0700, Olivier Gambier wrote:\n\n@wking if you would have time for a quick look at this one - I know\nit's not perfect, but IMO it would do some good on the general\nhealth - I'll rebase ASAP.\n\nThis one's big enough that a review should probably wait until I clock\nout from my day job ;).\n. On Fri, Sep 19, 2014 at 02:46:15PM -0700, Raymond Barbiero wrote:\n\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 10, in \n    from .extras import newrelic\n\nIt needs to be optional like it was in 0.8.1 1.\n. On Wed, Sep 10, 2014 at 10:05:14AM -0700, Olivier Gambier wrote:\n\nWe could import all existing installed modules with names like\ndocker_registry.addons.FOO.\nThat way, people could rely entirely on pip install without having\nto fiddle with the actual registry configuration. eg: if it's\ninstalled, it's gonna run.\n\nPersonally, I prefer the environment-variable/config approach, because\nit allows me to avoid using Setuptools' namespace packages 1.  It\nalso makes it easier to test a pre-built container by gradually\nenabling installed addons while you're debugging an incompatibility.\n. On Wed, Sep 10, 2014 at 10:34:58AM -0700, Olivier Gambier wrote:\n\nNow, it's also likely that you are going to bash into that container,\n\nWhere does Bash come in?\n$ docker pull someone/docker-registry\n  $ docker run -d --name -e IMPORT_MODULES=mod1,mod2,... docker-registry\nOr are you suggesting they run their debugging from an interactive\nsession, and add and remove packages from the namespace while they're\ntesting.  I think you'd have to do a full-blown 'pip uninstall \u2026' to\nactually get packages out of the namespace.\n\nThe reason I'm advocating against the config approach is that our\nconfig options are already (IMO) too complex, and I'm reluctant to\nadd more.\n\nYou're going to have to document this somewhere, and I think docs for\nthe environment-variable approach would be lower-complexity.  I don't\nfeel that strongly though.  Do we have any volunteers to work out an\nalternate namespace-package PR?\n. On Wed, Sep 10, 2014 at 11:44:53AM -0700, Olivier Gambier wrote:\n\nNow, thinking more about it, and either way, I don't feel good about\nhaving no formal extensions API. That just means various random\npieces of code will float around using various internal API and will\nbreak subtly (or not) with every new version...\n\nHeh, I have no idea what an extention API would look like.  I think we\nshould just let folks hook in however they like (when we load their\nmodule), and we should commit to keeping any non-underscored\nmethods/variables consistent unless we make a major version bump\n[1,2].  That's how generic Python package dependencies work, and I\nhaven't seen any problems with that workflow in my other projects.\n. On Wed, Sep 10, 2014 at 01:36:52PM -0700, Olivier Gambier wrote:\n\nNow, I'd be perfectly happy standardizing on top of signals as an\nAPI, like you suggested in the other ticket.\n\nIf going through and adding underscore prefixes to private stuff it\ntoo hard (it doesn't seem very hard to me ;), I'd also be fine with\njust posting docs saying \u201cdocker_registry.signals and\ndocker_registry.lib.config.load() are stable.  Stick to them if want\nforward-compatibility\u201d.  I don't see the point of passing stable stuff\nthrough to a target function, when it should be easy for users to\nimport what they need (and we'll hopefully have lots of stable goodies\nfor them to use).\n. On Wed, Sep 10, 2014 at 03:59:42PM -0700, Olivier Gambier wrote:\n\n@wking ok, agreed. (see? I'm not that much hard-headed ;-))\n\n;).  So are we ok with this PR, or do we want to document more of that\nbefore it lands?  I'm happy to write up our suggested workflow and API\n(or leave that to someone else), and make a PR against master or\n@ncdc's import_modules, if that makes this more appealing.\n. On Wed, Sep 10, 2014 at 04:28:51PM -0700, Olivier Gambier wrote:\n\nb. this is clear enough and simple to use (both for users and developers)\n\nI still prefer a 'modules_install' config to the namespace-package\napproach [1,2].  That's a small change relative to the rest of this\nthough, so I'm happy to has the rest out first ;).\nHere's some other low-hanging fruit, while I'm digesting the meaty\nbits:\n\nM docker_registry/lib/signals.py (2)\n\nWhy bind the blinker.Namespace() to namespace and _signals?  I don't\nthink the Namespace instance should be part of the public API.\nExtensions that need to create signals can setup their own namespace\n;).\n\nM docker_registry/run.py (1)\n\nI don't think we should be dropping --reload as part of this PR.  If\nyou have other motivation for dropping it, can that change happen in\nit's own PR?\n\nM docker_registry/wsgi.py (3)\n\nWhy boot after the app import?  If any extensions need things from\ndocker_registry.app (which seems unlikely), their imports should pull\nit in at the appropriate time.  Then docker_registry.extensions can\nboot itself.  Or we could just use the modules_install approach from\n565 ;).\n. On Wed, Sep 10, 2014 at 04:48:37PM -0700, W. Trevor King wrote:\n\nOn Wed, Sep 10, 2014 at 04:28:51PM -0700, Olivier Gambier wrote:\n\nb. this is clear enough and simple to use (both for users and\ndevelopers)\n\nI still prefer a 'modules_install' config to the namespace-package\napproach [1,2].  That's a small change relative to the rest of this\nthough, so I'm happy to has the rest out first ;).\n\nAs a counter to your \u201cwe are not going to ship every extension\u201d\nargument 1, one benefit to the config/environment-variable approach\nis that we can ship incompatible extensions.  For example, we can\nre-work the SQLAlchemy index implementation as an extension and have\nfolks use:\nIMPORT_MODULES=docker_registry.lib.index.db,\u2026 docker-registry\ninstead of the current:\nSEARCH_BACKEND=sqlalchemy docker-registry\nwe could also use IMPORT_MODULES to pull in the user's preferred\nrefcount cleaner (if any, #409).  That allows us to drop multiple\nindex or refcount implmentations into this repository (assuming they\nseem central enough to avoid livin in external repositories), and\nstill give users the flexibility to enable/disable them at runtime.  I\ndon't know how we'd bundle alternative refcounting implementations\notherwise, without a new config setting to select between them.\n. On Thu, Sep 11, 2014 at 10:11:43AM -0700, Olivier Gambier wrote:\n\nI'm new to blinker, and was under the impression that you need\naccess to the namespace to get the signals to connect() to. (sure,\nextensions that want to emit can do it without that).  Hence making\nnamespace public. I'll verify that you can connect from the root\nnamespace.\n\nOur existing connections just use the NamedSignal instance [1,2], so\nI'm pretty sure you don't need the namespace.\n. On Thu, Sep 11, 2014 at 11:42:39AM -0700, Olivier Gambier wrote:\n\nI guess we can sum-up the differences between the two suggested\nmodels as follow:\n\u2026\n\nThat hits everything I can think of.\n\n\nthe extension may decide to enable / disable itself (based on\n  config value)\n\n\nAh, I hadn't thought of this before.  That does let the\nnamespace-package approach avoid conflicts when there are incompatible\nextensions installed.\nI think you just have to weight those lists and make an executive\ndescision ;).  One of my main gripes with the namespace approach is\nthat it makes it harder to test development code (I have to install it\nafter each change before the Setuptools namespace stuff works, instead\nof just pointing PYTHONPATH at my development directory).  Maybe I'm\njust doing something wrong ;).\n. On Thu, Sep 11, 2014 at 01:27:13PM -0700, Olivier Gambier wrote:\n\nHave you tried python setup.py develop?\n\nNo, that would probably do the trick.\n. Why not put:\nboot()\nin docker_registry/extensions/factory.py instead of in\ndocker_registry/wsgi.py?  Basically, when would you want to import\nsomething from the extensions module but not load the extensions?\nAnd maybe call it load() to match docker_registry.lib.config?\nWhy the _config \u2192 conf rename?  It just seems like useless churn\n(although I agree that conf is nicer, I'd just make the change in a\nseparate commit).\nAnd I still think dropping --reload should be a separate PR 1.\nI guess I'll survive with namespaces instead of imports ;).\n. On Tue, Sep 16, 2014 at 02:34:41PM -0700, Olivier Gambier wrote:\n\n\nWhy the _config \u2192 conf rename?\n\nIt was previously code accessing the private global variable named\n_config. Now, it's code creating and returning a local variable.\n\nBut you still have a global _config in docker_registry/lib/config.py,\nyou just declare it later.\n\n\nWhy not put:  boot() in docker_registry/extensions/factory.py instead of in\n\nWell, this may be kind of \"philosophical\", but I don't like imports\nside-effects (and neither do linting tools?).  I agree that it's\nlikely more verbose that way.\nWhat do you think about the general \"side-effect\" question?\n\nIt's currently an import side-effect of docker_registry.wsgi.  I see\nno problem pushing the trigger deeper in the stack, since we've got\nthe same effect either way.\n. On Tue, Sep 16, 2014 at 02:40:47PM -0700, W. Trevor King wrote:\n\nTue, Sep 16, 2014 at 02:34:41PM -0700, Olivier Gambier:\n\nWhat do you think about the general \"side-effect\" question?\n\nIt's currently an import side-effect of docker_registry.wsgi.  I see\nno problem pushing the trigger deeper in the stack, since we've got\nthe same effect either way.\n\nThe best solution to this problem is to push this sort of activity\ninto docker_registry.app.init.  Then you don't have import-time side\neffects anywhere.\n. On Wed, Sep 17, 2014 at 06:43:11AM -0700, Andy Goldstein wrote:\n\ndo you think we need a standard/formal mechanism for\nenabling/disabling extensions via the config, with the check done in\nthe factory, instead of relying on each extension to implement the\ncheck itself?\n\nI think a centralized way of enabling/disabling extensions is #565 ;).\n. On Wed, Sep 17, 2014 at 09:20:58AM -0700, Andy Goldstein wrote:\n\n@wking true. Are we back to debating that vs this?\n\nI don't think so.  I just think the answer is going to be \"we don't\nwant a centralized enable/disable framework\".\n. On Wed, Sep 17, 2014 at 09:26:53AM -0700, Andy Goldstein wrote:\n\nSo it's more \"only install the extensions you want, and if you do\nwant to disable one, it either needs to support disabling itself, or\nyou just uninstall it\"\n\nThat's how this approach works, yes.\n. On Wed, Sep 17, 2014 at 12:45:40PM -0700, Andy Goldstein wrote:\n\n-            return Config()\n+            return Config('{}')\n\nIf an empty dict is a good default (and I think it is), maybe:\nclass Config(object):\n      \u2026\n      def init(self, config=None):\n          if config is None:\n              config = {}\n          if isinstance(config, compat.basestring):\n              try:\n                  self._config = yaml.load(config)\n              except Exception as e:\n                  raise exceptions.ConfigError(\n                      'Config is not valid yaml (%s): \\n%s' % (e, config))\n          else:\n              self._config = config\n. On Wed, Sep 17, 2014 at 01:12:33PM -0700, Olivier Gambier wrote:\n\n@wking re empty dict - not sure about that - do the tests run ok?\n\nNo idea, but if not I'd tweak the tests ;).  I can't see a situation\nwhere None would be more useful, and as @ncdc pointed out, sometimes\ndicts are more convenient 1.\n. On Mon, Sep 22, 2014 at 10:27:15AM -0700, Andy Goldstein wrote:\n\n\nI removed the sub-nodes part from config because entry points can\n  be any module you want - they don't have to be\n  docker_registry.extensions, and there's no way to force them to\n  be, so I couldn't think of a super easy way to limit the entry\n  point's config scope.\n\n\nDo we want to limit the scope anyway?  I think it's best to stick with\nPython's \u201cconsenting adults\u201d approach, and give extensions access to\nthe full config space.  As a trivial example, they may want to use\nloglevel to configure their own logger.\n. On Thu, Nov 13, 2014 at 12:53:03PM -0800, Olivier Gambier wrote:\n\nI'm ok removing these bits, but I want in place a mechanism to\nrefuse push from clients < 0.10 (exact http code to be devised -\n406? 417? none are technically exact here, but they are specific\nenough to distinguish from other errors).\n\nThe reason there's no correct response for \u201cyou have a User-Agent I\ndon't like\u201d is because we're not supposed to be switching on the\nUser-Agent value in the first place ;).  Ideally we'd have versioned\nthe API URLs so we didn't overload /v1/\u2026 with incompatible stuff, and\nold clients would just get a 404 once we dropped the old URLs.  If you\nwanted a nice client-side error message, we could always set an:\nX-Docker-Versions: 2.4, 3.1, ...\nheader listing the semantically versioned APIs that the registry did\nsupport, and the client could check that its version was supported\nwhenever it hit an endpoint (even if that endpoint claimed success,\nsee 1).\nIs there an alternate way to get the existing clients to report a\nclear error message (\u201cI'm too old for that registry\u201d)?  I'm not\nfamiliar with the client code, so maybe we do have an existing\nmechanism for that.\n. On Thu, Nov 13, 2014 at 12:53:03PM -0800, Olivier Gambier wrote:\n\nexact http code to be devised - 406? 417? none are technically exact\nhere, but they are specific enough to distinguish from other errors\n\nJoe Liversedge suggests \u201c412 Precondition Failed\u201d 1.\n. On Sat, Nov 15, 2014 at 08:54:36AM -0800, W. Trevor King wrote:\n\nX-Docker-Versions: 2.4, 3.1, ...\n\nAnd it looks like X-* headers are no longer recommended 1, so\nperhaps Docker-Versions: \u2026 1.  But a header returned from the\nregistry doesn't allow clients to pick which API version they'd like\nto use.  In that case, using a request version (in the URI, accepted\nMIME type, or a separate header), and having the server respond with\nthat version makes sense.  I'm not sure about endpoint deprecation\nthough.  Does the server have to remember all the previous endpoint\npatterns so it can 412 them (or whatever)?  I guess that's easy enough\nif you don't need to distinguish between \u201cthat's a version I don't\nsupport anymore\u201d and \u201cthat was never valid.\u201d\n. On Sat, Nov 15, 2014 at 02:40:03PM -0800, Olivier Gambier wrote:\n\nwe need a reasonably \"visible\" way to give feedback to older clients\npointing to that specific issue.\n\nSo what are our options?  I'm not sure what the client sees, or if the\nconnections are even from the command-line client (I think they're\nfrom the Docker daemon/engine).  I'm fine jumping through a per-client\nhoop here until we fix #734, but I'm not familiar enough with the\nclient/engine system to know what hoop I should jump through ;).\n. On Sat, Nov 15, 2014 at 03:24:39PM -0800, Olivier Gambier wrote:\n\nI believe for the targeted versions (<0.10) using an HTTP status\ncode that is not used already for other error conditions is the only\nway (+ informative error message in the body can't hurt).\n\nShould be done in ad9a8cc (which I've rebased onto the current master\ntoo).  I used docker_registry.toolkit.api_error for the error, so the\nbody will be:\n{'error': ''}\n. On Tue, Dec 02, 2014 at 02:28:58PM -0800, Olivier Gambier wrote:\n\n@wking something is not green it seems :)\n\nSloppy me.  Should be fixed now.\n. On Mon, Sep 15, 2014 at 11:04:59AM -0700, Matthew Fisher wrote:\n\nbackports.lzma released 0.0.4 a couple hours ago, which is\nbroken. We should pin to 0.0.2 or 0.0.3.\n\nOr just blacklist 0.0.4?\n. On Mon, Sep 15, 2014 at 11:16:27AM -0700, Matthew Fisher wrote:\n\n\nblacklist backports.lzma v0.0.4\n\n\nIt's probably worth explaining how 0.0.4 is broken in the commit\nmessage.\n. On Mon, Sep 15, 2014 at 11:30:53AM -0700, Matthew Fisher wrote:\n\nbetter? :smile: \n\nLooks good to me :)\n. On Tue, Sep 16, 2014 at 08:40:35AM -0700, Olivier Gambier wrote:\n\nDid we jump the gun a bit too fast? They removed it from pypi\nalready - faster than us releasing a new (branch) version from that\npatch.\n\nThe blacklist should still hold (assuming they push the next version\nas 0.0.5 and don't recycle the 0.0.4 name).\n\nNot too sure what the best course of action would be for that kind\nof scenario... (maybe hard pinning dependencies, hey wking? :-) )\n\nAny pre-built images would have already had 0.0.3 (or some such)\ninstalled.  You shouldn't be updating installed packages just because\nyou're launching a container.  If someone wants to build a new 0.8.1\nimage, I'm fine using strict == for the Python 2.7 and 2.6\ncompatibility requirements in setup.py, and relying on DEPS=loose to\nrelax those like everything else.\n. On Mon, Sep 15, 2014 at 02:44:12PM -0700, Igor wrote:\n\nUnfortunately when I try to extract a list of existing images via\nAPI - nothing comes back\n```\ncurl http://registry.host:5000/v1/search\n{\nnum_results: 0,\nquery: \"\",\nresults: [ ]\n}\n```\nAny ideas of what may be missing?\n\nMaybe you've turned it off via your config or environment variables.\nCan you post your (sanitized?) config and your launching command?\n. On Wed, Sep 17, 2014 at 10:19:43AM -0700, Andy Goldstein wrote:\n\nAdd image_id to tag_deleted signal payload.\n\nI know we use image_id in a lot of places, but I think we should\ngradually transition (and all new interfaces should start with)\n\u2018image\u2019 instead.  There's no ambiguity, because that's the only type\nof image reference we pass around.  It's an admittedly minor issue,\nbut it's much easier to start clean than it is to migrate to\ncleanliness later ;).\nOther than that, this looks good to me.\n. On Wed, Sep 17, 2014 at 10:30:51AM -0700, Andy Goldstein wrote:\n\n@wking updated - better?\n\nI'd use image (not image_id) as the local variable as well (image =\nstore.get_content\u2026).  This API-side stuff is not really my call\nthough, so it's probably better to wait for the go-ahead from a\nmaintainer before listening to me ;).\n. On Wed, Sep 17, 2014 at 10:36:06AM -0700, Andy Goldstein wrote:\n\nI didn't want to use it for the local variable to make it clear that\nthe data being retrieved from the store is supposed to be an image\nid, as opposed to metadata (json).\n\nSo use \u2018image\u2019 for the id (the name of the image) and \u2018metadata\u2019 for\nthe JSON (one part of the image).  That doesn't seem confusing to me.\nClearly the JSON is not the whole image, and if we ever get\ndeterministic image ids 1 the JSON won't even include the image id\n(to avoid a chicken-and-egg JSON \u2194 id cycle).\n. On Wed, Sep 17, 2014 at 10:42:36AM -0700, Andy Goldstein wrote:\n\nOk, updated.\n\n2d172a9 looks good to me :).\n. If Travis likes it, I like it ;).\n. On Fri, Sep 19, 2014 at 02:16:10PM -0700, Sean Boran wrote:\n\nWhen I set up another new host, can I copy of all containers and\nimages by copy scp'ing over /var/lib/docker/containers/, or is that\nnot allowed?\n\nIf you're using the same driver (e.g. aufs), I'd be surprised if\nstopping the initial host's Docker daemon, scp'ing over all of\n/var/lib/docker/, and firing up the new hot's Docker daemon didn't\nwork.  No promises though ;).\nA safer approach would be to use 'docker save' and 'docker load' to\nmove images (with 'docker commit' to convert any containers you need\nto move into images).  You're on your own for moving any volume stuff,\nbut I think there's a bug on docker/docker for making it easier to\nimport/export volume data.\n. On Fri, Sep 19, 2014 at 02:53:50PM -0700, Olivier Gambier wrote:\n\nI guess we may bring it to upstream?\n\nThe search index here was my first SQLAlchemy code, so I have no\nconnections upstream.  I imagine they'd welcome a pull-request (or at\nleast an issue report) though, since everyone loves those :).\n. On Fri, Sep 19, 2014 at 07:57:46AM -0700, sathlan wrote:\n\nBut from docker-registry-core available tests, I can't, unless I\nmissed something, test it.\nSo my question is how can I test _walk_storage (or database\nrecreation for that matter) from the driver's unittest ?\n\nIt's going to be hard to test the search index database from the\ndriver's unittest.  However, it should be possible to test the\nstorage-specific _walk_storage from a storage driver.  My preferred\napproach to this problem involves cleaning up the registry-facing\nstorage API (which would include iterating over available repository\nnames, currently handled by Index._walk_storage, get_tags 1, etc.).\nFor a summary of my position and a pointer to a WIP implementation,\nsee 2.\nWith this approach you can test your driver against the\nregistry-facing API (and those tests can be bundled with\ndocker-registry-core for easy application to any driver).  Then the\nsearch index can test itself against\ndocker_registry.drivers.dumb.Storage, and all will be well ;).\n. This looks good to me, but I'm curious about why you bumped the\nminimum backports.lzma from 0.0.2 to 0.0.3.  Are we no longer\ncompatible with 0.0.2?  Or was there an unacceptable bug that was\nfixed in 0.0.3?\n. On Sat, Sep 20, 2014 at 09:34:06PM -0700, Olivier Gambier wrote:\n\nI kind of assumed \"the latest the greatest\" - and given \"deps loose\" gives you >=0,<1...\n\nHey, if everything in \u2018>=0,<1\u2019 gets us the API that we're using, that\nworks for me ;).  And (barring dependency conflicts from other\npackages), it will give us the latest and greatest.\n. For 2a21552:\n- \u201cpython packages\u201d \u2192 \u201cPython packages\u201d.\n- \u201creview this extension configuration settings, and edit your\n  configuration file accordingly, then run your registry\u201d \u2192\n  \u201creview this extension's configuration settings, edit your\n  configuration file accordingly, and run your registry\u201d.\n- Capitalization for list entries is inconsistent (\u201c2. Create\u201d\n  and \u201c3. create\u201d).  Personally, I prefer capitalizing.\n- \u201c[some name]\u201d should be in backticks to avoid getting\n  interpreted as a Markdown link (and because it's an excerpt\n  from a preformatted block).  Same for \u201c[your.module]\u201d.\n- \u201cexplicitely\u201d \u2192 \u201cexplicitly\u201d.\n- \u201cdocker_registry API\u201d \u2192 \u201cdocker-registry API\u201d.\n- \u201cprefered\u201d \u2192 \u201cpreferred\u201d\n- I don't see the point of using Setuptools extensions over standard\n  Python imp orts ;).\n- I still think the boot() call should be either in the app-launching\n  function or inside the factory module.\n. 975363a is still missing my \u201c\u2026, and edit your configuration file\naccordingly, then run\u201d \u2192 \u201c\u2026, edit your configuration file accordingly,\nand run\u201d suggestion (although you're certainly welcome to disagree\nwith my suggestions ;).  Similarly, \u201c4. run\u201d is still not capitalized.\nI admit that I don't understand the point of Setuptools' entry-point\ngroups 1, but I don't think it's worth using Setuptools to save\nseven lines 2.\n. On Tue, Sep 23, 2014 at 12:14:50PM -0700, Andy Goldstein wrote:\n\nAlso, I think we have/had 2 extension loading mechanisms on the table:\n1. automatically load extensions if they exist\n   1. package namespaces\n   2. setuptools entrypoints\n2. manually explicitly specify which extensions to load\n   1. import_modules\n\nAh, I hadn't noticed the setup.py bit in the scaffolding docs that\nmakes the iter_entry_points approach automatic.  In that case, I guess\nwe're stuck with Setuptools if manually listing modules is too onerous\n;).\n. On Tue, Sep 23, 2014 at 12:15:42PM -0700, Andy Goldstein wrote:\n\nLatest commit bba8552 should have the remaining 2 readme updates in it.\n\nThis README looks good to me.  The only remaining issues I have are\nnot liking Setuptools and the location of the boot() call [1,2].  Feel\nfree to override me on these if you're comfortable with your position.\n. On Tue, Sep 23, 2014 at 02:44:16PM -0700, Olivier Gambier wrote:\n\n@wking thanks a lot for your input, time and reviews in all this.\n\nNo problem.\n\nI like what's happening here with the registry, and I like the\ninteractions.\n\nMe too :).\n\nI would like to merge this as-is (including these two bits you don't\nquite like).\n\nSure.  Rational people can disagree, and the buck stops with you ;).\n. On Fri, Sep 26, 2014 at 08:05:02AM -0700, Andy Goldstein wrote:\n\nNow that the registry supports custom extensions, what are your\nthoughts about creating a new GitHub repository to house extensions,\nsuch as docker/docker-registry-extensions? I'd be happy to convert\n@wking's image pruning PR into an extension.\n\nIf anything isn't core enough to land in this repository, I think it\nshould get it's own repository, and not be lumped into a second-tier\nextensions repository.  Maintenance for that kind of thing is tricky\n(github/hubot-scripts#1113).\n. On Fri, Sep 26, 2014 at 08:33:56AM -0700, Andy Goldstein wrote:\n\nI am thinking the extensions repository could be an easy way to find\na variety of extensions that users may want to run, but that aren't\nrequired in the core registry.\n\nThe Hubot folks have attacked the discoverability problem with a\nhubot-scripts organization 1, but you could also just have a\ndocker-registry-extensions repository which consists only of a README\nwith an alphabetized list of known extensions, and a one-sentence\ndescription and link for each one.  Then the index maintainers just\nhave to manage the index, and they can leave maintaining the\nextensions up to whoever wrote the extension (or their chosen\nsuccessors).\n. I haven't actually gone over the implementation yet, but I'd drop the\n\u201c[refcount]\u201d prefix from the logger calls.  If we want to prefix\nlogged information with module names (and I think we do) we should add\n\u201c%(name)s\u201d to the logging.Formatter in docker_registry.wsgi.  But that\nwould be a separate PR.\n. On Tue, Dec 02, 2014 at 02:30:34PM -0800, Olivier Gambier wrote:\n\n@wking @ncdc where do we stand with this?\n\nDo we care about supporting this for the Python registry?  Folks have\nsurvived so far without it built in, and while I think we're still\nproviding maintenance for the Python implementation, I don't see the\npoint in investing time in adding new features there.\n. On Tue, Dec 02, 2014 at 05:19:54PM -0800, Matt Hughes wrote:\n\nIs there a timeline for the go rewrite?\n\nAlpha is due 2014-11-24, and the first RC is due 2015-01-12 1.\n. On Mon, Oct 06, 2014 at 06:49:31AM -0700, Andy Goldstein wrote:\n\n\nAdd logger name to log format\n\n\nLooks fine to me, although I would have probably gone with:\n'%(asctime)s %(name)s %(levelname)s: %(message)s'\n. On Mon, Oct 06, 2014 at 08:39:49AM -0700, Andy Goldstein wrote:\n\nDo you want me to switch it now or wait for others to chime in?\n\nIt's not my call, so you should probably wait ;)\n. On Mon, Oct 06, 2014 at 10:22:33AM -0700, Matthew Fisher wrote:\n\n'format': '%(levelname)s %(asctime)s %(module)s %(process)d %(thread)d %(message)s'\n\nI'm happy to reuse an existing standard.  My suggestion was just\nbecause I'd rather sort first by name (i.e. module) and then by level.\nI've also not seen brackets in log messages, except for marking out\nprocess ids in syslog messages 1.\n. On Thu, Oct 30, 2014 at 08:07:08AM -0700, Andy Goldstein wrote:\n\nI've found that if we specify --log-config when running gunicorn,\nit will control the log format for everything, and the format set in\napp.py is ignored.\n\nAh, nice.  In that case, pick whatever default you like ;).\n. On Mon, Oct 06, 2014 at 12:26:40PM -0700, Adam Ever-Hadani wrote:\n\n@dmp42  newb question - whats best way to test run this?\n\nI've been using something like:\n$ virtualenv --python=python2.7 --no-site-packages .virtualenv\n  $ source .virtualenv/bin/activate\n  $ pip install -e depends/docker-registry-core/\n  $ pip install -e .\n  $ DOCKER_REGISTRY_CONFIG=${PWD}/config/config_sample.yml python -c 'import docker_registry.run as r; r.run_gunicorn()'\n. On Wed, Oct 08, 2014 at 09:16:02PM -0700, Matthew Fisher wrote:\n\nHowever, I got this error when running from the bleeding edge:\nhttps://gist.github.com/bacongobbler/ff34cd500c0689cbc1c1\n\nThe:\nUNIQUE constraint failed: version.id u'INSERT INTO version\nLooks like #518.  We never found a way to reproduce that, but perhaps\nyou should post your procedure there?\n. On Fri, Oct 10, 2014 at 03:16:28PM -0700, Olivier Gambier wrote:\n\nwild guess: could this be a race condition between different workers\ntrying to initialize the same db thing?\n\nOff topic, move to #518 ;).\n. On Tue, Oct 07, 2014 at 12:14:09PM -0700, Olivier Gambier wrote:\n\nYes. The new registry will be developed in go instead of python.\nThe reasons for that are:\n- reduce the \"gap\" inside the community and build on a common\n  technology, using common libraries (libtrust and @dmcgowan, I'm\n  looking at you)\n- thus easing integration test with the rest of the platform, etc\n\nI think this would make more sense if there was going to be more\nsharing of code between the registry and the daemon/client.  However,\nI don't think we need any brains in the registry, since I see\nprovanance as a contract between the builder and signer, and\ncompletely separate from the registry [1,2].\n\n\nstart with a clean slate\n\n\nThis is a benefit?\n\n\nbet on a language that has a good concurrency model from the get\n  go - no pun -\n  https://golang.org/doc/effective_go.html#concurrency\n\n\nIf this means we get transactional backends for free, then great :).\nOtherwise, I think the current implementation scales well (just add as\nmany threads as you need), since there's no need to communicate\nbetween threads.\n\n\nwhile python is a robust, mature and well established technology\n  (stack), it really starts smelling funny in a number of places -\n  some young blood / fresh air will do us all good :)\n\n\nWhere are the funny smells?\n. On Tue, Oct 07, 2014 at 02:16:58PM -0700, Olivier Gambier wrote:\n\n\nI think this would make more sense if there was going to be more\nsharing of code between the registry and the daemon/client.\nHowever, I don't think we need any brains in the registry, since I\nsee provanance as a contract between the builder and signer, and\ncompletely separate from the registry [1,2].\n\nTarsum verification will have to occur also on the registry\nside. And I would expect the registry to verify images signatures as\nwell.\n\nWhy?  It's going to have to happen in local Docker daemons after\ndownloads, so I don't see much benefit in checking on the registry\nside too.  And with clients doing verification, I doubt anyone will\nbother uploading broken signatures to the registry.\n\nCommon tooling is a plus as well. Common development guidelines,\netc.\n\nThis makes sense, although I don't really see the need for much more\ndevelopment since the existing registry code is fairly stable.\n\n\nOtherwise, I think the current implementation scales well (just\nadd as many threads as you need)\n\nIt does scale.\nNow, what about things breaking in not so subtle ways because of\nlibevent minor version differences?\nOr the need to call \"magical\" monkey patching \"before\" any other\ncode, that doesn't always seem to fully do the job?\n\nCan you links to the issues where these came up?  gevent is not my\nfavorite package, but I'd probably just pick a different Gunicorn\nworker (e.g. gaiohttp 1) instead of rewriting this whole project\nfrom scratch ;).\n\n\nsince there's no need to communicate between threads.\n\nThere is a need to communicate between threads, right now, or be\nbitten by eventual consistency (which is one of the reasons we use\nredis for that). But then one could argue this will disappear with\nthe new drivers.\n\n\u201cBut when we rewrite it, we'll do a better job\u201d is less convincing to\nme than \u201cbut when we use $TOOL, $PROBLEM will no longer be an issue\nbecause of $FEATURE [$LINK]\u201d ;).\n\n\nWhere are the funny smells?\n- namespaces\n- packaging\n- gevent\n\n\nI'm happy to drop our current Setuptools stuff (and gevent, see above)\n;).  What sort of packaging problems are you talking about?  I'm still\nnot sold on the whole docker-registry-core pull-out.  Anyhow, I think\nthese are things best solved incrementally.\n. On Tue, Oct 07, 2014 at 03:21:09PM -0700, Olivier Gambier wrote:\n\n\nAnd with clients doing verification, I doubt anyone will bother\nuploading broken signatures to the registry.\n\nThey will.\nAnd that will result in a DOS, at best (content-addressability comes\nat a cost).\n\nYou can't handle this the same way that you already presumably handle\nfolks uploading other objectionable content?\n\nAnd oh, ultimately, I would love to see multiple diverse\nimplementations of the V2 protocol (and this is what should be cool\nwith it, in allowing to do that more easily).\n\nI was coming from more of a \u201cI don't have time to learn idiomatic Go\nor work on untangling Gentoo's Go packaging\u201d perspective, which is\nprobably not compatible with \u201cI'm going to chip in to an unfunded\nPython v2 registry without help from Docker, Inc. folks\u201d ;).\n\nI also expect our \"design (a bit) more\" and \"think (a bit) before\"\napproach to prove more fruitful and also easier to maintain than our\nprevious organisation.\n\nBut I can still help with the design and thinking without needing to\nfigure out a sane Go development system locally ;).\n. On Tue, Oct 07, 2014 at 09:21:44PM -0700, Matthew Fisher wrote:\n\nAll I can say is \"specs, specs, specs\". For my own needs, I'd like a\nway to contribute to the project in a way that would support this\nchange. To facilitate that, we need a document so that external\nmaintainers (i.e. contributors to docker/docker-registry who are not\naffiliated with Docker Inc.) like myself can contribute in some way,\nwhether that be with the core API, the storage drivers, etc.\n\n+1.  In fact, I'd be tempted to put the specs in a separate repository\nfrom the implementation, to encourage folks to not get bogged down in\na particular implementation.  Of course, linking out to an external\nimplementation would be fine.\n. On Thu, Oct 09, 2014 at 12:11:42AM -0700, Matthew Fisher wrote:\n\nplease feel free to contact me on IRC/email and get things\nstarted. I'm online from 8-4PST :)\n\nI'll likely be around for those hours as well, but I personally prefer\nplanning this sort of thing via something less synchronous, since I\nusually have better ideas after sleeping on something overnight than I\ndo five minutes after prompting ;).\n. On Tue, Oct 21, 2014 at 07:06:37PM -0700, Bo Shi wrote:\n\nAs part of this investigation, I discovered that \"docker pull\nfoo/bar:sometag\" will incur a hit to the API endpoint that lists all\ntags for 'foo/bar'.\n\nI haven't looked at the client-side code (at least recently enough to\nremember), but I'd expect you'd need to do this for the new alias\ndetection (docker/docker#8141).  Unless you had a separate endpoint\nfor \"give me all aliases for $NAMESPACE/$REPO:$TAG\".  We could\nactually support something like that efficiently if we had something\nlike refcount tracking (#606, #409), since I was recording the\nreferring ([namespace, repository, tag], descendant_id) entry for each\nancestor image id [1,2].  We'd just have to iterate through referrers\nand return tags matching $NAMESPACE/$REPO where descendant_id ==\nimage_id.  With my new (proposed) atomic/streaming storage separation\n3, accessing the referrers is probably a single hit to your atomic\nstorage (to the image-references entry for the requested image).\n. On Thu, Nov 06, 2014 at 10:48:58AM -0800, Stephen Day wrote:\n\nThe new registry will likely require some sort of coordination layer\nto mitigate that.\n\nI'd just keep the mutable (i.e. not content-addressable) stuff in a\nstorage backend that does support transactions 1.  Then you can\noffload the coordination to that storage engine (e.g. let Redis handle\nthe transaction implementation and just use MULTI/EXEC/DISCARD).  Then\nyou get don't need to figure out a way to handle transaction locking\nbetween multiple registries that are sharing the same backing storage.\n. On Wed, Oct 08, 2014 at 08:51:39PM -0700, Matthew Fisher wrote:\n\nAs for the case with resumable push, I'm not too sure on how it can\nbe performed in the context of a file store. A filestore is simply a\nglorified key/value store which reads and writes to an external\nfilesystem. This is probably the registry's territory, as it would\nknow which bits of data still need to be written and which bits have\nalready been pushed. There could be a way to accomplish this. Maybe\nby doing a server-side check to see what bits have been pushed, and\nresume pushing the ones that have not already been pushed?\n\nFor resumable-push, I think the storage needs to be more than a\nglorified key/value store.  You also need stat (or some analog) to see\nhow big the already-stored object is.  Then you can return that size\nto the client, and the client can start pushing bytes after that\noffset.  The backend then needs to be able to append the new bytes to\nthe original, half-finished push.  So I'd append the API to:\ntype Filestore interface {\n  Get(key string) (string, error)\n  Put(key string, data string) error\n  List(delimiter string) []string\n    GetSize(path string) int\n    GetCurrentSize(path string) int\n  WriteStream(path string, data chan string, int size, int offset=0) error\n  ReadStream(path string, data chan string) error\n}\nWhere GetSize returns the total allocated size and GetCurrentSize\nreturns the amount actually stored on disk (so GetSize() ==\nGetCurrentSize() means the file is stored in its entirety).  I see no\nneed to use the atomic Get/Put interface and the streaming\nGetSize/GetCurrentSize/WriteStream/ReadStream interfaces on the same\npaths, and would be happy with Filestore subclassing both an\nAtomicStore interface and a StreamingStore interface (although I don't\nknow how to write that in Go).\nI also dropped the Move because I we don't need that for\ncontent-addressable storage.\n. On Thu, Oct 09, 2014 at 08:26:14AM -0700, W. Trevor King wrote:\n\nwould be happy with Filestore subclassing both an AtomicStore\ninterface and a StreamingStore interface (although I don't know how\nto write that in Go).\n\nActually, I'd be happy with Store subclassing (or providing, or\nwhatever) both an AtomicStore interface and a StreamingStore interface\n;).  We shouldn't care if the backend happens to use files or not.\nSplitting it up like this would let us use Redis (or whatever) as the\nAtomicStore, and S3 (or local storage or whatever) for the\nStreamingStore.\n. On Thu, Oct 09, 2014 at 08:26:14AM -0700, W. Trevor King wrote:\n\ntype Filestore interface {\n  Get(key string) (string, error)\n  Put(key string, data string) error\n  List(delimiter string) []string\n  GetSize(path string) int\n  GetCurrentSize(path string) int\n  WriteStream(path string, data chan string, int size, int offset=0) error\n  ReadStream(path string, data chan string) error\n}\n\nAnd if we're serious about the content-addressable part, we should\nprobably have:\nPut(data string) string\n  WriteStream(data chan string, int size) string\n  AppendStream(key string, data string) string\nand have the storage return an arbitrary key for each stored object\n(e.g. by hashing it).  In the case of a partial write, WriteStream\nwould return a temporary key for use by AppendStream, and you'd only\nget the final key from WriteStream/AppendStream once you'd finished\nwriting the object.\n. On Thu, Oct 09, 2014 at 08:39:07AM -0700, Matthew Fisher wrote:\n\n\nwould be happy with Filestore subclassing both an AtomicStore\ninterface and a StreamingStore interface (although I don't know\nhow to write that in Go).\n\nThere certainly are ways to do this:\n\nActually, I'm fine with just having my AtomicStore and StreamingStore\nconfigured separately.  I don't see much gain to wrapping them up in a\nsingle Store class, and it would be nice to be able to mix and match\nthe backends for both types independently.\n. On Tue, Oct 14, 2014 at 11:36:28AM -0700, Brian Bland wrote:\n\nI'm proposing that we add an offset parameter to the ReadStream\nmethod for resumable downloads if we're already planning on\nsupporting resumable uploads.\n\nAnother alternative would be to set these up as seek-able files.\n. On Wed, Oct 08, 2014 at 08:51:39PM -0700, Matthew Fisher wrote:\n\nList(delimiter string) []string\n\nThe Docker hub had 14000 \u201capplications\u201d ($namespace/$repository?) in\nJune 1.  I don't know how many layers that translates to, or how\nmany namespaces there were, but I'm concerned about performance with a\nflat List endpoint 2.  I'd prefer:\nCount(prefix string) int\n  List(prefix string, size int, from int) []string\nto let you see how many objects matched a (possibly empty) prefix and\niterate through them without making massive requests.  I borrowed\nsize/from from the Elasticsearch API 3, but I don't really care what\nthe strings are.\nAnyhow, something like this would allow the (streaming) storage driver\nto shard or fan-out transparently for efficient lookups and listings,\nwhile still preserving a flat namespace for object lookups.\n. On Wed, Oct 29, 2014 at 07:20:26AM -0700, W. Trevor King wrote:\n\nCount(prefix string) int\n  List(prefix string, size int, from int) []string\n\nRedis lets you do this for lists with 1:\nLRANGE key start stop \n(but not for sets, where SMEMBERS returns all the values 2).  I\nexpect most of the metadata-side requests will use a stand-alone\nsearch engine (connected by event webhooks [3,4]?) for listings like\nthis, but a Redis implementation of atomic storage could use lists for\nanything where listing would be important (e.g. like we currently list\na repository's tags, #614).\n. From #docker-dev today:\n11:00 < dmp42> 5. would love some gcs comments regarding the new\n    driver API before we freeze\nI'm not sure how solid this freeze is intended to be, but I'd love to\nsee semantic versioning for the API, with a version endpoint:\nversion() \u2192 [major, minor, patch]\nso the registry could ask what version of the API the driver was\nimplementing and decide if it was compatible.  That would also give us\na clean way to transition to new APIs while maintaining some backwards\ncompatibility, if we ever need to make a backwards-incompatible bump.\n. On Mon, Nov 03, 2014 at 12:40:50PM -0800, Olivier Gambier wrote:\n\n(understood that it's the driver interface version, not the main\nregistry version).\n\nYeah, and I think the driver API should be versioned independently of\nthe registry implementation.\n@BrianBland, good point about dropping the patch from the API version.\nI don't think we need a warning if the driver uses and older minor\nversion, but an info-level notice would be nice.  Here's how I'd\nhandle differences:\n- Storage versions match: no message:\n- Major versions match, but minor versions differ: log an info-level\n  notice.\n- Major versions differ: log an error-level notice, and:\n  - if the registry has appropriate compatibility code: carry on.\n  - otherwise: exit with an error.\nThat works whether the driver or the registry has the newer API, and\nsupports registry-side cross-major compatibility code.\nIt doesn't allow for negotiation though, which we'd need if we\nwanted to support driver-side compatibility code (e.g. if the driver\nhas endpoints for both the 1.x and 2.x API versions).  I don't think\nwe'd have much need for that though.\n. On Mon, Nov 03, 2014 at 01:06:08PM -0800, W. Trevor King wrote:\n\n\nMajor versions differ: log an error-level notice, and:\nif the registry has appropriate compatibility code: carry on.\notherwise: exit with an error.\n\n\nOops, this should be:\n- Major versions differ:\n  - if the registry has appropriate compatibility code: log a\n    warning-level notice and carry on.\n  - otherwise: log an error-level notice and exit with an error.\n. On Mon, Nov 03, 2014 at 01:15:32PM -0800, Brian Bland wrote:\n\nIf the registry has a newer minor version than the storage driver,\nthis could mean that the registry added a new method to the driver\napi that the storage driver does not respect, as per semantic\nversioning. We would either have to keep a mapping of which versions\nsupport which methods and operate differently depending on the\ndriver version, or we could just reject the storage driver for being\nout of date.\n\nAh, right.  I think I'd turned this around in my head, and had\nmajor-version backwards compatibility applying to the registry, when\nit's actually for the API/storage-driver.  So we should be dying with\nan error for any case where the registry version is greater than the\nstorage version, unless the registry has appropriate\nbackwards-compatibility code.\n. On Mon, Nov 03, 2014 at 01:06:08PM -0800, W. Trevor King wrote:\n\nIt doesn't allow the for negotiation though, which we'd need if we\nwanted to support driver-side compatibility code (e.g. if the driver\nhas endpoints for both the 1.x and 2.x API versions).  I don't think\nwe'd have much need for that though.\n\nIt also doesn't handle API bifurcation, e.g. if we want to use\nseparate atomic and streaming storage APIs 1 down the line.  That's\nnot a problem if we never split the API, or if splitting is infrequent\nenough that we don't mind an awkward, manual handoff when we do.  If\nwe did want to detect and handle API bifurcation, we'd want:\nversion() \u2192 {api_name: [major, minor], \u2026}\nwith a dictionary of supported APIs and their versions.  If we extend\nthat slightly to:\nversion() \u2192 {api_name: [[major, minor], \u2026], \u2026}\nit would also allow us to have drivers that simultaneously supported\nseveral major versions of the same API 2, they'd just return\nsomething like:\n{'storage': [[1, 8], [2, 3]]}\n. On Mon, Nov 03, 2014 at 03:39:10PM -0800, Brian Bland wrote:\n\nI'm not sure how a multi-version storage driver would function. How\nwould we handle changing the behavior, parameters, or return values\nof an API method without having to rename the method each time or\nproviding a version parameter with each API call? I'm not a fan of\neither of these approaches...\n\nI don't see major-version bumps as a frequent feature, and the web\nendpoints are already namespaced like this (/v1/_ping), so I'm fine\nwith new function names for anything that would break backwards\ncompatibility.\n. On Thu, Oct 09, 2014 at 02:16:03PM -0700, unclejack wrote:\n\nchange cmd exec docker-registry to CMD [\"docker-registry\"]\n\nI'm still not clear on how Docker distinguishes between:\nCMD \"executable\",\"param1\",\"param2\"\nand:\nCMD \"param1\",\"param2\"\nAt least:\nCMD exec docker-registry\nis unambiguous and explicit.\n. On Thu, Oct 09, 2014 at 02:43:11PM -0700, Olivier Gambier wrote:\n\n\nwhat about 10AMPST on mondays\nwhat about next monday topic: reach consensus on the auth architecture\n\n\nSounds good, although I think we've already reached a consensus ;).  I\nthink you should just hire @bacongobbler to wrap the Nginx config up\ninto a separate image ;).\n. On Fri, Oct 10, 2014 at 01:01:03PM -0700, Johan Euphrosine wrote:\n\nI also also wondering how multiple signatures would be implemented.\nIn particular if they will behave like envelopes\n\nMy understanding was that the're just a flat list, with no nesting.\nSee the \"signatures\" list in the example metadata in the\ndocker/docker#8093 proposal.  There's a later comment about\nmultiple-sigs still being a work-in-progress though 1.\n\nCorp D sign (Corp B sign (Corp A signature +  layer d34db33f))\n\nI don't see why you'd want something like this anyway.  If the user\nonly trusts Corp D, then:\nCorp D signs layer d34db33f with metadata c00fee as foo/bar:1.2.3\nis all you need.  It looks like GnuPG sigs are in the distant future\n2, and in that case you can have more useful webs\nlike 3:\n- Corp D trusts Corp B\n- Corp B signs layer d34db33f with metadata c00fee as foo/bar:1.2.3\n- User F trusts Corp D, and accepts at least two steps, so she can\n  validate layer d34db33f with metadata c00fee as foo/bar:1.2.3\n  using the chain F \u2192 D \u2192 B \u2192 layer.\n. On Fri, Oct 10, 2014 at 01:30:38PM -0700, Johan Euphrosine wrote:\n\n\nThere's a later comment about multiple-sigs still being a\nwork-in-progress though [1].\n\nI'm curious how the revocation would work?\n\nYou can always revoke your whole key ;).  Otherwise, you could\npresumably pull your sig off the registered image, or push a signed\nrevocation note.  I don't see how that protects folks from an attacker\npresenting a version of the image with just your original sig though,\nunless they're polling your registry to get an authoritative list of\nstill-applied sigs whenever they try to verify and want to protect\nagainst that sort of partial-info attack.\n. On Fri, Oct 10, 2014 at 02:30:13PM -0700, Johan Euphrosine wrote:\n\n\nOtherwise, you could presumably pull your sig off the registered image\n\nOh I didn't really think of the signatures array as mutable as\nthis. I thought you would only append extra sig to it.\n\nI don't know if that API has been worked out yet, but I see no reason\nwhy removing your own sig should be illegal.\n. On Fri, Oct 10, 2014 at 02:40:13PM -0700, Johan Euphrosine wrote:\n\nI liked the idea of having a guaranteed audit trail of signature event.\n\nThat sounds good to me too.  So maybe add a \"revoked-signature\" list with:\n{\n    \"reason\": \"explain why you revoked it\",\n    \"revoked-signature\": ,\n    \"signatures\": [list of signatures for this revocation]\n  }\nalthough by this point I think we're getting off topic here and should\nmove this sig discussion to docker/docker#8093.\n. On Thu, Oct 09, 2014 at 02:43:11PM -0700, Olivier Gambier wrote:\n\n\nwhat about 10AMPST on mondays\n\n\nDo we want summaries of these meetings (like 1)?  The first one runs\nfrom 10:02am 2 through 11:53am 3.  My takeaway was that we're\npretty happy with @dmp42's summary of use cases 4, but that there's\nstill not much consensus about the default auth infrastructure for the\nDocker-run hub and copycats (cases 2 and 4).  Front runners appear to\nbe:\n- OAuth/JWT 5\n- SASL/SAML 6\nwith the breakdown seeming to fall primarily on beauty (yay JSON) vs\nmarket penetration (lots of existing SAML).\nThere was also concern about the maturity of Go's HTTP scaffolding and\nhow easy it will be to bake-in the default auth implementation (cases\n2 and 4).\nayoung agreed with my preference to offload all of this to a\nreverse-proxy, but @dmp42 is still resisting due to our large images\n(but see my byte ranges comment in #623).\nI imagine this summary is biased by my pro-proxy viewpoint (I'm happy\nto include the hub's auth via Nginx's http_auth_request, #623), so I'd\nappreciate someone in favor of registry-native auth double checking\nthis to keep me honest ;).\n. On Mon, Oct 20, 2014 at 09:07:10AM -0700, Olivier Gambier wrote:\n\ndocker-dev, 10AM this morning, like every monday! Get your seat and popcorn ready!\n\nMy take on today's discussion is here 1, since we talked exclusively\nabout project structure and governance.\n. On Tue, Oct 28, 2014 at 01:48:25PM -0700, Olivier Gambier wrote:\n\n@wking and others: do you feel we should have a formal / defined way\nto store these discussions minutes?\n\nI do, in the spirit of the Harvey Birdman Rule from copyleft-next 1.  \u201cOh, we\ndiscussed that on IRC\u201d isn't very useful if you don't have logs or a\nsummary to point to, and without useful arguments about a spec or\nimplementation, it's hard to revise without re-hashing the earlier\narguments.\n\nMaybe something very simple like a open-design/chats folder with\ntimestamped summaries?\n\nI doubt most folks will care enough about these for it to be worth\ncluttering this repository.  I'd suggest a separate repository like\ndocker/irc-minutes (and maybe it should be irc-minutes?).\n\nWho would feel brave enough to handle that \"I'm writing the\nsummaries\" position? Seems like @wking has done it beautifully these\npast weeks :)\n\nI'd just leave it to folks to volunteer summaries for anything they\nwant summarized (since I'm not in a position to require summaries of\n\u201csubstantive development discussions\u201d in the HBR lingo by at least one\nof the participants).  I was involved in the first two discussions,\nbut may not be involved in future discussions.  Did we even have one\nthis week?\nI'd also suggest somewhere where you can sketch out an agenda for\nupcoming discussions, since an agenda-less discussion is even more\nlikely to run in circles than an agenda-ed discussion ;).\n. On Thu, Oct 09, 2014 at 04:35:08PM -0700, Olivier Gambier wrote:\n\n\nI want a simple private registry to test, or just to enjoy better\n   \"local-network\" speed, and I don't care about authentication\n\n\nThis is what I use now ;).  I'm looking to get some better local auth\nthough, and in that case, I'd be in this camp:\n\n\nI want to host my content and manage my users and authz myself,\n   using another exotic client authentication mechanism\n   \u2026\n   This is the reason why I want this number 2 scenario into the\n   registry - that doesn't force others to use it though (that would be\n   scenario 4) as they should be able to get away with proxy + basic if\n   they want (or go exotic if they can merge support for it in the\n   engine).\n\n\nThe registry \u2194 storage authentication seems to be a completely\nseparate issue to me, this discussion should just be about client \u2194\nregistry authentication.\n\nLike I said, my requirements for this number 2 are:\n- as simple as possible for the registry, without the need to\n  interact with another server: the \"token\" needs to be signed and\n  fully verifiable standalone, and it should convey an authorization\n  to perform the requested operation\n- built on something that is \"standard\" enough - even if an IETF\n  draft, but at least something we don't have to reinvent hot water\n  for\n\nWhere does the \u201cwithout the need to interact with another server\u201d\nrequirement come from?  From the registry code side, is calling out to\nanother server using the http_auth_request API not acceptable 1?\nYou should be able to do everything up through SPNEGO / Negotiate 2\nwith that, and it can definitely handle a cookie/token based auth\nscheme.  Then folks who want to use the central hub for their auth can\njust point their registry at the central auth.docker.com (or whatever\nyou end up calling that endpoint).  The central hub doesn't even need\nto know if the auth request is coming from the central registery or a\nprivate registry, it just negotiates auth for the client and the\nrequested endpoint.  Beyond that, I personally don't care what\nmechanism auth.docker.com uses to talk to the client.\nFolks worried about external server access from their registry can\njust use more localized auth in their reverse proxy.\n. On Thu, Oct 09, 2014 at 07:08:46PM -0700, Olivier Gambier wrote:\n\n\nWhere does the \u201cwithout the need to interact with another server\u201d\nrequirement come from?\n\nThere are different aspects there:\n- efficiency: the client needs to get authorization first from the\n  auth service, then the registry would have to call back home to\n  verify it,\n\nIf you're using the http_auth_request API 1, the client is\nauthenticating with the auth service via the registry.  There's no\nneed for a pre-auth request directly to the auth service.  Of course,\nif you're passing secrets around, that means the registry can MITM\nyou, so I'd use Kerberos if I was serious about not leaking secrets to\nthe registry.  If the registry and auth service are both controlled by\nDocker, Inc. or the registry is local, passing around hashed passwords\n/ tokens is probably fine.\n\n\nsimplicity: the registry wouldn't have to handle client to server\n  communication,\n\n\nIt's registry-to-server communication, just proxied through the\nregistry (or the registry's reverse-proxy).\n\nand error conditions on that, including returning meaningful\n  errors to the final client.\n\nFrom the http_auth_request specs 1:\n\u201cIf the subrequest returns a 2xx response code, the access is\n  allowed. If it returns 401 or 403, the access is denied with the\n  corresponding error code. Any other response code returned by the\n  subrequest is considered an error.\u201d\nthat doesn't seem like much burden on the registry.\n\nAlso, the auth component wouldn't have to implement endpoints\n  exposing a verification API.\n\nIt's just one endpoint.  The API behind the endpoint is just \u201cget some\nheaders, and return the appropriate HTTP status code\u201d with passthrough\nfor WWW-Authenticate headers on 401s to allow challenge/response\nnegotiation.  I don't foresee much maintenance overhead.\n\n\nsecurity: the less communications to secure, the better. The less\n  publicly exposed APIs, the better.\n\n\nIt's an auth endpoint.  You're registry should be hitting it over\nHTTPS with X.509 auth for the auth service, which certainly seems\nstrong enough to me.  Folks sniffing the network will know someone at\nyour enpoint is doing something that requires Docker auth, but they'd\nknow that with any centralized auth scheme.  If someone else tries to\nhit the auth endpoint to discover a user's authorization, they'll just\nkeep getting 401s until they can successfully authenticate.\n\n\nreliability: this is linked to efficiency, the less exchanges, the\n  less likely an error condition could affect the complete workflow\n\n\nWith a stand-alone token approach, you still need to hit the central\nauth service, and you need to hit it as frequently as your token\nexpiration requires (assuming each token contains a full list of\nauthorizations, otherwise you'd probably have to hit the auth service\nfor each transaction).  Personally, I folks who have a connection to\nunreliable for central auth will just want to run their own local auth\nservice.  And if you want, that local auth service could just be a\npersistent-token parser (at which point, you're basically back to\nKerberos with client-to-server tickets with an additionally encrypted\nlist of authorized permissions).\n. On Wed, Oct 15, 2014 at 04:31:50PM -0700, Olivier Gambier wrote:\n\nWhen we moved to nginx, we realized there is no way to prevent nginx\nfrom doing on disk buffering for big payloads, introducing both disk\nspace and I/O issues.\n\nThat's not just 1:\nproxy_buffering off;\n?\n\nNote that I'm also very un-keen to the \"call-back-home\" design\n(auth_request) - from an operational POV, and like stated\npreviously, this clearly doubles the load on the auth component\n\nI'm not clear on the doubled load 2.  Can you spell that out for me?\n\nrequires more code on both side,\n\nI'm not clear on this either.  The auth service and client will both\nneed to understand the protocol.  With http_auth_request, neither the\nreverse-proxy nor the registry will need to understand it.  On the\nother hand, signed tokens will require verification code in either the\nreverse-proxy or the registry.\n\nmakes debugging from a client perspective more complex,\n\nI don't think 401/403 from the auth service forwared through the\nregistry-proxy via http_auth_request will make client-side debugging\ncomplex 2.  Signed tokens seem more complicated, because you could\nhave problems with the client \u2194 auth token request and with the\nsubsequent client \u2194 registry token use.  With the http_auth_request\nrequest, all the client sees is client \u2194 registry (the fact that the\nregistry is offloading auth to the auth service is completely\ntransparent).\n\nand (every day, here) proves problematic.\n\nWell, I can't argue with that ;).\n\nAgain, this is certainly a matter of perspective and usage - I would\ncertainly argue that heavy use systems with high availability\nrequirements are probably best served using cryptographically signed\nself-contained \"tokens\" (as in \"can be verified without calling\nhome\").\n\nIf the auth service isn't available, how do you get your signed token\n2?  It would certainly be possible to sign long-duration tokens and\ncache those on the client side, but that wasn't my impression of how\nthe signed-token scheme was going to work out of the box.  The fact\nthat it's possible at all is a win for signed tokens, but personally I\nthink the ease of implementation of the http_auth_request approach (no\nnew registry/proxy code) outweighs that benefit.\n. On Wed, Oct 22, 2014 at 11:40:59AM -0700, Olivier Gambier wrote:\n\n\nproxy_buff\n\nNo,  proxy_buffering off if for backend responses.\n\nAh, maybe 1:\nproxy_max_temp_file_size 0;\nwhich is the suggested solution to \u201cWe would prefer to run without\nproxy_buffering to prevent the load-balancer's local storage from\nbeing overrun.\u201d\n\n\ndoubling the load\n\nStory 1:\n- engine -> log -> auth service\n- engine <- token <- auth service\n- engine -> token -> service (service verifies signed token standalone)\nStory 2:\n- engine -> log -> auth service\n- engine <- token <- auth service\n- engine -> token -> service\n- service -> verify token -> auth service\n\nI'm suggesting story 3a:\n- engine \u2192 registry (give me RESOURCE)\n-          registry \u2192 auth (is CLIENT auth-ed for RESOURCE)\n- engine \u2190 registry \u2190 auth (401 nope, please authenticate)\n- engine \u2192 registry \u2192 auth (here are my credentials, now give me RESOURCE)\n-          registry \u2190 auth (200 CLIENT is clear)\n- engine \u2190 registry (here you go)\nIf the client suspects they may not authenticate, they can skip the\ninitial 401-generating attempt and use story 3b:\n- engine \u2192 registry \u2192 auth (here are my credentials, now give me RESOURCE)\n-          registry \u2190 auth (200 CLIENT is clear)\n- engine \u2190 registry (here you go)\n\nStory 2, you need:\n- additional, properly protected endpoints on the auth service\n\nIt's an auth service, how could it not be properly protected 2?\nAnd you'll only need one endpoint for the auth service in story 3 (\u201cis\nthis request (which may have client credentials in its header)\nauthorized?\u201d).\n\n\nhttp client code on the service\n\n\nFor story 3, it's already built into Nginx with http_auth_request.  Of\ncourse, you could add it directly to the service (or alternate\nreverse-proxy) if you wanted to support Nginx-less, auth-ed deploys.\n\n\nadditional error handling and error conditions to be bubbled-up\n  for when the auth service doesn't answer properly (+ timeout\n  handling + throttling + etc)\n\n\nAuth timeouts are probably going to lead to 504 errors from Nginx\n(although I haven't tested this yet).  That seems appropriate for this\nsitutation, and the client should just try again later.\n\nBut there is a clear use-case for implementing a key + token based\nauthentication mechanism, and the proxy approach doesn't fly for it,\nfor all the reasons listed. :)\n\nI think the only serious gain is handling the \u201cwe have a really flaky\nconnection between the registry and the auth service\u201d case, and I'd\njust handle that by serving auth and the registry from the same local\nnetwork.  I see no reason why that local auth service used by the\nproxy (to protect against network outages) can't be a verifier for\nkey/token based auth, if you want to support long-lived tokens.\n. On Wed, Oct 22, 2014 at 12:04:48PM -0700, W. Trevor King wrote:\n\nI'm suggesting story 3a:\n- engine \u2192 registry (give me RESOURCE)\n-          registry \u2192 auth (is CLIENT auth-ed for RESOURCE)\n- engine \u2190 registry \u2190 auth (401 nope, please authenticate)\n- engine \u2192 registry \u2192 auth (here are my credentials, now give me RESOURCE)\n-          registry \u2190 auth (200 CLIENT is clear)\n- engine \u2190 registry (here you \nIf the client suspects they may not authenticate, they can skip the\ninitial 401-generating attempt and use story 3b:\n- engine \u2192 registry \u2192 auth (here are my credentials, now give me RESOURCE)\n-          registry \u2190 auth (200 CLIENT is clear)\n- engine \u2190 registry (here you go)\n\nIf you're using challenge/response auth to avoid authorizing a MITM\nregistry, you're obviously going to have to stick with the 3a\napproach.  But 3b will work for key/token auth or other schemes\n(e.g. basic auth) where you don't mind passing your key/token/secret\nto the registry (or its proxy) so it can act on your behalf.\n. On Wed, Oct 22, 2014 at 12:04:48PM -0700, W. Trevor King wrote:\n\nWed, Oct 22, 2014 at 11:40:59AM -0700, Olivier Gambier:\n\n\nproxy_buff\n\nNo,  proxy_buffering off if for backend responses.\n\nAh, maybe 1:\nproxy_max_temp_file_size 0;\nwhich is the suggested solution to \u201cWe would prefer to run without\nproxy_buffering to prevent the load-balancer's local storage from\nbeing overrun.\u201d\n\nAh, I see that you were clarifying direction, and not just saying I\nhad the wrong setting.  It looks like there is currently no way to\ndisable caching client-upload bodies locally 1, it was on the\nroadmap for Nginx 1.7 2.  The roadmap comment came after the 1.7.0\nreleased on 2014-04-24 3, so I'm not actually sure what the expected\ntimeline is here (the next comment asks for the expected point\nrelease, but there's no answer yet).\n. On Wed, Oct 22, 2014 at 01:50:42PM -0700, W. Trevor King wrote:\n\nIt looks like there is currently no way to disable caching\nclient-upload bodies locally 1, it was on the roadmap for Nginx\n1.7 [2].\n\nAnd there are existing patches which add this feature, they just\nhaven't been polished enough to land yet 1.\n. On Wed, Oct 22, 2014 at 12:04:48PM -0700, W. Trevor King wrote:\n\nI'm suggesting story 3a\u2026\n\nI've mocked this up in an orphan branch 1 if folks want to kick the\ntires.\n\nAuth timeouts are probably going to lead to 504 errors from Nginx\n(although I haven't tested this yet).\n\nI added a 70 second sleep (to get by Nginx's default 60-second\nproxy-read timeout 2) to the auth service's handler, and the test\nsuite showed a 500 response:\nHTTP Error 500: Internal Server Error\nWith the following server logs:\nnginx_1            | 2014/10/23 20:13:33 [error] 7#0: 1 upstream timed out (110: Connection timed out) while reading response header from upstream, client: 10.0.42.1, server: _, request: \"GET / HTTP/1.1\", subrequest: \"/auth\", upstream: \"http://10.0.0.213:80/\", host: \"0.0.0.0:8080\"\n  nginx_1            | 2014/10/23 20:13:33 [error] 7#0: 1 auth request unexpected status: 504 while sending to client, client: 10.0.42.1, server: _, request: \"GET / HTTP/1.1\", host: \"0.0.0.0:8080\"\n  nginx_1            | 10.0.42.1 - - [23/Oct/2014:20:13:33 +0000] \"GET / HTTP/1.1\" 500 192 \"-\" \"Python-urllib/3.4\"\nSo, yeah, that's not a very useful client-side error out of the box.\nI'll see if I can fix this up in the /auth location block.\n. On Thu, Oct 23, 2014 at 01:14:31PM -0700, W. Trevor King wrote:\n\nI've mocked this up in an orphan branch 1 if folks want to kick the\ntires.\n\u2026\n  nginx_1            | 2014/10/23 20:13:33 [error] 7#0: *1 auth request unexpected status: 504 while sending to client, client: 10.0.42.1, server: _, request: \"GET / HTTP/1.1\", host: \"0.0.0.0:8080\"\n  nginx_1            | 10.0.42.1 - - [23/Oct/2014:20:13:33 +0000] \"GET / HTTP/1.1\" 500 192 \"-\" \"Python-urllib/3.4\"\nSo, yeah, that's not a very useful client-side error out of the box.\nI'll see if I can fix this up in the /auth location block.\n1: https://github.com/wking/docker-registry/tree/reverse-proxy\n\nI've written a patch for Nginx adding auth_request_intercept_errors,\nwhich you can disable to pass the 504 error back to the client 1.\nHowever, there's currently no way to distinguish between 504 errors\nbecause the auth server timed out, and 504 errors because the registry\nitself timed out 2.  I'll build an Nginx container with both my\npatch and Weibin Yao's no_buffer patch 3, and slot that into my\nmockup.  Can anyone think of outstanding issues that will not be\naddressed by one of those two patches?\nOf course, there's the \u201cno Nginx proxy\u201d issue 4, but we can work\naround that by using the same handler-wrapping approach I mentioned\nfor logging 5 to build http_auth_request-style auth into the\nregistry itself, without changing anything behind DefaultServeMux 6.\n. On Tue, Oct 28, 2014 at 01:40:29PM -0700, Olivier Gambier wrote:\n\nThat might include using \"locks\" on the transactional storage\n(redis).\n\nRedis supports this already, we just need to add at least\nstart/commit/abort (in Redis: MULTI/EXEC/DISCARD) to the atomic\nstorage API.  I'm still not sure how accepted my independent\natomic/streaming strategy is 1, but that should be pretty doable.\nWith transactional atomic storage and content-addressable streaming\nstorage, I don't expect any problems beyond occasional orphan entries\nin the streaming storage.  An occasional garbage-collection pass could\ncheck for and clean that sort of thing up.  With reference information\nin the transactional atomic storage, it would even be pretty cheap,\njust remove anything from streaming storage that's over a day old and\nhas no references listed in it's atomic storage reference file.  Of\ncourse, you still have to iterate over all the items in the streaming\nstorage, so some way to slowly work through a complete list of entries\nwould be good.\n. On Tue, Oct 28, 2014 at 03:00:58PM -0700, Olivier Gambier wrote:\n\n\ncontent-addressable streaming storage\n\nThe content-addressability part is up to the registry, not to the\nstorage itself (drivers shouldn't need to know anything about that).\n\nIf you're going to have content-addressability I'd definately put it\nin at the storage level.  Knowing that content is write-once and named\nbased on its content allows you to make a bunch of simplifying choices\nin your implementation (e.g. no need to worry about racy writes).\nRequiring generic storage just so you can layer content-addressability\non top seems like a waste.\n\n\nOf course, you still have to iterate over all the items in the\nstreaming storage, so some way to slowly work through a complete\nlist of entries would be good.\n\nThat was the same problem with v1... such an approach is not\npractical with any crowded storage.\n\nThat just means your garbage collection process takes longer to\ncomplete its pass.  There's no reason it has to complete quickly\nthough, so I don't mind if it takes a month or two to gradually chew\nthrough looking for old orphans.  If you want faster cleanup, just run\na few garbage collectors in parallel on separate shards.  If you don't\ncare about cleanup, just don't run any garbage collectors.  I don't\nsee why it would be a large load on the storage driver.\n\nAbout the rest, I would rather keep the \"transactional storage\" as a\n\"simple cache\" and not as full-blown requirement.\n\nHow can a cache add transactionality?\n. On Tue, Oct 28, 2014 at 03:26:33PM -0700, Olivier Gambier wrote:\n\n\nIf you're going to have content-addressability I'd definately put\nit in at the storage level.\n\nThat would make for code duplication into every driver, doesn't\nsolve race conditions problems, requires the drivers to do more\nwork, and make it more difficult to fix the adressibility model if\nwe want to change it (eg: multiple version of tarsums for example).\n\nSo I'd use a simple hash for content addressability.  For another\nreason why using a braindead hash is useful, see 1.\nIf you go ahead with a fancy hash anyway, extending the API to have:\n- \u201cI'm writing $COUNT bytes to $FANCY_HASH which should have a sha1\n  hash $BRAINDEAD_HASH\u201d.\n- \u201cHow many bytes have been written to $FANCY_HASH?\u201d\n- \u201cI'm continuing an earlier write to $FANCY_HASH bith $COUNT\n  additional bytes starting at $OFFSET\u201d.\nwould be a fairly small change.  And the rest of the streaming\nstorage layer could be content-addressable (no changes after a\ncomplete write, no renames).\n. On Tue, Oct 28, 2014 at 03:26:33PM -0700, Olivier Gambier wrote:\n\nThat \u2026 doesn't solve race conditions problems\n\nWhy not?  With content-addressable storage (even with\nexternally-supplied names and resumable partial writes 1), you have\nthe same bytes behind the name (or $FANCY_HASH name).  If multiple\nclients are writing those bytes to the same path, who cares?  It's the\nsame bytes.  I don't expect the degredation in disk life from\noccasional duplicate writes to be a big deal ;).\n. On Tue, Oct 28, 2014 at 03:54:50PM -0700, Olivier Gambier wrote:\n\n\nyou don't know the hash before reading to the end of the file -\n  meanwhile, you need to write it somewhere - bottom line: you don't\n  know the hash beforehand - and if you are going to argue that the\n  client should send it, then you just allowed random content to\n  overwrite any other content :-)\n\n\nAh, that is another reason why calculating the name in the storage\ndriver would be easy.\n\n\nduplicating the hashing mechanism in every driver is bad -\n  duplicated code is bad - error prone - a maintenance headache\n\n\nI'd be surprised if any language folks would use wouldn't have a sha1\nor sha256 implementation.  So you could expect the client and engine\nto both use it, and they'd both share some library implementation.\nThat's not bad, it's just good separation of concerns.  So here's my\nlist of options in order of decreasing preference:\na. Use a hash that folks have already implemented (e.g. sha256), with\n   possible restrictions on clients to generate consistent tarballs\n   1.\nb. Write a tarsum library that storage-drivers and the engine can\n   share.  You'll need one for each language folks want to use.\nc. Create a streaming storage API that lets you centralize hash\n   calculation in the registry.  With post-write hashes, that looks\n   like:\n* \u201cI'm going to write $COUNT bytes which should have a SHA-1 hash\n   $BRAINDEAD_HASH, please give me a token for this write.\u201d\n * \u201cHow many bytes have been written to $TOKEN?\u201d\n * \u201cI'm continuing an earlier write to $TOKEN with $COUNT\n   additional bytes starting at $OFFSET.\u201d\n * \u201cI'm finishing my write to $TOKEN.  Please check the SHA-1 hash\n   of what you got against my initial request, and if it matches\n   store the content as $FANCY_HASH.\u201d\nThere's no need for a generic move here, we just get the name for\n   the final object at the end instead of the beginning.\n. On Tue, Oct 28, 2014 at 03:13:29PM -0700, W. Trevor King wrote:\n\nKnowing that content is write-once and named based on its content\nallows you to make a bunch of simplifying choices in your\nimplementation (e.g. no need to worry about racy writes).\n\nAnother possible optimization is that you can make the initial,\npartial writes (before you know the name [1,2]) to some faster media,\nand then after the file is complete with a verified wire checksum, you\ncan push it to slower storage.  For example, local filesystem storage\ncould write to a tmpfs and then push to disk, and S3 storage could\nwrite to tmpfs (or a local disk) and then push to S3.\nIs it worth avoiding copies on S3?  It looks like S3 has a transparent\ncopy operation that hides the move, but it only works on files that\nare \u2264 5 GB [3].  Maybe S3-wrapping libraries work around that\ntranparently, so it's not a big deal?\n. On Wed, Oct 29, 2014 at 10:18:51AM -0700, Olivier Gambier wrote:\n\nWe have very few layers above 500MB, not to mention above 5GB.\n\nIf you want a 5GB cap on layers, then that will work (assuming other\nstreaming storage backends don't have tighter restrictions.  Otherwise\nthings may get interesting when someone breaks that limit on S3.\n\nEither way, I still fail to understand what benefit we could\npossibly get from delegating (the same) intelligence into (each and\nevery) drivers :-).\n\nIt's not the same intelligence.  Maybe I have a dozen repositories\nlocally, never have write-contention or broken uploads, and want\nsomething that's ridiculously simple.  Maybe I have thousands of\nrepositories and want to maximize reliability and response times at\nthe cost of some extra complexity.  In this case, local filesystem\nstorage will likely benefit from having the initial unnamed file\nwritten to the same filesystem as it will eventually be stored on\n(because a local rename once we know the name is cheap).  But I can\nsee folks going either way for S3 (remote moves are expensive, but\nmaybe they don't have enough local storage to cache while we wait for\nthe name).  The benefit to delegating these \u201chow do we store things\nefficiently\u201d decisions to the drivers is that they are storage-side\nissues, not registry-side issues.  If some storage-driver maintainers\nwant to get together and write up shared intelligence in a library,\nthat would be great.  But baking that knowledge into the registry\njust breaks down the storage abstraction, and makes the registry code\nmore complex.  Since libraries are cheap in general, and\nwrite-then-move is easy to implement in particular, I see no benefit\nto weakening the abstraction here.\n. On Fri, Oct 10, 2014 at 11:29:45AM -0700, Olivier Gambier wrote:\n\nSome experiments is going on to use swagger for other parts of the\nproject\u2026\n\nI was initially skeptical about Swagger, but I've been playing with it\nfor the last few days and I've come around ;).  I haven't evaluated\nany alternatives to \u201cbrowsable docs from a machine-parsable API spec,\u201d\nbut if we end up using Swagger for the registry APIs I'll be happy.\n. On Thu, Dec 04, 2014 at 11:59:35AM -0800, Stephen Day wrote:\n\nHow well do you think it would fair describing something like the\nAPI in docker/docker#9015? Would you be willing to take a crack at\nit?\n\nSure, I can probably do that in the next week.  I'm not sure how well\nthat squares with docker/docker#8402 though, which was closed for a\nnumber of reasons including 1:\n\nThere are some implementation questions (lots of frontend assets,\nslow load times, esthetics) but we can get to that once the design\ndiscussion is resolved.\n\nI don't want to invest time in writting up Swagger specs unless we're\ncomfortable enough with the existing UI to not use \u201cI don't like\nSwagger\u201d as the reason they're rejected ;).\n. On Thu, Dec 04, 2014 at 01:52:40PM -0800, Olivier Gambier wrote:\n\n\nnothing is eternal, sure, and maybe this will change down the\n  road, but I really believe we should start working a prototype\n  using swagger (or a better alternative?) ASAP\n\n\nAs long as being ok with Swagger lasts for a few months, I'll be happy\n;).\n@stevvooe, do you want this as new docker/docker PR?\n. On Fri, Dec 05, 2014 at 09:14:56AM -0800, W. Trevor King wrote:\n\n@stevvooe, do you want this as new docker/docker PR?\n\nWork in progress in an orphan branch at 1.\nhttps://github.com/wking/docker.git client-registry-api-spec\nDo we want an early pull request (now?), or should I wait until I\nfinish implementing the current docker/docker#9015?\n. On Wed, Dec 10, 2014 at 11:35:21AM -0800, Olivier Gambier wrote:\n\nI'm ok with an early PR so people can get an early feel/play with it.\n\nDone with docker/docker#9598.\n. On Fri, Oct 10, 2014 at 04:41:18PM -0700, Brian Bland wrote:\n\nIncludes an IPC system for out-of-process drivers so users won't\nhave to recompile the registry to bring their own drivers. Also,\ndrivers can potentially be written in any language with a\nfunctioning libchan implementation.\n\nI'm not up on Go / libchan, so I'm not sure how this is going to scale\nwith many connecting clients.  Do you need a separate coroutine for\neach client?  If so, it seems like it would be more efficient to have\na single thread using select() or similar event-based processing (but\nmaybe Go's coroutines have absurdly small memory footprints?).\nI'm also missing my independent configs for atomic and streaming\nstorage 1 and storage-generated checksum names to keep us honest on\nthe content-addressable side (2 and docker/docker#8093), but if this\nis just a proof-of-concept, \u201clook it works in Go\u201d, then maybe it's too\nearly to be talking about that.\nOn the content-addressable side, it might be easier to have middleware\nthat translated between the content-addressable registry-side API and\na key/value storage side API (hashing on the fly and renaming from a\ntemporary filename after it finished writing) to make it easier to\nwrite storage drivers for key/value backends.\n. On Thu, Oct 16, 2014 at 10:22:05AM -0700, Derek McGowan wrote:\n\nDefine a standard way to log across in the registry in a uniform and\neasy to process manner.  Some topics to decide on...\n\nIf we use a reverse-proxy for serious deployments (see #623), we can\njust use that proxy to generate access/error logs.  For example, Nginx\ncan log to syslog out of the box 1.  That means we don't need much\nlogging in the registry itself, just debugging info if we have trouble\nwith a storage backend and things like that.\n. On Wed, Oct 22, 2014 at 11:25:45AM -0700, Olivier Gambier wrote:\n\nWe do want access logs from the registry - this is the only way\nyou are going to be able to tell if your proxy is actually sending\nthe request back\n\nWon't you get timeout errors in the reverse-proxy logs if the registry\nisn't responding?  And I'd expect the proxy logs would be where to\nlook to see if the reverse-proxy is responding (vs. hearing back from\nthe registry and then not forwarding that on to the client?).\n\nWe also want exhaustive application logging (including context,\netc). This is absolutely needed at least to debug.\n\nI don't think there will be much going on in the registry itself.\nIt's just a translation layer between client-facing REST API and\nstorage-facing Go/libchan APIs.  I expect reverse-proxy logs will\ncover the former, and storage-driver logs will cover the latter.  I'm\nfine logging the translation itself for debugging, I just don't think\nit's going to need all that much planning (since the translation code\nshould be fairly stable).\n\nThe default behavior should be to have these to stdout.\n\nThat would probably be fine, since I expect only registry devs will\nwant this debugging info.  I don't think anyone will need it for\nproduction installs.\n. On Wed, Oct 22, 2014 at 04:09:28PM -0700, Olivier Gambier wrote:\n\nOn the other hand, discarding valid use-cases is not :-)\n\nAgreed.  I'm just pushing to try and see what those use-cases are.\nI'll try to push a bit more politely ;).\n\nI do want access logs on the registry, since the registry is a\nstandalone product, and not everyone is willing to run it behind\nnginx.\n\nAssuming we get the proxy bits sorted (and with Nginx's ticket 251,\nit's certainly possible that we don't, see 1), I don't see why users\nwould even need to know that the default registry image bundled Nginx,\nwhich could be enabled for log-generation.  Of course that's a heavy\nstick if it's just for access/error logs, but if we're also using it\nfor auth (still up in the air, see #623) it seems like it's worth the\ntrouble to me.\n\n\nWon't you get timeout errors in the reverse-proxy logs if the\nregistry isn't responding?\n\nWhy would a timeout be the only case? What about hitting the wrong\nbackend?\n\nFor storage?  I'm not sure how the multi-backend storage works.  How\ndo we handle that now?\n\n\nI don't think there will be much going on in the registry itself. \n\nWell, there is a lot :-).\nExtensions come to mind.\n\nI'm hoping we can just provide HTTP notification for our actions, and\nleave things currently handled by extensions to external webhook\nrecipients (that's how I read 2, although now that I look at it\nagain I don't actually see \u201cHTTP\u201d in @shreyu86's comment).\n\n\nI'm perfectly happy with letting people do use a proxy if they\n  want - but you have to stop considering everybody will do as you\n  think is good :-)\n\n\nSure.  I'm happy if folks want to write a reverse-proxy layer in Go\ninstead of using Nginx.  I'm even fine if that gets compiled into the\nregistry [3].\n\nProxy support in registry v1 has been a terrible train wreck, and a\nliability for anyone who tried that in high-scale production\u2026\n\nI just didn't have a clear idea of how it was going wrong before 1.\nPrevious mentions of timeouts and such seemed easy to overcome with\nresumable actions.\n\n\nI just don't think it's going to need all that much planning\n(since the translation code should be fairly stable).\n\nSo, what happens then? Developers don't have a clean way to log\nthing, they start using various libraries. I refuse to merge that\nbecause it's messy, and they will switch to no log at all, or\nprintf, and we end-up with a poorly debuggable project failing to\nproperly log information about its behavior\u2026\n\nWhenever we get a report from someone that things are buggy, we ask\nfor their reverse-proxy and storage logs.  If they don't have those,\nwe have them set an environment variable that turns on whatever\nlogging our HTTP client (client-side) an libchan (storage-side)\nlibrary uses.\n\n\nI don't think anyone will need it for production installs.\n\nHaving auditable logs, and a robust logging infrastructure is IMHO a\nstandard feature of any serious software.\n\nI agree.  I'm just not convinced that the registry is the place for\nthe serious logging.  I think the HTTP / libchan layers are much more\nappropriate places for the logging.  And that HTTP layer should be a\nserious reverse-proxy or serious Go library (or both).\n\nI want this to be designed and dogfed from the start since it will\nbe harder to get right down the road.\n\nMaybe I'm just arguing that we should punt here until we actually have\nsome registry-side code that is complicated enough to be worth logging\n(indepently of the I/O logs, which I think we all agree are useful,\nbut which aren't code that will live in this repository).  We should\nhave time to spec out the logging in that case, and we won't have to\nargue about whether or not we'll actually need serious logs ;).\n. On Wed, Oct 22, 2014 at 04:58:36PM -0700, W. Trevor King wrote:\n\nWed, Oct 22, 2014 at 04:09:28PM -0700, Olivier Gambier:\n\nI want this to be designed and dogfed from the start since it will\nbe harder to get right down the road.\n\nMaybe I'm just arguing that we should punt here\u2026\n\nAnd for what it's worth, Go seems to have a logger 1 with an\noptional syslog backend 2 in the standard library.  That covers all\nthe serious logging needs I've ever had, although configurable output\nlevels would be nice (warning/info/debug).  Folks who do want logstash\nstorage can likely use the logstash's (alpha?) syslog handler 3.\nAlthough I have to admit I don't see anything wrong with syslog, so\nfolks who write all of these fancy new log formats likely know\nsomething I don't ;).  Or they just have too much time on their hands\n;).  In any case, I'm happy to punt everything beyond\nstdout/stderr/syslog to Docker itself (see docker/docker#7195).\n. On Wed, Oct 22, 2014 at 05:16:49PM -0700, W. Trevor King wrote:\n\nWed, Oct 22, 2014 at 04:58:36PM -0700, W. Trevor King:\n\nWed, Oct 22, 2014 at 04:09:28PM -0700, Olivier Gambier:\n\nI want this to be designed and dogfed from the start since it will\nbe harder to get right down the road.\n\nMaybe I'm just arguing that we should punt here\u2026\n\nAnd for what it's worth, Go seems to have a logger [1] with an\noptional syslog backend [2] in the standard library.\n\u2026\n[1]: http://golang.org/pkg/log/\n[2]: http://golang.org/pkg/log/syslog/\n\nAnd if the Docker client isn't using that \u2018log\u2019 package now, it was\npreviously (before docker/docker@92df943, daemon logging: unifying\noutput and timestamps, 2014-09-25) and likely will be again soon\n(docker/docker#8745).\n. On Fri, Oct 24, 2014 at 09:36:32PM -0700, Raymond Barbiero wrote:\n\n@wking said:\n\nIf we use a reverse-proxy for serious deployments (see #623), we\ncan just use that proxy to generate access/error logs.\n\nSo in my case I might actually terminate SSL with an F5 BIG-IP Local\nTraffic Manager and then run the registry behind that. For instance,\nuse a WIP for routing to the nearest region, a VIP per region, and\nthen put a registry or cluster of registries behind HAProxy per\nregion.\n\nYou can log to syslog with HAProxy too 1.  And obviously,\nstorage-driver logging doesn't depend on what's in front of the\nregistry.  What other logs (besides access/error logs from the\nregistry (via it's fronting proxy) and access/error logs from the\nstorage drivers) were you looking for?\nFWIW, Go's built-in server uses the built-in log framework for errors\n2 and you can inject access logging with a handler wrapper\n(e.g. 3).\n. On Tue, Oct 28, 2014 at 01:37:10PM -0700, Olivier Gambier wrote:\n\ndependent libraries (libchan, extensions, storage drivers) should\nuse that to do their logging\n\nI'm still pushing for extensions and storage drivers to be separate\nprocesses in separate containers with their own independent logging.\nBut for the core registry code and any in-container extensions and\nstorage drivers this summary looks good to me.\n. On Mon, Oct 20, 2014 at 09:36:51AM -0700, Olivier Gambier wrote:\n\nA .editorconfig (0)\nA .gitignore (1)\n\nI don't think we need these changes ;).\n\nA AUTHORS (0)\nA CHANGELOG.md (0)\n\u2026\nA LICENSE (0)\nA README.md (0)\n\nAll empty?  Why not leave them off until we have some condtent for\nthem?  I see no need to preserve history at all if you're going to\nclear all the old stuff away with a reset commit.  Why not just start\nan orphan branch?\n\nA CONTRIBUTING.md (53)\n\n+## You want some shiny new feature to be added?\n+\n+So, so.\n- \u201cSo, so.\u201d?  I'm not sure what this is saying, but I think this\n  section reads fine without it.\n- There's also a bit of trailing whitespace in this file.\n- It could probably use backticks around paths, filenames, etc.\n- How do we maintain a global DEP counter with the potential for\n  simultaneous, in-flight proposals?\n- \u201caccomodate\u201d \u2192 \u201caccommodate\u201d.  I haven't spell-checked the whole\n  file though ;).\n- \u201cOnce your proposal PR is accepted and merged, you may start working\n  on implementing it\u2026\u201d seems overly harsh.  Clearly folks can start\n  working on the implementation whenever they want ;).  It's just not\n  a good idea to sink too much time into the implementation if the\n  spec is going to be rejected or seriously altered, but we can just\n  warn folks about that and then let them make their own decision.\n- \u201cand ultimately approved by the counselors\u201d should probably be \u201c\u2026\n  approved (or rejected) \u2026\u201d to encourage folks to submit early ;).\n\nA open-design/MANIFESTO.md (121)\n- The \u201cProgrammer Manifesto\u201d seems a bit\u2026 abrasive\u2026 for an official\n  policy ;).\n\nWe discussed potential project organization in today's IRC meeting\n(#622, from 10:00 1 to 11:01 2).  My position (and I think I was\npretty convincing ;) is that we should have:\n- No official manifesto or governance policy.  Just implicit\n  benevolent dictatorship by the repository owners\n  (a.k.a. maintainers).\n- Tightly focused repositories for any deliverable interesting enough\n  to be discussed independently (e.g. docker/registry-api with just\n  the specs, docker/registry with just the registry API\n  implementation, docker/auth-api with the specs for Docker's token\n  auth (cased 2 and 4 from 3), docker/auth with the auth API\n  implementation, docker/atomic-storage-go-api with the specs for\n  compiled-in atomic storage #616, docker/atomic-storage-libchan-api\n  with the specs for external libchan atomic storage #616 / #630,\n  docker/streaming-storage-go-api with the specs for compiled-in\n  streaming storage #616 / #630, \u2026\n- Clear statements about the scope of each repository, with links to\n  related repositories.  For example, in docker/registry's\n  CONTRIBUTING.md:\nThis repository is only about this particular implementation, so\nplease report bugs and such at docker/registry/issues (although\nsee below for details about \u2018storage:\u2026\u2019 errors).  Otherwise see\nthese related projects:\n\n\ndocker/registry-api for the REST endpoint specs.  Send requests\n    for new endpoints or issues with the existing endpoints here.\ndocker/atomic-storage-go-api for the Go interface for atomic\n    storage.  Send requests here if you need to extend this\n    interface to support a new Go-based atomic storage\n    implementation.\ndocker/redis-atomic-storage for the Redis-backed atomic storage\n    implementation.  Send issues here if you get registry complaints\n    starting with \u2018storage:redis:\u2026\u2019.\n    \u2026\nBuild all of the api docs into http://docs.docker.com/ for easy\n  discoverability, with access to earlier API revisions (like 4).\nPush Go dependencies (libtrust, compiled-in storage backends, \u2026)\n  to the Go package repository 5, and install them in consuming\n  projects by using \u2018go get \u2026\u2019 in its Dockerfile.\nGive libchan dependencies their own Dockerfiles and use Fig or some\n  other orchestration tooling to spin up stand-alone registries\n  (e.g. docker/registry, docker/redis-atomic-storage,\n  docker/s3-streaming-storage, and docker/auth), using --link to\n  connect everything together.\n\nThis multi-repository setup makes it easy (with GitHub's \u201cwatch\u201d) to\nbuild communities around the individual pieces (e.g. folks interested\nin the registry's REST API) without the distraction of unrelated\nissues (e.g. discussion about the auth API or it's Go implementation).\n@dmp42 pushed back (10:49 [6]) saying that it would be:\n- harder to see the big-picture (I think my CONTRIBUTING.md references\n  address this).\n- more maintainer work (I don't see much initial difference in\n  maintaining a catch-all repository or the same content split across\n  several repositories, but I think that it makes it easier to build\n  communities and delegate maintenance, which should make things\n  easier in the long run).\nIf it turns out that #docker and the CONTRIBUTING.md references aren't\nhelping users successfully target their bug reports, I think a better\nsolution would be a catch-all docker/issues repository for issues,\nwhich could then be sorted into their appropriate target repository by\nbug wranglers, and have their initial docker/issues report closed when\nthe upstream report (maybe not even with docker/*) was opened.  Of\ncourse, you could do that same sort of targeting #docker or the\nmailing list, but some folks are just drawn to creating GitHub issues\n;).\n. On Mon, Oct 20, 2014 at 02:05:19PM -0700, sethdmoore wrote:\n\nif the environment variable $PORT is set, docker-registry loads this\nvalue and uses it for REGISTRY_PORT\n\nI didn't follow the rest of the discussion on #docker-dev this\nmorning, but what about:\n$ docker run -e REGISTRY_PORT=$PORT docker-registry\nIn any case, I think mapping environment variable names like this is a\nDocker-level problem (or an orchestration-level problem), which should\nhave a solution that doesn't need the registry to support all possible\nenvironment variable name choices ;).\n. On Tue, Oct 21, 2014 at 01:34:09PM -0700, Olivier Gambier wrote:\n\nA open-design/MANIFESTO.md (33)\n- What are ISVs?\nA open-design/specs/TEMPLATE.md (53)\n- Mmm, chunks of barbaz :).\n- ISO 8601 dates (YYYY-MM-DD instead of DD/MM/YY)?\n\nI think that this is basically:\n- CONTRIBUTING.md\n  - How to direct your bug (good stuff)\n  - How to submit a pull request (basic Git(Hub) workflow, not needed)\n  - Please use open-design/specs/TEMPLATE for largish features (see my\n    TEMPLATE notes)\n- open-design/MANIFESTO.md\n  - Scope of this project (why is this not front-and-center in a\n    README.md)\n  - Related projects (should also be in the README with links).\n  - Vision, mission, values (I'd just drop these.  Vision and mission\n    seem like rephrasings of the scope, and values sounds like \u201cwrite\n    something useful that works\u201d, which is hopefully implied without\n    an explicit note ;).\n- open-design/ROADMAP.md:\n  - I'd rather use GitHub's milestones to get better intergration with\n    the issue/PR tracker.\n- open-design/specs/TEMPLATE.md:\n  - I'm +1 to separating the spec from the implementation, e.g. with a\n    GitHub issue and separate implementation PR(s).  I'm ambivalent on\n    using a PEP-style template to do it.  I'd rather just tease any\n    relevant details out in the issue discussion and associated doc/\n    changes.  There's not much in the template besides what, why, and\n    how.  And only what and why are really critical for a feature\n    request.  I imagine some request lacking how, who, when, \u2026 will\n    cook for a while, but that doesn't mean users shouldn't submit\n    them.\nIf the weekly IRC meetings are going to be persistent, I'd like a\nstandard procedure for posting the agenda going in and archiving the\ndiscussion and summary going out.  Probably copy this from\ndocker/irc-minutes.\n. On Fri, Nov 07, 2014 at 11:28:49AM -0800, Olivier Gambier wrote:\n\n\nScope of this project (why is this not front-and-center in a\nREADME.md)\n\nI would like to keep README for end-users looking to use the\nproject - while this rather pertains to development. What do you\nthink?\n\nUsers are likely interested in the scope of the project as well, and\nalso in any external content that it closely tied to this repo\n(e.g. storage driver implementations).  But I agree that it's\nprobably best to have a deeper dive into the ecosystem in docs\ndirected at potential contributors.\n\nBut I like the idea of having a single, synthetic, one-place\ndocument outlining the big picture (yes, more maintenance, true).\n\nIf you've got the energy to maintain it, I'll happily read it :).\n. Some copy-editing:\n- \u201cinformations\u201d \u2192 \u201cinformation\u201d\n- \u201ca  title\u201d \u2192 \u201ca title\u201d (remove double space)\n- \u201csuccintly\u201d \u2192 \u201csuccinctly\u201d\n- There's some trailing whitespace ;).\n- \u201cadressing\u201d \u2192 \u201caddressing\u201d\n- \u201cadress\u201d \u2192 \u201caddress\u201d\n- \u201copen-design/specs\u201d \u2192 \u201copen-design/specs\u201d\n- \u201cDEP_MY_AWESOME_PROPOSAL.md\u201d \u2192 \u201cDEP_MY_AWESOME_PROPOSAL.md\u201d\n- \u201copen-design/specs/TEMPLATE.md\u201d \u2192 \u201copen-design/specs/TEMPLATE.md\u201d\n- \u201caccommodate with the feedback\u201d \u2192 \u201caccommodate the feedback\u201d\n- \u201cit's advised not to start\u201d \u2192 \u201cit's not advisable to start\u201d\nI still think that everything up to \u201cTechnical scope\u201d in MANIFESTO.md\nbelongs in the root README.md, but other than that this looks good to\nme.\n. On Mon, Oct 27, 2014 at 09:29:19PM -0700, Matthew Fisher wrote:\n\nWhy do drivers need to be spawned as separate processes?\n\nIt makes them language-agnostic.  Otherwise, folks would have to\ntranslate existing drivers to Go (where there may not be existing\nbindings for the backend in question).\n\nAs an example, in the current registry it's as simple as python\nsetup.py install or pip install --upgrade\ndocker_registry_driver_swift, changing around some configuration if\nit was necessary and SIGHUP the process. Boom, updated. If your\nsetup was anything more complex than that (such as in a\npublic-facing situation), you could just run the older and the newer\nversion of the registry side-by-side and gracefully migrate traffic\nto the new registry via your proxy.\n\nAll of that is still true with a network-linked storage process.\nYou're just SIGHUPing or proxy-handoff-ing the storage container\ninstead of the registry container.  With network IPC for storage\ndecoupling you from the registry itself, you can just leave the\nregistry container running while you upgrade your storage drivers.\n\nIf we were to ignore my above comment, how would installing/starting\na new third-party driver look like, both inside and outside of a\nrunning container?\n\nUsing something like my Fig config [1,2], it would look like:\n$ sed -i 's/image: old-streaming-storage:1.2.3/image: new-streaming-storage:1.0.3/' fig.yml\n  $ fig up\nfrom your host.  I don't see a point to being inside a running\ncontainer, when it's easier to just spin up new instances from\nscratch.  And if you're binding the storage process to a persistent\nnetwork address (or somewhere behind a proxy on a persistent network\naddress), there are no changes to make to the registry process at all.\n\nIn the eyes of the core maintainers, how would we maintain and\nensure that these drivers are up and running?\n\nI'm not a maintainer at all (just an interested third party), but I'd\nhave Docker images for the storage drivers, and I'd ensure the\ncontainers were up and running using the same infrastructure I\ncurrently use to make sure the registry container was up and running\n(currently Fig for me, but there are many orchestration frameworks).\n\nWith this change, configuration is no longer in a central\nconfig.yml; You have to specify configuration for every driver\nwhen spawning the process.\n\nI think we can have a central config.  You just need good\nnamespacing, and the registry process can use the same config that the\nstorage driver processes use 3.\n. On Tue, Oct 28, 2014 at 04:00:36AM -0700, Matthew Fisher wrote:\n\nAs I mentioned, there's a whole lot of code here but no\ndocumentation on the storage driver endpoints and with the IPC test\nsuite. Is documentation going to be supplied separately?\n\nI don't mind if it's separate or not, but I hope we'll get\ndocumentation on the interface, and a test harness to put a storage\ndriver process through its paces.  These seem like generic interfaces\n(the registry-specific translation is in the registry code, but there\nare likely non-registry users who would like streaming,\ncontent-addressable storage over libchan), so I've argued for\nsplitting them out (and splitting the specs out) into separate\nrepositories [1,2] with all the docs collected under docs.docker.com\n2.  I'm happy to start writing up my ideas for this more formally,\nbut I think we still have some details that need to get hashed out.\nDo we need a \u2018move\u2019 command (I think not 3)?  Do we want to\ndistinguish between streaming and atomic storage (I think we do 4)?\nI think this would be easier to figure out if we had a single point of\ndiscussion for the storage API (or APIs, with streaming and atomic\nbeing discussed separately), but I'm fine chasing this around the\nassorted docker-registry issues/PRs and pitching my ideas ;).\n. On Tue, Oct 28, 2014 at 10:10:21AM -0700, Brian Bland wrote:\n\nAs for swapping out drivers without restarting your registry\nprocess, this is not in the current design, and would have to be\napproached carefully. Switching to a different storage system with\ndifferent state is not well-defined behavior for the registry, so\nthis would only be supported for retaining the same driver name and\nparameters as originally defined while launching the registry\nprocess itself. We would also need to agree on the signal or action\nthat triggers a restart of the storage driver child process.\n\nObviously, the registry can only serve images that are stored in its\nstorage backend.  If you want to keep serving the same images when you\nswitch storage drivers, you're going to want to make sure both the old\nand new drivers are pointing at the same backing data, or that the new\ndriver points at a suitably upgraded copy of the old data.  It seems\neasier (to me) to manage this if the storage drivers are running in\nseparate containers, distinct from the registry code (which shouldn't\ncare that there's a new process serving its storage requests).\nSince it's harder to pass around links to unix sockets, I'd probably\nstart off with a networked libchan connection.  Then I'd add support\nfor a unix socket connection if folks wanted the increased efficiency\nat the cost of keeping the registry and storage-driver containers on\nthe same host.  You'd also have to teach your orchestration layer and\nlikely the registry how to handle rolling over the socket connection\nto a new storage driver's socket.\nI don't think we need to preserve the driver name (why would the\nregistry care about the name of the driver it was connecting to, so\nlong as the libchan protocol and connection endpoint stay the same?).\nI don't think we need to preserve the driver parameters either (for\nthe same reason).\nI'd leave swapping storage containers to the orchestration layer\n(e.g. Fig), so no need to teach the registry how to do that.  For 100%\nuptime, you would need a stable proxy between the registry and the\nstorage drivers to keep connections to the old driver alive until they\ncompleted and forward new connections to the new driver.  Then the\norchestration layer could reap the old driver once it's outstanding\nconnections closed.  Hmm, maybe libchan connections never close?  Can\nanyone chime in on proxying and rolling over libchan connections?\n. On Tue, Oct 28, 2014 at 10:27:32AM -0700, Brian Bland wrote:\n\n\nI don't mind if it's separate or not, but I hope we'll get\ndocumentation on the interface, and a test harness to put a storage\ndriver process through its paces.\n\nCurrently any storage driver can use the same test suites than any\nof the default drivers are using by calling\ntestsuites.RegisterInProcessSuite and\ntestsuites.RegisterIPCSuite. Both of these suites have the same\ntests and only differ in that one runs the storage driver locally\nand the other as a child process over IPC.\n\nIf I have a running container that exposes an endpoint for the\nlibchan-over-HTTP storage driver API, I'd like to be able to run:\n$ test-streaming-storage http://my-storage.example.com/\nand have it run through the test suite without my having to dip into the\nGo.  Maybe that's already possible, and I'm just not a good enough Go\nreader to tell?\n. On Tue, Oct 21, 2014 at 03:06:43PM -0700, Brian Bland wrote:\n\nA main/storagedriver/inmemory/inmemory.go (10)\n\nVery minor, and feel free to ignore, but I like \u201cmemory\u201d more than\n\u201cinmemory\u201d.\nRandom thoughts on the docs:\n- \u201cThe storage driver API is designed to model a filesystem-like\n  key/value storage\u2026\u201d.  Why mention \u201cfilesystem-like\u201d at all?  Why not\n  \u201cThe storage driver API allows key/value storage\u2026\u201d.\n- My interpretation of \u201cprefix\u201d was \u201cthe initial portion of the key\n  string\u201d (e.g. lists with a \u201cnamespace/\u201d, \u201cnamespace/b\u201d, or\n  \u201cnamespace/bob\u201d prefix should all return the \u201cnamespace/bob\u201d key,\n  and the \u201cnamespace/bobby\u201d key).  Maybe we only want to support\n  prefixes that end in a slash (which would make this more\n  filesystem-like)?\n- How do we hook up the local test suite to exercise an external,\n  IPC-based driver?  Maybe it's just not possible yet?\n- How do we register an external, IPC-based driver?  Do we have to\n  recompile the registry to do so?  A way to do this via the config\n  and/or environment variables would be nice.\n- You don't need libchan support in another language to have a non-Go\n  driver.  You could write a Go shim to convert the libchan IPC to a\n  protocol that your target language does support.  Mapping the\n  current storage driver API to HTTP should be pretty simple, since we\n  don't use libchan's special features 1.\n. On Wed, Oct 22, 2014 at 10:02:49AM -0700, Brian Bland wrote:\n\nProposed solutions:\n\nThese all sounds good to me, but I'd try and decouple the storage\nconfig settings (S3 parameters) from registry config settings\n(pointers to the S3 backend).  I see no reason to have the storage\nstuff be in-process with the registry, when libchan makes it easy to\nrun the storage in a separate process (see #630 / #643).  In that\ncase, we could just have the registry connect to an existing (possibly\nexternal) storage service, or spin up the storage service as a new\nsubprocess pointing at the same config the registry used.  In either\ncase, it would be nice to have sufficient namespacing in the\nregistry's YAML config that the storage config could live in the same\nfile.\nregistry:\n    storage:\n      streaming: s3://my-s3-service.example.com/\n      atomic: redis://my-redis-service.example.com/\n    log-level: debug\n    log-handler: syslog:///dev/log\n    debug-endpoints: true\n    \u2026\ns3:\n    bucket: foo\n    key: bar\n    log-level: warning\n    log-handler: file:///var/log/s3.log\n    \u2026\nredis:\n    database: redis://username:password@redis.example.com:6379/0\n    \u2026\n. On Wed, Oct 22, 2014 at 11:16:17AM -0700, Olivier Gambier wrote:\n\nAbout split configuration, and entirely split process, let people do\nthat, but that shouldn't be the default.\n\nIf folks want an in-process storage backend, they'd just replace the\nremote URL with a marker to the compiled-in service:\nregistry:\n     storage:\n       streaming: s3\nWhich would say \u201cuse the compiled-in s3 storage, and pass it the\nparsed config so it can grab what it needs (e.g. the s3: block).\u201d  Or:\nregistry:\n     storage:\n       streaming: s3+subprocess:///path/to/s3?--option=value\nto launch an S3 subprocess if you don't want to use a persistent\nservice.\n\nOr do we want implicit interpolation on any conf variable?\n\nThis sounds good to me.\n\nHow do we elegantly map nested names to env:\nfoo:\n   bar: baz\nwould that \"map\" to:\n- FOO_BAR\n- FOO.BAR\n\nNo dots [1].  FOO_BAR should be unambiguous.  I don't expect\ncollisions like:\nfoo:\n    bar: \u2026\nfoo-bar: \u2026\n\nAbout \"remote\" configuration files support (http, whatever), do we\nwant that in the core, or should that be the responsibility of a\ndeployment script to copy it over locally?\n\nI'm fine if the deployment script copies it down, but it should be\npart of the stock registry image so folks don't have to create\nspecial-case sub-images to get this behavior.\n[1]: From POSIX 2013 2:\nEnvironment variable names used by the utilities in the Shell and\n  Utilities volume of POSIX.1-2008 consist solely of uppercase\n  letters, digits, and the  ( '_' ) from the characters\n  defined in Portable Character Set and do not begin with a digit.\n. On Wed, Oct 22, 2014 at 05:51:06PM -0700, Brian Bland wrote:\n\nAs for the versioning scheme, \u2026\n\nThis sounds great to me.\n. On Wed, Oct 22, 2014 at 06:42:02PM -0700, Brian Bland wrote:\n\nIf we make the assumption that there is no ambiguity here, and also\nthat environment variables cannot distinguish between upper and\nlower case, then perhaps we should disallow hyphens and underscores\nin configuration key names.\n\nI'm fine with assuming there's no ambiguity even with hyphens or\nunderscores in the config names (I'd replace hyphens with underscores\nto find the environemt variable name).  Personally, I prefer hyphens\nto underscores in situations like this (YAML-parsing) that can can\nhandle them.  That's a really low-grade preference though, so I'll be\nhappy if the convention we eventually decide on is underscores.\n\nregistry:\n  loglevel: info # REGISTRY_LOGLEVEL=info\n  storage:\n    s3: # REGISTRY_STORAGE=s3\n\nI'm still pushing for a distinction between atomic and streaming\nstorage though ;) 1.\n. On Wed, Oct 22, 2014 at 05:51:06PM -0700, Brian Bland wrote:\n\nThe version value should be declared as a top level parameter in the\nconfiguration file like so: version: 1.8. We'll convert it to a\nstring internally to maintain the guarantee that 1.9 < 1.10 < 1.11\n\nLooking over #652, I think we should just use \u2018version: '1.8'\u2019 to make\nit obvious that we're parsing it as a string.  Or better yet,\n\u2018version: [1, 8]\u2019 so the YAML library can handle the parsing (and\nerror reporting) for us.\n. On Wed, Oct 22, 2014 at 01:55:25PM -0700, Olivier Gambier wrote:\n\nThanks! How do this compare to, say, redis?\n\nIt's content-addressable, so (from its README):\n- does not support versioned values. If key \"foo\" is value \"bar\",\n  key \"foo\" must always be \"bar\".\nThat means it's not going to work for anything we edit (e.g. tag files\nwhich list the tagged image id associated with that tag).\n. On Wed, Oct 22, 2014 at 02:28:44PM -0700, Bo Shi wrote:\n\nThe two are not mutually exclusive.\n\nSo the questions seems to be:\n- Do we have content-addressable data besides the image tarballs?  I\n  can't think of any off the top of my head, but I haven't looked\n  through our current storage data in detail, and I'm not fluent in\n  the v2 stuff.\n- Is it worth caching image tarballs?  I expect many tarballs will be\n  large, and optional caching based on size seems like more trouble\n  than it's worth.\nIf we do have content-addressable non-image data or want to cache\nsmall images, groupcache sounds like a good fit.\n. On Wed, Oct 22, 2014 at 03:04:57PM -0700, Bo Shi wrote:\n\n\nDo we have content-addressable data besides the image tarballs?\n\nComing from a different angle (assuming V2 is still under design),\nis there data that isn't content-addressable that could be made\ncontent-addressable?\n\nMy (sadly dead 1) detached signatures (docker/docker#6070) would\nhave let the image metadata and detached signatures both be\ncontent-addressable.  With the current embedded signatures\n(docker/docker#8093), I think the image metadata at least will need to\nbe mutable.\n. On Fri, Oct 24, 2014 at 07:50:04AM -0700, Eric Van Hensbergen wrote:\n\nAs far as I can tell, there is no mechanism for identifying an\narchitecture (ARM versus x86 for instance) for an image.  It would\nbe quite useful to allow for this both within the registry and\nwithin the build infrastructure (so that a docker build on ARM would\nonly use ARM pre-req images)\n\nThis sort of thing also sounds useful now that we have the possiblity\nof images that depend on the Windows-kernel in the pipe 1, as well\nas a range of architectures.  And who knows, maybe eventually images\nthat depend on OS X's XNU kernel.  I think it's best dealt with using\ntags in an image's metadata and a search index, and making such\nfirst-class members of the registry (or a free-standing component that\nlistens to webhook updates from the registry).  That would also let\nyou include things like kernel option dependencies\n(e.g. CONFIG_NFS_V4, or whatever) if your image depends on\nconfigurable kernel features.\n. On Tue, Oct 28, 2014 at 12:36:43PM -0700, Olivier Gambier wrote:\n\nThe suggested new format so far\n(https://github.com/docker/docker/issues/8093) does include an\ninformative-only architecture flag.\n\nDo we want to continue the arch discussion there (mixed in with the\nvalidation/signature stuff)?  Or keep it here?  Or move it to a new\ndocker/docker issue?\n\ndo we want \"fat images\"? - eg: multiple architectures, with\ndependent layers -\n\nI don't think so.  Otherwise how would I distinguish between \u201cI want\nto run an x86 Python container on my amd64 Linux kernel\u201d and \u201cI want\nto run an amd64 Python container on my amd64 Linux kernel\u201d.\n\nhow do we address tag resolution if we are not going with fat\nimages?)\n\nEither bake it into the tag (python:3.3-x86-linux) or the namespace\n(x86-linux/python:3.3).  For previous work on this sort of thing, see\nPEP 425 1.  The Python folks also decided that fat binaries were more\ntrouble then they were worth 2.\n\nAlso, there is in the engine an arch field AFAIK:\nhttps://github.com/docker/docker/pull/707/files\n\nAh good, but there's no kernel field (e.g. to distinguish \u201cWindows\nServer on amd64\u201d from \u201cLinux on amd64\u201d).\n. On Tue, Oct 28, 2014 at 03:04:52PM -0700, Olivier Gambier wrote:\n\nI kind of feel working this issue here is backward - the situation\nneeds to be clarified first in the engine itself (you are right\nabout both the \"usage\" and the kernel field).\n\nSure.  Should I open a new docker/docker issue, or should I summarize\nour discussion here in docker/docker#8093?\n. On Tue, Oct 28, 2014 at 03:43:47PM -0700, Olivier Gambier wrote:\n\nI guess a new issue altogether in docker is good\n\nOpened with docker/docker#8831, so we can probably close this.\n. On Sun, Nov 09, 2014 at 08:27:23PM -0800, J\u00e9r\u00f4me Petazzoni wrote:\n\nObviously native is better, but needs significant work on the engine\nside. IMHO before thinking about a native p2p transfer in the\nengine, we need to rethink how we store images. Short explanation:\nit would make a lot of sense to store images as hard links to a\ncommon file pool. This would achieve de-duplication on the file\nlevel (but wouldn't work for the devicemapper driver!) and allow\nbetter content download negotiation (figure out which files we\nalready have to avoid transferring them).\n\nThis is starting to sound like IPFS 1.\n. On Tue, Oct 28, 2014 at 04:45:55AM -0700, michaelheyvaert wrote:\n\nthe runner expects a list of arguments, but the environment variable\nis a string.\n\nWe shouldn't need shlex stuff here (I expect we just need to patch the\ntest suite?).  The string-to-list conversion should be happening with:\nargs += env.source('GUNICORN_OPTS')\nwhere docker_registry.server.env.source has:\nreturn yaml.load(\n      os.environ.get(key, _DEFAULT[key] if key in _DEFAULT else override))\nAnd docker_registry.server.env._DEFAULT has:\n'GUNICORN_OPTS': '[]'\n. On Tue, Oct 28, 2014 at 10:43:41AM -0700, Olivier Gambier wrote:\n\nThere must be another way to have the database creation be\nrace-safe, right?\n\nThis way seems fine to me.  Alternatives involve teaching the registry\nto efficiently handle parallel execution 1.  For a one-off task like\nbuilding the initial index, that seems like more trouble than it's\nworth.\n. On Tue, Oct 28, 2014 at 02:15:55PM -0700, Olivier Gambier wrote:\n\nI would like to minimize the ceremony around running tests /\nvalidation tools - also want to have a way to run only specific\ntests suites, list them, etc.\nAre there any standard tools to do that? Or should we go with some\n(basic, please!) shell scripting?\n\nTAP is nice [1,2,3].  Personally, I'd like the API specs and\nassociated test suites in one place, and implementations with any unit\ntests the maintainers want in another.  Then it's easy to test\nalternative implementations for compliance.\n. The \u201cinvalid character '<' looking for beginning of value\u201d string\nturned up in docker/docker#7485 (which I haven't read).  Maybe this is\nduplicate?  From the Apache logs, it looks like the client is\nmisbehaving?\nOn Wed, Oct 29, 2014 at 02:24:53PM -0700, Olivier Gambier wrote:\n\n@wking proxying ftw :)\n\nHey, I'm not suggesting proxying with Apache ;).  In this case, it\nlooks like a client problem (at least, I trust Apache more than I\ntrust the Docker client ;), and I'm not familiar with the client code.\n. On Wed, Oct 29, 2014 at 03:31:59PM -0700, Olivier Gambier wrote:\n\nIt's not a client problem. The client tries to use token based\nauthentication (hub default) instead of using the basic (proxy)\nauth. This is not periodic either - my current hunch is\n\"misconfiguration\" (either registry or apache - admittedly, no one\nmaintains that apache proxy file).\n\nSo when and how does the client choose to try token auth?  I think the\nusual proceedure is:\n1. Client assumes no auth and sends the request\n2. Server responds with \u201c401 Unauthorized\u201d, setting the\n    WWW-Authenticate header 1.\n3. Client resubmits the request with the requested auth.\nI suspect the client is just assuming token auth and skipping to step\nthree, which conflicts with Apache (who is expecting basic auth and\nnot Docker's token).  That sounds like a client-side bug to me (but\nI'm just guessing on my diagnosis).\n. On Wed, Oct 29, 2014 at 03:38:10PM -0700, W. Trevor King wrote:\n\nI suspect the client is just assuming token auth\u2026\n\nGrepping through the current docker/docker master, I only see\nWWW-Authenticate in registry/registry_mock_test.go.  It might be\nhandled by an external library, but I'm very suspicious ;).  I'll poke\naround with Wireshark when I get home tonight.\n. On Wed, Oct 29, 2014 at 03:57:42PM -0700, Olivier Gambier wrote:\n\nThen you are suggesting basic auth workflow never worked (which we\nknow it did) :)\n\nAh, I've never used it myself.  It must be something else then.  I'm\nvery hazy on the current auth protocol.  Interesting issues I've\nskimmed:\n- docker/docker#8265, which has notes about possible future TLS\n  handling between the client and daemon 1 that seems to conflate\n  auth with encryption (although auth will help avoid opening an\n  encrypted channel to a MITM attacker).  For example, you can use\n  basic auth over a plaintext channel (maybe you're using stunnel in\n  between?).\n- docker/docker#8283, which suggests adding username:pass@domain.com\n  and Basic Auth support between the Docker client and daemon.\nI don't see much on daemon \u2194 registry auth, other than a bare 2:\nAuthorization is done with basic auth over SSL\nfor the Hub API and a few Basic Auth notes about the hub 3, but a\nclaim of only Token support for the registry 4.  Can you point me to\ndocs for how Basic Auth is supposed to work for the registry?\n. On Wed, Oct 29, 2014 at 05:55:44PM -0700, Olivier Gambier wrote:\n\n@bacongobbler you are familiar with the nginx proxy - any insight\ninto this one?\n\nOr advice about how to set registry_endpoints.  I'm trying to work\nthis up with Fig (and Nginx, since there's no library/apache), and\nI've pushed my WIP to 1.  Here's how it works:\nSet thing up:\n$ fig up\nFrom the command line:\n$ docker login -u 'testing' -p 'testing' -e 'testing@invalid.net' http://localhost:8888/v1/\n  Login Succeeded\nWhich had the following transaction:\nClient says \u201cAre you listening on HTTP?\u201d:\nGET /v1/_ping HTTP/1.1\nHost: localhost:8888\nUser-Agent: Go 1.1 package http\nAccept-Encoding: gzip\nServer says, \u201cUse Basic Auth\u201d:\n```\nHTTP/1.1 401 Unauthorized\nServer: nginx/1.7.6\nDate: Thu, \u2026\nContent-Type: text/html\nContent-Length: 194\nConnection: keep-alive\nWWW-Authentication: Basic realm=\"Restricted\"\n\u2026\n```\nClient says, \u201cGive me a token.\u201d:\n```\nPOST /v1/users/ HTTP/1.1\nHost: localhost:8888\nUser-Agent: Go 1.1 package http\nContent-Length: 83\nContent-Type: application/json; charset=utf-8\nAccept-Encoding: gzip\n{\"username\": \"testing\", \"password\": \"testing\", \"auth\": \"\", \"email\": \"testing@invalid.net\"}\n```\nServer says, \u201cUse Basic Auth\u201d:\n```\nHTTP/1.1 401 Unauthorized\nServer: nginx/1.7.6\nDate: Thu, \u2026\nContent-Type: text/html\nContent-Length: 194\nConnection: keep-alive\nWWW-Authentication: Basic realm=\"Restricted\"\n\u2026\n```\nClient says, \u201cLog me in (with a GET?), and here's my Basic Auth.\u201d:\nGET /v1/users/ HTTP/1.1\nHost: localhost:8888\nUser-Agent: docker/1.1.0 go/go1.3.3 git-commit/79812e3 kernel/3.16.0 os/linux arch/amd64\nAuthorization: Basic dGVzdGluZzp0ZXN0aW5n\nAccept-Encoding: gzip\nServer says, \u201cWelcome aboard.\u201d:\n```\nHTTP/1.1 200 OK\nServer: nginx/1.7.6\nDate: Thu, \u2026\nContent-Type: application/json\nContent-Length: 4\nConnection: keep-alive\nExpires: -1\nPragma: no-cache\nCache-Control: no-cache\nX-Docker-Registry-Version: 0.8.1\nX-Docker-Registry-Config: dev\n\"OK\"\n```\nFrom the command line:\n$ docker tag busybox:latest localhost:8888/busybox:latest\n  $ docker push localhost:8888/busybox:latest\n  The push refers to a repository localhost:8888/busybox\n  Sending image list\n  Pushing repository localhost:8888/busybox (1 tags)\n  511136ea3c5a: Pushing \n  2014/10/29 19:58:29 Failed to upload metadata: Put http://registry:5000/v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json: dial tcp: lookup registry: no such host\nWhich had the following transaction:\nClient makes an HTTPS request.\nServer say, \u201cStop bothering me with that gibberish.\u201d:\n```\nHTTP/1.1 400 Bad Request\nServer: nginx/1.7.6\nDate: Thu, \u2026\nContent-Type: text/html\nContent-Length: 172\nConnection: close\n\u2026\n```\nClient says, \u201cAre you listening on HTTP?\u201d\nGET /v1/_ping HTTP/1.1\nHost: localhost:8888\nUser-Agent: Go 1.1 package http\nAccept-Encoding: gzip\nServer says, \u201cUse Basic Auth\u201d:\n```\nHTTP/1.1 401 Unauthorized\nServer: nginx/1.7.6\nDate: Thu, \u2026\nContent-Type: text/html\nContent-Length: 194\nConnection: keep-alive\nWWW-Authentication: Basic realm=\"Restricted\"\n\u2026\n```\nClient says, \u201cI'll be pushing these layers, and here's my Basic\n  Auth.\u201d:\n```\nPUT /v1/repositories/busybox HTTP/1.1\nHost: localhost:8888\nUser-Agent: docker/1.1.0 go/go1.3.3 git-commit/79812e3 kernel/3.16.0 os/linux arch/amd64\nContent-Length: 312\nAuthorization: Basic dGVzdGluZzp0ZXN0aW5n\nContent-Type: application/json\nX-Docker-Token: true\nAccept-Encoding: gzip\n[{\"id\": \"51\u2026\"}, {\"id\": \"42\u2026\"}, {\"id\": \"12\u2026\"}, {\"id\": \"a9\u2026\", \"Tag\": \"latest\"}]\n```\nServer say, \u201cSounds good.  Push them to registry:5000, and give them\n  $TOKEN.\u201d:\n```\nHTTP/1.1 200 OK\nServer: nginx/1.7.6\nDate: Thu, \u2026\nContent-Type: application/json\nContent-Length: 2\nConnection: keep-alive\nX-Docker-Token: Token signature=BIPXO0WLUBFTWE2A,repository=\"library/busybox\",access=write\nX-Docker-Endpoints: registry:5000\nPragma: no-cache\nCache-Control: no-cache\nExpires: -1\nWWW-Authenticate: Token signature=BIPXO0WLUBFTWE2A,repository=\"library/busybox\",access=write\nX-Docker-Registry-Version: 0.8.1\nX-Docker-Registry-Config: dev\n\"OK\"\n```\nwhich is why the client was dying with \u201clookup registry: no such\nhost\u201d.  I tried to set registry_endpoints 2, but maybe it has to be\nin the common or dev blocks.\n. On Wed, Oct 29, 2014 at 09:05:25PM -0700, W. Trevor King wrote:\n\nI tried to set registry_endpoints [2], but maybe it has to be in the\ncommon or dev blocks.\n\nThat was it (fixed with wking/docker-registry@edcc231), now I'm a bit\nfurther.  The last response from above now gives:\nX-Docker-Endpoints: localhost:8888\nSo the client can continue:\nClient checks an image:\nGET /v1/images/51\u2026/json HTTP/1.1\nHost: localhost:8888\nUser-Agent: docker/1.1.0 go/go1.3.3 git-commit/79812e3 kernel/3.16.0 os/linux arch/amd64\nAuthorization: Token Token signature=BIPXO0WLUBFTWE2A,repository=\"library/busybox\",access=write\nAccept-Encoding: gzip\nServer says, \u201cUse Basic Auth\u201d:\n```\nHTTP/1.1 401 Unauthorized\nServer: nginx/1.7.6\nDate: Thu, \u2026\nContent-Type: text/html\nContent-Length: 194\nConnection: keep-alive\nWWW-Authentication: Basic realm=\"Restricted\"\n\u2026\n```\nClient says \u201cGuess it's not there (?).  I'm uploading it\u201d:\n```\nPUT /v1/images/51\u2026/json HTTP/1.1\nHost: localhost:8888\nUser-Agent: docker/1.1.0 go/go1.3.3 git-commit/79812e3 kernel/3.16.0 os/linux arch/amd64\nContent-Length: 483\nAuthorization: Token Token signature=BIPXO0WLUBFTWE2A,repository=\"library/busybox\",access=write\nContent-Type: application/json\nAccept-Encoding: gzip\n{\"id\": \"51\u2026\", \"comment\": \"Imported from -\", \u2026}\n```\nServer says, \u201cUse Basic Auth\u201d:\n```\nHTTP/1.1 401 Unauthorized\nServer: nginx/1.7.6\nDate: Thu, \u2026\nContent-Type: text/html\nContent-Length: 194\nConnection: keep-alive\nWWW-Authentication: Basic realm=\"Restricted\"\n\u2026\n```\nAnd the client gives up to avoid sending the Basic Auth credentials in\nthe clear.\nI can add HTTPS to my test branch tomorrow, but I'm not sure about\n\u201cToken Token\u201d in the new client requests, and I'm not sure Nginx is\ngoing to do well with both the Token and Basic auth credentials\ncrammed into one Authorization header (or if cramming multiple\ncredentials into one header complies with the HTTP/1.1 spec).\n. On Thu, Oct 30, 2014 at 05:59:11AM -0700, Aaron Weitekamp wrote:\n\n$ docker push rhel7 --redirect-url https://cdn.redhat.com/registry/images/\nbef54b8f8a2f <- pushing metadata only \n\u2026\n$ docker push isv/app\nbef54b8f8a2f <- skipped, metadata already uploaded\n8da983e1fdd5 <- layer pushed to registry\n\nSo docker pushes to the push-time registry by default, but skips any\nlayers for which --redirect-url has added a URL to a different\nrepository?  When does the cdv.isv.\u2026 URL get injected into the\n8da983e1fdd5 metadata?  And you'll have to resign after each metadata\nchange with the proposed signature framework (docker/docker#8093).\nPersonally, I'd rather keep this out of the image metadata.  Instead,\nhow about a check-down list of registries for pulling:\n- Pull from https://cdn.isv.example.com/, but if the image isn't there\n- Pull from https://cdn.redhat.com/, but if the image isn't there\n- Pull from https://registry.hub.docker.com/\nWhich you setup in your ~/.dockercfg.  Then for pushing you have:\n$ git push rhel7 https://cdn.redhat.com/\n  $ git push rhel7..isv/app https://cdn.isv.example.com/\nto only push layers in isv/app but not rhel7 to cdn.isv.example.com.\n. On Thu, Oct 30, 2014 at 11:15:00AM -0700, Olivier Gambier wrote:\n\nAlso, if I understand well, crane does 302 to where the actual bits\nare. So, the company (content owner) has to trust the ISV's registry\n(/crane) to do what's right here - which to me kind of weakens the\n\"control-point\" - eg: I'm not sure I see a difference then between\n302, proxy-pass and mirroring, from a control standpoint.\n\nBut the ISV can put their own auth in front of their registry.  That\nmeans they can use their own auth there, but leave access to the Red\nHat images up to the folks running the Red Hat registry.  If you used\nproxy-pass from Red Hat, clients would have to give Red Hat their ISV\ncredentials.  You could use proxy-pass from the ISV (assuming the ISV\nhas read-access to the Red Hat repository), but then the ISV has to\nhandle the extra load of distributing the Red Hat images.\n\nAbout the v2 protocol - it's quite likely that layers urls are going\nto be namespaced, eg:\n```\nimage:\n/v2/manifest/redhat/rhel7/latest\n/v2/manifest/isv/foo/latest\nlayers:\n/v2/blob/redhat/rhel7/A_RHEL_LAYER_ID\n/v2/blob/isv/foo/A_RHEL_LAYER_ID\n/v2/blob/isv/foo/A_FOO_LAYER_ID\n```\nThe reason for that change is simpler access control (flat namespace\nfor layers - we currently have - doesn't work well).\n\nHmm.  GitHub seems to do fairly well with a flat namespace.  Is the\ngoal to make this easy to use with Basic Auth and a few reverse-proxy\nconfigs?  I think it would be easy for a more intelligent auth service\nto say \u201cok, you've authenticated as \u2018alice\u2019, and that user does have\nread (or write) access to the \u2018rhel7\u2019 namespace.\u201d  You'd just have to\nstore (user, namespace(/repository), permission) information in a\ndatabase for the auth service.  That seems both simpler and more\nflexbile than trying to sort all the repositories into a single\nhierarchy.\n\nThat doesn't mean content is actually duplicated on the registry -\nbut that inside the registry mechanics, there are \"mount points\" for\nlayers into namespaced url.\n\nAh, maybe you indent to have multiple hierarchies?  That would be as\nflexible as an auth service mapping users to namespace/repository\npermissions, but I don't see a point to exposing it outside of the\nauth service.\n\n\nhave some mechanism inside the registry to redirect specific layers\n\n\nNot inside the client to check several repositories for a given layer?\n\n... the part where I'm not that confident is the engine bits\nallowing to instruct a registry selectively that a given layer is to\nbe found \"elsewhere\". Actually, I'm wondering if this is at all an\nengine decision to make.\n\nI don't see why image hosting would be anyone's decision to make.\nIf Alice has an image, she should be able to host it wherever she\nlikes (modulo copyright/licensing/\u2026), without needing to inform\nanother registry about her choice.  And if Bob wants access to Alice's\nimages (modulo Alice's auth service), he should be able to have his\nclient check her registry (both implicitly via a registry preference\nlist, and explicitly via a command line option).\n. On Thu, Oct 30, 2014 at 01:00:19PM -0700, Olivier Gambier wrote:\n\n\nBut the ISV can put their own auth in front of their registry.\nThat means they can use their own auth there, but leave access to\nthe Red Hat images up to the folks running the Red Hat registry.\n\nDo you suggest there may be several layers of authentication\n(authenticate -> 302 -> authenticate again)?\n\nI'm suggesting we skip the 302 entirely.  The client would:\n1. Check the config.  The primary registry is Registry-A.\n2. Ask Registry-A for the image\n   - Registry-A says, 401, please auth\n3. Ask Registry-A for the image with the asked-for auth\n   - Registry-A says, 404, I don't have that\n4. Check the config.  The first fallback registry is Registry-B.\n5. Ask Registry-B for the image\n   - Registry-B says, 401, please auth\n     \u2026\n\nEither way, the point still stands: is there actually a strong\n\"control-point\" in trusting a downstream registry to do a redirect?\n\nI'm not trusting anyone to redirect.\n\nI don't think we are talking about anything requiring\nauthentication, but rather publicly available content.  Otherwise,\nyou might end-up with requiring multiple different authentication\nfor one image (eg: one for each layer), and that doesn't sound\nreasonnable.\n\nI don't think a few extra auth attempts are going to sink the service.\nIf you don't want your client doing that, just put the public\nregistries first in your list (e.g. check Red Hat for an image,\nfalling back to the ISV's registry).  That's less auth, but you'll be\nleaking the fact that you're trying to download an image and the id of\nthat image to Red Hat (instead of leaking it to your ISV when you ask\nfor an image that is hosted by Red Hat).\n\n\nYou could use proxy-pass from the ISV (assuming the ISV has\nread-access to the Red Hat repository), but then the ISV has to\nhandle the extra load of distributing the Red Hat images.\n\nWith caching, that's exactly the kind of stuff people want.\n\nIf the ISV wants to handle the extra bandwidth (with my scheme), they\njust have to serve a local copy of the image layer.  No need for any\nfancy redirects ;).  If they want to do that via proxy-pass and ask\nyou for your ISV credentials (so it's transparent to you), then great.\nIf they can assume the Red Hat layers are world readable, then they\ndon't need to ask for credentials at all.\n\n\nHmm.  GitHub seems to do fairly well with a flat namespace.  Is\nthe goal to make this easy to use with [...]\n\nThis is exactly what I am saying. Layers must be accessed under a\nnamespace (foo/myimage), like the manifest itself. Doing otherwise\n(eg: like we have, a flat namespace for layers) is a mess to get\nright as far as authorization is concerned.\n\nAh, I see.  If we kept 1:\nPUT /v1/images/(image_id)/layer\nor some such, then the auth decision would be something like (for a\nsingle PUT request):\n1. User claims to be \u2018alice\u2019.\n   - Query credential store and authenticate user.\n2. User asks for write access to image layer 8gz4tQt5.\n   - Query registry's atomic storage, layer 8gz4tQt5 is part of the\n     ancestry path for bob/foo:latest and charlie/bar:1.2.3.\n   - Query credential store, \u2018alice\u2019 has write access to \u2018bob/foo\u2019.\n3. Grant access for the PUT.\nWith cheap image-id \u2192 repository lookup, I think that should be fairly\nstraightforward.  There's no need to get the registry itself serving\n8gz4tQt5 under \u2018alice/\u2019, \u2018bob/\u2019, \u2026.  In fact, doing so would basically\njust reproduce my proposed image-id \u2192 repository lookup (which I'd\nstore in the registry's atomic storage 2).\n\n\nNot inside the client to check several repositories for a given\nlayer?\n\nThis sounds messy. Having to configure your client with multiple\nregistry endpoints in order to be able to pull a single image -\nending with situations where you try to figure out why you are\nmissing some layers.\n\nRegistries that want to support stand-alone usage for their users are\nfree to host all their ancestor images locally (although maybe they\nuse proxy-pass behind the scenes to do this).\n\nAnd craming registry urls into the manifest for every layer doesn't\nsound good either.\n\nThis we agree on ;).\n\nPeople will be allowed to push their content wherever they want, and\npull things from wherever they want. Still, so far, this is\nenvisioned as a single unit:\ndocker pull foo/bar --from https://whatever is expected to work\nas... it sounds... and retrieve all needed content without the need\nfor some acrobatic client configuration fiddling.\nNow the question raised here is how to allow certain registries to\ndelegate the responsibility of delivering specific layers to other\nregistries.\n\nThey can't do that with 302s, or proxy-pass, or whatever they like?\nIf the registry wants to cache (outside of the layer metadata) a list\nof possible mirror registries for a given image, that sounds great to\nme.  You could have a set of default mirrors for \u201cNever heard of that\none, you might want to check with\u2026\u201d.  Then folks could make their\nprimary registry whoever they trust the most.\n. On Fri, Oct 31, 2014 at 07:25:26AM -0700, Aaron Weitekamp wrote:\n\n\nWhen does the cdv.isv.\u2026 URL get injected into the 8da983e1fdd5 metadata?\n\n@wking I'm assuming URL information is never in image metadata. This\nis registry metadata only. It's doesn't go with the layer.\n\nAh, good, that means it won't conflict with image signing :).  When\ndoes the URL get injected into 8da983e1fdd5's registry metadata?  Your\nexample push for isv/app didn't have a --redirect-url flag, so I'm\nwondering how that (default?) was set in the JSON.\n. On Thu, Oct 30, 2014 at 08:59:09AM -0700, mirwan wrote:\n\nThe concatenation of a string (env.source('GUNICORN_OPTS')) into an\narray (args) splits string into single chars, not understandable by\nexecl.\n\nenv.source parses the value as YAML.  See #655.\n. On Fri, Oct 31, 2014 at 10:50:52AM -0700, Olivier Gambier wrote:\n\nWe need a better way (generate that from commits) - manually editing\nthis is a pain.\n\nI wrote a utility to do that 1.  It can also update the list of\ncopyright holders in copyright blurbs, but you can turn that off and\nget just AUTHORS updates if you like.\n. On Fri, Oct 31, 2014 at 01:21:45PM -0700, Olivier Gambier wrote:\n\nAlso curous about how this compares with\nhttps://github.com/docker/docker/blob/master/hack/generate-authors.sh\n?\n\nThat's doing the same thing, and is probably a better choice (as a\nlighter dependency you can just copy into your repository).  You'll\nprobably just want to cherry-pick the .mailmap from this PR, and use\n-f/--ignore-case to get your case-insensitive sort.\nWe can revisit update-copyright if/when you want to list contributing\nauthors or auto-bump the year ranges in your in-file copyright blurbs\n;).\n. On Fri, Oct 31, 2014 at 01:27:55PM -0700, W. Trevor King wrote:\n\nWe can revisit update-copyright if/when you want to list contributing\nauthors or auto-bump the year ranges in your in-file copyright blurbs\n;).\n\nOr if you want to credit authors of content that was copy-pasted into\nthis repository without it's associated history (e.g. @tianon, if\nwhoever copies generate-authors.sh over doesn't list him as the\nauthor of that commit).\n. On Mon, Nov 03, 2014 at 04:21:04AM -0800, Madhurranjan Mohaan wrote:\n\nHas anyone run docker registry on https?  We currently run it on\nport 80 but we already have requests that we should try and run it\nover https.\n\nBoth the Nginx and Apache reverse-proxy examples use SSL/TLS [1,2].\n. On Tue, Nov 04, 2014 at 08:07:37PM -0800, Olivier Gambier wrote:\n\nI'm very keen on the idea to have these as well for the go version,\nbut then, maybe there exist better solutions that we should consider\n- or maybe we should support alternative if the community want that.\n\nI like my error tracebacks in syslog, but I'm fine if they get there\nvia Docker grabbing the registry's stdout/stderr collection\n(e.g. using the proposed syslog driver from docker/docker#7195).\n. On Tue, Nov 04, 2014 at 08:19:52PM -0800, Olivier Gambier wrote:\n\nThis is (although related) kind of different from \"traditional\"\nlogging (#635) - and rather about supporting out of the box\nthird-party SAAS monitoring services.\n\nRight.  You can do that bit however you like as far as I'm concerned\n;).  I'm just putting my use-case out there so it gets included in (or\nat least considered for) whatever the final solution is.\n. On Thu, Nov 06, 2014 at 12:16:58PM -0800, Brian Bland wrote:\n\nThis adds a storage driver api version and a version check method\u2026\n\n\u2665\nCan we pull out the early driver.socket assignment (which drops\nparentSocket) into a separate commit?  It seems like a separate issue.\nI still prefer using a list of numbers [1,2] to a period-delimited\nstring, but whatever gets the job done.  Are we comfortable handling\nAPI bifurcation and driver-side compatibility by hand 3?\n. Please :).  IPC that's more widely supported than libchan currently is\nwould make writing extensions easier.  I was trying to decide how to\nhandle things like a search engine (#687) if it was driven via\nwebhooks.  The tricky bit would be that you wouldn't want to block\ncore registry activity just because the search engine was down.\nRather than queuing undelivered messages, I'd rather keep an event\ncounter, and archive past events in storage.  Then the\nwebhook-subscriber recovery lifecycle would look like this:\n1. Registry generates event 123 and sends it to the subscriber\n2. Registry generates event 124 and sends it to the subscriber\n3. Subscriber dies\n4. Registry generates event 125, and tries to send it to the\n   subscriber, but fails because the subscriber is dead.  The registry\n   continues on unphased.\n5. Suscriber recovers\n6. Registry generates event 126 and sends it to the subscriber\n7. Subscriber realizes that the last event it received was 124, so it\n   asks the registry to resend event 125.\n8. Registry retrieves event 125 from storage and sends it to the\n   subscriber.\nIf the delay between steps five and six is too long, you could always\nhave a \u201cplease send me the most-recent event\u201d endpoint on the\nregistry.\n. On Mon, Nov 10, 2014 at 04:01:55PM -0800, Olivier Gambier wrote:\n\n@wking @stevvooe @bacongobbler what do you think?\n\nI'd just add this to the docs for:\nGUNICORN_OPTS='[--ssl-version, 3, --certfile, /ssl/registry.cert, --keyfile, /ssl/registry.keys, --ca-certs, /ssl/ca.crt]'\nbut if folks want a shortcut environment variable for that, I'll go\nalong with it ;).\n. On Mon, Nov 10, 2014 at 04:32:28PM -0800, Matthew Fisher wrote:\n\n\u2026 though users would probably like to have /ssl configurable\u2026\n\nIn this case I'd really rather they just used GUNICORN_OPTS directly.\nYou don't save much by replacing --certfile with REGISTRY_CERTFILE ;).\n. On Mon, Nov 10, 2014 at 05:33:17PM -0800, Tibor Vass wrote:\n\nWe could debate having REGISTRY_TLS_VERIFY. It's explicit, but can\nalso be redundant with just checking the existence of an /ssl\ndirectory.\n\nThat works for me too, as does encouraging folks to terminate their\nSSL/TLS in a reverse-proxy in front of the registry.  I think that all\nof these options (or whichever ones get implemented or are external)\nshould get documented in the SSL/TLS section of ADVANCED.md, or in a\ndocument linked from there.  I think the main problem is informing the\nsysadmin, not saving them keystrokes while they write up their\nconfiguration ;).\n. On Mon, Nov 10, 2014 at 06:48:44PM -0800, Tibor Vass wrote:\n\nThe goal is to have simple TLS instructions on README.md.\n\nI'd just say:\nThe registry uses Gunicorn to manage workers.  If you want to setup\n  SSL/TLS, you can use any of the usual Gunicorn\n  options via GUNICORN_OPTS.  For example:\nGUNICORN_OPTS='[--ssl-version, 3, --certfile, /ssl/registry.cert, --keyfile, /ssl/registry.keys, --ca-certs, /ssl/ca.crt]'\nThat should be enough of a sketch for folks who are already familiar\nwith SSL/TLS, and it offloads the detailed docs to Gunicorn (which is\nappropriate, since it's a Gunicorn feature).\n\nfor production configuration, the recommended way is to use a\nreverse-proxy as described in ADVANCED.md\n\nI'm not sure its the recommended way, but it's certainly one way you\ncould do this.  It sounds like we also want a mini-tutorial in\nmaintaining client-side root CAs 1, so I'd just stick a more\nin-depth SSL/TLS dive somewhere outside the README where we could go\nover this in a bit more detail.\n. On Thu, Nov 06, 2014 at 06:25:32PM -0800, Stephen Day wrote:\n\nBecause layer files are not compressed, we may waste bandwidth\ntransferring them between registry services and docker agents, or\nvia external CDNs, such as cloudfront. Let's take some time to\nunderstand the tradeoffs here and what it would to support this.\n\nUncompressed images also waste space in the storage.  For example,\nunpacking Gentoo's package tree (~60 MB as a .tar.xz) adds a layer\nthat consumes ~300 MB.  I'd just have clients chose from a list of\nsupported compression schemes and set Content-Encoding appropriately\nwhen uploading.  That means the registry would have to decode on the\nfly to calculate/confirm the content-addressable storage id, but I\nthink the upload-time space savings are worth a few extra CPU cycles.\nOf course, this means that you'd need to lock out simultaneous writes\nto the same id, but we should be able to figure that out.  Folks who\ndon't want to spend CPU cycles on decompression could just configure\nan empty list of allowed encodings, and we'd return the list of\nallowed encodings for a given registry via the _ping endpoint.\n. On Fri, Nov 07, 2014 at 12:06:55AM -0800, Olivier Gambier wrote:\n\n\nbut I think the upload-time space savings are worth a few extra\nCPU cycles.\n\nActually, this is likely not accurate, and the reason why layer\ncompression was removed in the past. (cc @vieux)\n\nAh, maybe just a symptom of my local registry having few users and\nsmall disks ;).  With a configurable list of allowed upload encodings,\nit would be easy to pick the appropriate setting for a given registry,\nbut collecting the existing wisdom on this issue will help pick a good\ndefault and give good guidance in the docs for that config setting.\n. On Fri, Nov 07, 2014 at 12:25:22PM -0800, Andy Goldstein wrote:\n\nIn order for the registry to make the quota determination up front,\nthe client will likely need to begin by telling the registry the\nlayers in the image and the size of each layer.\n\nWith the flattened-metadata manifests from docker/docker#8093, the\ninitial:\nPUT /v2//image/\nwill be uploading the layer metadata for every layer in the image's\nhistory.  I'm not wild about that duplication 1, but in this case it\nmakes it easy to calculate the total image size (you have tarsums and\nsizes for all the layers).  That should let you calculate the new\nper-repo and per-namespace size, and make your quota rejection\nappropriately before anyone uploads layers.\nWith my preferred hash-linked metadata, you'd want to have the client\npush up the metadata chain before uploading layers.  Once you had the\nwhole metadata chain, you could make your accept/reject decision.\nYou'd want to either wrap the metadata-chain upload in a transaction\n(#626) for rejection cleanup, or have a garbage collection process\nremoving older metadata that lacked its associated tarball layer 2.\n. On Fri, Nov 07, 2014 at 02:02:12PM -0800, Andy Goldstein wrote:\n\nbut the V2 registry API proposal currently says this:\n\nAfter assembling the image manifest, the client must first push\nthe individual layers. When the layers are fully pushed into the\nregistry, the client should upload the signed manifest.\n\n\nAh, good point.  Can we turn that around?  I don't see why not, but\nI've been more pro-garbage-collection than @dmp42 1 ;).\n. On Fri, Nov 07, 2014 at 01:57:28PM -0800, W. Trevor King wrote:\n\nI'm not wild about that duplication 1, but in this case it makes\nit easy to calculate the total image size (you have tarsums and\nsizes for all the layers).\n\nAlthough this metadata-based size calculation will probably not\naccount for storage-side compression 1.\n. On Fri, Nov 07, 2014 at 02:40:43PM -0800, Olivier Gambier wrote:\n\nCompression is one thing, another is: if your image is based on X,\ndo you account for X base layers into your image size? If not, why?\n\nIf you link a layers from your namespace or repo, you get an $nth of\nit's size applied to your quota, where $n is the number of other\nnamespaces or repos that link to that layer.\nSo a 100 MB layer linked only from alice/foo, alice/bar, and bob/baz,\nwould count as 50 MB each to the alice and bob per-namespace quotas,\nand 33 MB each to the alice/foo alice/bar and bob/baz per-repository\nquotas.\nOf course, that's going to require you have (inside the registry)\nsomething like image refcounting so you can map from a layer id to a\nlist of dependent tags 1.\n. On Fri, Nov 07, 2014 at 02:51:10PM -0800, Olivier Gambier wrote:\n\nNow, should we \"deduplicate\" (eg: count a given layer only once\nper-user)?\n\nWhat are users?  Quotas should be per-namespace.\nIt's only in storage once, so I only charge alice and bob once for\ntheir per-namespace quotas.  However, I charged both alice/foo and\nalice/bar for the per-repository quotas.  Personally, I think only\nper-namespace quotas should matter.  As a registry maintainer, I don't\ncare how you want to partition that into repositories.\nFri, Nov 07, 2014 at 02:56:01PM -0800, Olivier Gambier:\n\nAnd I think the proposed design doesn't prevent this. What do you\nthink?\n\nNot if we can relax the \u201clayers first\u201d requirement 1.\n. On Fri, Nov 07, 2014 at 02:49:22PM -0800, W. Trevor King wrote:\n\nIf you link a layers from your namespace or repo, you get an $nth of\nit's size applied to your quota, where $n is the number of other\nnamespaces or repos that link to that layer.\n\nHmm, this doesn't work so well with private repositories, unless you\ndon't mind leaking information like:\n\u201cWe're charging you 10 MB for this 100 MB layer, because it's used\n  publically by 6 other namespaces (alice, bob, charlie, \u2026) and\n  privately by 3 namespaces we can't tell you about.\u201d\nThat's not too leaky, but it might be better to just have anyone\npaying for private storage on a pubic registry get a pass on their\npublic dependencies, and have:\n\u2211_{layers} size(layer)/count(layer)\nwhere count(layer) is the number of public dependers for public layers\nand one for private layers.  That means that private dependers don't\nhelp dilute the quota-load of a given layer, but that's probably not a\nbig deal.\n. On Mon, Nov 10, 2014 at 06:29:31AM -0800, Andy Goldstein wrote:\n\nI hadn't ever really considered charging multiple namespaces for\nportions of the same layer. I can see why that might be useful, but\nit wasn't something I was initially considering. My thinking is that\nthe layer's size is 100% charged to whatever namespace and/or\nrepository it's initially uploaded to.\n\nI think that folks uploading a widely useful layer (e.g. the ones\nbehind centos:centos7) shouldn't have to shoulder the whole cost.\nThat would incentivize not publishing your layers.  If you distribute\nthe cost among layer consumers, folks who publish popular layers can\nkeep at it without wondering if it's worth buying more quota.  Any\nwhen an image becomes less popular, folks who aren't as interested in\nit can drop their consuming images, which gives you a natural way to\nreap outdated layers (and of course, if those old layers are still\nimportant to some folks, they're free to shoulder the cost\nthemselves).\n\nIt makes sense for us to assign a quota to the project\n(i.e. namespace) as a whole. We may even want to have the quota\napply to a user account, since a user can have multiple projects. I\nthink if we were to try to do that, we'd probably need to find a way\nto assign multiple namespaces to the same quota bucket, \u2026\n\nYou can also have multiple users pushing to the same namespace.  I'd\njust track namespaces for quotas, and allow users to fund\nper-namespace quotas as they saw fit.  What benefits do you see from\nhaving additional per-user quotas?\n. On Mon, Nov 10, 2014 at 09:03:06AM -0800, Olivier Gambier wrote:\n\n\nneed to address the details (what happens with uncompleted\n  uploads? how long does it take to consider it \"dead?\n\n\nThis is important for more than quotas.  Maybe split it out into a new\nissue?\n\nis the quota authorization done before uploading or at the end?\n\nBefore is nicer (saves network load if the eventual image will be\nrejected) 1.\n\nif the earlier how do we handle \"liars\"?\n\nIf the uploaded layer has a different tarsum or size than the initial\nmetadata claimed, you just dump it and abort the push (and reap the\nbusted metadata).  And maybe flag the user in case you want to ban\nthem after $n corrupted uploads.\n\nhow do we handle simultaneous concurrent uploads?)\n\nNot a problem if you're using distributed cost.\n. On Mon, Nov 10, 2014 at 09:13:58AM -0800, Andy Goldstein wrote:\n\nFor OpenShift we may decide that widely useful layers such as\ncentos:centos7 are owned by the system and not applicable to quotas.\n\nHaving a way to set an infinite quota sounds good to me (and should be\nfairly easy to do).\n\nIf multiple users are uploading new layers to a namespace, the quota\nshould be charged to the owner of the namespace.\nFor per-user quotas, if we wanted to say \"you get a free account at\nOpenShift, and with that comes some amount of free image storage,\"\nwe might want the quota to apply to the user across all the registry\nnamespaces they own. So it's really just a way to tie multiple\nnamespaces together in a single quota group.\n\nThat sounds reasonable.  If you've got per-user quotas like that, do\nyou still need per-namespace quotas?\n. On Mon, Nov 10, 2014 at 09:24:23AM -0800, Andy Goldstein wrote:\n\n\nIf you've got per-user quotas like that, do you still need\nper-namespace quotas?\n\nWe may or may not - we're still working out our needs, so I'd like\nto keep our options open.\n\nWould per-namespace quotas be measured/managed independently of\nper-user quotas?  That sounds awkward.  Maybe per-namespace quotas are\njust a way for users to pin the allocation of their per-user quotas?\nI don't see a benefit to that.  Maybe I'll just sit tight until you've\nworked out what you want ;).\n. On Fri, Nov 07, 2014 at 04:05:52PM -0800, Steven Schlansker wrote:\n\nWe want to be able to select tags by a couple of criteria - date,\nuser, etc.\n\nI don't think the date is stored with the tag at all, but you may be\nable to figure out the user by looking through the signatures\n(docker/docker#8093).  In any case, this feels like a job for an\nexternal search index to me (docker/docker-registry#687).\n. On Sun, Nov 09, 2014 at 07:53:46PM -0800, Sam wrote:\n\nbut seriously, it would be really good if there was an easy way to\ncleanup old images / layers to free up disk space.\n\nFor the current Python registry, there's #409 and its rewrite as an\nextension in #606.  I couldn't find explicit plans for this in the\nnext-generation issues, but I've been trying to get the pieces in\nplace to implement it more efficiently (e.g. #626).\n. On Mon, Nov 10, 2014 at 09:07:43AM -0800, Olivier Gambier wrote:\n\nI would trash the database and restart the service.\n\nYup.  If there's no database, the registry will rebuild it by walking\nthe storage.  Don't forget to --preload ;) #655.\n. On Mon, Nov 10, 2014 at 07:23:48AM -0800, Boyd Hemphill wrote:\n\n2014/11/09 16:20:19 Error response from daemon: \u2026 x509: certificate\nsigned by unknown authority\u2026\n\nYou need the CA signing your registry's cert in your local list of\nroot certificates.  See 1.\n. On Mon, Nov 10, 2014 at 09:42:41AM -0800, Olivier Gambier wrote:\n\nWe can probably use some words in the FAQ or ADVANCED doc.\n\nThis is a client issue (in this case, the Docker engine/daemon), but\nyou're most likely to hit it if you're running your own registry, so\nsure.  I don't know if we have a particularly good place to put it\nthough.  Something like:\n\u201cIf you're using an uncommon root CA (e.g. for a self-signed cert),\n  make sure your users install that CA in their client's list of root\n  CAs.  For folks using the Docker engine as a registry client, that's\n  their OpenSSL implementation's default path (I think ;).  For folks\n  who are managing their OpenSSL root certs with ca-certificates, you\n  can do that by dropping your CA cert under\n  /usr/local/share/ca-certificates and running\n  update-ca-certificates.  You'll need to restart your OpenSSL-using\n  client so it notices the new CA certs.\u201d\nThat's a lot of ifs for this particular issue, but the real solution\nto this would be giving folks a better understanding of their OpenSSL\nconfig, and there's no good place in the Docker docs to do that ;).\n. What shell was this run on?  Square brackets are shell-sensitive, so I\nthink you'd want to quote them, and the shell shouldn't be passing\nthem through to the underlying process:\n$ echo \"${BASH_VERSION}\"\n  4.2.53(1)-release\n  $ python -c 'import sys; print(sys.argv)' -e GUNICORN_OPTS='[--preload]'\n  ['-c', '-e', 'GUNICORN_OPTS=[--preload]']\n. On Mon, Nov 10, 2014 at 10:29:43AM -0800, W. Trevor King wrote:\n\n$ python -c 'import sys; print(sys.argv)' -e GUNICORN_OPTS='[--preload]'\n  ['-c', '-e', 'GUNICORN_OPTS=[--preload]']\n\nAnd here's an example of the trouble of not quoting:\n$ touch 'GUNICORN_OPTS=-'\n  $ python -c 'import sys; print(sys.argv)' -e GUNICORN_OPTS=[--preload]\n  ['-c', '-e', 'GUNICORN_OPTS=-']\nFiles matching \u2018GUNICORN_OPTS=[--preload]\u2019 are unlikely, but I still\nthink we want this quoting, and don't understand why the quoting was\ncausing trouble for @stevenjack.\n. On Mon, Nov 10, 2014 at 02:04:17PM -0800, Kevin Littlejohn wrote:\n\nI suspect (but have not confirmed) that --preload means a single S3\nconnection is setup in advance and shared across threads.\n\nYup 1.\n\nThis failure exists in 0.8.1 and 0.9.0 at least (but 0.8.1 will run\nwithout --preload, where 0.9.0 was consistently failing to\ninitialise sqlalchemy for us).\n\nI'm not sure what changed between these.  A different Gunicorn version\nmaybe?\nAs a workaround, you can start a single-threaded registry to populate\nthe index.  Once it's up, kill it, and start without --preload and\nwith your usual number of workers.\n. On Thu, Nov 13, 2014 at 07:32:37AM -0800, Don Laidlaw wrote:\n\nThe problem is in docker_registry/lib/index/db.py in the results\nmethod: the session is not closed.\n\nOops.  I wonder how it's been working for me?  Since I just applied\nyour change exactly, I made you the commit author for #723.  I hope\nthat's ok with you ;).  I can change it if you like.\n\nThe session is not closed in _generate_index either. You might\nwant to look at that.\n\nThe idea was that methods that opened a session would close it.\n_generate_index is called from _setup_database, which opens and closes\nthat session.\n. On Thu, Nov 13, 2014 at 03:42:06PM -0800, Stephen Day wrote:\n\nWe've added a path mapper to support simple mapping between path\nobjects used in the storage layer and the underlying file system.\n\nI'm not sure why we need an \u201cunderlying file system\u201d model on top of\nour abstract storage 1.  Why assume that the storage engine should\nuse a filesystem-like layout?  Looking at the proposed registry-client\nAPI (docker/docker#9015), we'll need to be able to:\n- Refer to manifests by repository name and tag.\n- List tags for a given repository.\n- Refer to layers by their tarsum.\nFor other tasks, we'll want endpoints to:\n- Iterate over all layers (for garbage collection [2,3]).\n- Iterate over all repositories (for initializing the search index\n  4).\n- List repositories that consume a given layer (for auth 5 or\n  listing aliases 6).\nThose don't need to be registry endpoints though, if the utility can\ntalk directly to the storage driver.  However, with in-container\ndrivers 7, we will need registry endpoints.\nThe content-addressable mapping you're proposing is good (just flat\nsha256 sums, which the storage backend may chose to shard/fan-out if\nit likes 8).\nIt looks like layerindex is addressing the \u201cList repositories that\nconsume a given layer\u201d requirement, and handling that in a parallel\nnamespace (layerindex/ vs blob/) is fine with me.  I don't understand\nthe / bits though, can't we just use\nthe tarsum as an opaque string?\nlayerindex:tarsum+sha256:589\u2026e03\n(where I've used a colon instead of slashes between \u201clayerindex\u201d and\nthe tarsum because you don't need to be able to iterate over\nlayerindex entries).  The value for that key would be a list of\nreference tags in JSON.  Or, if you want per-tag entries:\nlayerindex:/:\nFor example:\nlayerindex:tarsum+sha256:589\u2026e03/library/debian:latest\nYou don't even need a value for that key.\nI don't understand the need for repositories//layers/\u2026.  Can't\nthe registry look that up in the manifest?  Dropping it would let you\nuse:\nrepositories//\nfor storing the manifests.\nThere's also which sounded to me like out-of-manifest certificate\nrevocations [9,10], and those would have to live somewhere too.\n. On Thu, Nov 13, 2014 at 06:36:19PM -0800, Stephen Day wrote:\n\nRight now, we are focused on standing up a working registry that\nimplements the V2 API so we can validate the concepts.\n\nThat's fair enough; obviously we don't need to have everything\npolished out of the gate.  If I'm pushing back here, it's mostly\nbecause the old docker_registry.core.driver.Base was a mess that was\nto complicated to touch ;).  I don't want to end up in that hole\nagain.\n\n\nI'm not sure why we need an \u201cunderlying file system\u201d model on top\nof our abstract storage 1.  Why assume that the storage engine\nshould use a filesystem-like layout?\n\nThis mapping scheme has the very nice property that it doesn't\nactually leverage file system operations for layer lookup (ie\ndirectory listing). While the scheme is hierarchical like a\nfilesystem, all the paths for the main operations are pre-calculated\nlookups followed by a read, which maps very well to keyed object\nstores.\n\nRight, key/value is easy with known keys.  The hard part is getting a\nlayout so that listing scales well, which means minimizing the number\nof bits where we need listing to work.  The current storage API\ndoesn't have a way to declare \u201cI'm goind to need a list for keys with\nprefixes like this\u201d, but if we can say that we'll only need to iterate\nover key lists at slash breaks 1 it gets easier to optimize your\nlisting.\n\nCurrently, the next generation storagedriver effectively implements\na VFS. The work I am doing now will definitely expose a higher-level\ninterface\u2026\n\nI don't want multiple levels of interfaces.  If the registry authors\nare thinking about the storage as a file system, they'll constrain\npossible storage implementations to things that are like filesystems.\nI want the storage API to be (and stay) as abstract as possible so\nstorage driver authors have maximum flexibility to optimize their\nimplementation for their particular backend.\n\n\nThose don't need to be registry endpoints though, if the utility\ncan talk directly to the storage driver.\n\nBy exposing the registry layout to out of band clients, we would be\neffectively exporting a new API, which needs to be versioned and\ncontrolled. We are still working on the V2 HTTP registry API and we\nwant to focus on that. Though the use cases (listing repos, layers,\netc.) you've listed will likely be added to the registry API to\nsupport management tasks, the general concept of safe,\napplication-level registry operations would likely be implemented as\npart of an extension mechanism.\n\nI'm fine with that, but they're not in the v2 HTTP client/registry API\nnow.  I'm just saying that I don't really care if the extra\nfunctionality is defined in the client/registry API or in a separate\nextension/registry API.  So long as I can bind to it from a\nwebhook-driven external extension (#689).\n\n\nIt looks like layerindex is addressing the \u201cList repositories that\nconsume a given layer\u201d requirement, and handling that in a\nparallel namespace (layerindex/ vs blob/) is fine with me.  ... I\ndon't understand the need for repositories//layers/\u2026.  Can't\nthe registry look that up in the manifest?\n\nWhile it can serve that purpose, this is actually part of the access\ncontrol methodology and partially a by-product of the v2 image\nmanifest format.\n\nThe registry/storage API shouldn't have to care about the\nclient/registry auth.\n\nThe layerindex allows one to lookup up the \"owning\" repository of\nthe target layer, identified by tarsum, without knowing the name of\nthe repository.\n\nAren't you just storing consuming :s?  You don't need a\nsingle owner with content-addressable layers 2.\n\nIf the user doesn't have access to one of the listed repositories,\nthey can't get to the blob id, stored under\nrepositories/<name>/layers/.\n\nAgreed, but I'd handle this in the registry itself (see the \u201cPUT\n/v1/images/(image_id)/layer\u201d discussion in 3), not offload it to\nyour storage layout.\n\nThe purpose of repositories/<name>/layers/ is to track the layers\nthat have been uploaded under the particular namespace. Parsing the\nmanifests wouldn't meet this use case and would be rather expensive.\n\nFirst, the storage driver is free to optimize this.  I just don't\nthink the registry needs to care about that optimization.  Second,\nwhy do you need to know which are already uploaded?  There's no API in\ndocker/docker#9015 for \u201cTell me which layers behind : you\nstill need\u201d.\n\nThis scheme supports the use case of two parties uploading identical\nlayers without requiring access to each others repositories. This\navoids the pathological case of \"claimed tarsums\". With this\napproach, two parties can use the same tarsum for a layer but both\nparties have to prove they have the content, even if they have the\nsame checksum. If this is not considered, the tarsum effectively\nbecomes a content access key, and any one that knows you're tarsum\ncan get access to your content. Conversely, if one uploads a simple\nlayer, they could claim that tarsum and prevent others using it,\neven if generated independently.\n\nFirst, you can't just claim a tarsum without content that has that\ntarsum, because the registry is also checking the sums before\nfinalizing the push [4,5].  Second, sha256 covers a big space.  You're\ngoing to have a hard time squatting on any meaningful fraction of it\nunless you can predict tarsums for layers you want to hijack ahead of\ntime.  Even if you do know the tarsum ahead of time (e.g. you're\nbroken into your target's Docker host), you still need time to hash\nall of your malicious tarballs to find one that matches your target\ntarsum.  I'm happy just trusting sha256 to be one-way, and so is the\nDocker security model, which is based on signing these tarsums\n(docker/docker#8093).\n\n\nI don't understand the <tarsum version>/<tarsum hash alg> bits\nthough, can't we just use the tarsum as an opaque string?\n\nThis allows multiple versions of indexes in the directory,\nreferencing the same content, without blowing up directory listing.\n\nWhy would you need to store multiple indexes to the same content with\ndifferent tarsum versions and hash algorithms?\n\nIt also keeps the + out of filenames, which might be problematic\nfor certain backends.\n\nThey're not filenames, they're keys ;).  If the storage driver wants\nto use a backing filesystem, it's up to it to chose an appropriate\nescape mechanism (e.g. URL-encoding the key).\n\nIt also allows other non-tarsum, indexes to exist over layer files\nin the future. We may want to change the scheme to be\nindex/layer/tarsum to free-up indexes over other objects, as well.\n\nI'm happy to kick this can down the road.  In the event that we need\nadditional indexes, there are always other root namespaces\n(thing1index/, thing2index/, \u2026).\n\n\nFor example:\nlayerindex:tarsum+sha256:589\u2026e03/library/debian:latest\nYou don't even need a value for that key.\n\nEven though the number of repositories with the same tarsum but no\nmutual access should be small, in accordance with your earlier\ncomments, directory listing should be avoided. Concurrent updates\nwill be problematic, but that is a fine tradeoff.\n\nI'm fine with a single-key:\nlayerindex:tarsum+sha256:589\u2026e03\ntoo 6.  I don't think listing this will be a big problem though.  If\nyou're looking for \u201cdo you have access to layer \u201d, the auth\nlookup will be:\n1. What namespaces or image names do you have access to?  (query\n   against the auth backend, no registry access needed)\n2. Iterate through the namespaces and image names found above, and\n   check if listing:\nlayerindex:tarsum+sha256:589\u2026e03/<namespace>/\n\nor\nlayerindex:tarsum+sha256:589\u2026e03/<namespace>/<repository>:\n\nreturns any keys, bailing (and granting access) after the first key\n   hit.  You can make all those requests concurrently, so it may even\n   be faster than the single-key approach for layers that have lots\n   of consumers.  If you don't want the concurrency, you'll need\n   another endpoint in the storage-driver API for:\nListAny(delimiters []string) ([]string, error)\n\nthat's like the current:\nList(path string, error) ([]string, error)\n\nbut takes a series of possible prefix strings.\nAlthough this was in reference to an earlier iteration of the\npull request, and no longer seems accessible on GitHub.  Relevant\nquote from @dmp42:\n\n\u201cWe are going to use move in the registry, a lot. Tarsums /\nchecksums need to be computed, and the file needs to be\ncomplete. So, the layers will be moved to their final destination\nonce they are complete. Whether the driver implements a smart\n\"move\", or is just smart when copying, should be hidden under the\nspecific method call.\u201d\n\n. On Fri, Nov 14, 2014 at 11:06:13AM -0800, Olivier Gambier wrote:\n\n\nwe do need provision for multiple versions of tarsums - tarsum\n  will have to be modified in the future - like it had to be\n  modified in the past (include xattrs, remove mtime), if only, for\n  security reasons, and we will live in a world where content will\n  link to layers through multiple different versions of tarsum -\n  there is no discussion on that\n\n\nAnd both of our schemes mention the tarsum version and hash.  I'm just\nsaying that we don't need to be able to iterate over keys starting\nwith:\nlayerindex/tarsum/v1/\nwe just need to be able to iterate over keys starting with:\nlayerindex:tarsum+sha256:589\u2026e03\n(if we take the per-tag entry approach 1).  If we take the\nmultiple-tags-in-a-single-entry approach, we don't need to be able to\niterate over layerindex at all (unless we want to garbage-collect it).\n\n\nit's trivial to produce two layers with the same tarsum that have\n  different shas - this is not only expected, but the reason tarsum\n  was designed in the first place\n\n\nI thought the issue was concern about different layers with the same\ntarsum (I don't think that will happen).  I don't think the transfer\nSHAs come into this at all.\n\n\nkeys vs. filesystems paths is not a debate, and doesn't change\n  things as far as allowed characters in path are concerned: unsafe\n  chars should be escaped to a common, safe subset, and this must be\n  done by the registry, not by individual drivers - and I can assure\n  you there are problems with the \"+\" sign for example, right now,\n  on S3 + Cloudfront\n\n\nEscaping that in the registry is fine, but personally, I think that's\njust working around buggy storage-drivers.\n\n\ninternal interfaces are just that: internal APIs to keep things\n  clear and reasonably modular. I don't think there should be a\n  discussion on \"I don't want multiple levels of interface\" :-)\n\n\nRight, and we have an existing abstract storage API.  I think the\nmental model of a filesystem will get us into trouble, because it\nmakes it easier to make implicit (and possibly false) assumptions\nabout the API.  For example, it's not currently clear to me if I can\nlist on non-slash-terminated prefixes:\nList(\"blob/sha256/01\")\nand get\n01a60e35df88d8b49546cb3f8f4ba4f406870f9b8e1f394c9d48ab73548d748d\n  01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n  \u2026\nwhich would be useful for parallel garbage-collection handlers (one\ncould garbage-collect 00_, another 01_, \u2026).  So there I want more\nlist flexibility than a filesystem offers (without globbing).\n. On Fri, Nov 14, 2014 at 11:24:58AM -0800, W. Trevor King wrote:\n\nFri, Nov 14, 2014 at 11:06:13AM -0800, Olivier Gambier:\n\n\nkeys vs. filesystems paths is not a debate, and doesn't change\n  things as far as allowed characters in path are concerned:\n  unsafe chars should be escaped to a common, safe subset, and\n  this must be done by the registry, not by individual drivers -\n  and I can assure you there are problems with the \"+\" sign for\n  example, right now, on S3 + Cloudfront\n\n\nEscaping that in the registry is fine, but personally, I think\nthat's just working around buggy storage-drivers.\n\nAnd the more workarounds like this you build into the registry, the\nleakier the storage API gets.  Now new registry code has to remember\nthat it's supposed to work around this storage-driver bug.  \u201cWhat key\ncharacters does my backend support\u201d is a question that only has a\nclear answer at the storage-driver level, and you can write generic\ntests (#731) so that the registry code can be confident that the\nstorage driver is doing a good job.\n. On Fri, Nov 14, 2014 at 02:15:57PM -0800, Olivier Gambier wrote:\n\n\nNow new registry code has to remember that it's supposed to work\naround this storage-driver bug.\n\nNo it does not.  Registry code uses an internal API that deals with\nthat once and for all, in one place.\n\nSo:\nbulk registry code\n    \u2190(internal API)\u2192\n     storage-backend-workarounds\n       \u2190(storage-driver API)\u2192\n         storage driver\nthat gives you back clean abstractions in the bulk registry code, but\nit also gives you more APIs to test and document.  The alternative\n(pushing the storage-backend workarounds into the storage drivers)\nminimizes the number of API layers, but means you'd need libraries\n(with their own APIs and docs) if you wanted to share workarounds\nbetween drivers.  For URL-encoding keys, suitable libraries and docs\nalready exist [1,2].\n. On Fri, Nov 14, 2014 at 02:20:03PM -0800, Olivier Gambier wrote:\n\n\n\u201cWhat key characters does my backend support\u201d\n\nSimple enough to devise. Likely something like: a-zA-Z0-9_.:-\n\nI agree that that's likely to work, but it's another bit you have to\nwrite up in the storage-driver API \u201cWe'll only use keys that match the\nregexp '[a-zA-Z0-9_.:-/]*', so you'll only need to escape keys if your\nstorage driver is sensitive to those characters.\u201d  There's less shared\ninformation if you say \u201cKeys can be any byte sequence.  If your\nstorage backend doesn't deal with some bytes as keys, make sure you\ntranslate the keys to something it does support.\u201d\n. On Fri, Nov 14, 2014 at 02:53:15PM -0800, Olivier Gambier wrote:\n\n\nI agree that that's likely to work, but it's another bit you have\nto write up in the storage-driver API \u201cWe'll only use keys that\nmatch the regexp '[a-zA-Z0-9_.:-/]*', so you'll only need to\nescape keys if your storage driver is sensitive to those\ncharacters.\u201d\n\nFortunately it's simple enough to test exhaustively (unlike testing\nexhaustively \"some bytes\").\n\nHow is it easier?  You have an infinite number of possible keys in\nboth cases.  Realistically, testing anything that you suspected might\nbe a problem (e.g. +, /, \u2018 \u2019, \u2026 in a key) is equally easy in both\ncases.\n\n\nThere's less shared information if you say \u201cKeys can be any byte\nsequence.  If your storage backend doesn't deal with some bytes as\nkeys, make sure you translate the keys to something it does\nsupport.\u201d\n\nThen people don't do the work properly because (insert random\nreason), and we end-up with a variety of bugs reports for\nthird-party code that we have to carry around.\n\nYou don't have to carry third-party code.  Suggested workflow:\n1. You get a bug report that identifies a problem in a storage driver.\n2. You write a storage-API test to expose the bug.\n3. You warn the driver authors and file a bug with them.  Now there\n   are some choices:\n   a. They fix their driver.\n   1. You bundle their new version in future official releases.\n      b. They don't fix their driver\n   2. You drop their driver from future official releases, and point\n      people at the upstream bug report.\nWith active upstream maintainers you get a.4, and everyone's happy.\nWith lazy upstream maintainers you get b.4, and you don't have to\nworry about it.\n\nOne step further: what about path traversal? or security at large?\ndo we trust the driver authors as well to do that properly?\n\nAbsolutely not.  We write a test suite to validate all of the APIs,\nand make it as detailed as we like to ensure we never see anything\nthat worries us or has been reported before.\n\n\n[shared libraries to do the work in drivers]\n\nSo we maintain them, recommend that people use them, have to upgrade\nthem and go through all drivers so they keep up with the latest\nversion?\n\nNo, whoever maintains net/url has you covered on the maintenance side,\nand your test suite has you covered on the updates side.  It's less\nwork than maintaining workaround code in the registry itself.\n\nMaybe I'm just opinionated :-) but I strongly believe that we do\nneed to own some critical stuff inside the registry and that a great\nAPI is a good API that don't let you shoot yourself in the foot.\n\nFair enough, I'm opinionated too ;).  I'm fine making the\nstorage-driver API the great API, but to me \u201cgreat API\u201d means \u201cthe\nsimplest mental model required for efficiency, with good docs and a\ncomprehensive test suite,\u201d not \u201cdoesn't let you shoot yourself in the\nfoot\u201d (although I think that my definition makes it hard to shoot\nyourself in the foot too).\n\nDrivers authors should focus on stuff that has value: provide\nstorage primitives - not implement petty sanity checking.\n\nEncoding your keys so your backend can handle them doesn't sound like\n\u201cpetty sanity checking\u201d to me.  It's not like the current storage API\nis a complicated mess of hard-to-implement features ;).  I think\nstorage driver authors are capable of implementing it as it stands,\nwithout the registry holding their hand.  And I think that a good test\nsuite monitoring for compliance will catch any bugs that they write\nwhile implementing their driver.\nBut all of my pushback hear is just me trying to save you future work\n;).  Until I learn Go, I won't have to work with or maintain the\ninternal API.  If my arguments for removing it aren't convincing,\nthat's fine with me.\n. On Thu, Nov 13, 2014 at 07:25:27PM -0800, Sean Chow wrote:\n\n2014/11/14 09:08:35 [error] 8299#0: *27 no user/password was\nprovided for basic authentication, client: 172.29.88.223, server:\nregistry.exmaple.com, request: \"POST /v1/users/ HTTP/1.1\", host:\n\"registry.example.com:8000\"\n\nIs that usually followed by a successful POST?  I'd guess the client\nis saying, \u201cLet me see if I can POST without auth,\u201d and Nginx is\nsaying, \u201cSorry, you need Basic Auth for that URL.\u201d  That's expected,\nand the next step should be the client saying, \u201cLet me see if I can\nPOST with Basic Auth,\u201d and Nginx saying \u201cthanks, POST accepted.\u201d\n. On Thu, Nov 13, 2014 at 10:12:57PM -0800, Sean Chow wrote:\n\nSure? But why? I have give username/password in ~/.dockercfg but\ndocker pull operation still try to post without auth?\n\nMaybe the endpoint you're requesting (e.g. /v1/_ping) doesn't require\nauth?  If the endpoint does require auth, what kind of auth does the\nregistry want?  Basic?  Digest?  A token from the hub?  It's easy to\njust assume you don't need to auth, and then respond with the\nrequested auth style if the registry asks for it.\n\nWould that be a bug?\n\nIt would be more efficient if you could encode \u201cAlways use\n{Basic|Digest|Token\nrealm=\"auth.docker.com\",service=\"registry.docker.com\"} auth for\nendpoints matching regexp POST http://registry.example.com/v1/users/\u201d\n(docker/docker#9081), but that's probably more complicated than it's\nworth.\n. On Sat, Nov 15, 2014 at 02:39:27PM -0800, Olivier Gambier wrote:\n\nWe might want in the future to \"blacklist\" some specific docker\nversions, or more simply to refuse push from too-old / unsupported\nanymore clients.\n\n:).\n\nAlthough UA sniffing is considered bad practice in the wider-web, it\nmay be the simplest path - but maybe there are other ways to\n\"identify\" a problematic client.\n\nLike \u201cyou asked for a /v1/ URL, but we only support /v2/ now\u201d?  The\ncurrent issue is that the payload checksum semantics changed with\nDocker 0.10, but the client\u2194registry API version wasn't bumped\n([1,2]).\n\nAs for the response, 412 + informative message?\n\nWorks for me.  Is the informative message just a text/plain string\n(\u201cYou asked for a /v1/ URL, but we only support /v2/ now.\u201d)?  Do we\nwant something more structured?\n{\n    \"message\": \"You asked for a /v1/ URL, but we only support /v2/ now.\",\n        \"references\": [\n      \"http://blog.docker.com/2014/04/docker-0-10-quality-and-ops-tooling/\",\n      \"http://docs.docker.com/reference/api/registry_api/#put-image-layer_2\"\n    ]\n  }\n. On Thu, Nov 20, 2014 at 04:05:38PM -0800, Stephen Day wrote:\n\nAll operations against storage driver implementations should\ntimeout, in the registry process.\n\nAnd then what?  Will the registry return a 504 to the client?  Will it\nretry the storage request?  I'd rather just leave the request open and\nleave it up to the client (or an intermediate proxy) to terminate any\nrequest if they're tired of waiting.  It's not like we'll be\nabandoning the connection to the storage driver if its too slow.\n. On Thu, Nov 20, 2014 at 05:35:39PM -0800, Stephen Day wrote:\n\nWe really don't want internal driver operations to hang indefinitely.\n\nWill the libchan connection hang indefinitely?  Is there a heartbeat\nor other means to check that the socket is still functional?  If it's\nnot functional, we'd have to reconnect to the storage driver.  That\nmight mean killing the old driver and spawn a new one (I'm not\nfamiliar with the current socket-creation logic).  In that case I\nthink it makes sense to 503 the registry clients.\nIf the libchan connection is still viable, I think it makes more sense\nto just wait for the slow call to complete.\n\nIts confusing for clients and it takes resources in the registry\nservice.\n\nI have no problem with client code having timeouts for registry\nresponses.  Their connection with the registry is ephemeral, and\nlikely over an unknown network.  The storage drivers on the other hand\nare living in the same container (at the moment), so I doubt you gain\nmuch more than a few additional config settings by adding such\nprotection there.\nIn the event that a client or intervening proxy doesn't time out the\nrequest for us, the resource overhead from a single long-running\nconnection shouldn't be too high 1.\n. On Mon, Nov 24, 2014 at 07:03:59AM -0800, hex108 wrote:\n\nIt seems that it will fail even without my code change.\n\nThe failure is 1:\n$ SETTINGS_FLAVOR=test DOCKER_REGISTRY_CONFIG=config_sample.yml flake8\n  ./docker_registry/lib/index/db.py:12:1: H305 imports not grouped correctly (logging: stdlib, sqlalchemy: third-party)\nThat's because you added the logging import to the third-party import\nblock (with the sqlalchemy imports).  \u2018logging\u2019 is a standard library\npackage, so it should go in a (new for db.py) standard-library block\n2:\nimport logging\nfrom ... import storage\n  from .. import config\n  from . import Index\n  import sqlalchemy\n  import sqlalchemy.exc\n  \u2026\nAlso on the style front, I usually use two blank lines between the\nfinal import and global like your \u2018logger\u2019.  PEP 8 isn't explicit on\nthis, but does recommend two blank lines between other top-level\ndefinitions 3:\nSeparate top-level function and class definitions with two blank\n  lines.\nAlthough it recommends a single blank line between import groups 2:\nImports should be grouped in the following order:\n1. standard library imports\n2. related third party imports\n3. local application/library specific imports\nYou should put a blank line between each group of imports.\nFor strict PEP 8 compliance, that means:\nimport logging\nimport sqlalchemy\n  import sqlalchemy.exc\n  \u2026\nfrom ... import storage\n  from .. import config\n  from . import Index\nBut I don't remember if flake8 distinguishes between local and\nthird-party imports.\n. Looks good to me too.\n. On Mon, Nov 24, 2014 at 01:22:30PM -0800, Stephen Day wrote:\n\nThe current storagedriver.WriteStream call is as follows:\nWriteStream(path string, offset, size uint64, readCloser io.ReadCloser) error\nThis has a few problems:\n1. io.ReadCloser is unnecessary. The lifecycle of reader should be\n   dictated by the caller.\n\n+1\n\n\nThe use of size and io.EOF presents several, overlapping\n   errors. Only one should dictate the course of the write.\n\n\n+1\n\n\nThere is not way to detect partial writes on failure.\n\n\nWhy not?  Can't the storage driver check how much it received and\nreturn an error if it's < size.\n\nGenerally, in Go, its more idiomatic to have a method such as this\ntake a reader and write until it returns io.EOF.\n\nThat's fine inside Go, but it would make it hard if the storage driver\nis connecting to a storage backend that expects the size ahead of time\n(e.g. if the storage backend uses HTTP for pushes, or if its on the\nlocal filesystem and the driver wants to allocate the whole block of\ndisk space ahead of time to avoid fragmentation).  Since we have the\nsize from the client\u2194registry POST, we might as well pass it on for\nthe storage driver to use (or not) as it sees fit.\n. On Tue, Dec 02, 2014 at 08:16:55PM -0800, Stephen Day wrote:\n\n\nWhy not?  Can't the storage driver check how much it received and\nreturn an error if it's < size.\n\nThere is no way for the caller of WriteStream to detect the\npartial write, thus why we add the number of bytes written to the\nreturn values.\n\nThe storage driver is isn't calling WriteStream, the registry is\ncalling WriteStream and the storage driver is implementing it.  All\nthe caller should care about is whether the call succeeded or not, and\na \u2018partial write\u2019 error returned by the driver should cover you there.\n\n\nThat's fine inside Go, but it would make it hard if the storage\ndriver is connecting to a storage backend that expects the size\nahead of time\u2026\n\nGenerally, we won't actually know the size beforehand. This function\nmay be passed an io.Reader directly from an http request. While the\n\"Content-Length\" header may provide a hint, the actual read may be\nshorter or longer.\n\nI'd consider either case (shorter or longer) reads an error.\nIn the event of a short read, I'd return the \u2018partial write\u2019 error,\nand record the actually-read size as part of that error object.  Then\nthe registry could pass that on to the client instead of requiring the\nclient to send a new request to figure out how much was written.\nIn the event of a long read, I'd just drop the whole thing, and\nconsider ignoring the client for a cool-off period.\n\nFrom a security perspective, it can't really be trusted.\n\nWhy is this a trust issue?  If the client says \u201cI'm uploading 100\nbytes\u201d, and I only get 25, I expect a later API call to try and push\nthe remaining 75.  If I get 200 bytes, I say, \u201cthis client is crazy\u201d,\nand I ignore them until they start talking sense.  In no case am I\ntrusting the client's Content-Length blindly.\n\nIf the driver does actually require the size, to support block\nallocation or sized http pushes, it has the option to buffer the\ndata in memory or local disk. The new proposed interface effectively\npasses on the lack of knowledge from incoming http requests. The\nreturned write size can be checked against incoming \"Content-Length\"\nto validate the write.\n\nSure, but that seems wasteful to me.  Why not push the nominal size\nand associated validation check into the storage driver, and let it\nskip the local buffering?  I don't think you gain anything by moving\nthe check to the registry core, and you'd want some benefit to justify\nthe cost of buffering in the storage driver.\n. On Wed, Dec 03, 2014 at 12:21:42AM -0800, Stephen Day wrote:\n\nNone of the driver implementations even use the size parameter or\nuse it inconsistently.\n\nThat's just a documentation/testing issue.\n\nIt's not needed and it makes error handling more complex.\n\nIt doesn't seem simpler to me to handle some errors in the driver, but\nkeep invalid-size error handling in the core.\n\nIt's also confusing in places where its assumed to be the total size\nof the target content at path rather than the write chunk size.\n\nThat's also a documentation/testing issue.  It should be the size of\ndata that the call to WriteStream is expected to deliver.\nFor chunked transfer encoding (where Content-Length isn't set), I'm\nnot sure how you distinguish between \u201cEOF, next chunk size is zero,\nsuccessfully received\u201d and \u201cEOF, connection lost, incomplete\ntransfer\u201d.  However you were planning on doing that in the core, you\nshould be able to take the same approach in the drivers.\n. On Wed, Dec 03, 2014 at 10:44:28AM -0800, Stephen Day wrote:\n\nI think the confusion here is that WriteStream will not be called\nmultiple times for http Chunked Transfer Encoding. The http library\nexposes Request.Body, which is an io.ReadCloser. The transfer\nencoding will be transparent to the request handler.\n\nI'm still not clear on how you distinguish between successfully\nreaching the end of chunked data and a broken connection that leads to\nan incomplete read.  The docs for NewChunkedReader only describe the\nhandling of a successful read (returning io.EOF 1).  In any case,\nhandling partial chunked uploads is going to be the same regardless of\nwhether you check for this error in the driver or in the core.\n\nIf the caller wants to write less data, they can wrap io.Reader in\nan io.LimitedReader, which can be restricted to size.\n\nWhy would you want to do that?\n\nThis puts buffering and data acceptance into the hands of the\ndriver, which has the best chance of correct and performant\nhandling.\n\nIf you pass an explicit expected size, the driver can still decide if\nit wants to buffer or accept the data.  This is just giving the driver\nless information to use when making that decision.\n\nIt also separates accepted bytes from the error type, allowing the\ndriver to control whether partial, full or chunked writes are\naccepted, transparent to the caller.\n\nIs that a decision we want to make on a per-driver level?\n\nI'll be posting PRs for this over the next couple of days. You'll\nsee that it simplifies the write flow and makes error handling a lot\ncleaner.\n\nOk.  Getting something more concrete to talk about will probably help\n;).\n. On Wed, Dec 03, 2014 at 10:44:28AM -0800, Stephen Day wrote:\n\nI'll be posting PRs for this over the next couple of days. You'll\nsee that it simplifies the write flow and makes error handling a lot\ncleaner.\n\nLooking at #820, it seems like the test suite now has (with 2037b1d6,\nUpdate testsuite with storagedriver interface changes, 2014-12-03,\n1):\nnn, err := suite.StorageDriver.WriteStream(filename, 0, bytes.NewReader(contentsChunk1))\n  c.Assert(err, check.IsNil)\n  c.Assert(nn, check.Equals, int64(len(contentsChunk1)))\ninstead of:\nerr := suite.StorageDriver.WriteStream(filename, 0, 3*chunkSize, ioutil.NopCloser(bytes.NewReader(contentsChunk1)))\n  c.Assert(err, check.IsNil)\nWhen I'd prefer:\nerr := suite.StorageDriver.WriteStream(filename, 0, chunkSize, bytes.NewReader(contentsChunk1))\n  c.Assert(err, check.IsNil)\nThe approach in 2037b1d6 doesn't seem a lot cleaner to me.  Maybe the\ncleanliness savings are coming somewhere else?\nLooking at ab9570f8 (Migrate filesystem driver to new storagedriver\ncalls, 2014-12-03, 2) and 2ebc373d (Refactor inmemory driver for\nStat and WriteStream methods, 2014-12-04), I see that the filesystem\nand inmemory drivers never used the \u2018size\u2019 argument for \u2018WriteStream\u2019\nat all.  Maybe that's what we should be fixing, by adding size-written\nchecks to the storage drivers?\n. On Fri, Dec 05, 2014 at 09:48:16AM -0800, Stephen Day wrote:\n\n@wking You're missing the point: none of the drivers used the size\nargument correctly. It was confusing, incorrect and\nredundant. Please see #814 for details.\n\nThe point of \u2018size\u2019 is that sometimes you do know ahead of time how\nmuch data the client is intending to send.  Passing that information\nalong to the storage driver allows it to:\n- Pass that along to the backend (if the backend cares),\n- Detect overly large payloads (and reject them as corrupt), and\n- Detect small payloads, likely due to a dropped socket.\nIf all you're looking at is EOF, you can't do any of that.\n\nI'm also confused why you are doubling down on this matter. What are\nyou trying to achieve?\n\nI'm just trying to keep the storage-driver API clean, and the returned\nerror code already seems like a good way to tell the registry if a\nwrite was shorter or longer than expected.  Putting this check in the\nstorage driver code means you don't have to remember to repeat it when\nyou call the storage driver from the core.  And having tests in the\nstorage driver test suite to exercise invalid sizes means you don't\nhave to remember to add the test when you write a storage driver\n(you'll just fail the tests until you add it).  Of course, I'm not\nimplementing code on either side of this API yet, and I'm not a\nmaintainer, so if my arguments don't make sense to you, you're\nobviously free to ignore me ;).\n. On Fri, Dec 05, 2014 at 11:24:57AM -0800, Stephen Day wrote:\n\n@wking WriteStream will really only be used in a few places,\nwhereas common size checks will have be implemented in each\nbespoke driver, \u2026\n\nBut the driver may have more information on why the sizes don't match,\nwhich you lose by squeezing the error down to a single number.\n\n\u2026 resulting in divergent behavior.\n\nNot if you're enforcing consistent behaviour in the test suite.\n\nLarge and small payload detection must be handled in the storage\nlayer and not the drivers, in one place.\n\nThis is not a complicated check, and I've expressed my preference for\nlibraries to share code for stuff that many drivers would want before\n1.  Maybe we'll just have to agree to disagree here ;).\n\nThe issue with size is that this is an incoming stream. The size\nis not known by definition.\n\nI'm still not clear on how anyone detects broken cnked uploads in Go\n2.  For the other cases, you will know the client's Content-Length.\nIt's not \u201ctrusted\u201d, but the idea is that you error out of WriteStream\nif you don't match the Content-Length.\n\nTypically, the driver doesn't have enough information to do proper\nsize checks that don't result in an invalid io.Reader.\n\nThen how are they returning the number of bytes written?  I'm just\nsuggesting the:\nc.Assert(nn, check.Equals, size)\ncheck happen in the driver instead of the core.  It's not a huge\ndifference ;).\n\nThere also needs to be very careful error handling in the case of a\nshort read/write that cannot be handled in the driver (check out my\nTODO in layerupload.go).\n\nThat's 3?  That looks like an issue with the previous length of the\nfile, not an issue with the size of the request body being written.\n\nWe have the same goals. API cleanliness is very important. You're\nfeedback is detailed but if there is specific action you'd like me\nto take based on your feedback, please say so. While I do have the\nbest visibility into this problem at this time, ignoring your\nfeedback would be unacceptable.\n\nMy feedback is to keep \u2018size\u2019 in the API and move the short/long write\ncheck into the driver.\nFri, Dec 05, 2014 at 11:26:52AM -0800, Stephen Day:\n\nAlso, I'm not sure if you noticed when quoting this, but the\nchunkSize argument changed from 3*chunkSize, which would be total\nsize to chunkSize. This is part of the confusion we're trying to\navoid with these changes.\n\nYeah, that was intentional 4.  And confusion is easily avoided\nthrough documentation and testing.\n. @stevvooe reached out to me on IRC and we kicked this around some\nmore.  The disagreement (to pass size to the driver and check there\nvs. getting back the size written and checking sizes in the registry\ncore) boiled down to whether or not the driver to trust data sent from\nthe registry.  I'd prefer a skeptical driver that gets both the\nexpected size and byte stream, so it can verify the read length\nitself.  @stevvooe prefers (for now) a trusting driver that expects\nthe registry to handle the error detection.\nWe'll want to revist if we find a backend that needs the\nContent-Length information up-front and can't handle chunked transfer\nencoding from the driver, or if we decide that drivers should be able\nto detect broken transfers on their own, or if we want to implement\nsome other efficiencies based on the nominal Content-Length.  Maybe\nnone of those will ever happen ;).  In any case, I'm comfortable\nagreeing to disagree here.\n. On Tue, Dec 02, 2014 at 10:56:43AM -0800, Stephen Day wrote:\n\n\nA signature-independent content-addressable digest taking into\n   account the following fields:\n\"schemaVersion\"\n\"architecture\"\n\"fsLayers.blobSum\"\n\"history.v2Compatibility\" (maybe omitting internal fields, like \"created\"\n\n\nI think you mean v1Compatibility, but other than that this is just the\nset of fields that get signed (docker/docker#8093) excepting \u2018name\u2019\nand \u2018tag\u2019.  That makes sense to me.\n\n\nA signature-dependent content-addressable digest that includes\n   the fields from above and the content in \"signatures\". This might\n   actually just be the sha256 hash of the content, given that the\n   signature is dependent on name and tag.\n\n\nYou don't want to add \u2018name\u2019 and \u2018tag\u2019 back here?  And can you explain\nwhy you want to embed the signature-independent content here instead\nof referencing it with a content-addressable hash of the\nsignature-independent content (giving us back thin tags).  Or is this\nan either/or proposal and not a both/and?  In that case, why not take\na both/and approach?\n\nThe main issue with this is that it makes registry garbage\ncollection nearly impossible, because all layers will technically be\nreferenced by all manifests.\n\nCan you elaborate on this?  Manifests should only reference the layers\nin their history, and I see no problem if many tags share references\nto common layers.  In fact, I think shared references like that are a\ngood thing for quota allocation [1,2].\n. On Tue, Dec 02, 2014 at 11:52:05AM -0800, Andy Goldstein wrote:\n\n\u2026 keeping at most n revisions of a manifest for\nnamespace/repo:tag.\n\nHow would you access these older manifests?  Are you imagining new API\nendpoints to get manifests by their content-addressable hash?  I don't\nsee why you can't just prune immediately, and have folks interested in\nmaintaining references create their own thin tags to the material they\ncare about.\n. On Tue, Dec 02, 2014 at 06:44:54PM -0800, Stephen Day wrote:\n\nAfter the push, we can see that all of the layers are still\nreferenced, and therefore not safe for delete, but a0 is\ntechnically orphaned:\n\nAh, I'd delete a0 and layer(1) once a1 had been pushed, since I don't\nsee a point to referencing untagged manifests 1.  Then you don't\nhave this garbage-collection issue.  Pushing and a deleting thin tags\nallows anyone to mark out existing manifests that they want to\npreserve access to (at the cost of sharing the quota load for that\nmanifest and its referenced layers 2).  There's also no need to\naccess a manifest by its ID over the client\u2194registry API, you just use\nthe ID when you create your thin tag, and push/get/delete that tag\nusing the existing API.\n\nI'm not saying that we should never implemented content-addressable\nmanifest storage, but its a lot of bookkeeping for a system should\nbe simplified. The current approach allows the users to control the\ndeletion pattern while still allowing one to keep multiple versions\nof the same image around without a lot of work on either side.\n\nI don't see the additional bookkeeping cost to my proposal, except for\na separation between the content-addressable parts of the manifest and\nthe name/tag/signature parts.  That can happen purely inside the\nregistry, with no need to adjust the client's API calls or the storage\ndriver API.\n\nThe real problem is programmatically associating a with a1 (or\n\"latest\" version) when a notification activates a deployment.\n\nIn my scheme, the lightweight tag body would embed the\ncontent-addressable manifest ID directly 3.  So going from a tag (a)\nto a content-addressable manifest ID (a1) is trivial.  If you don't\nwant to expose this to clients over the client\u2194registry API, you can\nalways inline the content-addressable manifest before returning the\ntag (but clients would still need to be able to calculate the\ncontent-addressable ID if they wanted to create new tags).\n\ncorollary, given an image tag a, what are its equivalent images\u2026\n\nThis is where refcount arrays come in 4, and I think that's handled\nby the layerindex/ stuff that landed with #729.\n\n\u2026 and how can a permanent reference be retained?\n\nJust add a lightweight tag pointing to any content-addressable\nmanifest you want to preserve, and remove your tag when you no longer\nneed access to that manifest.\n. On Thu, Dec 18, 2014 at 02:48:06PM -0800, Stephen Day wrote:\n\nAnother problem with a content-addressable manifest ids is that it\nmuddies the role of a name/tag reference. If name and tag are\nomitted from the calculation of such an id and multiple manifests\nwith different names and tags have identical ids, which manifest\nshould be returned?\n\nCan't we just return the content-addressable manifest without a\nname/tag?  Why does the Docker engine need a name?  Currently we get\nalong fine with arbitrarily generated names like\n511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158.\n\n\u2026 who signs the updated manifest?\n\nOne reason I like thin tags and detached signatures, which you can\npass back to the client so it knows:\n- Docker thinks that image is scratch:latest\n- Trevor thinks that image is wking/empty:5\n- \u2026\n. On Tue, Dec 30, 2014 at 11:50:48AM -0800, Stephen Day wrote:\n\n\nThe \"digest\" of the manifest is the sha256 of the \"unsigned\"\n   portion of manifest, with sorted object keys. This should only be\n   calculated by the registry for the time being.\nPUT operations on the manifest are no longer destructive. If\n   the content is different, the \"tag\" is updated to point at the\n   new content. All revisions remain addressable by digest.\n\n\nThe PUT operation will still be descructive if the unsigned portion of\nthe manifest changes (e.g. new signatures or signers).  I'm fine with\nthat, but thought I'd point it out for completeness.\n. On Mon, Jan 05, 2015 at 05:10:31PM -0800, Stephen Day wrote:\n\nThere seem to be problems with specialized hashes no matter what way\nwe try to cut this up.\n\nAnd the way to avoid this is to just hash the whole thing ;).  If \u201cthe\nwhole thing\u201d is too much (and I think it is), I'd break the manifest\nup into a bunch of individually content-addressable objects (e.g. the\nlayer tarballs, the image metadata, signed name/tag/image\ncertifications).\n\nWe may want to discuss storing the signatures separately from the\nmanifest.\n\nSounds good to me (this is where I started out with docker/docker#6070\nand 1).  I suggested (but didn't elaborate on) additional endpoints\nfor independently distributing opaque signatures too 2.  Of course,\nthose proposals were framed in terms of my preferred thin registry,\nwith all of the validation happening on the client side.  With the\ncurrent thick registry, you'd have to specify a signature format as\nwell as signature endpoints and have the registry validate signatures\nas they're uploaded, but I think all of that has been worked out for\nthe new registry code anyway.\n. On Thu, Dec 04, 2014 at 03:54:11PM -0800, Olivier Gambier wrote:\n\nThe UA string must contain productname/semver.\n\nTo what end?  I'd rather version the API instead of relying on\nUser-Agent parsing\u2026\n. On Thu, Dec 04, 2014 at 04:07:23PM -0800, Olivier Gambier wrote:\n\nYou can't version APIs to workaround client bugs.\n\nNo, but you can have a test suite so that clients can validate\nthemselves before they cut releases.  Then you can explicitly\nblacklist User-Agents for known-bad clients that slipped through a\nhole in the test suite.  Or just cut point releases in the clients\nfixing those bugs.\n. On Thu, Dec 04, 2014 at 04:35:53PM -0800, Olivier Gambier wrote:\n\nCertainly - now, on-top of that, what do you think is inherently bad\nin requesting clients to have a parsable UA string?\n\nRequesting is fine, requiring is not, and you said \u201cmust\u201d. ;) See\n375.\nThu, Dec 04, 2014 at 04:37:00PM -0800, Olivier Gambier:\n\n\nThen you can explicitly blacklist User-Agents for known-bad\nclients\n\nSure. So, let's require clients to have a parsable user-agent string\nso that we can identify them. Oh wait... :)\n\nI'm not suggesting we parse the User-Agent, I'm suggesting we use\nexact-string matching so we can say \u201cclient $x is known to be buggy.\nYou should upgrade to $y, see $z\u201d, and fill in the values from:\nBUGGY_USER_AGENTS = {\n    \"whatever version\": {\n      \"fixed-in\": \"point-release that fixed the bug\",\n      \"reference\": \"link to the bug report\",\n    },\n    \"some other UA\": {\u2026},\n    \u2026\n  }\nSo if a client was broken in 1.0.0, 1.0.1, and 1.0.2 you'd have\ndifferent entries in BUGGY_USER_AGENTS for each version.\n. On Thu, Dec 04, 2014 at 04:54:42PM -0800, W. Trevor King wrote:\n\nI'm not suggesting we parse the User-Agent, I'm suggesting we use\nexact-string matching\u2026\n\nThinking about this some more, I think my main issue is that detecting\nbuggy clients shouldn't really be the registry's job.  It is our job\nto appropriately 4xx any invalid requests, and give useful error\nmessages, but it's the client maintainer's job to fix any invalid API\ncalls.\nI'm not sure if the 1.0 docker clients are still getting point\nreleases (1.0.2, 1.1.3, etc.), but if they aren't being maintained by\ntheir authors, I don't see why the registry needs to try and support\nthem.  Users should install and run a version of the client that is\nbeing maintained, or run the risk that their client is irrecoverably\nbuggy.  If you want to give bug reports on the fly, you can have a\nclient that calls home periodically (and lets users opt-out of those\ncallbacks if they want), to check for newer point release or\nend-of-maintenance notices.  For example, Node's update-notifier 1\nmakes it easy to do this sort of thing in JavaScript clients.\n. On Thu, Dec 04, 2014 at 08:02:12PM -0800, Olivier Gambier wrote:\n\nUA strings usually embed machine dependent information (OS or\nplatform name / version, for example), making it hard / moot to aim\nfor exact string match, so, you will match \"for a substring\" anyhow.\n\nAh, good point.  You could presumably parse out the product-version\nfollowing RFC 7231's syntax 1, and use that to blacklist.  It would\nbe more expressive if the client used semantic versioning, but I doubt\nyou'll get any client to semantically version based on its\nregistry-API calls (which is backend stuff for the client).  They\nshould be bumping their semantic based on changes to their front-end\nAPI.\nIn any case, RFC 7231 lists User-Agent as a SHOULD, so I think any\nregistry-side User-Agent suggestions should be along the lines of \u201cif\nyou want the registry to try and work around your clients bugs, you\nshould use the User-Agent header in your requests with\nRFC-7231-compatible syntax\u201d.  Then clients can chose to opt-in (or\nnot).\n\nAbout the fact that older client versions are not \"maintained\"\nanymore, it's not on us to decide that people should stop using\nthem, that distros should stop shipping them, or that it's on\nsomeone else to maintain them and make them evolve\u2026\n\nNope, that's clearly up to the users and distro packagers.\n\n\u2026 we are to provide a service that support every software that did\nimplement the protocol at a point and that still has a significant\n(<- define) user base (well, at least that's what I think :-)).\n\nThis is the part that doesn't make sense to me.  It's easier to have\nthe compatibility fixes be patches to the buggy code that's not\nfolling the API spec.  If it's a client-side bug, it should be a\nclient-side fix.  I certainly don't expect you to cut and deploy a new\nregistry version just because I wrote a buggy client.  If users want\nto continue to use unmaintained clients, or to avoid upgrading to a\nnew patch-release for their client, I don't see why that's our\nproblem.\n. On Fri, Dec 05, 2014 at 10:46:30AM -0800, Olivier Gambier wrote:\n\nImagine there is a library released (for example: python 2.6.X) -\nimagine that library is the one shipped by a famous and widely-used\ndistro (for example: centos).\n\nI think this example is a bit off the mark, unless folks are putting\npackage/version information in their User-Agent for everything in\ntheir stack (and it's easy to create deep stacks now, e.g. the Docker\nclient depends on lots of libraries, the go language, the host kernel,\nthe host hardware, \u2026).  There could be bugs anywhere in the stack, and\njust knowing the package/version on top doesn't help much.\n\n\nI can try to have upstream python fix their 26 stuff\u2026 just use\n  py27\nI can try to have centos upgrade their release\u2026 just use a more\n  recent version of our distro\n\n\nThese both sound reasonable to me.  If you're going to use software,\nyou should use maintained software.\n\n\nI can instruct my users to manually install another python version\n  (not going to work well, as users are not always that smart, and\n  in the first place rely on their distro to do that kind of thing,\n  and as probably users choose that distro in the first place for\n  support and stability) - users are going to argue: your software\n  is not compatible with my distro, why should it be on me to do the\n  effort?\n\n\nIf their distro packagers are no longer maintaining the packages for\ntheir version of the OS, then they're probably not choosing that\ndistro for support ;).  If you want to run and old software/distro for\nstability alone, that's fine, but part of that stability will be\nliving with any bugs that you're not fixing yourself.\n\nIn our world, it's about taking action to fix problems - and\nusually, no-one does (there is always someone else to \"blame\" for\nit). It's my opinion that good software makes efforts to put users\nfirst, and to try and solve their problems, especially in that \"grey\nzone\" where no one else wants to take responsibility for that...\n\nWe can't take action in the client?  Installing a new client should be\nwithin mose user's reach, and it's certainly easier than upgrading\nyour system Python.  Then you can work around the issues in the\ncontext of the buggy stack, which makes writing the workarounds\neasier.  If the user refuses to install a patched client, despite it\nbeing a nice, Dockerized bundle (or whatever), and has no other\noptions because they're sticking with unmaintained software on an\nunmaintained distro, there's not much you can do.  I doubt there are\nsignificant numbers of users who fall into that category, but what do\nI know ;).  It's certainly not worth forcing everyone to set a\nUser-Agent header just to support abandoned clients.\n\nSorry for the long rambling - hope that clarifies :)\n\nNo problem.  Long, rambling discussions and clarity now make it less\nlikely that we are surprised by the side-effects of these decisions in\nthe future ;).\n. On Fri, Dec 05, 2014 at 11:46:49AM -0800, Olivier Gambier wrote:\n\n\nIt's certainly not worth forcing everyone to set a User-Agent\nheader just to support abandoned clients.\n\nWell, I dare to disagree: shipped and widely used clients, even\nabandoned, are worth supporting in the interest of the user - and if\ndevelopers can't be bothered to follow a very lightweight\nrequirement, how could they be bothered to fix actual bugs in their\nprotocol implementation?\n\nThey are two different sets of developers.  The ones who write and\nabandon the buggy clients (who should be setting the User-Agent as you\nsuggest, or doing whatever they can to make it easier for us to clean\nup their mess), and the ones who write working clients that just don't\nhappen to set the User-Agent you expect (e.g. #375).  If you really\nwant to go the extra mile for the first set of devs, that's fine, as\nlong as the approach doesn't make life more difficult for the second\nset of devs.\n. On Fri, Dec 05, 2014 at 12:06:13PM -0800, Olivier Gambier wrote:\n\nIt's not so much about developers. It's about taking ownership and\nresponsibility in order to have things work for the users.  Even\nyour \"good\" developers will refuse to be held accountable for what\ndistros ship, or for what users choose to base on or run in the end.\n\nSure.  In the end, getting a functioning client is the user's\nresposibility.  So read 1 as:\n\u201cIf you really want to go the extra mile for backwards users, that's\n  fine, as long as the approach doesn't make life more difficult for\n  the second set of devs [who take responsibility for using the API\n  correctly but don't want to set their User-Agent].\u201d\n\nEither way - this here is a detail compared to the rest of the\nwork going on...\n\nAgreed, but it's a detail we've gotten wrong before.  I'm just trying\nto avoid repeating history.\n. On Wed, Jan 21, 2015 at 03:58:42PM -0800, Stephen Day wrote:\n\nThe goal of this ticket is satisfied for v2 registry with User-Agent header from docker daemon:\ndocker/1.4.1 go/go1.3.3 git-commit/5bc2ff8 kernel/3.16.7-tinycore64 os/linux arch/amd64\n\nThe problem wasn't the Docker daemon, it was the registry choking when\nother clients were connecting (see #375).\n\nWe can easily ban with buggy clients with by detecting docker\nversion or git hash. We also have Docker-Distribution-API-Version\nto communicate from server to client.\n\nWe had been using User-Agent strings to work around buggy clients, and\nserver\u2192client communication isn't going to fix a buggy client.\nAs I've said before 1, I'm fine with registry-side workarounds tied\nto whatever you like, so long as they're optional.\n. On Wed, Jan 21, 2015 at 06:30:33PM -0800, Stephen Day wrote:\n\n@wking Using a User-Agent to deal with a buggy client is usually a\nbad idea.\n\nI absolutely agree.\n\nHowever, we currently have the hooks to do this if it comes to that.\n\nSo long as the hooks don't break the registry if the User-Agent isn't\nproperly formed, I'm happy.\n. On Fri, Dec 05, 2014 at 02:13:57PM -0800, Olivier Gambier wrote:\n\n@wking for good measure\n\nI'm in favor of distributing an image with Nginx all set up for the\nregistry.  That would avoid issues like \u201cI've upgraded Nginx. Now it\nworks\u201d.\n. On Mon, Dec 15, 2014 at 10:44:57AM -0800, Phil Estes wrote:\n\nThis code is a first step in making the current registry aware of\nimages for non-Intel architecture.  While os==linux today, if we are\nadding architecture we might as well add both arch and os for\nfuture use by Microsoft and/or others where os != linux.\n\nSounds good to me, but I think getting the new model documented 1\nshould happen before the implementation here.  The public hub uses a\ndifferent search implementation, and we don't want the two\nimplementations growing too far out of sync.\n. On Mon, Jan 05, 2015 at 06:22:14PM -0800, Olivier Gambier wrote:\n\n@shin- @wking what do you think?\n\nI still want the search result structure in the spec 1.  Having\nmultiple quasi-official search implementations with different result\nstructures is just going to be confusing.  Sharing the same endpoint\nis not enough for client compatibility ;).\n. On Thu, Dec 18, 2014 at 09:33:22PM -0800, Stephen Day wrote:\n\n\nAutomatically generate V2 API specification\n\n\nIt looks like the only connection between the internal specification\nfor routeDescriptors and the implementation is:\n\n\nfor _, descriptor := range routeDescriptors {\nrouter.Path(descriptor.Path).Name(descriptor.Name)\n}\n\n\nIs that a tight enough binding to be worth binding the spec into the\nimplmentation?  Or is it worth using someone else's spec language and\nhaving independent Swagger docs like docker/docker#9598 was starting\non?  On the other hand, machine parseable specs that exist (these) are\nbetter than machine parsable specs that are still in my queue\n(docker/docker#9598) ;).  And apologies for the slow progress on that\nfront, it's been a busy few weeks :p.\n@thaJeztah also pointed out go-restful 1, but it doesn't look like\nyou've used that here.  Care to weigh in on why not?\n. On Mon, Apr 07, 2014 at 12:13:02PM -0700, Sam Alba wrote:\n\n\n@@ -0,0 +1,34 @@\n+from future import absolute_import\n\nCan we keep the coding style and structure as the other modules?\n\nlib/index/sqlalchemy.py is importing the global sqlalchemy module, so\nI think we need the absolute import to avoid confusing Python 2.  I\ncan test without this line to confirm though.\n. I needed to drop --debug so I could set --preload.  I can spin those commits out into a separate PR if you'd like.\n. I'm just declaring the public API here (I saw a few other __all__ declarations in this repo).  I'm fine dropping it if you don't like it, but registry.search certainly seems like a library to me ;).\n. On Mon, Apr 07, 2014 at 01:58:12PM -0700, Sam Alba wrote:\n\n\n@@ -0,0 +1,34 @@\n+from future import absolute_import\n\nOh I get it now. I would rename the file lib/index/sqlalchemy.py\nthen. I got many side effects with this in the past.\n\nDo you have a preference?  index.sqlalchemyindex seems redundant :p.\nThis sort of namespacing is what absolute imports are designed to make\neasy.  If we want consistency, I'm happy to add absolute_import lines\neverywhere ;).  All of the other changes needed to support that will\nhave to land before we can support Python 3 anyway.\n. On Mon, Apr 07, 2014 at 01:57:16PM -0700, Sam Alba wrote:\n\nI think having __all__ or not won't change anything\n\nOk, I'll rebase it out tonight.\n. On Mon, May 12, 2014 at 08:04:15AM -0700, Joffrey F wrote:\n\n\n@@ -1,20 +0,0 @@\n-test:\n\nAssuming this is a symlink, seems like it would be cleaner to just\nremove those files (especially since they were just here for\nreference in the first place)\n\nMaybe you think they were only there for reference, but I don't see\nwhy someone couldn't have been using:\nDOCKER_REGISTRY_CONFIG=config_test.yml\nor whatever when starting their registry.  I don't mind removing the\nfiles, since I still think we're in the \u201cusers can be expected to read\nrelease notes\u201d stage of development.  However, @dmp42 at least seems\nto prefer more backward compatibility 1.  Maybe the risk of breakage\nfor folks using DOCKER_REGISTRY_CONFIG to point at example configs is\nacceptable, and we don't need that much backward compatibility?\n. On Tue, Aug 12, 2014 at 09:27:49AM -0700, Olivier Gambier wrote:\n\n\n@@ -23,7 +23,7 @@\n     app.debug = True\n     app.run(host=host, port=port)\n     # Or you can run:\n-    # gunicorn --access-logfile - --log-level debug --debug -b 0.0.0.0:5000 \\\n\nWhy remove the --debug switch? \n\nBecause it's useless ;) [1,2].\n. On Tue, Sep 16, 2014 at 12:14:18PM -0700, Andy Goldstein wrote:\n\n\n+3. Create folders docker-registry and docker-registry/extensions in your package \n\n@dmp42 I tried docker-registry but it looks like it needs to be\ndocker_registry with an underscore. Can you review & confirm?\n\nI'll confirm that for him (should be an underscore ;).\n. On Thu, Sep 18, 2014 at 06:23:17PM -0700, Andy Goldstein wrote:\n\n\n'--graceful-timeout', env.source('GUNICORN_GRACEFUL_TIMEOUT'),\n-        '--reload', False if env.source('SETTINGS_FLAVOR') == 'prod' else True,\n\nFYI this line has been removed from the master branch and should be\nremoved from this PR as well - it breaks the docker-registry wrapper\nscript.\n\nRebasing onto something after #548 landed should do the trick.\n. On Mon, Oct 13, 2014 at 04:43:12PM -0700, Matthew Fisher wrote:\n\nIn terms of previous filesystem driver configuration, there was a\nlot more than just the root directory. For example, in the S3\ndriver, you had to set up your access key/secret key to connect to\nthe S3 bucket. For the swift driver, you needed to specify your\ntenant name, region name, your keystone API key, etc. How will this\nfit in with this implementation?\n\nDetached libchan storage drivers can just read their own config files\nor CLI arguments or whatever when they spin up.  For same-process\ndrivers, I'm fine with either the current \u201cjust pass through the whole\nconfig hash structure\u201d or a similar approach using environment\nvariables.\n. On Tue, Oct 21, 2014 at 02:24:58PM -0700, Matthew Fisher wrote:\n\n\n+## You want some shiny new feature to be added?\n\nGeneral contribution guidelines for styling, formatting and testing\ntheir code may be useful here, such as go fmt, go vet, go test\netc.\n\nCONTRIBUTING.md is read by everyone.  I think the style guide and test\ndocs should live in separate files, since they're not general\ninterest.  We should definitely link to them from here though.  On the\nother hand, if the advice is so short as:\nRun for fmt to check you formatting and go test to run the test\n  suite.\nthen having it inline here is fine.  (I'm not actually sure what go\nfmt does, maybe it automatically fixes your formatting ;)\n. On Tue, Oct 28, 2014 at 03:04:32PM -0700, Matthew Fisher wrote:\n\nRight. This is a big win for specific implementations so there's not\na large hassle to manage a Move method in downstream drivers. I'm\nsure drivers would appreciate if a move operation was supported. I\ndon't see a big reason to remove this, so I'm happy with keeping it\nin.\n\nI'm not following this.  What is the big win, who is appreciating\nMove, and why?\nIf a storage-driver author wants to implement a move endpoint, I have\nno problem with that.  If the docker-registry wants to use such an\nendpoint (optionally?), I'd like to understand why we need it.  I\ndon't see a reason why docker-registry would need a move endpoint, so\nI'd rather leave it out of the API, and have storage-driver authors\nnot need to bother with it.\n. On Tue, Oct 28, 2014 at 03:23:37PM -0700, Olivier Gambier wrote:\n\nWe are going to use move in the registry, a lot. Tarsums / checksums\nneed to be computed, and the file needs to be complete. So, the\nlayers will be moved to their final destination once they are\ncomplete.\n\nI'd rather offload that implementation to the storage driver itself.\nThe registry just needs to say \u201cI'm going to stream $COUNT bytes [to\n$PATH]\u201d (with the presense of $PATH depending on whether or not the\nstreaming storage is content-addressable).  Then the storage driver\ncan handle that however it likes, to avoid accidentally serving a\npartially-complete file when someone asks for $PATH.\n. On Tue, Oct 28, 2014 at 03:48:10PM -0700, Olivier Gambier wrote:\n\nMaybe we don't hear the same thing with\n\"content-adressibility\". This is about being able to retrieve a\ngiven binary payload from an id that can be computed again from that\nbinary payload. This is what tarsum is for, and I simply fail to see\na point in asking driver author to copy the same code over and over\nagain in every implementation.\n\nRight.  To me \u201can id that can be computed again from that binary\npayload\u201d sounds like \u201cwe don't need move\u201d.  If you want to specify\nthat id for the initial write, that's fine 1.\n. On Mon, Nov 03, 2014 at 01:22:40PM -0800, Matthew Fisher wrote:\n\nWhat would be the use case for using from and size? How do you\nsee them being used in the registry? If we're concerned about having\nthe prefix at a high enough level being slow, we could make it\noptionally non-recursive:\ngolang\nCount(prefix string) (int, error)\nList(prefix string, recursive bool) ([]string, error)\nThat should make List(\"/\", false) only display [\"images\",\n\"repositories\"] and List(\"/images\", false) significantly less\ntime-consuming than recursively listing * every* directory under\n/images.\n\nAnd List(\"/repositories\", false) would list all the namespaces?  I'd\nrather say:\nList(\"namespace/\", 0, 100)\nfor \u201cgive me the first hundred namespaces\u201d.  I'd avoid recursion by\nusing namespaced keys (intead of heirarchical keys):\nnamespace/alice\n  namespace/bob\n  namespace/library\n  \u2026\n  repository/alice/busybox\n  repository/bob/debian\n  repository/library/debian\n  \u2026\n  tag/alice/busybox/0.1\n  tag/bob/debian/latest\n  tag/library/debian/latest\nso you could iterate through namespaces (with a \u201cnamespace/\u201d prefix),\nor through repositories in a namespace (with a \u201crepositories/alice/\u201d\nprefix), or through all repositories (with a \u201crepositories/\u201d prefix),\n\u2026\n. On Mon, Nov 03, 2014 at 01:56:15PM -0800, Brian Bland wrote:\n\nThe S3 list equivalent GET Bucket supports max-keys and a marker\nargument, which is the last key of then previous result set, or the\nkey before the first that should be returned from the\ncall. Implementing the interface with List(prefix string, size int,\nfrom int) with this model seems like it would require loading all\nresults up until the from argument so that we can determine what\nthe appropriate marker is.\n\nIn the abstract, I'm fine with:\nList(prefix string, size int, [from string]) ([]string, error)\nwhere \u2018from\u2019 is an optional name used to determine the initial offset\n(e.g. List(\"/repositories/\", 100, \"bob\").  I'm not sure how easy\nthat would be to implement in Redis or Elasticsearch or other folks\nthat do support the integer offset.  Can anyone think of an API that\nwould allow efficient subset lists in both backends?\n. On Mon, Nov 03, 2014 at 02:13:37PM -0800, W. Trevor King wrote:\n\nIn the abstract, I'm fine with:\nList(prefix string, size int, [from string]) ([]string, error)\nwhere \u2018from\u2019 is an optional name used to determine the initial\noffset (e.g. List(\"/repositories/\", 100, \"bob\").  I'm not sure how\neasy that would be to implement in Redis or Elasticsearch or other\nfolks that do support the integer offset.\n\nAh, it looks like you could implement this in Redis if you used sorted\nsets 1 and ZRANGEBYSCORE 2 with scores that ensured alphabetic\nsorts.\n. On Mon, Nov 03, 2014 at 03:31:25PM -0800, Stephen Day wrote:\n\nShould there be a ParseVersion function? Will the YAML library\nautomatically extract this type?\n\nOr have the version be a [major, minor] list 1.\n. On Mon, Nov 03, 2014 at 03:30:33PM -0800, Brian Bland wrote:\n\nOne thing we could do is change the method signature to List(prefix\nstring, count int, interface{} marker) (keys []string, nextMarker\ninterface{}, err error) and return an untyped (interface{})\n\nI'm fine with this opaque type approach, since I don't see a need to\nbe intelligent about where you start a list (except for the key\nprefix).  It's just a way to pace yourself while you iterate through a\nlot of hits.\nIf we decide to drop the request/response form, we could also list\neverything but with a streaming response.  In Python, that would mean\nthe API returned a generator [1].  That basically hard-codes size to\n1, hides the opaque cursor inside the returned generator, and leaves\nit up to the backend whether or not it wants to cache upcoming entries\nfor efficiency.\n. On Mon, Nov 03, 2014 at 07:05:04PM -0800, Stephen Day wrote:\n\nWithout introducing problems with missing or inconsistent directory\nlistings, the variation in the drivers makes getting a reentrant\ninterface right problematic.\n\nUsing either a marker string 1 or an opaque marker type 2 seems to\nsupport all proposed backends.  With the marker stored in the\nregistry, the storage backend doesn't need to keep track of open list\ncalls, so a reentrant interface is easy.  Or am I missing something?\n\nThe other issue here is that whether or not these calls are\nreentrant, you end up allocating the same amount of memory. Thus,\nlarge directories will cause latency in registry requests no matter\nwhat.\n\nHow so?  ZRANGEBYSCORE is O(log(N)+M) with N being the number of\nelements in the sorted set and M the number of elements being\nreturned.  That scales pretty well.  More importantly, it lets you\nstart streaming results from the registry to the client before you've\nfinished listing the whole directory.\n\nWe can always revisit this in the future if we've made the wrong\ndecision today.\n\nTrue, but we're already having trouble with the current layout (#614).\n. On Mon, Nov 03, 2014 at 09:07:51PM -0800, Stephen Day wrote:\n\n\nUsing either a marker string [1] or an opaque marker type [2]\nseems to support all proposed backends.  With the marker stored in\nthe registry, the storage backend doesn't need to keep track of\nopen list calls, so a reentrant interface is easy.  Or am I\nmissing something?\n\nYes, the marker or index will support most backends, but the\nbehavior may be different on updates to that directory. With a\nmarker string, an insert before the marker after the first call will\nlead to a missed entry.\n\nThat entry wouldn't have been part of the results from an initial\nlist-all call anyway.  I'm fine skipping over it during an iterative\nlist.\n\nWith an index, one may have duplicate entries.\n\nSo just have the registry remember the last entry it got (as well as\nthe opaque marker), and then skip anything <= that entry when parsing\nthe next batch of entries.\n\n\nHow so?  ZRANGEBYSCORE is O(log(N)+M) with N being the number of\nelements in the sorted set and M the number of elements being\nreturned.  That scales pretty well.  More importantly, it lets you\nstart streaming results from the registry to the client before\nyou've finished listing the whole directory.\n\nThat's not the case I'm worried about. If we want to iterate over\nthe entries of a large directory to support an API operation, the\nlatency of the registry request will still be proportional to the\nsize of the directory, reentrant or not.\n\nNo, if the request is \u201cgive me a list of tags and their associated\nimage IDs so I can pull them\u201d (#614), you can start writing back to\nthe client immediately, without waiting for the tag list to complete.\nThat's just a:\nstorage \u2192 registry \u2192 client\npipe, and there's no reason to block the registry \u2192 client leg.\n\nLet's proceed with this simplified API and table this discussion,\nfor now. We have collected some really good suggestions but we\nreally need to look at how we can avoid this bottleneck. Once the V2\npaths are laid out, we can come back and see if we really need the\nreentrant API call.\n\nWell, the API is versioned, so it shouldn't be too hard to adopt this\nlater.  Still, it doesn't seem that complicated to me, and I think\nwe'll need to adopt something that lets us stream results.  This is\nwhat other storage engines already support for streamed listings, so I\nexpect it's a reasonable choice for how to structure this ;).\n. ",
    "dmp42": "We can now close the oldest registry issue \\o/\nhttps://pypi.python.org/pypi?%3Aaction=search&term=docker-registry-&submit=search\n. @bacongobbler drivers maintainer have to implement the driver Base interface as seen here: https://github.com/dotcloud/docker-registry/blob/master/depends/docker-registry-core/docker_registry/core/driver.py and nothing more\ndocker-registry-core is strictly versioned (and the dependency in the driver follows that), and adheres the API exposed by the driver Base - it won't change if we can avoid it, and if it does, I will communicate timely on that.\n. @ankushagarwal \nAddressing this is part of the new golang-registry (https://github.com/docker/distribution).\n. Hi all,\nThis exchange is not productive IMHO.\nYou are right @petarmaric, we should handle things in a better way.\nAdmittedly, it's not always easy, and admittedly, sometimes we suck at it.\nMind you, I believe we are successful at it most of the time, and you will find out that https://github.com/docker/distribution is indeed a place where constructive input and fast moving development happens, for the benefit of everybody.\nLet me remind a couple of things to all people here:\n- this repo here is clearly marked as deprecated, and has been for more than six months, and I'm still wondering why people would keep commenting here while this is plastered all over the repo, and while I myself barely read notifications anymore (and please check https://github.com/docker/docker-registry/graphs/contributors before assuming I'm just a random joe around here). This here is the home of the legacy python, v1-protocol registry - no discussion or bug report is likely to lead to any other response than: please move to the v2 protocol based, golang implementation (here again: https://github.com/docker/distribution)\n- priorities in an open-source project are driven by the willingness of people who actually write code. If you don't write code, that's perfectly ok, and you are definitely an important part of the community, and yes, your input is definitely the basis on top of which we are building the development effort, but now: blind, serial +1s, or blanket statements about what is supposedly easy to do are definitely not going to lead to a magical resolution - on the contrary, these are just adding to the developers frustration, especially when plenty of discussion already took place\n- if you are still an active user of the v1, python registry, there are plenty of home grown solutions to garbage collect your storage: please just click https://github.com/docker/docker-registry/issues?q=is%3Aopen+is%3Aissue+label%3Adelete and look around\n- if you are interested in the v2 registry, please look around here:\n  - https://github.com/docker/distribution/issues/743\n  - https://github.com/docker/distribution/issues/422\n  - https://github.com/docker/distribution/pull/508\n  - https://github.com/docker/distribution/pull/572\n  - https://github.com/docker/distribution/pull/677\n  - and finally here: https://github.com/docker/distribution/blob/master/ROADMAP.md#deletes\n- if you have some constructive feedback to provide as to why you need (?:API-delete|garbage-collection|specific-usecase), please do so, by expressing a clear, motivated use case, and be opened to suggestions to address your actual concerns. I can assure you that it will be taken into account.\n- if your feedback is just \"I need to delete wtf you morons delete is so important what are doing you are doing nothing\", I can assure you I will close your ticket on sight\n- if you still think the official contributors here suck at it, that's fair: again, you will find out that major contributions are made by the community, and your code will be reviewed in a timely and fair fashion\nI would like to end this with a strong and positive note: a community is a whole, and you are an integral part of it. If someone is not acting up to your standards, be the bigger man and make this community better by easing the dialog instead of fueling a flame war.\nI'm closing and locking this issue, and wish this message goes through all participants here.\n. @wiwengweng you have a problem with ubuntu then.\nSee over there for similar issues: http://ubuntuforums.org/archive/index.php/t-2131760.html\nAs for the registry itself, this is why we urge people to use the official docker container instead of trying it on their own :-) \n. @wiwengweng I don't quite get it. Why do you want to install libssl-dev in the first place? I don't think we require it any longer, do we?\n. @wiwengweng yeah, you are reading an old blog post.\nAbout your other questions please start by giving the official doc a read: https://github.com/docker/docker-registry/blob/0.8.1/README.md - and save yourself the trouble, use the registry container.\nIf you need help, go ahead and ask on irc (#docker), the community is usually quite responsive - github tickets on the other hand are better used for issues.\nHope that helps. \n. Hi @mzdaniel \nI'm sorry this wasn't merged earlier... and given the extent of the changes on the next version (\"plugins\" branch), it would be difficult to get this in without a major rewrite - especially since (and this is the good news) the next version does something very similar (monkeypatch redis, and exhaustively test the lru cache engine).\nGiven all that, I'm going to close this PR - and would like to thank you again for your work.\nBest.\n. Hi there @noxiouz \nWe are looking for someone to re-adopt this code. Details in #359.\nBest.\n. Hi @bacongobbler \nWe are looking for someone to \"re-adopt\" the swift code. See #361 for details, if you are interested.\nThanks!\n. Hi @brendandburns \nWe are moving to a plugins architecture that will let the community author storage driver on their own and publish them as separate pip packages.\nWhile cleaning the code through that end, I repackaged the gcs driver as one of these new \"plugins\".\nThe repo is here: https://github.com/dmp42/docker-registry-driver-gcs\nUnfortunately, I don't have a GCS paying account, and it seems that you can't create a bucket with the trial (!!??$$??google!!).\nAlso, I couldn't find a way to mock it.\nSo:\n- right now, I'm not sure if it's working or if it is broken - several earlier cleanups likely broke it IMO - and obviously running the tests doesn't make sens without a working GCS\n- we are looking for someone to re-adopt that code and maintain it\nThe package is squeaky clean. It's all tox-ed / travis-ed and ready to be pypi-ed :-) - all that misses is some google love, and someone to run tox and fix the problems / documentation.\nIf you are interested in taking this over, ping me here and I'll transfer the github repo to you.\nThanks a lot!\n. Hi @brendandburns \nThanks for this! \nProject id: opportune-box-582\nBest\n. Hi @brendandburns \nIt's done - now you got my credit card :-)\n. @fommil I would need more details than that.\ndocker version and docker info plus docker daemon logs (in debug mode) is a bare minimum.\nAlso, exact error message.\nFinally, please open a new issue, since yours is obviously unrelated.\nThanks.\n. People:\nWithout:\n- docker info\n- docker version\n- command line used to launch the registry\n- docker daemon logs\n- registry logs\n... it's just impossible to help you.\nFurthermore, \"EOF on push\" is vague, and can refer to a number of very different conditions (one of them having been fixed, and this is what this specific report is about). \"Same for me\" / \"+1\" and/or \"Still seeing this\" unfortunately will simply be ignored, as it's likely not the same bug, and as there is no helpful information to diagnose it.\nSuggestions:\n- at least provide the aforementioned informations\n- if you realize this is not the bug described here, please open a new issue\nThanks!\n. Likely fixed by #135 - and we don't use cookies anymore.\n. @mmalecki do you think the standalone mode addresses your concerns?\n. @mmalecki if it doesn't please reopen this.\nThanks!\n. This is an ongoing engine issue, documented here: https://github.com/docker/docker/issues/7291\n. Hi @go1dshtein \nSorry for the long delay on this - we had a lot to figure out with the \"plugins\" landing.\nSo, you can now author your driver as a separate pip package. This is very straightforward. You can have a look here for some documentation:\n- https://github.com/dotcloud/docker-registry/blob/master/CONTRIBUTE.md#storage-driver-developer-howto (and more generally the CONTRIBUTE document)\n- pretty much, all you need to do is extend the driver.Base class from registry-core - most of your code here can be kept as-is\n- get inspiration from one of the existing drivers - I recommend looking at the elliptics driver (https://github.com/noxiouz/docker-registry-driver-elliptics) or at the file driver which is located inside registry-core\n- ping me when you are ready and if you need review / help / guidance\nI'm going to close this PR itself, as this is not going to land in the registry, but I definitely encourage you to author it and push it on pip!\nThanks a lot for your work, and hope to talk to you soon about your new driver!\n. The documentation got updated. The recommended way is to pull and run the docker registry image. The manual installation has notes about red hat, xz (https://github.com/dotcloud/docker-registry#on-red-hat-based-systems).\nI believe this is fixed - but if you feel it's still not, please reopen this.\n. You probably want to check #363 \n. @wking still an issue now that we repackaged everything?\n. Ok, closing then. Thanks.\n. @coderlol so, this should be working on the mainline, current registry. If you still have problems with this, please reopen.\nThanks!\n. @chilicat @samalba is this still an issue? Thanks!\n. Closing then. Please reopen if you guys feel this deserves another look.\n. There is now a console_script associated with the setuptools package (\"docker-registry\"), and it does  use -k gevent correctly.\nThe dockerfile itself uses it as well. \nIt's now a matter of updating the documentation to delete manual gunicorn stances (that are not needed anymore) and a couple remaining scripts (likely unmaintained stuff living in contrib)\n. Fixed.\n. Done.\n. @ktheory what exact version of the registry have you tested against?\nThanks!\n. registry:0.7.3 is the same thing as registry:latest - so, you should get the same thing from there.\nsamalba/registry is master on the other hand (and master is unlikely to work these days). \nAbout the error you get, it looks to me as S3 telling you it can't find the bucket.\n. Hey guys,\nCan you clarify the use case?\nI'm willing to look into it, but I would really like to understand what you want - versus what @shin- described (eg: renaming using docker, layers won't get pushed again).\nThanks a lot!\n. @Ancipital @vegansk the both of you have thrown +101000, so can you provide some light in there about what you want that @shin- suggestion doesn't cover? Thanks a lot! \n. Ok thanks.\nHere is how I'd like things to work:\n- I'm going to close this, but it doesn't mean I don't want a \"rename\" feature - just that this ticket is not very sensical for now\n- so, if you have a need that is not covered easily otherwise, please open a new ticket with:\n  - a clear description of what you want to achieve\n  - optionally a short technical proposal on what you would expect (helper script, registry feature, client-server modification)\n... and I would then be happy to qualify that and see what we can do.\nBest regards.\n. @bacongobbler sorry this somewhat dried out - still feel courage enough to get this in?\n+1 on restrict push + pull\nIMHO this is what most people expect from a private registry - and those who expect something different are likely to have a different setup anyhow.\n. @bacongobbler : I would like the redirection snippet to be left in, but commented out by default (with some nice comments :-)).\n. @bacongobbler can you keep the other nginx conf file in synch as well? (for nginx >= 1.3.9)\n. No, we can leave the root nginx.conf file aside (it's going to be deleted).\nI'm talking about the two different nginx files in contrib: https://github.com/dotcloud/docker-registry/tree/master/contrib (maybe you need to rebase)\nThanks for your time on this Mr. Bacon! :-)\n. @mzdaniel we now use nose, and the coverage plugin, so we get coverage cutoff for free (https://github.com/dotcloud/docker-registry/blob/plugins/setup.cfg)\nSo I'll close this now.\n. Hi @tokeefe \nSorry this slipped through - had a lot going on.\nIs this one still biting you?\nThanks!\n. :+1: \n. All,\nThere has been a lot of changes on unicode handling these past months.\nI tend to think these issues are fixed.\nCan you guys have a test with the official 0.7.3 registry?\nIf it's not fixed, please reopen this.\n. @shin- can we test it and make sure #553 does indeed fix it?\nIf we are positive, then let's close this.\nThanks a lot @shin- !\n. I believe this is fixed\n. Haven't seen this for a while. I'm letting it open until the docker-con, just to see :-)\n. Reenabled on the fixes branch: https://github.com/dotcloud/docker-registry/commit/8b9965726c117a6ec902c7fd75a6e205069c9c07\n. Confirmed (still buggy locally).\n. Hi @SkyLothar \nSorry for the long delay on this - we had a lot to figure out with the \"plugins\" landing.\nSo, you can now author your driver as a separate pip package. This is very straightforward. You can have a look here for some documentation:\n- https://github.com/dotcloud/docker-registry/blob/master/CONTRIBUTE.md#storage-driver-developer-howto (and more generally the CONTRIBUTE document)\n- pretty much, all you need to do is extend the driver.Base class from registry-core - most of your code here can be kept as-is\n- get inspiration from one of the existing drivers - I recommend looking at the elliptics on (https://github.com/noxiouz/docker-registry-driver-elliptics) or at the file driver which is located inside registry-core\n- ping me when you are ready and if you need review / help / guidance\nI'm going to close this PR itself, as this is not going to land in the registry, but I definitely encourage you to author it and push it on pip!\nThanks a lot for your work, and hope to talk to you soon about your new driver!\n. No one expressed interest in helping maintain the glance driver (which bitrots over there: https://github.com/dmp42/docker-registry-driver-glance)\nI'm closing this as anyhow drivers are now standalone.\n. As a quick win, I would just try except all redis get / set / delete calls and logger.warn the exception if any - this is relatively trivial to do in lru.\nIn a better world, I would want tests instrumenting an actual redis server that we could make fail at will to test the driver behavior precisely instead of that (somewhat) blind patch.\n. Hi @stuart-warren \nSorry for the long delay on this. Are you still motivated to have this work?\nIf yes, can you rebase against the current master (a LOT has changed since).\nThanks!\n. @homerjam @stuart-warren: how does @bacongobbler suggestion looks? Is this enough for you guys?\n. @bacongobbler can you elaborate on how you would deploy the registry on heroku and what (mostly) :-) compatible means?\n. @bacongobbler thanks a lot for all the infos, it's much clearer (at least for me!) :-)\n@stuart-warren @homerjam if I understand correctly, Heroku compat is not going to happen unless we can add libzma-dev? Is this feasible? Or should that be addressed at heroku?\n. @shin- can you test this?\n. @newhoggy You need to use smtp_secure: true with gmail. \n. @stefanfoulis did you manage to get it working?\n. @mzsanford @chilicat where do we stand with this now?\nIs this still an issue?\nThanks!\n. Please reopen if this is still an issue.\n. @shin- what do you think?\n. I don't think this is going to happen for v1 - but I'll make sure v2 ships with this to boot with.\n. That was fixed upstream, and is on 0.8 (see #320 for details). Pretty much: https://github.com/benoitc/gunicorn/commit/62f6fb2d33182d985229d873c597ecbaa9801662\n. @amaltson we really need more info\nWhat version of the registry are you running? Are you running the official image? Or a custom driver? Is this on boot2docker? What kind of hosting? What's your configuration?\nThanks a lot! \n. @amaltson \nHow do you launch the registry? (command line used, including env vars)\nIs the OS the one you run docker engine on? What about the os where you run the registry?\nAlso, can you copy your registry configuration?\nSorry for these questions, but as we are not able to reproduce, we need the details :-)\n. @amaltson anything else fancy? nginx or apache in front of the registry?\nOr running anything from inside a VM? or using docker in docker? funky network configuration? \nFinally, what docker engine version is it?\nThanks again!\n. @amaltson any info you can get is definitely useful.\nIf you can reproduce the misbehavior, try again but this time \"removing\" any option (like the search_backend) to obtain the simplest setup to reproduce the problem.\n. If you can curl successfully but can't use docker... Then it looks to me as a docker engine bug.\nIs 10.254.204.56 the ip of the container? How do you map \"repository\" to it?\nWhat happens when you restart the docker engine daemon?\n. @amaltson can you try again, but this time pushing to localhost?\n. Indeed - that's kind of what I expected. I'm closing this bug for now, since it's the client bug linked here, and not a registry server bug.\nIf you guys still have issues with this and it's something else, please reopen. \nThanks for your time digging into this @amaltson !\n. It looks like python 2.6: /usr/lib/python2.6/site-packages/gunicorn/arbiter.py\n@lcheng61 2.6 is not supported - it's highly recommended you use the official docker-registry image, or run the registry against python 2.7.\n. @jalaziz \nThanks!\nThe documentation certainly still is imperfect and possibly outdated in places - please report anything that you might encounter and/or PR.\nBest.\n. V2 protocol being able to be served by a \"static\" web-server is a hard requirement, so, this is going to happen.\nNow, v1 will stay as this, hence I'm closing this.\n. @vbatts heads-up - we are currently fixing something else related to tar (see the linked PR from above). There is a possibility that this was causing the discrepancies you noticed.\n. /me thinks that @jlhawn deserves a medal for killing that one :-) \n. Fixed on master.\n. @treyhyde is this against the docker registry, or a private one? If the later, with what configuration / backend?\n. @S1R10N @dsw88 and others\n1. what version of the registry are you using?\n2. Can you confirm the error you get is \nFile \"<string>\", line 1, in getsockname\nerror: [Errno 9] Bad file descriptor\n. @treyhyde @sdwr98 your problem being a 504 gives a good hint this is unrelated to what is reported here.\n@bjaglin @thesamet I believe this was fixed upstream (and available in the registry branch 0.8). See:\n- https://github.com/benoitc/gunicorn/issues/712\n- https://github.com/benoitc/gunicorn/commit/62f6fb2d33182d985229d873c597ecbaa9801662\nSo, I'm going to close this.\nIf you guys see again this error:\nFile \"<string>\", line 1, in getsockname\nerror: [Errno 9] Bad file descriptor\nplease reopen!\nThanks for the report.\n. @dsw88 sorry I overlooked this - it's on master, just not on the 0.8 branch.\nI'm sorry but you will have to wait for 0.9, or build yourself from master (beware as some things might be broken currently).\n. @dsw88 ah! yes, nice move, rebuilding stable with gunicorn 19 is indeed possible and should work.\n. @kyle-crumpton what error are you running into? \nIs that:\nFile \"<string>\", line 1, in getsockname\nerror: [Errno 9] Bad file descriptor\nIf not, this is not the same.\n. @kyle-crumpton thanks - 0.9 is due by the end of the month. Have you tried the workaround to verify the proposed patch fixes the problem?\n. @bhuvaneswaran @adamhadani thanks! any help to reduce this list would definitely be welcome:\nhttps://github.com/docker/docker-registry/issues?q=is%3Aopen+is%3Aissue+milestone%3A0.9\n. We also merged 26 PR last I checked, so, 0.9 is big enough indeed. Now, release won't happen before the end of next week - so, hang on, and please jump in on any of these if you can.\nThanks!\n. @drobison00 if it's working directly to the registry, then your issue is likely with your nginx configuration.\nWithout at least your nginx version number, your nginx configuration and nginx logs during the failure, it will be very hard to advise...\n. #317 has been implemented - which means people are free to implement whatever backend they want.\nSee https://github.com/dotcloud/docker-registry/blob/master/CONTRIBUTE.md#storage-driver-developer-howto for a (minimal) starter.\nThere is another HDFS backend being worked on - maybe you guys should join forces? Check here: https://github.com/dotcloud/docker-registry/pull/480 \nThanks for the interest guys! Please keep me posted (and ping me directly from your drivers projects).\n. \\o/ @lyda \\o/\nI encourage you to have a look / discuss with the other drivers authors around (thinking about elliptics @noxiouz and swift @bacongobbler).\nOtherwise be sure to ping me on irc if I can help on anything related to that.\nCongrats on the great work!\n. @wking @samalba Do you mean we should handle only unicode strings internally (eg: decode early from flask)?\nIf so, and if understand correctly, it means that stores must be able to handle unicode as well (which I think they don't right now - at least file localstorage).\n. We are unlikely to be compatible with python3 anytime soon IMO (gevent for one), but making the effort is good anyhow (that's the idea behind https://github.com/dotcloud/docker-registry/blob/master/docker_registry/lib/core/compat.py)\nSo, I'll go with:\n- assume utf8 on the input, and try to decode ASAP at the client request handling level\n- manipulate ONLY unicode objects everywhere else in registry core code\n- encode before passing data to storage drivers - eg: pass bytes to them so they don't need to mess with encoding\n. Thanks a lot for the detailed feedback on this!\n\nWe don't want encoding code duplicated in each driver, but I'd rather\navoid lots of:\n store.put_content(path, json.dumps(data).encode('UTF-8'))\nand:\n json.loads(unicode(store.get_content(path), 'UTF-8'))\n\nAgreed.\nNow, I have mixed feelings about just providing an inherited/helper method write_json:\n- that do (somewhat) prevent duck-typing (at least people who want to not inherit will have to handle unicode on their own\n- people will have to remember to call that method anyway in their driver - so, it's just a shortcut to data.encode('UTF-8')...\nWhat do you think?\nWhat about decorators? (it's a bit better but they still need to think about using that)\nOr what about returning an intermediate object to the registry, that does delegation to the actual driver and transparently handle the encoding issues on its own?\n. Yes, sorry for the misunderstanding - storage drivers are going to be extracted from the core soon and become standalone. My concerns hence are about making the \"API\" the smaller, less error prone and the more straightforward for third-parties who are going to write and maintain said drivers.\nAnyhow, this solution is indeed excellent.\nThanks again for the tips!\n. +1\n. @shin- unicode issues are likely fixed from the plugins branch - this might be closed.\n. Closing this - if this is still an issue please say so and I'll reopen!\nThanks.\n. @bjaglin on its way - thanks for the heads up\n. @bjaglin it's ok - like I said in #352 I really think we could use better automation here\n. @bodenr related to #256 ?\n. Ok.\n. I believe the extensions system and signals API addressed this.\nFurther work may go into the v2 extension mechanism.\n. Hi @mmdriley \nCookies are bygone now - the upcoming 0.7 version won't send any - which makes me believe this should be closed.\nIs this ok with you?\nBest.\n. Hello @consultantRR \nsetup-configs.sh indeed is not aware of DOCKER_REGISTRY_CONFIG\nAnyhow, this is NOT your issue here. Rather, it's telling you that the environment variable CACHE_REDIS_PORT is not set - which means that you have set SETTINGS_FLAVOR to prod and using a copy of the sample file.\nAdmittedly, this is a mess.\nNow, trying to solve your problem:\n- are you sure you want to run out of the \"prod\" settings?\n- if so, then you must review your configuration, specially the LRU/Redis configuration\nPlease keep me updated on how it goes. \nBest.\n. I believe this is fixed with the configuration cleanup at #350 that just got merged. You can expect a way better situation with the upcoming 0.7 version.\nI'm going to close this ticket, but feel free to comment here if you are still having issues with the current configuration mechanism.\nBest. \n. @pmyjavec what is?\n. > @dmp42Well, It seems from what I'm experiencing and what you've said above this is pretty much unusable unless we use what is in the sample config right?\nI don't think so.\n\nI would love to be proven wrong right now.\n\nYes, you are wrong.\nJust use:\nsudo docker run -p 5000:5000 -v /home/user/whateverconfigfolder:/registry-conf -e DOCKER_REGISTRY_CONFIG=/registry-conf/mysuperconf.yml registry\nAnd disregard entirely what setup-configs.sh says, as it doesn't matter.\nOr run from master (which is pretty much 0.7, to be released in a matter of hours now), and disregard this ticket (and the others as well) entirely.\nIf you still have issues (with either 0.6.x or 0.7 and need help), I would be glad to help indeed, but I would need something more specific than \"wow this is absolutely stoopid\" :-)\nBest.\n. > Wow, this thread can be hard to follow.\nYes, the original poster (@consultantRR) thought his problem was with setup-config.sh, while it was not, which made this more confusing than needed.\n\nHow does one possibly set the configuration if DOCKER_REGISTRY_CONFIG is ignored?\n\nDOCKER_REGISTRY_CONFIG is NOT ignored by the registry code. Whatever it points to WILL be used by the registry code: https://github.com/dotcloud/docker-registry/blob/0.6.9/docker_registry/lib/config.py#L58\nDOCKER_REGISTRY_CONFIG IS ignored by the (legacy, broken, useless, and removed :-)) setup-config.sh script, though.\n\nI'm also having no luck at all setting environment variables either BTW.\n\nOk, can you copy me (here or gist) the command line you use to launch the registry, and possibly the content of your config file?\nHope that helps\n. #338 \n. Merged.\n. @matthughes you are using glance with swift + file system, right?\n. @matthughes \nBy any chance, would you be willing to help writing a .travis.yml file that would install and configure swift?\nThat would let unit tests be run on travis against an actual swift server and would help us reproduce / fix your problem.\nThat would be there: https://github.com/bacongobbler/docker-registry-driver-swift/blob/master/.travis.yml\nIf you would need help converting your installation steps into travis lingo, ask me (on irc: channel docker-dev or by mail).\nNow, from your logs:\nClientException: Object PUT failed: http://swiftds:8080/swift/v1/docker-registry-retry/registry/images/7064731afe90d78da2c117d64a1221c826234cd7145fd330ae7e207ff5606980/json 404 Not Found   NoSuchBucket\nIs it possible you have not configured swift container properly?\nThanks!\n. @bacongobbler ping - any idea on this? Looks like it's in your garden :-)\n. Thanks for this @mhrivnak \n@samalba what do you think?\n. I'm not sure I'm understanding everything yet in there, so, take these as questions:\nWe still use flask session mechanism to pass around some data (https://github.com/dotcloud/docker-registry/blob/master/docker_registry/toolkit.py#L192) - which IIUC means we are going to keep sending cookies (although we won't read/use them to authenticate) - at least for what is billed here as \"non cacheable\" objects (https://github.com/dotcloud/docker-registry/blob/master/docker_registry/images.py#L53)\n- am I correct?\n- if so: does it make sense to keep using flask.session?\n- if it does, shouldn't we stop sending cookies at all (if we no longer use them)? not doing it will definitely cause problems caching-wise\n- if it does, shouldn't we get rid of all the (now) dead code pertaining to flask app_secret, including SECRET_KEY dance and documentation?\n. e.g.: I guess we could use flask.g instead of flask.session.\n. I see.\nThat kind of sucks :-)\n. Smells like older version monkeying? https://github.com/dotcloud/docker-registry/blob/master/docker_registry/images.py#L233\n. After discussing this with @shin- :\nKilling the cookie entirely requires some non-trivial rewrite. We probably can't afford recomputing the checksums (likely overkill), so, we need an alternative way to store that data between the two requests (possibly on the store), and that may be the object of another PR (to come next).\n. LGTM otherwise.\n. Hello @xiamx \nA configured RSA key allows one to emit http requests to the registry using that RSA key (using the special header X-Signature) instead of the normal token authentication.\nI'm not positive about how it is used, but IMHO it's useful for command line tools (eg: based on curl for eg) to perform administrative tasks on the registry without a use account.\nI don't think docker cli can make use of it though.\nBut let's ping @shin- to confirm this.\nBest.\n. @shin- thanks a lot!\nDo we have any documentation on the exact format expected in X-Signature and/or how to generate a request?\nOr should we just remove it from the documentation and consider it's not documented? (right now it's hard to get it without digging into the code).\n. Hi @abonas \nThanks a lot for this!\nLGTM, but I'd like @shin- opinion on this.\n. @abonas the AUTHORS file was merged two days ago (https://github.com/dotcloud/docker-registry/pull/335), so, this is fairly new (and which is why it's not in the README nor requested before merging).\nSo, yes, please add your name to it and PR.\n. @abonas this is being discussed - I'll post you when I get the answer, but I hope this may be relatively soon\n. @abonas a new minor version is on its way right now, so you will get this very soon. Next major version though is scheduled for the end of the month.\n. @abonas it's a tad late for #345 but maybe we will have a new minor version soon (and that will ship with it if it gets merged).\nThanks for all the work!!!\n. @abonas on its way: https://github.com/dotcloud/stackbrew/pull/56\n. From a closer read: we also use the cookie to prevent access from changing ips.\n. I like the graph :-)\nIs is bugsnag that provide them?\n. Yes, it's a widget company, right? :-)\n. The dev env section sure needs love.\nNow, I find it a bit strange that tox would need extra dependencies, but not the registry itself?\nWould you be kind enough to verify that this section is still accurate and enough: https://github.com/dotcloud/docker-registry#on-red-hat-based-systems - just to run the registry?\nThanks a lot!\n. I'm asking because the Ubuntu section explicitly requires liblzma-dev, so I wonder why Fedora/RedHat wouldn't... \n. @abonas You don't really need git to develop - you can still get a tarball with wget and send PRs by editing files on the github web interface - but then, I probably wouldn't hire the guy :-)))\nNow, I think we do require git to run the tests (look here: https://github.com/dotcloud/docker-registry/blob/master/test-requirements.txt#L4)\nAnyhow, enough musing. Can you edit the README accordingly to your findings? (eg: update the generic usage readme section about fedora, and add that dev env section)\nThanks a lot for this!\n. @abonas Yeah, the fast vs. old way is pretty much: \"run the provided docker container\" vs. \"install it manually on your server\".\nI would definitely merge a change of vocabulary to something more clear like:\n-  \"a. run the registry docker container (recommended)\"\n- \"b. install it on an existing server (advanced)\"\n. LGTM\n. @abonas Thanks a lot!\n. Option 1 requires installing and running only docker (which is why it's \"fast\" :-)).\nSo, as far as requirements are concerned, we should point to the docker installation documentation here: http://docs.docker.io/installation/#installation\nAnd that should be enough.\n. Thanks a lot @abonas !!!\nThe various pieces of documentation definitely need love - please don't hesitate if you spot anything that can use a hairbrush, a wording clarification, or plain and simple deletion / update: edit, PR, ping me.\n. @abonas we have a lot going on right now, but I'll definitely get to this soon.\nAnd thanks a lot for your work - it really is appreciated.\nAbout that google topic: @shin- can you take a look?\n. I'm closing this in favor of #484 which seems more workable.\n. Version should really not be duplicated. I'll patch that eventually when the times comes for init cleanup.\nLGTM!\n. Admittedly, it's a mess (and see #332).\nI'll gladly review a PR that cleanup this, or I'll do it myself but that will to wait.\nAlso note that storage drivers are going to change drastically in the near future.\n. As far as I'm concerned:\nYes, we should just get rid of setup-configs.sh, keep DOCKER_REGISTRY_CONFIG and let users influence things with env variables.\nBut let's summon @shin- and have his opinion on all this! :-)\n. Hear hear. @phemmer it's all yours!\n. I would like to have it both ways:\n- let the user specify DOCKER_REGISTRY_CONFIG, so that he can use an entirely custom config path, and mount its config from outside the container\n- and let him use SETTINGS_FLAVOR as well\n. @wking I'm concerned about the extent of this PR.\nGetting rid of setup-configs.sh and harmonizing the use of env vars is one thing - introducing breaking changes is another.\nI certainly agree both are needed, just probably not in the same timeframe.\nI would definitely be glad to merge the first ASAP, but killing SETTINGS_FLAVOR and changing the way storages are selected will probably have to wait.\nThat being said, this is certainly good work, I just want to make sure that we are not breaking things unexpectedly for people using this, and that breaking changes are introduced timely.\nWhat do you think?\nAlso pinging @shin- and @samalba for their opinions\nBest regards\n. @wking that certainly makes a lot of sense, and we definitely agree here.\nNow, I think that what we don't want for the future is (I'm sure you will agree as well with that):\n- tons of pending PR (either because they were too ambitious, or not timely)\n- PR that bit-rot (trying to do too much)\n- low-quality, broken code lying around (cleaning-up is not a sexy task...)\n- breaking changes being slipped through for lack of planification\nUnfortunately, this is (partly) where we are right now.\nI'm not blaming the contributors there. Actually, I'm not blaming anyone at all :-)\nI'm just saying there is room for improvement for us here.\nOne way to improve might be to ensure that:\n- low hanging fruits and easy changes with clear benefits gets driven and merged fast (eg: getting rid of setup-configs.sh for example, which is responsible for two open issues and a lot of confusion)\n- more ambitious (API breaking changes) are concerted, clearly identified as such and part of a milestone plan\n- we have people willing to test ambitious PR exhaustively (ain't sexy either...) - the bigger the changes, the more complex they are to review\n- and/or we break things in smaller, more digest-able chunks\nPlease note I certainly don't want to kill the enthusiasm, or impose my ways on people :-)\nThis is an open discussion.\nBest.\n. Thanks a lot - this is great work.\nI just have a couple of things blocking this, and would like to merge ASAP:\n- @phemmer your changes on the elliptics driver (docker_registry/storage/ellipticsbackend.py) will get discarded as soon as we land the \"plugins\" branch - the reason for that being it is now standalone. If you want these changes, you should PR there: https://github.com/noxiouz/docker-registry-driver-elliptics\n- @shin- are we confident these changes will work for docker prod? how can we test that as fast and painless as possible?\n. @wking thanks a lot for driving this through!\n@samalba a quick validation on this before we merge would be swell\n. @phemmer great!\n. LGTM - let's merge\n. LGTM (save the nitpicking on doc)\nWe should have this in ASAP.\n. Hello @dpritchett \nNo - this here is not https://registry.hub.docker.com/ ...\nFor feature requests / support / feedback on the hub, you should rather use feedback@docker.com or http://support.docker.com/ \nBest.\n. Here is how plugins are expected to attach:\n- https://github.com/dmp42/docker-registry-driver-swift/blob/master/setup.py#L53\n- here is how plugins benefits from central tests: https://github.com/dmp42/docker-registry-driver-file/blob/master/tests/test.py\n. We are almost there.\n- Elliptics and swift have been adopted externally now (the packages are squeaky clean, including travis int., running tests and pip readyness)\n- we still have boto inside core, and file and s3 as bundled drivers in registry itself.\n- we are now testing against python26 as well, and the tests pass (there is a number of fellows running 2.6 apparently)\n- we now support coverage threshold (40%) that breaks the build\n- the build is back green on travis\n- build time on travis dropped from 10 minutes to 3\n- the behavior on remove and dir_list have been largely harmonized (when the dir is empty, or doesn't exists)\n- we are now using nose for tests and coverage\n- we support a simpler call for the tests through python setup.py nosetests\nStill todo for tomorrow, before we can merge:\n- fix up gcs and glance so that tests pass\n- plug the unicode patch and unicode breaking tests\n- merge #350 \n- update the documentation\n. Ok. We are trying staging right now.\n. @shin- ok for you?\n. Let's drop young, then.\n. @paulczar:\nGlance is work in progress and will get fixed before the release.\nAny help in setting up glance for tests is welcome in https://github.com/dmp42/docker-registry-driver-glance/blob/master/.travis.yml\nAbout wether to (re)include swift and glance as a default, this is not my call to make, but the whole idea is to make it as simple as pip install docker-registry-driver-glance to have an additional driver.\n. @shin- the docker-registry-driver-glance repo can easily provide a Dockerfile as well extending the registry image with glance crammed in (makes sense to maintain it there?).\n. cc @stevvooe \n. Ok, thanks!\n. I was under the impression that json in the standard library (>=py27) was exactly simplejson, with the same perf (https://pypi.python.org/pypi/simplejson/)\nAdmittedly, it's not the same with py26 (but we don't support 2.6).\nAnyhow - I'm replacing the calls by calls to our own compat.json layer - if we feel later on that we should reintroduce it, that will be a matter of changing one line around here https://github.com/dotcloud/docker-registry/blob/plugins/depends/docker-registry-core/docker_registry/core/compat.py#L58\n. Fixed.\n. Ok, I'm pushing this to \"next\" then - I don't think I'll have time enough to do it in the 0.7 timeframe.\n. Fixed\n. Hi @noxiouz \nI'm really sorry I missed this PR... otherwise I would have merged it before splitting the driver in its own repo :-(\nAny chance you could port that in the standalone driver-elliptics instead of master here?\nThanks and sorry again! \n. Thanks a ton! :)\n. Same question about \"test/workflow.py\". The test runner avoids it, and it requires an ENV var to be set to be run. Anyone still using it?\n. _postinstall and setup-configs.sh are out - the rest stays (has been synched/updated)\n. @noxiouz: \\o/ - I'll transfer it as soon as we land then!\nFYI, I reported this: https://github.com/reverbrain/elliptics/issues/466\n. Transfer requested.\n. Cool! I just updated the urls to reflect the change - you now need to review setup.py to update author/maintainer informations.\nAlso, what's your pypi account?\n. @noxiouz you are now the sole father of that little package on pypi :-)\nI'm happy to keep commits rights on the git repo though I won't sneak commit but rather PR there (if I need to commit).\nThanks again for taking this over!\n. @samalba @shin- Good to go? LGTM as far as I'm concerned.\n. @bacongobbler \ngithub repo transfer request sent!\nHere is your checklist:\n- sed /dmp42/bacongobbler/ in the repo\n- review setup.py and update the author infos with your infos\n- add yourself to the authors file\n- give me your pypi nickname so that I add you as a package owner\nThanks for the baby :-) \n. @bacongobbler Congratulations on the baby, it's... a package! :-)\nYou are now the sole owner on pypi, with the responsibility to publish there.\nFor now, I'm happy to keep commit rights on the git repo, though I will PR you instead of sneak committing so that you can LGTM me (in case I would need to commit).\nThere is little to no documentation for now about the new plugins architecture, though we tried to make it self-explicit, so, you should be able to find your way without much trouble - either way, you can tox the tests without requiring the actual docker-registry (which is the beauty of it).\nThe two main issues right now with swift are listed in the repo issue (though, they are not blockers).\nThanks again for taking this over! Really appreciated.\n. Hi @hansthen,\nThanks for this.\nI don't get it. From https://github.com/peterjc/backports.lzma it states that backports.lzma is exposed when that module is installed (and we require it), no matter the python version.\nSo, from backports import lzma should work on all platforms, including the one with an official lzma package.\nAm I missing something?\nThanks again\n. Either way, we are not anywhere near python 2.6 support currently...\n. @hansthen \nNo inconvenience, and no need for an apology :-)\nSorry if I was not clear:\n- contributions are always welcome - though they are discussed, so that we are sure things are the best they can\n- about python 2.6 support: I just wanted to say that fixing it is a daunting task, and that it will require a lot more\n- I sincerely don't quite understand this patch, so the request for clarification - I'll gladly merge it once I understand\nThank you again for your time!\nBest.\n. Yes, you definitely should use python 2.7 if you want a running registry soon.\nAs for contributing, indeed:\n- you just need to run tox to verify that everything is ok before submitting (otherwise, travis will cry, like here)\n- if you plan on fixing python 2.6 support, you should enable it in the tox.ini file to have the tests run against that target\nBest regards.\n. I'm going to close this for now.\nThe next version will likely have better 2.6 support hopefully.\nThanks again for your time on this.\n. I'm closing this since the question has been answered (and we have tons of tickets about this)\n. I buried the lru cache deep down inside anyhow - like the drivers, it should receive and handle bytes only - so the hell with the tests now that it does what I expect it to. I'll fix this situation eventually, later on.\n. Hi guys,\nI would like to reproduce this, and possibly add it to the tests.\nDo you have any info for me to get started on that? Do you believe the push broke when sending a layer?\nThanks!\n. Ok, thanks - I'll try to smoke it out.\n. Hi @xiamx \nWe cleaned-up the configuration system recently (which admittedly is a mess in the stable version), so it's quite possible you used the README from master instead of the stable version documentation.\nI updated the readme with a warning header about that, and a link to the stable documentation link.\nHere, it tells you that the \"storage\" configuration property is not set in the used configuration file.\nKeep me posted on how it goes - or if you feel adventurous, you can try the master instead and help us test the latest and greatest :-).\n. @xiamx Nice to hear it!\nShall I close this ticket then?\nThanks.\n. List of candidates to deletion:\n0 commits ahead (likely merged already):\n- cache-lru\n- mirroring_fixes\nUpdated in 2014 (1 commit ahead):\n- heartbleed 1\n- heartbleed 2\n- 234-unicode-fix\n2013, X commits ahead:\n- indexer\n- mock-s3\n- s3_cdn_support\n. @shin- can you kill these of yours which are useless?\nI know, I know, nobody likes to clean their bedroom :-)\n. Assuming travis gets back one day, LGTM\n. The Redis LRU is used to cache values from the backend drivers, so neither nginx nor varnish are of any use for that.\n. Indeed, there is no point in using it for file storage (it's quite useful for S3 for eg).\nEither way, after giving it some thoughts, I'm not coming to cram-in Redis into the Docker image - rather provide detailed instructions about how to set it up.\n. Well, I guess this will get kind of philosophical :-) (disclaimer: I sort of agree not bumping the version in that case).\n. >  As soon as docker 1.0 is released, we can remove the version check and remove the obsolete code.\nThis will go into registry 1.0.\nMoving this. \n. Fixed by #389\n. Fixes #248 \n. It's in :)\nI'd like to keep the branch alive for now.\n. Hi @vbatts \nCan't this be monkeypatched?\nI'm diffing with python 2.7 and getting this:\n```\n #---------\n # Imports\n@@ -117,7 +125,7 @@\n# Fields from a pax header that override a TarInfo attribute.\n PAX_FIELDS = (\"path\", \"linkpath\", \"size\", \"mtime\",\n-              \"uid\", \"gid\", \"uname\", \"gname\")\n+              \"uid\", \"gid\", \"uname\", \"gname\", \"SCHILY.xattr.\")\n# Fields in a pax header that are numbers, all other fields\n # are treated as strings.\n@@ -127,7 +135,8 @@\n     \"mtime\": float,\n     \"uid\": int,\n     \"gid\": int,\n-    \"size\": int\n+    \"size\": int,\n+    \"SCHILY.xattr.\": dict\n }\n#---------------------------------------------------------\n@@ -1390,8 +1399,17 @@\n             length = int(length)\n             value = buf[match.end(2) + 1:match.start(1) + length - 1]\n\nkeyword = keyword.decode(\"utf8\")\nvalue = value.decode(\"utf8\")\n\nsome of the xattrs are packed structs, and not utf8\n\n\nstruct.unpack(\"<IIIII\", val)\n\ntry:\nkeyword = keyword.decode(\"utf8\")\nexcept UnicodeDecodeError as e:\nkeyword = keyword\n+\ntry:\nvalue = value.decode(\"utf8\")\nexcept UnicodeDecodeError as e:\nvalue = value     pax_headers[keyword] = value\n     pos += length\n\n@@ -2589,4 +2607,4 @@\n     return False\n\n\nbltn_open = open\n-open = TarFile.open\n+open = TarFile.open\n```\nIf I'm correct, we just need to monkeypatch tarfile.PAX_FIELDS tarfile.PAX_NUMBER_FIELDS and TarInfo._proc_pax?\nFinally, given the nature of that change, I would really love to see tests for this...\nBest regards.\n. FWIW, the build broke because your tarfile doesn't follow our style settings. If we commit it entirely, you can exclude it from flake8 audited files from here: https://github.com/dotcloud/docker-registry/blob/master/tox.ini#L10\n. @vbatts ok.\nLet's start with tox: what OS do you run? Can you gist (or mail / irc) me the gcc failures?\n. About tox:\n1. flake8 is not happy with your new file: exclude xtarfile from there https://github.com/dotcloud/docker-registry/blob/master/tox.ini#L11\n2. you don't have python 2.6 installed - remove py26 from the tox.ini list of tested env (https://github.com/dotcloud/docker-registry/blob/master/tox.ini#L5), or install python 2.6 as well\n3. about the failure for python 2.7: zipimport.ZipImportError: bad local file header in /home/vbatts/src/docker/docker-registry/.tox/py27/lib/python2.7/site-packages/WebOb-1.4-py2.7.egg - sounds fishy - corrupted download? anyhow, webob is a dep of bugsnag - remove bugsnag from requirements.txt and you will be fine (https://github.com/dotcloud/docker-registry/blob/master/requirements.txt#L9).\nLet's cut the chase. Try:\n- python setup.py nosetests\nIf that one fails, try to pip install -rrequirements.txt first\n. @vbatts cool.\nTests indeed are a must-have for this to be merged.\nAlso, I still believe we can monkeypatch tarfile instead of vendorizing it - we will discuss that later on.\n. @vbatts \nHere: http://pastebin.com/2zL5F1f6\nNow, from anywhere you need tarfile, instead of:\nimport tarfile\ndo\nfrom dtarfile import tarfile\nand you get the monkeypatched version.\nThis is the way to go.\n. About how I want this:\n- put the monkeypatcher inside docker_registry/lib, and name your file compat.py\n- your imports are going to look like from compat import tarfile or from .lib import compat and tarfile = compat.tarfile\nI want exhaustive tests on this:\n- a test successfully reading a tarfile, using both the unpatched and patched version, doing comparisons checks between the two\n- a test failing to read a tarfile, using the vanilla tarfile - and successfully reading it using the patched version\n- tests on what you expect from the content of extended tarfiles\n- python26, python27 tests must pass (travis will test, but you can do it locally as well using tox)\n- python34 shouldn't do the patching - to do that, base on ver = sys.version_info to decide wether you alter tarfile in the compat file\nThanks for you work on this Vincent!\nBest.\n. @vbatts unless there is a specific docker agenda / time constraints here, this patch is not going to make it in for the 0.7 release, due in a couple of days. It will have to wait for a later release.\nJust managing expectations :-)\n. That starts to look good.\nLet's have tests indeed, and this is gonna make it!\n. @vbatts I think you might not need to split strings up - just the key / values on different lines.\n. Try to # noqa them.\n. @vbatts pip install hacking\n. @vbatts once you fix the assert calls, it will LGTM.\nIf you still feel courageous, I would love to see two additional things:\n- tests on the same test-cases that demonstrate crashes / misbehavior of the vanilla tarfile implementation (you case use nosetests raise decorator, and import tarfile as tarfile_vanilla)\n- if python >= 3.3 implementation is correct, then we should NOT monkeypatch - you may test for sys.version_info inside your monkeypatching file and decide or not to do the patching there\nPing @wking @samalba @shin- for additional review.\nThanks a lot for the efforts on this!\n. @vbatts there is some minor divergence with master. Can you rebase so we can merge?\n@wking are you satisfied with this?\nThank you both!\n. Merged! Thanks a lot for your work @vbatts !\n. IIRC cache-control (meant for 1.1) overrides pragma directives (meant for http 1.0 clients).\nI'm not saying this is right - just that:\n- this is likely a cloudflare half-assed configuration - and not registry code\n- it likely does not impact anyone (save old http 1.0 clients?)\n. You are right, this is the registry.\nThe discrepancy between the two endpoints has its origin in the fact one method calls tookit.response and the other directly flask.response.\nI'll patch it.\n. @vbatts #381 is going to be merged soon - this is next: any test we can add for this?\n. ping @vbatts get me some test and let's dance :-)\n. @vbatts I don't see a reason to have this into registry v1. Can we close it?\n. @vbatts the registry v1 will never handle the v2 protocol... (unless someone steps up to backport golang v2 into python v1)\n. So, you mean: how will it work for future docker engines using versioned tarsum to talk to v1 registries? <- right?\n. Only (with) docker <=0.9 (registry) cares about tarsum - so, I expect any new engine to keep working as-is for v1 (and the registry v1 to be happy with that).\nshrugs matrix of hell is not only about distributing content - it's also about retro-compat...\nEither way, my suggestion is to focus on v2 full-throttle, and not merge anything new for v1.\n. Closing this, then - scream if not happy! :)\n. Ok, pushing it then.\n. @shin- Do you want it in for 0.7?\n. The first release to be on pypi is scheduled for the 0.7 version, due in a couple of days.\n. It's on pypi.\n. Hi @shankarj \nYou should really stick with the officially released version.\npip install docker-registry\nAnd that's version 0.7.3: https://pypi.python.org/pypi/docker-registry/0.7.3\n0.8.0 is the master - and the only reason to use that is if you are developing on it - in that case, you should either stick to using it inside venvs, using the provided tox ini, or call python setup.py develop on the depends/docker-core sub-package.\nHope that helps.\n. It's not native - we are throwing that ourselves https://github.com/dotcloud/docker-registry/blob/master/docker_registry/drivers/s3.py#L135 which is bad IMHO - we should rather raise FileNotFoundError like everywhere else \nAlso, we are testing for this condition below here https://github.com/dotcloud/docker-registry/blob/034de2c053fe6d6fdcbbdb6e3d1817c15c3f626a/docker_registry/images.py#L91 and raising.\nSo, IMHO:\n- move the existence test over there: https://github.com/dotcloud/docker-registry/blob/034de2c053fe6d6fdcbbdb6e3d1817c15c3f626a/docker_registry/images.py#L67\n- don't try catch on the S3 redirect\n- modify S3 driver so that it throws FileNotFoundError instead of IOError\nWhat do you think?\n. Arr, haven't noticed I was late :-)\nYour fix works ok.\nI'll change this for 0.8.\nThanks for catching it @shin- \n. Documentation (and config likely) need a through cleanup as a whole.\n. You likely read the master documentation, while you are running 0.6.9.\nThe documentation and config files for 0.6.9 are here: https://github.com/dotcloud/docker-registry/tree/0.6.9\nIf you want to stay with config_sample, the s3 settings flavor doesn't exist (for 0.6.9) - try \"prod\" instead: https://github.com/dotcloud/docker-registry/blob/0.6.9/config/config_sample.yml#L47\nKeep me posted on how it works.\nBest.\n. You need to mount that configuration file inside the container to have it used:\nhttps://github.com/dotcloud/docker-registry/tree/0.6.9#location-of-the-config-file\nAlso, 0.7.0 is now the official latest release :-)\n. @samalba @wking extras requirements are a setup tools feature: https://pythonhosted.org/setuptools/setuptools.html#declaring-extras-optional-features-with-their-own-dependencies\nIt got implemented in pip around 2011 I think (see over there for more: https://github.com/pypa/pip/issues/7).\nThe idea is sexy, being able to name a \"feature\" and attach an array of dependencies - the downside is when installing from filesystem (it turns ugly).\n@wking thanks for the thorough review\n\n[snip] churning, confusing commits\n\nYes, that's a bit messy, sorry for that, and I'll squash / reformat.\nAbout requirements/test - style, folder vs. root, yes, why not group them. Ultimately, we may even get rid of them and use a docker-python-dev package to express tooling for all docker python development if we decide we need coherence.\n\nYou shift this to server/init.py late. Personally, I'd prefer to have this\nmetadata in docker_registry/init.py\n\nWell, that's not possible - that's the caveat of python namespaces: you can't have anything package specific in the init file of a namespaced folder.\n\nwhich should allow you to avoid the execfile wonkiness in 7b7d021 (Fix namespace collision,\n2014-05-23).\n\nNo, unfortunately.\nAnother shit-trap of python namespaces, but you simply can't import something that is in a namespace that way (flat) from inside a setup.py without breaking the other things installed in that namespace.\n\nI don't understand the \u201c(pip)\u201d parentheticals.  Is that suggesting pip as the installer for the subsequent packages?\n\nYes, and you are right, it's unclear. Will fix.\n\nWhat does the namespace breakage look like?  Can't all this metadata live in a boring docker_registry/init.py?\n\nNo, unfortunately.\nSee https://github.com/pypa/pip/issues/3#issuecomment-1659959  http://stackoverflow.com/questions/17338925/cannot-install-two-packages-that-use-the-same-namespace for a starter.\n. > I like explicit requirement files more than splitting the dependencies out into a separate virtual package.  You'll still need the requirements listed somewhere, and it seems simplest to list them in the package that's doing the imports and using the APIs ;).\nThe idea I'm talking about is to enforce common tooling across several different projects, but that's irrelevant and not going to happen (soon) for the registry itself.\n\nWith PEP 420 and Python 3.3, namespace packages move into the standard library [1], but they will still lack an init.py [2].\n\nI know PEP 420 but unfortunately, moving to python3 and dropping py2 support is science fiction here (gevent is not compatible yet, and flask broke it), so we have to stick with the old crappy way.\n\nWhy do we need a namespace package, though?\n\nThe PR you link to actually asked for comment :-)\nEither way, there are a number of advantages in using namespaces, especially not multiplying different global names between different packages (-core, -registry), or ability to list all available drivers by enumerating docker_registry.drivers, etc.\nSure, there are other ways to do \"plugins\" - that one has upsides and downsides (admittedly I'm disappointed about how broken python namespaces / packaging are, but it's still a powerful and clean formalism).\nBest.\n. > - Fix workflow test: This adds colorama and assorted stuff that's (mostly?) removed again by c943b41 (Remove debug, 2014-05-29).  Maybe drop both commits?  Or squash them together?\nThis more essentially makes workflow usable again - it is entirely broken on master, and has been for quite some time.\nI'll clean up the commits though, to avoid adding / removing debug helpers.\n. > It was a big PR, and I was too busy to look it over ;).  Apologies for the late review.\nPeace :-)\n\n[@wking doesn't like namespaces, at all, he thinks they are all ugly, and smelly, and they are bad and... :-)]\n\nWell I do like namespaces, and do wish they were less  \"hoopy\" :-)\nNow:\n- we are talking (lengthily) about a one-line inconvenience in a setup file\n- I don't see a such a minor \"problem\" (<- I paid the price researching the origin of it) as a good reason enough to change a design and discard quite a lot of work\nI completely agree that you have reasons not to like namespaces (I too was bitten when I was young - it was by a PEAR package though :-)) but I can assure you namespaces can be good if you tame them:\n- easy to scaffold, normalized drivers all sharing the exact same structure and folders is good\n- consistent naming is good (yes, I'm a bit of a naming freak :-))\n- ability to run tests for all installed drivers from the registry package itself is good\n- ability to programmatically report what drivers are installed is good (unless you really think you can trust users to report accurately what they did :-))\nAnyhow, we are entirely OT here (*), and I'm on a GMT timezone :-)\n((*) I'd be happy to have a beer with you someday, though)\n. Hear hear :-)\n. Hi @mccrodp \nThe documentation doesn't instruct you to pip install -r requirements.txt but rather pip install . - or did I missed where it say that?\nAnyhow, you should do the latter.\nKeep me updated on how it goes.\nBest.\n. Technically, you should now do pip install docker-registry without the need to clone the repository (as the pip package got published, and you don't really need the source), but that's the brand new 0.7.0 version.\nAlso, I don't recommend running by calling gunicorn.\nSo, if you insist on running outside of the docker container, once you installed (whatever the pip call), just run docker-registry (and set the appropriate environments variables to point to the proper config file/environment) - that will launch gunicorn with the recommended settings.\nI'm closing this ticket for now as it seems your problem was addressed, but feel free to add more here if you need further help (or on irc).\n. Pointing to a configuration file and choosing a flavor: \nSETTINGS_FLAVOR=test DOCKER_REGISTRY_CONFIG=config_sample.yml docker-registry\n. @jeremyjjbrown the recommended way to run is documented here:\nhttps://github.com/docker/docker-registry/#quick-start\nAdvanced use (direct binary launch, or using gunicorn) is documented in https://github.com/docker/docker-registry/blob/master/ADVANCED.md\n(though these are provided more as a convenience)\nLet me know if you feel anything is missing there (my comments are quite old here and the documentation moved a lot meanwhile).\nHope that helps.\n. 1. please keep hacking separate (in style.txt) - installing it is only useful when you want to flake, and unneeded when you just want the tests, hence slows down tox setup a lot when not needed (and see below)\n2. pinned down versions for development packages: this is why CONTRIBUTE list requirements without explicit versions (so that people who are addict to system-wide installs can do whatever pleases them and use whichever version they want and won't work :-)) while the req files are used by automated, isolated testing environments (tox and travis) and have explicit pinned-down requirements (that we know for sure will work and won't break the tests for stupid reasons) - I would really like to keep these pinned-down versions - people who like to test outside of virtualenv and who love to install stuff globally should know what they do, while I would like to encourage others to work in the venv/tox clean rooms. Would be happy to discuss this further though.\n\ndocker_registry.server.env.defined is a global variable not used\noutside the module, so I'd rather call it _SETTINGS (or at least _DEFINED).\n\nSure.\n\nThere's still some non-trivial stuff going on here.\n\nSorry if some slipped through - it was not always easy to split the different modifications into coherent commits.\n\nWhy don't we keep the flake8 config?\n\nIt's useless. Flake ignores it as it found configuration already in the tox file (this (admittedly strange) behavior is documented somewhere on the flake doc site).\n\ndocker_registry/lib/init.py was empty, so why the UTF-8 declaration?\n\nSo that people who might add something in it don't forget it. Not that useful, agreed - but not that harmful either.\n\nHow does the docker-registry-core shift into setup.py ease tox?\n\nBecause we want people who pip install docker-registry to get the released version of docker-registry-core, while we want tox tests to be ran against the local version of the core (under depends). \nHence tox would need a separate requirements file listing all the other dependencies, which I think is a burden to maintain and prone to error (that existed earlier, FYI).\n\nDo you expect folks to want the style requirements without the test requirements (test without style makes sense to me)?  If not, maybe:\n\nAgain, this is about automation, not about what folks install system-wide (and I really think global install is bad practice) - but see above for the reason why I want \"style\" to stay standalone.\n\nI prefer the nestable $(\u2026) to \u2026 for command substitution.  But PWD is a POSIX-specified environment variable [2], so I'd rather have:\n\nAgreed.\n\n\u201c# XXX revert\u201d ?  Does this have some magic effect?\n\nYes, it makes differential anisotropic Higgs boson follow the abelian group underneath the unicorn that runs the bike inside the registry :-)\nThanks for the thorough review!\n. @shin- I would love to cleanup the root of the package - and have under server the things that are pertinent only to the server app itself - lib should still contain things that are also pertinent to maintenance / debugging scripts or other tools accessing the data - and root may only contain entrypoints (or even nothing at all, with the entrypoints in another subpackage).\nThe point being to have a clearer organization, and possibly to re-expose our scripts more elegantly as cmd entrypoints.\nAnyhow, this is rather medium term, as reliability and bugfixes come first.\n. Hi @getitlive \nDo you have a specific repo name that exhibits such shorten ids as tags?\nThanks.\n. Hi @shreyu86 \nAny error message in the logs when you try to push / pull?\nAny error message at startup?\nThanks,\n- Olivier\n. > also if I ping registry I get a 404\nWhat do you mean by that?\n\nLogs don't say anything special from which I can deduce something is broken even tough logging is at debug level\n\nStill, can you gist your registry logs / output?\nThanks a lot.\n. So, /ping is not 404, right? Instead, the server appears not to be started / bound.\nLet me try tomorrow with a s3 region and see if that works for me.\nKeep me posted if anything new meanwhile.\nBest.\n. I haven't had time yet to get to it, sorry for that - will sure do later today and keep you posted though!\n. @shreyu86 \nI just created a new bucket, in the us-west-2 region - I'm using us-west-2 as a region in the configuration and it works for me...\nSo, we need to dig deeper.\nCan you provide with additional infos?\n- the logs you copied (the \"with region\" scenario) seems to be cut too early - what happens exactly? it hangs there and does nothing more? it crashes? or is there more after that?\n- what version of the registry are you running? (I assume 0.7.0) - did you try and/or have the problem with 0.6.9?\n- are you running it from the docker container? or from a pip install?\nThanks a lot!\n. Can you get this gist: https://gist.github.com/dmp42/8436a9be2c569bd75965\nAnd try run it from where you are? (obviously replace bucketname = 'XXXX'\nawsid = 'XXX'\nawssec = 'XXX' with appropriate values).\nAlso, can you put debug = 2 inside your boto.cfg file (in your home)?\nThanks a lot!\n. This is getting really weird.\n@shreyu86:\n-  do you use the configuration variable storage_redirect?\n- can you copy the exact command lines you are using to launch the registry, both with region and without region\n- same thing for your configuration files, with and without region\nSorry for the long walk, but I see no other way to get to the bottom of this...\n. ```\nCommands to invoke registry\ndocker run -d -p 5000:5000 -v /etc/docker-registry/config.yml:/opt/docker-registry/config.yml -e SETTINGS_FLAVOR=prod -e DOCKER_REGISTRY_CONFIG=/opt/docker-registry/config/config.yml\n```\nUnless I'm mistaken, this can't work: you are telling the registry to use  /opt/docker-registry/config/config.yml while you mount /opt/docker-registry/config.yml\nIs this a typo or did you move some stuff and were launching with this?\nThanks.\n. Ok... this doesn't make any sense...\nIf you successfully connected using the test script I provided, there is absolutely no reason why the registry wouldn't connect. This is almost exactly the same code...\nOne last shot: did you run the test script on the same machine that you use to run the registry?\nOtherwise, I see no resolution to this :-(\nAs I can't reproduce on my own setup - there is only two possibilities:\n- we find how to reproduce\n- you manage to debug deep inside boto to see what happens\n- you trust me enough to grant me a temporary access to your bucket\nSorry for not being able to be more helpful...\n. @shreyu86 Ok - please keep me update of any progress on this.\n. @ddeaguiar or @phemmer \nBy any chance, would any of you be able to grant me a (temporary) access to your problematic bucket?\nLike I said earlier, I can't reproduce here, and that would really help diagnosing this.\nThanks!\n. Thanks a ton! I'll look into it ASAP.\nAbout what changed from then - mainly we upgraded boto to the latest version.\n. Works from outside the container (vs: pip install docker-registry) but doesn't from inside the container...\nI'm thinking:\n- libevent issue\n- a/o networking issue\n. @mdub standalone True is the default since 0.7.3 release, which you should use.\n. @frankamp are you using boot2docker?\n. @frankamp yeah, I was thinking more about a timeout issue somewhere.\nCan you expand on these \"upload errors\" you mentioned?\nThanks!\n. @shin- any idea / input on this? Could this be related to a docker udp bridge issue?\n. @shin- the weird thing is that it's working if the registry is not inside a docker container.\nIf it is inside a container, it does NOT work.\n. Then it's a library version problem. Had it running ok outside of a container here. You are likely running Ubuntu 14:04?\n. @mattheworiordan read my comment please\n. Same python libraries, different system deps. Pointing at libevent-dev ?\nCan anyone do tests with different ubuntu versions?\n. @shreyu86 for now, I would support specifying the region inside the boto.cfg file.\n. @chuegle !!!!\nAwesome research.\nIs there an upstream (gevent) ticket for this? Otherwise, do you have an ETA for this to hit a boto release? \n. Fixed thanks to @chuegle incredible work!\n. @hartym was released with 0.9 IIRC.\nAre you still experiencing this issue?\n. @hartym if you delay your deployment to next year, fair enough - if you still have time to test, can you copy your configuration, launch command, and log?\nThanks a lot.\n. @hartym happy you got it working!\n. Hi @superbacker \nThe configuration variable is named storage_path, not repository_path.\nCan you share your current configuration?\nBest.\n. I'll fix it ASAP.\n. This was indeed fixed with 0.7.1\nThere has been some delay on stackbrew, sorry for that.\nStackbrew has just been updated with the latest (0.7.2) and that should be available very soon (as soon as the images are rebuilt)\n. Hi @bjaglin \n/tmp/registry is already the default - see here: https://github.com/dotcloud/docker-registry/blob/master/config/config_sample.yml#L47\nNow, due to #401 it is not honored, but it should (I will fix it ASAP).\nThe point being I would like to avoid multiplying default values across the codebase - so, I would rather not merge this - and leave the standalone driver at ~/tmp (otherwise you will break default tox tests).\nThanks for your effort on this, though.\n. @bjaglin I PRed a fix (#404) which should address this. You may close this PR here if you feel the other one is satisfactory.\nThanks again for your time and involvement.\n. @wking let's go with that, and see how we manage :-)\n. It's somewhat working. Kind of a PITA to track fixes on two branches, but I guess that's what happens when you grow up :-)\nClosing this.\n@wking thanks for your help \n. Ah, right! That's helpful.\n. Also see #403 for open questions on release management / branching strategy.\n. @wking strong coupling between an opaque config object (registry side) and driver is IMO bad, and I want to break/reduce it.\nI would really prefer explicit arguments whenever possible. Indeed, most if not all drivers have a concept of base path (or prefix), which is why this one should really be different from the other driver-specific configs.\nJust thinking aloud, though - and admittedly, that point wasn't discussed when the abstract interface was designed, but it can sure be now.\nBest.\n. @shin- sure! I'll look into it tomorrow morning - ping me if you want anything else into 0.7.1 before I pypi it.\n. @shin- it's on master but not on \"stable/0.7\" (which I cut from 0.7.0).\nDon't bother though - I'll backport it.\nPlease focus on docker 1.0 for now and keep it rolling! We will discuss all that later.\nBest.\n. > Maybe all existing drivers do, but I could imagine a storage backend based on a database that doesn't need a base path or prefix.\nWe can imagine it - then it would be an exception to the general rule, and it should just ignore the argument (as the path arg is optional anyhow).\nEither way, even with a database, a \"translated\" path make sense (table prefixes for eg).\n\nIn any case, this is still guessing about the semantics of a hypothetical (or at least external) driver's config, and I don't see any gains by doing that in the core code.\n\nThe gain is that whenever / if we change the configuration format for storage_path, we don't need to patch ALL drivers, but just the registry (mapping between the config key and the arg) - which is exactly what happens here - I don't want to touch the driver itself.\nAnyhow, the future may be like what we did for Hipache - use uris as a configuration provider for most \"common\" variables (see https://github.com/dotcloud/hipache/tree/master/lib/drivers).\neg:\n- s3://bucketname/storage_path\n- file://storage_path\n- protocol://login:password@host:port/database_or_path_or_whatever#prefix\nThe benefit is that uris are a well-defined way to represent said information, and are both expressive and compact notations.\nStill need to review the drivers and see if that make sense - but this is getting OT - more on that later!\nThanks for the review!\n. Hi @skarnik-rmn\nLet's discuss this on #400 \nBest.\n. @skarnik-rmn I suggest we close this PR for now, and keep the discussion open on #400.\nWhat do you think?\nThanks for your persistence on this!\n. @discordianfish which client version are you using? (eg: docker )\n. @discordianfish my question was about the client (docker) - not the registry.\nAny complete log of your push scenario would help as well.\nThanks a lot!\n. @asim it seems your policy doesn't allow DELETE, which the registry needs to operate\n@discordianfish looks like amz consistency issue to me.\n. I believe that was fixed with consistency hardening. If it's not, please say so I'll reopen. \n. Happy you figured the right option for your case :-)\n. I suspect this affects #406 and possibly others.\nAnd this (https://aws.amazon.com/s3/faqs/) clearly states that what we use in the standard us region is under eventual consistency - not read-after-write consistency.\n. >  When are we doing this sort of \u201cwrite and then read back again\u201d\nThat's the one easy answer :-) -> PUT checksum\n. @wking IIRC this was about performance.\n. @wking ?\nAMZ consistency is this ticket. I'm answering your question about: \"Why we need a separate endpoint for pushing checksums to the registry side of things (when it can calculate those\nchecksums on upload) is beyond me ;).\" which (again IIRC) was motivated by performance issues (but you can check that with sam over there: https://github.com/dotcloud/docker-registry/commit/ad14a7bca74ee1d99338f1a4e089295d50557a0c )\nEither way, we are getting largely OT here, and removing PUT checksum will not change AMZ behavior and / or root problem.\nThe fact is there is at least one stacktrace that demonstrates exists() returns true and on the following line read() raises 404.\nWhich endpoint / request doesn't matter, and we need a generic solution to handle S3 behavior.\nBest.\n. @fermayo there is a pending PR over there: #418 \nAny help is welcome in reviewing / discussing it :-)\nHope to have it for 0.8!\n. A fix for the main symptom of this has been merged, so I believe this should be closed.\n. Mainly:\n- It's a complex change not fitting well with the short term milestones\n- this is not going to be used by the official docker registry itself (see discussion about the existing dataset) unless there is a practical solution to handle the existing (growing) volume of images \n- or it should be made optional\nEither way, if you want to try it, I guess nothing prevents you from giving it a spin from @wking branch.\n. @wking sorry about the lag on this - will look\n. About the open questions:\nI would like this to live in its own subclass, and leave Base as-is.\nAlso, I would be happy with separate drivers (s3-rf, file-rf - where rf=ref-counting - but any other, better, suffix can do) that would subclass both their parent (s3, file) and the refcounting base if that's possible.\nI really want this as standalone and \"separate\" as possible - and to minimize the effort on the drivers developers - and make this an explicit (simple) choice for the users.\nSorry for the long wait on this, and for the bikeshedding - and thank you for your work on this @wking \nBest.\n. @wking @ncdc do you guys intend to work on this for v1?\n. That was fixed on the \"next\" branch - it's not landed on master yet.\nSee https://github.com/dotcloud/docker-registry/commit/4c8cfc511464ad5f4caf9709851a69e59c7d1ad2\nThe 0.7 branch is not affected.\n. This is now landed on master with #414\n. Hi @sfitts\nI took a look at Artifactory, and read #168.\nAbout what you describe specifically (eg: ability to point docker to an alternative specific repository where to get standard images, and fallback to the central repository) that would require significant changes in docker itself - and as far as I can tell shykes is not too keen on the idea (as detailed in #168)\nEither way, I don't think this is registry issue per-se, and would rather encourage you to start a discussion on docker core itself (you will get more feedback than here), as this would require changes in ressources naming, etc.\nAbout the ability to use Artifactory as a private registry though, it might be feasible (disclaimer: I have a very limited knowledge of Artifactory), but from where I sit it seems that it's a matter of configuring Artifactory so that it reproduces the registry REST API. Again, I'm a not too sure there would be something to be done on the registry project side itself.\nAm I missing something?\nBest regards.\n. Ok - keep me updated on how this goes and if I can help on the registry side.\nThanks!\n. I don't see a diff here?\nAnyhow, yeah, go with just self.signer = None and that's it.\nWhile we are at it, inside makeConnection, we should move the cloudfront bits above the region connection, otherwise if the region is specified you won't be able to get cloudfront signing. \n. LGTM\n. Merged.\nThanks a lot!\n. samalba/docker-registry is not the official registry.\nLook here:\nhttps://registry.hub.docker.com/_/registry/\n. Yes, global refers to the current global scope, not to something else in other modules.\nHere, lru.redis_conn and redis_conn are different variables, and global redis_conn in lru and global redis_conn in cache refer each to their separate global scope.\n. @wking I somewhat agree - but simultaneously feel like this being a (sad) limitation of execv it should be buried, and rather let people say \"a port is an int\" in places where we don't mind / are not aware of execv.\nNot a big deal either way though :-)\n. The main un-handled issue here is the fact that a temporarily unavailable redis that becomes available again might then hold an inconsistent cache.\nExample:\n- set(foo, bar)\n- get(foo)\n- [redis unavailable]\n- set(foo, baz)\n- [redis available again]\n- get(foo) <- now this is inconsistent\nNow, I don't see an easy way out of this... and it's probably already the case today, so maybe this should be handled as a new different issue.\nWhat do you think?\n. @wking that won't work - unless you only use one registry (in which case having a distributed datastore is not that meaningful...).\n. > How does this work now?\nThis likely does not. As far as I can tell, we mainly rely on the fact that writes are expected to be performed sequentially (the same user usually don't do concurrent writes on the same resource).\nWhat's \"new\" (with redis failure \"resistance\") here is that even that assumption (eg: sequential writes) is no longer a guarantee that things will work ok - and I was barely pointing out that per-registry \"dirty\" flags won't help.\nBest.\n. @shin- agreed\nLGTM otherwise, let's merge this.\nI created follow-up tickets for tests / bugsnag instrumentation (your opinions on the latter is welcome if you have previous experience with bugsnag reporting API).\n. Thanks for the PR!\nLet me get back to you next week on that.\n. Ok, you need to fix the build.\nStart by running flake8 . and fix whatever it complains about :-) - then run tox and check for green.\n. Sorry I made you wait so much on this. I'm still... freaked out :-)\n. @pchaussalet hey Pierre, I upgraded \"hacking\", which broke your patch (for silly reasons).\nCan you fix these silly longlines and I finally merge this?\nThanks!\n. @johanneswuerbach you can fix hacking complaints (longlines to rewrap with () and other silly style issues)\n(and rebase it)\n. @pchaussalet Pierre, we merged a rebased/fixed version of this.\nThanks for the initial PR and hope to ttys!!!\n. That looks like #417 - do you run docker 0.12?\n. Ok, then if it's indeed the same issue, that should be fixed on docker master now (https://github.com/dotcloud/docker/pull/6260). If you can't / won't live on master, a workaround is to downgrade docker to 0.11.\nIf you believe this is it, can you close this as a dupe of #417?\nThanks for your time and report!\n. Stackbrew has been updated to refresh the build.\n. I believe this was fixed by #444 with a more logic config evaluation system.\n. LGTM.\nWill cherry-pick on 0.7\n. @dsw88 did you have a chance to test again?\n. @jhspaybar if you have an idea how to do that without changes to the client code, please reopen this. Thanks a lot!\n. @danzy are you running the latest registry (eg: 0.7.3)?\nIf so, what configuration / flavor are you using?\nThanks!\n. @xbgmsharp don't understand your last question.\nAre you looking for third-party authentication and user management on top of standalone registry?\n@danzy @shayneoneill did @shin- answer shed light enough?\nThanks.\n. @shayneoneill the docker client issues a standard message for non official registries: https://github.com/docker/docker/blob/master/registry/auth.go#L203\nthat doesn't mean that users need to do anything - in fact, you chose what to do - this message is provisional.\nAnyhow, I'm more than willing to help, but you need to be specific about:\n- what doesn't work in your setup - \"Still broken. Accounts can not be activated.\" <- is not helping. As it was said earlier, there is no need to activate account if you have no extra activation procedure (and yes, admittedly, docker client message is confusing)\n- your current registry version and configuration\n. I like the idea - likely this needs to be discussed by the docker team.\nI'll follow-up on this - I just can't promise it will be fast.\nThanks!\n. Working on it :-)\n. Phantom? Secret?\nHere's the tag: https://github.com/dotcloud/docker-registry/tree/0.7.3\nHere's the CHANGELOG: https://github.com/dotcloud/docker-registry/blob/0.7.3/CHANGELOG.md\nHere's the PR with the change for it : https://github.com/dotcloud/docker-registry/pull/436\nAs stated earlier, this is a quick fix to alleviate the problem described here: https://github.com/dotcloud/docker-registry/issues/432 - which clearly states that 0.7.3 has been released.\nNow, about this ticket, as I said, I'm working on it for a better fix, and it will be closed when that work will be ready.\nHope that addresses your concerns :-)\n. > I was just wondering why aren't the related issues referenced in commits (As in FIXES # 123, REFS # 135 # 154: Commit message here), as it would make issue tracking a lot easier (not having to review commits on a daily basis).\nI understand your frustration.\nA start would be to update CONTRIBUTE.md to clearly state said better commit guidelines.\n. That was fixed by #444 \n. @jfromaniello it assumes config_sample.yml and settings flavor \"dev\"\nYes, you may mount your own config volume dir and specify standalone True there.\n. 0.7.3 has been released and I believe fixes this (the default is now standalone true).\nI this the problem described here is fixed - though better work is ongoing to fix the root of it.\nIf you are still having an issue, please reopen.\nThanks for the report!\n. @thesamet @amaltson can you describe what is still happening?\nStandalone true is now the default in 0.7.3 - so, there is no longer a need to specify it as an env var.\nAlso, env vars parsing has been fixed on master.\n. @huataihuang why not use the container? what (complete) command did you use to launch the registry?\n. @hugoduncan you miss standalone=True, as pointed out in this thread, which is the default in the shipped configuration.\n. @fairbairn please read the previous comments.\nWhat's your registry configuration? Are you positive you use standalone=True ?\n. @fairbairn\n1. don't run 0.7.0 or 0.8.0, please use the latest . tags (0.7.3 and 0.8.1) - that's the only way you will get a secure version\n2. your configuration doesn't inherit common (neither dev nor prod), so you are NOT running with standalone=True\nHave a look at how proper configuration is done in the sample configuration:\nhttps://github.com/docker/docker-registry/blob/0.8.1/config/config_sample.yml\nor better, use the sample configuration and override behavior with ENV variables instead\n. @fairbairn I'm sorry you got tripped over configuration changes. We are getting better at documenting things properly, but then in the future you should expect having to review the changelog and upgrade your configuration for every revision before we hit 1.0.\nAbout https, we don't natively support it - the recommended way is to use a front nginx (look into the contrib folder and documentation for some config snippets).\nHope that (finally) helps.\n. @bhuvaneswaran can you read the previous comments and confirm that your registry is running standalone=True?\n. @bhuvaneswaran can you copy your registry launch command, and any other \"flavor\" from your config that your registry uses? Thanks. \n. To everyone - before you add a comment here, please consider doing this:\n1. verify that your config does specify standalone=True, either in config or on the command line\n2. verify that your registry agrees with you: curl -i registry-1.docker.io/_ping | grep Standalone\n3. if you see X-Docker-Registry-Standalone: False from step 2, then you messed your config, so please start over again using the provided default configuration that matches the registry version you are using, and go back to 1\n4. if you are positive (from 2) that you are running standalone and still see 405 errors, please open a new ticket to report\n5. in all cases, please refrain from adding more comments about \"seeing this as well\" without any configuration / launch command detail\nThanks a lot. \n. @bhuvaneswaran have you read my last comment?\nAlso, have you read the previous one? (https://github.com/docker/docker-registry/issues/432#issuecomment-56873148)\nYour configuration fails to inherit common.\n. @fairbairn you are right.\nThanks.\n. Readme is in line with the default configuration now.\n. I would prefer a python like True (capped T) but that really doesn't matter. LGTM - let's have this in fast.\n. The samalba/docker-registry container, although famous, is not official. Furthermore, it's based on trunk, which is by definition NOT stable.\nI would strongly suggest you start by using the official registry: https://registry.hub.docker.com/_/registry/\nIt's quite likely the problem you describe has been fixed in a released version (see  https://github.com/dotcloud/docker-registry/issues/401 )\nI'm closing this for now, but please re-open if you still are having problems.\nHope that helps. \n. Happy it helped :-)\nBest.\n. You said docker-registry version is 0.8, which means you are using the master.\nPlease don't do that, unless you know exactly what you are doing.\nThe master is by definition a version under heavy development, likely broken.\nUse a released version (right now, 0.7.2): pip install docker-registry will get you just that.\nOr better, use the official docker container.\n. ping @samalba @shin- \nHow do you see this?\nShall we go with supervisord?\n. The mechanism is in place to drop privileges - but yes, there are a number of other considerations to look at before this is ready.\n. As a storage driver, it can be called (directly) using either file or local. See: https://github.com/dotcloud/docker-registry/blob/master/docker_registry/storage/init.py#L31 - though indeed for glance storage_alternate: _env:GLANCE_STORAGE_ALTERNATE:file is definitely better.\nAbout the \"flavor\" in the configuration file itself, whichever name is ok and it bears no relation to the driver itself - so, I wouldn't change that.\nNow, the glance driver is largely untested / unmaintained since nobody expressed interest in helping on it - can you detail on what you did to get it work and/or help on setting it a testable environment for travis?\nThanks!\n. Yes, like I said, this: storage_alternate: _env:GLANCE_STORAGE_ALTERNATE:file should fix it (can you confirm?) - the other changes are not needed and I would prefer not to have them for now. \n. @paulczar check that you are using the latest registry (0.7.3), then that your config (if not the default) uses standalone: True\n(and avoid master for now)\n. Hi again @paulczar \n444 is going to take care of this (the storage_alternate part at least, that fixes the problem), so I'm going to close this for now.\nKeep me posted on your progress though.\nThanks for your time on this!\n. ping @shin- \n. ping @shin- where are we with that?\n. @shin- ping\n. Thanks @shin- !\n@wking would you have time to exert your x-rays on this before it gets in and mistakenly hurt unicorns?\n. We can hook a custom error handler that will send to bugsnag -\nhttps://bugsnag.com/docs/notifiers/python\nLooks convenient.\n. Hi @Sirupsen \nIsn't https://docs.docker.com/reference/api/registry_api/ covering your needs?\n. Well, the API is of very little interest (save for docker-core/registry developers) - and it doesn't seem that casual users deploying a private registry need it... \nMaybe in the contributing doc.\n. Related, see: #386 \nping @shin- : have you been following on byte-ranges implementation progress? Any idea where we are with it on the registry side?\n. Ok. Why does the client perform a head request in the first place?\nThanks.\n. Is there a spec somewhere about what was decided about bytes-range support, and how it was going to be implemented?\n. I'm really not keen toward making an extra HEAD request for the sake of testing if the server supports resume or not (that's an extra round-trip to the hub for authentication, hence a good half second extra on top of each layer request - not speaking about doubling the load on the hub).\nHave you tried just going ahead with the ranged request, no matter what?\nWhat will you get?\nEither a 206 (does support range request), or a 200 (doesn't), and you get your answer without the extra round-trip - am I wrong on this?\nObviously needs to be tested against private registry WITHOUT range support and against the actual staging/prod registry.\n. Client now support resume properly.\n. We don't invalidate, but then we don't use extensive caching (save on layers, but that's O.T., and that shouldn't be an issue).\nGot a specific url for me to audit? (eg: on the registry) and/or can you dump the requests from the docker server?\nThanks!\n. Put otherwise, I need a way to reproduce / know what to look for on a simple test case.\nYou can even use the python script I sent you if you don't want to fiddle with docker.\n. That url is the hub (ex-index), not the registry (that's confusing already).\nSo, I need to understand what resource we are talking about exactly.\nEg: if it's anything in the registry.hub.docker.com domain, then you should likely report this to docker-index instead.\n. > I don't think I can come up with a simple test case, but maybe pulling an image, update the repo and pull right after the build is enough?\nYes, and dump the requests from the docker daemon and/or run the python script we used earlier - this is needed to pinpoint which resource is/may be problematic.\nSorry for the bikeshedding :-)\n. @discordianfish any update on this?\nThanks\n. Ok, give me a ping when you get more on this.\nThanks.\n. Closing as dupe of #408 then.\nThanks!\n. There is currently a problem when specifying regions (see #400).\nCan you try without specifying the region and report here if that works better?\nThanks!\n. Indeed. LGTM and merged.\n. Hello @weil \nIt seems to me the push failed, and left the repo in a broken state.\nAs a short term solution to workaround that, I would recommend you remove the repo and push again. If the push fails again, I would love to see some logs.\nIf I'm right, this is likely the same as #366 \n. @chewmanfoo what kind of storage do you use? you can find your data there and manually verify what's in and what's not.\n. If there was a successful push, then there is data.\nCan you check there what you have?\nNote that the storage path points to filesystem inside the container.\n. Ok, run your docker daemon with -D so that you see the daemon output, then try to pull again.\nIt will eventually give you the exact url that fails.\n. docker pull myrepo:PORT/foobar/baz is how you pull foobar/baz from a private registry.\n. @chewmanfoo I'm happy if we can help you - now, this ticket was about a specific issue that you don't seem to have - and which is getting drowned now.\nIf you would, as your problems are more related to docker usage and help on it, can you join on irc and ask for help there?\nThanks for your understanding.\nBest regards.\n. Closing as a dupe of #366 \n. @djl197 can you check that your registry is reachable from where you try to push? (simply curl docker-registry.DOMAIN).\nBtw, does your registry really run on port 80? If not, add the port in your tag and push snippet.\nFinally, can you report what registry version is that, and possibly the way you launch it and config/environment vars.\nThanks!\n. I would suggest you first try to get it running without Apache in front - and also please skip the init scripts in contrib and any other third-party / advanced / alternative stuff until you at least get the basics running.\nSo, just start with the recommended, simple way, from the official documentation:\nhttps://github.com/dotcloud/docker-registry/tree/0.7.3#quick-start\nThat's: pull the official registry image and docker run it. Once it's up, try to docker push and pull to it.\nWe will get you running ;)\n. @Furze so your sql database is not configured properly.\n\nWhich is not great for a production environment..\n\nWhat do you mean?\n@hcguersoy \n\nThe only workaround that I've found is to use the IP instead of the hostname. \n\nThen you have a DNS problem - this is likely an engine (/go) problem, which you should search for/report on the main docker repository.\n. @chewmanfoo your macafee antivirus / proxy seems to be the problem, right?\n. @djl197 is it working for you?\n. @djl197 any update?\n. Closing. @djl197 if you still need help with this, please reopen.\n. ping @shin- \n. Merged\n. Why?\nBoth will be parsed correctly by YAML\nyaml.load(\"True\") == True\nyaml.load(\"true\") == True\nand True feels more pythonic.\n. Consistency is a good point :-)\nMerged.\n. ping @bacongobbler :)\nJust an idea: what about adding a dockerfile extending the official image plus nginx with that configuration?\nThat would definitely help people looking to setup with nginx.\n. @bacongobbler we have issues like https://github.com/docker/docker/issues/7485 where people still have difficulties setting this up.\nI would love to have a canonical nginx configuration (+ Docker image would sure be awesome).\nEither way, you don't seem to pass on /v1/users here - is it not needed? Any idea why it's not working for @matthughes ?\nThanks a ton!\n. @bacongobbler let's start with this PR - let's make sure that this suggested configuration works flawlessly, and let's merge this - then I'll bug you again with the rest on another ticket indeed :-)\nSo, @rosstimson @matthughes or anyone else interested, can you start with @bacongobbler suggested nginx configuration and report:\n- if that works\n- if it doesn't, what doesn't :-)\n. I don't get it.\nIn the sample config, the LRU config is commented.\nAm I missing something?\nPut otherwise, it seems correct to me to have these commented (and people enabling it in their config if they need it).\nNow, this can be changed, sure - I just want to understand what's the benefits.\nThanks!\n. Thanks for your comments and time on this.\nI don't like the way it is today indeed - and I don't like the way it was before (default values spread out in code rather than in config).\nI'll look into this soon.\nThanks again!\n. LGTM.\nMerging this as-is as the fastest path to 0.8.\nWill get back to a better solution later.\n. So, boto is complaining about not being able to reach your bucket.\nI'm not 100% positive, but this:\n-e HTTP_PROXY=http://PROXY/ \\\n-e http_proxy=http://PROXY/ \\\n-e HTTPS_PROXY=http://PROXY/ \\\n-e https_proxy=http://PROXY/ \\\nlikely doesn't have any effect, and boto just choke on your proxy trying to reach your bucket.\nQuite frankly, I would rather discourage the use of the registry like that, separated from the bucket by a proxy - you are likely opening a huge can of worm, and this is entirely untested (and unlikely to receive much love / development focus <- unless by you?)...\n. > Setting those environment variables is required for anything to even begin to function, since without them the S3 request times out. This is expected and is the reason why the proxy is being used.\nYes, I get that.\nThen again:\n- working from behind a proxy is not supported or tested, nor developed for - which means other parts of the registry are likely to simply ignore proxy settings \n- this is likely a boto bug (look at the stack trace), not a registry (per-se) bug\nIf you have a patch, it's certainly welcome, sure.\n. I'm closing this for now, as no work is planned on this:\n- for the lack of a test environment, to boot with\n- likely a boto bug\n- no interest in supporting such a configuration\nalthough like stated, we would certainly accept a PR.\n. LGTM\n@shin- @wking you guys want to take a look?\n. Ok. @shin- it's yours to merge if you are happy with it.\n. Hi @Aigeruth \nThanks a lot for this!\nI commented inline.\nAbout the build failure - hacking is very picky and doesn't like commit messages to end with a period. I'll likely disable that (admittedly not so smart) rule - meanwhile, you can simply rephrase your commit message.\n. Thanks a lot!\nLGTM - @shin- what do you think?\n. @shin- I was sure you couldn't resist it :-)\n@Aigeruth thanks for this!\n. @wking @shin- what do you think?\n. Yeah, I would like 2.6 compatibility (we are not that far from it) - because it seems it's still the standard on some linux distros :/\nThanks for the review @wking \nLGTM\n. @thebyrd indeed, you should edit a repository description using the web interface on the hub.\nThere is nothing else to it for now AFAIK - though there is a PR on docker core to add a description command: https://github.com/dotcloud/docker/pull/6667\nI'm going to close this as there is no registry issue per-se (it's either hub or core).\nIn the hope that helps!\n. Thanks @shin- - maybe we should even add that on top of the README?\n. @autumnw python 2.6 is unsupported / untested. You should stick with python 2.7 - which is why the recommended way to use the registry is from the official docker image.\nAbout the glance issue, I'm closing this ticket in favor of https://github.com/dmp42/docker-registry-driver-glance/issues/6\n. @autumnw why not run the official docker image for the registry instead of installing it on the host?\n. There is no other reason than to be absolutely sure we have a 100% reproductible environment with no surprise.\nI certainly understand distribution packagers' frustration - and I certainly agree with most of the points one can make in favor of weak(er) dependencies.\nNow, fact is we had bad surprises in the past with that...\nUltimately, I guess I could live with something like bugsnag>=2.0.1,<2.1\nWould that help?\n. How do other python application projects resolve this?\nLike I said, I understand the packagers' pain, but hear mine (when I'll get flocks of people complaining about impossible to reproduce bugs originating from broken deps :-))\n. Thanks @wking \nThe question though is more what to put/use inside setup.py to accommodate both package maintainers and us.\neg: do we link to frozen.txt or to main.txt from setup.py ?\nObviously (to me) we link to frozen.txt - but then again, how does that help @mika / package maintainers? Do we expect them to NOT use the provided setup.py, or to fork it so it points to the less strict req file?\n. > Folks who are interested in installing their own non-dev docker-registry should run something like:\n\n$ pip install -r requirements/frozen.txt\n  $ python setup.py install\n\nI would like people to go only with:\npip install docker-registry (or pip install . if they insist on using master)\nThat's the only way they are going to get exactly what they need (including python version dependent additional packages and optional packages).\nDevelopers can  sure mess it up, but the general public is encouraged to stick with that :-)\n\nIf you can't have softer dependencies in setup.py\n\nIndeed, I'm not keen on that. We already had problems in the past with minor revisions breaking on us, and going that way (eg: soft deps) is a PITA for us (people already have a hard time stating clearly what registry version they are using...) - and we are just not mature enough for non-fixed deps, sorry.\n\nplease at least have them in one place, and not scattered around multiple files, so it's easier to patch the dependencies away\n\nRight now, you need what's in requirements/main.txt for all pythons versions, and you also need to have a look at setup.py for version specific deps and additional gymnastic (and I don't think I can merge them in just one file, but if you have a solution for that I'm listening).\nIf you intend to install with options (eg: [bugsnag]), then you need the additional dep as well. Here again, I don't think I can make that much simpler.\nNow, we can probably help: if you are maintaining a deb/rpm package, please ping us here and let's find the best way to make your job easier while keeping ours sane.\nHope that helps!\n. > \u201cour users have trouble reporting the version their using, but will get better as our project matures\u201d\nNot what I meant - sorry I wasn't clear:\n\"our users have trouble reporting the version they are using, but WE will get better at providing them with simple to use debug/reporting tools giving us the information we need as our project matures - right now, we don't, and debugging the registry is admittedly voodoo to the general public\".\n:-)\n. @shin- can you move us to bugsnag>=2.0,<2.1 so we close this ticket specifically?\n. LGTM\nThis hasn't been touched for quite some time. Saying that it \"doesn't provide all the features of the Python implem\" is likely an understatement...\n. Hi there\nCan you provide:\n- your registry version\n- your registry configuration\n- the complete command you used to start the registry\n- docker client version\n- docker daemon log in debug mode (-D) could help as well\nThanks.\n. On 0.7.3, the last official release:\n- setting STANDALONE=true is\n  - useless - this is the default behavior in the provided configuration\n  - likely won't work (the 0.7 branch is broken on that)\nMaster is fixed, and work as expected as far as STANDALONE is concerned.\nSo, remove any standalone true statement in your launch command - if you have a custom config file, specify standalone=true in it (like the default configuration does).\nSorry for the inconvenience about this.\n. @veverjak can you detail what you do to run the glance driver using a built registry from git? Thanks.\n. @veverjak thanks a lot! so, you are not running the glance driver, are you?\n. @veverjak \n- the public docker registry itself runs of a tag (the latest stable tag, right now: 0.7.3) (this is deployed manually)\n- the image of the registry available on the hub is also the latest tag (this is set manually in stackbrew): you find it here: https://registry.hub.docker.com/_/registry (and we don't make master available)\n- if you want an image that is built of master (automatically), you can try the unstable tag on  https://registry.hub.docker.com/u/samalba/docker-registry\nYou can also setup an automated build for yourself using the git master.\n. @davemackintosh I believe this is fixes (by the various PR we merged about this) and the suggestions in this thread - hence I'm closing.\nIf you still feel this need addressing, please go ahead and reopen.\nThanks!\n. @wking if I understand correctly, @matthughes problem is that he wants to work offline - meaning pip can't fetch the dependencies.\nAbout the canonical syntax, it's: pip install docker-registry or pip install . - just installing the (main) requirements will (likely) fail and is not recommended.\nNow, I fail to understand how a deb (or rpm) package would solve this - that package would still depend on a number of additional dependencies that need to be fetched, correct?\n. @wking \n\nWhat doesn't get pulled in?  Looking at setup.py, it looks like some additional dependencies are injected dynamically\n\nThen you got your answer, right?\n\nFolks on Python 2.6 could:\n\n$ pip install -r requirements/python2.6.txt\n  $ pip install .\nI don't want it.\nThis makes it uselessly complex and generally not usable (save for registry developers and other maniacs), and the general public should get the most simple solution possible...\n... which is, and will stay pip install docker-registry (no matter what python version they use).\nOptions and extra you should get with pip install docker-registry[someextra].\nSorry to be stubborn, but that piece of software is already overly complex for most people, and I really want it to be more simple to use generally.\n. @matthughes \n\n[about the rpm and deb] How does that get built?\n\nI don't know :-) - but the debian and red hat guys likely do.\nI guess we can look into their source for that?\nAbout your manual method, you are likely missing dependencies.\nWhy don't you just: pip install --download=stagingDir docker-registry then pip install --no-index --find-links=stagingDir docker-registry instead?\nAlso, python 2.6 unfortunately is not yet fully supported, and likely won't work correctly in some cases.\n. I like some of your ideas there @wking - and you are right this is a somewhat complex problem to solve, given the variety of user stories here.\nWe should definitely look into this (likely post 0.8 though - I've plenty on the plate right now :/)\nBest.\n. @shankarj \nWhy are you doing that?\n. You missed steps following that tutorial: it's clear that you haven't done git checkout 0.6.3.\nAnyhow, this tutorial is height months old and give instructions for a quite old registry version that we don't support anymore (0.6.3). You should refer to the official documentation instead, and try the latest version.\nHope that helps.\nBest.\n. LGTM\n. Like it.\nNow, do we need a separated conf var to enable this?\nMy point being, could we \"centralize\" things, and when (say) a given \"debug\" variable is true, activate all debug endpoints / logging / etc, instead of (possibly) ending up with different vars for different infos?\nWhat do you think?\n. > Fine with me.  It's easy to swap in whatever variable name you like, just tell me what you want ;).\nLet's have a \"debug\" key.\nComment should read something in the line of: \"enables additional endpoints and system informations to be logged or displayed (endpoints: _versions). Please be aware that this WILL leak possibly sensitive system informations and that you should NOT enable this for production systems.\"\nMake that wording yours (your english being vastly superior to mine :-)), I don't quite care as long as the spirit is there.\n. I would still prefer \"debug\" - true, for now it's only about enabling endpoints, but ultimately, that's for putting the registry in \"give me all the infos\" mode, whatever it does (endpoints, extended log info, whatever).\nThe text is perfect :-)\n. @wking ping :)\n. Ok. Can we then call this \"debug_versions\", then?\n. LGTM and merged.\n. @peterwillcn would love to help more on this. I'm closing this for now, but if you can follow on @bacongobbler suggestion and report, be sure to reopen and let's follow up on this.\nBest.\n. @Aigeruth \nIt is ignored now - but it probably should not be.\nThe engine should/could do additional verifications on the checksum (when it tries to push a layer and first check if that layer is already there). \nSo, I don't think this code should be removed (if admittedly unused, like you pointed out).\nBest regards.\n. @Aigeruth is there an issue tracking that on docker engine?\n. @Aigeruth Yes, that's what I meant (if that issue doesn't already exist).\nTo sum it up:\n- change the PR so that you rename the header to match the request header name\n- ensure there is a ticket on docker engine to check the checksum when pulling\nThanks!\n. @Aigeruth forget about my second request for now (there is an ongoing talk about this in docker core).\nStill willing to have it renamed rather than removed, though.\nThanks!\n. LGTM - @shin- ping\n. Merged.\n. @tommyblue the users endpoint is not mounted unless you are standalone.\nSo, it seems to me you are missing standalone True in your config?\n. > Sorry but the config options seems to me poorly documented\nWe are guilty on all counts.\nYou may find some confused bits over there: https://github.com/dotcloud/docker-registry#authentication-options\n... and the default configuration does have standalone at true (https://github.com/dotcloud/docker-registry/blob/master/config/config_sample.yml#L6).\nI'll try to make the doc a bit better for 0.8.\nHappy you got it working still!\nBest.\n. Please use 0.7.3\n. Can you copy your config_sample.yml ?\nThanks.\n. Put otherwise: are you sure you copied config_sample from 0.7.3 and not from master?\nHere, things work OK when using the config from 0.7.3 with registry 0.7.3 - and they don't when using config from master with registry 0.7.3 (which is expected, given #444).\nCan you confirm?\n. LGTM\n. Hi @lyda \nStorage drivers should be implemented as standalone packages.\nHere is (a good) one, for inspiration: https://github.com/noxiouz/docker-registry-driver-elliptics\nAnd some (very minimal) notes about how that works: https://github.com/dotcloud/docker-registry/blob/master/CONTRIBUTE.md#storage-driver-developer-howto\nWhat you did here looks very good. My point is only that I won't merge this into core - but you don't need me to :-) as you can author your own (github+pip) package, which is the beauty of it.\nSo, I would advise you start a github repo, named docker-registry-driver-hdfs and go with that.\nBe sure to ping me there for any help / question.\nThanks for your work on this!\n. ;)\nKeep me posted on your progress.\n. I only vaguely recall a flask key thing (was it related to sessions persistence?).\nAnyhow, it's no longer there. You don't need anything special as far as flask / the registry itself is concerned.\n. LGTM\n@shin- what do you think?\n. Ok, then the first thing would be to add a new method to the driver interface, like \"status\", that returns a default meaning \"everything ok\" (that way we don't need to bump the API) <- format to be defined.\nThen drivers could/should override it to return actual, useful infos <- format to be defined.\nThen we need an http endpoint that maps to that <- activated by default? only available with debug / some env config?.\nPing @wking and @shin- : what do you guys think about this? Kind of related to the effort started by @wking on #473\nThanks @noxiouz !\n. @wking I see a benefit in simplifying/centralizing things for the user - that could even be rolled into a generic \"get-everything-debug-infos\" endpoint on the registry that would spit out everything needed to report a bug.\n. @noxiouz cool!\nWhenever you are ready, go ahead and PR it.\nMy requirements are:\n- work with existing drivers without having to do any change on them\n- limited footprint on the driver interface\n- default behavior should take care of not accidentally disclosing sensitive information to casual users\n- minimal maintenance for me :-)\nAnything else is up to you guys!\nMaybe we can also ping @bacongobbler as well (he is working on another active driver) in case he would have interest for this.\n. Travis doesn't like you today :-)\n. Github is not happy :/\nWeird.\nCan you have a look at it?\n. LGTM\n. What are the security risks associated with this?\nIs this disabled by default?\nThanks!\n. Hello @S1E11 \nWhy not use application wide settings for this, instead of decorating each and every method?\nhttp://flask-cors.readthedocs.org/en/latest/#application-wide-settings\nThanks a lot!\n. Note that #345 is related.\n. Then let's remove these already CORS-enabling snippet, clean-up all that and have application-wide setting.\nPeople who want \"*\" can still have it in the configuration.\nWhat do you think?\n. Yeah, I would love a decorator-less, cleanly aside solution. Please go ahead with this and see where that leads.\nThanks for your efforts on this!\n. Ok, I'm starting to like this better.\nIs there a way we could simply get rid of all decorators? Yeah, all endpoints are then accessible, but that's ok IMO.\n@shin- what do you think?\n. @S1E11 any update on my question?\n@shin- what do you think of it now?\n. Ok, @shin- what do you think?\n. @shin- ping\n. @S1E11 only one nitpick on my side and this can be merged. Thanks!\n. Ok @S1E11 this is starting to look very good.\nGetting back to a global setup without the need for decorators - what about this? https://github.com/wcdolphin/flask-cors/blob/master/docs/index.rst#simple-usage\nCan't we simply do that:\napp = Flask(__name__)\ncors = CORS(app)\nThanks!\n. Thanks for your patience @S1E11 !\nI have a simple question (see above).\nI would like an update to the README with the doc.\nI would love you that you \"squash\" all these commits into one.\nand then we can merge this!\nBest\n. @S1E11 flake8 is failing - you can run tox locally to identify the reason for the failure, or pip install hacking then flake8 \n. LGTM\n. LGTM\n. I don't think it's linked to S3 (vs. filesystem), but rather dns issues.\nCan you:\n- restart your registry, using S3 as backend\n- dig your domain name from the host where you run docker, and verify the ip match\n- restart your docker daemon\n- push something named \"test/busybox\" onto your registry\n- pull from ip\n- pull from address\nThanks!\n. @shin- ping\n. @shin- STORAGE_PATH being an issue (on S3) does ring a bell\n. Ah, nice one! LGTM\n. Let me digest this and get back to you soon.\n. @shin- @wking what do you think?\n. @bacongobbler where are you with this?\n. @bacongobbler if that were cleanly abstracted enough from the core of the code, I would merge this here, so, feel free to start working on a clean PR for this if you want / have time.\n. Hi,\nCan you:\n- curl http://localhost:5000 and copy the output ?\n- dig docker-registry.xxx.com\nThanks!\n. LGTM\n. As you guessed, there is a strong sentiment against this, as modularizing components (eg: S3) that are part of mainline registry only had extra maintenance burden with no benefit for the core team.\nNow, you are right on principles - and we already went this far with modularized drivers, so, why not go the last mile? I'll look into this again after 0.8.\nKeep-up the good work on swift, mr. bacon!\n. @wking @shin- what do you guys think?\n. @smarterclayton we merged a new extension model that let you author (and use) python modules consuming the core signals (look under extensions for the README).\nIf you feel that system fulfills your initial need, please close this.\nBest.\n. > docker pull my-resgistry/ubuntu:12.04\nTry pull my-resgistry:443/ubuntu:12.04\n. Ok, when things go sideways, get back to basics: \n1. dig your domain name and check the ip match what you expect\n2. curl on registry endpoints and check it replies\n3. restart docker daemon, and try to pull again\n. location /v1users {\n    rewrite /v1/users permanent;\n}\nYou miss a slash here, right? That's /v1/users, not /v1users\n. /v1/users/ being 404 is a clear indication that it's not running standalone.\nEither way, standalone True is the default behavior - so, you don't need to add it to your launch statement and you really shouldn't with the 0.7 branch.\nSo, please double check how your registry is really launched, and if it's really running of master.\nAlso, please consider simplifying things to start with, eg try to have it work without nginx.\nHope that helps.\n. Reading this again: you pulled from git master from inside the container - which is likely quite tricky - it doesn't seem you were updating the configuration for eg.\n0.8.0 is out, with fixes for most of the issues you pointed at - so I'll consider this resolved for now, but please reopen if it still doesn't work.\n. When running in standalone mode, the registry mimics the index and provide some of its API.\nCan you be more specific on what you think is missing?\nThanks. \n. Have you tried the registry search? https://github.com/docker/docker-registry#search-engine-options\n. https://bugsnag.com/docker/registry/errors/53d8c86b397156561adf9611\n. Prometheus is too buggy for now, and is rather complex to setup.\nUntil we need such a powerful (though buggy) tool, we are going to stick with industry standard reporting tools.\n. @bacongobbler not likely\nI would like it to be transparent - the default interface would additionally provide a new method that could be overridden (but not necessarily).\nNow, this one is more generally about the registry side of things.\nAbout what could be done on driver land, this one is related: #482\n. Can we log \"db\" as well?\n. LGTM\n. Ok, so, your issue is with pip - any python package (not registry) you would try to install would fail the same, right?\nI suggest you seek advice from:\n- your system administrator to verify that your https proxy works as expected\n- SO and other user resources - like http://stackoverflow.com/questions/14149422/using-pip-behind-a-proxy\n- pip support\nI'm really sorry to shut you down on this, but this really is not something we can help you with...\nThe best I can do is suggest you to manually download all packages from pypi in your browser, then manually install - but that really is a PITA and you should better have pip behave.\nHope that still helps...\nRegards\n. https://bugsnag.com/docker/registry/errors/53fb4436397156561a18490a\n. Well. To boot with, I don't trust mocks (#365) - so, yeah, I don't trust tests - whether I'm right on this or just a psycho on the loose is yet to be determined :-)\nNow, our tests ((un)fortunately?) are too much unit tests - and we lack integration tests (including simulating complete user operations).\nBut yeah, taking the occasion to enhance tests is welcome! \n. @johnae the images you try to push, do they come from a previous save/load operation?\n. @johnae I'm trying it now.\nOn your side, can you copy:\n- your registry configuration\n- the command you used to install and to launch the registry\nThanks a lot!\n. @johnae: can't reproduce. Just started a fresh latest registry (0.7.3), using docker 1.1.2. Pulled busybox. Tagged it. Pushed it. Ok.\nCan you provide the requested infos, then try to provide a reproducible test case with step by step to reproduce?\nThanks a lot! \n. @johnae can you run:\ncat /var/lib/docker/graph/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json\n. Can you ls -lA that file?\n. @johnae \nOld docker versions used to have camelcased key - that was normalized some time ago.\nBelow is what I have:\n{\"id\":\"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\"comment\":\"Imported from -\",\"created\":\"2013-06-13T14:03:50.821769-07:00\",\"container_config\":{\"Hostname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":null,\"Cmd\":null,\"Dns\":null,\"Image\":\"\",\"Volumes\":null,\"VolumesFrom\":\"\"},\"docker_version\":\"0.4.0\",\"architecture\":\"x86_64\"}\nand below is what I read on the official docker registry backend:\n{\"id\":\"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\"comment\":\"Imported from -\",\"created\":\"2013-06-13T14:03:50.821769-07:00\",\"container_config\":{\"Hostname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":null,\"Cmd\":null,\"Dns\":null,\"Image\":\"\",\"Volumes\":null,\"VolumesFrom\":\"\"},\"docker_version\":\"0.4.0\",\"architecture\":\"x86_64\"}\nSo, it might very well be that you pulled busybox sometime ago with an old docker version, and that layer stayed here.\nUnfortunately, my only suggestion is that you rmi the images - then pull them again.\nIf you can do it, can you confirm this fixes the problem (and that the layer json contains a lowercase id)?\nI wish I had a better solution...\n. Yeah, like I said, load and save were terribly buggy, even recently - so imported tarballs is likely guilty here...\nHappy it helped, anyway.\nShall I close this?\n. LGTM\n. LGTM\n. LGTM\n. @kencochrane so far it looks like this is intermittent name resolution failure. The infra is reluctant to do something about it for lack of manpower and interest, so the suggestion is that we bundle a caching nameserver (bind or else) into the containers - which I will do, and see what that gives.\n. @sheldonh likely a configuration issue on your side indeed. This specific issue is unrelated, and is about docker registry itself having issues connecting to the internal infrastructure. I'm closing it btw, since it was solved.\n. This looks to me as a boot2docker networking issue. Can you report there https://github.com/boot2docker/boot2docker and link this ticket?\n. Still, the docker engine tries to connect to 153.88.253.150:8080 - this is a boot2docker (or docker engine) issue.\nThe fact your curl reaches the registry demonstrates the registry does work properly here, and there is unfortunately nothing we can do on the registry side to help you.\n. If you have a system wide proxy configured, docker will use that - and this is likely here.\nDefault bind for the registry container should be on 0.0.0.0.\n. Good catches. Must have been sleepy coding - will amend.\nre: python compat - this is for python3 actually, but you are right.\n. You got your semver zen, mister :-)\n. @shin- ping\n. Fixes #447 and fixes #511 \n. Hi there.\nI'm willing to merge this, if you can make this patch cleaner / smaller.\nSpecifically, the bits in Dockerfile are useless - also, you are likely breaking cloudfront signed redirects, etc.\nThanks.\n. @lorieri please, take the time you need :-) - it's ok.\nThanks for your interest and contribution.\n. Thanks @lorieri \nI now realize there may be some overlap between this and #461\nMay I ask that you guys (both here and on the other ticket) sort it out and select the best solution for all use cases?\nThanks a lot!\n. @carmstrong I'd be happy to merge something that would cleanly cover both this and #461\nIf everybody is happy with (and has tested) this in its current state as a solution, then let's go.\n. Ok. Some trivial nitpicking on the README.\n@shin- are you happy with it?\n. And yeah, best of worlds, I would love the commit history to be clean - if you can squash...\n. @lorieri @carmstrong I understand the keys were there already, but that's cleaned-up in #563\nwhich is why I would like the new additions here to not use them.\n. Closing here since there is a new PR.\n. @shin- ping\n. @tdooner does this adress your question? If not, please reopen.\n. @envygeeks I guess you are talking about the hub?\nThen your question should go to https://support.docker.com\n. Thanks @bacongobbler \nI would just add that you need to add this: --basic -u username:password if you want access to your private stuff.\n. It seems the exception is unrelated to the storage used / the way it's used.\nIntegrityError: (IntegrityError) UNIQUE constraint failed: repository.name u'INSERT INTO repository (name, description) VALUES (?, ?)' ('library/cem_base_centos', '')\nThis tells me the search index is not happy about something being already there.\nPing @wking: any suggestion on this?\n. @wking we are speaking filesystem storage here (not S3) - so, consistency issue seems unlikely.\n. @chewmanfoo \n\nJust to be clear - this registry has no persistent storage - it's just for test. \n\nWell, you told it to have persistent storage, right?\n\nsudo docker run -d -e STORAGE_PATH=/tmp/docker-store -v /tmp/docker-store /login/sg218049/docker-store\n\nOr am I missing something?\n@wking so, does that change the scenario / explain why we fail?\n. @chewmanfoo any update?\n. @wking I reproduced.\n1. start fresh\n2. DOCKER_REGISTRY_CONFIG=config_sample.yml SETTINGS_FLAVOR=test nosetests so you get some content\n3. rm the index: rm /tmp/docker-registry.db\n4. start the registry and crash: DOCKER_REGISTRY_CONFIG=config_sample.yml SETTINGS_FLAVOR=test docker-registry\n. Can't reproduce any longer :/\n/me scratches head.\n. @wking any idea?\nMight we consider disabling the search engine in the default configuration?\n. @michaelheyvaert I guess that's possible - though that would make things more complex for the end-user. I don't quite understand why there is a race-condition (and it looks like there is) in the first place...\n. @wking why does preloading prevent the race condition?\nAlso, can't we solve the underlying issue (would that help? -> http://skien.cc/blog/2014/02/06/sqlalchemy-and-race-conditions-follow-up/ )\n. I still don't quite understand it.\nDo you say that our current implementation is not safe to be used in a context where there are multiple workers? Or is it \"just\" for the database creation?\nIs there no mechanism in sqlalchemy to prevent such race condition?\n. > It's hard to lock on database creation, since you'd need an out-of-band lock.\nI don't understand this. Filesystem lock should kick in in the first place (unless running some exotic fs). So, other db creation attempts should fail (gracefully).\n\n[the rest]\n\nArrr, that sucks... that kind of thing should be thought to be safe from the ground-up...\nI hear you it might not be worth the effort, but this is definitely something we don't want to repeat in the future. No two workers should try to initialize asynchronously...\nLet's merge the preload hack then.\n. @ckulla run with preload option: https://github.com/docker/docker-registry#sqlalchemy\n. @davidkelley you can use preload to workaround this (see previous comments)\n. @stevenjack what version of the registry are you running?\nWhat shell is this?\nAnything else custom here? (eg: configuration, other env vars)\nThanks!\n. @stevenjack sure, go ahead!\n. @tianon that affects only the search endpoint. If you don't need search capabilities on your private registry, just disable it (bottom-line: beware with preload, as it causes thread issues with S3).\nIf you do need search, use preload for the first run, then drop it.\n. @silarsis can you squash the two commits? (makes for a cleaner commit history)\nIf hacking is giving you a hard time, don't put too long a commit line.\n. @wking I assume this is LGTM from you?\n@shin- ping\n. @shin- I need a validation about the log format change - otherwise LGTM\nThanks a lot @wking and @silarsis - I like this!\n. Thanks a lot.\nMerged - will do for 0.9.\nGuys, keep up the excellent work!\n. There is in the readme and config_sample, right?\nAnyhow, for now, this doesn't seem to move, so let's keep it that way.\n. You are right, I got confused - haven't merged yet.\nThanks for the heads-up!\n. @danielkraaij thanks a lot for the thorough report.\nPing @shin- @bacongobbler - what do you guys think?\n. So, could we add the same message on pull as well?\n. See? Told you that was trivial :-)\n. Yeah, net benefit is now we are running the core test on travis as well - and we have an actual redis installed, which means we could get rid of the (crap) mock.\n. @mika this is implemented at setup.py level, so that should work for anything that uses it.\nKeep me posted on how this goes for you.\n. @wking thanks!\n@shin- can you review?\n. This was reported earlier IIRC - going to fix it for the next release.\nMeanwhile, I'm afraid your only solution is to manually remove the image from the backend storage.\n. Closing as dupe of #366 \n. @shin- can you look into that?\n. @danielkraaij is correct - you need a CI / build time that will get webhooked on github and that will then build and push the images to your private registry.\n. @zedtux so, you suggestion would mean using ssh as a transport (instead of http) - which is an idea.\nThough, this is largely irrelevant to this project (docker-registry) and something you should rather suggest on the main docker repo / discussions.\n. It is a security concern.\nIf you return a 401, then you leak information (you acknowledge that there is a repository by that name - attacks like that typically enumerate all possible names to discover which exist and which not).\nFurthermore, look at how github operates with private repositories: it does return 404 (if not logged-in), so, I don't think we are doing something that goes against common practice here.\nI'm going to close this ticket for these reasons, but am certainly open to continuing this discussion if you want - or reopen this ticket if there are compelling reasons.\nThanks for bringing this up!\n. @dsw88 by scrubbing through this: https://github.com/docker/docker-registry/issues?q=is%3Aopen+is%3Aissue+label%3Adelete\nyou will find other attempts at cleanup scripts and more information about deletion.\nHope that helps. \n. Can you please report this to the support? https://support.docker.com\nThis is an issue with the hub - not the open-source registry (here).\nThanks!\n. @matthughes yes, simultaneous push of content is likely not well supported. Admittedly, this is unfortunate.\n. @jessemyers I would rather fix the root of the problem.\n. Well:\n- I don't want a specific flag for that in docker - either the current behavior is correct, or it's not\n- the API will change with content addressable ids - making it unlikely that it will be fixed for the current v1 version (where concurrent upload hasn't been given much thoughts)\n. I'm definitely open to a PR fixing this.\n. This tells me you are having issues reaching your S3 bucket.\nCan you look into details on that front?\nAWS region?\n@shin- what do you think?\n. Right - it's not possible if you are running it inside a docker container (possibly a gevent version issue).\nKeep me posted.\n. @Henkis the 404 are irrelevant.\nWhat indeed matters is the timeouts reaching your bucket.\nUnfortunately, I have little help to offer as far as debugging is concerned. I would go starting a python script from scratch using boto S3 (https://github.com/boto/boto) and query my bucket repeatedly until I trigger the issue, then dig into boto.\nKeep me posted on any progress on this.\n. @Henkis can you try bumping your boto timeout values:\n[Boto]\nhttp_socket_timeout = 60\ninside boto.cfg\n... and report here if that helps?\nThanks.\n. Ok, the greenlet is timing out actually. That might be a gunicorn bug you are hitting.\nBy any chance, would you be able to run of master? (I know there are quite some changes...) - or better, force gunicorn to 19.1 instead of 18.0 (inside requirements/main.txt).\nThanks!\n. At least the greenlet is no longer going in lalaland.\nSo:\n- do you experience degradation and ultimately unresponsiveness after these errors?\n- are you running nginx in front of your registry?\nThis smells to me like a known regression where cancelled requests wouldn't be handled correctly.\nAssuming you are indeed behind nginx, can you try killing keep-alive? (eg: proxy_set_header  Connection \"\";)\n. @mcadam that's likely this: https://github.com/benoitc/gunicorn/issues/818\n. @garo and others\n1. as a workaround, I would suggest you delegate the layer delivery directly to S3 (see storage_redirect under https://github.com/docker/docker-registry/#general-options - doc sucks but it should be easy to figure out from S3 driver code) - alternatively, you may even use cloudfront directly on top of the S3 bucket and have the registry 302 to CF signed urls (ping me on irc docker-dev if you need help)\n2. this will receive some scrutiny for 1.0 release - now, most of the energy is dedicated to V2 (in go - branch \"next-generation\"), expected to land Q1, so, I don't want you to expect too much from the current version and would advise you use 1. from above to workaround your actual problem...\n. @all we recently removed ParallelKey fetch from S3 from the registry - it was triggering timeouts for large objects, and cluttering disk space with orphaned temporary files.\nIt's hard to tell if any of the side-effects you experienced here are linked to it or not, but that removal can sure only help.\n. It's code removal (#961).\nYou can test by using the latest master.\n. I would definitely recommend using the Redis LRU cache.\nI would also advise to delegate actual bits delivery to either S3 or a Cloudfront view of it (https://github.com/docker/docker-registry/blob/master/ADVANCED.md#s3-storage-with-cloudfront)\n. @dsw88 I now think you are not talking about the same issue that was initially described here (the recommended workarounds were/are here to address registry side EOF/issues communicating with S3).\nYour problem is more likely on the client side pipeline.\nDo you use an http proxy? or a reverse-proxy in front of your registry? (nginx, HAproxy)\nDo you use boot2docker?\nAlso, you are running a quite old version of docker (1.1.2).\nI would strongly suggest upgrading if you can... \n. @matleh there are unfortunate discrepancies between the official, docker operated index (hence API documentation) and what lives in the open-source registry.\nThat being said, I would rather encourage you to mimick the standalone behavior of the registry (in that mode, it doesn't need tokens from the index, and you are supposed to implement your own authentication means, say, using nginx auth).\nFurthermore, you will likely not be able to use the official docker client against the official index with a custom registry.\nI guess it depends on what you want to achieve.\n. @matleh do you run standalone? I need infos about how you launch your registry and your registry configuration in order to help you.\n. @jdiaz5513 simple auth on top of the registry should be straightforward: https://github.com/docker/docker-registry/blob/master/contrib/nginx.conf\nNow, you are right, this is a problem with docker, and we plan on changing that.\n. @shin- (or @bacongobbler if you have time) can you look into this and confirm that docker 1.2 works with the latest registry and proposed nginx simple auth config (or if it doesn't, investigate why)?\n@matleh thanks\n. @adamhadani are you running the registry standalone? Can you copy your registry launch command and/or configuration?\nThanks\n. @bacongobbler thoughts?\n. 0.8 is released and I wouldn't like to merge in anything breaking things for people - so, if we merge that on 0.8, we need to provide backward compatibility for RSA (possibly with a fat warning).\nThis is low priority anyhow (confirmed by @kencochrane), so, I would rather have it on master - and let's not forget this needs to be done in lock step with the index.\n. @shin- @kencochrane ping\n. @shin- ping\n. Any update on this? (cc @shin- @kencochrane )\n. @huangsam I believe @wking answer address your question.\nIf it doesn't, please reopen this.\nThanks!\n. @bacongobbler it is in your backyard :-)\n. @bacongobbler good point\n@harrykao what do you think?\n. I'm willing to accept a PR that would fix your problem (running on non-standard port) as long as it's verified that it doesn't break running on standard ports :-)\n. Bump @harrykao :)\n. LGTM\n. Ping @shin- \n. @visualphoenix @shin- is owning everything \"mirroring\" and will look into this soon.\n. LGTM\nMerged.\n. LGTM.\nThanks for this!\n. This is Docker's hub - this here is the open-source registry that doesn't provide UI.\nFor such problems, you should rather use the official Hub support platform. (http://support.docker.com/)\nThanks a lot!\n. I'm changing _ping semantic for next release (see #563 and specifically: https://github.com/docker/docker-registry/commit/0570610d9f1ef7acfed5e00d34b5fa608bd9edcf).\nPing will output {} in production usage, and a bunch of additional infos as an object if DEBUG=true \n. > Hmm, changing semantics without an API version bump seems tricky. Will clients have to support both the old syntax and the new syntax?\nI know. I'm barely acknowledging that what we had previously:\n- indeed wasn't very interesting :-)\n- wasn't documented properly in the spec\n- was not valid json (this here)\n\nThe specs suggest an empty response [1], maybe we can update those at the same\ntime to explain the expected empty-object response.  It would be nice to say that the implementation was free to add arbitrary fields to the object, which clients were free to ignore.\n\nAbsolutely. I have good hopes that the documentation will get a facelift soon.\n. Fixed by https://github.com/docker/docker-registry/pull/563\n. Hello @razic \nThis here is the registry. Your issue is likely with docker itself: https://github.com/docker/docker\n. Again, this is a docker issue - that likely got fixed with recent versions, not a registry issue.\nHappy it works now anyhow!\n. @shin- can we add tests (using STORAGE_PATH and not using it) that validate the fact this does fix #486 ? (eg: calling get_tag would fail without your patch and pass with it)\n. @shin- does it fail with the current master?\n. Ok, let's merge it.\n. @nicgrayson can you try bumping your boto timeout value and report if that helps?\neg:\n[Boto]\nhttp_socket_timeout = 20\ninside boto.cfg?\nThanks a lot.\n. @wking ping\n. @irachex ignore hacking / travis for now (looks like something is broken over there).\n@wking this can be merged as soon as it does LGTY\n@shin- can you have a look?\n. Ok, thank you both @wking @irachex \nMerged.\n. @shin- ping\n. Strange that it didn't blew up with the initial PR that introduced that code.\nDouble strange that it broke only for python 2.6.\n. Travis wonkyness... I'm closing this for now - will patch if need be.\nSorry for the inconvenience.\n. Checksums are not used on the index which you are querying here (and likely the engine doesn't send them to the index).\nYou would get individual checksums from GET /v1/images/(image_id)/json on the registry itself I think.\nAdmittedly this is not really consistent, and documentation is likely lagging behind.\n. @zhangpeihao I think the docker engine simply no longer send them\n. Can you copy your complete nginx.conf file?\n. Something is wrong.\nThe log states that nginx tries to find \"ping\" on the localfilesystem while it should query the backend (the actual registry).\nCan you try to ping directly your registry? And/or verify that other endpoints are accessible? (eg: /v1/images/...) \n. Thanks. Keep me posted on this.\n. @mattes did you manage to get it fixed? should we close this?\nThanks.\n. Mixed feelings here. Running a S3 backed registry without caching is generally a bad idea enough.\n@wking @shin- what do you think?\n. Thanks guys.\n@johanneswuerbach what do you think? Would keeping it as a warning with a more explicit / enhanced message help your case?\n. @johanneswuerbach \nThis entirely depends on the chosen storage backend.\nIf you use filesystem, LRU caching won't benefit you much (although, on some systems with bad I/O...)\nIf you use S3 as a storage on the other hand, or any other distributed filesystem, then there is quite some latency accessing that, and caching (small objects) on Redis does give you a huge perf boost (again, this is for small objects only - the layers are never cached).\nWe use a 2GB LRU (but then we host the earth - you likely don't need anything that huge).\nI'm not familiar enough with your architecture, but indeed you might consider shipping with a LRU cache by default - you might pay a small penalty if using direct filesystem, but you will benefit from it in most cases IMO.\n. If you were using the hub, you could add a description manually on the web interface.\nIf not (like in standalone = True), there is no simple way to do that for now (eg: there is no API endpoint that allows you to send a description).\nIndeed, you may edit the database manually.\n. @wking \nYeah, would be cool.\nWhat about adding that into search.py, and have a new endpoint like:\n@app.route('/v1/search/metadata/<repo>', methods=['PUT']) (or anything better in the line) and that would manipulate the database directly.\nI know it's a bit hackish, but at least it's clearly segregated. I would merge that.\n. Put otherwise: I can't make such a thing happen in the docker engine, so, that would be a \"manual\" endpoint that a UI (or a curl command) would access.\n. Yeah, I broke it, and a fix is pending.\nLike @shin- said though, use a stable release, not master.\nsamalba/docker-registry is not official. Please use the official docker-registry: https://registry.hub.docker.com/u/library/registry/\n. @wking if you would have time for a quick look at this one - I know it's not perfect, but IMO it would do some good on the general health - I'll rebase ASAP.\nThe only \"significant\" changes are \"uwsgi/run refactor\" and simplified \"debugging endpoints\".\n. Please, take the time you need! and thanks for your continuous help on this ;)\n. @visualphoenix thanks - merged\n. Rebased\n. @visualphoenix \n\ngunicorn dies with an error about being unable to import extras.\n\nYou need to be more specific than that (eg: exact error message / stacktrace).\n. @shin- will amend according to your review, and rebase.\nThanks a lot for this! (I know this was meaty) \n. It is optional (the extras folder wraps that).\nNow, setup.py does miss the package.\nWill fix.\nThanks @visualphoenix  and @wking !\n. I just forced it.\n. Rebased. PTAL.\n. I hereby summon you, @bacongobbler for thy savvy review and expertise on all things nginx.\nAlso calling @shin- \n. Looks like your new tests are failing.\nEither way, I like the idea of optional custom modules.\nNow, what about going one step further?\nWe could import all existing installed modules with names like docker_registry.addons.FOO.\nThat way, people could rely entirely on pip install without having to fiddle with the actual registry configuration. eg: if it's installed, it's gonna run.\n. And you want @shin- with a trailing dash  :-)\n. @wking \n1. dislike of namespaces: well, we had that chat before :-)\n2. easier to dichotomize for incompatibilities in container: you have a point. Now, it's also likely that you are going to bash into that container, in order to do anything useful (past the regression bisect) - where you can easily do whatever you want. My point being: it does make it easier to restart a container without changing its content, but that in itself holds relatively little value IMO.\nThe reason I'm advocating against the config approach is that our config options are already (IMO) too complex, and I'm reluctant to add more.\n. @wking good points.\nThough:\n\nYou're going to have to document this somewhere, and I think docs for the environment-variable approach would be lower-complexity.\n\nWell, we are not going to ship every extension bundled in the official container, are we?\nSo, the installation instructions can't be (either way) summed-up as \"ENV var this, and you are good\". It will definitely involve some manual installation and/or container rebuild, plus possibly some configuration. I tend to suggest the magical approach to enforce a common installation / organization scheme on extensions authors, and remove the extra \"configuration\" step on the user side of things.\nAnyhow, I'm not obsessed either - just trying to think it through.\nMainly, I'm concerned about the API (see below).\n@ncdc \nI was suggesting:\n```\nimport pkgutil\nImport all available extensions:\nfor importer, modname, ispkg in pkgutil.iter_modules(docker_registry.extensions.path):\n    import('docker_registry.extensions.%s' % modname, globals(), locals())\nOr list available extensions:\ninstalled_addons = [modname for importer, modname, ispkg\n            in pkgutil.iter_modules(docker_registry.drivers.path)]\n```\nTo implement an extension:\ndocker_registry/__init__.py\ndocker_registry/extensions/__init__.py\ndocker_registry/extensions/myextension.py\nWith the init files being (sorry, namespaces, the part that no one can love) boilerplates:\ntry:\n    import pkg_resources\n    pkg_resources.declare_namespace(__name__)\nexcept ImportError:\n    import pkgutil\n    __path__ = pkgutil.extend_path(__path__, __name__)\nAnd myextension.py as the developer wishes.\nNow, thinking more about it, and either way, I don't feel good about having no formal extensions API.\nThat just means various random pieces of code will float around using various internal API and will break subtly (or not) with every new version...\nNow, I don't think anyone has time or energy to define an extension API (like we did for storage drivers)?\nFood for thoughts...\n. > we should commit to keeping any non-underscored methods/variables consistent unless we make a major version bump That's how generic Python package dependencies work, and I haven't seen any problems with that workflow in my other projects.\nWell, we are not (yet?) a library, and it has only been a couple of months since this is a valid python package...\nI'm not saying we shouldn't try harder, just that we are not ready for the burden IMO to maintain compat over another (implicit) API on top of what we already have (REST API + driver storage API).\nNow, I'd be perfectly happy standardizing on top of signals as an API, like you suggested in the other ticket.\nmyextension.py:\n```\ndef main(signals, config):\n  # Optional, simple way to support disabling extension from config\n  # if not config:\n  #   return\n  # Standardize on signals as an API\n  signals.tag_created.connect(somehandler)\ndef somehandler(args):\n  bla\n```\nthen when importing, something in the line of:\nfor importer, modname, ispkg in pkgutil.iter_modules(docker_registry.extensions.__path__):\n    extension = __import__('docker_registry.extensions.%s' % modname, globals(), locals())\n    extension.main(docker_registry.signals, cfg.extensions[extname] if extname in cfg.extensions else None)\nExtensions would get the benefit of using the standard config mechanism for free, cleanly abstracted. And they could opt-in for a \"disable if not configured\" approach easily. \nThat would NOT prevent people from using undocumented stuff from docker_registry (at their own risk) but at least it would encourage them to stick to that as a stable mean of communications.\n@wking how would you feel about this? (spare the namespace thing :-))\n@ncdc sorry for the bikeshedding on this - I believe this conversation will make all this better, simpler and easier to maintain in the long run :)\n. @wking ok, agreed. (see? I'm not that much hard-headed ;-))\n. Following all these, I gave a shot at a complete proposal including some doc on the branch \"extensions\".\nLet's see how that pans, and let's converge and merge either of these.\nhttps://github.com/docker/docker-registry/tree/extensions/docker_registry/extensions\nI want this for 0.9, which is due soon.\n. @ncdc we can close this in favor of #589 right?\n. IMO, the bulk of the work remains to be done:\n- review and enhance existing signals\n- add new (\"missing\") signals to satisfy tickets like #329 \n. > Why bind the blinker.Namespace() to namespace and _signals?  I don't think the Namespace instance should be part of the public API. Extensions that need to create signals can setup their own namespace ;)\nI'm new to blinker, and was under the impression that you need access to the namespace to get the signals to connect() to. (sure, extensions that want to emit can do it without that).\nHence making namespace public. I'll verify that you can connect from the root namespace.\nMore about the rest later, and thanks for the input on this!\n. Ha, I see - using the named signals directly - was looking rather at namespace.connect('somesignal').\nPR updated.\n. @wking \n+100% on re-expressing things like the index as an \"extension\" (seems to me we will be able to do that either way).\nThe refcleaner as well, I would love to see as an extension.\nI guess we can sum-up the differences between the two suggested models as follow:\n1. conditional import\n   - installed extensions are only imported if (based on config value)\n   - extensions are disabled by default\n   - the registry makes the decision to import or not (which enables the extension)\n   - naming is loose (eg: \"FOO\") and extension inner organization is free\n   - similar organization to index backend databases drivers\n2. automatic import\n   - installed extensions are always imported\n   - extensions are enabled by default\n   - the extension may decide to enable / disable itself (based on config value)\n   - strict naming is enforced (\"docker-registry.extensions.FOO\") plus namespaces shenanigans\n   - similar organization to storage drivers\nLike I said, I think we can reach exactly the same functionalities from a end-user perspective (including ability to disable extensions from ENV) - but the responsibility, default behavior, and characteristics are different.\n@wking about\n\nWhy boot after the app import?  If any extensions need things from docker_registry.app (which seems unlikely), their imports should pull it in at the appropriate time.  Then docker_registry.extensions can boot itself.  Or we could just use the modules_install approach from #565 ;).\n\nYeah, should boot right away (note run/wsgi are getting heavily refactored in the next PR).\n@ncdc \nWe will have this in 0.9, no matter what :-) \n. > One of my main gripes with the namespace approach is that it makes it harder to test development code\nAh...\nHave you tried python setup.py develop?\nYou do it once (in every project you need to work on) - and then you get a \"symlinked\" installed version (both can edit live and get the installed setuptools magic done)\n. The factory does now boot before app import.\n@ncdc would this proposal cover all your extensions need?\nWould you be willing to have a look at our existing signals and fix them if need be so that we are confident they are useful? and/or review the README for extensions to see if it's ok?\nThanks a lot!\n. Both changes in.\n@shin- @wking are you reasonably happy with going ahead with this PR?\n. > And I still think dropping --reload should be a separate PR [1].\nYou are right. Reverted.\n\nWhy the _config \u2192 conf rename?\n\nIt was previously code accessing the private global variable named _config. Now, it's code creating and returning a local variable.\nI agree churn is not good - but that felt necessary here (and accessing global vars is usually bad IMHO - so, the less we have of these the better)\n\nWhy not put:  boot() in docker_registry/extensions/factory.py instead of in\n\nWell, this may be kind of \"philosophical\", but I don't like imports side-effects (and neither do linting tools?).\nI agree that it's likely more verbose that way.\nWhat do you think about the general \"side-effect\" question?\n. We can add the additional engine-side \"enabling\" layer (based on config) later down the road.\nI'll emphasis more strongly that the first version of this (0.9) will likely evolve and is to be considered experimental.\n. @wking re empty dict - not sure about that - do the tests run ok?\n. I like it I guess. A few comments @ncdc \n- you stripped the bits about config returning sub-nodes to extensions - was that on purpose?\n- the entry points thing let us simplify things a bit in boot, right?\n- @mhrivnak: you suggested we could get rid of the namespace thing as well using entry_points - can you elaborate a bit on that? possibly commenting on @ncdc PR?\nThanks guys!\n. Ok thanks a lot for this.\nNot using namespaces will make @wking happy (and me as well), so, we could go with your PR here.\nAre you willing to go ahead with it @ncdc ?\n. Closing in favor of #589 \n. @silarsis\nYeah, I guess we should just use the standard ENV vars, and call initialize().\nI'm all for simplification, so, please submit a PR (base it against the next branch).\n. Fixed\n. Thanks a lot for this.\nJust one remark, and then we can merge this.\n. Merged.\nThanks again!\n. Hi,\nWhat version of docker do you run?\nWhat version of the registry?\nCan you try with just docker run -d -p 5000:5000 registry and curl it?\n. @yaronr \nboto.exception.S3ResponseError: S3ResponseError: 403 Forbidden\nThat tells me you don't have proper ACL on your bucket.\nIf restarting boot2docker fixes it, can you ask the question on boot2docker support/tickets?\nThanks. \n. Pretty much, here we are:\n6.  docker/0.9.1->push   1,873(3.41%) 1,000(5.41%)\nAdd to that about half a percent of even older clients.\nTrend looks like that:\n\nI would give it another month for these to flat out to something we can really ignore - that would match with 1.0 timeline hopefully.\nThanks for this Mr. wking :-)\n. Ok, time to revisit this.\nI'm ok removing these bits, but I want in place a mechanism to refuse push from clients < 0.10 (exact http code to be devised - 406? 417? none are technically exact here, but they are specific enough to distinguish from other errors).\nDo you want to take it from here?\n. Unfortunately, client side error reporting is not good (for past and present docker versions), and it's (very) unlikely that old docker versions are going to get an update - while I do agree that none of my suggestions are technically \"the right thing to do\", we need a reasonably \"visible\" way to give feedback to older clients pointing to that specific issue.\nNow, for the future: #734\n. > So what are our options? \nI believe for the targeted versions (<0.10) using an HTTP status code that is not used already for other error conditions is the only way (+ informative error message in the body can't hurt).\nConnections are made by the daemon - these will show in daemon logs.\n. @wking something is not green it seems :)\n. LGTM\nThanks a lot for this!\n. LGTM\nThanks for the heads-up on this!\n. Hey guys,\nDid we jump the gun a bit too fast? They removed it from pypi already - faster than us releasing a new (branch) version from that patch.\nNot too sure what the best course of action would be for that kind of scenario... (maybe hard pinning dependencies, hey wking? :-) )\n. @Priyanka5 like @igormoochnick we can't help you if you don't provide:\n- the command line you are using to launch the registry\n- the version of the registry you are using\n- your configuration file if any\nThanks.\n. @Priyanka5 the search database by default is under the /tmp folder inside the container.\nAlso, if you need help, you will find that irc, the forums or google groups are the best place for that.\nCheck out https://docker.com/resources/help/ and https://forums.docker.com/ \nGithub tickets are best used for actual bugs and RFEs.\n. Let's keep it open, and see if someone comes up with a PR.\nBest.\n. @zubryan did you do docker login?\n. Can you copy the launch command you use to start the registry and/or your configuration?\nThanks.\n. 1. what's the output of docker login?\n2. what command do you type to pull an image?\n. Thanks for this guys.\n@shin- can you take a quick look and merge?\n. Can you report this to docker support?\nThis here is about the open-source registry - and your suggestion would apply to the hub.\nThanks!\n. Aaaahh well needed!\n. So do I :)\n. @zubryan what happens when you docker pull kozmic/ubuntu-base?\n. @zubryan please don't open multiple tickets (#576)\nIf you feel #576 should not be closed, reopen it instead of creating a new one.\nThanks.\n. @Boran I don't understand.\nsudo docker run -p 5000:5000 registry will run the registry (inside a container).\nThere is no reason why you would need to then run /usr/local/bin/docker-registry, or that it would be installed on your host system.\n. What version of the registry have you pulled?\nCan you copy the exact command you ran to pull it?\nThanks.\n. @Boran thanks for the details!\nI can't reproduce for now, but let's dig: is there anything specific about your setup?\nCan you provide the output of docker info?\n@wking we have a mystery in your garden here - any idea? (this is not about pysqlite2 IMO, but more about sqlite3 failing to load - from a look at pysqlite code)\n. Ok, thanks a lot. We will get to the core of this!\nCan you please bash into the registry container, then:\n1. python\n2. copy the output of from sqlite3 import dbapi2 as sqlite\n. Care to join on irc? ping me there.\nOtherwise, let's try:\nls -lA /usr/lib/x86_64-linux-gnu/libsqlite3.so.0.8.6 (replace 0.8.6 with whatever version is there)\nand\nfile /usr/lib/x86_64-linux-gnu/libsqlite3.so.0.8.6\n. Also, please ls /usr/lib/x86_64-linux-gnu/libsqlite3.so.0\n. @Boran can you please run (from both the \"broken\" host, and the new linode):\ncurl -Sls https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh | bash\nAlso, after talking with @crosbymichael, it seems that devicemapper is really not a good choice. Why the engine had to fallback to that is yet to be determined (that script will help figure it out).\n. Usually, docker picks the best available driver (at install time). AUFS, then BTRFS, then fallback to devicemapper.\nSo, it seems that at the time you installed docker, the only viable driver was devicemapper, and it started using that.\nThe host filesystem might be a problem as well (what fs type to you use?)\nDepending on how you installed docker (script, or apt-get) we might get more information from the install logs / output about that (dpkg logs?).\nI guess we now have two questions:\n- why did your install picked devicemapper while it should have picked another driver?\n- why does devicemapper crap your container?\nI would love to get to the bottom of this, but unfortunately, that would likely require that you grant access to your host so that we can investigate. I would perfectly understand if you wouldn't want that.\nNow, about workarounding this - it seems this issue is very rare (and indeed your second try on linode works ok), so, I would just start from a fresh VM - if this turns to be reproducible, we could definitely get somewhere with it.\n. I don't think there is a way to do that, but then you should ask on irc as there would be people knowledgeable for that kind of black magic operations :) \nThanks.\n. @wking btw - the error we get back from sqlalchemy is particularly misleading.\nThis is due to this code:\n https://github.com/zzzeek/sqlalchemy/blob/master/lib/sqlalchemy/dialects/sqlite/pysqlite.py#L339\nwhich returns \"the wrong exception\" (IMO) whenever something make importing sqlite3 fail.\nI guess we may bring it to upstream?\n. Thanks a lot @Boran for all these infos.\n. I reopened it. (you could still comment on it, even closed)\n. @sathlan I'm not sure I understand the whole thing.\nIt seems to me the bug was with swift's implementation of list_directory, is that correct?\nI understand the manifestation of this was visible through a call to _walk_directory, but then again testing the behavior of list_directory would be enough, right?\nMy point being:\n- if it's atomic and pertain to driver interface's methods, then it should get into registry-core base test-suite (or into the driver additional tests)\n- if it's not, then it's no longer a (driver) unit test, and it may be added to the registry (integration/functional) test suite\nMaybe I'm missing something?\n. @sathlan yes, please add your new test in: https://github.com/docker/docker-registry/blob/master/depends/docker-registry-core/docker_registry/testing/driver.py\nAny driver that uses the recommend base test class will enjoy it.\n. I assume this was fixed by merging #596 ? \n. Thanks for this!\nMerged.\n. That's a small one for once. ping @wking @shin- for review\n. That was merely conscious.\nI kind of assumed \"the latest the greatest\" - and given \"deps loose\" gives you >=0,<1...\n. Just a couple of comments, mainly documentation.\n. @ncdc make sure you run tox at the root of your branch and fix any possible problem there.\nAlso, I guess we should declare docker_registry.extensions in the docker-registry setup.py package list.\n. Ok, LGTM - I'm happy with that whole extensions story.\n@wking @shin- can you take a quick look? Would love to merge tomorrow.\n. LGTM\nPlease PR the inclusion of setuptools as well.\n. LGTM\n. @wking thanks a lot for your input, time and reviews in all this.\nI like what's happening here with the registry, and I like the interactions.\nI would like to merge this as-is (including these two bits you don't quite like).\nWould like a last (quick) review from @shin- though. \n. It's in! Thanks a lot, all!\n. I would certainly accept a PR :-)\n. Merged.\nThanks a lot!\n. @ncdc can you try build the Dockerfile and check things are ok?\n. LGTM\n. Are you trying to push the same image from multiple different places simultaneously?\n. What do you mean by \"multiple processes\"?\n. @snowsky so, you are trying to push the same image from multiple different clients? why would you do that?\n. Bottom-line: registry v1 is not able to handle concurrent upload of the same image (cc @stevvooe )\n. Hi, these are warnings, not errors.\nHave a look at the doc for more about the way we use redis.\n. Interestingly, your new test tripped over bugs in other drivers.\nRun tox locally to find out and fix what's wrong there (likely in s3?).\n. I see. Indeed, I don't like mocks at all (for the very reason we usually end-up patching them to match what the tests expect, kind of defeating the initial purpose).\nmoto could be a nice enhancement indeed - I'd otherwise be happy if you would test the real S3 manually and blacklist the test for the mock.\n. Ok, great.\nIs there a way we could skip the test based on the driver name? (eg: driver.scheme == s3)\n. Ah yeah :)\n. It's merged.\nThanks a lot!\n. A couple of thoughts:\n- \"first-class\" extensions can definitely live in the main repo and be shipped by default (though disabled)\n- like @wking pointed out: docker is not likely to officially maintain a secondary-tier repository (not enough resources) - but see below\n- I'd be perfectly happy with either a community maintained secondary-tier repository, or whatever good alternative solution you guys could come-up with, and I would certainly advertise it in the core docs\n- summoning @shin- @bacongobbler and @noxiouz on this thread\nConditions for inclusion in the main repo:\n- the extension provides a feature that is clearly a widely demanded/used one (like search, refcounting delete) or is very interesting (a bit subjective, but I'm confident we can reach agreement on that)\n- code quality is good and passes thorough review from well known community members (looking at you @wking)\n- there is a clearly identified, active and responsive maintainer for the extension\n- code should rely only on the signal API and doesn't use internal API - or if it does, it's clearly identified and the maintainer is ready to take on the extra work maintaining that\nSo please do go ahead and find out the best solution for this.\n. @shin- already LGTM-ed this\nWe can merge as soon as you fix the README (just amend your commit).\nThanks a lot for this!\n. Great - thanks!\n. Merged. Thanks a lot @carmstrong !\n. Merged. Thanks @pchaussalet for the initial effort, and @johanneswuerbach for closing this.\n. @bacongobbler ping\n. @kerr23 can you share your registry config?\n. @shin- can you have a look? Thanks!\n. > What would be the right development setup on Mac OS X? \n\nI understand that I can fork elliptics plugin (I did it already), change it, run tests with tox and, finally, upload to PyPI. Is that the right procedure? Did I miss anything?\n\nThat's a good way to start.\n\nHow can I debug my driver? \n\ncd into your driver folder, then python setup.py develop.\nNow, whenever you start your registry, your driver is available.\nAlso, running tox inside your driver folder is great to have the tests do the heavy work for you.\n\nThe only way I see is to download official docker registry image with docker run registry, get my code inside running registry container (via github with push and pull, for example. Unfortunately, mounting volumes doesn't work on boot2docker.), install with python setup.py install command, commit the container and create a new image, kill old container, run a new container from a new image. \nIs there a simpler way?\n\nYes:\n- pip install docker-registry (or git clone and pip install .)\n- cd mydriver; python setup.py develop\n- tox (either from the driver dir, or from a clone of the registry dir)\n- docker-registry (<- use env variables to select the proper configuration)\n\nI also have problems running tox tests. When I run tox I get the following:\n\nCan you try pip install virtualenv and report what it says?\nHope that helps.\n. It's hard to tell what's wrong there, but I would encourage you to not use OSX system python, and rather go with homebrew python (http://brew.sh/).\n. Can this be closed. Do you still need help here?\n. > Then I edit backend driver, run python setup.py develop, copy new backend driver .py file to docker-registry/docker_registry/drivers/ and start registry with\nYou need to run python setup.py develop only once (per repository), inside the repository(-ies) you want to modify (after making sure you un-installed any previous version).\nYou then should not need to \"copy\" any file over.\nAbout starting the registry, I would really suggest you just use docker_registry instead of calling gunicorn.\n\nerror: gunicorn executable not found\n\nThis is normal and expected, it's not a hard error.\n\nERROR: Failure: ImportError (No module named s3)\n\nThat one means something is wrong, probably boto is not installed.\nI would have to look at your complete Dockerfile.\n\ntox.ConfigError: ConfigError: substitution env:'TOX_INCLUDE': unkown environment variable 'TOX_INCLUDE'\n\nIf you are inside a container, set these to \"\" (eg: TOX_INCLUDE=\"\" TOX_LIBRARY=\"\").\nIf you are on OSX, you can set them to additional include and lib path (brew).\nI will fix this to make it more transparent.\n. There should not exist any file like docker-registry/docker_registry/drivers/gitdriver.py\nYou should make a clean clone of docker-registry.\nAlso, for your driver structure, you may look at: https://github.com/noxiouz/docker-registry-driver-elliptics\n(don't forget the __init__.py files with namespace initialization)\n. You don't need to do any of that.\nJust run python setup.py tests from inside the docker-registry folder - it will test any installed driver (including yours).\n. It seems you are back onto using (partly?) the system python instead of the brew one? (nose from system site-packages)\nEither way, these errors do indicate a problem in your driver...\n. Thanks @dashohoxha \nIndeed, this here is the open-source registry, and it doesn't pertain to automated builds at all. Reaching docker official support is the way to go for anything related to the hub / builds.\nBest regards.\n. Howdy there,\nThis is fun stuff!\nNow, you can (and should) maintain this as a separate package (the driver API is meant to let you do that).\nYou can have a look at other third-party drivers for inspiration, like:\nhttps://github.com/noxiouz/docker-registry-driver-elliptics\n(and some info here: https://github.com/docker/docker-registry/blob/master/CONTRIBUTING.md#storage-driver-developer-howto)\nIndeed, we only keep filesystem and S3 in the main/official tree.\n. @ncdc re: flake, tweak this: https://github.com/docker/docker-registry/blob/master/tox.ini#L6\n. Guys, like I said, I'm happy with the community owning extensions. So give me a ping when:\n- things are complete\n- tests are happy\n- you gathered at least one LGTM review\nand I'll merge (eg: don't wait for a review from me on these).\n. @wking @ncdc where do we stand with this?\n. @wking I'm happy with it either way.\n. I believe v2 will support pruning / gc, either in core or an extension.\nNow, mind the roadmap (to be updated soon by @stevvooe) - I don't expect this specific feature to land very soon (more likely sometimes down during Q1) - unless someone starts working on it sooner?\n. LGTM\n. I don't have hard feelings on this.\nA couple comments though:\n- @wking can you elaborate (briefly) on why your suggestion would be better?\n- same train of thoughts: why/what should we include from https://docs.python.org/2/library/logging.html#logrecord-attributes ?\n- what is considered \"standard\"/best practices by other widely used projects?\n. @wking @bacongobbler @ncdc please go ahead:\n- review other frameworks/well known python libraries position on this (Django is a good start indeed)\n- champion a choice / reach an agreement\n- amend this PR with it\n. @ncdc willing to own this and get it through?\n. friendly bump :)\n. Ok, thanks @ncdc !\n. Can you elaborate a bit on the use-case? And how does that add value compared to pointing index_endpoint to an in-house index?\n. Allowing to override the index that would lead to a situation where docker pull library/ubuntu could mean different things is not going to happen.\nNow, what happens if you docker push/pull index.mydomain.io/namespace/repo and run your own index on mydomain.io, pointing to it from your registry?\nSeems to me somebody did it over there: https://github.com/docker/docker/issues/7939\n. ping @bhuvaneswaran @adamhadani since you guys were eager to see 0.9 out the door :-).\n. Thanks a lot community!\nAll these suggestions about starting a virtual-env and DOCKER_REGISTRY_* env variables look good to me!\nNow, newbies, about launching the registry, I'd recommend calling directly the entrypoint docker-registry - direct gunicorn calls or other call stances are somewhat \"advanced\".\nThanks again! \n. And yeah, I guess \"testing\" the documentation is good as well :)\n. @adamhadani @bacongobbler \nIndeed, the needed libraries are listed in: https://github.com/docker/docker-registry/blob/master/ADVANCED.md#alternative-uses\nAbout CORS, you should be able to have it by using the \"extras\".\neg, from the pip package (not published yet!):\npip install docker-registry[bugsnag,newrelic,cors]\nor from a local checkout:\npip install file:///abs_path_to_clone/.#egg=docker-registry[bugsnag,newrelic,cors]\nThese should be documented in:\nhttps://github.com/docker/docker-registry/blob/master/ADVANCED.md#extras\nAll these should be found from https://github.com/docker/docker-registry/blob/master/README.md#advanced-use\nif you guys feel they deserve more exposure, I'm definitely open to a PR - I just want to make sure the \"basic\" instructions are kept reasonably short for newcomers and others \"just-testing\" people.\n. @wking does that sound familiar? the @bacongobbler exception at least I'm pretty sure we crossed before\n. @wking wild guess: could this be a race condition between different workers trying to initialize the same db thing?\n. @silarsis yes, we have a workaround for the search index breakage.\nI'll push it out this week, hopefully.\n. Releasing.\n. > I think this would make more sense if there was going to be more sharing of code between the registry and the daemon/client.  However, I don't think we need any brains in the registry, since I see provanance as a contract between the builder and signer, and completely separate from the registry [1,2].\nTarsum verification will have to occur also on the registry side. And I would expect the registry to verify images signatures as well.\nThese are the area of \"shared\" code I'm thinking about (so, libtrust, some bits for tarsum, and probably some other \"engine\" code related to manipulation of the image format).\nCommon tooling is a plus as well. Common development guidelines, etc.\n\nThis is a benefit?\n\nI do believe there is benefit there - yes, I know http://www.joelonsoftware.com/articles/fog0000000069.html \n\nIf this means we get transactional backends for free, then great :).\n\nWhy not?  :)\n\nOtherwise, I think the current implementation scales well (just add as many threads as you need)\n\nIt does scale.\nNow, what about things breaking in not so subtle ways because of libevent minor version differences?\nOr the need to call \"magical\" monkey patching \"before\" any other code, that doesn't always seem to fully do the job?\n\nsince there's no need to communicate between threads.\n\nThere is a need to communicate between threads, right now, or be bitten by eventual consistency (which is one of the reasons we use redis for that). But then one could argue this will disappear with the new drivers.\n\nWhere are the funny smells?\n- namespaces\n- packaging\n- gevent\n\nOther things are rather a matter of taste - don't get me wrong on this though, I do like python.\n. > And with clients doing verification, I doubt anyone will bother uploading broken signatures to the registry.\nThey will (broken tarsums).\nAnd that will result in a DOS, at best (content-addressability comes at a cost).\n\nCan you links to the issues where these came up?  gevent is not my favorite package, but I'd probably just pick a different Gunicorn worker (e.g. gaiohttp [1]) instead of rewriting this whole project from scratch ;).\n\nThe most baffling ones are here:\n- https://github.com/docker/docker-registry/issues/400\n- https://github.com/docker/docker-registry/issues/540\nNow, I'm not stating that go is a magic bug-free shiny new thing and solves everything - barely stating that concurrency is not a python core feature and that I expect a better situation on that front.\nFinally, we are not rewriting from scratch because \"X is so superior Y\" - we are rewriting from scratch because we need to break things - the fact that we are going to change language as well is a different issue IMO.\nAnd oh, ultimately, I would love to see multiple diverse implementations of the V2 protocol (and this is what should be cool with it, in allowing to do that more easily).\nI wouldn't be shocked if someone would do a nodejs registry, or... a python one.\nThe one I want to focus on though is this one here in go, along with this community :-)\n\n\u201cBut when we rewrite it, we'll do a better job\u201d is less convincing to me than \u201cbut when we use $TOOL, $PROBLEM will no longer be an issue because of $FEATURE [$LINK]\u201d ;).\n\nI expect the following features:\n- go concurrency model\n- go typing\n- depart from python unicode mess (this one is so obviously a python shortcoming that I forgot to even mention it - and if you remember, clearing that up was one of my first contributions, and I can't say I enjoyed it - and it seems we still have one of these lurking around unfortunately)\n- easier to integration-test with the other pieces (engine)\n- more shared code, shared tools\n... to give us a more resistant codebase, easier to maintain, easier to contribute to.\nI also expect our \"design (a bit) more\" and \"think (a bit) before\" approach to prove more fruitful and also easier to maintain than our previous organisation.\n\nI'm still not sold on the whole docker-registry-core pull-out. \n\nWell, it was ugly, but it did benefit us a lot:\n- remove large chunks of code from the registry (yes, I'm lazy :-))\n- allow third-parties to maintain their own thing all by themselves (yes, my excuse for laziness is \"empowering others\" :-))\n- foster the community\n\nAnyhow, I think these are things best solved incrementally.\n\nI hear you. I do think there are indeed strong benefits in solving things incrementally, and I think we did a lot on that front already, from 0.6 up to 0.9.\nBut then defining what is an \"increment\" and what is \"disruptive\" is a matter of \"scale\".\nAnd I do believe the important parts to preserve here are:\n- the community\n- the spirit\n- the backend driver design + good extensibility approach\nThe rest is not so much if you ask me.\nAnd I still enjoy chitchatting with you, and I hope you will keep that voice up during that new journey :-)\n. @noxiouz #613 for specific extension model discussion (your ideas seem close to @dmcgowan 's)\n. @wking\n\nYou can't handle this the same way that you already presumably handle folks uploading other objectionable content?\n\nRight now, for most DOS or security issues, we get away with ownership verifications. \nNow, content-addressable ids (vs. random ids) makes the question of \"ownership\" more difficult.\nAlso, reducing the coupling to the auth. component is something I want.\nBelieve me, I hate the idea of computing tarsums on the server side - but for now I can't figure a way out...\nAbout the dev env, I want to make this easy/easier to setup for contributors, so, efforts on that front are definitely worth it.\nNow, about using gentoo, who am I to lecture you? :-))) http://www.motivationals.org/demotivational-posters/demotivational-poster-16518.jpg\n. @bacongobbler \n\nSo what happens to config/exposed ports/etc? Is that all going into the \"some opaque metadata\" format? Isn't this just an aggregation of all the concepts in the v1 API and just slapping on the v2 sticker?\n\nWell, maybe it is :-)\nLayers are still layers.\nImages on the other hand are no longer \"a specific layer\". They are a chunk of json listing layers.\nContent-adressability is a major change as well.\nIndeed the per-layer config ends-up in the opaque part.\nAnd yes, the engine itself will keep working as is - it's a transport-level format change - not (yet) an engine level change.\n\nThis change does not only affect the registry as well, but it also kills off all of the current python storage driver implementations, which may be affected substantially.\n\nThis is the one thing that really bugs me.\nNow, maybe we can get creative on this? maybe some \"special compat driver\" that would let you use old drivers through a combination of (http?) socket communication magical wrap? -> let's move that discussion over here #616\n\nI assume that this issue is more of a \"hey, we're doing this regardless but I wanted to give you a heads-up\" more than an actual proposal. ;)\n\nMakes me think I should clarify things here.\nI won't lie to you: in the end, I'm the one with write-powers on the repo :-) - and I will have to make some calls, veto some things, take the blame and suffer the insults :-)\nNow, what I want to try here is not some BS open-source parody where I would just dump source code and tell you guys \"live with it\".\nI want to build an open-design process that works for all of us:\n- efficient: I don't want things to languish for months before we can reach an agreement and move onto implementation\n- usage focused: I think usage should come first, technology / tool / technical-beauty second\n- concise: I would really love to see a small, extensible core - a basic set of flexible enough ideas that would let the community go crazy with custom stuff on top of it\nI don't know how much we will succeed in making that open-design process mesh with the need to deliver and ship a usable product with strong time constraints, but I really, genuinely want to try to pull this of and end-up with a stronger, better, more satisfied community (and less work for me :-)).\nAny help here, I can definitely use.\nI think the idea of commiting proposal and architecture notes as PR is a good one and will help managing the discussion.\n. @smarterclayton \n\nis this to mutate an image into a new image? I.e. given image A, PUT link B -> Image B with new signature? While useful for simple clients, it also makes the registry a bit more complex to implement - might there be an advantage in only having GET/PUT images, GET layer, GET tags?\n\nAh, no.\nHere it goes: since (layer) ids will now be content-addressable instead of random, there will no longer be clear ownership on a given layer (you AND me can legitimately generate it).\nAlso, I want access control to be simpler and be \"set\" at push time rather than at pull time (right now, layers live flat in a non specific namespace, making auth lookup mandatory for every layer).\nSo, the idea would be to allow NOT pushing again something you already had access to and \"linked\" into another repository you have access to.\n. @wking @bacongobbler (and others) do you want we try a irc hack session / meetup / gathering thing?\nOr even a hangout?\n. Thanks @shykes \nWe have a preliminary backend driver implementation using libchan here: https://github.com/docker/docker-registry/pull/630\nand some discussion going on extensions there:\nhttps://github.com/docker/docker-registry/issues/613\nand on drivers there:\nhttps://github.com/docker/docker-registry/issues/616\nDrivers and extensions have different targets though, and different speed/reliability/deployments strategy requirements, so we might end-up with different solutions here - libchan is definitely a strong lead.\n. @visualphoenix nice to have you in ;)\n@govidiupl definitely welcome!\nIRC meetings every monday 10AM PST. Otherwise, have a look around at tickets with the next generation label.\n. @shreyu86 that should certainly be part of the new extensions model: #613\n. @noxiouz I tend to think backend storage drivers (like elliptics) should NOT be \"extensions\".\nThere are some shared concerns between the two things, but also different requirements.\nSpecific discussion about the drivers may go there: #616\n. So, what are the merits of the various solutions here?\nAnyone willing to do some comparison between libchan, progrium, mitchell solution / others? \n. @bshi I don't think reporting this is worth the effort - energy IMO is better focused on registry v2 development.\n. One of the terribly inefficient thing right now is that we do not only ls but also read the (tag) file contents.\nThat second part is going away.\nDriver will still need to provide an efficient ls (and we can alleviate part of the pain by caching the result).\n. @bshi (and other gcs people?) - the new go drivers API #643 is going final and will be merged soon. The time is right to voice concerns :-)\n. Consistency... we think about it, a lot :-) cc @stevvooe \n. Hi @prune998\nFor a start, I would need debug logs from your docker daemon. You can restart your docker daemon using the -D argument to obtain them.\n. The config is the problem.\nYou need to inherit common explicitely:\n```\ncommon: &common\n    foo: bar\nprod:\n    <<: *common\n   baz: bar\n```\n. @prune998 how did you install the registry?\nPing @wking as well since this is related to search.\n. Happy it helped.\n. FWIW, S3 does currently support multipart, resumable push.\n. The way registry V1 worked was semver compatible (your driver would depend on core>=X,<X+1 and would work that way).\nThat might be trickier to get right with go, but then I'm ok bumping major versions more often (understood that it's the driver interface version, not the main registry version).\n. I believe this, to a large extent, should be closed given #643 was merged.\nSpecial thanks to @BrianBland for his continuous work on this, to @wking @bacongobbler @noxiouz @visualphoenix and others who took time for the thorough reviews and discussions.\nWhat we have certainly still has some rough edges and can use some tire-kicking and bullet tests, but this is a very good first shot.\nOne single char to sum this first \"next-generation\" journey: U+2661 \u2661\n. @bruce problem is the AMZ region (see #400)\nWe don't know yet what cause this - likely a libevent issue.\nThe recommended workaround is to specify the region from inside the boto.cfg file, but you will find on that ticket more workarounds from people.\nI'm closing this, since it looks to me as a dupe of #400, but please scream if you feel this is wrong and I will reopen :-)\nHope that helps!\n. Thanks @bacongobbler !\n@jokeyrhyme if you can confirm this does fit your need, can you close the ticket? Thanks a lot!\n. @silarsis can you s3cmd ls s3://BUCKET/repositories/USER/IMAGENAME/ \nThanks!\n. Thanks a lot. Will look into it for the next version.\n. LGTM\n. @tianon if you would like to dogfood your gorgeous python langpack image, I would certainly accept to base the registry on-top of it (2.7 please). \nThen I need python-dev liblzma-dev libevent1-dev so maybe there is some work/testing required here (since I'm not sure the debian -dev package will fly well with your thing).\n. LGTM\n. Also want to discuss next monday: details on how to organize for \"specing\"\n. @ncdc please bring over in your luggage any other RH buddies you feel are useful / interested by that discussion. Also pinging @vbatts here, and the same goes for @proppy & acolytes. Would also love to see the Deis posse (@bacongobbler I summon you :-)), and I guess it could be nice to hear from Tutum? (not too sure how to reach out to them)\n. Als want to discuss #627 (if there is time enough on monday)\n. @proppy \"trust\", image signing, and other related server side components are definitely worth discussing down the road. To be scheduled for an upcoming irc gathering.\n. ... and would love to have @noxiouz (and other yandexers?) on monday as well.\n. @wking thanks a lot!!!!\n. Just a quick ping for everyone:\ndocker-dev, 10AM this morning, like every monday! Get your seat and popcorn ready!\n. @wking and others: do you feel we should have a formal / defined way to store these discussions minutes? Maybe something very simple like a open-design/chats folder with timestamped summaries?\nWho would feel brave enough to handle that \"I'm writing the summaries\" position? Seems like @wking has done it beautifully these past weeks :) \n. Hi there,\nI would suggest you dig into next-generation docs and issues here: https://github.com/docker/docker-registry/issues?q=is%3Aopen+is%3Aissue+label%3ANext-generation to get a feel of what's happening - looking into the code being landed into the next-generation branch is certainly good as well.\nHope that helps!\n. Below is some more food for thoughts for monday meeting.\nPossible user scenarios we are aware of so far:\n1. I want a simple private registry to test, or just to enjoy better \"local-network\" speed, and I don't care about authentication\n2. I want to host my content on my own registry, but I want to manage my users and authorization from the hub\n3. I want to host my content and manage my users and authz myself, using basic auth\n4. I want to allow users to use per-engine, key based authentication, like the hub does, and I want to manage my users and authz myself\n5. I want to host my content and manage my users and authz myself, using another exotic client authentication mechanism\nIf you guys can please kind of \"vote\" on what scenarios you think are useful / most likely, that would help prioritize.\nNow, from the engine POV it will then need to be able to talk to registries with:\n1. no auth\n2. key/token based auth protocol\n3. basic authentication support\n4. key/token based auth protocol\n5. additional exotic authentications mechanism\nFrom a server side perspective:\n1. nothing\n2. to be defined\n3. through proxy\n4. to be defined\n5. through proxy\nIMO, basic auth has a drawback: you have to scatter your credentials all over the place, including where you maybe don't want them to be (example: a CI server, etc), and there is no way to \"revoke\" that situation. This is why I won't run that for the official hub, and I want something else that let people authorize and revoke something (a key) that a given engine will use.\nThis is the reason why I want this number 2 scenario into the registry - that doesn't force others to use it though (that would be scenario 4) as they should be able to get away with proxy + basic if they want (or go exotic if they can merge support for it in the engine).\nNow, I would rather choose for story 2 something that do please others and that they would be likely to use themselves rather than something that only us use and maintain (like what we have right now) - and this is why I want this conversation to happen on that front.\nLike I said, my requirements for this number 2 are:\n- as simple as possible for the registry, without the need to interact with another server: the \"token\" needs to be signed and fully verifiable standalone, and it should convey an authorization to perform the requested operation\n- built on something that is \"standard\" enough - even if an IETF draft, but at least something we don't have to reinvent hot water for\n. > The registry \u2194 storage authentication seems to be a completely separate issue to me, this discussion should just be about client \u2194 registry authentication.\nAbsolutely.\n\nWhere does the \u201cwithout the need to interact with another server\u201d requirement come from?\n\nThere are different aspects there:\n- efficiency: the client needs to get authorization first from the auth service, then the registry would have to call back home to verify it, that seems to me like something inefficient, that likely double the load on the auth component - while if the auth token would be cryptographically signed and self-describing, making it temper resistant, that would save the additional roundtrip\n- simplicity: the registry wouldn't have to handle client to server communication, and error conditions on that, including returning meaningful errors to the final client. Also, the auth component wouldn't have to implement endpoints exposing a verification API.\n- security: the less communications to secure, the better. The less publicly exposed APIs, the better.\n- reliability: this is linked to efficiency, the less exchanges, the less likely an error condition could affect the complete workflow\n. Minimalism.\n\n. Relevant: #624\n. My concerns arose from first hand experience of running the official registry. None of the issues we had were easy to solve, and ultimately some of them simply can't be solved by simply tuning / configuring.\nWhen we were using Hipache (on top of node http-proxy) we never got rid entirely of neither suspicious timeouts nor memory leaks - eventually I started rewriting parts of http-proxy and node core library (and it's a quite bad thing when you end-up there...)\nWhen we moved to nginx, we realized there is no way to prevent nginx from doing on disk buffering for big payloads, introducing both disk space and I/O issues. We also experienced docker specific (?) problems (TIME_WAITs) due to the additional communication between the (three!) http layers (they are crammed into containers).\nNow, we are getting rid of nginx, to try something purely with HAProxy - fingers crossed.\nAlso, debugging timeouts is not exactly something easy (in a stateless environment where it's hard to track a given client request through layers). The more layers, the more complex (add to that docker engine weird behavior, and the fact it's often used inside a VM with its own problems...).\nNow:\n- it's understood: docker operations are not the most common use-case, and most people would probably be happy to sacrifice what I call \"reliability/robustness\" for an easier / layered design\n- definitely want to let people do use auth proxies in front of the registry if they want to\n- whether through a proxy, or \"natively\" in the registry shouldn't matter much to pick an authnz design - it should be usable in both scenarios, and I feel we should focus on that (and ultimately we should be able to switch from one design to the other using the same auth scheme)\nNote that I'm also very un-keen to the \"call-back-home\" design (auth_request) - from an operational POV, and like stated previously, this clearly doubles the load on the auth component, requires more code on both side, makes debugging from a client perspective more complex, and (every day, here) proves problematic. Again, this is certainly a matter of perspective and usage - I would certainly argue that heavy use systems with high availability requirements are probably best served using cryptographically signed self-contained \"tokens\" (as in \"can be verified without calling home\"). Cloudfront signed urls is a good example of that.\nTo sum-it up:\n- the main challenge is to find the auth mechanism we want to ship by default for hub-like usage\n- whether proxied or native should not matter (and both should be possible)\n- I'm looking for something that minimizes network communication, uses cryptography to guarantee integrity, provides decent security and preferably interoperates well with existing libraries\n2 cents...\n. > proxy_buff\nNo,  proxy_buffering off if for backend responses.\n\ndoubling the load\n\nStory 1:\n- engine -> log -> auth service\n- engine <- token <- auth service\n- engine -> token -> service (service verifies signed token standalone)\nStory 2:\n- engine -> log -> auth service\n- engine <- token <- auth service\n- engine -> token -> service\n- service -> verify token -> auth service\nYou have two requests instead of one.\n\nmore code\n\nStory 1, you need to be able to verify a signature (probably share a secret).\nStory 2, you need:\n- additional, properly protected endpoints on the auth service\n- http client code on the service\n- additional error handling and error conditions to be bubbled-up for when the auth service doesn't answer properly (+ timeout handling + throttling + etc)\nOn the other hand, verification code doesn't have to be asynchronous, or handle network issues, securing endpoints, etc.\n\nif the auth service is not available\n\n... the client stops there and says: \"the auth engine is not available\"\n\nIt would certainly be possible to sign long-duration tokens and cache those on the client side, but that wasn't my impression of how the signed-token scheme was going to work out of the box. \n\nThis is the idea. Tokens have a lifetime and can (and should) be reused (with proper nonce counting and replay protection, etc).\n\npersonally I think the ease of implementation of the http_auth_request approach (no new registry/proxy code) outweighs that benefit.\n\nYeah, well, nothing prevents you from going that \"proxy\" way.\nBut there is a clear use-case for implementing a key + token based authentication mechanism, and the proxy approach doesn't fly for it, for all the reasons listed. :)\n. @visualphoenix I don't know CAS well.\nWould you require the docker client/engine to implement CAS, or would you be happy hiding CAS behind a proxy (apache/nginx) and using basic auth on the client? \n. Here is a proposal for a v2 authentication workflow:\nhttps://github.com/docker/docker/pull/9081\n. @jokeyrhyme @danzy That kind of scenario is not supported (at all, or correctly, or in a simple way) by the current protocol.\nThis is one of the reason we are going with a V2.\nYour voice and use cases are welcome since we are in the design phase.\nAll currently open discussions are there: https://github.com/docker/docker-registry/issues?q=is%3Aopen+is%3Aissue+label%3ANext-generation\n. Ok. Can you update the README (https://github.com/docker/docker-registry#storage-s3) - right now it says that this only useful for non-S3 backends.\n. Then you are merged :)\nLGTM.\nThanks a lot!\n. I don't expect miracles unfortunately. Distributed store are unlikely to grow wings...\nOn the other hand, we had problems with that aspect because of the design / workflow of registry v1.\nI expect (and wish, and will pay extra attention) the new workflow to be more resistant with that.\nThat might include using \"locks\" on the transactional storage (redis).\n. > content-addressable streaming storage\nThe content-addressability part is up to the registry, not to the storage itself (drivers shouldn't need to know anything about that).\n\nOf course, you still have to iterate over all the items in the streaming storage, so some way to slowly work through a complete list of entries would be good.\n\nThat was the same problem with v1... such an approach is not practical with any crowded storage.\nAbout the rest, I would rather keep the \"transactional storage\" as a \"simple cache\" and not as full-blown requirement.\n. > If you're going to have content-addressability I'd definately put it in at the storage level. \nThat would make for code duplication into every driver, doesn't solve race conditions problems, requires the drivers to do more work, and make it more difficult to fix the adressibility model if we want to change it (eg: multiple version of tarsums for example).\nSo, it's no on this :-)\n. - you don't know the hash before reading to the end of the file - meanwhile, you need to write it somewhere - bottom line: you don't know the hash beforehand - and if you are going to argue that the client should send it, then you just allowed random content to overwrite any other content :-)\n- duplicating the hashing mechanism in every driver is bad - duplicated code is bad - error prone - a maintenance headache\n. We have very few layers above 500MB, not to mention above 5GB.\nBut then again, if we wanted to use local fs before pushing to the driver (like we did in the past!), the registry would do that and have ALL drivers benefit from it, instead of putting  (again) that complexity into the driver.\nEither way, I still fail to understand what benefit we could possibly get from delegating (the same) intelligence into (each and every) drivers :-).\n. :)\n. - swagger wasn't exactly \"rejected\" - but anyhow, it looks like it - now the motivations to do so don't affect the registry (we can exhaustively document the API using it)\n- if there is a viable alternative we can use, and the community prefers it, I'd be happy to consider it\n- nothing is eternal, sure, and maybe this will change down the road, but I really believe we should start working a prototype using swagger (or a better alternative?) ASAP\n2 cents...\n. Thanks @dnephin !\nIf you are interested on starting something for the registry, please jump in!\n. @wking fantastic!\nI'm ok with an early PR so people can get an early feel/play with it. Just flag it \"WIP not to be merged yet\" and update it at will.\nThanks a ton for this!\n. Thanks a LOT for this!!!\n. Relevant: #616 #626 \nPing @wking @bacongobbler @noxiouz \n. Can you copy your docker daemon logs? (eg: restart docker with -D)\n. @shreyu86 would you be able to try docker 1.3 (just released)?\nI would like to see if the panic itself was fixed, first (which is definitely an engine bug).\n. Happy to hear that!\nEnjoy ;)\n. You likely have different problems here, but the first thing to solve is your base docker install.\nI wonder why you would use lxc, but either way, you should report the lxc bug to the main docker bugtracker (https://github.com/docker/docker).\nAlso, you may use docker 1.2 instead of 1.1 IMO, since it included network fixes.\nAbout your registry logfile, the only one pointing to a registry problem in itself is the first one - socket.gaierror: [Errno -2] Name or service not known tells me that it cannot DNS resolve your S3 bucket - that would point to either a typo or a dns resolution problem. The other logs don't show anything suggesting the registry crashed, so, I would rather look into these lxc issues.\n. Ok, so, now there is only two ways:\n- either your registries are still stopping without anything useful in the logs, which would point back to a docker engine problem\n- either they do say something in the logs\nWhere do we stand now?\n. This is going in docker main, since for now this is the contact surface with the engine and its super important to get good feedback from the engine perspective.\nThanks @stevvooe !! \nhttps://github.com/docker/docker/issues/9015#issuecomment-62074787\n. @wking \nWe do want access logs from the registry - this is the only way you are going to be able to tell if your proxy is actually sending the request back\nWe also want exhaustive application logging (including context, etc). This is absolutely needed at least to debug.\nThe default behavior should be to have these to stdout.\n. @wking arguing about technical points (and agreeing, or disagreeing on them) is perfectly fine.\nOn the other hand, discarding valid use-cases is not :-)\nI do want access logs on the registry, since the registry is a standalone product, and not everyone is willing to run it behind nginx.\nThis is a standard feature of about every standalone web application, and just because it's possible to do otherwise with proxy monkeying (that I'm advising against) doesn't mean we should ignore it altogether.\nStill, to keep addressing these:\n\nWon't you get timeout errors in the reverse-proxy logs if the registry isn't responding?\n\nWhy would a timeout be the only case? What about hitting the wrong backend?\nAnd again, what about you don't have nginx in front of this?\n\nI don't think there will be much going on in the registry itself. \n\nWell, there is a lot :-).\nExtensions come to mind.\n\nI expect reverse-proxy logs will cover the former,\n\nRead my lips:\n- we will not ship the registry by default with a nginx proxy \n- we will advise against using proxies unless people know what they do and are willing to solve problems themselves\n- I'm perfectly happy with letting people do use a proxy if they want - but you have to stop considering everybody will do as you think is good :-)\nProxy support in registry v1 has been a terrible train wreck, and a liability for anyone who tried that in high-scale production. I don't expect things to go very differently with v2, and I intend in making sure we don't bind ourselves to such a design - although I genuinely want people to be able to rope themselves with it if they would want that :-)\n\nI just don't think it's going to need all that much planning (since the translation code should be fairly stable).\n\nSo, what happens then? Developers don't have a clean way to log thing, they start using various libraries. I refuse to merge that because it's messy, and they will switch to no log at all, or printf, and we end-up with a poorly debuggable project failing to properly log information about its behavior.\nSince it's so crappy, it's not picked-up by ISV who want to build on it or ship it as a product, it's hard to post-mortem errors in it, and ultimately I'm not happy :)\n\nI don't think anyone will need it for production installs.\n\nHaving auditable logs, and a robust logging infrastructure is IMHO a standard feature of any serious software.\nI want this to be designed and dogfed from the start since it will be harder to get right down the road. \n. Summing things up:\n- if people want to work their access logs / errors logs from a proxy / lb, that's fine, and I don't see a reason to prevent that\n- registries should still produce http access logs: in a multi-instance registry cluster, it's very helpful to be able to track down a request to the instance (and log flow point) handling it\n- I do insist on this third point: we need application level logging, with context, and configurable output - this must be handled by the registry since it's through registry configuration that we are going to determine that (level, output) - dependent libraries (libchan, extensions, storage drivers) should use that to do their logging\nWhat needs to be done:\n- pick a poison (since docker main chose logrus, we would need a strong point against it, or for something else to not use that one as well)\n- write some docs to instruct extension/drivers authors on how to do their logging properly to integrate with that\n. > I'm still pushing for extensions and storage drivers to be separate processes in separate containers with their own independent logging.\nThat should not be the default (and not what we encourage people to do) - but then I don't see a reason to prevent it if someone wants to go crazy with that :)\n. What's in nginx error log? Also, what's in the registry log? Finally, please:\n- curl -i \"yourregistryurl_not_nginx/_ping\" | grep -i standalone\n- copy your registry configuration\nThanks!\n. Ok, I need your docker registry logs then...\n. @joshk0 previously, running docker pull foo was pulling every tag in foo.\nWith 1.3, it now pulls only latest (to be consistent with FROM which already does that).\nIf you want to restore the former behavior, you may use docker pull -a foo.\n(you will find that in the updated documentation under pull: https://docs.docker.com/reference/commandline/cli/)\n. Nice catch!\nflake8/hacking is not happy with your style (under-indented lines).\nAlso, I would prefer something shorter than bugsnag_wrapper - maybe ebugsnag and enewrelic?\nThanks a lot!\n. Hacking is still complaining, sorry about that.\nIf you want, you can test it locally by either running tox, or directly:\npip install hacking\nflake8 .\n. Fantastic. Merged! Thanks a lot.\n. Thanks for the review @wking.\nI'm going to close this and resubmit on a new orphan branch, with updated proposals.\n. ping @stevvooe\n. Updated.\n\nWhat are ISVs?\n\nIndependent software vendors (eg: other people building registry solutions on-top of this).\n\nMmm, chunks of barbaz :).\n\n:)\n\nISO 8601 dates (YYYY-MM-DD instead of DD/MM/YY)?\n\n+1\n\nHow to submit a pull request (basic Git(Hub) workflow, not needed)\n\nModified with some additional useful stuff (squash, split).\n\nopen-design/MANIFESTO.md\n\nUpdated.\n\nScope of this project (why is this not front-and-center in a README.md)\n\nI would like to keep README for end-users looking to use the project - while this rather pertains to development. What do you think?\n\nI'd rather use GitHub's milestones to get better intergration with the issue/PR tracker.\n\nMilestone I added as well, and they are good enough for active contributors for day-to-day work and detailed information.\nBut I like the idea of having a single, synthetic, one-place document outlining the big picture (yes, more maintenance, true).\n\nIf the weekly IRC meetings are going to be persistent, I'd like a\n\nNeed to figure that one out indeed.\n@stevvooe @wking PTAL\n. @wking thanks! they are in\n@bacongobbler @jlhawn @stevvooe @dmcgowan thanks a lot!\n. @bacongobbler @wking @BrianBland \nThanks a lot for this.\nI feel a quick (short) summary can be useful here:\n- drivers authors shouldn't have to bother about IPC / libchan at all in their implementation, and how their driver is used (IPC or hard-compiled) is rather up to the person running the registry\n- compiling your driver into the registry and using it as-is is definitely something that should be possible and that we will use for \"mainline\" drivers (with the downside it would require you to build a modified registry to import your driver package)\n- this IPC solution is not (IMHO) meant to solve availability or hot-switching questions (though people may use it for that if they want)\n- the remote-network-driver use case might be interesting - for now we should focus on having it run with local drivers and subprocesses\n- that IPC design was meant to solve the following problems specifically:\n  - ability to use a custom driver against a vanilla build of the registry without having compile a custom registry\n  - open the possibility to have different languages for drivers\nNow, I'm happy if that design let other things to be done, and opens-up creativity, but the bottom-line is (@bacongobbler): make like simple to drivers authors and clean separation between the drivers and the registry.\n. @BrianBland let's go for another review of this as soon as @BrianBland provides some documentation and drivers authors (@noxiouz @bacongobbler) can kick the philosophy in the tires.\n. @ahmetalpbalkan registry do need a move operation at the driver level\nThe main reason for that is ids used to store blobs will be content-adressable - hence requiring to first store some given blob in a temporary place, then move it to the final destination.\nIf you don't have a move operation, then no big deal, wrap a copy+delete (as long as your copy is efficient enough).\n. @ahmetalpbalkan fair enough\n. LGTM\nThis has been under scrutiny for two weeks, long enough I guess.\nWe can certainly fix things down the road now.\nMerging.\n. Hello @bshi \nBuild failure is a style issue.\n./docker_registry/tags.py:9:1: H306  imports not in alphabetical order (gevent, flask)\nYou may want to run tox before PR-ing to have everything checked.\nAbout this, it's too late for 0.9 unfortunately - so, I'll merge it after the release.\n. It's pretty much pip install tox and you should be set.\n. 0.9 is released - master is open for this.\nLGTM\n@stevvooe what do you think?\n. Merging for 1.0\n. 8.8.8.8:53 is google DNS server.\nSmells to me like temporary DNS resolution failure on your side.\n. @wiwengweng any update?\n. If this is still an issue, please say so, and I'll reopen.\n. +1\nBetter namespacing is definitely something to get right from the get go.\nAbout split configuration, and entirely split process, let people do that, but that shouldn't be the default.\nAbout env variables mapping, this needs to be detailed more.\nDo we want the previous behavior?\nfoo: _env:SOME_FOO:default_value\nOr do we want implicit interpolation on any conf variable?\nfoo:default_value and FOO sourcing?\nHow do we elegantly map nested names to env:\nfoo:\n   bar: baz\nwould that \"map\" to:\n- FOO_BAR\n- FOO.BAR\n- other?\nAbout \"remote\" configuration files support (http, whatever), do we want that in the core, or should that be the responsibility of a deployment script to copy it over locally?\n. @BrianBland +1 for underscores - it makes it more clear to people what they need to look for inside the configuration  file.\nAnd +1 on everything else.\n. Implemented with https://github.com/docker/docker-registry/pull/652\n. Thanks! How do this compare to, say, redis?\n. A couple notes:\nContent-adressibility is for layers only. There doesn't seem to be a benefit in content-adressibility for manifest files.\n\"Caching\" layers is not something we have been looking into - many people offload actual delivery to a CDN (we do), which does provide more benefits than caching big objects on the service would - and this pattern is likely to be kept for v2.\nMaking it possible to use alternative cache engines for manifest files (memcache or otherwise) is something that we should consider - though (being lazy) I would likely support redis as being the default, officially maintained. Would be nice to have alternatives, sure (provided it's easy to mutate the objects).\n. Bottom-line being: I would rather focus on making the content easy to be cached at the transport layer (http), rather than dedicating too much intelligence in application level caching.\n. The suggested new format so far (https://github.com/docker/docker/issues/8093) does include an informative-only architecture flag.\nThe upside of that proposal is that it's easy to index.\nThis is NOT final, and should be considered work in progress. There are unaddressed questions for now (do we want \"fat images\"? - eg: multiple architectures, with dependent layers - how do we address tag resolution if we are not going with fat images?)\nAlso, there is in the engine an arch field AFAIK: https://github.com/docker/docker/pull/707/files\n. I kind of feel working this issue here is backward - the situation needs to be clarified first in the engine itself (you are right about both the \"usage\" and the kernel field).\n. I guess a new issue altogether in docker is good - and see what core maintainers and shykes think about the future for this and the big picture about arch/kernel features.\n. @duglin close enough, yes\n. Nothing this major is going to happen on registry v1, but that's definitely something to consider for registry v2.\nNow, that may be expressed as a storage driver I assume. But then, you would need a way to know which images are complete and which are not. Does btsync provide such information?\n. Ok, then you might consider authoring a driver for the v2 registry based on btsync. There is little documentation right now, but more will come out during the following weeks.\n. V2 is in the making.\nAny work in progress you can find here:\nhttps://github.com/docker/docker-registry/issues?q=is%3Aopen+is%3Aissue+label%3ANext-generation\nand specifically, a first driver proposal:\nhttps://github.com/docker/docker-registry/pull/643\nBest.\n. What you describe sounds like mirroring - which exists for registry v1, and is planned for v2 as well.\nIndeed, using local registries on every machines, and a cache mechanism (or probably better, a btsync storage driver) would solve that kind of use case.\n. @bobrik FYI the new mirroring model is discussed here: #658\nAbout bt(sync) / p2p synching, there is some interest (this ticket indeed, but also other people in the community). I'd be happy to have a driver for the v2 release (doc here https://github.com/docker/docker-registry/tree/next-generation/storagedriver) :-)\n. The registry itself expose an HTTP API to be consumed by the docker engine. Now, the registry uses a backend storage (usually filesystem, or s3, or elliptics, etc) to retrieve data from.\nThe idea here would be to run a registry for each docker engine and have each docker talk its registry, then have the backend storage driver do the heavylifting by implementing p2p file synchronization (that should be fairly simple with btsync).\nGiven the design of these drivers, in the future it might be possible to directly use the backend storage from docker client without code modification (but that's science fiction for now).\nSo, if I were to do this short-term, I would implement this interface https://github.com/docker/docker-registry/blob/next-generation/storagedriver/storagedriver.go using a btsync backend.\n. Hi @keyvanfatehi \nThis here is the open-source registry - Hub bugs should be reported to the support instead: http://support.docker.com/\n. Anyone want to review this?\nAlso @BrianBland can I ask for some basic documentation?\n. Then it's in.\n. I'm not aware of any code in the registry that would alter json payloads.\n@snowsky Can you copy the content of the offending file?\n. @snowsky is this still an issue? If so, please say and I will reopen.\n. Not needed indeed.\n@michaelheyvaert can you confirm it works as expected if passing a list from the env?\n. Thanks @michaelheyvaert \n. There must be another way to have the database creation be race-safe, right?\n. ping @dmcgowan @jlhawn @BrianBland\n. @bacongobbler @wking thanks a lot for the tips!\nWe want something like swagger on top of that. The rest sounds good to me.\n. Now using drone, along with golint, go fmt, go vet, go test + goverage.\nDocumentation will be handled separately, so I believe this can be closed!\n. Thanks a lot for catching this.\nThat code is terrible.\n. @wking proxying ftw :)\n@sybeck2k can you share your registry configuration file (redacted), registry launch command you used, and the result of: curl -v -o/dev/null https://YOURREGISTRY/v1/_ping\n. It's not a client problem. The client tries to use token based authentication (hub default) instead of using the basic (proxy) auth. This is not periodic either - my current hunch is \"misconfiguration\" (either registry or apache - admittedly, no one maintains that apache proxy file).\nAnyhow :-)\n. Then you are suggesting basic auth workflow never worked (which we know it did) :)\n. @bacongobbler you are familiar with the nginx proxy - any insight into this one?\n. @sybeck2k happy you figured it out.\n... and it helped @wking plunge into the delices of proxying! :)\n. What version of the registry are you using?\nWhat is your launch command?\nWhat configuration do you use?\n. Can you try without AWS_REGION?\n. @satyrius @thogg4 \nThe Region bug is tracked over here: #400\n@satyrius can you try with specifying a boto_bucket entry pointing to your bucket as well?\n. @satyrius that's because the dependencies are pinned to specific versions.\nYou can workaround that by setting the env variable DEPS=loose\n. @satyrius yeah, not really convenient.\nThis should do:\nFROM registry:0.8.1\nENV DEPS loose\nRUN pip uninstall -y docker-registry-core && pip uninstall -y boto && pip install boto==2.34.0 && pip install docker-registry-core\n. Fixed\n. Thanks a lot @aweiteka \nA couple questions, and some infos about what's going on with v2:\n\nMany companies require to host their own bits. It's their control point. It's an important legal and provenance issue for them.\n\nDoes image signing (coming with v2) change that situation for them?\nMy point being image signing allow bits to be served from (any) untrusted source while still ensuring the bits are untempered.\nAlso, if I understand well, crane does 302 to where the actual bits are. So, the company (content owner) has to trust the ISV's registry (/crane) to do what's right here - which to me kind of weakens the \"control-point\" - eg: I'm not sure I see a difference then between 302, proxy-pass and mirroring, from a control standpoint.\nAbout the v2 protocol - it's quite likely that layers urls are going to be namespaced, eg:\n```\nimage:\n/v2/manifest/redhat/rhel7/latest\n/v2/manifest/isv/foo/latest\nlayers:\n/v2/blob/redhat/rhel7/A_RHEL_LAYER_ID\n/v2/blob/isv/foo/A_RHEL_LAYER_ID\n/v2/blob/isv/foo/A_FOO_LAYER_ID\n```\nThe reason for that change is simpler access control (flat namespace for layers - we currently have - doesn't work well).\nThat doesn't mean content is actually duplicated on the registry - but that inside the registry mechanics, there are \"mount points\" for layers into namespaced url.\nRight now, the way I see it, it should be pretty easy to:\n- ensure docker engine behaves correctly regarding 302\n- have some mechanism inside the registry to redirect specific layers\n... the part where I'm not that confident is the engine bits allowing to instruct a registry selectively that a given layer is to be found \"elsewhere\". Actually, I'm wondering if this is at all an engine decision to make.\n. > But the ISV can put their own auth in front of their registry.  That\n\nmeans they can use their own auth there, but leave access to the Red\nHat images up to the folks running the Red Hat registry.\n\nDo you suggest there may be several layers of authentication (authenticate -> 302 -> authenticate again)?\nEither way, the point still stands: is there actually a strong \"control-point\" in trusting a downstream registry to do a redirect?\n\nIf you used proxy-pass from Red Hat, clients would have to give Red Hat their ISV\ncredentials.\n\nI don't think we are talking about anything requiring authentication, but rather publicly available content.\nOtherwise, you might end-up with requiring multiple different authentication for one image (eg: one for each layer), and that doesn't sound reasonnable.\n\nYou could use proxy-pass from the ISV (assuming the ISV has read-access to the Red Hat repository), but then the ISV has to handle the extra load of distributing the Red Hat images.\n\nWith caching, that's exactly the kind of stuff people want.\n\nHmm.  GitHub seems to do fairly well with a flat namespace.  Is the goal to make this easy to use with [...]\n\nThis is exactly what I am saying. Layers must be accessed under a namespace (foo/myimage), like the manifest itself. Doing otherwise (eg: like we have, a flat namespace for layers) is a mess to get right as far as authorization is concerned.\n\nAh, maybe you indent to have multiple hierarchies?  That would be as flexible as an auth service mapping users to namespace/repository permissions, but I don't see a point to exposing it outside of the auth service.\n\nHere:\nv1/layers/SOMEID\nmeans when wking wants to fetch this, the registry/auth has to decide whether wking is entitled to read SOMEID.\nAnd this has to be done for every layer.\nv2/blob/foo/bar/SOMEID\nmeans we need to verify that wking has access to foo/bar - and this authorization is the same for all layers.\nWhether SOMEID was \"authorized\" to be made available under foo/bar in the first place is a one-time operation, at push.\n\nNot inside the client to check several repositories for a given layer?\n\nThis sounds messy. Having to configure your client with multiple registry endpoints in order to be able to pull a single image - ending with situations where you try to figure out why you are missing some layers.\nClient side configuration doesn't fly.\nAnd craming registry urls into the manifest for every layer doesn't sound good either.\n\nI don't see why image hosting would be anyone's decision to make. If Alice has an image, she should be able to host it wherever she likes (modulo copyright/licensing/\u2026), without needing to inform\nanother registry about her choice.  And if Bob wants access to Alice's images (modulo Alice's auth service), he should be able to have his client check her registry (both implicitly via a registry preference\nlist, and explicitly via a command line option).\n\nYou are missing the point.\nPeople will be allowed to push their content wherever they want, and pull things from wherever they want. Still, so far, this is envisioned as a single unit:\ndocker pull foo/bar --from https://whatever is expected to work as... it sounds... and retrieve all needed content without the need for some acrobatic client configuration fiddling.\nNow the question raised here is how to allow certain registries to delegate the responsibility of delivering specific layers to other registries.\n. > I'm suggesting we skip the 302 entirely.  The client would: [...]\nSimple answer: no. :-)\nBlind trying a bunch of services one after the other for every layer is nonsense.\n\nI'm not trusting anyone to redirect.\n\nWhat about we let @aweiteka speak for himself? This is what they do with crane currently if I understand correctly.\n\n[the rest]\n\nThis discussion is getting largely of-topic.\nI would ask you if you wish to keep these long chitchats style for irc chat sessions, or focused on tickets where they are relevant.\nAll this is just muddying the water here - @aweiteka has a use-case, let's try to see clear in it instead of trying to defend your opinionated opinions on things like atomic storages Mr. @wking  :-)\nTo sum-it up:\n- we are NOT going with a complex system where you have to know beforehand where to find the various pieces of a single image (read: a list of registries to try and fail where layers are scattered, all with possibly different authentications). This idea is simply broken, sorry to be harsh.\n- the image federation idea here needs to be studied since it's being suggested, specially in the light of what's coming next (eg: image signing), but the first thing is to listen and understand to the use-case. We can argue about the nitty-gritty-techy afterwards. eg: \"I'm not trusting anyone to redirect.\" isn't helping :-)\nHang-on ;)\n. Love it :)\nMerged.\n@ahmetalpbalkan registry v2 is in the making (in go instead of python). If you want to chip in on the new storage driver interface, join here:\nhttps://github.com/docker/docker-registry/pull/643\nmore generally here:\nhttps://github.com/docker/docker-registry/issues?q=is%3Aopen+is%3Aissue+label%3ANext-generation\n. Thanks a lot @ahmetalpbalkan \nI would prefer these bits to be in ADVANCED.md instead (like for Ceph).\nThe reasoning is to keep the main README slick.\n. And S3 should stay in the main README (being the default for registry v1) - this will change with v2, and if you guys are willing to go with the effort of writing a v2 driver for Azure, that could be first class citizen ;)\n. Merged. Thanks!\n. Thanks for this. Merged.\n. We need a better way (generate that from commits) - manually editing this is a pain.\n. +1\nMerged.\n. Love it :)\nCan I ask for a case-insensitive result?\nAlso curous about how this compares with https://github.com/docker/docker/blob/master/hack/generate-authors.sh ?\n. @stevvooe yes I probably want the same script that docker core uses - will sort that one later this week.\n. Agreed.\nThanks @wking!\n. Were you able to confirm this does fix the problem?\nMy point being that gunicorn using gevent does the patching before this code path gets a chance to be executed.\n. That's fantastic. Thanks a lot for this fix!!!!\n. Merged. Thanks!\n. I assume you are missing a boto_bucket: _env:AWS_BUCKET entry.\nThere is some duplication in there - the conf has grown messy, sorry about that.\nI would suggest you start from: https://github.com/docker/docker-registry/blob/master/config/config_sample.yml#L103\n. Something is wrong is your setup. The 404 answer doesn't come from the registry. I would bet on a dns issue.\n. We have a new boto-wizard on-board, so let's ping him :-) (ping @chuegle)\n. @chuegle in light of this, I'm considering vendorizing the boto lib. If you want to PR that I'll definitely merge it - or just applying the inline patch inside the Dockerfile.\nForcing simplejson should be simple: https://github.com/docker/docker-registry/blob/a409762c1f8fe7fff611c66fbdbafce88d75c648/depends/docker-registry-core/setup.py#L38\nThen you can relax deps requirements on the registry by running it with the env vars DEPS=loose (will just require the major version to match).\n. about the last bits - again, vendorizing might be a better solution - or patch the fixed version in place.\n. @c-schmitt yeah, please create a new issue and/or PR and let's see what the best way to support this.\n. @chuegle good point.\nAbout loosening the version (by default), I'm not for it - if only for the reason it makes it more difficult when we need to patch.\nPatching in the dockerfile definitely sounds good though.\nAbout letting people do what they want (eg: when using the registry in whatever env), the DEPS=loose mechanism should do the trick.\nThanks a lot for the input!\n. @sday @wking @chuegle what do you think?\nI'm ok with the extra env var\n. +1 for -e S3_USE_SIGV4=1 (needs to be documented).\nSavvy people can still do some acrobatic stuff with boto.cfg, but I don't want to promote that as the main way to go (although, I'm ok adding some infos about that into ADVANCED.md).\n@c-schmitt yours to take if you want :-)\n. Thanks!\n. I like it, let's merge it :-)\n. @tobegit3hub if you are not aware about that, we are starting \"from scratch\" with a new registry, implemented in go.\nThe dev branch is here https://github.com/docker/docker-registry/tree/next-generation\nThe PR that merged (a couple minutes ago!) the new driver interface was here: https://github.com/docker/docker-registry/pull/643\nThe new documentation about v2 storage drivers is here:\nhttps://github.com/docker/docker-registry/tree/next-generation/storagedriver\nAnd the expected timeline is to release production ready registry by Q1 2015.\n... my point being: storage drivers for V1 are perfectly ok, and the V1 registry is here to stay for some time (https://github.com/docker/docker-registry/issues/612), but the future is in V2.\nNow, for v1, the best approach is to mimick one of the (very good) drivers, like elliptics. \n. LGTM\n@stevvooe very interesting to see the current issues with boto - goamz for next-gen (or whatever dep lib we pick does need to be up for the game there)\n@chuegle thanks a lot!\n. All: 0.9.0 is available on the hub\n. @tobegit3hub my point being google took over ownership on the gcs driver -> @proppy your call :)\n. And it's in! Thanks a ton!\n. @proppy any name is ok!\nWe just wanted to check with you where one could find and install the GCS driver itself.\n. Thanks @wking \nThis is (although related) kind of different from \"traditional\" logging (#635) - and rather about supporting out of the box third-party SAAS monitoring services.\n. Bugsnag and new-relic support are in.\n. You need to raise if there is no file:\nraise exceptions.FileNotFoundError(\"File not found %s\" % path)\nHave a look at the dumb driver for reference: \nhttps://github.com/docker/docker-registry/blob/master/depends/docker-registry-core/docker_registry/drivers/dumb.py\n. It's hard to tell without more context. You are using 2.7, so that can't be the same issue IMO.\nCan you share your image details / config / launch stance?\n. mmmm - the error log is fairly non-specific. Anything else in the log that could help (above?)?\nAlso, is this something happening specifically with your mysql enabled build? What happens if you use your build (including mysql) but don't use mysql?\n. Thanks a lot for that. Sorry not to be more helpful on this but this is kind of baffling.\nYou might want to go with the newly released 0.9.0 though, since it contains numerous boto, gevent and gunicorn related bug fixes.\nIt's on pypi, and will appear on the hub momentarily (https://github.com/docker-library/official-images/pull/300)\n. @Rob-Johnson any update on this with 0.9?\n. Sorry about the inline noise :-)\nLGTM, thanks a lot for this!\n. cc @stevvooe \n. LGTM. Thanks @BrianBland !\n. LGTM\n. User-agent and time-to-process request are pertinent informations that I don't see in the proposal.\nIf it's simple to add them there, I would say yes, the need here would be covered.\n. @zedtux registry v1 (the currently released 0.9 version) supports extensions: https://github.com/docker/docker-registry/tree/master/docker_registry/extensions#docker-registry-extensions\nFrom there it should be easy to implement a webhook extension yourself.\n. I believe it does: https://github.com/docker/docker-registry/blob/0dd4fc818da236c3e5df8c6c87fd4f3da19492d7/docker_registry/index.py#L76\nAm I missing something?\n. @BrianBland needs a rebase.\n. Travis down?\n. Fixed by #685\n. @wking @stevvooe @bacongobbler what do you think?\n. >  but I think the upload-time space savings are worth a few extra CPU cycles.\nActually, this is likely not accurate, and the reason why layer compression was removed in the past. (cc @vieux)\nWe need to support both compressed and uncompressed uploads - and content-negotiation for download (likely being able to serve both).\n. +1 ;)\n. Already installed as an extra:\nhttps://github.com/docker/docker-registry/blob/master/setup.py#L103\nhttps://github.com/docker/docker-registry/blob/master/Dockerfile#L29\n. @yaronr sql initialization failing - using preload should fix that for the first run: https://github.com/docker/docker-registry#sqlalchemy\n@wking this is growing out of control...\n. @yaronr thanks\nI updated the documentation to no longer include the search endpoint by default - at least it will prevent the bulk of people from shooting themselves in the foot right away...\nWill figure out something better ASAP.\n. Hi @nickandrew \nPrivileged access and key are unrelated.\nThe currently supported way to have ssl is to use nginx in front of the registry for SSL termination.\nYou can look here for config snippets: https://github.com/docker/docker-registry/tree/master/contrib/nginx\nIf using nginx is not acceptable, we recently upgraded gunicorn, meaning you can now have native gunicorn SSL support.\nBeware this is untested so far.\nThat would go with something like:\ndocker run -p 5000:5000 -v /mycertsfolder:/ssl -e GUNICORN_OPTS=\"['--certfile','/ssl/registry.crt','--keyfile','/ssl/registry.key','--ca-certs','/ssl/ca.cert','--ssl-version',3]\" registry\n. Since you are going with load-balancing, I would definitely encourage you to do SSL-termination there instead of at the registry level. Don't know Bamboo, but definitely worth a try (we use HA in production).\n. Compression is one thing, another is: if your image is based on X, do you account for X base layers into your image size? If not, why?\nSharing layers between images means \"quota\" needs to be defined clearly.\n. So, how do you see quota handling in such a context (eg: there is no \"layer owner\", layers are in a \"shared\" space - only the authorization to use them makes them visible as \"owned\" by an image).\nSpecifically, what happens if your repush an image under a different name (eg: the layers will not be reuploaded).\nI'm not opposed to the feature - we need to discuss it more. monday irc?\n. Sounds straight-forward.\nNow, should we \"deduplicate\" (eg: count a given layer only once per-user)?\n@ncdc that kind of approach would be ok for your use case?\nI would love this to be an extension.\n. And I think the proposed design doesn't prevent this. What do you think?\n. Even if we don't, that can still fly. Garbage collection can work.\n. So:\n- need to devise the exact quota strategy (I'm fine with your suggestion that whoever uploads the layer get that of their quota)\n- need to address the details (what happens with uncompleted uploads? how long does it take to consider it \"dead? is the quota authorization done before uploading or at the end? if the earlier how do we handle \"liars\"? how do we handle simultaneous concurrent uploads?)\n- it should be possible to express as an extension, so, it's a good exercise to try and see what is required from the core registry for this to fly\n. LGTM\n. LGTM\n. @channel ping\n. I removed it from the default example in the documentation since it's partly broken right now (requires gunicorn to be started with preload to avoid race-concurrency).\nSo, let this PR be open for now - we will merge it when the underlying problem is fixed.\n. Merging for next release\n. Your git commit title is a bit long. Can you amend it to under 50 chars? (sorry about the inconvenience, we are enforcing quite pedantic style/syntactic rules...)\n. ping @chuegle \n. ping @c-schmitt you have comments from @chuegle our S3 guru :)\n. LGTM\n. Merged, thanks a lot!\n. The v2 design should make it way easier to walk the storage and detect dangling layers. Would still have to be done as a script / extension (and shouldn't require core changes, unlike with the current python implem).\n. I would trash the database and restart the service. @wking might know more about this.\n. @galitz I believe you can use preload by default under production condition without adverse effects.\n. @bacongobbler @wking you are the experts for these :-)\n. We can probably use some words in the FAQ or ADVANCED doc.\n. Let's try to add some basic stuff in there: https://github.com/docker/docker-registry/blob/master/FAQ.md\n. Thanks!\n. @michaelheyvaert I would rather have the code fixed to not be racy...\n. LGTM\n. LGTM\n. LGTM\n. LGTM\n. Merged.\n. Thanks a lot! Merged.\n. Thanks a lot for the report.\nThe official search endpoint (index.docker.io) is not using the docker-registry implementation, so, I assume these to be two (somewhat) different matters. I reported the issue on index to the hub team.\nNow, about the registry implementation itself, do you experience slow results with your private registry?\n. Agreed.\n. Travis build sayz: ur'z doing it wrong.\n. Needs a rebase?\n. Thanks for the report.\nI'll have a look - but see my comment #540\n. I like that :-)\nPing @noxiouz @bacongobbler @ahmetalpbalkan \n. @chris-jin done\nThanks for this (and thanks @bacongobbler )\n. @dlaidlaw current master contains a fix - if you want to test it out.\nThanks for reporting this, and thanks @wking for the quick patch!\n. The code make sense and Travis is happy, I'm happy :)\n. LGTM\ncc @BrianBland\n. LGTM\n. Aleluia\n. @bacongobbler that one is yours :-)\n. @shreyu86 thanks a ton! alongside that, I would love to see some worded explanation about it somewhere here https://github.com/docker/docker-registry/blob/master/ADVANCED.md\nPossible?\n. Thanks!\n. More inspiration (for later?)\nhttps://www.owasp.org/index.php/Path_Traversal\nSpaces and url-encoding of dangerous characters is paramount.\nLGTM about this anyhow.\n. @BrianBland PTAL\n. LGTM\n. cc @BrianBland\n. @wking\nI believe you guys are pretty much saying the same thing, although probably with different words and looking at it from different perspectives.\nA couple of notes, though:\n- I need to insist again on that: drivers must be simple to implement, and safe by default - the more responsibility you delegate to drivers, the more duplication you will get into them, the riskier and the more complex they are going to be\n- we do need provision for multiple versions of tarsums - tarsum will have to be modified in the future - like it had to be modified in the past (include xattrs, remove mtime), if only, for security reasons, and we will live in a world where content will link to layers through multiple different versions of tarsum - there is no discussion on that\n- it's trivial to produce two layers with the same tarsum that have different shas - this is not only expected, but the reason tarsum was designed in the first place\n- keys vs. filesystems paths is not a debate, and doesn't change things as far as allowed characters in path are concerned: unsafe chars should be escaped to a common, safe subset, and this must be done by the registry, not by individual drivers - and I can assure you there are problems with the \"+\" sign for example, right now, on S3 + Cloudfront\n- internal interfaces are just that: internal APIs to keep things clear and reasonably modular. I don't think there should be a discussion on \"I don't want multiple levels of interface\" :-)\n- discussions on the extension mechanism should be kept in the corresponding issue\nHope that helps.\n. > Now new registry code has to remember that it's supposed to work around this storage-driver bug. \nNo it does not need to.\nRegistry code uses an internal API that deals with that once and for all, in one place.\n. > \u201cWhat key characters does my backend support\u201d\nSimple enough to devise. Likely something like: a-zA-Z0-9_.:-\n. > I agree that that's likely to work, but it's another bit you have to write up in the storage-driver API \u201cWe'll only use keys that match the regexp '[a-zA-Z0-9_.:-/]*', so you'll only need to escape keys if your storage driver is sensitive to those characters.\u201d \nFortunately it's simple enough to test exhaustively (unlike testing exhaustively \"some bytes\").\nPeople who pass the test suite have nothing to do. People who fail can look into the (pretty self-explicit) test.\n\nThere's less shared information if you say \u201cKeys can be any byte sequence.  If your storage backend doesn't deal with some bytes as keys, make sure you translate the keys to something it does support.\u201d\n\nThen people don't do the work properly because (insert random reason), and we end-up with a variety of bugs reports for third-party code that we have to carry around.\nOne step further: what about path traversal? or security at large? do we trust the driver authors as well to do that properly?\n\n[shared libraries to do the work in drivers]\n\nSo we maintain them, recommend that people use them, have to upgrade them and go through all drivers so they keep up with the latest version?\nMaybe I'm just opinionated :-) but I strongly believe that we do need to own some critical stuff inside the registry and that a great API is a good API that don't let you shoot yourself in the foot.\nDrivers authors should focus on stuff that has value: provide storage primitives - not implement petty sanity checking.\nEither way :)\n. Thanks for this @tianon !\nI'll do some tests to help with it - seems odd that the lzma && event libs would \"be there\"\n. Ok, I gave it a spin, and it does work so far. lzma-dev and libevent are there by default for some reason.\nHere are my concerns so far:\n- it seems the image is two times bigger? am I mistaken here? (from 411.6 to 853.9)\n- is jessie a moving target in term of versions? I'm especially concerned about libevent\nThanks for your help on this @tianon!\n. Howdy @tianon \nany news on this?\nThanks!\n. Thanks a lot @tianon !\nYes, I think we should wait for \"slim\" images. The registry itself (and derived products like google version) account for almost a million download now - any byte matter :).\n. LGTM\n. Hi @dlaidlaw \nThe NG registry extension model (#613) and endpoint for search has not been decided yet - but it's certainly open to proposals, and that doesn't prevent work from starting. I'd suggest devising a good extensibility model to start (see the issue I linked).\nAs for registry v1, replacing the existing sql backend can be considered.\n. As an extension would be cool, if feasible https://github.com/docker/docker-registry/tree/master/docker_registry/extensions#docker-registry-extensions\n. Thanktastic!\n@wking @shin- what do you think?\n. Agreed.\n. LGTM\n. LGTM, thanks for this.\n. ping @stevvooe @BrianBland\n. @BrianBland \\ooooo/////\n. Interestingly, the build doesn't fail though golint does...\n. Changed things in the meantime. PTAL.\n. You... are... green!\nLooks Green To Me\n. Looks Not Green To Me\n. LGTM\n. LGTM\ncc @BrianBland\n. How do retry / resume mixes with that?\n. Sounds good. Just wanted to make sure that indeed individual layer resume is supported, and that we don't just fail every layer in the window because one failed and needs to be resumed.\n. LGTM then - understood this will be updated later.\n. Some discussion happening here: https://github.com/docker/docker/issues/8831\n. Thanks!\n. @joda70 You need to enable the search endpoint I assume (SEARCH_BACKEND=sqlalchemy)\n. Ok, happy you figured it out. I'm closing this ticket then - but please say so if you still have problems.\n. I believe it is. Now, any reason not to run 0.9?\n. Ok.\n. Is your question answered? I'm closing this then, but if it's not, please say so and I'll reopen.\n. I assume you can do that by restricting access to the PUT http method - but this is untested territories.\n. I never went down that road myself, and I don't believe there is specific docs about it - but configuring nginx shouldn't be too hard.\n. Is there anything else in the logs? Or is it just hanging there?\nAlso, I believe a number of dns related issues were fixed with docker 1.3.\n. Thanks! Keep me posted if you still have difficulties there.\n. LGTM\n. Also, like pointed out earlier, assuming anything about the storage side is OT for now.\n. Needs a rebase.\n. ping @BrianBland \n. LGTM\n. LGTM\n. The \"client\" is just a cli connecting to your (daemon) \"docker machine\" and doesn't do anything on its own.\nI assume you changed something related to certificates on your daemon machine?\nFrom the daemon machine, can you try: curl -o/dev/null -v \"https://index.docker.io/v1/search&q=centos\"\nAlso, the bug tracker here is best used for actual bug reports - if you are looking for general help, you will find irc (#docker) to be faster / more efficient (or the forums). \n. I believe your issue here is a generic certs management issue. You might indeed get some more helpful comments on irc/forum about all this - it seems unrelated to the registry project in itself though.\nSorry not to be more helpful...\nClosing since it's about userland CA configuration, not related to the registry itself.\n. Hi @galitz \nIs the repo in a \"broken\" state after such a failed layer upload?\n. Hello, any update?\n. cc @metalivedev\n. LGTM\n. cc @BrianBland @stevvooe\n. @stevvooe PTAL\n. LGTM\n. The new registry (in the making, see the next-generation branch) will support better authentications methods, including JWT based from a separate auth service:\nSee here for the discussion: https://github.com/docker/docker-registry/issues/623\nand here for a first proposal: https://github.com/docker/docker/pull/9081\nI'm closing this as a duplicate in favor of these.\n. You're welcome :)\n. Hi @tduffield \nYour issue is unrelated to dind. It's about a race condition in SQL db initialization.\nYou should be able to workaround this by using preload as a gunicorn arg for your first registry launch. Or if you don't need the search endpoint, disable search entirely.\nOnce the db is inited, you can run without preload.\nSee here for more: https://github.com/docker/docker-registry#sqlalchemy\n. Exactly :)\n. Need to account for \"plain http\" static registries.\n. API for v1 needs to remain untouched.\n. I don't think the 1.3 daemon uses /v2/_ping.\n. Agreed.\nUnfortunately (or fortunately?) most efforts go into the registry \"next-generation\" these days - PR are definitely welcome then, but I prefer that expectations are not set too high about the existing registry possible improvements - it's in maintenance.\n. @tcurdt Yes, the roadmap is here: https://github.com/docker/docker-registry/blob/next-generation/open-design/ROADMAP.md (warning! subject to change)\nAbout the node implem: nice to see more compatible products! I'll look into it.\n. Ok, flagging this as Next-Gen, as a reminder that we need to have better documentation :)\n. LGTM\n. LGTM thanks a lot @noxiouz!\n. LGTM\n. LGTM\n. LGTM\n. Hi, this is unfortunate, but there is currently a bug hitting at search database initialisation.\nThere are at least two ways to workaround this:\n- disable search entirely if you don't need it (SEARCH_BACKEND)\n- have a first run to initialize the db using --preload (see https://github.com/docker/docker-registry/#sqlalchemy)\n. There are quite a number.\nThe recent one that comes to mind is: #760\nThere is a proposal to fix this here: #772 that will likely be merged fast given how painful this is.\n. Thanks a lot @shin- !!!\n. Hello @rchaudha \nI would suggest you use the latest registry version to start (0.9).\nAlso, please read https://github.com/docker/docker-registry/blob/master/DEBUGGING.md and provide the information listed there (\"Basics\" and \"Your private registry\").\nAfter that, might need as well your docker registry logs, and docker daemon logs.\nI know it's boring... but really we can't help without at least these.\n. Any update @rchaudha ?\n. Ah, consistency and distributed filesystems...\nThanks @BugRoger !\n. cc @BrianBland\n. cc @wking @shin- \n. Fixed by #779\n. Please (first pip install tox) run tox to have all tests and style check be run before you PR.\n. +cc @shin- \n. Merged. Thanks all!\n. I rebuilt after merging #783 but it seems like it's still failing.\n. LGTM\n. LGTM\n. Let's start by making sure that you do hit your local registry: you can check that by looking at your registry logs and see if your docker engine is hitting it.\nAbout layers size v1 vs. v2: they are exactly of the same size (for they are the same objects).\n. Also, running your docker daemon in debug (docker -D -d) and looking at the daemon logs will definitely help.\n. > The Docker daemon is certainly hitting the registry as I can see it GETing the layers and the json files\nOk. Is it hitting the layer files as well? (just want to triple check before more digging)\n. So yes, positive - sorry about the bikeshedding.\nBottom-line:\nYour registry is (probably) well configured and does deliver correctly (as demonstrated by your wget query to it) \nI believe the \"limitation\" is simply with docker itself - when you state that \"The above even accounts for Disk I/O.\" I don't think this is accurate. Docker has to expand/untar the layer, then put files into devicemapper (or whatever driver you are using).\nI'm surprised about the size difference between v2 and v1(likely some compression setting being different). Can you time docker pull ubuntu for both v1 and v2 (out of curiosity about actual IO impact + tcp throttling vs. download size/compression factor).\n. >  I don't think there is any difference in I/O overhead between the wget and Dockers download. Reason being is that the \"extracting\" (untaring and expanding) the layer happens in a different stage and is not part of my timing in the download stage\nI don't think this is correct (for v1). Untarring and processing happens \"on the fly\" IIRC.\n\nAs far as I can tell, this works perfectly fine and does the same thing it would normally do against V1.\n\nIt does not - since the layers might be cached already on the mirror.\nEither way, enough arguing :-)\nThe bottom line here is:\n- wget is faster than docker when downloading - which points to a limitation in docker rather than in the registry\nccing @unclejack \n. And virtual-size is the size of the complete image: with read-only filesystems, you might very well have total layers size way above the resulting size.\n. > @xeoncore Sadly, this isn't Disk I/O related (as far as I can tell). \ndocker does a lot more than just untarring a tar file to the local filesystem.\n\nsize issue\n\nI believe compression settings are differents. I'll have a look next monday to that.\n. @xeoncore again, there are several additional factors in what docker does:\n- gzip is slow in go (not in your tar test)\n- extracting to, say, devicemapper, or AUFS introduces more latency\n- ungzipping, then extracting to the graph driver on the fly will (likely) trigger TCP congestion control, further throttling your download speed (<- this might be the most likely explanation)\nEither way, I'm not saying there is no problem - but if it's fast with curl or wget, then it's safe to assume the problem is in docker - specifically, in the v1 approach.\n@unclejack any hunch on @xeoncore infos?\n. If you are using btrfs you may find this relevant: https://github.com/docker/docker/issues/9445\n. @sammcj @xeoncore uses AUFS (https://github.com/docker/docker-registry/issues/785#issuecomment-65335437)\n. Parallel pull is a docker feature, not a registry feature.\nThe bottom-line is:\n- previously docker was pulling only one layer at a time\n- the v2 protocol allows for parallel pulls, which is leveraged right now only on a small subset of the data, and will be released alongside with the next-generation registry (see branch: https://github.com/docker/docker-registry/tree/next-generation)\nHope that helps.\nBest.\n. So? What happens when you hit the redirect point? (eg: http://licantropo1.cnaf.infn.it/v1/users/)\nAlso, what kind of information do you expect? Standalone registry doesn't hold user accounts.\n. Again what happens when you hit http://licantropo1.cnaf.infn.it/v1/users/ ? (the target of the redirect)\n. ping @shin-\n. @phopkins I assume @shin- answer addresses your question - if it doesn't, sure say so.\nThanks!\n. I would certainly merge any enhancement to the doc... so, yes, would love that.\n. LGTM\n. LGTM\n. LGTM\n. Drone likely stuck. Moving away.\n. Thanks, community!\n@hordemark does it solve your issue?\n. Yes, this is the recommended workaround until the fix lands with the next version. Please use --preload for the first run, then restart without it.\n. There are a couple of them out there (https://registry.hub.docker.com/u/atcol/docker-registry-ui/ comes to mind), but I have no first-hand experience.\n. Maybe it's a boto issue. If you have more specific information about this (eg: exact queries that are in the wrong region), that could help.\n. cc @stevvooe interesting for v2 - we should:\n- handle gracefully backends that are configured read-only\n- return and display appropriate read-only error to the engine\n. Can you copy the output of the docker daemon in debug mode (on cf5-bastion), when you attempt the pull? (eg: stop docker then docker -d -D then docker pull docker.example.com:5000/john/qpid:latest)\nAlso, please dig docker.example.com and curl https://docker.example.com/v1/_ping, both from docker-host and from cf5-bastion.\n. Ok, now that your docker daemon is running in debug, can you copy the daemon logs?\nAlso, can you pull successfully from docker-host?\n. busybox is OT.\nCan you: curl -iv http://docker.example.com:5000/v1/repositories/john/qpid/images from BOTH hosts?\n. One more test might help:\ncurl -iv https://docker.example.com:5000/v1/_ping\ncurl -iv http://docker.example.com:5000/v1/_ping\nfrom both hosts\n. I don't understand your output.\nYou ran for https twice on docker-host with two different results (and not on cf5-bastion)? Can you clarify (and use the --insecure flag for curl as well).\nThanks!\n. Something is lying/missing.\nAre you using the same docker version on both hosts?\nAre you using --insecure-registry on both hosts?\nThe same http requests cannot end-up with different situations...\n. Ah, that sure explains things.\nHappy you had it work!\n. Hi @ahmetalpbalkan !\nThere are style issues with your PR. You can check what is in the drone.yml file (in a shell go fmt vet and lint) and fix it for the build to pass.\n. In a shell, the default configuration runs in debug. If you disable debug, you should be ok:\ndocker run -d --name docker-registry -v /var/lib/docker/registry:/tmp/registry -p 5000:5000 -e CORS_ORIGINS=[\\'*\\'] -e DEBUG=false registry:0.9.0\nand actually, not running with debug should be the right choice in production.\nLong story: something is wrong with the six package that is required by CORS - I'll keep this issue opened until 0.9.1 fixes it.\nThanks for the report!\n. @xiaods don't run with debug true.\n. @xiaods add -e DEBUG=false or use a flavor that doesn't set debug.\nHope that helps!\n. Yes, _ping (now) returns a json object.\n. (http) status code is 200.\n. Thanks for the writeup @ncdc \nWhat you ask for (immutable \"references\" to tags) equals \"complete history of every tag version\" in the new lingo. Indeed, this is a casualty of the new design - by making clear the difference between an image (a list of layers, name and signature) and a layer (a binary blob), tags are no longer aliases of immutable layers ids.\nI'm not sure I have a good solution for you yet, so, let's keep this open and give it some thinking.\n. LGTM\n. LGTM\n. LGTM\n. Do you launch your registry with SETTINGS_FLAVOR=prod? https://github.com/docker/docker-registry#configuration-flavors\n. How do you launch the registry? Please copy the used command.\n. I assume that's a distro provided package, right?\nSo, how did you specify your SETTINGS_FLAVOR env variable? \n. So it's a CENTOS package problem, I believe? What version of the registry are they shipping?\n. Hi @joda70 - any update?\n. Please be more specific about what your problem is and what you are trying to achieve. I'm closing this for now.\n. Please at least provide nginx and registry logs and configuration - no one can read the truth out of \"502\".\nAlso, this is typical of a misconfigured backend, so, you probably want to start there (is your registry running? can you curl it directly?)\n. Looks your problem is with your distro / nginx package then...\n. I would say you miss libssl-dev\n. Commit message looks good.\nTravis looks... green :)\n@shin- what do you think? Can you run a quick test with a key that we use?\n. @lsm5 can you verify it's working with a sample key that you would generate according to the instructions here, so that @shin-'s life is easier and he can test something that does work for the premisces?\nThanks!\n. - @noxiouz @bacongobbler \n. Hi @pmoosh \nThat might be boto / gevent related, indeed. cc @chuegle\nEnabling debug on boto might help.\n. @pmoosh can you try without --preload?\n. Yes, please disable search (just to test) - I'm pretty preload causes threading issues that hits boto.\nI'll release 0.9.1 soon with a fix for the sqlalchemy trainwreck.\n. I would say this is quite likely - fortunately affects only the last version, and the fix for that got merged into master.\n. The fix is on master (you should not need to use preload anymore). To use master, you just need to docker build it and use the resulting image.\nI'll push 0.9.1 out of the door early next week - expect a couple more days on top of that for it to land on the pypi/hub.\n. The last patch for 0.9.1 was pushed - will publish the release now - sorry for the delay.\nI am closing this since it was fixed on master. \n. Heads-up:\n- --preload is causing issues (this is what this ticket is about)\n- 0.9.1 does not need --preload anymore\n-> run the latest version, without --preload\nHope that helps.\n. God I hate that code :-)\n@clkao what happens if you start again the same container after the first failure? \n. Can you then get the logs? (docker logs containerid)\nOr run it without daemonizing so you see if it boots properly or not? (sudo docker run -e DOCKER_REGISTRY_CONFIG=/path/to/config.yml -e FLAVOR=prod -v /docker-registry-storage:/docker-registry-storage registry)\nThanks!\n. FileNotFoundError: Heads-up! File is missing: /home/administrator/dev/mtc-registry/config.yml\nYou need to mount your config folder into the container for it to be accessible (or use the default config, overriding select keys config variables using environment vars).\n. That one is a (known) bug, and not a pleasant one :(.\nYou can use the environment variable GUNICORN_OPTS=[--preload] to workaround it (a bugfix release is on its way).\n. You're welcome :)\n. You can't version APIs to workaround client bugs.\n. Certainly - now, on-top of that, what do you think is inherently bad in requesting clients to have a parsable UA string?\n. Over-read this\n\nThen you can explicitly\nblacklist User-Agents for known-bad clients\n\nSure. So, let's require clients to have a parsable user-agent string so that we can identify them. Oh wait... :)\n. UA strings usually embed machine dependent information (OS or platform name / version, for example), making it hard / moot to aim for exact string match, so, you will match \"for a substring\" anyhow. The difference between that, and actually parsing that substring is in the constraints we put (or not) on the client to format its UA id in a certain way, with the possible benefit of having slightly cleaner/more expressive code on our side.\nAgreed the difference is kind of subtle...\nAbout the fact that older client versions are not \"maintained\" anymore, it's not on us to decide that people should stop using them, that distros should stop shipping them, or that it's on someone else to maintain them and make them evolve - we are to provide a service that support every software that did implement the protocol at a point and that still has a significant (<- define) user base (well, at least that's what I think :-)).\n. > This is the part that doesn't make sense to me.  It's easier to have the compatibility fixes be patches to the buggy code that's not folling the API spec.  If it's a client-side bug, it should be a client-side fix.  I certainly don't expect you to cut and deploy a new registry version just because I wrote a buggy client.  If users want to continue to use unmaintained clients, or to avoid upgrading to a new patch-release for their client, I don't see why that's our problem.\nImagine there is a library released (for example: python 2.6.X) - imagine that library is the one shipped by a famous and widely-used distro (for example: centos).\nNow, py26 has a lot of bugs, things that don't work like they should (and like they do in later versions, like py27).\n- I can try to have upstream python fix their 26 stuff (not going to happen - since even if still security supported, that version no longer receive dev love, plus, see below) - python devs are going to argue: just use py27\n- I can try to have centos upgrade their release to move to a more recent python version (not going to happen - it would break many other things, and this distro version is frozen anyhow) - centos devs are going to argue: just use a more recent version of our distro\n- I can instruct my users to manually install another python version (not going to work well, as users are not always that smart, and in the first place rely on their distro to do that kind of thing, and as probably users choose that distro in the first place for support and stability) - users are going to argue: your software is not compatible with my distro, why should it be on me to do the effort?\n- I can instruct my users to change distro (well...) \n- I can instruct my users to shove it up their... (...) \nSo, in the best possible world, yes, maybe \"It's easier to have the compatibility fixes be patches to the buggy code that's not folling the API spec.  If it's a client-side bug, it should be a client-side fix.\".\nIn our world, it's about taking action to fix problems - and usually, no-one does (there is always someone else to \"blame\" for it). It's my opinion that good software makes efforts to put users first, and to try and solve their problems, especially in that \"grey zone\" where no one else wants to take responsibility for that...\nSure, this has to be reasonable, and that kind of effort should only go when there is a significant number of users affected, and no simpler solution to fix the issue is available...\nSorry for the long rambling - hope that clarifies :)\n. > It's certainly not worth forcing everyone to set a User-Agent header just to support abandoned clients.\nWell, I dare to disagree: shipped and widely used clients, even abandoned, are worth supporting in the interest of the user - and if developers can't be bothered to follow a very lightweight requirement, how could they be bothered to fix actual bugs in their protocol implementation?\n. It's not so much about developers. It's about taking ownership and responsibility in order to have things work for the users.\nEven your \"good\" developers will refuse to be held accountable for what distros ship, or for what users choose to base on or run in the end.\nEither way - this here is a detail compared to the rest of the work going on... \n. cc @stevvooe \n. > Much of the problems described above are avoided if we only allow absolute paths\nThen let's do that.\n. LGTM\n. So, you have a problem with pip, it seems.\nIt's possibly this: https://github.com/pypa/pip/issues/1805\nCan you copy the content of /home/i841712/.pip/pip.log ?\n. @MitchK sorry I couldn't help more :/\nIt really feels like a pip (or maybe boto) install bug - and when your package manager system dies on you...\nA couple of things to try:\n- attach the complete output log\n- try to pip install boto directly and see how it goes\n- seek help with the pip community\nFeel free to ping me on irc #docker-dev if you feel I can help further.\n. Mmmm... not sure what docker does exactly with this - is it honoring system-wide settings (eg: http_proxy / no_proxy)?\n. > This is really odd, because it just pushes 511136ea3c5a. Am I tagging/pushing wrong?\nI don't know - it really depends on what's in your dockerfile, and a number of other factors.\nAbout the rest, what happens if you push latest?\n(eg: sudo docker tag username/imagename registry.mycompany.corp/username/imagename:latest)\n. Smells vaguely familiar...\nSo, your docker_registry logs don't show anything related to that image.\nWhat you probably want to check:\n- can you pull from the same docker where you pushed?\n- if yes, then what's different on the other machine? proxy?\n- what's in the docker daemon logs on both machines?\n- what's in the registry logs in both cases?\n- what if you curl the registry for that image from both machines?\n. Happy you got it working!\nI wish we could get rid of nginx... so many reports of people struggling to set it up and hitting strange bugs... (cc @wking for good measure)\n. @qiyubing did you try upgrading nginx?\n. What storage are you using?\n. Are you trying to concurrently push the same layer using multiple clients?\n. What version of the registry? What version of docker? (also see https://github.com/docker/docker-registry/blob/master/DEBUGGING.md#basics)\n. Is this reproducible?\n. Can you copy the output of the commands listed in that document? (\"basics\" and \"your private registry\")\n. Hi @rchaudha \nAny news on this? Can you copy the requested info?\nThanks.\n. LGTM\n. @shin- are you ok with this going into production next week? or do you prefer I wait after the release to merge it?\n. cc @stevvooe \n. +1\n. Can you:\n- try pushing and pulling without nginx\n- copy the information listed here: (https://github.com/docker/docker-registry/blob/master/DEBUGGING.md)\n- copy your docker daemon logs (in debug mode -D)\n- copy your registry logs in debug (DEBUG=true)\n. Ok... my 2 cents:\n- something went wrong during the upload, and it failed (silently?) (this is why it seems from the registry logs that the tags are being put \"before\" the image)\n- the image is now in a broken state (you now get a 400)\n- the reason it broke is possibly nginx configuration (maybe chunk size)\nTo start over:\n- manually delete the image from the storage\n- see if you can reproduce\n- if you can reproduce, please copy over your nginx configuration and version\n. Hi, any update on this?\n. It's a known issue, linked to the use of the search backend.\nTLDR: start with GUNICORN_OPTS=[--preload] to initialize the db (see https://github.com/docker/docker-registry/#sqlalchemy)\nOr disable search entirely if you don't need it.\nTLDR2: a fix is merged and will be shipped with next version.\n. Have you looked into the mirroring options? (https://github.com/docker/docker-registry#mirroring-options and corresponding docker flag)\n. It acts as a proxy (downloading all images would be unrealistic due to the size of it).\nThe corresponding option in the engine is --registry-mirror.\n. I saw your other issue on docker/docker.\nYou need to upgrade to docker 1.3.2.\n. LGTM\n. Ok.\n. cc @jfrazelle \n. @stevvooe needs rebase\n. LGTM\n. Fixed. Thanks!\n. LGTM\n@stevvooe circle is slap happy now.\n. There still is something slimy lurking into the Circle setup though.\n. LGTM\n. LGTM\n. LGTM\n. \n. ;)\n. LGTM\n. @keith- also, providing curl output hitting https://myregistry:XXXX/v1/ would help diagnosing this.\n. @keith- can you start your docker daemon in debug mode (-D) and copy the complete log output when attempting to reach 4443?\nAlso, can you copy your apache configuration?\n. Happy it helped!\nBest.\n. cc @stevvooe proper proxy functioning and extensive testing on that front is definitely an important goal for v2\n. @stevvooe agreed, in this specific case.\nMy point was more generic - we live in an (unfortunate) world where enterprise proxies are the rule, and v1 didn't exactly shine on that front.\nWhat I expect of v2 is:\n- better error reporting (client) (maybe smart enough to tell that's a proxy answering without talking to the registry itself?)\n- docker should use system wide proxy settings, and other standard ways to specify proxies (not too sure what it's doing right now)\n- integration tests should include proxies scenarios\n. What kind of limitation are we going to enforce on tags?\n. About the ampersand: http://stackoverflow.com/questions/6651275/what-do-the-mean-in-this-database-yml-file\ngcs should use it as well.\n. LGTM\n. Thanks a lot for this.\nMerged.\n. You can use GUNICORN_USER and GUNICORN_GROUP to start the registry with a different user. Note that if you are using the official docker container, you will probably have to fix some permissions.\n. Thanks!\n. @MitchK currently the way to go is to use nginx in a reverse proxy function and use an auth subrequest to implement your authz/authn model (http://nginx.org/en/docs/http/ngx_http_auth_request_module.html)\nIf you don't want to go that complex, indeed you could start with nginx with simple basic auth, and limit PUT operations to a subset of users.\nFWIW, the registry is being rewritten now (what we call v2), and will make this easier (\"next-generation\" branch and label on the issue tracker).\nHope that helps...\n. @tobegit3hub \nEverything related to the design of registry-ng is labelled \"next-generation\": \nhttps://github.com/docker/docker-registry/issues?q=is%3Aissue+label%3ANext-generation+is%3Aopen\nSpecifically, some discussion around authn/authz issues:\nhttps://github.com/docker/docker-registry/issues/623\nAnd more in the docker engine, with the official proposals:\nurl layout and API: https://github.com/docker/docker/issues/9015\nauth: https://github.com/docker/docker/pull/9081\nTLDR?\n- the new url layout will namespace layers, making it easier to fine-grain authorization to specific repositories, without external knowledge about which layer is used in what image\n- two different auth designs will be supported: JWT token based (as described in the proposal), and through (nginx-)reverse-proxy implementing basic auth\nThese two elements combined will make it possible to get better control, more easily on what they authorize for.\nThe hub itself will use the JWT based auth model.\n. You can ignore circleci, but the travis build report a number of errors that should be fixed, for example:\nhttps://travis-ci.org/docker/docker-registry/jobs/45954757#L1205\nalternatively, you should run the tests suite locally - python setup.py tests or tox (or TOX_INCLUDE='' TOX_LIBRARY='' tox)\n. @shin- @wking what do you think?\n. LGTM\n. Yes, let's go - at least we have multi go testing, which is a real plus. I'll fix (?:g|c)overalls down the road.\n. Here is a description of what and how to obtain a bunch of useful information:\nhttps://github.com/docker/docker-registry/blob/master/DEBUGGING.md#basics\n(the \"basics\" and \"Your private registry\" sections)\nThese infos will help a lot figuring out what's happening.\nFrom a quick glance at your stacktrace, it looks like your communication with your S3 bucket is not working well.\n. @pwaller about this not being supported by next-generation, yes by all means go ahead and open a new issue (cc @BrianBland @AndreyKostov )\nAbout your v1 issue, can you try without AWS_ENCRYPT and AWS_SECURE?\n. @pwaller any news on trying without AWS_ENCRYPT and AWS_SECURE?\n. I believe this was fixed by #961\n. LGTM\n. LGTM\n. @titotp your problem is with the new registry - not the python one.\nFurthermore, there is simply not enough information to diagnose this.\nI answered you about that on the other ticket you opened.\n. Looks very much like one of the db issues mentioned earlier.\nTry starting your registry with GUNICORN_OPTS=[--preload] (see https://github.com/docker/docker-registry#sqlalchemy)\n. You are on OSX, using system python, right?\nDoes it work when you use brew python?\nDid you pip install the registry using sudo? Do you then start the registry with sudo? What is the path of the database file? \n. I would recommend that you:\n- rm the database file\n- start from scratch, launching your registry with DEBUG=true and GUNICORN_OPTS as indicated\n- copy the output of the first launch\nAlternatively, you should probably use homebrew python instead of the OSX system python.\n. Python 2.7.7 is an old version. You should see python 2.7.9 with brew.\n. Any update?\n. > [install vs. develop]\nCorrect.\nAbout the rest: if you are using python brew, then you don't need to sudo. You should be able to do everything with your regular user account.\nNow, looking at your logs, you are still using the system python it seems.\nYou probably need to adjust your path. \n. One last check now - are you running the latest master?\n. Happy we got this working!\nKeep me posted on your progress.\n. So, it's rather a docker bug, not a registry bug, correct?\n. Indeed I overread and thought the layer was saved by docker, sorry about that.\n@shin- are there any check done on layer download with mirroring?\n. Please at least provide some basic information about how you started your registry, your configuration, and the result of curl https://docker.yunat.com/v1/_ping from the machine you run your docker daemon on.\n. Sorry, but we can't help you without the requested information, so, I'm closing this for now.\n. Thanks. PTAL.\n. Done\n. Either the image is stored on your private registry (eg: its name is myprivateregistry/foo/bar), and it will be fetched from there.\nEither the image is stored on the central registry (eg: foo/bar) - in which case it will be pulled from there.\nCan you elaborate a bit on what you are trying to achieve?\n. The closest you can get to that would be to use mirrors.\n- configure your private registry as a mirror\n- docker pull ubuntu --registry-mirror=https://myregistry\n. Delete doesn't delete the layers themselves. You can search the \"delete\" label for more information and other reports about this.\n. cc @shin- @stevvooe \n. Thanks @shin- and @stevvooe!\nFixed the url version.\nThe rest is as-is - @shin- if you think this is acceptable, you may merge it at will :).\n. \\o/ Thanks a lot Joffrey \\o/\n. Can you copy the output? What commands are failing?\n. Sure. How do you usually handle things? What happens if you try to run apt-get update on a bare ubuntu system from where you are?\n. Mmmm... pip...\nA couple of additional things - this here:\nhttps://github.com/docker/docker-registry/issues/821\nAnd this from pip:\nhttps://github.com/pypa/pip/issues/1805\nThat error about werkzeug is weird.\nAnything helpful in the pip.log itself?\n. @wenlock - your efforts on this are just awesome - thanks a lot for going into this and fixing it.\nI don't understand why werkzeg install would fail here (doesn't make sense), but that's OT.\nFor inclusion of this, the bottom line to me is: I would like to reduce as much as possible the added complexity into the dockerfile and would love to see a helper outside instead.\nDo you think we can go with something like:\nDockerfile:\n```\nENV http_proxy\nENV https_proxy\nENV HTTP_PROXY\nENV HTTPS_PROXY\nENV ftp_proxy\nENV socks_proxy\nENV no_proxy\nENV PIP_OPTIONS\n[...]\npip install $PIP_OPTIONS bla \npip install $PIP_OPTIONS foo\n```\nThen we could just ship into contrib a helper script that would do sthing like below, without cramming it into the dockerfile:\nif [ ! -z \"$PROXY\" ]; then\n  export PIP_OPTIONS=\"--allow-external Werkzeug --proxy $PROXY\"\n  export HTTP_PROXY=$PROXY\n  [...]\nfi\nand we would instruct people to just ./contrib/setup_proxy.sh foo_proxy then build as usual.\nThe question that remains is: do we need the apt-conf bits? or can apt-get live happily with the env vars?\nWhat do you think?\n. Let's ping @tianon to find out :-)\n. Hi. Can you PR the change so that others can use your fix?\nThat would be awesome.\n. This is different.\nI'll look into it.\n. @ankushagarwal Does this happen consistently?\n. Thanks.\nWeird that it would crash there then. I'll have another look.\n. The code just looks racy from the get go...\n. @pavelz can you elaborate on what doesn't work?\nThis specific code path is not in 0.8.1 - so, you are likely having a different issue here. \n. People: if you have that issue, please don't add more comments unless the information does add value.\nAdding -e GUNICORN_OPTS=[\"--preload\"] to your docker run command will workaround the issue as a short-term remediation.\nThanks.\n. @bunop this has nothing to do with local storage.\nThe lock file always exist no matter the storage, as long as you enable the search index.\n. @ashish235 the security model and ACL model are different.\nv2 is optimized for security and pull performance, so, push might be slower depending on the scenario (although there is work ongoing to optimize that). \n. @andrask \n1. there is work going on to enhance that, and also ways to optimize it depending on your use case - if you can elaborate a bit on your specific usage, there may be low hanging fruits for you\n2. do you have any specific issue number for this one? would like to look into it\n3. there are plenty of existing UI for v2, including search (built on top of the notification API). Portus comes to mind for example, or DTR. Furthermore, the catalog API has been merged, unlocking advanced tooling/search to be built.  \n@dalanlan \nRegistry 2.1 release candidate have just been cut.\nYou will find the release notes here:\nhttps://github.com/docker/distribution/releases/tag/v2.1.0-rc.0\nand the full list of closed issues and merged PRs (about 150 PRs merged for this release) is here: https://github.com/docker/distribution/issues?utf8=%E2%9C%93&q=milestone%3ARegistry%2F2.1+is%3Aclosed+\nSince development for 2.2 will start soon, your feedback is welcome on what still needs to be addressed.\n. What do you call \"redis docker registry image\"?\nWhat command did you run exactly?\n. So, your problem is with the redis image, which is unrelated to the docker-registry.\nEither way:\n- you should report your issue to the image author - likely here: https://github.com/docker-library/official-images\n- I just ran docker run --name some-redis -d redis myself - it works - so please provide informations about:\n  - what exactly you are doing (exact command)\n  - what version of docker you are using\nHope that helps.\n. Hi, thanks for the feedback on this.\nIndeed, next-gen is under heavy development right now. \nI modified the readme slightly to make things clearer.\nLet me know if you think this helps.\nBest.\n. SqlAlchemy configuration let you specify where your database live: https://github.com/docker/docker-registry#sqlalchemy\nBy mounting that database file from a volume, it will survive container destruction.\nDoes that answer your question?\n. If you want that, then don't use sqlite - but instead postgres / mysql / other.\nMore about that in the SQLAlchemy documentation.\n. Use-cases may vary. Allowing to search \"freely\" does disclose possibly restricted information.\ncc @wking @shin- \n. None of these services are letting you search and list private content.\nI'm happy if people want to do that, and we can certainly provide them with documentation on how to do it, or example configuration files, but I don't think it's the right decision to ship something that by default leaks private information.\n. Hi! thanks for the report.\nAs we don't maintain the CentOS package ourselves, I'm not sure how we can help here.\nAlso, the output log is quite terse...\nI would be glad to fix whatever is causing this to fail, however I would need a pointer.\nIf on the other hand the rpm itself is faulty, probably you can report this here? https://bugs.centos.org/\n. Hi @omribahumi \nI believe this is being fixed inside docker: https://github.com/docker/docker/pull/10050\n. Closing - for the record, what you see there indicates debug is true indeed.\n. 504 indicates a gateaway timeout.\nCan you copy both your nginx and registry logs when this happens?\nThanks.\n. If there is no stacktrace in your registry log, there should be no crash.\nI can think of a couple of issues:\n- timeout too short on nginx config\n- registry getting stuck writing to the backend\nCan you:\n- configure DEBUG=true for the registry and restart it\n- copy the output of curl https://myregistry/_ping\n- copy your registry configuration\n- copy your nginx configuration\nThat would certainly help pinpointing your issue.\nThanks!\n. I don't see either chunked_transfer_encoding (depends on your nginx version):\nSee:\n- https://github.com/docker/docker-registry/blob/master/contrib/nginx/nginx_1-3-9.conf\n- or https://github.com/docker/docker-registry/blob/master/contrib/nginx/nginx.conf\n. @smiller171 Please report this to https://github.com/docker/distribution\n. Possibly the index limits repo name length to 30 characters?\ncc @kencochrane @shin- : what do you think?\n. I guess the question is then: should this be made consistent across engine/services? And/or is there any reason for the index to enforce this restriction? (eg: db limits?)\n. Your nginx server does not have permission to access your .htpasswd file.\nI assume nginx is running as www-data, so make sure that unix account has at least read access to your .htpasswd file.\n. What user is used by nginx? (ps aux | grep nginx should give you a hint)\nThen su  THATUSER and try to cat /var/www/example.domain/.htpasswd to validate your perm are ok.\n. Can you run:\n- ls /var/www/example.domain/.htpasswd \nThanks.\n. Are all directories along that path executable for the nginx user?\neg: /var /var/www /var/www/example.domain\nEither way, this really is a nginx configuration problem, not a registry issue.\nhttp://stackoverflow.com/questions/25774999/nginx-stat-failed-13-permission-denied\n. The bugtracker here is rather meant for feature requests and bug report, not support.\nI'm happy to help as time permits, but you should rather direct support questions to irc / forums.\n. Thanks. I'll look into it.\n. Hello,\nCan you copy the headers of your _ping request?\n(eg: curl -i myregistry/v1/_ping)\nAlso, what version of docker are you running? (docker version and docker info) \n. @shin- ping\n. Thanks!\n. How did you install the registry?\nDid you use the pip package?\nAre you running from source?\n. gevent is explicitly listed as a dependency: https://github.com/docker/docker-registry/blob/master/requirements/main.txt and 0.9.0 could not have possibly ran without it.\nDid you change something on your python after installing the registry?\nCan you pip install docker-registry again and copy the output?\nThanks.\n. Your installation is failing because swig is missing.\napt-get install swig (or flavor this to your own distro) will likely solve your issue.\nIndeed, the ADVANCED.md file has not been updated.\nAlthough, the travis.yml always contain exact steps for manual installation on a bare system: https://github.com/docker/docker-registry/blob/master/.travis.yml\nand so does the Dockerfile: https://github.com/docker/docker-registry/blob/master/Dockerfile\nSorry about the inconvenience on this - tell me if the suggestion does fix your problem.\n. Dead.. Thanks.\n. This is an example config, in which one can use any env value they wish.\nNow, being consistent with the default config file is certainly something desirable - but then, the \"correct\" env var is AWS_KEY (see https://github.com/docker/docker-registry/blob/master/config/config_sample.yml#L86)\n. Hi there.\nProbably you forgot to paste your message? \n. Hi @teodorescuserban this here is the open-source registry.\nTo report bugs on the hub (specially automated builds), you should report to https://support.docker.com instead.\n. Hello @ghaering \nCan you give some details as to why this is required? Is it required by boto?\n. Ok, merged. Thanks!\n. So, the lock file issue needs to change path.\nYou can try to get past it by disabling search.\n. Can you provide your docker-registry logs (of a failed push)?\nThanks\n. I don't get it.\nUnder what condition may that file not exist?\nhttps://github.com/harrisonfeng/docker-registry/blob/master/docker_registry/toolkit.py#L325\n. Ok. The better fix here would be to use lockfile (http://pythonhosted.org/lockfile/). Would you be willing to try this?\nThanks.\n. @docwhat @pires the current implementation is a parody of a proper file locking mechanism, and it fails miserably as this report demonstrates...\nSo yeah, let's use something that does work :)\n. GUNICORN_OPTS=[--preload] should help you out (or disable the search).\nIt's a duplicate - will be fixed soon.\n. Yes to all questions :)\n. Indeed, an additional \"scaling\" paragraph may definitely be added to https://github.com/docker/docker-registry/blob/master/ADVANCED.md\n. LGTM\n. Nice! LGTM\n. Both should work: https://github.com/docker/docker-registry/blob/388d30366f279470d50bec7895238a71973d72c1/docker_registry/app.py#L27\n. LGTM\nThanks for this.\n. TLDR: yes :)\nMore infos in https://github.com/docker/distribution with the registry go-rewrite, and/or support on irc #docker-distribution\n. Can you copy the docker daemon logs? preferably using debug (eg: start the docker daemon using docker -d -D) \n. @shin- ping\n. Can you copy the exact request you are doing, including all parameters and headers?\n. Please provide the exact arguments you used.\nLikely your repository name is not valid.\n. Repository names are only allowed to contain: [a-z0-9-_.]\nAnd repository is not the same thing as registry.\n. Are you sure you were previously able to use authentication over plain http?\ncc @shin- on this but I'm pretty sure auth+plain-http never was supported.\n. cc @tibor @dmcgowan \n. ping @tiborvass \nIs there documentation about that and/or info about whether the syntax changed?\n. Trust the certificate at the os level.\n. People, this here \"docker-registry\" is deprecated - if you are using registry:2 (and you should) please ask for support on forums or irc (#docker-distribution).\nIf you have actual bugs, please report them to https://github.com/docker/distribution\nAlso, there is comprehensive documentation including getting started in two lines over here: http://docs.docker.com/registry/deploying/\nClosing this since this ticket is getting really messy.\n. I would really recommend you use the container: docker run registry\nAbout the exception here, your environment vars are probably not taken into account.\nPut them in a script with your docker-registry launch command, and call that script instead is a solution.\n. which docker-registry should give you the location.\nThe output does NOT look like the official command - is this a wrapper script from your distro?\n. And because of the way you launch it (with sudo) your environment is NOT honored.\n. So, can you copy the output of which docker-registry?\nThan copy the output of cat $(which docker-registry) \n. Either way, about your issue, like suggested, make sure your environment is copied (eg: put your export in a script along with your docker-registry invocation).\n. Thanks!\n. Unfortunately, a failed push usually leaves the image in a \"broken\" state that the registry can not handle.\nThe only (not so) good recommendation is to manually delete the image from the storage and push it again...\n. @sgandon this error is non specific.\nPlease at least provide docker version, docker info, and both docker registry and docker daemon (debug) logs.\nAlso, if this is the same issue, have you tried removing the \"broken\" image like suggested in my previous comment?\nThanks.\n. I would really encourage everybody to start looking into registry:2 (available as an official image: docker pull registry:2 documentation is here: https://docs.docker.com/registry/ and github repository is here: https://github.com/docker/distribution\nThe python registry here is not being worked on actively anymore.\n. @Jamlee it has been released officially last month, and is being used in production by a number of people, including Docker Hub.\n. Thanks @shin- !\nLGTM\n. This is correct: you will receive mail only if there is a call for logger.error().\nThe document ADVANCED should be rephrased to better explain that.\n. @xiekeyang the registry v1 (here) is in maintenance, and most of the efforts are now focused on the golang rewrite under https://github.com/docker/distribution\nIf you want to enhance error logging and submit a PR for that, that's fair, and I will review it, but you should not expect things to be worked on beside that.\n. Ok, there is a lot here.\nLet's start with basic stuff:\n- docker run -p 5000:5000 registry should give you a running registry WITHOUT any authentication required\n- if you get anything else (like 401 or 403) then you are not talking to your registry, but to another http service\n- auth is NOT required to have a registry work\n- SSL is NOT required either (though no SSL means --insecure-registry)\nCan you confirm that you got that step ok?\nAfter that, adding SSL in the mix should go:\n- nginx configuration from here: https://github.com/docker/docker-registry/tree/master/contrib/nginx should get you started (disable the authentication to start with)\n- if your cert is self-signed, instructions here should get your docker host working by trusting your certificate: https://docs.docker.com/reference/commandline/cli/#insecure-registries\nOnce you get that second step ok, you can enable authentication back.\nLet me know where you are now, and/or reach out on irc #docker-distribution so we can figure this out.\n. @danielschwartz \n- your registry is working ok without authentication nor ssl - correct?\n- can you make it work ok with just SSL and NO authentication first? (so, no docker login)\n. @danielschwartz \n- why would you try to login if there is no auth for your registry?\n- save for docker login, can you docker push and pull successfully? \n- what version of docker are you using?\n- are you using the insecure registry argument for the docker daemon (not the cli)?\n. @danielschwartz please (with SSL enabled) curl -iv https://yourregistry:port/v1/_ping and:\n- check if curl is happy\n- copy the resulting output \n. Nope...\nIn such cases, have you checked the storage? is the image still physically there?\n. It's in your storage_path configuration var. Probably under /registry/\n. Hi, this is a hub issue.\nThis here is the open-source registry bug tracker, that does not provide this feature.\nCan you report this directly to docker support instead?\nhttps://support.docker.com\nThanks.\n. Can you copy:\n- your registry version\n- your registry configuration\n- your registry launch command\n. Can you curl -iv https://mylocalregistry:5000/v1/search?q=base?\n. My point is: I don't think the search is enabled on your registry - that curl command would make sure about that.\n. If the search endpoint is not enabled, make sure what configuration you are actually using and/or that the search backend settings is actually set.\n. @akaspin the OP closed it himself.\nThe recommended way to run the registry is from a container.\nThis type of situation is exactly why running directly on your system is not recommended.\nNevertheless:\n- this probably affect only when ran as a mirror\n- if you really want to run this on your system, try other (more recent) versions of requests\n- I'm open to merging a PR that would upgrade requests, but it will probably require some extra work to make sure it does not cause regressions. There is an open issue here to do that: #611\n. 0.7 is a (quite) old version. You should consider using at least 0.8, or 0.9.\nAlso, can you investigate and point out what exactly is cluttering disk space? (if you are indeed using S3 there absolutely no reason for this, since there is no filesystem caching).\nThanks.\n. The boto library uses parallel keys to multiplex requests to the s3 backend, and is apparently responsible for this...\nI guess we should disable ParallelKeys entirely (see also #956)\n. Still, you should really update your registry.\n. If you disable authentication for the search endpoint, does it work?\nI'm pretty sure this is an engine bug with regard to auth. with search.\n. \"Docker bug\". https://github.com/docker/docker/pull/10050\n. @calavera np - closing either way :)\n. Ok, that sounds bad enough.\nGiven the effort is now on https://github.com/docker/distribution (new golang implementation), little effort is going to be put in the python-registry here.\nMy suggestion is either remove the parallel key code entirely, or better, make it configurable (and disable it by default).\nIf you are willing to PR this, that would be awesome...\n. Fantastic.\nGive me a ping as soon as you have a PR, I'll make sure it's reviewed quickly.\n. Thanks a lot for your work on this!\n. Thanks.\n. No problem, it happens :-)\n. The registry v1 golang implementation is entirely unsupported, and no effort goes into it.\nFor the sake of it, I'm merging this PR, but I would rather suggest you start looking into the next-generation registry over there: https://github.com/docker/distribution\n. Mirroring support in the registry so far is not well done, nor supported.\nThere are a number of cases where it breaks namespace consistency, nor to mention there is poor error handling, and almost no speed benefit from it.\nYou are free to try to make it work, but it's unlikely that your use case will be supported / fixed.\nThe new golang implementation of the registry (here: https://github.com/docker/distribution) supports an upgraded protocol and is where the effort is focused now.\nIf you want a short-term solution, sure, you can try to make the python-registry mirroring fit.\nIf you are looking for the long run, then it's probably better to participate in the effort there, and explain your exact use case so that a proper solution is devised.\nHope that helps.\n. One simple nit, LGTM otherwise.\n. In order to make flake/hacking stop complaining here, you can:\n- gevent.monkey.patch_all()\n+ gevent.monkey.patch_all()  # noqa\nLGTM otherwise - merging.\n. Fixed #956 and #954\n. Can you clarify what you are trying to do exactly? (eg: command you are using, expected output, actual output).\nFor now, I don't understand how/if this is related to the registry.\n. Can you copy the logs from your registry when you get the 500 error?\n. Yes, it should...\n. This is a hub feature (this here is just the open-source registry).\nPlease report your request to https://support.docker.com \nThanks.\n. Hello Matt,\nWe don't have any specific relations with distros as far as packaging this is concerned.\nFurthermore, we strongly recommend users to use the docker image and not the distros packages that we don't support.\nNow, if you want to have a custom image, I would recommend you start your own with:\nFROM registry as a starting point (if I were to do it I'd probably go with supervisor to launch both apache and the registry).\nFinally, most of the effort these days is focused on https://github.com/docker/distribution the new golang version of the registry - v1 is in maintenance now.\n. LGTM\n. Hi,\nThanks for this.\nIf you are willing to make a PR for this, I would definitely merge it.\nNow, the python registry is in maintenance mode now, and the future is in https://github.com/docker/distribution\nHope that helps.\n. Guys, I need your docker version and docker info.\nAdditionally, running docker in debug mode and getting the docker logs usually helps. Also, the registry logs if possible.\n. Thanks a lot for the infos.\nEveryone:\n- sorry for the bad news there (at least, I don't try to pretend otherwise...), but I'm not going to spend time into docker <= 1.4 - a lot has changed since then, and these versions only receive security and critical bug fixes support\n- if you encounter this with docker client >= 1.4, can you please copy me (or gist it):\n  - docker version\n  - docker info\n  - registry launch command\n  - registry version\n  - registry logs\n  - docker daemon logs (when run in debug mode)\nThat will definitely help. (EOF on pull can be the result of a number of very different things).\nThanks!\n. What version of the registry is this?\n. This here is the docker registry.\nSupport for the hub is here: https://support.docker.com\n. Do you use nginx or apache in front of your registry?\n. Can you send the output of:\n- docker info\n- docker version\n- docker daemon logs (ran in debug mode)\n- registry launch command used\n- registry logs\n. Did you set STANDALONE=false ? Additionally to the requested info, can you copy your registry config if you are using a custom config?\n. cc @shin- mirroring stuff, can use your savvy-ness :-)\n. Happy you figured it out.\nIt can be confusing :)\n. Thanks for this!\n. Thanks @nob13 \nI'm adding this request to the new golang registry home.\nThis is probably not going to happen for the current python registry (here).\n. Did you add --insecure-registry site.com:443 to your daemon as pointed out?\n. dial tcp 1.2.3.4:443: i/o timeout\nCan you:\n- curl https://1.2.3.4:443/v1/_ping\n- dig site.com\n- check that your registry is started and does listen on 1.2.3.4?\n. So, you are starting the registry at step 1 inside a container, then install and start it system-wide as well?\nWhy do that? \n. Put otherwise.\nYou just need to:\nsudo docker run -d -p 5000:5000 -v /opt/data/registry:/tmp/registry registry\nThat's enough.\n. cc @shin- \n. There is an extension mechanism in v1: https://github.com/docker/docker-registry/tree/master/docker_registry/extensions on top of which I think it's fairly easy to implement webhooks.\nThere is also a notification mechanism inside the v2 registry: https://github.com/docker/distribution/tree/master/notifications\nThought admittedly the documentation is pretty sparse right now :)\n. @michaelneale yeah, the Hub and privately operated registries are really two different beasts, with widely different targets and use cases.\nThe Hub is really meant as a zero maintenance, pure SAAS system, while operating a private registry gives you both more control, and more \"responsibility\" in implementing your own pipeline.\nWhether the APIs should be unified or not is open for debate, but if you ask me, the kind flexibility you find in registry:2 (https://github.com/docker/distribution) can simply not make its way into the hub, if only for security reasons.\nHope that helps.\n. @michaelneale I believe the open-source registry is meant to provide lower-level \"primitives\", powerful base \"bricks\" that advanced users can leverage by integrating with other micro-services that in turn deliver higher-level functionalities.\nFor example, the registry should be just CRUD, and the notification API kept as-is (generic json events) - then developers can come up with a standalone notification-endpoint that receive these, and transform/spit them back into either a queue (celeri, rabbit, whatever) or as \"simple\" webhooks.\nPut otherwise: the registry is meant to be extended and built upon.\nIf you are looking for a universal hub&registry compatible pipeline into your \"app\", then you should add such an intermediate notification endpoint micro service between your registry and your app that will turn registry notifications into simple webhook similar to hub.\nIn the hope that helps.\n. @kmadel DTR supports whatever we ship with the open-source registry.\nAppart from them, if you want a specific feature in it, you should ask for it on the support.\n. Here: https://github.com/docker/docker-registry/tree/master/contrib/nginx\nYou will find some contributed nginx configuration to achieve that.\n. Hi,\nWhat do you mean by \"the time looks weird\"?\n. Delete will remove the tag, but not the actual payloads.\nSo, your image will no longer be available, it will still exist on your backend storage.\n. Some people have contributed garbage-collector scripts to do the cleanup. You will find them clicking the \"delete\" label over here. \n. @squarism so, you are using the golang registry2.\nPlease head over to https://github.com/docker/distribution and/or irc #docker-distribution\nTLDR: delete is not supported yet, in design phase: https://github.com/docker/distribution/issues/210\n. ;)\n. This repo here is deprecated, and any message here is likely to go unnoticed.\nEither report problems on the docker/distribution repo, or on irc (#docker-distribution)\n. Hey @Trefex \nCan you copy your docker registry logs?\n. Anything in nginx logs?\n. No errors of any kind, neither in registry or nginx?\nIf the client connection is closed, that should show somewhere...\n. Hey @Trefex \nFrom our irc chitchat, you mentioned that your proxy was responsible for the messup?\nSo, is this still an issue or can we close this?\n. Image squashing is definitely an interesting idea, though a complicated one.\nI guess you can do some squashing on the client already, using docker export then docker import. Your image is now a one layer image. Sure, you loose the benefits of shared layers, but then.\n. Yes, it was fixed. Thanks for caring @ayumi !\n. Please provide:\n- docker info\n- docker version\n- command line you used to install and launch your docker-registry\n- your docker-registry configuration \n. Hi @montgomery1944 \nIMO this is a docker engine issue (not registry).\nNevertheless, I would be happy to investigate.\nI think that should work with a registry2. If it's not, that should be fixed.\n. @montgomery1944 have you tried registry:2 ?\ncc @stevvooe \n. There has been an issue with authentication on the hub.\nhttps://status.docker.com/pages/incident/533c6539221ae15e3f000031/554293d934de855a3800061b\nIt's fixed now.\nLet me know if you still experience issues.\n. Hub problems must be reported to https://support.docker.com\nThis here is a closed issue.\n. Your search index is somehow corrupted. I would recommend deleting the index.\n. Hi @ronin13 @ventres @benhamner \nAutomated builds is not the open-source python registry - it's a Hub feature.\nI'm sorry to close this, but really there is nothing I can do here - this is definitely a Hub issue, that you should report directly to the support over here: https://support.docker.com \n. From your first problem: your search index is corrupted. Disable search if you don't need it, otherwise delete the db and start over.\nSecond problem/comment: you don't have write access to your mounted folder apparently (/reg/data/repositories/library)\n. > For the second problem I disabled selinux.\nMakes sense. You are on RedHat, or Fedora?\n. Can you copy the actual exception and log around it?\n. Can you provide docker daemon logs (preferably in debug mode: -D)?\nThanks.\n. There are a number of user contributed solutions, but no clean implementation of this.\nYou will find discussion and ideas here: https://github.com/docker/docker-registry/labels/delete\n. @arnos can you copy the output of:\n- curl -i https://localhost:5000/v2\n- curl -i https://172.27.25.59:5000/v2\nCan you also copy the output of your docker daemon logs (preferably ran in debug mode: -D)\nThanks.\n. @eaoliver are you referring to the image registry:2? or registry:latest?\n. @eaoliver registry:latest is still pointing to the python registry, which is this one repository here.\nregistry:2 is now the recommended way to go, and lives in https://github.com/docker/distribution\n@eaoliver can you clarify what's wrong in your case?\n@arnos problem is that his registry is probably not listening on the public interface (dial tcp 172.27.25.59:5000: connection refused)\nYours might very well be different, but I have no way to know without logs.\n. @eaoliver where does you docker daemon live?\nIs it on the same host from where you are running your curl command?\nOr is it inside a VM? (eg: boot2docker)\nAlso, can you provide a copy of your docker daemon logs, preferably ran in debug mode? (-D)\nThanks.\n. @eaoliver let me know if I can help you debug this on irc (#docker-distribution)\nFWIW I recommend you move to Docker Machine instead of b2d.\n. @funkytaco moving to irc because github issues are not a proper forum for fast paced discussions about mis-configuration (slow answers, and lots of back and forth).\nFurthermore, this here was not an issue, but rather a misconfiguration IIRC.\nFinally, I strongly recommend to look into https://github.com/docker/distribution instead (registry:2) since the python registry here is now deprecated.\n. @funkytaco https://github.com/docker/distribution/blob/master/docs/insecure.md#using-self-signed-certificates\nEvery docker daemon that wants to pull or push to this registry will have to be configured that way.\n. @funkytaco so create the folder\n. ping @shin- \n. Insecure registry + authentication is not a supported scenario.\nThe reason for that is it's possible / simple for an attacker to downgrade your traffic to http, then making the use of plain basic passwords moot.\nMy recommendation is that instead you put your server.crt into /etc/docker/certs.d/ec2-52-17-207-222.eu-west-1.compute.amazonaws.com:443/ca.crt\nHope that helps.\n. @mpas it can - this is why you need to trust that self-signed certificate inside docker (because it's self-signed).\nCopying the crt over to /etc/docker/certs.d/ec2-52-17-207-222.eu-west-1.compute.amazonaws.com:443/ca.crt does that. (don't forget to restart docker after doing that)\n. you have to create the mentioned directory.\nAnd you have to do that on any host that wants to run docker accessing that registry, not on the registry host itself.\nIt's like running a website with a self-signed certificate, you have to configure your browser to trust it.\n. @mpas it does.\nYou might have to trust the cert at the OS level as well, though (since authentication here).\n. LGTM\n. Can you check?\nI believe that was fixed on the hub side.\n. I assume you are using the new registry.\nThis repository here is about the deprecated, python registry, as advertised in the readme and project description.\nHave you followed the documentation?\nSpecifically:\n- https://github.com/docker/distribution/blob/master/docs/deploying.md\n- https://github.com/docker/distribution/blob/master/docs/authentication.md\nAlso, the information you provide is insufficient to help you.\nYou can find out more here: https://github.com/docker/distribution/blob/master/CONTRIBUTING.md#if-you-have-not-found-an-existing-issue-that-describes-your-problem\nI would suggest to review these first, then if you still have a problem, ask for help on irc (#docker-distribution) or the support forums, and if there is still a problem, open an issue on https://github.com/docker/distribution\n. @garystafford the VM is a different host.\nYou cannot expect \"localhost\" from machine A to be \"localhost\" for machine B.\nSo, if you start your registry on machine A, with ip X.Y.Z.W, use X.Y.Z.W instead of localhost to pull images from.\n@evanbeard same thing, likely. Expecting localhost to resolve to a remote machine is unrealistic.\nHope that helps. \n. dial tcp 10.146.3.120:443: connection refused. tells you your registry is not runninng at that ip.\n. @olibob \n1. you don't have the same issue as the O.P. Usually and when in doubt, better to open a new issue.\n2. this github repo here is about the old registry, marked as deprecated. You will usually get answers on https://github.com/docker/distribution\n3. for basic auth to be enabled, you are missing an env variable: -e \"REGISTRY_AUTH=htpasswd\" (see https://github.com/docker/distribution/blob/master/docs/deploying.md#native-basic-auth )\n4.TLS error on login, if you still experience it (docker 1.6?), see at the bottom here: https://github.com/docker/distribution/blob/master/docs/nginx.md#docker-still-complains-about-the-certificate\nLet me know if that helps.\n. @mahesh3267 closing this for bookkeeping. Let me know if you still have issues.\n. Welcome @olibob \nHappy you got it working.\n. @xuedihualu please read the comments.\nThis here is not the place to report registry:2 issues.\nPlease also start by following the documentation step by step: https://github.com/docker/distribution/blob/master/docs/deploying.md\nIf you still have issues doing so, open a new ticket on https://github.com/docker/distribution\n. https://github.com/docker/distribution/blob/master/docs/deploying.md#2-instruct-docker-to-trust-your-registry-as-insecure\nUse exactly that syntax. This is a daemon flag, so you need to restart your daemon. Finally, if you use an ip, specify an ip, if you use a domain name, same thing, that needs to be consistent.\nHope that helps.\n. And this github repository is clearly marked as deprecated.\nRegistry 2 repository is linked on the front page.\n. You are on the legacy registry repository.\nThe new registry that you are trying to use lives under https://github.com/docker/distribution \nNevertheless, can you provide at least the following information?\nhttps://github.com/docker/distribution/blob/master/CONTRIBUTING.md#if-you-have-not-found-an-existing-issue-that-describes-your-problem\nFrom your log it seems that you have dangling antislashes (%5C). Please copy the exact command you ran. \n. @dmcgowan any idea?\n. @xialingsc see resolution over at https://github.com/docker/distribution/issues/794\n. Hi, \nHub issues should be reported to https://support.docker.com/\nThis repository here is about the (deprecated) open-source registry, and is unrelated to your issue.\n. Hi,\nI strongly recommend you move away from registry:1 - or at least first try to make things work for registry:2 only.\nTry to follow the instructions in the official documentation, specifically: https://github.com/docker/distribution/blob/master/docs/authentication.md\nand see if you can setup basic auth successfully.\nThen when you are done, add ldap.\n. Hi @deviscalio \nThe documentation page was moved.\nHave a look here: https://github.com/docker/distribution/blob/master/docs/nginx.md\nAnd if you don't want to use v1, please remove the v1 sections from your configuration (or start from the exemple in the documentation).\n. @deviscalio I suggest you start from scratch, using the documentation I linked.\nOnce you get it working, you move ahead and add your LDAP snippets.\n. Did you docker login myregistrydomain?\nCan you provide your registry+docker daemon logs and configuration?\nDid you try following the documentation step by step?\nThanks.\n. Documentation is faulty.\nYou  need to use:\n```\nregistry:\n  environment:\n    REGISTRY_STORAGE: \"s3\"\n    REGISTRY_STORAGE_S3_ACCESSKEY: \"XXXX\"\n    REGISTRY_STORAGE_S3_SECRETKEY: \"XXXX\"\n    REGISTRY_STORAGE_S3_BUCKET: \"my-docker-registry\"\n    REGISTRY_STORAGE_PATH: \"/registry\"\n    REGISTRY_STORAGE_S3_REGION: \"eu-west-1\"\n    REGISTRY_SEARCH_BACKEND: \"sqlalchemy\"\n    REGISTRY_LOG_LEVEL: \"debug\"\n    REGISTRY_HTTP_TLS_CERTIFICATE: /certs/docker.crt\n    REGISTRY_HTTP_TLS_KEY: /certs/docker.key\n    REGISTRY_AUTH: \"htpasswd\"\n    REGISTRY_AUTH_HTPASSWD_REALM: \"Registry Realm\"\n    REGISTRY_AUTH_HTPASSWD_PATH: /auth/htpasswd\n  volumes:\n    - ssl:/certs\n    - auth:/auth\n  image: registry:2\n  ports:\n    - \"5000:5000\"\n```\nNote the extra REGISTRY_AUTH: \"htpasswd\" before the other keys.\nWill fix the doc.\n. Let me know if that fixes it.\n. --engine-insecure-registry does not install a registry.\nYou may start here to install one: https://github.com/docker/distribution/blob/master/docs/index.md\n. registry:2 issue tracker is here: https://github.com/docker/distribution \nAlso, there is no search in the v2 protocol yet.\n. All the documentation you need is here:\nhttps://github.com/docker/distribution/tree/master/docs\nSpecifically:\nhttps://github.com/docker/distribution/tree/master/docs/deploying.md\nYou don't need nginx - if you insist on using it, there is a recipe for it specifically: https://github.com/docker/distribution/blob/master/docs/nginx.md\nAlso, you are in the wrong place - docker-registry is the deprecated, python implementation.\nFrom your logs, it seems you try to connect to port 60053 - which doesn't seem to match neither your nginx nor your backend registry.\n. Disclosing the existence of content that is unauthorized is considered bad security.\nTry it with Github: create a private github repo, and try to access it while not logged-in. You will get a 404, not a 403.\n. Can you report this to https://github.com/docker/docker ?\nBad file descriptor points to an engine bug, or some other local filesystem corruption.\n. This here is the python registry, which is deprecated.\nCan I ask that you head over to https://github.com/docker/distribution?\nThanks.\n. For Docker Hub issues, you should contact https://support.docker.com\nThis here is the open-source registry (and it's the deprecated version - the current version lives in docker/distribution).\n. This here (the open-source python docker registry) will not stop \"working\".\nThough, it is not actively maintained.\nThe docker engine will still interact with it for the time being, though v1 protocol support may be removed from an upcoming docker engine version (no date decided yet).\nIn all cases, I really encourage you to use the golang registry instead (docker/distribution).\n. At least pip install it if you don't want to use the official container. Or python setup.py install\nEither way, I strongly encourage you to look into docker/distribution instead.\nHope that helps.\n. Please disregard https://hub.docker.com/_/registry/ and also this repository here, which is flagged \"deprecated\" (https://github.com/docker/docker-registry)\nFollowing the documentation https://docs.docker.com/registry/overview/ should get you started.\n. @Perfect-Web what's your configuration? what did you type to start your registry? what's in your logs? did you read the docs?\nMore importantly: what version of the registry are you running?\n. Ok. The bug tracker is there: https://github.com/docker/distribution/issues\nThis here is the deprecated registry v1 repository, as advertised in the description.\nAbout the permission issue:\n- can you bash into the container and copy the output of ls /auth?\n- what version of docker are you running?\n- do you use SELinux?\n- do things work without authentication?\nFinally, you will probably receive faster back and forth / help on irc (#docker-distribution on freenode).\n. I really suggest upgrading to a more recent docker version (1.10 was released recently), as it's quite likely your bug has been fixed by one of these.\nAlso saying what registry version you are running, your registry logs and config, along with your docker daemon logs (preferably in debug) will be necessary to debug this.\n. Can you copy your docker daemon logs?\n. @jmlrt none of this is showing any pull attempt.\nSuggestion:\n- stop the docker daemon\n- enable debug mode on the daemon (add -D to the default launch arguments)\n- start docker\n- tail -f your daemon logs\n- try to pull, and reproduce the problem\n- copy the resulting logs\n. Hi, DTR is a commercial product, with dedicated support.\nI encourage you to reach out to them to get help ASAP.\nYou can certainly start with support@docker.com\nFurthemore, non-default ports bug in DTR is known, though it's really a DTR-only issue (and not upstream open-source), hence there is unfortunately nothing we can do here.\nI'm going to close this for book-keeping, but please mention this ticket in your mail to support.\nHope that helps.\n. @mikehaertl also worth mentioning: for Hub specific questions and updates, I recommend you reach out to the Hub Support Team: support@docker.com \n. Garbage collection is available on both versions (DTR and open-source v2).\nBtw, you are in the legacy repo.\n. @jeusdi can you copy paste the command you ran to create your htpasswd file?\n. @Annihil8ted to be able to provide insight in this, we need your configuration (redacted), the exact command line you have been using, and both the registry logs and docker daemon logs (from where you are trying to pull).. Shouldn't we clear the session in all (fail) cases? (eg: also if check_token is false). Minor anyhow...\n. I guess we want to remove references to secret_key elsewhere as well:\n- in the various config files\n- in the gunicorn config file: https://github.com/dotcloud/docker-registry/blob/master/config/gunicorn_config.py#L10\n- in the README\n  https://github.com/dotcloud/docker-registry/blob/master/README.md#general-options\n  https://github.com/dotcloud/docker-registry/blob/master/README.md#what-about-a-production-environment\n. You missed a bit in the README (down that paragraph: https://github.com/dotcloud/docker-registry/blob/master/README.md#what-about-a-production-environment)\nI don't know if I'm really a talented software engineer, but I sure can be a killer janitor :-)\n. That part is hard to review for me - missing some background.\nAnyhow, I guess it's safe to assume that you don't break legacy behavior by not setting X-Docker-Checksum anymore?\n. The changes in init here are meant for the namespace to be used - thus, this have been moved to the run file below.\n. Registry itself won't depend on swift (as only driver-swift plugin will).\nAnd redis is now provided by registry-core.\n. This is the way to express dependency on the storage API that we expect to break only on a major version change. \n. We should probably depend only on the later version of drivers - or maybe drivers should follow the core versioning\n. Namespacing\n. Because we moved stuff from init to run\n. Just wondering (as I'm not familiar with dc env):\n- other envs (travis and Dockerfile) don't seem to require libffi.\n- also, they do require libevent-dev (or libevent1-dev).\n. done\n. Right, let's remove it.\n. err, thanks for catching it :p \n. I would rather have:\n- remove the import tarfile from line 6\n- add:\n  -  from .lib import xtarfile\n  -  tarfile = xtarfile.tarfile\nIt makes it more clear what happens and where things come from. (and you can remove your flake F401 workaround).\nThe same goes for the other files below.\n. Mmm... pass instead of this?\n. Same as above?\n. You need to remove the import tarfile from line 6 then ;)\n. Hear hear (about raw-bytes and unicode).\nMy point is keyword = keyword does absolutely nothing :-) - and if decode fails, you will get the raw bytes no matter what.\nSuggestion:\ntry:\n  keyword = keyword.decode(\"utf8\")\nexcept:\n  pass\n. I would remove that line now :)\n. I don't think you want to use assert(expression, \"Message\") as a function\nbut rather assert expression, \"Message\"\nThis is true everywhere in your test file. Actually, travis is complaining about this (and so should tox/nose)\n. I don't get it. mock_boto is already there (two lines below)\n. You likely don't need it - unless I missed something.\nYou can access (in your test) the driver by accessing: self._storage\n. Not familiar with that piece of code, but why would \"/\" + filename[5:] necessarily start with /.wh.?\n. Not sure we want the extra dependency, just to be able to replace assert X == Y with X.should.equal(Y)\n. So, like I said in the other ticket, let's have the redirect snippet commented out in the file, with some explanatory comment on it.\n. Not familiar with this - what exactly is this timeout for? connect? first-byte?\n. Why remove the --debug switch? (here and elsewhere)\n. Ping @shin- - any objection to that? anything we do with the logs in production?\n. Removing useless stuff indeed seems like an acceptable reason (note that in v18, --debug or its absence had strange side-effects on logging IIRC) - but since we are moving up, I'm happy :-)\n. Can you rather iterate over these and build the config dynamically instead?\neg: \nfor i in cfg.cors.keys():\n    app.config['CORS_%s' % i.upper()] = cfg.cors[i]\n. What happens if we have only origins: _env:CORS_ORIGINS?\n. Can you add there both NEW_RELIC_LICENSE_KEY and NEW_RELIC_CONFIG_FILE (with default '')?\nIt makes it easier to know what env vars we are using overall.\n. Agreed.\n. Yes. Will do.\n. If search_backend is None, the endpoint is not imported (https://github.com/docker/docker-registry/blob/next/docker_registry/wsgi.py#L24)\n. Indeed.\n. So, is there a need to namespace this then?\n. Let's remove the namespace bits here then.\n. Elaborate (now that config is no longer \"segregated\" on what the consumer needs to do - eg: \"myconfig.extensions.mysuperextension\").\nMake it explicit we want people who are going to use the configuration that they must use a subnodes of ROOT.extensions.\n. Loose contract is fine for now. We'll see after 0.9 if it's worth it making things more strict.\nCan we have a list method in there that would return all available extensions?\n. Is your extension living under docker_registry/extensions? (eg: if we opt-out of namespaces, we don't want that)\n. Make that file empty.\n. No, I'm thinking more about debugging. eg: expose the list of installed extensions via some debug hook (see the \"next\" PR which have some of that) \n. Or very bluntly:\n```\ndef boot():\n    for ep in pkg_resources.iter_entry_points('docker_registry.extensions'):\n        ep.load()\ndef list():\n   return [ep for ep in pkg_resources.iter_entry_points('docker_registry.extensions')]\n```\n. LGTM\n. Please replace these examples by a string that doesn't resemble valid keys. \n. Same here\n. I would use a separate README and LICENSE for the extension.\n. That should disappear entirely. Authorization is not up to the registry to decide, nor the driver. \n. Layer, manifest paths etc: should be entirely opaque to the driver.\n. We are going to use move in the registry, a lot. Tarsums / checksums need to be computed, and the file needs to be complete. So, the layers will be moved to their final destination once they are complete. Whether the driver implements a smart \"move\", or is just smart when copying, should be hidden under the specific method call.\nI'm +1 on keeping move.\n. Maybe we don't hear the same thing with \"content-adressibility\". This is about being able to retrieve a given binary payload from an id that can be computed again from that binary payload. This is what tarsum is for, and I simply fail to see a point in asking driver author to copy the same code over and over again in every implementation.\n. er -> err\n. Keep that change here, and add a link to ADVANCED.md from there.\n. That's the part I would like in advanced\n. @stevvooe We don't need/want recursive directory listing (disclaimer: haven't read all comments, and assume recursive means -r)\nAnd simpler is better :)\n. Oh yeah, can you please add @noxiouz elliptics driver to the list? (and remove my own gcs implementation, which is kind of crap)\n. Instead of inheriting common, can you inherit \"s3\"? that will save you the S3-specific config in there\n. I don't think this is necessary. Just move up the COPY . /docker-registry from below up here and that should fly :).\n. But then you are adding one extra layer (which does have a cost) - also kinds of muddy things about what's happening. Do you think this is rebuilt often enough to be worth it?\n. That directory changes very rarely, while the rest changes very often.\nNow, the dockerfile is not used at all during development (the workflow is simply not practical): python setup.py develop + gunicorn running in debug mode does pick-up changes without the need to rebuild anything (not to mention the extra VM slowness if working on OSX). Even tests are run much faster using plain virtualenvs, also giving much more flexibility (can test multiple different versions of python).\nColor me heretic :)\n. I assume 1.4 release will match our timeline. What do you think?\n. Let's bet on 1.4. You sell it to the powers. If they are not happy, you can still roll your own with 1.3 (I guess you will ship a custom Dockerfile anyhow). After-all, this is only a drone-test image.\n. All these (try import except) seem unrelated to this PR, right?\n. Please pin it to 0.21 - people looking for loose deps can use the magical environment var DEPS=loose\n. I think this script is bitrot anyhow - the proper way to do this is to import from core compat: https://github.com/docker/docker-registry/blob/9124375c560a974dacff6cb63634a74808327493/tests/test_mirrors.py#L12\n... or to simply ignore this (old) script.\n. Same as above.\n. Same as above.\n. You don't need to install (or actually use) redis server - just the python client package.\nSo, yes, this bit has to go :)\n. Let's go with 0.22.3 indeed.\n. Any reason to deviate from the circleci syntax? (eg: maybe it would make maintenance easier down the road if they were \"closer\"?)\n. Good enough! Thanks for clarifying Brian. LGTM\n. This is breaking OSX.\n. This is a hack to hide the fact the current SQL_ALCHEMY code is racy, so, I would rather avoid making this a first class configurable thing.\n. @shin-\n``\n            _________\n         /'        /|\n        /         / |_\n       /         /  //|\n      /_________/  ////|\n     |   _ _    | 8o////|\n     | /'// )_  |   8///|\n     |/ // // ) |   8o///|\n     / // // //,|  /  8//|\n    / // // /// | /   8//|\n   / // // ///__|/    8//|\n  /.(_)// /// |       8///|\n (_)'()//| |       8////|_____\n() /\\ ()'| |        8///////////////\n() \\\"/ ()'||         8/////////////\n ()..() d' Hb         8oooooooopb'\n   (_)'  d'  Hb\n         d'   bb\n        d'     H b\n       d'b b\n      d'b\n     d'             `b\n```\n. Fair enough.\n. Again a vile hack to hide the misery... let's have this in there until further cleanup.\n. I would not take the chance to remove gevent patching here - though it seems ludicrous, I'm pretty sure there was a reason for it being duplicated here.\n. ",
    "samalba": "https://github.com/dotcloud/docker/blob/v0.3.4/docs/sources/api/registry_api.rst\n. Available on staging\n. Sounds legit :-)\n. Thanks @tobstarr \n. The current web UI is closed source. It also deal with user accounts in order to provide a token to be authenticated on the Registry. It would be actually super trivial to have a tiny user/accounts token mechanism (with a small web ui) to plug with a self-hosted registry. Let me know if it's interesting for you, I could provide a example in the Registry code.\n. Sounds good, I am adding this to my todo list and will keep you posted (I think I could add a prototype by the end of the week).\n. Not yet unfortunately, I've been busy lately with integrating with OpenStack. This one should not take too long and still on my todo list though, you're not the only one who asked. Bumping the prio and will try to release something soon.\n. @unclejack, the Registry now supports a \"standalone\" mode (see the config_sample.yml). This modes implements a fake Index internally so Docker can use the standalone Registry without having to reach the central Index.\nI am currently running extensive tests with Docker to make sure everything works as expected.\n. The Index API has been merged into master.\n. 302 is usually used for redirection (passed along with a Location header).\n304 is usually returned when passing a If-Modified-Since header.\n208 is not part of the HTTP protocol, it has been added by WebDAV.\nIt is actually an error (since you try to create a resource which already exists). The problem for you is to have a less generic error code than 400 which does not describe what's wrong.\nI think it would make sense to return a \"409 Conflict\" since the resource pointed exists. If you're fine with the code, I can implement it.\n. Now available on staging.\n. Nice! :+1: \n. Implemented and on prod.\n. @schmatz, please re-open if you have trouble using it. It's indeed the default mode.\n. The Registry allows all characters. Only the Index validates the repository name. A test has been added to the Registry to ensure this.\n. Hi Dave,\nthis error usually means that the checksum provided by Docker at the beginning of the push is different after finishing to push the layer. Normal behavior is usually can be a connection interruption. This check is to prevent from having a layer corrupted.\nDo you have this error several times on the same layer?\n. Can you provide the result of a docker version?\n. This will be fixed in the next Docker release. Would you mind testing the exact same thing with Docker built from the master branch? It'll help to test some more and confirm that it will fix your issue.\nThanks!\n. This problem has been fixed a while ago, closing\n. Sounds like a quick workaround. I think slashes will be confusing. But I am not against it, let me know when you completed your tests.\n. Sorry, I missed it in yesterday's commit... Looks like it's time to package it to freeze versions. I'd recommend using the version 0.5.2 until I stabilize the new checksums mechanism:\nhttps://github.com/dotcloud/docker-registry/tree/0.5.2\n. Hi Jared,\nwhat version of Docker are you running? Also, did you setup anything special like setting a \"INDEX_SERVER_URL\" in your environment?\n. I tried to reproduce with 0.4.8 but it works perfectly fine on my side. I guess it might be related to your account, I'll investigate the logs on the Auth server. Could you try to reproduce with the freshly released 0.5?\n. No reason, just a matter of huge list of important things to do and a lack of time!\nI love when such comments come with a pull request :+1: \n. It's a good start. it would be nice to implement checksums\n. Following on #29\n. I think this feature should be implemented in Docker and not in the Registry. What do you think?\n. I'd imagine a docker command to generate it. If it gets easy to generate on the fly, no need to store it.\n. The same applies for the app source code. I think the Dockerfile should be\npart of the source code.\nOn Mon, Jul 29, 2013 at 10:50 AM, Jared Forsyth notifications@github.comwrote:\n\n@samalba https://github.com/samalba well the thing is, sometimes I\ndon't want to downlaod 700M just to look at the dockerfile...\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/33#issuecomment-21737910\n.\n\n\n@sam_alba\n. Good idea, now we have a Dockerfile, it should not be hard.\n. Registry's prod runs Python 2.7.2 without any issue. Could you provide more details about the problems encountered with Python 2.7?\nAbout support for Python 3, there are not any plans yet but I am open to maintain it if a pull request brings its support in the future.\n. Ok, I get your point now. Tarsum is supported on Docker master and uses gzip (instead of xz). The IOError is caught on purpose for backward compatibility[1]. When this occurs, the registry only computes the \"simple checksum\" and won't check the tarsum.\nIt's weird that it breaks your stuff though, what output do you get when you push?\n[1] https://github.com/dotcloud/docker-registry/blob/94be4e6a1fe5306594fcbc88610cd72406810a44/registry/images.py#L61\n. Interesting. Can you provide me a way to reproduce? As long as the version of docker you're using. Once the bug is confirmed, I can fix it quickly.\n. Also, do you have the same error on the public Registry? The reason I am surprised is that the code which is failing for you locally is the exact same one that we run on production. Did you make any changes to your local copy?\n. 1. I don't see any mention about \"search\" in the README.md. Maybe I did not get your point. If you find a mistake in a README, feel free to contribute a patch, it already improved a lot thanks to community contribution.\n2. docker search makes requests on index.docker.io. The search and web ui (as you can see on index.docker.io) are not being part of docker-registry. It might change later. Right now the Registry is minimal on purpose.\n3. This one might be a bug, could you include the server's logs in this issue?\n. Thanks for the Readme typo, fixed\n. Yes, just run:\ndocker rmi 192.168.1.1:49576/share_name\nOn Fri, Aug 2, 2013 at 5:28 PM, Daniel YC Lin notifications@github.comwrote:\n\nBy the way, I don't know how to remove the old tag, is there any method? I\nmean (192.168.1.1:49576/share_name) in the following command.\ndocker tag my_image 192.168.1.1:49576/share_name\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/40#issuecomment-22044993\n.\n\n\n@sam_alba\n. :+1: \n. The app does not find the config.yml file. Make sure you followed the steps in the Readme:\nEspecially the one about creating the config file:\ncp config_sample.yml config.yml\nLet me know if it works. I can investigate further.\n. Agreed. Updated the issue title.\n. It's true that it's not mapped in Docker command set right now for the private Registry. The calls exists in the Rest API but it's not mapped on the CLI.\nHowever you can also remove the corresponding repos name in the \"repositories\" directory (or S3 bucket), it'll have the same effect.\nI keep this ticket open for adding this feature in Docker.\n. I think this conflict message is because you have different repos names\npointing to the same image id. You can verify that by doing\ndocker images\nHowever I don't know if there is a way to bypass it, it'd be worth looking\ninto Docker's issue to find if it's an active problem or open a new one.\nOn Fri, Aug 9, 2013 at 12:45 PM, juicefoods notifications@github.comwrote:\n\nthanks. That's what I am doing right for the time being. Just want to make\nsure.\nAlso it seems I still cannot delete such images in CLI: docker rmi\nlocalhost:5000/machinelearning\n2013/08/09 15:45:21 DELETE /v1.4/images/localhost:5000/machinelearning\nError: Conflict, localhost:5000/machinelearning wasn't deleted\nI am using:\nClient version: 0.5.2\nServer version: 0.5.2\nGo version: go1.1\nThanks for your help.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/45#issuecomment-22419016\n.\n\n\n@sam_alba\n. Could you provide the output of a \"docker version\"?\n. Ok, we'll work on reproducing it and update the case.\ncc @shin- \n. Ok, thanks for the details, we'll update the case asap.\n. I am trying to find a pattern: does it work with gunicorn only and S3 storage?\n. No, it should work without nginx.\nOn Thu, Aug 22, 2013 at 6:08 PM, James Carr notifications@github.comwrote:\n\nMAybe this is the piece I'm missing? I didn't run nginx because I assumed\nthat the docker container either had it baked in or some other things were\ngoing on. If that's not the case then I am more than happy to try putting\nnginx in front of it.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/47#issuecomment-23137558\n.\n\n\n@sam_alba\n. Hmm, do you guys have several workers on gunicorn (-w option)? If you can\nreproduce the problem, could you retry after adding a \"secret_key\" option\nin your config? Set a random string.\nOn Fri, Aug 23, 2013 at 9:50 AM, James Carr notifications@github.comwrote:\n\nYep.. proxied behind nginx, same problem. :(\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/47#issuecomment-23176131\n.\n\n\n@sam_alba\n. This has been fixed in Docker.\n. Could you share your config file (hide any password of course)? Are you using S3 or local storage? It looks like the a file previously created cannot be opened.\n. Ok so by default, the data gets stored into /tmp/registry. Could you check\nthat this directory exists locally and makes sure the user used by the\nRegistry process has rights to access it?\nOn Mon, Aug 12, 2013 at 3:39 PM, juicefoods notifications@github.comwrote:\n\nI am using the default \"dev\" mode with standard config.yml which means I\ndidn't make any changes to this file. And I have it stored on an amazon\ninstance that is connected to my local machine using openvpn.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/49#issuecomment-22531399\n.\n\n\n@sam_alba\n. Ok. Actually this message is not an error, it's a debug message saying that\nthe tarsum could not be computed. Which makes sense since your version of\nDocker does not support this checksum method. It succeeds to compute the\nsimple checksum anyway. Does your push go through or is it breaking after\nthat?\nOn Mon, Aug 12, 2013 at 3:52 PM, juicefoods notifications@github.comwrote:\n\nSam, Thanks for your immediate response.\nI checked that the directory /tmp/registry exists and permission of the\ndirectory is drwxr-xr-x 4 4.0K Aug 12 22:15 registry\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/49#issuecomment-22532012\n.\n\n\n@sam_alba\n. Ok, I'll close this issue then.\nYes I got the other issue. I don't have the answer for the \"double size\" problem. It might be a problem on Docker. I need some time to reproduce and investigate. If it confirms that it's a problem on Docker, I'll open an issue on github.com/dotcloud/docker\n. Implemented by https://github.com/dotcloud/docker-registry/commit/bd92eb99d20b20e1304e4d5d0bf299cfa7e91f52\n. You're completely right about the Registry. You can specify the manually generated one by adding \"secret_key: YOUR_KEY\" in your config file.\nThis is how we scale it on prod.\n. I'm open for both (config_sample.yml more clear and doc update). Pull requests are more than welcome!\n. Thanks for the help!\n. It's not a bug. The run commands actually runs the process and attach to it.\nAfter your container is running, you can actually re-attach:\n% docker attach 2458b528bdda\n. I think I found the issue. samalba/docker-registry uses 2 workers. And secret_key is not set in the config, so the 2 workers generate their own keys to sign the cookie (which is the problem).\nI'll release a new image today.\n. Could you retry? I re-pushed a new version of samalba/docker-registry (pull first).\n. It's a config problem. Could you make sure the \"secret_key\" is set? I\nupdated the config_sample.yml to make this more clear. Let me know if that\nworks.\nOn Tuesday, August 27, 2013, Matt Wallington wrote:\n\nI am still having this problem while using 8 workers with the latest pull\nfrom github and 0.6.1\n2013/08/27 22:18:23 HTTP code 400 while uploading metadata: {\n\"error\": \"Checksum not found in Cookie\"\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/55#issuecomment-23392131\n.\n\n\n@sam_alba\n. Do you run the registry from the source? I guess so. Could share your config file?\n. LGTM.\nDid you test the behavior on the central index as well (index.docker.io)?\n. Thanks for the investigation!\n. I am merging this now, I'll release a new docker-registry today. Also I need the script to generate a secret_key (needed for several workers).\nThanks for the contribution!\n. yes\nOn Fri, Aug 23, 2013 at 9:39 AM, Jon-Erik Schneiderhan \nnotifications@github.com wrote:\n\nIf #62 https://github.com/dotcloud/docker-registry/issues/62 LGTY can\nwe get that included in the release as well?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/pull/60#issuecomment-23175498\n.\n\n\n@sam_alba\n. Hey Jake,\nthanks for the report. Did you notice any weird behavior on the server app? Or is it the docker daemon which crases? I guess the problem comes from Docker itself, it would be better to re-open this on https://github.com/dotcloud/docker.\nSorry for the confusion, since a lot of people changed some related code recently on Docker, it might be interesting for them to follow it as well (hence better to centralize on Docker repos).\nObviously ignore my comment if the docker-registry app crashes.\n. Ok. Please CC me on the other one as well.\nClosing this one (you can just copy/paste I think).\n. LGTM\n. How would you handle the limit? The list of namespaces can be huge.\n. This requires an Index local to the Registry. It would be really bad to do this on the filesystem (same for S3)\n. If we implement list as described, it will have to work with all storage backends (local fs, swift, S3, etc...). I don't think a directory list will do the trick.\nI am thinking we should add an optional Indexer to the Registry:\n- If the indexer does not run, the registry works the same as right now.\n- The Indexer will be able to use different db to store the index (*sql, redis, etc...)\n- The Indexer will have to listen to signals from the registry app to update the Index in realtime (create image, delete, etc...)\nIf this is done correctly, there is no need for list, everything will go through search.\nAre we on the same page?\n. Cool. Now we have to find the time to get this done :-)\n. I noticed some issues between cloudflare and S3 latency lately. I'll\ncontact them to understand what's going on.\nOn Wednesday, August 28, 2013, James FitzGibbon wrote:\n\nI am also seeing this, after five retries on two different networks, so\nthe problem may be at source (or with multiple paths into the source):\nvagrant@docker:~$ docker pull samalba/docker-registry\nPulling repository samalba/docker-registry\n0b5de618610a: Error pulling image (latest) from samalba/docker-registry,\nendpoint: https://cdn-registry-1.docker.io/v1/, exit status 2:\ngzip: stdin: unexpected end of file\ntar: Unexpected EOF in archivee\ntar: Unexpected EOF in archivee\ntar: Error is not recoverable: exiting now\n0b5de618610a: Error pulling image (latest) from samalba/docker-registry,\nexit status 2:\ngzip: stdin: unexpected end of fileendend layers\ntar: Unexpected EOF in archive\ntar: Unexpected EOF in archive\ntar: Error is not recoverable: exiting now\n2013/08/28 18:29:10 Internal server error: 404 trying to fetch remote\nhistory for samalba/docker-registry\nvagrant@docker:~$\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/66#issuecomment-23445250\n.\n\n\n@sam_alba\n. The official image is now stackbrew/registry. Also delivering issues have been addressed by setting up CloudFlare CDN in front of the public registry.\n. I agree with the problem, however I don't think this change addresses the problem. What happens on an app with high load (doing s3 requests all the time), if you start the app and it gets traffic right away, the problem will still occur.\nWhat about handling the connect failure / reconnect in a generic way in the class? So if there is a connect problem, the code would try to reconnect gracefully.\n. Is there still a need for that (also, we moved to boto 2.19.0). Closing for now, feel free to re-open the discussion.\n. Also, make sure you pass all the tests. Click on the travis-ci link, it'll show you the errors.\nOther than that, this change is useful and it should be definitely merged.\n. Closing for now. Feel free to re-open when you're ready to address those comments.\n. It's something that we also discussed recently with @kencochrane and @shin- \nI cannot promise any release date for the moment, but it's definitely something that we want to work on in short term. I completely agree with your point.\n. Closing this for now (we'll re-open if needed later).\n. (let's keep the branch for now)\n. LGTM\n. cc @shin- \n. Not that I know of. The limit I am worried about it the number of fd on the client. Anyway, those connections are really short. I'll reduce it to 20 for production use.\n. @chasballew, this option defaults to False: https://github.com/boto/boto/blob/develop/boto/s3/key.py#L1311 . So it's better to make it equal to True or False (not None).\n. LGTM.\n@shin-, what do you think?\n. @chasballew can you rebase? I'll merge this.\n. apparently not, I'll merge manually\n. No worries, my bad. Merged PRs in the wrong order :-)\n. After some investigation with @mzdaniel , the test fails because of a non-related code. However it probably hides a bigger problem (this KeyError in the log). Let's use this PR to address this and wait for the test to pass.\n. Mainly performance. Also because the current production registry server is hosted on AWS ec2, it uses AWS internal network to reach the S3 service.\nIt's true though that some people are using docker-registry outside ec2 where SSL would make more sense. Do you want to contribute a PR to make this configurable?\n. LGTM\n. actually setup-configs.sh is deprecated since it's possible to read environment variable from the config. I should update the README.\n. Sorry for the delay. Could you rebase and I'll merge this right away.\nThanks\n. What's the point of including newrelic in app.py, can we just keep using the daemon to keep any newrelic-related code out?\n. Closing for now. Looks abandoned.\n. Since moto does not allow testing stream functions, let's close this for now. @mzdaniel will submit a version not depending on moto.\n. Thanks for the contribution, the code looks good!\nCould you fix the small pep8 warning (the Travis test is failing). Also another minor comment, it would make sense to rename lib/storage/ell.py to lib/storage/elliptics.py?\nIt's great to see such a major contribution coming from the community!\n. LGTM!\ncc @shin-, confirmed?\n. duplicate of #7.\n. I think @noteed is right. Honestly I was not aware of this and I double checked the JSON rfc. A JSON text (like the request's body) is indeed an array or object.\nThis should be changed later but I don't think it's a huge priority. Also changing this on the Registry will break docker on the client side. We'll have to implement to backward compatibility.\n. Otherwise, you can also create a volumes with a directory on your host (outside the container), so images won't be lost, even by re-using a new container everytime.\n. Can someone contribute a change to the README? Using a volume to persist data on the docker-registry is an important topic.\n. Thanks @theopolis \n. :+1:  Great idea!\nThe keystone token can be passed in config properties. It's also perfectly fine to add the Python Swift client as a dependency. The local storage module is probably easier to understand, there are just a couple of methods to implement in a storage module.\n. It's strange to see remote requests on \"https://region-a.geo-1.objects.hpcloudsvc.com\", are those calls hardcoded somewhere?\n. If only the layer is missing, it means that the error comes from the stream_read implementation (this is where the layer is fetched). Looking at your branch, why do you read the swift object returned through a \"buffered_string\"? I don't know the Swift API but, can the object be read in binary as a stream?\n. I did not tested it yet but after looking at the code, it looks like an exact duplicate of the file storage/s3.py except that you use \"boto.gs\" instead of \"boto.s3\".\nIt's nice though to rely on the same library for 2 different backends. What about replacing s3 and gs by a \"storage/boto.py\" module which would expose 2 storage classes inherited from the same base?\n. Looks much better! There is still small pep8 error that you should fix, then I think we can move forward. We'll have to test this pretty intensively before releasing.\n. Well, I'll test it on our staging to make sure there is no regression.\n@shin- what do you think?\n. Could you fix the test failures in another PR, then rebase this one so it would pass the tests?\n. It looks like you missed to add a package requirement (see travis log).\nOtherwise it looks good\n. @shin- please update the staging to run the functional tests as well\n. Hi Andrey,\nIt looks like the workers time out. I' advise you to increase the timeout\nsetting on gunicorn (check the default config). Also if you proxy\ndocker-registry behind an nginx or hipache, you should increase the tcp\ntimeout as well\nOn Sunday, November 10, 2013, Andrey Romanov wrote:\n\nWanted to use my beaglebone black as docker-registry, but having an issue.\nHere is what I see in debug logs on server:\n2013-11-10 10:10:16,687 DEBUG: check_session: Session is empty\n2013-11-10 10:10:16,772 DEBUG: check_session: Session is empty\n2013-11-10 10:10:16,777 DEBUG: api_error: Image is being uploaded, retry later\n2013-11-10 10:10:16,831 DEBUG: check_session: Session is empty\n2013-11-10 10:10:17,200 DEBUG: check_session: Session is empty\n2013-11-10 10:10:31 [1042] [CRITICAL] WORKER TIMEOUT (pid:1047)\n2013-11-10 10:10:31 [1042] [CRITICAL] WORKER TIMEOUT (pid:1047)\n2013-11-10 10:10:31 [1069] [INFO] Booting worker with pid: 1069\n2013-11-10 10:10:47 [1042] [CRITICAL] WORKER TIMEOUT (pid:1050)\n2013-11-10 10:10:48 [1042] [CRITICAL] WORKER TIMEOUT (pid:1050)\n2013-11-10 10:10:49 [1042] [CRITICAL] WORKER TIMEOUT (pid:1050)\n2013-11-10 10:10:50 [1042] [CRITICAL] WORKER TIMEOUT (pid:1050)\n2013-11-10 10:10:51 [1042] [CRITICAL] WORKER TIMEOUT (pid:1050)\n2013-11-10 10:10:52 [1042] [CRITICAL] WORKER TIMEOUT (pid:1050)\n2013-11-10 10:10:53 [1042] [CRITICAL] WORKER TIMEOUT (pid:1050)\n2013-11-10 10:10:54 [1042] [CRITICAL] WORKER TIMEOUT (pid:1050)\n2013-11-10 10:10:54 [1072] [INFO] Booting worker with pid: 1072\nHere is what I can see while sending data:\nandrey@andrey-ThinkPad-X201:~$ sudo docker push somedomain.com:5000/aromanov/some\nThe push refers to a repository somedomain.com:5000/aromanov/some\nSending image list\nPushing repository somedomain.com:5000/aromanov/some (1 tags)\nPushing 8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\n2013/11/10 02:10:53 Failed to upload layer: Put http://somedomain.com:5000/v1/images/8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c/layer: EOF\nIn config file specified registry storage type as local storage. Did the\nsame locally and it worked. Is there some fundamental difficulty with ARM\narchitecture? Is there some way I can try to supply more logs so that you\ncould look at the problem more closely? (just tell me what file should I\nget them from)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/118\n.\n\n\n@sam_alba\n. @bfosberry thanks for digging into the root cause.\n. agreed. I'll do this now\n. Could you guys confirm which version of ubuntu works and which does not?\n. done\n. Looks like a timeout issue. Could you have a look at https://github.com/dotcloud/docker-registry/issues/118 and review the config?\nLet me know if it addresses your issue.\n. Regarding the API change, I would rename\n/v1/repositories//access\nby\n/v1/repositories//properties\nOtherwise \"access\" is redundant. And it makes sense to say \"access is a property of the repos\", it's possible to edit properties on an existing repos using the endpoint.\n. Merged\n. Yes\n. Could you reopen the issue on https://github.com/dotcloud/docker. I think the error is not related to docker-registry but to docker remote API.\n. First of all, the Swift support implem looks great. I think it's important to find a way to run the tests though. I guess it would require to mock Swift?\n. Could you take care of it? It's important to implement unittests and you did it already so most of the job is done. Running them on Travis is the very last step. As the mock you mentioned, you don't have to use a full mock implementation, just mocking the calls you need is way enough.\n. Sorry, I did not have time to dig into this. Did you have any progress on your end?\n. @bacongobbler Is the branch up to date or did you add some more code? I wanted to try to reproduce the issue locally to bring some help.\n. Ok, let me know when ready to review.\nThanks!\n. I am glad that you figured out your problem. Just 1 question:\nThere is something I don't get about the file \"test/utils/mock_swift_storage.py\"\nInstead of mocking just the accesses to Swift (basically the swiftclient), you mocked the whole swift storage module. Am I right? If yes, it does not test the storage module?\nThe good point though is that the swift module itself looks good to be merged. It's just a matter of testing. I might have some time next week, do you need some help to mock this swiftclient thing?\n. You're correct.\n. Related issue: https://github.com/dotcloud/docker/issues/2461\n. cc @shin- \n. It looks like a configuration issue. How do you run the Registry on ec2? Which webserver are you using? Please take a look at: https://github.com/dotcloud/docker-registry/issues/118\n. Are you guys running your tests using a Redis cache? https://github.com/dotcloud/docker-registry#performance-on-prod\n. Could you make sure the tests pass?\n. The PR looks good to me, however it changes the initial design which states that tags are mutable (like git tags for instance).\nI'd like to have some feedback from @kencochrane @shin- @dustinlacewell \n. I think this behavior should be aligned with Docker. First because we need Docker to expose the option. Second because Docker itself should drive this notion of immutable/mutable tags.\nCould you launch the topic in a new issue on https://github.com/dotcloud/docker ?\nWe'll keep this PR for now.\n. I am closing this PR since we cannot agree on how to include such a feature. Feel free to open a separate issue to discuss the need (and spec a solution).\n. We're open to support more storage backend and this code looks good.\nLeft 1 inline comment. Also, could you provide unit tests?\n. Those are more functional tests since they rely on a remote service (a real selectel endpoint). Is it possible to mock the selectel calls (with a fake class) so you can test only the storage backend code without calling a remote service.\nIf this works, then you don't need to skip the test anymore.\n. My comment is actually the same than https://github.com/dotcloud/docker-registry/pull/128\nYou would just need to mock the \"selectel\" client lib (only the method you use) to implement real unit tests.\nYou did not answer my last comment though. Any update?\n. The code looks good but we need a clean way to support this without having it in the core. I created an issue: https://github.com/dotcloud/docker-registry/issues/317\nBut we keep the PR open for now.\n. :+1: \nApparently Docker follows the redirections, so it'll work out of the box if the Registry serves 302 for pulling the layers.\nI'd make this optional in the config though since it's related to the storage backend being used.\n. No, it's something that has been only discussed so far. But we accept pull requests :-)\n. I think it's good to be merged. To answer @shin- question, in token based auth env, the get_repository is always called after the check_token. In other words, when we reach this line of code, we already made sure the token was valid.\nLGTM.\n@shin- ?\n. Could you fix the small pep8 error? Then I'll merge\n. Sorry, could you rebased? we changed this part recently, I just want to make sure your PR is still relevant (and still work)\n. another pep8 warning and we're good to go\n. CC @proppy shin-\n. :+1: \n. Any update on this?\n. Everytime we init a logger using \"name\", it inits a new logger that needs to be re-configured.\nThe logging config only happens once. We should centralize the logger configuration in 1 method and call this method everytime we init a new logger.\n. Shame on me\n. Looks good to me, but could you move \"boto.cfg\" in config/ ? Will it still work?\n. It's fine to create a new PR if you don't find how to update this. Just\nclose this one\nOn Thursday, December 12, 2013, Gabe Rosenhouse wrote:\n\nHere you go: MissionSt@aab0d04https://github.com/MissionSt/docker-registry/commit/aab0d04b745ebaeef946b0f8a26484a3a183991a\n(Not sure how to add it to this Pull Req directly. Opening a new PR seemed\ninappropriate.)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/pull/156#issuecomment-30444958\n.\n\n\n@sam_alba\n. It's fine, it's a config example anyway. But it's good to show such examples.\nCould you rebase? I'll merge\n. Great! LGTM\nCC @shin- \n. Could you rebase?\nAlso, the file registry/images.py is getting huge, it should contain only URL routes for /v1/images/* plus some helpers.\nWould it make sense to move all the code involved for diffing images (except the request route) in a new module (in lib/)? Then it'll be better for sharing the code between the images.py module and the diff worker. Actually the diff worker should never call a module in the registry/ dir since it stores the flask app, not libs.\n. Also, we need unit tests for testing the diff code.\n. The cache_lru option is enabled only for the prod flavor, as an example. Did you force the SETTINGS_FLAVOR=prod to produce this error? By default the flavor is dev to avoid any error when using the config_sample out of the box.\n. Ok, if you use the prod flavor, I recommend using your own config (not sample anymore).\nIt's possible to specify your own config by using a volume. So you can override the config/ directory when starting the registry container.\n. Since you implemented the swift support for the Registry, I would have asked you the same question.\nDid you encounter such errors when you tested the backend before?\n. What's the need fo this? If your config is that specific, could you just use env vars in a new config.yml or use a volume to mount /config/ and override with your own?\nI think we should deprecate setup-configs.sh\n. Left few comments inline. We need to address those before merging. Good job on the tests.\n. CC @shin- @dustinlacewell \n. I guess it means it's a configuration issue not a bug. Seeing the log, it looks like you guys hit a timeout.\n@kklepper could you retry using the run.sh script like @qqshfox did? It properly set the timeout.\nIf it works, let's close this issue.\n. In order to push \"localhost:5000/ubuntu\", such an image name must exist locally.\nIf it's the same image than ubuntu, here is how you can tag the ubuntu one to \"localhost:5000/ubuntu\" and then push it to your local registry:\ndocker tag ubuntu localhost:5000/ubuntu\ndocker push localhost:5000/ubuntu\n. Good guess, I noticed a similar problem. I have to dig into docker\npush/pull code. If you have time to do it, let me know.\nOn Tuesday, January 14, 2014, tamsky wrote:\n\nI think the docker CLI client is not closing connections which is probably\nthe root cause for this (compared to expectations that one gunicorn worker\nwill suffice for local testing.)\nWhen running gunicorn with sync workers (the default when no '-k' flag is\ngiven), and a # of workers '-w 1' or '-w 2' then things like 'docker pull'\ndo not work.\nA minimum of '-w 3' appears to be required.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/177#issuecomment-32329550\n.\n\n\n@sam_alba\n. I confirm, Redis should not been enabled by default. Let me reproduce the issue, the stackbrew image might have a problem.\n. I tested myself and the image stackbrew/registry works just fine, properly configured. From what I see you launched it using \"-p 5000\". Even though the registry inside the container binds on 5000, it's not the same port on the host. So I guess you have another registry misconfigured running on that port. Am I right?\nTo make sure the registry runs on 5000 on both ends, use the following arg:\ndocker run -p 5000:5000 stackbrew/registry\nLet me know what you have.\n. What version are you running?\n. There is an issue on Travis right now, I'll let you know when you can rebase this PR, it'll fix the issue.\n. Could you rebase your PR? It should fix the Travis-CI build\n. CC @kencochrane \n. Could you double check the code to make sure this does not involve any FD leak?\nMeanwhile I'll merge and test\n. ok, I see. We'll update a new version. Re-opening the issue now.\n. Merged already.\n. I agree.\nI would add some cleaning on the run.sh, there are a couple of scripts that we don't need. Like the way we manage env vars by replacing using sed. The lib/config now supports en vars, that should be used.\nI'd go for a good standard config and a only one entry point \"run.sh\".\nWould you have time to contribute a PR so start this work?\n. CC @shin-\n. I added it manually to avoid you to rebase.\nGood catch.\n. I see there are some changes in the API from gevent 1.0. I think we should do this. I have to check if unit tests pass.\n. LGTM\ncc @dustinlacewell @shin- \n. A 500 http error is not a network issue.\nI confirm the error, we have 2 new exceptions raised on our end. A\nregression from last release. We'll push a hotfix today.\nOn Monday, January 20, 2014, Joffrey F notifications@github.com wrote:\n\nCan you try again? This was most likely a network issue on our end (docker\nindex). Let us know if it persists.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/212#issuecomment-32769636\n.\n\n\n@sam_alba\n. @jacobgroundwater We just pushed a fix to prod and released a new version. Could you try again?\n. LGTM.\nWaiting for tests to pass and merge.\n. I am closing this for now since the right fix will be to remove tarsum computation from the Registry side (and only use simple tarsum).\n. LGTM\n. Yes, I agree with Joffrey on that.\nI renew my LGTM though :-)\n. I agree refcounting is better. I was about to do a small script to check our prod dataset and estimate the size of orphans at least. But no work in progress right now on the opensource side.\n. Trying locally\n. LGTM\n. Looks like you're still actively on it. Feel free to update the PR as soon as you have another patchset to push and I'll review the whole thing at once.\n. Left few comments inline.\nI think we need to separate the mirroring code more cleanly (all the \"if toolkit.is_mirror()\").\nAlso, you're missing unit tests.\n. @shin- How can I test this locally?\n. Ok. Do you have an estimate for unit tests and completion of this PR? I'll\naim for testing this tomorrow afternoon.\nOn Mon, Mar 3, 2014 at 9:23 AM, Joffrey F notifications@github.com wrote:\n\nThere's currently an issue with the tags cache that causes the auth token\nto not be consumed properly. For now, you can disable the tag cache in the\nconfig to be able to test locally.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/pull/229#issuecomment-36534171\n.\n\n\n@sam_alba\n. Not sure about 1) and 4).\nIf SSL is always correctly enabled, I think it's ok to redirect everything to 443. Should we keep a separate file for example without SSL?\n. cc @shin- Could you review this one?\n. Sorry for the delay for merging this. Still some tests since this can slow down review (but this is needed!). Testing and feedback welcome from anyone.\n. You're right.\n. Looks like a bug in the python-glanceclient. Would you mind trying with a newer version? The last one is 0.12, but 0.10 is pinned right now.\nhttps://github.com/dotcloud/docker-registry/blob/master/config/config_sample.yml#L23\nIf the bug disappears with 0.12, feel free to submit a PR to update the requirements.txt.\n. @bodenr could you identify the kind of metadata that break the push? An quick&easy way to reproduce will help anyone who will have time to make this PR\n. Much better!\n. It should work yes.\nI think this is a configuration problem. Could you make sure you have a secret key set? https://github.com/dotcloud/docker-registry/blob/master/config/config_sample.yml#L23\nThe issue if you have several worker and not set the secret_key, each of them will generate a different one. This key is used to read the cookie.\n. Maybe we should remove the auto-generate and raise an exception if not set. What do you think?\n. I think this deserves an easy Pull Request if you have time.\n. I know this is WIP but here are some early feedback:\n1. I don't understand why you need to modify several API endpoint for implementing search, can it be just implementing the \"/search\" in the Index stub class?\n2. I'd use sqlalchemy for storing the search index.\n3. The search must be optional like some other features on the registry. I should be able to use the registry as is, the search would be just like it does today: nothing.\nLooks like a big chunk, I think it'll need some discussions / improvements / changes / testing before thinking about merging back to master.\n. > Ok.  Any pointers on setting that up in the tox tests?\nSqlalchemy configured with the sqlite driver should pass any test just fine.\n. @wking Tests have to pass. I agree with you to make an exception here. You can actually force flake8 to ignore those lines. But anyway, tests have to pass.\nThe PR is fairly big (and contains some stuff unrelated ;-), so I need some more time to do another round of review.\nMeanwhile, anyone is invite to read/comment the code.\n. Reviewing this is hard since you made a bunch of changes in tags handling (that can be critical to merge without a lot of testing). However the search module looks good and I'd like to merge this.\nWhat do you think of the following plan:\n- Re-submit a PR with changes to tags (delete, lists, etc...)\n- Re-submit a PR later for search\nThen we can make sure the tags change are ok to be merged before moving to search module review. It'll be easier and safer that way.\n. This is much better. Thanks.\n. @shin- any update on that one?\n. Sorry my bad. I'll review today. On my todo list.\nOn Mon, Apr 7, 2014 at 9:23 AM, W. Trevor King notifications@github.comwrote:\n\nOn Thu, Mar 27, 2014 at 06:30:24AM -0700, Joffrey F wrote:\n\nLGTM!\n\nIs there anything else I can do to get this landed?\n\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/pull/247#issuecomment-39751531\n.\n\n\nSam Alba\nDocker, inc\n. Except few comments, the code looks good to me.\nHowever how to you plan to index a non-indexed registry? Should we add an indexer scripts which uses the lib/indexer?\n. Sorry, let me rephrase:\nLet's say I have my registry hosted somewhere and I want to enable your search feature. I already have 1000 images pushed to it, how I index them?\nAlso I spotted a possible issue, do you handle pagination? What happens if my search query matches 100K images, I guess they are all returned in results with the current implementation.\n. What do you guys think? I am ok to merge.\n@wking One last rebase would be welcome :-)\n. > What am I rebasing?  sqlalchemy \u2192 db and dropping absolute_import?\nYes, if you agree with this.\n. Weird, I still cannot merge...\n@wking Did you correctly rebased on top of master?\n. Hmm, could you check why Travis is complaining?\n. Ok, just left 1 last comment on gunicorn options and I think we're good to\ngo\nOn Wed, Apr 16, 2014 at 7:57 AM, W. Trevor King notifications@github.comwrote:\n\nOn Tue, Apr 15, 2014 at 08:31:39PM -0700, Sam Alba wrote:\n\nHmm, could you check why Travis is complaining?\n\nAh, it's because without absolute_import,\nimport index\nis importing registry.index instead of lib.index. We can either\nrestore the absolute_import or use something like:\nimport lib.index as index\nI've rebased and pushed the latter, but let me know if you'd prefer\nthe absolute_import approach for that module.\n\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/pull/247#issuecomment-40608967\n.\n\n\nSam Alba\nDocker, inc\n. Forget about my comment.\nLGTM\n@shin- Feel free to merge if fine with you\n. Last commits show this bug is not fixed\n. I think we have too many storage backends... How can we make them pluggable?\n. https://github.com/dotcloud/docker-registry/issues/317\n. @wking sendfile is perfect when you deliver your object from a local storage (from the local disk for instance). However since docker-registry supports a set of storage backend (s3, google storage, etc...), objects are served from those remote sources. Sendfile is useless unless we would buffer on the disk first.\nTo answer @strcrzy problem, I think it looks like a configuration issues. Have you tried with the registry container[1]?\nIf you use s3, I recommend configuring a LRU cache[2] on the front, it will cache most of the calls to S3.\n[1] https://index.docker.io/_/registry/\n[2] https://github.com/dotcloud/docker-registry#performance-on-prod\n. Have you tried the prod setup on AWS + S3? After you set the right config fields, you can switch the registry to prod flavor by setting the env var SETTINGS_FLAVOR=prod before starting the registry. If the registry runs in a docker container, you can specify the env var using the docker cli as an argument to 'run'.\nNote that you can also use the container and provide your own config by mounting the config directory in a volume.\n. Glance v2 api support can wait (v1 is not deprecated). And your previous fix addressed the problem with \"headers too long\".\n. Would you mind enabling debug on the docker daemon (docker -D -d) and give me the output (on the daemon) for the same command?\n. Could you check the \"secret_key\" is set in the configuration and retry?\nToo many users fell in this issue. If you don't set the secret_key, the registry will generate one. The problem is that each workers (in a multi-worker environment) will then have a different one. I will prevent commit a patch to prevent the Registry to start if the secret_key is not set.\n. Which other key gets removed by doing this whitelisting?\n. It sounds weird to have a second GCS plugin just for a login option. Can it be an option in the current GCS plugin? Also the tests don't pass.\ncc @proppy\n. Closing then.\n. DELETE /v1/repositories/foo/bar/tags is currently used internally.\nWhen the Registry is configured with \"standalone: False\", it relies on an external entity to act as an \"Index\" and all routes inside the module \"index.py\" are then not used.\nThe logic of the resource called \"/tags\" when delete is that right now, deleting a repos means deleting all its tags.\nHowever I do agree it makes sense to have both routes from index.py and repositories.py aligned. Here is what I propose:\nWe leave the DELETE /v1/repositories/foo/bar/ code in the tags.py module, which means that the query will work if the index.py module is used or not (standalone True/False).\n. > The endpoint is used internally, or the logic attached to it?  I\n\ncouldn't find someone calling DELETE on that URL internally (except\nfor the tests I fixed).\n\nIt's used by index.docker.io when you delete your image from the interface. But it's not a problem at all to change it there as well and align the module index.py as I suggested earlier.\n. I was about to merge and got another idea:\ncould you add the old route for deleting tags so we allow both (so we ensure backward compatibility)?\n. Nevermind, I'll add it.\n. As soon as we support images removal as well, we can split both routes.\nOn Sun, Mar 2, 2014 at 7:17 PM, W. Trevor King notifications@github.comwrote:\n\nOn Sun, Mar 02, 2014 at 06:02:44PM -0800, Sam Alba wrote:\n\ncould you add the old route for deleting tags so we allow both (so\nwe ensure backward compatibility)?\n\nIs the plan to split off the /tags handler once\nregistry.tags.delete_repository grows support for removing\nunreferenced images? Or are /tags users comfortable with the full\nrepository deletion? It's hard to imagine someone wanting to remove\nall the tags for a repository but not cleanup unreferenced images,\nbut I can imagine folks being surprised when a /tags URL removes more\nthan just tags.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/pull/268#issuecomment-36478753\n.\n\n\n@sam_alba\n. LGTM\ncc @shin- \n. This brought down the public registry today.\nRaising prio.\nStacktrace: https://bugsnag.com/docker/registry/errors/538f65f9b6b3c495fed794eb\nredis.exceptions.ConnectionError: Error 111 connecting registry-docker.azva.dotcloud.net:2755. Connection refused.\nredis/connection.py:253 connect\nredis/connection.py:316 send_packed_command\nredis/connection.py:334 send_command\nredis/client.py:464 execute_command\nredis/client.py:705 get\ndocker_registry/core/lru.py:80 wrapper\ndocker_registry/tags.py:71 get_tags\ndocker_registry/tags.py:83 <genexpr>\ndocker_registry/tags.py:85 _get_tags\ndocker_registry/lib/mirroring.py:60 wrapper\ndocker_registry/toolkit.py:264 wrapper\ndocker_registry/toolkit.py:290 wrapper\nflask/app.py:1461 dispatch_request\nflask/app.py:1475 full_dispatch_request\nflask/app.py:1381 handle_user_exception\nflask/app.py:1477 full_dispatch_request\nflask/app.py:1817 wsgi_app\n. Memory graph on prod.\n\nIt corresponds to the Redis used as a messaging, not the LRU one: https://github.com/dotcloud/docker-registry#cache-options\nI lost the data since the redis restart and dump disabled but I guess it's because of a message queue that get cleared...\n. @wking correct, the client reconnects internally.\n. Ok found the issue, actually the last release broke the way Redis is configured and uses the same one for both LRU and messaging. So the LRU is done on a Redis configured without maxmemory policy...\nThe data that blows up the Redis is cache from the LRU library.\nPR on the way.\n. Assigning to @shin- \n1. Need to fix the lru config\n2. Need to make redis call failover nicely\n. I would also trigger a bugsnag (if enabled) notification actually. Even though it's a warning, it will slow down the registry a lot and having a notification to operate the redis would be helpful.\n+1 on try/except on the right exception class as @dmp42 and @wking suggested.\n. Yes the tests are broken right now...\n. You're specifying the FLAVOR \"prod\" without specifying S3 information (you corrected the LRU one with Redis). Just try with the FLAVOR \"dev\" or just remove the SETTINGS_FLAVOR, dev will be used by default. It looks like you don't need the prod flavor for now, you can use dev for testing, it'll be much easier for a first try.\n. The public registry (available at https://index.docker.io/) is using S3 behind with the s3 storage driver: https://github.com/dotcloud/docker-registry/blob/master/docker_registry/storage/s3.py The code there is the same we use on prod, that's why we maintain it actively.\nIt can be a configuration issue, I can provide help with it. We enabled the LRU cache on Redis so most lookups to S3 are cached to speed up the access as documented there: https://github.com/dotcloud/docker-registry/blob/master/README.md#performance-on-prod\nNote that if you don't want to do it yourself, you can use https://index.docker.io/ directly, we support private repositories.\n. 1 inline comment, other than that, looks good to merge.\n. This LGTM, I am testing locally.\n. LGTM\n. Yes, if you scale your datastore (Swift, S3, GCS, etc...), the registry app is then totally stateless, so you can scale it horizontally.\n. Are you using the Glance backend with Swift behind? Or are you using the Swift backend directly?\n. Ok, makes more sense.\nTags are stored separately for now (PR are welcome there too :-)...\nJust configure the config directive \"storage_alternate\"[1]. It's the place were to store the tags (by default: local...), if you choose swift there, you will have tags and images at the same place.\n[1] https://github.com/dotcloud/docker-registry/blob/master/config/config_sample.yml#L88\n. Could you confirm you specified the storage_alternate?\n. You're right, it's linked to gunicorn. Could you share some info about how you launche the registry? Gunicorn config? Do you have nginx in front of it? Etc..\n. Apparently nginx forward requests to the URL \"https:///v1/_ping\"\nI bet on a nginx configuration issue, I invite you to checkout our example: https://github.com/dotcloud/docker-registry/blob/master/contrib/nginx.conf\n. LGTM\n. Yes there is interest to merge this. I left a comment. Sorry for the lag in PR reviewing...\nLeft a comment inline to make this configurable to not break the current behavior.\n. LGTM\ncc @shin- \n. What about adding a bin/script to start the registry like this:\n$ pip install docker-registry\n$ docker-registry -c config/config.yml\nThis would take care of spawning the gunicorn. So we can also remove the run.sh script. What do you think?\n. cc @shin- Could you review?\n. Working on merging this and we do want to merge it, believe me :-)\nI am testing locally and I have few more comments.\n1. What's the motivation behind having a directory named \"docker_registry\", any way to keep all files under \"registry\" and still have the main package called \"docker-registry\"? Maybe I am totally wrong there, I don't know setuptools enough but I thought it's possible to map a package_name to a directory...\n2. for the bin/script, I am thinking of a script located in \"bin/docker-registry\" that spawns gunicorn (it can be a shell script that forward its arguments). Then if you want to spawn gunicorn yourself, you use gunicorn docker_registry:wsgi?\nNot sure if your console script addresses the second point, I tried to test it but got conflict in the checkout... Probably need a rebase.\n. cc @shin- \n. Would it make sense to have this shared with all boto_* backends instead?\nThe prefix would be \"boto_\" instead of \"s3_\"\n. Updated with previous comments.\nSince https://github.com/dotcloud/docker/pull/4945, the target will be 0.10.\n. This can be merged now. @shin- and anyone else, please review\n. merging manually\n. We support https://bugsnag.com already which is similar.\n. Hmm I think we should go for this one or #247 but we have to decide, cannot keep both...\nPersonally I prefer #247 because having a Index with a db is important for scaling the size of the Index. Also, if you don't  want to rely on a db, you can still use sqlite which is still faster than walking the storage directly.\nWhat do you guys think?\n. Closing this in favor of #247\n. The docker daemon will init the connection (not the cli), so you need to setup your tunnel from your boot2docker VM.\n. cc @ewindisch\n. LGTM\n. LGTM\n. How do you handle backward compatibility? Since you change the way tags are updated, that's tricky to roll-out on Registries with existing dataset. What do you think?\n. LGTM\n. LGTM\n. Could you list all the commands you used to create and push the image before pulling it?\n. Hmm not sure why the tests failed on this one...\n. LGTM\n. LGTM\n. I agree with @wking if we have a nice way to move the code down (in toolkit).\n. LGTM\n. It's consistent with docker https://github.com/dotcloud/docker/blob/master/AUTHORS\nLGTM\n. Small pep8 warning broke the test.\nOne question though: what happens with current version of docker? Removing the cookie is ok I think, however is the token sent for every single request? Or does the client require changes to make this work?\nIgnoring the questions above, this looks like the spec we talked about, hence it looks good to me.\n. if we use flask.g, it won't be enough since the app has to scale to several instances.\nHowever I don't get the purpose of keeping flask.session if we pass the token with the repos name for every request... Is it needed? \n. no, we need to keep it until 1.0\n. LGTM\n. LGTM\n. Have you heard about dotCloud? :-)\n. I'm disabling the bandwidth parser which is totally optional for now...\nWill reopen if it occurs again.\n. :+1: for adding docker install instructions\n. Sorry for the inconvenience...\nWould you mind sending this to support: https://index.docker.io/help/support/\nI think it's a problem with the live service.\n. CC @shin- @dmp42 \n. Cleanup last references in the README :-)\n. LGTM\n. simplejson is much faster than the native python implementation. It's not mandatory to keep it though.\nBe careful, we do import it explicitly https://github.com/dotcloud/docker-registry/blob/master/docker_registry/images.py#L8. A cleaner way to abstract is it to import it from flask.json which imports simplejson only if it's available: https://github.com/mitsuhiko/flask/blob/master/flask/json.py\n. :+1: \n. build.sh, nginx.conf and redis.conf and dotcloud.yml are currently use (to run docker-registry on dotCloud paas)\nsetup-configs and _postinstall can be removed I think.\nI would wait for https://github.com/dotcloud/docker-registry/pull/350 to go through before removing setup-configs.sh. But I am for removing it.\n. yes, it runs the equivalent of a docker push&pull. It's useful from time to time.\n. cc @shin- \n. Can it be replaced by a simple dict for mocking? I don't think we use more than get/set/del.\n. @jpetazzo did you get anything new on this?\n. Also it looks like introducing the docker_registry package and the run_gunicorn method replicated the way gunicorn is launched and configured.\nI like using config/gunicorn_config.py actually...\n. I think it's just to avoid keeping a sh child process... Maybe docker should do that directly.\n. cc @shin- \n. :+1: \n. I think it's fine to require the version. We had the same conversation with @shin- when I committed the code. We need the client to announce its version to behave correctly.\nHowever I see the part where we need the version as temporary, we should change major things based on client version. As soon as docker 1.0 is released, we can remove the version check and remove the obsolete code.\n. Externalizing the drivers shrink the requirements list already. So that remaining one is super low prio IMHO.\n. LGTM\n. I agree with @dmp42 here. I'd rather not ship a fork of tarfile. Can we block this PR before removing the tarfile fork?\n. I agree with @vbatts that as long as we keep the tarsum code in docker-registry (even if relevant only for docker < 0.10), it makes sense to maintain it to reflect docker side.\nEDIT: Blocked by #381 \n. LGTM\nOn Fri, May 23, 2014 at 3:48 AM, Mangled Deutz notifications@github.comwrote:\n\nping @shin- https://github.com/shin- @samalbahttps://github.com/samalba\nYou can merge this Pull Request by running\ngit pull https://github.com/dotcloud/docker-registry fix-#382\nOr view, comment on, or merge it at:\nhttps://github.com/dotcloud/docker-registry/pull/385\nCommit Summary\n- Fix #382\nFile Changes\n- M docker_registry/toolkit.pyhttps://github.com/dotcloud/docker-registry/pull/385/files#diff-0(5)\nPatch Links:\n- https://github.com/dotcloud/docker-registry/pull/385.patch\n- https://github.com/dotcloud/docker-registry/pull/385.diff\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/pull/385\n.\n\n\nSam Alba\nDocker, inc\n. In the process to be integrated with docker client-side. Not critical for now. @shin- followed this, he might have more info.\n. It would be ideal.\n. lgtm\n. LGTM\n. LGTM\nAlso at some point aren't storage drivers supposed to not raise native exceptions?\n. Thanks @MattParker89 \nCould you also remove s3_bucket? boto_bucket replaced it at some point.\n. did not see this was already closed, nevermind\n. About pip install docker-registry[bugsnag], how does it work? I guess it's standard, recent addition to pip?\n. lgtm\n. LGTM\n. Could you explicitly enable the standalone mode in your config and try again?\nstandalone: True\nhttps://github.com/dotcloud/docker-registry/blob/master/config/config_sample.yml#L6\n. @discordianfish Could you run the docker daemon in debug and paste the daemon logs as well?\nThis is important, thanks for your help!\n. I think we should have that completely optional. The registry should work without it. On a large dataset, if a refcount is not updated and the refcount index gets corrupted, what are the options to rebuild it? I think it's suitable only for small dataset by design.\n. > - Crashed during a write.  When restarted, this can be detected by\n\ninvalid JSON in the references file, and we'd have to walk the whole\n  repositories/tags tree to see if any of those files depended on our\n  corrupted image.  Slow, but that's ok for recovering from such an\n  unlikely situation.\n\nTo rephrase what you explain, the operation is not atomic. Walking the whole dataset means it's not suitable for big dataset.\n. It's weird to have to introspect, can we default signer to None in the base class instead?\n. Hmm, yeah actually this would define it in the driver api...\n@dmp42 what do you think?\n. it means you have to implement some S3 specific code on the client side (docker push). Same for each backend. S3 implements requires some non-http-standard multipart queries that the registry abstracts. IMO, everything which is not standard HTTP should be abstracted by docker-registry.\n. What about adding a simple config to gunicorn to change user: http://docs.gunicorn.org/en/latest/settings.html#user\nSince we bind on 5000 by default, it should not be an issue.\nWhat I would do:\nFrom the Dockerfile:\n1. create a registry user\n2. declare a ENV var called DOCKER_REGISTRY_USER that contains this username\n3. From config/gunicorn.py, if the env var exist, I set the user config directive to the right user.\n. LGTM\n. if not buf:\n    return\nNo need to call the handlers when EOF.\n. Not needed if EOF is handled in SocketReader.read()\n. Can we avoid \"real private\" members? The rest of the code only uses \"protected\" members everywhere (simple '').\n. No need for '' prefix here. I'd rather put an explicit name like \"config\" or \"cfg\".\n. Instead of the big if-else, I'd rather do:\nself._s3_conn = boto.s3.connection.S3Connection(\n    self._config.s3_access_key,\n    self._config.s3_secret_key,\n    host=self._config.get('s3_host', 's3.amazonaws.com'))\n. Why not just:\nself._s3_bucket.initiate_multipart_upload(path, encrypt_key=(self._config.s3_encrypt is True))\n. This will change the s3 code for the whole app, not just the test. This cannot be merged.\nWhat about monkey patching the method get_size from the test code?\n. This change broke the tests apparently\n. What about returning None here?\nIt would avoid having all those try-except in the functions calling the helper.\n. If you were doing useful thing with the exception, why not. However you just catch it and return every time. That's why I think using an exception is overkill here.\n. I think this should be less generic. IOError makes sense for the Registry (when a file does not exist for instance), but most errors should not be handled in the same way.\n. Can we move this method in the status.py module?\n. Moving all the status code in the status.py module will require to move those 2 lines too\n. pyenv is the name of your virtualenv, it's not something generic that we can recycle, I'd recommend you putting your env somewhere else instead of ignoring this directory for everyone\n. Let's call this service \"redis\", we need to use it for other stuff.\n. after the rename mentioned above, those env vars need to be renamed as well.\n. You don't need to keep the status routes in app.py. You can declare the routes directly in status.py and remove those lines.\nIf you don't know how to get the Flask app object: http://flask.pocoo.org/docs/api/#flask.current_app\n. You should not rely environment variables for configuration. The user can use the config file and set environment variables directly from there.\nI'd remove the changes on lib/cache.py\n. Could you capitalize your class names as everywhere else in the code? It's also a convention from the PEP8: http://www.python.org/dev/peps/pep-0008/#class-names\n. Is this FIXME still true? maybe the usual return code is ok there?\n. All \"print\" should be replaced by a properly configured logger: http://docs.python.org/2/howto/logging.html#configuring-logging\nIt'll give the timestamp on each line when watching the script running on prod.\n. How do you plan to start the diff-worker in a prod environment? You should run it from: https://github.com/dotcloud/docker-registry/blob/master/supervisor.conf\n. Is there a python package registered that we can depend on?\n. Could you prefix the file with \"tag_\"?\n. Set your github handle inside the TODO()\n. You should mark the route above as deprecated:\n@app.route('/v1/repositories/<path:repository>/json', methods=['GET'])\n. TTL is usually expressed in seconds\nAlso it makes sense to have short TTLs for some use cases\n. So I guess the Registry would trigger a 500 here instead of a 404?\n. You should use the main buffer_size declared in the storage lib instead of hardcoding a new one: https://github.com/dotcloud/docker-registry/blob/master/lib/storage/init.py#L23\n. We need to find a way to avoid replaying the whole stream and deliver while we store.\n. You don't need to load the json text and dumps it again in the Response.\nYou can use:\ntoolkit.response(resp.text, raw=True)\n. same as above\n. KeyError if tags_cache not in cfg\n. Also, I would do this outside the generator, the code will be more logic\n. My comment apply for all the changeset in the file. I would make another PR for such refactoring. I think this PR should contain only code related to search.\n. Why refactoring this in this PR?\n. If we do that, we enforce the client to have a UserAgent all the time...\nMight be a good thing actually since we already get data from the user-agent for meta data associated to the layer.\n. Yes :-)\nThis new object relies on the WSGI env (flask.request), so it should stay in the flask app context (registry/toolkit.py)\n. You convinced me.\nAlso spot another glitch in the code, I'll push an update\n. I would not modify the run.sh as part of this PR\n. Can we keep the coding style and structure as the other modules?\n. Same as the above. I don't think it's necessary since it's not a library.\n. Not to me, that's why we have lib/ (libraries) and registry/ (flask app).\nI think having __all__ or not won't change anything, it's just important to stick to the same coding style I think...\n. Oh I get it now. I would rename the file lib/index/sqlalchemy.py then. I got many side effects with this in the past.\n. Instead of just checking if the storage driver supports this, I would also enable a config directive (\"storage_redirect is True/False\").\n. Why changing the run options in this PR? Should we leave them?\n. Could you restore the options?\n. Ok\n. I agree, let get the session.clear out of the if\n. Right!\nDone.\n. Correct. Actually nothing consumes this checksums right now. They're usually served by the Index. It's to make it clear the registry part only serves checksum on the payload rather than full image.\n. ",
    "jpetazzo": "On the one hand, I agree.\nOn the other hand, finding out if an image is referenced by others might be\ntricky.\nI mean \u2014 this is easy when you run a \"full blown\" registry. But if you run\na more light-weight registry, it could be more complicated.\nWould it be alright if, instead of exposing a way to delete an image, there\nwas a way to rollback to the previous set of tags? (i.e. possibly \"no tags\")\nThis would essentially make the image unreachable (unless you specify its\nfull ID).\nOn Mon, May 13, 2013 at 4:09 PM, unclejack notifications@github.com wrote:\n\nSome people have run into all sorts of troubles with images they've\nuploaded. They couldn't delete those images in any way.\nI've seen a few persons complain that:\n- push operations have failed and they ended up having corrupted\n  images on the registry\n- they ran into some error on the local machine, but the image was\n  still added on the registry\n- the server threw an error\nIt should be possible for an user to delete their own images. If the image\nisn't referenced by any other image, the user should be able to delete it.\nThis will become an issue as more people start uploading images to the\nregistry.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/7\n.\n. Yes, in the sense that if the next build is the same, with the exception of members with undecodable names, then the checksum will be the same but the content will be different. For caching purposes, it's probably not an issue, but it will compromise using the tarsums as a security feature.\n. /cc @samalba @shin- \n. I was drawn to this issue by this message on docker-user:\nhttps://groups.google.com/forum/#!topic/docker-user/QVWneEWcMk4\n\nOn Tue, Jun 3, 2014 at 9:08 PM, Sam Alba notifications@github.com wrote:\n\n@jpetazzo https://github.com/jpetazzo did you get anything new on this?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/dotcloud/docker-registry/issues/366#issuecomment-45049762\n.\n\n\n@jpetazzo https://twitter.com/jpetazzo\nLatest blog post:\nhttp://jpetazzo.github.io/2014/03/23/lxc-attach-nsinit-nsenter-docker-0-9/\n. We've had a short conversation with @bobrik on Twitter, so I wanted to give more feedback here :-)\nIMHO, there are multiple avenues to explore.\nFirst: push vs pull.\nIn a pull model, the data is transferred only when you need it. That means that when the Docker daemon issues a pull to a p2p registry, the p2p registry stalls the request, downloads the content, and then serves it. The daemon will see no data at all for a bit, and suddenly a (hopefully fast :-)) transfer.\nIn a push model, each time you have a new image, you seed it via p2p to the registries. That way, when a Docker engine needs it, it can be transferred right away. This is probably a great option for base images that are used by a large fraction of your servers and tend to be updated often.\nThen, there is another dimension: HTTP vs native data transfer. With HTTP, we keep the current API. It's great because we don't have to change the existing API and Docker engine code; but it limits our options. With \"native\", it means that the Docker engine could pull the bits it needs using p2p.\nObviously native is better, but needs significant work on the engine side. IMHO before thinking about a native p2p transfer in the engine, we need to rethink how we store images. Short explanation: it would make a lot of sense to store images as hard links to a common file pool. This would achieve de-duplication on the file level (but wouldn't work for the devicemapper driver!) and allow better content download negotiation (figure out which files we already have to avoid transferring them).\nMeanwhile, IMHO, the p2p push method is the easiest to implement. You could even use the existing registry code, but just have something getting torrent content and moving it at the right place (where the registry code expects it!); and of course you'll need something to automatically pick up new images on your registry and seed them accordingly.\n. ",
    "unclejack": "Rolling back by removing the tags would help remove the list of images from the user's profile, but the space needed to store those files is still needed.\nIf I make 5 attempts to upload a 300 MB image and it fails for some reason, there could be 1 GB or more of data around which is completely useless.\nPerhaps a correct and complete download should be tagged as such somehow? e.g: use the checksum to ensure everything is what it should be. \n. @samalba That would be great to have. I'm currently looking into setting up a full local docker test environment for continuous testing. Being able to do end to end testing without hitting the official registry would be great.\nI'd like to build a minimal registry with a web interface, so your help would be greatly appreciated.\n. @samalba Did you get a chance to look into writing the auth example?\n. @shin- I've switched from stackbrew/ubuntu to the unprefixed ubuntu. Thank you!\n. @samalba It looks like it failed to install the requirements.\n. ping @shin- @samalba\n. The live registry doesn't seem to be returning \"bytes\" for the \"Accept-Ranges\" for the initial HEAD request, but the CDN is returning that header like it should.\nI think this should be fixed on the registry side.\n. @dmp42 I've prepared a patch which just requests the first 6 bytes and adds some extra error handling.\nThis also fixes both problems and we also avoid dropping the much needed initial request which tells us if we can actually retrieve the layer.\n. @xeoncore Please provide the following information from the system where you're pulling the images and seeing slow pulls:\n- full output of docker info\n- the type of storage you're using (SSD, 5400 RPM HDD IDE HDD, 7200 RPM SATA HDD, etc), amount of RAM memory, disk space, network link type (gigabit, 10mbps, 100mbps, 10gbps)\n- the output of cat /proc/mounts\n. @xeoncore I'd like to help you out with this. Can you please run the script from https://gist.githubusercontent.com/unclejack/b068857487558280e34c/raw/692ce5833c3cb67c25fd07ec2932f5af6ae7c4e1/docker-save-load.sh and provide the output, please?\nWe'll have to look at the results and figure out what to do.\n. @xeoncore There's something wrong with your storage or with your server. On my SSD which has seen a lot of writes, it loads the image in 3-4 seconds.\ndocker pull downloads compressed tarballs. These tarballs are either saved locally (for registry V2) or loaded on the fly. The flow of data looks like this when mirroring today: http download > decompression > extraction to disk. This would be equivalent to something like curl | gzip | tar.\nThere's a whole lot more data coming out of gzip than going into it: it's anywhere between 1.01x and 6.0x more data, depending on the actual contents of the image and how compressible that content proves to be.\nThe tar part of the curl | gzip | tar equivalent is what can slow down everything. It takes a lot of time because there's a lot of disk IO and a lot of syscalls are made to create everything on the file system.\nDisk IO performance is very important. While it may seem like your server has really good disk IO for your applications, this might not be enough to extract these layers quickly enough so that you see a speed increase with a private registry.\nPulling at a very high speed from the registry won't fix the slow disk IO or its high latency on the servers where you're pulling. That will remain a problem and it will slow things down.\nMore information about the hardware or virtual server you're using is needed.\nI'll also look into the code to see how we can improve performance.\n. If we don't have two elements after splitting by '-', we should check that we have at least something like \"123-\" so we can assume the request was for everything starting from 123 until the end.\n. You're right, I've missed \":\" in \"-1:\". I'll update the code now.\n. I've updated the code to remove this check.\n. @shin- We need to return 416 when we get requests like bytes 1-1. Some http servers don't even allow anything less than 3 bytes, so this is reasonable.\n. @tianon You're right. I'm sorry, I'll fix it.\n. ",
    "aidanhs": "Would be nice to have this to delete big images pushed by mistake that you need to roll back.\n. Looks good to me, thanks.\n. ",
    "tamsky": "I'm guessing one of these locations is where this gets fixed?\nhttps://github.com/dotcloud/docker-registry/blob/master/registry/tags.py#L177\nhttps://github.com/dotcloud/docker-registry/blob/master/registry/index.py#L107\n. Answering my own question, yes, docker-registry's use of boto honors the standard locations:\n/etc/boto.cfg\n  ~/.boto\nhttps://gist.github.com/hectcastro/5623007 helped me find what I needed to use Riak CS with docker-registry:\n[Credentials]\ns3_host = <data.riakfs.example.com>\ns3_port = 8080\n. I believe I've replicated this issue.  Your original command\nsudo gunicorn --access-logfile - --log-level debug --debug -b 0.0.0.0:5000 -w 1 wsgi:application\nwas missing this (apparently required) argument:\n-k gevent\nWithout it, I get the same i/o timouts.\nWith it, docker push/pull commands start working.\n. I think the docker CLI client is not closing connections which is probably the root cause for this (compared to expectations that one gunicorn worker will suffice for local testing.)\nWhen running gunicorn with sync workers (the default when no '-k' flag is given), and a # of workers '-w 1' or '-w 2' then things like 'docker pull' do not work.\nA minimum of '-w 3' appears to be required.\n. Having used the sqlalchemy backend within docker-registry myself, I have\nnot experienced any external dependencies.\n@dbason - Please include what you're observing that makes you believe that\n\"it's trying to pull down an sqlachemy image\".\nOn Jul 15, 2014 6:40 PM, \"dbason\" notifications@github.com wrote:\n\nHi, I've tried running the docker-registry image with sqlalchemy enabled\nbut it's trying to pull down an sqlachemy image and there are none in the\ndocker registry. So I guess the options are to build my own sqlaclchemy\nimage or put it into the docker-registry image.\nAny pointers/suggestions on where to start would be appreciated.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/dotcloud/docker-registry/pull/247#issuecomment-49114393\n.\n. See if the fixes proposed in #197 help?  ( increase the # of workers )\n. reviewing @dlewanda's original report, I'd agree with @samalba,\nand also guess that proxy_set_header Host   $http_host; may be missing from your active config.\n. \n",
    "stoopsj": "Extending the point made by @unclejack, let's imagine I build an image 5 times with no shared history (e.g. from scratch or without the docker build cache). For example, it takes me five tries to build customubuntu:12.04, and I pushed all five of those attempts as 192.168.0.1:9000/customubuntu:12.04.\nWhen I go to pull 192.168.0.1:9000/customubuntu:12.04 on another machine, I get 5 times as many layers as are needed, and the output of docker images shows one image tagged correctly, and 4  images that correspond to my previous attempts.\nPrecious time (which is money) and bandwidth (which is also money) is wasted! :-)\n. \"4 images\" should be 4 \"none\" images. Markdown ate my angle bracketed text...\n. After some investigation, it looks like there are multiple entries for tags in $storage_path/repositories/library/customubuntu/_index_images\nI can avoid pulling all the previous attempts by removing tags other than the one corresponding to the id in $storage_path/repositories/library/customubuntu/tag_12.04 from _index_images.\n. ",
    "shepmaster": "\nthere are multiple entries for tags\n\n@stoopsj thanks so much for posting this! We were suffering from pulling far too much, and this seems to have been the cause. Here is the script that I ran to clean up all of our repositories _index_images in one fell swoop!\n. In case you come here looking for a script to clean up your private repository right now, here's the script that we have to look for unused images and report how much space is taken.\nIt leaves a file in /tmp with all the unused images. You can use that to perform the deletion of images. I didn't want to automatically delete things, so it should be safe to run. :-) Caveat Emptor, and all that.\n. @bjaglin I never actually deleted all of the images reported. I modified the script to just focus on a single repository and moved those images to another directory for a while (as good as deleted, but I could restore if something went horribly wrong). That worked fine. I also cleaned up _index_images as described in #7 \n. ",
    "bacongobbler": "Could we also please see migration documentation on what storage driver maintainers need to implement with this functionality? Looking at the source, my guess is that we just need to implement a remove function that takes the namespace and the repository name?\n. This could be a more docker-related issue rather than the registry. I'd be happy to migrate this issue onto the docker repository if it seems more relevant there.\n. I can confirm that this is not working for myself either. I'm trying to delete an image that I pushed to the registry, but I cannot delete it. I've set up the registry with a signed SSL cert and basic auth. Logs below. \ncurl -u 'username:password' -XDELETE https://docker-registry.example.com/v1/repositories/busybox/\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>405 Method Not Allowed</title>\n<h1>Method Not Allowed</h1>\n<p>The method is not allowed for the requested URL.</p>\n. I asked @stevvooe 2 weeks ago about this issue. AFAIK it looks like this will be resolved with the v2 implementation of the registry: https://botbot.me/freenode/docker-dev/2015-01-16/?msg=29800131&page=6\nWithout looking too far into the code for v0.9, this could be resolved by tracking an image's refcount (AKA the ancestry of an image). If it is not a parent of any other image, then it's safe to say that the image could be removed without damaging the integrity of other images.\n. I'm trying with basically the same nginx.conf as listed above, but I'm not able to push images:\n```\n\u03bb cat nginx.conf \nupstream docker-registry-upstream {\n  server localhost:5001;\n}\nserver {\n  listen 80;\n  server_name docker-registry..com;\n# ssl on;\n  # ssl_certificate /etc/ssl/certs/docker-registry;\n  # ssl_certificate_key /etc/ssl/private/docker-registry;\nproxy_set_header Host             $http_host;   # required for docker client's sake\n  proxy_set_header X-Real-IP        $remote_addr; # pass on real client's IP\n  proxy_set_header Authorization    \"\"; # see https://github.com/dotcloud/docker-registry/issues/170\nclient_max_body_size 0; # disable any limits to avoid HTTP 413 for large image uploads\nlocation / {\n    auth_basic              \"Restricted\";\n    auth_basic_user_file    docker-registry.htpasswd;\nproxy_pass http://docker-registry-upstream;\nproxy_set_header Host $host;\nproxy_read_timeout 900;\n\n}\nlocation /_ping {\n    auth_basic off;\n    proxy_pass http://docker-registry-upstream;\n    proxy_read_timeout 900;\n  }\nlocation /v1/_ping {\n    auth_basic off;\n    proxy_pass http://docker-registry-upstream;\n    proxy_read_timeout 900;\n  }\n}\n```\nWhen I try to push an image, I get this:\n``` bash\n\u03bb docker images\nREPOSITORY                   TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nbacongobbler/redis           latest              1f0891e21ff4        2 days ago          372.9 MB\nbacongobbler/rabbitmq        latest              7d2ed36f6b09        2 days ago          231.5 MB\nbacongobbler/python          latest              d08b8215cf2c        2 days ago          530.7 MB\nbacongobbler/mongodb         latest              474f9577f414        2 days ago          479.6 MB\nbacongobbler/memcached       latest              8f4559332a8e        2 days ago          205.7 MB\nbacongobbler/jenkins         latest              77cfe4c61836        2 days ago          368.3 MB\nbacongobbler/elasticsearch   latest              e61268a8919b        2 days ago          365.6 MB\nbacongobbler/couchdb         latest              211ef56edc1f        2 days ago          564.3 MB\nbacongobbler/cedar           latest              cfc410891599        2 days ago          818.9 MB\nubuntu                       12.04               8dbd9e392a96        9 months ago        128 MB\nubuntu                       latest              8dbd9e392a96        9 months ago        128 MB\nubuntu                       precise             8dbd9e392a96        9 months ago        128 MB\nubuntu                       12.10               b750fe79269d        9 months ago        350.6 MB\nubuntu                       quantal             b750fe79269d        9 months ago        350.6 MB\nbusybox                      latest              e9aa60c60128        9 months ago        4.964 MB\n\u03bb docker tag busybox 192.168.69.67/busybox\n\u03bb docker login 192.168.69.67\nLogin against server at http://192.168.69.67/v1/\nUsername: test\nPassword: \nEmail: test@test.com\nLogin Succeeded\n\u03bb docker push 192.168.69.67/busybox\nThe push refers to a repository [192.168.69.67/busybox] (len: 1)\nSending image list\nPushing repository 192.168.69.67/busybox (1 tags)\ne9aa60c60128: Pushing \nPlease login prior to push:\nLogin against server at http://192.168.69.67/v1/\nUsername (test): test\nLogin Succeeded\nThe push refers to a repository [192.168.69.67/busybox] (len: 1)\nSending image list\nPushing repository 192.168.69.67/busybox (1 tags)\ne9aa60c60128: Pushing \n2014/01/08 15:21:21 Authentication is required.\n.\n\u03bb docker version\nClient version: 0.7.3\nGo version (client): go1.2\nGit commit (client): 8502ad4\nServer version: 0.7.3\nGit commit (server): 8502ad4\nGo version (server): go1.2\nLast stable version: 0.7.4, please update docker\n```\nI tried updating to 0.7.4 and got the same output. Here is my registry config:\n```\n\u03bb cat /opt/docker-registry/config/config.yml \ncommon:\n    secret_key: ***\n    standalone: true\nThis is the default configuration when no flavor is specified\ndev:\n    storage: local\n    storage_path: /data/registry\n    loglevel: debug\n```\n. All right, I got further with this, and I can now push images to the server. I have a wildcard SSL cert installed on the server. Again, here is my docker version, nginx.conf and config.yml for posterity:\n/etc/nginx/sites-enabled/docker-registry.conf:\n```\nupstream docker-registry {\n  server localhost:5000;\n}\nserver {\n  listen 443;\n  server_name docker-registry.*****.com;\nssl on;\n  ssl_certificate /etc/ssl/certs/docker-registry.crt;\n  ssl_certificate_key /etc/ssl/private/docker-registry.key;\nproxy_set_header Host             $http_host;   # required for docker client's sake\n  proxy_set_header X-Real-IP        $remote_addr; # pass on real client's IP\n  proxy_set_header Authorization    \"\"; # see https://github.com/dotcloud/docker-registry/issues/170\nclient_max_body_size 0; # disable any limits to avoid HTTP 413 for large image uploads\n# required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\n  chunkin on;\n  error_page 411 = @my_411_error;\n  location @my_411_error {\n    chunkin_resume;\n  }\nlocation / {\n    auth_basic              \"Restricted\";\n    auth_basic_user_file    docker-registry.htpasswd;\nproxy_pass http://docker-registry;\nproxy_set_header Host $host;\nproxy_read_timeout 900;\n\n}\nlocation /_ping {\n    auth_basic off;\n    proxy_pass http://docker-registry;\n  }\nlocation /v1/_ping {\n    auth_basic off;\n    proxy_pass http://docker-registry;\n  }\n}\n```\n/opt/docker-registry/config/config.yml:\n```\ncommon:\n    secret_key: super-duper-secret\n    standalone: true\nThis is the default configuration when no flavor is specified\ndev:\n    storage: local\n    storage_path: /data/registry\n    loglevel: debug\n```\ndocker version:\n```\n\n<> docker version\nClient version: 0.7.4\nGo version (client): go1.2\nGit commit (client): 010d74e\nServer version: 0.7.4\nGit commit (server): 010d74e\nGo version (server): go1.2\nLast stable version: 0.7.5, please update docker\n```\n\nI can now run the following command chain to push an example image to the server:\n```\n\n<> docker login docker-registry.**.com\nLogin against server at https://docker-registry.*.com/v1/\nUsername (): bacongobbler\nLogin Succeeded\n<> docker pull busybox\nPulling repository busybox\ne9aa60c60128: Download complete \n<> docker tag busybox docker-registry.**.com/busybox\n<> docker push docker-registry.*.com/busybox\nThe push refers to a repository [docker-registry.**.com/busybox] (len: 1)\nSending image list\nPushing repository docker-registry.*.com/busybox (1 tags)\nPushing tags for rev [e9aa60c60128] on {https://docker-registry.*****.com/v1/repositories/busybox/tags/latest}\ne9aa60c60128: Image already pushed, skipping\n```\n\n:tada: \n. There is a PR for self-signed certs underway. See dotcloud/docker#2687\n. @dlewanda are you using a self-signed cert? See the comment above. Also please post your logs\n. Anything you see on STDOUT while running the commands listed in https://github.com/dotcloud/docker-registry/issues/82#issuecomment-32049319. If this is a self-signed cert (i.e. you did not purchase an SSL cert from an official Certificate Authority), those errors are to be expected until dotcloud/docker#2687 is merged and pushed into release.\n. Looks like a RapidSSL issue more than a registry issue. A couple quick searches find me that RapidSSL certs are not trusted from the get-go. Try following the support article at https://knowledge.rapidssl.com/support/ssl-certificate-support/index?page=content&id=AR1549&actp=search&viewlocale=en_US and install the intermediate CAs, and see where it gets you. \n\nOn Feb 12, 2014, at 7:11 PM, dlewanda notifications@github.com wrote:\nI received the cert from a colleage claiming it was a wildcard cert in .pem form. I split it up into a .key and .crt as described on the 'net. I'm pretty sure nginx is happy with it as I can use curl or my browser to get the standard \"docker-registry server (dev)\" string without any SSL warning or error The browser's lock icon looks good and the details show it's a RapidSSL cert.\nThe only Docker output I get is\n2014/02/13 03:09:43 Invalid Registry endpoint: Get https://xxxxxx.xxx/v1/_ping: x509: certificate signed by unknown authority\nright after I try to login and can't get any further.\n\u2014\nReply to this email directly or view it on GitHub.\n. Starting to see these errors when I'm running the test suite:\n\n$ tox\npy27 runtests: commands[0] | python -m unittest discover -s /home/bacongobbler/git/github.com/bacongobbler/docker-registry/test\n.EE2013-10-23 16:24:32,810 INFO: Starting new HTTPS connection (1): region-a.geo-1.identity.hpcloudsvc.com\nF2013-10-23 16:24:34,452 ERROR: Object GET failed: https://region-a.geo-1.objects.hpcloudsvc.com:443/v1/94165097550687/bacongobbler/registry/images/o2jlzlooxsf9b1j8/json 404 Not Found  [first 60 chars of response] <html><h1>Not Found</h1><p>The resource could not be found.<\nTraceback (most recent call last):\n  File \"/home/bacongobbler/git/github.com/bacongobbler/docker-registry/.tox/py27/local/lib/python2.7/site-packages/swiftclient/client.py\", line 1110, in _retry\n    rv = func(self.url, self.token, *args, **kwargs)\n  File \"/home/bacongobbler/git/github.com/bacongobbler/docker-registry/.tox/py27/local/lib/python2.7/site-packages/swiftclient/client.py\", line 758, in get_object\n    http_response_content=body)\nClientException: Object GET failed: https://region-a.geo-1.objects.hpcloudsvc.com:443/v1/94165097550687/bacongobbler/registry/images/o2jlzlooxsf9b1j8/json 404 Not Found  [first 60 chars of response] <html><h1>Not Found</h1><p>The resource could not be found.<\n2013-10-23 16:24:34,954 ERROR: Object GET failed: https://region-a.geo-1.objects.hpcloudsvc.com:443/v1/94165097550687/bacongobbler/registry/images/o2jlzlooxsf9b1j8/layer 404 Not Found  [first 60 chars of response] <html><h1>Not Found</h1><p>The resource could not be found.<\nTraceback (most recent call last):\n  File \"/home/bacongobbler/git/github.com/bacongobbler/docker-registry/.tox/py27/local/lib/python2.7/site-packages/swiftclient/client.py\", line 1110, in _retry\n    rv = func(self.url, self.token, *args, **kwargs)\n  File \"/home/bacongobbler/git/github.com/bacongobbler/docker-registry/.tox/py27/local/lib/python2.7/site-packages/swiftclient/client.py\", line 758, in get_object\n    http_response_content=body)\nClientException: Object GET failed: https://region-a.geo-1.objects.hpcloudsvc.com:443/v1/94165097550687/bacongobbler/registry/images/o2jlzlooxsf9b1j8/layer 404 Not Found  [first 60 chars of response] <html><h1>Not Found</h1><p>The resource could not be found.<\nThis is off my branch at https://github.com/bacongobbler/docker-registry/tree/106-swift-storage\nconfig_test.yml:\ntest:\n    storage: swift\n    storage_path: /registry\n    authurl: _env:OS_AUTH_URL\n    user: _env:OS_USERNAME\n    password: _env:OS_PASSWORD\n    swift_container: bacongobbler\n    tenant_name: _env:OS_TENANT_NAME\n    region_name: _env:OS_REGION_NAME\nAnyone know why the layer would be completely empty?\n. > It's strange to see remote requests on \"https://region-a.geo-1.objects.hpcloudsvc.com\", are those calls hardcoded somewhere?\nNo. I thought the same thing, but these values are not hardcoded. region-a.geo-1 refers to the region name. All of the authentication happens here: https://github.com/bacongobbler/docker-registry/commit/d6b0038ae2434737cfe3fd6b558ffe1a4e7562f3#diff-22fccdfa36043b889dc3aae204e600a2R19\nEDIT: changed 'the tenant name' to 'the region name'\n. All of the other URLs resolve as per normal:\nhttps://region-a.geo-1.objects.hpcloudsvc.com:443/v1/94165097550687/bacongobbler/registry/images/o2jlzlooxsf9b1j8/_inprogress\nhttps://region-a.geo-1.objects.hpcloudsvc.com:443/v1/94165097550687/bacongobbler/registry/images/o2jlzlooxsf9b1j8/_checksum\nhttps://region-a.geo-1.objects.hpcloudsvc.com:443/v1/94165097550687/bacongobbler/registry/images/o2jlzlooxsf9b1j8/json\nhttps://region-a.geo-1.objects.hpcloudsvc.com:443/v1/94165097550687/bacongobbler/registry/images/o2jlzlooxsf9b1j8/ancestry\nThe only one that's missing here is the layer object.\n. > why do you read the swift object returned through a \"buffered_string\"?\nI was using this before I realized that you could actually retrieve a chunk using resp_chunk_size in the swift client: https://github.com/openstack/python-swiftclient/blob/master/swiftclient/client.py#L709\nI'll try using that instead :)\n. Travis just wants to complain because the envvars aren't set to use the test suite for the swift backend :P\n. I agree. getting the tests running on travis would be awesome.\n\nI think it's important to find a way to run the tests though. I guess it would require to mock Swift?\n\nSome way of running a mock Swift datastore would probably be the best way to approach this. I see that there's a mock S3 implementation in test/utils/mock_boto_s3.py, so that would be a good starting point.\n. > Could you take care of it?\nYep! I have no issue with that.\n. I almost have the test suite up and running, except for a few errors:\nThe problem at hand is that I am trying to retrieve a large string from a specific url (about 7MB), chunk the string into smaller bits, and send a generator class back. in the test suite, this is just a string that's sent to the monkeypatched class for processing. The code in the monkeypatched class looks like this:\n```\nfrom swiftclient import client\nimport StringIO\nimport utils\nclass Connection(client.Connection):\n    metaclass = monkeypatch_class\ndef get_object(self, path, obj, resp_chunk_size=None, ...):\n    contents = None\n    headers = {}\n\n    # retrieve content from path and store it in 'contents'\n    ...\n\n    if resp_chunk_size is not None:\n        # stream the string into chunks\n        def _object_body():\n            stream = StringIO.StringIO(contents)\n            buf = stream.read(resp_chunk_size)\n            while buf:\n                yield buf\n                buf = stream.read(resp_chunk_size)\n        contents = _object_body()\n    return headers, contents\n\n```\nAfter returning the generator object, it was called by a stream function in the storage class:\n```\nclass SwiftStorage(Storage):\ndef get_content(self, path, chunk_size=None):\n    path = self._init_path(path)\n    try:\n        _, obj = self._connection.get_object(\n            self._container,\n            path,\n            resp_chunk_size=chunk_size)\n        return obj\n    except Exception:\n        raise IOError(\"Could not get content: {}\".format(path))\n\ndef stream_read(self, path):\n    try:\n        return self.get_content(path, chunk_size=self.buffer_size)\n    except Exception:\n        raise OSError(\n            \"Could not read content from stream: {}\".format(path))\n\n```\nAnd finally, in my test suite:\ndef test_stream(self):\n    filename = self.gen_random_string()\n    # test 7MB\n    content = self.gen_random_string(7 * 1024 * 1024)\n    self._storage.stream_write(filename, io)\n    io.close()\n    # test read / write\n    data = ''\n    for buf in self._storage.stream_read(filename):\n        data += buf\n    self.assertEqual(content,\n                     data,\n                     \"stream read failed. output: {}\".format(data))\nThe output ends up with this:\n```\nFAIL: test_stream (test_swift_storage.TestSwiftStorage)\nTraceback (most recent call last):\n  File \"/home/bacongobbler/git/github.com/bacongobbler/docker-registry/test/test_local_storage.py\", line 46, in test_stream\n    \"stream read failed. output: {}\".format(data))\nAssertionError: stream read failed. output: \n```\nI tried isolating this with a simple python script, which passed without issues:\n```\n$ cat test_genenerators.py \ndef gen_num():\n    def _object_body():\n        for i in range(10000000):\n            yield i\n    return _object_body()\ndef get_num():\n    return gen_num()\ndef stream_read():\n    return get_num()\ndef main():\n    num = 0\n    for i in stream_read():\n        num += i\n    print num\nif name == 'main':\n    main()\n```\nAnyone know where I can start debugging this? I've been working on this issue for at least a couple hours, and can't seem to find a solution.\n. No progress was made on this. Asides this issue, the rest of the test cases all pass. I'll try throwing it on Stack Overflow and see if anyone knows the answer :)\n. hey @samalba, sorry but all my mock changes were never committed, so I lost my work. Starting again along with the help from Stack Overflow to help solve my previous issue in https://github.com/dotcloud/docker-registry/pull/128#issuecomment-29116641 :)\nhttp://stackoverflow.com/questions/20429971/issues-working-with-python-generators-and-openstack-swift-client\n. It's almost up and ready. I'm trying to read a very large string stream using cStringIO in a python dictionary:\nhttps://github.com/bacongobbler/docker-registry/blob/106-swift-storage/test/utils/mock_swift_storage.py#L25\nYet in the test suite for the local storage, I'm catching an AssertionError:\n```\nFAIL: test_stream (test_swift_storage.TestSwiftStorage)\nTraceback (most recent call last):\n  File \"/home/bacongobbler/.../test/test_local_storage.py\", line 44, in test_stream\n    self.assertEqual(content, data)\nAssertionError: '[squelched]' != ''\n\nRan 28 tests in 20.495s\nFAILED (failures=1)\n```\nIt looks related to the issue I posted last week, but I'm not quite sure I understand why line 27 for the mock swift storage would be making this fail again.\n. Solved in http://stackoverflow.com/questions/20646809/using-generators-and-cstringio-in-python-to-stream-strings\n. @samalba the branch has been updated to master, and is ready for review :)\n. Yes, I mocked the entire storage module. Oops :blush: \n\nI might have some time next week, do you need some help to mock this swiftclient thing?\n\nIt's all right, I think I have it figured out now. We have to mock the swiftclient module itself (specifically the swiftclient.client.Client class). Am I correct?\n. I've wrecked the commit history on this branch while merging. I'll make a new pull request when I've fixed it up today. Thanks for all your help on this, @samalba!\n. Hi @coderlol, see my latest blog post on ActiveState's site: http://www.activestate.com/blog/2014/01/deploying-your-own-private-docker-registry\n. I encountered the 404's only when the registry would make a GET request for an image, such as at /images/<image ID>. I assumed this is normal, since swift returns a 404 when you try to retrieve an object that contains no data (such as a pseudo-directory). GET requests to /images/<image ID>/ancestry and the likes would all work happily.\n. Never mind. That was because I was running it as ./wsgi.py. When in doubt, RTFM.\n. The build failed, but I swear it wasn't me! :angel: \n. Done.\n. Sorry, I've been too busy with other stuff to get a fix out for this one.\n. closing in favour of https://github.com/bacongobbler/docker-registry-driver-swift/issues/28 so we can track it there\n. Good question. I'm fairly sure that it's not necessary to keep a separate file just for HTTP connections. They can roll with the default nginx config with just a proxy redirect to port 5000 (or whatever port they have the registry running on) and it's good to go. Authentication is also sent in plain sight, unless they have no authentication behind their registry (which would be bad for production sites). I'm happy to make one if it would help you sleep better at night, though! :smile:\nThe redirect to HTTPS is not necessary, as the docker commands will fail anyways:\n```\n\n<> docker login http://docker-external.example.com\nLogin against server at http://docker-external.example.com/v1/\nUsername: bacongobbler\nPassword: \nEmail: me@bacongobbler.com\n2014/02/06 13:31:06 Error: auth: \n```\n\nIt's just more of a convenience thing when cURL'ing against the registry's endpoints. You'll actually get a response from the client saying that you should direct your client to https:// instead of saying that this server does not exist.\nAs an aside: I've found that there's some weird output from the docker client when trying to push as an unauthorized user (similar to the above docker login output), but that's because this use case never came up before with a private registry, so I didn't see a need to file a bug on a specific use case (unless you guys would like more bug reports!)\nAs for point 4, this is just a specific use case that we had with the registry, so I'm happy to revert that as I've already mentioned in the OP.\nHappy to help!\n. no worries. So you'd prefer the default to require authentication on both push and pull? What about the comment @samalba made about 1)?\n. > I would like the redirection snippet to be left in, but commented out by default (with some nice comments :-)).\nSure, I can make that change.\n\ncan you keep the other nginx conf file in synch as well? (for nginx >= 1.3.9)\n\nYou're referring to the nginx.conf in the root directory of the project, correct?\n. I've made a replacement PR with the comments made in this discussion. Closing in favour of #458 :smiley: \n. shameless plug FWIW, you can deploy the docker registry from source on Deis:\n$ cd docker-registry\n$ deis create\n$ git push deis master\nremote: -----> Building Docker image\n[...]\nremote:        Launching... done, v3\nremote:\nremote: -----> bamboo-larkspur deployed to Deis\nremote:        http://bamboo-larkspur.local.deisapp.com\n$ curl http://bamboo-larkspur.local.deisapp.com\n\"docker-registry server (dev) (v0.8.0)\"\nSince Deis can build a project using either a Dockerfile or a Procfile, no modification to the project is required. In this case, it just reads the CMD entry from the Dockerfile. We're also (mostly) Heroku-compatible, so you have all the app management niceties like domains:add, config:set, releases:rollback et al.\n:smiley: \n. Well for starters, the registry cannot be deployed to heroku at the moment because their buildpacks are missing liblzma-dev for both the default and the custom buildpack that @stuart-warren proposed (which I'm assuming was added into registry core after this PR was proposed):\n```\n$ heroku create\n$ git push heroku master\n[...]\ngcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/app/include -I/opt/local/include -I/usr/local/include -I/app/.heroku/python/include/python2.7 -c backports/lzma/_lzmamodule.c -o build/temp.linux-x86_64-2.7/backports/lzma/_lzmamodule.o\n   backports/lzma/_lzmamodule.c:115:18: error: lzma.h: No such file or directory\n\n[...]\n$ heroku config:set BUILDPACK_URL=https://github.com/mfenniak/heroku-buildpack-python-libffi.git\n$ git push heroku master\n[...]\ngcc -pthread -fno-strict-aliasing -g -O2 -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/app/include -I/opt/local/include -I/usr/local/include -I/app/.heroku/python/include/python2.7 -c backports/lzma/_lzmamodule.c -o build/temp.linux-x86_64-2.7/backports/lzma/_lzmamodule.o\n   backports/lzma/_lzmamodule.c:115:18: error: lzma.h: No such file or directory\n\n[...]\n```\nI couldn't find any buildpacks with python and lzma support out-of-the-box.\nAs for Deis, we are an open source PaaS based on Docker with a similar workflow to Heroku. The reason why this app is able to deploy to Deis is because we added a feature to build an app using its Dockerfile in the project root, otherwise we deploy the app via buildpack a la Heroku. In the registry's case, that means no project modification is required to get this app running on Deis, only docs. This workflow will be even further improved when we merge https://github.com/deis/deis/pull/1190, which means that your docker image can be pulled from a registry (such as DockerHub) instead of being built from source every time :)\nWhen I mean we are \"mostly\" compatible with Heroku means that we do not have all of the newer client commands such as heroku fork, but the app workflow stuff like pushing an app, adding a domain and running one-off processes with deis run are all possible.\nDoes that make sense, or is there something you'd like me to clarify further?\n. I am running on Mavericks, and restarting boot2docker worked for me. Seems to be something up with the docker daemon rather than the registry.\n. LGTM.\n. Maybe they could be their own python package that's separated from the docker registry, similar to how django-storages is built by providing a generic storage backend driver.\n. Sounds like you need to create the docker-registry-retry bucket from the log you pointed out. You have to create them beforehand unlike s3, which creates them on the fly.\n. @matthughes did my above comment solve it for you?\n. ping @matthughes \n. I originally developed and contributed the swift storage driver. I'd be happy to adopt it :)\n. Will do!\n\ngive me your pypi nickname so that I add you as a package owner\n\n@dmp42 my pypi nickname is the same as it is here. Pretty easy to remember ;)\n. @jgatkinsn can you please post logs so we can see what issues you're running into, as well as what version of the registry you're using and a docker version?\n. @shin- I think the confusion is coming from right here: https://github.com/dotcloud/docker-registry/blob/0.7/README.md#how-do-i-setup-user-accounts\nThis is applicable for the public index, but not for in-house registries as you mentioned in your previous comment.\n. :+1:, would be great for storage driver maintainers :)\n. > Note that this may not be backwards compatible for some users because of permission issues with local storage\n:+1: to elaborate, volume mounts are always mounted with root permissions, so you won't have RW access to the mount unless you explicitly chown the mountpoint.\n. I had a great amount of headway with setting up basic auth on v0.6.3 when I submitted #230. I hope to address this issue when I have some free time this week :)\n. right. IIRC the -H option binds to a docker daemon on a remote host, not the private registry. Try docker -H tcp://10.14.244.190:2375 run 10.14.244.190:5000/cem_centos -i /bin/bash, assuming that IP address is routable from your host. Note the port change in -H. You're binding to the wrong port. That should pull your image (I think).\n. > what about adding a dockerfile extending the official image plus nginx with that configuration?\nConsider it done!\nSorry about the delay on fixing up this PR, been swamped lately :)\n. How would you like the project structure laid out? Can't think of a proper directory structure for a community project and I'm horrible with names. :blush: \nMaybe we add that in another bug so we can move the discussion there? I don't want to make it hard on reviewers due to feature creep :)\n. as for passing on /v1/users, I didn't find it necessary when I originally wrote this PR for http://www.activestate.com/blog/2014/01/deploying-your-own-private-docker-registry. Then again, that was 8 months ago. Things change over time. I don't have any insight into his issue, sadly. I'd have to kick the tires again and see if I can reproduce his issue.\n. @peterwillcn I see that you've cut access off using basic auth. Can you try docker login with a valid username/password which is stored in your user file and see if you can pull images again?\n. By default, the search backend uses a sqlite database locally. You can configure that by changing the connection string to a database somewhere external to your autoscaling group or within the confines of the VPC (if you're using one, that is). Could this be the issue you're seeing?\n. I'd be interested in something like this. I wouldn't necessarily care about the current status of the underlying backend (i.e. disk usage, # of incoming requests, available bandwidth etc.) as usually there's stuff in place for the backend. My use case here would be to check if the cloud storage (either in-house or cloud-based such as HP Cloud Object Storage) is up and running and can take requests/we can retrieve images. Not much else to it :)\n. > Also, are you merging a pull request from yourself in 44fe2da?  And\n\nsourcing the pull request from a deis branch instead of a branch of\nyour own fork?  To me, it makes more sense if you just pushed 3cf76a4\nto your own repository-import branch and dropped 44fe2da.\n\nWe're actively developing these features for this branch and using it in Deis (see https://github.com/deis/deis/blob/master/registry/Dockerfile#L16-L18, https://github.com/deis/deis/pull/1496, and deis/docker-registry#1) so we need to review/merge these features in before we integrate it into Deis. I can clean up the commits at a later time, of course.\nThis is something we had to integrate with our platform to get this feature in, so this is more of a posing question of \"does upstream want this sort of feature?\".\n. > I'm not a maintainer, so it's not my call.  I like the idea here, but\n\nI haven't looked through the implementation yet.  How will Deis handle\nit if a rebased version of this branch lands here?\n\nIf the idea behind this PR gets the :ok_hand:, I'll probably close this PR and open up another with a different branch which we can actively hack, refactor, review, add docs/tests without affecting Deis core. If it gets merged upstream and cut into a release, we'll update our endpoints as such so we can keep up with mainline releases. :)\n. Works great on our end for the most part. We're actively using it in Deis and it's holding out well. The only issue is that there's no response until all the images are pulled from the remote registry, which may cause timeout issues.\n. closing. I'll make this an extension when I get the time :)\n. Look in http://docs.docker.com/reference/api/registry_api/. You can only delete tags and repositories, but not individual images.\nDELETE /v1/repositories/(namespace)/(repository)/tags/(tag)\nDELETE /v1/repositories/(namespace)/(repository)/\n. Would this be a backwards-incompatible change and we'd need to update our storage drivers?\n. I agree with @jamtur01 in a sense. MkDocs/Markdown are great tools, but there are tradeoffs. Markdown has a few less features than ReST (definition lists, sidenotes, footnotes, etc.) due to its nature of being HTML-specific in the sense that processors expect to be generating HTML. From my perspective, I know we've (meaning deis/deis) have wanted to switch over to MkDocs for a while, but ebook and PDF exports built right into Sphinx have been too good of a feature for us to warrant a complete refactor of our documentation, so you might want to look into that and see if you want to make that tradeoff.\nOn the plus side, Markdown is a great tool for writing documentation. The way headers are generated with Markdown is a little more strict which makes it easier to keep a consistent standard across pages. Markdown's arguably more human-readable than ReST's syntax, and it seems like it's more widely adopted than ReST, which might make for a better contributor's experience.\nJust my two cents on the discussion :)\n. Agreed, I think it's simply networking. boot2docker just sets up a NAT bridge between your host and the boot2docker VM and forwards ports 22 and 2375 to your host. Other machines (even other VMs on the same host) cannot resolve to this machine due to how NAT bridges work (as well as the fact that port 2350 was not forwarded).\nA workaround is to put your machine in \"bridged\" mode or you can do some port forwarding... but I agree with @dmp42 that this would be better off on boot2docker/boot2docker's issue list. \n. Agreed; I had to work around this issue quite a bit in #489.\n. For public reference, you have to make a GET call to the repository in question to retrieve the token.\n```\n\n<> curl -si https://registry.hub.docker.com/v1/repositories/library/ubuntu/images -H 'X-Docker-Token: true' | grep X-Docker-Token\nX-Docker-Token: signature=01b8e3d3ef56515b33d9f68824134e3460de3a1a,repository=\"library/ubuntu\",access=read\n```\n\nYou can then use this token against the CDN for an image:\n```\n\n<> curl https://cdn-registry-1.docker.io/v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json -H 'Authorization: Token signature=01b8e3d3ef56515b33d9f68824134e3460de3a1a,repository=\"library/ubuntu\",access=read'\n{\"id\":\"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\"comment\":\"Imported from -\",\"created\":\"2013-06-13T14:03:50.821769-07:00\",\"container_config\":{\"Hostname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":null,\"Cmd\":null,\"Dns\":null,\"Image\":\"\",\"Volumes\":null,\"VolumesFrom\":\"\"},\"docker_version\":\"0.4.0\",\"architecture\":\"x86_64\"}\n```\n\nNote that you need to append \"Token\" to the beginning of the HTTP_AUTHORIZATION header.\nAgain, this is just for DockerHub, not for the open source registry. :)\n. @devilankur18 'docker tag foo/bar xyz/bar && docker push' won't work for you?\n. > I will have a look into what can be done with nginx auth, which you suggested.\nJust gonna leave this one here: https://github.com/docker/docker-registry/blob/master/contrib/nginx.conf\nIf you want to use the registry to distribute software, it is completely possible to roll your own authentication system in front of the registry. It's not built into the open source registry as you have noted, and others have had to write their own authentication server in front of it.\nHowever, if you just want end users to have the ability to retrieve images, then you can limit everything but GET requests with auth_basic inside nginx. That way, customers can view and retrieve images via docker pull but cannot push anything unless they have the right username/password. In this setup, the images are read-only unless they have the credentials necessary to push images. Does that work for you?\n. Would removing this line break the registry when running nginx on port 80/443? Would anybody miss it if the HTTP_HOST header was gone?\nThe intention of the script was originally made in the context of running nginx on those standard ports. Personally I'd rather not go ahead with this change and just create a registry-nginx container instead that exposes port 80. That way, you can run on a non-standard port and won't have to mess around with these details.\n. PR LGTM, but I can't seem to get my SSL key to be verified by docker login. It seems like this is a common complaint, but I haven't had a chance to look at hacking docker yet. Has anyone else got custom SSL certs working with docker? The registry's all set up nicely, but it's the client that's being a pain.\nhttps://gist.github.com/bacongobbler/224c79a86ebf16aa5630\nRegardless, LGTM.\n. And so https://github.com/docker/docker/issues/8132 was born. I'll keep the discussion moving there. :)\n. that works too. I'll whip a PR up for that.\n. better? :smile: \n. :+1: indeed\n. I agree with the first point (getting it into the driver's tests). The reason why this issue came to be is that I asked for regression tests from this behaviour with _walk_directory (https://github.com/bacongobbler/docker-registry-driver-swift/pull/18). @sathlan told me that the issue was specific to the swift driver and how the registry calls list_directory with a swift driver. I have yet to test this and confirm it myself but it seems like something wrong with the driver itself. In either case, the tests should only exist for the driver itself (i.e. it's an issue with only a specific driver), or it's an upstream bug and tests should be supplied there.\n. good catch, thanks :)\n. I'm no longer with ActiveState, but there are a few here who may be able to help. :)\nping @philwhln @srid \n. @kerr23 do you know how this change allows you to push images? Your guess is as good as mine at this point.\n. after further research, this looks like a duplicate of #320 to me.\n. @shin- looking at the nginx config listening on port 80, the authentication is being done through HTTP. That would most likely contribute to this issue as well.\n. good catch. LGTM\n. FYI django's recommended logger formats look like so:\n'formatters': {\n        'verbose': {\n            'format': '%(levelname)s %(asctime)s %(module)s %(process)d %(thread)d %(message)s'\n        },\n        'simple': {\n            'format': '%(levelname)s %(message)s'\n        }\n}\nTake what you will from it :)\nFrom https://docs.djangoproject.com/en/1.7/topics/logging/\n. definitely will try this out and give it a good kick of the tires :)\n. @adamhadani for reference I typically look at the Dockerfile as it includes the steps for deploying the registry, however if the documentation is sparse for running locally then it would be nice to put that in the contributor's documentation even if it's only a mini sidenote.\n. I know for certain that we patched Deis' docker registry so we can use the rados gateway for ceph as of https://github.com/deis/deis/pull/1910, so that part of master is working quite smoothly. However, I got this error when running from the bleeding edge: https://gist.github.com/bacongobbler/ff34cd500c0689cbc1c1\nSteps to reproduce:\n- check out a4012beca6f7ff2514076297cbc4fb9b1b126590\n- docker build -t registry:master .\n- docker run -d -p 5000:5000 registry:master\n. Here's my two cents on the proposal:\nI can totally understand the desire to completely start from scratch. Having the ability to share packages from docker is a huge benefit to the registry. Speaking from Deis's perspective, this will require us to migrate from the old registry to the newer version which may take some time, so hopefully docker/docker will be able to maintain the v1 endpoints from an older registry for the forseeable future or we may be stuck on an older version until we update our infrastructure. That shouldn't affect the decision made here, but I wanted to give you a bit of insight on one of my concerns with this change. This is a big change, but I personally support the decision if you feel like it is a step in the right direction.\nOn that front...\n\nThe new image format drastically simplifies the concepts:\nan image is a json file, with a mandatory, namespaced name, a list of tarsums (eg: content-addressable layers ids), some opaque metadata, a signature\na layer is a binary blob, mapping to a tarsum\nExit \"ancestry\" (now implicit from the order of layers inside the image \"manifest\").\nExit \"layers are images are layers\".\nExit \"layer json\" etc.\n\nSo what happens to config/exposed ports/etc? Is that all going into the \"some opaque metadata\" format? Isn't this just an aggregation of all the concepts in the v1 API and just slapping on the v2 sticker?\n\nStarting from scratch sure has its downsides, and I can't say I'm happy ditching the accumulated experience with V1/python (especially all the good work done on drivers), but in the end it's a reasonned choice, and I believe the benefits out-weight the downsides.\n\nMost certainly. This change does not only affect the registry as well, but it also kills off all of the current python storage driver implementations, which may be affected substantially. For example, support for obscure drivers that only a small subset of users may be completely gone in a year or two because the maintainer has no time or experience in maintaining a second Go implementation of the same driver. If there's a way we can somehow make the drivers easy to develop and maintain, I'm happy with that. \nIn regards to store driver changes, I propose that these drivers should be maintained separately from core docker-registry, but instead of being separated into their own separate packages like last time, it should be maintained as a separate cloud-agnostic driver package as a key/value store on certain providers. That way, if someone wants support for an Openstack swift driver, a local filesystem driver, an in-memory key/value store like https://github.com/kelseyhightower/memkv or for an S3-based driver, for example, they only have to go to one repository that supports these drivers. This means that the registry has a hard dependency on this package, but it keeps maintenance of this package outside the context of docker-registry and other users can benefit from this package (e.g. someone else needs a cloud-agnostic filesystem driver for their backend). Thoughts on that?\nAll I can say is \"specs, specs, specs\". For my own needs, I'd like a way to contribute to the project in a way that would support this change. To facilitate that, we need a document so that external maintainers (i.e. contributors to docker/docker-registry who are not affiliated with Docker Inc.) like myself can contribute in some way, whether that be with the core API, the storage drivers, etc. This includes possibly some political discussion on the technology involved (do we handle API requests with something similar to Flask like Martini, or do we want to re-implement the world with the net/http package? Do we handle dependency management from third party libraries natively or do we use something like Godep?). I'm happy and comfortable with Go, so I'd definitely like to be in the loop if at all possible with the ongoing development of the v2 API. I assume that this issue is more of a \"hey, we're doing this regardless but I wanted to give you a heads-up\" more than an actual proposal. ;)\nTo note, the above point about getting into a discussion about the technologies/frameworks used to implement registry v2 is completely optional if we don't want to open that can of worms. It could potentially end up in a flamewar between what practice is better. Still, it'd be nice if there was some kind of heads-up or a day which we can discuss these changes in greater detail would be very much appreciated :)\nAll I can say is... Docker Registry hack day? :D\n. please feel free to contact me on IRC/email and get things started. I'm online from 8-4PST :)\n. > find the proper abstraction in go world\nThe standard idiom for this seems to be something along using an interface (following your simplification in #612's OP):\ntype Filestore interface {\n    Get(key string) (string, error)\n    Put(key string, data string) error\n    List(delimiter string) []string\n    Move(src, dest string) error\n    WriteStream(path string, r io.Reader) error\n    ReadStream(path string, w io.Writer) error\n}\nThen, in the registry we can just reference that interface and perform operations against it:\n```\nvar fileStore store.Filestore\nfunc main() {\n    fileStore = store.NewS3FileStore(...)\n}\nfunc putImage(key string, datastream io.Reader) error {\n    if err := fileStore.WriteStream(key, datastream); err != nil {\n        return err\n    }\n    return nil\n}\n```\nThis is just off the top of my head, but it seems to be the standard idiom in this case :)\nAs for the case with resumable push, I'm not too sure on how it can be performed in the context of a file store. A filestore is simply a glorified key/value store which reads and writes to an external filesystem. This is probably the registry's territory, as it would know which bits of data still need to be written and which bits have already been pushed. There could be a way to accomplish this. Maybe by doing a server-side check to see what bits have been pushed, and resume pushing the ones that have not already been pushed?\n. > would be happy with Filestore subclassing both an\n\nAtomicStore interface and a StreamingStore interface (although I don't\nknow how to write that in Go).\n\nThere certainly are ways to do this:\n``` go\ntype FileStore interface {\n    Get\n    Put\n    List\n    ...\n}\ntype StreamingFileStore interface {\n    FileStore\n    ReadStream\n    WriteStream\n}\ntype AtomicFileStore interface {\n    FileStore\n    ...\n}\n```\nAt that point you can use the power of duck-typing to figure out what interface the filestore implements :)\n. Hi @jokeyrhyme,\nWould the storage_path config value solve this issue? It allows you to specify a prefix within an S3 bucket.\n. @michielbdejong This is the open-source Docker registry implementation. If you have issues with the closed-source hub, please report them to support-index@docker.com instead of here. Thanks!\n. (sorry if I sounded rude, just copying the message from https://github.com/docker/docker-registry/blob/master/CONTRIBUTING.md) :smile: \n. no complaints here. Looks like the bases are covered :)\n. I'm a little confused with a lot of the added complexity that this storage driver implementation brings vs. the current registry's storage implementation. There's a lack of documentation here so there's not a lot to go on asides from the discussions in IRC and the NG-related issues, so apologies for my negligence if I missed something:\nWhy do drivers need to be spawned as separate processes? Given a new release of a filesystem driver in a third party package, wouldn't it be easier to just update that specific package and reload the server config to use the newest version? I just don't feel like an IPC system is the solution for zero-downtime registry reloads, and we should be relying on a more package-based approach for drivers. As an example, in the current registry it's as simple as python setup.py install or pip install --upgrade docker_registry_driver_swift, changing around some configuration if it was necessary and SIGHUP the process. Boom, updated. If your setup was anything more complex than that (such as in a public-facing situation), you could just run the older and the newer version of the registry side-by-side and gracefully migrate traffic to the new registry via your proxy.\nIf we were to ignore my above comment, how would installing/starting a new third-party driver look like, both inside and outside of a running container? In the eyes of the core maintainers, how would we maintain and ensure that these drivers are up and running?\nWith this change, configuration is no longer in a central config.yml; You have to specify configuration for every driver when spawning the process. Doesn't this go against the discussion in https://github.com/docker/docker-registry/issues/646?\n. @wking all points SGTM (sounds good to me).\nAs I mentioned, there's a whole lot of code here but no documentation on the storage driver endpoints and with the IPC test suite. Is documentation going to be supplied separately?\n. > This is the open-source Docker registry implementation. If you have issues with the closed-source hub, please report them to support-index@docker.com instead of here.\nThanks!\n. At deis/deis we use very common tooling for our Go packages. Go gets the job done quite nicely for 99% of our use cases. This is a synopsis of our current infrastructure:\n- go fmt formats the code nicely, style guide is pretty loose otherwise: https://golang.org/doc/effective_go.html#formatting\n- godoc.org for package documentation (example)\n- if official documentation is necessary (API endpoint docs, provisioning, reference guides etc.), mkdocs or sphinx seem to be the popular documentation tools used in other OSS projects in tandem with https://readthedocs.org/. Docker uses mkdocs with the assets hosted on S3 (I think) so it may make more sense to use mkdocs here\n- go test -v -cover ./... for testing the entire package as well as code coverage, go test -v -cover ./packagename for package-specific tests\n  - Makefiles are ridiculously useful here (example)\n- go vet for general code inspection: http://godoc.org/code.google.com/p/go.tools/cmd/vet\n- golint as our linter: https://github.com/golang/lint\n- For CI/CD pipelines, people either use TravisCI or they roll their own for integration tests\n  - We use Jenkins but others like dokku use TravisCI w/ a microservice for integration tests. We use Jenkins because we need to deploy and distribute docker images of each component when we cut a release, which Travis did not support at the time (and that may still be true to this day). We also need to run end-to-end integration tests using a CoreOS cluster via Vagrant, which TravisCI does not support either (and for good reason)\nI'm not saying you have to use these tools, but this is our workflow and it has treated us very well. You can also write benchmark tests etc. with Go tests, which is another huge plus: http://golang.org/pkg/testing/\nHope this helps!\n. nice catch\n. We're using the docker registry with ceph. Here's our configuration: https://github.com/deis/deis/blob/master/registry/templates/config.yml#L92-L106\n. LGTM and +1 on separation of concerns, though users would probably like to have /ssl configurable in case their certs are in another container or are located somewhere else on the local filesystem (--volumes-from doesn't allow you to choose the src and dest directories IIRC). Maybe $REGISTRY_KEYFILE, $REGISTRY_CERTFILE AND $REGISTRY_CA_CERTFILE?\ne.g. I usually like to have my certs located at /etc/ssl/certs/..., keys in /etc/ssl/private/... and the CA cert chain at /etc/ssl/root.ca\n. @haosdent it looks like you are managing the release process instead of the maintainer of the project, according to https://github.com/chris-jin/docker-registry-driver-alioss/pull/1#issuecomment-62751949. Shouldn't we be waiting for them to properly fix up the packages before we consider this package ready for use?\n. Looks like you set up an invalid config option:\n13/Nov/2014:17:46:58 +0000 DEBUG: REQ: curl -i -X POST https://identity.example.com/v2.0/tokens -H \"Content-Type: application/json\" -H \"Accept: application/json\" -H \"User-Agent: python-keystoneclient\" -d '{\"auth\": {\"tenantName\": \"swift-registry\", \"pas\nswordCredentials\": {\"username\": \"keystoneuser\", \"password\": \"keystonepass\"}}}'\n13/Nov/2014:17:46:58 +0000 INFO: Starting new HTTPS connection (1): identity.example.com\n13/Nov/2014:17:46:58 +0000 ERROR: Authorization Failure. Authorization Failed: __init__() got an unexpected keyword argument 'server_hostname'\nCould you please post this at https://github.com/bacongobbler/docker-registry-driver-swift so we can better focus the issue? Please try building from scratch using the supplied Dockerfile as that will help check if this is on master or not.\n. let me know if this should be lowered. 15 minutes is a long time to wait for a connection.\n. gotcha. I forgot about that from the other PR :)\n. From http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_read_timeout, it says\n\nDefines a timeout for reading a response from the proxied server. The timeout is set only between two successive read operations, not for the transmission of the whole response. If the proxied server does not transmit anything within this time, the connection is closed.\n\nSo I guess it's for first-byte? I wouldn't know. Been too long.\n. you can remove the call to 0.0.4 as well, but not necessary\n. Driver seems too generic. How about FileStore?\n. To keep consistency, should this be changed to MoveContent, or should GetContent/PutContent be changed to Get/Put?\n. imports outside of Go's stdlib should be listed below the stdlib imports\n. In terms of previous filesystem driver configuration, there was a lot more than just the root directory. For example, in the S3 driver, you had to set up your access key/secret key to connect to the S3 bucket. For the swift driver, you needed to specify your tenant name, region name, your keystone API key, etc. How will this fit in with this implementation?\n. Small mini-rant, but I think there's potential in breaking this package out of the registry completely as a filesystem API package to contribute back to the Go community, similar to other projects in the pkg/ directory in docker/docker: https://github.com/docker/docker/tree/master/pkg\n. This seems to add additional cruft, as it's just a one-liner function. There's no true difference between calling d.subPath() and path.Join(), IMO.\n. I've hit that too, though this seems to be the \"proper\" format. Upstream's bug, not ours! ;)\n. General contribution guidelines for styling, formatting and testing their code may be useful here, such as go fmt, go vet, go test etc.\n. Would it make more sense for List to return a slice of os.FileInfo structures instead for verbosity? Not sure if it'd be compatible with certain key/value store bindings, but it's a thought. http://golang.org/pkg/os/#FileInfo\nWe should also be pointing out whether List is recursive or non-recursive on child directories.\n. As mentioned in #630, Move will typically involve full copies followed by deletion, save for the local filesystem. I can see the benefit for the local filesystem but would it make sense to remove this to reduce boilerplate between storage drivers?\n. In all 3 cases, ResumeWritePosition just returns the file size. Why not just call it Size instead?\n. Right. This is a big win for specific implementations and there's not a large hassle to manage a Move method in downstream drivers. I'm sure drivers would appreciate if a move operation was supported. I don't see a big reason to remove this, so I'm happy with keeping it in.\n. What would be the use case for using from and size? How do you see them being used in the registry? If we're concerned about having the prefix at a high enough level being slow, we could make it optionally non-recursive:\ngolang\nCount(prefix string) (int, error)\nList(prefix string, recursive bool) ([]string, error)\nThat should make List(\"/\", false) only display [\"images/\", \"repositories/\"] and List(\"/images\", false) significantly less time-consuming than recursively listing every directory under /images.\n. the same discussion was posted above: https://github.com/docker/docker-registry/pull/693#issuecomment-62482388\n. ",
    "yukw777": "Any update on this issue?\n. ",
    "calum-hunter": "so we can push images to a private registry, and remove \"tags\" but we can't actually remove the image that was uploaded? How do you remove the image from the registry?\n. If i run docker logs -f registry, when i push using the IP i see entries appear, but when i use the FQDN nothing appears in the logs at all\nHmm ok i'll try 1.3. Could be a CentOS thing too, i'll try running it on ubuntu.\n. Docker 1.3 still does not resolve the DNS name issue for me. Will try on Ubuntu later\n. ",
    "devinrsmith": "+1\n. ",
    "jwshea": "+1\n. +1\n. I'm also affected by this bug, images are trying to be opened from ./tmp regardless of storage_path setting inside config.yml\nopen(\"./tmp/repositories/library//_index_images\", O_RDONLY) = -1 ENOENT (No such file or directory)\n. As far as I can tell, this was fixed on 0.7.1 \u2013 but I haven't tested that yet.\n. ",
    "jianlinliu": "+1\n. ",
    "kwk": "hunty1 commented on 26 Nov 2014\n\nso we can push images to a private registry, and remove \"tags\" but we can't actually remove the image > that was uploaded? How do you remove the image from the registry?\n\nMaybe my answer on stackoverflow helps: http://stackoverflow.com/questions/25436742/deleting-images-from-a-private-docker-registry/25551160#25551160\n. ",
    "gesellix": "see https://github.com/docker/distribution/issues/106 for an overview of the planned new registry implementation named \"Distribution\", including garbage collection of non-tagged images.\n. ",
    "tobstarr": "See #24\n. So that would mean verifying and writing the checksum file when pushing a layer and adding that content to the responses? Or is there more to do?\n. +1\n. ",
    "atcol": "I have created a web UI for private/local registry installations. It's FOSS and containerized. See https://registry.hub.docker.com/u/atcol/docker-registry-ui/\n. It's moved. Sorry, I should have updated the link: https://registry.hub.docker.com/u/atcol/docker-registry-ui/\n. ",
    "rubytastic": "@atc- Your repo seems down.\nIs there a good opensource alternative or not currently?\n. Thanks that would be a good starter point, it seems to have all the needed functionality.\nUnless someone has other alternatives Im going to give this one a try :+1: \n. ",
    "Jud": "Looks as if this is the default as of cf2506a82a1f36ea029ff482c0b5136\n. ",
    "dpaola2": "Yes, it happens consistently with any image I attempt to push. I'm on mountain lion, using vagrant.\n\nDave Paola\nOn May 29, 2013, at 2:12 PM, Sam Alba notifications@github.com wrote:\n\nHi Dave,\nthis error usually means that the checksum provided by Docker at the beginning of the push is different after finishing to push the layer. Normal behavior is usually can be a connection interruption. This check is to prevent from having a layer corrupted.\nDo you have this error several times on the same layer?\n\u2014\nReply to this email directly or view it on GitHub.\n. vagrant@precise64:~$ docker version\n0\n0\nVersion: 0.3.3\nGit Commit: \nWARNING: No swap limit support\nvagrant@precise64:~$\n. \n",
    "keli": "Try rebuild your image with a more up-to-date version, that solved my problem. @dpaola2 \n. ",
    "adieu": "Try remove /var/lib/docker/graph/checksums which stores the cached checksum.\nI found I will get another checksum if the cached one is removed. Maybe docker changed the algorithms to compute the checksum in recent versions. @dpaola2 \n. ",
    "rca": "Great suggestion by @adieu!  I removed the checksum file and the upload finally worked.\n. More info; I bumped up the logging in nginx and got this; might this be on the docker end?\n27228#0: *68 client prematurely closed connection (104: Connection reset by peer) while sending response to client:\n```\n==> /var/log/nginx/access.log <==\n10.41.142.200 - - [15/Sep/2013:09:38:20 +0000] \"GET /v1/images/e0f7c47bd69aaa0850c35b6204df077cf9ef0fae3e08ce3c44276f4a1af7760c/json HTTP/1.1\" 200 1438 \"-\" \"docker/0.6.1 go/go1.1.2 git-commit/5105263 kernel/3.8.0-30-generic\" \"-\"\n==> /var/log/nginx/error.log <==\n2013/09/15 09:38:20 [info] 27228#0: *63 client 10.41.142.200 closed keepalive connection\n==> /var/log/nginx/access.log <==\n10.41.142.200 - - [15/Sep/2013:09:38:20 +0000] \"GET /v1/images/afe0c306cb73c6fd8c47d6c8d7aedc935641cfb01af37ec646d44debdbaa4adb/json HTTP/1.1\" 200 1427 \"-\" \"docker/0.6.1 go/go1.1.2 git-commit/5105263 kernel/3.8.0-30-generic\" \"-\"\n10.41.142.200 - - [15/Sep/2013:09:38:20 +0000] \"GET /v1/images/8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c/json HTTP/1.1\" 200 437 \"-\" \"docker/0.6.1 go/go1.1.2 git-commit/5105263 kernel/3.8.0-30-generic\" \"-\"\n==> /var/log/nginx/error.log <==\n2013/09/15 09:38:20 [info] 27228#0: 64 client 10.41.142.200 closed keepalive connection\n2013/09/15 09:38:20 [info] 27228#0: 65 client 10.41.142.200 closed keepalive connection\n2013/09/15 09:38:25 [info] 27228#0: 39 client 10.41.142.200 closed keepalive connection\n2013/09/15 09:38:25 [info] 27228#0: 68 client prematurely closed connection (104: Connection reset by peer) while sending response to client, client: 10.41.142.200, server: registry.baremetal.io, request: \"GET /v1/images/e0f7c47bd69aaa0850c35b6204df077cf9ef0fae3e08ce3c44276f4a1af7760c/layer HTTP/1.1\", upstream: \"http://127.0.0.1:5000/v1/images/e0f7c47bd69aaa0850c35b6204df077cf9ef0fae3e08ce3c44276f4a1af7760c/layer\", host: \"registry.baremetal.io\"\n==> /var/log/nginx/access.log <==\n10.41.142.200 - - [15/Sep/2013:09:38:25 +0000] \"GET /v1/images/e0f7c47bd69aaa0850c35b6204df077cf9ef0fae3e08ce3c44276f4a1af7760c/layer HTTP/1.1\" 200 4390569 \"-\" \"docker/0.6.1 go/go1.1.2 git-commit/5105263 kernel/3.8.0-30-generic\" \"-\"\n==> /var/log/nginx/error.log <==\n2013/09/15 09:38:25 [info] 27228#0: 71 client closed connection while waiting for request, client: 10.41.142.200, server: 0.0.0.0:443\n2013/09/15 09:38:25 [info] 27228#0: 70 client prematurely closed connection (104: Connection reset by peer) while sending response to client, client: 10.41.142.200, server: registry.baremetal.io, request: \"GET /v1/images/afe0c306cb73c6fd8c47d6c8d7aedc935641cfb01af37ec646d44debdbaa4adb/layer HTTP/1.1\", upstream: \"http://127.0.0.1:5000/v1/images/afe0c306cb73c6fd8c47d6c8d7aedc935641cfb01af37ec646d44debdbaa4adb/layer\", host: \"registry.baremetal.io\"\n==> /var/log/nginx/access.log <==\n10.41.142.200 - - [15/Sep/2013:09:38:25 +0000] \"GET /v1/images/afe0c306cb73c6fd8c47d6c8d7aedc935641cfb01af37ec646d44debdbaa4adb/layer HTTP/1.1\" 200 114346 \"-\" \"docker/0.6.1 go/go1.1.2 git-commit/5105263 kernel/3.8.0-30-generic\" \"-\"\n```\n. I found the problem on my end, apologies for the spam.\n. OK, I made the variable change and added a couple of tests to verify the behavior.\nThanks!\n. ",
    "shykes": "Fyi we are getting rid of checksum caching. So in the future will generate checksums on-the-fly and ignore /var/lib/docker/graph/checksums.\n\u2014\n@solomonstre\n@getdocker\nOn Tue, Jul 9, 2013 at 11:25 PM, Roberto Aguilar notifications@github.com\nwrote:\n\nGreat suggestion by @adieu!  I removed the checksum file and the upload finally worked.\nReply to this email directly or view it on GitHub:\nhttps://github.com/dotcloud/docker-registry/issues/15#issuecomment-20723814\n. The current plan for making this happen is a feature of the index called\n\"Trusted Builds\". See the proposal at\nhttps://groups.google.com/forum/#!topic/docker-dev/NDigdAk9trc\n\nOn Tue, Sep 24, 2013 at 5:32 AM, Juan Batiz-Benet\nnotifications@github.comwrote:\n\nAgree with @jaredly https://github.com/jaredly\nThe Dockerfile should be pushed to the index -- it should be available to\nsee on the website repo listing.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/33#issuecomment-24998731\n.\n. ping @samalba @shin- @kencochrane \n. Absolutely. This should be done as soon as possible. We're putting together\na build and release toolchain called Stackbrew, to make the process of\nmaintaining these images more automated and more open. That would be a good\nplace to add signing capabilities.\n\nOn Thu, Sep 12, 2013 at 12:16 PM, Rich Jones notifications@github.comwrote:\n\nAny chance of getting some kind of cryptographic security into docker? If\nthis is going to become a major way of pushing linux images around, I\nreally think that security should be baked in by default, early on.\nIf dotCloud is recommending that everybody use 'ubuntu', can we make sure\nthat the version we get from the registry has been signed by dotCloud, and\nisn't some MITM'd version containing malware?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/75\n.\n. This would be a good time to promote it to the standard library. @shin- could you pre-emptively add it to Stackbrew?\n\n\u2014\n@solomonstre\n@getdocker\nOn Fri, Sep 20, 2013 at 8:07 AM, Joffrey F notifications@github.com\nwrote:\n\nThanks! @samalba is the maintainer of the public image and he's on vacation atm so it may take some time until it gets updated on the index. We'll try to let you know when that happens.\nReply to this email directly or view it on GitHub:\nhttps://github.com/dotcloud/docker-registry/pull/80#issuecomment-24817028\n. Yeah, definitely not a replacement. But a potential integration would make sense I think.\n\nOn Mon, Dec 23, 2013 at 3:51 AM, James Turnbull notifications@github.com\nwrote:\n\nI'll put my ten cents in here: Maven is powerful but it's a) not for everyone and b) not universal, and c) in some corners it's even hated. I think there's a role for integration with Nexus/Artifactory but I don't see it as a replacement for the registry. YMMV :)\n/cc @shykes\nReply to this email directly or view it on GitHub:\nhttps://github.com/dotcloud/docker-registry/issues/168#issuecomment-31115295\n. Allow me to elaborate my answer.\n\nIntegrating with Artifactory/Nexus makes it possible for you to use docker with your favorite tool, and not reinvent the wheel. That's a good thing.\nStandardizing on it, on the other hand, makes it mandatory for every docker user. Many of them have another favorite tool which they would have to replace with artifactory - in other words they would have to reinvent the wheel. That is not a good thing.\nFor us to bring software into the core we need to be comfortable telling every developer and sysadmin that they should use it. There are few programs in the world that qualify for that, and nexus/artifactory is not one of them.\nThe same is true for almost all software tools: we live in a fragmented world and not everyone has the same preferences. What you consider \"agnostic\" your neighbour probably calls \"opiniated\" which is codeword for \"we are not from the same tribe therefore your code is irrelevant to me\". It's silly but it's true. There are very few things that can be reasonably called standard: posix, tcp/ip, tar, ssl. We're still debating whether git and special filesystem attributes are standard enough to be part of the core!\nThe docker core should build and be useful on everything from a developer laptop to a farm of octocore servers and a rasberry pi. I don't see how any java artifact repository fits in that picture, no matter how generic it is.\nOn Sun, Dec 29, 2013 at 13:01, coderlol notifications@github.com=\"mailto:notifications@github.com\"> wrote:\nIntegration would make much sense -- less infrastructure to deploy.  The current Registry model is simply too immature.  For example, no federation, tagging the actual Registry location in a repository name is just nasty.  It's better to have a config file with a set of locations where a repository may be found.\nBut then again, why re-invent the wheel?  Artifactory and Nexus are dead easy to deploy.  There is nothing to \"hate\" about them.  There is no need to use adhere to the \"Maven\" model as Artifactory and Nexus can host arbitrary blobs.\n\u2014\nReply to this email directly or view it on GitHub.\n. On Sun, Dec 29, 2013 at 10:41 PM, coderlol notifications@github.com wrote:\n\nI was going to fiddle with Apache to map Registry Restful API, but I can't\nget past the fact that the host name including the PROTOCOL port (arrgh) is\nhard coded in the repository name/tag -- that's just nastier than nasty...;)\nAre you able to articulate why you find it \"nasty\"? Because I don't agree\nwith you at all.\n\nHere's why we do it: it allows mapping images to URLs. The U in URL stands\nfor \"universal\", because the same URL should resolve to the same underlying\nresource, instead of resolving to different resources depending on where on\nthe network you happen to be. This is a very useful property, and mapping\nimage names to URLs allows us to benefit from it.\nA second, less important reason is that it mirrors how Go packages are\norganized, which in practice has proven to be quite usable and not at all\n\"nasty\".\nDo you any tangible arguments against this approach?\n\nAnd, it seems like Docker is trying to implement its own\nauthentication/authorization scheme? Why waste the energy on those standard\nstuff? Use standard stuff so we can easily enjoy HTTP, HTTPS, client cert,\nbasic auth, SSO, etc...\nI'm really not sure what you're referring to. Currently the docker registry\nuses vanilla HTTP auth, and allows you to drop arbitrary http middleware in\nfront of your private registry. There are discussions on adding support for\nssl client certificates and AWS-compatible hmac request signature. The most\nprominent argument in those discussions is how standard these options are\nand how much code we would be able to re-use.\n. On Mon, Dec 30, 2013 at 12:49 AM, coderlol notifications@github.com wrote:\nWhy should a user have to be bothered with where desired\nrepositories/resources are located? Users just want a resource X. Does it\nmatter if it is on my computer, on a remote system, on the moon? What if\nresources need to be relocated? Do users or providers have to go about\nre-tagging everything?\nAnd hard coding of port number on the repository name/url/whatever that is\ncalled, well, that's a winner of nasty of nasties ;) Why do we use DNS\ninstead of IP?\nThis criticism is not specific to docker. You are basically criticizing\nURLs as a means of identifying resources.\n\n\"why should a user have to type www.google.com? Users just want google.\nDoes it matter if hosted is hosted on my computer, a remote system, or the\nmoon? What if the website is relocated? Do users and providers have to\nabout changing all their bookmarks?\"\n\"why do web browsers support specifying optional port numbers in a url? why\ndo we use DNS instead of IP?\"\nHopefully you can figure out the answer to these questions for yourself.\n\nIn many ways, the practice of using URL/port hard-coding provides too\nlow of an abstraction level -- the model needs to bring it up a notch.\n\nSure. URLs are not perfect. I will be happy to implement a higher-level\nnaming convention, if you have one to suggest.\n. On Mon, Dec 30, 2013 at 12:28, coderlol notifications@github.com=\"mailto:notifications@github.com\"> wrote:\nWell, how about we put the address / URLs for the docker registries in dockercfg and let docker try each of the registry in turn to pull a given repository?  That way, you decouple of the name of the repository (data id), its location (URL/servers/ports) and access control (login, client certs, etc).  \n\u200bThat would mean that when I build a container from source, the result of my build would be completely unpredictable because, for example, \"FROM ubuntu:12.04\" would yield an entirely different image depending on how my docker installation was configured. Since it might have been configured by site administrators and I might not have permission to change or even view that configuration, I may not even be able to verify what exactly is being built. All I know is that someone, somewhere, decided to call it \"ubuntu:12.04\".\nHow do you propose we solve that problem?\nI think the design of docker artifact distribution model has simply misunderstood the use of \"URL\".\nIt wouldn't be the first time I do something stupid. But so far you're not \u00a0 \u00a0giving me much substance to find out.\n. > What about some kind of a binary protocol with multiplexing of read/writes streams? Draft of HTTP/2 looks good as a concept. \n\nWe can take a look at some common binary serialization libraries (for example msgpack) and use one of them to communicate between core and plugins over tcp/unix domain socket. It allows us to implement a fast, flexible, easy-to-extend protocol. This protocol should be bidirectional to provide a full control over communication.\n\n@noxiouz you are describing libchan :) It uses msgpack for serialization and implements multi-plexing over http2. https://github.com/docker/libchan. @dmp42 @dmcgowan for communication with extensions, I strongly recommend using libchan, since that is the direction we're going for Docker extensions also. If one of the goals is cohesion with the rest of the Docker platform, this one is a no-brainer.\n. ",
    "mattwallington": "Any updates on this?  Still having the issue in 0.5.3 and it takes forever for the checksums file to regenerate when you have a few images stored locally.\n. I won't get a chance to test this until tomorrow but will let you know. \nOn Aug 19, 2013, at 6:54 PM, Sam Alba notifications@github.com wrote:\n\nThis will be fixed in the next Docker release. Would you mind testing the exact same thing with Docker built from the master branch? It'll help to test some more and confirm that it will fix your issue.\nThanks!\n\u2014\nReply to this email directly or view it on GitHub.\n. I'm still having issues with this as well.  Both on 0.5.3 as well as a freshly compiled version of what's currently in master (as of 10:30 PM PST 8/21)\n\nWhen I push I notice it doesn't push the whole image.  Dies somewhat through pushing a layer.  If I repush it keeps dying at different spots.  Running the latest docker-registry (freshly pulled down via github).\n. Well, I completely rebuilt my entire environment: registry server using latest in github and nginx 1.4.1, clients with docker 0.5.3-1 and now everything seems to be working.  Of course that provides no helpful information to debug but whatever I did, it's working now.\n. S3 store.  EC2 instance for registry server.\n. Update: In the interest of full disclosure, I may have made a rookie mistake and had not previously been using nginx >1.3.9 for my proxy which is documented to not support chunking which is required by Docker.  So my images may have not pushed properly hence why it would fail to pull on the other client.\n. Now it works for me with nginx and S3 as well as gunicorn only and S3 since I rebuilt my setup.  So I have to assume that something was funky with my old setup.\n. James, if it's not working without nginx, it's definitely not going to work with Nginx.  Are you launching gunicorn using the method Sam documented on the docker-registry github repo page?\n. Unfortunately I didn't keep my old environment around that was broken.  When I rebuilt everything started working.  Which has me believing that my entire problem was because I was using a really old version of nginx (the version in the stable apt repository).  Since rebuilding and installing nginx 1.4.1 from source, the problem has disappeared.  If I run into the issue again i'll shoot you a message and help you debug it.\nAlso, I have experimented with using a single worker and multiple workers but that was awhile back.  Currently i'm just using 1.\n. Not an issue for me any longer but I think it was due to my nginx proxy.  New version seems to work great for me.  I love the multithreaded layer transfer with 0.6\n. I would as well.  It would be greatly beneficial if there was an authentication service built into it that allowed us to create and administer users that have specific permissions for certain images.\n. I am still having this problem while using 8 workers with the latest pull from github and 0.6.1\n2013/08/27 22:18:23 HTTP code 400 while uploading metadata: {\n    \"error\": \"Checksum not found in Cookie\"\n. Ahh!  Makes sense, I just kept my existing config file and ported it to each new version.  I should stop that.  :)\n. ",
    "kencochrane": "The problem is that the %2F's get converted to slashes before flask does the url mapping, it does the same thing with django, so I needed to allow the slashes in order to get it to work.. We will need to test on staging to make sure this doesn't break anything that was previously working.\n. @rgbkrk Thanks for the report. The index repo is private so you can't add issues to it, so this is fine for now. I'll look at fixing the bug now.\n. @rgbkrk This has been fixed. please try it out and let us know if you see any issues. Thanks for reporting the bug.\n. @shin- can you close this issue.\n. The code looks fine, now just need to decide if we want to change the initial design of tags.\n@kgoudeaux  Can you give a use case where you would want the tags to be updatable? \n. OK, so it looks like you need more of a tag alias (or symlink) then an updatable tag, something that will always point to the latest version of a tag assuming blah-#, blah-latest will point to the newest one. \nquestion how would it work with this.\nblah-1\nblah-2\nblah-latest -> blah-2\nblah-1\nblah-a\nblah-fish\nblah-latest -> ?\nWould it be alphabetical sorting, age, how do we know what the latest is?\nWould we need to move tags around in order to do that, could we just calculate the latest of something assuming they use the blah-# syntax. I guess it depends on how we currently handle the latest tag today, ideally we would want to do something similar, without adding any crazy logic to the registry to handle it.\n. Looks like Jerome found the cause of the memory spikes. it was bandwidth parser \n. we have something similar on the index (registry hub) as well. would be nice if we had more info, look to see if something is going on with the service at the time of the request.\n. ",
    "mmerickel": "Great, thanks for the update.\n. ",
    "jaredly": "no, nothing fancy in my environment. I'm running 0.4.8\nTry \"docker pull jaredly/strider\"; it's giving me (and a friend on a different machine) an \"internal server error\" response.\n. Ok it must have gotten sorted out on the server side. Works just fine with 0.4.8.\n. yeah, that makes total sense. So docker push would construct the\nDockerfiles and send them too?\nOn Mon, Jul 29, 2013 at 10:54 AM, Sam Alba notifications@github.com wrote:\n\nI think this feature should be implemented in Docker and not in the\nRegistry. What do you think?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/33#issuecomment-21734010\n.\n. @samalba well the thing is, sometimes I don't want to downlaod 700M just to look at the dockerfile...\n. Awesome idea!\n\nNevertheless, the metadata that docker saves for every image /already\ncontains/ all of the information necessary to recreate a Dockerfile. So\neven if it wasn't built by the docker team, it would be trivial to make the\ndockerfile available.\nOn Tue, Sep 24, 2013 at 12:16 PM, Solomon Hykes notifications@github.comwrote:\n\nThe current plan for making this happen is a feature of the index called\n\"Trusted Builds\". See the proposal at\nhttps://groups.google.com/forum/#!topic/docker-dev/NDigdAk9trc\nOn Tue, Sep 24, 2013 at 5:32 AM, Juan Batiz-Benet\nnotifications@github.comwrote:\n\nAgree with @jaredly https://github.com/jaredly\nThe Dockerfile should be pushed to the index -- it should be available\nto\nsee on the website repo listing.\n\u2014\nReply to this email directly or view it on GitHub<\nhttps://github.com/dotcloud/docker-registry/issues/33#issuecomment-24998731>\n.\n\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/33#issuecomment-25029239\n.\n. \n",
    "nisaacson": "I am running into a similar problem. The registry is running in default configuration on another vagrant machine with no authentication controls\n. This would a be a really nice addition.\n. The best would be if we could query a private registry \nQuery By Term\nGET /v1/images/search?term=foo\nQuery All with wildcard\nGET /v1/images/search?term=*\n. ",
    "rsampaio": "Same issue here but could make it push after docker login using docker index credentials (that will create a .dockercfg), still not working but doing some strace it quietly raises an exception trying to push to localhost:5000 even when the repository is registry.domain.com:80/base the output says it's Pushing with no errors.\nread(3, \"8d\\r\\n{\\\"error\\\":\\\"Failed to upload metadata: Put http://localhost:5000/v1/images/27cf784147099545/json: dial tcp 127.0.0.1:5000: connection refused\\\"}\\r\\n0\\r\\n\\r\\n\", 4096)\nLooks like a docker related issue actually.\n. ",
    "jbenet": "Agree with @jaredly \nThe Dockerfile should be pushed to the index -- it should be available to see on the website repo listing.\n. ",
    "sridatta": "@samalba  Issue: Computing the \"tarsum\" is skipped because Python 2.7 cannot read tarballs compressed with lzma compression, which Docker uses. Python issue tracker: http://bugs.python.org/issue5689\nIs checksum verification enabled in prod registry? I recall hearing that it was not. Is there any option to disable checksum verification in a private registry?\n. The problem is that the simple checksum fails every time. The temporary tarballs that are generated when pushing to the registry are slightly different from each other, so the cached checksum that docker sends is always wrong. File contents are identical but tarballs differ by 2 bytes (some kind of flag or timestamp?).\n. I just pulled Docker master and things are working. The issue I was encountering before may or may not have been real, but it doesn't matter since 0.5.2 works. Closing.\nThanks!\n. ",
    "dlintw": "\nDo you mean readme.md should be modified from 'samalba/registry' to 'samalba/docker-registry'\nWhere is the place to improve  index.docker.io's search function?\nI'll keep log when I get try again after weekend, (But I don't know should I just use the newest version, or keep on 0.5.0 to testing?)\n. By the way, I don't know how to remove the old tag, is there any method? I mean (192.168.1.1:49576/share_name)  in the following command.\n\ndocker tag my_image 192.168.1.1:49576/share_name\n. ",
    "juicefoods": "I cannot remove the following image. \n$ docker rmi localhost:8900/machinelearning \nError: Conflict, localhost:8900/machinelearning wasn't deleted\n. thanks. That's what I am doing right for the time being. Just want to make sure.\nAlso it seems I still cannot delete such images in CLI: docker rmi localhost:5000/machinelearning \n2013/08/09 15:45:21 DELETE /v1.4/images/localhost:5000/machinelearning\nError: Conflict, localhost:5000/machinelearning wasn't deleted\nI am using:\nClient version: 0.5.2\nServer version: 0.5.2\nGo version: go1.1\nThanks for your help.\n. Thanks, samalba. You're right. There are two different repos names pointing to that same image id. But I'd like to delete both of them. I will look into Docker's issue to see whether there is similar active issue. If not I will create one.\n. Just add another note to this. I tried to run both of the images using:\ndocker run -i -t df5f2d7b1bf7 /bin/bash\nand checked the size inside the two containers. They show the same size.\n. I am using the default \"dev\" mode with standard config.yml which means I didn't make any changes to this file. And I have it stored on an amazon instance that is connected to my local machine using openvpn.\n. Sam, Thanks for your immediate response.\nI checked that the directory /tmp/registry exists and permission of the directory is drwxr-xr-x 4  4.0K Aug 12 22:15 registry\n. The push actually went through. But when I pulled that repository from the private registry on the amazon instance the size of the pulled repository is twice of the original on my local machine. I created an issue reporting it just before this one. Do you know what's the reason for that? Thanks.\n. o.k. Sounds good to me. Thanks.\n. ",
    "voltagex": "You are correct. Sorry @samalba.\nMaybe it'd be better to catch that IOError and provide a different error message?\n. ",
    "bwhaley": "I ran in to this too so went ahead an opened an issue.\n. ",
    "ureyes84": "Quoting @samalba:\n\nThe calls exists in the Rest API but it's not mapped on the CLI.\n\nCould somebody point to the documentation? We'd like to remove images from our private registry but haven't found much information on this. \nI thought I had found the answer to how to delete a repository on the docs, but I think this only applies to the public registry. Please correct me if I'm wrong.\n. ",
    "brainsiq": "Also interested in how this works. I have tried using the REST API as in the docs, but for anything other than ping I get \"Method Not Allowed\", maybe because I'm not providing the authentication but I'm not sure how I need to do this.\n. ",
    "RicSwirrl": "I was trying the exact same as @bacongobbler above, but with no joy (same 405 error). I have basic auth and SSL.  What's the right way to set up private registries to allow deletes?\n. ",
    "ferrouswheel": "The only implemented DELETE method I can find in the repo is for tags:\nhttps://github.com/dotcloud/docker-registry/blob/master/registry/tags.py#L176\nDeleting images is a no-op at the moment:\nhttps://github.com/dotcloud/docker-registry/blob/master/registry/index.py#L106\nThe golang implementation seems to indicate it supports deletion:\nhttps://github.com/dotcloud/docker-registry/blob/7b5291f082c125b42f3f27a4ed87406567b8eb0f/contrib/golang_impl/README.md\nI'm not sure what the history of the golang and python implementations are.\n. ",
    "mindscratch": "+1\n. @nandarajxxx I believe your question would be better suited on IRC, the Google Group or Stackoverflow. See the community page for links to those community resources.\n. ",
    "jonbrouse": "+1 \nI am also looking to delete repositories in a private registry. \n. ",
    "tobegit3hub": "+1 :smiley: \n. Thanks @dmp42 . I'm also implementing the driver for our backend storage. It would be better to add more annotation in docker-registry-core so that I can know the meaning of all the methods.\n. Here's all the log of docker-registry.\n\n2014-11-06 19:26:04,455 WARNING: Cache storage disabled!\n2014-11-06 19:26:04,455 WARNING: LRU cache disabled!\n2014-11-06 19:26:04 [21250] [INFO] Starting gunicorn 18.0\n2014-11-06 19:26:04 [21250] [INFO] Listening at: http://0.0.0.0:5000 (21250)\n2014-11-06 19:26:04 [21250] [INFO] Using worker: gevent\n2014-11-06 19:26:04 [21257] [INFO] Booting worker with pid: 21257\n2014-11-06 19:26:04 [21260] [INFO] Booting worker with pid: 21260\n2014-11-06 19:26:04,638 WARNING: Cache storage disabled!\n2014-11-06 19:26:04,638 WARNING: LRU cache disabled!\n2014-11-06 19:26:04 [21263] [INFO] Booting worker with pid: 21263\n2014-11-06 19:26:04 [21264] [INFO] Booting worker with pid: 21264\n2014-11-06 19:26:04,721 WARNING: Cache storage disabled!\n2014-11-06 19:26:04,721 WARNING: LRU cache disabled!\n2014-11-06 19:26:04,775 WARNING: Cache storage disabled!\n2014-11-06 19:26:04,775 WARNING: LRU cache disabled!\n2014-11-06 19:26:04,786 WARNING: Cache storage disabled!\n2014-11-06 19:26:04,786 WARNING: LRU cache disabled!\n127.0.0.1 - - [06/Nov/2014:19:26:10] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-11-06 19:26:10,818 INFO: 127.0.0.1 - - [06/Nov/2014:19:26:10] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n*************** Call get_content *******************\n2014-11-06 19:26:10,824 INFO: Starting new HTTP connection (1): files.fds.api.xiaomi.com\n2014-11-06 19:26:11 [21250] [INFO] Handling signal: winch\n2014-11-06 19:26:11 [21250] [INFO] SIGWINCH ignored. Not daemonized\nGet object failed,status=404,reason=Object Not Found: null\n2014-11-06 19:26:11,118 ERROR: Exception on /v1/repositories/ubuntu/ [PUT]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1817, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1477, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1381, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1475, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1461, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 276, in wrapper\n    return f(namespace=namespace, repository=repository, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 250, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/index.py\", line 99, in put_repository\n    update_index_images(namespace, repository, flask.request.data)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/index.py\", line 61, in update_index_images\n    data = json.loads(data.decode('utf8')) + store.get_json(path)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/driver.py\", line 185, in get_json\n    return json.loads(self.get_unicode(path))\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/driver.py\", line 191, in get_unicode\n    return self.get_bytes(path).decode('utf8')\nAttributeError: 'NoneType' object has no attribute 'decode'\n\n. I have printed the log about calling get_content. When I try to push the image \"ubuntu:10.04 127.0.0.1:5000/ubuntu:10.04\", docker-regsitry immediately get_content in path \"repositories/library/ubuntu/_index_images\".\nIs this unreasonable because it hasn't put any object in the backend storage? If it does, when will it put the objects? And will it call put_content before get_content?\n. Thanks @dmp42 and it works like a charm after fixing this. It's a little confusing because I don't know I have to throw the specified error and list all directories and files in method list_directory. Maybe I will write a post about the whole processes of implementing the third-party docker-registry driver :smile: \n. I have written the post about how we implemented docker-registry driver. Hope it helps http://chendihao.cn/implement-docker-registry-driver/.\n. Thanks @MitchK . I have to say this's really what we want as well.  I hope v2 could support it easily. @dmp42 Could you tell more about the detail of the authz/authn model? Can it support authentication for each user and is it the way that docker hub is using?\n. Of course, I will update this immediately.\n. ",
    "ChauChicken": "+1\n. ",
    "ghost": "I'm actually shocked that the registry doesn't just export a local docker installation.\nIt would have been nice to just use \"docker rmi\" to delete images locally from the registry server to accomplish this.\nHooking them together would make great sense I think.\n. still seeing in on docker 0.9.1\n. Fantastic, I was about to ask this as well. Thank you!\n. Hello, \nIs this already available in the current release?\n. I am running into both the 500 invalid character < and 404 invalid character in my case a p character.  Debug Docker logs are useless.  This just started happening today.  I am noting my issue here but I am going to be opening a case with Docker's support team.  I attempted using both my Development and Integration Docker servers and they both are producing this error when I am pushing an image that I've just built.\n. Case # 5390 waiting for support to get back to me.\n. Support would like me to upgrade to 1.6 for V2 registry.  They've asked me to turn on debug and supply them the logs.  Unfortunately the logs already provided by me were with debug switched on for the daemon.  Below is their response.  I've asked if there is anything else I can do without upgrading.\n\nIt seems to me like you are using docker version 1.5. Looks like a problem with V1 registry right now. It looks like a server error to me, not an error on your side.\nCan you try pushing with docker version 1.6 or above ( which uses V2 registry )?\nIf you cannot upgrade, can you please run your docker daemon in debug mode ( with -D -d ) and then reproduce the same error and send the daemon logs to us? We can check with the issue that way.\n\n. I typo'd 'storage_path'  as 'repository_path'.\nMy current configuration is sample configuration (/docker-registry/config/config_sample.yml).\n. v0.7.2 is work .  3q\n. I'd definitely say so, yes.\n. @dmp42 Sorry, should have mentioned I did all of those. The Docker daemon is certainly hitting the registry as I can see it GETing the layers and the json files, thats what made me post the issue here as it leads me to believe its an issue with the registry, not Docker.\nAs per your comment on the different sizes in v1 vs v2, It doesn't seem to look that way with the sizes Docker reports when pulling the layers. In V1 it says layer 5bc37dc2dfba is 67MB, in V2 it says its over 200MB (and actually downloads 200MB too). Not sure whats going on there.\nV1:\nconsole\n$ docker pull ubuntu\nPulling repository ubuntu\n86ce37374f40: Pulling dependent layers\n511136ea3c5a: Download complete\n5bc37dc2dfba: Downloading [====>                                              ] 5.933 MB/67.48 MB 41s\nV2:\nconsole\n$ docker pull ubuntu\nubuntu:latest: The image you are pulling has been verified\n5bc37dc2dfba: Downloading [====>                                              ] 17.56 MB/201.6 MB 1m9s\n61cb619d86bc: Download complete\n3f45ca85fedc: Download complete\n78e82ee876a2: Download complete\ndc07507cef42: Download complete\n86ce37374f40: Download complete\n511136ea3c5a: Already exists\n. @dmp42 Yes, as far as I can tell, it is. Here is my docker registry logs (an extract at least):\n172.17.42.1 - - [25/Nov/2014:13:46:41 +0000] \"GET /v1/images/d497ad3926c8997e1e0de74cdd5285489bb2c4acd6db15292e04bbab07047cd0/json HTTP/1.1\" 200 1479 \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/39fa2fa kernel/3.17.0-119 os/linux arch/arm\"\n25/Nov/2014:13:46:41 +0000 DEBUG: args = {'image_id': u'd497ad3926c8997e1e0de74cdd5285489bb2c4acd6db15292e04bbab07047cd0'}\n25/Nov/2014:13:46:41 +0000 DEBUG: Auth Token = Token signature=xxxxxxxxxxxxxxxxxxxxxxxxxx,repository=\"library/ubuntu\",access=read\n25/Nov/2014:13:46:41 +0000 DEBUG: auth = {u'access': u'read', u'repository': u'library/ubuntu', u'signature': u'xxxxxxxxxxxxxxxxxxxxxxxxxx'}\n25/Nov/2014:13:46:41 +0000 INFO: Starting new HTTPS connection (1): index.docker.io\n25/Nov/2014:13:46:42 +0000 DEBUG: \"GET /v1/repositories/library/ubuntu/images HTTP/1.1\" 200 None\n25/Nov/2014:13:46:42 +0000 DEBUG: validate_token: Index returned 200\n25/Nov/2014:13:46:42 +0000 DEBUG: Source provided, registry acts as mirror\n25/Nov/2014:13:46:42 +0000 DEBUG: Status code is not 404, no source lookup required\n172.17.42.1 - - [25/Nov/2014:13:47:06 +0000] \"GET /v1/images/d497ad3926c8997e1e0de74cdd5285489bb2c4acd6db15292e04bbab07047cd0/layer HTTP/1.1\" 200 67487454 \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/39fa2fa kernel/3.17.0-119 os/linux arch/arm\"\n25/Nov/2014:13:47:16 +0000 DEBUG: args = {'image_id': u'ccb62158e97068cc05b2f0927a8acde14c64d0d363cc448238357fe221a39699'}\n25/Nov/2014:13:47:16 +0000 DEBUG: Auth Token = Token signature=xxxxxxxxxxxxxxxxxxxxxxxxxx,repository=\"library/ubuntu\",access=read\n25/Nov/2014:13:47:16 +0000 DEBUG: auth = {u'access': u'read', u'repository': u'library/ubuntu', u'signature': u'xxxxxxxxxxxxxxxxxxxxxxxxxx'}\n25/Nov/2014:13:47:16 +0000 INFO: Starting new HTTPS connection (1): index.docker.io\n25/Nov/2014:13:47:17 +0000 DEBUG: \"GET /v1/repositories/library/ubuntu/images HTTP/1.1\" 200 None\n25/Nov/2014:13:47:17 +0000 DEBUG: validate_token: Index returned 200\n25/Nov/2014:13:47:17 +0000 DEBUG: Source provided, registry acts as mirror\n25/Nov/2014:13:47:17 +0000 DEBUG: Status code is not 404, no source lookup required\n172.17.42.1 - - [25/Nov/2014:13:47:17 +0000] \"GET /v1/images/ccb62158e97068cc05b2f0927a8acde14c64d0d363cc448238357fe221a39699/json HTTP/1.1\" 200 2687 \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/39fa2fa kernel/3.17.0-119 os/linux arch/arm\"\n25/Nov/2014:13:47:17 +0000 DEBUG: args = {'image_id': u'ccb62158e97068cc05b2f0927a8acde14c64d0d363cc448238357fe221a39699'}\n25/Nov/2014:13:47:17 +0000 DEBUG: Auth Token = Token signature=xxxxxxxxxxxxxxxxxxxxxxxxxx,repository=\"library/ubuntu\",access=read\n25/Nov/2014:13:47:17 +0000 DEBUG: auth = {u'access': u'read', u'repository': u'library/ubuntu', u'signature': u'xxxxxxxxxxxxxxxxxxxxxxxxxx'}\n25/Nov/2014:13:47:17 +0000 INFO: Starting new HTTPS connection (1): index.docker.io\n25/Nov/2014:13:47:17 +0000 DEBUG: \"GET /v1/repositories/library/ubuntu/images HTTP/1.1\" 200 None\n25/Nov/2014:13:47:17 +0000 DEBUG: validate_token: Index returned 200\n25/Nov/2014:13:47:17 +0000 DEBUG: Source provided, registry acts as mirror\n25/Nov/2014:13:47:17 +0000 DEBUG: Status code is not 404, no source lookup required\n172.17.42.1 - - [25/Nov/2014:13:47:17 +0000] \"GET /v1/images/ccb62158e97068cc05b2f0927a8acde14c64d0d363cc448238357fe221a39699/layer HTTP/1.1\" 200 71467 \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/39fa2fa kernel/3.17.0-119 os/linux arch/arm\"\n25/Nov/2014:13:47:18 +0000 DEBUG: args = {'image_id': u'e791be0477f28fd52f7609aed81733427d4cc0da620962d072e18ebcb32720a4'}\n25/Nov/2014:13:47:18 +0000 DEBUG: Auth Token = Token signature=xxxxxxxxxxxxxxxxxxxxxxxxxx,repository=\"library/ubuntu\",access=read\n25/Nov/2014:13:47:18 +0000 DEBUG: auth = {u'access': u'read', u'repository': u'library/ubuntu', u'signature': u'xxxxxxxxxxxxxxxxxxxxxxxxxx'}\n25/Nov/2014:13:47:18 +0000 INFO: Starting new HTTPS connection (1): index.docker.io\n25/Nov/2014:13:47:19 +0000 DEBUG: \"GET /v1/repositories/library/ubuntu/images HTTP/1.1\" 200 None\n25/Nov/2014:13:47:19 +0000 DEBUG: validate_token: Index returned 200\n25/Nov/2014:13:47:19 +0000 DEBUG: Source provided, registry acts as mirror\n25/Nov/2014:13:47:19 +0000 DEBUG: Status code is not 404, no source lookup required\n172.17.42.1 - - [25/Nov/2014:13:47:19 +0000] \"GET /v1/images/e791be0477f28fd52f7609aed81733427d4cc0da620962d072e18ebcb32720a4/json HTTP/1.1\" 200 1451 \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/39fa2fa kernel/3.17.0-119 os/linux arch/arm\"\n25/Nov/2014:13:47:19 +0000 DEBUG: args = {'image_id': u'e791be0477f28fd52f7609aed81733427d4cc0da620962d072e18ebcb32720a4'}\n25/Nov/2014:13:47:19 +0000 DEBUG: Auth Token = Token signature=xxxxxxxxxxxxxxxxxxxxxxxxxx,repository=\"library/ubuntu\",access=read\n25/Nov/2014:13:47:19 +0000 DEBUG: auth = {u'access': u'read', u'repository': u'library/ubuntu', u'signature': u'xxxxxxxxxxxxxxxxxxxxxxxxxx'}\n25/Nov/2014:13:47:19 +0000 INFO: Starting new HTTPS connection (1): index.docker.io\n25/Nov/2014:13:47:19 +0000 DEBUG: \"GET /v1/repositories/library/ubuntu/images HTTP/1.1\" 200 None\n25/Nov/2014:13:47:19 +0000 DEBUG: validate_token: Index returned 200\n25/Nov/2014:13:47:19 +0000 DEBUG: Source provided, registry acts as mirror\n25/Nov/2014:13:47:19 +0000 DEBUG: Status code is not 404, no source lookup required\n172.17.42.1 - - [25/Nov/2014:13:47:19 +0000] \"GET /v1/images/e791be0477f28fd52f7609aed81733427d4cc0da620962d072e18ebcb32720a4/layer HTTP/1.1\" 200 586 \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/39fa2fa kernel/3.17.0-119 os/linux arch/arm\"\n25/Nov/2014:13:47:20 +0000 DEBUG: args = {'image_id': u'3680052c0f5cf8ecb86ddf4d6ed331c89cdb691554572a80ec04724cf6ee9436'}\n25/Nov/2014:13:47:20 +0000 DEBUG: Auth Token = Token signature=xxxxxxxxxxxxxxxxxxxxxxxxxx,repository=\"library/ubuntu\",access=read\n25/Nov/2014:13:47:20 +0000 DEBUG: auth = {u'access': u'read', u'repository': u'library/ubuntu', u'signature': u'xxxxxxxxxxxxxxxxxxxxxxxxxx'}\n25/Nov/2014:13:47:20 +0000 INFO: Starting new HTTPS connection (1): index.docker.io\n25/Nov/2014:13:47:20 +0000 DEBUG: \"GET /v1/repositories/library/ubuntu/images HTTP/1.1\" 200 None\n25/Nov/2014:13:47:20 +0000 DEBUG: validate_token: Index returned 200\n25/Nov/2014:13:47:20 +0000 DEBUG: Source provided, registry acts as mirror\n25/Nov/2014:13:47:20 +0000 DEBUG: Status code is not 404, no source lookup required\n172.17.42.1 - - [25/Nov/2014:13:47:20 +0000] \"GET /v1/images/3680052c0f5cf8ecb86ddf4d6ed331c89cdb691554572a80ec04724cf6ee9436/json HTTP/1.1\" 200 1492 \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/39fa2fa kernel/3.17.0-119 os/linux arch/arm\"\n25/Nov/2014:13:47:20 +0000 DEBUG: args = {'image_id': u'3680052c0f5cf8ecb86ddf4d6ed331c89cdb691554572a80ec04724cf6ee9436'}\n25/Nov/2014:13:47:20 +0000 DEBUG: Auth Token = Token signature=xxxxxxxxxxxxxxxxxxxxxxxxxx,repository=\"library/ubuntu\",access=read\n25/Nov/2014:13:47:20 +0000 DEBUG: auth = {u'access': u'read', u'repository': u'library/ubuntu', u'signature': u'xxxxxxxxxxxxxxxxxxxxxxxxxx'}\n25/Nov/2014:13:47:20 +0000 INFO: Starting new HTTPS connection (1): index.docker.io\n25/Nov/2014:13:47:21 +0000 DEBUG: \"GET /v1/repositories/library/ubuntu/images HTTP/1.1\" 200 None\n25/Nov/2014:13:47:21 +0000 DEBUG: validate_token: Index returned 200\n25/Nov/2014:13:47:21 +0000 DEBUG: Source provided, registry acts as mirror\n25/Nov/2014:13:47:21 +0000 DEBUG: Status code is not 404, no source lookup required\n172.17.42.1 - - [25/Nov/2014:13:47:21 +0000] \"GET /v1/images/3680052c0f5cf8ecb86ddf4d6ed331c89cdb691554572a80ec04724cf6ee9436/layer HTTP/1.1\" 200 818 \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/39fa2fa kernel/3.17.0-119 os/linux arch/arm\"\n25/Nov/2014:13:47:21 +0000 DEBUG: args = {'image_id': u'22093c35d77bb609b9257ffb2640845ec05018e3d96cb939f68d0e19127f1723'}\n25/Nov/2014:13:47:21 +0000 DEBUG: Auth Token = Token signature=xxxxxxxxxxxxxxxxxxxxxxxxxx,repository=\"library/ubuntu\",access=read\n25/Nov/2014:13:47:21 +0000 DEBUG: auth = {u'access': u'read', u'repository': u'library/ubuntu', u'signature': u'xxxxxxxxxxxxxxxxxxxxxxxxxx'}\n25/Nov/2014:13:47:21 +0000 INFO: Starting new HTTPS connection (1): index.docker.io\n25/Nov/2014:13:47:22 +0000 DEBUG: \"GET /v1/repositories/library/ubuntu/images HTTP/1.1\" 200 None\n25/Nov/2014:13:47:22 +0000 DEBUG: validate_token: Index returned 200\n25/Nov/2014:13:47:22 +0000 DEBUG: Source provided, registry acts as mirror\n25/Nov/2014:13:47:22 +0000 DEBUG: Status code is not 404, no source lookup required\n172.17.42.1 - - [25/Nov/2014:13:47:22 +0000] \"GET /v1/images/22093c35d77bb609b9257ffb2640845ec05018e3d96cb939f68d0e19127f1723/json HTTP/1.1\" 200 1522 \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/39fa2fa kernel/3.17.0-119 os/linux arch/arm\"\n25/Nov/2014:13:47:22 +0000 DEBUG: args = {'image_id': u'22093c35d77bb609b9257ffb2640845ec05018e3d96cb939f68d0e19127f1723'}\n25/Nov/2014:13:47:22 +0000 DEBUG: Auth Token = Token signature=xxxxxxxxxxxxxxxxxxxxxxxxxx,repository=\"library/ubuntu\",access=read\n25/Nov/2014:13:47:22 +0000 DEBUG: auth = {u'access': u'read', u'repository': u'library/ubuntu', u'signature': u'xxxxxxxxxxxxxxxxxxxxxxxxxx'}\n25/Nov/2014:13:47:22 +0000 INFO: Starting new HTTPS connection (1): index.docker.io\n25/Nov/2014:13:47:23 +0000 DEBUG: \"GET /v1/repositories/library/ubuntu/images HTTP/1.1\" 200 None\n25/Nov/2014:13:47:23 +0000 DEBUG: validate_token: Index returned 200\n25/Nov/2014:13:47:23 +0000 DEBUG: Source provided, registry acts as mirror\n25/Nov/2014:13:47:23 +0000 DEBUG: Status code is not 404, no source lookup required\n172.17.42.1 - - [25/Nov/2014:13:47:23 +0000] \"GET /v1/images/22093c35d77bb609b9257ffb2640845ec05018e3d96cb939f68d0e19127f1723/layer HTTP/1.1\" 200 1910657 \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/39fa2fa kernel/3.17.0-119 os/linux arch/arm\"\n25/Nov/2014:13:47:25 +0000 DEBUG: args = {'image_id': u'5506de2b643be1e6febbf3b8a240760c6843244c41e12aa2f60ccbb7153d17f5'}\n25/Nov/2014:13:47:25 +0000 DEBUG: Auth Token = Token signature=xxxxxxxxxxxxxxxxxxxxxxxxxx,repository=\"library/ubuntu\",access=read\n25/Nov/2014:13:47:25 +0000 DEBUG: auth = {u'access': u'read', u'repository': u'library/ubuntu', u'signature': u'xxxxxxxxxxxxxxxxxxxxxxxxxx'}\n25/Nov/2014:13:47:25 +0000 INFO: Starting new HTTPS connection (1): index.docker.io\n25/Nov/2014:13:47:26 +0000 DEBUG: \"GET /v1/repositories/library/ubuntu/images HTTP/1.1\" 200 None\n25/Nov/2014:13:47:26 +0000 DEBUG: validate_token: Index returned 200\n25/Nov/2014:13:47:26 +0000 DEBUG: Source provided, registry acts as mirror\n25/Nov/2014:13:47:26 +0000 DEBUG: Status code is not 404, no source lookup required\n172.17.42.1 - - [25/Nov/2014:13:47:26 +0000] \"GET /v1/images/5506de2b643be1e6febbf3b8a240760c6843244c41e12aa2f60ccbb7153d17f5/json HTTP/1.1\" 200 1454 \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/39fa2fa kernel/3.17.0-119 os/linux arch/arm\"\n25/Nov/2014:13:47:26 +0000 DEBUG: args = {'image_id': u'5506de2b643be1e6febbf3b8a240760c6843244c41e12aa2f60ccbb7153d17f5'}\n25/Nov/2014:13:47:26 +0000 DEBUG: Auth Token = Token signature=xxxxxxxxxxxxxxxxxxxxxxxxxx,repository=\"library/ubuntu\",access=read\n25/Nov/2014:13:47:26 +0000 DEBUG: auth = {u'access': u'read', u'repository': u'library/ubuntu', u'signature': u'xxxxxxxxxxxxxxxxxxxxxxxxxx'}\n25/Nov/2014:13:47:26 +0000 INFO: Starting new HTTPS connection (1): index.docker.io\n25/Nov/2014:13:47:26 +0000 DEBUG: \"GET /v1/repositories/library/ubuntu/images HTTP/1.1\" 200 None\n25/Nov/2014:13:47:26 +0000 DEBUG: validate_token: Index returned 200\n25/Nov/2014:13:47:26 +0000 DEBUG: Source provided, registry acts as mirror\n25/Nov/2014:13:47:26 +0000 DEBUG: Status code is not 404, no source lookup required\n172.17.42.1 - - [25/Nov/2014:13:47:26 +0000] \"GET /v1/images/5506de2b643be1e6febbf3b8a240760c6843244c41e12aa2f60ccbb7153d17f5/layer HTTP/1.1\" 200 185 \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/39fa2fa kernel/3.17.0-119 os/linux arch/arm\"\n. In case you want to do any testing, feel free to test against the mirror running here: 212.47.228.118:5000\n. @dmp42 I don't think there is any difference in I/O overhead between the wget and Dockers download. Reason being is that the \"extracting\" (untaring and expanding) the layer happens in a different stage and is not part of my timing in the download stage (the download stage takes up 40s of the 51s of the docker pull times). This is what stumped me, its clearly \"downloading\" the image at the same speed.\nHere are the time differences between pulling the ubuntu image on v1 and v2\nV1:\n``` console\ntime docker pull ubuntu\nPulling repository ubuntu\n86ce37374f40: Pulling dependent layers\n86ce37374f40: Download complete\n5bc37dc2dfba: Download complete\n61cb619d86bc: Download complete\n3f45ca85fedc: Download complete\n78e82ee876a2: Download complete\ndc07507cef42: Download complete\nStatus: Downloaded newer image for ubuntu:latest\nreal    0m53.689s\nuser    0m0.160s\nsys     0m0.070s\n```\nV2:\n``` console\ntime docker pull ubuntu\nubuntu:latest: The image you are pulling has been verified\n5bc37dc2dfba: Pull complete\n61cb619d86bc: Pull complete\n3f45ca85fedc: Pull complete\n78e82ee876a2: Pull complete\ndc07507cef42: Pull complete\n86ce37374f40: Pull complete\n511136ea3c5a: Already exists\nStatus: Downloaded newer image for ubuntu:latest\nreal    1m19.904s\nuser    0m0.610s\nsys     0m0.210s\n```\nIt might be worth noting that the way I'm forcing it to use the V1 code path is by specifying the following flag to force it to use V1 against the official registry: --registry-mirror=https://index.docker.io. As far as I can tell, this works perfectly fine and does the same thing it would normally do against V1.\nAlso, I'm not sure if its intended or not with the V2 code path, but under V2 it only pulls the \"latest\" tag, whereas under V1 it would pull all the tags that point to the same layer (eg. latest, trusty, 14.04.1, 14.04).\n. I'll also add that the expanded image size when installed is only 192MB. That means that one layer its downloading is actually bigger than the image after being decompressed and extracted... Something about that doesn't sound right to me\nREPOSITORY          TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nubuntu              latest              86ce37374f40        4 hours ago         192.7 MB\n. @dmp42 Sadly, this isn't Disk I/O related (as far as I can tell). I just downloaded the layer manually and untared it and it untars in about 14 seconds. 14 seconds + the 3 seconds to download it from the local mirror should still be way faster than the 52 seconds it takes to download it from the official registry.\n``` console\n$ time tar xzf layer\nreal    0m12.810s\nuser    0m0.580s\nsys     0m1.870s\n```\nWhen I said I used the mirror to force it to V1, I meant I listed the normal registry (index.docker.io) in the registry-mirror flag so it would force it down a V1 codepath but still use the official registry.\nRE: The size issue, it still doesn't explain why that layer is 200MB in V2 and 67MB in V1...\n@unclejack \nconsole\n$ docker info\nContainers: 26\nImages: 122\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Dirs: 174\nExecution Driver: native-0.2\nKernel Version: 3.17.0-119\nOperating System: Ubuntu 14.10\nUsername: xeoncore\nRegistry: [https://index.docker.io/v1/]\nThe storage is SSD, RAM is 2GB, and network link is irrelevant considering its using localhost as the mirror,\nconsole\n$ dd bs=1M count=256 if=/dev/zero of=test conv=fdatasync\n256+0 records in\n256+0 records out\n268435456 bytes (268 MB) copied, 3.34805 s, 80.2 MB/s\nconsole\n$ cat /proc/mounts\nrootfs / rootfs rw 0 0\nnone /sys sysfs rw,nosuid,nodev,noexec,relatime 0 0\nnone /proc proc rw,nosuid,nodev,noexec,relatime 0 0\nnone /dev devtmpfs rw,relatime,size=1032156k,nr_inodes=186351,mode=755 0 0\nnone /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=000 0 0\n/dev/nbd0 / ext4 rw,relatime,data=ordered 0 0\nnone /sys/fs/cgroup tmpfs rw,relatime,size=4k,mode=755 0 0\nnone /sys/kernel/debug debugfs rw,relatime 0 0\nnone /run tmpfs rw,nosuid,noexec,relatime,size=206928k,mode=755 0 0\nnone /run/lock tmpfs rw,nosuid,nodev,noexec,relatime,size=5120k 0 0\nnone /run/shm tmpfs rw,nosuid,nodev,relatime 0 0\nnone /run/user tmpfs rw,nosuid,nodev,noexec,relatime,size=102400k,mode=755 0 0\ncgroup /sys/fs/cgroup/cpuset cgroup rw,relatime,cpuset 0 0\ncgroup /sys/fs/cgroup/cpu cgroup rw,relatime,cpu 0 0\ncgroup /sys/fs/cgroup/cpuacct cgroup rw,relatime,cpuacct 0 0\ncgroup /sys/fs/cgroup/memory cgroup rw,relatime,memory 0 0\ncgroup /sys/fs/cgroup/devices cgroup rw,relatime,devices 0 0\ncgroup /sys/fs/cgroup/freezer cgroup rw,relatime,freezer 0 0\ncgroup /sys/fs/cgroup/net_cls cgroup rw,relatime,net_cls 0 0\ncgroup /sys/fs/cgroup/blkio cgroup rw,relatime,blkio 0 0\ncgroup /sys/fs/cgroup/perf_event cgroup rw,relatime,perf_event 0 0\n/dev/loop2 /containers/fedora/mnt ext4 rw,relatime,data=ordered 0 0\ntmpfs /tmp tmpfs rw,nosuid,nodev,relatime 0 0\n/dev/nbd0 /var/lib/docker/aufs ext4 rw,relatime,data=ordered 0 0\nnone /var/lib/docker/aufs/mnt/46045e5cc790539586299b37ef6f5810c1b14a65dbbec6831ab0c375210abd8a aufs rw,relatime,si=da207b9a 0 0\n. @dmp42 \n\ndocker does a lot more than just untarring a tar file to the local filesystem.\n\nI understand that to be the case, but when I download from localhost at 30MB/s and I download  from the registry at 1MB/s, I expect there to be a slightly bigger difference in runtime than 2 seconds for the docker pull command. I understand it won't get down to like 3 seconds, but 49s vs 52s clearly shows that there is something going on that isn't right. \n\nI believe compression settings are differents. I'll have a look next monday to that.\n\nBut doesn't the local docker daemon compress it before uploading the layer? So it shouldn't really matter what version the API is running?\n. Sadly its aufs :P\n. @unclejack \nFrom Mirror:\n``` console\n\ndocker pull fedora:latest\n\nPulling repository fedora\n7d3f07f8de5f: Download complete\n511136ea3c5a: Download complete\n782cf93a8f16: Download complete\nStatus: Downloaded newer image for fedora:latest\nreal    1m4.438s\nuser    0m0.190s\nsys     0m0.110s\n\ndocker save -o /tmp/fedora-latest.tar fedora:latest\n\nreal    0m21.532s\nuser    0m1.400s\nsys     0m3.170s\n\ndocker rmi fedora:latest\n\nUntagged: fedora:latest\nreal    0m0.522s\nuser    0m0.050s\nsys     0m0.050s\n\ndocker load -i /tmp/fedora-latest.tar\n\nreal    0m13.228s\nuser    0m1.360s\nsys     0m3.300s\n```\nFrom Official Registry:\n``` console\n\ndocker pull fedora:latest\n\nPulling repository fedora\n7d3f07f8de5f: Download complete\n511136ea3c5a: Download complete\n782cf93a8f16: Download complete\nStatus: Downloaded newer image for fedora:latest\nreal    1m25.054s\nuser    0m0.230s\nsys     0m0.060s\n\ndocker save -o /tmp/fedora-latest.tar fedora:latest\n\nreal    0m19.486s\nuser    0m0.900s\nsys     0m3.220s\n\ndocker rmi fedora:latest\n\nUntagged: fedora:latest\nreal    0m0.544s\nuser    0m0.080s\nsys     0m0.020s\n\ndocker load -i /tmp/fedora-latest.tar\n\nreal    0m11.951s\nuser    0m1.360s\nsys     0m3.170s\n```\n. @unclejack Whilst I understand that the speed of the Disk IO is slow (I know that already), the times posted above clearly show that it is not the issue here.\nIf what you are saying is true when mirroring, then downloading it, decompressing it, and untarring it all happen in a \"pipe\", in parallel. This effectively means that the speed will be equal to the slowest \"link\" in the pipe. To test this, I whipped up a test script that simulated all the things docker does but using native commands instead to see if the times were remotely similar (it doesn't test aufs overhead tho):\n``` console\n$ time curl -s -H 'Authorization: Token signature=xxxxx,repository=\"library/fedora\",access=read' http://localhost:5000/v1/images/7d3f07f8de5fb3a20c6cb1e4447773a5741e3641c1aa093366eaa0fc690c6417/layer | gzip -cd - | tar -x -C /home/temp\nreal    0m26.638s\nuser    0m14.420s\nsys     0m14.900s\n```\nI highly doubt that aufs is adding an overhead of almost 40 seconds to the pull, or am I wrong? I'm not trying to replicate the entire pull operation here, just trying to prove that Disk I/O and compression are not the reason here. However, I think we can safely assume aufs overhead isn't 40 seconds because docker load (which has to untar and import into aufs) completes in 12 seconds.\n. I wrote a bit of a simulation script that replicates a docker pull but purely in bash to test the performance. Whilst its obviously still dependent on Disk I/O, its a heck of a lot faster than a docker pull:\n``` bash\n!/bin/bash\nA bash script that is designed to simulate a docker pull.\nMIRROR=\"$1\"\nREPOSITORY=$(echo \"$2\" | awk -F: '{ printf $1 }')\nEXPORT_NAME=\"$REPOSITORY\"\nTAG=$(echo \"$2\" | awk -F: '{ printf $2 }')\nTAG=\"${TAG:-latest}\"\nif [[ $(dirname \"$REPOSITORY\") == \".\" ]]\nthen\n        REPOSITORY=\"library/$REPOSITORY\"\nfi\nPreperation: get auth token and other metadata\ndeclare -a FILES\nSIGNATURE=$(curl -isH \"X-Docker-Token: true\" \"https://index.docker.io/v1/repositories/$REPOSITORY/images\" | awk '{print $2}' | grep signature | tr -d \"\\r\\n\")\nIMAGE=$(curl -sH \"Authorization: Token $SIGNATURE\" \"$MIRROR/v1/repositories/$REPOSITORY/tags/$TAG\" | tr -d '\"')\nLAYERS=( $(curl -sH \"Authorization: Token $SIGNATURE\" \"$MIRROR/v1/images/$IMAGE/ancestry\" | tr -d '\"[],' | awk '{ for (i=NF;i>0;i--) { printf $i\" \" } }') )\nprintf \"Importing:\\n\\tRepository: %s\\n\\tTag: %s\\n\\tImage: %s\\n\\tLayers: %s\\n\\tMirror: %s\\n\" $REPOSITORY $TAG $IMAGE \"$(echo \"${LAYERS[*]}\" | awk '{ i=NF; for (;i>1;i--) { printf $i\"\\n\\t\\t\" } printf $i }')\" $MIRROR\nFile Preperation\nTEMP=$(mktemp -d)\npushd $TEMP > /dev/null\nfor LAYER in \"${LAYERS[@]}\"\ndo\n        mkdir \"$LAYER\"\n    echo \"1.0\" > \"$LAYER/VERSION\"\n\n    # Download the json metadata\n    curl -#LH \"Authorization: Token $SIGNATURE\" \"$MIRROR/v1/images/$LAYER/json\" > \"$LAYER/json\"\n\n    # Download and ungzip the layer\n    curl -#LH \"Authorization: Token $SIGNATURE\" \"$MIRROR/v1/images/$LAYER/layer\" > \"$LAYER/layer.tar\"\n    [[ $(file -b \"$LAYER/layer.tar\") =~ ^gzip ]] && { mv \"$LAYER/layer.tar\" \"$LAYER/layer.tar.gz\"; gzip -d \"$LAYER/layer.tar.gz\"; }\n\n    FILES+=( $LAYER/{VERSION,json,layer.tar} )\n\ndone\necho \"{\\\"$EXPORT_NAME\\\":{\\\"$TAG\\\":\\\"$IMAGE\\\"}}\" > repositories\nFILES+=( \"repositories\" )\nAction: this creates a tar and loads it into docker, faster than piping it apparently.\ntar -cvf image.tar \"${FILES[@]}\"\ndocker load -i image.tar\nCleanup\npopd > /dev/null\nrm -fr $TEMP\n```\nRun against my mirror:\n``` console\ntime ./simulated-pull.sh http://localhost:5000 fedora:latest\nImporting:\n        Repository: library/fedora\n        Tag: latest\n        Image: 7d3f07f8de5fb3a20c6cb1e4447773a5741e3641c1aa093366eaa0fc690c6417\n        Layers: 7d3f07f8de5fb3a20c6cb1e4447773a5741e3641c1aa093366eaa0fc690c6417\n                782cf93a8f16d3016dae352188cd5cfedb6a15c37d4dbd704399f02d1bb89dab\n                511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\n        Mirror: http://localhost:5000\n################################################################## 100.0%\n################################################################## 100.0%\n################################################################## 100.0%\n################################################################## 100.0%\n################################################################## 100.0%\n################################################################## 100.0%\n511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/VERSION\n511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json\n511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/layer.tar\n782cf93a8f16d3016dae352188cd5cfedb6a15c37d4dbd704399f02d1bb89dab/VERSION\n782cf93a8f16d3016dae352188cd5cfedb6a15c37d4dbd704399f02d1bb89dab/json\n782cf93a8f16d3016dae352188cd5cfedb6a15c37d4dbd704399f02d1bb89dab/layer.tar\n7d3f07f8de5fb3a20c6cb1e4447773a5741e3641c1aa093366eaa0fc690c6417/VERSION\n7d3f07f8de5fb3a20c6cb1e4447773a5741e3641c1aa093366eaa0fc690c6417/json\n7d3f07f8de5fb3a20c6cb1e4447773a5741e3641c1aa093366eaa0fc690c6417/layer.tar\nrepositories\nreal    0m39.094s\nuser    0m14.360s\nsys     0m8.110s\n```\nAgainst the docker registry:\n``` console\ntime ./simulated-pull.sh https://registry-1.docker.io fedora:latest\nImporting:\n        Repository: library/fedora\n        Tag: latest\n        Image: 7d3f07f8de5fb3a20c6cb1e4447773a5741e3641c1aa093366eaa0fc690c6417\n        Layers: 7d3f07f8de5fb3a20c6cb1e4447773a5741e3641c1aa093366eaa0fc690c6417\n                782cf93a8f16d3016dae352188cd5cfedb6a15c37d4dbd704399f02d1bb89dab\n                511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\n        Mirror: https://registry-1.docker.io\n################################################################## 100.0%\n################################################################## 100.0%\n################################################################## 100.0%\n################################################################## 100.0%\n################################################################## 100.0%\n################################################################## 100.0%\n511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/VERSION\n511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json\n511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/layer.tar\n782cf93a8f16d3016dae352188cd5cfedb6a15c37d4dbd704399f02d1bb89dab/VERSION\n782cf93a8f16d3016dae352188cd5cfedb6a15c37d4dbd704399f02d1bb89dab/json\n782cf93a8f16d3016dae352188cd5cfedb6a15c37d4dbd704399f02d1bb89dab/layer.tar\n7d3f07f8de5fb3a20c6cb1e4447773a5741e3641c1aa093366eaa0fc690c6417/VERSION\n7d3f07f8de5fb3a20c6cb1e4447773a5741e3641c1aa093366eaa0fc690c6417/json\n7d3f07f8de5fb3a20c6cb1e4447773a5741e3641c1aa093366eaa0fc690c6417/layer.tar\nrepositories\nreal    1m12.196s\nuser    0m14.630s\nsys     0m8.380s\n```\n. Yep, using AUFS driver :smile:\n. Hi, I can confirm this, as I've just run into the exact same problem. \nPrior to upgrading to 1.5, I had EXTRA_ARGS=\"--insecure-registry 192.168.2.5:5000\" in /etc/sysconfig/docker and this worked. Post the upgrade I get the same FATA[00040] Error. \nI changed the entry to -> EXTRA_ARGS=\"--insecure-registry http://192.168.2.5:5000\" in /etc/sysconfig/docker, restarted and its working again.\nI'm running this on Cent OS 6.5\nPush is failing....\n. It looks like the problem was solved in the docker hub side. All the envs are working fine.\n. ",
    "tfhartmann": "+1\n. ",
    "schmunk42": "+1\n. As a workaround you could also use this docker image, which offers a delete button for a repo.\n. @vincentvanouytsel Uh, to be honest, I didn't check that, sorry. I just saw that the image is not listed anymore.\n. I wanted to mention a similar workaround :) with this one https://github.com/chadoe/docker-cleanup-volumes\n. I am using the registry in a fig.yml and only if I use \nenvironment:\n  GUNICORN_OPTS: \"[--preload]\"\nI can safely start the image, otherwise it fails with the worker errors mentioned above.\n@dmp42 Since I am also using S3, do you have any additional information how to avoid issues with the above options ans S3?\n. ",
    "szaffarano": "+1\n. ",
    "ankushagarwal": "+1 My registry size is getting out of control. I want to delete old images from the registry. There is no way to do this via API today.\n. @stevvooe : I am running the registry locally on my Mac in boot2docker with 50GB space and it ran out of space (Yes, I pushed a lot of images each about 1G in size).\n. I have switched to Quay.io's private registry for now which has a nice API to delete tags (It then deletes all images which are not reachable by any tags).\n. I was just hoping that there was a inbuilt API to remove unused images,\ninstead of running an external script which needs maintenance.\n\u1427\nOn Fri, Feb 13, 2015 at 5:37 PM, Pawel notifications@github.com wrote:\n\nyou can remove entire repository using curl:\ncurl -X DELETE /v1/repositories//\nto remove unused images you have to remove tag first:\n$ curl -X DELETE /v1/repositories//tags/\nthen run this script:\nhttps://gist.github.com/kwk/c5443f2a1abcf0eb1eaa\non docker registy server, make sure you install 'jq' package first\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/45#issuecomment-74244366\n.\n\n\nAnkush Agarwal\nhttp://ankush.io\n@ankushio https://twitter.com/ankushio\nThere are 10 types of people in this world. Those who understand binary\nand those who don't.\n. I have seen similar issues(#696, #829, #796, etc.) related to registry startup in Issues, but I was not sure if this was exactly the same as none of them throw the error [Errno 2] No such file or directory: './registry._setup_database.lock'\n. Tried reproducing it without any success.\n. It happens sometimes. I can't reproduce it deterministically.\n. I can reproduce it deterministically.\nWhenever I restart boot2docker and do a docker start registry. It happens and the registry container crashes. I then do a docker start registry for the second time and it works fine.\nThis is my registry config. The only change I have made from the default is on line 214\nThis is my b2d version :\n$ boot2docker version\nBoot2Docker-cli version: v1.4.1\nGit commit: 43241cb\n. ",
    "murali44": "+1\n. ",
    "kiwenlau": "+1\n. ",
    "svenkroll": "+1\n. ",
    "mrh666": "+2!\n. ",
    "stevvooe": "@ankushagarwal when you say your registry size is out of control, what size of data you referring to? How big is too much?\n. @jadedfire Yes, coding from an armchair is quite easy.\nWe are indeed looking into this set of solutions. There are a number of ways a registry can be deployed and we'd like to have a solution that works well for everyone, rather than restricting deletion to a narrow set of deployments.\nIf you believe the current analysis is flawed, I welcome your proposal, an implementation or any modifications that would help us to achieve this.\n. @ripienaar To help others understand why deletes are non-trivial, we put together an analysis. To have it dismissed off-hand, without consideration for the detailed points covered, is both insulting and inconsiderate, especially to those working at finding a solution.\nIf you or @jadedfire believe deletes to be straightforward, I invite you to contribute.\n. @ripienaar And how is what you are doing currently constructive? Just ignore my snark and let's move on.\nWe are implementing Stop-The-World GC in with https://github.com/docker/distribution/issues/743. This should help in a lot of simple deployments.\n. Superseded by docker/distribution#54.\n. Removing next-generation label since this is less-related to the new registry. We will have extensions but the model will involve go packages.\n. Closing this, citing the existence of docker/distribution.\n. Superseded by docker/distribution#136.\n. @bshi Consistency and coordination are definitely something we are thinking about. Unfortunately, many of the storage backends lack consistency and don't have any kind of transactional coordination. The new registry will likely require some sort of coordination layer to mitigate that. Watch out for upcoming proposals, as you're input will be appreciated.\n. We've already started doing this at 10AM PST on Mondays.\n. We are using this discussion in the ongoing roadmap for distribution but don't have actionable work based on this issue. Closing for now.\n. @wking I am familiar with Swagger. How well do you think it would fair describing something like the API in docker/docker#9015? Would you be willing to take a crack at it?\n. We are following this model for the next-generation registry with the descriptors. These can be used to generate documentation with templates or definitions for implementing clients in other languages. They could also be used to directly generate swagger documentation.\nClosing as completed for now. We may have specific tasks that are related to this in the docker/distribution repository.\n. Superseded by docker/distribution#86.\n. TL; DR LGTM!\nI think this change is looking good. I'm putting together the specification for the V2 API tonight/tomorrow morning and will have a better idea of the underlying storage layout. We may make some changes going forward with information there but let's go ahead and get this PR merged.\n. LGTM!\n. The next generation registry and the distribution project are making plans to ensure content addressability across layers and manifests. While there are no immediate plans to integrate groupcache, As the V2 registry currently stands, distribution can benefit directly from http caching. It follows that groupcache would only help in addition.\nThis issue is going to be closed. A Proposal and PR is welcome for groupcache integration in docker/distribution.\n. Superseded by docker/distribution#200.\n. LGTM!\n. Superseded by docker/distribution#133.\n. This issue has been superseded by docker/distribution#19, which includes an overview of the intended implementation.\n@bobrik Actual storage for mirroring will be accomplished within the registry. If the registry is not configured as a mirror, content will not be pulled from that registry. We may be able to make caching the content locally an optional configuration parameter to allow for local, non-cached mirrors. The client can be pointed at the local registry, deferring the decision of whether to fetch the remote content to the local registry. The new registry will be able to run as a cluster with a shared storage backend, allowing for HA and scaling. For the most part, nodes are shared-nothing except for the storage backend and possibly a caching system, such as redis.\n. Superseded by docker/distribution#19.\n. Superseded by docker/distribution#88.\n. @dmp42 Should this still be marked Next-Generation? Are you going to cherry-pick this into the next-generation branch?\n. @dmp42 @wking Let's merge this PR into master and make a new PR for the next-generation branch.\n. LGTM!\n. > I still prefer using a list of numbers [1,2] to a period-delimited\n\nstring, but whatever gets the job done.\n\n@wking Lists (slices really) are a different beast in Go. Keeping the Version opaque-ish will keep things more straightforward and avoid over-specification.\n. LGTM!\n. This should be covered by docker/distribution#113. An microservice accepting notification requests could index repositories.\nIf not, please re-open.\n. Would this be covered by docker/distribution#113? Webhooks provide quite a lot of information. If yes, we can close this ticket or open a new one.\nPerhaps, a metrics reporting microservice would be a good demo of processing webhooks.\n. User agent has been added but response processing time has not yet been added. It might be better to get response processing time from the logs (checkout the example in docker/distribution#86).\n. This issue has been superseded by docker/distribution#42. Please take further conversation there.\n. @BrianBland Are we going to take care of these issues within this PR or refactor the storage backend later?\n. LGTM! Pending travis, of course.\n. The next generation registry accepts a superset of what the engine uses. Considering this closed for NG. Please open an issue in docker/distribution if there is more work to be done here.\n. @BrianBland Wicked!\nDo you want to make the move to cmd with that change, as well?\n. Superseded by docker/distribution#65.\n. LGTM!\n. @BrianBland @dmp42 @dmcgowan @jlhawn \n. @brianbland @dmp42 \n. @BrianBland \nThe reasoning behind the opaque integer type is to ensure that the zero-value of ErrorCode always maps to unknown. This allows the equality for unknown error codes to always work without special code.\nFor example, we get these properties:\nvar ec ErrorCode\nec == ErrorCodeUnknown // always true\nParseErrorCode(\"UNKNOWN\") == ErrorCodeUnknown\nParseErrorCode(\"\") == ErrorCodeUnknown\nWith a string type, code is required to ensure that \"UNKNOWN\" and the empty string (\"\") compare equal, which can be tricky.\n. @BrianBland @dmp42 @dmcgowan @jlhawn \n. I don't think there is anything in this PR worth holding up a merge.\nLGTM!\n. @dmp42 This PR is really a very strong TODO. ;)\n. The proposed Stat call doesn't implement the full fs API. It will just be the subset that makes sense for storage backends. The value returned by Path() should return the path provided as the argument to Stat(path string). All \"paths\" for the storage driver should abstract the backend away, in that it should never mention \"buckets\". It should be an isolated path namespace, usable by the registry storage package. This is to allow one to pass around FileInfo without having to figure out how to compute its Path (roughly equivalent to Name() but provides a full path).\nThe use case of the ModTime or CreatedAt would be to support http.ServeContent. IsDir is to allow the backend to decide what exactly constitutes a directory entry.\n\nStuff like ModTime() or IsDir() are not easy things to implement and unless are directly going to use these in the logic there is no point of making an API out of these and currently these are YAGNI.\n\nDoes the storage backend (ie azure) not export a timestamp on blob objects? It seems to be covered by BlobProperties. IsDir can be implemented by convention, which you've already done in you're PR (directDescendents, I believe).\n. @ahmetalpbalkan Just so you don't miss this again:\nThe use case of ModTime is to support layer fetches with http.ServeContent.\nPlease see https://github.com/docker/docker-registry/blob/next-generation/layer.go#L60.\n. @ahmetalpbalkan yes, that is correct.\n. @dmp42 Working on this today. I'll probably just blow this old branch away...\n. This is superceded by #820.\n. LGTM!\n. LGTM!\n. @dmp42 Admittedly, this is a silly PR. We should add several cases, but I'd like to have the end-to-end interaction implemented before enumerated those test cases. Filed #728.\n. Superseded by docker/distribution#199.\n. @wking \nAfter I clarify some of the issues you brought up, I think you'll find this layout points the next generation registry in the direction you're looking for. Right now, we are focused on standing up a working registry that implements the V2 API so we can validate the concepts.\n\nI'm not sure why we need an \u201cunderlying file system\u201d model on top of\nour abstract storage [1].  Why assume that the storage engine should\nuse a filesystem-like layout? \n\nThis mapping scheme has the very nice property that it doesn't actually leverage file system operations for layer lookup (ie directory listing). While the scheme is hierarchical like a filesystem, all the paths for the main operations are pre-calculated lookups followed by a read, which maps very well to keyed object stores.\nCurrently, the next generation storagedriver effectively implements a VFS. The work I am doing now will definitely expose a higher-level interface and possibly a mid-level interface, both of which could be candidates for future driver interfaces, but not yet.\n\nThose don't need to be registry endpoints though, if the utility can\ntalk directly to the storage driver.\n\nBy exposing the registry layout to out of band clients, we would be effectively exporting a new API, which needs to be versioned and controlled. We are still working on the V2 HTTP registry API and we want to focus on that. Though the use cases (listing repos, layers, etc.) you've listed will likely be added to the registry API to support management tasks, the general concept of safe, application-level registry operations would likely be implemented as part of an extension mechanism.\n\nIt looks like layerindex is addressing the \u201cList repositories that\nconsume a given layer\u201d requirement, and handling that in a parallel\nnamespace (layerindex/ vs blob/) is fine with me.\n... I don't understand the need for repositories/<name>/layers/\u2026.  Can't\nthe registry look that up in the manifest?\n\nWhile it can serve that purpose, this is actually part of the access control methodology and partially a by-product of the v2 image manifest format. The layerindex allows one to lookup up the \"owning\" repository of the target layer, identified by tarsum, without knowing the name of the repository. If the user doesn't have access to one of the listed repositories, they can't get to the blob id, stored under repositories/<name>/layers/. The purpose of repositories/<name>/layers/ is to track the layers that have been uploaded under the particular namespace. Parsing the manifests wouldn't meet this use case and would be rather expensive.\nThis scheme supports the use case of two parties uploading identical layers without requiring access to each others repositories. This avoids the pathological case of \"claimed tarsums\". With this approach, two parties can use the same tarsum for a layer but both parties have to prove they have the content, even if they have the same checksum. If this is not considered, the tarsum effectively becomes a content access key, and any one that knows you're tarsum can get access to your content. Conversely, if one uploads a simple layer, they could claim that tarsum and prevent others using it, even if generated independently.\nThis will become more obvious as we put the access controls in place.\n\nI don't understand\nthe <tarsum version>/<tarsum hash alg> bits though, can't we just use\nthe tarsum as an opaque string?\n\nThis allows multiple versions of indexes in the directory, referencing the same content, without blowing up directory listing. It also keeps the + out of filenames, which might be problematic for certain backends. It also allows other non-tarsum, indexes to exist over layer files in the future. We may want to change the scheme to be index/layer/tarsum to free-up indexes over other objects, as well.\n\nFor example:\nlayerindex:tarsum+sha256:589\u2026e03/library/debian:latest\nYou don't even need a value for that key.\n\nEven though the number of repositories with the same tarsum but no mutual access should be small, in accordance with your earlier comments, directory listing should be avoided. Concurrent updates will be problematic, but that is a fine tradeoff.\n. Superseded by docker/distribution#199.\n. There are a number of approaches we are taking with the new registry that will make this easier:\n1. All docker clients do have a user agent with docker version. This won't change.\n2. Future api implementations will require strict media type declarations in Accept headers for requests, making \"turning off\" old clients much easier.\nPlease open a specific issue in docker/distribution if the intent of this ticket isn't covered.\n. I'm not sure if this applies to the next generation registry. I'm removing those labels for now. Feel free to pursue for v1 registry.\n. LGTM!\n. LGTM!\n. Superseded by docker/distribution#200.\n. LGTM!\n. LGTM!\nA few comments, but they are mostly discussion points and upcoming refactoring plans.\n. LGTM!\n. @BrianBland @dmp42 \n. Superseded by docker/distribution#/198.\n. LGTM!\n. I was thinking some sort of decomposable API:\n```\nGET /api/\n{\n routes: [\"/api/v1\", \"/api/v2\"]\n}\n```\nThis might be covered by swagger documentation.\nReally, we need to ensure that clients that don't know about the new API gracefully fail.\n. We've work this out in docker/docker#9784 and in docker/docker#9015.\n. Removing next-generation flag from this ticket. Covered in docker/distribution#135. Leaving open to cover current registry doc problems.\n. Superseded by docker/distribution#95.\n. LGTM!\n. @wking While my wording may not be clear, you may be missing the purpose of this ticket. We really don't want internal driver operations to hang indefinitely. Its confusing for clients and it takes resources in the registry service. Internal to the registry service, operations need to have bounded execution. When making calls to an out of process storagedriver, the in-process ipc client needs to timeout on rpc calls.\nAnd yes, if the registry cannot service a request to the API client, a 503 or 504 should be returned.\n. @wking Everything you bring up are items that need to be considered and are conditions of closing this ticket. We'll investigate this closely once we have a working registry up.\n. Also, we need to decide if we would like to use content.Context to manage these timeouts.\n. Closing this as non-actionable. We are integrating context.Context. We'll start getting more interesting timeout support with that effort.\n. LGTM!\n. I think this can be covered by docker/distribution#52 and docker/distribution#86.\nWe may want to add a debug endpoint that people can hit and dump to an issue to aid support requests.\n. @dmp42 @brianbland\n. LGTM!\nWe may want to revisit this approach systemically. There are definitely a few patterns emerging in the client code surrounding request/response goroutine lifecycles that could be consolidated.\nMerging!\n. This looks good!\nThe use of the filesystem my not have enough visibility to manage locking when multiple nodes are running upgrade. If this covers the common use case, this may not matter.\n. LGTM!\n. LGTM!\n. @dmp42 @brianbland @dmcgowan @jlhawn \n. @BrianBland @dmp42 @ahmetalpbalkan\n. > Why not?  Can't the storage driver check how much it received and\n\nreturn an error if it's < size.\n\nThere is no way for the caller of WriteStream to detect the partial write, thus why we add the number of bytes written to the return values.\n\nThat's fine inside Go, but it would make it hard if the storage driver\nis connecting to a storage backend that expects the size ahead of time\n(e.g. if the storage backend uses HTTP for pushes, or if its on the\nlocal filesystem and the driver wants to allocate the whole block of\ndisk space ahead of time to avoid fragmentation).  Since we have the\nsize from the client\u2194registry POST, we might as well pass it on for\nthe storage driver to use (or not) as it sees fit.\n\nGenerally, we won't actually know the size beforehand. This function may be passed an io.Reader directly from an http request. While the \"Content-Length\" header may provide a hint, the actual read may be shorter or longer. From a security perspective, it can't really be trusted. If the driver does actually require the size, to support block allocation or sized http pushes, it has the option to buffer the data in memory or local disk. The new proposed interface effectively passes on the lack of knowledge from incoming http requests. The returned write size can be checked against incoming \"Content-Length\" to validate the write.\n. None of the driver implementations even use the size parameter or use it inconsistently. It's not needed and it makes error handling more complex. It's also confusing in places where its assumed to be the total size of the target content at path rather than the write chunk size.\n. I think the confusion here is that WriteStream will not be called multiple times for http Chunked Transfer Encoding. The http library exposes Request.Body, which is an io.ReadCloser. The transfer encoding will be transparent to the request handler. WriteStream implementations will do best effort to read all of incoming io.Reader until io.EOF. The io.Reader object is effectively \"sized\" with io.EOF (hence size being redundant). If the caller wants to write less data, they can wrap io.Reader in an io.LimitedReader, which can be restricted to size.\nThis puts buffering and data acceptance into the hands of the driver, which has the best chance of correct and performant handling. It also separates accepted bytes from the error type, allowing the driver to control whether partial, full or chunked writes are accepted, transparent to the caller.\nI'll be posting PRs for this over the next couple of days. You'll see that it simplifies the write flow and makes error handling a lot cleaner. I'd recommend you read up on the [io package][http://golang.org/pkg/io/#LimitedReader]. That knowledge will be helpful for understanding how some of these decisions are being made.\n. @wking You're missing the point: none of the drivers used the size argument correctly. It was confusing, incorrect and redundant. Please see #814 for details.\nI'm also confused why you are doubling down on this matter. What are you trying to achieve?\n. Furthermore, these changes aren't about preference. They are about consistency. It's inconsistent to both pass in an io.Reader, that will be read in full and an argument indicating its size. It adds an unnecessary order of complexity in calls to the WriteStream function and the errors it will produce.\n. @wking WriteStream will really only be used in a few places, whereas common size checks will have be implemented in each bespoke driver, resulting in divergent behavior.  Large and small payload detection must be handled in the storage layer and not the drivers, in one place.\nThe issue with size is that this is an incoming stream. The size is not known by definition. For the use case of the registry, the size is not known by a trusted source beforehand. Typically, the driver doesn't have enough information to do proper size checks that don't result in an invalid io.Reader. There also needs to be very careful error handling in the case of a short read/write that cannot be handled in the driver (check out my TODO in layerupload.go).\nWe have the same goals. API cleanliness is very important. You're feedback is detailed but if there is specific action you'd like me to take based on your feedback, please say so. While I do have the best visibility into this problem at this time, ignoring your feedback would be unacceptable.\nWith this change and additions described in #814, based on what I understand you are after, we really are getting the right behavior while avoiding confusion.\n. > instead of:\n\nerr := suite.StorageDriver.WriteStream(filename, 0, 3*chunkSize, ioutil.NopCloser(bytes.NewReader(contentsChunk1)))\n  c.Assert(err, check.IsNil)\nWhen I'd prefer:\nerr := suite.StorageDriver.WriteStream(filename, 0, chunkSize, bytes.NewReader(contentsChunk1))\n c.Assert(err, check.IsNil)\n\nAlso, I'm not sure if you noticed when quoting this, but the chunkSize argument changed from 3*chunkSize, which would be total size to chunkSize. This is part of the confusion we're trying to avoid with these changes.\n. @wking Thank you! We'll proceed with this for now and definitely revisit later.\n. Addressed in #820.\n. This is still breaking due to a few missed references during a previous merge. Submitting fix now and will rebase this one.\n. This PR must be merged before this will build: https://github.com/docker/docker-registry/pull/783\n. I have to rebase against the #783 fixes...\n. @dmp42 @BrianBland\n. We have random tests that work both as unseeded (most of the storage and api tests) and seeded. A seed is usually required if the tests are dependent on specific data values (ie, seed generator with number and then generate specific values based on that number).\nWhat specific test failure are you seeing?\n. Is this a problem you are having or are you foreseeing a future problem?\nIf a blob is partially written during a cancelled test, the next test run should clear out the test registry storage backend before re-running tests. This should have nothing to do with using rand.Seed (unless you're saying the root cause is filename collisions).\n. > I'm not sure how that cleanup is done. I can't see any calls to any delete in advance in many of the tests. The root cause is filename collision, that's right.\nThis should be part of the initial test configuration, before deferring control to the testsuite package. The tests expect an empty backend before running and anything already present may cause issues with the test.\n\nIt's no big deal for me, I was just wondering if it's intentionally not seeded.\n\nThe prng in math/rand is seed with a value of 1. This seed is intentionally not changed to ensure generated values are consistent between test runs. This greatly aids debugging.\n. This is a pretty common technique. Basically, it keeps tests \"random\" but determinant. Imagine a complex failure case where random objects need to be examined in detail, possibly requiring adding print statements. If all the randomly generated content changes with each test, it becomes remarkably hard to keep track of the actual failure.\nIts also helpful to ensure that tests don't \"flap\", failing on one run but not another. For example, let's say there is a length that is random, possibly covering a failure boundary. With completely random tests, it may fail one run and not another. Not only does this lead to flapping tests but it also means that behavior is being inconsistently tested. Fixing the PRNG cycle avoids this.\nTo address your question directly, random test input on one test should not be used to test whether filename variation is handled. If variation handling is required (ie random ascii outside of expected range), a specific fuzz test should be developed, and can even use the same PRNG. A random sample of the input space remains tested, but failures are consistent and determinant.\n. LGTM!\n. I am happy with the current state of the drivers and am closing this issue. We may still need to update other drivers, but we should open individual tickets in docker/distribution to handle this.\n. @BrianBland @dmp42 \n. LGTM!\nOnce this merged, please setup a running registry instance and see if this works.\n. Closed by #807\n. @ncdc With V2, it's very likely we'll have changes to the way things must work. While we should be avoiding serious changes, I hope your team can be somewhat open to this in the hopes that we'll get some benefits.\nThat said, it seems like we need some sort of content-addressable digest of V2 image manifests to make this work. There are two levels that might be interesting to applications:\n1. A signature-independent content-addressable digest taking into account the following fields:\n   - \"schemaVersion\"\n   - \"architecture\"\n   - \"fsLayers.blobSum\"\n   - \"history.v2Compatibility\" (maybe omitting internal fields, like \"created\"\n2. A signature-dependent content-addressable digest that includes the fields from above and the content in \"signatures\". This might actually just be the sha256 hash of the content, given that the signature is dependent on name and tag.\nNumber 1 would provide addressability of \"identical\" images, whereas number 2 would provide addressability over specific builds. The main issue with this is that it makes registry garbage collection nearly impossible, because all layers will technically be referenced by all manifests.\nPlease note that this is just a thought exercise.\n. @ncdc It sounds like the real problem is creating a consistent identifier to a\nmanifest, such that it doesn't change when a deployment is kicked off. I'm\ngoing to go over some stuff about garbage collection and then we'll address\nthat root issue.\nThe issue with permanently exporting references becomes apparent when one\nlooks at the garbage collection implementation. Let's say we have the\nfollowing list of pushes for the image tagged with a (in the same repo), each\nimage version identifed by \"revision\":\n```\ntag revision  layers\n\na   a1        layer(0), layer(2)\na   a0        layer(0), layer(1)\n```\nThe above table would represent two pushes. One relying on layer(0) and\nlayer(1) and one replacing layer(1) with layer(2). The accessible roots\n(or \"image ids\") are a, a0, and a1. The target of a is temporally\ndependent on whether a1 has been pushed or not.\nWe can use some diagrams to understand how these references develop with each\npush. Before the push of a1, all known layers are referenced:\n[a]\n          |\n         [a0]\n         / \\ \n        /   \\\n[layer(1)] [layer(0)]\nAfter the push, we can see that all of the layers are still referenced, and\ntherefore not safe for delete, but a0 is technically orphaned:\n[a]\n                    |\n         [a0]      [a1]\n         / \\       / \\\n        /   \\     /   \\\n[layer(1)] [layer(0)] [layer(2)]\nIn the approach where no references are ever removed, such as this content-\naddressable proposal, all layers must remain forever, unless you allow\ndeletion by id (a0, a1, etc.).\nUnder the current V2 registry scheme, a0 and a1 don't exist, so layer(1)\nbecomes unreferenced after the push of a1 and can be \"safely\" deleted. We\ncan see that layer(1) is trivially identified for deletion in this approach:\n[a]\n                    / \\\n                   /   \\\n[layer(1)] [layer(0)] [layer(2)]\nThe proposed content-addressable scheme can be mimicked by pushing manifests\nthat link different versions of a (where / represents the same manifest\npushed with different tags):\n[a0]        [a/a1]\n       /  \\         / \\\n      /    \\       /   \\\n[layer(1)] [layer(0)] [layer(2)]\nI'm not saying that we should never implemented content-addressable manifest\nstorage, but its a lot of bookkeeping for a system should be simplified. The\ncurrent approach allows the users to control the deletion pattern while still\nallowing one to keep multiple versions of the same image around without a lot\nof work on either side.\nFor @ncdc's use case, referring to most recent diagram, a0 would be version\n0 and a1 would be version 1 of the pushed manifest but the latest would be\nreferenced via a. The real problem is programmatically associating a with\na1 (or \"latest\" version) when a notification activates a deployment. In corollary, given an image tag a, what are its equivalent images and how can a permanent reference be retained?.\nThis use case is what we really need to focus on.\n. Another problem with a content-addressable manifest ids is that it muddies the role of a name/tag reference. If name and tag are omitted from the calculation of such an id and multiple manifests with different names and tags have identical ids, which manifest should be returned?\nIt would stand to reason that the id reference would have to be namespaced by the repository, rather than by pure id. And, at the same time, if an image could be referenced by both name+id and name+tag, it would break the guarantee that the url /v2/<name>/manifests/<tag> always returns a manifest with name and tag. Does the registry change the manifest? If so, who signs the updated manifest?\n. @jlhawn I think am on board with this approach, although id references will be explicitly namespaced by the tag. From your example, jlhawn/my-app:3.1.4@e36eb1f73548649b... would be valid, but jlhawn/my-app@e36eb1f73548649b... would not be. This is a simplifying restriction due to the way \"tags\" work in the V2 manifest format that we may lift at a later point in time.\nUltimately, I feel a lot of the contention comes from the term \"tag\". @mmdriley is correct in that the git model is implied by the nomenclature. I also agree that the git model of tags is appropriate. However, the field known as \"tag\" in the V2 manifest is not the right way to implement that style of \"alias tags\". Arguably, we should change this field to \"version\".\nI think we can avoid some premature decisions by doing the following:\n1. For the initial version, the manifest id is controlled by the registry. The manifest id should be returned as part of the response to a manifest PUT, in addition to a Location header with the canonical URL for the manifest (ie /v2/<name>/manifests/<tag>/<digest>).\n2. The \"digest\" of the manifest is the sha256 of the \"unsigned\" portion of manifest, with sorted object keys. This should only be calculated by the registry for the time being.\n3. PUT operations on the manifest are no longer destructive. If the content is different, the \"tag\" is updated to point at the new content. All revisions remain addressable by digest.\n4. The DELETE method on /v2/<name>/manifests/<tag> should be clarified to delete all revisions of a given tag, whereas DELETE on /v2/<name>/manifests/<tag>/<digest> should only delete the revision with the request digest.\nI'll repeat the proposed supporting endpoints from docker/docker#9015 comments for reference:\n| Method | Path | Entity | Description |\n| --- | --- | --- | --- |\n| GET | /v2/<name>/manifests/<tag>/<digest> | Manifest | Fetch the manifest identified by name, tag and digest. |\n| DELETE | /v2/<name>/manifests/<tag>/<digest> | Manifest | Delete the manifest identified by name, tag and digest |\nIf we can agree on this as an interim compromise, I think we can move forward and meet the requirements of this request.\nPlease let me know if any clarification is required.\ncc @ncdc\n. @ncdc\n\nAre these concerns still an issue for you, or are they mitigated by the delete mechanics listed in bullet 4 above?\n\nThey are mitigated by bullet 4 above. This approach saves everything unless specifically asked to delete it. An external webhook service can then be used to control manifest lifecycle. This keeps GC simple (ref counting) and separates it from lifecycle management. It also reduces the possibility of data loss upon manifest updates.\n. @wking We may lean towards just taking the entire hash of the content to address this. There seem to be problems with specialized hashes no matter what way we try to cut this up. We may want to discuss storing the signatures separately from the manifest.\n. @wking We may actually be able to merge the signatures on the registry side.\n. I've spec'ed out a proposal for implementation in docker/distribution#46.\n. I'm going to close this issue, for now, since it has been superseded by docker/distribution#46. If there is further discussion to be had, please take it there.\n. @AndreyKostov LGTM! Let's just clarify that the return value nn is the number bytes read from reader.\n. @ahmetalpbalkan The return value nn is effectively implemented by io.Copy. If the code is structured to use io.Copy or io.CopyN, as opposed to io.ReadFull, its free.\n. @ahmetalpbalkan I realized that I only addressed half of your question: the nn return value is to indicate to the caller the number of bytes read from io.Reader. It allows the caller to evaluate the success of the write when no error is returned.\n. @BrianBland This is great to hear. We'll hold off on the addition of adding a Commit/Complete/Sync method and update the s3 implementation. Will we have to fork the goamz driver to support this?\nI'll close out this ticket completely where we hear back from @ahmetalpbalkan on the azure driver.\n. @ahmetalpbalkan Checkout @BrianBland's comment. Basically, it's more effective to commit with each write, then use UploadPartCopy for s3, eliding the need for the Commit/Complete/Sync. If you could weigh in on the azure blob API, we can close this ticket and not implement the extra storage driver call.\n. As far as I can recall, we've worked out most of the issues with the driver API. If there are still problems, let's open up a new issue in docker/distribution referring back to this issue.\n. The goal of this ticket is satisfied for v2 registry with User-Agent header from docker daemon:\ndocker/1.4.1 go/go1.3.3 git-commit/5bc2ff8 kernel/3.16.7-tinycore64 os/linux arch/amd64\nWe can easily ban with buggy clients with by detecting docker version or git hash. We also have Docker-Distribution-API-Version to communicate from server to client.\n. @wking Using a User-Agent to deal with a buggy client is usually a bad idea. It does nothing but increase complexity and create future problems. However, we currently have the hooks to do this if it comes to that.\nI'm not going to port this issue into the docker/distribution tracker unless it has a clear, actionable design. Right now, its open-ended and I'm not sure what a resolution would look like.\n. Superseded by docker/distribution#29.\n. @BrianBland yes, this could be considered closed. I'd like to have a blip in the readme about the path formats, but if time is tight, we can document it later.\n. We'll consider this closed for now. We may want to open a documentation bug in docker/distribution, but the bulk of the work is completed.\n. This PR is superseded by #827.\n. Merging because other PRs are waiting on reliable CI configuration.\n. Superseded by docker/distribution#28.\n. LGTM!\nJust double check the directory creation in the inmemory driver.\n. @BrianBland @dmp42\n. Unfortunately, I need to get this merged. If there are problems with these changes, please feel free to comment on this PR and we'll get them fixed.\n. @jlhawn @dmcgowan @dmp42 @BrianBland @AndreyKostov \n. @BrianBland I'm pretty unhappy with go vet + golint. I have never been into linting tools, as they end up noisy, but I consider go vet to be critical.\nLet's hold off on the merge until I fix this.\n. @dmp42 Rebased. Ready to go.\n. LGTM!\n. @dmp42 @dmcgowan \n. These new tests seem to great at exercising the cooling system on my laptop...\nHave you ran them with GOMAXPROCS set to something other than 1?\n. LGTM!\n. @dmp42 @dmcgowan @jlhawn \n. Must have been in a hurry...\n. It looks like the issue is not the port but rather that the docker daemon doesn't trust the certificate of the registry. The error message explains the condition in detail, but to get it working immediately, it suggests passing the following flag:\n--insecure-registry myregistry:4443\nLet us know if that doesn't work!\n. @keith- Glad we could help! Thanks for following up with the resolution.\n@dmp42 The problem here seems to be on the docker core side. We'll have to setup an environment to test this but I suspect we can avoid this kind of issue by supporting a cached mirror. That way, all pulls go through the internal registry host, avoiding the need for internal proxy settings on the docker daemon. Only the registry's upstream client would have to be configured with correct proxy settings, eliding the need to switch the proxy settings based on which image is being accessed.\n. LGTM! The regular expressions look correct.\n. @dmp42 @dmcgowan @jlhawn\n. LGTM!\n. Is this not automated?\n. LGTM!\n. > Alright, I tested this most recent version with various gofmt errors across multiple files and it properly includes the changes in the actual commit.\nDoes it give you a change to review the changes before adding them to the commit?\n. LGTM!\n. LGTM!\n. Are we going forward with this, even though goveralls is struggling with multipackage coverage reports?\n. LGTM!\n. This is looking good. My only real concerns are:\n1. Stateful token validity. https://github.com/docker/docker-registry/pull/859#discussion_r21991249\n2. Unnecessary exports on challenge interface. https://github.com/docker/docker-registry/pull/859#discussion_r21993278\n. LGTM!\n. LGTM!\n. LGTM!\n. LGTM!\nCode looks fine, but I'll wait till it gets an LGTM from @dmcgowan.\n. LGTM!\n. LGTM!\n. cc @jlhawn @dmcgowan @dmp42 \n. cc @jlhawn @dmcgowan @dmp42\nConsider this PR a preflight for addition to the main client PR.\n. @wking The entire v2 package has been designed to provide API definitions for use across specification compliant implementations. The binding between the spec and the implementation is limited to these definitions but I'd also like to employ these definitions to manage API implementation coverage. Ultimately, the checked version of the generated SPEC.md is the binding spec, so changes to the code that affect the specification will show up in the specification and be turned down if not allowed.\nWhile using swagger was an option, I took the simpler route to just write out the definitions to generate a specification in the markdown format accepted by docker core documentation. The integration with Go's template system should allow one to generate code for other languages:\n```\n$ registry-api-descriptor-template pydefs.py.tmpl <<EOF\nfrom collections import namedtuple\nError = namedtuple('Error', [\"code\", \"message\", \"detail\"])\n{{range .ErrorDescriptors}}{{.Value}} = Error(\n    code=\"{{.Code}}\",\n    message=\"{{.Message}}\")\n{{end}}\nEOF\n```\nA similar template could be used to generate swagger documentation.\nI've avoided go-restful, or any other web framework for that matter, to restrict the number of external dependencies. net/http is sufficient for the needs of the registry and keeps the bar for future contributions low, since one doesn't have to know a particular framework to work with the project.\n. I don't think this is rendering correctly. Use the quotation syntax:\n\nNotice: This repository hosts the classic (stable) python docker-registry. If you are looking for the next-generation of docker distribution tools (including the new golang registry), please head over to docker/distribution instead.\n. LGTM!\n. +1\n. LGTM!\n. LGTM! Wait for release to merge.\n. @montgomery1944 This is a daemon issue. Please file in github.com/docker/docker.\n. This is a duplicated of docker/distribution#975.\n. Please file this in https://github.com/moby/moby.. How big do we expect directories to get? Do we have any reasonable bound on the number of directory entries?\n. Let's make sure the fields are heavily documented.\n. Should there be a ParseVersion function? Will the YAML library automatically extract this type?\n\nIt might be better to declare type Version string, then have methods Major and Minor that actually parse the contents. If the answer to the second question above is no, this will keep the field valid without an extra parse step.\n. I wouldn't get too fancy. Just leave it as a string then interpret when needed. (Also, the suggested usage of a slice isn't very idiomatic in Go)\n. After thinking about this more carefully, we thought it might be better to keep this IPC call simple. Without introducing problems with missing or inconsistent directory listings, the variation in the drivers makes getting a reentrant interface right problematic. The other issue here is that whether or not these calls are reentrant, you end up allocating the same amount of memory. Thus, large directories will cause latency in registry requests no matter what.\nAs an alternative, we can mitigate listing sizes by controlling underlying directory layout. For now, let's keep List(prefix string) ([]sting, error) as the interface method. We should also define toList` to not return recursive results. (@dmp42 Was there a reason to support recursive directory listings?)\nWe can always revisit this in the future if we've made the wrong decision today.\n. @wking Before I address your points, fear not, we intend to address #614, in one way or another. We just need to understand the right way to do it.\n\nUsing either a marker string [1] or an opaque marker type [2] seems to\nsupport all proposed backends.  With the marker stored in the\nregistry, the storage backend doesn't need to keep track of open list\ncalls, so a reentrant interface is easy.  Or am I missing something?\n\nYes, the marker or index will support most backends, but the behavior may be different on updates to that directory. With a marker string, an insert before the marker after the first call will lead to a missed entry. With an index, one may have duplicate entries. The behavior between various backends under these conditions may vary greatly, requiring calling code (ie the registry) to handle that variation. By going with the simpler call, we avoid this variation and leave it up to the driver to present a \"consistent\" view of the directory state.\n\nHow so?  ZRANGEBYSCORE is O(log(N)+M) with N being the number of\nelements in the sorted set and M the number of elements being\nreturned.  That scales pretty well.  More importantly, it lets you\nstart streaming results from the registry to the client before you've\nfinished listing the whole directory.\n\nThat's not the case I'm worried about. If we want to iterate over the entries of a large directory to support an API operation, the latency of the registry request will still be proportional to the size of the directory, reentrant or not. This should be avoided, if possible.\nLet's proceed with this simplified API and table this discussion, for now. We have collected some really good suggestions but we really need to look at how we can avoid this bottleneck. Once the V2 paths are laid out, we can come back and see if we really need the reentrant API call.\n. Misspelling here: \"propocal\".\n. Perhaps, \"Scope\" for this part, instead.\n. I'm not sure that this comment belongs with this commit, but it might be reasonable to use an integration test build tag to separate the unit tests from the integration tests.\n. This isn't an entirely safe operation. $GOPATH can be a list and this would append \"/bin\" to the end of that, making  the search path for the binary only work for the last $GOPATH entry.\nWe should just require the running process container to ensure that the plugins are in the standard executable path.\n. IMHO, we should go with the $PATH approach. This is pretty familiar in that its used with git and protobuf compiler for binary plugin implementations. It can be disorienting for the uninitiated, but we can cover that with documentation.\n. Assuming this is to deal with channel close. The pattern for channel close typically has it only closed by the sender to avoid this panic. Not sure if this is worth a change.\n. This error may need to stored on the driver interface to inform later calls that the driver has error'ed out and should be restarted. It looks like all the methods above will return an error on the first subprocess exit but may proceed if called again.\n. Maybe capture this pattern in a single spot:\n```\nif err := driver.exited(); err != nil {\n    return nil, err\n}\nfunc (driver *StorageDriverClient) exited() error {\n    select {\n    case err := <-driver.exitChan:\n        return err\n    default:\n        return nil\n    }\n}\n```\nThere may be some adjustments, so please see the other comments.\n. Just make this definition in client.go. These generic \"types\" files easily get out of hand.\n. Go ahead and throw this in errors.go, even if it is client-specific? (This is both a statement and a question...)\n. FSLayers for Go identifier.\n. Could you fix up DetailUnknownLayer with this type? Also, should be FSLayer.\n. TarSum for consistency.\n. GetImageLayer(name, tarsum string, offset int)\nAvoid \"overtyping\" the integers. Generally, int is 64 bits in Go and using it avoids type conversions with other methods.\nWe may actually want to support some sort of ReadSeeker for this method to support resumable downloads. The endpoint will support range requests, but I am not sure about the idiomatic way of handling this on the client-side. \n. I not sure we need both FinishChunkedLayerUpload and UploadLayer. They should effectively be the same operation.\n. The plan here was to actually use the the gorilla/mux router to build urls. I am not thrilled with gorilla/mux on the server-side (see some of my TODOs) so we may change that, but it seems like we should generate the route urls and build the urls in the same set of code.\nI guess, place a TODO here to create some sort of \"url manager\", oriented towards making urls for client and server-side use.\n. The field tags must be exported. This should be:\ntags := struct {\n        Tags []string\n}\n. Agreed.\n. Do we like that type? If so, yes, we should place it on the exported interface.\nI see this type as a general communication channel for various digest methods:\ntarsum.v1+sha256:6c3c624b58dbbcd3c0dd82b4c53f04194d1247c6eebdaab7c610cf7d66709b3b\nsha512:cf83e1357eefb8bdf1542850d66d8007d620e4050b5715dc83f4a921d36ce9ce47d0d13c5d85f2b0ff8318d2877eec2f63b931bd47417a81a538327af927da3e\nsha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\nsha1:da39a3ee5e6b4b0d3255bfef95601890afd80709\nA utility function might be something like this:\n// Verify takes a digest and returns a verifier. If the digest is unknown or\n// not supported, an error will be returned.\nVerify(digest Digest) (Verifier, error)\nThen, it could be used in an io efficient hash pipeline:\n```\nverifier := VerifyDigest(digest)\nlengthVerifier := VerifyLength(size)\nreader := getData()\nreader = io.TeeReader(reader, verifier)\nreader = io.TeeReader(reader, lengthVerifier)\nio.Copy(dst, reader)\nif lengthVerifier.Err() != nil {\n    // oops!\n}\nif verifier.Err() != nil {\n    // oops!\n}\n``\n. Per the discussion on ACLs, this all has to change. Do we mind letting this slip by for now and I'll fix it up when I make the path layout changes?\n. This needsbzr, as well.\n. Use https here to ensure its coming from trusted source.\n. We might want to unify some of these interfaces with the registry storage backend. For example, in the storage package, we havelayerFile,LayerandLayerUpload. While these will probably be named \"blobXXX\" in the future, there is definitely shared functionality.\n. In the backend, I've opted for supporting this with a fast seeker implementation (ie no io seek). It might be a bit more idiomatic.\n. In the registry backend, http api and the client code, we seem to be usingsizeto mean, well, \"size\". In the http api, we end an upload with thelength` parameter.\nDo you see an issue updating the specification to take the \"size\" parameter, instead of the \"length\" parameter?\n. This error should be checked and logged on non-nil. The process should exit non-zero if there was an error.\nThe other drivers don't follow this convention, but #803 has been filed to fix this.\n. This construct is a little awkward. The pointer type is unnecessary; copying string types is cheap. Just check the parameters explicitly.\n. These seem to only be used in the course of testing. Could we move them to the test file?\n. By not holding onto the api object, can the storage driver be correctly shutdown?\n. Does this work in partial blocks? For instance, if a transfer is interrupted, leaving a block partially written, what is the behavior? Is the block never written to the backend? How do prevent a partial, corrupt block from being written?\n. Is this compatible with WriteStream? A client should be able to call this method and use its return value as an argument to WriteStream.\n. More specifically, let's say a client uploads 10 blocks, and 1 partial one and is interrupted. Where is the client able to resume from? What is reported by CurrentSize?\nThe backend needs to be able to accept partial data for a given path.\n. Can the value returned by CurrentSize be used as argument to WriteStream to continue an interrupted write? The issue I see is that WriteStream uses a different approach to get the continuation point for the ongoing write. CurrentSize uses GetBlobProperties, while WriteStream fetches the committed blocks.\n. Panic is too forceful. logrus.Fatalln is the right call:\nif err := ipc.StorageDriverServer(driver); err != nil {\n    logrus.Fatalln(\"driver error:\", err)\n}\nThat provides non-zero exit status with error information.\n. Fair enough.\n. What if the client uploads 17MB (Block1-Block4, partial Block5) of a 20MB file? Do you commit a partial Block5?\nThe driver really needs to do its best to hide this from the storage client.\n. Once its committed. can the client continue adding new blocks to that path?\nShouldn't WriteStream commit what it can and the result can be introspected by a CurrentSize call?\n. Ok, so this sounds like it should work. I'm getting into the storagedriver code over the next few days, so I'll make sure to add tests around this kind of thing.\n. Could you check that when the dir paths are created, we normalize them properly to have a trailing slash?\nIt might be better to have all paths start with \"/\", then end without slash, whether its a directory or not. Then a path normalization function can be consistent between file and dir.\n. Odd. Should have been captured by golint.\n. Not here. That only needs to happen on the client. However, it seem BuildBlobUploadURL needs to be variadic, as well.\n. Will this place the gofmt changes in the index and proceed with the commit? I would prefer that it fails and the results are uncommitted. Also, I am not sure about gating commits on golint. Looking here, it seems like the intent is to make golint quite noisy. This will be problematic when exported APIs migrate from wishes of golint maintainers.\n. // Authorized returns non-nil if the request is granted access. If one or\n    // more Access structs are provided, the requested access will be compared\n    // with what is available to the request. If the error is non-nil, access\n    // should always be denied. The error may be of type Challenge, in which\n    // case the caller may have the Challenge handle the request or choose\n    // what action to take based on the Challenge header or response status.\n. I am not sure how I feel about making validity a stateful property, especially since it's dependent on provided verify options. It's a little dangerous if the package gets refactored or modified. It looks like this is done as an optimization, however, we should probably optimize by ensuring that verify is only every called once.\n. We may want some package documentation here that explains the usage of this package. I suspect that other parties will want to provide their out implementation (such as silly auth).\n// Package auth yada yada\n//\n// Yada yadoo ... import desired auth packages like this:\n//\n//     import _ myauth/package/that/gets/registered\n//\n// Yada, here's how to yada:\n//\n//     access := Access{ ... }\n//     if err := accessController.Authorized(req, access); err != nil {\n//         switch err := err.(type) {\n//         case Challenge:\n//             err.ServeHTTP(w, req)\n//             return\n//         default:\n//             // Safety zone.\n//         }\n//\nIt's up to you how crazy we want to go with this.\n. // ServeHTTP prepares the request to conduct the appropriate challenge\n// response. For most implementations, simply calling ServeHTTP should be\n// sufficient. Because no body is written, users may write a custom body after\n// calling ServeHTTP, but any headers must be written before the call and may\n// be overwritten.\n. I think we can actually get rid of Status and SetHeader in the challenge interface. Careful implementations can set their own headers before the call to ServeHTTP and write their own body after. Anything more custom than that should require a custom error type (such as SuperChallenge or OAuth2Challenge).\n. Per our conversation, let's add this to the common package as stringset.go. There may be future uses and I'd rather export a type once and not use it elsewhere rather than end up with 10 string set implementations.\n. Does strconv do the right thing? I think I had this as a hack in the prototype, but is Go's string quoting appropriate for use in the header? I suspect it's sufficient if we're careful about what get's into realm and service.\n. You don't need to call http.CanonicalHeaderKey. (http.Header).Add does it for you.\n. If you have time, it might be better to declare these a time type that overrides TextMarshaler behavior to encode and decode from unix time. I don't see these types being used outside of the library, so this may have limited value.\n. Nice catch!\n. Could you add some package documentation on the approach for the s3 driver? This is probably short, but it might cover things like slight path-handling differences, if they apply.\n. Should probably check if the lookup was success and return error if not. It will make configuration errors easier to track down.\n. This should be a type assertion, rather than a fmt.Sprint call:\nregionName.(string) == \"\"\nYou can even check if the type conversion proceeded but a panic is fine, since the configuration is broken if it is not a string.\n. We shouldn't be parsing bools here. It should already be a bool referenced by the interface. Should just require a type assertion.\n. Why is this removed? Let's be explicit if we're relaxing requirements.\n. This may need a comment about what it is trying to do.\n. So, this passes as long as the number of missed writes is not 1024? What it its less?\n. Why make this a for loop? Should WriteStream consume tf if there is not an error? Also, shouldn't the offset be updated to totalRead on each call to ensure the the write worked correctly?\nSomething seems wrong with this test.\n. I had something like this sitting around in my stash:\n// WriteStream stores the contents of the provided io.ReadCloser at a\n       // location designated by the given path. The driver will know it has\n       // received the full contents when the reader returns io.EOF. The number\n       // of successfully written bytes will be returned, even if an error is\n       // returned. May be used to resume writing a stream by providing a nonzero\n       // offset. Offsets past the current size will write from the position\n       // beyond the end of the file.\n. Just saw that its a benchmark... still confused.\n. This method is coming in at 250 lines. Let's break it down a bit. Leveraging Go's io package primitive plus some s3 specific implementations would keep the complexity down. Perhaps, some sort of ChunkedWriterAt that takes a ReadSeeker would help manage the hardest parts. \n. Should this call fullPath or should it replace fullPath?\n. Let's document that, because its not clear from the code.\n. Ok, so let's remove this completely and add make it clear that directories don't have to support modtime. When you leave it commented out, it makes it unclear.\n. We need to be strict here. This is not parsing code. That is the responsibility of the configuration system.\n. I guess the comment above is what, and I'm interested in the intent of how.\n. No worries, glad we caught it.\n. I don't see the benefit of enforcing these non-standard, default directory paths when simply adding them to GUNICORN_OPTS would suffice and actually provides more flexibility. Otherwise, I'd say make an environment variable for each path.\n. Apologies for the repetition. Let's figure out whether or not we want to break these out so this PR can get merged.\n. Should this be a configurable path?\n. @moxiegirl This is working now. Shall we merge it?\n. ",
    "ripienaar": "+1, am using https://gist.github.com/shepmaster/53939af82a51e3aa0cd6 now to clean orphaned stuff from the registry, having it happen magically would be great\n. @stevvooe classy. \n. @stevvooe oh I see of course it's ok to be dismissive and belittling because it someone did it to you. Gotcha. \n. ",
    "noren": "you can remove entire repository using curl:\ncurl -X DELETE registry-url/v1/repositories/repository-name/\nto remove unused images you have to remove tag first:\n$ curl -X DELETE registry-url/v1/repositories/repository-name/tags/\nthen run this script:\nhttps://gist.github.com/kwk/c5443f2a1abcf0eb1eaa\non docker registy server, make sure you install 'jq' package first\n. ",
    "robdbirch": "+1 it shouldn't be this hard\n. ",
    "nandarajxxx": "how to create private repository using docker api?\nI have created registry using docker api by default it is public .\nimport requests\nimport json\nfrom requests.auth import HTTPBasicAuth\ndata=[{\"id\":\"8c2e06607696\"}]\ndata=json.dumps(data)\nheaders={'content-type':'application/json','Accept':'application/json','X-Docker-Token': 'true'}\nurl='http://index.docker.io/v1/repositories/nandaraj/busynew/'\nr=requests.put(url,data=json.dumps(data),headers=headers,auth=HTTPBasicAuth('nandaraj', '12345'))\nthis is my code.By executing the above code repository is created a public .How to create a private repository using docker api?\n. Thank You\n. ",
    "vincentvanouytsel": "@schmunk42 That doesn't remove the images from the storage, does it?\n. @schmunk42 I don't think it does. A workaround for this issue could be removing the tags (with the delete button) and then run this  script. \nThis should remove the untagged images from your storage. \n. ",
    "dacay": "+1\n. ",
    "Zolmeister": "+1\n. ",
    "jadedfire": "are any of the four mentioned potential solutions being looked at? (https://github.com/docker/distribution/blob/master/ROADMAP.md#deletes) it makes sense to me that coding the entire solution to meet the needs of the \"lower common denominator among the storage drivers\" can be likened to coding a solution around exceptions. one would think that with proper modularization, the software could adapt to specific deployments and act accordingly. for example, if deployed to a single node for development purposes, and leverages the OS filesystem, the system should load an \"OS driver\" that could manage deletes rather easily.\n. ",
    "petarmaric": "To be quite honest, this recent discussion is just one of the examples why I have much more respect for the Django project and its community than Docker. \nYou really need to review your community guidelines, if you wish to have respect from and within the community. \n. Having the exact same issue myself after upgrading to 0.7.2\n. I see that you've created https://github.com/dotcloud/docker-registry/commit/34dea1e9ef17cba456ef9bcbab505bda69443116 which merely migitates this issue somewhat. It would be nice if the developers would reference the relevant issues in ther commit messages.\n. \n. So there's now a \"phantom\" 0.7.3 release with a quick-fix to this issue. Why so secret?\n. Yes, I've seen all of those and thank you for the rapid quick-fix. I was just wondering why aren't the related issues referenced in commits (As in FIXES #123, REFS #135 #154: Commit message here), as it would make issue tracking a lot easier (not having to review commits on a daily basis).\nNo quarrels with your effort and results, only the issue tracking process.\n. Thanks for keeping me happy :+1: \nNice work, this pull request LGTM.\n. ",
    "jonmorehouse": "I got the error while running a vagrant vm build from the docker repo on Mac. Thanks for helping.\nClient version: 0.5.2\nServer version: 0.5.2\nGo version: go1.1\n. Thanks guys. One last note ... I uploaded the docker-registry to dotcloud and via the samalba/docker-registry container (locally). Let me know if there's anything I could work on to help out here.\n. Has anyone gotten it running properly with dotcloud? Away from my computer\nnow but I'm wondering what the dotcloud nginx version is and whether or not\nthat is causing issues with the install in my case as well.\nOn Thursday, August 22, 2013, Sam Alba wrote:\n\nI am trying to find a pattern: does it work with gunicorn only and S3\nstorage?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/47#issuecomment-23122854\n.\n. Updated to the most current docker and everything seems to be working fine! \n. \n",
    "johncosta": "I've been able to reproduce this using:\n```\nroot@precise64:~# docker tag aced262afc58 localhost:49153/registry-test\nroot@precise64:~# docker push localhost:49153/registry-test\nThe push refers to a repository [localhost:49153/registry-test] (len: 1)\nProcessing checksums\nSending image list\nPushing repository localhost:49153/registry-test (1 tags)\nPushing 8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\nBuffering to disk 58367664/? (n/a)\nPushing 58.37 MB/58.37 MB (100%)\nroot@precise64:~# docker images\nREPOSITORY                      TAG                 ID                  CREATED             SIZE\nubuntu                          12.04               8dbd9e392a96        4 months ago        131.5 MB (virtual 131.5 MB)\nubuntu                          12.10               b750fe79269d        4 months ago        24.65 kB (virtual 180.1 MB)\nubuntu                          latest              8dbd9e392a96        4 months ago        131.5 MB (virtual 131.5 MB)\nubuntu                          precise             8dbd9e392a96        4 months ago        131.5 MB (virtual 131.5 MB)\nubuntu                          quantal             b750fe79269d        4 months ago        24.65 kB (virtual 180.1 MB)\nsamalba/docker-registry         latest              aced262afc58        6 days ago          24.58 kB (virtual 996.3 MB)\nlocalhost:49153/registry-test   latest              aced262afc58        6 days ago          24.58 kB (virtual 996.3 MB)\nroot@precise64:~# docker pull localhost:49153/registry-test\nPulling repository localhost:49153/registry-test\nPulling image 8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c (latest) from localhost:49153/registry-test\nError while retrieving image for tag:  (Internal server error: 400 trying to fetch remote history for 8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c); checking next endpoint\nroot@precise64:~# docker version\nClient version: 0.5.3\nServer version: 0.5.3\nGo version: go1.1\n```\nDebug output from docker\n```\nroot@precise64:~# docker -d=True -D=True\n[debug] runtime.go:139 Container 68d2516a52d8f7454250b35fe0142d28757e88f2ab1aa29d5f4a37c6e93607f2 was supposed to be running be is not.\n[debug] runtime.go:150 Marking as stopped\n[debug] runtime.go:220 Loaded container 68d2516a52d8f7454250b35fe0142d28757e88f2ab1aa29d5f4a37c6e93607f2\n[debug] runtime.go:220 Loaded container e361b01de04dbcbd4cde5aec6274f974bd0d28cdde2bf18fb1776148800c16d0\n[debug] runtime.go:220 Loaded container e732d4fce27d0659ad75fa9bcf6a3a0edac36a4725617a456cd70940c028c126\n[debug] runtime.go:220 Loaded container f355ebc8b3330385baa28b1293302daf95c7f9806f80ce44b2470ffd1ffc634c\n[debug] runtime.go:220 Loaded container f4f1849a3f86a85a37d23e05e24fae2ba1fe504006328d165b261eccc6d0921a\n2013/08/20 15:18:52 WARNING: Your kernel does not support cgroup swap limit.\n2013/08/20 15:18:52 Listening for HTTP on /var/run/docker.sock (unix)\n[debug] api.go:921 Registering GET, /images/search\n[debug] api.go:921 Registering GET, /containers/json\n[debug] api.go:921 Registering GET, /containers/{name:.}/export\n[debug] api.go:921 Registering GET, /containers/{name:.}/json\n[debug] api.go:921 Registering GET, /info\n[debug] api.go:921 Registering GET, /version\n[debug] api.go:921 Registering GET, /images/viz\n[debug] api.go:921 Registering GET, /containers/ps\n[debug] api.go:921 Registering GET, /containers/{name:.}/top\n[debug] api.go:921 Registering GET, /events\n[debug] api.go:921 Registering GET, /images/json\n[debug] api.go:921 Registering GET, /images/{name:.}/history\n[debug] api.go:921 Registering GET, /images/{name:.}/json\n[debug] api.go:921 Registering GET, /containers/{name:.}/changes\n[debug] api.go:921 Registering POST, /commit\n[debug] api.go:921 Registering POST, /images/create\n[debug] api.go:921 Registering POST, /images/{name:.}/push\n[debug] api.go:921 Registering POST, /images/{name:.}/tag\n[debug] api.go:921 Registering POST, /images/{name:.}/insert\n[debug] api.go:921 Registering POST, /containers/{name:.}/restart\n[debug] api.go:921 Registering POST, /images/getCache\n[debug] api.go:921 Registering POST, /containers/create\n[debug] api.go:921 Registering POST, /containers/{name:.}/kill\n[debug] api.go:921 Registering POST, /containers/{name:.}/start\n[debug] api.go:921 Registering POST, /containers/{name:.}/stop\n[debug] api.go:921 Registering POST, /containers/{name:.}/resize\n[debug] api.go:921 Registering POST, /auth\n[debug] api.go:921 Registering POST, /build\n[debug] api.go:921 Registering POST, /containers/{name:.}/wait\n[debug] api.go:921 Registering POST, /containers/{name:.}/attach\n[debug] api.go:921 Registering DELETE, /containers/{name:.}\n[debug] api.go:921 Registering DELETE, /images/{name:.}\n[debug] api.go:921 Registering OPTIONS,\n[debug] api.go:927 Calling GET /containers/json from @\n2013/08/20 15:18:58 GET /v1.4/containers/json\n[debug] api.go:927 Calling POST /containers/create from @\n2013/08/20 15:19:02 POST /v1.4/containers/create\n[debug] api.go:927 Calling POST /containers/{name:.}/start from @\n2013/08/20 15:19:02 POST /v1.4/containers/0bcfcd30c893/start\n[debug] network.go:350 Acquiring 0\n[debug] network.go:350 Acquiring 49153\n[debug] network_proxy.go:102 Starting proxy on tcp/127.0.0.1:49153 for tcp/172.17.0.1:5000\n[debug] container.go:796 Waiting for process\n[debug] api.go:927 Calling POST /images/{name:.}/tag from @\n2013/08/20 15:19:08 POST /v1.4/images/aced262afc58/tag?repo=localhost%3A49153%2Fregistry-test\n[debug] api.go:927 Calling POST /images/{name:.*}/push from @\n2013/08/20 15:19:30 POST /v1.4/images/localhost:49153/registry-test/push\n[debug] network_proxy.go:74 Forwarding traffic between tcp/127.0.0.1:53994 and tcp/172.17.0.1:5000\n[debug] network_proxy.go:96 111 bytes transferred between tcp/127.0.0.1:53994 and tcp/172.17.0.1:5000\n[debug] registry.go:90 Registry https://localhost:49153/v1/ does not work (Get https://localhost:49153/v1/_ping: EOF), falling back to http\n[debug] network_proxy.go:74 Forwarding traffic between tcp/127.0.0.1:53996 and tcp/172.17.0.1:5000\n[debug] registry.go:437 PUT http://localhost:49153/v1/repositories/registry-test/\n[debug] registry.go:438 Image list pushed to index:\n[{\"id\":\"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\",\"checksum\":\"sha256:94eba4f595b00527169b4b39606dc7a1d4aae63399917d8514acf403df731087\",\"Tag\":\"latest\"},{\"id\":\"2303339707bfb0f9414ce01031b39d9e5dee4b5dff7b1eb2a8f33e64b09cdd40\",\"checksum\":\"sha256:fff6cc96c4e34f268b33b5d55988b327fd04bcf7233033776a9d0c03869c3a82\",\"Tag\":\"latest\"},{\"id\":\"b1de96c1b4f061850aebcc457a648f9ba57df2d16a19f386f5d7267774219de5\",\"checksum\":\"sha256:f49efcef28f22dd3f6af3807c3f5c3786f58d8a6b94f3b4dd1d0757a37f31301\",\"Tag\":\"latest\"},{\"id\":\"4108ac77d4c287b131db80122767dde8f1482adc05c22f0ace58f7e4d53664b0\",\"checksum\":\"sha256:f02d0b5f81c12c169d7aa01176197cf09e076c0527603585f50dd835a09e9439\",\"Tag\":\"latest\"},{\"id\":\"04be77da07a8e5edb57d8a019453fd2baebb0c6bbff5933a89b36aa163a35fe1\",\"checksum\":\"sha256:e0cbb98b792bb4e66c345a614406667672ee051300abdf308b2523d6da8fae7c\",\"Tag\":\"latest\"},{\"id\":\"dc6f5fa24786e21b7df05800793744814c7b49dc5326b6ee68566fd0e8a64fd3\",\"checksum\":\"sha256:864f3fa18a871b401b90c84168925733ab25f33a34f1b7d84ac8e1eab32bea4b\",\"Tag\":\"latest\"},{\"id\":\"80842b7e0b3214fc1e6993c51ea8752cc572c79d66997c25673005392e6603c8\",\"checksum\":\"sha256:5db88144d76dda1577abd7e0bd249c673193dfda093bcba7b972f5629ae46919\",\"Tag\":\"latest\"},{\"id\":\"aced262afc58bc03ce46f01b7306e70a9e3c21a02cb45f216709fc10442da2ff\",\"checksum\":\"sha256:ae5b7d648a8ebcf567ca1114d8b1c73d5d7c4ab620b3d8374763b7059ad9b739\",\"Tag\":\"latest\"}]\n[debug] network_proxy.go:74 Forwarding traffic between tcp/127.0.0.1:53998 and tcp/172.17.0.1:5000\n[debug] registry.go:490 Auth token: [Token signature=68FARLNI0X0E6XOG,repository=\"library/registry-test\",access=write]\n[debug] network_proxy.go:96 2178 bytes transferred between tcp/127.0.0.1:53998 and tcp/172.17.0.1:5000\n[debug] network_proxy.go:74 Forwarding traffic between tcp/127.0.0.1:54000 and tcp/172.17.0.1:5000\n[debug] registry.go:345 Setting checksum for 8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c: sha256:94eba4f595b00527169b4b39606dc7a1d4aae63399917d8514acf403df731087\n[debug] network_proxy.go:96 495 bytes transferred between tcp/127.0.0.1:54000 and tcp/172.17.0.1:5000\n[debug] network_proxy.go:74 Forwarding traffic between tcp/127.0.0.1:54002 and tcp/172.17.0.1:5000\n[debug] network_proxy.go:96 1249 bytes transferred between tcp/127.0.0.1:54002 and tcp/172.17.0.1:5000\n[debug] network_proxy.go:96 386 bytes transferred between tcp/127.0.0.1:53996 and tcp/172.17.0.1:5000\n[debug] network_proxy.go:74 Forwarding traffic between tcp/127.0.0.1:54004 and tcp/172.17.0.1:5000\n[debug] network_proxy.go:96 58382603 bytes transferred between tcp/127.0.0.1:54004 and tcp/172.17.0.1:5000\n[debug] api.go:927 Calling POST /images/create from @\n2013/08/20 15:23:20 POST /v1.4/images/create?fromImage=localhost%3A49153%2Fregistry-test&tag=\n[debug] network_proxy.go:74 Forwarding traffic between tcp/127.0.0.1:54006 and tcp/172.17.0.1:5000\n[debug] network_proxy.go:96 111 bytes transferred between tcp/127.0.0.1:54006 and tcp/172.17.0.1:5000\n[debug] registry.go:90 Registry https://localhost:49153/v1/ does not work (Get https://localhost:49153/v1/_ping: EOF), falling back to http\n[debug] network_proxy.go:74 Forwarding traffic between tcp/127.0.0.1:54008 and tcp/172.17.0.1:5000\n[debug] network_proxy.go:74 Forwarding traffic between tcp/127.0.0.1:54010 and tcp/172.17.0.1:5000\n[debug] server.go:457 Updating checksums\n[debug] server.go:463 Retrieving the tag list\n[debug] network_proxy.go:96 2110 bytes transferred between tcp/127.0.0.1:54010 and tcp/172.17.0.1:5000\n[debug] network_proxy.go:74 Forwarding traffic between tcp/127.0.0.1:54012 and tcp/172.17.0.1:5000\n[debug] registry.go:246 Got status code 200 from http://localhost:49153/v1/repositories/library/registry-test/tags\n[debug] server.go:478 Registering tags\n[debug] network_proxy.go:74 Forwarding traffic between tcp/127.0.0.1:54014 and tcp/172.17.0.1:5000\n[debug] network_proxy.go:96 569 bytes transferred between tcp/127.0.0.1:54012 and tcp/172.17.0.1:5000\n[debug] network_proxy.go:74 Forwarding traffic between tcp/127.0.0.1:54016 and tcp/172.17.0.1:5000\n[debug] network_proxy.go:96 386 bytes transferred between tcp/127.0.0.1:54008 and tcp/172.17.0.1:5000\n[debug] network_proxy.go:96 651 bytes transferred between tcp/127.0.0.1:54014 and tcp/172.17.0.1:5000\n[debug] network_proxy.go:96 497 bytes transferred between tcp/127.0.0.1:54016 and tcp/172.17.0.1:5000\n```\n. ",
    "giskarda": "same happening here :(\n. ",
    "jamescarr": "Same problem. I'm wondering if it is because we build a base docker image that is public and push it to the public registry, with successive images being based on that? Perhaps it is looking for the base layers on the local registry and they don't exist because they exist in the public registry?\n. @mattwallington are you using the S3 backed store or the local filesystem?\n. MAybe this is the piece I'm missing? I didn't run nginx because I assumed that the docker container either had it baked in or some other things were going on. If that's not the case then I am more than happy to try putting nginx in front of it.\n. Yep.. proxied behind nginx, same problem. :(\n. I basically just run it via the docker container.\nI'll try just running it straight outside of docker tonight.\nSent from my iPhone\nOn Aug 23, 2013, at 10:45 AM, Matt Wallington notifications@github.com wrote:\n\nJames, if it's not working without nginx, it's definitely not going to work with Nginx. Are you launching gunicorn using the method Sam documented on the docker-registry github repo page?\n\u2014\nReply to this email directly or view it on GitHub.\n. So I ran docker-registry fresh from git sources via the same gunicorn command with both multiple and single workers. No dice. I switched to other projects today and returned to it tonight, provisioning a fresh box with the latest docker release and running the most recent samalba/docker-registry image against S3.\n\nIt works! :+1:\nI can't say how happy I am because this has been a major pain point for me as the registry is a crucial part of our architecture. Could others encountering this problem verify that it is no longer an issue?\n. Yeah! It's hella fast!\n. https://github.com/dotcloud/docker-registry/pull/60\n. I think it's a decent idea. I only needed to modify the number of workers but other people may want to modify the options more.\n. Might try again? I am using it successfully, it sounds like a network connectivity issue (I noticed a problem this a.m.)\n. I'm a dummy... env variable names changed in a recent release.\n. ",
    "shin-": "I'll close it then. Feel free to reopen if the same issue resurfaces.\n. Rewritten from scratch, see #68 \n. Might be interesting to take a look at https://github.com/dotcloud/docker-registry/tree/indexer ; i started working on this on  a hackday but never really got a chance to go all the way, but it's probably a good starting point for anyone interested in listing images in a private registry.\nHTH\n. Indeed, we had noticed a bug with the latest version but this has nothing to do with the issue you're having. Are you sure you have the right host/port info? Can you hit curl https://my.docker.registry.com/v1/_ping ?\n. Hi,\nCan you clarify what you mean by \"not working properly\"? \nIf you're trying to authenticate on your private registry through docker, I will say that this is unfortunately not implemented yet, but you can follow the progress of this particular feature here: https://github.com/dotcloud/docker/pull/1564\nIf you're under the impression that the nginx configuration is not taken into account by your dotcloud service, or you need help setting it up, I think you'll have a much better shot by contacting support!\nHope that helps.\n. Hi,\nFirst of all, thanks for the contribution! I just commented on a small naming issue, as soon as you can get this resolved I'll be able to merge this.\nThanks!\n. Thanks!\n. The requirements file specifies use of boto==2.8.0. Have you tried using this specific version?\n. You'll want to report the issue to the boto maintainers I think.\n. Thanks for the report. This is actually an index bug... pinging @dhrp and @kencochrane :)\n. thanks @kencochrane =)\n. Thanks! @samalba is the maintainer of the public image and he's on vacation atm so it may take some time until it gets updated on the index. We'll try to let you know when that happens.\n. @shykes sure! It's done: docker pull -t latest stackbrew/registry\n. Awesome, thanks for the contribution!\nJust a minor nitpicking: can you make it so the contents in README.md wrap to 80 columns? Once that's done I'll be able to merge it.\nThanks!\n. Perfect, thank you!\n. Probably a client bug/shortcoming. I'll try to make some time to look at it, as the docker client should send credentials when it's requested to.\nAs a workaround, can you try requesting auth only on /v1/repositories/*?\n. Okay, I think I fixed the problem in the client. Can you try to run from this branch? https://github.com/shin-/docker/tree/auth-on-401 ? I've uploaded a binary for convenience here\n. I'll have to take a closer look at it. Thank you for the report!\n. Okay, braindumping here, sorry if it doesn't make a ton of sense as is.\n- First idea was to implement {error 401 -> retry with basic auth} for all registry requests\n  - This seems to work for pull\n  - Push fails with Failed to upload metadata: Put http://localhost/v1/images/[ID]/json: http: Request.ContentLength=437 with Body length 0\n  - http.Request.Body is a ReadCloser so it can only be read once unless we implement a custom Request class where Body can be rewinded\n    - This creates new limits and might break some existing things (basically means anything we pass as request body should be able to do Seek(0), i.e. no streaming)\n- We could be lazy and always send credentials\n  - But it's basic auth, i.e. vulnerable to MITM attacks and such if not send over SSL.\n    - Could be mitigated by only allowing it over HTTPS\n. Thanks for the thoughts guys, this is helpful!\nAs soon as I can make some time for it I'll take another look at the whole thing.\n. I started a pull request here: https://github.com/dotcloud/docker/pull/2339. I've been running some simple tests against a local private registry behind Apache, but if you guys could test it against your own configuration(s) it would be super helpful.\nFWIW, here's the Apache config I'm using.\n```\n\n  ServerAdmin joffrey@dotcloud.com\n  ServerName my.registry.io\n\n    Order allow,deny\n    Allow from all\n    AuthType Basic\n    AuthName \"auth required\"\n    AuthUserFile /etc/apache2/htpasswd\n    Require valid-user\n  \n\n    Satisfy any\n    Allow from all\n  \n\n    Satisfy any\n    Allow from all\n  \nProxyPreserveHost  On\n  ProxyRequests      Off\n  ProxyPass          /  http://localhost:5000/\n  ProxyPassReverse   /  http://localhost:5000/\nErrorLog ${APACHE_LOG_DIR}/error.log\n  LogLevel warn\nCustomLog ${APACHE_LOG_DIR}/access.log combined\n\n``\n. I'll try to take a look asap. Anything special about your configuration that could help me pinpoint the problem?\n. docker version?\n. ah yes, sorry, I see what the problem is now. At the moment basic auth is only enabled over HTTPS. See https://github.com/dotcloud/docker/pull/2339 and https://github.com/dotcloud/docker/pull/2687 for more information.\n. @ehazlett We don't do anything fancy, we let the go http client do that for us. Maybe try to write a simple go test case that just hits the/v1/_pingroute on your registry. If that doesn't work you'll probably need to take it up to the go maintainers, otherwise we can look further into what could be causing it in docker.\n. @bacongobbler Cool, glad you were able to make it work!\n. @iapilgrim Have you tried adding the certificate to your system's certificates? Clearly this seems to be an issue with your cert. Alternatively, which version of docker are you using?\n. Have you tried adding your CA certificate in/usr/local/share/ca-certificates/inside the vagrant box?\n. That should be a docker issue then. Could you post on the [docker-user mailing list](https://groups.google.com/forum/#!forum/docker-user) with that info? I'm afraid this is not really my area of expertise.\n. Let's continue the discussion on https://github.com/dotcloud/docker/issues/1220. Closing this one.\n. @rogaha Thanks for taking the time to do this! I have a few questions before I can analyze this further:\n- can you elaborate on the terminology here? Which URLs do you refer to bycdn_indexandstaging_index` ?\n- which repository(ies) did you pull? How many time did you repeat the process?\nThanks!\n. So you spawn 50 concurrent jobs to download chunks from S3 for each layer download? Doesn't S3 have a limit on these connections?\n. LGTM\n. You might want to take a look at #82 \n. :+1:  LGTM\n. All the JSON parser implementations I know of support strings as top-level objects.\nAre you encountering a problem with a specific client because of this?\n. From taking a quick glance at the doc, couldn't you just use value instead of json ? [link]\n. This is my personal opinion and I'll admit to be biased, but honestly, if you use \"a na\u00efve Javascript library that parses JSON data using eval\", you have more serious problems than accepting to parse strings. Every modern browser has implemented JSON.parse for several years now, and even moreso for every server-side javascript implementation.\nWhile the documentation claims that \"JSON implementations in other languages conform to that same restriction to preserve interoperability and security\", It doesn't seem to hold true for Javascript,\n```\n\nJSON.parse('\"latest\"')\n\"latest\"\n```\n\nPython,\n```\nIn [1]: import json\nIn [2]: json.loads('\"latest\"')\nOut[2]: u'latest' \n```\nor Go.\nI could be wrong, but it seems to me that this is just an archaic standard inherited from the dark ages of Javascript. I'd rather \"break\" it and have types that make sense regarding the value I'm expecting to receive (i.e. receive a string when I'm expecting text, rather than an object containing a \"text\" key...)\nThis is all highly philosophical, and @samalba may have an entirely different opinion on the matter, but that's my 2 cents. =)\n. Thanks for the report!\nIt's totally appreciated - we'll look into fixing this asap.\n. LGTM!\n. cc @samalba @kencochrane \n. FIxeneered it!\n. Thanks! Can you look into making the pep8 tests pass? I'll take a closer look when I have a moment.\n. Hi!\nThis is a duplicate of #82 ! The docker client needs to be patched, and we're working on it with PR dotcloud/docker#2339.\nHope that helps!\n. This is an index issue (/feature request), and would be more appropriate on the docker-user mailing list.\nClosing this!\n. I don't have much experience with that distro family, but would be more than happy to accept a pull request with this information! =)\n. It's the compression algorithm used by the docker client. In the \"early days\" we were using bzip2 compression, but we switched off it for performance reasons.\nI don't think we can really do without, unfortunately.\n. I think it's fine, but we have to make sure this doesn't compromise security in token based auth environments (namely the public registry)\n. Merging. Thanks for the contribution!\n. LGTM\n. A pull request would be most welcome, absolutely!\nThanks for sharing =)\n. Cool, LGTM!\n. I think your issue pertains to the index (index.docker.io), not the docker-registry.\nI'll close this issue because it's not the right place to discuss this, but I'll say that getting more information on the repository page is something we are actively working on. If you want to discuss this further, best place to do it would be on the docker-user mailing list.\nHope that helps!\n. LGTM. =)\n. the lru_cache section should be optional, you can safely remove it if you don't have and/or don't want redis. It's in config_sample.yml because this particular file aims to showcase as many of the config options as possible. =)\n. Wow, @dmp42 digging up graves there :D @bacongobbler Are there still outstanding issues with the nginx configs or can we close this?\n. Do you want to update the sample nginx config to reflect this, or do you have another fix to suggest that you think would fit best?\nThanks for the report!\n. I checked that this has been fixed. Closing.\n. LGTM\n. Are there other storage backends that have their own config variables that could be included here?\n. LGTM\n. Closing since this is a docker index issue. If the problem persists please send an email to index@docker.io\n. Thanks for the report, we'll look into it!\n. Do we have to dissociate test/ and dockertest/ ?\nOtherwise LGTM\n. It's mostly aesthetic but I also like it better that way. LGTM!\n. Waiting on dotcloud/docker#3140 . I proposed a fix but it was veto'ed by the core team, not much more I can do unfortunately.\n. Closing; dotcloud/docker#3140 has been merged.\n. Fixed in 2794c8b7541c10bd9d94678e0ca9583da41be9fb\nThanks! :)\n. LGTM\n. The error message you get seems to indicate that you are trying to push ubuntu/ping, which would be the image on the index. Since you don't own the \"ubuntu\" namespace, you're getting denied. You need to docker push http://localhost:5000/ubuntu/ping.\n. Can you try again? This was most likely a network issue on our end (docker index). Let us know if it persists.\n. If you just rename an image in docker (with docker tag for example) and repush it, it won't repush the layers on the registry (since they're already there). So I'm not sure what you mean?\n. For suggestions and problems pertaining to the index (everything that you can see and do on http://index.docker.io ), please send an e-mail to index@docker.io !\nThis issue tracker is for issues encountered while using the open-source registry project, usually in the form of private registries.\n. As remarked, this is a docker issue. Feel free to reopen if necessary.\n. Awesome, thanks!\n. For suggestions and problems pertaining to the index (everything that you can see and do on http://index.docker.io ), please send an e-mail to index@docker.io !\nThis issue tracker is for issues encountered while using the open-source registry project, usually in the form of private registries.\n. No worries! Thank you for the feedback.\n. While we're at it, can we replace stackbrew/ubuntu by ubuntu? Other than that, LGTM =)\n. WFM now, let's see what Travis has to say.\n```\n    # rm -rf .tox/\n    # tox\n    py27 create: /home/shin/work/registry/.tox/py27\n    py27 installdeps: -r/home/shin/work/registry/requirements.txt, -r/home/shin/work/registry/test-requirements.txt\n    py27 runtests: commands[0] | coverage run -m unittest discover -s /home/shin/work/registry/test\n    ........................................\n    ----------------------------------------------------------------------\n    Ran 40 tests in 57.541s\nOK\npy27 runtests: commands[1] | coverage html -d reports --include=./* --omit=*test*,.tox*\npep8 create: /home/shin/work/registry/.tox/pep8\npep8 installdeps: -r/home/shin/work/registry/requirements.txt, -r/home/shin/work/registry/test-requirements.txt\npep8 runtests: commands[0] | flake8 /home/shin/work/registry\n________________________________________ summary _________________________________________\n  py27: commands succeeded\n  pep8: commands succeeded\n  congratulations :)\n\n``\n. Spin up a local registry using the config_mirror.yml configuration ; thendocker pull local.registry/[repo.name]whererepo.name` is a repository that can be found on the official registry. You should be able to obtain the images you requested. Subsequent pulls should be much faster since the layers are saved locally.\n. There's currently an issue with the tags cache that causes the auth token to not be consumed properly. For now, you can disable the tag cache in the config to be able to test locally.\n. Thank you for the additional information. I'll take a crack at it this week.\n. Guys, can you try #322 and let me know if it fixes the issue for you?\n. There's only one index, \"index.docker.io\". By default docker login will effectively register/login on the index, but for everything else, it's logging in against a registry. The terminology here is fine as far as I'm concerned.\n. Thanks!\n. Issues pertaining to the index (https://index.docker.io) and suggestions for improvement are better forwarded by e-mail to index@docker.io .\nThis issue tracker is used for bug reports and issues with the open-source registry. Sometimes the distinction is unclear and that's a totally okay mistake to make, so don't worry!\nAlso -- closing this issue, which is being tracked internally.\n. Do you have a config.yml in the config/ folder?\n. This should be fixed by #553. @dmp42 do you want to leave it open until we release the next version or should I close this?\n. I'll review this ASAP. Thanks!\n. LGTM!\n. Personally,\n- I think the run.sh modifications are fine. As you outlined earlier, it doesn't serve any practical purpose right now.\n- Just renaming sqlalchemy.py to something like db.py, models.py or data.py seems like it would be a more manageable solution (I have nothing against absolute imports, but if such a refactor should happen, I'd rather it be independent of that PR).\nThanks for the summary and great work so far, hopefully we can have that merged in soon!\n. Yes... I think we should cross that bridge when we get to it. =)\n. Thanks -- LGTM!\n. Thanks!\n. Thanks!\nNot a fan of the additional dependency in requirements.txt (which to be fair needs to be cleaned up anyway). Can we maybe comment it out and add a comment saying \"add this when pushing to Heroku/Dokku\"?\n. @mzsanford Please use the official registry image instead. The stackbrew namespace is not being updated at the moment (but we're looking into fixing this, too).\n. Possibly related: dotcloud/docker#2938\n. For issues and suggestions related to the index (anything that happens on index.docker.io, including trusted builds) please use the support-index@docker.com e-mail address.\nClosing since this issue is not related to the docker registry.\n. Thanks for the contribution!\nPlease make sure the tests pass, both unit tests and pep8. You can run them locally using tox.\n. LGTM\nThank you for the contribution!\n. LGTM\n. Thanks!\n. Sorry that took so long. Works for me, only had to update the boto dependency.\nLGTM.\n. I'm fine with run_gunicorn living there for now. We can open a new PR if it proves to be detrimental at some point.\n. Thanks!\n. @wking I've rebased it before merging, see https://github.com/dotcloud/docker-registry/commit/9baaddaf2a836f047e785c78ffa27fa35d036a2a . If you spot any inconsistency or see something I missed don't hesitate to let me know / call me out on it, but I think we're good.\n. Thanks everyone for the reports and detailed posts. I'll be looking into this and report back asap (hopefully with a PR that fixes it!)\n. Alright, I've been able to set up a test environment and I think I know what the issue is.\nAs it is implemented right now, basic authentication is only supported over HTTPS (see dotcloud/docker#2339). The docker client will not send basic auth headers when pushing/pulling on a registry over HTTP.\nThis is done by design to prevent people sending their credentials over insecure channels, and I don't believe we will move away from this. Using SSL should get rid of the issue.\nWe had a PR going to allow unverified certs but it has been stalled, so if you're using self-generated certificates, make sure to add the certificate to your client machine(s) too.\nIf those of you who have hit the issue could confirm that they were actually trying to login over HTTP, that would help a ton. In the meantime, I'll make sure we display a clear error message on the client when that happens.\nThanks again for the reports.\n. Created dotcloud/docker#5083 \n. Thanks guys, I'll be closing this now that dotcloud/docker#5083 has been merged. Feel free to open a new ticket for related issues.\n. Please contact support for hub-related issues. I've notified people internally about the issue but having those tickets in the system versus on an old github report on a deprecated repo is infinitely more valuable and helps us track and update you about status much better.\n. :+1: \n. LGTM!\n. LGTM\n. Conflicting with #247 I think\n. Thanks for the contribution! LGTM.\n. Thanks!\n. Thanks for the contribution! If you can clarify the situation with the additional requirements I'll be happy to merge it.\n. Thanks!\n. Thanks Eric!\n. Marcus,\nCan you provide the registry logs? That would help us a lot in identifying the issue here.\nThanks!\n. Thanks! Is there an ancestry file for your 242172258e6f899e7198d467184fb8765e979f6c3ab7b33ee8d76fb1f76ee139 layer?\nYou would find it at {STORAGE_ROOT}/images/242172258e6f899e7198d467184fb8765e979f6c3ab7b33ee8d76fb1f76ee139/ancestry\n. Interesting. Also makes sense since that layer didn't get pushed. Have to investigate why docker would try to pull it. Thanks for the info. I'll let you know when I have something.\n. woops! Thanks Cristian :)\n. LGTM!\n. Thanks!\n. We're open to PRs, but I'd hold off until #317 gets resolved because it might impact this part of the API heavily.\n. Please send index-related issues and suggestions to support-index@docker.com\n. @cressie176 There is no username requirement on private registries.\n@ccverak Does it work if you docker pull the image first, then docker run?\n. Sorry, we've been a little slow on that, I'll make sure this is sorted out today.\n. Official registry repository now has tag 0.6.8 (and latest points to 0.6.8, too)\n. Hi @cattz ,\nlib/signals.py might interest you. We always welcome pull requests :)\n. @kencochrane @samalba @dmp42 \n. > what happens with current version of docker? Removing the cookie is ok I think, however is the token sent for every single request? Or does the client require changes to make this work?\nCurrently the token is sent for every request done by the official docker client, unless something has changed that I'm unaware of. So normally no client changes are required.\n\nam I correct?\n\nYup, I thought we were gonna be able to get rid of the cookie entirely too, but it looks like it's still used  for carrying the checksum between the \"put layer\" and \"put checksum\" requests, so I went for the most conservative changes.\n\ne.g.: I guess we could use flask.g instead of flask.session.\n\nflask.g gets wiped when a request ends, and we use the session cookie explicitly to carry information over multiple requests, so I don't think it's the replacement we're looking for.\n. Getting rid of flask.session entirely is a valid discussion to have, I think, but independently from this PR.\n. @samalba I think we still do, but you implemented that so you might know better.\nStoring checksum after layer PUT: https://github.com/dotcloud/docker-registry/blob/master/docker_registry/images.py#L262\nFetching checksum in session during checksum PUT: https://github.com/dotcloud/docker-registry/blob/master/docker_registry/images.py#L283\n. Don't know if we can afford to drop support for docker < 0.10\n. Pretty much what you said, except we use it internally to authenticate requests made by the index. Admin tools is a good use case for it too, but for most people using private registries it's not particularly relevant.\n. It's true that we don't have anything public (other than the code itself) about the format of X-Signature. Like I said, it's irrelevant to people using standalone private registries (i.e. 99.9% of people who will read this) and as such I understand the argument of removing it from the doc altogether, but ideally for us at dotcloud we'd like to keep this info somewhere. Maybe there's a better place than this README, arguably.\n. Large push/pulls being bufferized?\n. I replied on the mailing list. Seems likes this stems from an index/registry confusion (which is a common problem).\nNot sure that these CORS headers are necessary at all, because I don't think there's any reason for the registry API to be used by browser apps -- especially not the official registry.\n. We provide a public search endpoint on the index which seems best suited for that particular use case: https://index.docker.io/v1/search?q=ubuntu\n. No ! Sorry, I know this is really confusing -- like I said on the mailing list, index.docker.io isn't the registry, it is our index, which is a different project entirely, and closed source. In standalone mode, the registry will emulate the index to the best of its ability to make pushing/pulling to private registries as seamless as possible, which is why some endpoints (namely, those defined in the index.py file) will be replicated in both APIs.\nTo the point, changes made here will not have any influence on the index.docker.io API. It's probably valid to want the /v1/search endpoint on the index be cross-origin ready though. Can you send an e-mail to support-index@docker.com describing your use case and requesting cross origin headers on those endpoints? It will be easier for us to follow up with the people working on the index project that way.\nThanks!\n. Yes, it will work for private registries.\n. > But let's summon @shin- and have his opinion on all this! :-)\nWho dares summon me?!\nFor real though, setup-config.sh is the old script we had before we introduced support for environment variables, so it's pretty much legacy code at that point. I'm totally OK with getting rid of it.\n. Thanks! Left a couple comments.\n. Ok, LGTM!\n. Ok, LGTM! =)\n. Just one last thing, now that config_test.yml has been removed you need to update the tox.ini file to use config_sample.yml instead.\n. >  are we confident these changes will work for docker prod? how can we test that as fast and painless as possible?\nI don't expect this to break anything, and it will get through staging first anyway.\n. LGTM!\n. lgtm!\n. Would releasing a nova-registry image with glance and swift baked in be a viable solution?\n. I don't think we ever import it explicitly anyway\n. mostly to avoid unnecessarily breaking things.\n. sure\n. You can use the mirroring_fixes for now. It fixes this bug and should make it into master soon.\n. done\n. cool, LGTM\n. 'kay, LGTM! =)\n. Yep -- beta testers have confirmed this works for them, so it's ready to be merged, pending you guys' approval\n. LGTM,\njust wondering what purpose docker_registry.server will serve?\n. This is an index issue. This change was made for security reasons. Please contact support-index@docker.com if you need further assistance.\nThis issue tracker is solely for the open source registry issues, i.e. issues encountered when deploying the code found in this repo or using the official registry image.\nClosing.\n. No such change has been made to https://github.com/dotcloud/docker-registry/blob/master/docker_registry/tags.py , so I am 100% sure that he's talking about the similar index route.\n. > the closed-source code handled registry-side endpoints\nThat's not actually the case =3 just so happens that we have repositories and tags resources on both sides, and we're consistent with our REST, so some similitudes happen with routes and stuff.\nHence the confusion, I guess.\n. Have we ruled out the boto library as a potential cause here? Have we at least reported the issue over there to see if they've encountered the issue before?\n@mcobzarenco @dlaidlaw People above have suggested that not using AWS_REGION or the s3_region config key solved the problem for them, have you guys tried that?\n. Can you add this to the 0.7.1 changelog? https://github.com/dotcloud/docker-registry/commit/3faf0297f2aa552eae018e5c3c8788545abea1c1\notherwise lgtm!\n. It's already in master, just having it documented in the Changelog.md file would be great. :)\n. Does global in python only apply to the current module? Otherwise redis_conn would get overwritten.\n. LGTM\n. I don't think this PR creates any problem that wasn't here before.\nWhat's most likely to happen is we overwrite tags information while the redis is down, and once it comes back up, tags would be out of sync, which is unfortunate but not exactly critical.\nOther than that, image IDs are unique and can only be written once.\n. Thanks guys, I'm looking into it.\n@rlpowell, docker 0.12.0 for you too?\n. What @wking said :)\nClosing this as a result.\n. Hi!\n\naccording to the instructions, when I (or any user) pushes to the registry, it should prompt to create an account.\n\nCan you point me to the place you were able to read this? It's actually wrong, standalone registries don't require authentication unless you combine them with an Apache or Nginx frontend that requires authentication. If this is doc we have control over, we'll want to correct it =)\nOther than that, the behavior you're seeing is the expected one (the user creation endpoint just responds true to stay compatible with the official registry protocol, there is no user management on standalone registries).\n. @shayneoneill see my response here: https://github.com/dotcloud/docker-registry/issues/427#issuecomment-46267429\n. The problem is that you have no repository called localhost:5000/xm. Try tagging an image before pushing docker tag <image_id|repo_name> localhost:5000/xm\nHTH\n. This is just intended as a short term fix, we agree that there's a bigger issue at hand, please bear with us.\n. Is standalone set to true in your configuration file?\n. sorry, this is a security hotfix that went through and we released 0.7.2 with it in a hurry. We'll do a better job of documenting these kind of changes in the future, although you can find the information in the Changelog at the moment.\nLet me know if it fixes the issue and we'll close this.\n. This has been fixed in boto 2.25.0 and we're requiring boto 2.27.0\n. LGTM\n@dmp42 \n. @wking short term is this fix, medium term (i.e. next release) is fixing _walk_object in config.py\n. If you want to check for tag existence, just hit /v1/repositories/<repository>/tags/<tag> instead. This works as intended AFAIC\n. Thanks for the report. \nWe generally try to avoid rebuilding images for that exact reason, but we had to here to fix a security vulnerability in ubuntu. I'll look into fixing this, but I would highly recommend you upgrade to the latest version (0.7.3) instead.\n. Official images for 0.6.8 and 0.6.9 will be updated sometime today.\n. Pretty sure this was fixed by #456 \u2014 please reopen if that's not the case.\n. LGTM\n. ``` python\nIn [16]: requests.head('http://registry-1.docker.io/v1/images/03b14aa32df28f1f0bbb5c17cfb9991a149982396370d899b6f47d892feaff02/layer', headers=headers)\nOut[16]: \nIn [17]: requests.get('http://registry-1.docker.io/v1/images/03b14aa32df28f1f0bbb5c17cfb9991a149982396370d899b6f47d892feaff02/layer', headers=headers)\nOut[17]: \n```\n. I don't think there's a written spec, but @unclejack might have notes somewhere. Client implementation is pretty straightforward: https://github.com/dotcloud/docker/blob/master/registry/registry.go#L266 . The HEAD request is made to verify the remote server supports bytes range.\n. Does it work if you use the 0.7.3 tag instead of master?\n. @dmp42 \n. LGTM\n. @lorieri @sprin Can we close this and continue the discussion on #515 then? (sorry just catching up on reviews)\n. Can't say no to more test coverage =) LGTM!\n. Just to complete what @dmp42 said, for questions/issues regarding the official registry, please contact support@docker.com =)\n. Looks like #481 would interest you.\n. lgtm\n. > Then let's remove these already CORS-enabling snippet, clean-up all that and have application-wide setting.\n+1 !\n. Looks fine. LGTM. Maybe @ewindisch should chip in to make sure this is not making us vulnerable to something, but I don't believe so.\n. That's very odd. Does that happen consistently (every time you try to pull this repository)? From your logs it looks like the call to store.get_content() in get_tags() might be failing on the tag_latest file.\n. @dsw88 I think I identified the issue, could you give #553 a bash when you have some time to make sure I got it right?\nThanks!\n. Wow, thanks for the report.\nSince this is a Docker Hub issue, we're going to continue tracking it internally and I'm going to close this, If you want to follow up on it in the future please mention \"Docker Hub #1329\". Thanks!\n. @dmp42 \n. LGTM! :shipit: \n. yeah, once the README comments are addressed, this LGTM\n. Yes, the registry hub API is different from the registry API. It's confusing and it sucks, and we plan to work on this on the hub side somewhere down the line. Ideally the right way to proceed is to obtain a token from the hub and then query https://registry-1.docker.io/v1/repositories/centos/tags using said token.\n. I think that's fine. LGTM.\n. there's no index.docker.io URL in the code. Our internal config could be updated though.\n. This was addressed long ago in #298 -- see this comment: https://github.com/docker/docker-registry/issues/298#issuecomment-39845868\nWe don't support auth over plain HTTP, period. Rationale is because the auth used by the registry is basic auth, you never want this information to travel unsecured.\nError message for this issue should be clear in the client as per https://github.com/docker/docker/pull/5083 , if it's not anymore that's something we should fix, but otherwise I think we can close this.\n. LGTM!\n. @mattheworiordan To clarify, are you using the same redis instance for both your registry servers, or one for each?\n. How do you expect the 2 caches to synchronize with each other then? I'm not saying it's impossible to do (although it's definitely not something the registry does now), it's just very confusing how you expect 2 servers to be completely independent and still share data with one another.\nIn other words, that design won't work. If you want to use a cache with multiple registry instances, you'll need to make a single (ideally replicated) redis cache accessible.\n. As advertised in the past, we do not support authentication over HTTP. If docker tries to send credentials over HTTP, then it is a bug and it needs to be fixed, but very clearly we have no intention to support non-HTTPS auth.\n. > . I think, docker does not fully support the \"username:password@my.registry.url\" syntax that you use.\nThat is correct. The correct way to use auth over a private registry is to docker login on that private registry, then push the image normally.\n. Can I close this or is there anything that's still unclear regarding private registry auth?\n. When pushing to a private registry over HTTP, do you not get this message? I'll look into adding a note for the HTTPS stuff in our docs.\n$ docker push my.registry.io/ubuntu\nThe push refers to a repository [my.registry.io/ubuntu] (len: 1)\nSending image list\nPushing repository my.registry.io/ubuntu (1 tags)\n511136ea3c5a: Pushing \n2014/04/08 16:40:27 HTTP code 401, Docker will not send auth headers over HTTP.\n. I may have a chance to work on this in the next week, I will let you know. \n. Closed in favor of #825 \n. Shouldn't we check for local results first?\n. You may have pushed an image to your local mirror and not to your source. Not saying that's how I would do it, but it's technically possible so I think it should be accounted for.\n. > I'll look into enhancing this PR. So how should we handle merging the results? Should we flag the private registries entries with official=true?\nHm, \"official\" has a specific meaning, I wouldn't want to override that. \nYou can either merge the two arrays and ignore duplicates, or let local results override remote results with the same name (It might be a bit counter-intuitive because the remote results have more info, but because the client will end up pulling the local repository anyway it's better to maintain clarity on that point in my opinion).\n. ```\n\nJSON.parse('true')\ntrue\n```\n\nLooks like valid JSON to me =/\n. @wking I can't verify that the content of fname is what I expect it to be, but I can make sure that tag_path() will return the proper result when given the correct parameters. I'm pretty sure that fixes it, but even if it didn't, it's still a more sanitized way of doing things.\n. @dmp42 Added test\n. No, I'm not calling get_tags() directly because it's an S3-specific test and I don't want the imports to get crazy. Instead, I'm reproducing the current master's get_tags behavior and check that I get a FileNotFound error, then verify that I am able to access the data using the new get_tags behavior.\n. @fantapop Care to share your mirror's logs when pulling of an image happens?\n. That's weird, I don't see any layer pulling happening. Did you remove the busybox images from your local environment? If the client doesn't see anything in the ancestry it needs to pull, it won't trigger the download on the server side. That might be the issue here from what I see, let me know how that goes.\n. Cool, glad you were able to solve it. I'll close the ticket for now, @fantapop if you're observing different results feel free to follow-up.\n. I think leaving it as a warning makes the most sense. People who don't really care about it can safely ignore it, but it won't fly under the radar of people who intend to use it but end up misconfiguring it.\n. Don't run from the master branch, or at least pull the latest changes. But you'll have a much better time sticking to tagged versions. =)\nFeel free to reopen if that doesn't fix it.\n. A few comments, other than that should be good to go :+1: \n. LGTM, merging\n. LGTM too.\n@bacongobbler Can you try and create a ticker for this on docker/docker? Either it's a config issue, or a client one.\n. What happens if you docker pull registry:0.8.1?\n. You probably need to run daemonized, i.e. docker run -d -p 5000:5000 registry:0.8.1.\nAs for the search endpoint, you need to enable a search backend if you haven't already: https://github.com/docker/docker-registry#search-engine-options\n. Explanation in the README is straightforward, you most likely want to use something like this:\nsearch_backend: sqlalchemy\n  sqlalchemy_index_database: sqlite:////tmp/docker-registry.db\n. You'll need to install system dependencies (sqlite3) and python dependencies (sqlalchemy), the DB will then be created automatically.\n. You need to enable a search backend, see this section of the README for details\n. Hi,\nSorry \u2013 that's not something we support at the time. A patch adding sentinel support would of course be most welcome :)\n. Try this instead:\ndocker run -d -p 5000:5000 -e MIRROR_SOURCE=<your_source> MIRROR_SOURCE_INDEX=<your_source_index> registry:0.8.1\n. In that case \nyour_source = https://registry-1.docker.io\nyour_source_index = https://index.docker.io\nThen when pulling an image, immutables will be fetched on the source and stored by the mirror.\n. Mirroring the official registry isn't compatible with authentication, because the basic auth header overrides the auth token that's needed by the official registry.\n. Like I said, you can't use authentication and mirroring together, so you'll have to find another way to limit access to your private registry (if you're only going to use it as a mirror to the official registry, there's probably no harm in having it accessible anyway?) You could use firewalling to limit access to clients in a certain IP range for example.\n\nIs there a way to sync all images to my registry in advance?\n\nThere isn't. That would take months even if there was...\n. Please share your registry logs.\n. For security reasons, you can't mirror private repositories.\n. Ah nevermind, looking into it.\n. You're still sending auth headers:\nHeaders: {'X-Docker-Token': u'true', 'Accept-Encoding': u'gzip', 'Authorization': u'Basic enVicnlhbjp3aG9hbWk=', 'User-Agent': u'docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-24-generic os/linux arch/amd64'}\nRemove you ~/.dockercfg and try again.\n. Looks like a network issue on your side.\nI'll close this issue since we've determined there's no bug with the registry. If you need more help setting this up, please use the docker-user mailing list, which is more appropriate for general support (and will allow more people to see it and chime in).\n. LGTM\n. LGTM\n. LGTM\n. :+1: LGTM\n. We could do something like node had in the past (before npm became what it is now) with their list of modules. It can be a wiki page, a Markdown file in the main repo, or a secondary repo containing only a README. That way the content can stay community-driven and there's no additional maintenance required.\n. Are you using HTTPS on the server side? Authentication will only work over HTTPS. Normally we would have a nice error message telling you this when you docker push but an update might have unintentionally disintegrated it.\n. lgtm\n. cache is only used for the diff worker (disabled on private registries) and mirroring (if you enable it). Performance related caching is cache_lru.\n. THanks @lsm5 ! \nUnfortunately I'm getting the following exception when I try to test the code:\n04/Dec/2014:17:12:17 +0000 ERROR: Exception on /v1/repositories/toto/toto/tags [GET]\nTraceback (most recent call last):\n  File \"/home/joffrey/.envs/registry/local/lib/python2.7/site-packages/flask/app.py\", line 1817, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/home/joffrey/.envs/registry/local/lib/python2.7/site-packages/flask/app.py\", line 1477, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/home/joffrey/.envs/registry/local/lib/python2.7/site-packages/flask/app.py\", line 1381, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/home/joffrey/.envs/registry/local/lib/python2.7/site-packages/flask/app.py\", line 1475, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/home/joffrey/.envs/registry/local/lib/python2.7/site-packages/flask/app.py\", line 1461, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/home/joffrey/.envs/registry/local/lib/python2.7/site-packages/docker_registry/toolkit.py\", line 279, in wrapper\n    return f(namespace=namespace, repository=repository, *args, **kwargs)\n  File \"/home/joffrey/.envs/registry/local/lib/python2.7/site-packages/docker_registry/toolkit.py\", line 252, in wrapper\n    if check_signature() is True or check_token(kwargs) is True:\n  File \"/home/joffrey/.envs/registry/local/lib/python2.7/site-packages/docker_registry/toolkit.py\", line 237, in check_signature\n    except RSA.ValueError:\nAttributeError: 'module' object has no attribute 'ValueError'\n. @lsm5 Thanks for the jumpstart!\n@dmp42 Should be okay to have this in production any time. :)\n. I don't think there is, sorry. \n. Couple comments, otherwise LGTM\n. LGTM!\n. That's correct. Repository names should be 3-30 characters on registry hub.\n. That's not necessary, because that case will never happen.\n. Your index implementation should be aware of repositories existence and simply not make that call if the repository doesn't exist. \nThis PR as it stands will just degrade the official registry performance, which I'm not okay with.\n. You do realize there's a difference in time cost between checking the type of a variable and accessing a network filesystem, right? I don't think your argument is very solid there.\n. > Repository is saved in registry, but not in index.\nThe index's primary purpose is to have a record of the repositories that were pushed to your registry (as the name suggests). It should definitely be aware of their existence.\n\nBut when I use this API directly, and get incorrect response, it make me feel this function not good.\n\nIt is not meant to be used directly, at least in the official implementation. As an implementer, it is your responsibility to conform to this expectation, not the other way around.\nBasically:\n1. This function is only used in the context of non-standalone registries (i.e. coupled with an index). There are no private repositories in standalone registries.\n2. The most important implementation of the non-standalone is the official one. It serves thousands of users and can not afford to needlessly sacrifice performance.\n3. People are welcome to create other alternative index implementations, and we will do our best to accomodate this, but (2) will always take precendence in cases where there is a trade-off.\n4. This is an open-source project, if you feel that it doesn't fit your expectations in terms of API, I certainly encourage you to create a fork that will better suit your needs. I don't mean this in a bad way, either: some of the things you may see as incorrect are conscious choices we made when implementing this, and it's totally fine to have a different opinion about those choices.\nI hope this helps explaining my point of view once and for all.\n. @dmp42 FWIW:\n```\n$ tox\nGLOB sdist-make: /home/joffrey/work/registry/setup.py\nflake8 create: /home/joffrey/work/registry/.tox/flake8\nflake8 installdeps: ./depends/docker-registry-core/, -rrequirements/style.txt\nflake8 inst: /home/joffrey/work/registry/.tox/dist/docker-registry-1.0.0-dev.zip\nflake8 runtests: PYTHONHASHSEED='1302191585'\nflake8 runtests: commands[0] | flake8 /home/joffrey/work/registry\npy26 create: /home/joffrey/work/registry/.tox/py26\nERROR: InterpreterNotFound: python2.6\npy27 create: /home/joffrey/work/registry/.tox/py27\npy27 installdeps: ./depends/docker-registry-core/, -rrequirements/test.txt\npy27 inst: /home/joffrey/work/registry/.tox/dist/docker-registry-1.0.0-dev.zip\npy27 runtests: PYTHONHASHSEED='1302191585'\npy27 runtests: commands[0] | python setup.py nosetests\nrunning nosetests\nrunning egg_info\nwriting top-level names to docker_registry.egg-info/top_level.txt\nwriting entry points to docker_registry.egg-info/entry_points.txt\nwriting requirements to docker_registry.egg-info/requires.txt\nwriting namespace_packages to docker_registry.egg-info/namespace_packages.txt\nwriting dependency_links to docker_registry.egg-info/dependency_links.txt\nwriting docker_registry.egg-info/PKG-INFO\nreading manifest file 'docker_registry.egg-info/SOURCES.txt'\nreading manifest template 'MANIFEST.in'\nwriting manifest file 'docker_registry.egg-info/SOURCES.txt'\n/home/joffrey/work/registry/.tox/py27/local/lib/python2.7/site-packages/pkg_resources.py:1032: UserWarning: /home/joffrey/.python-eggs is writable by group/others and vulnerable to attack when used with get_resource_filename. Consider a more secure location (set with .set_extraction_path or the PYTHON_EGG_CACHE environment variable).\n  warnings.warn(msg, UserWarning)\n...........................................................................S...........................................................................error: gunicorn executable not found\n...............S...........................\nCoverage.py warning: Module docker_registry.status was never imported.\nName                            Stmts   Miss  Cover   Missing\n\ndocker_registry.app                58     34    41%   30-54, 59, 66-81, 95-102\ndocker_registry.drivers.s3        100     28    72%   40-52, 55-57, 66, 77, 84, 95-98, 101, 148-161\ndocker_registry.images            312     92    71%   56, 65, 78, 84-89, 98-100, 102, 107, 115, 122, 127-128, 132-133, 145-146, 149-150, 152-156, 159-160, 166, 168, 186-187, 201, 206-207, 211, 216, 236, 241, 243, 249-252, 270-271, 299-306, 313, 325-328, 339-340, 342, 344, 346, 348, 352, 354, 358, 385-397, 406-426\ndocker_registry.index              82      9    89%   44-45, 64, 68, 94-95, 97, 113-114\ndocker_registry.lib                 0      0   100% \ndocker_registry.lib.cache          27      0   100% \ndocker_registry.lib.checksums      53      0   100% \ndocker_registry.lib.config         83     17    80%   67-68, 99, 106-123\ndocker_registry.lib.index          48     11    77%   42-52, 88\ndocker_registry.lib.index.db      100      9    91%   58-64, 90-91, 115\ndocker_registry.lib.layers        141      1    99%   294\ndocker_registry.lib.mirroring     151     64    58%   28-33, 37-64, 78-80, 96-134, 150, 153-155, 160, 166, 172-183, 231\ndocker_registry.lib.rqueue        107     72    33%   15, 25-28, 31-48, 60, 64-71, 75, 79-82, 86-89, 93-98, 102-107, 111-112, 116-119, 123, 127, 131, 135, 150-155, 159-163, 166-171\ndocker_registry.lib.signals         8      0   100% \ndocker_registry.lib.xtarfile       40      9    78%   23, 44-46, 60-61, 72-75\ndocker_registry.run                37     10    73%   74-85\ndocker_registry.search             13      0   100% \ndocker_registry.server             13      0   100% \ndocker_registry.server.env          6      0   100% \ndocker_registry.storage            20      0   100% \ndocker_registry.tags              169     34    80%   32-53, 60-63, 132-137, 158-160, 187, 194-195, 197, 199, 239-240, 271-272\ndocker_registry.toolkit           259    132    49%   52, 55, 69, 74, 80-85, 116-117, 124-147, 151-172, 176-180, 184-189, 193-200, 207-234, 241-259, 263-267, 271-273, 281-282, 301-302, 317-324, 339-346\n\nTOTAL                            1827    522    71%\nRan 194 tests in 16.340s\nOK (SKIP=2)\n_____ summary ______\n  flake8: commands succeeded\nERROR:   py26: InterpreterNotFound: python2.6\n  py27: commands succeeded\n```\n. bump @dmp42 :3\n. https://registry-1.docker.io/v1/repositories/library/registry/tags is the correct URL. You do need to obtain a token to access it, which docker pull should automatically take care of.\nhttps://index.docker.io/v1/repositories/library/registry/tags has different data which isn't compatible with the registry endpoint of the same name.\n. info isn't a very descriptive variable name. How about something like use_accel or nginx_accel ?\nedit: looking back, this is actually a directory path. How about renaming the config key to nginx_accel_source_dir and the var to accel_source_dir? \n. We still have to handle the None afterwards... at least with an exception we know what exactly is happening and handle it appropriately. That's my opinion.\n. AFAICT this section should not be needed anymore. Have you experienced otherwise?\n. No, I meant bypassing the /v1/users endpoint should not be necessary when using auth.\n. Should we rename this to elliptics_namespace to avoid name conflicts in the future? For example, all S3 specific options are prefixed by s3_\n. Should we rename this to elliptics_groups to avoid name conflicts in the future? For example, all S3 specific options are prefixed by s3_\n. Might wanna clean that up :)\n. No, the error is already caught downstream. I would have left it for additional security but catching the error later lets us trigger the source lookup in some cases (public repos) but not all (private repos).\n. you're right, I used this as a temp hack but forgot to change it.\n. good catch\n. I think if there's no match you should raise an exception, otherwise this could pose problems in the future.\n. Do we really want to add more stuff into toolkit?\n. We're free to handle the exception downstream in any way we want, but I think it's a mistake to silently ignore it.\n. Ok!\n. Are these dependencies of gcs-oauth2-boto-plugin? If so I don't think they should be included in our requirements (since they're hopefully already in the requirements of gcs-oauth2-boto-plugin). And if not, I don't understand why they were added?\n. Assuming range_header[-1] is the last character in the string, what's the point of doing range_header[-1:] ?\n. Yes, sorry, my comment was unclear. :) That's what I meant, just wasn't sure whether I was missing something.\n. Not sure whether the default should be \"allow pull, restrict push\" or \"restrict push and pull\".\n. @samalba the only changes here are\n- The disappearance of --debug, which is being deprecated anyway: benoitc/gunicorn#701. I do think we should keep this change.\n- The addition of $GUNICORN_OPTS \"$@\" which allows for more control when running the script but doesn't affect the default.\nAs a result, I don't think we should get rid of this.\n. Assuming this is a symlink, seems like it would be cleaner to just remove those files (especially since they were just here for reference in the first place)\n. I think it's safe to remove them.\n. Literally the first section of the README advises to do cp config/config_sample.yml config/config.yml, so I think people using DOCKER_REGISTRY_CONFIG to point to a pre-defined sample config will be a very small minority.\n. This is the same as length < 0, so it's already covered by your length < 2 check a couple lines further.\n. So we can't ask for a 1 byte slice? like Range: bytes=1-1?\n. cleanup?\n. I don't know if this slipped through or if it was intentionally put in. Don't think we want editor-related files in the repo?\n. Might wanna leave a comment explaining what _sp is so we don't confuse it for a typo when rereading this later.\n. Looks like a mistake, and likely to break the behavior of anything that consumes this.\n. I think the second comma here is unnecessary...\n. I find the old syntax more legible, but no big deal\n. Isn't that going to break horribly if cfg.search_backend is None?\n. You probably want to call f() once more after reconnecting?\n. Ah, you're right, sorry.\n. People use OSX?\n. URL must be updated too!\n. Does this maybe belong in toolkit? Keep only view functions in images.py as much as possible.\n. Well, this is pretty low-effort as far as improvements go, do you mind if I amend the PR with that particular change? It's not like we're getting rid of this code anytime soon.\n. ",
    "vieux": ":thumbsup: \n. @b83s I guess this works only when you use the local flavor. \nIf we do this, it needs to be baked in the registry to work with all the flavors (S3, Glance, ...)\n. @wking I'm not a maintainer here, but maybe you could start by fixing travis: https://travis-ci.org/dotcloud/docker-registry/builds/19065311\n. @jordansissel could you please retry with 0.9.0 your version is quite old\n. ping @samalba \n. :+1: \n. ",
    "rohansingh": "Yup, that's what we ended up doing as well. I guess my main concern is that it's non-obvious. Perhaps all that's required is a documentation update? Or perhaps an update to config_sample.yml? I'm willing to submit a pull request for either, depending on what makes sense.\n. ",
    "devfacet": "Yes. Kind a dockerhub would be nice...\n. @jakedt : I like your approach. I will definitely advise around.\nP.S. : Do you have a quay twitter account so I can follow. And also please check this out: https://twitter.com/cmuratfatih/docker/members\n. ",
    "rimusz": "I would love to have such service too.\n. ",
    "asbjornenge": ":+1: \n. Awesome! :+1: \n. Yup, I'm actually running the container. I just tried updating real quick to see if it helped. Same error, both versions. There are some files being created inside the bucket, but still getting the error.\n. Well knock me out and call me Johnny... A few hours later it just works!? Exact same container. Might have been the bucket API having some issues. Works now :-)\n. Sorry, not much help in me here. It just worked all of a sudden, didn't change anything. Has been working ever since.\n. This is very confusing for me also.\n. ",
    "avaer": "Pleased to report I'm pretty far along with a hosted container+image registry service :). (I'm not officially affiliated with dotcloud/docker or anything, just giving it a shot)\nStatus? Alpha, just-for-me right now. It works, but in practice there's still some docker tool-related issues to sort through before this is beta-ble:\n1. Mainline client experience really sucks with pushing/logging into unofficial registries. Lots of weirdness and some security mess-ups (tries to log into the wrong place and leaks credentials, etc.). Luckily the fixes are in the pipeline and I'm trying to shepherd them along.\n2. Client auth api is inconsistent and doesn't know what it wants to be. I've managed to work around it for the most part, but it's hacks.\n3. Registry management with the current api/sources is lacking in management features. We can't yet do things like cleanly delete or flatten container layers, things which are kinda needed for such a service in practice. There are open issues for this stuff, but if I need to get my hands dirty to push the train along then I will.\nAnyway, just thought I'd chime in. If anyone wants to chat about the details (either to join the fun or to get on the beta list), I'm a@modules.io.\n. This was never a problem I ran into pushing to the central index/registry, but that could be for many reasons. The docker client special-cases the central index in a lot of places.\nThis change basically makes the user-visible behavior the same between what's observed with the central index and /dotcloud/docker-registry.\nI'll try to capture the data between the central index to see for sure what the difference is though.\n. And thanks for the merge :).\nFor the record I intercepted the requests to the main index and indeed it allows the empty list, giving us a 204.\n. ",
    "kyleconroy": "@asbjornenge @rimusz @cmfatih @mattwallington @stuartrexking All of you are in luck! We've just launched our beta private Docker registry over at stackmachine.com. If you have any more questions, you can email me at kyle@stackmachine.com\n. ",
    "jakedt": "@asbjornenge @rimusz @cmfatih @mattwallington @stuartrexking At the risk of sounding like a me too post, we have also built a private Docker registry service called Quay at https://quay.io/. We're offering unlimited public repositories and a pay for privacy model similar to (read: exactly the same as) Github's. We have a user model that includes delegation with roles for read-only, read-write, and administrator access. We unveiled it publicly Oct 2 at the Docker NYC Meetup. Have at it, and send us all of your feedback and ideas!\n. @cmfatih Yes we just opened up a Twitter account when we launched the product: @quayio Feel free to follow us, we will put all future announcements for product enhancements there.\n. ",
    "gandalfio": "@asbjornenge @rimusz @cmfatih @mattwallington @stuartrexking  Announcing our own hosted private registry service - we went into open beta with http://gandalf.io last week. @gandalfdocker on twitter. Btw, the free plan comes with a single private docker too, so you can play around with the collaboration features (unlimited team members) without upgrading.\n. ",
    "a9k": "I'm getting this same error. Log show \"Session is empty\" debug messages.\nDockerfile:\nShell\nFROM      denibertovic/sid\nRUN echo \"deb http://mirrors.kernel.org/debian/ sid main\"  > /etc/apt/sources.list\nSteps:\nShell\ndocker run -p 5000:5000 samalba/docker-registry (run in separate screen to watch)\ndocker build -t localhost:5000/fail .\ndocker push localhost:5000/fail\nOutput of pusher:\n``` Shell\nroot@argh:~/containers/repofail# docker push localhost:5000/fail\nWARNING: The Auth config file is missing\nThe push refers to a repository [localhost:5000/fail] (len: 1)\nSending image list\nPushing repository localhost:5000/fail (1 tags)\nPushing c41511461f00f2a05053dc3c1a25791718ab53b7b71b51eef05e76a221f1801f\nPushing 163.7 MB/164 MB (100%)\n2013/08/23 01:53:46 HTTP code 400 while uploading metadata: {\n    \"error\": \"Checksum not found in Cookie\"\n}\n```\nOutput of docker-registry starting after the tons of hex stuff:\nShell\n2013-08-23 07:53:45,671 DEBUG: checksums.compute_tarsum: return tarsum+sha256:b76c8c8a02684cd9cbb2d8c6e40e4ccbc5f8a3a5054bf8ac92b2a24e2d5c5b83\n\"172.17.42.1 - - [23/Aug/2013:07:53:46] \"PUT /v1/images/c41511461f00f2a05053dc3c1a25791718ab53b7b71b51eef05e76a221f1801f/layer HTTP/1.1\" 200 4 \"-\" \"docker/0.5.3-dev go/go1.1.2 kernel/3.10.0-2-amd64\"\n2013-08-23 07:53:46,070 INFO: \"172.17.42.1 - - [23/Aug/2013:07:53:46] \"PUT /v1/images/c41511461f00f2a05053dc3c1a25791718ab53b7b71b51eef05e76a221f1801f/layer HTTP/1.1\" 200 4 \"-\" \"docker/0.5.3-dev go/go1.1.2 kernel/3.10.0-2-amd64\"\n2013-08-23 07:53:46,127 DEBUG: check_session: Session is empty\n2013-08-23 07:53:46,127 DEBUG: api_error: Checksum not found in Cookie\n\"172.17.42.1 - - [23/Aug/2013:07:53:46] \"PUT /v1/images/c41511461f00f2a05053dc3c1a25791718ab53b7b71b51eef05e76a221f1801f/checksum HTTP/1.1\" 400 47 \"-\" \"docker/0.5.3-dev go/go1.1.2 kernel/3.10.0-2-amd64\"\n2013-08-23 07:53:46,133 INFO: \"172.17.42.1 - - [23/Aug/2013:07:53:46] \"PUT /v1/images/c41511461f00f2a05053dc3c1a25791718ab53b7b71b51eef05e76a221f1801f/checksum HTTP/1.1\" 400 47 \"-\" \"docker/0.5.3-dev go/go1.1.2 kernel/3.10.0-2-amd64\"\n. Yup that would make the session invalid on one worker.\n. It works now. Ran build, tag, push then pulled from a remote machine. This bug is squashed. Thanks.\n. ",
    "strcrzy": "i'm running into this on the latest registry --\n```\n10.0.1.192 - - [12/Feb/2014:23:09:05] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-02-12 23:09:05,851 INFO: 10.0.1.192 - - [12/Feb/2014:23:09:05] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n10.0.1.192 - - [12/Feb/2014:23:09:07] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-02-12 23:09:07,858 INFO: 10.0.1.192 - - [12/Feb/2014:23:09:07] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-02-12 23:09:07,862 DEBUG: check_session: Session is empty\n10.0.1.192 - - [12/Feb/2014:23:09:07] \"PUT /v1/repositories/registry/ HTTP/1.1\" 200 2 \"-\" \"docker/0.8.0 go/go1.2 git-commit/cc3a8c8 kernel/3.11.0-12-generic os/linux arch/amd64\"\n2014-02-12 23:09:07,864 INFO: 10.0.1.192 - - [12/Feb/2014:23:09:07] \"PUT /v1/repositories/registry/ HTTP/1.1\" 200 2 \"-\" \"docker/0.8.0 go/go1.2 git-commit/cc3a8c8 kernel/3.11.0-12-generic os/linux arch/amd64\"\n2014-02-12 23:09:07,865 DEBUG: check_session: Session is empty\n2014-02-12 23:09:07,866 DEBUG: api_error: Image not found\n10.0.1.192 - - [12/Feb/2014:23:09:07] \"GET /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1\" 404 34 \"-\" \"docker/0.8.0 go/go1.2 git-commit/cc3a8c8 kernel/3.11.0-12-generic os/linux arch/amd64\"\n2014-02-12 23:09:07,866 INFO: 10.0.1.192 - - [12/Feb/2014:23:09:07] \"GET /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1\" 404 34 \"-\" \"docker/0.8.0 go/go1.2 git-commit/cc3a8c8 kernel/3.11.0-12-generic os/linux arch/amd64\"\n2014-02-12 23:09:07,868 DEBUG: check_session: Session is empty\n10.0.1.192 - - [12/Feb/2014:23:09:07] \"PUT /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1\" 200 4 \"-\" \"docker/0.8.0 go/go1.2 git-commit/cc3a8c8 kernel/3.11.0-12-generic os/linux arch/amd64\"\n2014-02-12 23:09:07,870 INFO: 10.0.1.192 - - [12/Feb/2014:23:09:07] \"PUT /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1\" 200 4 \"-\" \"docker/0.8.0 go/go1.2 git-commit/cc3a8c8 kernel/3.11.0-12-generic os/linux arch/amd64\"\n2014-02-12 23:09:07,889 DEBUG: check_session: Session is empty\n2014-02-12 23:09:07,891 DEBUG: checksums.compute_tarsum: return tarsum+sha256:5699598ee6ed92e23bcd683ed020d195b0409f5e26f371bd6c2d12ce45feb5c9\n10.0.1.192 - - [12/Feb/2014:23:09:07] \"PUT /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/layer HTTP/1.1\" 200 4 \"-\" \"docker/0.8.0 go/go1.2 git-commit/cc3a8c8 kernel/3.11.0-12-generic os/linux arch/amd64\"\n2014-02-12 23:09:07,892 INFO: 10.0.1.192 - - [12/Feb/2014:23:09:07] \"PUT /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/layer HTTP/1.1\" 200 4 \"-\" \"docker/0.8.0 go/go1.2 git-commit/cc3a8c8 kernel/3.11.0-12-generic os/linux arch/amd64\"\n2014-02-12 23:09:07,896 DEBUG: check_session: Session is empty\n2014-02-12 23:09:07,896 DEBUG: api_error: Checksum not found in Cookie\n10.0.1.192 - - [12/Feb/2014:23:09:07] \"PUT /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/checksum HTTP/1.1\" 400 47 \"-\" \"docker/0.8.0 go/go1.2 git-commit/cc3a8c8 kernel/3.11.0-12-generic os/linux arch/amd64\"\n2014-02-12 23:09:07,897 INFO: 10.0.1.192 - - [12/Feb/2014:23:09:07] \"PUT /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/checksum HTTP/1.1\" 400 47 \"-\" \"docker/0.8.0 go/go1.2 git-commit/cc3a8c8 kernel/3.11.0-12-generic os/linux arch/amd64\"\n```\nand on the client: \noot@ip-10-0-1-192:~/docker-registry# docker push index.fanfd.us:5000/registry\nThe push refers to a repository [index.fanfd.us:5000/registry] (len: 1)\nSending image list\nPushing repository index.fanfd.us:5000/registry (1 tags)\n511136ea3c5a: Pushing [==================================================>] 1.536 kB/1.536 kB\n2014/02/12 23:09:07 push: }\ni'm kind of at a loss as to what to do now -- i have set the secret in the config to a 64 char string, but i still can't push.\n. i did indeed run the registry from source, using the Dockerfile provided. I copied the s3 config included in the config folder, except i specified a 64 char secret_key. i can't log in to that machine right now or i would paste it, sorry.\nincidentally, this problem went away when i set standalone and disable_token_auth to true. i'm actually really confused about how this index based auth is supposed to work, is there some further documentation on it somewhere?\nthanks!\n. my config:\n```\ncommon:\n    loglevel: _env:LOG_LEVEL\n    secret_key: _env:REGISTRY_SECRET\n    standalone: true\n    disable_token_auth: true\nprod:\n    storage: s3\n    boto_bucket: _env:AWS_BUCKET\n    s3_access_key: _env:AWS_KEY\n    s3_secret_key: _env:AWS_SECRET\n    s3_bucket: _env:AWS_BUCKET\n    s3_encrypt: true\n    s3_secure: true\n    storage_path: /images\nlocal:\n    storage: local\n    storage_path: _env:REGISTRY_ROOT\ndev:\n    storage: local\n    storage_path: /tmp/registry\n    loglevel: debug\n```\n. @samalba - thanks for the suggestion! \ni haven't tried the registry container, how would i configure it? there doesn't seem to be a default config.yml bundled with it. \nthe only way i could figure out to bundle my own config was to build a container for myself with the dockerfile. is there a config provided at some later stage somehow that i am interfering with?\n. thank you that makes sense now.\nso switching to the registry container and passing in env vars appropriately, including redis vars, does fix my speed issue pulling remotely, but i still have a problem with pulling from AWS machines. it does not appear to be a connectivity issue, as both pushing and the GET requests for both the /v1/_ping and the /v1/images calls make it to the server, but then it just hangs again.\ni'm at a loss really, any more hints would be greatly appreciated\n. this is how i currently invoke the registry container:\ndocker run \\\n-e STORAGE_PATH=/registry \\\n-e SETTINGS_FLAVOR=prod \\\n-e AWS_ACCESS_KEY_ID=<key> \\\n-e AWS_SECRET_KEY=<secret> \\\n-e S3_BUCKET=sra-registry \\\n-e CACHE_REDIS_HOST=172.17.42.1 \\\n-e CACHE_LRU_REDIS_HOST=172.17.42.1 \\\n-e CACHE_REDIS_PORT=6379 \\\n-e CACHE_LRU_REDIS_PORT=6379 \\\n-e CACHE_REDIS_PASSWORD=password \\\n-e CACHE_LRU_REDIS_PASSWORD=password \\\n-p 5000:5000 \nregistry\nthis configuration still stalls on docker pull unfortunately :(\n. ",
    "zareone": "+1!\n. ",
    "jschneiderhan": "This is no longer valid now that #60 has been merged\n. what if the gunicorn command stayed in the CMD line like so:\nThis way, if someone wants to overwrite the gunicorn options when they run the container they can do so, but still get the environment variables injected:\nIf this is a decent idea (I'm a bit out of my comfort zone, so I apologize if it isn't) then the name of the sh file could changed to setup-config.sh or something.\n. If #62 LGTY can we get that included in the release as well?\n. I just pulled the latest (debd567e95df51f8ac91d0bb69ca35037d957ee6) and ran docker build to create a new image, ran it, and got the same error upon trying to push.\n. Just did the same with the latest again (d1a86b6f990cf335a0757d0e90db2216ad03965b) and am still seeing the same behavior. I'm not going to reopen this issue, as I still have a sneaking suspicion that I'm doing something stupid, but will come back and update if I find a legitimate issue.\n. Yup. I can curl as as well as see some requests come through in the docker-registry logs\nPing:\ncurl https://my.docker.registry.com/v1/_ping\ntrue\nDocker-registry logs\n\"172.17.42.1 - - [30/Aug/2013:19:38:43] \"GET /v1/_ping HTTP/1.0\" 200 4 \"-\" \"Go 1.1 package http\"\n2013-08-30 19:38:43,554 INFO: \"172.17.42.1 - - [30/Aug/2013:19:38:43] \"GET /v1/_ping HTTP/1.0\" 200 4 \"-\" \"Go 1.1 package http\"\n\"172.17.42.1 - - [30/Aug/2013:19:38:43] \"PUT /v1/repositories/testing123/ HTTP/1.0\" 200 2 \"-\" \"docker/0.6.1 go/go1.1.2 git-commit/5105263 kernel/3.8.0-19-generic\"\n2013-08-30 19:38:43,968 INFO: \"172.17.42.1 - - [30/Aug/2013:19:38:43] \"PUT /v1/repositories/testing123/ HTTP/1.0\" 200 2 \"-\" \"docker/0.6.1 go/go1.1.2 git-commit/5105263 kernel/3.8.0-19-generic\"\n\nDocker logs in debug mode:\n[debug] api.go:985 Calling POST /images/{name:.*}/tag\n2013/08/30 19:38:32 POST /v1.4/images/c869d5ef508a/tag?repo=my.docker.registry.com%3A443%2Ftesting123\n[debug] api.go:985 Calling POST /images/{name:.*}/push\n2013/08/30 19:38:43 POST /v1.4/images/my.docker.registry.com:443/testing123/push\n[debug] registry.go:475 [registry] PUT https://my.docker.registry.com:443/v1/repositories/testing123/\n[debug] registry.go:476 Image list pushed to index:\n[{\"id\":\"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\",\"Tag\":\"latest\"},{\"id\":\"c869d5ef508a814b3258ebc3b66bc6a6476cd1193cc57df1d82e0fdd562922f9\",\"Tag\":\"latest\"}]\n[debug] registry.go:526 Auth token: [Token signature=FVYOZZUBBFIESEH2,repository=\"library/testing123\",access=write]\n[debug] registry.go:369 [registry] Calling PUT https://docker-registry/v1/images/8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c/json\nAt some point after pushing the repository and before pushing the images the domain domain name used changes.\n. I switched over to straight HTTP and noticed something weird when strace'ing the docker daemon process:\nCould the \"Docker-Endpoints: docker-registry\" header be the cause of the domain name change in the requests?\n. Ok - I was able to figure this one out. I'm using the following nginx directive to push requests upstream to a docker container running the docker-registry:\nupstream docker-registry {\n  server localhost:5000;\n}\nserver {\n  listen 443;\n  server_name my.docker.registry.com;\n  proxy_set_header Host $http_host; // THIS FIXED MY PROBLEM\n  ssl on;\n  ssl_certificate /etc/ssl/certs/docker-registry;\n  ssl_certificate_key /etc/ssl/private/docker-registry;\n  location / {\n    proxy_pass http://docker-registry;\n  }\n}\n\nI didn't have the \"proxy_set_header Host $http_host;\" directive set, so the Host header was being set to \"docker-registry\" and sent back as the value of an X-Docker-Endpoints header. After putting the directive in my pushes are at least attempting to store the images in S3 (hitting a new problem :) ).\nSo this was me, not a problem with docker or the docker-registry. I was going to offer to add to the documentation, but I see now that it's already mentioned (facepalm). https://github.com/dotcloud/docker-registry#what-about-a-production-environment\n. What version of nginx are you using? Is it >= 1.3.9?\n. Also, what does you nginx config look like and what is the gunicorn command that you are running?\n. ",
    "jakedahn": "/cc @samalba \n. Word - i'll reopen on the other repo, its docker client that crashes\n. ",
    "drewcsillag": "I propose adding the following two url paths that both return JSON lists.\nGET /v1/repository/namespaces\nLists the namespaces in the registry, so if I have an image named drewc/docker:latest, this would list 'drewc' in the result list.\nGET /v1/repository/namespace/\nList the images in that namespace, so if I have an image named drewc/docker:latest, and did a GET on /v1/repository/namespace/drewc, it would list 'docker' in the result list\nI've got working code illustrating how it would work here: https://github.com/drewcsillag/docker-registry/compare/dotcloud:master...master\n. ",
    "robszumski": "+1 for this idea.\n@samalba If keeping the index and registry separate is the way it has to be, would you guys be willing to open source a proof of concept listing for private registries?\n. ",
    "Zer0-": "I would also really like this. +1\n. ",
    "pmyjavec": "I think this is mandatory to be honest, it is virtually impossible to operate without a image and tag listing/searching. Especially in a large team.\n@samalba, what is your main concern with storing this stuff on a filesystem, performance? I think in most peoples cases it would be fine, especially to start with, more high performance backends could be integrated down the track.\nHappy to have a stab at this one but only if there is any chance of acceptance...\n. @samalba \nI didn't articulate myself very well but that is exactly what I meant. When I said \"on disk\" I meant \"I would implement a super simple SQLite backend to the system to begin with\". Just so you don't need to run anything else to get started, just need somewhere to store the index.\nI like the search idea, and like I said definitely on the same page with the hooks.\nPauly\n. Yup, same for me, the official image seems to be out of date. As soon as I run the latest from the repo everything seems to work perfectly again. \nHappy to help out in anyway I can, getting this thing running against a CI server like Travis  or something.\n. If using a private registry how is mirroring achieved. Where do I get an index from?\n. Hey mate,\u00a0\nJust to get you moving... Set the registry URL option (I forgot the name specifically) and that's it.\nNow when you pull a container from your mirror, if it doesn't have it locally it will pull it from your master working effectively as a caching proxy.\u00a0It's really elegant and works super well.\nMy bad as I should have sent in a PR for the docs, I will tomorrow.\nPauly\nOn Sun, Nov 2, 2014 at 6:36 AM, Samuel Huang notifications@github.com\nwrote:\n\n@pmyjavec I'm interested in the answer as well. I have setup the search and cache endpoints, but I am still unsure about what the source index exactly is. I see index.docker.io, but it simply points to the Docker Hub. Not sure if the source index is a UI, like atcol/docker-registry-ui and konradkleine/docker-registry-frontend; or the search backend; or a combination of both.\nI would suggest expanding the mirroring docs, as it has created ambiguity/misinterpretation. See #556 #576 #597 #624\nReply to this email directly or view it on GitHub:\nhttps://github.com/docker/docker-registry/pull/280#issuecomment-61380329\n. Wow, this thread can be hard to follow. It sounds like you cannot set the configuration at all.\n. @dmp42Well,  It seems from what I'm experiencing and what you've said above this is pretty much unusable unless we use what is in the sample config right?\n\nI would love to be proven wrong right now. \nHow does one possibly set the configuration if DOCKER_REGISTRY_CONFIG is ignored?\nI'm also having no luck at all setting environment variables either BTW. \n. Updated my comment ;)\nThanks for the help, will try again.\n. ",
    "dlewanda": "+1 for me. I think this would be awesome!\n. I have followed the example provided by @bacongobbler, but I'm still getting stuck with an unknown authority. I know that the docker registry is running on it's own on port 5000, but can't get the cert to play nice over HTTPS via nginx. Any other suggestions?\n. I have a wildcard cert like you as far as I can tell. Which logs should I post? I can see when I load the URL in my web browser that the certificate is valid and I can authenticate against the username and password I set up with htpasswd.\n. I received the cert from a colleage claiming it was a wildcard cert in .pem form. I split it up into a .key and .crt as described on the 'net. I'm pretty sure nginx is happy with it as I can use curl or my browser to get the standard \"docker-registry server (dev)\" string without any SSL warning or error The browser's lock icon looks good and the details show it's a RapidSSL cert. \nThe only Docker output I get is \n2014/02/13 03:09:43 Invalid Registry endpoint: Get https://xxxxxx.xxx/v1/_ping: x509: certificate signed by unknown authority\nright after I try to login and can't get any further.\n. @bacongobbler you rock! Thanks for the tip. Works great now. I will discuss it with my colleague who provided the certificate so we don't make the same mistake in the future.\n. Thanks for the suggestion, but is that something that would have changed underneath my registry? It has been working fine for a couple weeks until yesterday, where I had a sporadic problem with it from one computer. I thought it was a network connectivity issue as it got resolved, but now 2 separate computers here are having the problem. Also, if the configuration isn't working, how would docker login work from the machine hosting the registry?\n. Thanks @tamsky for the suggestion, but I tried upping the number of workers to (-w option in run.sh) from 4 to 8 and I'm still getting the timeout when I am pretty much sure I'm the only one hitting the registry. Any other ideas?\n. I would agree with @bacongobbler. I did the same thing by restarting boot2docker locally on Mavericks and it started working again after it had stopped working. It seems pretty intermittent, and I can't locate any pattern. I had recently restarted the boot2docker to upgrade to 0.10, so it doesn't take that long to start happening.\n. I have seen this issue a couple times recently on OS X with boot2docker where restarting a few times even didn't fix it. I can't seem to pin down when it goes bad like that.\nOn Tuesday, April 29, 2014 3:58 PM, Mark Olliver notifications@github.com wrote:\nAgreed - to make it clearer for others following (as it took me a while to realise) restart the docker daemon on the client side after changing the DNS for the registry server. This refreshed the DNS cache on the client side and allowed the connection to succeed to the server.\n\u2014\nReply to this email directly or view it on GitHub.\n. ",
    "jefferai": "Adding my +1 as well. A private registry is a must for any corporation's internal use, but not having parity with the global registry makes it much harder on developers to actually use the thing :-)\n. ",
    "devzsolt": "+1 it is an important feature\n. ",
    "b83s": "hi,\nFor now i lanched a docker apache/php instance that mounts the registry folder and lists the directory structure. Not the best method but it works forn now.\n```\n$array = array();\nforeach (glob(\"/registry/imgages/repositories/library/*\") as $filename) {\n$repo = str_replace('/registry/repositories/library/','', $filename);\n\n$array[$repo] = array();\nforeach (glob(\"/registry/imgages/repositories/library/\".$repo.\"/tag_*\") as $filename01) {\n   $array[$repo][] = str_replace('/var/registry/repositories/library/'.$repo.'/tag_','', $filename01);\n}\n\n}\necho json_encode($array);\n```\n. ",
    "ciokan": "That's basically what we did for www.dockify.it. We ended up hacking the toolkit.py file in a lot of places to add authentication and permissions. The problem with nginx in front and basic auth is that, with correct credentials, users can pull repositories from other users and that's probably unwanted for most people deploying privately hosted registries.\nMaybe not a full wrapper for database or authentication mechanisms but a middleware which receives user credentials, accessed namespace, accessed repository and access requested (read, write, delete) on each user request. We could build our own security systems based on these parameters with various endpoints. It should be fairly easy to add in the registry and very elegant for developers to adapt to their own needs.\n. $nginx -v\nnginx version: nginx/1.4.1\ngunicorn is from your README (I run it with supervisord):\n```\n!/bin/sh\nexport DOCKER_REGISTRY_CONFIG=/home/ubuntu/docker-registry/config.yml\nexport SETTINGS_FLAVOR=prod\ngunicorn -k gevent --max-requests 100 --graceful-timeout 3600 -t 3600 -b 0.0.0.0:5000 -w 8 wsgi:application\n```\n. Forgot nginx sry:\n```\nserver {\n      listen 80;\n      listen 443 ssl;\n      server_name uss.mydomain.it;\n      proxy_set_header Host $http_host;\n      ssl_certificate /home/ubuntu/ssl/mydomain_combined.crt;\n      ssl_certificate_key /home/ubuntu/ssl/mydomain.key;\n  location /_ping {\n          auth_basic off;\n          proxy_pass        http://localhost:5000;\n          proxy_set_header  X-Real-IP  $remote_addr;\n  }\n\n  location /v1/_ping {\n          auth_basic off;\n          proxy_pass        http://localhost:5000;\n          proxy_set_header  X-Real-IP  $remote_addr;\n  }\n\n  location / {\n          auth_basic \"Restricted\";\n          auth_basic_user_file /home/mydomain/.htpasswd;\n          proxy_pass        http://localhost:5000;\n          proxy_set_header  X-Real-IP  $remote_addr;\n  }\n\n}\n``\n. I had it running allright with http basic auth until I started adding the SSL part so I can close port 5000 from the public. SSL is installed ok I can access all the info in the browser. I'm pushing but I don't know why it tries to upload metadata tohttps://localhost:5000when nginx clearly states http for theproxy_pass`. Tried also with gunicorn running on localhost instead of 0.0.0.0 with no results.\n. ",
    "rbucker": "Downloading 194.4 MB/197.2 MB (99%)\nError while retrieving image for tag:  (exit status 2: \ngzip: stdin: unexpected end of file\ntar: Unexpected EOF in archive\ntar: Unexpected EOF in archive\ntar: Error is not recoverable: exiting now\n); checking next endpoint\n2013/08/24 15:20:12 Error: No such image: samalba/docker-registry\n. ",
    "jf647": "I am also seeing this, after five retries on two different networks, so the problem may be at source (or with multiple paths into the source):\nvagrant@docker:~$ docker pull samalba/docker-registry\nPulling repository samalba/docker-registry\n0b5de618610a: Error pulling image (latest) from samalba/docker-registry, endpoint: https://cdn-registry-1.docker.io/v1/, exit status 2:\ngzip: stdin: unexpected end of file\ntar: Unexpected EOF in archivee\ntar: Unexpected EOF in archivee\ntar: Error is not recoverable: exiting now\n0b5de618610a: Error pulling image (latest) from samalba/docker-registry, exit status 2:\ngzip: stdin: unexpected end of fileendend layers\ntar: Unexpected EOF in archive\ntar: Unexpected EOF in archive\ntar: Error is not recoverable: exiting now\n2013/08/28 18:29:10 Internal server error: 404 trying to fetch remote history for samalba/docker-registry\nvagrant@docker:~$\n. ",
    "rynonl": "Hey guys, when you do docker pull without a version it tries to pull \"latest\". I noticed on my machine anyway that there is no \"latest\" for this repo.  If you manually specify the version(I think the actual latest version is like 1.5.7 or something) it should work.\n. ",
    "bshi": "Pushed a rebase.  Let me know if this is not the preferred workflow for this project.\nRenamed _c => config as suggested.\nGood point about the step function load.  Unfortunately, AWS does not publish the rate limits for the metadata service so we can't really provide bullet proof defaults without introducing some sort of global lock or proxy.  Boto introduced the ability to tweak retry and timeouts for the AWS metadata service.  These are distinct from other retry/timeout settings for the rest of the API.  I've added a lightly tested configuration in contrib/boto.cfg.  I've also ammended the comments to indicate that the bucket lazy loading is only a partial fix.\n. Oh the boto settings were introduced somewhere around boto 2.10.  I've bumped to the latest release.\n. As part of this investigation, I discovered that \"docker pull foo/bar:sometag\" will incur a hit to the API endpoint that lists all tags for 'foo/bar'.  This seems a bit wasteful.  @dmp42 - as I'm not too familiar with the details of 'docker pull', perhaps you know offhand whether this is indeed unnecessary work and whether it's worth filing a bug in docker?\n. Skimmed the discussion in #643 - it seems like you guys are aware and thinking about the issue of unbounded looping over driver interface methods.  One other concern is the underlying storage consistency model and what the registry expects of the drivers.  S3 doesn't even have consistent (:P) consistency models across S3 regions.\n. Build failures appear unrelated?\n. Thanks; it was perhaps overly optimistic of me to skip installing the development environment.\n. groupcache solves a narrower class of problems but by virtue of the extra constraints addresses several difficult caching problems (hot spots, thundering herd, etc) inherent in the more general purpose caches like memcache or redis.  The two are not mutually exclusive.  As @wking points out - groupcache, unmodified, will not be suitable for mutable state.\n. > Is it worth caching image tarballs?  I expect many tarballs will be large,\n\nand optional caching based on size seems like more trouble than it's worth.\n\nGood point - it also occurred to me that the benefit may be marginal.  Anecdotally at least, a lot of images  I've interacted with are composed of small numbers of large layers and many tiny layers.  Would it be difficult to crawl the public index to generate a histogram of layers and their sizes?\n\nDo we have content-addressable data besides the image tarballs?\n\nComing from a different angle (assuming V2 is still under design), is there data that isn't content-addressable that could be made content-addressable?\n. Sure - what do you suggest?\n. ",
    "diasjorge": "@rca what was the problem on your side?\n. ",
    "dmahlow": "Error is still occuring for me.\n- docker-registry from container\n- european bucket\n- initial files and directories are being created (so I guess it's no permissions issue)\n- s3_encrypt on/off does not make a difference.\n2013-10-22 16:18:30,519 ERROR: Exception on /v1/images/8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c/layer [PUT]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1687, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1360, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1358, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1344, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/docker-registry/registry/toolkit.py\", line 169, in wrapper\n    return f(*args, **kwargs)\n  File \"/docker-registry/registry/images.py\", line 104, in put_image_layer\n    store.stream_write(layer_path, sr)\n  File \"/docker-registry/lib/storage/s3.py\", line 179, in stream_write\n    mp.complete_upload()\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/multipart.py\", line 304, in complete_upload\n    self.id, xml)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 1571, in complete_multipart_upload\n    response.status, response.reason, body)\nS3ResponseError: S3ResponseError: 400 Bad Request\n<Error><Code>MalformedXML</Code><Message>The XML you provided was not well-formed or did not validate against our published schema</Message><RequestId>3606D9E34BDCB68A</RequestId><HostId>nvM7TcpTdJvNwkZfr/SfyvXnaBnPVseiDnQMuNsdZq6SEv5x/hxz/VsJ04x8m5Em</HostId></Error>\n2013-10-22 16:18:30,519 ERROR: Exception on /v1/images/8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c/layer [PUT]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1687, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1360, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1358, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1344, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/docker-registry/registry/toolkit.py\", line 169, in wrapper\n    return f(*args, **kwargs)\n  File \"/docker-registry/registry/images.py\", line 104, in put_image_layer\n    store.stream_write(layer_path, sr)\n  File \"/docker-registry/lib/storage/s3.py\", line 179, in stream_write\n    mp.complete_upload()\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/multipart.py\", line 304, in complete_upload\n    self.id, xml)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 1571, in complete_multipart_upload\n    response.status, response.reason, body)\nS3ResponseError: S3ResponseError: 400 Bad Request\n<Error><Code>MalformedXML</Code><Message>The XML you provided was not well-formed or did not validate against our published schema</Message><RequestId>3606D9E34BDCB68A</RequestId><HostId>nvM7TcpTdJvNwkZfr/SfyvXnaBnPVseiDnQMuNsdZq6SEv5x/hxz/VsJ04x8m5Em</HostId></Error>\n. I tested it with a US bucket, and that worked fine. Since Amazon's API is just plain weird when it comes to addressing european (or generally non-US) buckets, I suspect this is the culprit; in one of the libraries (boto maybe?).\n. ",
    "aluedeke": "i tried a fresh bucket in irleand and it didn't worked. then i created a fresh one in us standard and it worked. got the same error as mentioned in the bug. would like to see this bug reopend :-)\n. ",
    "mcobzarenco": "I had exactly the same problem as described by the OP and indeed it seems to have gone away when using a US bucket instead of the European one I tried first..\n. +1 I have identical issues as described above\n. ",
    "stevenschlansker": "I too am seeing this, and I am even on a US bucket.\nI don't think this issue is fixed.  Can we reopen it?\nS3ResponseError: S3ResponseError: 400 Bad Request\n<Error><Code>MalformedXML</Code><Message>The XML you provided was not well-formed or did not validate against our published schema</Message><RequestId>A274B84A8BBBE2A5</RequestId><HostId>5lJPcjiLq2MMUgmzxLkZTs/Oc9pn+DN1J0lE5tzS+2h8wy9Ii78auHfgvLEHXBJi</HostId></Error>\n2014-04-21 22:28:10,607 ERROR: Exception on /v1/images/a028e535b2c8dc2ae7c3bab8abfd6688c2115ac4ee9d5a4b133e5b0d9a50d555/layer [PUT]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1687, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1360, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1358, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1344, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/docker-registry/registry/toolkit.py\", line 224, in wrapper\n    return f(*args, **kwargs)\n  File \"/docker-registry/registry/images.py\", line 182, in put_image_layer\n    store.stream_write(layer_path, sr)\n  File \"/docker-registry/lib/storage/s3.py\", line 61, in stream_write\n    mp.complete_upload()\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/multipart.py\", line 308, in complete_upload\n    self.id, xml)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 1649, in complete_multipart_upload\n    response.status, response.reason, body)\n. ",
    "josephg": "I'm seeing this problem too, using a US bucket.\n. +1 I'm having the same issue.\n. ",
    "rgbkrk": "Oh is there a different repo I should bring this up in?\n. @kencochrane, @shin- Thanks! I don't see any other issues, this works as expected.\n. ",
    "allardhoeve": "Done\n. Wireshark says the client offers a JSON string with creds, but no basic\nauth headers...\nWill try the repo limit.\n. Sorry, no cigar.\nIf I'd run Wireshark, I expect Docker still just sends the JSON auth, no headers.\n```\n\n  ServerAdmin allard@hoeve.nl\n  ServerName my.registry.io\n\n    Order allow,deny\n    Allow from all\n    AuthType Basic\n    AuthName \"auth required\"\n    AuthUserFile /etc/apache2/htpasswd\n    Require valid-user\n  \n\n    Satisfy any\n    Allow from all\n  \nProxyPreserveHost  On\n  ProxyRequests      Off\n  ProxyPass          /  http://localhost:5000/\n  ProxyPassReverse   /  http://localhost:5000/\nErrorLog ${APACHE_LOG_DIR}/error.log\n  LogLevel warn\nCustomLog ${APACHE_LOG_DIR}/access.log combined\n\n```\n``` yaml\nThe `common' part is automatically included (and possibly overriden by all\nother flavors)\ncommon:\n    loglevel: info\n    storage: local\n    storage_path: /tmp/docker/\nprod:\n    loglevel: debug\n    storage: local\n    storage_path: /srv/docker/\n    standalone: true\n    email_exceptions:\n        smtp_host: localhost\n        smtp_login:\n        smtp_password:\n        from_addr: docker-registry@my.com\n        to_addr: allard@my.com\n```\nOutput:\n```\nallard@dev-allard2 ~ # ./docker pull my.registry.io/mydocker\nPulling my.registry.io/mydocker\nPlease login prior to push:\nLogin against server at http://my.registry.io/v1/\nUsername (allardhoeve): myuser\nPassword: mypass\nEmail (allardhoeve): my@example.com\n2013/09/20 19:34:18 Error: Unexpected status code [401] : <!DOCTYPE HTML PUBLIC \"-//IETF//DTD HTML 2.0//EN\">\n```\nlogs:\n```\n==> /srv/docker-registry/logs/access.log <==\n\"10.1.2.239 - - [20/Sep/2013:19:34:14] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n==> /var/log/apache2/access.log <==\n10.1.2.239 - - [20/Sep/2013:19:34:14 +0200] \"GET /v1/_ping HTTP/1.1\" 200 258 \"-\" \"Go 1.1 package http\"\n10.1.2.239 - - [20/Sep/2013:19:34:18 +0200] \"POST /v1/users/ HTTP/1.1\" 401 578 \"-\" \"Go 1.1 package http\"\n```\n. Token bases auth is the better solution, but can not be archieved from\nwithin Apache without application logic. The tokens and headers that the\nclient currently sends mean nothing to Apache. So the docker registry will\nneed to handle any token-based auth.\nThis will open up a can of worms for you. Example: people will have their\nusers LDAP, couplings for which are available for Apache pr Nginx easily,\nbut which they or you will have to implement yourself. Reinventing tested\ntechnical security methods rarely ends well :-)\nAlso resending requests has been done in HTTP land for a long time. You\ncould keep in mind the authentication requirement once per issued docker\ncommand. And in this case also entirely optional in a private setting.\nYes, resending would slightly increase chances of someone intercepting the\npasswords one a second request, but only if DNS or routing changed in\nbetween requests, which isn't likely. But if you run an authenticated\nserver with a plain text password scheme, you should use TLS anyway.\nThe client already performs a ping of the server (/v1/_ping). When\nencountering a 401 with basic auth headers, the client dies. It could\ninstead user this knowledge to use basic auth throughout the session\n(current docker command).\nOr, if this solution is not preferred, users could white-list the ping URL\nand could add a flag to the registry config telling it to send a flag in\nping responses so the client knows to expect the basic auth on future\nrequests.\n. ",
    "mhennings": "i think it should be enough to implement the token mechanism based on basic auth for the /auth call. it is called first and afterwards the client will send the token for the registry calls.\nsome kind of session cookie would be working, too\nboth ideas would require an implementation inside the regstry.\nbut ... i think most users would consider this as an easier solution than running/maintaining a proxy as well\n. ",
    "Tranquility": "@shin- I tried the config you posted here with version 0.7.1 of docker but it doesn't work. When I try to push the error message from docker is \"Authentication is required.\" and in the apache/error.log \"client used wrong authentication scheme: /v1/images/016e2c15fcae9e7a4d47ab18e3c2de7ead0810c6f8b19761d33e5cd0cd353997/json\" \n. Thanks!\nNo, I really used exactly the one you posted a few comments above I just removed the ServerName option. \n. ",
    "GaretJax": "This is working for me with docker 0.7.1 and the following nginx config:\n```\nserver {\n    listen 443;\n    server_name ***;\nssl on;\nssl_protocols             TLSv1 TLSv1.1 TLSv1.2;\nssl_ciphers               ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AES:RSA+3DES:!ADH:!AECDH:!MD5:!DSS;\n\nssl_prefer_server_ciphers on;\nssl_certificate /etc/ssl/certs/***;\nssl_certificate_key /etc/ssl/***;\n\nproxy_set_header Host       $http_host;   # required for docker client's sake\nproxy_set_header X-Real-IP  $remote_addr; # pass on real client's IP\n\nclient_max_body_size 0; # disable any limits to avoid HTTP 413 for large image uploads\n\n# required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\nchunkin on;\nerror_page 411 = @my_411_error;\nlocation @my_411_error {\n    chunkin_resume;\n}\n\nlocation / {\n    # Basic authentication\n    auth_basic            \"Restricted\";\n    auth_basic_user_file  auth/registry.htpasswd;\n\n    proxy_pass http://$host_ip:5000;\n    proxy_read_timeout 900;\n}\n\nlocation /_ping {\n    auth_basic off;\n    proxy_pass http://$host_ip:5000;\n    proxy_read_timeout 900;\n}\n\nlocation /v1/_ping {\n    auth_basic off;\n    proxy_pass http://$host_ip:5000;\n    proxy_read_timeout 900;\n}\n\n}\n```\n. Frankly I don't know. S3 and GS are the only two supported by Boto (along with Amazon Glacier, but it doesn't fit the use case).\n. @samalba for any other container I would 100% agree with you, but the docker registry is a little different. I like the fact to be able to just pull a repo and run the container without any other additional files. Once the registry is in place, I can then push my personal (and more customized) containers to it.\nNothing that can't be solved by other means (scp the config file, for example), but this change would allow to keep the actions to run a custom registry at a minimum.\nI understand your point of view, though. So feel free to close this PR if you consider it's not worth it.\n. ",
    "ehazlett": "@shin- how are the certs checked?  I'm using a certificate that is from a valid CA (currently running in production) but when attempting to use in the docker registry i get \"certificate signed by unknown authority\".\n. @shin- thx\n. ",
    "winggundamth": "Do you have plan on authentication with normal http or self-signed https?\nWhen I tried with normal http it shows error \"Authentication is required.\"\nIf I tried with self-signed https it shows error \"Invalid Registry endpoint: Get https://docker-registry.xxx.com/v1/_ping: x509: certificate signed by unknown authority\"\n. Ok sure.\nFor curl -i -X POST https://identity.example.com/v2.0/tokens -H \"Content-Type: application/json\" -H \"Accept: application/json\" -H \"User-Agent: python-keystoneclient\" -d '{\"auth\": {\"tenantName\": \"swift-registry\", \"pas\nswordCredentials\": {\"username\": \"keystoneuser\", \"password\": \"keystonepass\"}}}'\nI can use curl command and it returns keystone token in json correctly but let me try to compile source and I'll post an issue on your swift driver repo.\n. ",
    "iapilgrim": "@bacongobbler: I've got the same issue with @diewanda\nroot@precise64:~# docker login https://docker-internal.xxxx.com\nUsername: docker\nPassword:\nEmail: xxxx\n2014/09/15 09:12:41 Error response from daemon: Invalid Registry endpoint: Get https://docker-internal.xxxx.com/v1/_ping: x509: certificate signed by unknown authority\nNote: i'm following your great post: http://www.activestate.com/blog/2014/01/deploying-your-own-private-docker-registry\nmy cert is verified by Geo Trust. \nAny advice?\n. ",
    "pilgrim2go": "@shin- : Here my info\ndocker version\nClient version: 1.2.0\nClient API version: 1.14\nGo version (client): go1.3.1\nGit commit (client): fa7b24f\nOS/Arch (client): linux/amd64\nServer version: 1.2.0\nServer API version: 1.14\nGo version (server): go1.3.1\nGit commit (server): fa7b24f\nFor more information, \n1) From Docker host ( vagrant):\ncurl -u iapilgrim:***** https://docker-internal.xxx.com\ncurl: (60) SSL certificate problem, verify that the CA cert is OK. Details:\nerror:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed\nMore details here: http://curl.haxx.se/docs/sslcerts.html\ncurl performs SSL certificate verification by default, using a \"bundle\"\n of Certificate Authority (CA) public keys (CA certs). If the default\n bundle file isn't adequate, you can specify an alternate file\n using the --cacert option.\n2) But it's OK from my host ( OSX)\n curl -u iapilgrim:*** https://docker-internal.xxx.com\n\"docker-registry server (prod) (v0.8.1)\"\nI guess it could be docker issue ( the way docker client is handling ssl) or do I need to do something else?\n. Using the following gist to add to system store\nhttps://gist.github.com/aaronzirbes/1729503\nbut it still fail\nError response from daemon: Invalid Registry endpoint: Get https://docker-internal.xxx.com/v1/_ping: x509: certificate signed by unknown authority\n. @shin- : it was fixed. I'm using Geotrust wildcard certificate.\nBy adding it to our cert.\ncat /path/to/intermediate.pem >> ssl/wildcard-xxxxxxx.xx.xx.crt\nIt works fine now.\nThanks\n. ",
    "denmat": "Have had the same issue, have submitted a pull request which updates the example config.yml. #88\n. Yes, 'fixed' was a too strong a word. wasn't it. Work around is probably more accurate. This does however make the doc match the current workings. I will try to fix the underlying bug in the meantime.\n. ",
    "rstiller": "It would be fine if the registry could process both config paths with and without trailing slash.\nThe bug is in the code and should be fixed there - not in any example documentation.\n. great :+1: \n. ",
    "rogaha": "Hi @shin-, as @samalba asked me, I did some benchmark tests to measure the bandwidth performance for pulling and pushing images between cdn_index and staging_index. Apparently, the cdn index was more stable than staging index for pulling images in terms of bandwidth, however for pushing images on both places had almost the same results. \nPlease feel free to comment on that. The conclusion is not clear. On both sites I was able to achieve the max speed at some point, as we can see on the pictures attached below. \nSummary:\nON CDN_INDEX\nPushing images \n- MAX: 805 KB/s\n- AVG: 599.52 KB/s\nPulling images\n- MAX: 8228 KB/s\n- AVG: 6817.50 KB/s\n\n\nON STAGING_INDEX\nPushing images\n- MAX: 808 KB/s\n- AVG: 555.67 KB/s\nPulling images\n- MAX: 8256 KB/s\n- AVG: 4202.57 KB/s\n\n\n. @samalba, \nThere are some efforts towards the stream_write and stream_read support from the moto contributors. \nhttps://github.com/spulec/moto/issues/51\nMaybe we will be able to test it in a near future. \n. Hi @shin-,\nSure. I'll do that! \nThanks! \n. Hi @vmalloc,\nCould please certify that your local time is synced?\nThanks\n. Done! :) \n. Ok. I got it! Good idea! :) \n. Ok\n. ",
    "benjaminws": ":thumbsup: thanks for this\n. ",
    "wiwengweng": "Hi, when I install libssl-dev, I met errors.\u3000I am using a Ubuntu 12.04.4 host here. I have tried apt-get update and upgrade, but don't seem to be solved.\nThe following packages have unmet dependencies:\n libssl-dev : Depends: libssl1.0.0 (= 1.0.1-4ubuntu5.13) but 1.0.1-4ubuntu5.17 is to be installed\nE: Unable to correct problems, you have held broken packages.\nother useful info maybe:\nroot@ubuntu-VirtualBox:/home/ubuntu# aptitude show libssl1.0.0 \nPackage: libssl1.0.0 \nState: installed \nAutomatically installed: no \nMulti-Arch: same \nVersion: 1.0.1-4ubuntu5.17 \nPriority: important \nSection: libs \nMaintainer: Ubuntu Developers ubuntu-devel-discuss@lists.ubuntu.com \nArchitecture: amd64 \nUncompressed Size: 2,992 k \nDepends: libc6 (>= 2.14), zlib1g (>= 1:1.1.4), debconf (>= 0.5) | debconf-2.0 \nPreDepends: multiarch-support \nBreaks: openssh-client (< 1:5.9p1-4), openssh-client (< 1:5.9p1-4), openssh-server (< 1:5.9p1-4), openssh-server (< 1:5.9p1-4), libssl1.0.0 (!= 1.0.1-4ubuntu5.17) \nReplaces: libssl1.0.0 (< 1.0.1-4ubuntu5.17) \nDescription: SSL shared libraries\nroot@ubuntu-VirtualBox:/home/ubuntu# apt-cache policy libssl1.0.0\nlibssl1.0.0:\n  Installed: 1.0.1-4ubuntu5.17\n  Candidate: 1.0.1-4ubuntu5.17\n  Version table:\n *** 1.0.1-4ubuntu5.17 0\n        100 /var/lib/dpkg/status\n     1.0.1-4ubuntu5.13 0\n        500 http://10.1.2.12/ubuntu/ precise-security/main amd64 Packages\n        500 http://10.1.2.12/ubuntu/ precise-updates/main amd64 Packages\n     1.0.1-4ubuntu3 0\n        500 http://10.1.2.12/ubuntu/ precise/main amd64 Packages\nroot@ubuntu-VirtualBox:/home/ubuntu# apt-cache policy libssl-dev\nlibssl-dev:\n  Installed: (none)\n  Candidate: 1.0.1-4ubuntu5.13\n  Version table:\n     1.0.1-4ubuntu5.13 0\n        500 http://10.1.2.12/ubuntu/ precise-security/main amd64 Packages\n        500 http://10.1.2.12/ubuntu/ precise-updates/main amd64 Packages\n     1.0.1-4ubuntu3 0\n        500 http://10.1.2.12/ubuntu/ precise/main amd64 Packages\n. Thanks, dmp. I can manage this through aptitude software manager. But what I feel wired is that higher version is not supported, and this will cause some discomfort and extra work when install docker registry . :-)\n. @dmp42 I think I got an old guide in [1]. So now we can directly using pip install to set up docker registry, right? But how about the ssl connection if we want https access? And I also see that now we don't need to login the local registry before pushing images to it. So how about the basic authentication for the local registry?\n[1] http://www.activestate.com/blog/2014/01/deploying-your-own-private-docker-registry\n. OK, thanks. Will go irc then.\n. ",
    "chasballew": "I'm not super familiar with how PyYAML pulls booleans. I think we can even do key.set_contents_from_string(content, encrypt_key=self._config.s3_encrypt)? I think boto only cares if it's True. @samalba any preference?\n. Excellent, thanks for the clarification.\n. @samalba: should be good now\n. Apologies, I thought you meant rebase the commits. Didn't realize I needed a merge from upstream.\n. ",
    "mzdaniel": "Here is the coverage report for reference.\ncoverage report --include='./' --omit='./test/'\nName                   Stmts   Miss  Cover\nlib/cache                 55      0   100%\nlib/checksums             63     46    27%\nlib/config                44      3    93%\nlib/signals                4      0   100%\nlib/storage/init      82     27    67%\nlib/storage/glance       170    139    18%\nlib/storage/local         67      3    96%\nlib/storage/s3           166    132    20%\nregistry/init          7      0   100%\nregistry/app              31      7    77%\nregistry/images          187     36    81%\nregistry/index            76      8    89%\nregistry/tags             70     10    86%\nregistry/toolkit         144     76    47%\nTOTAL                   1166    487    58%\nTox is reporting a few pep8 issuesthat are not directly related to this PR. It's interesting to notice that this PR had uncovered them.\nTox report of this PR (5b7eef194165)\ntox\npy27 runtests: commands[0] | python -m unittest discover -s /data/dotcloud/docker-registry/test\n.............2013-10-09 14:17:54,672 INFO: Enabling storage cache on Redis\n2013-10-09 14:17:54,672 INFO: Redis config: {'host': 'localhost', 'password': None, 'db': 0, 'port': 6379}\n2013-10-09 14:17:54,673 INFO: Enabling storage cache on Redis\n2013-10-09 14:17:54,673 INFO: Redis config: {'host': 'localhost', 'password': None, 'db': 0, 'port': 6379}\n.2013-10-09 14:17:54,673 INFO: Enabling storage cache on Redis\n2013-10-09 14:17:54,673 INFO: Redis config: {'host': 'localhost', 'password': None, 'db': 0, 'port': 6379}\n.2013-10-09 14:17:54,674 INFO: Enabling storage cache on Redis\n2013-10-09 14:17:54,674 INFO: Redis config: {'host': 'localhost', 'password': None, 'db': 0, 'port': 6379}\n..\nRan 17 tests in 3.940s\nOK\npep8 runtests: commands[0] | flake8 /data/dotcloud/docker-registry\n/data/dotcloud/docker-registry/lib/storage/init.py:118:1: H302  import only modules.'from s3 import S3Storage' does not import a module\n/data/dotcloud/docker-registry/registry/images.py:16:1: H302  import only modules.'from storage.local import LocalStorage' does not import a module\n/data/dotcloud/docker-registry/test/test_images.py:26:9: H302  import only modules.'from storage.local import LocalStorage' does not import a module\nException KeyError: KeyError(59293936,) in  ignored\nERROR: InvocationError: '/data/dotcloud/docker-registry/.tox/pep8/bin/flake8 /data/dotcloud/docker-registry'\n________ summary ___________\n  py27: commands succeeded\nERROR:   pep8: commands failed\ntox report of master (f3652f0621f31)\ntox\npy27 runtests: commands[0] | python -m unittest discover -s /data/dotcloud/docker-registry/test\n...............\nRan 15 tests in 3.953s\nOK\nException KeyError: KeyError(20362896,) in  ignored\npep8 runtests: commands[0] | flake8 /data/dotcloud/docker-registry\n_______ summary ________\n  py27: commands succeeded\n  pep8: commands succeeded\n  congratulations :)\n. Test didn't pass as tox keeps finding pep8 issues in other parts of the code\n/home/travis/build/dotcloud/docker-registry/registry/images.py:16:1: H302 import only modules.'from storage.local import LocalStorage' does not import a module\n/home/travis/build/dotcloud/docker-registry/test/test_images.py:26:9: H302 import only modules.'from storage.local import LocalStorage' does not import a module\nThis PR has raised total registry coverage from 56% to 62%\nWith this PR:\nName                    Stmts   Miss  Cover\nlib/cache                  55     29    47%\nlib/checksums              63     46    27%\nlib/config                 44      4    91%\nlib/signals                 4      0   100%\nlib/storage/init       88     31    65%\nlib/storage/boto_base     141     59    58%\nlib/storage/gcs            27     13    52%\nlib/storage/glance        170    139    18%\nlib/storage/local          67      3    96%\nlib/storage/s3             40      0   100%\nregistry/init           7      0   100%\nregistry/app               30      6    80%\nregistry/images           187     36    81%\nregistry/index             76      8    89%\nregistry/tags              70     10    86%\nregistry/toolkit          144     76    47%\nTOTAL                    1213    460    62%\n. Done. PR #111 in place and this one rebase on top of it.\n. Sure.  PR updated.\n. ba16c9c now includes host information in the status json and pushed into staging for review.\n. flask.current_app didn't quite work for me  (RuntimeError: working outside of application context) when trying to execute @route), but this PR update did.\n. @shin-: There are 2 reasons for this: In this case, I wanted to minimally interact with different project code bases  and organization while adding new functionality, and second, team-ci and docker-ci looks in /dockertest to build and run the tests\n. @shin-: I am really glad you made that comment as I thought more about it. You are right that is better and certainly more elegant to have the Dockerfile under test. Here is the updated code.\n. Strange. travis is complaining the tests are failing, though they pass in my system and in staging. Moreover seems the same issue than in the build at HEAD~ ( 211d8bf (master)) https://travis-ci.org/dotcloud/docker-registry/builds/19946729 \nThis PR in itself is a change in /test/Dockerfile, so it should not affect travis results.\n. ",
    "ProbablyRusty": "Done.\n. I am seeing the same issue. The container launch hangs when specifying AWS_REGION, and works OK when AWS_REGION is not specified. I have seen this issue on every version of docker-registry >0.6.9.\n. Duplicate Issue #400. Sorry.\n. ",
    "noxiouz": "\nit would make sense to rename lib/storage/ell.py to lib/storage/elliptics.py\n\nPython package for Elliptics has elliptics.py, so I renamed ell.py to ellipticsbackend.py to prevent clashes of names.\nThank you!\n. It's a good idea to rename options. So naming is fixed.\n. Hi @dmp42 \nOh, It's not a problem for me to apply this patch to the standalone driver :). So PR is closed.\n. Hello!\nAs Docker Registry is used in our open source PaaS, and Elliptics is being developed in our team, so I'm the best candidate =). Be sure, I'll support it.\n. OK. Could you transfer repos to my github account?\n. Thanks! I've accepted.\n. mmm... Could you replace capital letters in repository name (i.e. SM_realse -> sm_releasr), please?\n. As mainteiner of driver what information is  useful to make a suggestions about health status. So each maintener should take resposibility for quality of information. Of course if someone needn't this it seems ok to return 405 'Method not allowed'. The main purpose is to provide SRE or system administrator some information about health without log aggregation. The bottom line is if driver developer wants to make monitoring system easier and more reliable there's an ability to do this. For example in Yandex we use Elliptics driver and Elliptics has its own monitoring system and  it would be great to use this information in the status checking withoug going to storage nodes.\n. Let's go step by step. The first point is that the current '_ping' is absolutely useless as it doesn't check anything except gunicorn/nginx (i.e. network access to S3 is down, it's quite clear that driver would start but not work. In case of local storage free disk space is a good point). Not only piping, implement any logic. And when it comes to abstraction I think only java-guys cann't live without it. But ok. 200 is good, any other should include error description in plaintext/json, whatever. Is it suitable abstraction? And the last one to build really fault tolerant system you must get rid of a single point of failure, so all registries should stay behind a balancer (not DNS RR) like IPVS. It requires to have simple and possible fast way to check status and according to this information make a decision to unjoin bad node.\n. Please, don't let me down! It's hardly possible to know more then the mainteiner about a driver needs. It's common and well-known practice to implement a checking handler which really checks status. It goes without saying that this handle will check Flask code (it's immutable during one installation, yeah? So what the purpose to check it?). I have no more argumentation, don't be so enterprise :) \n. Also we have guys who are responsible for keeping an eye on storage. They provide this storage as a service for other Yandex services. So they needn't know about the registry and their balancer, they maintain storage. And guys who maintain registry should think and imagine this storage as blackbox. So they need to have this handler.\n. Thwy need to remove anounce from balancer in one data center without knowing about recovery time.\n. Sorry for that close/reopen stuff. My mobile made an error. Anyway I find this discussion very interesting and useful.\n. Sorry for that delay... Imma make PR during the next week.\n. In my opinion this\n\ndocker-registry-extensions repository which consists only of a README with an alphabetized list of known extensions, and a one-sentence description and link for each one.\n\nis the best way to solve a plugin discovery problem. One huge repo for all extensions... It might be hard to maintain.\nPS. If this repo is supposed to be in docker organization and I become a member as an extension maintainer it's a great idea! =)\n. May be Flask or Tornado formats?\n. @dmp42 the same story. It will be installed on our test cluster tomorrow.\n. > HTTP based communication is fine by me (in a micro-services world), and also elegantly solve scalability and delegation problems.\nWhat about some kind of a binary protocol with multiplexing of read/writes streams? Draft of HTTP/2 looks good as a concept. \nWe can take a look at some common binary serialization libraries (for example msgpack) and use one of them to communicate between core and plugins over tcp/unix domain socket. It allows us to implement a fast, flexible, easy-to-extend protocol. This protocol should be bidirectional to provide a full control over communication. \n. > I'd be in favor of starting simple with this. You can always switch to something else when you start hitting scale problem (more than 10 extensions).\nFor the first step it's reasonable. But it's really hard to maintain (as far as I know extensions were removed from docker-registry core repo beacause of that). Registry could provide an interface for registering your own plugin like http://golang.org/pkg/database/sql/#Register. In this case each plugin developer, who can't put extension implementation to the docker-registry core repo, has to distirbute special build with his/her plugin. For example, I can't distribute my Elliptics extension with core as it needs Elliptics binary package to be built. In python epoch, it was not imported if it wasn't used, so didn't bring problems.\nAlso I suppose there're a lot of people who don't like/want to learn go, but they're mad about Docker Registry =) and that guys possibly want to write extensions in other languages (for example, there's no golang binding for their technology), so clean RPC communication protocol would help them.\n. > ... and would love to have @noxiouz (and other yandexers?) on monday as well.\n@dmp42 I'll try... But I can't make a promise to join a discussion this monday.\n. > Do you need a separate coroutine for each client? If so, it seems like it would be more efficient to have a single thread using select() or similar event-based processing (but maybe Go's coroutines have absurdly small memory footprints?).\nIt's a common practice in golang. Each goroutine needs ~4KB. Of course, golang has epoll/kqueue under the hood, but we're not allowed to use it directly and have no need, as golang schedules our goroutines.\n. @BrianBland I sent your PR into next-generation branch. Should I have sent PR here instead of your fork?\n. > Error handling (stack traces and error wrapping)\nI believe, that this approach looks good:\nhttps://github.com/facebookgo/stackerr \n. Looks like #823. And nginx version is the same. What's ur nginx configuration file? Is it proper for version < 1.3.9?\n. Seems it's connected with a versions of python  in 14.04 & 14.10. ssl package was changed between them. \nPython requests of your version rely on the fact that ssl package from standart library hasn't got SSLContext:\nhttps://github.com/kennethreitz/requests/blob/v2.3.0/requests/packages/urllib3/util/ssl_.py#L95\nBut it was backported from python 3.2+ to python 2.7.9.\nSo the bottom line is you can try to update python requests :-)\n. I suppose, receiver.Receive supports unpacking into an anonymous struct. It should help to get rid of type conversions.\nIt looks like:\n``` go\nvar response struct {\n    Contents []byte\n    Error string\n}\nerr = receiver.Receive(&response)\nif err != nil {\n    return nil, err\n}\nif p.Error != \"\" {\n   err = errors.New(p.Error)\n// err = fmt.Errorf(\"blabla error %s\", p.Error)\n}\nreturn p.Contents, err\n``\n.parentSocketshould be saved inDriverClientstruct. It must be closed inStop()as well asd.transport.\n. Although *File fromos.NewFilecloses byruntime.SetFinalizer`, it's better to close explicitly.\ngo\ndefer childSocket.Close()\n. go\ndefer conn.Close()\n. Do we really need panic or it's ok to return error\n. Although Server has error as return type, it actually doesn't return any error when something goes wrong, only panic\n. go\n// It doesn't fill the slice with  empty strings\n// and allocates enough memory\nkeys := make([]string, 0, len(fileNames)) \nfor _, fileName := range fileNames {\n  keys = append(keys, prefix + \"/\" + fileName) // or path.Join(prefix, fileName)\n}\n. It's better to get rid of i index:\ngo\nkeys := make([]string, 0, len(keySet))\nfor k := range keySet {\n    keys = append(keys, key)\n}\n. According to ipc.Server driver is passed to many goroutines at the same time. So there's a concurrent access to storage. It should be thread-safe to avoid data races in tests.\n. ",
    "noteed": "@shin- Yes I did: http://hackage.haskell.org/package/aeson-0.6.1.0/docs/Data-Aeson.html (see in the Pitfalls section).\n. @shin- Sure but the problem remains (i.e. it is not about being able to parse or not the payload, it is about respecting the standard, as the doc you linked to say, for interoperability and security reasons).\n. ",
    "tianon": "Please see my reply: https://github.com/dotcloud/docker/issues/1220#issuecomment-26641835\nThis is by design, due to the way docker run works.  You want docker start after the first run, because run creates a new container every time. :)\n. I'm getting this on a fresh build from master without any bind mount or even special options (just docker run -it --rm --name registry <image-id>).\n. Adding -e GUNICORN_OPTS=[--preload] fixed it, too. :smile:\n. :heart: :heart: :heart: :purple_heart: !\n. If I missed some dependency that's only necessary in certain situations though, I'm happy to iterate from here.  I only really tested getting it started up and pushing an image into it to verify that it actually runs and functions as expected.\n. Sorry for the delay!\n- this is likely 2x bigger due to https://registry.hub.docker.com/_/buildpack-deps/ -- we're working on \"slim\" versions of the images in case you'd prefer to wait for those; since you don't use the image for development, that's probabaly something worth considering\n- jessie is in freeze, as I noted over in https://github.com/docker-library/official-images/issues/319#issuecomment-63345644, so the versions of packages are very, very unlikely to change, and if they do it's either going to be a minor update or a security fix\n. Ok, fair enough!  I'll revisit this when we've got the slim versions ready. :+1:\nSee https://github.com/docker-library/python/issues/16 for the issue where we're tracking the progress of the slim versions. :smile:\n. Since Dockerfiles can't be parameterized, the cleanest way I've seen this handled is via transparent proxy in iptables (either for all traffic, or just for the mirrors you need explicit proxies for).\n. Shouldn't cmd here be uppercase too? :wink:\n. :heart:\n. lol indentation\n. $ROOTFS?  Someone copy-pasta'd too much. :wink:\n. You mean:\nDockerfile\nENTRYPOINT [\"/docker-registry/run.sh\"]\nCMD [\"docker-registry\"]\n. So it might be good to rename run.sh now to be less misleading. :wink:\n. It's not a script; it's a compiled Go binary.\n. This is specifically to improve the caching - if these files haven't\nchanged, the \"pip install\" won't be repeated.\n. That's up to you -- don't you have to rebuild often while doing development on it?  I'm not worried about space as much as time.  How often does this directory change vs the rest of the codebase?\n. ",
    "theopolis": "Should this issue be closed?\nIf you're running the registry with a local store then you'll want to run the registry docker with a volume.\ndocker run -d -p 5000:5000 -v /var/lib/docker/registry-data/:/var/lib/docker/registry-data/ docker-registry so the push-ed data will persist on your host at /var/lib/docker/registry-data.\n. How does #151 look?\n. Sounds good! \nI'm not sure if anyone else is running a standalone registry (and receiving this error) but just incase I wanted to include the exception exec trace:\nERROR: Exception on /v1/images/<IMAGE_HASH>/json [GET]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1687, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1360, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1358, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1344, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/docker-registry/registry/toolkit.py\", line 202, in wrapper\n    return f(*args, **kwargs)\n  File \"/docker-registry/registry/images.py\", line 31, in wrapper\n    return f(*args, **kwargs)\n  File \"/docker-registry/registry/images.py\", line 53, in wrapper\n    return f(*args, **kwargs)\n  File \"/docker-registry/registry/images.py\", line 206, in get_image_json\n    repository = toolkit.get_repository()\n  File \"/docker-registry/registry/toolkit.py\", line 237, in get_repository\n    parts = auth.get('repository').rstrip('/').split('/', 1)\nAttributeError: 'NoneType' object has no attribute 'rstrip'\n. No problem, I got the auth-bypass for ping and users from one of your issue comments, thanks for that! \nAnd for posterity, to bring a registry into a semi-scalable multi-user environment I've been using Atlassian's Crowd and the various directories configured within Crowd (e.g., LDAP) and their Apache authz module.\n```\n\n    Order deny,allow\n    Allow from all\nAuthName \"Registry Authentication\"\nAuthType basic\nAuthBasicProvider crowd\n\nCrowdAppName registry\nCrowdAppPassword <YOUR_APPLICATION_PASSWORD>\nCrowdURL http://internal.crowd.server:8095/crowd/\n\nRequire valid-user\n\n\n```\nUser then login with their directory account when prompted by the Docker client as they would with any other directory-enabled application, pretty sweet. :)\n. Not sure what you mean, the authentication isn't needed at all, right, but I figured it would be nice to add the auth example to highlight the two exception paths.\n. ",
    "fabiofalci": "Yes, i think so.\nThe only difference is that I have to point to /tmp/registry because that's the default storage_path for dev.\ndocker run -d -p 5000:5000 -v /mnt/hd/docker-registry-images/registry/:/tmp/registry/ stackbrew/registry\n. ",
    "brendandburns": "There are some small changes (notably multi part upload doesn't seem to work quite perfectly in google storage)\nI can probably re-factor into a single shared class with extensions. I'll work on that.\n. Ok, refactored.  please take another look.\nThanks\n--brendan\n. Ok, pep8 error fixed.  This passes my new test/gcs.py test which mirrors the test/s3.py test.\nI don't have access to an s3 bucket, so I can't run the s3 test, please let me know how you'd like me to proceed.\nThanks!\n--brendan\n. Thanks!  Let me know if there are any problems.\n--brendan\n. awesome, thanks!\n--brendan\n. Hey\nThanks for doing this.  We can definitely set you up with a free trial\naccount, send me your project I'd and I'll take care of it.\nAlso I would be happy to maintain this plugin.  Feel free to transfer\nownership over to me.\nThanks again!\nBrendan\nOn May 16, 2014 2:52 AM, \"Mangled Deutz\" notifications@github.com wrote:\n\nHi @brendandburns https://github.com/brendandburns\nWe are moving to a plugins architecture that will let the community author\nstorage driver on their own and publish them as separate pip packages.\nWhile cleaning the code through that end, I repackaged the gcs driver as\none of these new \"plugins\".\nThe repo is here: https://github.com/dmp42/docker-registry-driver-gcs\nUnfortunately, I don't have a GCS paying account, and it seems that you\ncan't create a bucket with the trial (!!??$$??google!!).\nAlso, I couldn't find a way to mock it.\nSo:\n- right now, I'm not sure if it's working or if it is broken - several\n  earlier cleanups likely broke it IMO - and obviously running the tests\n  doesn't make sens without a working GCS\n- we are looking for someone to re-adopt that code and maintain it\nThe package is squeaky clean. It's all tox-ed / travis-ed and ready to be\npypi-ed :-) - all that misses is some google love, and someone to run toxand fix the problems / documentation.\nIf you are interested in taking this over, ping me here and I'll transfer\nthe github repo to you.\nThanks a lot!\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/pull/107#issuecomment-43315091\n.\n. Hey,\nSorry for the delay.  Can you finish enabling billing for this project?\n\nOnce that's done we can comp you some free resources.\nThanks\nBrendan\nOn May 16, 2014 9:16 AM, \"Mangled Deutz\" notifications@github.com wrote:\n\nHi @brendandburns https://github.com/brendandburns\nThanks for this!\nProject id: opportune-box-582\nBest\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/pull/107#issuecomment-43350300\n.\n. Great, thanks.  Looks like the PM who does free trials is OOO but should\nappear on your account by the end of the week.\n\nThanks!\nBrendan\nOn May 20, 2014 12:35 PM, \"Mangled Deutz\" notifications@github.com wrote:\n\nHi @brendandburns https://github.com/brendandburns\nIt's done - now you got my credit card :-)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/pull/107#issuecomment-43672632\n.\n. Ok, you should now be all set up.\n\nThanks!\n--brendan\nOn Tue, May 20, 2014 at 12:43 PM, Brendan Burns bburns@google.com wrote:\n\nGreat, thanks.  Looks like the PM who does free trials is OOO but should\nappear on your account by the end of the week.\nThanks!\nBrendan\nOn May 20, 2014 12:35 PM, \"Mangled Deutz\" notifications@github.com\nwrote:\n\nHi @brendandburns https://github.com/brendandburns\nIt's done - now you got my credit card :-)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/pull/107#issuecomment-43672632\n.\n. \n\n",
    "proppy": "PTAL\n. /cc @govidiupl from Google.\n. > hard compile the extensions in\nI'd be in favor of starting simple with this. You can always switch to something else when you start hitting scale problem (more than 10 extensions).\n. I was also wondering how multiple signatures would be implemented.\nIn particular if they will behave like envelopes\ni.e:\nCorp A sign layer d34db33f\nCorp B sign (Corp A signature +  layer d34db33f)\nCorp C sign (Corp A signature +  layer d34db33f)\nCorp D sign (Corp B sign (Corp A signature +  layer d34db33f))\nCorp E sign (Corp C sign (Corp A signature +  layer d34db33f))\nCan I raise that as a topic for next next Monday?\n. > There's a later comment about\n\nmultiple-sigs still being a work-in-progress though [1].\n\nI'm curious how the revocation would work?\nI was thinking that you could revoke by signing the previous envelope you created with your sig, w/ some additional metadata.\n. > Otherwise, you could presumably pull your sig off the registered image\nOh I didn't really think of the signatures array as mutable as this. I thought you would only append extra sig to it.\n. > I don't know if that API has been worked out yet, but I see no reason\n\nwhy removing your own sig should be illegal.\n\n@wking I liked the idea of having a guaranteed audit trail of signature event.\n. @wking agreed, sorry about the noise.\n. Happy to rename the repo https://github.com/GoogleCloudPlatform/docker-registry to match the naming scheme.\nBut I need to ping the \"hub people\" first to fix my automated build (they get easily broken if you rename your github repo).\n. ~~wondering if @dmp42 would have preferred a python impl and keep the ENTRYPOINT intact.~~\n. consider adding documentation about accepted values.\n. wonder if we should keep the ENTRYPOINT in python and push the extra logic about in-place cert generation to generate_certs\n. If feel this could be easier to follow with explicit checks.\nIf the dir empty: generate\nElse: which files are missing\n. Sorry to go back and worth on this, but after taking a closer look I'm not that refers to the same thing.\nREGISTRY_HOST refer to the hostname within the container, i.e: 0.0.0.0 means listen on the container IP.\nHere we want to refer to the container as it is addressed from the host, when publishing its port: i.e boot2docker ip.\n. I think certs should go in /ssl/$EXTERNAL_REGISTRY_HOST/, if you bindmount /etc/docker/certs.d\n. I wonder if we should assume that it's named registry.* or just assumed they get passed as their own bind mount?\n. ",
    "blalor": ":thumbsup: The default config isn't sufficient for working with S3.\n. I'm also having this problem.  S3-backed private registry is incredibly slow.  I actually see very little activity (via netstat) between the registry and S3.  Most of the time seems to be spent doing nothing.  Very frustrating.\n. Yes. I'm using blalor/docker-s3-registry (now blalor/docker-local-registry).\n. ",
    "romanoff": "That worked. Thanks\n. ",
    "fommil": "I'm seeing this sort of thing when using docker hub, damn.\n. ",
    "bfosberry": "+1 for this issue\nroot@0f737cc54253:/docker-registry# gunicorn --access-logfile - --debug --max-requests 100 --graceful-timeout 3600 -t 3600 -k gevent -b 0.0.0.0:5000 -w 4 wsgi:application\nIllegal instruction\nroot@0f737cc54253:/docker-registry# gunicorn\nIllegal instruction\nLinux localhost 3.11.7+ #1 SMP Thu Nov 14 02:46:16 UTC 2013 x86_64 AMD Opteron(tm) Processor 4332 HE AuthenticAMD GNU/Linux\nfrom dmesg:\n[ 6192.514952] traps: gunicorn[3573] trap invalid opcode ip:7f7579b025fc sp:7fff7b71ceb0 error:0 in libm-2.15.so[7f7579ac0000+f9000]\n. I discovered the issue was an incompatibility with the underlying hardware. The solution was to use a newer ubuntu base:\nFROM ubuntu 13.04 should fix it\n. Let me know if you need a test server :P\n. ",
    "charliek": "I'm seeing this same issue on a rackspace VM running Ubuntu 13.04, while the same container works great for me on a Ubuntu 12.04 with the same docker version 0.7.\nI can't build the registry on the 13.04 machine either and get the following message when I try:\n``````\nRUN cd /docker-registry && pip install -r requirements.txt\n---> Running in 24d68568f755\nIllegal instruction\nError build: The command [/bin/sh -c cd /docker-registry && pip install -r requirements.txt] returned a non-zero code: 132```\n``````\n. Should the Dockerfile be updated to pull from ubuntu 13.04 in this repo based on this?\n. I just had a server working with: stackbrew/ubuntu:13.04\nHowever I had to add python-openssl to the apt-get line above the pip install or I ended up getting errors.\n. ",
    "vincentwoo": "I'm still seeing this on docker 0.8.0 today:\ne708a097fc47: Pushing [=======================>                           ]   202 MB/431.9 MB 1m28s\n2014/02/09 04:48:57 push: Failed to upload layer: Put https://registry-1.docker.io/v1/images/e708a097fc47018a3a19061b104ce1e6041d36fe747439cbe800969d96ab75fd/layer: EOF\n. ",
    "vmalloc": "We're using ntp. From what I can tell both machines are synced.\u00a0\n\u2014\nSent from Mailbox for iPhone\nOn Thu, Nov 28, 2013 at 2:06 AM, Roberto Gandolfo Hashioka\nnotifications@github.com wrote:\n\nHi @vmalloc,\nCould please certify that your local time is synced?\nThanks\nReply to this email directly or view it on GitHub:\nhttps://github.com/dotcloud/docker-registry/issues/130#issuecomment-29430577\n. \n",
    "mzsanford": "I noticed this recently and dug into it. My docker pul call to /v1/repositories/mycompany/myname/images was returning an entry with an id which did not exist in the registry (not in the images directory). Once in this state the only way I could find out of it was to manually edit the _index_images to remove the offending entry. After that everything was back to normal. I'm still unsure how the registry was able to get into that state but once it did no amount of rebuilding mycompany/myname:latest seemed to fix it.\n. I'm also seeing ValueError: I/O operation on closed file at /opt/docker-registry/registry/images.py, line 162, in put_image_layer. This is when running on EC2 with S3 storage enabled. I'm not yet running behind Apache/nginx as this is still a prototype deployment. Is this the error uncovered when running without chunked transfer-encoding (mentioned in the README)?\n. Cloned directly from master at the moment\n. Well, that was a quick fix. Thanks. I had just tracked it down. \n. I have run into the same error message while running the stackbrew/registry container directly (docker run -p 5000:5000 stackbrew/registry:latest). The image referenced in the error message did not exist in my registry's local storage (I presume it was a failed push?). The fix once I was in this state was to manually edit the _index_images file in the container to remove the referenced image.\n. For an additional data point: I'm seeing this running the registry image from index.docker.io directly with SETTINGS_FLAVOR set to dev and no other configuration.\n. ",
    "gangleri": "I'm getting the same error. I have started the registry with:\nsudo docker run -p 5000:5000 -v /home/gangleri/docker:/config -e DOCKER_REGISTRY_CONFIG=/config/config.yml -e SETTINGS_FLAVOR=dev -v /home/gangleri/docker/images/:/opt/images registry\nThe config file looks like:\n```\ncommon:\n    loglevel: info\ndev:\n    loglevel: debug\n    storage: local\n    storage_path: /opt/images\ntest:\n    storage: local\n    storage_path: /tmp/tmpdockertmp\n```\nI then pull an image from the index:\nsudo docker pull ubuntu\nAnd tag it as follows\nsudo docker tag 9cd978db300e localhost:5000/myubuntu\nWhen push is being performed I see the following on the server\n```\n172.17.42.1 - - [20/Mar/2014:17:06:24] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-03-20 17:06:24,371 INFO: 172.17.42.1 - - [20/Mar/2014:17:06:24] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-03-20 17:06:24,376 DEBUG: check_session: Session is empty\n172.17.42.1 - - [20/Mar/2014:17:06:24] \"PUT /v1/repositories/myubuntu/ HTTP/1.1\" 200 2 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0-31-generic os/linux arch/amd64\"\n2014-03-20 17:06:24,378 INFO: 172.17.42.1 - - [20/Mar/2014:17:06:24] \"PUT /v1/repositories/myubuntu/ HTTP/1.1\" 200 2 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0-31-generic os/linux arch/amd64\"\n2014-03-20 17:06:24,382 DEBUG: check_session: Session is empty\n172.17.42.1 - - [20/Mar/2014:17:06:24] \"GET /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1\" 200 483 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0-31-generic os/linux arch/amd6\n4\"\n2014-03-20 17:06:24,385 INFO: 172.17.42.1 - - [20/Mar/2014:17:06:24] \"GET /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1\" 200 483 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0\n-31-generic os/linux arch/amd64\"\n2014-03-20 17:06:24,388 DEBUG: check_session: Session is empty\n2014-03-20 17:06:24,389 DEBUG: api_error: Image not found\n172.17.42.1 - - [20/Mar/2014:17:06:24] \"GET /v1/images/6170bb7b0ad1003a827e4dc5253ba49f6719599eac485db51eaafd507c13c311/json HTTP/1.1\" 404 34 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0-31-generic os/linux arch/amd64\n\"\n2014-03-20 17:06:24,391 INFO: 172.17.42.1 - - [20/Mar/2014:17:06:24] \"GET /v1/images/6170bb7b0ad1003a827e4dc5253ba49f6719599eac485db51eaafd507c13c311/json HTTP/1.1\" 404 34 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0-\n31-generic os/linux arch/amd64\"\n2014-03-20 17:06:24,394 DEBUG: check_session: Session is empty\n172.17.42.1 - - [20/Mar/2014:17:06:24] \"PUT /v1/images/6170bb7b0ad1003a827e4dc5253ba49f6719599eac485db51eaafd507c13c311/json HTTP/1.1\" 200 4 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0-31-generic os/linux arch/amd64\"\n2014-03-20 17:06:24,398 INFO: 172.17.42.1 - - [20/Mar/2014:17:06:24] \"PUT /v1/images/6170bb7b0ad1003a827e4dc5253ba49f6719599eac485db51eaafd507c13c311/json HTTP/1.1\" 200 4 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0-3\n1-generic os/linux arch/amd64\"\n2014-03-20 17:06:24,423 DEBUG: check_session: Session is empty\n2014-03-20 17:06:24,424 DEBUG: checksums.compute_tarsum: return tarsum+sha256:3baf0dc0e2713fe1b705feee05d7221a0e4859c901f0f33c393a63adb5c208a1\n172.17.42.1 - - [20/Mar/2014:17:06:24] \"PUT /v1/images/6170bb7b0ad1003a827e4dc5253ba49f6719599eac485db51eaafd507c13c311/layer HTTP/1.1\" 200 4 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0-31-generic os/linux arch/amd64\n\"\n2014-03-20 17:06:24,426 INFO: 172.17.42.1 - - [20/Mar/2014:17:06:24] \"PUT /v1/images/6170bb7b0ad1003a827e4dc5253ba49f6719599eac485db51eaafd507c13c311/layer HTTP/1.1\" 200 4 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0-\n31-generic os/linux arch/amd64\"\n2014-03-20 17:06:24,428 DEBUG: check_session: Session is empty\n2014-03-20 17:06:24,429 DEBUG: api_error: Checksum not found in Cookie\n172.17.42.1 - - [20/Mar/2014:17:06:24] \"PUT /v1/images/6170bb7b0ad1003a827e4dc5253ba49f6719599eac485db51eaafd507c13c311/checksum HTTP/1.1\" 400 47 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0-31-generic os/linux arch/a\nmd64\"\n2014-03-20 17:06:24,431 INFO: 172.17.42.1 - - [20/Mar/2014:17:06:24] \"PUT /v1/images/6170bb7b0ad1003a827e4dc5253ba49f6719599eac485db51eaafd507c13c311/checksum HTTP/1.1\" 400 47 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.\n8.0-31-generic os/linux arch/amd64\"\n```\nOn the client I see \n```\nsudo docker push localhost:5000/myubuntu                                                                                                                         \nThe push refers to a repository [localhost:5000/myubuntu] (len: 1)\nSending image list\nPushing repository localhost:5000/myubuntu (1 tags)\nImage 511136ea3c5a already pushed, skipping\n6170bb7b0ad1: Pushing [==================================================>] 3.072 kB/3.072 kB\n2014/03/20 17:06:24 }\n```\nWhen I try and pull this image I get the following error:\nsudo docker pull localhost:5000/myubuntu                                                                                                                           \nPulling repository localhost:5000/myubuntu\n9cd978db300e: Error pulling image (latest) from localhost:5000/myubuntu, Server error: 404 trying to fetch remote history for 9cd978db300e27386baa9dd791bf6dc818f13e52235b26e95703361ec3c94dc6 \n2014/03/20 17:10:23 Could not find repository on any of the indexed registries.\nAnd in the registry I see\n^[[A172.17.42.1 - - [20/Mar/2014:17:10:23] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-03-20 17:10:23,441 INFO: 172.17.42.1 - - [20/Mar/2014:17:10:23] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-03-20 17:10:23,444 DEBUG: check_session: Session is empty\n172.17.42.1 - - [20/Mar/2014:17:10:23] \"GET /v1/repositories/myubuntu/images HTTP/1.1\" 200 238 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0-31-generic os/linux arch/amd64\"\n2014-03-20 17:10:23,446 INFO: 172.17.42.1 - - [20/Mar/2014:17:10:23] \"GET /v1/repositories/myubuntu/images HTTP/1.1\" 200 238 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0-31-generic os/linux arch/amd64\"\n2014-03-20 17:10:23,449 DEBUG: check_session: Session is empty\n2014-03-20 17:10:23,449 DEBUG: [get_tags] namespace=library; repository=myubuntu\n172.17.42.1 - - [20/Mar/2014:17:10:23] \"GET /v1/repositories/library/myubuntu/tags HTTP/1.1\" 200 2 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0-31-generic os/linux arch/amd64\"\n2014-03-20 17:10:23,451 INFO: 172.17.42.1 - - [20/Mar/2014:17:10:23] \"GET /v1/repositories/library/myubuntu/tags HTTP/1.1\" 200 2 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0-31-generic os/linux arch/amd64\"\n2014-03-20 17:10:23,454 DEBUG: check_session: Session is empty\n2014-03-20 17:10:23,454 DEBUG: api_error: Image not found\n172.17.42.1 - - [20/Mar/2014:17:10:23] \"GET /v1/images/9cd978db300e27386baa9dd791bf6dc818f13e52235b26e95703361ec3c94dc6/ancestry HTTP/1.1\" 404 34 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0-31-generic os/linux arch/amd64\"\n2014-03-20 17:10:23,456 INFO: 172.17.42.1 - - [20/Mar/2014:17:10:23] \"GET /v1/images/9cd978db300e27386baa9dd791bf6dc818f13e52235b26e95703361ec3c94dc6/ancestry HTTP/1.1\" 404 34 \"-\" \"docker/0.9.0 go/go1.2.1 git-commit/2b3fdf2 kernel/3.8.0-31-generic os/linux arch/amd64\"\nWhen I browse to the images directory of the registry I see the following entries\n1c7f181e78b90d347996d754ffa38c4c6b395e7cf0388bffffbda00365b45077\n27cf784147099545\n511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\n58394af373423902a1b97f209a31e3777932d9321ef10e64feaaa7b4df609cf9\n6170bb7b0ad1003a827e4dc5253ba49f6719599eac485db51eaafd507c13c311\n8abc22fbb04266308ff408ca61cb8f6f4244a59308f7efc64e54b08b496c58db\n9f676bd305a43a931a8d98b13e5840ffbebcd908370765373315926024c7c35e\nb750fe79269d2ec9a3c593ef05b4332b1d1a02a62b4accb2c21d589ff2f5f2dc\ndocker version gives the following output\nClient version: 0.9.0\nGo version (client): go1.2.1\nGit commit (client): 2b3fdf2\nServer version: 0.9.0\nGit commit (server): 2b3fdf2\nGo version (server): go1.2.1\nLast stable version: 0.9.0\n. ",
    "kscherer": "I am having a similar problem using the registry container on the Docker index. I was able to work around it by updating the git repo in the container to the latest.\ndocker run  -e DOCKER_REGISTRY_CONFIG=/registry/config.yml -p 5000:5000 -v /opt/registry:/registry -m 0 registry /bin/sh -c 'cd /docker-registry && git remote add origin https://github.com/dotcloud/docker-registry.git && git pull origin master && ./setup-configs.sh && exec ./run.sh'\nThe documentation suggests that the \"preferred\" way of running a private registry is to use the registry container in the docker index, but the fact that:\n- it is out of date\n- does not use nginx\n- does not rotate the logs\nsuggests that the registry container still needs some work.\n. ",
    "mikeatlas": "Hi Sam, I actually followed your blog post for How to Use Your own Registry, my config uses gunicorn and S3 for storage. I was able to even run the registry on heroku successfully, and also do small pushes, but due to the 30s heroku router limit, uploading times out when the push takes 30s+ for larger docker images. Perhaps docker 'push' could be implemented as many smaller requests some day in order to keep the request times lower?\nAnyways, my next step was to try spinning up an Amazon EC2 machine (first time I've done that), so I could set a larger request timeout setting, so I just ran a barebones ubuntu image. #118 doesn't actually refer to me - my problem appears to be more fundamental.\nMy startup command line:\ngunicorn --access-logfile - --log-level debug --debug -b 0.0.0.0:5000 -w 1 -t 6000 wsgi:application\nI opened up port 5000 for 0.0.0.0/0 on the security settings for the EC2 instance as well. At this point I may try a different unbuntu image, as it must be machine configuration related and not docker-registry.\n. I guess the next question is to ask: What protocol(s) and port(s) is docker push using beyond HTTP that could fail due to my registry not being configured correctly? \n. Even better might be a request for dotcloud/docker to create a public AMI instance on AWS for running docker-registry...\nOr a docker image!!! :+1: \n. Update:\nthis syntax works at least for connecting:\ndocker push dockerimage h=http://blah.compute.amazonaws.com:5000\nLooks like the hostname and port syntax parsing isn't working. I'll open a bug on docker for this.\n. nevermind, -h is being ignored... it's hitting https://index.docker.io/v1/ :(\n. ",
    "rthewitt": "Mike, I had this problem as well after following the blog too closely.  There is a -c flag for incorporation of the gunicorn_config.py file.  This file specifies connection timeout and worker information.\n. ",
    "tomfotherby": "I'm stuck with the same error message and the same issue (registry works on local vagrant coreos machine but not on remote EC2 machine).\nInterestingly, if I use the IP of my registry (instead of the DNS alias) it works.\nFail: docker push registry.example.com:5000/myname\nSuccess: \ndocker tag registry.example.com:5000/myname 54.844.153.3:5000/myname\ndocker push 54.844.153.3:5000/myname\nBut this isn't a good workaround for me. I need the domain name to work.  Perhaps it's something to do with the hostname of the registry machine? or EC2 secrurity groups needing extra ports? don't know.\n. I have the same issue with my private registry. It was working for weeks and suddenly it now has a error.\n\n2014/03/14 16:08:23 Error: Invalid Registry endpoint: Get http://<my-url>:5000/v1/_ping: dial tcp: i/o timeout\n\nIf I hit the url it complains about with a browser, I get 200 response, \"true\".\nIt works on machines inside the AWS network but not on my laptop. Is there a way to increase the timeout?\nMy registry isn't behind nginx, it is just a straight docker run -d -p 5000:5000 samalba/docker-registry, no extra config. I pulled a new samalba/docker-registry image but get the same issue. Is there a api endpoint that can tell me the version of the registry that I'm using?\n. ",
    "boothead": "I'm also seeing extremely sluggish performance and and eventual 504 Gateway timeout when I push to my s3 backed docker registry with the LRU cache. The pull is also really slow, even on the same physical machine.\nI have set it up largely per this tutorial http://www.activestate.com/blog/2014/01/deploying-your-own-private-docker-registry and I have everything in an couple of ansible roles if you'd like me to share them.\nThe nginx error log gives me:\n2014/02/05 08:48:36 [error] 888#0: *1682 upstream timed out (110: Connection timed out) while sending request to upstream, client: ******, server: *****,\nrequest: \"PUT /v1/images/16a2d86a19e4b17a3b7ccac95c74a1c3f9032f7c76e7ebb037d2d2885c67d952/layer \nHTTP/1.1\", upstream: \n\"http://127.0.0.1:5000/v1/images/16a2d86a19e4b17a3b7ccac95c74a1c3f9032f7c76e7ebb037d2d2885c67d952/layer\", host: \"******\"\nDocker registry version:\n```\ncommit d853ea84da3a4431dac902959bfc881fd42b47b0\nAuthor: Sam Alba sam.alba@gmail.com\nDate:   Mon Jan 6 17:54:16 2014 -0800\nBumped version\n\n```\nDocker client version:\n$ docker -v\nDocker version 0.7.6, build bc3b2ec\nUpstart script command\nexec gunicorn -k gevent --max-requests 100 --graceful-timeout 3600 -t 3600 -b 0.0.0.0:5000 -w 8 --access-logfile /var/log/docker-registry/access.log --error-logfile /var/log/docker-registry/server.log wsgi:application\nLet me know if there's anything else I can do to help debug.\n. ",
    "goldmann": "I'll create a PR for this.\n. Hah, one letter :) Fixed!\n. Done!\n. Rebased one more time.\n. ",
    "kgoudeaux": "Our use-case is to put versioned application artifacts in containers. We'd like to explicitly version the container as well to guarantee the software version in use.\nFor this use-case we don't really have a need for updatable tags. I had considered pushing further and making tags ending with 'latest' virtual. Given tags foo-1 and foo-2, pulling foo-latest would resolve foo-2.\nReally I'm thinking more along the lines of software versions as opposed to git tags. You can edit a git tag but you can't edit the version of a released artifact e.g. pypi, maven central etc.\n. For sorting I was looking at LooseVersion in python's distutils.version module which gives numbers priority over letters:\n```\n\n\n\nfrom distutils.version import LooseVersion\na = ['blah-1', 'blah-a', 'blah-fish']\na.sort(key=LooseVersion, reverse=True); a\n['blah-fish', 'blah-a', 'blah-1']\n```\n\n\n\nYou could exempt the \"latest\" tag to avoid conflicts with the default push from docker. get_tag could become:\nfrom distutils.version import LooseVersion\ndata = None\nif tag.endswith(\"latest\") and tag != \"latest\":\n    prefix = tag[:-6]\n    tags = []\n    for fname in store.list_directory(store.tag_path(namespace,\n                                                     repository)):\n        tag_name = fname.split('/').pop()\n        if not tag_name.startswith('tag_{0}'.format(prefix)):\n            continue\n        tags.append(tag_name)\n    if tags:\n        tags.sort(key=LooseVersion, reverse=True)\n        data = store.get_content(namespace, repository, tags[0])\nelse:\n    data = store.get_content(store.tag_path(namespace, repository, tag))\nUpdates to latest \"aliases\" would be rejected. Extending this PR, the only tag you could update would be latest, everything else would be immutable or an alias.\n. ",
    "dustinlacewell": "I see a couple problems with this.\nFirstly, tags can already be confusing for some. I think that adding in multiple types of tags will exacerbate this end-user difficulty. Secondly, tags are, generally speaking, a delivery mechanism. I think that this request would be conflating their use possibly contributing to further confusion.\nThirdly, and the most important, there is no reasonable way to sort tags that belong to a repo. Tags belonging to a repo are not required to be related to each other at all. Another way of putting this is that you can have two tags in a repo and neither share an ancestry. The \"latest\" tag is a slight misnomer for \"default\", as far as I understand it. \n. Looks good to me. LGTM.\n. Actually if you could please rebase ontop of master, then we'll get this merged.\n. All of the above concerns have been addressed in the commits above.\n. @kklepper Okay so the confusion I think stems from how the tags and repositories work. I will try to give you a brief overview.\nA repository, whether it be of the form \"namespace/name\" or just \"name\" is a collection of tags.\nA tag is a label that points at a layer. You can have many tags that point to the same layer. The thing which apparently hung you up, is the use of tags as a delivery mechanism. The fully qualified schema for a tag is:\n$REGISTRYSERVER/$NAMESPACE/$REPO:$TAGNAME\nWhen $REGISTRYSERVER is omitted it defaults to: index.docker.io\nWhen $NAMESPACE is omitted it defaults to \"library\" - only docker staff are allowed to push to this namespace.\n$REPO may not be omitted.\nWhen $TAGNAME is omitted, it defaults to: latest\nSo when you type \"docker push 0.0.0.0:5000/ubuntu\" you are really telling docker to push a local tag named \"0.0.0.0:5000/ubuntu\". Like Sam mentioned if there is no local image tagged with this name, you will receive the errors your reported.\nIf you have any further confusion about repositories or tags and how they are delivered, please come to the IRC for some help. :)\n. Yeah the closed file issue should be resolved but I'm not entirely sure it is related to the originally reported issue here. Trying to reproduce now.\n. Okay so I just pulled down stackbew/registry and ran\ndocker run -p 5000:5000 stackbrew/registry\nI then tagged an image:\ndocker tag $IMAGEID 0.0.0.0:5000/testimage\nI was then successfully able to push to the locally running registry.\nSo what I think @k21j10 is experiencing is a bug in the documentation. The README's fast way offers the old \"-p 5000\" flag for mapping the port. However, in modern versions of docker this will actually assign a random host-port to map to the container's 5000 port. To push an image to the repository in this case you actually have to tag the image with the random host-port that is assigned - not 5000.\nIn order to ensure that the registry is running on the host's 5000 port please specify \"-p 5000:5000\". \nClosing this issue for now. Please reopen or seek help in the IRC channel if you continue to have trouble with this.\n. Not sure why the merge-status for this PR is an error.\n. It doesn't. I checked both calls to this function and both users of the function end up closing the FD.\n. LGTM :+1: \n. LGTM +10\n. I am very unsure about this patch because it is involved in the tarsumming of the layer. If we are skipping members due to the inability to decode their filenames does this affect the repeatability, etc of the tarsumming?\n. LGTM!\n. LGTM. Code looks well written. Much needed feature.\n. I see that the Travis CI build is failing. \nhttps://travis-ci.org/dotcloud/docker-registry/builds/18149293\nHere are my local results:\nRan 40 tests in 37.885s\nOK\npy27 runtests: commands[1] | coverage html -d reports --include=./* --omit=test,.tox*\npep8 runtests: commands[0] | flake8 /home/dlacewell/dev/docker/docker-registry\n______________ summary ______________\n  py27: commands succeeded\n  pep8: commands succeeded\n  congratulations :)\nCan anyone confirm that tests are either passing or failing?\n. @shin- confirms that the tests pass for him as well. @samalba what do you think?\n. Doesn't look like this fixed the issues with travis, but this is a good fix anyhow so LGTM.\n. ha\n. ",
    "go1dshtein": "Thanks for review.\nI've added unittest and made exceptions less generic.\nI could not understand how you skip tests for backends if credentials are not available, and made it with unittest.skipIf.\n. I'm sorry I could not answer. I'll do it this weekend.\n. Thank you for your patience. Please review latest commit.\n. I made rebase with top of master. Please check it.\n. Yes, it is registered as  selectel-api==0.1.\n. ",
    "gregwebs": "docker run -p 5000:5000 registry\nThis will spin up the registry, but it uses pre-existing configuration making it useless. It is sad that I can't run the registry with docker. Has there been any progress on this? I don't think everything has to be exposed as an environment variable: another option would be to mount a configuration file. Is that possible now?\n. ahh, I see there is a DOCKER_REGISTRY_CONFIG var that I could use to mount a config file, I will try that...\n. This seems to work. I can try and add some documentation\n. ok, I will lelt you commit it with proper syntax that is faster than talking about it\n. closing for #274\n. ",
    "pwaller": "This feature is important to me, and should be important to you, too!\nI want to run the registry on a puny machine so I don't have to pay much for that machine.\nCurrently, I host my images on S3. If I run the registry on a puny machine and launch several machines who simultaneously pull, I get a thundering herd which kills the puny machine. If the puny machine just did a HTTP 302 redirect to a pre-signed S3 URL, then it wouldn't have to have any data flowing through it except for the HTTP request. S3 can handle pushing the bytes to the server in a distributed manner, as S3 is good at.\n. I can't see any way of pulling this off with the current StorageDriver interface. Is there any space to fit in the required interfaces for a HTTP-redirecting storage driver in docker-registry-ng?\n. I just encountered this from a fresh docker run registry for the first time.\n. By the way, I wanted to report the docker registry version, but I cannot tell which version it is. I'm running image ID 985e98f8266b from the official docker registry which was pulled about 3 days ago.\n. Another thing to mention is that I've reproduced the issue on two separate EC2 instances.\n. I don't think \"the basics\" section matter since the issue is reproducible just with cURL and without docker in the way.\nResult from the _ping endpoint with DEBUG:\n{\n  \"versions\": {\n    \"zlib\": \"1.0\",\n    \"yaml\": \"3.11\",\n    \"werkzeug\": \"0.9.6\",\n    \"urllib2\": \"2.7\",\n    \"urllib\": \"1.17\",\n    \"tarfile\": \"$Revision: 85213 $\",\n    \"simplejson\": \"3.6.2\",\n    \"rsa\": \"3.1.4\",\n    \"requests.utils\": \"2.3.0\",\n    \"requests.packages.urllib3.packages.six\": \"1.2.0\",\n    \"requests.packages.urllib3\": \"dev\",\n    \"requests.packages.chardet\": \"2.2.1\",\n    \"flask\": \"0.10.1\",\n    \"email\": \"4.0.3\",\n    \"docker_registry.server\": \"0.9.0\",\n    \"docker_registry.core\": \"2.0.3\",\n    \"docker_registry.app\": \"0.9.0\",\n    \"distutils\": \"2.7.6\",\n    \"decimal\": \"1.70\",\n    \"ctypes\": \"1.1.0\",\n    \"SocketServer\": \"0.4\",\n    \"argparse\": \"1.1\",\n    \"backports.lzma\": \"0.0.3\",\n    \"blinker\": \"1.3\",\n    \"boto\": \"2.34.0\",\n    \"boto.vendored.six\": \"1.7.2\",\n    \"cPickle\": \"1.71\",\n    \"cgi\": \"2.6\",\n    \"gevent\": \"1.0.1\",\n    \"greenlet\": \"0.4.5\",\n    \"gunicorn\": \"19.1.0\",\n    \"gunicorn.arbiter\": \"19.1.0\",\n    \"gunicorn.config\": \"19.1.0\",\n    \"gunicorn.six\": \"1.2.0\",\n    \"jinja2\": \"2.7.3\",\n    \"json\": \"2.0.9\",\n    \"logging\": \"0.5.1.2\",\n    \"parser\": \"0.5\",\n    \"pickle\": \"$Revision: 72223 $\",\n    \"platform\": \"1.0.7\",\n    \"python\": \"2.7.6 (default, Mar 22 2014, 22:59:56) \\n[GCC 4.8.2]\",\n    \"re\": \"2.2.1\",\n    \"redis\": \"2.10.3\",\n    \"requests\": \"2.3.0\"\n  },\n  \"launch\": [\n    \"/usr/local/bin/gunicorn\",\n    \"--access-logfile\",\n    \"-\",\n    \"--error-logfile\",\n    \"-\",\n    \"--max-requests\",\n    \"100\",\n    \"-k\",\n    \"gevent\",\n    \"--graceful-timeout\",\n    \"3600\",\n    \"-t\",\n    \"3600\",\n    \"-w\",\n    \"4\",\n    \"-b\",\n    \"0.0.0.0:5000\",\n    \"--reload\",\n    \"docker_registry.wsgi:application\"\n  ],\n  \"host\": [\n    \"Linux\",\n    \"61d7f9799171\",\n    \"3.17.2\",\n    \"#2 SMP Thu Dec 11 02:25:47 EST 2014\",\n    \"x86_64\",\n    \"x86_64\"\n  ]\n}\nStarting with:\ndocker run \\\n    --rm \\\n    --env SETTINGS_FLAVOR=s3 \\\n    --env DEBUG=true \\\n    --env AWS_REGION=eu-west-1 \\\n    --env AWS_BUCKET=foo \\\n    --env AWS_SECURE=true \\\n    --env AWS_ENCRYPT=true \\\n    --name docker-registry \\\n    --publish 80:5000 \\\n    registry:latest\n. Authentication is done via IAM instance role, hence why AWS_SECRET etc aren't supplied, it \"just works\" with boto.\n(Side note: I'm concerned this might be broken with the NG registry, is it worth bringing this up in its own issue now, or should I wait?).\n. We haven't done much over the holiday period. Glad to hear IAM authentication is in the rewrite.\nPlease re-ping if you come across this issue in greater than 10 days from now, since I should continue to experience it and be frustrated enough to do the AWS_ENCRYPT experiment if it is still a problem.\n. Tidying up my personal issues list, so closing this. Please create a new issue if you're still interested in tracking it.\n. ",
    "zmmwl": "@samalba Is this available now? I still can't get redirection yet. . ",
    "kittylyst": "Ok, confused - so why does it pull some layers OK and not others? Are you using different compression algorithms for different purposes?\nGiven that you can control both client & server, the docker client could probe its local OS & determine which compression algorithms are available, and then set a suitable Accept-Encoding: header. The server could then be holding versions of the same layer in multiple compression formats and then auto-select which one to send back.\nThis has zero impact on the server side (apart from disk space) and would allow graceful (& transparent) failback on the client side - which is, of course, the type of thing that HTTP was designed to allow as a transport protocol.\n. ",
    "rdrewd": "configure make \"make install\" of xz on Redhat 6.4 is easy enough.   Puzzler to me is why it isn't done automatically when installing the registry.  Are you saying I'm going to have to remember to do that on each client?   Annoying, but not horrible.\n. ",
    "Soulou": "I've just seen that the error is simply canceled, so that's the reason\npython\n    @cache.remove\n    def remove(self, path):\n        path = self._init_path(path)\n        if os.path.isdir(path):\n            shutil.rmtree(path)\n            return\n        try:\n            os.remove(path)\n        except OSError:\n            pass\n. In fact, I mean that there is no server which has all the images. Basically, a server may have the last one but may be able to rollback on a previous one, fetching it from the registry. That is why I'm not able to rename by pushing the tags only.\n. Is there currently some work on that, it would be really useful :+1: \n. There is a nginx front-end here is its configuration file for the registry:\n```\nserver {\n  listen 80;\n  server_name registry.mydomain.tld;\naccess_log /var/log/nginx/registry-access.log;\n  error_log /var/log/nginx/registry-error.log;\nclient_max_body_size 0;\nchunkin on;\n  error_page 411 = @my_411_error;\n  location @my_411_error {\n    chunkin_resume;\n  }\nlocation / {\n    proxy_pass http://localhost:5000;\n    proxy_set_header X-Forwarded-Proto $scheme;\n    proxy_set_header Host $host;\n    proxy_set_header X-Real-IP $remote_addr;\n    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n  }\n}\n```\nI'm using the official registry image with local storage:\nprod:\n    storage: local\n    storage_path: /var/lib/docker-registry\n    email_exceptions:\n       ...\nI'm not touching anything to the gunicorn config, it's the default way to run the registry. (I launch it with run.sh)\n. ",
    "rosenhouse": "I believe the break is due to commit 900d26221805889e617a11aa0f67ab04e5252491 by @samalba\nWith this commit, the Trusted Build works correctly.\n. :+1: This fixes the problem for me.  Thank you!\n. Here you go: https://github.com/MissionSt/docker-registry/commit/aab0d04b745ebaeef946b0f8a26484a3a183991a\n(Not sure how to add it to this Pull Req directly.  Opening a new PR seemed inappropriate.)\n. That should do it.\n. ",
    "xiangdao": "Thanks guys. That's a good place for it to be.\n. Hi @sam_alba. I've updated my PR to include moving boto.cfg into config directory.\nOn 13/12/2013, at 13:08, Sam Alba notifications@github.com wrote:\n\nIt's fine to create a new PR if you don't find how to update this. Just \nclose this one \nOn Thursday, December 12, 2013, Gabe Rosenhouse wrote: \n\nHere you go: MissionSt@aab0d04https://github.com/MissionSt/docker-registry/commit/aab0d04b745ebaeef946b0f8a26484a3a183991a \n(Not sure how to add it to this Pull Req directly. Opening a new PR seemed \ninappropriate.) \n\u2014 \nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/pull/156#issuecomment-30444958 \n. \n\n\n@sam_alba\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "thijsterlouw": "This is a duplicate of https://github.com/dotcloud/docker-registry/pull/146\n. ",
    "thesamet": "Yes, I am using the prod flavor and following \"The fast way\" on\nhttps://github.com/dotcloud/docker-registry so I can just set up\nenvironment variables and run it. When working this way there's no way to\nprovide a different configuration file, so I think this redis stanza should\nbe provided in a way that doesn't break it for existing users.\nOn Fri, Dec 20, 2013 at 7:49 AM, Sam Alba notifications@github.com wrote:\n\nThe cache_lru option is enabled only for the prod flavor, as an example.\nDid you force the SETTINGS_FLAVOR=prod to produce this error? By default\nthe flavor is dev to avoid any error when using the config_sample out of\nthe box.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/163#issuecomment-31018698\n.\n\n\n-Nadav\n. We are hitting this issue several times a day. This is what I just found in the logs:\nERROR:gunicorn.error:Socket error processing request.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 45, in handle\n    self.handle_request(listener, req, client, addr)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 151, in handle_request\n    super(GeventWorker, self).handle_request(*args)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 83, in handle_request\n    listener.getsockname(), self.cfg)\n  File \"\", line 1, in getsockname\nerror: [Errno 9] Bad file descriptor\n. +1: still happening in 0.7.3, docker 0.12.0.\nSolved by downgrading to 0.7.0.\n. Sorry for the delay - this started to happen again after I upgraded by mistake to the latest version... I am pushing to Docker like this:\n$ docker push -v docker.corp.domain.com/image_name:6f3ff205f78e345e80abe6bb6a88605f37113cbd\nThe push refers to a repository [docker.corp.domain.com/image_name] (len: 1)\nSending image list\nand it just quits with status 1 sending anything.\nIn the logs, I found this. I haven't seen any exception logged.\n172.17.42.1 - - [08/Aug/2014:14:02:53] \"PUT /v1/repositories/ferte/ HTTP/1.1\" 405 178 \"-\" \"docker/0.12.0 go/go1.2.1 git-commit/14680bf kernel/3.13.0-27-generic os/linux arch/amd64\" 2014-08-08 14:02:53,466 INFO: 172.17.42.1 - - [08/Aug/2014:14:02:53] \"PUT /v1/repositories/ferte/ HTTP/1.1\" 405 178 \"-\" \"docker/0.12.0 go/go1.2.1 git-commit/14680bf kernel/3.13.0-27-generic os/linux arch/amd64\"\nI run the registry within docker under supervisor with the following command:\ndocker run -p 5000:5000 -v /etc/supervisor/conf.d/config/:/docker-registry/config/:rw -e SETTINGS_FLAVOR=prod -e DOCKER_REGISTRY_CONFIG=/docker-registry/config/config.yml registry\n. ",
    "coderlol": "will look into it...had it working a while back with Apache, but we do want to standardize on nginx...thx\n. Integration would make much sense -- less infrastructure to deploy.  The current Registry model is simply too immature.  For example, no federation, tagging the actual Registry location in a repository name is just nasty.  It's better to have a config file with a set of locations where a repository may be found.\nBut then again, why re-invent the wheel?  Artifactory and Nexus are dead easy to deploy.  There is nothing to \"hate\" about them.  There is no need to adhere to the \"Maven\" model as Artifactory and Nexus can host arbitrary blobs.\n. I was going to fiddle with Apache to map Registry Restful API, but I can't get past the fact that the host name including the PROTOCOL port (arrgh) is hard coded in the repository name/tag -- that's just nastier than nasty...;)\nAnd, it seems like Docker is trying to implement its own authentication/authorization scheme?  Why waste the energy on re-inventing standard stuff?  Use standard stuff so we can easily enjoy HTTP, HTTPS, client cert, basic auth, SSO, etc...\n. Why should a user have to be bothered with where desired repositories/resources are located?  Users just want a resource X.  Does it matter if it is on my computer, on a remote system, on the moon?   What if resources need to be relocated?  Do users or providers have to go about re-tagging everything?\nAnd hard coding of port number on the repository name/url/whatever that is called, well, that's a winner of nasty of nasties ;)  Why do we use DNS instead of IP? \nIn many ways, the practice of using URL/port hard-coding provides too low of an abstraction level -- the model needs to bring it up a notch.\n. Well, how about we put the address / URLs for the docker registries in dockercfg and let docker try each of the registry in turn to pull a given repository?  That way, you decouple of the name of the repository (data id), its location (URL/servers/ports) and access control (login, client certs, etc).  \nI think the design of docker artifact distribution model has simply misunderstood / misapplied the use of \"URL\".\n. Well, perhaps the following can satisfy the \"trust\" requirements and still stay flexible without too much implementation complexity.\nAs a user, I setup a series of trusted registries in, say, dockercfg.  In other words, I trust what the registries contain and what the registries  say they contain; hence, I add the registries to my dockercfg.  Additionally, perhaps, stuff such as MD5/SHA/signed could be added to ensure the integrity of the repository. \nThe above ought to cover the FROM requirements.  Then,\nWhen I do a docker pull ubuntu:12.04,  Docker checks my dockercfg for candidate registries that contain ubuntu:12.04.\nWhen I pull specifically from a given registry I could \"docker pull -registry https://docker.io:1234 ubuntu:12:04\".  now, docker will specifically pull from docker.io:1234.  Perhaps a few additional command line params for credentials could be added.\nWhen it comes to push, docker could push to all registries in dockercfg and/or docker could push just to a registry specified on the command line?\n. Btw, ubuntu:12.04 may be too broad a name, so a global namespace such as ubuntu:12.04 (like what docker does now) is reserved specifically to repositories maintained by docker.io.\nAll other parties will need something like myco.com/ubuntu:12.04 or something similar scheme.  So, a repository name can uniquely identify it on a global basis, but at the same time, the registries hosting a given repository could be \"any\" registry.\n. I have a bash script that rebuild and push a number of images in short order.  I found that adding a \"sleep\" of 2s between pushes seems to help.\n. ",
    "jamtur01": "I'll put my ten cents in here: Maven is powerful but it's a) not for everyone and b) not universal, and c) in some corners it's even hated. I think there's a role for integration with Nexus/Artifactory but I don't see it as a replacement for the registry. YMMV :)\n/cc @shykes \n. What version of Docker are you running? \n. We'd welcome a PR with that change. Thanks! \n. Thanks!\n. LGTM\n. @schlamar Hi - I think you're referring to the Docker Index not the Registry. \nPlease log issues for that here.\nThanks!\n. Can we use a consistent tool with Docker? \n. ",
    "ajf": "Artifactory (don't know about Nexus) already implements RubyGem and Yum endpoints, and I don't think it'd be too difficult for JFrog to implement the docker registry API and then it should work seamlessly with the docker client.\n. ",
    "dkirrane": "Artifactory JIRA: (Pro version now supports this)\nhttps://www.jfrog.com/jira/browse/RTFACT-6494\nNexus JIRA:\nhttps://issues.sonatype.org/browse/NEXUS-8242\n. ",
    "dferrin": "Well,  for the time being, this line\nrepository.rstrip('/').split('/', 1)\nshould be called only if there is a / in the Authorization header. The check could be done with a simple regex test. What do you think?\nYes, you can also update the nginx.conf example. This is my working nginx conf file: \n```\nupstream docker-registry {\n  server $REGISTRY_PORT_5000_TCP_ADDR:$REGISTRY_PORT_5000_TCP_PORT;\n}\nserver {\n  listen 443;\n  server_name registry.example.com;\n  ssl on;\n# disable any limits to avoid HTTP 413 for large image uploads\n  client_max_body_size 0;\nproxy_set_header Host           $host;\n  proxy_set_header X-Real-IP      $remote_addr;\n  proxy_set_header Authorization  \"\";\nlocation / {\n    auth_basic \"Example.com Registry\";\n    auth_basic_user_file /etc/nginx/registry_passwd;\nproxy_pass  http://docker-registry;\n\n}\nlocation /v1/_ping {\n    proxy_pass  http://docker-registry;\n  }\nlocation /v1/users {\n    proxy_pass  http://docker-registry;\n  }\n}\n```\nThose lines are essential.\n```\n  location /v1/_ping {\n    proxy_pass  http://docker-registry;\n  }\nlocation /v1/users {\n    proxy_pass  http://docker-registry;\n  }\n```\nWithout them, it won't work. \nBy the way, in the current contrib example:\nThis block will no longer work with Nginx 1.4.x  since chunking is enabled by default in Nginx.\n# required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\n  chunkin on;\n  error_page 411 = @my_411_error;\n  location @my_411_error {\n    chunkin_resume;\n  }\nRegards,\n. ",
    "miracle2k": "Seems this is implemented now.\n. ",
    "qqshfox": "Same problem here.\n$ docker version\nClient version: 0.7.2\nGo version (client): go1.2\nGit commit (client): 28b162e\nServer version: 0.7.2\nGit commit (server): 28b162e\nGo version (server): go1.2\ndocker-register revision:\nde18ead46303fbf782687f2c6020d2b220b2db9d\n. Managed to push the image running registry with the run.sh script.\n. ",
    "kklepper": "This hint was fine, although I apparently still don't understand. What is the syntax to be used?\nThanks a lot, both of you. \nAfter running run.sh, I see in XP/Firefox http://192.168.0.42:5000/ \"docker-registry server (dev)\" as before. \nIn another ssh-shell-window, I try to do a push as best I can:\n\nvagrant@precise64:/$ sudo docker push localhost:5000/ubuntu \nThe push refers to a repository localhost:5000/ubuntu\n2014/01/02 20:13:32 No such id: localhost:5000/ubuntu\nvagrant@precise64:/$ sudo docker push 0.0.0.0:5000/ubuntu \nThe push refers to a repository 0.0.0.0:5000/ubuntu\n2014/01/02 20:13:57 No such id: 0.0.0.0:5000/ubuntu\nvagrant@precise64:/$ sudo docker push 192.168.0.42:5000/ubuntu \nThe push refers to a repository 192.168.0.42:5000/ubuntu\n2014/01/02 20:14:24 No such id: 192.168.0.42:5000/ubuntu\n\nI know: Once you do it right, it works - but what is the right syntax here?\nThe server says:\n\n127.0.0.1 - - [02/Jan/2014:20:09:21] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-01-02 20:09:21,724 INFO: 127.0.0.1 - - [02/Jan/2014:20:09:21] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n127.0.0.1 - - [02/Jan/2014:20:09:23] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-01-02 20:09:23,774 INFO: 127.0.0.1 - - [02/Jan/2014:20:09:23] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n192.168.0.1 - - [02/Jan/2014:20:09:42] \"GET / HTTP/1.1\" 200 30 \"-\" \"Mozilla/5.0 (Windows NT 5.1; rv:26.0) Gecko/20100101 Firefox/26.0\"\n2014-01-02 20:09:42,492 INFO: 192.168.0.1 - - [02/Jan/2014:20:09:42] \"GET / HTTP/1.1\" 200 30 \"-\" \"Mozilla/5.0 (Windows NT 5.1; rv:26.0) Gecko/20100101 Firefox/26.0\"\n\nThis is obviously from XP/Firfox - now the push with variations:\n\n127.0.0.1 - - [02/Jan/2014:20:10:15] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-01-02 20:10:15,681 INFO: 127.0.0.1 - - [02/Jan/2014:20:10:15] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n127.0.0.1 - - [02/Jan/2014:20:10:17] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-01-02 20:10:17,701 INFO: 127.0.0.1 - - [02/Jan/2014:20:10:17] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n127.0.0.1 - - [02/Jan/2014:20:12:50] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-01-02 20:12:50,902 INFO: 127.0.0.1 - - [02/Jan/2014:20:12:50] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n127.0.0.1 - - [02/Jan/2014:20:12:52] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-01-02 20:12:52,917 INFO: 127.0.0.1 - - [02/Jan/2014:20:12:52] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n127.0.0.1 - - [02/Jan/2014:20:13:30] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-01-02 20:13:30,789 INFO: 127.0.0.1 - - [02/Jan/2014:20:13:30] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n127.0.0.1 - - [02/Jan/2014:20:13:32] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-01-02 20:13:32,797 INFO: 127.0.0.1 - - [02/Jan/2014:20:13:32] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n127.0.0.1 - - [02/Jan/2014:20:13:55] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-01-02 20:13:55,344 INFO: 127.0.0.1 - - [02/Jan/2014:20:13:55] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n127.0.0.1 - - [02/Jan/2014:20:13:57] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-01-02 20:13:57,688 INFO: 127.0.0.1 - - [02/Jan/2014:20:13:57] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n192.168.0.42 - - [02/Jan/2014:20:14:22] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-01-02 20:14:22,911 INFO: 192.168.0.42 - - [02/Jan/2014:20:14:22] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n192.168.0.42 - - [02/Jan/2014:20:14:24] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-01-02 20:14:24,947 INFO: 192.168.0.42 - - [02/Jan/2014:20:14:24] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n. Wow!\nvagrant@precise64:/$ docker tag ubuntu localhost:5000/ubuntu\nvagrant@precise64:/$ docker push localhost:5000/ubuntu\nThe push refers to a repository localhost:5000/ubuntu\nSending image list\nPushing repository localhost:5000/ubuntu (1 tags)\n8dbd9e392a96: Pushing [=================================================> ] 133.8 MB/134.1 MB 0\nPushing tags for rev [8dbd9e392a96] on {http://localhost:5000/v1/repositories/ubuntu/tags/latest}\n\nBut (what I tried before):\n\nvagrant@precise64:/$ docker push localhost:5000/kklepper/apache2\nThe push refers to a repository localhost:5000/kklepper/apache2\n2014/01/03 00:03:45 No such id: localhost:5000/kklepper/apache2\n\nWell, I tried something like \n\ndocker tag apache2 localhost:5000/kklepper/apache2\n\nand some more to finally find\n\ndocker tag kklepper/apache2 localhost:5000/kklepper/apache2\n\nworks...\nSo I have to tag the images first, correct? (Edited later: I think I begin to understand... With this command, I replicated the image in my image list with another repository name - correct? See below my images list... And with this name I could push it to the right repository...)\n\ndocker tag kklepper/apache2 localhost:5000/kklepper/apache2\ndocker push localhost:5000/kklepper/apache2\n\nworks like a charm. \n\nThe push refers to a repository localhost:5000/kklepper/apache2\nSending image list\nPushing repository localhost:5000/kklepper/apache2 (1 tags)\n8dbd9e392a96: Image already pushed, skipping \n539b5a01e5cd: Pushing [=================================================> ] 72.72 MB/72.83 MB 0\nPushing tags for rev [539b5a01e5cd] on {http://localhost:5000/v1/repositories/kklepper/apache2/tags/latest}\n\nThanks a lot! \nAnother question: I expected the server to show me the results:\n\nhttp://192.168.0.42:5000/\n\nBut still I get an empty list.\n\n\"docker-registry server (dev)\"\n\nI try something else:\n\nvagrant@precise64:/$ docker search localhost:5000/kklepper/apache2\nNAME      DESCRIPTION   STARS     OFFICIAL   TRUSTED\n\nHow can I see what is in my local repository? \nI see that the first push appears in my local image list (shortened):\n\nvagrant@precise64:/$ docker images\nREPOSITORY               TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nkklepper/apache2         latest              539b5a01e5cd        4 days ago          198.9 MB\nubuntu                   12.04               8dbd9e392a96        8 months ago        128 MB\nubuntu                   latest              8dbd9e392a96        8 months ago        128 MB\nubuntu                   precise             8dbd9e392a96        8 months ago        128 MB\nlocalhost:5000/ubuntu    latest              8dbd9e392a96        8 months ago        128 MB\nubuntu                   12.10               b750fe79269d        9 months ago        175.3 MB\nubuntu                   quantal             b750fe79269d        9 months ago        175.3 MB\n\nPulling works as expected:\n\nvagrant@precise64:/$ docker pull localhost:5000/kklepper/apache2\nPulling repository localhost:5000/kklepper/apache2\n539b5a01e5cd: Download complete \n8dbd9e392a96: Download complete \n\nThe first ID is from my image, identical to the push:\n\nvagrant@precise64:/$ docker images\nREPOSITORY                        TAG                 IMAGE ID            CREATED             VIRTUAL SIZE\nlocalhost:5000/kklepper/apache2   latest              539b5a01e5cd        4 days ago          198.9 MB\nkklepper/apache2                  latest              539b5a01e5cd        4 days ago          198.9 MB\nubuntu                            12.04               8dbd9e392a96        8 months ago        128 MB\nubuntu                            latest              8dbd9e392a96        8 months ago        128 MB\nubuntu                            precise             8dbd9e392a96        8 months ago        128 MB\nlocalhost:5000/ubuntu             latest              8dbd9e392a96        8 months ago        128 MB\nubuntu                            12.10               b750fe79269d        9 months ago        175.3 MB\nubuntu                            quantal             b750fe79269d        9 months ago        175.3 MB\n\nWhat does the second message tell? The ID is the one from ubuntu... I can't remember if I created apache2 from ubuntu or from \n\nbalaji/apache2                    latest              f98325a010b1        3 months ago        217.3 MB\n\nDoes the ID tell me? \nAlso: Should the size of my image be bigger than the base? If so, it cannot be derived from balaji/apache2.\n(Edited later: If true what I speculated above: The tagging respetively image list doesn't tell me which images are pushed already - correct?)\n. ",
    "mastef": "Error: Invalid Registry endpoint: Get http://0.0.0.0:5000/v1/_ping: read tcp 127.0.0.1:5000: i/o timeout\n2013-12-31 12:57:29,807 DEBUG: Ignored premature client disconnection. No more data after: '\\x16\\x03\\x01\\x00\\x80\\x01\\x00\\x00|\\x03\\x03R\\xc2\\xbf/A\\x99\\xda\\xc7r$\\xa75\\xd9\\x1e\\xc5)\\x95aQ\\x94\\x17\\xf6\\n\\xf5}\\xc8v\\xec\\xb2\\x86\\xb6\\xe9\\x00\\x00\\x1a\\xc0/\\xc0+\\xc0\\x11\\xc0\\x07\\xc0\\x13\\xc0\\t\\xc0\\x14\\xc0\\n\\x00\\x05\\x00/\\x005\\xc0\\x12\\x00\\n\\x01\\x00\\x009\\x00\\x00\\x00\\x0c\\x00\\n\\x00\\x00\\x070.0.0.0\\x00\\x05\\x00\\x05\\x01\\x00\\x00\\x00\\x00\\x00\\n\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x18\\x00\\x19\\x00\\x0b\\x00\\x02\\x01\\x00\\x00\\r\\x00\\n\\x00\\x08\\x04\\x01\\x04\\x03\\x02\\x01\\x02\\x03'\n2013-12-31 12:57:29 [15414] [DEBUG] GET /v1/ping\n2013-12-31 12:57:29,808 DEBUG: GET /v1/ping\nI had similar issues with it, as Docker was first attempting to connect through https to gunicorn, and gunicorn was keeping the connection open until the timeout, which was very high. Basically the connection timed out for Docker on the https, and Docker gave up. Gunicorn then noticed that the Docker client closed the connection 'prematurely' and gave up with an error of its own.\nReducing the -t timeout value at gunicorn start immediately resolved that issue, as the initial https connection simply failed within a much shorter time amount and Docker continued with the http protocol requests. ( Subsequently uploading images now timed out if the timeout value was reached )\nNewbie resolution : Finally resolved it by dropping the gunicorn command line version, and simply ran with the properly configured standalone container docker run -p 5000:5000 -d -v /tmp/registry:/tmp/registry registry\n. ",
    "moonchaser": "@samalba: Thank you! I have been struggling with this for a while. All I had to do it tag it right and was able to push it to the registry.\n. ",
    "leior": "said by k21j10:when I run with docker run -p 5000 stackbrew/registry.The host port is mapped to 14533.Then  I can see url 127.0.0.1:14533 shows \"docker-registry server (dev)\" with curl.I think it means the websever starts up. And I am sure only one registry container is running.So back to the question,do I need to do something additional with the registry container or the host?\n. ",
    "smurthas": "I am getting a very similar error on pushes. Here's my configuration:\nCoreOS (176.0.0)\nDocker 0.7.2\ndocker run -d -p 5000 -name docker-registry samalba/docker-registry\nI realize that samalba/docker-registry is not the official image, so I'm in the process of re-running with the official version, but I hope this stack trace helps:\n172.17.42.1 - - [08/Jan/2014:22:15:24] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-01-08 22:15:24,500 INFO: 172.17.42.1 - - [08/Jan/2014:22:15:24] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [08/Jan/2014:22:15:26] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-01-08 22:15:26,511 INFO: 172.17.42.1 - - [08/Jan/2014:22:15:26] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-01-08 22:15:26,515 DEBUG: check_session: Session is empty\n2014-01-08 22:15:26,520 DEBUG: check_session: Session is empty\n2014-01-08 22:15:26,520 DEBUG: api_error: Image is being uploaded, retry later\n2014-01-08 22:15:26,529 DEBUG: check_session: Session is empty\n172.17.42.1 - - [08/Jan/2014:22:15:26] \"GET /v1/images/27cf784147099545/json HTTP/1.1\" 400 55 \"-\" \"docker/0.7.2 go/go1.2 git-commit/28b162e kernel/3.11.7\"\n172.17.42.1 - - [08/Jan/2014:22:15:26] \"PUT /v1/repositories/nacs-app-hello.git/ HTTP/1.1\" 200 2 \"-\" \"docker/0.7.2 go/go1.2 git-commit/28b162e kernel/3.11.7\"\n2014-01-08 22:15:26,530 INFO: 172.17.42.1 - - [08/Jan/2014:22:15:26] \"GET /v1/images/27cf784147099545/json HTTP/1.1\" 400 55 \"-\" \"docker/0.7.2 go/go1.2 git-commit/28b162e kernel/3.11.7\"\n2014-01-08 22:15:26,532 INFO: 172.17.42.1 - - [08/Jan/2014:22:15:26] \"PUT /v1/repositories/nacs-app-hello.git/ HTTP/1.1\" 200 2 \"-\" \"docker/0.7.2 go/go1.2 git-commit/28b162e kernel/3.11.7\"\n172.17.42.1 - - [08/Jan/2014:22:15:26] \"PUT /v1/images/27cf784147099545/json HTTP/1.1\" 200 4 \"-\" \"docker/0.7.2 go/go1.2 git-commit/28b162e kernel/3.11.7\"\n2014-01-08 22:15:26,542 INFO: 172.17.42.1 - - [08/Jan/2014:22:15:26] \"PUT /v1/images/27cf784147099545/json HTTP/1.1\" 200 4 \"-\" \"docker/0.7.2 go/go1.2 git-commit/28b162e kernel/3.11.7\"\n2014-01-08 22:15:27,366 DEBUG: check_session: Session is empty\n2014-01-08 22:15:53,208 ERROR: Exception on /v1/images/27cf784147099545/layer [PUT]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1687, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1360, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1358, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1344, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/docker-registry/registry/toolkit.py\", line 202, in wrapper\n    return f(*args, **kwargs)\n  File \"/docker-registry/registry/images.py\", line 162, in put_image_layer\n    tmp.seek(0)\nValueError: I/O operation on closed file\n2014-01-08 22:15:53,208 ERROR: Exception on /v1/images/27cf784147099545/layer [PUT]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1687, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1360, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1358, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1344, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/docker-registry/registry/toolkit.py\", line 202, in wrapper\n    return f(*args, **kwargs)\n  File \"/docker-registry/registry/images.py\", line 162, in put_image_layer\n    tmp.seek(0)\nValueError: I/O operation on closed file\n172.17.42.1 - - [08/Jan/2014:22:15:53] \"PUT /v1/images/27cf784147099545/layer HTTP/1.1\" 500 291 \"-\" \"docker/0.7.2 go/go1.2 git-commit/28b162e kernel/3.11.7\"\n2014-01-08 22:15:53,218 INFO: 172.17.42.1 - - [08/Jan/2014:22:15:53] \"PUT /v1/images/27cf784147099545/layer HTTP/1.1\" 500 291 \"-\" \"docker/0.7.2 go/go1.2 git-commit/28b162e kernel/3.11.7\"\n. ",
    "mminke": "@jamtur01 thx for the quick reply.\nThis is happening on the https://index.docker.io website. So completely independent from my own docker installation.\nI do not know which version of docker is used on index.docker.io!\n. This issue is now fixed on the index.docker.io website. Not sure what has been done, but I guess someone noticed things went wrong and fixed it.\n. ",
    "crosbymichael": "This error message comes from docker not the registry archive/tar: invalid tar header\nWhy do you think this is not a duplicate of #3423 ?\n. ",
    "chilicat": "Just a minor thing, but everything which helps to find the user mistake faster.... :)\nI would prefer if the application can fail as early as possible, during startup, if possible.\nthanks \n. @ewindisch @samalba I build a own docker-registr container from latest source. It is working. Can you update the version provided at http://get.docker.io/images/openstack/docker-ut.tar.gz (as far as I know used by devstack). \n. @samalba Can you please merge this, it is a little but annoying bug. \n. Nope, unfortunately python-glanceclient 0.12 doesn't fix the problem. \n. From: Nicolas Fillot \nThread:  https://groups.google.com/forum/#!searchin/docker-user/glance/docker-user/0ZQqc38Es50/_t9eZLVH6iQJ\n```\nThe 500 error on the compute node is generated by the docker-registry node.\nThe docker-registry node tries to update meta properties of the image on the glance node but glance fails with a HTTP/1.0 400 Header Line Too Long.\nThis is caused by the \"x-image-meta-property-meta__files\" header containing every single file of the image.\nThe x-image-meta-property-meta__files header is generated in the put_image_layer function of docker-registry/registry/images.py\n186     csums = []\n187     tar = None\n188     tarsum = checksums.TarSum(json_data)\n189     try:\n190         tmp.seek(0)\n191         tar = tarfile.open(mode='r|*', fileobj=tmp)\n192         tarfilesinfo = layers.TarFilesInfo()\n193         for member in tar:\n194             tarsum.append(member, tar)\n195             tarfilesinfo.append(member)\n196         layers.set_image_files_cache(image_id, tarfilesinfo.json())\n```\n. Glance with shift behind.\n. @samalba I use swift (don't know why I always write shift :) ). Must be a bug in the swift/glance kombination.\n. I use the configuration: openstack-swift. I wrote this config :) \n. ",
    "ewindisch": "The nova driver no longer requires a registry proxy. It now speaks directly\nto glance.\nOn Aug 15, 2014 7:41 PM, \"Olivier Gambier\" notifications@github.com wrote:\n\n@chilicat https://github.com/chilicat @samalba\nhttps://github.com/samalba is this still an issue? Thanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/195#issuecomment-52373891\n.\n. Thank you @dmp42 \n\n+1; while I'm sure someone might still find this code useful, it's original and primary purpose has been deprecated.\n. This patch is an incomplete work-in-progress.\n. Linking bugs #256 and #254\n. LGTM! Thank you @srid \n. If the tarsum calculation in the registry is only performed for Docker <0.10, then supporting xattrs here will NOT be consistent with the engine as <0.10 won't support xattrs.\n. ping @dmp42 \n. ",
    "SamSaffron": "+1 also noticing some weirdness with tags, thinking of giving up on /latest and always using specific revs to avoid the chance of this happening\n. just re-confirming this, if for any reason your primary dns goes down and you start running on secondary, expect this failure. \nI do think its a client bug, however easily fixable \n. ",
    "scivm": "It can be done like this:\nExists:\nroot@Ubuntu-1204-precise-64-minimal ~ # :;  http GET https://xxx.xxx.com/v1/repositories/gromacs-4.6.5/tags\nHTTP/1.1 200 OK\nCache-Control: no-cache\nConnection: keep-alive\nContent-Length: 84\nContent-Type: application/json\nDate: Thu, 16 Jan 2014 11:50:08 GMT\nExpires: -1\nPragma: no-cache\nServer: nginx/1.4.4\nX-Docker-Registry-Config: dev\nX-Docker-Registry-Version: 0.6.2\n{\n    \"latest\": \"1ab8a4558382973a6a7982eca3063b6636bec711dc497f8186818829e95c2462\"\n}\nDoes not exist:\nroot@Ubuntu-1204-precise-64-minimal ~ # :;  http GET https://xxx.xxx.com/v1/repositories/gromacs-4.6.5-notexisting/tags\nHTTP/1.1 404 NOT FOUND\nCache-Control: no-cache\nConnection: keep-alive\nContent-Length: 39\nContent-Type: application/json\nDate: Thu, 16 Jan 2014 11:50:27 GMT\nExpires: -1\nPragma: no-cache\nServer: nginx/1.4.4\nX-Docker-Registry-Config: dev\nX-Docker-Registry-Version: 0.6.2\n{\n    \"error\": \"Repository not found\"\n}\n. ",
    "elharo": "This would be a violation of HTTP. Per the HTTP 1.1 spec:\n\"The HEAD method is identical to GET except that the server MUST NOT\n   send a message body in the response (i.e., the response terminates at\n   the end of the header section). \"\n. ",
    "hlascelles": "After some more investigation, it seems this code may have made it into some versions of registry and not others ( or the correct version of boto). The global image \"registry\" fails, but \"samalba/docker-registry\" succeeds.\nI'll close this pull request.\n. ",
    "ktheory": "Looks like this has regressed. If I launch the registry (either the 'registry' global image, or 'samba/docker-registry') without S3 access & secret keys, it does not fall through to the IAM role instance profile.\n. The 'latest' of both. And I tried registry:0.7.3. Here's the stack trace:\n2014-07-16 15:04:58,456 WARNING: No S3 region specified, using boto default region, this may affect performance and stability.\n** [Bugsnag] No API key configured, couldn't notify\nTraceback (most recent call last):\n  File \"/usr/local/bin/docker-registry\", line 9, in <module>\n    load_entry_point('docker-registry==0.7.3', 'console_scripts', 'docker-registry')()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 351, in load_entry_point\n    return get_distribution(dist).load_entry_point(group, name)\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2363, in load_entry_point\n    return ep.load()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2088, in load\n    entry = __import__(self.module_name, globals(),globals(), ['__name__'])\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/run.py\", line 16, in <module>\n    from .tags import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/tags.py\", line 21, in <module>\n    store = storage.load()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/storage/__init__.py\", line 38, in load\n    config=cfg)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/drivers/s3.py\", line 65, in __init__\n    super(Storage, self).__init__(path, config)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/boto.py\", line 135, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 471, in get_bucket\n    return self.head_bucket(bucket_name, headers=headers)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 515, in head_bucket\n    raise err\nboto.exception.S3ResponseError: S3ResponseError: 404 Not Found\nMy config.yaml is:\nprod:\n    loglevel: warn\n    storage: s3\n    s3_bucket: redacted\n    boto_bucket: redacted\n    storage_path: /srv/docker\n. Ah, looks like I'm running in to issue #400. Will add comments there.\n. Update July 19, 2014\nI was able to fix the hung containers by granting IAM privileges on the bucket itself instead of just the keys in the bucket. I.e., allow access to arn:aws:s3:::my-bucket and not just arn:aws:s3:::my-bucket/*\nHere's my IAM policy:\n{\n  \"Statement\": [\n    {\n      \"Resource\": \"arn:aws:s3:::my-bucket/*\",\n      \"Action\": \"s3:*\",\n      \"Effect\": \"Allow\"\n    },\n    {\n      \"Resource\": \"arn:aws:s3:::my-bucket\",\n      \"Action\": \"s3:*\",\n      \"Effect\": \"Allow\"\n    }\n  ]\n}\nTada. :tada:\noriginal comment\nI've been struggling to get docker-registry (0.7.3) running with S3 for the past few days, and here's what I've found:\n1. If you specify an AWS_REGION env var, or s3_region in the YAML config, the container prints some output, then hangs.\nIf you try to curl the running container, you get 'connection reset by peer'.\nHere's an example output:\n```\ndocker run -e SETTINGS_FLAVOR=s3  -e AWS_BUCKET=bucket-in-us-west-2 -e AWS_KEY=redacted -e AWS_SECRET='redacted' -e AWS_REGION=us-west-2  -p 5000:5000 registry:0.7.3\n2014-07-18 01:08:10,922 DEBUG: Will return docker-registry.drivers.s3.Storage\n2014-07-18 01:08:10,924 DEBUG: Using access key provided by client.\n2014-07-18 01:08:10,924 DEBUG: Using secret key provided by client.\n2014-07-18 01:08:10,924 DEBUG: path=/\n2014-07-18 01:08:10,924 DEBUG: auth_path=/bucket-in-us-west-2/\n2014-07-18 01:08:10,925 DEBUG: Method: HEAD\n2014-07-18 01:08:10,925 DEBUG: Path: /\n2014-07-18 01:08:10,925 DEBUG: Data:\n2014-07-18 01:08:10,925 DEBUG: Headers: {}\n2014-07-18 01:08:10,925 DEBUG: Host: bucket-in-us-west-2.s3-us-west-2.amazonaws.com\n2014-07-18 01:08:10,926 DEBUG: Port: 80\n2014-07-18 01:08:10,926 DEBUG: Params: {}\n2014-07-18 01:08:10,926 DEBUG: establishing HTTP connection: kwargs={'port': 80, 'timeout': 70}\n2014-07-18 01:08:10,926 DEBUG: Token: None\n2014-07-18 01:08:10,927 DEBUG: StringToSign:\nHEAD\nFri, 18 Jul 2014 01:08:10 GMT\n/bucket-in-us-west-2/\n2014-07-18 01:08:10,927 DEBUG: Signature:\nAWS redacted:redacted=\n```\n2. If your bucket is in the US-Standard region (the default for AWS buckets), and you don't specify AWS_REGION to your docker container, the container crashes:\nroot@ip-10-0-2-82:~# docker run -e SETTINGS_FLAVOR=s3  -e AWS_BUCKET=bucker-in-us-standard -e AWS_KEY=redacted -e AWS_SECRET='redacted'  -p 5000:5000 registry:0.7.3\nFri, 18 Jul 2014 01:09:34 GMT\n/bucker-in-us-standard/\n2014-07-18 01:09:34,826 DEBUG: Signature:\nAWS redacted:redacted=\nTraceback (most recent call last):\n  File \"/usr/local/bin/docker-registry\", line 9, in <module>\n    load_entry_point('docker-registry==0.7.3', 'console_scripts', 'docker-registry')()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 351, in load_entry_point\n    return get_distribution(dist).load_entry_point(group, name)\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2363, in load_entry_point\n** [Bugsnag] No API key configured, couldn't notify\n    return ep.load()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2088, in load\n    entry = __import__(self.module_name, globals(),globals(), ['__name__'])\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/run.py\", line 16, in <module>\n    from .tags import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/tags.py\", line 21, in <module>\n    store = storage.load()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/storage/__init__.py\", line 38, in load\n    config=cfg)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/drivers/s3.py\", line 65, in __init__\n    super(Storage, self).__init__(path, config)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/boto.py\", line 135, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 471, in get_bucket\n    return self.head_bucket(bucket_name, headers=headers)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 515, in head_bucket\n    raise err\nboto.exception.S3ResponseError: S3ResponseError: 404 Not Found\n3. What does work is making a bucket in us-west-2, and not passing AWS_REGION to the container:\ndocker run -e SETTINGS_FLAVOR=s3  -e AWS_BUCKET=bucket-in-us-west-2 -e AWS_KEY=redacted -e AWS_SECRET='redacted'  -p 5000:5000 registry:0.7.3\nEureka!\nSo the bug seems to be that setting AWS_REGION breaks things, and the US-Standard S3 region doesn't work at all.\n. ",
    "rockyfaninus": "Thanks for your information.\nI have my issue resolved. \n. ",
    "Yotta2": "@rockyfaninus \nhow do you resolve this problem? I am facing the same issue right now.\nthanks\n. ",
    "jufan": "The URL I used is incorrect when I try to push image. To push the image, you should use full name like: http://:5000/ubuntu/ping, when the 'ubuntu/ping' is image alias. The registry server full name use full DNS name and make sure it can be DNS resolved and be reached. (when I run in VM, it does not resolve DNS). By the way, I run dock client from another machine to push image to registry. \n. ",
    "saurabhtandon13": "whenever there is an error\n FATA[0000] Error response from daemon: v1 ping attempt failed with error: Get https://registry.access.redhat.com/v1/_ping: dial tcp: lookup registry.access.redhat.com: no such host. If this private registry supports only HTTP or HTTPS with an unknown CA certificate, please add --insecure-registry registry.access.redhat.com to the daemon's arguments. In the case of HTTPS, if you have access to the registry's CA certificate, no need for the flag; simply place the CA certificate at /etc/docker/certs.d/registry.access.redhat.com/ca.crt\nsolution to this is creation of account using the docker \ndocker login registry.access.redhat.com ( complete the details like username, password , email address) after sometime account will be created and message will be get display.\nlogin to the server using the account created \n # docker login < user_name>\nand run the docker pull command to download the registry from redhat.  \n. ",
    "SadiqueManzar": "Didnot worked. Adding --insecure-registry. ",
    "groundwater": "I just pushed it, and it's working again.\nThanks!\n. ",
    "romanoaugusto88": "+1\n. ",
    "vegansk": "+100500\n. ",
    "voidzero": "alright\nI'll throw in an extra +500\n. Yes, sorry for the late reply @dmp42.\nRegarding this feature request: I think the title, not the description, was what got my attention.\nI'm running a private repo (just for testing and for my own purposes, not for general production) and I did find it difficult to rename (and delete) repositories in my private registry. I did manage, but with manual edits to the sqlite db and so forth.\nIn his comments, to me it looks like @Soulou is requesting a client feature, not a registry feature. I'm not sure though. But, it's not really what I had in mind, I was searching for a way to manage more attributes of the private registry.\nSo in conclusion - I have to withdraw my support :smile_cat: \n. ",
    "dmp1ce": "CTRL-C works fine if I run gunicorn in a bash session, inside of the container.  So, I don't think this is an issue with gunicorn but the way in which Docker handles CTRL-C.\nThis might be the same issue seen here: https://github.com/dotcloud/docker/issues/2838\n. ",
    "lhazlewood": "I think this would be resolved by #2855.\n. ",
    "mschulkind": "This is the easiest way to fix right now:\nhttps://github.com/fog-engine/docker-registry/commit/2c38e5be45e007bef2fe0b29584e9fb38bac552b\nProblem is detailed here:\nhttps://github.com/dotcloud/docker/pull/3240\nIf you just want ctrl-c to work though, you can just run with -i -t and then when you press ctrl-c, it'll get sent to gunicorn, instead of docker, and everything works.\n. ",
    "lcarstensen": "+1, I hit this as well recently.  Anyone have a patch worked up on this yet?\n. Same here, anyone have a work in progress on that?\n. ",
    "mdedetrich": "Also getting this issue\n. ",
    "bjaglin": "@shepmaster thanks for the script! Did you have any luck actually removing images deemed unused by your script? I just ran it and I am planning to remove the orphan images, but I am guessing _index_images also needs to reflect the deletions?\n. For the record, to reclaim space I am currently experimenting with https://gist.github.com/bjaglin/1ff66c20c4bc4d9de522 which:\n1. uses a @shepmaster's script to identify orphan images\n2. remove references to these images in the _index_images repo indices\n3. remove the actual images on disk\nDISCLAIMER: Use it at your own risk though, as it might break some invariants otherwise enforced by the registry, and is heavily dependent on the implementation details of the registry (version 0.7.0 at the time of writing).\n. Duplicate of https://github.com/dotcloud/docker-registry/issues/320?\n. Same issue with docker 0.11.1 against registry 0.6.9 through nginx, seeing this randomly, about once a day from our CI pushing to our private registry.\n2014-05-19 14:21:52 [1194] [ERROR] Socket error processing request.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 45, in handle\n    self.handle_request(listener, req, client, addr)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 151, in handle_request\n    super(GeventWorker, self).handle_request(*args)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 83, in handle_request\n    listener.getsockname(), self.cfg)\n  File \"<string>\", line 1, in getsockname\nerror: [Errno 9] Bad file descriptor\n2014-05-19 14:21:52,311 ERROR: Socket error processing request.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 45, in handle\n    self.handle_request(listener, req, client, addr)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 151, in handle_request\n    super(GeventWorker, self).handle_request(*args)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 83, in handle_request\n    listener.getsockname(), self.cfg)\n  File \"<string>\", line 1, in getsockname\nerror: [Errno 9] Bad file descriptor\n. :thumbsup: \n. Sorry for asking it again, but could you publish 0.6.9 on the official registry?\n. thanks! I didn't know the registry tags was produced from stackbrew, otherwise I could have bumped it myself...\n. +1\n. perfect, thanks\n. There is probably a nicer way to do that with nginx, but I have been succesfully using this for HTTPS termination and PUT/POST/DELETE basic auth protection:\n```\nserver {\n  listen 80;\n  server_name myregisty.net;\nlocation / {\n    rewrite ^(.*)$ https://myregisty.net$1 last;\n  }\n}\nserver {\n  listen 443;\n  server_name myregisty.net;\nerror_page 588 = @readrequests;\nssl                       on;\n  ssl_certificate           /etc/nginx/ssl/wildcard.myregisty.net.pem;\n  ssl_certificate_key       /etc/nginx/ssl/wildcard.myregisty.net.pem;\n  ssl_dhparam               /etc/nginx/ssl/wildcard.myregisty.net.pem;\nif ($request_method = 'GET') {\n    return 588;\n  }\nif ($request_method = 'HEAD') {\n    return 588;\n  }\nif ($request_method = 'OPTIONS') {\n    return 588;\n  }\nlocation / {\n    auth_basic \"Restricted\";\n    auth_basic_user_file /etc/nginx/.htpasswd;\n    proxy_set_header X-Forwarded-Proto $scheme;\n    proxy_set_header Host $http_host;\n    proxy_set_header X-Real-IP $remote_addr;\n    client_max_body_size 800M; # avoid HTTP 413 for large image uploads\n    chunked_transfer_encoding on; # required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\n    proxy_pass http://127.0.0.1:5000;\n  }\nlocation @readrequests {\n    proxy_set_header X-Forwarded-Proto $scheme;\n    proxy_set_header Host $http_host;\n    proxy_set_header X-Real-IP $remote_addr;\n    client_max_body_size 800M; # avoid HTTP 413 for large image uploads\n    chunked_transfer_encoding on; # required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\n    proxy_pass http://127.0.0.1:5000;\n  }\n}\n```\nUsers can be added via htpasswd -c /etc/nginx/.htpasswd newuser\n. That's exactly what the snippet above achieves: no auth for pulls, but auth required (via docker login which just creates a ~/.dockercfg) for push/deletes.\n. @larrycai @eolamey thanks for the tips! My config is now down to:\n```\nserver {\n  listen 80;\n  server_name myregisty.net;\nlocation / {\n    rewrite ^(.*)$ https://$host$1 last;\n  }\n}\nserver {\n  listen 443;\n  server_name myregisty.net;\nssl                       on;\n  ssl_certificate           /etc/nginx/ssl/wildcard.myregisty.net.pem;\n  ssl_certificate_key       /etc/nginx/ssl/wildcard.myregisty.net.pem;\n  ssl_dhparam               /etc/nginx/ssl/wildcard.myregisty.net.pem;\nlocation / {\n    limit_except GET HEAD OPTIONS {\n      auth_basic \"Restricted\";\n      auth_basic_user_file /etc/nginx/.htpasswd;\n    }\n    proxy_set_header X-Forwarded-Proto $scheme;\n    proxy_set_header Host $http_host;\n    proxy_set_header X-Real-IP $remote_addr;\n    client_max_body_size 800M; # avoid HTTP 413 for large image uploads\n    chunked_transfer_encoding on; # required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\n    proxy_pass http://127.0.0.1:5000;\n  }\n}\n```\n. That did the trick, thanks!\n. ",
    "XiaokunHou": "+1\n. I added DOCKER_OPTS=\"$DOCKER_OPTS --insecure-registry=10.27.19.230:5000\" to /etc/default/docker file. All works well.\nYou are using a wrong file. init.d folder is used for service.\n. you should add these lines in client docker machine, rather than the registry host machine.  Add it and restart service.\nhttp://stackoverflow.com/questions/27792969/using-private-registry-hosted-on-docker/30478338#30478338\n. ```\nJun 26 03:45:06 docker-registry1 docker: Failed to upload layer: Put http://docker-registry1:5000/v1/images/b36f572eda3e885f7d3dddf8e42b3efa65f7459c1672fdf49770cf8887896709/layer: write tcp 10.27.20.78:5000: connection reset by peer\n```\n. Increase socket timeout solve my problem. \nInside boto.cfg\n[Boto]\nhttp_socket_timeout = 60\nRefer to https://github.com/docker/docker-registry/issues/540\n. ",
    "prologic": "Ahh gotcha :) Did not realize that!\nSorry!\ncheers\nJames\nJames Mills / prologic\nE: prologic@shortcircuit.net.au\nW: prologic.shortcircuit.net.au\nOn Wed, Feb 5, 2014 at 1:27 AM, Joffrey F notifications@github.com wrote:\n\nClosed #226 https://github.com/dotcloud/docker-registry/issues/226.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/226\n.\n. Oh sorry did I post this in the wrong place? :) Oops\nWhere was I meant to?\n\n+1 Individual Image Sibscription would be nice too!\nJames Mills / prologic\nE: prologic@shortcircuit.net.au\nW: prologic.shortcircuit.net.au\nOn Wed, Feb 12, 2014 at 12:16 PM, Thatcher notifications@github.com wrote:\n\nLike [image: :+1:]\nWhat if you would get updates on images that you have starred into a feed\n(potentially with other things)? And what if you could subscribe to that\nfeed using RSS?\nThis is technically not the right place for these issues but I'll\ncross-post it to the right place.\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/issues/244#issuecomment-34832160\n.\n. \n",
    "tokeefe": "@dmp42 I understand. Let me upgrade everything and give it another whirl.\n. ",
    "psviderski": "Here is the curl list:\n``` python\n['curl -i -X PUT',\n \"-H 'x-glance-registry-purge-props: false'\",\n '-H \\'x-image-meta-property-meta_json: {\"id\":\"bf747efa0e2fa9f7c691588ce3938944c75607a7bb5e757f7369f86904d97c78\",\"parent\":\"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\"created\":\"2014-02-03T15:58:08.52236968Z\",\"container\":\"28c54617d310c57a3dae782434f2fd18120d1d7056dd5617a24468c5cc85fa27\",\"container_config\":{\"Hostname\":\"28c54617d310\",\"Domainname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":[\"HOME=/\",\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"],\"Cmd\":[\"/bin/sh\",\"-c\",\"#(nop) MAINTAINER J\\xc3\\xa9r\\xc3\\xb4me Petazzoni \\u003cjerome@docker.com\\u003e\"],\"Dns\":null,\"Image\":\"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\"Volumes\":null,\"VolumesFrom\":\"\",\"WorkingDir\":\"\",\"Entrypoint\":null,\"NetworkDisabled\":false,\"Privileged\":false},\"docker_version\":\"0.6.3\",\"author\":\"J\\xc3\\xa9r\\xc3\\xb4me Petazzoni \\u003cjerome@docker.com\\u003e\",\"config\":{\"Hostname\":\"28c54617d310\",\"Domainname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":[\"HOME=/\",\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"],\"Cmd\":null,\"Dns\":null,\"Image\":\"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\"Volumes\":null,\"VolumesFrom\":\"\",\"WorkingDir\":\"\",\"Entrypoint\":null,\"NetworkDisabled\":false,\"Privileged\":false},\"architecture\":\"x86_64\",\"Size\":0}\\'',\n u\"-H 'X-Auth-Token: MIINnAYJKoZIhvcNAQcCoIINjTCCDYkCAQExCTAHBgUrDgMCGjCCC-IGCSqGSIb3DQEHAaCCC+MEggvfeyJhY2Nlc3MiOiB7InRva2VuIjogeyJpc3N1ZWRfYXQiOiAiMjAxNC0wMi0wOVQxMjo1Mjo0My45Nzk5NDEiLCAiZXhwaXJlcyI6IC [...]\",\n \"-H 'Content-Type: application/octet-stream'\",\n \"-H 'User-Agent: python-glanceclient'\",\n u'http://188.226.156.15:9292/v1/images/7a8cb07e-8225-492f-865b-0ed91d0ddd3b']\n\n\n\nmap(type, curl)\n[str, str, str, unicode, str, str, unicode]\n```\n\n\n\nYou can see the curl list is a mixin of 'str' and 'unicode' objects. It seems that when 'join' is called it tries to decode 'str' objects to 'unicode' using ascii encoding. It fails because curl[2] contains non-ascii characters \\xc3\\xa9r\\xc3\\xb4 (in maintainer's name J\u00e9r\u00f4me Petazzoni).\nIt could be decoded manually curl[2] = curl[2].decode('utf8'), this fix the issue, but I'm not sure that this is the right way to handle utf8 encoded 'meta_json' property. By the way it also causes similar exception in another modules.\n. ",
    "bodenr": "As indicated in previous comments the issue is in the encoding of image metadata...\nI was able to hack around this by adding encode('utf-8') and decode('utf-8') in various places of the registry code.\nFor example:\n./docker-registry/registry/images.py:210:        json_data = json_data.encode('utf-8')\n./docker-registry/lib/storage/glance.py:199:        props = {propname: content.decode('utf-8')}\n./docker-registry/registry/images.py:382:        data = json.loads(flask.request.data.decode('utf-8'))\nAgain this is a pure in-place hack to push images to glance.. At present I do not have the time to dig into a real fix.\n@samalba  -- this seems like a substantial issue as I hit it with a few images.\n. @samalba \nsteps to repo:\n- run docker-registry using the document means for devstack https://github.com/stackforge/nova-docker/tree/master/contrib/devstack\n- pull the guillermo/mysql image\n- tag guillermo/mysql for your docker-registry (see section on \"How to use it\" here https://wiki.openstack.org/wiki/Docker)\n- push the tagged guillermo/mysql image (to glance)\nYou will get the error described in this defect.\nHere's a dump of the error and an added print statement (line starting with !!!) to show the failing content:\n!!!propname: meta_json -- content: {\"id\":\"e6e7b84765282429acd3cb29a72a2d0db77064b8d8ec03629f44d0feb34048cb\",\"parent\":\"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\",\"created\":\"2013-10-08T15:08:52.97421396+02:00\",\"container\":\"d03b89660aec41a385dde1564834259eed311295e1fd11c53350a01e6ef5cb57\",\"container_config\":{\"Hostname\":\"d03b89660aec\",\"Domainname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":[\"HOME=/\",\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"],\"Cmd\":[\"/bin/sh\",\"-c\",\"#(nop) MAINTAINER Guillermo \u00c1lvarez \\u003cguillermo@cientifico.net\\u003e\"],\"Dns\":null,\"Image\":\"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\",\"Volumes\":null,\"VolumesFrom\":\"\",\"WorkingDir\":\"\",\"Entrypoint\":null,\"NetworkDisabled\":false,\"Privileged\":false},\"docker_version\":\"0.6.3\",\"author\":\"Guillermo \u00c1lvarez \\u003cguillermo@cientifico.net\\u003e\",\"config\":{\"Hostname\":\"d03b89660aec\",\"Domainname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":[\"HOME=/\",\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"],\"Cmd\":null,\"Dns\":null,\"Image\":\"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\",\"Volumes\":null,\"VolumesFrom\":\"\",\"WorkingDir\":\"\",\"Entrypoint\":null,\"NetworkDisabled\":false,\"Privileged\":false},\"architecture\":\"x86_64\",\"Size\":0} -- props: {'meta_json': '{\"id\":\"e6e7b84765282429acd3cb29a72a2d0db77064b8d8ec03629f44d0feb34048cb\",\"parent\":\"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\",\"created\":\"2013-10-08T15:08:52.97421396+02:00\",\"container\":\"d03b89660aec41a385dde1564834259eed311295e1fd11c53350a01e6ef5cb57\",\"container_config\":{\"Hostname\":\"d03b89660aec\",\"Domainname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":[\"HOME=/\",\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"],\"Cmd\":[\"/bin/sh\",\"-c\",\"#(nop) MAINTAINER Guillermo \\xc3\\x81lvarez \\\\u003cguillermo@cientifico.net\\\\u003e\"],\"Dns\":null,\"Image\":\"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\",\"Volumes\":null,\"VolumesFrom\":\"\",\"WorkingDir\":\"\",\"Entrypoint\":null,\"NetworkDisabled\":false,\"Privileged\":false},\"docker_version\":\"0.6.3\",\"author\":\"Guillermo \\xc3\\x81lvarez \\\\u003cguillermo@cientifico.net\\\\u003e\",\"config\":{\"Hostname\":\"d03b89660aec\",\"Domainname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":[\"HOME=/\",\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"],\"Cmd\":null,\"Dns\":null,\"Image\":\"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\",\"Volumes\":null,\"VolumesFrom\":\"\",\"WorkingDir\":\"\",\"Entrypoint\":null,\"NetworkDisabled\":false,\"Privileged\":false},\"architecture\":\"x86_64\",\"Size\":0}'}\n2014-04-21 11:58:21,651 ERROR: Exception on /v1/images/e6e7b84765282429acd3cb29a72a2d0db77064b8d8ec03629f44d0feb34048cb/json [PUT]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1687, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1360, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1358, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1344, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/docker-registry/registry/toolkit.py\", line 237, in wrapper\n    return f(*args, **kwargs)\n  File \"/docker-registry/registry/images.py\", line 412, in put_image_json\n    store.put_content(json_path, flask.request.data)\n  File \"/docker-registry/lib/storage/glance.py\", line 68, in dispatcher\n    return attr(*args, **kwargs)\n  File \"/docker-registry/lib/storage/glance.py\", line 201, in put_content\n    image.update(properties=props, purge_props=False)\n  File \"/usr/local/lib/python2.7/dist-packages/glanceclient/v1/images.py\", line 46, in update\n    self.manager.update(self, **fields)\n  File \"/usr/local/lib/python2.7/dist-packages/glanceclient/v1/images.py\", line 291, in update\n    'PUT', url, headers=hdrs, body=image_data)\n  File \"/usr/local/lib/python2.7/dist-packages/glanceclient/common/http.py\", line 273, in raw_request\n    return self._http_request(url, method, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/glanceclient/common/http.py\", line 183, in _http_request\n    self.log_curl_request(method, url, kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/glanceclient/common/http.py\", line 141, in log_curl_request\n    LOG.debug(strutils.safe_encode(' '.join(curl)))\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 687: ordinal not in range(128)\n2014-04-21 11:58:21,651 ERROR: Exception on /v1/images/e6e7b84765282429acd3cb29a72a2d0db77064b8d8ec03629f44d0feb34048cb/json [PUT]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1687, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1360, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1358, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1344, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/docker-registry/registry/toolkit.py\", line 237, in wrapper\n    return f(*args, **kwargs)\n  File \"/docker-registry/registry/images.py\", line 412, in put_image_json\n    store.put_content(json_path, flask.request.data)\n  File \"/docker-registry/lib/storage/glance.py\", line 68, in dispatcher\n    return attr(*args, **kwargs)\n  File \"/docker-registry/lib/storage/glance.py\", line 201, in put_content\n    image.update(properties=props, purge_props=False)\n  File \"/usr/local/lib/python2.7/dist-packages/glanceclient/v1/images.py\", line 46, in update\n    self.manager.update(self, **fields)\n  File \"/usr/local/lib/python2.7/dist-packages/glanceclient/v1/images.py\", line 291, in update\n    'PUT', url, headers=hdrs, body=image_data)\n  File \"/usr/local/lib/python2.7/dist-packages/glanceclient/common/http.py\", line 273, in raw_request\n    return self._http_request(url, method, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/glanceclient/common/http.py\", line 183, in _http_request\n    self.log_curl_request(method, url, kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/glanceclient/common/http.py\", line 141, in log_curl_request\n    LOG.debug(strutils.safe_encode(' '.join(curl)))\nUnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 687: ordinal not in range(128)\nThe root cause is this content:\n\"Cmd\":[\"/bin/sh\",\"-c\",\"#(nop) MAINTAINER Guillermo \u00c1lvarez \\u003cguillermo@cientifico.net\\u003e\"]\nAs you can see the offending \\u003c and \\u003e\nThis appears to exist in a number of images based on what others have mentioned in this defect. The fix is to ensure that content is properly utf-8 encoded/decoded... Again I didn't dig too deep into this so only initial findings presented here.\n. @shin- still running into issues:\n```\nx-openstack-request-id: req-10438c8c-f299-4ee5-8765-79adcf1607ca\n{\"images\": [{\"status\": \"queued\", \"deleted_at\": null, \"name\": null, \"deleted\": false, \"container_format\": \"docker\", \"created_at\": \"2014-04-24T15:17:15\", \"disk_format\": \"raw\", \"updated_at\": \"2014-04-24T15:17:16\", \"min_disk\": 0, \"protected\": false, \"id\": \"015f010d-e49a-49e6-bf45-37496fdb09ad\", \"min_ram\": 0, \"checksum\": null, \"owner\": \"a32112ebe5ba42f4a73e50806a3c4464\", \"is_public\": true, \"virtual_size\": null, \"properties\": {\"meta_ancestry\": \"[\\\"e6e7b84765282429acd3cb29a72a2d0db77064b8d8ec03629f44d0feb34048cb\\\", \\\"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\\\"]\", \"meta_json\": \"{\\\"id\\\":\\\"e6e7b84765282429acd3cb29a72a2d0db77064b8d8ec03629f44d0feb34048cb\\\",\\\"parent\\\":\\\"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\\\",\\\"created\\\":\\\"2013-10-08T15:08:52.97421396+02:00\\\",\\\"container\\\":\\\"d03b89660aec41a385dde1564834259eed311295e1fd11c53350a01e6ef5cb57\\\",\\\"container_config\\\":{\\\"Hostname\\\":\\\"d03b89660aec\\\",\\\"Domainname\\\":\\\"\\\",\\\"User\\\":\\\"\\\",\\\"Memory\\\":0,\\\"MemorySwap\\\":0,\\\"CpuShares\\\":0,\\\"AttachStdin\\\":false,\\\"AttachStdout\\\":false,\\\"AttachStderr\\\":false,\\\"PortSpecs\\\":null,\\\"Tty\\\":false,\\\"OpenStdin\\\":false,\\\"StdinOnce\\\":false,\\\"Env\\\":[\\\"HOME=/\\\",\\\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\\\"],\\\"Cmd\\\":[\\\"/bin/sh\\\",\\\"-c\\\",\\\"#(nop) MAINTAINER Guillermo \\u00c1lvarez \\u003cguillermo@cientifico.net\\u003e\\\"],\\\"Dns\\\":null,\\\"Image\\\":\\\"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\\\",\\\"Volumes\\\":null,\\\"VolumesFrom\\\":\\\"\\\",\\\"WorkingDir\\\":\\\"\\\",\\\"Entrypoint\\\":null,\\\"NetworkDisabled\\\":false,\\\"Privileged\\\":false},\\\"docker_version\\\":\\\"0.6.3\\\",\\\"author\\\":\\\"Guillermo \\u00c1lvarez \\u003cguillermo@cientifico.net\\u003e\\\",\\\"config\\\":{\\\"Hostname\\\":\\\"d03b89660aec\\\",\\\"Domainname\\\":\\\"\\\",\\\"User\\\":\\\"\\\",\\\"Memory\\\":0,\\\"MemorySwap\\\":0,\\\"CpuShares\\\":0,\\\"AttachStdin\\\":false,\\\"AttachStdout\\\":false,\\\"AttachStderr\\\":false,\\\"PortSpecs\\\":null,\\\"Tty\\\":false,\\\"OpenStdin\\\":false,\\\"StdinOnce\\\":false,\\\"Env\\\":[\\\"HOME=/\\\",\\\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\\\"],\\\"Cmd\\\":null,\\\"Dns\\\":null,\\\"Image\\\":\\\"8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\\\",\\\"Volumes\\\":null,\\\"VolumesFrom\\\":\\\"\\\",\\\"WorkingDir\\\":\\\"\\\",\\\"Entrypoint\\\":null,\\\"NetworkDisabled\\\":false,\\\"Privileged\\\":false},\\\"architecture\\\":\\\"x86_64\\\",\\\"Size\\\":0}\", \"id\": \"e6e7b84765282429acd3cb29a72a2d0db77064b8d8ec03629f44d0feb34048cb\", \"meta__inprogress\": \"true\"}, \"size\": 0}]}\n2014-04-24 15:17:17,575 ERROR: Exception on /v1/images/e6e7b84765282429acd3cb29a72a2d0db77064b8d8ec03629f44d0feb34048cb/layer [PUT]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1687, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1360, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1358, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1344, in dispatch_request\n    return self.view_functionsrule.endpoint\n  File \"/docker-registry/docker_registry/toolkit.py\", line 237, in wrapper\n    return f(*args, kwargs)\n  File \"/docker-registry/docker_registry/images.py\", line 227, in put_image_layer\n    h, sum_hndlr = checksums.simple_checksum_handler(json_data)\n  File \"/docker-registry/docker_registry/lib/checksums.py\", line 64, in simple_checksum_handler\n    h = hashlib.sha256(json_data + '\\n')\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\xc1' in position 650: ordinal not in range(128)\n2014-04-24 15:17:17,575 ERROR: Exception on /v1/images/e6e7b84765282429acd3cb29a72a2d0db77064b8d8ec03629f44d0feb34048cb/layer [PUT]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1687, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1360, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1358, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1344, in dispatch_request\n    return self.view_functionsrule.endpoint\n  File \"/docker-registry/docker_registry/toolkit.py\", line 237, in wrapper\n    return f(*args, kwargs)\n  File \"/docker-registry/docker_registry/images.py\", line 227, in put_image_layer\n    h, sum_hndlr = checksums.simple_checksum_handler(json_data)\n  File \"/docker-registry/docker_registry/lib/checksums.py\", line 64, in simple_checksum_handler\n    h = hashlib.sha256(json_data + '\\n')\nUnicodeEncodeError: 'ascii' codec can't encode character u'\\xc1' in position 650: ordinal not in range(128)\n10.16.109.34 - - [24/Apr/2014:15:17:17] \"PUT /v1/images/e6e7b84765282429acd3cb29a72a2d0db77064b8d8ec03629f44d0feb34048cb/layer HTTP/1.1\" 500 291 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-38-generic os/linux arch/amd64\"\n2014-04-24 15:17:17,577 INFO: 10.16.109.34 - - [24/Apr/2014:15:17:17] \"PUT /v1/images/e6e7b84765282429acd3cb29a72a2d0db77064b8d8ec03629f44d0feb34048cb/layer HTTP/1.1\" 500 291 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-38-generic os/linux arch/amd64\"\n```\nalso note it appears your logic for nova-docker needs updating to account for your recent merge which removes run.sh ... e.g.\n-CMD cd /docker-registry && ./setup-configs.sh && exec ./run.sh\n +CMD cd /docker-registry && ./setup-configs.sh && exec docker-registry\nneeds to be incorporated into: https://github.com/stackforge/nova-docker/blob/master/contrib/devstack/lib/nova_plugins/hypervisor-docker#L106\nthanks\n. You'll probably want to check your docker-registry logs and post related info here...\nOn the docker host system find the container ID for the docker registry and then do docker logs CONTAINER_ID where CONTAINER_ID is the ID of the docker-registry container.\nI hit something like this in a 2 node install and turned out that I forgot to add the docker container format to glance.conf.. See here: https://wiki.openstack.org/wiki/Docker\n. @dmp42 - could be... However since #256 was created I believe glance added a property to configure the header line length (see: max_header_line here: http://docs.openstack.org/trunk/config-reference/content/section_glance-api.conf.html). In my testing that line length for glance should have been sufficiently large based on my calculations.\n. ",
    "drnic": "Or I'm confused about registry v index. :)\n. Thanks for mentioning that.\nI also saw that secret_key is autogenerated (and am only running a single\nregistry in a docker container to test), but it is now working when I\nexplicitly set the secret_key.\n. Yep, if auto-generate isn't actually useful. Erroring nicely is better.\nOn Sun, Feb 9, 2014 at 10:37 AM, Sam Alba notifications@github.com\nwrote:\n\nMaybe we should remove the auto-generate and raise an exception if not set. What do you think?\nReply to this email directly or view it on GitHub:\nhttps://github.com/dotcloud/docker-registry/issues/239#issuecomment-34581826\n. \n",
    "davidhoyt": "Closing, see PR #243 \n. ",
    "dhrp": "Like :+1: \nWhat if you would get updates on images that you have starred into a feed (potentially with other things)? And what if you could subscribe to that feed using RSS? \nThis is technically not the right place for these issues but I'll cross-post it to the right place. \n. ",
    "leetrout": "I'd like to suggest abstracting this out in to \"backends\" so that it is pluggable and shipping the current work as the default SQLAlchemy backend.\nSo the conf would look like:\nsearch_index: \"_env:SEARCH_INDEX:\"\nsearch_backend: \"_env:SEARCH_BACKEND:\"\nWhere search_index is passed to the backend init and backend is a string to a backend class that is loaded at runtime from a module available from sys.path via __import__.\ne.g. to enable the default engine would be -e SEARCH_BACKEND backends.sqlalchemy  -e SEARCH_INDEX sqlite:///foo.db\nAnd if I wanted to use a custom Xapian backend from my own lib I could -e SEARCH_BACKEND foo.backends.xapian -e SEARCH_INDEX /var/docker/registry-index.\n. ",
    "dbason": "Hi, I've tried running the docker-registry image with sqlalchemy enabled but it's trying to pull down an sqlachemy image and there are none in the docker registry.  So I guess the options are to build my own sqlaclchemy image or put it into the docker-registry image.\nAny pointers/suggestions on where to start would be appreciated.\n. Sorry I had a Noddy moment and in the command line for my docker image did -e SEARCH_BACKEND sqlalchemy rather than -e SEARCH_BACKEND=sqlalchemy.  This meant it tried to run sqlalchemy as the image which doesn't exist.\n. @wking Sorry if this has been covered off (I'm having problems following the various discussions on deletion of images).  Will the code as is clean up images on an s3 compatible back end (I ask because most of the scripts/cleanups I have seen in other threads look to be local file system only)?\n. Thanks for this - we are also in a similar position in that we use ceph object gateway internally and want to store images there.  I have put these changes (except for the log level) into the docker image and pushed it as dbason/registry:461\nEdit: actually scrub that - the image I pushed is a bit average in that it was built using -t then a commit so it won't run unless it's started in -t -i mode.  I'll fix this up in a couple of days\n. I've also looked over the code base more and you don't actually need to modify the s3.py file if you change the config_sample variables to be boto_host and boto_port; the base package will automatically add these if they are configured.  I don't believe you need to explicitly pass OrdinaryCallingFormat as this is default(?).\n. I think it's still nicer to have env variables so they can be passed easily through to a docker container.\n. I'm running the docker image and on startup with s3(ceph) when it tries to create a new sqlalchemy database for search it runs into a bunch of locking errors and crashes.  I assume this is the threading issue mentioned, or should I create a new issue?\n. Got the same thing 2 times in a row then it worked the 3rd time. @dmp42 do you want me to provide anymore information about our environment or have you got enough to go on here?\n. ",
    "xh3b4sd": "Sorry for the noice.\n. ",
    "pmenglund": "[debug] registry.go:451 [registry] Calling PUT http://docker.x.com/v1/images/3816777d84dae56ba64616b03c80aabd3c3bddff3bb159dff55b9d4af258947a/layer\n[debug] http.go:168 http://docker.x.com/v1/images/3816777d84dae56ba64616b03c80aabd3c3bddff3bb159dff55b9d4af258947a/layer -- HEADERS: map[User-Agent:[docker/0.8.1 go/go1.2 git-commit/a1598d1 kernel/3.11.0-15-generic os/linux arch/amd64 ]]\n[debug] tarsum.go:171 -->3787cc295c3100f034f8c7320e5ba791d6e774e02f6b3ba2846b095b24c95d85<--\n[debug] tarsum.go:171 -->7cc95efed0ddb2199cd77ebc2b59d67dd85d5c0d5042cd9a9fc29fa2bc89c16c<--\n[debug] tarsum.go:171 -->94717bb479012f88065c5b07748e9bf09d82593f36510ffdb2f42385d51ab133<--\n[debug] tarsum.go:171 -->a13d28ca8d1c23da02b1258038f4b96230a611b44f911b05b33277ca87cc550f<--\n[debug] tarsum.go:171 -->a9d192f87f3715d6c6d15882b37780f24b63c945706612965d8886f163a8bdf2<--\n[debug] tarsum.go:171 -->c5e292baaa2625895da8c59215de6b409cdb3aed4f91600176f8dd72da966355<--\n[debug] tarsum.go:175 checksum processed: tarsum+sha256:d0e99b57349abc27ed5a8c34243fedced854ef2d1df5fc3cc6ceec01ab49c565\n[debug] registry.go:383 [registry] Calling PUT http://docker.x.com/v1/images/3816777d84dae56ba64616b03c80aabd3c3bddff3bb159dff55b9d4af258947a/checksum\n[debug] http.go:168 http://docker.x.com/v1/images/3816777d84dae56ba64616b03c80aabd3c3bddff3bb159dff55b9d4af258947a/checksum -- HEADERS: map[User-Agent:[docker/0.8.1 go/go1.2 git-commit/a1598d1 kernel/3.11.0-15-generic os/linux arch/amd64 ]]\nHTTP code 400 while uploading metadata: {\n    \"error\": \"Checksum not found in Cookie\"\n}[/var/lib/docker|8fb6a285] -job push(docker.x.com/registry) = ERR (1)\n. Ah, I didn't - I'll add secret_key and retry. I'll see if I can add a fix to require a key...\n. :+1: \n. ",
    "billumina": "In an above post it mentioned enabling debug mode. I receive a list of available parameters -D is included.\nsudo docker run -D -d -p 5000:5000 registry\n. ",
    "ktintc": "Agree, will fix. Thanks for taking a look!\n. I updated the requirements.txt\nThank you!\n. Yes, those were the dependencies of gcs-oauth2-boto-plugin, removed.\n. ",
    "newhoggy": "I seem to have gotten a little further by specifying the redis cache:\n```\nFROM samalba/docker-registry\nRUN sudo apt-get update\nRUN sudo apt-get install -y vim\nRUN sudo apt-get install -y redis-server\nRUN sudo apt-get clean\nRUN sudo sed -i 's/s3_access_key: REPLACEME/s3_access_key: XXXXX/g' docker-registry/config/config.yml\nRUN sudo sed -i 's|s3_secret_key: REPLACEME|s3_secret_key: XXXXX|g' docker-registry/config/config.yml\nRUN sudo sed -i 's|s3_bucket: REPLACEME|s3_bucket: XXXXX|g' docker-registry/config/config.yml\nRUN sudo sed -i 's|s3_encrypt: REPLACEME|s3_encrypt: false|g' docker-registry/config/config.yml\nRUN sudo sed -i 's|s3_secure: REPLACEME|s3_secure: false|g' docker-registry/config/config.yml\nRUN sudo sed -i 's|storage_path: \"_env:STORAGE_PATH:/prod\"|storage_path: /prod|g' docker-registry/config/config.yml\nEXPOSE 5000\nCMD export SETTINGS_FLAVOR=prod; \\\n    export CACHE_REDIS_HOST=localhost; \\\n    export CACHE_REDIS_PORT=6379; \\\n    export CACHE_LRU_REDIS_HOST=localhost; \\\n    export CACHE_LRU_REDIS_PORT=6379; \\\n    cd /docker-registry; \\\n    ./setup-configs.sh; \\\n    exec ./run.sh\n```\nException:\ncore@ip-10-0-0-166 ~/gocatch-docker-registry $ docker run -p 5000:5000 51e5950fa43c\n2014-03-04 14:17:14 [1] [INFO] Starting gunicorn 18.0\n2014-03-04 14:17:14 [1] [INFO] Listening at: http://0.0.0.0:5000 (1)\n2014-03-04 14:17:14 [1] [INFO] Using worker: gevent\n2014-03-04 14:17:14 [13] [INFO] Booting worker with pid: 13\n2014-03-04 14:17:14 [14] [INFO] Booting worker with pid: 14\n2014-03-04 14:17:14 [15] [INFO] Booting worker with pid: 15\n2014-03-04 14:17:14 [16] [INFO] Booting worker with pid: 16\n2014-03-04 14:17:18 [14] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 165, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/docker-registry/wsgi.py\", line 11, in <module>\n    import registry\n  File \"/docker-registry/registry/__init__.py\", line 5, in <module>\n    from .tags import *\n  File \"/docker-registry/registry/tags.py\", line 17, in <module>\n    store = storage.load()\n  File \"/docker-registry/lib/storage/__init__.py\", line 138, in load\n    store = s3.S3Storage(cfg)\n  File \"/docker-registry/lib/storage/s3.py\", line 22, in __init__\n    BotoStorage.__init__(self, config)\n  File \"/docker-registry/lib/storage/boto_base.py\", line 107, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 431, in get_bucket\n    bucket.get_all_keys(headers, maxkeys=0)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 392, in get_all_keys\n    '', headers, **params)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 333, in _get_all\n    query_args=query_args)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 535, in make_request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 93, in build_auth_path\n    path = '/' + bucket\nTypeError: cannot concatenate 'str' and 'NoneType' objects\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 165, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/docker-registry/wsgi.py\", line 11, in <module>\n    import registry\n  File \"/docker-registry/registry/__init__.py\", line 5, in <module>\n    from .tags import *\n  File \"/docker-registry/registry/tags.py\", line 17, in <module>\n    store = storage.load()\n  File \"/docker-registry/lib/storage/__init__.py\", line 138, in load\n    store = s3.S3Storage(cfg)\n  File \"/docker-registry/lib/storage/s3.py\", line 22, in __init__\n    BotoStorage.__init__(self, config)\n  File \"/docker-registry/lib/storage/boto_base.py\", line 107, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 431, in get_bucket\n    bucket.get_all_keys(headers, maxkeys=0)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 392, in get_all_keys\n    '', headers, **params)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 333, in _get_all\n    query_args=query_args)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 535, in make_request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 93, in build_auth_path\n    path = '/' + bucket\nTypeError: cannot concatenate 'str' and 'NoneType' objects\nERROR:gunicorn.error:Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 165, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/docker-registry/wsgi.py\", line 11, in <module>\n    import registry\n  File \"/docker-registry/registry/__init__.py\", line 5, in <module>\n    from .tags import *\n  File \"/docker-registry/registry/tags.py\", line 17, in <module>\n    store = storage.load()\n  File \"/docker-registry/lib/storage/__init__.py\", line 138, in load\n    store = s3.S3Storage(cfg)\n  File \"/docker-registry/lib/storage/s3.py\", line 22, in __init__\n    BotoStorage.__init__(self, config)\n  File \"/docker-registry/lib/storage/boto_base.py\", line 107, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 431, in get_bucket\n    bucket.get_all_keys(headers, maxkeys=0)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 392, in get_all_keys\n    '', headers, **params)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 333, in _get_all\n    query_args=query_args)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 535, in make_request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 93, in build_auth_path\n    path = '/' + bucket\nTypeError: cannot concatenate 'str' and 'NoneType' objects\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 165, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/docker-registry/wsgi.py\", line 11, in <module>\n    import registry\n  File \"/docker-registry/registry/__init__.py\", line 5, in <module>\n    from .tags import *\n  File \"/docker-registry/registry/tags.py\", line 17, in <module>\n    store = storage.load()\n  File \"/docker-registry/lib/storage/__init__.py\", line 138, in load\n    store = s3.S3Storage(cfg)\n  File \"/docker-registry/lib/storage/s3.py\", line 22, in __init__\n    BotoStorage.__init__(self, config)\n  File \"/docker-registry/lib/storage/boto_base.py\", line 107, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 431, in get_bucket\n    bucket.get_all_keys(headers, maxkeys=0)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 392, in get_all_keys\n    '', headers, **params)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 333, in _get_all\n    query_args=query_args)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 535, in make_request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 93, in build_auth_path\n    path = '/' + bucket\nTypeError: cannot concatenate 'str' and 'NoneType' objects\n2014-03-04 14:17:18 [14] [INFO] Worker exiting (pid: 14)\nINFO:gunicorn.error:Worker exiting (pid: 14)\n2014-03-04 14:17:18 [16] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 165, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/docker-registry/wsgi.py\", line 11, in <module>\n    import registry\n  File \"/docker-registry/registry/__init__.py\", line 5, in <module>\n    from .tags import *\n  File \"/docker-registry/registry/tags.py\", line 17, in <module>\n    store = storage.load()\n  File \"/docker-registry/lib/storage/__init__.py\", line 138, in load\n    store = s3.S3Storage(cfg)\n  File \"/docker-registry/lib/storage/s3.py\", line 22, in __init__\n    BotoStorage.__init__(self, config)\n  File \"/docker-registry/lib/storage/boto_base.py\", line 107, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 431, in get_bucket\n    bucket.get_all_keys(headers, maxkeys=0)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 392, in get_all_keys\n    '', headers, **params)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 333, in _get_all\n    query_args=query_args)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 535, in make_request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 93, in build_auth_path\n    path = '/' + bucket\nTypeError: cannot concatenate 'str' and 'NoneType' objects\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 165, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/docker-registry/wsgi.py\", line 11, in <module>\n    import registry\n  File \"/docker-registry/registry/__init__.py\", line 5, in <module>\n    from .tags import *\n  File \"/docker-registry/registry/tags.py\", line 17, in <module>\n    store = storage.load()\n  File \"/docker-registry/lib/storage/__init__.py\", line 138, in load\n    store = s3.S3Storage(cfg)\n  File \"/docker-registry/lib/storage/s3.py\", line 22, in __init__\n    BotoStorage.__init__(self, config)\n  File \"/docker-registry/lib/storage/boto_base.py\", line 107, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 431, in get_bucket\n    bucket.get_all_keys(headers, maxkeys=0)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 392, in get_all_keys\n    '', headers, **params)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 333, in _get_all\n    query_args=query_args)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 535, in make_request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 93, in build_auth_path\n    path = '/' + bucket\nTypeError: cannot concatenate 'str' and 'NoneType' objects\nERROR:gunicorn.error:Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 165, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/docker-registry/wsgi.py\", line 11, in <module>\n    import registry\n  File \"/docker-registry/registry/__init__.py\", line 5, in <module>\n    from .tags import *\n  File \"/docker-registry/registry/tags.py\", line 17, in <module>\n    store = storage.load()\n  File \"/docker-registry/lib/storage/__init__.py\", line 138, in load\n    store = s3.S3Storage(cfg)\n  File \"/docker-registry/lib/storage/s3.py\", line 22, in __init__\n    BotoStorage.__init__(self, config)\n  File \"/docker-registry/lib/storage/boto_base.py\", line 107, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 431, in get_bucket\n    bucket.get_all_keys(headers, maxkeys=0)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 392, in get_all_keys\n    '', headers, **params)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 333, in _get_all\n    query_args=query_args)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 535, in make_request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 93, in build_auth_path\n    path = '/' + bucket\nTypeError: cannot concatenate 'str' and 'NoneType' objects\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 165, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/docker-registry/wsgi.py\", line 11, in <module>\n    import registry\n  File \"/docker-registry/registry/__init__.py\", line 5, in <module>\n    from .tags import *\n  File \"/docker-registry/registry/tags.py\", line 17, in <module>\n    store = storage.load()\n  File \"/docker-registry/lib/storage/__init__.py\", line 138, in load\n    store = s3.S3Storage(cfg)\n  File \"/docker-registry/lib/storage/s3.py\", line 22, in __init__\n    BotoStorage.__init__(self, config)\n  File \"/docker-registry/lib/storage/boto_base.py\", line 107, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 431, in get_bucket\n    bucket.get_all_keys(headers, maxkeys=0)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 392, in get_all_keys\n    '', headers, **params)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 333, in _get_all\n    query_args=query_args)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 535, in make_request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 93, in build_auth_path\n    path = '/' + bucket\nTypeError: cannot concatenate 'str' and 'NoneType' objects\n2014-03-04 14:17:18 [16] [INFO] Worker exiting (pid: 16)\nINFO:gunicorn.error:Worker exiting (pid: 16)\n2014-03-04 14:17:18 [13] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 165, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/docker-registry/wsgi.py\", line 11, in <module>\n    import registry\n  File \"/docker-registry/registry/__init__.py\", line 5, in <module>\n    from .tags import *\n  File \"/docker-registry/registry/tags.py\", line 17, in <module>\n    store = storage.load()\n  File \"/docker-registry/lib/storage/__init__.py\", line 138, in load\n    store = s3.S3Storage(cfg)\n  File \"/docker-registry/lib/storage/s3.py\", line 22, in __init__\n    BotoStorage.__init__(self, config)\n  File \"/docker-registry/lib/storage/boto_base.py\", line 107, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 431, in get_bucket\n    bucket.get_all_keys(headers, maxkeys=0)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 392, in get_all_keys\n    '', headers, **params)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 333, in _get_all\n    query_args=query_args)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 535, in make_request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 93, in build_auth_path\n    path = '/' + bucket\nTypeError: cannot concatenate 'str' and 'NoneType' objects\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 165, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/docker-registry/wsgi.py\", line 11, in <module>\n    import registry\n  File \"/docker-registry/registry/__init__.py\", line 5, in <module>\n    from .tags import *\n  File \"/docker-registry/registry/tags.py\", line 17, in <module>\n    store = storage.load()\n  File \"/docker-registry/lib/storage/__init__.py\", line 138, in load\n    store = s3.S3Storage(cfg)\n  File \"/docker-registry/lib/storage/s3.py\", line 22, in __init__\n    BotoStorage.__init__(self, config)\n  File \"/docker-registry/lib/storage/boto_base.py\", line 107, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 431, in get_bucket\n    bucket.get_all_keys(headers, maxkeys=0)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 392, in get_all_keys\n    '', headers, **params)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 333, in _get_all\n    query_args=query_args)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 535, in make_request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 93, in build_auth_path\n    path = '/' + bucket\nTypeError: cannot concatenate 'str' and 'NoneType' objects\nERROR:gunicorn.error:Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 165, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/docker-registry/wsgi.py\", line 11, in <module>\n    import registry\n  File \"/docker-registry/registry/__init__.py\", line 5, in <module>\n    from .tags import *\n  File \"/docker-registry/registry/tags.py\", line 17, in <module>\n    store = storage.load()\n  File \"/docker-registry/lib/storage/__init__.py\", line 138, in load\n    store = s3.S3Storage(cfg)\n  File \"/docker-registry/lib/storage/s3.py\", line 22, in __init__\n    BotoStorage.__init__(self, config)\n  File \"/docker-registry/lib/storage/boto_base.py\", line 107, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 431, in get_bucket\n    bucket.get_all_keys(headers, maxkeys=0)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 392, in get_all_keys\n    '', headers, **params)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 333, in _get_all\n    query_args=query_args)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 535, in make_request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 93, in build_auth_path\n    path = '/' + bucket\nTypeError: cannot concatenate 'str' and 'NoneType' objects\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 165, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/docker-registry/wsgi.py\", line 11, in <module>\n    import registry\n  File \"/docker-registry/registry/__init__.py\", line 5, in <module>\n    from .tags import *\n  File \"/docker-registry/registry/tags.py\", line 17, in <module>\n    store = storage.load()\n  File \"/docker-registry/lib/storage/__init__.py\", line 138, in load\n    store = s3.S3Storage(cfg)\n  File \"/docker-registry/lib/storage/s3.py\", line 22, in __init__\n    BotoStorage.__init__(self, config)\n  File \"/docker-registry/lib/storage/boto_base.py\", line 107, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 431, in get_bucket\n    bucket.get_all_keys(headers, maxkeys=0)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 392, in get_all_keys\n    '', headers, **params)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 333, in _get_all\n    query_args=query_args)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 535, in make_request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 93, in build_auth_path\n    path = '/' + bucket\nTypeError: cannot concatenate 'str' and 'NoneType' objects\n2014-03-04 14:17:18 [13] [INFO] Worker exiting (pid: 13)\nINFO:gunicorn.error:Worker exiting (pid: 13)\n2014-03-04 14:17:19 [15] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 165, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/docker-registry/wsgi.py\", line 11, in <module>\n    import registry\n  File \"/docker-registry/registry/__init__.py\", line 5, in <module>\n    from .tags import *\n  File \"/docker-registry/registry/tags.py\", line 17, in <module>\n    store = storage.load()\n  File \"/docker-registry/lib/storage/__init__.py\", line 138, in load\n    store = s3.S3Storage(cfg)\n  File \"/docker-registry/lib/storage/s3.py\", line 22, in __init__\n    BotoStorage.__init__(self, config)\n  File \"/docker-registry/lib/storage/boto_base.py\", line 107, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 431, in get_bucket\n    bucket.get_all_keys(headers, maxkeys=0)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 392, in get_all_keys\n    '', headers, **params)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 333, in _get_all\n    query_args=query_args)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 535, in make_request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 93, in build_auth_path\n    path = '/' + bucket\nTypeError: cannot concatenate 'str' and 'NoneType' objects\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 165, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/docker-registry/wsgi.py\", line 11, in <module>\n    import registry\n  File \"/docker-registry/registry/__init__.py\", line 5, in <module>\n    from .tags import *\n  File \"/docker-registry/registry/tags.py\", line 17, in <module>\n    store = storage.load()\n  File \"/docker-registry/lib/storage/__init__.py\", line 138, in load\n    store = s3.S3Storage(cfg)\n  File \"/docker-registry/lib/storage/s3.py\", line 22, in __init__\n    BotoStorage.__init__(self, config)\n  File \"/docker-registry/lib/storage/boto_base.py\", line 107, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 431, in get_bucket\n    bucket.get_all_keys(headers, maxkeys=0)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 392, in get_all_keys\n    '', headers, **params)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 333, in _get_all\n    query_args=query_args)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 535, in make_request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 93, in build_auth_path\n    path = '/' + bucket\nTypeError: cannot concatenate 'str' and 'NoneType' objects\nERROR:gunicorn.error:Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 165, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/docker-registry/wsgi.py\", line 11, in <module>\n    import registry\n  File \"/docker-registry/registry/__init__.py\", line 5, in <module>\n    from .tags import *\n  File \"/docker-registry/registry/tags.py\", line 17, in <module>\n    store = storage.load()\n  File \"/docker-registry/lib/storage/__init__.py\", line 138, in load\n    store = s3.S3Storage(cfg)\n  File \"/docker-registry/lib/storage/s3.py\", line 22, in __init__\n    BotoStorage.__init__(self, config)\n  File \"/docker-registry/lib/storage/boto_base.py\", line 107, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 431, in get_bucket\n    bucket.get_all_keys(headers, maxkeys=0)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 392, in get_all_keys\n    '', headers, **params)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 333, in _get_all\n    query_args=query_args)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 535, in make_request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 93, in build_auth_path\n    path = '/' + bucket\nTypeError: cannot concatenate 'str' and 'NoneType' objects\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 165, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/docker-registry/wsgi.py\", line 11, in <module>\n    import registry\n  File \"/docker-registry/registry/__init__.py\", line 5, in <module>\n    from .tags import *\n  File \"/docker-registry/registry/tags.py\", line 17, in <module>\n    store = storage.load()\n  File \"/docker-registry/lib/storage/__init__.py\", line 138, in load\n    store = s3.S3Storage(cfg)\n  File \"/docker-registry/lib/storage/s3.py\", line 22, in __init__\n    BotoStorage.__init__(self, config)\n  File \"/docker-registry/lib/storage/boto_base.py\", line 107, in __init__\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 431, in get_bucket\n    bucket.get_all_keys(headers, maxkeys=0)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 392, in get_all_keys\n    '', headers, **params)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 333, in _get_all\n    query_args=query_args)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 535, in make_request\n    auth_path = self.calling_format.build_auth_path(bucket, key)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 93, in build_auth_path\n    path = '/' + bucket\nTypeError: cannot concatenate 'str' and 'NoneType' objects\n2014-03-04 14:17:19 [15] [INFO] Worker exiting (pid: 15)\nINFO:gunicorn.error:Worker exiting (pid: 15)\nTraceback (most recent call last):\n  File \"/usr/local/bin/gunicorn\", line 9, in <module>\n    load_entry_point('gunicorn==18.0', 'console_scripts', 'gunicorn')()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 71, in run\n    WSGIApplication(\"%(prog)s [OPTIONS] [APP_MODULE]\").run()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 143, in run\n    Arbiter(self).run()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 203, in run\n    self.halt(reason=inst.reason, exit_status=inst.exit_status)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 298, in halt\n    self.stop()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 341, in stop\n    self.reap_workers()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 452, in reap_workers\n    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\ngunicorn.errors.HaltServer: <HaltServer 'Worker failed to boot.' 3>\n. Thanks for the reply.\nI've verified that FLAVOR dev works fine.\nCan you clarify what you mean by not specifying s3 information?  I'm trying to get s3 to work.  For example:\nRUN sudo sed -i 's/s3_access_key: REPLACEME/s3_access_key: XXXXX/g' docker-registry/config/config.yml\nRUN sudo sed -i 's|s3_secret_key: REPLACEME|s3_secret_key: XXXXX|g' docker-registry/config/config.yml\nRUN sudo sed -i 's|s3_bucket: REPLACEME|s3_bucket: XXXXX|g' docker-registry/config/config.yml\nRUN sudo sed -i 's|s3_encrypt: REPLACEME|s3_encrypt: false|g' docker-registry/config/config.yml\nRUN sudo sed -i 's|s3_secure: REPLACEME|s3_secure: false|g' docker-registry/config/config.yml\nWhere XXXX are my credentials and the name of the bucket.\n. @Analect, I did, but in the end I decided it wasn't worth it maintaining my own docker repository.  I'm going for hosted service at quay.io instead.\n. @Analect, here is what I have.  Most likely won't work since, in my experience, the docker-registry image could change and the config structure with it, which was one of the reasons I decide not to maintain a registry myself.  With some inspection of actual config files and changing the commands to match, it is possible to get it to work.\nFROM samalba/docker-registry\nRUN apt-get update\nRUN dpkg-divert --local --rename /usr/bin/ischroot && ln -sf /bin/true /usr/bin/ischroot\nRUN apt-get upgrade -y\nRUN apt-get install -y fakeroot curl dnsutils git golang jq openssh-client openssh-server redis-server telnet unzip vim wget\nRUN apt-get install -y postfix\nRUN apt-get clean\nRUN cp docker-registry/config/config_s3.yml docker-registry/config/config-setup.yml\nRUN sed -i 's|boto_bucket: REPLACEME|boto_bucket: $(S3_BUCKET)|g' docker-registry/config/config-setup.yml\nRUN sed -i 's|s3_access_key: REPLACEME|s3_access_key: $(S3_ACCESS_KEY)|g' docker-registry/config/config-setup.yml\nRUN sed -i 's|s3_secret_key: REPLACEME|s3_secret_key: $(S3_SECRET_KEY)|g' docker-registry/config/config-setup.yml\nRUN sed -i 's|s3_bucket: REPLACEME|s3_bucket: $(S3_BUCKET)|g' docker-registry/config/config-setup.yml\nRUN sed -i 's|s3_encrypt: REPLACEME|s3_encrypt: false|g' docker-registry/config/config-setup.yml\nRUN sed -i 's|s3_secure: REPLACEME|s3_secure: false|g' docker-registry/config/config-setup.yml\nRUN sed -i 's|storage_path: \"_env:STORAGE_PATH:/prod\"|storage_path: /prod|g' docker-registry/config/config-setup.yml\nRUN sed -i 's|smtp_host: REPLACEME|smtp_host: $(SMTP_SERVER)|g' docker-registry/config/config-setup.yml\nRUN sed -i 's|smtp_login: REPLACEME|smtp_login: $(SMTP_LOGIN)|g' docker-registry/config/config-setup.yml\nRUN sed -i 's|smtp_password: REPLACEME|smtp_password: $(SMTP_PASSWORD)|g' docker-registry/config/config-setup.yml\nRUN sed -i 's|# requirepass foobared|requirepass $(REDIS_PASSWORD)|g' /etc/redis/redis.conf\nRUN grep -v REPLACEME docker-registry/config/config-setup.yml > docker-registry/config/config.yml\nENV REGISTRY_HOME /docker-registry\nENV SETTINGS_FLAVOR prod\nENV CACHE_REDIS_HOST localhost\nENV CACHE_REDIS_PORT 6379\nENV CACHE_REDIS_PASSWORD $(REDIS_PASSWORD)\nENV CACHE_LRU_REDIS_HOST localhost\nENV CACHE_LRU_REDIS_PORT 6379\nENV CACHE_LRU_REDIS_PASSWORD $(REDIS_PASSWORD)\nEXPOSE 5000\nCMD /usr/local/bin/docker-registry\n. Apart from this, it looks to be working, so I'm very pleased.  Thanks for your help!\n. ",
    "lafolle": "You are not setting the boto_bucket variable in docker-registry/config/config.yml.\n. ",
    "Analect": "@newhoggy Did you ever get this working for S3? I'm confused by all the conflicting info on the READMEs. Could I trouble you to post an updated version of your dockerfile from above that worked and maybe also the docker run script you used.  It seems that your approach was to pass the S3 credentials into the config file on a docker-registry container as part of the build process, whereas other instructions on here look to pass those credentials at run-time?  Thanks.  Colum\n. Thanks for the info @newhoggy .  Would you be able to post the final Dockerfile that you managed to get working?  Much appreciated.\n. Thanks @newhoggy . I'll give it a try.\n. ",
    "homerjam": "I'm looking for exactly this! Any plans to merge soon?\nThanks\n. Sorry, I'm having difficulty remembering this far back! I checked out Deis and it looks good, but it'd still be great to have this working on Heroku - are you asking if Deis is enough or if the implementation style is ok?\n. ",
    "stuart-warren": "Hi,\nI'm no longer interested in this seeing as Deis now does what I need :)\nI'm going to close this\n. ",
    "garo": "This is being done in https://github.com/dotcloud/docker-registry/pull/280\n. We're having similar problems with 0.9.0. Everything seems to be working fine when just one or two machines are pulling at the same time, but when we let our frontend array (30+ machines) pull at the same time then all hell breaks loose.\nOur workaround has been putting a haproxy in front and limiting that the haproxy allows only as many connections to the registry as there are worker threads running and to spread out the image pulls over a larger time period. We're also running registry in three different machines, all using the same S3 backend so that the haproxy has more backends to spread the requests out.\n. Thanks! The storage_redirect seems to fix all our issues. To others: Just add \"storage_redirect: true\" to the registry.yaml. You can verify that it works by using tcp dump: \"tcpdump -l  -A -s 1500 src port 5000 |grep Location:\" and you should get nice headers like: \"Location: https://yourbucket.s3.amazonaws.com/...\"\n. ",
    "huangsam": "@pmyjavec I'm interested in the answer as well. I have setup the search and cache endpoints, but I am still unsure about what the source index exactly is. I see index.docker.io, but it simply points to the Docker Hub. Not sure if the source index is a UI, like atcol/docker-registry-ui and konradkleine/docker-registry-frontend; or the search backend; or a combination of both.\nI would suggest expanding the mirroring docs, as it has created ambiguity/misinterpretation. See #556 #576 #597 #624\n. I got it to work when I installed python-mysqldb to my Ubuntu machine.\nI also ran this SQL on a separate MySQL container:\nsql\nCREATE DATABASE containers_db;\nCREATE TABLE containers_db.version(id INT PRIMARY KEY);\nCREATE TABLE containers_db.repository(id INT AUTO_INCREMENT PRIMARY KEY, name VARCHAR(95) NOT NULL UNIQUE, description VARCHAR(100));\nAfter that, the docker-registry worked with a containerized MySQL like a charm. Feel free to add this to the README.md as needed. Or if you would like me to add this to the contrib folder in a PR, I can do that as well. Will also be looking at how we can do this with master-master and master-slave configurations.\n. Apparently I encountered SQLAlchemy exceptions in the 0.8.0 release. Was that functionality available during that time?\n. That makes sense. My team and I will look into this for the coming two weeks. Feel free to check this link out as it helped us think in the right direction. Should I close this issue, or keep it hanging for a while?\n. ",
    "stefanfoulis": "Cool. I used https://github.com/dynport/docker-private-registry before I switched. It had two directories:\n- \"images\" which contained actual image data and looks a lot like the images directory that dotcloud/docker-registry created on the CloudFiles container\n- \"repositories\" which contained metadata in json format about which image parts belonged to which repository.\nI can't find an equivalent of \"repositories\" on the CloudFiles container. Maybe it needs additional configuration? Or is the data just saved in a different way?\nMy Config:\nyaml\nprod:\n    storage: swift\n    storage_path: /registry\n    swift_authurl: https://identity.api.rackspacecloud.com/v2.0/\n    swift_container: my-docker-registry-cloudfiles-container\n    swift_user: my-rackspace-user-id\n    swift_password: my-rackspace-password\n    swift_tenant_name: \" \"\n    swift_region_name: IAD\n    loglevel: debug\n. We've taken another stab at it and now have a prototype using the latest docker-registry code and an nginx in front of it doing SSL termination and basic auth.\nWe're still having troubles pushing to it sometimes (timeouts) though.\nAs for the original question: I just assume all state is saved in a different way than with the go implementation I mentioned (https://github.com/dynport/docker-private-registry). Hence the difference.\nSo this issue can be closed.\n. ",
    "jordansissel": "For what it's worth, I don't really judge '2 months ago' to mean 'quite old'  ;)\nUpgrading to 0.9.0 seems to have solved my problem.\n. ",
    "abrgr": "I also get this sporadically when pulling an image from a registry using the s3 backend.  I'll spin up a registry instance and then about 10 additional boxes that all pull a few images each from the registry and 9 will work and 1 will occasionally fail.  My config is as follows:\nprod:\n    standalone: true\n    storage: s3\n    boto_bucket: my-bucket\n    s3_bucket: my-bucket\n    s3_encrypt: true\n    s3_secure: true\n    s3_access_key: _env:ACCESS_KEY\n    s3_secret_key: _env:SECRET_KEY\n    secret_key: something_different\n    storage_path: /docker-images\nThe actual error the registry reports is:\n2014-05-10 21:28:14,545 INFO: 10.0.200.44 - - [10/May/2014:21:28:14] \"GET /v1/images/c830f37b505aa095d7b13379b99dcfda95445b178e7b6115653c6f9519761737/json HTTP/1.1\" 200 1874 \"-\" \"docker/0.9.0 go/go1.2 kernel/3.10.34-37.137.amzn1.x86_64 os/linux arch/amd64\"\n2014-05-10 21:28:14 [18] [ERROR] Socket error processing request.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 45, in handle\n    self.handle_request(listener, req, client, addr)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 151, in handle_request\n    super(GeventWorker, self).handle_request(*args)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 83, in handle_request\n    listener.getsockname(), self.cfg)\n  File \"<string>\", line 1, in getsockname\nerror: [Errno 9] Bad file descriptor\n2014-05-10 21:28:14,484 ERROR: Socket error processing request.\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 45, in handle\n    self.handle_request(listener, req, client, addr)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 151, in handle_request\n    super(GeventWorker, self).handle_request(*args)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 83, in handle_request\n    listener.getsockname(), self.cfg)\n  File \"<string>\", line 1, in getsockname\nerror: [Errno 9] Bad file descriptor\n2014-05-10 21:28:14,613 INFO: ParallelKey: <Key: my-bucket,docker-images/images/c830f37b505aa095d7b13379b99dcfda95445b178e7b6115653c6f9519761737/layer>; size=22467825\nI'll update if I'm able to reproduce it consistently.\n. ",
    "discordianfish": "Agree, this is the same as #320 \n. PS: This looks similar but seems to be different than #320/#285.\n. @dmp42 I pulled it from the registry a few hours ago. The headers show 0.7.0. A second push went through just fine and at least now 'registry/images/f6aab3b6b4a5aeb3954c7b9141278bec3c33bfc60d8e2fbc80c60357514b07aa/_checksum' exists.\n. Ah sorry, docker version is 0.111.1 Here is the output. The output is from a 'git push' which triggered the build and docker push:\nremote: + docker push builder.dckr.io:5000/collins\nremote: The push refers to a repository [builder.dckr.io:5000/collins] (len: 2)\nremote: Sending image list\nremote: Pushing repository builder.dckr.io:5000/collins (2 tags)\nremote: Image 511136ea3c5a already pushed, skipping\nremote: Image 6170bb7b0ad1 already pushed, skipping\nremote: Image 9cd978db300e already pushed, skipping\nremote:\nremote:\nremote:\nremote:\nremote:\nremote:\nremote:\nremote:\nremote:\nremote:\nremote:\nremote:\nremote: 2014/06/02 16:08:49 HTTP code 500 while uploading metadata: invalid character '<' looking for beginning of value\nDo you need the complete registry log for the push as well? It's really talky. I would need to scrub that first but can provide if necessary.\n. @samalba I can't reproduce it (but can try tomorrow if you want) and the daemon wasn't set to debugging, so the only thing I have right now is:\n2014-06-02_16:06:04.59661 [9ac1d130] +job push(builder.dckr.io:5000/collins)\n2014-06-02_16:08:49.97379 HTTP code 500 while uploading metadata: invalid character '<' looking for beginning of value\n2014-06-02_16:08:49.97386 [9ac1d130] -job push(builder.dckr.io:5000/collins) = ERR (1)\n. @dmp42 I've linked to it already (see 'image' above), but here it's again: https://registry.hub.docker.com/u/fish/haproxy/\nI don't think I can come up with a simple test case, but maybe pulling an image, update the repo and pull right after the build is enough?\n. @dmp42 But the repo on the registry is the same as the name of my trusted build, right? So should be fish/haproxy. And in the \"hub registry\" link you see all available build details.\n. @dmp42 I think this is just the known write/read inconsistency with S3 \n. @dmp42 Not sure if dedicated health checks are the best way, but I guess you can say best. My first approach would be to monitor the same things people actually hit like pulling/pushing things etc.\n. ",
    "jonasfj": "I've had similar issues, and I just run my registry with:\nsudo gunicorn -k gevent --max-requests 100 --graceful-timeout 3600 -t 3600 -b 0.0.0.0:80 -w 4 wsgi:application\nI've however, found that when I restart docker locally. sudo /etc/init.d/docker restart it starts working fine again. Could this be docker-server that catches DNS records for too log or something?\n(Perhaps it's only for CNAMEs)\nI wiresharked docker and I don't speak raw-dns very :) Anyway, I suspect that might be the offender here.\nAgain, /v1/_ping worked fine in my browser and took no where near the 10s it took docker to timeout.\n. Note, I changed s3_region to be an optional configuration key. I think people should specify it, as it ensures the right S3 end-point is used. I've seen this cause intermittent errors with boto in other projects.\n(But like I said, it could just be me who is paranoid).\n. I suspect this wont merge automatically anymore... I don't mind unbitrotting it, if there is an interest in merging this.\nNote, I'm using a variation of this on EC2 with S3 in the same region, my instances pull with about 10 MB/s.\nSo big 4GB images aren't much of a problem. Using the old code, I can only imagine the strains it would put on my registry server, when multiple nodes wants to pull a 4G image.\n. Thanks for the feedback I'm all pro configurability...\n- I'll take look at fixing this tomorrow :)\n. I rebased and squashed all changes into one commit... Added the key storage_redirect to configure if redirects should be used.\nNote, regarding region specification, the importance of this is greatly understated in boto documentation.\nBut for performance and stability this is very important. At least that is my experience..\nPlease let me know if there is any other details I should address... I would like to tweak multipart upload buffer size, but that I'll leave that to another PR :)\n. Good, catch we should definitely be using 302. I'll get right on this.\n. ",
    "jaredm4": "Ditto here. Although, I recently upgraded from Docker Registry image 0.6.0 to 0.6.5, and I blamed it on that. Has any one else here also upgraded, or is this problem unrelated?\nEdit: I also upgraded from Nginx 1.2 to 1.4 so I no longer had to do hacks for Chunkin.\n. ",
    "bradsokol": "FWIW, my tests did not detect this until we upgraded our local Docker client/daemon to v0.10. I'm using the 'registry' image to host a private registry.\n. ",
    "pixie79": "Agreed - to make it clearer for others following (as it took me a while to realise) restart the docker daemon on the client side after changing the DNS for the registry server. This refreshed the DNS cache on the client side and allowed the connection to succeed to the server.\n. ",
    "rajneeshl3": "also seeing this issue on Mavericks 10.9.2 . restarting boot2docker fixed it. \nboot2docker info: \"* Kernel 3.13.3 with AUFS, Docker 0.10.0, LXC 0.8.0\"\n. ",
    "Jaykah": "Have the same thing on Ubuntu 14.04\nHave tried various ports, disabled ufw, iptables, tried both localhost, local ip, and domain.\nNo luck thus far.\n. ",
    "mikhailponomaryov": "Yep, I have the same issue with this. But in my case Error is:\n\"read tcp 127.0.0.1:5000: connection reset by peer\"\nor\n\"dial tcp 127.0.0.1:5000: connection refused\"\ndepending on system configuration. Still no success with pushing to registry.\n. ",
    "amaltson": "I'm having the same and similar issues. My nameserver hasn't changed, and I've definitely restarted the docker service a few times, so it's probably not dotcloud/docker#2938. I continue to get timeouts, but if I push enough times, it actually starts to do the push and I've seen it fail with connection refuse:\n```\nPastebin mKLV4WSo\n$ sudo docker push repository:5000/acme/progrium-busybox\nThe push refers to a repository [repository:5000/acme/progrium-busybox] (len: 1)\nSending image list\nPushing repository repository:5000/acme/progrium-busybox (1 tags)\n511136ea3c5a: Image successfully pushed\n1fe6c0d97328: Image successfully pushed\n225b28474924: Image successfully pushed\n3d4d03483da1: Image successfully pushed\ncddc38a43bee: Image successfully pushed\n4533064bc586: Image successfully pushed\nddc177395a37: Image successfully pushed\nefdf59ade75c: Buffering to disk\n2014/07/18 13:33:32 Failed to upload layer: Put http://repository:5000/v1/images/efdf59ade75c6744dcc7695425639aaad2ea5bee6e4654f4e0eeee0c65cebff1/layer: dial tcp :5000: connection refuse\n```\n. And now if I do it enough times it actually fully uploaded.... this is so bizarre :disappointed: \n. @dmp42 sorry didn't provide details.\nThe registry version is the latest official image\n$ curl http://repository:5000/\n\"docker-registry server (local) (v0.7.3)\"\nDocker info of the host running the registry\n$ sudo docker info\nContainers: 1\nImages: 18\nStorage Driver: devicemapper\n Pool Name: docker-253:0-928115-pool\n Data file: /var/lib/docker/devicemapper/devicemapper/data\n Metadata file: /var/lib/docker/devicemapper/devicemapper/metadata\n Data Space Used: 1092.1 Mb\n Data Space Total: 102400.0 Mb\n Metadata Space Used: 1.9 Mb\n Metadata Space Total: 2048.0 Mb\nExecution Driver: native-0.2\nKernel Version: 2.6.32-431.20.3.el6.x86_64\nThe OS:\n$ lsb_release -a\nLSB Version:    :base-4.0-amd64:base-4.0-noarch:core-4.0-amd64:core-4.0-noarch:graphics-4.0-amd64:graphics-4.0-noarch:printing-4.0-amd64:printing-4.0-noarch\nDistributor ID: RedHatEnterpriseServer\nDescription:    Red Hat Enterprise Linux Server release 6.5 (Santiago)\nRelease:        6.5\nCodename:       Santiago\n. @dmp42 happy to oblige, it gets run with the YAML configuration. Yes, the OS is RHEL 6.5 and that's what Docker is running on and where the registry is running. I also push to the internal registry from RHEL 6.5 and from my boot2docker on OS X, and that was failing completely Friday but works now o_O.\nThe configuration:\nlocal:\n  loglevel: info\n  storage: local\n  storage_path: /opt/docker-registry\n  search_backend: sqlalchemy\n  sqlalchemy_index_database: sqlite:////opt/docker-registry/db/docker-registry.db\n  standalone: true\n. No nginx yet. Nothing fancy. Using the latest docker (the binary version):\n$ docker -v\nDocker version 1.1.1, build bd609d2\nWhat's super strange is after trying to push over and over, the final successful push seemed to put the registry into a state where it's working perfectly now. I'm going to try restart the service and see if it behaves OK or not.\n. @dmp42 Restarting the service did break it. I've also now started the registry with simple docker run -p 5000:5000 registry, no search or other configuration. Here's what I get when I try to push from a RHEL box:\n$ sudo docker push repository:5000/acme/progrium-busybox\n2014/07/21 13:32:46 Error: Invalid Registry endpoint: Get http://repository:5000/v1/_ping: dial tcp <IP of registry>:5000: connection refused\nI'm watching the logs on the registry side and don't see anything. I can however:\n$ curl http://repository:5000/v1/_ping\ntrue\n$ curl http://repository:5000/v1/search\n{\"num_results\": 0, \"query\": \"\", \"results\": []}\nAnd I see those requests come in in the registry logs.\n. And like before, if I hit push enough times, it started working again, although still sporadic. Here's the gist. It has the output from the client machine trying to push and the docker registry logs.\n. @dmp42 yes, updated the comment, it is the IP of the box that's running the Docker Registry. It maps to repository via our internal DNS.\n. @dmp42 well... I think I've finally uncovered the issue. It looks like the underlying IP address of the box changed and the DNS was update, but the Docker daemon didn't pick it up. So, at the end of the day, @shin- was right, it's exactly related to dotcloud/docker#2938. Gah!\n. No problem, I'm glad I could help. Hope that gets patched in Docker soon.\n. So, just to beat a dead horse, it turns out that the whole problem I had this whole time was because someone screwed up the DNS entry for the server that was hosting the Docker Registry and had it point to two different servers (one Linux, one Windows). This perfectly explains the inconsistent behaviour where it would sometimes work and sometimes wouldn't (as the DNS server load balanced). So sorry to waste everyone's time :disappointed:\n. I'm still seeing this in 0.7.3..\n. ",
    "serbrech": "I experienced the same thing on OS X Yosemite. \nIt was failing with  \nFATA[0032] Post https://<IP address>:2376/v1.16/images/my-image/push?tag=: dial tcp <IP address>:2376: i/o timeout\nRunning boot2docker restart did the trick.\n. ",
    "nickdgriffin": "@SamSaffron I've just come across a similar issue where /etc/resolv.conf container two nameserver entries where the first does not exist - docker gives an i/o timeout as it doesn't appear to consider the second nameserver at all, yet curl will. If the first/only nameserver in /etc/resolv.conf is the correct working DNS server then docker works just fine. I'm wondering if this is another netgo issue, but I can't find any related issues for it and it's probably worth testing against Go 1.5 anyway.\n. ",
    "jalaziz": "Closing since it appears the documentation has been updated.\n. @dmp42 Will do! I'm just closing this for now until I have time to go through the documentation again and make sure everything works from scratch now.\nI believe some of the issues I brought up are still valid, but don't want to waste anybody's time until I can confirm that's the case.\n. ",
    "retrohacker": "@wernerb https://registry.hub.docker.com/u/wblankenship/dockeri.co/\nThis is a start.\n. @freewil Glad to talk it through. Will open an issue on https://github.com/wblankenship/dockeri.co/issues\n. ",
    "freewil": "@wblankenship :sunglasses: Feature request: ~~SSL~~ TLS support for serving images\n. ",
    "wesgurn": "Check IP tables to confirm it isn't getting blocked there.\n. ",
    "Telmo": "IPTables is turned off\nHere is my nginx conf: https://gist.github.com/Telmo/9765398\n. any chance of pushing the image to docker index? .0.7.0 is broken with this bug for local storage.\n. ",
    "shannon": "I am getting this same issue\nSending image list\nPushing repository localhost:80/modit/docker-registry-nginx(1 tags)\n511136ea3c5a: Pushing \n2014/03/29 01:21:14 HTTP code 401 while uploading metadata: invalid character '<' looking for beginning of value\nI am trying to build a single container that runs both the registry and nginx. That way I can just volume in the necessary configs and have a pretty portable setup.\nHere is my Setup\nDockerfile\n```\nFROM samalba/docker-registry:latest\nRUN echo \"deb http://archive.ubuntu.com/ubuntu precise main universe\" > /etc/apt/sources.list\nRUN apt-get update\nRUN apt-get upgrade -y\nRUN apt-get install -y nginx supervisor\nRUN mkdir -p /var/log/supervisor\nADD supervisord.conf /etc/supervisor/conf.d/supervisord.conf\nEXPOSE 80 443\nCMD [\"/usr/bin/supervisord\"]\n```\nsupervisord.conf\n```\n[supervisord]\nnodaemon=true\n[program:registry]\ncommand=/docker-registry/run.sh\n[program:nginx]\ncommand=/usr/sbin/nginx\n```\nnginx.conf (/etc/nginx/sites-enabled/docker-registry)\n```\nupstream docker-registry {\n  server localhost:5000;\n}\nserver {\n  listen 80;\n  server_name registry.mod.it;\n#  ssl on;\n  #  ssl_certificate /etc/ssl/certs/docker-registry.crt;\n  #  ssl_certificate_key /etc/ssl/private/docker-registry.key;\nproxy_set_header Host $http_host; # required for docker client's sake\n  proxy_set_header X-Real-IP $remote_addr; # pass on real client's IP\n  proxy_set_header Authorization \"\"; # see https://github.com/dotcloud/docker-registry/issues/170\nclient_max_body_size 0; # disable any limits to avoid HTTP 413 for large image uploads\nlocation / {\n    auth_basic \"Restricted\";\n    auth_basic_user_file htpasswd/docker-registry;\nproxy_pass http://docker-registry;\nproxy_set_header Host $host;\nproxy_read_timeout 900;\n\n}\nlocation /_ping {\n    auth_basic off;\n    proxy_pass http://docker-registry;\n  }\nlocation /v1/_ping {\n    auth_basic off;\n    proxy_pass http://docker-registry;\n  }\n}\n```\nThen my run command is just putting the volumes/configs in the right place\ndocker run -d --name drn -p 80:80 -v /root/conf/htpasswd:/etc/nginx/htpasswd/:ro -v /root/conf/sites-enabled:/etc/nginx/sites-enabled/:ro modit/docker-registry-nginx\nEverything seems to be working ok. If I login using docker login localhost:80 I get login succeeded. A bad password get the expected result too. But when I try to push I get the above error. If I expose port 5000 and push directly to the registry bypassing nginx, all works great. So I think somewhere between nginx and the registry is messed up\n. I can confirm that we are using HTTP. The reason is the registry is on an internal network but we are building a PaaS system so we don't want user containers to be able to access the registry. I would prefer to just use an RSA key but I can't seem to find any example of how to do this from the client.\n. ",
    "WonderBeat": "Same issue here.\nLooks like I found the reason of this issue:\nHere is a part of network dump for\ndocker push domain.domain/image\n```\nPUT /v1/repositories/busybox/ HTTP/1.1.\nHost: docker.domain.net.\nUser-Agent: docker/0.9.1 go/go1.2.1 git-commit/3600720 kernel/3.8.0-29-generic os/linux arch/amd64 .\nContent-Length: 312.\nAuthorization: Basic d2hhdSomePass==.\nContent-Type: application/json.\nX-Docker-Token: true.\nAccept-Encoding: gzip.\n.....\nHTTP/1.1 200 OK.\nServer: nginx/1.1.19.\nDate: Mon, 31 Mar 2014 13:09:07 GMT.\nContent-Type: application/json.\nContent-Length: 2.\nConnection: keep-alive.\nX-Docker-Token: Token signature=YSIGNATUREV,repository=\"library/busybox\",access=write.\nX-Docker-Endpoints: docker.domain.net.\nPragma: no-cache.\nCache-Control: no-cache.\nExpires: -1.\nWWW-Authenticate: Token signature=YSIGNATUREV,repository=\"library/busybox\",access=write.\nX-Docker-Registry-Version: 0.6.5.\nX-Docker-Registry-Config: dev.\n.......\nGET /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1.\nHost: docker.domain.net.\nUser-Agent: docker/0.9.1 go/go1.2.1 git-commit/3600720 kernel/3.8.0-29-generic os/linux arch/amd64 .\nAuthorization: Token Token signature=YSIGNATUREV,repository=\"library/busybox\",access=write.\nAccept-Encoding: gzip.\n.......\nHTTP/1.1 401 Unauthorized.\nServer: nginx/1.1.19.\nDate: Mon, 31 Mar 2014 13:30:43 GMT.\nContent-Type: text/html.\nContent-Length: 195.\nConnection: keep-alive.\nWWW-Authenticate: Basic realm=\"Restricted\".\n```\nNginx will block last request with \"no user/password was provided for basic authentication\". Yes, no basic-auth header.\nBut, according\nhttp://docs.docker.io/en/latest/reference/api/registry_index_spec/\nWWW-Authenticate header content should replace Authorization header content.\nAs a result\nHTTP code 401 while uploading metadata: invalid character '<' looking for beginning of value\n. disable_token_auth config parameter doesn't help here.\n. Investigation continues ;)\nAs I found, this is a \"LookupImage\" request:\nGET /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json\nHere this code for docker client\nhttps://github.com/dotcloud/docker/blob/e13b4fd869c4f6065267acd99957fcec33349212/registry/registry.go#L197-L202\nreq, err := r.reqFactory.NewRequest(\"GET\", registry+\"images/\"+imgID+\"/json\", nil)\nif err != nil {\nutils.Errorf(\"Error in LookupRemoteImage %s\", err)\nreturn false\n}\nsetTokenAuth(req, token)\nIt always uses TokenAuth.\nOther method are able to use BasicAuth, but not whis one. Check code!\nSame issue for \"GetRemoteHistory\".\nGuys, we need to update client!\n. ",
    "tuhrig": "I have the same problem. I run the registry behind nginx on AWS and get the same invalid character '<' looking for beginning of value error. When I push directly to the registry, everything works. Any fix for this yet?\n. ",
    "ahazem": "Same issue over here. Running docker-registry 0.6.6 and docker 0.9.1.\n. I can confirm that I was using HTTP as well. Using SSL eliminates the issue. Great job by the way :)\n. ",
    "marcusramberg": "I just hit this issue too.\n. @samalba tested now from scratch:\ngit clone git@github.com:ehazlett/docker-py-helloworld.git\ncd docker-py-helloworld/\ndocker build -t registry.nordaaker.com/hello .\n.....\ndocker push registry.nordaaker.com/hello\nThe push refers to a repository [registry.nordaaker.com/hello] (len: 1)\nSending image list\nPushing repository registry.nordaaker.com/hello (1 tags)\n8dbd9e392a96: Image successfully pushed\n9ea990fcb321: Image successfully pushed\n403304db1bed: Image successfully pushed\ne04ea56e596e: Image successfully pushed\n1737690344b3: Pushing [==================================================>] 317.1 MB/317.1 MB\nthen on another host \nmarcus@cloudia:~$ sudo docker pull registry.nordaaker.com/hello\nPulling repository registry.nordaaker.com/hello\n242172258e6f: Error pulling image (latest) from registry.nordaaker.com/hello, Server error: 404 trying to fetch remote history for 242172258e6f899e7198d467182014/04/09 21:08:24 Could not find repository on any of the indexed registries.ee139\n. 2014-04-09 19:05:28,983 INFO: 172.17.42.1 - - [09/Apr/2014:19:05:28] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [09/Apr/2014:19:05:29] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-04-09 19:05:29,106 INFO: 172.17.42.1 - - [09/Apr/2014:19:05:29] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [09/Apr/2014:19:05:29] \"PUT /v1/repositories/hello/ HTTP/1.1\" 200 2 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:05:29,201 INFO: 172.17.42.1 - - [09/Apr/2014:19:05:29] \"PUT /v1/repositories/hello/ HTTP/1.1\" 200 2 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:05:29] \"GET /v1/images/8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c/json HTTP/1.1\" 404 28 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:05:29,276 INFO: 172.17.42.1 - - [09/Apr/2014:19:05:29] \"GET /v1/images/8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c/json HTTP/1.1\" 404 28 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:05:29] \"PUT /v1/images/8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c/json HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:05:29,350 INFO: 172.17.42.1 - - [09/Apr/2014:19:05:29] \"PUT /v1/images/8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c/json HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:06:10] \"PUT /v1/images/8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c/layer HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:06:10,478 INFO: 172.17.42.1 - - [09/Apr/2014:19:06:10] \"PUT /v1/images/8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c/layer HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:06:10] \"PUT /v1/images/8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c/checksum HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:06:10,732 INFO: 172.17.42.1 - - [09/Apr/2014:19:06:10] \"PUT /v1/images/8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c/checksum HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:06:11] \"GET /v1/images/9ea990fcb3218bf3ac77e56459f757e1b398e553a9e267e2306adaee55752f0d/json HTTP/1.1\" 404 28 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:06:11,105 INFO: 172.17.42.1 - - [09/Apr/2014:19:06:11] \"GET /v1/images/9ea990fcb3218bf3ac77e56459f757e1b398e553a9e267e2306adaee55752f0d/json HTTP/1.1\" 404 28 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:06:11] \"PUT /v1/images/9ea990fcb3218bf3ac77e56459f757e1b398e553a9e267e2306adaee55752f0d/json HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:06:11,219 INFO: 172.17.42.1 - - [09/Apr/2014:19:06:11] \"PUT /v1/images/9ea990fcb3218bf3ac77e56459f757e1b398e553a9e267e2306adaee55752f0d/json HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:06:11] \"PUT /v1/images/9ea990fcb3218bf3ac77e56459f757e1b398e553a9e267e2306adaee55752f0d/layer HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:06:11,305 INFO: 172.17.42.1 - - [09/Apr/2014:19:06:11] \"PUT /v1/images/9ea990fcb3218bf3ac77e56459f757e1b398e553a9e267e2306adaee55752f0d/layer HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:06:11] \"PUT /v1/images/9ea990fcb3218bf3ac77e56459f757e1b398e553a9e267e2306adaee55752f0d/checksum HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:06:11,387 INFO: 172.17.42.1 - - [09/Apr/2014:19:06:11] \"PUT /v1/images/9ea990fcb3218bf3ac77e56459f757e1b398e553a9e267e2306adaee55752f0d/checksum HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:06:11] \"GET /v1/images/403304db1bed4abe93e9c28e124d2c60740d2c97d1d81df2f012785abe01b272/json HTTP/1.1\" 404 28 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:06:11,454 INFO: 172.17.42.1 - - [09/Apr/2014:19:06:11] \"GET /v1/images/403304db1bed4abe93e9c28e124d2c60740d2c97d1d81df2f012785abe01b272/json HTTP/1.1\" 404 28 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:06:11] \"PUT /v1/images/403304db1bed4abe93e9c28e124d2c60740d2c97d1d81df2f012785abe01b272/json HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:06:11,600 INFO: 172.17.42.1 - - [09/Apr/2014:19:06:11] \"PUT /v1/images/403304db1bed4abe93e9c28e124d2c60740d2c97d1d81df2f012785abe01b272/json HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:06:11] \"PUT /v1/images/403304db1bed4abe93e9c28e124d2c60740d2c97d1d81df2f012785abe01b272/layer HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:06:11,744 INFO: 172.17.42.1 - - [09/Apr/2014:19:06:11] \"PUT /v1/images/403304db1bed4abe93e9c28e124d2c60740d2c97d1d81df2f012785abe01b272/layer HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:06:11] \"PUT /v1/images/403304db1bed4abe93e9c28e124d2c60740d2c97d1d81df2f012785abe01b272/checksum HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:06:11,844 INFO: 172.17.42.1 - - [09/Apr/2014:19:06:11] \"PUT /v1/images/403304db1bed4abe93e9c28e124d2c60740d2c97d1d81df2f012785abe01b272/checksum HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:06:12] \"GET /v1/images/e04ea56e596ee2d95b61da7ee41277b30309e8e12a125411920458a44650b2fb/json HTTP/1.1\" 404 28 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:06:12,887 INFO: 172.17.42.1 - - [09/Apr/2014:19:06:12] \"GET /v1/images/e04ea56e596ee2d95b61da7ee41277b30309e8e12a125411920458a44650b2fb/json HTTP/1.1\" 404 28 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:06:12] \"PUT /v1/images/e04ea56e596ee2d95b61da7ee41277b30309e8e12a125411920458a44650b2fb/json HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:06:12,974 INFO: 172.17.42.1 - - [09/Apr/2014:19:06:12] \"PUT /v1/images/e04ea56e596ee2d95b61da7ee41277b30309e8e12a125411920458a44650b2fb/json HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:06:47] \"PUT /v1/images/e04ea56e596ee2d95b61da7ee41277b30309e8e12a125411920458a44650b2fb/layer HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:06:47,281 INFO: 172.17.42.1 - - [09/Apr/2014:19:06:47] \"PUT /v1/images/e04ea56e596ee2d95b61da7ee41277b30309e8e12a125411920458a44650b2fb/layer HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:06:47] \"PUT /v1/images/e04ea56e596ee2d95b61da7ee41277b30309e8e12a125411920458a44650b2fb/checksum HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:06:47,421 INFO: 172.17.42.1 - - [09/Apr/2014:19:06:47] \"PUT /v1/images/e04ea56e596ee2d95b61da7ee41277b30309e8e12a125411920458a44650b2fb/checksum HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:06:47] \"GET /v1/images/1737690344b315313658f0c26519ff89485d134643396f8e3becc53a4b4b41d5/json HTTP/1.1\" 404 28 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:06:47,764 INFO: 172.17.42.1 - - [09/Apr/2014:19:06:47] \"GET /v1/images/1737690344b315313658f0c26519ff89485d134643396f8e3becc53a4b4b41d5/json HTTP/1.1\" 404 28 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:06:47] \"PUT /v1/images/1737690344b315313658f0c26519ff89485d134643396f8e3becc53a4b4b41d5/json HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:06:47,853 INFO: 172.17.42.1 - - [09/Apr/2014:19:06:47] \"PUT /v1/images/1737690344b315313658f0c26519ff89485d134643396f8e3becc53a4b4b41d5/json HTTP/1.1\" 200 4 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:08:23] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-04-09 19:08:23,705 INFO: 172.17.42.1 - - [09/Apr/2014:19:08:23] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [09/Apr/2014:19:08:23] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-04-09 19:08:23,809 INFO: 172.17.42.1 - - [09/Apr/2014:19:08:23] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [09/Apr/2014:19:08:23] \"GET /v1/repositories/hello/images HTTP/1.1\" 200 756 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:08:23,911 INFO: 172.17.42.1 - - [09/Apr/2014:19:08:23] \"GET /v1/repositories/hello/images HTTP/1.1\" 200 756 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:08:23] \"GET /v1/repositories/library/hello/tags HTTP/1.1\" 200 2 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:08:23,986 INFO: 172.17.42.1 - - [09/Apr/2014:19:08:23] \"GET /v1/repositories/library/hello/tags HTTP/1.1\" 200 2 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n172.17.42.1 - - [09/Apr/2014:19:08:24] \"GET /v1/images/242172258e6f899e7198d467184fb8765e979f6c3ab7b33ee8d76fb1f76ee139/ancestry HTTP/1.1\" 404 28 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n2014-04-09 19:08:24,058 INFO: 172.17.42.1 - - [09/Apr/2014:19:08:24] \"GET /v1/images/242172258e6f899e7198d467184fb8765e979f6c3ab7b33ee8d76fb1f76ee139/ancestry HTTP/1.1\" 404 28 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-26-generic os/linux arch/amd64\"\n. /images/242172258e6f899e7198d467184fb8765e979f6c3ab7b33ee8d76fb1f76ee139 does not even exist:\nudo ls -l /registry/images/\ntotal 24\ndrwxr-xr-x 2 root root 4096 Apr  9 21:06 1737690344b315313658f0c26519ff89485d134643396f8e3becc53a4b4b41d5\ndrwxr-xr-x 2 root root 4096 Apr  9 00:28 27cf784147099545\ndrwxr-xr-x 2 root root 4096 Apr  9 21:06 403304db1bed4abe93e9c28e124d2c60740d2c97d1d81df2f012785abe01b272\ndrwxr-xr-x 2 root root 4096 Apr  9 21:06 8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c\ndrwxr-xr-x 2 root root 4096 Apr  9 21:06 9ea990fcb3218bf3ac77e56459f757e1b398e553a9e267e2306adaee55752f0d\ndrwxr-xr-x 2 root root 4096 Apr  9 21:06 e04ea56e596ee2d95b61da7ee41277b30309e8e12a125411920458a44650b2fb\n. Sure, just tell me if you need anything else from my setup to debug.\n. I just get {} \n- does that mean I haven't pushed any tags? \n. ",
    "IreneKnapp": "For the record, I had hit the issue over what seemed to be https.  I was using a self-signed certificate, but I had added it to the system roots on all relevant machines.\nFrom the Docker daemon's server logs, I noticed that it falls back to http silently (which is pretty wtf, honestly) if it can't connect via https, which can happen for any number of proxy-configuration problems.\nSo, I finally figured out from the vague messages that the cert wasn't properly added in one place, fixed that, and it's all working for me now.  If you have come to this bug report wondering why it isn't working for you, it could be anything - check your full configuration carefully.\n. ",
    "sinchb": "I have the similar problem, but 404... I run the registry on nginx with ssl and basic auth:\n```\nsudo docker login https://my.regirstry.dom.com:5000\nUsername: xxx\nPassword: xxx\nEmail: xxx\nLogin Succeeded\nsudo docker push my.registry.dom.com:5000/test-images\nThe push refers to a repository [d.quatanium.com:5000/test-image] (len: 1)\nSending image list\nPushing repository my.registry.dom.com:5000/test-image (1 tags)\n511136ea3c5a: Pushing \n2014/11/17 20:17:43 HTTP code 404 while uploading metadata: invalid character '<' looking for beginning of value\n```\nIs the same problem here?\nmore infomation:\n1. use docker registry container, latest\n2. local storage\n3.\nsudo docker run -d -v /data --name registry_data ubuntu:14.04 echo Data-only container for registry\nsudo docker run --volumes-from registry_data \\\n    --restart=always \\\n    --name registry \\\n    -e SETTINGS_FLAVOR=local \\\n    -e STORAGE_PATH=/data \\\n    -e SEARCH_BACKEND=sqlalchemy \\\n    -e LOGLEVEL=DEBUG \\\n    -e GUNICORN_OPTS=[--preload] \\\n    -e DEBUG=true \\\n    -p 127.0.0.1:5001:5000 registry\n4.\nqhome@vm:~$ curl https://d.quatanium.com:5000/_ping \n{\"host\": [\"Linux\", \"76aa8c4b4bb6\", \"3.13.0-40-generic\", \"#69-Ubuntu SMP Thu Nov 13 17:53:56 UTC 2014\", \"x86_64\", \"x86_64\"], \"launch\": [\"/usr/local/bin/gunicorn\", \"--access-logfile\", \"-\", \"--error-logfile\", \"-\", \"--max-requests\", \"100\", \"-k\", \"gevent\", \"--graceful-timeout\", \"3600\", \"-t\", \"3600\", \"-w\", \"4\", \"-b\", \"0.0.0.0:5000\", \"--reload\", \"--preload\", \"docker_registry.wsgi:application\"], \"versions\": {\"SocketServer\": \"0.4\", \"argparse\": \"1.1\", \"backports.lzma\": \"0.0.3\", \"blinker\": \"1.3\", \"cPickle\": \"1.71\", \"cgi\": \"2.6\", \"ctypes\": \"1.1.0\", \"decimal\": \"1.70\", \"distutils\": \"2.7.6\", \"docker_registry.app\": \"0.9.0\", \"docker_registry.core\": \"2.0.3\", \"docker_registry.server\": \"0.9.0\", \"email\": \"4.0.3\", \"flask\": \"0.10.1\", \"gevent\": \"1.0.1\", \"greenlet\": \"0.4.5\", \"gunicorn\": \"19.1.0\", \"gunicorn.arbiter\": \"19.1.0\", \"gunicorn.config\": \"19.1.0\", \"gunicorn.six\": \"1.2.0\", \"jinja2\": \"2.7.3\", \"json\": \"2.0.9\", \"logging\": \"0.5.1.2\", \"parser\": \"0.5\", \"pickle\": \"$Revision: 72223 $\", \"platform\": \"1.0.7\", \"python\": \"2.7.6 (default, Mar 22 2014, 22:59:56) \\n[GCC 4.8.2]\", \"re\": \"2.2.1\", \"redis\": \"2.10.3\", \"requests\": \"2.3.0\", \"requests.packages.chardet\": \"2.2.1\", \"requests.packages.urllib3\": \"dev\", \"requests.packages.urllib3.packages.six\": \"1.2.0\", \"requests.utils\": \"2.3.0\", \"rsa\": \"3.1.4\", \"simplejson\": \"3.6.2\", \"sqlalchemy\": \"0.9.4\", \"tarfile\": \"$Revision: 85213 $\", \"urllib\": \"1.17\", \"urllib2\": \"2.7\", \"werkzeug\": \"0.9.6\", \"yaml\": \"3.11\", \"zlib\": \"1.0\"}}\n5.nginx config\n```\nupstream docker-registry {\n server 127.0.0.1:5001;\n}\nserver {\n    listen *:5000;\n    server_name xxx;\naccess_log  /var/log/nginx/registry_access.log;\nerror_log   /var/log/nginx/registry_error.log;\n\nssl on; \nssl_certificate  /etc/nginx/sites-cert/xxx-unified.crt;\nssl_certificate_key  /etc/nginx/sites-cert/xxx.key;\n\nproxy_set_header HOST           $http_host;\nproxy_set_header X-Real-IP      $remote_addr;\nproxy_set_header Authorization  \"\";\n\n\nclient_max_body_size 0; # disable any limits to avoid HTTP 413 for large image uploads\n\n# required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\n#chunkin on; \n#chunked_transfer_encoding on;\n\nlocation / { \n     # let Nginx know about our auth file\n     auth_basic              \"Restricted\";\n     auth_basic_user_file    /etc/nginx/docker-registry.htpasswd;\nproxy_pass http://docker-registry;\nproxy_set_header Host $host;\nproxy_read_timeout 900;\n\n}\n location /_ping {\n     auth_basic off;\n     proxy_pass http://docker-registry;\n } \n location /v1/_ping {\n     auth_basic off;\n     proxy_pass http://docker-registry;\n }\n}\n```\n. Forget it\u2026I solved it\u2026\n. @ssalvatori @rchaudha \nThis is my nginx config. It works well:\n```\nFILE: registry.nginx\nupstream docker-registry {\n    server localhost:5001;\n}\nserver {\n    listen *:5000;\n    server_name www.example.com;\nssl on; \nssl_certificate  /etc/nginx/sites-cert/example.crt;\nssl_certificate_key  /etc/nginx/sites-cert/example.key;\n\nclient_max_body_size 0; # disable any limits to avoid HTTP 413 for large image uploads\n\n# required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\nchunked_transfer_encoding on;\n\nlocation / { \n    # let Nginx know about our auth file\n    auth_basic              \"Restricted\";\n    auth_basic_user_file    /etc/nginx/docker-registry.htpasswd;\n    include /etc/nginx/docker-registry.conf;\n}   \nlocation /_ping {\n    auth_basic off;\n    include /etc/nginx/docker-registry.conf;\n}   \nlocation /v1/_ping {\n    auth_basic off;\n    include /etc/nginx/docker-registry.conf;\n}\n\n}\n```\nFILE: docker-registry.conf\nproxy_pass                       http://docker-registry;\nproxy_set_header  Host           $http_host;   # required for docker client's sake\nproxy_set_header  X-Real-IP      $remote_addr; # pass on real client's IP\nproxy_set_header  Authorization  \"\"; # see https://github.com/dotcloud/docker-registry/issues/170\nproxy_read_timeout               900;\n. ",
    "rchaudha": "sinchb: what was the issue?\n. Guys any help here?\n. Have been looking thru the registry code.  Haven't upgraded to the latest registry yet.  My setup is actually little complex.  I have the registries writing to the NFS filer and I think there might be some kind of race condition in the registries being able to see the data on the NFS filer between writing the layer and the metadata.  Same thing sometimes between writing json and layer too.\n. It is using a NFS filer. Though this is the first time I have seen this issue.\n. Nope this was just one docker build and one docker push command running.\n. Using docker 1.3 with registry 0.8.1.\nRohit\nOn Dec 5, 2014 1:27 PM, \"Olivier Gambier\" notifications@github.com wrote:\n\nWhat version of the registry? What version of docker? (also see\nhttps://github.com/docker/docker-registry/blob/master/DEBUGGING.md#basics)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/824#issuecomment-65857170\n.\n. \n",
    "ssalvatori": "sinchb: +1 what was the issue?\n. Hi\nThis is how we're deal with push using apache 2.2 + ldap (centos6) insted of nginx\n```\n\n        AuthLDAPURL 'ldap://xxx.yyy.zzz o=example,dc=com?uid?sub?(objectClass=*)'\n\n    <Location />\n            AuthName \"Registry Authentication\"\n            AuthType basic\n\n            AuthBasicProvider ldap_config\n            AuthzLDAPAuthoritative on\n\n            <Limit PUT>\n                    AuthLDAPURL \"ldap://xxx.yyy.zzz o=example,dc=com?uid?sub?(objectClass=*)\"\n                    AuthLDAPGroupAttributeIsDN on\n                    AuthLDAPGroupAttribute member\n                    AuthLDAPRemoteUserAttribute uid\n                    AuthLDAPRemoteUserIsDN on\n                    Require ldap-group cn=admins,ou=metagroups,o=example,dc=com\n            </Limit>\n\n            <LimitExcept PUT>\n                    Require valid-user\n            </LimitExcept>\n\n    </Location>\n    <Location /v1/_ping>\n            Satisfy any\n            Allow from all\n    </Location>\n\n    <Location /_ping>\n            Satisfy any\n            Allow from all\n    </Location>\n\n```\nNow I'm trying to add a new feature , a user can only push a img to it's own repository /v1/repositories/\n. ",
    "cdwertmann": "I've had this issue when using mirroring in the docker registry container. Mirroring and basic auth do not work together, so turning off mirroring fixed the issue for me. See\nhttps://github.com/docker/docker-registry/issues/576#issuecomment-56017584\n. ",
    "mboersma": "This problem occurs constantly when uploading to Docker Hub itself.\nHTTP code 500 while uploading metadata: invalid character '<' looking for beginning of value\nWe are using Docker v1.5.0 which should contain docker/docker#5083.\n. > I am running into both the 500 invalid character < and 404 invalid character in my case a p character\nWe are also getting that error:\n773995669d83: Pushing\ntime=\"2015-07-07T14:36:19-06:00\" level=\"fatal\" msg=\"HTTP code 404 while uploading metadata: invalid character 'p' after top-level value\"\nSomething changed in Docker Hub's configuration on Sunday morning; that's when this all broke.\n. I have also filed support tickets for these issues.\n. @kvashishta is this with Docker 1.8.3? I see a similar issue pushing to a private V1 registry, but I think it doesn't occur with Docker 1.7.1. See deis/deis#4671.\n. ",
    "gopal-gemini": "its fixed for me after changed the following value in nginx conf.\nerror : \nHTTP code 413 while uploading metadata: \"invalid character '<' looking for beginning of value\"\nfix:\nclient_max_body_size 0;\n. ",
    "hufman": "Ok, now the s3 and gcs backends use a common set of boto_ settings, and can override with s3_secure or gs_secure. Documentation has been updated to reflect this change.\n. It does, but it provides a simpler implementation that does not require a database backend.\n. That would be nice, indeed! I didn't see that PR when I did this, and it seemed small enough.\nWhen it gets merged in, I'll look into using it! Even if the client doesn't support it, it would be easy enough to curl it manually.\n. ",
    "brosander": "For this to work with the local filesystem, I had to tweak the path a little bit (remove trailing slash instead of adding it, look 1 spot after path for slash):\nhttps://github.com/brosander/docker-registry/commit/b9e1e22ef78da94f42d329b918446c38b4a03092\n. ",
    "andreaslundahl": "I just realized that this probably belongs in dotcloud/docker. I just want confirmation on that I'm not missing something obvious before I close this one and open a new one over there.\n. Of course. It's too late for this. Thank you. :+1:\n. ",
    "srid": "\nSince you change the way tags are updated, \n\nthis has not changed. the 'tags' file is created as an extra. the original tag_* files continue to be read/updated as before.\n\nthat's tricky to roll-out on Registries with existing dataset.\n\nthere is the rename of '_index_images' to 'images. to support existing data set, i suppose i can change this to make 'images' a copy of '_index_images.\nwould this achieve backward compatibility?\n. ",
    "vbatts": "cross-posted to https://github.com/dotcloud/docker/issues/5055\n. :-)\nOn Sep 16, 2014 5:31 PM, \"Olivier Gambier\" notifications@github.com wrote:\n\n/me thinks that @jlhawn https://github.com/jlhawn deserves a medal for\nkilling that one :-)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/310#issuecomment-55816804\n.\n. @dmp42 I have been trying to get a monkey patch working for this, but being that it is in a separate module, many of the techniques i'm aware of, do not succeed in overriding that _proc_pax (.e.g. setting global TarInfo resolver, or tarfile.TarInfo.bases (which causes an inheretance loop)). Ideas of how to do this?\n\nI would love to add tests.  I could not get tox to set up correctly. Long, nasty gcc failures on installing dependencies for the environment. I'm hoping to ensure TarSum tests line up with testing for the golang implementation too, so we can ensure they move in lock step.\n. @dmp42 ok. I'll rerun, and grab a log of the output. This is on slackware64-current (gcc 4.8.2, glibc 2.19, pysetuptools 3.1, tox 1.7.0).\nAlso, I'm presently building out an ubuntu:14.04 container to use in as a build env for this.\n. @dmp42 here is the tox output http://fpaste.org/103826/40068444/  I did rm -rf .tox though it looks to have begun the tests this time... no idea. I've never touched tox before. \n. @dmp42 things are looking much better. though now I need to get tests added. And I'd like to get the same testdata added, that is in the docker tarsum tests.\n. While I want the best implementation, I'm not sure that monkey patching is\ngoing suite this. I emailed some efforts to @dmp42 hoping that we could\ncome up with a minimal, less fork-ish way.\nOn May 21, 2014 8:04 PM, \"Sam Alba\" notifications@github.com wrote:\n\nI agree with @dmp42 https://github.com/dmp42 here. I'd rather not ship\na fork of tarfile. Can we block this PR before removing the tarfile fork?\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/pull/381#issuecomment-43832331\n.\n. Fair. I'm on the road right now and won't be able to work on this til later\nOn May 22, 2014 8:24 AM, \"Mangled Deutz\" notifications@github.com wrote:\n@vbatts https://github.com/vbatts unless there is a specific docker\nagenda / time constraints here, this patch is not going to make it in for\nthe 0.7 release, due in a couple of days. It will have to wait for a later\nrelease.\nJust managing expectations :-)\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/pull/381#issuecomment-43880826\n.\n. @dmp42 i've rebased to master (to get your testing rearrangements) and now monkey patched instead of vendored. \n\nNext I'm going to add tests.\n. @dmp42 plz2review. unit tests, nose tests are OK.\n. ... but not flake8\n/me addresses that ...\n. @dmp42 thoughts on the E501 long lines? it's more readable now, than it would be if I split strings up ...\n. Some of the keys and values exceed the limit by themselves.\nOn Jun 4, 2014 6:40 PM, \"Mangled Deutz\" notifications@github.com wrote:\n\n@vbatts https://github.com/vbatts I think you might not need to split\nstrings up - just the key / values on different lines.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/dotcloud/docker-registry/pull/381#issuecomment-45161352\n.\n. K. Flake8 is clean.\nOn Jun 4, 2014 7:18 PM, \"Mangled Deutz\" notifications@github.com wrote:\nTry to # noqa them.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/dotcloud/docker-registry/pull/381#issuecomment-45164423\n.\n. Hrm. flake8 was clean on my laptop, but apparently not on Travis :-/\nOn Jun 4, 2014 10:38 PM, \"Vincent Batts\" vbatts@hashbangbash.com wrote:\nK. Flake8 is clean.\nOn Jun 4, 2014 7:18 PM, \"Mangled Deutz\" notifications@github.com wrote:\n\nTry to # noqa them.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/dotcloud/docker-registry/pull/381#issuecomment-45164423\n.\n. @dmp42 one more try\n. But that would require exporting all of tarfile in xtarfile. I don't think\nwe're game for that.\nOn Jun 6, 2014 10:35 AM, \"W. Trevor King\" notifications@github.com wrote:\n\nYou could be less invasive with:\nimport xtarfile as tarfile\nThat would allow you to drop the other tarfile \u2192 xtarfile changes.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/dotcloud/docker-registry/pull/381#issuecomment-45343278\n.\n. @dmp42 fixed the asserts, and added a test for vanilla tarfile. I didn't add a python version check, because it's still an issue in the latest python.\n\n@wking i like the from .lib.xtarfile import tarfile better too. I to that.\n. @dmp42 ping\n. @dmp42 @wking done\n. @wking I hated to see that in place, and feel like it could be reverted. Regardless, having the TarSum be consistent for the golang and python implementation is important. \nBesides, that checksum is only for in flight. Once a layer is pulled down by docker, that tar is discarded. TarSum is the best way to have fixed-time consistent checksum of the layer. not the in-flight checksum that docker-registry opted for with #300 \n. I saw that part of the conversation, and part of what a detached signature would be signing would include the TarSum ... so you'll still need a way to validate that sum.\n. @dmp42 surely\n. relevant https://github.com/docker/docker/issues/7526\n. Correct. Will need tarsum versioning everywhere that tarsum is calculated\nor validated.\nOn Aug 29, 2014 7:24 AM, \"Eric Windisch\" notifications@github.com wrote:\n\nIf the tarsum calculation in the registry is only performed for Docker\n<0.10, then supporting xattrs here will NOT be consistent with the engine\nas <0.10 won't support xattrs.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/pull/384#issuecomment-53881453\n.\n. @dmp42 so the v1 registry will never handle tarsum versions?\n. @dmp42 sorry. Not that. I'm referring to support of TarSum versioning https://github.com/docker/docker/blob/master/pkg/tarsum/versioning.go\n. yes!\n\nperhaps this will be a thing? or just a noop/deprecation?\nOn Wed, Oct 29, 2014 at 2:59 PM, Olivier Gambier notifications@github.com\nwrote:\n\nSo, you mean: how will it work for future docker engines using versioned\ntarsum to talk to v1 registries? <- right?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/pull/384#issuecomment-60984114\n.\n. That does look cleaner, but results in \n\n./docker_registry/images.py:26:1: F811 redefinition of unused 'tarfile' from line 6\n./docker_registry/lib/layers.py:17:1: F811 redefinition of unused 'tarfile' from line 3\n. nope. because the xattr security capabilities are little endian packed unsigned longs, which are not utf8. If it fails to make it unicode, it should pass through the raw bytes.\n. OHMAN\n. Oh oh oh.\n/me derps\nI'll fix that.\nOn Jun 2, 2014 11:30 AM, \"Mangled Deutz\" notifications@github.com wrote:\n\nIn docker_registry/lib/xtarfile.py:\n\n\n\nthe newline. keyword and value are both UTF-8 encoded strings.\n\nregex = re.compile(r\"(\\d+) ([^=]+)=\", re.U)\npos = 0\nwhile True:\nmatch = regex.match(buf, pos)\nif not match:\nbreak\n  +\nlength, keyword = match.groups()\nlength = int(length)\nvalue = buf[match.end(2) + 1:match.start(1) + length - 1]\n  +\ntry:\nkeyword = keyword.decode(\"utf8\")\nexcept Exception:\nkeyword = keyword\n\n\nHear hear (about raw-bytes and unicode).\nMy point is keyword = keyword does absolutely nothing :-) - and if decode\nfails, you will get the raw bytes no matter what.\nSuggestion:\ntry:\nkeyword = keyword.decode(\"utf8\")\nexcept:\npass\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/dotcloud/docker-registry/pull/381/files#r13290266.\n. I saw the complaints, but when the lines wrapped, it had SyntaxError's without the parenthesis :-|\nI'll see what I can do.\n. \n",
    "j0hnsmith": "I'm seeing this as well.\n. ",
    "opszunzun": "I got this issue a while ago and it turned out to be request size limits on Nginx, which was proxying requests to the docker registry container.... \n. ",
    "rmohr": "We are also using nginx as a proxy and indeed increasing the body size solved the problem:\nclient_max_body_size 800M;\n. ",
    "djlewis78": "can confirm that this happened to me yesterday... \nThis happened while using version 0.6.6\nRe-running the job normally resolves the issue. Also I find that pushing files in my build server to be problematic - so i run \ndocker push registry.example.com/myimage:latest-snapshot || docker push registry.example.com/myimage:latest-snapshot \nI am guessing this might have something to do with the size of the image layer timing out.\n. ",
    "treyhyde": "I get a 504 instead of a 502\n539a918a9e31: Image already pushed, skipping \n523e00e20703: Image already pushed, skipping \n30431d526e42: Pushing \n2014/08/08 11:08:46 HTTP code 504 while uploading metadata: invalid character '<' looking for beginning of value\n. against docker.io.  It was just once or twice when I was doing my first pushes in the middle of large image transfers.  It hasn't happened since I posted.\n. ",
    "sdwr98": "I'm getting this too pushing to docker hub.  The scenario I have is I build an image with a tag, then tag it with a second tag, then issue two push commands (one for each tag).  The first push succeeds, but the second will occasionally fail with the 504 version of this error.  Re-pushing the image won't work, I have to rebuild it. \n. ",
    "frederik-b": "I have this repeatedly with a private registry sometimes up to 4 times for the same image, then it suddenly works...\n. ",
    "dsw88": "I'm also seeing this issue. It mostly manifests itself when multiple pulls are going on at once. I can pretty consistently get this \"bad file descriptor\" error when pulling 10-20 images at once. \nI'm running the registry in a Docker container with the S3 flavor enabled.\n. @dmp42 Thanks for the information, I see the updated Gunicorn version in the 0.8 branch. I just have one quick question: Is this fix available in an image on Docker Hub? In other words, can I pull down a version of the image from Docker Hub that contains this fix yet? I see a 0.8.1 tag, but it's not immediately apparent how to tell whether it contains the fix.\nSorry if that's a dumb question, I'm still learning. :) I'm just trying to figure out how to get the fix since I'm currently using the Docker Container approach for running the registry.\n. Good to know thanks for your help! Yeah I'm kind of in a rush to get this fixed since it's causing deployment breakages at work. :) \nIt's a bit scary for me building off master since I haven't been tracking a lot of the progress in the commits yet, so I ended up checking out the stable 0.8.0 release tag and upgrading the Gunicorn server to 19.1. I then built the resulting docker image and put it in my user repository on Docker Hub.\nSo for anyone who's interested in getting this fix before 0.9.0, you can pull the image at https://registry.hub.docker.com/u/dsw88/registry/. If you do use it, note that I'm not going to be putting much work into it, since my plans are to go back to the standard image when 0.9.0 is released. No plans to fork long-term for me, it's just a stopgap so I can fix my issues at work until the fix is in a release.\n. I actually am putting a reverse proxy in front of this (haproxy), and I could do the path manipulation to rewrite requests to /my-path to go to /. In practice with most applications, however, I've found this approach to not work because the application has no idea that a reverse proxy is sitting in front of it rewriting its URLs. So when the running webapp returns back any links on API requests, it sends it back thinking that it's sitting on the root path. So instead of myhost.com/docker-registry/some/path/, it sends a path for myhost.com/some/path. That's no longer in the context getting rewritten by the reverse proxy, so it results in 404s.\nThis problem manifests itself in any application that sends back any links to the client. If your API never does that, then there's no problem using a reverse proxy.\nMost applications that do send back links to the clients solve offer the configurable ability to specify a path prefix to use when constructing links. Then when building links to send back to clients it includes that prefix in the URL so the reverse-proxy can correctly rewrite it and direct it to the right location.\n. Cool, thanks I'll try it out then!\n. Hey sorry it took me so long to get back to that. So I tried just hosting it off root, then putting a reverse proxy in front of it to rewrite a subpath (/registry) to root (/). Here's what I'm trying to do (just push a busybox image up to the registry):\nbash\ndocker push <my-proxy-url>/registry/busybox:latest\nThat command exits with a status code of 1, and produces the following output:\nSending image list\nSo for some reason the image isn't correctly pushed up when I try it like that. However, when I hit the instance directly (i.e. without going through the reverse proxy) things work fine:\nbash\ndocker push <my-app-url-without-proxy>/busybox:14.04\nThis produces the expected output:\nThe push refers to a repository [<my-app-url-without-proxy>/busybox] (len: 1)\nSending image list\nPushing repository <my-app-url-without-proxy>/busybox (1 tags)\n511136ea3c5a: Image successfully pushed \nd7ac5e4f1812: Image successfully pushed \n2f4b4d6a4a06: Image successfully pushed \n83ff768040a0: Image successfully pushed \n6c37f792ddac: Image successfully pushed \ne54ca5efa2e9: Image successfully pushed \nPushing tag for rev [e54ca5efa2e9] on {http://<my-app-url-without-proxy>/v1/repositories/busybox/tags/latest}\nSo something about the reverse proxy rewriting the URL seems to be causing a failure. Any idea what's happening there?\n. Ah I got some more information on this. It looks like the Docker client isn't handling well the fact that my URL is hosted off a sub-path like /registry. It gets 404s because it tries to push to just .https://github.com/dotcloud/docker/issues/7067\nThis is now seeming to be related to the Docker client, and not the registry, so I opened this issue on the Docker repo: https://github.com/dotcloud/docker/issues/7067.\n. I'm closing this issue for now since this seems to be Docker-client related. I'll re-open this if for some reason it turns out to be related to docker-registry.\n. Thanks for the help! Ah that makes sense, I didn't realize that option was available to point to a different database. So I'm not familiar with SQLAlchemy, If I wanted to configure a database at  on port , how would I represent that in a SQLAlchemy string? Also, how does your registry know the credentials for my database? Is there some way to pass those in?\n. Sweet this helps a lot, thanks! \nOne more question about this: If my database dies for some reason, what is the recommended way for repopulating this search index? Obviously restoring from backups is one option, although we'll experience some data loss. Is there a way to have the registry go look in S3 and repopulate the search index?\n. Awesome thanks. You guys are great, keep up the great work.\n. Thanks for the help on this question!\n. Ah the plot thickens: This behavior only manifests itself when I'm trying to pull from an ELB-addressed registry whose backend is S3. If I change that registry to use local filesystem storage, it suddenly works correctly.\nSo something seems to be different in the behavior of the S3 backend storage that's causing this error to be returned.\nAnyone have any idea why this behavior would be manifesting itself only on my registry when it's backed by S3?\n. Ok so I tried your suggested steps. First off, I did a dig on the ELB, and it returned two IPs that are not the IP of the instance I'm running in the registry. That's no surprise, however, as the returned IPs are for the ELB appliances. They then act as a reverse proxy by forwarding requests to the appropriate location.\nPushing an image to the registry using the ELB address worked, and I was able to see the pushed contents in my S3 bucket where I would expect them to be.\nHowever, pulling the images using the ELB address didn't work. I did look at the registry instance logs and saw that the requests did make it to the appropriate host; it's just that they returned a 404 when they should have appropriately found the images and returned them. Here's the registry instance log output from that failed pull:\n```\n10.36.248.73 - - [25/Jul/2014:18:57:16] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-07-25 18:57:16,242 INFO: 10.36.248.73 - - [25/Jul/2014:18:57:16] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-07-25 18:57:16,251 DEBUG: args = {}\n2014-07-25 18:57:16,251 DEBUG: path=/vpc-8e3c2bec/repositories/library/busybox/_index_images\n2014-07-25 18:57:16,252 DEBUG: auth_path=/fs-paas-dockerregistry-us-east-1-074150922133/vpc-8e3c2bec/repositories/library/busybox/_index_images\n2014-07-25 18:57:16,252 DEBUG: Method: HEAD\n2014-07-25 18:57:16,253 DEBUG: Path: /vpc-8e3c2bec/repositories/library/busybox/_index_images\n2014-07-25 18:57:16,253 DEBUG: Data: \n2014-07-25 18:57:16,253 DEBUG: Headers: {}\n2014-07-25 18:57:16,254 DEBUG: Host: fs-paas-dockerregistry-us-east-1-074150922133.s3.amazonaws.com\n2014-07-25 18:57:16,254 DEBUG: Port: 80\n2014-07-25 18:57:16,254 DEBUG: Params: {}\n2014-07-25 18:57:16,255 DEBUG: establishing HTTP connection: kwargs={'port': 80, 'timeout': 70}\n2014-07-25 18:57:16,255 DEBUG: Token: None\n2014-07-25 18:57:16,255 DEBUG: StringToSign:\nHEAD\nFri, 25 Jul 2014 18:57:16 GMT\n/fs-paas-dockerregistry-us-east-1-074150922133/vpc-8e3c2bec/repositories/library/busybox/_index_images\n2014-07-25 18:57:16,279 DEBUG: path=/vpc-8e3c2bec/repositories/library/busybox/_index_images\n2014-07-25 18:57:16,279 DEBUG: auth_path=/fs-paas-dockerregistry-us-east-1-074150922133/vpc-8e3c2bec/repositories/library/busybox/_index_images\n2014-07-25 18:57:16,280 DEBUG: Method: GET\n2014-07-25 18:57:16,280 DEBUG: Path: /vpc-8e3c2bec/repositories/library/busybox/_index_images\n2014-07-25 18:57:16,280 DEBUG: Data: \n2014-07-25 18:57:16,281 DEBUG: Headers: {}\n2014-07-25 18:57:16,281 DEBUG: Host: fs-paas-dockerregistry-us-east-1-074150922133.s3.amazonaws.com\n2014-07-25 18:57:16,281 DEBUG: Port: 80\n2014-07-25 18:57:16,282 DEBUG: Params: {}\n2014-07-25 18:57:16,282 DEBUG: Token: None\n2014-07-25 18:57:16,283 DEBUG: StringToSign:\nGET\nFri, 25 Jul 2014 18:57:16 GMT\n/fs-paas-dockerregistry-us-east-1-074150922133/vpc-8e3c2bec/repositories/library/busybox/_index_images\n10.36.251.31 - - [25/Jul/2014:18:57:16] \"GET /v1/repositories/busybox/images HTTP/1.1\" 200 304 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.2.0-4-amd64 os/linux arch/amd64\"\n2014-07-25 18:57:16,302 INFO: 10.36.251.31 - - [25/Jul/2014:18:57:16] \"GET /v1/repositories/busybox/images HTTP/1.1\" 200 304 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.2.0-4-amd64 os/linux arch/amd64\"\n2014-07-25 18:57:16,312 DEBUG: args = {}\n2014-07-25 18:57:16,312 DEBUG: [get_tags] namespace=library; repository=busybox\n2014-07-25 18:57:16,312 DEBUG: path=/\n2014-07-25 18:57:16,312 DEBUG: auth_path=/fs-paas-dockerregistry-us-east-1-074150922133/\n2014-07-25 18:57:16,313 DEBUG: path=/?prefix=vpc-8e3c2bec/repositories/library/busybox/&delimiter=/\n2014-07-25 18:57:16,314 DEBUG: auth_path=/fs-paas-dockerregistry-us-east-1-074150922133/?prefix=vpc-8e3c2bec/repositories/library/busybox/&delimiter=/\n2014-07-25 18:57:16,314 DEBUG: Method: GET\n2014-07-25 18:57:16,314 DEBUG: Path: /?prefix=vpc-8e3c2bec/repositories/library/busybox/&delimiter=/\n2014-07-25 18:57:16,315 DEBUG: Data: \n2014-07-25 18:57:16,315 DEBUG: Headers: {}\n2014-07-25 18:57:16,315 DEBUG: Host: fs-paas-dockerregistry-us-east-1-074150922133.s3.amazonaws.com\n2014-07-25 18:57:16,315 DEBUG: Port: 80\n2014-07-25 18:57:16,316 DEBUG: Params: {}\n2014-07-25 18:57:16,316 DEBUG: Token: None\n2014-07-25 18:57:16,316 DEBUG: StringToSign:\nGET\nFri, 25 Jul 2014 18:57:16 GMT\n/fs-paas-dockerregistry-us-east-1-074150922133/\n2014-07-25 18:57:16,359 DEBUG: <?xml version=\"1.0\" encoding=\"UTF-8\"?>\nfs-paas-dockerregistry-us-east-1-074150922133vpc-8e3c2bec/repositories/library/busybox/1000/falsevpc-8e3c2bec/repositories/library/busybox/_index_images2014-07-25T18:56:00.000Z\"abcb6c6b2fc56f23f35ef06e75608b71\"30476f59969c3d31591012db03a96402d9a4febcfb8cee2d30eda4f236f398a3bb9FH-AWS5STANDARDvpc-8e3c2bec/repositories/library/busybox/json2014-07-25T18:56:00.000Z\"da3906c7f488033e36d53016f1f1ae4f\"14576f59969c3d31591012db03a96402d9a4febcfb8cee2d30eda4f236f398a3bb9FH-AWS5STANDARDvpc-8e3c2bec/repositories/library/busybox/tag_latest2014-07-25T18:56:00.000Z\"19fa2eea8c69970efeeb93d9d7bdeb0b\"6476f59969c3d31591012db03a96402d9a4febcfb8cee2d30eda4f236f398a3bb9FH-AWS5STANDARDvpc-8e3c2bec/repositories/library/busybox/taglatest_json2014-07-25T18:56:00.000Z\"da3906c7f488033e36d53016f1f1ae4f\"14576f59969c3d31591012db03a96402d9a4febcfb8cee2d30eda4f236f398a3bb9FH-AWS5STANDARD\n2014-07-25 18:57:16,362 DEBUG: path=/repositories/library/busybox/tag_latest\n2014-07-25 18:57:16,362 DEBUG: auth_path=/fs-paas-dockerregistry-us-east-1-074150922133/repositories/library/busybox/tag_latest\n2014-07-25 18:57:16,362 DEBUG: Method: HEAD\n2014-07-25 18:57:16,363 DEBUG: Path: /repositories/library/busybox/tag_latest\n2014-07-25 18:57:16,363 DEBUG: Data: \n2014-07-25 18:57:16,363 DEBUG: Headers: {}\n2014-07-25 18:57:16,363 DEBUG: Host: fs-paas-dockerregistry-us-east-1-074150922133.s3.amazonaws.com\n2014-07-25 18:57:16,364 DEBUG: Port: 80\n2014-07-25 18:57:16,364 DEBUG: Params: {}\n2014-07-25 18:57:16,364 DEBUG: Token: None\n2014-07-25 18:57:16,365 DEBUG: StringToSign:\nHEAD\nFri, 25 Jul 2014 18:57:16 GMT\n/fs-paas-dockerregistry-us-east-1-074150922133/repositories/library/busybox/tag_latest\n2014-07-25 18:57:16,383 DEBUG: api_error: Repository not found\n10.36.251.31 - - [25/Jul/2014:18:57:16] \"GET /v1/repositories/library/busybox/tags HTTP/1.1\" 404 33 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.2.0-4-amd64 os/linux arch/amd64\"\n2014-07-25 18:57:16,384 INFO: 10.36.251.31 - - [25/Jul/2014:18:57:16] \"GET /v1/repositories/library/busybox/tags HTTP/1.1\" 404 33 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.2.0-4-amd64 os/linux arch/amd64\"\n```\nYeah I'm just confused why the registry would return a 404, when I have definitely verified that the appropriate repository and images are in the S3 bucket.\n. So I've been looking through the source of the registry, and it appears that the \"Repository not found\" message gets returned by the _get_tags method in tags.py. It looks like that error gets returned when an exception.FileNotFoundError gets thrown.\nThe confusing part is that based on what you see returned from \"ListBucket\" in the debug logs shows that there is a valid tag in the path we're looking in, called \"tag_latest\". I went and downloaded that file manually and successfully got it, which contained the string \"a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721\".\nNothing seems out of the ordinary, and it seems like this request should have worked instead of returning an error to the client. I may be missing something important, however.\n. Yes it happens every time. Since I last wrote, I think I may have figured out roughly what causes the issue. I set the STORAGE_PATH environment variable when running the registry, which you can see in the logs on this line:\n2014-07-25 18:57:16,314 DEBUG: Path: /?prefix=vpc-8e3c2bec/repositories/library/busybox/&delimiter=/\nHowever, when trying to pull the tag, the logs show that it uses this path:\n/fs-paas-dockerregistry-us-east-1-074150922133/repositories/library/busybox/tag_latest\nSo somewhere in the code when getting tags, the STORAGE_PATH variable is not getting used to build the path to the tag.\nI verified that the STORAGE_PATH is causing the problem by running the registry without it. When using the default, things work great and I can push and pull images normally.\nI haven't found exactly where in the code this problem is occurring yet, but I believe that is the cause of the problem.\n. Sure I'll take a look at it and test it out. I'll try to get to it in the next day or so if possible.\n. Given that the pull request hasn't been merged yet, I have a possible temporary roll-your-own solution. It's a sort of a mark-and-sweep algorithm that can be run asynchronously periodically:\n- Begin by assuming all images will be deleted.\n- Walk through your \"repositories\" directory and look at _index_images for each of them to see what images are consumed by that repository. Mark all of those images to be kept.\n- Once you've scanned through all the repositories, the set of remaining images that were not marked to be kept are the ones that can be deleted.\nThat's based on my limited experience with the registry, just looking at the way the image and repository data is stored in my S3 bucket. Can someone with more experience with the registry than me confirm that this is a good algorithm for removing unused images?\n. We've got this issue too on our registries. We're running them inside an AWS VPC. We're running one registry container per host, with 6 instance currently. Those instances are fronted by an elastic load balancer whose timeout it set to 5 mins. We're using S3 as a storage backend.\nAfter finding this thread, we set the registry with the -e STORAGE_REDIRECT=true option to delegate image downloads directly to S3. That has helped tremendously with the myriad of errors we were getting such as EOF.\nHowever, we're still getting EOF errors on some registry calls that aren't actual image layer retrievals. For example, I saw an EOF error on a /ancestry call. I don't see any errors in the Docker Registry logs when these types of issues happen so I'm sort of at a loss.\nThese errors seem to happen when we have a heavier load on the registries such as 5-10 images pulling from the registries from the same time. However, that doesn't seem like a heavy load especially with storage redirect and 6 instances of the registry behind a load balancer.\n. Hmm that might help, is there any documentation on how to disable that feature on our registries? I'm using the container so I've usually just had environment variables that map to what's in the YAML config file, but I don't see the option you mentioned in that YAML config file schema.\n. Ok I'm trying the fix out now, I'll have to wait a few days to validate whether our percentage of deploy failures when pulling from the registries goes down.\nIf anyone else is interested in trying this fix quickly, I have a container built on https://registry.hub.docker.com/u/dsw88/registry/. The tag is 0.9.1-parallelkey-fix. I built the container from master, which appears safe at this point since there haven't been any major code changes since the 0.9.1 stable release on Jan. 8: https://github.com/docker/docker-registry/commits/master\n. Ok it looks like I'm still seeing the EOF errors after applying the parallel key fix. I don't know yet whether their occurrence has been reduced, since I'll need to wait a few days to have enough real-world data from deploys.\nBut regardless, it appears that parallel key isn't the ultimate fix for this issue. Any other suggestions? Do you think it would be good to set up a Redis LRU cache? I'm wondering if having that cache would decrease the number of errors since it will need to make fewer round-trips to S3.\n. Ok so I'm now using the LRU cache and doing S3 storage redirect. We continue to get EOF errors, but we'll see if this at least cuts it down.\nI'm still concerned about the underlying issue here, however, as adding a cache is at best masking the problem. I'll try to do some debugging in the registry to find out what's causing this error, but I'm not super familiar with the codebase or technology stack so it'll be slow going.\nOne of my problems is I can't see any errors in the registry logs when this problem occurs. @dmp42 do you have any suggestions about how I could go about getting enough information about the error to start debugging? The Docker client isn't much help when pulling images because it just says \"unexpected EOF\".\n. It appears that the small files like /ancestry and others are still failing periodically for us:\n$ docker pull <host_name>/<image_name>:238   \nPulling repository <host_name>/<image_name>                      \n2015/03/18 20:44:16 Error pulling image (<tag_name>) from <host_name>/<image_name>, \nGet http://<host_name>/v1/images/8a39dc87bd3e270444da2b7316ffcc8f7c2e65f5d91e5a3c3d2bcf17b905a7f6/ancestry: EOF\nWhen I look in the registry logs, it shows that it got that request and even returned a 200, so presumably it thought it returned the image layer correctly:\n[18/Mar/2015:20:44:16 +0000] \"GET /v1/images/8a39dc87bd3e270444da2b7316ffcc8f7c2e65f5d91e5a3c3d2bcf17b905a7f6/ancestry HTTP/1.1\" 200 3196 \"-\" \"docker/1.1.2 go/go1.2.1 git-commit/d84a070 kernel/3.14.0-0.bpo.2-amd64 os/linux arch/amd64\"\nSo are the threads in the webapp dying or timing out while it's streaming the response or something?\n. @dmp42 Sorry about that, I'll open a new issue related to the EOF issues we're seeing even though we've already implemented the S3 storage redirect. I'll post there about the details of my setup.\n. Oh also on the other issue you suggested upgrading the version of Docker used to pull the image. I'll try that first.\n. Here's the registry version of Docker:\nClient version: 1.3.0\nClient API version: 1.15\nGo version (client): go1.3.3\nGit commit (client): c78088f\nOS/Arch (client): linux/amd64\nServer version: 1.3.0\nServer API version: 1.15\nGo version (server): go1.3.3\nGit commit (server): c78088f\nHere's the docker info for the registry:\nContainers: 1\nImages: 15\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Dirs: 17\nExecution Driver: native-0.2\nKernel Version: 3.16.0-0.bpo.4-amd64\nOperating System: Debian GNU/Linux 7 (wheezy)\nWARNING: No memory limit support\nWARNING: No swap limit support\nHere's the docker version for one of the docker daemons that had a failed pull:\nClient version: 1.2.0\nClient API version: 1.14\nGo version (client): go1.2\nGit commit (client): fa7b24f/1.2.0\nOS/Arch (client): linux/amd64\nServer version: 1.2.0\nServer API version: 1.14\nGo version (server): go1.2\nGit commit (server): fa7b24f/1.2.0\nAnd here's the docker info for that client:\nContainers: 0\nImages: 13\nStorage Driver: devicemapper\n Pool Name: docker-202:1-263625-pool\n Pool Blocksize: 64 Kb\n Data file: /var/lib/docker/devicemapper/devicemapper/data\n Metadata file: /var/lib/docker/devicemapper/devicemapper/metadata\n Data Space Used: 1129.6 Mb\n Data Space Total: 102400.0 Mb\n Metadata Space Used: 1.5 Mb\n Metadata Space Total: 2048.0 Mb\nExecution Driver: native-0.2\nKernel Version: 3.14.20-20.44.amzn1.x86_64\nOperating System: Amazon Linux AMI 2014.09\nThis one is old because we run on Elastic Beanstalk and they don't yet have support for configuring insecure registries yet. This means with 1.3 and above we get failures because Docker complains about an insecure registry. It looks like others on this issue are having the same failures even with up-to-date versions of Docker.\n. ",
    "kyle-crumpton": "Hi all. I'm running into this same error. I'm wondering if there's any news on when 0.9.0 will be released? I understand there is a fix proposed here (build off master and update gunicorn); I just found that doing an additional push typically works as well. \n. @dmp42 \nThe error I'm getting is the following: \n{\"log\":\"  File \\\"\\u003cstring\\u003e\\\", line 1, in getsockname\\n\",\"stream\":\"stderr\",\"time\":\"2014-08-07T22:36:12.569857315Z\"}\n{\"log\":\"error: [Errno 9] Bad file descriptor\\n\",\"stream\":\"stderr\",\"time\":\"2014-08-07T22:36:12.569865982Z\"}\n. ",
    "bhuvan": "@dmp42 any ETA for 0.9?\n. facing same issue in 0.8.1 and not in 0.7.1. unable to push images to registry.\n172.16.1.75 - - [29/Sep/2014:22:07:06] \"PUT /v1/repositories/user/image-name/ HTTP/1.1\" 405 178 \"-\" \"docker/1.2.0 go/go1.3.1 git-commit/fa7b24f kernel/3.8.13-26.\n2.2.el6uek.x86_64 os/linux arch/amd64\"\n. @dmp42 yep. i was running registry with standalone=True. I also defined --env STANDALONE=True just in case. Here is my common snippet.\ncommon:\n    loglevel: info\n    search_backend: _env:SEARCH_BACKEND:sqlalchemy\n    sqlalchemy_index_database: _env:SQLALCHEMY_INDEX_DATABASE:sqlite:////indexes/docker-registry.db\n    standalone: true\n    disable_token_auth: true\n    mirroring:\n        source: https://registry-1.docker.io\n        source_index: https://index.docker.io\n        tags_cache_ttl: 15552000\nMy findings:\nThe issue occur only in a fresh install of 0.8.1. It doesn't occur when upgrading from 0.7.1 -> 0.8.1.\nThe workaround to this problem:\nRun registry with 0.7.1 and then run using 0.8.1, using same images/indexes directory.\n. @dmp42 i'm using dev mode. here is my config file docker-registry.yaml and registry launch command.\ndocker-registry.yaml:\n```\ncommon:\n    loglevel: info\n    search_backend: _env:SEARCH_BACKEND:sqlalchemy\n    sqlalchemy_index_database: _env:SQLALCHEMY_INDEX_DATABASE:sqlite:////tmp/docker-registry.db\n    #\n    standalone: true\n    disable_token_auth: true\n    mirroring:\n        source: https://registry-1.docker.io\n        source_index: https://index.docker.io\n        tags_cache_ttl: 15552000\ndev:\n    loglevel: debug\n    storage: local\n    storage_path: /images\n```\nhere is how i launch my registry:\ndocker run --net=host \\                                                         \n    --user=nobody \\                                                             \n    --detach=true                                                               \n    --cidfile=\"docker_id.txt\" \\                                                 \n    --volume $PWD/config:/config \\                                              \n    --volume /data/docker-registry/images:/images \\                             \n    --volume /data/docker-registry/indexes:/indexes \\                           \n    --volume /etc/passwd:/etc/passwd:ro                                         \n    --volume /etc/group:/etc/group:ro \\                                         \n    --env DOCKER_REGISTRY_CONFIG=/config/docker-registry.yaml \\                 \n    --env SQLALCHEMY_INDEX_DATABASE=sqlite:////indexes/docker-registry.db \\     \n    --env REGISTRY_PORT=5000 registry:0.7.1                                     \nsleep 2                                                                         \nCONTAINER_ID=$(< docker_id.txt)                                                 \ndocker logs --follow=true --timestamps=true $(< docker_id.txt)\n. @dmp42 registry built from master (a4012beca6f7ff2514076297cbc4fb9b1b126590) dont work for me. It complain following error:\n2014-10-09T05:08:41.587854048Z OperationalError: (OperationalError) table version already exists u'\\nCREATE TABLE version (\\n\\tid INTEGER NOT NULL, \\n\\tPRIMARY KEY (id)\\n)\\n\\n' ()\nI was bringing up a fresh registry instance. The db file /data/docker-registry/indexes/docker-registry.db is created. It contain the version table with id 1.\n$ sqlite3 docker-registry.db \nSQLite version 3.6.20\nEnter \".help\" for instructions\nEnter SQL statements terminated with a \";\"\nsqlite> .tables\nrepository  version   \nsqlite> select * from version;\n1\nsqlite> select * from repository;\nSee this gist for how I launch registry, config file and complete traceback.\nhttps://gist.github.com/bhuvaneswaran/5d8d6617d6fe35363779\n. ",
    "adamhadani": "+1 ETA 0.9 would love to see it coming, any open issues that are holding it up that community can help with?\n. @dmp42 - looking through the issues, I see there's already a good 22 issues closed.\nDo you think it would be possible to push all or most of the open issues out into a 1.0 release so 0.9 can be released quicker with all the issues already closed? (specifically this issue #320 which seems to be an easy win with 0.9)\n. Pretty sure i'm running into the same or related issue -\n1. I set up a docker-registry instance\n2. I setup an nginx front proxy to it which uses (not self signed) SSL certs and http basic auth \n3. The following flow for example fails on a 401 which I can trace in nginx logs (see immediately below):\ndocker pull foo/bar  # some example public container\ndocker tag foo/bar user:pass@docker-images-local.mydomain.com:443/bar\ndocker push user:pass@docker-images-local.mydomain.com:443/bar\nResults in:\nThe push refers to a repository [...] (len: 1)\nSending image list\nPushing repository user:pass@docker-images-local.mydomain.com:443/bar (1 tags)\n8dbd9e392a96: Pushing \n2014/09/17 16:10:00 HTTP code 401 while uploading metadata: invalid character '<' looking for beginning of value\nand nginx logs show:\n172.16.91.1 - default [17/Sep/2014:23:10:59 +0000] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n172.16.91.1 - default [17/Sep/2014:23:10:59 +0000] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n172.16.91.1 - default [17/Sep/2014:23:10:59 +0000] \"PUT /v1/repositories/bar/ HTTP/1.1\" 200 2 \"-\" \"docker/1.2.0 go/go1.3.1 git-commit/fa7b24f kernel/3.16.1-tinycore64 os/linux arch/amd64\"\n172.16.91.1 - - [17/Sep/2014:23:10:59 +0000] \"GET /v1/images/8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c/json HTTP/1.1\" 401 194 \"-\" \"docker/1.2.0 go/go1.3.1 git-commit/fa7b24f kernel/3.16.1-tinycore64 os/linux arch/amd64\"\n172.16.91.1 - - [17/Sep/2014:23:10:59 +0000] \"PUT /v1/images/8dbd9e392a964056420e5d58ca5cc376ef18e2de93b5cc90e868a1bbc8318c1c/json HTTP/1.1\" 401 194 \"-\" \"docker/1.2.0 go/go1.3.1 git-commit/fa7b24f kernel/3.16.1-tinycore64 os/linux arch/amd64\"\nso you can see it switches at the last two http requests to giving 401s, possibly related to other reports of it 'switching' to use Token auth or something like that?\n. Here's the nginx site config (this is a template populated in an Ansible deploy, final values are guaranteed to be valid). This template is virtually identical to the example file (https://github.com/docker/docker-registry/blob/master/contrib/nginx_1-3-9.conf) minus the port 80 redirect. I am not listening at all on port 80 (verified this, nginx default site is deactivated as well) so I don't assume this is the problem i'm seeing.\n```\nupstream docker-registry {\n    server {{ facter_ipaddress_docker0 }}:5000;\n}\nserver {\n    listen 443 ssl;\n    server_name {{ docker_registry_hostname }};\nssl on;\nssl_certificate      /etc/nginx/certs/{{ docker_registry_ssl_certificate }};\nssl_certificate_key  /etc/nginx/certs/{{ docker_registry_ssl_certificate_key }};\n\nproxy_set_header Host $http_host; # required for docker client's sake\nproxy_set_header X-Real-IP $remote_addr; # pass on real client's IP\nproxy_set_header Authorization \"\"; # see https://github.com/dotcloud/docker-registry/issues/170\n\nclient_max_body_size 0; # disable any limits to avoid HTTP 413 for large image uploads\n\n# required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\nchunked_transfer_encoding on;\n\nlocation / {\n    auth_basic  \"Restricted\";\n    auth_basic_user_file conf.d/docker-registry.htpasswd;\n\n    proxy_pass http://docker-registry;\n    proxy_set_header Host $host;\n    proxy_read_timeout 900;\n}\n\nlocation /_ping {\n    auth_basic off;\n    proxy_pass http://docker-registry;\n}\n\nlocation /v1/_ping {\n    auth_basic off;\n    proxy_pass http://docker-registry;\n}\n\n}\n```\nAs for the docker-registry, i'm using the config_example file (https://github.com/docker/docker-registry/blob/master/config/config_sample.yml) and passing in the docker environment vars:\nSETTINGS_FLAVOR=local\nSEARCH_BACKEND=sqlalchemy\nDOCKER_REGISTRY_CONFIG=/etc/docker-registry/config.yml\nthe registry seems to work fine btw when I just hit it directly via port 5000. My problems seem to be around the usage of HTTP Basic Auth via nginx proxy.\n. Some more info. doing some packet sniffing, looks like where things go wrong happens after docker-registry returns back this 'token' thing in response header, e.g the response to the 'PUT /v1/repositories/myapp/ HTTP/1.0' part\nHTTP/1.0 200 OK\nServer: gunicorn/18.0\nDate: Thu, 18 Sep 2014 22:28:22 GMT\nConnection: close\nX-Docker-Token: Token signature=8IRV2GB1G8FHRB9G,repository=\"library/myapp\",access=write\nX-Docker-Endpoints: docker-images-local.mydomain.com:443\nPragma: no-cache\nCache-Control: no-cache\nExpires: -1\nContent-Type: application/json\nWWW-Authenticate: Token signature=8IRV2GB1G8FHRB9G,repository=\"library/myapp\",access=write\nContent-Length: 2\nX-Docker-Registry-Version: 0.8.1\nX-Docker-Registry-Config: local\nAfter this response, the next request coming out of docker client has a much shorter 'Authorization' http header which triggers the 401 assumably. I can't see any other authentication-related http header in there either:\nX-Original-URI: /v1/images/<hash>/json\nHost: auth\nConnection: close\nUser-Agent: docker/1.2.0 go/go1.3.1 git-commit/fa7b24f kernel/3.16.1-tinycore64 os/linux arch/amd64\nAuthorization: Basic Og==\nAccept-Encoding: gzip\n. With the risk of getting ahead of myself, I looked around the docker_registry code. Inside https://github.com/docker/docker-registry/blob/master/docker_registry/index.py, there's a generate_headers function which seem to generate the WWW-Authenticate: Token ... stuff. \nIf running in standalone automatically dictates no token auth should be used is that a potential bug? e.g is that an instruction to the docker client to switch to use that auth token method which causes a basic auth authentication failure in subsequent requests as I saw above?\n. Thanks for replies, going to try these suggestions now and report back with findings\n. OK, so got it to work using some combination of the feedback provided here. I'm gonna report my steps and some points of interest in the hope this helps other people who might come across this issue and perhaps provide pointers for how to improve documentation / make behaviour more consistent.\nPushing to a private repo which is using HTTPS and Basic Auth\n\nLogin using a user/pass combo which exists in the .htpasswd for the basic auth (docker registry currently accepts any login credentials with 'account created', however docker client assumably also uses the same login details for the http basic auth). The correct login cli invocation seems to be:\n\ndocker login -u <user> -p <password> -e doesnt@matter.com https://docker-images-local.mydomain.com\non the nginx proxy logs this command causes the following stream of requests, notice the 401 on the POST:\n172.16.91.1 - - [23/Sep/2014:19:18:41 +0000] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n172.16.91.1 - - [23/Sep/2014:19:18:41 +0000] \"POST /v1/users/ HTTP/1.1\" 401 194 \"-\" \"Go 1.1 package http\"\n172.16.91.1 - default [23/Sep/2014:19:18:41 +0000] \"GET /v1/users/ HTTP/1.1\" 200 4 \"-\" \"docker/1.2.0 go/go1.3.1 git-commit/fa7b24f kernel/3.16.1-tinycore64 os/linux arch/amd64\"\n- Tag some image you already have using the following syntax in order to designate it as \"push-able\" to the private repo . In this case i'm just tagging and pushing a publicly available container I pulled earlier (dockerfile/nginx):\ndocker tag dockerfile/nginx docker-images-local.mydomain.com/nginx\n- Push the image using the same path you used for the tag command:\ndocker push docker-images-local.mydomain.com/nginx\n- Similarly, pulling is done using the same syntax:\ndocker pull docker-images-local.mydomain.com/nginx\n\nSo a few things worth mentioning that are probably worth documenting (I've gone over a few blog posts and documentation pages and wasn't able to 'deduce' this, so its not clear at all to most people I assume, which is a shame since otherwise setting up a private docker repo is very useful and was pretty straight forward):\n- Docker by default always tries to use HTTPS (port 443) first - no need to be explicit about ports / protocol in CLI invocations, (if the 'docker login' command was invoked in the context of https:// ? or always?)\n- Docker by default will use the user/pass combo given in the 'docker login' command as the basis for the base64-encoded token in the HTTP 'Authorization' header (e.g the basic auth credentials).\n  Interestingly, the following syntax also seems to work (providing the user/pass inline in the url in addition to the -u and -p params) and not result in the POST failure i've mentioned above in step 1., and there's no GET on the users resource following it either:\ndocker login -u <user> -p <pass> -e doesnt@matter.com https://<user>:<pass>@docker-images-local.mydomain.com\n172.16.91.1 - default [23/Sep/2014:19:24:29 +0000] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n172.16.91.1 - default [23/Sep/2014:19:24:29 +0000] \"POST /v1/users/ HTTP/1.1\" 201 14 \"-\" \"Go 1.1 package http\"\n- One needs to tag an image with the private repo url / repo path before he can push to it. Personally this syntax / logical flow doesn't make sense to me at all, being able to push images to different repositories should not necessitate a \"tag\" invocation, I would try to take cue from the git / github semantics of having repositories and remotes, and being able to add a remote, pull/push to different remotes etc., but I digress.\n- As mentioned before in this thread and other ones, this whole flow will only work when using HTTPS (e.g using SSL certificate in the nginx proxy), as docker client will not send credentials in the clear. This makes it a requirement to use https if one wishes to also use basic auth\n. @dmp42  thanks, Will most likely build from master and try it out this week will post any issues if i come across any here\n. @dmp42  newb question - whats best way to test run this? e.g should I just check out master, build the docker image using the Dockerfile and upload it to my private repo on dockerhub or so, and then relaunch my docker-registry instance using my image? or any shortcuts?\n. @ncdc got it thanks\n. looks like (on ubuntu 14.04), I had to install following dependencies for running this locally, might be worth documenting somewhere (e.g in CONTRIBUTING.md, the link to which from README.md seems broken atm btw):\napt-get install python-dev liblzma-dev\n. @bacongobbler agreed - for setting up local development env (or testing an RC as we're doing in this case) should have some minimal documentation of any steps / dependencies that developer would need to install on his own.\nAdding to this - I had to manually install flask CORS ext, this seems like a bug as should probably be in the setup.py:\npip install -U flask-cors\n. just noticed these are mostly documented somewhere in https://github.com/docker/docker-registry/blob/master/ADVANCED.md  , would be nice to have that information more visible though\n. @dmp42 @silarsis  same here, been running it for 2-3 days (using the gunicorn command-line mentioned above in this thread), been working well for the most part it seems - notably haven't seen the intermittent 502 http errors issue we've been seeing in 0.8\n. @dmp42  after close to a week now of running the 0.9 RC, looking good. we mostly do basic pull/push requests running it behind an HTTPS/Basic Auth nginx frontend, using local storage and sqlalchemy search, seems fine and notably fixed the 502 errors we've had before that plagued our deploys.\nthumbs up!\n. ",
    "drobison00": "Seeing this running a private docker registry running https authentication with an nginx proxy, and a swift backend. No errors if I push directly to the registry (port 5000).\ndocker login ....\nLogin Succeeded\n511136ea3c5a: Pushing \n2015/02/05 12:47:14 HTTP code 502 while uploading metadata: invalid character '<' looking for beginning of value\ndocker version\nClient version: 1.3.3\nClient API version: 1.15\nGo version (client): go1.3.3\nGit commit (client): 4e9bbfa/1.3.3\nOS/Arch (client): linux/amd64\nServer version: 1.3.3\nServer API version: 1.15\nGo version (server): go1.3.3\nGit commit (server): 4e9bbfa/1.3.3\n. @dmp42 \nThanks, you're right. I was setting a header value in two places and it was causing a problem.\n. @mimizone \nThanks for this. My setup is almost identical; however, timeouts weren't was was getting me.  I'd set the 'Host' header to $host in one place, and $http_host in another. :( \n. For the swift plugin, I built up a new container from the basic registry.\n=== Dockerfile ===\nOfficial registry image\nFROM registry:latest\nUpdate config\nADD ./reg_config.yml /docker-registry/config/config.yml\nUpdate repository information\nRUN apt-get -y update && apt-get -y upgrade\nInstall build packages for docker swift driver\nRUN apt-get -y install libxml2-dev && apt-get -y install libxslt-dev\nInstall swift driver\nRUN pip install docker-registry-driver-swift\n=== End === \nThe back end i'm using doesn't support swift's v2 authorization, so I added the following to my swift block in the .yml config used above:\nswift_auth_version: _env:OS_AUTH_VERSION\nThen at launch: -e OS_AUTH_VERSION=1\n=== /etc/nginx/sites-enabled/docker-registry ===\nupstream docker-registry {\n    server localhost:5000;\n}\nserver {\n    listen 8080;\n    server_name repo;\n```\nssl on;\nssl_certificate /etc/ssl/certs/docker-registry-bundle.crt;\nssl_certificate_key /etc/ssl/private/docker-registry.key;\nproxy_set_header Host       $http_host;   # required for Docker client sake\nproxy_set_header X-Real-IP  $remote_addr; # pass on real client IP\nclient_max_body_size 0; # disable any limits to avoid HTTP 413 for large image uploads\nrequired to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\nchunked_transfer_encoding on;\nlocation / {\n    let Nginx know about our auth file\n    auth_basic              \"Restricted\";\n    auth_basic_user_file    docker-registry.htpasswd;\nproxy_pass http://docker-registry;\n\n}\nlocation /_ping {\n    auth_basic off;\n    proxy_pass http://docker-registry;\n}\nlocation /v1/_ping {\n    auth_basic off;\n    proxy_pass http://docker-registry;\n}\n```\n}\n==== end ====\n. ",
    "mimizone": "In case it helps, I have the same issue with similar setup : nginx in front, authentication, https, swift backend.\nit seems to be a timeout on the backend. I am still investigating.\nthe error message on the docker client side\nubuntu@docker-registry:~/nginx$ sudo docker push docker-registry.os/ubuntu\nThe push refers to a repository [docker-registry.os/ubuntu] (len: 1)\nSending image list\nPushing repository docker-registry.os/ubuntu (1 tags)\nImage 511136ea3c5a already pushed, skipping\n27d47432a69b: Pushing\nFATA[0060] HTTP code 504 while uploading metadata: invalid character '<' looking for beginning of value\nsome info on my setup.\nnginx version 1.6.2\ndeployed within a container from latest on docker.io\nregistry\nin a container, based on the docker.io image, with the addition of the swift module and with config files embedded in the container\n/etc/nginx/nginx.conf\n```\nroot@617a0715e38d:/var/log/nginx# cat /etc/nginx/nginx.conf\nuser www-data;\nworker_processes 4;\npid /run/nginx.pid;\nevents {\n        worker_connections 768;\n        # multi_accept on;\n}\nhttp {\n    ##\n    # Basic Settings\n    ##\n\n    sendfile on;\n    tcp_nopush on;\n    tcp_nodelay on;\n    keepalive_timeout 65;\n    types_hash_max_size 2048;\n    # server_tokens off;\n\n    # server_names_hash_bucket_size 64;\n    # server_name_in_redirect off;\n\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n\n    ##\n    # SSL Settings\n    ##\n\n    ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Dropping SSLv3, ref: POODLE\n    ssl_prefer_server_ciphers on;\n\n    ##\n    # Logging Settings\n    ##\n\n    access_log /var/log/nginx/access.log;\n    error_log /var/log/nginx/error.log;\n\n    ##\n    # Gzip Settings\n    ##\n\n    gzip on;\n    gzip_disable \"msie6\";\n\n    # gzip_vary on;\n    # gzip_proxied any;\n    # gzip_comp_level 6;\n    # gzip_buffers 16 8k;\n    # gzip_http_version 1.1;\n    # gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;\n\n    ##\n    # Virtual Host Configs\n    ##\n\n    include /etc/nginx/conf.d/*.conf;\n    include /etc/nginx/sites-enabled/*;\n\n}\nmail {\n# See sample authentication script at:\n# http://wiki.nginx.org/ImapAuthenticateWithApachePhpScript\n\n# auth_http localhost/auth.php;\n# pop3_capabilities \"TOP\" \"USER\";\n# imap_capabilities \"IMAP4rev1\" \"UIDPLUS\";\n\nserver {\nlisten     localhost:110;\nprotocol   pop3;\nproxy      on;\n}\n\nserver {\nlisten     localhost:143;\nprotocol   imap;\nproxy      on;\n}\n}\ndaemon off;\n```\n/etc/nginx/sites-enabled/default\n```\nupstream docker-registry {\n server registry:5000;\n}\nserver {\n listen 443;\n server_name docker-registy;\nssl on;\n ssl_certificate /etc/ssl/certs/docker-registry;\n ssl_certificate_key /etc/ssl/private/docker-registry;\nproxy_set_header Host       $http_host;   # required for Docker client sake\n proxy_set_header X-Real-IP  $remote_addr; # pass on real client IP\nclient_max_body_size 0; # disable any limits to avoid HTTP 413 for large image uploads\n# required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\n chunked_transfer_encoding on;\nlocation / {\n     # let Nginx know about our auth file\n     auth_basic              \"Restricted\";\n     auth_basic_user_file    docker-registry.htpasswd;\n     proxy_pass http://docker-registry;\n }\n location /_ping {\n     auth_basic off;\n     proxy_pass http://docker-registry;\n }\n location /v1/_ping {\n     auth_basic off;\n     proxy_pass http://docker-registry;\n }\n}\n```\n/var/log/nginx/access.log\nroot@617a0715e38d:/var/log/nginx# cat access.log\n172.30.8.33 - - [06/Feb/2015:00:22:54 +0000] \"GET /v1/_ping HTTP/1.1\" 200 2 \"-\" \"Go 1.1 package http\"\n172.30.8.33 - - [06/Feb/2015:00:22:54 +0000] \"GET /v1/_ping HTTP/1.1\" 200 2 \"-\" \"Go 1.1 package http\"\n172.30.8.33 - octopus [06/Feb/2015:00:22:55 +0000] \"PUT /v1/repositories/ubuntu/ HTTP/1.1\" 200 2 \"-\" \"docker/1.4.1 go/go1.3.3 git-commit/5bc2ff8 kernel/3.13.0-40-generic os/linux arch/amd64\"\n172.30.8.33 - octopus [06/Feb/2015:00:22:55 +0000] \"GET /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1\" 200 698 \"-\" \"docker/1.4.1 go/go1.3.3 git-commit/5bc2ff8 kernel/3.13.0-40-generic os/linux arch/amd64\"\n172.30.8.33 - octopus [06/Feb/2015:00:22:55 +0000] \"GET /v1/images/27d47432a69bca5f2700e4dff7de0388ed65f9d3fb1ec645e2bc24c223dc1cc3/json HTTP/1.1\" 400 49 \"-\" \"docker/1.4.1 go/go1.3.3 git-commit/5bc2ff8 kernel/3.13.0-40-generic os/linux arch/amd64\"\n172.30.8.33 - octopus [06/Feb/2015:00:23:55 +0000] \"PUT /v1/images/27d47432a69bca5f2700e4dff7de0388ed65f9d3fb1ec645e2bc24c223dc1cc3/json HTTP/1.1\" 504 182 \"-\" \"docker/1.4.1 go/go1.3.3 git-commit/5bc2ff8 kernel/3.13.0-40-generic os/linux arch/amd64\"\n/var/log/nginx/error.log\ncat error.log\n2015/02/06 00:23:55 [error] 17#0: *11 upstream timed out (110: Connection timed out) while reading response header from upstream, client: 172.30.8.33, server: docker-registy, request: \"PUT /v1/images/27d47432a69bca5f2700e4dff7de0388ed65f9d3fb1ec645e2bc24c223dc1cc3/json HTTP/1.1\", upstream: \"http://172.17.0.3:5000/v1/images/27d47432a69bca5f2700e4dff7de0388ed65f9d3fb1ec645e2bc24c223dc1cc3/json\", host: \"docker-registry.os\"\n. I see some issues between the registry and the swift backend:\nin docker registry logs:\n06/Feb/2015:00:43:06 +0000 INFO: Starting new HTTP connection (1): 172.30.80.20\n06/Feb/2015:00:43:16 +0000 INFO: REQ: curl -i http://172.30.80.20:8080/swift/v1/octopus_docker/registry/images/27d47432a69bca5f2700e4dff7de0388ed65f9d3fb1ec645e2bc24c223dc1cc3/json -X PUT -H \"X-Auth-Token: 58e927ec80674f4ca23bc93e7e6440d5\"\n06/Feb/2015:00:43:16 +0000 INFO: RESP STATUS: 408 Request Time-out\n06/Feb/2015:00:43:16 +0000 INFO: RESP HEADERS: [('connection', 'close'), ('content-type', 'text/html'), ('cache-control', 'no-cache')]\n06/Feb/2015:00:43:16 +0000 INFO: RESP BODY: <html><body><h1>408 Request Time-out</h1>\nYour browser didn't send a complete request in time.\n</body></html>\ntrying the curl request separately from the docker host:\ncurl -i http://172.30.80.20:8080/swift/v1/octopus_docker/registry/images/27d47432a69bca5f2700e4dff7de0388ed65f9d3fb1ec645e2bc24c223dc1cc3/json -X PUT -H \"X-Auth-Token: 58e927ec80674f4ca23bc93e7e6440d5\"\nHTTP/1.1 411 Length Required\nDate: Fri, 06 Feb 2015 00:44:22 GMT\nServer: Apache/2.2.22 (Ubuntu)\netag:\nAccept-Ranges: bytes\nContent-Length: 20\nVary: Accept-Encoding\nConnection: close\nContent-Type: text/plain; charset=utf-8\n. @drobison00 \nthanks, I'll check on my side, but it doesn't seem to be the issue for me.\ncould you share your nginx config? and how you added the swift plugin and if you have specific config for this?\n. thanks for the info.\nI built my containers basically the same way.\nMy swift supports v2 authentication. Switching to v1 as you did actually doesn't work at all for me unfortunately.\n. For info, my issue was network related, with MTU discrepancy between nodes.\n. ",
    "tarnfeld": "This is probably something we're looking at contributing soon-ish. Have you guys considered using PyFilesystem as a neat abstraction to enable a wide variety of storage backends? I think some of the filesystem implementations might need to be adjusted to behave properly (using streams instead of writing to tmp files, aka the S3Filesystem).\nWe're putting the final touches on a HDFS Implementation which would work with Hadoop2 for other purposes, perhaps this could be the basis of implementing this issue. Note the PR is work in progress.\nHas anyone else begun looking into this?\n. I'm also seeing this uploading a large layer on the latest version of docker (and docker-registry)... fails around the same time, every time... Small layers push successfully though.\n\n6150c6a21982: Pushing [==============================>           ] 379.3 MB/480.2 MB 16s\n\nUpdate: If I just apt-get install lxc-docker-0.11.0 to go back from 0.12.0 and try pushing again, it works fine. Without restarting the registry, too.\n. Nice one guys! Thanks.\n. ",
    "lyda": "Sorry for the delay. Took me a while to debug and get time to work on this.\nThis should work. And thanks @dmp42 for the encouragement and the pointers. Not sure if you still need this @Sirupsen but feedback would be appreciated. In a future iteration I'll switch to snakebite for the reads. In addition in my testing (by hand, sorry) it works better if /hdfs is a volume, but not sure how to play that out with mesos just yet.\nhttps://github.com/lyda/docker-registry-driver-hdfs\n. Excellent.  Still, your travis build helped me fix a few style bugs since I failed to run pylint/flake8.  Bad Kevin.  Thanks for the link and the speedy response.  Will follow that.\n. Thanks - elliptics was a great starting point.  I still need to add tests and using snakebite for hdfs read-access would be far better.\nI do have one question. I've lost the link, but there was discussion that flask(?) had a secret key that was auto-generated at start up and that if you wanted to run multiple docker-registries using the same store, you'd need to set it yourself.  But I can't find any such code.  I suspect I'll need to add some locking (zookeeper or something like that) to have multiple docker-registries accessing the same store, but also want to make sure there's no special key I need to deal with.\n. ",
    "cressie176": "I think your fully qualified image name is missing the username. Typing docker tag with no arguments will give you the format...\nbash\n$ docker tag\nUsage: docker tag [OPTIONS] IMAGE [REGISTRYHOST/][USERNAME/]NAME[:TAG]\nYou in your example...\nbash\ndocker build -t docker-index.my.com/username/base:1.0 .\ndocker run -i -t docker-index.my.com/username/base /bin/bash\nWe tend to use our company name for the username in our private repo, but you could maybe use the project / program name instead.\nOne unfortunate feature is that the username must be >= 4 characters, which might make sense for a public repo but not so useful for a private one. Especially since the two companies I've used docker at so far both had a common three letter acronym.\n. ",
    "ccverak": "No, it doesn't work either\n. The same logs for me!\n. I ended up using registry directly with gunicorn (port 5000) and not behind nginx and it worked fine.\n. ",
    "lyapun": "Hello!\nI have just same issue. I run private registry at my server (docker run -p 5000:5000 registry), and I can succesfully push image to this repo from my local machine (mac os):\n```\n\ndocker tag 0dae3c8a4038 docker.myhost.com/postgresql\ndocker push docker.myhost.com/postgresql          \nThe push refers to a repository [docker.myhost.com/postgresql] (len: 2)\nSending image list\nPushing repository docker.myhost.com/postgresql (2 tags)\nImage 511136ea3c5a already pushed, skipping\nImage 6170bb7b0ad1 already pushed, skipping\n79fdb1362c84: Pushing [==================================================>] 215.5 MB/215.5 MB\n2014/04/24 18:14:06\n```\n\nAnd I can pull it at my server:\n```\n\ndocker pull docker.myhost.com/postgresql\nPulling repository docker.myhost.com/postgresql\n```\n\nBut, when I'm trying to run I get:\n```\n\ndocker run docker.myhost.com/postgresql\nUnable to find image 'docker.myhost.com/postgresql' locally\nPulling repository docker.stage.myhost.com/postgresql\n2014/04/24 08:17:05 Error: No such image: docker.myhost.com/postgresql (tag: latest)\n```\n\nThis is in registry container logs:\n2014-04-24 15:16:26,827 INFO: 172.17.42.1 - - [24/Apr/2014:15:16:26] \"GET /v1/_ping HTTP/1.0\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-04-24 15:16:26,833 DEBUG: check_session: Session is empty\n172.17.42.1 - - [24/Apr/2014:15:16:26] \"GET /v1/repositories/postgresql/images HTTP/1.0\" 200 1368 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-38-generic os/linux arch/amd64\"\n2014-04-24 15:16:26,834 INFO: 172.17.42.1 - - [24/Apr/2014:15:16:26] \"GET /v1/repositories/postgresql/images HTTP/1.0\" 200 1368 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-38-generic os/linux arch/amd64\"\n2014-04-24 15:16:26,842 DEBUG: check_session: Session is empty\n2014-04-24 15:16:26,842 DEBUG: [get_tags] namespace=library; repository=postgresql\n172.17.42.1 - - [24/Apr/2014:15:16:26] \"GET /v1/repositories/library/postgresql/tags HTTP/1.0\" 200 2 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-38-generic os/linux arch/amd64\"\n2014-04-24 15:16:26,843 INFO: 172.17.42.1 - - [24/Apr/2014:15:16:26] \"GET /v1/repositories/library/postgresql/tags HTTP/1.0\" 200 2 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-38-generic os/linux arch/amd64\"\n172.17.42.1 - - [24/Apr/2014:15:16:30] \"GET /v1/_ping HTTP/1.0\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-04-24 15:16:30,261 INFO: 172.17.42.1 - - [24/Apr/2014:15:16:30] \"GET /v1/_ping HTTP/1.0\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-04-24 15:16:30,267 DEBUG: check_session: Session is empty\n172.17.42.1 - - [24/Apr/2014:15:16:30] \"GET /v1/repositories/postgresql/images HTTP/1.0\" 200 1368 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-38-generic os/linux arch/amd64\"\n2014-04-24 15:16:30,268 INFO: 172.17.42.1 - - [24/Apr/2014:15:16:30] \"GET /v1/repositories/postgresql/images HTTP/1.0\" 200 1368 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-38-generic os/linux arch/amd64\"\n2014-04-24 15:16:30,273 DEBUG: check_session: Session is empty\n2014-04-24 15:16:30,274 DEBUG: [get_tags] namespace=library; repository=postgresql\n172.17.42.1 - - [24/Apr/2014:15:16:30] \"GET /v1/repositories/library/postgresql/tags HTTP/1.0\" 200 2 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-38-generic os/linux arch/amd64\"\n2014-04-24 15:16:30,277 INFO: 172.17.42.1 - - [24/Apr/2014:15:16:30] \"GET /v1/repositories/library/postgresql/tags HTTP/1.0\" 200 2 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-38-generic os/linux arch/amd64\"\n172.17.42.1 - - [24/Apr/2014:15:17:05] \"GET /v1/_ping HTTP/1.0\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-04-24 15:17:05,136 INFO: 172.17.42.1 - - [24/Apr/2014:15:17:05] \"GET /v1/_ping HTTP/1.0\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-04-24 15:17:05,143 DEBUG: check_session: Session is empty\n172.17.42.1 - - [24/Apr/2014:15:17:05] \"GET /v1/repositories/postgresql/images HTTP/1.0\" 200 1368 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-38-generic os/linux arch/amd64\"\n2014-04-24 15:17:05,144 INFO: 172.17.42.1 - - [24/Apr/2014:15:17:05] \"GET /v1/repositories/postgresql/images HTTP/1.0\" 200 1368 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-38-generic os/linux arch/amd64\"\n2014-04-24 15:17:05,149 DEBUG: check_session: Session is empty\n2014-04-24 15:17:05,150 DEBUG: [get_tags] namespace=library; repository=postgresql\n172.17.42.1 - - [24/Apr/2014:15:17:05] \"GET /v1/repositories/library/postgresql/tags HTTP/1.0\" 200 2 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-38-generic os/linux arch/amd64\"\n2014-04-24 15:17:05,150 INFO: 172.17.42.1 - - [24/Apr/2014:15:17:05] \"GET /v1/repositories/library/postgresql/tags HTTP/1.0\" 200 2 \"-\" \"docker/0.10.0 go/go1.2.1 git-commit/dc9c28f kernel/3.8.0-38-generic os/linux arch/amd64\"\n. ",
    "rinrinne": "Me too.\n- docker 1.0.1\n- registry 0.7.3 image\nregistry container runs behind nginx proxy on host os.\n$ docker pull node:0.10.28\n$ docker tag node:0.10.28 HOSTNAME/node:0.10.28\n$ docker push HOSTNAME/node:0.10.28\nThe below is registry log:\n172.17.42.1 - - [30/Jun/2014:11:13:43] \"GET /v1/_ping HTTP/1.0\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-06-30 11:13:43,852 INFO: 172.17.42.1 - - [30/Jun/2014:11:13:43] \"GET /v1/_ping HTTP/1.0\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-06-30 11:13:43,862 DEBUG: args = {}\n172.17.42.1 - - [30/Jun/2014:11:13:43] \"PUT /v1/repositories/node/ HTTP/1.0\" 200 2 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-30-generic os/linux arch/amd64\"\n2014-06-30 11:13:43,899 INFO: 172.17.42.1 - - [30/Jun/2014:11:13:43] \"PUT /v1/repositories/node/ HTTP/1.0\" 200 2 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-30-generic os/linux arch/amd64\"\n2014-06-30 11:13:43,908 DEBUG: args = {'image_id': u'511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158'}\n172.17.42.1 - - [30/Jun/2014:11:13:43] \"GET /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.0\" 200 483 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-30-generic os/linux arch/amd64\"\n2014-06-30 11:13:43,911 INFO: 172.17.42.1 - - [30/Jun/2014:11:13:43] \"GET /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.0\" 200 483 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-30-generic os/linux arch/amd64\"\n2014-06-30 11:13:43,919 DEBUG: args = {'image_id': u'1e8abad02296ab9c600564b43d4f3e34855cfd40433f32e2ba90aaab37b07f7d'}\n2014-06-30 11:13:43,920 DEBUG: api_error: Image is being uploaded, retry later\n172.17.42.1 - - [30/Jun/2014:11:13:43] \"GET /v1/images/1e8abad02296ab9c600564b43d4f3e34855cfd40433f32e2ba90aaab37b07f7d/json HTTP/1.0\" 400 49 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-30-generic os/linux arch/amd64\"\n2014-06-30 11:13:43,922 INFO: 172.17.42.1 - - [30/Jun/2014:11:13:43] \"GET /v1/images/1e8abad02296ab9c600564b43d4f3e34855cfd40433f32e2ba90aaab37b07f7d/json HTTP/1.0\" 400 49 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-30-generic os/linux arch/amd64\"\n2014-06-30 11:13:43,936 DEBUG: args = {'image_id': u'1e8abad02296ab9c600564b43d4f3e34855cfd40433f32e2ba90aaab37b07f7d'}\n172.17.42.1 - - [30/Jun/2014:11:13:43] \"PUT /v1/images/1e8abad02296ab9c600564b43d4f3e34855cfd40433f32e2ba90aaab37b07f7d/json HTTP/1.0\" 200 4 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-30-generic os/linux arch/amd64\"\n2014-06-30 11:13:43,942 INFO: 172.17.42.1 - - [30/Jun/2014:11:13:43] \"PUT /v1/images/1e8abad02296ab9c600564b43d4f3e34855cfd40433f32e2ba90aaab37b07f7d/json HTTP/1.0\" 200 4 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-30-generic os/linux arch/amd64\"\nThe below is nginx log:\n10.158.51.66 - - [30/Jun/2014:20:13:43 +0900] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n10.158.51.66 - - [30/Jun/2014:20:13:43 +0900] \"PUT /v1/repositories/node/ HTTP/1.1\" 200 2 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-30-generic os/linux arch/amd64\"\n10.158.51.66 - - [30/Jun/2014:20:13:43 +0900] \"GET /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1\" 200 483 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-30-generic os/linux arch/amd64\"\n10.158.51.66 - - [30/Jun/2014:20:13:43 +0900] \"GET /v1/images/1e8abad02296ab9c600564b43d4f3e34855cfd40433f32e2ba90aaab37b07f7d/json HTTP/1.1\" 400 49 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-30-generic os/linux arch/amd64\"\n10.158.51.66 - - [30/Jun/2014:20:13:43 +0900] \"PUT /v1/images/1e8abad02296ab9c600564b43d4f3e34855cfd40433f32e2ba90aaab37b07f7d/json HTTP/1.1\" 200 4 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-30-generic os/linux arch/amd64\"\n(Sorry timestamp is different since I'm in Japan (UTC+9))\nSeems like docker has not pushed any tags to private registry...\nThere is only '_index_images' file in registry/repositories/library/node.\n(no tag* file)\n. Issue was solved by moving listen port from 80 to 5100 in nginx.\nSeems image name should have \"host:port\" explicitly...\n. ",
    "skarnik-rmn": "Noticed this error with v 0.6.8 \n. Agree, I was deploying new version of the registry and our config had some anomaly that led registry to be not so happy and I discovered the error.\n. Possibly duplicate of #400\n. ",
    "cattz": "Thanks a lot!!!\nI'd be more than happy to collaborate once I get my python skills up to date :blush: \n. Hi,\nLet me explain the nature of our problem, as may be a better way to address this. The most important hook for us at this moment is the pre-push, to prevent tags from being overwritten.\nOur process\n\nAs part of our continuous delivery pipeline, we'll be generating docker images as build artifacts.\nThose images (or repositories in registry lingo) are tagged and then pushed to our private docker registry by our CI (Bamboo)\nTo deploy this into production, we plan to do a docker pull image:tag and run the container.\n\nIn order to make sure we know exactly what is running in production, we should prevent tags from being overwritten with a different image, hence the need of this hook.\nApproach\nSomehow, we're trying to mimic the git or svn hooks for this, so anyone trying to write their own hooks won't need to mess around with docker-registry code.\nSo far, we've added a call to before_request in app.pythat will call some scripts in a hooks package:\n``` python\n@app.before_request\ndef before_request():\n    from hooks.block_existing_tag import process_request\n    process_request(flask.request)\n@app.errorhandler(InvalidUsage)\ndef handle_invalid_usage(error):\n    response = jsonify(error.to_dict())\n    response.status_code = error.status_code\n    return response\n```\nWe added an excepcions.py file:\n``` python\nclass InvalidUsage(Exception):\n    status_code = 400\ndef __init__(self, message, status_code=None, payload=None):\n    Exception.__init__(self)\n    self.message = message\n    if status_code is not None:\n        self.status_code = status_code\n    self.payload = payload\n\ndef to_dict(self):\n    rv = dict(self.payload or ())\n    rv['message'] = self.message\n    return rv\n\n```\nAnd a dummy hook in hooks/block_existing_tag.py:\n``` python\nfrom docker_registry.exceptions import InvalidUsage\ndef process_request(request):\n    if request.path == u'/v1/repositories/pof/images':\n        raise InvalidUsage(\"You can't do that\", status_code=403)\n```\nPlease, note all code above is totally experimental, any advise on it will be welcome, but it's been written just for testing purposes\n@shin- I'm trying to understand how lib/signals.py can help with this, but not entirely sure on how this works\n@joelwurtz We're also interested in the post-push hook, but that is secondary at the moment. It will be interesting to know a bit more about your approach.\nI our case, both approaches (per docker index and per docker repo) will be valid.\nI don't understand this bit:\n\nBut is it better to set the hook within the put image api call or should we add an api for setting / getting an hook on a namespace/repository ?\n\nI'm still unsure on where is the best place to call this hooks, but would like to make it as open and reusable as possible.\n. ",
    "joelwurtz": "Hi,\nJust see this issue, wanted to do the same, i will be glad to make the PR for this (but only for hook when an image has been pushed), if it's ok with you @cattz ?\nBut before i got some questions on how to implement this, there is 2 ways and i would like to know your opinion before.\nConfiguration\nThe first way will simply be a configuration value in the yaml file, which indicate a http entrypoint for the hook.\nIt will be simple to implement the only problem is that the entrypoint will have all the images pushed to it and this cannot be applied to a public repository.\nPer docker repository\nThe second way is to have a push per image repository name this will allow to be useful for public repository.\nBut is it better to set the hook within the put image api call or should we add an api for setting / getting an hook on a namespace/repository ?\nI have also a question on where to call the hook ? From what i have seen it's better to run this post push hook in the create tag api call, but i'm not sure :s \nCheers,\n. Hi,\nMy use case is completely different, and yours is an interesting one. \nFor my part, i want some externals services to be notified when a new version of an image is available (for continuous integration / deployment by example). A developer push a image to the registry, which send a hook to the CI, pull the new image and run tests within. \nIn this order i need to need to set a http url to be called with image informations (after all the layer has been stored)\n\nI don't understand this bit:\n\nBut is it better to set the hook within the put image api call or should we add an api for setting / getting an hook on a namespace/repository ?\n\n\nAs you can seen we have 2 differents use case, it will be ok if the registry is only private, but he can also be used for a public one or private shared (PaaS provider for example), and like everyone can have different use case it's better to separate/configure how are handle hooks for each repository/namespace couple as it's the identifier for a vendor in the registry.\nAnd i'm wondering on how we set this, as the vendor may not have access to write plugins/python in the registry. (The best part \n(Like a setting in index.docker.io to set a http hook on image in the same way as a Webhook on github)\nFor the lib/signals.py part from what i have seen it's an event system, there is actually 5 event dispatched : \n- repository_created\n- repository_updated\n- repository_deleted\n- tag_created\n- tag_deleted\nFor your use case i think repository_updated is the best option. But be aware, this event it's called twice when an image is push, once when storage is created for the new image in the beginning, and the second time when checksum are checked at the end of the call : http://docs.docker.io/reference/api/registry_index_spec/.\nYou just have to attach your call to the function, it may be good also to stop execution of the following code when a response is returned in it but i do not know if it's something possible within signals /cc @shin- ?\n. ",
    "matthughes": "I was actually trying to just use the swift option. \nhttps://github.com/dotcloud/docker-registry/blob/master/config/config_sample.yml\n```\nThis flavor is for storing images in Openstack Swift\nswift: &swift\n    storage: swift\n    storage_path: _env:STORAGE_PATH:/registry\n    # keystone authorization\n    swift_authurl: _env:OS_AUTH_URL\n    swift_container: _env:OS_CONTAINER\n    swift_user: _env:OS_USERNAME\n    swift_password: _env:OS_PASSWORD\n    swift_tenant_name: _env:OS_TENANT_NAME\n    swift_region_name: _env:OS_REGION_NAME\n```\n. @dmp42 @bacongobbler It's unclear to me exactly what changes you'd expect me to make.  The only one that I see in his that isn't in mine is the proxy_read_timeout 900;  Default is set to 60 seconds.  I find it hard to believe that is the issue.\nIf you look at my original issue (https://github.com/docker/docker/issues/7485), everything is solved when I delete the .dockercfg and relog in.  And it is sporadic in nature.\nAs a side note, how long are the credentials in .dockercfg good for?  It's my understanding that it's just a hash of the password right?  It's not tied to the server version, etc is it?\n. @dmp42 In my build environment, I already have controlled yum mirrors.  I wanted to avoid having controlled pip mirrors as well.  Regardless, there is a doker-registry rpm/deb on various Linux distros.  How does that get built?\nI ended up doing it in two stages:\n1) pip install --download=stagingDir depends/registry-common/requirements.txt && pip install --download=stagingDir requirements.txt\nThis is done once outside of build environment.  Then using the 'stagingDir', I am free to pip install everything locally:\n2) pip install --no-index --find-links=stagingDir depends/registry-common/ && pip install --no-index --find-links=stagingDir .\nFor step 1, I also had to add some of the optional dependencies listed in setup.py because I'm running python 2.6.6 on Centos.\n. Appears this is a regression introduced in 0.7.2.  The above command worked fine when run against 0.7.1.\n. Same config running against 0.7.1 vs 0.7.2.  The standalone mode stays the same in the headers, but still no users endpoint:\n```\n\n< HTTP/1.1 200 OK\n Server gunicorn/18.0 is not blacklisted\n< Server: gunicorn/18.0\n< Date: Fri, 18 Jul 2014 19:25:53 GMT\n< Connection: keep-alive\n< X-Docker-Registry-Standalone: mirror\n< Expires: -1\n< Content-Type: application/json\n< Pragma: no-cache\n< Cache-Control: no-cache\n< Content-Length: 4\n< X-Docker-Registry-Version: 0.7.1\n< X-Docker-Registry-Config: dev\n<\n Connection #0 to host localhost left intact\ntruevagrant@vagrant-ubuntu-trusty-64:/vagrant/docker/registry$ curl --insecure -v http://localhost:5000/_ping\n Hostname was NOT found in DNS cache\n   Trying 127.0.0.1...\n Connected to localhost (127.0.0.1) port 5000 (#0)\nGET /_ping HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: localhost:5000\nAccept: /*\n< HTTP/1.1 200 OK\n Server gunicorn/18.0 is not blacklisted\n< Server: gunicorn/18.0\n< Date: Fri, 18 Jul 2014 19:26:02 GMT\n< Connection: keep-alive\n< X-Docker-Registry-Standalone: mirror\n< Expires: -1\n< Content-Type: application/json\n< Pragma: no-cache\n< Cache-Control: no-cache\n< Content-Length: 4\n< X-Docker-Registry-Version: 0.7.2\n< X-Docker-Registry-Config: dev\n<\n Connection #0 to host localhost left intact\ntruevagrant@vagrant-ubuntu-trusty-64:/vagrant/docker/registry$ curl --insecure -v http://localhost:5000/v1/users/\n Hostname was NOT found in DNS cache\n   Trying 127.0.0.1...\n Connected to localhost (127.0.0.1) port 5000 (#0)\nGET /v1/users/ HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: localhost:5000\nAccept: /*\n< HTTP/1.1 404 NOT FOUND\n Server gunicorn/18.0 is not blacklisted\n< Server: gunicorn/18.0\n< Date: Fri, 18 Jul 2014 19:26:16 GMT\n< Connection: keep-alive\n< Content-Type: text/html\n< Content-Length: 233\n< X-Docker-Registry-Version: 0.7.2\n< X-Docker-Registry-Config: dev\n<\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n404 Not Found\nNot Found\nThe requested URL was not found on the server.  If you entered the URL manually please check your spelling and try again.\n Connection #0 to host localhost left intact\n```\n. That's what I am using.  I just was trying to help by figuring out where the bug was introduced.\n. Yes that was the issue!  Thanks.  Did seem a bit too strange to be real.\n. @wking Is there a timeline for the go rewrite?  I had not even heard about it until now.  Just curious.\n. \n",
    "abonas": "@dmp42, @samalba thanks for reviewing and approving this.\nI just noticed that there's an authors file that lists all contributors - was I supposed to add credentials there as part of this patch? (it wasn't mentioned in project's readme file as contribution instruction). If yes, is it OK to add it even after my patch was merged?\nthank you\n. @dmp42, do you know when this change will be reflected in the public registry?\n. @dmp42  thank you for the update!!\nI just added #345 that is related, it would be nice to have it as well.\n. @dmp42 is the change applied already in the main registry? I didn't see it when accessing with REST api. thank you\n. @dmp42 I just checked on a clean Fedora 20 - gcc xz-devel libffi-devel are all needed in order to run the registry itself. I'll move them to that section.\nAs for git - though it isn't required to run, but it would be strange not to mention it at all , because otherwise it will be impossible to clone the repo/send pull requests :)\n. @dmp42  I just did an update of the readme. Please have a look.\nOn a related topic: there's \"an old way\" and \"fast way\" of running the registry mentioned in the README. Very confusing and not clear why both are needed. Would be great if someone could clarify/arrange it to be clear in the README. Thank you.\n. @dmp42 I moved the git to the relevant section next to the tests as you suggested.\n. @dmp42 @samalba  added link to docker installation as well.\n. @dmp42 if you could have a look on this - much appreciated.\nAnd if you know someone who can reply on the below - appreciated as well:\nhttps://groups.google.com/forum/#!topic/docker-user/aWbEBrxrqzQ\n. @shin- thank you.\nThe use case for using the api from a browser is to allow users of other applications (for example - oVirt) to search and select repos/images to pull and run from within that application.\n. @shin-, indeed, but it lacked CORS. didn't #338 add CORS to it? \n. @shin- thanks for clarification. it's really confusing.\nso the #338 is at least valid for whoever installs a private repo?\n. @shin- thanks, I'll contact the support as you suggested. \nAs per this specific pull request that deals with tags - the use case I was thinking about is the following:\n1. search for repos (that was the other pull request that added CORS to search)\n2. for a specific selected repo, see all available tags in order to select one tag - not to pull the entire repo. That is what this pull request is about - to add CORS support for use case when user needs to see tags per repo.\n. ",
    "phemmer": "I'm happy to start on this. Shouldn't be hard.\nOne question though. Do we want to have separate config files for each storage engine and let the user set DOCKER_REGISTRY_CONFIG, or default to a single config and let the user set SETTINGS_FLAVOR?\n. Closing as #350 has been merged.\n. I'd also be up for adding another commit to this PR to update the README. A lot of variables are missing (or are scattered around), and the supported storage engines aren't listed very well. I just want to make sure the code is proper before continuing to documentation.\n. > Why did you drop the 'exec' in your Dockerfile CMD?  I don't see a\n\nreason to keep the shell process around.\n\nJust an omission when cleaning it up. I can put it back, or we can go with the CMD [\"/usr/local/bin/docker-registry\"] syntax and avoid the shell. I would lean a bit towards exec though, as it's more flexible and resistant to breakage.\n\nI still think it would be better to just have a STORAGE variable\ninstead of SETTINGS_FLAVOR blocks [1].  If STORAGE=s3, it doesn't\nmatter if there are also a bunch of swift or glance configs set,\nyou're not going to use them.  Having a single block would also avoid\nfragmentation like AWS_BUCKET and GCS_BUCKET variables for the same\nboto_bucket setting.  It also avoids the whole glance-swift business,\njust set:\nSTORAGE=glance GLANCE_STORAGE_ALTERNATE=swift\n\nI think I see what you're getting at.\nI think that would work as long as the app doesn't do anything if a config param is set and not used. Like if storage: s3 and elliptics_addr_family is set.\nI wasn't sure if this is the case, so I kept everything separate.\nThe other argument for keeping them separate is to have different defaults for the different storage engines. Like for local storage, storage_path defaults to /tmp/registry. But for S3, it defaults to /registry.\n. Ok, i've updated for the exec in the Dockerfile since that seems to be agreed upon.\nAre we settled upon leaving env.yml as is? If so I'll update the README.\n. Ok, I've merged most of into config_sample.yml.\nI did not symlink config_mirror.yml to it though. This is because config_mirror.yml defaults to enabling mirroring, while the config_sample.yml did not. Thus joining them together would be a significant breaking change.\nIt's a separate commit for now, but if this is the route we go, I'll squash them.\n. I've fixed tox.ini and rebased the branch. Not sure where the travis build went.\nAnyway, I'll work on updating the README to reflect the changes.\n. Ok, I've gone through and updated the documentation.\nI moved a few things around so that it's not so scattered. I moved all the config stuff that is independent of the storage engine to the top of the 'config options' section. All the storage engines and the parameters they support are then at the bottom.\nI also added a quick start to the very top.\nI also added several parameters I ran across during my work that weren't documented. However there are several which I didn't add an explanation on as I'm not sure how to best describe them. These are: the mirroring options, the swift options, the glance options, and cache/cache_lru (whats the difference).\nQuick link to view README: https://github.com/phemmer/docker-registry/tree/config_cleanup#docker-registry\nAdditional note: I also wanted to remove the numbered lists and use bullet lists instead (but didn't). I personally think they look cleaner, especially once you start nesting. However I didn't want to change formatting since numbered lists were originally used.\n. elliptics_nodes change has been opened as noxiouz/docker-registry-driver-elliptics#4\n. Having this issue as well. When I start with a region set, it just hangs during startup. Without the region it starts up fine.\n```\ndocker run \\\n-e SETTINGS_FLAVOR=s3 \\\n  -e AWS_REGION=us-west-1 \\\n  -e AWS_BUCKET=cloudcom-cliff-packages \\\n  -e AWS_KEY=FOO \\\n  -e AWS_SECRET=BAR \\\n  -e AWS_PATH=/docker/registry \\\n  -e STORAGE_PATH=/docker/registry \\\n  -e SEARCH_BACKEND=sqlalchemy  \\\n  -t -i \\\n  registry:0.7.0              \n2014-06-05 19:53:16,282 DEBUG: Will return docker-registry.drivers.s3.Storage\n2014-06-05 19:53:16,284 DEBUG: Using access key provided by client.\n2014-06-05 19:53:16,284 DEBUG: Using secret key provided by client.\n2014-06-05 19:53:16,285 DEBUG: path=/\n2014-06-05 19:53:16,285 DEBUG: auth_path=/cloudcom-cliff-packages/\n2014-06-05 19:53:16,285 DEBUG: Method: HEAD\n2014-06-05 19:53:16,285 DEBUG: Path: /\n2014-06-05 19:53:16,285 DEBUG: Data: \n2014-06-05 19:53:16,286 DEBUG: Headers: {}\n2014-06-05 19:53:16,286 DEBUG: Host: cloudcom-cliff-packages.s3-us-west-1.amazonaws.com\n2014-06-05 19:53:16,286 DEBUG: Port: 80\n2014-06-05 19:53:16,286 DEBUG: Params: {}\n2014-06-05 19:53:16,286 DEBUG: establishing HTTP connection: kwargs={'port': 80, 'timeout': 70}\n2014-06-05 19:53:16,286 DEBUG: Token: None\n2014-06-05 19:53:16,287 DEBUG: StringToSign:\nHEAD\nThu, 05 Jun 2014 19:53:16 GMT\n/cloudcom-cliff-packages/\n2014-06-05 19:53:16,287 DEBUG: Signature:\nAWS AKIAI72GUHBWOGEM6O7A:UCa/+gWFxmm3+zg7r7YEedATvXI=\n```\n. I created a brand new bucket (bucket currently completely empty), and a brand new set of credentials. The credentials below are functional, but have read only access to the bucket.\n```\ndocker run \\\n  -e SETTINGS_FLAVOR=s3 \\\n  -e AWS_BUCKET=docker-registry-400-test \\\n  -e AWS_KEY=AKIAI4CU3BUIRSLWC54Q \\\n  -e AWS_SECRET=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 \\\n  -e AWS_PATH=/docker/registry \\\n  -e STORAGE_PATH=/docker/registry \\\n  -e SEARCH_BACKEND=sqlalchemy \\\n  -e AWS_REGION=us-west-1 \\\n  -t -i \\\n  registry:0.7.0\n2014-06-06 01:49:27,853 DEBUG: Will return docker-registry.drivers.s3.Storage\n2014-06-06 01:49:27,854 DEBUG: Using access key provided by client.\n2014-06-06 01:49:27,854 DEBUG: Using secret key provided by client.\n2014-06-06 01:49:27,854 DEBUG: path=/\n2014-06-06 01:49:27,854 DEBUG: auth_path=/docker-registry-400-test/\n2014-06-06 01:49:27,854 DEBUG: Method: HEAD\n2014-06-06 01:49:27,854 DEBUG: Path: /\n2014-06-06 01:49:27,854 DEBUG: Data: \n2014-06-06 01:49:27,854 DEBUG: Headers: {}\n2014-06-06 01:49:27,855 DEBUG: Host: docker-registry-400-test.s3-us-west-1.amazonaws.com\n2014-06-06 01:49:27,855 DEBUG: Port: 80\n2014-06-06 01:49:27,855 DEBUG: Params: {}\n2014-06-06 01:49:27,855 DEBUG: establishing HTTP connection: kwargs={'port': 80, 'timeout': 70}\n2014-06-06 01:49:27,855 DEBUG: Token: None\n2014-06-06 01:49:27,855 DEBUG: StringToSign:\nHEAD\nFri, 06 Jun 2014 01:49:27 GMT\n/docker-registry-400-test/\n2014-06-06 01:49:27,855 DEBUG: Signature:\nAWS AKIAI4CU3BUIRSLWC54Q:AVbAuKqhCl0fLI9uLjz7mws8zSY=\n^CKeyboardInterrupt\n** [Bugsnag] No API key configured, couldn't notify\nTraceback (most recent call last):\n  File \"/usr/local/bin/docker-registry\", line 9, in \n    load_entry_point('docker-registry==0.7.0', 'console_scripts', 'docker-registry')()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 351, in load_entry_point\n    return get_distribution(dist).load_entry_point(group, name)\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2363, in load_entry_point\n    return ep.load()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2088, in load\n    entry = import(self.module_name, globals(),globals(), ['name'])\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/run.py\", line 16, in \n    from .tags import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/tags.py\", line 21, in \n    store = storage.load()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/storage/init.py\", line 36, in load\n    _storage[kind] = engine.fetch(kind)(None, config=cfg)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/drivers/s3.py\", line 65, in init\n    super(Storage, self).init(path, config)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/boto.py\", line 135, in init\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 471, in get_bucket\n    return self.head_bucket(bucket_name, headers=headers)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 490, in head_bucket\n    response = self.make_request('HEAD', bucket_name, headers=headers)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 633, in make_request\n    retry_handler=retry_handler\n  File \"/usr/local/lib/python2.7/dist-packages/boto/connection.py\", line 1046, in make_request\n    retry_handler=retry_handler)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/connection.py\", line 922, in _mexe\n    request.body, request.headers)\n  File \"/usr/lib/python2.7/httplib.py\", line 973, in request\n    self._send_request(method, url, body, headers)\n  File \"/usr/lib/python2.7/httplib.py\", line 1007, in _send_request\n    self.endheaders(body)\n  File \"/usr/lib/python2.7/httplib.py\", line 969, in endheaders\n    self._send_output(message_body)\n  File \"/usr/lib/python2.7/httplib.py\", line 829, in _send_output\n    self.send(msg)\n  File \"/usr/lib/python2.7/httplib.py\", line 791, in send\n    self.connect()\n  File \"/usr/lib/python2.7/httplib.py\", line 772, in connect\n    self.timeout, self.source_address)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 570, in create_connection\n    for res in getaddrinfo(host, port, 0 if has_ipv6 else AF_INET, SOCK_STREAM):\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 621, in getaddrinfo\n    return get_hub().resolver.getaddrinfo(host, port, family, socktype, proto, flags)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/resolver_thread.py\", line 34, in getaddrinfo\n    return self.pool.apply_e(self.expected_errors, _socket.getaddrinfo, args, kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/threadpool.py\", line 222, in apply_e\n    success, result = self.spawn(wrap_errors, expected_errors, function, args, kwargs).get()\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/event.py\", line 233, in get\n    result = self.hub.switch()\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/hub.py\", line 331, in switch\n    return greenlet.switch(self)\nKeyboardInterrupt\n```\nAlso, this was working just fine as of 3964f29bc8. I don't know what commit broke it, but I was running a custom build from that commit until when I tried to upgrade today.\n. @danielschonfeld Make sure your key & secret have permission to access the bucket. Removing AWS_REGION is the current workaround for this issue, so you've got something else going on for that 403 error.\n. @frankamp What you describe sounds like #348.\n. This, and the above changes, are to treat defined but unset config parameters as undefined. This is so that if the config references an environment variable which doesn't exist, we act as if that config param weren't defined.\n. This is to allow passing the elliptics node list through a string (so an env var can be used). Previously elliptics_nodes only accepted a hash/dictionary. This change makes it so that it also accepts a list/array or a string.\nFor example:\nelliptics:\n  elliptics_nodes:\n    - 1.1.1.1:2222\n    - 1.1.1.2:2222\nor\nelliptics:\n  elliptics_nodes: \"1.1.1.1:2222 1.1.1.2:2222\"\n. I'm just going off what wking said. If we feel they should be just removed, I can go with that.\n. Done\n. ",
    "dpritchett": "Not sure if this issue is a good place for it, but I was surprised to only see webhooks available for successful push events on https://registry.hub.docker.com/ (I hope this repo is what's behind that site...)\nCI tools usually let you know when a build fails, not just when it passes.  Would anyone else like to see that here?\n. Whoops :blush:\n. ",
    "paulczar": "This breaks the nova-docker driver from easily using the registry with a glance backend.\n'Traceback (most recent call last):\n  File \"/usr/local/bin/docker-registry\", line 9, in <module>\n    load_entry_point('docker-registry==0.7.0', 'console_scripts', 'docker-registry')()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 351, in load_entry_point\n    return get_distribution(dist).load_entry_point(group, name)\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2363, in load_entry_point\n    return ep.load()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2088, in load\n    entry = __import__(self.module_name, globals(),globals(), ['__name__'])\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/run.py\", line 16, in <module>\n    from .tags import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/tags.py\", line 21, in <module>\n    store = storage.load()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/storage/__init__.py\", line 36, in load\n    _storage[kind] = engine.fetch(kind)(None, config=cfg)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/driver.py\", line 241, in fetch\n    % (name, name, available())\ndocker_registry.core.exceptions.NotImplementedError: You requested storage driver docker_registry.drivers.glance\nwhich is not installed. Try `pip install docker_registry-drivers-glance`\nor check your configuration. The following are currently\navailable on your system: ['dumb', 'file', 's3']\nit would be good for the default Dockerfile in the repo to include the glance and swift storage drivers.\n. @shin- @dmp42 that sounds like a good viable option ...   would be good to get a trusted build out of in the root namespace on index.docker.io\n. without this patch the following error occurs when running the glance driver\noot@a5dcb4d7ff32:/# SETTINGS_FLAVOR=glance OS_USERNAME=demo OS_PASSWORD=admin OS_TENANT_NAME=demo OS_GLANCE_URL=\"http://10.0.2.15:9292\" OS_AUTH_URL=http://127.0.0.1:5000/v2.0 docker-registry\n2014-06-20 14:50:54,557 DEBUG: Will return docker-registry.drivers.glance.Storage\nlocal\n** [Bugsnag] No API key configured, couldn't notify\nTraceback (most recent call last):\n  File \"/usr/local/bin/docker-registry\", line 9, in <module>\n    load_entry_point('docker-registry==0.7.3', 'console_scripts', 'docker-registry')()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 351, in load_entry_point\n    return get_distribution(dist).load_entry_point(group, name)\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2363, in load_entry_point\n    return ep.load()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2088, in load\n    entry = __import__(self.module_name, globals(),globals(), ['__name__'])\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/run.py\", line 16, in <module>\n    from .tags import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/tags.py\", line 21, in <module>\n    store = storage.load()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/storage/__init__.py\", line 38, in load\n    config=cfg)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/drivers/glance.py\", line 74, in __init__\n    alt = driver.fetch(kind)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/driver.py\", line 228, in fetch\n    % (name, name, available())\ndocker_registry.core.exceptions.NotImplementedError: You requested storage driver docker_registry.drivers.local\nwhich is not installed. Try `pip install docker-registry-driver-local`\nor check your configuration. The following are currently\navailable on your system: ['dumb', 'file', 'glance', 's3']\n. Yeah I can confirm that fixes it to the point that the registry will now start.    Pushing images gives a 405 error and no good debug info in the logs ...  haven't been able to pin anything specific down in the glance storage driver yet ... but I haven't spent much time on it.\n. ",
    "hansthen": "Never mind. I had hoped it would be useful, but if not, my apologies for the inconvenience.\n. I experienced the error quoted below. I recontructed the error message using an import from the prompt. I could import the regular lzma module. After that the registry worked. I did not run a full testsuite, so I was a bit sloppy to submit the patch so soon.\nI guess I better upgrade my python version to 2.7.\n\n\n\nimport backports.lzma as lzma\nTraceback (most recent call last):\n  File \"\", line 1, in \nImportError: No module named lzma\n. \n\n\n",
    "panjunyong": "I have this problem too.\n. ",
    "jbardin": "I don't have that registry up any longer, since it needed to be repiared.\nWe think that the push failed while sending a layer, and never completed.\nI think we ended up with a layer in _index_images which didn't exist in S3. \n. no, the cleanup wasn't done. The registry may have had a slightly older version than I thought when that tag was committed. Let me verify this again :confused: \n. Thanks!\nRunning 0.6.9, and cleaning the index files, and clearing the cache fixes this :+1: \n. ",
    "Sciencegeek123": "Has any progress been made on this issue? I'm currently running into it using AWS ECR.\n. ",
    "xiamx": "Hi @dmp42 built master and everything works fine. Thanks for the help!\n. sure\n. ",
    "jlhawn": "also in the Dockerfile: CMD exec docker-registry seems strange. Why not just CMD docker-registry?\n. +1 on everything here\n. okay! LGTM\n. @ncdc said:\n\ndocker pull foo/bar:latest@D1\ndocker pull foo/bar:latest@D2\n\nI kind of like this style of specifying a version a la a git commit.\n@stevvooe I don't think it would be too difficult to add this feature. The registry can still be dumb about the content, just hash the manifest/jws payload getting some content-addressable ID for the manifest like e36eb1f73548649b.... So when I push an image like:\ndocker push jlhawn/my-app:3.1.4\nThe registry will store a manifest that is addressable either by that name (until the name is deleted/updated) or by the hash e36eb1f73548649b... until it is explicitly deleted by the user/administrator.\nSo I could pull it using:\njlhawn/my-app:3.1.4\nor:\njlhawn/my-app@e36eb1f73548649b...\nPersonally, I would prefer that people not rely on \"alias\" tags but instead treat tags like version numbers and not allow users to push the same version again. The registry could see that there already exists a manifest with this name and refuse to overwrite it. This would force the user/admin to explicitly delete that\nversion of the manifest from the registry. I think this would essentially enforce the desired behavior - with the exception being that a user may delete a manifest version then re-upload a changed one with the original name, but they couldn't do it by accidentally overwriting it.\n. ping @dmcgowan @dmp42 @stevvooe @BrianBland \n. implements https://github.com/docker/docker/pull/9081\n. okay, @stevvooe do you want to merge this and I'll work on the refactor separately so you can start using this in the app?\n. ping @stevvooe @dmcgowan \n. LGTM\n. LGTM\n. yes, as long as we are sticking with printable characters (which we are) it's okay.\n. This is to verify that the signing key in the jwk header field and the public key in the first cert of the jwk->x5c field are indeed the same. From Section 4.7 of the Draft JWK spec:\nThe PKIX certificate containing the key value MUST be the first certificate.\nHowever, the JWS Spec says that the header may contain only the x5c field, which would allow us to avoid this check.\n. hmm, I should probably refactor this to handle both. Right now it only handles the case where the JWT header has either a kid or jwk field, but not a x5c field. I could then separate the x5c chain validation into a separate function that verifies it and returns the leaf key as a libtrust.PublicKey\n. oh, cool. Do the string formatter functions call strconv.Quote internally then?\n. I went looking, yes it does: http://golang.org/src/fmt/format.go#L358\n. ",
    "sirupsen": "Why is LRU a concern of the registry in the first place? Shouldn't we encourage an Nginx or Varnish setup that does this instead?\n. Ah, I see. We use local file storage, where I assume this doesn't give us much.\n. Would love to see this in. What's holding it back?\n. Note that this may not be backwards compatible for some users because of permission issues with local storage. \n. oh sweet. I must've missed that. We should have a big fat link in the README for it\n. ",
    "titanous": "No idea where the header is coming from, but other CloudFlare headers are consistently capitalized and the registry headers consistently lowercase.\n. A similar issue is docker/docker#4106, and I added a comment here: https://github.com/docker/docker/issues/9015#issuecomment-67552053\n. @ncdc As I mentioned earlier, we can't assume write access on the part of the user (or robot) doing the deploy. So unless the mark API only required read access to the image, this is not viable for us.\nFor instance, this would preclude using any public images on Docker Hub, as well as images created by other teams in larger organizations.\n. @ncdc Correct, I'm not proposing any policies with regards to garbage collection.\n. @ncdc As long as we can fetch manifests by digest without marking them, then it's totally fine.\n. ",
    "mattdm": "Yeah, I was just looking at the code -- sorry for putting this in the wrong place. Is there a better place to report this?\n. Okay, I'll do that. Thanks!\n. ",
    "shankarj": "@dmp42 Getting the error :\nDownloading/unpacking docker-registry-core>=2,<3 (from docker-registry==0.8.0)\n  Could not find a version that satisfies the requirement docker-registry-core>=2,<3 (from docker-registry==0.8.0) (from versions: 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.0.6)\nCleaning up...\nNo distributions matching the version for docker-registry-core>=2,<3 (from docker-registry==0.8.0)\nStoring debug log for failure in /.pip/pip.log\nI Searched PyPI and I found docker-registry-core 1.0.6. But the setup.py has this :\nrequirements.insert(0, 'docker-registry-core>=2,<3')\nAm I missing something here ?\n. @dmp42 Got it. I actually checked out version 0.7.3 from git and then built it. Worked like a charm ! Thank you for getting back quickly :) Appreciate it.\n. http://www.activestate.com/blog/2014/01/deploying-your-own-private-docker-registry. Was following the instructions here. (under deploy and configure the registry) @dmp42 \n. @dmp42 Appreciate it. Thanks a lot for the reply.\n. ",
    "developerinlondon": "ok now its running fine.\ni set the environment variable to the path of the config.yml file but not sure its using the config.yml, i tried running the following:\nexport DOCKER_REGISTRY_CONFIG=/home/coreos/docker_index/config.yml\ndocker run -p 5000:5000 registry\nbut it doesnt seem to be storing anything on s3. i filled in the values for \nboto_bucket: <s3_bucket_name>\ns3_access_key: <my aws access key>\ns3_secret_key: <my aws secret>\ns3_bucket: <my s3 bucket name>\ns3_encrypt: true\ns3_secure: true\nstorage: s3 # in all sections\nfeels like its an incorrect config somewhere?\n. ",
    "mccrodp": "Hi @dmp42 \nThat's great, thank you very much for your quick response. Ahhh I missed the pip install . or did not fully know what it was referring to when I read previously. It is clear to me now that you clone the repository, and run the pip install . command from within the repository. \nI got the pip install -r requirements.txt from this blog post, which may be a little dated as the directory structure did not match the current repository completely either - http://blog.docker.io/2013/07/how-to-use-your-own-registry/ \nI have now reinstalled and re-run using gunicorn and it seems to be up and running. Many thanks, I'm going to test custom directory structures now.\n. Thank you @dmp42. I still have a few issues with this new method so far. I used pip uninstall docker-registry and ensured that the script at /usr/local/bin/docker-registry was removed. I re-ran the install then using pip pip install docker-registry and ensure it was up-to-date using --upgrade flag pip install --upgrade docker-registry\n\ndocker-registry\n** [Bugsnag] No API key configured, couldn't notify\nTraceback (most recent call last):\n  File \"/usr/local/bin/docker-registry\", line 9, in \n    load_entry_point('docker-registry==0.7.0', 'console_scripts', 'docker-registry')()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 351, in load_entry_point\n    return get_distribution(dist).load_entry_point(group, name)\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2363, in load_entry_point\n    return ep.load()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2088, in load\n    entry = __import__(self.module_name, globals(),globals(), ['__name__'])\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/run.py\", line 15, in \n    from .app import app  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/app.py\", line 20, in \n    cfg = config.load()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/config.py\", line 71, in load\n    'Heads-up! File is missing: %s' % config_path)\ndocker_registry.core.exceptions.FileNotFoundError: Heads-up! File is missing: /usr/local/lib/python2.7/dist-packages/docker_registry/lib/../../config/config.yml\n\nCan you let me know what is supposed to happen here. Is pip install script supposed to download initial config to /usr/local/lib/python2.7/dist-packages/docker_registry/lib/../../config/config.yml. \nCould you please detail what environment variables you are referring to here \"set the appropriate environments variables to point to the proper config file/environment\".\nThank you.\n. ",
    "jeremyjjbrown": "@dmp42  Maybe your comments above could be in the README.\n. ",
    "getitlive": "@dmp42 : Thanks for your help. Not sure how many are affected. But any repo I tried so far returns such truncated IDs:\n\n/repositories/cpuguy83/ipsec/tags : [{\"layer\": \"7e292fc3\", \"name\": \"latest\"}]\n\n\n/repositories/library/ubuntu/tags : [{\"layer\": \"5cf8fd90\", \"name\": \"latest\"}, {\"layer\": \"3db9c44f\", \"name\": \"10.04\"}, {\"layer\": \"cc0067db\", \"name\": \"12.04\"}, {\"layer\": \"6006e634\", \"name\": \"12.10\"}, {\"layer\": \"7656cbf5\", \"name\": \"13.04\"}, {\"layer\": \"d2099a5b\", \"name\": \"13.10\"}, {\"layer\": \"5cf8fd90\", \"name\": \"14.04\"}, {\"layer\": \"3db9c44f\", \"name\": \"lucid\"}, {\"layer\": \"cc0067db\", \"name\": \"precise\"}, {\"layer\": \"6006e634\", \"name\": \"quantal\"}, {\"layer\": \"7656cbf5\", \"name\": \"raring\"}, {\"layer\": \"d2099a5b\", \"name\": \"saucy\"}, {\"layer\": \"5cf8fd90\", \"name\": \"trusty\"}]\n\nI'm willing to help as much as I can, so anything I could do to assist, feel free to ask.\n. @wking : Was definitely a mistake in my code, misunderstood the distinction between index and registry. Calling the same endpoint (from the registry this time) works as expected and returns full IDs.\nThanks all for your quick help!\n. ",
    "shreyu86": "I get 502 trying to pull or push also if I ping registry I get a 404. Logs\ndon't say anything special from which I can deduce something is broken even\ntough logging is at debug level.\nOn May 30, 2014 12:08 PM, \"Mangled Deutz\" notifications@github.com wrote:\n\nHi @shreyu86 https://github.com/shreyu86\nAny error message in the logs when you try to push / pull?\nAny error message at startup?\nThanks,\n- Olivier\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/dotcloud/docker-registry/issues/400#issuecomment-44674901\n.\n. I'll send the gists soon (out for lunch). Thanks for the fast reply.\nOn May 30, 2014 12:18 PM, \"Mangled Deutz\" notifications@github.com wrote:\nalso if I ping registry I get a 404\nWhat do you mean by that?\nLogs don't say anything special from which I can deduce something is\nbroken even tough logging is at debug level\nStill, can you gist your registry logs / output?\nThanks a lot.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/dotcloud/docker-registry/issues/400#issuecomment-44675955\n.\n. This is the gist of the logs: https://gist.github.com/shreyu86/cc4ae0f6b7f4e329438a have redacted some contents. \n\nalso if I ping registry I get a err not 404 but I cant see the page which shows registry version and setting flavor. \nwhen I do a ping using curl here is the response I get:\ncurl -X GET HOST:5000/v1/_ping\ncurl: (56) Recv failure: Connection reset by peer\n. Thanks! No Ping is not 404, that was a confusion, as of now I have commented out the S3 region. Registry seems to be running fine without s3_region. \nAlso in similar situations I used following method to connect to S3:\n``` python\nimport boto.s3.connection\ndef connect_to_s3(settings):\nreturn boto.s3.connection.S3Connection(\n    aws_access_key_id=settings.aws.auth.access_key_id,\n    aws_secret_access_key=settings.aws.auth.secret_access_key,\n    host=settings.s3.host\n)\n\n```\nIn this method assume settings to be a dot dictionary object.\nhost here being s3-us-west-2.amazonaws.com for west-2 and I use this connection object returned above to do all the S3 operations, I have not faced any issues with the above method. \nAlso I am out of sync with the latest changes (couple of them major) so opened an issue instead of PR. \nWill surely dig into this more. \n. Just curious @dmp42 were you able to repro the issue?\n. Thanks!\nOn Jun 2, 2014 7:39 AM, \"Mangled Deutz\" notifications@github.com wrote:\n\nI haven't had time yet to get to it, sorry for that - will sure do later\ntoday and keep you posted though!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/dotcloud/docker-registry/issues/400#issuecomment-44831943\n.\n. I've opened #405 as a solution to this issue. I have patched our private\nregistry and can confirm that everything (Push, Pull, Search) operations\nare working normally after this change.\n\n-Shreyas\nShreyas\nOn Mon, Jun 2, 2014 at 7:40 AM, Shreyas Karnik shreyu86@gmail.com wrote:\n\nThanks!\nOn Jun 2, 2014 7:39 AM, \"Mangled Deutz\" notifications@github.com wrote:\n\nI haven't had time yet to get to it, sorry for that - will sure do later\ntoday and keep you posted though!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/dotcloud/docker-registry/issues/400#issuecomment-44831943\n.\n. @dmp42  here is the information you requested. \n- the logs you copied (the \"with region\" scenario) seems to be cut too early - what happens exactly? it hangs there and does nothing more? it crashes? or is there more after that?\n  - It just hangs at those logs, nothing further happens.\n- what version of the registry are you running? (I assume 0.7.0) - did you try and/or have the problem with 0.6.9?\n  - I am running 0.7.0 I did not try 0.6.9. \n- are you running it from the docker container? or from a pip install?\n  - I am running it inside docker container pulled from index.docker.io\n\n\nThanks for looking into this. \n. It worked. Here is the output:\nGonna connect without region\n0.257856845856\nGonna connect with region\n0.0846657752991\n. - do you use the configuration variable storage_redirect?\n  - no it is set to false\n- can you copy the exact command lines you are using to launch the registry, both with region and without region\n- command lines are:\n```\nCommands to invoke registry\ndocker run -d -p 5000:5000 -v /etc/docker-registry/config.yml:/opt/docker-registry/config.yml -e SETTINGS_FLAVOR=prod -e DOCKER_REGISTRY_CONFIG=/opt/docker-registry/config/config.yml -e GUNICORN_WORKERS=10 registry:latest\n``\n- same thing for your configuration files, with and without region:\n  - for this I edit the configuration file variables3_region` to set a region I set it to us-west-2 and to connect without region I comment it out. \nHere are my redacted settings just in case you need to take a look at those:\nhttps://gist.github.com/shreyu86/52652677440596e669b4\n. Err typo in cleaning and redacting.\nCorrect set of commands are\ndocker run -d -p 5000:5000 -v\n/etc/docker-registry/config.yml:/opt/docker-registry/config/config.yml -e\nSETTINGS_FLAVOR=prod -e\nDOCKER_REGISTRY_CONFIG=/opt/docker-registry/config/config.yml -e\nGUNICORN_WORKERS=10 registry:latest\n. One last shot: did you run the test script on the same machine that you use to run the registry?\nYes both on my dev machine and on the registry machine inside of ec2 here is the output for the registry machine:\nGonna connect without region\n0.0115849971771\nGonna connect with region\n0.0101661682129\nI guess inside of EC2 it is a minimal difference with and without regions but not sure about its impact. \nI will try my hand at debugging boto. \nRegarding the bucket, will have to jump through a lot of permission issues so right now will try to debug this issue at my end and post findings here. \nThanks for the help, also I will try it with a fresh S3 bucket as whenever I update the registry I use the same bucket, I've been using private registry since a long time so will do this test with a fresh S3 bucket and see if I face the same issue or not. \nI will surely post any findings here. Thanks @dmp42 I really appreciate your help.\n. Actually I think I am OK with not using explicit region declaration in the registry config because while debugging this issue I found that boto redirects the bucket to the underlying region, some logs (not in that order from registry logs)\n2014-06-02 17:00:27,119 DEBUG: Host: BUCKET.s3.amazonaws.com\n2014-06-02 17:00:27,210 DEBUG: Redirecting: http://BUCKET-us-west-2.amazonaws.com/\nI think right now this is one short term optimization I can make, but will visit this during off hours to get to the bottom. \n. @shin- https://github.com/skarnik-rmn/docker-registry/commit/18320e276c3c5b6405490554047be4881098893c works for me, have been using it for quite some time now. \n. @dmp42 any official update/workaround on this?\n. Thanks!\nShreyas\nOn Fri, Sep 5, 2014 at 4:32 PM, Olivier Gambier notifications@github.com\nwrote:\n\n@shreyu86 https://github.com/shreyu86 for now, I would support\nspecifying the region inside the boto.cfg file.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/400#issuecomment-54684350\n.\n. Alright lets close this, thanks @dmp42 .\n. @dmp42 got bit by this when upgraded to 0.9.0 preload helped. \n. @stevenjack use GUNICORN_OPTS=[--preload], can you try that and let me know? It did work for me.\n. +1\n. Will give the new wheels a spin too! @dmp42\nOn Mon, Oct 6, 2014 at 2:19 PM, Anton Tyurin notifications@github.com\nwrote:\n@dmp42 https://github.com/dmp42 the same story. It will be installed on\nour test cluster tomorrow.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/610#issuecomment-58077288\n.\n. @dmp42 and other contributors to the project, I was curious that in the docker-registry next generation implementation will there be an similar event stream just as the docker daemon which lists events like pull, push, delete, create tags and so on and so fourth. This kind of event stream will be useful to monitor events in the registry and opens up the possibility for having loosely connected consumers which monitor the events to create summary of the events occurring with the registry. If this thread is not the right place to open these kinds of requests/discussions I can open a new proposal as well, I wanted to do a temperature check first before opening a detailed proposal. \n. Thanks @dmp42 \n. Docker client logs: https://gist.github.com/shreyu86/8f56ad7e783b235468ed \nWhat  is interesting is that after opening this issue, I tried to change the config default params, when I changed s3_secure: _env:AWS_SECURE:true to s3_secure: _env:AWS_SECURE:false the  EOF problem went away. Is the s3_secure option incompatible with storage_redirect  or running the registry insecure is the base issue? \n. @dmp42 any word on it? Not a high priority since storage_redirect:true works great for us, still wanted to know if this is an actual issue or just incompatible set of flags. \n. @dmp42 just updated the boxes will try in sometime will let you know for sure, thanks for the pointers. \n. @dmp42 that worked! Thanks a lot! Closing this issue. \n. @ianneub can you try running the registry without S3_REGION=us-east-1, this might be #400 which stops registry from booting up.\n. @ianneub just as a FYI the engine currently does not support registry with self signed SSL certificates, there is a PR open for the docker engine to accept self signed certificates https://github.com/docker/docker/pull/8467\n. @chuegle Thanks! #400 finally will be put to rest! \n. @dmp42 absolutely, I was waiting on hint as to what would be the right place to add that information. Will update the documentation. \n. :+1: \n. https://github.com/kwk/docker-registry-frontend is also an nice and clean UI alternative for private registries. \n. Yes, I will make that change. Thanks for the feedback. \n. \n",
    "ddeaguiar": "I'm also running into this issue. I'm using registry:latest on docker.io. Here's my startup command:\ndocker run \\\n       --rm \\\n       -it \\\n       -e SETTINGS_FLAVOR=s3 \\\n       -e AWS_REGION=us-east-1 \\\n       -e AWS_BUCKET=my-bucket \\\n       -e AWS_ENCRYPT=true \\\n       -e AWS_SECURE=true \\\n       -e STORAGE_PATH=/registry \\\n       -e AWS_SECRET=REDACTED \\\n       -e AWS_KEY=SECRET \\\n       -e SEARCH_BACKEND=sqlalchemy \\\n       -P \\\n       registry\nIt seems that setting the region doesn't solve the broken_pipe error I'm getting on push.\n. ",
    "danielschonfeld": "+1 Here.... same issue, tried 0.7.0 and 0.7.3 both are affected.\nInterestingly enough, if you omit the AWS_REGION from the environment, the container fails instead of hanging with an boto.exception.S3ResponseError: S3ResponseError: 403 Forbidden trying to call return self.head_bucket(bucket_name, headers=headers).  Adding the AWS_REGION=us-east-1 makes it hang.\n. @phemmer my key/secret belong to an IAM user who is part of a group that has the \"Administrator\" policy which looks like this:\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"*\",\n      \"Resource\": \"*\"\n    }\n  ]\n}\nI've also used that key/secret combination with s3cmd on my macbook to verify i can upload, read and list files and directories inside that s3 bucket\n. EDIT: My comment fails to mention that I've been running docker-registry on CoreOS inside a VM in my case VirtualBox via Vagrant) as a docker container.  VBox has a problem keeping the time up to date and as detailed below that affected the validity of the signature relayed to AWS.\nSo it seems like the 403 Forbidden errors has nothing to do with omitting the region and this bug.  In fact it has to do with the time drifting away on a virtual box (at least for me).  @mdub were you running your test on a VirtualBox machine as well? I have found out that disable ntpd and running ntpdate on my own fixed that issue and brought up the docker registry.\nie:\nsudo systemctl stop ntpd\nsudo ntpdate time.apple.com\nand then something like that:\n/usr/bin/docker run -d -p 5000:5000 -e LOGLEVEL=DEBUG  -e SETTINGS_FLAVOR=prod -e AWS_BUCKET=my_bucket -e STORAGE_PATH=/registry -e AWS_KEY=REDACTED -e AWS_SECRET=REDACTED -e STANDALONE=true -e SEARCH_BACKEND=sqlalchemy registry:0.7.3\nIt appears that the signature needed for S3 includes date+time inside it and when the time drifts the signature becomes invalid.  That's why we get the 403 problems.  Now as far as adding the region and why it hangs... we're back to what this bug is all about.\n. I wonder if this might be related https://github.com/boto/boto/issues/2179\n. ",
    "mdub": "Similar story to @danielschonfeld here; omitting AWS_REGION prevents the hang on startup, but now I'm plauged by \"401 Unauthorized\" errors, which I'm assuming is due to the inability to enable \"standalone\" mode (#431).\n. @dmp42 Thanks. That did it.\n. Yup, looks like you're right.  I tried without setting AWS_REGION, and the registry comes up okay.\ndocker run -it -p 5000:5000 -e AWS_KEY=\"XXXX\" -e AWS_SECRET=\"YYYY\" -e SETTINGS_FLAVOR=prod -e AWS_BUCKET=my-docker-registry registry:0.7.2\nClosed as duplicate of #400.\n. Nice! :thumbsup: \n. ",
    "dbaba": "+1\nWithout AWS_REGION, docker-registry won't start, stops with 403 Forbidden message.\nI'm using 0.7.3 release, running on boot2docker/OSX. My bucket was created in Oregon region.\n. Restarting boot2docker image solved my problem. Now omitting AWS_REGION works for me.\nNot sure what happened to me though.\n. As @danielschonfeld  mentioned, the time wasn't synced automatically.\nFor Boot2docker users, run the following command on boot2docker after resuming a saved boot2docker instance:\ndocker@boot2docker:~$ sudo ntpclient -s -h time.apple.com\nThen run docker-registry and 403 error is gone.\n. ",
    "frankamp": "I get this and am hoping that once this is resolved I'll be able to push more than ~50MB to S3. \nUsing:\ndocker run \\\n         -e SETTINGS_FLAVOR=s3 \\\n         -e AWS_BUCKET=s3.docker.bucket_goes_here \\\n         -e STORAGE_PATH=/registry \\\n         -e AWS_KEY=KEYHERE \\\n         -e AWS_SECRET=SECRETEHERE \\\n         -e SEARCH_BACKEND=sqlalchemy \\\n         -p 5000:5000 \\\n         registry\nIf I specify -e AWS_REGION=us-west-2 \\ also, it hangs in the same location as posters above.\nattempting to do this:\n2014/07/08 13:34:46 bash-3.2$ docker push localhost:5000/ubuntu\nThe push refers to a repository [localhost:5000/ubuntu] (len: 1)\nSending image list\nPushing repository localhost:5000/ubuntu (1 tags)\n511136ea3c5a: Image successfully pushed \nd7ac5e4f1812: Pushing [==================================================>] 201.6 MB/201.6 MB\nYou can see it pushes the first tiny image ok, but when d7ac5e4f1812 gets to ~50 megs it has some upload error, attempts to restart a few times and then dies.\n. @dmp42  I am using a newish boot2docker\nClient version: 1.1.0\nClient API version: 1.13\nGo version (client): go1.2.1\nGit commit (client): 79812e3\nServer version: 1.1.0\nServer API version: 1.13\nGo version (server): go1.2.1\nGit commit (server): 79812000\nI am able (without region) to upload a few items such as 511136ea3c5a. Since AWS does timestamp authentication against every request, its not a time issue. Just for fun I ran the time commands in the boot2docker instance, verified the time, it matched system time but did not change my outcome.\n. @dmp42 I just now attempted to reproduce it so I could show what I was getting on the docker registry process during upload. I couldn't, everything worked this time. To be clear: the registry against s3 is usable for me now, as long as I don't specify region, no help to be had on the intermittent upload issue. The only reason I commented here is the warning:\nWARNING: No S3 region specified, using boto default region, this may affect performance and stability.\nThank you\n. ",
    "dlaidlaw": "+1 Same problem in us-east-1\n. @shin Yes, I can confirm that not having AWS_REGION set as an environment variable solves the problem. This is how I am running right now. I have not personally tried changing the yaml config. \n. Thanks to you both for the extremely quick turnaround! I tested this.\nThat was a partial fix. It actually fixed the problem I reported, but exposed another problem. With this fix, you could request the /v1/search address exactly twice successfully. The third request, and all subsequent requests, reported an error in iterating the results of the query.\nThe problem is in docker_registry/lib/index/db.py in the results method: the session is not closed. I added a session.close() and that solved the problem. Note that there is another method in this file that may also have a problem. The session is not closed in _generate_index either. You might want to look at that.\nHere is my updated copy of the results method:\npython\n    def results(self, search_term=None):\n        session = self._session()\n        repositories = session.query(Repository)\n        if search_term:\n            like_term = '%%%s%%' % search_term\n            repositories = repositories.filter(\n                sqlalchemy.sql.or_(\n                    Repository.name.like(like_term),\n                    Repository.description.like(like_term)))\n        results = [\n            {\n                'name': repo.name,\n                'description': repo.description,\n            }\n            for repo in repositories]\n        session.close()\n        return results\n. Fantastic. Thanks!\n. I will start by looking at what is required in 0.9. It looks like I can just add it as a module there and set the appropriate config variable, which now defaults to sqlalchemy. The question here is, do you want it contributed to this project, or will I make it a separate project? Either way is OK with me.\n. I have this working. It is set up as a github project that builds a public docker image on docker hub.\nGitHub: https://github.com/dlaidlaw/docker-registry-index-dynamodb\nThe image name is docker_registry_index_dynamodb. You just have to set the search_backend to docker_registry_index.dynamodb. There are other optional configuration parameters to set the table names, different credentials, etc, but most of them default to the S3 settings, so you do not need to set them again.\nLet me know what you think.\n-Don\n. ",
    "mattheworiordan": "I have a bucket in eu-west-1 and was unable to get Docker Registry to start, it would hang every time without any indication of what was wrong. The only way I could get this to work without hanging was to:\nCreate the file /etc/boto.cfg with:\n```\n[Boto]\nmetadata_service_timeout = 2.0\nmetadata_service_num_attempts = 3\n[S3]\nregion = <%= ENV['DOCKER_S3_REGION'] %>\n```\nAnd then my registry config.yml is configured as follows under the s3 section:\ns3: &s3\n  <<: *common\n  storage: s3\n  # s3_region: Providing this will cause Registry to fail, this is configured in /etc/boto.cfg instead\n  s3_bucket: _env:DOCKER_S3_BUCKET\n  boto_bucket: _env:DOCKER_S3_BUCKET\n  storage_path: _env:DOCKER_S3_PREFIX\n  s3_encrypt: false\n  s3_secure: true\n  s3_access_key: _env:AWS_ACCESS_KEY_ID\n  s3_secret_key: _env:AWS_SECRET_ACCESS_KEY\nNote s3_region is deliberately left out, but set within the boto settings.  This is an odd bug and shoudl be fixed in docker registry.\n. I tried it outside the container and had the same issue\n. @dmp42 apologies, misread @stongo's comment.  I too am on Ubuntu 14.04\n. I have a bucket in eu-west-1, and the only way I could get this to work without hanging was to:\nCreate the file /etc/boto.cfg with:\n```\n[Boto]\nmetadata_service_timeout = 2.0\nmetadata_service_num_attempts = 3\n[S3]\nregion = <%= ENV['DOCKER_S3_REGION'] %>\n```\nAnd then my registry config.yml is configured as follows under the s3 section:\ns3: &s3\n  <<: *common\n  storage: s3\n  # s3_region: Providing this will cause Registry to fail, this is configured in /etc/boto.cfg instead\n  s3_bucket: _env:DOCKER_S3_BUCKET\n  boto_bucket: _env:DOCKER_S3_BUCKET\n  storage_path: _env:DOCKER_S3_PREFIX\n  s3_encrypt: false\n  s3_secure: true\n  s3_access_key: _env:AWS_ACCESS_KEY_ID\n  s3_secret_key: _env:AWS_SECRET_ACCESS_KEY\nNote s3_region is deliberately left out, but set within the boto settings.  This is an odd bug and shoudl be fixed in docker registry.\n. Each server has it's own redis cache. This is an intentional design decision so that each region is fully independent of every other region. If we had a centralised Redis we'd not only need to think about what happens if that redis goes down, but also we'd need to think about provisioning an SSL connection to the redis server as redis does not support SSL. \nI'm really trying hard to improve the performance of the registry but am struggling a bit. \n. ",
    "gerhard": "@mattheworiordan thanks, suggested fix worked a charm. I was getting this on latest stable, v0.7.3\n. ",
    "kmccormack": "Hi, @mattheworiordan's fix didn't work for me on v0.7.3. I ended up spending a lot of time debugging and then just hacked the /docker_registry/drivers/s3.py file to hard-code the endpoint I'm interested in. So, I changed the following lines (96-99): \nreturn boto.s3.connection.S3Connection(\n            self._config.s3_access_key,\n            self._config.s3_secret_key,\n            **kwargs)`\nto \nreturn boto.s3.connection.S3Connection(\n            aws_access_key_id=self._config.s3_access_key,\n            aws_secret_access_key=self._config.s3_secret_key,\n            host=\"s3-eu-west-1.amazonaws.com\",\n            **kwargs)`\nand I didn't specify the AWS_REGION environment variable. \nIn order to try to reproduce the bug, I created the following small python script: \nimport gevent.monkey\ngevent.monkey.patch_all()\nimport logging\nlogging.basicConfig(filename=\"boto.log\", level=logging.DEBUG)\nimport boto\nboto.config.set('Boto', 'debug', '2')\nboto.config.set('Boto', 'is_secure', 'no')\nimport os\nimport boto.s3\ns3 = boto.s3.connect_to_region(\n    region_name='eu-west-1',\n    aws_access_key_id=os.environ['AWS_KEY'],\n    aws_secret_access_key=os.environ['AWS_SECRET'])\nmybucket = s3.get_bucket('MY_BUCKET_IN_EU_WEST')`\nWhere MY_BUCKET_IN_EU_WEST is obviously a bucket in the problematic EU_WEST_1 region. This script worked fine when I ran it in the Docker registry container on the command line. I added lots of debug info to various python scripts to see what was going on and it seemed things just stopped in the gevent code at DNS resolution - It doesn't even end up making a HTTP request to Amazon. I got the same stack trace as @phemmer . My best guess is that something is happening when you use the region block of code in s3.py where the gevent.monkey code isn't properly swapping the calls for DNS resolution to use the gevent Socket instead of the standard Python Socket, but I'm not an expert on Python so can't be sure. Hope this helps anyone trying to resolve this bug or find a workaround. \n. ",
    "spesnova": "Here is my starting script. I don't specify region, and it works.\nIf I specify -e AWS_REGION=ap-northeast-1 \\ also, it hangs me too.\nI hope this helps.\n``` bash\n!/bin/bash\ndocker run \\\n   --name registry \\\n   -e SETTINGS_FLAVOR=prod \\\n   -e AWS_BUCKET= \\\n   -e AWS_KEY= \\\n   -e AWS_SECRET= \\\n   -e DOCKER_REGISTRY_CONFIG=/docker-registry/config/config_sample.yml \\\n   -e STORAGE_PATH=/ \\\n   -e SEARCH_BACKEND=sqlalchemy \\\n   -e GUNICORN_WORKERS=2 \\\n   -p 5000:5000 \\\n   -d \\\n   registry:0.7.3 \\\n   docker-registry\n```\n. ",
    "stongo": "Having same issue running from python install outside docker, so definitely not related to being inside a docker container as suggested by some\n. using Ubuntu 12.04\n. Actually just compared my error more closely and it seems to be a slightly different error I'm experiencing.\n. Also experiencing this with Docker 1.8.2\n. ",
    "jvimr": "I've created a set of small dockerfiles that do the workaround (boto.conf && remove aws region from sampe_config.yml ) - https://github.com/jvimr/docker.registry\n. ",
    "adamlc": "@jvimr you are a legend! works perfectly :)\n. ",
    "chuegle": "This looks like a gevent bug dealing with unicode.\nIt's triggered by specifying the region in boto, which pulls the URL from the boto region file which has unicode strings.  If you don't specify the region, it uses the default which is not unicode, so therefore it works.\nIt can be reproduced (in both docker/ubuntu and Mac OS X) by doing the following:\nin a.py:\n import b\nin b.py:\n import gevent.socket\n print gevent.socket.getaddrinfo(u\"s3.amazonaws.com\", 443)\npython a.py # hangs\nIf you change the host to a non-unicode, it works.\nOddly, if you put the gevent.socket call in a.py instead of b.py, it works.\nAlso, if you have it in both a.py and b.py, it works.\nIf you put a unicode string in b.py and a normal string in a.py, it will hang.\nThis would be an ugly workaround:\nif self._config.s3_region is not None:\n            return boto.s3.connection.S3Connection(\n                host=str(boto.regioninfo.load_regions()['s3'][self._config.s3_region]),\n                aws_access_key_id=self._config.s3_access_key,\n                aws_secret_access_key=self._config.s3_secret_key,\n                **kwargs)\nor some way if specifying a host=str(host) in the kwargs w/o region.\nSide note discovered while investigating:\nThe gevent.monkey.patch_all()  ends up being called after most everything in requirements files are imported.  It seems that python entry_point code is nice enough to import the requirements for you before importing the code to patch_all().   It should be less important in gevent 1.0+ that can handle threading, but something to be aware of.\nADDENDUM\nA bit more research found a couple other fixes:\n- u'fix gevent'.encode('idna') - Prevent the deadlock by initializing outside of the getaddrinfo call.\n- install simplejson - Boto will use this if it exists, and json if not.   simplejson returns its results a str() while json returns it as unicode()\nI went into a bit more detail on how it works in boto in:\nhttps://github.com/boto/boto/issues/2179#issuecomment-61396805\n. The gevent ticket (although there may be more than one):\nhttps://github.com/gevent/gevent/issues/349\nAs far as a boto eta, I haven't submitted a patch and haven't heard back on my comments there, so I can't really say.  I'll see if I have time this week to implement one of the fixes for boto and do a pull request, but would still have no idea how long it is until the next rev is pushed or if this change would make it in.\n. The issue here had errors before it reached the region bug.\nThe 400 Bad Request is because you're trying to access a eu-central-1 bucket from us-east-1 (the default region).   This works to some buckets, but apparently not eu-central-1.  The awscli also fails to be able to do this.\nThe original error with 'NoneType' object has no attribute 'get_bucket' is because the version of boto being used in the registry docker does not know eu-central-1.\n```\ndocker run --rm -i -t -e LOGLEVEL=debug -e AWS_BUCKET=ta-region-test -e AWS_KEY=a -e AWS_SECRET=b -e SETTINGS_FLAVOR=s3 -e AWS_REGION=eu-central-1 registry bash\nroot@ba03283fba71:/# python\n\n\n\nimport boto.regioninfo\nr = boto.regioninfo.load_regions()\nr['s3'].keys()\n[u'us-east-1', u'cn-north-1', u'ap-northeast-1', u'eu-west-1', u'ap-southeast-1', u'ap-southeast-2', u'us-west-2', u'us-gov-west-1', u'us-west-1', u'sa-east-1']\nroot@ba03283fba71:/# pip install --upgrade boto\n...boto installs and updates...\nroot@ba03283fba71:/# python\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\nimport boto.regioninfo; r = boto.regioninfo.load_regions(); r['s3'].keys()\n[u'us-east-1', u'cn-north-1', u'ap-northeast-1', u'eu-west-1', u'ap-southeast-1', u'ap-southeast-2', u'us-west-2', u'us-gov-west-1', u'us-west-1', u'eu-central-1', u'sa-east-1']\n```\n\n\n\nSo, you need to create your own docker container for the registry and update boto.\nYou'll also either need to use registry from HEAD to fix https://github.com/docker/docker-registry/issues/400 or you can pip install --upgrade simplejson boto to work around issue 400 manually.\nOf course, there should also be something submitted for docker-registry to point out that the current boto doesn't support eu-central-1.\n. Fyi, eu-central-1 has another issue with a boto bug.\nYou can see the details and workaround here:\nhttps://github.com/docker/docker-registry/issues/674\n. Yes, I have successfully ran a docker instance with HEAD that hangs, then added just this fix and it works fine.\nThe issue isn't just calling the u'a'.encode('idna').  I think it has to do with gevent.socket.getaddrinfo going into C code and calling back to python causing the two imports to deadlock.  This means that it can pretty much be called from anywhere in python before the getaddrinfo to work.\nFrom my testing, calling the encode() is safe and it fixes the issue by preventing the need to import later.\n. Easy way to test the patch, if you have docker installed:\n```\ndocker run --rm -i -t -e LOGLEVEL=debug -e AWS_BUCKET=test -e AWS_KEY=a -e AWS_SECRET=a -e AWS_REGION=us-east-1 -e SETTINGS_FLAVOR=s3 registry bash\ndocker registry\n.... starts up, but hangs after calculating the signature ...\nctrl+c\nsed -i 's/import gevent.monkey/u\"test\".encode(\"idna\")\\nimport gevent.monkey/' /usr/local/lib/python2.7/dist-packages/docker_registry/drivers/s3.py\n....applies the patch inplace....\ndocker-registry\n....runs and fails with a 403 due to the bogus credentials, but works fine with correct settings...\n```\n. It looks like it's a straight boto bug in the HEAD of boto.\nThey set a int() Content-Length (i think with chunked encoding), and then later assume its a string.\nYou need to:\n- pip install --upgrade boto simplejson\n- patch boto: sed -i \"s/self.headers..Content.Length.. . len.self.body./self.headers['Content-Length'] = str(len(self.body))/\" boto/connection.py\n- run registry directly instead of through the docker-registry entry-point, since it'll complain about the boto version being wrong\n  python -c \"import docker_registry.run; docker_registry.run.run_gunicorn()\"\nHere's a way I can get a container running that works:\ndocker run -e AWS_BUCKET=<YOUR_BUCKET_HERE> -e AWS_KEY=<YOUR_KEY_HERE> \\\n  -e AWS_REGION=eu-central-1 -p 5000:5000 registry bash -c 'pip install --upgrade simplejson boto \\\n && sed -i \"s/self.headers..Content.Length.. . len.self.body./self.headers['\\'Content-Length\\''] = str(len(self.body))/\" \\\n /usr/local/lib/python2.7/dist-packages/boto/connection.py  \\\n&&  python -c \"import docker_registry.run; docker_registry.run.run_gunicorn()\"'\n(escaping may be off a little)\n. @dmp42 I'd personally be kind of hesitant to vendorize boto.  It's a 1000+ file repo (400+ if you just include the python libs) that changes very frequently, and most of the files aren't very necessary for the registry in the first place.\nI think I would lean towards loosening the version (boto>=2.34 instead of ==) and then patching in the Dockerfile when necessary.\n...\nRUN pip install ...\nRUN patch $(python -c 'import boto; import os; print os.path.dirname(boto.__file__)')/connection.py < /docker-registry/contrib/boto_content_length_fix.diff\nENV DOCKER_REGISTRY_CONFIG....\nBasically, fix boto when needed in the default registry container, but don't be responsible for maintaining and controlling boto for anyone who uses the registry in whatever environment they choose.\n. I believe you can just set:\n-e S3_USE_SIGV4=1\nand tell boto directly to use sigv4 for s3.\nAnother idea may just be to make it clear/easier to inject a boto.cfg into the default registry container.\ne.g.\nmake_boto_config > boto.cfg\ndocker run -v /boto -v $PWD:/mnt --name boto_cfg busybox cp /mnt/boto.cfg /boto/boto.cfg\ndocker run ... --volumes-from boto_cfg -e BOTO_CONFIG=/boto/boto.cfg docker-registry\n. @c-schmitt The 0.9.0 release can access eu-central-1 without having to specify use-sigv4 at all.\nHowever, it looks like the docker repo has yet to add 0.9.0 and is still on 0.8.1.\nBuilding should be pretty easy:\ngit clone https://github.com/docker/docker-registry\ncd docker-registry\ndocker build -t registry .\nshould do it.  You can then run with a S3 bucket from eu-central-1 w/o any extra configuration.\ndocker run --rm -e AWS_BUCKET=... -e AWS_KEY=... -e AWS_SECRET=... \\\n -e SETTINGS_FLAVOR=s3 -p 5000:5000 -e AWS_REGION=eu-central-1 \\\n -e SEARCH_BACKEND=sqlalchemy -e GUNICORN_WORKERS=1 registry\nwas all I needed to run it successfully for testing purposes.\n. usg_sigv4 typo at the end.\nI also think the last part could just be \"boto_host needs to be set or use_sigv4 will be ignored by boto.\"  No need for the Warning or ().\n. I think you should warn when the host isn't set, but still set use-sigv4.  Just remove the else and indent out the lines below.\nif ..host is None:\n    logger.warn(..)\nboto.config.add_section..\nboto.config.set('s3', 'use-sigv4', 'True')\nI think its better to have the config file do as told, since we may not know of a valid reason to set this while the config isn't specified.\n. This would probably be more appropriate to default to None s3_use_sigv4: _env:AWS_USE_SIGV4 since when it's not True, nothing is set instead of setting it to False.\n. ",
    "hartym": "Any idea when / version in which this will be made available ?\n. @dmp42 I could not start a registry container using 0.9/latest tag, backed on S3. The container is working fine with local filesystem backend, but S3 connection never worked. I could connect to the same bucket with the same aws keys using raw boto though. At best, the container was hanging after AWS ...somecrypticstring... line.\nFor now, I did put aside the setup of my own registry, I will dive more into it early next year. Not sure my problem is related to this ticket, but all pointers I found around were heading me here.\n. @dmp42 seems like the \"latest\" image was not the same as 0.9.0, and after running brand new tests, I achieved to run the registry (with S3 storage on europe region and sqlalchemy search backend). Thanks for your help and sorry for bothering.\n. ",
    "ross-w": "Seeing this behaviour here also\nI have a temporary workaround- having to symlink $DOCKER_REGISTRY_PATH/tmp to where the actual storage is\nWould be nice if this could be fixed though.\n. ",
    "jvanarragon": "any idea when this tag will be available?\n. ",
    "fernandoacorreia": "I'm also waiting for this fix to be published. Thanks.\n. ",
    "pantaoran": "Same here. Registry is broken for me as long as this doesn't work!\n. We are waiting for 0.7.1 to be published as an image in the public docker index, don't want to have to build it myself...\n. Thank you for the update!\nUnfortunately the change from 0.7.0 to 0.7.2 breaks something for me, I can't seem to push anything to the registry anymore. The client log at /var/log/upstart/docker.io.log says that the method isn't allowed (anonymized for IP address):\n2014/06/17 11:07:55 POST /v1.10/images/IP:5000/debian/push\n[/var/lib/docker|21e863c0] +job push(IP:5000/debian)\nError: Status 405 trying to push repository debian: <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n&lt title &gt 405 Method Not Allowed &lt /title &gt\n&lt h1 &gt Method Not Allowed &lt /h1 &gt\n&lt p &gt The method is not allowed for the requested URL. &lt /p &gt\n[/var/lib/docker|21e863c0] -job push(IP:5000/debian) = ERR (1)\nI have tried to see what else has changed between those two versions but the changelog hasn't been updated apparently. Does anyone else observe this?\nEdit: noticed that this textbox interprets html code, trying to make it clearer\n. That solved it. Thank you!\n. ",
    "sammcj": "@dmp42 any update with your fix?\n. Excellent, thank you for the information, I eagerly await v2!\n. As @xeoncore dmp42 asked - are you using BTRFS?\nWe're having the same issue and at the moment https://github.com/docker/docker/issues/7291 is looking like it might be the problem.\n. ",
    "asim": "I am having the same issue when trying to push an image to a local repository backed by s3\n2014/07/17 11:14:00 HTTP code 500 while uploading metadata: invalid character '<' looking for beginning of value\nStack trace returned\n2014-07-17 11:14:00,261 ERROR: Exception on /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/checksum [PUT]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1817, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1477, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1381, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1475, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1461, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 264, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/images.py\", line 290, in put_image_checksum\n    store.remove(mark_path)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/boto.py\", line 235, in remove\n    key.delete()\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/key.py\", line 523, in delete\n    headers=headers)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 757, in delete_key\n    query_args_l=None)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/bucket.py\", line 776, in _delete_key_internal\n    response.reason, body)\nS3ResponseError: S3ResponseError: 403 Forbidden\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<Error><Code>AccessDenied</Code><Message>Access Denied</Message><RequestId>7F2995E0E56E2286</RequestId><HostId>FFdVmSa6hMhPyra1AgJU3DBeCHnM0u1rTQvALMrrh9WaGQSLyythC2B9/qvdzDYZ</HostId></Error>\nI've used a policy that allows all access to the bucket. Strangely it can upload other files to the registry but fails at this point.\n. ",
    "doubleyou": "Wow.\nTried to mount /var/lib/docker to the laptop. Resulted in 102GB of space.\ndevicemapper FTW\n. Figured the best way \u2013\u00a0start another VBox that doesn't get killed along with the regular ones and just runs the registry inside.\n. ",
    "pchaussalet": "Indeed, it looks like there is something wrong in the way S3 is used in that part. Using servers both on east and west coast, it takes around 100ms to ensure consistency (it could take more in case of loaded networks).\nThe only (simple) ways to avoid such errors would be to :\n 1/ Add a wait time before accessing data (and hope that this wait time is long enough for all cases)\n 2/ Implement a retry function until the data is available\nI'll propose a patch for the second solution ASAP.\n. Current checksum handling seems to be the most secure way to ensure that the upload is successful from the registry client to the storage backend (as secure as possible without being able to calculate checksum on storage).\nEven if the checksum were calculated on the layer upload request, to ensure this security level we need to store then  read the checksum file, so the consistency problem due to S3 us-standard behavior would be the same.\nUsing a HTTP 202 status could be better from an API point of view, but would it not make the upload process more complex (need to add an upload status endpoint, need to periodically check this status, handle a checksum reject asynchronously,...) and it would just transfer the specific S3 behavior handling to the registry client ?\nBTW, I agree that returning a 400 when a checksum is wrong is weird as the checksum is not send in the same request as the image layer, but I can't see any HTTP status more appropriate.\n. Ok, I fixed the build by removing code style rules violations... ;)\n. ",
    "fermayo": "We are also getting hit by this :-)\n. In the class Storage?\n. Updated with your comments\n. +1 :-)\n. ",
    "ncdc": "@wking would you like me to try to convert this to an extension?\n. @wking I wasn't actually going to try to modify Storage at all. I was going to treat all the reference counting code (and the files it creates) as something solely managed by the extension (even though _references goes in .../images/[id]/).\n. If we use your existing code in this PR, just like you've been doing it via image_references_path.\n. I'd put image_references_path in the extension code, and use signals to react to tags being created and deleted.\n. @wking do you think we need to worry about file locking and/or mutexes when updating image references?\n. @wking @shin @smarterclayton please review\n. @dmp42 hmm, my tests work locally. I'll take a look in a bit.\n. @dmp42 are you talking about using setuptools entry points to allow third party modules to register themselves as add ons?\n. @dmp42 if you look in #493, our initial attempt was to try to define some sort of formal interface, at least for listeners for events such as tag-created (albeit without the extension boilerplate you describe above, which would make more sense), but we created this PR as a possible replacement for 493 based on a comment from @wking :-)\n. @dmp42 FYI, got the tests fixed\n. @dmp42 yes, closing now\n. @dmp42 since the signals themselves are public, making the namespace public shouldn't be required, as best I can tell. You can just access the individual signals via the module.\n. @dmp42 @wking have you all gotten any closer to which path you want to go down?\n. LGTM once the docker-registry -> docker_registry and the empty Config changes are in\n. @dmp42 @wking do you think we need a standard/formal mechanism for enabling/disabling extensions via the config, with the check done in the factory, instead of relying on each extension to implement the check itself?\n. @wking true. Are we back to debating that vs this?\n. So it's more \"only install the extensions you want, and if you do want to disable one, it either needs to support disabling itself, or you just uninstall it\"\n. @dmp42 fyi the empty Config() isn't working for me - trying to find out the root cause\n. @dmp42 here's a fix:\n```\ndiff --git a/docker_registry/lib/config.py b/docker_registry/lib/config.py\nindex 5a44b78..e6ce4e5 100644\n--- a/docker_registry/lib/config.py\n+++ b/docker_registry/lib/config.py\n@@ -130,7 +130,7 @@ def load():\n         if ('extensions' not in _config.keys()\n                 or not _config.extensions\n                 or short not in _config.extensions.keys()):\n-            return Config()\n+            return Config('{}')\n         return _config.extensions[short]\nreturn _config\n```\n. @wking that looks better :-)\n. @dmp42 @wking see #580\n. @dmp42 @wking @shin- @mhrivnak here is a similar implementation using entry points: https://github.com/ncdc/docker-registry/compare/entrypoints\n. @dmp42 \n- I removed the sub-nodes part from config because entry points can be any module you want - they don't have to be docker_registry.extensions, and there's no way to force them to be, so I couldn't think of a super easy way to limit the entry point's config scope.\n- which boot simplification(s) are you referring to?\n- you don't need namespaces because entry points allow you to use any module naming convention you want - as long as it's on sys.path, it can be loaded.\n. @wking it makes sense to me to allow them to have full access to the entire config\n. @dmp42 for slightly more clarification, https://github.com/ncdc/docker-registry/compare/entrypoints#diff-3d831210b96f990411946a7ca45704e2R7 loads all entry points in the group 'docker_registry.extensions'. This doesn't have any real meaning in a Python sense - the group does not refer to a specific module or anything like that. I could have called it something else\u2026 what's important is that an extension specify the same group that the registry passes to iter_entry_points, like this in setup.py:\nentry_points = {\n  'docker_registry.extensions': [\n    '[some arbitrary name] = [some.real.python.module]'\n  ]\n},\n. @dmp42 sure - I just turned my branch into #589 \n. @wking updated - better?\n. I didn't want to use it for the local variable to make it clear that the data being retrieved from the store is supposed to be an image id, as opposed to metadata (json).\n. Ok, updated.\n. @dmp42 @wking @shin- FYI\n. @dmp42 Updated with a list method, updated setup.py, and my local tox run passed\n. @mhrivnak thanks, fixed\n. @mhrivnak when I run tox as well as docker build, the tests pass and the image that's built works (it has pkg_resources installed). It may be better to list the dependency explicitly, but that probably should go as a separate PR.\n. @dmp42 @wking @shin- @mhrivnak please review the latest commit and let me know if you need any other changes.\n. Updated to address comments in the readme.\n. @wking sorry, didn't mean to leave out those 2 edits - I just missed them :-/\nAlso, I think we have/had 2 extension loading mechanisms on the table:\n1. automatically load extensions if they exist\n   1. package namespaces\n   2. setuptools entrypoints\n2. manually explicitly specify which extensions to load\n   1. import_modules\nOf those 3 potential implementations, we eliminated package namespaces as it requires more setup steps for the extension developer and it makes packaging extensions in formats such as RPM more difficult.\nWith the remaining 2, I guess it all comes down to if we want extensions loaded automatically vs manually.\n. Latest commit bba8552 should have the remaining 2 readme updates in it.\n. @smarterclayton any comments on https://github.com/docker/docker-registry/pull/589#issuecomment-56573944 ?\n. @dmp42 here you go :)\n. @dmp42 I did, and the image starts up ok.\n. I am thinking the extensions repository could be an easy way to find a variety of extensions that users may want to run, but that aren't required in the core registry.\n. @wking @dmp42 thoughts? There are some TODOs in the extension's setup.py so I'd appreciate your input on those as well as the overall format and structure of the extension.\n. Assuming you like this, how do we get flake8 to ignore the setup.py in the extension's folder, or do I just need to fix the file to pass the style tests?\n. @wking I created a separate PR for the log format: #608 \n. @wking I added a license and readme for the extension. Also, I changed the author in setup.py to you, which I think makes sense if you're ok with it. I still have TODO for author_email and url...\n. 1 other thing - how would you write tests for this?\n. @dmp42 we're ok closing this as long as the v2 registry supports pruning/gc in some fashion (either directly or via an extension)\n. @wking I can do that. Do you want me to switch it now or wait for others to chime in?\n. @dmp42 sure, I'll update the PR.\n@wking the reason I put the logger's name in [brackets] is that I'm used to JBoss log formatting, which typically does the class name that way.\nI've found that if we specify --log-config when running gunicorn, it will control the log format for everything, and the format set in app.py is ignored. I am thinking this is a good thing, as it allows end users to control the log format instead of having it hardcoded. We can add an example/default logging.conf (see e.g. https://github.com/benoitc/gunicorn/blob/master/examples/logging.conf) and allow users to override it with a GUNICORN_LOG_CONFIG environment variable.\nThoughts?\n. Exactly :-) So we just need a reasonable default logging.conf.\n. @dmp42 we're ok closing in favor of waiting for v2\n. @dmp42 thoughts? I also want to modify the 3 places that currently short-circuit if cfg.standalone is True (validate_parent_access, check_token, check_images_list) and make the authorization call-outs pluggable as well.\n. You can't currently point the Docker CLI to an in-house index. If I want to docker push myregistry.internal/namespace/repo, there's no way that I know of to have the CLI talk to myindex.internal for all the interactions it would otherwise have with the Hub.\nIt looks like https://github.com/docker/docker/issues/8329 is attempting to allow the CLI to use different indexes.\nThis at least allows the registry to serve the index routes and delegate their handling to something else that's pluggable.\n. ... although I did just notice that in my attempt to run a local registry with a local index, the registry isn't returning an auth token from the put_repository route, so the CLI isn't able to proceed with a push.\n. @dmp42 yes, you're right - I wasn't thinking straight :-) Sorry for the noise\n. @adamhadani I created a virtualenv for it, activated it, and then ran python setup.py develop in the directory where I cloned docker-registry. And here's how I run it:\nexport DOCKER_REGISTRY_CONFIG=$(realpath $(dirname $0))/config/config_sample.yml\nexport STORAGE_PATH=$HOME/registry\ngunicorn --access-logfile - --error-logfile - --max-requests 100 -k gevent --graceful-timeout 3600 -t 3600 -w 1 -b 0.0.0.0:5000 --reload docker_registry.wsgi:application\n. That time works for me, as long as it doesn't go more than 30 minutes - I have another meeting after.\n. I'd like to jump back onto the always-proxy bandwagon here. I think it will be cleanest if we leave the authentication and authorization tasks to a proxy that sits in front of the registry. The proxy could take a few different forms:\n1. Use a common web server, such as Apache or Nginx\n2. Use a completely custom-written standalone web server that is specific to your auth* needs\n3. Compile the custom proxy logic into a custom-built registry binary\nI think all 3 options can be viable, although for 3 we will need to make sure we design the registry's code so that it lends itself to this approach.\nI think separating the concerns of registry functionality and auth* functionality is the right approach to take. We still should be able to come up with a token-based auth* mechanism that is the \"standard\" token-based approach the engine and the registry use to communicate, in addition to supporting the other 2 options discussed on irc (basic auth, custom auth).\n@dmp42 I know you've expressed concerns about proxies and timeouts, but I believe we can work through those issues with a combination of proxy tuning and resumable operations.\n. > But there is a clear use-case for implementing a key + token based authentication mechanism, and the proxy approach doesn't fly for it, for all the reasons listed. :)\nWhy couldn't someone do either story 1 or story 2 in a proxy, and disallow direct client access to the registry? (With story 2, the proxy would be responsible for calling out to the auth service to verify the client's token, instead of the registry.)\n. AFAIK this isn't possible the way things are currently coded in Docker and the Registry.\n. That's so you can have the registry communicate with a custom index, if you have one. When you issue push/pull commands, they would be against your index, and your index would be responsible for informing the Docker CLI/daemon about your registry, but that is all behind the scenes.\n. OpenShift keeps track of Docker image metadata and can perform actions such as updating a deployment when it notices there's an updated image in a repository. We're currently doing this with the v1 registry using an extension we wrote that does an http post to OpenShift whenever there's a new tag.\nI'll second @wking's thoughts re the ability for a downstream client to request events from history. If we're unable to receive \"new image\" / \"new tag\" events in real time (e.g. our server is down, there's a network hiccup, or whatever), we'll need an efficient mechanism to reconcile OpenShift's view of all the Docker repositories it's watching with the actual state of those repositories. I suppose a single request to reconcile would be ideal (e.g. give me all the changes to all the repositories starting with event 123). Not sure how feasible that is.\n/cc @smarterclayton\n. @wking but the V2 registry API proposal currently says this:\n\nAfter assembling the image manifest, the client must first push the individual layers. When the layers are fully pushed into the registry, the client should upload the signed manifest.\n\nIf that is still true, you can only do the quota check as each layer is presented to the registry. This is the primary reason for my concern about not being able to do a transactional quota check up front. And also that if we proceed with this API flow, it will be impossible to later add support for an up-front quota check.\n. I hadn't ever really considered charging multiple namespaces for portions of the same layer. I can see why that might be useful, but it wasn't something I was initially considering. My thinking is that the layer's size is 100% charged to whatever namespace and/or repository it's initially uploaded to. Here's an example:\nBob creates an image based on centos:centos7 and adds 1 new layer that's 50MB. Bob isn't charged for any portion of centos:centos7 because all of those layers already exist. He's uploading just a single new layer, so the 50MB from that layer is charged to his namespace and/or repository.\nI can definitely see quotas at the namespace level. In OpenShift, we'll most likely equate 1 OpenShift \"project\" with a corresponding Docker registry namespace (a project being a grouping of related services, pods/containers, volumes, and so on). It makes sense for us to assign a quota to the project (i.e. namespace) as a whole. We may even want to have the quota apply to a user account, since a user can have multiple projects. I think if we were to try to do that, we'd probably need to find a way to assign multiple namespaces to the same quota bucket, or we'd need to externalize the go/no-go decision to OpenShift.\nI'm not sure about use cases for quotas per repository... Need to think some more on that.\n. For OpenShift we may decide that widely useful layers such as centos:centos7 are owned by the system and not applicable to quotas.\nIf multiple users are uploading new layers to a namespace, the quota should be charged to the owner of the namespace.\nFor per-user quotas, if we wanted to say \"you get a free account at OpenShift, and with that comes some amount of free image storage,\" we might want the quota to apply to the user across all the registry namespaces they own. So it's really just a way to tie multiple namespaces together in a single quota group.\n@smarterclayton any thoughts here?\n. > If you've got per-user quotas like that, do you still need per-namespace quotas?\nWe may or may not - we're still working out our needs, so I'd like to keep our options open.\n. One way that could work to satisfy most/any/all needs would be to determine all the locations in the code where quota needs to be checked and enforced, and delegate the handling in all those places to an extension or plugin.\n. Closing as this repo is deprecated. > The main issue with this is that it makes registry garbage collection nearly impossible, because all layers will technically be referenced by all manifests.\nIf pruning is pluggable, you could have a default implementation that prunes layers and manifests as soon as they become stale (i.e. no longer actively referenced by a tag). This would require thin tags I would imagine. Other pluggable implementations could defer to an external system (e.g. OpenShift) for quota enforcement and pruning decisions, or support simple caps such as keeping at most n revisions of a manifest for namespace/repo:tag.\n. I was thinking about accessing via the content-addressable hash, yes. Although if we can have thin tags back, I think that would likely be sufficient for our needs. We'd either migrate our v1 extension to v2, where the registry extension automatically adds a thin tag on every push, or we'd have our deployment code create the thin tag on the fly and then use that as the image name when deploying.\n. @stevvooe @wking @titanous @dmp42  how about something like this:\nNormal push command as we know it today:\ndocker push foo/bar:latest (where the digest for the image is D1)\n~~Command to flag an image (by tag & digest) so it won't be garbage collected on future pushes:~~\n~~docker mark foo/bar:latest (marks the current image)~~\n~~docker mark foo/bar:latest@D1 (marks a specific image by digest)~~\nDo another push. This time, because we asked to keep the previous version of the image around, it's not deleted:\ndocker push foo/bar:latest (digest=D2)\nRetrieve latest version of image:\ndocker pull foo/bar:latest (gets you the most recent, D2)\nRetrieve a specific version:\ndocker pull foo/bar:latest@D1\ndocker pull foo/bar:latest@D2\n~~Command to remove the hold on a particular image + digest:~~\n~~docker unmark foo/bar:latest@D1~~\nI'm trying to think of something that doesn't require huge changes to the v2 proposals, but that still can give us \"pull by immutable identifier.\" This gives control to the user to decide which images to hold on to, doesn't require the manual use of mutable tags, and still allows the registry to do garbage collection. What do you all think?\nEDIT: instead of mark/unmark described above, a better UX would be to automatically preserve n revisions/digests per manifest. The value of n could be controlled globally by the registry operator, and optionally set by operators and/or users per repository as well. A value of 1 would keep the current behavior of only allowing 1 revision per tag.\n. @titanous if Bob wants to deploy Alice's alice/apache:latest (digest=D1) and Alice then pushes a new version of the image (digest=D2), it should be up to Alice and/or the registry operator to decide if D1 is preserved, wouldn't you think? Bob shouldn't be able to impact that decision, since it's Alice (and/or the registry operator) who is paying for storage of her images.\n. In other words, Bob could refer to alice/apache:latest:D1, but there's no guarantee that image will always be around, since it's not Bob's decision to make.\n. @titanous so... is my suggestion still an issue for you? The marking is strictly a means to inform the registry not to GC a particular manifest.\n. @titanous yes, marking shouldn't be required to fetch by digest\n. @smarterclayton I don't think there's a need for ref counting, since layers can be deleted when they're referenced by 0 manifests.\n. Some additional questions I just thought of: what do you do if you have a few different revisions of foo/bar:latest, and then you delete foo/bar:latest? Does \"latest\" move back to the previous revision? Do we delete all traces of that image? Do we just delete the ability to pull foo/bar:latest but keep the ability to pull foo/bar:latest:$digest?\n. FYI, I have \"pull by tag + digest\" working locally, and there's support for \"docker run\" as well. Here's the quickly hacked together prototype:\nhttps://github.com/ncdc/docker-registry/compare/docker:next-generation...ng-pull-by-digest\nhttps://github.com/ncdc/docker/compare/dmcgowan:v2-registry-pushpull...v2-registry-pullbydigest\n. And in the prototype above, the TagStore json file now looks like this: https://gist.github.com/ncdc/c6fb6cba18dfe679a3b6\n. @titanous @wking if you have time, please give my POC a spin and let me know what you think.\n. @stevvooe I definitely like your suggestion of having the registry compute the digest and return it to the client.\nI know you had previously expressed concerns about the ability to perform GC if the registry retains copies of every revision of every manifest (unless they're somehow deleted, either manually by a user or automatically via some sort of policy). Are these concerns still an issue for you, or are they mitigated by the delete mechanics listed in bullet 4 above?\ncc @smarterclayton - any additional thoughts on the previous comment?\n. @dmp42 I tried docker-registry but it looks like it needs to be docker_registry with an underscore. Can you review & confirm?\n. Would it make more sense to return an empty Config here? That way, extensions can do\nif cfg.some_key:\n   \u2026\nwithout first having to check if cfg was None or valid\n. FYI this line has been removed from the master branch and should be removed from this PR as well - it breaks the docker-registry wrapper script.\n. This effectively calls import [entrypoint] for each entry point it finds. There are other ways you can use entry points, however. You can reference attrs inside a module (e.g. foo.bar:MyClass), so we conceivably could come up with something more detailed/contract-oriented beyond just importing each entry point module. I didn't have anything in mind specifically, so I went with this simple approach for now.\n. I couldn't get my test extension to work without this, for some reason.\n. I'm not able to start the registry unless this file exists (I'm testing locally by running gunicorn directly - not via a Docker image, so I don't know if it would behave differently in that scenario)\n. Are you thinking you'd want boot to call list?\n. Updated - PTAL\n. Ah, yeah, that works - thanks\n. No, it's under openshift. You do at least need an empty __init__.py (but not one that specifies the namespace package setup). I'll update.\n. Something like this?\n```\nextensions = []\ndef boot():\n    for ep in pkg_resources.iter_entry_points('docker_registry.extensions'):\n        extensions.append(ep)\n        ep.load()\ndef list():\n    return extensions\n```\n. @mhrivnak I'm not a Python expert - is that a better way to do it?\n. @mhrivnak updated to cast it to a list\n. s/Id/Identifier/ ?\n. ",
    "acasanova": "I checked out the 0.7.1 tag and it seems to work now.\n. @dmp42 I initially was getting the error below\n2014/06/19 20:40:34 unable to remount sys readonly: unable to mount sys as readonly max retries reached\nSo i changed  /etc/sysconfig/docker to have this:\nother_args=\"--exec-driver=lxc --selinux-enabled\"\nThen that issue went away and this one started. The version in the EPEL for CentOS 6 is 1.1.2. I haven't found any docs on upgrading to a later version.\n. Thank you guys, @visualphoenix your solution worked. \n. ",
    "sfitts": "Correct.  One follow up.  The ideal solution for us would be to be able to use Artifactory for Docker in much the same was as we currently use it for Maven based repositories.  That is we would prefer to use it as a intermediate repository that sits between us and the central Docker repository.  So if I do a pull of an image it would first check my local repo and then go to the configured remote repo(s) if necessary.  The ability to configure the intermediate repository as a cache (ala Maven) would also be useful.\n. @larrycai thanks for the heads up -- voted for it.\n@dmp42 thanks for your earlier reply (and sorry I went radio silent, things are very busy right now).  I agree that to use Artifactory as anything other than an alternative private registry would take core Docker changes.  Personally I'd like to see those as I agree with many of the critiques of the current approach to naming raised in #168.  However, you are right that this seems very unlikely given @shykes current position (this is part of why I didn't see any reason to engage with Docker core on the topic, they don't seem to be welcoming that right now).  Thanks again though.\n. @dmp42 \n\nPossible user scenarios we are aware of so far:\n1. I want a simple private registry to test, or just to enjoy better \"local-network\" speed, and I don't care about authentication\n2. I want to host my content on my own registry, but I want to manage my users and authorization from the hub\n3. I want to host my content and manage my users and authz myself, using basic auth\n4. I want to allow users to use per-engine, key based authentication, like the hub does, and I want to manage my users and authz myself\n5. I want to host my content and manage my users and authz myself, using another exotic client authentication mechanism\nIf you guys can please kind of \"vote\" on what scenarios you think are useful / most likely, that would help prioritize.\n\nFWIW, we would like to use option 2.  \nActually what I'd really like is option 2a) \"... but I want to manage my users and authorizations from Github\".  But that's probably more of a Hub thing than a registry thing. \n. ",
    "larrycai": "Probably it is better to vote in Artifactory RTFACT-6494 for supporting this\n. Noticed, if I pull from localhost but using ip address (10.236.48.135), it doesn\u2019t work either\nembms@ubuntu135:~$ docker pull  10.236.48.135:2350/ubuntu\n 2014/08/05 16:07:14 Error: Invalid Registry endpoint: Get http://150.236.48.135:2350/v1/_ping: read tcp 153.88.253.150:8080: i/o timeout\nI tried to use curl command to test using registry API, it is there.\nNew to registry service, this is a trial setup in docker\n. No, as I commented secondly.\nEven I do this in locally using IP address, it doesn't work.\nOn machine, 10.236.48.135\n$ docker pull localhost:2350/ubuntu # this WORKS\n$ docker pull  10.236.48.135:2350/ubuntu # this DOESN\"T WORK\n2014/08/06 16:20:54 Error: Invalid Registry endpoint: Get        http://10.236.48.135:2350/v1/_ping: read tcp 153.88.253.150:8080: i/o timeout\nAnd in boot2docker, it can access 10.236.48.135 verified by curl command\nI agreed this is network issues, but how does docker check this ?\n. Ok, looks this is more docker client issue or question.\n153.88.253.150 is kind of nameserver in the company, just don't know how the docker pull command navigate the network. \nit can be connected directly, but docker tries to discover by another way.\nthank for your time, if I find the solution, will leave message here.\n. does official registry container list in localhost only for 5000 by default ? \n. @dmp42  yes, it is a proxy issue in docker daemon.\nI add ip address into no_proxy into /etc/default/docker, it works\nexport no_proxy=\".company.com,10.236.48.135\"\nI am not aware this proxy will be involved in this command as well.\nThank your very much\n. +1\n. @bjaglin I tried it and works perfect for me on pull/push/login commands !!! this shall be default configuration in nginx-registry, recommend to put under https://github.com/docker/docker-registry/tree/master/contrib/nginx \nOne extra questions for messages on unauthorized request (push without login)\ndocker@boot2docker:~$ docker push dokk.co/hello-world\nThe push refers to a repository [dokk.co/hello-world] (len: 1)\nSending image list\nIt doesn't clearly indicate the http 401 error, which I can see in nginx log, like \n172.17.42.1 - - [01/Dec/2014:04:27:59 +0000] \"PUT /v1/repositories/hello-world/ HTTP/1.1\" 401 194 \"-\" \"docker/1.3.2 go/go1.3.3 git-commit/39fa2fa kernel/3.16.7-tinycore64 os/linux arch/amd64\"\nor docker.log like\n```\n[debug] http.go:162 https://dokk.co/v1/repositories/hello-world/ -- HEADERS: map[User-Agent:  \n[docker/1.3.2 go/go1.3.3 git-commit/39fa2fa kernel/3.16.7-tinycore64 os/linux arch/amd64]   \nAuthorization:[Basic Og==]]\nError: Status 401 trying to push repository hello-world: \n401 Authorization Required\n\n401 Authorization Required\nnginx/1.6.2\n\n\n[998cd712] -job push(dokk.co/hello-world) = ERR (1)\n```\nCan we add extra configuration in nginx to response better warning \n. @bjaglin for line\nrewrite ^(.*)$ https://myregisty.net$1 last;\ncould be better for below ?\nrewrite ^(.*)$ https://$host$1 last;\nso if I test it locally, curl http://localhost it can redirect to https://localhost (I run it under nginx docker environment)\n. @bjaglin did you solve the problem I mentioned above ? cc @joda70 \nnot clear message indocker pushwithout authentication, though it shows 401 error in daemon logs.\nIt looks there are code in docker daemon to handle this logic, don't know why it doesn't work.  https://github.com/docker/docker/blob/master/api/client/commands.go (func CmdPush)\nif err := push(authConfig); err != nil {\n    if strings.Contains(err.Error(), \"Status 401\") {\n        fmt.Fprintln(cli.out, \"\\nPlease login prior to push:\")\n. @bjaglin like the new conf, quite clean, I also add simple LDAP certification as reference, see https://github.com/larrycai/nginx-registry , but above problem still there\n. it is ok for me for registry:0.9.0, and I used customized nginx conf (but shall be the same) see detail https://github.com/larrycai/nginx-registry\n. +1 for boot2docker/Windows for this error, and it disappear in 2nd time.\nregistry:0.9.1 \nboot2docker 1.4.1\n. ",
    "jtheuer": "Hi, I'm using the latest docker image of the registry (samalba/docker-registry:latest) and I'm not sure if i'm affected by this bug but symptoms are similar:\n2014-11-25 18:37:24,055 ERROR: Exception on /v1/images/175fcbbb61d0409fe2e181148d61bc575e62d61caa840cd88c276dbc35527a7d/layer [GET]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1817, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1477, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1381, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1475, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1461, in dispatch_request\n    return self.view_functionsrule.endpoint\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 264, in wrapper\n    return f(_args, _kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/images.py\", line 35, in wrapper\n    return f(_args, _kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/images.py\", line 56, in wrapper\n    return f(_args, _kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/mirroring.py\", line 128, in wrapper\n    resp = f(_args, _kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/images.py\", line 207, in get_image_layer\n    return _get_image_layer(image_id, headers, bytes_range)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/images.py\", line 82, in _get_image_layer\n    content_redirect_url = store.content_redirect_url(path)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/drivers/s3.py\", line 138, in content_redirect_url\n    if not self.signer:\nAttributeError: 'Storage' object has no attribute 'signer'\nIt happened when I selected set redirect_storage to true \nMy bucket is in the eu-west-1 region (and setting AWS_REGION to that value didn't work either: https://github.com/docker/docker-registry/issues/538)\nIs it the same error, should it be already fixed? Do you have other suggestions to fix it (or another issue that fits better?)\n. ",
    "BrianHicks": "I've found out the problem, with the help of jleimgruber on IRC. My routing layer was configured to time out after 10 seconds. Raising the limit to 100s solved the problem, in this case.\n. BTW if you're trying out Vulcan (like I was here) the issue is addressed in mailgun/vulcand#19\n. ",
    "genedna": "I have the same problem when I update the docker form 0.11.1 to 0.12.0.\n. ",
    "keeb": "When will this fix be pushed? It's happening on the public registry as of this morning.\n. @wking - thanks for fixing that issue!\n. ",
    "rlpowell": "Yes, also 0.12 for me, fwiw.\n. ",
    "johanneswuerbach": "+1, anything I could help to get this merged?\n. @dmp42 I wasn't aware that there is a hugh speed difference. What kind of difference are we talking about? The README makes it sound for me more like an optional thing, then something which is recommended. It's possible instead of You should or something like that.\nI'm currently running the registry inside deis, which uses local storage by default, but allows to switch to S3. Currently redis is used only for non LRU caching. This warning was confusing people using deis. Does it make sense to run an LRU cache also for local transfer and what is a recommended LRU size?\n. @dmp42 thanks for you input, seems like something that should be changed in deis.\n. ",
    "tclavier": "On my laptop with SSD i have no error :-(\n. yes\n```\ndocker --version\nDocker version 0.12.0, build 14680bf\n```\n. Ha yes, you're write, with \n```\ndocker --version\nDocker version 1.0.0, build 63fe64c\n```\nthat work fine.\n. ",
    "jgatkinsn": "docker-registry -> 0.7.0\ndocker  -> 1.0.0  on my machine that pushes\ndocker ->  0.9.1 that runs with the registry\nOf course, I come in this morning to recreate everything, and it all works.  I did restart the registry not in a container after playing with the container version to debug problems.  I still don't know why it kept giving me errors yesterday.  Somehow it got happy.  docker build -t let me put the port number in the registry address, and it just pushed to it.  Feels so weird after wasting 3 hours yesterday trying to get it to work again.\n. Nope, I take that back. I tried to build another image that doesn't exist on the local registry and now it fails.\nThis one just worked (which is something i've been working with):\ndocker build --tag=\"omadabot.adtran.com:5000/tacore_base\" .\nThis one won't work:\ndocker build --tag=\"omadabot.adtran.com:5000/SM_release\" .\nError:\n2014/06/11 08:55:50 Invalid repository name (SM_release), only [a-z0-9-_.] are allowed\nTell me what logs I need to add.\n. that worked.  I feel like an idiot now.  Thanks:-)\n. ",
    "danzy": "Hi there!\nThanks for the quick response; the part in question is here:\nhttps://github.com/dotcloud/docker-registry/blob/0.7/README.md\nTowards the bottom (under \"Run it\"; above \"What about a production environment?\").\n. @jokeyrhyme @coaic @benbarclay @ashishtilara\n. Thanks for the response! \nI just had a clarifying question though, what's the purpose of that index_endpoint setting?\n. ",
    "shayneoneill": "Yeah this happens to me as well both in the version I grabbed from here and the version being shipped with deis. According to the guys in the office they've had the same problem. Out of the box essentially if you run the container and docker login  to it, you get asked to activate it at said url, which is simply a 404 page. \nKind of a show stopper :(\n. Nope. Still broken. Accounts can not be activated. Re-set up at home cleanroom, and same result. Its a non-functioning product, people.\n. Oh. Thats strange. Shouldn't the username/password prompt be removed if the registry doesn't support it?\n. ",
    "xbgmsharp": "Hi all.\n:+1: for this. The documentation need re-phrase.\nI try to create user too but no activation.\nIf I understand correctly the comment in this issue, there is no need for account creation.\nBut as there is no user management on standalone registries, how can I multiple users repository image?\n. ",
    "holsety": "thanks, it works!\n. ",
    "hugoduncan": "I'm seeing the same behaviour with the 0.7.2 image, but not with 0.7.1.\n. I didn't have standalone set to true.  I'll try it later.  If this is the resulting error from not setting it, then it seems a little obscure.\n. Running with -e STANDALONE=true still exhibits a 405 error for me.\n. ",
    "jfromaniello": "@hugoduncan  thanks! 0.7.1 works well for me too\n. hey @shin- when you run the registry with docker like this:\n$ sudo docker run -d -p 5000:5000 registry\nwhat's the config it assumes? Should I mount a volume dir with a config file?\n. @dmp42 thanks a lot\n. ",
    "iamlittle": "man, struggled with this all morning. glad its a simple fix\n. The STANDALONE=True flag didn't take for me either, however adding standalone: True to the prod section of the config yml, using the some env flags, and adding a mount I was able to get it to work. \nFor example  \ndocker run -d -p 5000:5000 -e SETTINGS_FLAVOR=prod \\\n-e DOCKER_REGISTRY_CONFIG=/etc/docker/config.yml \\\n-v $(pwd)/config.yml:/etc/docker/config.yml registry\n. ",
    "huataihuang": "I have try docker-registry 0.8.1, the latest version, follow README.md to install.\nyum install python-devel libevent-devel python-pip gcc xz-devel\npython-pip install docker-registry[bugsnag,newrelic]\nI got same \"405 Method Not Allowed\" response when I \"docker push\".\nI downgrade to docker-registry 0.7.0 and docker-registry-core 1.0.6 , then solved the problem.\n. @dmp42 \nI have existing server CentOS 7, which is upgrade from CentOS 6.5 follow Upgrade to CentOS 7. I just want direct running the registry on an existing server to try docker-registry.\nrpm -ivh http://mirrors.neusoft.edu.cn/epel/7/x86_64/e/epel-release-7-1.noarch.rpm\nyum install python-devel libevent-devel python-pip gcc xz-devel\npython-pip install docker-registry[bugsnag,newrelic]\n/usr/lib/python2.7/site-packages/config/config.yml:\n```\ncommon:\n    loglevel: info\n    search_backend: \"_env:SEARCH_BACKEND:\"\n    sqlalchemy_index_database:\n        \"_env:SQLALCHEMY_INDEX_DATABASE:sqlite:////tmp/docker-registry.db\"\nprod:\n    loglevel: warn\n    storage: s3\n    s3_access_key: _env:AWS_S3_ACCESS_KEY\n    s3_secret_key: _env:AWS_S3_SECRET_KEY\n    s3_bucket: _env:AWS_S3_BUCKET\n    boto_bucket: _env:AWS_S3_BUCKET\n    storage_path: /srv/docker\n    smtp_host: localhost\n    from_addr: docker@myself.com\n    to_addr: my@myself.com\ndev:\n    loglevel: debug\n    storage: local\n    storage_path: /data/registry\ntest:\n    storage: local\n    storage_path: /tmp/tmpdockertmp\n```\ncreate the registry directory:\nmkdir -p /data/registry\nchmod 777 /data/registry\n/etc/profile \nexport REGISTRY_PORT=80\nlaunch docker-registry as follow:\ndocker-registry\n. I follow @bhuvaneswaran's commit, It need add \nstandalone: true\n    disable_token_auth: true\nto \"Example config\" of README.md.\nNow I can use docker-registry 0.8.1\nMaybe these two lines need add to \"Example config\" for the newbie.\nThanks @dmp42 @bhuvaneswaran \n. ",
    "fairbairn": "I can confirm the problem still exists with docker-registry version 0.8.0 with nginx 1.6.2.\nRunning the exact same docker run command using the same config.yml but specifying registry:0.7.0 as the image version bypasses the nginx PUT 405 issue over HTTP and seems to work.\n. Absolutely, and we've been running with that flag set with no issue for months, but after upgrading to registry:0.8.0, it started to fail.  Going back to 0.7.0 allowed us to continue to work.  We're running over HTTP to an internal deployment of docker-registry right now because apparently it doesn't yet support self-signed certs over HTTPS.\ncommon:\n  loglevel: debug\n  standalone: true\n  search_backend: sqlalchemy\n  sqlalchemy_index_database: sqlite:////tmp/Registry/docker-registry.db\ndev:\n  loglevel: debug\n  storage: file\n  storage_path: /tmp/Registry/\nprod:\n  storage: s3\n  s3_bucket: *\n  boto_bucket: **\n  storage_path: /registry/\n  s3_access_key: ***\n  s3_secret_key: ***\n  s3_secure: true\n. We're running under supervisor and this is our config...we run two instances, one with a file store and one with an S3 store...\n[program:registry]\ncommand=docker run --rm -p 0.0.0.0:5000:5000 -v /home/docker:/registry -v /share/Registry:/tmp/Registry -v /etc/hosts:/etc/hosts -e DOCKER_REGISTRY_CONFIG=/registry/config.yml registry:0.7.0\nautostart=true\nstopsignal=INT\n[program:registry-prod]\ncommand=docker run --rm -p 0.0.0.0:5001:5000 -v /home/docker:/registry -v /etc/hosts:/etc/hosts -e SETTINGS_FLAVOR=prod -e DOCKER_REGISTRY_CONFIG=/registry/config.yml registry:0.7.0\nautostart=true\nstopsignal=INT\n. We'll upgrade to 0.8.1 and use your template as a guideline.  BTW, our format was also documented on that same github page, which is why we adopted it.  It said nothing about common inheritance, and I guess we didn't realize that was necessary when we added the standalone=true tag above.\nDoes docker allow us to use self-signed certs at this point over HTTPS, using this latest registry with nginx up front, or will it still require HTTP only?  Thanks.\n. @dmp42, I understand your frustration and desire to streamline these questions by following your stated guidelines; however, it might go a long way to reducing confusion if the README.md file on the github page be updated to be clearer.  Right now, your primary example config file does not inherit common, yet a lot of people will initially grab that file and use it to get a private registry up quickly.  I might suggest you update that doc to reflect your new config file requirements.  Inheriting common is key, as the documentation (and forum questions) relating to standalone mode to date are not crystal clear.  We can set it true all day long, but if we're using the config file example from the primary landing page, then it will never work.  Hope this feedback is helpful.\n. ",
    "brahman81": "Nice one. Thanks.\n. ",
    "StumbleUponIT": "Thank you, that's extremely helpful.\n. AWESOME. The new image worked immediately. Thank you!\n. ",
    "odk211": "Request to \"/v1/repositories//json\"\nresults the same.\n. Thank you for your reply. and sorry for lack of security.\nNow, I disabled this feature by default, and you can enable this by CORS_ORIGIN environment variable.\nPlease check.\n[How to Use]\n- return Access-Control-Allow-Origin: null (default)\ndocker run -d  registry\n- return Access-Control-Allow-Origin: http://hogehoge.com/\ndocker run -d -e CORS_ORIGIN=http://hogehoge.com/ registry\n- return Access-Control-Allow-Origin: *\ndocker run -d -e CORS_ORIGIN registry\n. Hello, thank you for reply.\n\nWhy not use application wide settings for this, instead of decorating each and every method?\n\nThis is because, some APIs(for example, GET /v1/search) are already allowed cors to all origin. So if I use application wide settings, those API's behavior is not the same as before.\nthanks.\n. yeah, I think it's better. \nNow, I plan\n- enable to load flask_cors settings by libs/config.py\n- enable to inject user-setting by the same name environment variables as flask_cors (prefixed with CORS_).\n- default settings are the same as flask_cors default, except for CORS_ORIGINS. (I'll set null by default)\n- set these settings to flask.current_app.config at application initializing, So it is applied. \n- cleanup the snippet. finally all cors allowed API marked by \"@flask_cors.cross_origin()\"\nNote\n- all method(GET POST ...) are allowed.\n  It is difficult to improve this, I think. Because all APIs are added the OPTIONS routing, so pre-flight OPTIONS request routing is undecided.\nfor example below,\npre-flight OPTIONS request for PUT is routed to someGet and, Access-Control-Allow-Methods header is set to GET...\n@app.route(\"/\", methods=['GET'])\n@cross_origin(methods=['GET'])\ndef someGet():\n//\n@app.route(\"/\", methods=['PUT'])\n@cross_origin(methods=['PUT'])\ndef somePut():\n//\nif you agree this, I try. if you have another idea, please tell me.\nthanks!\n. or like http://coalkids.github.io/flask-cors.html\nthis do not need decorator at each method.\n. Now I applied flask_cors application wide settings. \nAll flask_cors settings is configured by environment variable.\nThe API that is enabled CORS should be decorated by @flask_cors.cross_origin().\n(Sorry, I can not removed)\nHow to use\n\nreturn Access-Control-Allow-Origin: null\n\n```\nNo Origin is allowed cors by default.\ndocker run -d registry\n```\n- return Access-Control-Allow-Origin: http://hogehoge.com/\ndocker run -d -e CORS_ORIGINS=http://hogehoge.com/ registry\n- return Access-Control-Allow-Origin: * (Be careful, This behavior changed. I do not know the reason...)\ndocker run -d -e CORS_ORIGINS=\\'*\\' registry\n- return Access-Control-Allow-Origin: \"requested client Origin\"\ndocker run -d -e CORS_ORIGINS=\\'*\\' -e CORS_SEND_WILDCARD=false registry\n. and I changed to use latest version of flask_cors.\n. @dmp42 Sorry, I tried, but I can't...\nIt seems that Flask do not provide any endpoints to inject global decorator.\n. @dmp42 thanks! I fix it and flask_cors version is updated.\nIt works.\nThere is a little change at how to use.\nHow to use\n\nAccess-Control-Allow-Origin: null\n\n```\nNo Origin is allowed cors by default.\ndocker run -d registry\n```\nAccess-Control-Allow-Origin: http://worksap-ate.github.io\ndocker run -d -e CORS_ORIGINS=[\\'http://worksap-ate.github.io\\'] registry\nAccess-Control-Allow-Origin: * \ndocker run -d -e CORS_ORIGINS=[\\'*\\'] registry\nAccess-Control-Allow-Origin: \"requested Origin\"\ndocker run -d -e CORS_ORIGINS=[\\'*\\'] -e CORS_SEND_WILDCARD=false registry\n. @dmp42 Thank you for your suggestion. Finally I can remove all decorators!\nHow to use\n\nAccess-Control-Allow-Origin: null\n\n```\nNo Origin is allowed cors by default.\ndocker run -d registry\n```\nAccess-Control-Allow-Origin: http://worksap-ate.github.io\ndocker run -d -e CORS_ORIGINS=[\\'http://worksap-ate.github.io\\'] registry\nAccess-Control-Allow-Origin: * \ndocker run -d -e CORS_ORIGINS=[\\'*\\'] registry\nAccess-Control-Allow-Origin: \"requested Origin\"\ndocker run -d -e CORS_ORIGINS=[\\'*\\'] -e CORS_SEND_WILDCARD=false registry\nAllow CORS only '/v1/search'\ndocker run -d -e CORS_ORIGINS=[\\'*\\'] -e CORS_RESOURCES=[\\'/v1/search\\'] registry\n. @dmp42  I squash it. But it seems that current build is broken.\n\nI have a simple question (see above).\n\nI replied it, Please check.\n\nI would like an update to the README with the doc.\n\nThanks please.\nIf you can, please introduce our web-ui :)\nhttps://github.com/worksap-ate/docker-registry-ui\n. Thank you :)\n. Maybe no problem, We can write like \norigins: _env:CORS_ORIGINS:\"null\"\nBut sorry I can not confirm, because current build is broken.\nflask_cors example looks that we can use single value.\nhttps://github.com/wcdolphin/flask-cors/blob/master/examples/app_based_example.py\nIf you like above style. I will change.\n. ",
    "esnunes": "+1\nthanks for fixing it @dmp42, it worked perfect!!!\n. ",
    "pkaeding": "@dmp42 Thanks, that indeed solves my problem!\n. ",
    "rosstimson": "@quentinsf I'm trying to do the exact same thing and running in to similar issues.  I have auth disabled on /_ping and /v1/_ping but I'd not come across the recommendation for also disabling on /v1/users.\nI'd also not read anything about redirecting everything to HTTPS, I currently only accept HTTPS traffic and nothing is listening on port 80.\nWould you mind posting a gist of your nginx.conf so I can compare against mine, I'm sure that would help more knowledgable people to troubleshoot this too.\n. I second @dmp42's wish for a canonical Nginx config (alongside a Docker image).  \nI had numerous issues when I last attempted this and eventually removed it all and just locked by IP for now - not ideal.\n. ",
    "quentinsf": "Thanks all, for your suggestions - will explore more as soon as I can - probably tomorrow.\nIn the meantime, here's my nginx config - if I uncomment the lines enabling basic auth, I can no longer push anything up.  With them commented, it works fine.\nhttps://gist.github.com/quentinsf/b99f19c6f32df20e8eda\nOh, and re #230 , I haven't tried it yet because I do need authentication on both push and pull, but I've experimented with some of the other suggestions made there,\n. Dear all - a quick follow-up to say that by the time I got around to testing this again, merging in the latest master sorted out my 'basic auth' problems.  Many thanks to all!\n. ",
    "weil": "seems like it was pushing successfully. Setting up xfonts-encodings (1:1.0.4-1ubuntu1) ...\nSetting up xfonts-utils (1:7.6+1) ...\nSetting up gsfonts-x11 (0.22) ...\nProcessing triggers for libc-bin ...\nldconfig deferred processing now taking place\n ---> 0da4fc088e62\nRemoving intermediate container 6fbca4dc736f\nStep 8 : add ./extraction /home/\n ---> 73d331a4444f\nRemoving intermediate container 052fc7d49cb3\nStep 9 : cmd /home/extraction-siri.sh\n ---> Running in 89850987102e\n ---> fadf158a4f8d\nRemoving intermediate container 89850987102e\nSuccessfully built fadf158a4f8d\nThe push refers to a repository liaoweien/extraction2\nSending image list\nPushing repository liaoweien/extraction2 (1 tags)\n511136ea3c5a: Image already pushed, skipping\n65b7e9ccb809: Image already pushed, skipping\nf8dd6bd14f58: Image already pushed, skipping\na343823119db: Image already pushed, skipping\nea7d6801c538: Image already pushed, skipping\nd52d5d5ec46d: Image successfully pushed\n6c369cb4f356: Image successfully pushed\ne52802b3ec0f: Image successfully pushed\ndf32e8182a6c: Image successfully pushed\nddbef340c586: Image successfully pushed\nff20cd6c1615: Image successfully pushed\n0da4fc088e62: Image successfully pushed\n73d331a4444f: Image successfully pushed\nfadf158a4f8d: Image successfully pushed\nPushing tag for rev [fadf158a4f8d] on {https://registry-1.docker.io/v1/repositories/liaoweien/extraction2/tags/latest}\nI will try it again. @dmp42  Thank you!\n. ",
    "chewmanfoo": "I have a similar problem - my docker push commands have been return a non-zero exit code for a week of banging my head against the wall - but today somehow they're working and the exit code is 0.  \nNow I try to pull the image onto my laptop and I see:\n[root@localhost ~]# docker -H tcp://10.14.244.190:5000 run 10.14.244.190:5000/cem_centos -i /bin/bash\nUnable to find image '10.14.244.190:5000/cem_centos' locally\n2014/08/05 15:00:50 Error: <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>404 Not Found</title>\n<h1>Not Found</h1>\n<p>The requested URL was not found on the server.  If you entered the URL manually please check your spelling and try again.</p>\nTo verify connectivity:\n[root@localhost ~]# curl http://10.14.244.190:5000\n\"docker-registry server (dev) (v0.7.3)\"\nAre there a list of commands one can run on the registry to see what images are successfully stored there?\n. Probably should also mention - I'm just running the 'Quick start' with local storage.  I started it like this:\nsudo docker run -d -e STORAGE_PATH=/login/sg218049/docker-store -p 5000:5000 registry\nThat storage path exists, but is empty.\n. the docker log on the client shows a successful push:\n[e32c1134] +job push(ixregistry.sabre.com:5000/cem_centos)\n[e32c1134] -job push(ixregistry.sabre.com:5000/cem_centos) = OK (0)\n. the registry stdout shows this when I try the pull:\n2014-08-05 20:26:02,814 INFO: 10.16.55.118 - - [05/Aug/2014:20:26:02] \"POST /v1.12/containers/create HTTP/1.1\" 404 233 \"-\" \"Docker-Client/1.0.0\"\n10.16.55.118 - - [05/Aug/2014:20:26:02] \"POST /images/create?fromImage=10.14.244.190%3A5000%2Fcem_centos&tag=latest HTTP/1.1\" 404 233 \"-\" \"Docker-Client/1.0.0\"\n2014-08-05 20:26:02,831 INFO: 10.16.55.118 - - [05/Aug/2014:20:26:02] \"POST /images/create?fromImage=10.14.244.190%3A5000%2Fcem_centos&tag=latest HTTP/1.1\" 404 233 \"-\" \"Docker-Client/1.0.0\"\n. learning a lot here!\nok after getting nsenter working, I got into the registry container and I see this:\nroot@a6c86e181bc0:/# ls /login/sg218049/docker-store/\nimages  repositories\nroot@a6c86e181bc0:/# ls /login/sg218049/docker-store/images\n0605476eff7b121618d2000b985115a654e21693d141dc3311686f525754481c  511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\n199d66c03145fdbb58590785b6e57526d03975f10ef9c53af1ed6df4324300a5  61cda1f9c68361757155dff8ac8c8e4a9406fd910c1430ade5afcb483c1a3187\n1a7dc42f78ba213ec1ac5cd04930011334536214ad26c8000f1eec72e302c041  72c9d271078e4ae8b153cdbc2bcd28164d944b3b7969913d42b03a4c6789dc33\n1c4cc6e54ee8bac6b26075986f3a5e46b34c1908e2dead00180d21692e84a8b4  770fcc04ecda8e4838a3733b67607bc3735b130c7efa1da0aa9090e41fff01a4\n2a7240b26864ce3c049614673d385741eb43bc3a9982cf7e5f1796b49c4f81a1  8609615f0dd3f16c4642a295d43e27764a6f835b3f11c67b3a047a62e62bd465\n34e94e67e63a0f079d9336b3c2a52e814d138e5b3f1f614a0cfe273814ed7c0a  b2fcf87ee1f91244939aa4f0ef3d92d4ef59618170441cc044a2bf8b378c1339\n4609760f419f109787e439f4f1d05a256309de5ce1ed375a08b0def49cb0da39  bf6edea8467ccb1720ce4342333f0c7b48d4871d5e3ef0ed2faffcd6ad03f71a\n46a9af2211dda2dbe707efb536f9ce4afcc986644d3485d7c8abdfdb114c902e  d6a90ca7e301adaeba02dc454028b343b2fc27ffaa6b37edc7b6cabb27d34dd7\n50696fe8658e93d54f1d520be3272d6956ed17c65553d977b8b898a27b6d41dd  f4910976ead4e667854ec36b12ce317b5720367a983c7b79b365ecc36ce42cd0\nroot@a6c86e181bc0:/# ls /login/sg218049/docker-store/repositories\nlibrary\nroot@a6c86e181bc0:/# ls /login/sg218049/docker-store/repositories/library\ncem_centos\nroot@a6c86e181bc0:/# ls /login/sg218049/docker-store/repositories/library/cem_centos\n_index_images  json  tag_latest  taglatest_json\n. do you mean the daemon on the client or the daemon on the host running the registry?\n. I ran /usr/bin/docker -d -D on the client doing the pulling - I see [debug] messages in stdout now.\nbut now when I execute docker -H tcp://10.14.244.190:5000 run 10.14.244.190:5000/cem_centos -i /bin/bash I don't see anything in stdout about the puil:\nroot@localhost ~]# /usr/bin/docker -d -D\n2014/08/05 17:04:06 WARNING: You are running linux kernel version 2.6.32-431.20.3.el6.x86_64, which might be unstable running docker. Please upgrade your kernel to 3.8.0.\n2014/08/05 17:04:06 docker daemon: 1.0.0 63fe64c/1.0.0; execdriver: native; graphdriver:\n[b8b81cfd] +job serveapi(unix:///var/run/docker.sock)\n[b8b81cfd] +job initserver()\n[b8b81cfd.initserver()] Creating server\n2014/08/05 17:04:06 Listening for HTTP on unix (/var/run/docker.sock)\n[debug] server.go:1114 Registering GET, /containers/{name:.*}/changes\n[debug] server.go:1114 Registering GET, /containers/{name:.*}/logs\n[debug] server.go:1114 Registering GET, /images/search\n[debug] server.go:1114 Registering GET, /containers/json\n[debug] server.go:1114 Registering GET, /containers/{name:.*}/export\n[debug] server.go:1114 Registering GET, /containers/{name:.*}/json\n[debug] server.go:1114 Registering GET, /containers/{name:.*}/attach/ws\n[debug] server.go:1114 Registering GET, /info\n[debug] server.go:1114 Registering GET, /version\n[debug] server.go:1114 Registering GET, /images/viz\n[debug] server.go:1114 Registering GET, /containers/ps\n[debug] server.go:1114 Registering GET, /containers/{name:.*}/top\n[debug] server.go:1114 Registering GET, /_ping\n[debug] server.go:1114 Registering GET, /events\n[debug] server.go:1114 Registering GET, /images/json\n[debug] server.go:1114 Registering GET, /images/{name:.*}/get\n[debug] server.go:1114 Registering GET, /images/{name:.*}/history\n[debug] server.go:1114 Registering GET, /images/{name:.*}/json\n[debug] server.go:1114 Registering POST, /commit\n[debug] server.go:1114 Registering POST, /images/create\n[debug] server.go:1114 Registering POST, /images/load\n[debug] server.go:1114 Registering POST, /images/{name:.*}/push\n[debug] server.go:1114 Registering POST, /images/{name:.*}/tag\n[debug] server.go:1114 Registering POST, /containers/{name:.*}/pause\n[debug] server.go:1114 Registering POST, /containers/{name:.*}/unpause\n[debug] server.go:1114 Registering POST, /containers/{name:.*}/restart\n[debug] server.go:1114 Registering POST, /containers/create\n[debug] server.go:1114 Registering POST, /containers/{name:.*}/kill\n[debug] server.go:1114 Registering POST, /containers/{name:.*}/start\n[debug] server.go:1114 Registering POST, /containers/{name:.*}/stop\n[debug] server.go:1114 Registering POST, /containers/{name:.*}/resize\n[debug] server.go:1114 Registering POST, /auth\n[debug] server.go:1114 Registering POST, /build\n[debug] server.go:1114 Registering POST, /containers/{name:.*}/wait\n[debug] server.go:1114 Registering POST, /containers/{name:.*}/attach\n[debug] server.go:1114 Registering POST, /containers/{name:.*}/copy\n[debug] server.go:1114 Registering DELETE, /containers/{name:.*}\n[debug] server.go:1114 Registering DELETE, /images/{name:.*}\n[debug] server.go:1114 Registering OPTIONS,\n[debug] server.go:1204 docker group found. gid: 498\n[debug] deviceset.go:552 Generated prefix: docker-253:0-1973426\n[debug] deviceset.go:555 Checking for existence of the pool 'docker-253:0-1973426-pool'\n[debug] deviceset.go:574 Pool doesn't exist. Creating it.\n[debug] attach_loopback.go:94 Error retrieving the next available loopback: open /dev/loop-control: no such file or directory\n[debug] attach_loopback.go:94 Error retrieving the next available loopback: open /dev/loop-control: no such file or directory\n[debug] daemon.go:782 Using graph driver devicemapper\n[debug] daemon.go:795 Creating images graph\n[debug] graph.go:66 Restored 35 elements\n[debug] daemon.go:807 Creating volumes graph\n[debug] graph.go:66 Restored 0 elements\n[debug] daemon.go:812 Creating repository list\n[b8b81cfd] +job init_networkdriver()\n[debug] /sbin/iptables, [-C POSTROUTING -t nat -s 172.17.42.1/16 ! -d 172.17.42.1/16 -j MASQUERADE]\n[debug] /sbin/iptables, [-I POSTROUTING -t nat -s 172.17.42.1/16 ! -d 172.17.42.1/16 -j MASQUERADE]\n[debug] /sbin/iptables, [-D FORWARD -i docker0 -o docker0 -j DROP]\n[debug] /sbin/iptables, [-C FORWARD -i docker0 -o docker0 -j ACCEPT]\n[debug] driver.go:209 Enable inter-container communication\n[debug] /sbin/iptables, [-I FORWARD -i docker0 -o docker0 -j ACCEPT]\n[debug] /sbin/iptables, [-C FORWARD -i docker0 ! -o docker0 -j ACCEPT]\n[debug] /sbin/iptables, [-I FORWARD -i docker0 ! -o docker0 -j ACCEPT]\n[debug] /sbin/iptables, [-C FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT]\n[debug] /sbin/iptables, [-I FORWARD -o docker0 -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT]\n[debug] /sbin/iptables, [-t nat -D PREROUTING -m addrtype --dst-type LOCAL -j DOCKER]\n[debug] /sbin/iptables, [-t nat -D OUTPUT -m addrtype --dst-type LOCAL ! --dst 127.0.0.0/8 -j DOCKER]\n[debug] /sbin/iptables, [-t nat -D OUTPUT -m addrtype --dst-type LOCAL -j DOCKER]\n[debug] /sbin/iptables, [-t nat -D PREROUTING -j DOCKER]\n[debug] /sbin/iptables, [-t nat -D OUTPUT -j DOCKER]\n[debug] /sbin/iptables, [-t nat -F DOCKER]\n[debug] /sbin/iptables, [-t nat -X DOCKER]\n[debug] /sbin/iptables, [-t nat -N DOCKER]\n[debug] /sbin/iptables, [-t nat -A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER]\n[debug] /sbin/iptables, [-t nat -A OUTPUT -m addrtype --dst-type LOCAL ! --dst 127.0.0.0/8 -j DOCKER]\n[b8b81cfd] -job init_networkdriver() = OK (0)\n[debug] daemon.go:342 Loaded container 0a1e58ac893643bc431052ba5bf53bdb4f68042290763d81ec2582bc289d79ba\n[debug] daemon.go:342 Loaded container 1f442784cf5d0e246f39cfb1a453beec8a73f0c1d71e460c0144fd6c7bf686ce\n[debug] daemon.go:342 Loaded container 23a99efb8f48ae1b41f4d2fcb316adf702fdc1d16652499e93e7cc5bf5614d58\n[debug] daemon.go:342 Loaded container 3b9abc46eee16003965c175f8fae410241f395fe07eb40373a45ea472e27918a\n[debug] daemon.go:342 Loaded container 423b6f923d32346458d839dc67220242467db89450743b29c5899eceaee652a7\n[debug] daemon.go:342 Loaded container 42d34465bed17229549b7dfe7a17a2ab7611003f7483c6497e175a870f52df8a\n[debug] daemon.go:342 Loaded container 52a39b584611c4a5c8609376026a2eb2ec13073a054527b01bbc13119473d128\n[debug] daemon.go:342 Loaded container 5945964e733ad61fe9a5f8bcac639ac0548d0d3ccb7726847c2dc48a05f0b0a3\n[debug] daemon.go:342 Loaded container 6170843cecd4577884cb2f680f15eba0b16c1b44f0894994792e80ece224f5f3\n[debug] daemon.go:342 Loaded container 68e2bd3b906932517ddb206fc33c8eda7767b39074b984de540720b3c001c770\n[debug] daemon.go:342 Loaded container 8b2bf03f810d6a439bacb48a9b9901cf33d22967d8a925ba0b6c0646f259f016\n[debug] daemon.go:342 Loaded container 9359bd873df7e66785a635897b2ac888a3a95757d41defa8853523f529dd29bd\n[debug] daemon.go:342 Loaded container 9707f775aa7a14af661e44f9ce44c6d8b15d5f55bf3f62d3895ebdd14ac6c4bb\n[debug] daemon.go:342 Loaded container 9ac32fbf9971b87f6dd950819e91a9c5027341179b6bbec7f3224465abded18d\n[debug] daemon.go:342 Loaded container abbc3a159eb17c79a94db419066282a54f197e01104d9ea9442c952490b37489\n[debug] daemon.go:342 Loaded container b843fe57c6466cf4ad16c0b2eb0faf58fd19fd05afeac316a3c4a226552b2fd4\n[debug] daemon.go:342 Loaded container c0a55605d0a95b34e1552fc6fa99cc3e378f05d9b1d6a40cb432c7c8636a9d09\n[debug] daemon.go:342 Loaded container cdb8b1fbb041bff6ed62140d79cc21d537b302b263bc57c202025702561e5d1b\n[debug] daemon.go:342 Loaded container d2cf95aae09d55a3168b3052e0c0579ea0dd525a35c526cc1b8660d07e6da461\n[debug] daemon.go:342 Loaded container d4f546f9812f0665071625f6e115aee24627b41c9fa2362f2428d49361fecc94\n[debug] daemon.go:342 Loaded container e7e033e622358d04934793545f50eaef16227a4de402c37bb516100c892f0d1a\n[debug] daemon.go:342 Loaded container ee256466a0880dd8d7c01c50f5c480258ed8b899f0590562e987d148618d0ec4\n[debug] daemon.go:342 Loaded container f67cf7babc102efcf21cb75a6ad3c59c960329c8faaef24e8e7a63a882f9edc7\n[debug] daemon.go:342 Loaded container f70e11e32fe600ccff0050e615d4ddb933230aa84410a5cbdace430d0865f6b3\n[debug] daemon.go:342 Loaded container fbfa47c84c17d1cc31619598d9d7bb2cbd938d617760a3ffc3769f167f6281f4\n[debug] daemon.go:342 Loaded container ff10f4a679b41dfbc85bc7cf139648dd86745f455f4e8b3a3e2fb1bc2bd622cd\n[b8b81cfd.initserver()] Creating pidfile\n[b8b81cfd.initserver()] Setting up signal traps\n[b8b81cfd] -job initserver() = OK (0)\n[b8b81cfd] +job acceptconnections()\n[b8b81cfd] -job acceptconnections() = OK (0)\n. This ought to not be so confusing,  What is the proper way to pull images from a private repo?  Please post an example of pulling images\n. so, since I have a docker daemon running on my laptop, then there is no need to bind to the remote daemon (where the registry is running), right?  Then how do I tell my local docker daemon about the remote registry?\n. I'm having a similar issue as above.\nI have an entry in /etc/hosts for a hostname in both the docker client and registry host.  I try \ncurl ixregistry.example.com:5000\nand I get\n\"docker-registry server (dev) (v0.7.3)\"\nI try\nsudo docker tag cem_centos ixregistry.example.com:5000/cem_centos\nsudo docker push ixregistry.example.com:5000/cem_centos\nand I see\nThe push refers to a repository [ixregistry.example.com:5000/cem_centos] (len: 1)\nSending image list\nThe docker log on the client shows\n```\n   [b4120e5b] -job push(ixregistry.example.com:5000/cem_centos) = ERR (1)\n   2014/08/05 10:17:33 POST /images/ixregistry.example.com:5000/cem_centos/push?tag=\n   [b4120e5b] +job push(ixregistry.example.com:5000/cem_centos)\n   Error: Status 502 trying to push repository cem_centos: Head\n\n\n\nMcAfee Web Gateway - Notification\n\n/Head\nContents\n FileName: notresolvable.html\n        Language: [en]\n   \nTitle\n\n\n\n         Host not resolvable.\n       \n\n\n/Title\nContent\n\n\n\n         The Host is not resolvable.\n       \n\n\n/Content\nInfo\n\n\n\nURL: http://ixregistry.example.com:5000/v1/repositories/cem_centos/\n\n\n\n/Info\n/Contents\n```\neven though\n-bash-4.1$ ping ixregistry.example.com\n   PING ixregistry.example.com (10.14.244.190) 56(84) bytes of data.\n   64 bytes from ixregistry.example.com (10.14.244.190): icmp_seq=1 ttl=64 time=1.99 ms\n   64 bytes from ixregistry.example.com (10.14.244.190): icmp_seq=2 ttl=64 time=0.503 ms\n   ^C\n   --- ixregistry.example.com ping statistics ---\n   2 packets transmitted, 2 received, 0% packet loss, time 1865ms\n   rtt min/avg/max/mdev = 0.503/1.246/1.990/0.744 ms\n. yep - working now it seems\n. confirmed - works perfectly when I remove the volume in the run command. \n. Just to be clear - this registry has no persistent storage - it's just for test.  When I kill the container, I assume the images the registry contains disappear, as well as any internal database data.  Am I wrong about that?  I never commit any changes to this container\n. ",
    "djl197": "I tried the curl statement from another machine and I got:\n\"\"docker-registry server (prod) (v0.7.3)\"\nI have configured the webserver by using the suggested apache config here:\nhttp://code.egym.de/2014/05/hosting-private-docker-registry.html?m=1\nExcept I dont enable SSL or LDAP and the virtual host runs on port 80.\nMy /etc/default/docker-registry file looks like:\nexport RUNAS=docker\nexport GUNICORN_WORKERS=8\nexport DOCKER_REGISTRY_HOME=\"/home/docker/docker-registry\"\nexport SETTINGS_FLAVOR=\"prod\"\nexport DOCKER_REGISTRY_CONFIG=\"/home/docker/docker-registry/config.yml\"\nexport STORAGE_PATH=\"/home/docker/docker-registry/${SETTINGS_FLAVOR}\"\nexport SEARCH_BACKEND=\"sqlalchemy\"\nexport LOG_LEVEL=\"debug\"\nI use an init script from the contrib directory.\nI dont even see attempts to access the repository (looking at apache logs) when I do docker push on another machine.\nAny help much appreciated\nDan\n. ",
    "hcguersoy": "Hi,\nI've the same issue here using 0.7.3 in a container, running in a vagrant environment:\ngoofy@host1:~$ docker push dcr1.localdomain:5000/sample:0.1.0 \nThe push refers to a repository [dcr1.localdomain:5000/sample] (len: 1)\nSending image list\n2014/07/09 11:28:55 goofy@host1:~$\nA pull is answered with an 502 error (bad gateway).\ndcr1 is reachable from host1, tested it with wget, responding with \"docker-registry server (dev) (v0.7.3)\". All systems are in the /etc/hosts.\nI'm running Ubuntu 12.04 with Kernel 3.8.0-39-generic, docker 1.1.0, build 79812e3. \nThe only workaround that I've found is to use the IP instead of the hostname. \nE.g. I've to tag with 192.168.56.12:5000/... instead of dcr1.localdomain:5000/...\n. ",
    "Furze": "Hey I have just witnessed this and solved it for me, and was checking issues before creating a issue. \nIf you downgrade gunicorn privileges (-e GUNICORN_USER=www-data) the gunicorn error log contains messages saying:\n\"OperationalError: (OperationalError) attempt to write a readonly database u'INSERT INTO repository (name, description) VALUES (?, ?)' ('library/ubuntu', '')\"\nWhich is not great for a production environment.. \n. Hi @davemackintosh \nLooks like you have two options.\n1) Clone the latest with my changes in giving you access log and error log redirection.\n     export GUNICORN_ACCESS_LOG_FILE=/var/logs/gunicorn.access.log\n     export GUNICORN_ERROR_LOG_FILE=/var/logs/gunicorn.error.log\n2) run the gunicorn command yourself, with something along the lines of:\n%path-to-gunicorn% gunicorn --access-logfile /var/logs/gunicorn.access.log --error-logfile /var/log/gunicorn.error.log --debug --max-requests 100-k gevent --graceful-timeout 3600 -t 3600 -w 4 -b 0.0.0.0:5000 docker_registry.wsgi:application\n. I do agree.\nI found out while looking into migrating my work place's registry that there was no way to change the logs and that it had been hard coded to print to stdout. So my fix was just pushing something we were using into master so that other people could benefit from it. I hope to see GUNICORN_OPTS added back into run command for gunicorn. \nThe two options I gave him do the exact same thing, however the second would be useful if he wants to stay on the revision he is currently using.\nCheers Troy.\n. ",
    "mlnsharma": "@chewmanfoo How did you get around the mcafee firewall problem ? I am also observing \"Error: Status 502 trying to push repository \" issue. Can you please help.. ",
    "activars": "yah, I agree they are the name. But this is more consistent in this example. \n. The test passes, I think my assumption of this bug in config is correct.\n. My prospective is that I believe a lot of people running docker-registry via docker image. e.g:\ndocker run -e REDIS_HOST redisserver.com -e REDIS_PORT 5348 registry:0.7.3\nThe current master will break it as those caching related environment variables will stop working.\nHope this explains my concern.\n. > Put otherwise, it seems correct to me to have these commented (and people enabling it in their config if they need it).\nI agree with you on this. If you see my original PR comment, people was be able to optionally turn caching on via ENV variables. Commented out configuration removes the ability to enable this option by those environment variable (I have to provide my own full config set as replacement). Thus this  breaks the backwards compatibility of running via docker image.\nTake a look the caching logic in python script, it says cache is turned on when cache and cache.host information is provided - part of the swiching on logic is relying on the host is null or a provided value. This YAML caching config shouldn't be commented out and host default should be left as null value. Please read git diffs carefully.\nhttps://github.com/dotcloud/docker-registry/blob/d69590228b03b4d54a871bda2a7557d3298c7528/docker_registry/lib/cache.py#L40\npython\ndef enable_redis_lru(cache, path):\n    if not cache or not cache.host:\n        logger.warn('LRU cache disabled!')\n        return\n    logger.info('Enabling lru cache on Redis')\n    logger.info('Redis lru config: {0}'.format(cache))\n    lru.init(\n        host=cache.host,\n        port=cache.port,\n        db=cache.db,\n        password=cache.password,\n        path=path or '/'\n    )\n. This is my thought: I quite like to see all the default values being presented in this single file. It would be difficult to find them if defaults are specified in each implementation. Alternatively you can write a config module, but I feel this serves the same purpose. With current implementation, you have less code to maintain at this stage.\nor maybe this configuration should be refactored to this format:\nyaml\ncache:\n    enabled: true\n    type: redis\n    # followed by Redis cache options\nwhen an implementation of other cache storage is available:\nyaml\ncache:\n    enabled: true\n    type: memcached\n    # followed by memcached options\ndefault should be disabled:\nyaml\ncache:\n    enabled: false\n    type: redis\n    # followed by all other cache options default setting\n. Thank you for reviewing\n. ",
    "afex": "Setting those environment variables is required for anything to even begin to function, since without them the S3 request times out.  This is expected and is the reason why the proxy is being used.\n. ",
    "sprin": "@dbason \nThanks for having a look! You are right, Base._build_connection_params takes care of the parameter munging. I removed the changes from s3.py. The diff is very minimal now.\nBoto's default calling format is SubdomainCallingFormat, which constructs the path by prepending the bucket to the host. In my case, \"registry.riak-cs.service.consul\". OrdinaryCallingFormat instead builds a path like \"riak-cs.service.consul/registry\". I have not found a nice way (using Consul for service discovery) to handle the subdomain, so I need to specify OrdinaryCallingFormat.\n. I saw that this appears to have been enabled through the use of /etc/boto.cfg, which is now documented in the readme courtesy of 1b1375ad. I think we can close this without merging.\n. Yes, agreed. Having to bind mount lots of configuration files can get messy.\n. @shin- @lorieri \n515 looks good. Closing.\n. ",
    "lorieri": "Hi,\nI will try to put PORT, DEBUG and BOTO_CALLING_FORMAT in the #515 and run some tests, \nlooks like the changes in the driver were not even needed.\ndoes it help ?\n. Hi,\nI did the merge between them\nCheers\nEm 19/09/2014 13:49, \"Joffrey F\" notifications@github.com escreveu:\n\n@lorieri https://github.com/lorieri @sprin https://github.com/sprin\nCan we close this and continue the discussion on #515\nhttps://github.com/docker/docker-registry/pull/515 then? (sorry just\ncatching up on reviews)\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/pull/461#issuecomment-56210769\n.\n. Hi Oliver,\n\nSo sorry, it was my first merge, I didn't know it would do that mess :)\ncould you give me a day ?\nthanks!\n-lorieri\nOn Mon, Sep 8, 2014 at 6:25 PM, Olivier Gambier notifications@github.com\nwrote:\n\nHi there.\nI'm willing to merge this, if you can make this patch cleaner / smaller.\nSpecifically, the bits in Dockerfile are useless - also, you are likely\nbreaking cloudfront signed redirects, etc.\nThanks.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/pull/515#issuecomment-54889779\n.\n. changed region \"ceph\" to \"generic\"\nrebased to 97234d7e368d6d593e9d86ac51fd3398289bf606\n. Hi, it does.\nEm 26/09/2014 00:28, \"Olivier Gambier\" notifications@github.com escreveu:\n@carmstrong https://github.com/carmstrong I'd be happy to merge\nsomething that would cleanly cover both this and #461\nhttps://github.com/docker/docker-registry/pull/461\nIf everybody is happy with (and has tested) this in its current state as a\nsolution, then let's go.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/pull/515#issuecomment-56915400\n.\n. the keys were there before\n\nOn Fri, Sep 26, 2014 at 11:11 AM, Joffrey F notifications@github.com\nwrote:\n\nyeah, once the README comments are addressed, this LGTM\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/pull/515#issuecomment-56966291\n.\n. \n",
    "Aigeruth": "I've updated the PR, Travis is OK now.\n. @dmp42 Yes, I agree, it should not be ignored. But IMHO it is should match the received header. I can modify the pull request to do the header renaming.\n. Sorry, but I don't understand your question fully. Should I open an issue on docker about checking the checksum when the client pulls the image?\n. I've updated the PR.\n. I misunderstood that piece of code, I've updated the PR.\n. I've removed it.\n. \"Changed in version 2.7: The positional argument specifiers can be omitted, so '{} {}' is equivalent to '{0} {1}'.\"\nhttps://docs.python.org/2/library/string.html#formatstrings\n. ",
    "autumnw": "The problem is: centos and rhel 6 are using python 2.6. And upgrade system python from 2.6 to 2.7 is not easy. Upgrade openstack from Python 2.6 to 2.7 (I. E. Using virtual env) is also not easy. I believe in lots of customers are still running on Python 2.6 and it will continue for a while.\nBtw, I had a pull request to fix it, the code looks urgly but works.\nRegards,\nAutumn\nAutumn Wang\n415-589-0380\n\n\u00d4\u00da Jul 9, 2014\u00a3\u00ac2:16 AM\u00a3\u00acOlivier Gambier notifications@github.com \u00d0\u00b4\u00b5\u00c0\u00a3\u00ba\n@autumnw python 2.6 is unsupported / untested. You should stick with python 2.7 - which is why the recommended way to use the registry is from the official docker image.\nAbout the glance issue, I'm closing this ticket in favor of dmp42/docker-registry-driver-glance#6\n\u00a1\u00aa\nReply to this email directly or view it on GitHub.\n. Hi Olivier,\n\nYou mean the official docker container image? Maybe that is a good choice,\nthanks!\nThe reason I did this installation is: I am trying to install openstack\nicehouse with docker. According to the openstack RDO official document,\ndocker registry can be installed easily through RPM package. But\nunfortunately, it is not working: first of all, the docker-registry RPM\npackage of centos 6.5 is 0.7.1, which is missing the glance-drive. Then I\nfound some articles on web site and tried installing and debugging, and\nfound the issue.  For openstack deploying automation perspective, if I use\nthe docker image, I need to figure out the automation of this new piece,\nwhich is different against the existing automation method of my openstack (\ncobbler + chef).\nRegards,\nAutumn Wang.\nOn Wed, Jul 9, 2014 at 7:34 AM, Olivier Gambier notifications@github.com\nwrote:\n\n@autumnw https://github.com/autumnw why not run the official docker\nimage for the registry instead of natively installing it?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/dotcloud/docker-registry/issues/465#issuecomment-48479049\n.\n\n\n[image: --]\n\u6c6a \u5b88\u91d1\n[image: http://]about.me/shoujinwang\nhttp://about.me/shoujinwang\n. Another thing is: for openstack, we need to configure docker-registry to\nuse glance as its backend. Is there any easy way to change the\ndocker-registry configuration inside of that container image?\nRegards,\nAutumn Wang\nOn Wed, Jul 9, 2014 at 10:02 AM, Shoujin Wang shoujinwang@gmail.com wrote:\n\nHi Olivier,\nYou mean the official docker container image? Maybe that is a good choice,\nthanks!\nThe reason I did this installation is: I am trying to install openstack\nicehouse with docker. According to the openstack RDO official document,\ndocker registry can be installed easily through RPM package. But\nunfortunately, it is not working: first of all, the docker-registry RPM\npackage of centos 6.5 is 0.7.1, which is missing the glance-drive. Then I\nfound some articles on web site and tried installing and debugging, and\nfound the issue.  For openstack deploying automation perspective, if I use\nthe docker image, I need to figure out the automation of this new piece,\nwhich is different against the existing automation method of my openstack (\ncobbler + chef).\nRegards,\nAutumn Wang.\nOn Wed, Jul 9, 2014 at 7:34 AM, Olivier Gambier notifications@github.com\nwrote:\n\n@autumnw https://github.com/autumnw why not run the official docker\nimage for the registry instead of natively installing it?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/dotcloud/docker-registry/issues/465#issuecomment-48479049\n.\n\n\n[image: --]\n\u6c6a \u5b88\u91d1\n[image: http://]about.me/shoujinwang\n http://about.me/shoujinwang\n\n\n[image: --]\n\u6c6a \u5b88\u91d1\n[image: http://]about.me/shoujinwang\nhttp://about.me/shoujinwang\n. ",
    "mika": "Yeah, bugsnag>=2.0.1,<2.1 would definitely help!\nIs that something which might be considered for requirements/main.txt as well?\n. Inside Debian packaging it's basically \"python setup.py build\" and \"python setup.py install\" what gets executed (via debian/rules). If this - instead of pip install - works with \"DEPS=loose\" as well (which seems to be the case looking at the commits, but I didn't try yet) this should be working fine. Thanks for working on that! (I'm short of time this week, but maybe I can allocate some time to take a closer look at it next week)\n. ",
    "zeha": "Package maintainers still need metadata from setup.py to build packages for their distribution.\nThe metadata in there can't be too tight, as the packages we'll end up with will depend on stuff that's not available, plus setuptools will run anyway and may decide that it doesn't like the available versions.\nIf you can't have softer dependencies in setup.py, please at least have them in one place, and not scattered around multiple files, so it's easier to patch the dependencies away.\n. ",
    "x1b2j": "Hi,\nThanks, for your response. Here is the info.\n. Version : 0.7.1 (using \"yum info\", but different in daemon debug logs).\nRelease : 2.el6\n\n. I am using docker with openstack, here is my (with 3 changes) \"/etc/docker-registry.yml\" :\n\nstandalone: _env:STANDALONE:true\ndisable_token_auth: _env:DISABLE_TOKEN_AUTH:true\nglance: &glance\n          storage: glance\n          #storage_alternate: _env:GLANCE_STORAGE_ALTERNATE:local\n          storage_path: /var/lib/docker-registry\n    openstack:\n          <<: *glance\n\n. I am using the standard \"service\" command to start \"docker-registry\".\n. My docker client version is \"Docker version 1.0.0, build 63fe64c/1.0.0\".\n. you can check the debug mode logs :\nhttps://gist.github.com/x1b2j/ea4f318f436450294230\nThanks.\n. ",
    "jwerak": "Hi, I have similar issue. \nI am running latest version of registry using following command:\n/usr/bin/docker run --rm --name docker-registry -v /home/core/docker-registry:/registry-conf -v /registry:/registry -e DOCKER_REGISTRY_CONFIG=\"/registry-conf/config.yml\" -e STANDALONE=true -e STORAGE_PATH=/registry -e SETTINGS_FLAVOR=local -e LOGLEVEL=debug -e SEARCH_BACKEND=sqlalchemy -p 5000:5000 registry:0.7.3\nThe registry is listening properly and answers for example on \nhttp://172.17.8.101:5000/v1/search\nDocker client version is 1.0.1 (running coreos beta)\nI didn't make any changes to config.yml, using local flavor\nWhen I try to push image I get no error on client side\ndocker push 172.17.8.101:5000/mesos\nThe push refers to a repository [172.17.8.101:5000/mesos] (len: 1)\nSending image list\n2014/07/31 14:48:53\nregistry image gives me following error:\n172.17.8.101 - - [31/Jul/2014:14:48:53] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-07-31 14:48:53,454 INFO: 172.17.8.101 - - [31/Jul/2014:14:48:53] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n172.17.8.101 - - [31/Jul/2014:14:48:53] \"PUT /v1/repositories/mesos/ HTTP/1.1\" 405 178 \"-\" \"docker/1.0.1 go/go1.2 git-commit/990021a kernel/3.15.2+ os/linux arch/amd64\"\n2014-07-31 14:48:53,461 INFO: 172.17.8.101 - - [31/Jul/2014:14:48:53] \"PUT /v1/repositories/mesos/ HTTP/1.1\" 405 178 \"-\" \"docker/1.0.1 go/go1.2 git-commit/990021a kernel/3.15.2+ os/linux arch/amd64\"\nDocker log gives me:\njournalctl -f _SYSTEMD_UNIT=docker.service\nJul 31 14:54:08 core-01 docker[4766]: 2014/07/31 14:54:08 POST /images/172.17.8.101:5000/mesos/push?tag=\nJul 31 14:54:08 core-01 docker[4766]: [180c68fe] +job push(172.17.8.101:5000/mesos)\nJul 31 14:54:10 core-01 docker[4766]: Error: Status 405 trying to push repository mesos: <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\nJul 31 14:54:10 core-01 docker[4766]: <title>405 Method Not Allowed</title>\nJul 31 14:54:10 core-01 docker[4766]: <h1>Method Not Allowed</h1>\nJul 31 14:54:10 core-01 docker[4766]: <p>The method is not allowed for the requested URL.</p>\nJul 31 14:54:10 core-01 docker[4766]: [180c68fe] -job push(172.17.8.101:5000/mesos) = ERR (1)\nI hope I just have some misconfiguration issue...\nEdit\nI think this is related/replica of this issue https://github.com/docker/docker-registry/issues/432\nIssue is resolved but some claims issue is still there and setting STANDALONE=true won't solve it. \n. I've tried to clone latest version from git and build image. This solved my problem.\n. No problem @dmp42 I've kept STANDALONE=true just because I was testing behavior with older versions. \nThis led me to successful push/pull to private registry: \ngit clone https://github.com/docker/docker-registry.git\ncp config/config_sample.yml config.yml \ndocker build -t jveverka/registry .\n/usr/bin/docker run --rm --name docker-registry -v /home/core/docker-registry:/registry-conf -v /registry:/registry -e DOCKER_REGISTRY_CONFIG=\"/registry-conf/config.yml\" -e STANDALONE=true -e STORAGE_PATH=/registry -e SETTINGS_FLAVOR=local -e LOGLEVEL=debug -e SEARCH_BACKEND=sqlalchemy -p 5000:5000 jveverka/registry\nI am about to write systemd unit file for cores so I will attach it here once its tested. \n. Nope, I am not. I am probably writing to wrong issue... \n@dmp42 Do you know how quickly are changes propagated to public docker registry from master?\n. ",
    "davemackintosh": "Thanks for the update all, I'll have a gander properly in the next few days.\n. ",
    "Davidrjx": "@Furze in docker/distribution registry 2.6.2, is your first solution valid? did you rebuild registry image with gunicorn?  . ",
    "fs-build-system": "So I tried using a different database (postgres), but I get a startup error:\n2014-07-18 23:34:25,052 INFO: Boto based storage initialized\n** [Bugsnag] No API key configured, couldn't notify\nTraceback (most recent call last):\n  File \"/usr/local/bin/docker-registry\", line 9, in <module>\n    load_entry_point('docker-registry==0.7.3', 'console_scripts', 'docker-registry')()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 351, in load_entry_point\n    return get_distribution(dist).load_entry_point(group, name)\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2363, in load_entry_point\n    return ep.load()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2088, in load\n    entry = __import__(self.module_name, globals(),globals(), ['__name__'])\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/run.py\", line 20, in <module>\n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 17, in <module>\n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/__init__.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 57, in __init__\n    self._engine = sqlalchemy.create_engine(database)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/__init__.py\", line 344, in create_engine\n    return strategy.create(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/strategies.py\", line 73, in create\n    dbapi = dialect_cls.dbapi(**dbapi_args)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/dialects/postgresql/psycopg2.py\", line 401, in dbapi\n    import psycopg2\nImportError: No module named psycopg2\nI installed the psycopg2 module on my host, but still got the same error. Then I remembered that I'm running the docker-registry inside a Docker container. So although psycopg2 is installed on my host, it's not installed in the Docker container.\nSo I need to get that psycopg2 module inside the container in order to be able to use a postgres database on the registry. Is there any way I can do that?\n. ",
    "tommyblue": "@dmp42 it works! thanks.\nSorry but the config options seems to me poorly documented, where I can find section about the standalone options and its link with the users endpoint?\n. Thanks @dmp42 ! :)\n. ",
    "ykxpb": "```\ncurl http://localhost:5000\ncurl: (7) Failed to connect to localhost port 5000: Connection refused\ndig docker-registry.xxx.com\n; <<>> DiG 9.9.5-3-Ubuntu <<>> docker-registry.xx.com\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: FORMERR, id: 64277\n;; flags: qr ra; QUERY: 0, ANSWER: 0, AUTHORITY: 0, ADDITIONAL: 0\n;; WARNING: EDNS query returned status FORMERR - retry with '+noedns'\n;; Query time: 4 msec\n;; SERVER: 10.0.2.3#53(10.0.2.3)\n;; WHEN: Mon Jul 28 02:17:24 UTC 2014\n;; MSG SIZE  rcvd: 12\n```\n. After turn on the chunked_transfer_encoding configure in nginx\uff0cpushing works.\nnginx config\n. ",
    "mfojtik": ":+1: \n. ",
    "smarterclayton": "Are you asking why the listeners are configurable (so folks can inject customer listeners via registry conf) or why there is a custom listener class instead of just directly hooking up to signals?\n. The concrete example is so we can define a listener that makes rest calls when things happen in the registry as simple glue (tag created, make a API call to foo to notify).  Makes some simple registry tie ins easy.  We're doing it with Kubernetes/OpenShift so you can push to a registry and trigger a deploy without having to use filesystem watches.\nThe listener is binding directly to the signal, but I created an example to demonstrate the concept.  It's probably possible to tie the named config class closer to the signal handler (what BaseListener is doing) in the registration, if that would reduce the code impact.\n. Here's the prototype listener the pull was created around (notifying external system of new tag)\n. That's certainly simpler - will revise.\n. Andy is happy so I'm happy.\n. Regarding:\n\nPUT link layer into image\n\nis this to mutate an image into a new image?  I.e. given image A, PUT link B -> Image B with new signature?  While useful for simple clients, it also makes the registry a bit more complex to implement - might there be an advantage in only having GET/PUT images, GET layer, GET tags?\n. Marking for preservation (or whatever you call it) is more composable than having registry collection be under the docker registry.  The idea of the registry defining rules for collection and quota, but the user being able to tag and mark as well as to pull by id, seems like it would allow the registry to function as a general purpose store for images for both humans and systems.\nInvoking mark on an image is a request by a user to preserve that image.  I guess one downside is the next question is whether you need ref counting.  I'm assuming that mark is the image owners choice, and that higher level systems that are managing images are really the image owners anyway, so they can implement their own ref counting.\n. ",
    "daviddyball": "Odd, somethings seriously broken here... now I can't even authenticate properly. Getting 404 on the /v1/users/ URI.\n. No, that was to get around a completely different issue I was seeing on 0.7.3\n. OK, so I did this just now:\n```\ndocker pull registry:0.7.3\ndocker run -d -e SETTINGS_FLAVOR=s3          \\\n                 -e AWS_BUCKET=my-docker             \\\n                 -e STORAGE_PATH=/images        \\\n                 -e STORAGE_REDIRECT=true\n                 -e SEARCH_BACKEND=sqlalchemy \\\n                 -e STANDALONE=true \\\n                 --name registry registry:0.7.3\nUpdate codebase and restart gunicorn\nnsenter -t $(docker inspect --format '{{.State.Pid}}' registry-0.7.3) -m -u -n -i -p -w\n\napt-get install -y git\ncd /docker-registry\ngit checkout master\ngit pull master\nkill -HUP 1\nexit\n```\n\nNow try loging in\ndocker login https://my-registry/v1/\nUsername: docker\nPassword:\nEmail: docker@my-domain.com\n2014/07/28 11:17:17 Error response from daemon:\nHere's the logs from the registry\n172.17.42.1 - - [28/Jul/2014:11:17:17] \"GET /v1/_ping HTTP/1.0\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-07-28 11:17:17,934 INFO: 172.17.42.1 - - [28/Jul/2014:11:17:17] \"GET /v1/_ping HTTP/1.0\" 200 4 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [28/Jul/2014:11:17:17] \"POST /v1/users/ HTTP/1.0\" 404 233 \"-\" \"Go 1.1 package http\"\n2014-07-28 11:17:17,961 INFO: 172.17.42.1 - - [28/Jul/2014:11:17:17] \"POST /v1/users/ HTTP/1.0\" 404 233 \"-\" \"Go 1.1 package http\"\nEDIT: wget output as well\nwget -S --http-user docker --http-password password https://my-registry/v1/users/\n  HTTP/1.1 404 NOT FOUND\n  Server: nginx/1.4.6 (Ubuntu)\n  Date: Mon, 28 Jul 2014 11:26:28 GMT\n  Content-Type: text/html\n  Content-Length: 233\n  Connection: keep-alive\n  X-Docker-Registry-Version: 0.7.3\n  X-Docker-Registry-Config: s3\n2014-07-28 11:24:49 ERROR 404: NOT FOUND.\n. Tried without the redundant override to standalone in the environment, but same issue. 404 on login\nHere's the launch command without the STANDALONE environmen variable and also without nginx proxying:\ndocker run -d -e SETTINGS_FLAVOR=s3 \\\n            -e AWS_BUCKET=my-docker \\\n             -e STORAGE_PATH=/images \\\n             -e STORAGE_REDIRECT=true \\\n            -e SEARCH_BACKEND=sqlalchemy \\\n             -e STORAGE_REDIRECT=true \\\n             -p 80:5000 registry:latest\nHere is a dumped version of process 1's environment after launch.\nHOSTNAME=8353903a0217\nHOME=/\nSEARCH_BACKEND=sqlalchemy\nSETTINGS_FLAVOR=s3\nSTORAGE_REDIRECT=true\nDOCKER_REGISTRY_CONFIG=/docker-registry/config/config_sample.yml\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\nAWS_BUCKET=my-docker\nPWD=/STORAGE_PATH=/images\n. Happy to continue debugging this with you, but for production I've had to drop storage redirection and just go with vanilla S3 on 0.7.3 (which is working behind nginx with the above posted config).\n. ",
    "cc272309126": "yes\uff0ci know there is some api\u3002but sometimes i want list the namespaces , repository and tags  in my private registry. like this api  \"/v1/repositories/foo/bar/tags/\", this api could list tags for imges, but Is there a api for list the reposity, like \"/v1/repositories/foo/\". and the more requirs for list the namespace like \"/v1/repositories/\".  and  cat the private registry  integrate with \"docker search \"?\n. thanks very much, this is what I want. I have set the search_backend null.\n. ",
    "streamnsight": "was trying to delete an Image with API but can't figure out how this is supposed to work:\nCan't seem to find the way to get the digest for an image...\nsee \nhttp://stackoverflow.com/questions/38795240/delete-image-from-docker-registry-v2\nAny idea how to do this?\n. layout in docker registry v2 has changed... this doesn't work anymore.\nHas there been improvement in the registry v2 to support this feature now?\n. ",
    "johnae": "I'm using Docker 1.1.2 by the way.\n. No they don't. They do build on images from the docker registry from Docker Inc.\n. @dmp42 Also I've tried running this with local storage, same error. Tried to simply pull the busybox image, tagging it and then pushing that - same error.\n. @dmp42 Sure. I removed all refs to S3 since I get the same error running it using local storage:\ndocker run --name registry -p 172.17.42.1:5000:5000 registry:latest\ndocker pull busybox\ndocker tag busybox:ubuntu-14.04 172.17.42.1:5000/busybox:ubuntu-14.04\ndocker push 172.17.42.1:5000/busybox:ubuntu-14.04\nThe push refers to a repository 172.17.42.1:5000/busybox\nSending image list\nPushing repository 172.17.42.1:5000/busybox (1 tags)\n511136ea3c5a: Pushing \n2014/08/01 15:37:11 HTTP code 400 while uploading metadata: {\"error\": \"Missing key `id' in JSON\"}\nFrom the registry I get this output:\n172.17.42.1 - - [01/Aug/2014:13:37:11] \"PUT /v1/repositories/busybox/ HTTP/1.1\" 200 2 \"-\" \"docker/1.1.2 go/go1.2.1 git-commit/d84a070 kernel/3.13.0-30-generic os/linux arch/amd64\"\n2014-08-01 13:37:11,634 INFO: 172.17.42.1 - - [01/Aug/2014:13:37:11] \"PUT /v1/repositories/busybox/ HTTP/1.1\" 200 2 \"-\" \"docker/1.1.2 go/go1.2.1 git-commit/d84a070 kernel/3.13.0-30-generic os/linux arch/amd64\"\n2014-08-01 13:37:11,635 DEBUG: args = {'image_id': u'511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158'}\n2014-08-01 13:37:11,636 DEBUG: api_error: Image not found\n172.17.42.1 - - [01/Aug/2014:13:37:11] \"GET /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1\" 404 28 \"-\" \"docker/1.1.2 go/go1.2.1 git-commit/d84a070 kernel/3.13.0-30-generic os/linux arch/amd64\"\n2014-08-01 13:37:11,637 INFO: 172.17.42.1 - - [01/Aug/2014:13:37:11] \"GET /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1\" 404 28 \"-\" \"docker/1.1.2 go/go1.2.1 git-commit/d84a070 kernel/3.13.0-30-generic os/linux arch/amd64\"\n2014-08-01 13:37:11,640 DEBUG: args = {'image_id': u'511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158'}\n2014-08-01 13:37:11,641 DEBUG: api_error: Missing key `id' in JSON\n172.17.42.1 - - [01/Aug/2014:13:37:11] \"PUT /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1\" 400 37 \"-\" \"docker/1.1.2 go/go1.2.1 git-commit/d84a070 kernel/3.13.0-30-generic os/linux arch/amd64\"\n2014-08-01 13:37:11,642 INFO: 172.17.42.1 - - [01/Aug/2014:13:37:11] \"PUT /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1\" 400 37 \"-\" \"docker/1.1.2 go/go1.2.1 git-commit/d84a070 kernel/3.13.0-30-generic os/linux arch/amd64\"\n. @dmp42 \ncat /var/lib/docker/graph/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json \n{\"Architecture\":\"x86_64\",\"Author\":\"\",\"Comment\":\"Imported from -\",\"Config\":null,\"Container\":\"\",\"ContainerConfig\":{\"AttachStderr\":false,\"AttachStdin\":false,\"AttachStdout\":false,\"Cmd\":null,\"CpuShares\":0,\"Cpuset\":\"\",\"Domainname\":\"\",\"Entrypoint\":null,\"Env\":null,\"ExposedPorts\":null,\"Hostname\":\"\",\"Image\":\"\",\"Memory\":0,\"MemorySwap\":0,\"NetworkDisabled\":false,\"OnBuild\":null,\"OpenStdin\":false,\"PortSpecs\":null,\"StdinOnce\":false,\"Tty\":false,\"User\":\"\",\"Volumes\":null,\"WorkingDir\":\"\"},\"Created\":\"2013-06-13T14:03:50.821769-07:00\",\"DockerVersion\":\"0.4.0\",\"Id\":\"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\"Os\":\"\",\"Parent\":\"\",\"Size\":0}\n. @dmp42 Thanks for your help!\nI don't think it was actually the pull of busybox. I've never run any docker version older than 1.0 on this box, I have however imported a few images from tar balls so I suspect it came from that. Those were possibly created on an older docker version.\nI went ahead and removed the docker installation completely and I no longer really need to import the tar balls since I've rebuilt those images on the latest docker version.\nThanks again.\n. Yeah, actually I'm closing it myself now.\n. ",
    "sheldonh": "I don't understand why this happens in a registry container when name resolution works just fine on the host. Problem manifests even with a docker-registry-0.8.1 container under docker-1.3.0.\n. Oh, nevermind. Inside the docker-registry container, I can't ping registry-1.docker.io, so it's just a docker configuration issue on the host, must be.\n. Log here: https://gist.github.com/sheldonh/4c3bf1b77691110a6274\n. cough I deleted all the images off the test client and docker-registry behaved as expected. Sorry for the trouble. I don't know if @fantapop was making the same mistake, though.\nSo happy now. Thank you. :-)\n. ",
    "silarsis": "Quick question - I know this is closed (and in production), but new relic supports calling newrelic.agent.initialize() without any arguments and leaning on (NEW_RELIC_CONFIG_FILE and NEW_RELIC_EVIRONMENT) or NEW_RELIC_LICENSE_KEY. Is there a reason that docker-registry uses different environment variable names to achieve the same thing?\nIf the standard names were used (ie. just call initialize() without args if any of the three env variables existed), it would allow us to pass in a license key only without having to bake a new version of the registry that has an extra config file...\nhttps://docs.newrelic.com/docs/agents/python-agent/customization-extension/python-management-api#initialize\n. The second commit only exists because the first failed due to >50 char commit message - the update to -w and --max-requests just make the docco match the actual use a bit more closely (and sorry about the whitespace, I'll have words with my editor).\nI can fix all the above, but newbie question - do I submit this as a new PR with a single commit, or can I cleanly adjust what's been pushed to be a single commit (and if so, how)?\nThanks :)\n. Ok, believe that's done now - let me know if there's any other alterations needed.\n. I actually set the workers to 10 based on observation of running 0.7.3 out of the box - but it appears that's been changed between 0.7.3 and now, so my bad :) I've adjusted in my commit.\n. Fixed.\n. We've been running master in production since the 6th October (haven't refreshed it since then). #619 was the only thing that came up - otherwise it's been working fine.\n. Heya,\nAny movement on stamping 0.9?\n. Sorry, I deleted the offending files to get things working again (via the\nAWS web console, which showed the files fine). I will try replicate the\nconditions again this morning.\nOn Fri, Oct 10, 2014 at 8:23 AM, Olivier Gambier notifications@github.com\nwrote:\n\n@silarsis https://github.com/silarsis can you s3cmd ls\ns3://BUCKET/repositories/USER/IMAGENAME/\nThanks!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/619#issuecomment-58579994\n.\n. s3cmd seems to want access credentials that I don't have - the registry is using an instance policy and the s3cmd seems to want keys etc. So I'm not sure I can test in the way you've requested. I can give you the following demonstration of the problem (registry name not real):\n\npuck:~ silarsis$ docker tag dev my.registry.com.au/kevinl/dev:$\\{test\\}\npuck:~ silarsis$ docker images | grep dev\ndev                                                                            latest              108d6a247018        8 minutes ago       882.1 MB\nmy.registry.com.au/kevinl/dev                          latest              108d6a247018        8 minutes ago       882.1 MB\nmy.registry.com.au/kevinl/dev                          ${test}             108d6a247018        8 minutes ago       882.1 MB\npuck:~ silarsis$ docker push my.registry.com.au/kevinl/dev:$\\{test\\}\nThe push refers to a repository [my.registry.com.au/kevinl/dev] (len: 1)\nSending image list\nPushing repository my.registry.com.au/kevinl/dev (1 tags)\n...\n108d6a247018: Image successfully pushed\nPushing tag for rev [108d6a247018] on {https://my.registry.com.au/v1/repositories/kevinl/dev/tags/${test}}\npuck:~ silarsis$ docker pull my.registry.com.au/kevinl/dev\nPulling repository my.registry.com.au/kevinl/dev\n2014/10/13 09:23:36 Repository not found\n. The referenced PR (#772) above should fix it, from looks, but hasn't been released. In the meantime, we got around it by simply disabling search via config (don't provide a -e SEARCH_BACKEND) - if no-one's using the API on your registry for search, that may be sufficient for you.\n. ",
    "wyaeld": "Extremely interested in ths\n. ",
    "carmstrong": "What is the status of this? We need this for Deis. I am happy to take over this PR and do any work to take it over the finish line. ping @dmp42\n. I can confirm that these are the settings necessary for Ceph, and should work. \n. > the keys were there before\nHe's right - the examples use the same keys as the examples directly before them (which already existed) - these are just adding the new boto settings.\n. I squashed this in #599\n. @dmp42 Gotcha. I want to test a few things first - seeing some issues locally, and want to ensure I don't need a follow-up PR.\n. @dmp42 Cleaned up the README as per your request. I also added a separate config section for ceph-s3, since I figured that would be easier for people to use.\n. ",
    "rvrignaud": "@wking Thank you for the informations. It was a problem on swift-driver (see https://github.com/bacongobbler/docker-registry-driver-swift/issues/14).\nClosing\n. ",
    "envygeeks": "And how do we go about obtaining a token because our builders are struggling to delete old tags and keep our account clean but I can't see anywhere on my settings page how to even get a token on both private and public registries.\n. That's exactly what I needed so we could automate our tag removals, thanks @bacongobbler \n. :+1: \n. ",
    "mconigliaro": "Before anyone else wastes half a day trying to figure out how to delete tags from Docker Hub using the API, here's what they told me:\n\"The documentation for deleting tags are only supposed to be used for private registries. DockerHub, on the other hand, does not allow these API calls to be made. Even, if you get the token, they have weird effects on images. We will very soon expose the APIs with V2 registry. For now, you can only use them to list the tags. DELETE and PUT are not being exposed yet.\"\nI ended up having to scrape the web UI. :smile: \n. ",
    "michaelheyvaert": "I just encountered the same issue with current head (388d30366f279470d50bec7895238a71973d72c1).\nBackend is an s3 compatible backend, no caching configured.\nIt looks like a race condition, I have to try multiple times to get the repository started without the uniqueness error (I have seen both the version and the repository insert statements fail).\nPlease let me know if I can help by providing more information.\n. I think that the search engine is disabled by default, I enabled it explicitly.\nWould it be possible to move the database initialisation out of the index constructor\nand run it in an initialization script before starting the gunicorn workers?\n. The fix works, but I had to patch the run.py file to make the option work: https://github.com/docker/docker-registry/pull/654\nthanks for the help!\n. It works when I wrap the flag in the list.\nI will close the MR, as the change is not needed.., thanks\n. Would you accept a patch that runs the db initialization before starting gunicorn?\n. ",
    "ckulla": "Having the same problem. Are there older versions of the registry which doesn't have this problem? I can confirm that this problem doesn't exist with 0.8.0.\n. ",
    "davidkelley": "Just tried running the latest Docker Registry 0.9.0 and have received the same error:\nIntegrityError: (IntegrityError) UNIQUE constraint failed: repository.name u'INSERT INTO repository (name, description) VALUES (?, ?)' ('system/api', None)\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    __import__(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in <module>\n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in <module>\n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/__init__.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 61, in __init__\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 76, in _setup_database\n    self._generate_index(session=session)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 85, in _generate_index\n    session.commit()\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py\", line 765, in commit\n    self.transaction.commit()\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py\", line 370, in commit\n    self._prepare_impl()\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py\", line 350, in _prepare_impl\n    self.session.flush()\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py\", line 1903, in flush\n    self._flush(objects)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py\", line 2021, in _flush\n    transaction.rollback(_capture_exception=True)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/util/langhelpers.py\", line 57, in __exit__\n    compat.reraise(exc_type, exc_value, exc_tb)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py\", line 1985, in _flush\n    flush_context.execute()\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/unitofwork.py\", line 370, in execute\n    rec.execute(self)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/unitofwork.py\", line 523, in execute\n    uow\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/persistence.py\", line 64, in save_obj\n    mapper, table, insert)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/persistence.py\", line 594, in _emit_insert_statements\n    execute(statement, params)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 720, in execute\n    return meth(self, multiparams, params)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/sql/elements.py\", line 317, in _execute_on_connection\n    return connection._execute_clauseelement(self, multiparams, params)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 817, in _execute_clauseelement\n    compiled_sql, distilled_params\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 947, in _execute_context\n    context)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 1108, in _handle_dbapi_exception\n    exc_info\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/util/compat.py\", line 185, in raise_from_cause\n    reraise(type(exception), exception, tb=exc_tb)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 940, in _execute_context\n    context)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/default.py\", line 435, in do_execute\n    cursor.execute(statement, parameters)\nAn identical stack trace is produced by each worker.\n. ",
    "stevenjack": "@shreyu86 @dmp42 I tried the preload option as per the documentation and got the following error:\nbash\nTraceback (most recent call last):\n  File \"/usr/local/bin/gunicorn\", line 11, in <module>\n    sys.exit(run())\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 74, in run\n    WSGIApplication(\"%(prog)s [OPTIONS] [APP_MODULE]\").run()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 185, in run\n    super(Application, self).run()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 71, in run\n    Arbiter(self).run()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 169, in run\n    self.manage_workers()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 477, in manage_workers\n    self.spawn_workers()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 542, in spawn_workers\n    time.sleep(0.1 * random.random())\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 209, in handle_chld\n    self.reap_workers()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 459, in reap_workers\n    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\ngunicorn.errors.HaltServer: <HaltServer 'Worker failed to boot.' 3>\n2014-11-10 15:42:47 [15] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    __import__(module)\nImportError: No module named [\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    __import__(module)\nImportError: No module named [\nseems like it doesn't like the bracket in docker run -e GUNICORN_OPTS='[--preload]' -p 5000:5000 registry... anyone else having this issue?\n. @shreyu86 that fixed it, thanks! Be worth updating the Readme for this, shall I stick a pull request in?\n. @dmp42 As mentioned in https://github.com/docker/docker-registry/issues/518\n. ",
    "qxo": "At #223,I found @shepmaster and @bjaglin scripts solve my problem:)\nhttps://gist.github.com/bjaglin/1ff66c20c4bc4d9de522\nhttps://gist.github.com/shepmaster/53939af82a51e3aa0cd6\nand improve the scripts:\nhttps://gist.github.com/qxo/db0c31a67511625610f6\n. ",
    "alrayyes": "Fixed it by upgrading docker-registry-core\n. ",
    "devilankur18": "Thanks @bacongobbler, it worked. \n. ",
    "danielkraaij": "You would need some extra tooling to build the docker images. The private registry doesn't facilitate in this.\nWe use Jenkins for building docker images. We pull from Bitbucket (in our case) and push the images when successfully build to our private registry. \n. ",
    "stephaneeybert": "The web hook in github.com triggers the Jenkins build, which calls the build of which image ? Does your Jenkins build do a re build of all your images whatever the impacted branch in the hierarchy ?. ",
    "lrodri29": "@danielkraaij  Do you have open source instructions on how you do that? I am actually looking to do the exact same thing.. ",
    "zedtux": "What I mean is to be able to do then:\ndocker push docker@docker.zedroot.org:zedtux/my-docker-image\nThis would then push to my private server as I would have imported my SSH key in the docker registry (updating then the .ssh/authorised file).\n. @dmp42 yes that is exactly the idea, to add another transport protocol to docker.\nI have opened the ticket in this repo as I guess this repo will need to implement the SSH key management, isn't it ?\nAnyway, I'm going to open another issue on the docker project itself mentioning this issue.\nThank you in any cases.\n. Done: docker issue #7650\n. Is there any workaround until the final feature is implemented ?\n. Thank you @dmp42 \n. I have checked the document action and it seems that the receiver method doesn't receive information like the namespace, image name and tag name. Am I right ?\n. Thank you. I have only checked the link you gave me where there isn't any parameters. Your second link looks much better :smiley: \n. Please have a look here : http://stackoverflow.com/a/24272203/1996540\n. ",
    "vitalyisaev2": "I guess I have just faced with a need of the same feature. +1. \n. ",
    "ph-One": ":+1: \nHere's what I've done in the mean time...\n```\nServer 1: Registry Server\ndocker run -d -p 127.0.0.1:5000:5000 registry:2\n```\n```\nServer 2: SSH Auth Server\nssh -f -N -L 5000:localhost:5000 user@registry_server_address -g\n```\n```\nClient: Test, should fail\ncurl http://registry_server_address:5000/v2/_catalog\n```\n```\nClient: Test, should pass\ncurl http://ssh_auth_server_address:5000/v2/_catalog\n```\nAnd now only those with SSH pub keys on Server 2 have access to the registry. Make sure to add --insecure-registry ssh_auth_server:5000 to your client's Docker daemon command.\n. ",
    "errordeveloper": "+1\n. ",
    "ra2637": "really need it +1\n. ",
    "matutter": "I agree, docker is becoming as essential to development as git.\n. ",
    "cypof": "+1. ",
    "antonin42": "+1. ",
    "sizeoftank": "+1. ",
    "xanview": "+1. ",
    "EgorOmelyanenko": "+1. ",
    "boeboe": "+1. D* is dead soon anyway :)\nhttps://www.youtube.com/watch?v=BeRr3aZbzqo&t=1940s. ",
    "aplut": "+1. ",
    "varadgunjal": "+1. ",
    "dvolosnykh": "+1. ",
    "av8ramit": "+1. ",
    "cfriedt": "+1. /me hears crickets chirping... ",
    "jgh-": "+1. ",
    "bttscut": "+1. ",
    "Sadykh": "+1. ",
    "Natril": "+1. ",
    "philipianni": "+1. ",
    "SukiCZ": "+1. ",
    "3pns": "+1. ",
    "bwail": "+1. ",
    "temporafugiunt": "+1. ",
    "jessemyers": "How about a flag to docker push that allows 409 errors to be ignored?\n. Can you elaborate? I'm probably missing something, but I'd think that if two pushes of the same image occur simultaneously, your choices are to either redefine the PUT response on a duplicate so that it is not a 409 or change the client so that it doesn't consider the 409 an error.\nIs there a deeper, root problem?\n. That's fair. I'd argue that the current behavior is not correct -- it causes related, but parallel builds to break. I'd also argue that it would be unfortunate to wait for a v2 for a fix.\n. ",
    "Henkis": "A little more from the log:\nWed, 27 Aug 2014 06:03:14 GMT\n/docker-registry/prod/images/6e66087f3ffe002664507d225d07b6929843c3f0299f5335a70c1727c8833737/layer\n2014-08-27 06:03:14,189 DEBUG: Signature:\nAWS AKIAIXQJ3M45M46VAQ2A:7gRFO3tyNcihZCHvmLPGn9WWVFI=\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/greenlet.py\", line 327, in run\n    result = self._run(*self.args, **self.kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/boto.py\", line 76, in _fetch_part\n    boto_key.get_contents_to_file(f, headers={'Range': brange})\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/key.py\", line 1603, in get_contents_to_file\n    response_headers=response_headers)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/key.py\", line 1435, in get_file\n    query_args=None)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/key.py\", line 1488, in _get_file_internal\n    for bytes in self:\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/key.py\", line 368, in next\n    data = self.resp.read(self.BufferSize)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/connection.py\", line 416, in read\n    return httplib.HTTPResponse.read(self, amt)\n  File \"/usr/lib/python2.7/httplib.py\", line 567, in read\n    s = self.fp.read(amt)\n  File \"/usr/lib/python2.7/socket.py\", line 380, in read\n    data = self._sock.recv(left)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 392, in recv\n    self._wait(self._read_event)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 298, in _wait\n    self.hub.wait(watcher)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/hub.py\", line 341, in wait\n    result = waiter.get()\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/hub.py\", line 568, in get\n    return self.hub.switch()\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/hub.py\", line 331, in switch\n    return greenlet.switch(self)\ntimeout: timed out\n<Greenlet at 0x7f8bbb7269b0: <bound method ParallelKey._fetch_part of <docker_registry.core.boto.ParallelKey object at 0x7f8bbb2be550>>('/tmp/tmpPPltfO', 1, 13495293, 26990585)> failed with timeout\n. After a few more pulls i stops responding and nothing is written to the container log.\nClient:\n2014/08/27 15:53:22 Error pulling image (97a9cc4076fd69d8a35be028a74f0e00b8066555) from 256.31.14.183/server, Failed to download json: Get http://256.31.14.183/v1/images/31f7549bbb3e5ffba93ed7a37ae6cc86fafe92cfd343eca00a788a39c1e57023/json: read tcp 256.31.14.183:80: i/o timeout\nroot@ip-172-31-7-6:~# ./test.sh\n2014/08/27 16:07:14 Error: Invalid Registry endpoint: Get http://256.31.14.183/v1/_ping: read tcp 256.31.14.183:80: i/o timeout\n. If it's S3 related I will deploy a boto.cfg with region set and debug enabled, initially a reboot of the registry container helped so I did not blamed S3 connectivity but I'm not entirely sure any longer.\nIt's currently not possible to set S3 region in the registry config, correct?\n. I'we hard wired boot against eu-west-1 but I am still seeing the same issues. I noticed in the log today that worker threads reported problems inside the container:\n2014-09-01 20:27:14 [108] [WARNING] Worker graceful timeout (pid:108)\n2014-09-01 20:27:14,172 WARNING: Worker graceful timeout (pid:108)\n2014-09-01 20:27:15 [108] [INFO] Worker exiting (pid: 108)\nIf it's S3 related, any ides on how to proceed with debug? I'we seen quite a few 404:s in the logs but they are always for an _inprogess file is that in order ?\nAnother error with current debug:\nClient:\nb848adeb3e09: Error downloading dependent layers\n2014/09/01 22:58:37 Error pulling image (fb469e5e3584041a51bdd35175776033a829e6b6) from 172.31.9.215/adserver, flate: corrupt input before offset 13576377\nServer:\n```\nMon, 01 Sep 2014 20:58:25 GMT\n/arkivdigital-docker-registry/prod/images/b747adeb3e09f87206e8e6a9da5c56a126dd0e6475278e79fa3ec38f1366fe6c/layer\n2014-09-01 20:58:25,804 DEBUG: Signature:\nAWS AKIAIXQJ3M45M46VAQ2A:oyWlGXzFPB36R4WVfRw7m+esL1g=\n2014-09-01 20:58:25,822 DEBUG: path=/prod/images/b747adeb3e09f87206e8e6a9da5c56a126dd0e6475278e79fa3ec38f1366fe6c/layer\n2014-09-01 20:58:25,822 DEBUG: auth_path=/arkivdigital-docker-registry/prod/images/b747adeb3e09f87206e8e6a9da5c56a126dd0e6475278e79fa3ec38f1366fe6c/layer\n2014-09-01 20:58:25,822 DEBUG: Method: HEAD\n2014-09-01 20:58:25,822 DEBUG: Path: /prod/images/b747adeb3e09f87206e8e6a9da5c56a126dd0e6475278e79fa3ec38f1366fe6c/layer\n2014-09-01 20:58:25,822 DEBUG: Data:\n2014-09-01 20:58:25,822 DEBUG: Headers: {}\n2014-09-01 20:58:25,822 DEBUG: Host: arkivdigital-docker-registry.s3.amazonaws.com\n2014-09-01 20:58:25,823 DEBUG: Port: 80\n2014-09-01 20:58:25,823 DEBUG: Params: {}\n2014-09-01 20:58:25,823 DEBUG: Token: None\n2014-09-01 20:58:25,823 DEBUG: StringToSign:\nHEAD\nMon, 01 Sep 2014 20:58:25 GMT\n/arkivdigital-docker-registry/prod/images/b747adeb3e09f87206e8e6a9da5c56a126dd0e6475278e79fa3ec38f1366fe6c/layer\n2014-09-01 20:58:25,823 DEBUG: Signature:\nAWS AKIAIXQJ3M45M46VAQ2A:oyWlGXzFPB36R4WVfRw7m+esL1g=\n2014-09-01 20:58:25,834 DEBUG: path=/prod/images/b747adeb3e09f87206e8e6a9da5c56a126dd0e6475278e79fa3ec38f1366fe6c/layer\n2014-09-01 20:58:25,834 DEBUG: auth_path=/arkivdigital-docker-registry/prod/images/b747adeb3e09f87206e8e6a9da5c56a126dd0e6475278e79fa3ec38f1366fe6c/layer\n2014-09-01 20:58:25,834 DEBUG: Method: HEAD\n2014-09-01 20:58:25,834 DEBUG: Path: /prod/images/b747adeb3e09f87206e8e6a9da5c56a126dd0e6475278e79fa3ec38f1366fe6c/layer\n2014-09-01 20:58:25,834 DEBUG: Data:\n2014-09-01 20:58:25,834 DEBUG: Headers: {'Range': 'bytes=13561477-67820546'}\n2014-09-01 20:58:25,834 DEBUG: Host: arkivdigital-docker-registry.s3.amazonaws.com\n2014-09-01 20:58:25,834 DEBUG: Port: 80\n2014-09-01 20:58:25,834 DEBUG: Params: {}\n2014-09-01 20:58:25,835 DEBUG: Token: None\n2014-09-01 20:58:25,835 DEBUG: StringToSign:\nHEAD\nMon, 01 Sep 2014 20:58:25 GMT\n/arkivdigital-docker-registry/prod/images/b747adeb3e09f87206e8e6a9da5c56a126dd0e6475278e79fa3ec38f1366fe6c/layer\n2014-09-01 20:58:25,835 DEBUG: Signature:\nAWS AKIAIXQJ3M45M46VAQ2A:oyWlGXzFPB36R4WVfRw7m+esL1g=\n2014-09-01 20:58:25,854 DEBUG: path=/prod/images/b747adeb3e09f87206e8e6a9da5c56a126dd0e6475278e79fa3ec38f1366fe6c/layer\n2014-09-01 20:58:25,854 DEBUG: auth_path=/arkivdigital-docker-registry/prod/images/b747adeb3e09f87206e8e6a9da5c56a126dd0e6475278e79fa3ec38f1366fe6c/layer\n2014-09-01 20:58:25,854 DEBUG: Method: GET\n2014-09-01 20:58:25,854 DEBUG: Path: /prod/images/b747adeb3e09f87206e8e6a9da5c56a126dd0e6475278e79fa3ec38f1366fe6c/layer\n2014-09-01 20:58:25,855 DEBUG: Data:\n2014-09-01 20:58:25,855 DEBUG: Headers: {}\n2014-09-01 20:58:25,855 DEBUG: Host: arkivdigital-docker-registry.s3.amazonaws.com\n2014-09-01 20:58:25,855 DEBUG: Port: 80\n2014-09-01 20:58:25,855 DEBUG: Params: {}\n2014-09-01 20:58:25,856 DEBUG: Token: None\n2014-09-01 20:58:25,856 DEBUG: StringToSign:\nGET\nMon, 01 Sep 2014 20:58:25 GMT\n/secret-docker-registry/prod/images/b747adeb3e09f87206e8e6a9da5c56a126dd0e6475278e79fa3ec38f1366fe6c/layer\n2014-09-01 20:58:25,856 DEBUG: Signature:\nAWS AKIAIXQJ3M45M46VAQ2A:QaHXpfcR+J4yOoAEKiLTHNqW28Y=\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/greenlet.py\", line 327, in run\n    result = self._run(self.args, *self.kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/boto.py\", line 76, in _fetch_part\n    boto_key.get_contents_to_file(f, headers={'Range': brange})\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/key.py\", line 1603, in get_contents_to_file\n    response_headers=response_headers)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/key.py\", line 1435, in get_file\n    query_args=None)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/key.py\", line 1488, in _get_file_internal\n    for bytes in self:\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/key.py\", line 368, in next\n    data = self.resp.read(self.BufferSize)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/connection.py\", line 416, in read\n    return httplib.HTTPResponse.read(self, amt)\n  File \"/usr/lib/python2.7/httplib.py\", line 567, in read\n    s = self.fp.read(amt)\n  File \"/usr/lib/python2.7/socket.py\", line 380, in read\n    data = self._sock.recv(left)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 392, in recv\n    self._wait(self._read_event)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 298, in _wait\n    self.hub.wait(watcher)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/hub.py\", line 341, in wait\n    result = waiter.get()\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/hub.py\", line 568, in get\n    return self.hub.switch()\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/hub.py\", line 331, in switch\n    return greenlet.switch(self)\ntimeout: timed out\n>('/tmp/tmp36R_mv', 1, 13564110, 27128219)> failed with timeout\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/greenlet.py\", line 327, in run\n    result = self._run(self.args, *self.kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/boto.py\", line 76, in _fetch_part\n    boto_key.get_contents_to_file(f, headers={'Range': brange})\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/key.py\", line 1603, in get_contents_to_file\n    response_headers=response_headers)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/key.py\", line 1435, in get_file\n    query_args=None)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/key.py\", line 1488, in _get_file_internal\n    for bytes in self:\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/key.py\", line 368, in next\n    data = self.resp.read(self.BufferSize)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/connection.py\", line 416, in read\n    return httplib.HTTPResponse.read(self, amt)\n  File \"/usr/lib/python2.7/httplib.py\", line 567, in read\n    s = self.fp.read(amt)\n  File \"/usr/lib/python2.7/socket.py\", line 380, in read\n    data = self._sock.recv(left)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 392, in recv\n    self._wait(self._read_event)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 298, in _wait\n    self.hub.wait(watcher)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/hub.py\", line 341, in wait\n    result = waiter.get()\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/hub.py\", line 568, in get\n    return self.hub.switch()\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/hub.py\", line 331, in switch\n    return greenlet.switch(self)\ntimeout: timed out\n>('/tmp/tmp36R_mv', 4, 54256440, 67820545)> failed with timeout\n```\n. I have tried adding the socker timeout to my boto.cfg, it doesn't help.\nSame problems maybe some more debug output:\n[debug] http.go:162 http://55.67.195.101/v1/images/b026380b9b1536f67fc9db483808d236e2aaa02fe7dbf9db3377dfc830941f6a/json -- HEADERS: map[User-Agent:[docker/1.2.0 go/go1.3.1 git-c94034bf8411f: Download complete\n[debug] image.go:325 Json string: {{\"id\":\"b026380b9b1536f67fc9db483808d236e2aaa02fe7dbf9db3377dfc830941f6a\",\"parent\":\"94034bf8411f38a7a280d612bff163da58ea5b622e7695b6c62edffb001ee1ab\",\"created\":\"2014-08-26T19:40:30.559806204Z\",\"container\":\"9328ff054fc27a5163c1ac44bfcb026380b9b15: Pulling fs layer\nainname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"ExposedPorts\":{\"443/tcp\":{},\"80/tcp\"b026380b9b15: Downloading 40.68 MB/67.82 MB 1s\nb026380b9b15: Error pulling image (59c2ef25615aee308a641a79d79547445b6a2831) from 55.67.195.101/server, flate: corrupt input before offset 40690609 orrupt input before offset 40690609 7] -job pull(55.67.195.101/server, 59c2ef25615aee308a641a79d79547445b6a2831) = ERR (1)le install --gemfile /home/app/webapp/Gemfile --path /home/app/bundle --deploymentb026380b9b15: Error downloading dependent layers\n assets:precompile;    cp config/database.production.yml config/database.yml\"],\"Dns\":null,\"Image\":\"94034bf8411f38a7a280d612bff163da58ea5b622e7695b6c62edffb001ee1ab\",\"Volumes\":null,\"VolumesFrom\":\"\",\"WorkingDir\":\"\",\"Entrypoint\":null,\"NetworkDisabled\":false,\"OnBuild\":[]},\"docker_version\":\"0.9.1\",\"config\":{\"Hostname\":\"95699cec7d3e\",\"Domainname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"ExposedPorts\":{\"443/tcp\":{},\"80/tcp\":{}},\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":[\"HOME=/root\",\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"],\"Cmd\":[\"/sbin/my_init\"],\"Dns\":null,\"Image\":\"94034bf8411f38a7a280d612bff163da58ea5b622e7695b6c62edffb001ee1ab\",\"Volumes\":null,\"VolumesFrom\":\"\",\"WorkingDir\":\"\",\"Entrypoint\":null,\"NetworkDisabled\":false,\"OnBuild\":[]},\"architecture\":\"amd64\",\"os\":\"linux\",\"Size\":69388467}}\n[debug] http.go:162 http://55.67.195.101/v1/images/b026380b9b1536f67fc9db483808d236e2aaa02fe7dbf9db3377dfc830941f6a/layer -- HEADERS: map[User-Agent:[docker/1.2.0 go/go1.3.1 git-commit/fa7b24f kernel/3.13.0-24-generic os/linux arch/amd64]]\n[debug] session.go:188 server supports resume\n[debug] image.go:97 Start untar layer\n[debug] archive.go:88 [tar autodetect] n: [31 139 8 0 0 9 110 136 0 255]\n[info] encountered error during pull and clearing it before resume: unexpected EOF\n2014/09/14 14:09:54 Error pulling image (59c2ef25615aee308a641a79d79547445b6a2831) from 55.67.195.101/server, flate: corrupt input before offset 40690609\n. ",
    "mcadam": "Hi,\nI have tried your solution by forcing gunicorn to 19.1, and I still have the same problem but a different error. First error was just like Henkis :  the greenlet was timing out.\nNow I have that: \nSep 23 10:15:59  docker[23048]: 2014-09-23 10:15:59 [18] [ERROR] Error handling request\nSep 23 10:15:59  docker[23048]: Traceback (most recent call last):\nSep 23 10:15:59  docker[23048]: File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 108, in handle_request\nSep 23 10:15:59  docker[23048]: resp.write(item)\nSep 23 10:15:59  docker[23048]: File \"/usr/local/lib/python2.7/dist-packages/gunicorn/http/wsgi.py\", line 344, in write\nSep 23 10:15:59  docker[23048]: util.write(self.sock, arg, self.chunked)\nSep 23 10:15:59  docker[23048]: File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 301, in write\nSep 23 10:15:59  docker[23048]: sock.sendall(data)\nSep 23 10:15:59  docker[23048]: File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 458, in sendall\nSep 23 10:15:59  docker[23048]: data_sent += self.send(_get_memory(data, data_sent), flags)\nSep 23 10:15:59  docker[23048]: File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 443, in send\nSep 23 10:15:59  docker[23048]: return sock.send(data, flags)\nSep 23 10:15:59  docker[23048]: error: [Errno 104] Connection reset by peer\nSep 23 10:16:10  docker[23048]: 2014-09-23 10:16:10 [19] [ERROR] Error handling request\nSep 23 10:16:10  docker[23048]: Traceback (most recent call last):\nSep 23 10:16:10  docker[23048]: File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 108, in handle_request\nSep 23 10:16:10  docker[23048]: resp.write(item)\nSep 23 10:16:10  docker[23048]: File \"/usr/local/lib/python2.7/dist-packages/gunicorn/http/wsgi.py\", line 344, in write\nSep 23 10:16:10  docker[23048]: util.write(self.sock, arg, self.chunked)\nSep 23 10:16:10  docker[23048]: File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 301, in write\nSep 23 10:16:10  docker[23048]: sock.sendall(data)\nSep 23 10:16:10  docker[23048]: File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 458, in sendall\nSep 23 10:16:10  docker[23048]: data_sent += self.send(_get_memory(data, data_sent), flags)\nSep 23 10:16:10  docker[23048]: File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 443, in send\nSep 23 10:16:10  docker[23048]: return sock.send(data, flags)\nSep 23 10:16:10  docker[23048]: error: [Errno 32] Broken pipe\nor that : \nSep 23 10:35:08  docker[23048]: 2014-09-23 10:35:08 [16] [ERROR] Error handling request\nSep 23 10:35:08  docker[23048]: Traceback (most recent call last):\nSep 23 10:35:08  docker[23048]: File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 108, in handle_request\nSep 23 10:35:08  docker[23048]: resp.write(item)\nSep 23 10:35:08  docker[23048]: File \"/usr/local/lib/python2.7/dist-packages/gunicorn/http/wsgi.py\", line 344, in write\nSep 23 10:35:08  docker[23048]: util.write(self.sock, arg, self.chunked)\nSep 23 10:35:08  docker[23048]: File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 301, in write\nSep 23 10:35:08  docker[23048]: sock.sendall(data)\nSep 23 10:35:08  docker[23048]: File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 458, in sendall\nSep 23 10:35:08  docker[23048]: data_sent += self.send(_get_memory(data, data_sent), flags)\nSep 23 10:35:08  docker[23048]: File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 435, in send\nSep 23 10:35:08  docker[23048]: return sock.send(data, flags)\nSep 23 10:35:08  docker[23048]: error: [Errno 104] Connection reset by peer\n. Right now no nginx, we acces the registry directly on local network on port 5000. After these errors, the docker pull command just fail and thats it after multiple times it works...\n. ",
    "ofrasergreen": "I've been experiencing exactly the same symptoms and it appears the problem, at least in my case, is in fact downloading from S3. I created a tiny boto script to download a 69M layer:\n```\n!/usr/bin/python\nfrom boto.s3.connection import S3Connection, Location\nconn = S3Connection('', '')\nbucket = conn.get_bucket('link-docker')\nkey = bucket.get_key('test/images/d497ad3926c8997e1e0de74cdd5285489bb2c4acd6db15292e04bbab07047cd0/layer')\nkey.get_contents_to_filename(\"test_layer\")\n```\nand found it took 4s to run the first time, then 15s, 23s, 9s and finally after 2m3s:\nTraceback (most recent call last):\n...\n  File \"/usr/lib/python2.7/dist-packages/boto/s3/key.py\", line 1471, in _get_file_internal\n    for bytes in self:\n  File \"/usr/lib/python2.7/dist-packages/boto/s3/key.py\", line 365, in next\n    data = self.resp.read(self.BufferSize)\n  File \"/usr/lib/python2.7/dist-packages/boto/connection.py\", line 415, in read\n    return httplib.HTTPResponse.read(self, amt)\n  File \"/usr/lib/python2.7/httplib.py\", line 567, in read\n    s = self.fp.read(amt)  \n  File \"/usr/lib/python2.7/socket.py\", line 380, in read\n    data = self._sock.recv(left)\n  File \"/usr/lib/python2.7/ssl.py\", line 341, in recv\n    return self.read(buflen)\n  File \"/usr/lib/python2.7/ssl.py\", line 260, in read\n    return self._sslobj.read(len)\nsocket.error: [Errno 104] Connection reset by peer\nPresumably this kind of unreliability with S3 isn't normal so I'm investigating further...\n. ",
    "rodlogic": "Same issue here:\nb1b7444f3af2: Error downloading dependent layers \n2014/10/29 21:09:51 Error pulling image (latest) from myregistry.com:5000/img-myimage, flate: corrupt input before offset 86796209\nRestarting does not solve the problem.\n. ",
    "trenton42": "To add to @garo 's comment (and to save you some digging) you can add -e STORAGE_REDIRECT=true to your docker command if you are running this in a container.\n. ",
    "trinitronx": "I'm seeing errors similar to the ones mentioned in this comment:\n2015-03-27 20:35:14 [685] [ERROR] Error handling request\nTraceback (most recent call last):\n File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 108, in handle_request\n   resp.write(item)\n File \"/usr/local/lib/python2.7/dist-packages/gunicorn/http/wsgi.py\", line 344, in write\n   util.write(self.sock, arg, self.chunked)\n File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 301, in write\n   sock.sendall(data)\n File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 458, in sendall\n   data_sent += self.send(_get_memory(data, data_sent), flags)\n File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 435, in send\n   return sock.send(data, flags)\nerror: [Errno 32] Broken pipe\nI am running docker-registry (registry:latest   Image ID: e33e81d7024c, \"Created\": \"2015-03-20T22:14:39.683558078Z\") behind trinitronx/nginx-proxy (trinitronx/nginx-proxy:latest Image ID: 95cc04d9d18e, \"Created\": \"2014-12-18T23:08:25.909249961Z\").   My nginx-proxy container is simply based off of jwilder/nginx-proxy except I added client_max_body_size 0; to the server { block to avoid silent docker push failures on large image pushes.\nHowever, the only reason that I'm running the nginx-proxy is to handle SSL, and when I run docker-registry with storage_redirect: true it causes SSL validation errors because docker daemon can't validate the SSL cert when it gets 302 redirected to *.amazonaws.net.\nThe error I see in /var/log/docker is:\nError pulling image (version_676) from registry.example.com:1234/example/web_container, Server error: Status 0 wh\nile fetching image layer (235056cffc249f5f3bfa5ba4425231ab2cf813876a43e1bc05737910c41c53f2)\n[2a81fb14] -job pull(registry.example.com:1234/example/web_container, version_123) = ERR (1)\nThis seems to come from one of these lines in docker's source code:\nregistry/session.go\n180:                            return nil, fmt.Errorf(\"Server error: Status %d while fetching image layer (%s)\",\n191:            return nil, fmt.Errorf(\"Server error: Status %d while fetching image layer (%s)\",\nThe reason I think this is an SSL verification issue is because we tested adding --insecure-registry=registry.example.com:1234 to our docker daemon startup command line, and the error went away.\nTo see what was happening with the requests, I manually tried running: curl -v -s http://registry.example.com:1234/v1/images/235056cffc249f5f3bfa5ba4425231ab2cf813876a43e1bc05737910c41c53f2/layer and got a 302 redirect which looked something like this (actual request scrubbed for privacy):\n```\n Hostname was NOT found in DNS cache\n   Trying 127.0.0.1...\n* Connected to localhost (127.0.0.1) port 5000 (#0)\n\nGET /v1/images/235056cffc249f5f3bfa5ba4425231ab2cf813876a43e1bc05737910c41c53f2/layer HTTP/1.1\nUser-Agent: curl/7.36.0\nHost: registry.example.com:1234\nAccept: /\n< HTTP/1.1 302 FOUND\n* Server gunicorn/19.1.1 is not blacklisted\n< Server: gunicorn/19.1.1\n< Date: Thu, 02 Apr 2015 23:17:44 GMT\n< Connection: keep-alive\n< Content-Type: text/html; charset=utf-8\n< Content-Length: 681\n< Location: https://registry.example.com.s3.amazonaws.com/images/235056cffc249f5f3bfa5ba4425231ab2cf813876a43e1bc05737910c41c53f2/layer?Signature=Scrubbed1234VRyRanD0mStr1ng%3D&Expires=1428017864&AWSAccessKeyId=AKIAWSACCESSKEYID\n```\n\nWhen I manually followed the redirect by hand by running curl -v   -s 'https://registry.example.com.s3.amazonaws.com/images/235056cffc249f5f3bfa5ba4425231ab2cf813876a43e1bc05737910c41c53f2/layer?Signature=Scrubbed1234VRyRanD0mStr1ng%3D&Expires=1428017864&AWSAccessKeyId=AKIAWSACCESSKEYID' I saw this:\n* About to connect() to registry.example.com.s3.amazonaws.com port 443 (#0)\n*   Trying 54.123.34.56... connected\n* Connected to registry.example.com.s3.amazonaws.com (54.123.34.56) port 443 (#0)\n* Initializing NSS with certpath: sql:/etc/pki/nssdb\n*   CAfile: /etc/pki/tls/certs/ca-bundle.crt\n  CApath: none\n* SSL: certificate subject name '*.s3.amazonaws.com' does not match target host name 'registry.example.com.s3.amazonaws.com'\n* NSS error -12276\n* Closing connection #0\nSo it's a catch-22.  I need HTTPS security so I can run docker-registry with the STORAGE_REDIRECT=true argument (since it seems to pass Location: headers with AWS temporary S3 keys), but when I run nginx-proxy with HTTPS (and docker daemon without --insecure-registry=registry.example.com:1234, it silently fails due to SSL validation errors.   I've verified this by running curl commands against the registry when storage_redirect is on.\n. I've tracked my SSL issue down to this problem with S3 bucket names.  This seems to have solved the issue with using -e STORAGE_REDIRECT=true, and allows HTTPS access direct to S3.\nTo fix my issue I had to:\n1. Copy S3 bucket to new name without dots . or underscores _ ./bin/aws s3 sync  s3://registry.example.com s3://registry-bucket-name-without-dots-and-underscores\n2. Set -e AWS_BUCKET=registry-bucket-name-without-dots-and-underscores\n3. Restarted docker-registry container\nNow I no longer see the error: [Errno 32] Broken pipe or this error:\nError pulling image (version_676) from registry.example.com:1234/example/web_container, Server error: Status 0 wh\nile fetching image layer (235056cffc249f5f3bfa5ba4425231ab2cf813876a43e1bc05737910c41c53f2)\n[2a81fb14] -job pull(registry.example.com:1234/example/web_container, version_123) = ERR (1)\n. # Oops... I spoke too soon :sob:\nCorrection.... I still get the Broken pipe error for requests that the docker-registry is not giving a 302 response for normally.  That is for things like /images/<id>/ancestry, /images/<id>/json, etc...\ndocker-registry log:\n172.17.0.12 - - [09/Apr/2015:23:32:34 +0000] \"GET /v1/_ping HTTP/1.1\" 200 1603 \"-\" \"Go 1.1 package http\"\n172.17.0.12 - - [09/Apr/2015:23:32:35 +0000] \"GET /v1/_ping HTTP/1.1\" 200 1603 \"-\" \"Go 1.1 package http\"\n172.17.0.12 - - [09/Apr/2015:23:32:35 +0000] \"GET /v1/repositories/example/web_container/images HTTP/1.1\" 200 97356 \"-\" \"docker/1.3.3 go/go1.3.3 git-commit/d344625 kernel/2.6.32-431.el6.x86_64 os/linux arch/amd64\"\n172.17.0.12 - - [09/Apr/2015:23:36:35 +0000] \"GET /v1/_ping HTTP/1.1\" 200 1603 \"-\" \"Go 1.1 package http\"\n172.17.0.12 - - [09/Apr/2015:23:36:35 +0000] \"GET /v1/_ping HTTP/1.1\" 200 1603 \"-\" \"Go 1.1 package http\"\n[2015-04-09 23:36:35 +0000] [65] [ERROR] Error handling request\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 108, in handle_request\n    resp.write(item)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/http/wsgi.py\", line 344, in write\n    util.write(self.sock, arg, self.chunked)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 301, in write\n    sock.sendall(data)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 458, in sendall\n    data_sent += self.send(_get_memory(data, data_sent), flags)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 435, in send\n    return sock.send(data, flags)\nerror: [Errno 32] Broken pipe\n172.17.0.12 - - [09/Apr/2015:23:36:35 +0000] \"GET /v1/repositories/example/web_container/images HTTP/1.1\" 200 97356 \"-\" \"docker/1.3.3 go/go1.3.3 git-commit/d344625 kernel/2.6.32-431.el6.x86_64 os/linux arch/amd64\"\n[2015-04-09 23:36:38 +0000] [65] [ERROR] Error handling request\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 108, in handle_request\n    resp.write(item)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/http/wsgi.py\", line 344, in write\n    util.write(self.sock, arg, self.chunked)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 301, in write\n    sock.sendall(data)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 458, in sendall\n    data_sent += self.send(_get_memory(data, data_sent), flags)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 435, in send\n    return sock.send(data, flags)\nerror: [Errno 32] Broken pipe\n172.17.0.12 - - [09/Apr/2015:23:36:39 +0000] \"GET /v1/repositories/example/web_container/tags HTTP/1.1\" 200 27959 \"-\" \"docker/1.3.3 go/go1.3.3 git-commit/d344625 kernel/2.6.32-431.el6.x86_64 os/linux arch/amd64\"\n172.17.0.12 - - [09/Apr/2015:23:36:39 +0000] \"GET /v1/images/2b0c589adcfd8d65c643179f0cb243e9ba93c5ae80c0c1b4a5a194d6a868e659/ancestry HTTP/1.1\" 200 2176 \"-\" \"docker/1.3.3 go/go1.3.3 git-commit/d344625 kernel/2.6.32-431.el6.x86_64 os/linux arch/amd64\"\nThe failing docker pull command was:\nsudo docker pull registry.example.com:443/example/web_container:version_123\nError it gave was:\n2015/04/09 17:32:18 Error pulling image (version_123) from registry.example.com:443/example/web_container, Get https://registry.example.com:443/v1/images/2b0c589adcfd8d65c643179f0cb243e9ba93c5ae80c0c1b4a5a194d6a868e659/ancestry: read tcp 123.231.123.45:443: i/o timeout\nThe docker daemon log /var/log/docker shows some info, but unfortunately is not logging timestamps for all lines:\n[info] POST /v1.15/images/create?fromImage=registry.example.com%3A443%2Fexample%2Fweb_container%3Aversion_123\n[55949c70] +job pull(registry.example.com:443/example/web_container, version_123)\nError pulling image (version_123) from registry.example.com:443/example/web_container, Get https://registry.example.com:443/v1/images/2b0c589adcfd8d65c643179f0cb243e9ba93c5ae80c0c1b4a5a194d6a868e659/ancestry: read tcp 123.231.123.45:443: i/o timeout\n[55949c70] -job pull(registry.example.com:443/example/web_container, version_123) = ERR (1)\n[info] POST /v1.15/images/create?fromImage=registry.example.com%3A443%2Fexample%2Fweb_container%3Aversion_123\n[55949c70] +job pull(registry.example.com:443/example/web_container, version_123)\n[55949c70] -job execStart(2089a2b3bc9d72b6e6913ab075fa27fe1d79669fbb273eb423f03c84f44288d1) = OK (0)\n2015/04/09 17:32:38 http: response.WriteHeader on hijacked connection\n[info] GET /v1.15/containers/example_web_1/json\n[55949c70] +job container_inspect(example_web_1)\n[55949c70] -job container_inspect(example_web_1) = OK (0)\n[error] pull.go:175 Get https://registry.example.com:443/v1/repositories/example/web_container/tags: read tcp 12.34.56.78:443: i/o timeout\nGet https://registry.example.com:443/v1/repositories/example/web_container/tags: read tcp 12.34.56.78:443: i/o timeout\n[55949c70] -job pull(registry.example.com:443/example/web_container, version_123) = ERR (1)\nThe single line with a timestamp is: 2015/04/09 17:32:38 http: response.WriteHeader on hijacked connection.\nThe nginx-proxy logs show the same requests coming in, with response codes: 200\nnginx.1    | 12.34.56.78 - - [09/Apr/2015:23:36:35 +0000] \"GET /v1/_ping HTTP/1.1\" 200 1603 \"-\" \"Go 1.1 package http\" \"-\"\nnginx.1    | registry.example.com 12.34.56.78 - - [09/Apr/2015:23:36:35 +0000] \"GET /v1/_ping HTTP/1.1\" 200 1603 \"-\" \"Go 1.1 package http\"\nnginx.1    | 12.34.56.78 - - [09/Apr/2015:23:36:35 +0000] \"GET /v1/repositories/example/web_container/images HTTP/1.1\" 200 97356 \"-\" \"docker/1.3.3 go/go1.3.3 git-commit/d344625 kernel/2.6.32-431.el6.x86_64 os/linux arch/amd64\" \"-\"\nnginx.1    | registry.example.com 12.34.56.78 - - [09/Apr/2015:23:36:35 +0000] \"GET /v1/repositories/example/web_container/images HTTP/1.1\" 200 97356 \"-\" \"docker/1.3.3 go/go1.3.3 git-commit/d344625 kernel/2.6.32-431.el6.x86_64 os/linux arch/amd64\"\nnginx.1    | 12.34.56.78 - - [09/Apr/2015:23:36:39 +0000] \"GET /v1/repositories/example/web_container/tags HTTP/1.1\" 200 27959 \"-\" \"docker/1.3.3 go/go1.3.3 git-commit/d344625 kernel/2.6.32-431.el6.x86_64 os/linux arch/amd64\" \"-\"\nnginx.1    | registry.example.com 12.34.56.78 - - [09/Apr/2015:23:36:39 +0000] \"GET /v1/repositories/example/web_container/tags HTTP/1.1\" 200 27959 \"-\" \"docker/1.3.3 go/go1.3.3 git-commit/d344625 kernel/2.6.32-431.el6.x86_64 os/linux arch/amd64\"\nnginx.1    | 12.34.56.78 - - [09/Apr/2015:23:36:39 +0000] \"GET /v1/images/2b0c589adcfd8d65c643179f0cb243e9ba93c5ae80c0c1b4a5a194d6a868e659/ancestry HTTP/1.1\" 200 2176 \"-\" \"docker/1.3.3 go/go1.3.3 git-commit/d344625 kernel/2.6.32-431.el6.x86_64 os/linux arch/amd64\" \"-\"\nnginx.1    | registry.example.com 12.34.56.78 - - [09/Apr/2015:23:36:39 +0000] \"GET /v1/images/2b0c589adcfd8d65c643179f0cb243e9ba93c5ae80c0c1b4a5a194d6a868e659/ancestry HTTP/1.1\" 200 2176 \"-\" \"docker/1.3.3 go/go1.3.3 git-commit/d344625 kernel/2.6.32-431.el6.x86_64 os/linux arch/amd64\"\nI believe this is still an issue somewhere in the docker-registry gunicorn app.\nThe setup is (all IPs, layer ids, and containers anonymized to protect the innocent):\ndocker daemon (12.34.56.78) -->  nginx-proxy (Host IP: 123.231.123.45  container IP: 172.17.0.12) --> docker-registry (container running on same host as nginx-proxy container)\n. Just noticed in my previous logs that the docker daemon (the client accessing docker-registry to pull images) has the timestamp of the error to be: 2015/04/09 17:32:18\nI searched for that time in the logs (17:32:18) and couldn't find it, so I noticed that clocks on the (docker daemon host) and (docker-registry / nginx-proxy) host are set in different time zones.\nI checked the date on the host running docker daemon and the one running the nginx-proxy and docker-registry containers to determine if there was a time offset and prove to myself that the clocks were synchronized correctly via NTP.  This will help us in lining up the timing of events.  Here are the results:\n\"docker daemon\" host: Thu Apr  9 18:57:44 MDT 2015 = 2015-04-09 18:57:44 -0600 = 2015-04-10 00:57:44 UTC\ndocker-registry host: Fri Apr 10 00:57:44 UTC 2015 = 2015-04-10 00:57:44 UTC\nSo both hosts are synchronized to correct time, but are just in different time zones.  One is UTC, one is UTC-6 hours.  So we just add 6 hours to the docker daemon host time:  \n'2015/04/09 17:32:18' => 2015-04-09 17:32:18 -0600 => 2015-04-09 23:32:18 UTC\nSearching nginx-proxy logs for 23:32:18 gives 2 matches:\nnginx.1    | 12.34.56.78 - - [09/Apr/2015:23:32:18 +0000] \"GET /v1/images/2b0c589adcfd8d65c643179f0cb243e9ba93c5ae80c0c1b4a5a194d6a868e659/ancestry HTTP/1.1\" 499 0 \"-\" \"docker/1.3.3 go/go1.3.3 git-commit/d344625 kernel/2.6.32-431.el6.x86_64 os/linux arch/amd64\" \"-\"\nnginx.1    | registry.example.com 12.34.56.78 - - [09/Apr/2015:23:32:18 +0000] \"GET /v1/images/2b0c589adcfd8d65c643179f0cb243e9ba93c5ae80c0c1b4a5a194d6a868e659/ancestry HTTP/1.1\" 499 0 \"-\" \"docker/1.3.3 go/go1.3.3 git-commit/d344625 kernel/2.6.32-431.el6.x86_64 os/linux arch/amd64\"\nSearching docker-registry logs for 23:32:18 gives NO matches!  Something fishy is going on...\nSo nginx is seeing the request from docker daemon for GET /v1/images/2b0c589adcfd8d65c643179f0cb243e9ba93c5ae80c0c1b4a5a194d6a868e659/ancestry.  It then logs HTTP code 499.\nAnd nginx shows HTTP Code 499\nTimeline\nMaking the assumption that Nginx is telling us the truth:  the client closed it's connection before it could hear a response.  Here is what I think is happening:\n1. sudo docker pull registry.example.com:443/example/web_container:version_123 is run\n2. 2015-04-09 23:32:18 UTC:  docker daemon performs the request against nginx-proxy: GET /v1/images/2b0c589adcfd8d65c643179f0cb243e9ba93c5ae80c0c1b4a5a194d6a868e659/ancestry\n3. docker daemon disconnects for some reason with error: Error pulling image (version_123) from registry.example.com:443/example/web_container, Get https://registry.example.com:443/v1/images/2b0c589adcfd8d65c643179f0cb243e9ba93c5ae80c0c1b4a5a194d6a868e659/ancestry: read tcp 123.231.123.45:443: i/o timeout\n4. 2015-04-09 23:32:18 UTC:  nginx-proxy logs the request & HTTP Status 499\n5. 2015-04-09 23:32:38 UTC: docker daemon writes log line: 2015/04/09 17:32:38 http: response.WriteHeader on hijacked connection   (Time for log line was: '2015/04/09 17:32:38' => 2015-04-09 17:32:38 -0600 => 2015-04-09 23:32:38 UTC)\n6. 2015-04-09 23:36:35 UTC & 2015-04-09 23:36:38 UTC: docker-registry logs the Broken pipe errors:\n[2015-04-09 23:36:35 +0000] [65] [ERROR] Error handling request\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 108, in handle_request\n    resp.write(item)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/http/wsgi.py\", line 344, in write\n    util.write(self.sock, arg, self.chunked)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 301, in write\n    sock.sendall(data)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 458, in sendall\n    data_sent += self.send(_get_memory(data, data_sent), flags)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 435, in send\n    return sock.send(data, flags)\nerror: [Errno 32] Broken pipe\n172.17.0.12 - - [09/Apr/2015:23:36:35 +0000] \"GET /v1/repositories/example/web_container/images HTTP/1.1\" 200 97356 \"-\" \"docker/1.3.3 go/go1.3.3 git-commit/d344625 kernel/2.6.32-431.el6.x86_64 os/linux arch/amd64\"\n[2015-04-09 23:36:38 +0000] [65] [ERROR] Error handling request\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/async.py\", line 108, in handle_request\n    resp.write(item)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/http/wsgi.py\", line 344, in write\n    util.write(self.sock, arg, self.chunked)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 301, in write\n    sock.sendall(data)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 458, in sendall\n    data_sent += self.send(_get_memory(data, data_sent), flags)\n  File \"/usr/local/lib/python2.7/dist-packages/gevent/socket.py\", line 435, in send\n    return sock.send(data, flags)\nerror: [Errno 32] Broken pipe\nSo the docker-registry throws the final Broken pipe error at: 2015-04-09 23:36:38 UTC.\nSo from the time that the docker daemon makes the request 2015-04-09 23:32:18 UTC to the time that the error occurs at 2015-04-09 23:36:38 UTC, this much time has elapsed:\n0:04:20   (h:m:s)\n260.00    total seconds\n4.33      total minutes\n0.07      total hours\nPotential Related Gunicorn Issue\nGiven the symptoms, and proposed chain of events... This could potentially related to benoitc/gunicorn#414\nContainer Debug Info\nThe docker-registry container is running:\nroot@docker-registry:/# gunicorn -v\ngunicorn (version 19.1.1)\nAnd it has these python packages installed:\nroot@docker-registry:/# pip list\nargparse (1.2.1)\nbackports.lzma (0.0.3)\nblinker (1.3)\nboto (2.34.0)\nbugsnag (2.0.2)\nchardet (2.0.1)\ncolorama (0.2.5)\ndocker-registry (0.9.1)\ndocker-registry-core (2.0.3)\nFlask (0.10.1)\nFlask-Cors (1.10.3)\ngevent (1.0.1)\ngreenlet (0.4.5)\ngunicorn (19.1.1)\nhtml5lib (0.999)\nitsdangerous (0.24)\nJinja2 (2.7.3)\nM2Crypto (0.22.3)\nMarkupSafe (0.23)\nnewrelic (2.22.1.20)\npip (1.5.4)\nPyYAML (3.11)\nredis (2.10.3)\nrequests (2.3.0)\nsetuptools (5.8)\nsimplejson (3.6.2)\nsix (1.5.2)\nSQLAlchemy (0.9.4)\nurllib3 (1.7.1)\nWebOb (1.4)\nWerkzeug (0.10.1)\nwsgiref (0.1.2)\n. I'm able to reproduce this error easily by running the docker-registry-ui container:\n```\nboot2docker init\nboot2docker up\neval $(boot2docker shellinit bash)\nstart\ndocker run -d \\\n  -p 80:80 \\\n  -p 443:443 \\\n  -v /var/run/docker.sock:/tmp/docker.sock \\\n  -v /etc/docker/certs.d/:/etc/nginx/certs \\\n  --name=nginx-proxy \\\n  -t trinitronx/nginx-proxy\nstart docker-registry-ui\ndocker run -d \\\n          -p 8080:8080 \\\n          -e REG1=https://registry.example.com:443/v1/ \\\n          --name=docker-registry-ui\n          atcol/docker-registry-ui\n```\nWatch the logs by opening 3 terminals:\n```\nTerminal 1\ndocker logs -f nginx-proxy\nTerminal 2\ndocker logs -f docker-registry\nTerminal 3\ndocker logs -f docker-registry-ui\n```\nI then simply access the \"Images\" page through the docker-registry-ui Web UI via URL: http://ip-of-boot2docker-box:8080/repository/index.\nHere are are all 3 anonymized logs\nThe other way to reproduce it, although not as easy perhaps, is to do lots of docker pull commands in succession for different images / tagged containers.\n. Got another instance of this bug, with different request for /repositories/*/*/images endpoint.  However, same 504 Gateway Timeout, similar stacktrace & error Broken Pipe from gevent.\nHere are the logs.  There are a couple requests in the logs.. the first couple are from the initial docker pull that failed.  The next couple are from 2 manual cURL requests I performed.  One reproduced the error, the other didn't.  This bug is odd in that it only occurs sometimes, but not always.  Usually a second retry is successful.  Like I mentioned before, the easiest way to reproduce it is to run the registry UI and just hit some pages to search around for images.\nThis bug looks a lot like gevent/gevent#445\n. There are other similar bugs on gevent's GitHub issue tracker\nThe ones that most notably seem related:\n- gevent/gevent#377\n- gevent/gevent#136\n. After enabling the SEARCH_BACKEND and GUNICORN_OPTS='[--preload]' options to the docker registry, we ran into stability issues (docker/docker-registry#540).\nAfter disabling both of these options, these stability issues went away and errors became much less frequent (although seemed to be present but still sometimes occurred).\nDidn't have enough time to investigate further, but turning off search and disabling the --preload option did alleviate the issue.\nSince this project is deprecated, hopefully this helps anyone still stuck on Registry V1.\n. Just to add a data point:  After enabling the SEARCH_BACKEND and GUNICORN_OPTS='[--preload]' options to the docker registry, we ran into stability issues with docker/docker-registry#540.\nAfter disabling both of these options, these stability issues went away and errors became much less frequent (although seemed to be present but still sometimes occurred)\n. sudo for docker CLI commands has always been needed unless your user is not in the docker group (or whichever group is configured to have rw access to the docker daemon unix socket).  That behavior is unrelated to this bug.\n. @smiller171:  Registry v2 lives in the docker/distribution project as @dmp42 has helpfully pointed out.  This repo is the old / legacy v1 python-based Registry.\n. ",
    "matleh": "@dmp42 thanks for you answer. I am not sure whether I understand everything.\nThe documentation gives the impression, that Docker is striving for a distributed architecture with a more or less clear divide of responsibilities between index, registry and docker client. The index is responsible for authentication and to know what exists and where it exists. The registries are responsible to store the images and contact the index to see if a request is allowed. The docker client first contacts the index and is then redirected to the right registry for the particular repository...\nThat architecure does make sense to me (even though is has some rough edges) and appears open for everybody to operate either a registry or both an index and registry. But after I digged deeper, I get the impression that this architecture and this openness is not really where the real implementation is heading to (maybe because the Docker Hub is the main business model of Docker Inc.?). That is understandable from a business perspective but regrettable from an Open-Source perspective.\nBut on the other hand, all this code code is there - it is possible to give a index_endpoint to the registry, it is possible to do \"docker login some.private.registry\", the code is there for this token handling an everything ... Are these just relicts from the past?\nTo me it looks like there are only two \"officially\" supported scenaries: \n(1) use the Docker Hub operated by Docker Inc. and have everything (also the registry) under their control\n(2) use a docker-registry in standalone mode\n(I know there is also quay.io, but that is more or less just a variation of (1))\n(1) has the disadvantages\n(a) I have to entrust all my intellectual property into the hands of Docker Inc.\n(b) authentication and permission handling is limited to what the Docker Hub provides - that is I can give access to my private repositories only to other users\n(2) has the disadvantages\n(a) there is no authentication taking place inside the docker-registry, so this is only suitable for public repositories or internal use \nI want to be able to use a docker registry to distribute commercial software to customers. That is, I want the registry to be publicly available but have access to the repositories restricted to customers (and be able to create new customers ad hoc). As far as I can see, that kind of setup is currently not supported.\nI will have a look into what can be done with nginx auth, which you suggested.\n. I made good progress today writing some authentication system in front of the registry using nginx and ngx_http_auth_request_module. Thanks for your suggestions.\nThe only problem I see with this, is that the authentication server does not know which image-ids belong to which repositories, so it has to allow GETing of all images to everybody who is allowed to GET any repository. For our current setup, that might be tolerable (at least only known customers have access to the images and without having access to the repository one has to guess image-ids), but it would be nice to be able to close this security hole...\nMaybe I will have a look, if I can hook into the registry to gain access to this information.\n. Well, after I spend a considerable amount of time implementing access control with nginx and everything looked very nice, today came the disenchantment. It looks like access control with Basic Auth is broken with current versions of docker (I am on arch-linux which has docker version 1.2).\nThe first request to the registry is made with Authorization:Basic but after that, docker switches to using Authorization:Token (even if no token is provided in the response from the registry) which breaks access control checking based on Basic Auth information. \nLooks like I can throw away my work of the last days and have to fork the docker-registry to implement the access control right within the registry itself.\nAny other ideas or did I misunderstand/misuse anything? \n. I run the \"registry\" image (docker run  -e STANDALONE=true -e STORAGE_PATH=/data -p 5001:5000 -v /var/registry-data:/data --name local_registry --rm registry) which is on version 0.8.1 and I use the docker client with version 1.2.0.\nI have nginx as a reverse proxy before that with the following configuration:\n```\npid /home/mat/checkouts/docker-auth/nginx.pid;\nworking_directory /home/mat/checkouts/docker-auth;\nerror_log /home/mat/checkouts/docker-auth/nginx-error.log info;\nevents {\n    worker_connections  1024;\n}\nhttp {\n    access_log /home/mat/checkouts/docker-auth/nginx-access.log;\n    # the following temp-dirs are needed for nginx to start\n    proxy_temp_path /home/mat/checkouts/docker-auth/tmp/proxy_temp 1 2;\n    client_body_temp_path /home/mat/checkouts/docker-auth/tmp/client_body 1 2;\n    fastcgi_temp_path /home/mat/checkouts/docker-auth/tmp/fastcgi 1 2;\n    uwsgi_temp_path /home/mat/checkouts/docker-auth/tmp/uwsgi 1 2;\n    scgi_temp_path /home/mat/checkouts/docker-auth/tmp/scgi 1 2;\nserver {\n    listen       5000;\n    server_name  hub.example.com;\n\n    location / {\n        proxy_pass http://127.0.0.1:5001;\n        proxy_set_header Host $host:5000;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header Authorization \"\";\n        proxy_hide_header X-Docker-Token;\n        proxy_read_timeout 900;\n\n        client_max_body_size 0;\n        auth_request /_auth;\n        auth_request_set $token $upstream_http_x_token;\n        add_header X-Docker-Token $token;\n    }\n\n    location /_ping {\n        auth_basic off;\n    }\n\n    location /v1/_ping {\n        auth_basic off;\n    }\n\n    location /_auth {\n        proxy_pass http://127.0.0.1:8999/auth;\n        proxy_pass_request_body off;\n        proxy_set_header Content-Length \"\";\n        proxy_set_header X-Original-URI $request_uri;\n        proxy_set_header X-Original-Method $request_method;\n    }\n}\n\n}\n```\nThere is a custom access-control application running on http://127.0.0.1:8999 which uses the username and password in the Authorization header and the requested URL to check if the request is allowed.\nAs you may see in the nginx configuration, I found a workaround for this problem: my access-control application hijacks the X-Docker-Token header and uses it to identify following requests. Since the registry does not check the X-Docker-Token in standalone mode, this should work (did not have time for extensive testing, yet). But I wouldn't mind if I could do without that.\n. @dmp42 as for my experience of the last days, the ngnx.conf you linked to does not work any more, precisely for the reasons I wrote about above.\n. All right, what I wrote only is true when docker-registry is not running over HTTPS.\nIf docker-registry runs over HTTPS, docker just sends Basic Authorization headers with every request.\nIf docker-registry run over HTTP, docker sends Basic Authorization headers with the first request and after that switches to Token Authorization. It is questionable, whethe that makes sense. If docker sends basic authentication data in plain text over HTTP once, it may as well continue doing so (it is insecure anyways). That way, one would at least have consistent behavior.\nFor anyone who reads this - be aware that to use docker-registry over HTTPS, one needs a \"real\" SSL certificate - self-signed certificates do not work and there is no way to make docker accept them for testing and development purposes, besides installing the test-CA certificate on the system that runs docker. (see https://github.com/docker/docker/pull/2687)\n. @tangicolin I don't think this is a problem in your setup,  since it is docker and not the docker-registry who refuses to do basic Auth over http. So the haproxy can be the ssl endpoint. \n. @adamhadani are you sure that the communication happens over HTTPS? Are you sure, Nginx only listens on port 443? You nginx.conf would be helpful, too.\n. @adamhadani No, docker will not work, if it does not get a X-Docker-Token header (at least in the version 1.2.0 that I use). I had to include it in the response again after I tried to remove it, because docker started to complain. \nAuthorization: Basic Og== is just a : (base64-decoded), so docker sends basic auth with an empty username and password. I think, docker does not fully support the \"username:password@my.registry.url\" syntax that you use. It works on the first call, but for subsequent calls, docker did not remember username and password and therefore send empty ones instead (just my theory). I think, you need to do docker login -u username -p password -e email my.registry.url and then just docker push my/imagg my.registry.url.\n. The inconsistent behaviour over HTTP and HTTPS is not really resolved and the lack of documentation is neither.\n. ",
    "jdiaz5513": "This is somewhat annoying; I was planning on implementing auth on my own registry as well. \nI think the problem here lies with docker; if it doesn't receive an auth token from the registry it should continue to use basic auth instead. I'm still ok with docker-registry not handling auth on its own but it should always be clear how to implement it for those who need it. That depends on reliable behavior from the docker client.\n. ",
    "tangicolin": "But if you want to have docker registry in High Availability mode you will have following schema: \nFrontend: HAproxy ... HAproxy  \nAuth  :  Nginx reverse proxy ... Nginx reverse proxy \nApp: Docker registry ... Docker registry\nStorage : Distributed storage\nSo for HAproxy we have three strategy : \nSSL termination (No working because Docker registry doesn't allow auth over HTTP)\nSSL pass-trough (Working but only at tcp level, we can't use HAproxy Http features like url routing)\nSSL decode and re-encode (Work but very complex setup)\nAnd having SSL decode is not CPU-free, having feature to have docker auth over http will simplify HA setup. It's not really a security issue if network behind HAproxy is a local network. \n. ",
    "dephee": "Basic auth only works for https. Thus to make https works few things that help me:\n- restart docker after adding ca.pem to /etc/ssl/certs , \n  sudo systemctl restart docker\n- login using https\n  docker login https://yourdomain:443\nafter this, I can do docker pull and docker push\n. ",
    "tzz": "After setting up with nginx auth (following https://github.com/docker/docker-registry/blob/master/contrib/nginx/nginx.conf and https://github.com/docker/docker-registry/blob/master/contrib/nginx/docker-registry.conf) to a socket file, the following magic worked (Fedora 20, nginx 1.4.7, docker-registry 0.8.1, docker-io 1.3.0):\ndocker login -u USER -p PASS -e whatever@pwnie.com https://USER:PASS@myhost\ndocker tag ubuntu USER:PASS@myhost/ubuntu\ndocker push USER:PASS@myhost/ubuntu\nIt's not ideal because the USER and PASS are exposed in the tags, but it's the only way I've found.  I hope it helps research this issue.\n. ",
    "paturuv": "I am facing issue with docker client to connect to registry. I have the following:\n1. I have a private registry running on a server which is ssl enabled (nginx)\n2. The basic authentication implemented with Nginx \n3. The curl is working fine\n4. When tried to login using docker client that fails with the following error message\n    Error response from daemon: Server Error: Post https://example.domain.com/v1/users/: x509: certificate signed by unknown authority\nI am using a CA certificate to enable SSL and that works fine with curl command too...  Not sure if this is due to x509 certificate??\n. I could resolve this issue of login by following the below steps:\n1. Place the certificate file (ca-bundle.crt) under /etc/pki/tls/certs\n2. Running  update-ca-trust command\n3. Restart the the docker service \n. Though the login is successful, my pull requests are failing.\nI looked in to the nginx log file then I could see a permission denied response\n43543#0: *17 open() \"/var/www/example.domain/.htpasswd\" failed (13: Permission denied), client: <>, server: example.domain.com:5000, request: \"GET /v1/repositories/base/apache-2.2-base/images HTTP/1.1\", host: \"host name\"\n. Hi -\nI have added the user as owner of the file, still permission denied error.\nAm I missing any thing here?\nThanks,\nVenkat\nSent from my iPhone\n\nOn Jan 20, 2015, at 12:30 PM, Olivier Gambier notifications@github.com wrote:\nYour nginx server does not have permission to access your .htpasswd file.\nI assume nginx is running as www-data, so make sure that unix account has at least read access to your .htpasswd file.\n\u2014\nReply to this email directly or view it on GitHub.\n. I have changed the .htpasswd permissions to 777\n\nThe nginx service is run by root\nThanks\nVenkat\nSent from my iPhone\n\nOn Feb 17, 2015, at 12:31 AM, Olivier Gambier notifications@github.com wrote:\nWhat user is used by nginx? (ps aux | grep nginx should give you a hint)\nThen su THATUSER and try to cat /var/www/example.domain/.htpasswd to validate your perm are ok.\n\u2014\nReply to this email directly or view it on GitHub.\n. I have changed the owner to root, still the same error\nFollowing is the result of\nls -l\n-rwxrwxrwx 1 root root 23 feb 16 21:00 /var/www/example.domain/.htpasswd\n\nThanks\nVenkat\nSent from my iPhone\n\nOn Feb 17, 2015, at 1:59 PM, Olivier Gambier notifications@github.com wrote:\nCan you run: ls /var/www/example.domain/.htpasswd\nThanks.\n\u2014\nReply to this email directly or view it on GitHub.\n. I shall check the permissions recursively. Thank you.\n\nI have another issue related to docker registry api, set a tag for specific image is is not working, not sure if I can work with you to resolve the same\nThanks,\nVenkat\nSent from my iPhone\n\nOn Feb 17, 2015, at 3:35 PM, Olivier Gambier notifications@github.com wrote:\nAre all directories along that path executable for the nginx user?\neg: /var /var/www /var/www/example.domain\nEither way, this really is a nginx configuration problem, not a registry issue.\n\u2014\nReply to this email directly or view it on GitHub.\n. Thank you so much. I really appreciate your support\n\nThanks,\nVenkat\nSent from my iPhone\n\nOn Feb 17, 2015, at 4:04 PM, Olivier Gambier notifications@github.com wrote:\nThe bugtracker here is rather meant for feature requests and bug report, not support.\nI'm happy to help as time permits, but you should rather direct support questions to irc / forums.\n\u2014\nReply to this email directly or view it on GitHub.\n. It is a PUT request.\nhttp://registry-host:port/v1/repositories/namespace/repository:port/tags/v1.0\n\nThanks\nVenkat\nSent from my iPhone\n\nOn Feb 17, 2015, at 6:23 AM, Olivier Gambier notifications@github.com wrote:\nCan you copy the exact request you are doing, including all parameters and headers?\n\u2014\nReply to this email directly or view it on GitHub.\n. It is a PUT request through spring rest template\n\nhttp:///v1/repositories/<>/<>/tags/<>/\nNote:strings included within <<>> are specific values that I have provided\nPlease let me know if you see an issue\n. It is a PUT request using spring rest template with 3 parameters\nParam:1 : ImageName (abc/pqr_base) \nParam:2: RepositoryName (docker-registry-host:port)\n Param-3: newTag\nhttp:///v1/repositories/<>/<>/tags/<>/\nPlease let me know if you see issue in the above format / parameters\n. ",
    "hibooboo2": "@paturuv The specific issue you are talking about is likely that you are not fully chaining all the certs.\nI just went through this issue today when setting up a private registry using a ssl cert that has an intermediate one.\nI had to add the bundled ssl cert to the cert that was for me so that docker would resolve the full chain. In order to do this you can do this:\ncat yourssl.crt sslcertbundle.crt > thecerttouse.crt\nAnd then use the generated cert as the cert that nginx serves up. \n. @olibob The certs i am talking about are not in the boot2docker vm they are in the nginx setup that does the reverse proxy to provide authentication to the registry.\n. ",
    "olibob": "@hibooboo2 could you please elaborate how you fixed the issue? Where is the sslcertbundle.crt located?  Are you talking about /etc/ssl/certs/ca-certificates.crt in a boot2docker VM (docker-machine)?\n. Same issue here.\n. Hi,\nI have the same issue.\nConditions:\n- myreg.net as registry domain\n- 2 VMS to test (registry VM and standard user VM)\n- self signed certificate as indicated here.\n- certificate copied to /etc/docker/certs.d/myreg.net:5000/ca.crt on both VMs\n- docker daemon restarted\ncommand used\ndocker run -d -p 5000:5000 --restart=always --name registry -v /root/certs:/certs -v /reg:/var/lib/registry -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key registry:2\npush/pull works\nAdd authentication as indicated in Basic native auth\nWhen trying to log on with docker login myreg.net:5000, I got some TLS handshake errors. So I trusted the certificate as indicated here\nTLS handshake errors are gone. A curl works as well:\n```\n[root@reg02 anchors]# curl -v https://myreg.net:5000/v2/_catalog\n About to connect() to myreg.net port 5000 (#0)\n   Trying 192.168.60.11...\n Connected to myreg.net (192.168.60.11) port 5000 (#0)\n Initializing NSS with certpath: sql:/etc/pki/nssdb\n   CAfile: /etc/pki/tls/certs/ca-bundle.crt\n  CApath: none\n SSL connection using TLS_RSA_WITH_AES_128_CBC_SHA\n Server certificate:\n       subject: CN=myreg.net,O=Agile Partner,L=Luxembourg,ST=Luxembourg,C=LU\n       start date: Aug 17 12:34:01 2015 GMT\n       expire date: Aug 16 12:34:01 2016 GMT\n       common name: myreg.net\n       issuer: CN=myreg.net,O=Agile Partner,L=Luxembourg,ST=Luxembourg,C=LU\n\nGET /v2/_catalog HTTP/1.1\nUser-Agent: curl/7.29.0\nHost: myreg.net:5000\nAccept: /\n< HTTP/1.1 200 OK\n< Content-Type: application/json; charset=utf-8\n< Docker-Distribution-Api-Version: registry/2.0\n< Date: Mon, 17 Aug 2015 14:28:58 GMT\n< Content-Length: 46\n<\n{\"repositories\":[\"busybox\",\"httpd\",\"ubuntu\"]}\n* Connection #0 to host myreg.net left intact\n```\n\nI still can push/pull without being logged in.\ncommand used\ndocker run -d -p 5000:5000 --restart=always --name registry -v /root/certs:/certs -v /root/auth:/auth -v /reg:/var/lib/registry -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd registry:2\nOn the \"client\"\n[root@reg02 anchors]# docker login https://myreg.net:5000\nUsername: testuser\nPassword:\nEmail: mymail@gmail.com\nError response from daemon: no successful auth challenge for https://myreg.net:5000/v2/ - errors: []\nLogs on the registry\ntime=\"2015-08-17T14:41:21Z\" level=info msg=\"response completed\" http.request.host=\"myreg.net:5000\" http.request.id=3914780f-6c01-44ff-a710-b0313808380e http.request.method=GET http.request.remoteaddr=\"192.168.60.12:49459\" http.request.uri=\"/v2/\" http.request.useragent=\"docker/1.8.1 go/go1.4.2 git-commit/d12ea79 kernel/3.10.0-229.11.1.el7.x86_64 os/linux arch/amd64\" http.response.contenttype=\"application/json; charset=utf-8\" http.response.duration=1.962546ms http.response.status=200 http.response.written=2 instance.id=c5bc7d6b-1f0c-4cc3-9612-d3b145cd9afd version=v2.1.1\n192.168.60.12 - - [17/Aug/2015:14:41:21 +0000] \"GET /v2/ HTTP/1.1\" 200 2 \"\" \"docker/1.8.1 go/go1.4.2 git-commit/d12ea79 kernel/3.10.0-229.11.1.el7.x86_64 os/linux arch/amd64\"\nIt's as if the basic authentication is not active at all.\nHere is the begining of the registry logs\ntime=\"2015-08-17T14:26:56Z\" level=warning msg=\"No HTTP secret provided - generated random secret. This may cause problems with uploads if multiple registries are behind a load-balancer. To provide a shared secret, fill in http.secret in t\nhe configuration file or set the REGISTRY_HTTP_SECRET environment variable.\" instance.id=c5bc7d6b-1f0c-4cc3-9612-d3b145cd9afd version=v2.1.1\ntime=\"2015-08-17T14:26:56Z\" level=info msg=\"redis not configured\" instance.id=c5bc7d6b-1f0c-4cc3-9612-d3b145cd9afd version=v2.1.1\ntime=\"2015-08-17T14:26:56Z\" level=info msg=\"using inmemory blob descriptor cache\" instance.id=c5bc7d6b-1f0c-4cc3-9612-d3b145cd9afd version=v2.1.1\ntime=\"2015-08-17T14:26:56Z\" level=info msg=\"Starting upload purge in 55m0s\" instance.id=c5bc7d6b-1f0c-4cc3-9612-d3b145cd9afd version=v2.1.1\ntime=\"2015-08-17T14:26:57Z\" level=info msg=\"listening on [::]:5000, tls\" instance.id=c5bc7d6b-1f0c-4cc3-9612-d3b145cd9afd version=v2.1.1\ntime=\"2015-08-17T14:27:05Z\" level=info msg=\"response completed\" http.request.host=\"myreg.net:5000\" http.request.id=0dff0ae8-cc12-4a79-9317-bdc59caf8268 http.request.method=GET http.request.remoteaddr=\"192.168.60.1:60053\" http.request.uri=\n\"/v2/_catalog\" http.request.useragent=\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.155 Safari/537.36\" http.response.contenttype=\"application/json; charset=utf-8\" http.response.du\nration=2.614145ms http.response.status=200 http.response.written=46 instance.id=c5bc7d6b-1f0c-4cc3-9612-d3b145cd9afd version=v2.1.1\n192.168.60.1 - - [17/Aug/2015:14:27:05 +0000] \"GET /v2/_catalog HTTP/1.1\" 200 46 \"\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.155 Safari/537.36\"\n192.168.60.1 - - [17/Aug/2015:14:27:06 +0000] \"GET /favicon.ico HTTP/1.1\" 404 19 \"https://myreg.net:5000/v2/_catalog\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.155 Safari/537.\n36\"\ntime=\"2015-08-17T14:27:43Z\" level=info msg=\"response completed\" http.request.host=\"myreg.net:5000\" http.request.id=2042909c-d692-4fd8-9132-a7abe675f75d http.request.method=GET http.request.remoteaddr=\"192.168.60.12:49452\" http.request.uri\n=\"/v2/\" http.request.useragent=\"docker/1.8.1 go/go1.4.2 git-commit/d12ea79 kernel/3.10.0-229.11.1.el7.x86_64 os/linux arch/amd64\" http.response.contenttype=\"application/json; charset=utf-8\" http.response.duration=1.848427ms http.response.\nstatus=200 http.response.written=2 instance.id=c5bc7d6b-1f0c-4cc3-9612-d3b145cd9afd version=v2.1.1\n192.168.60.12 - - [17/Aug/2015:14:27:43 +0000] \"GET /v2/ HTTP/1.1\" 200 2 \"\" \"docker/1.8.1 go/go1.4.2 git-commit/d12ea79 kernel/3.10.0-229.11.1.el7.x86_64 os/linux arch/amd64\"\ntime=\"2015-08-17T14:28:01Z\" level=info msg=\"response completed\" http.request.host=\"myreg.net:5000\" http.request.id=8ed7927f-c741-4008-8c1b-9eec9bfa2a7e http.request.method=GET http.request.remoteaddr=\"192.168.60.12:49453\" http.request.uri\n=\"/v2/\" http.request.useragent=\"docker/1.8.1 go/go1.4.2 git-commit/d12ea79 kernel/3.10.0-229.11.1.el7.x86_64 os/linux arch/amd64\" http.response.contenttype=\"application/json; charset=utf-8\" http.response.duration=2.055284ms http.response.\nstatus=200 http.response.written=2 instance.id=c5bc7d6b-1f0c-4cc3-9612-d3b145cd9afd version=v2.1.1\n192.168.60.12 - - [17/Aug/2015:14:28:01 +0000] \"GET /v2/ HTTP/1.1\" 200 2 \"\" \"docker/1.8.1 go/go1.4.2 git-commit/d12ea79 kernel/3.10.0-229.11.1.el7.x86_64 os/linux arch/amd64\"\ntime=\"2015-08-17T14:28:01Z\" level=info msg=\"response completed\" http.request.host=\"myreg.net:5000\" http.request.id=0a97f37a-62ee-4fb6-9601-96084c1cd130 http.request.method=GET http.request.remoteaddr=\"192.168.60.12:49454\" http.request.uri\n=\"/v2/busybox/manifests/latest\" http.request.useragent=\"docker/1.8.1 go/go1.4.2 git-commit/d12ea79 kernel/3.10.0-229.11.1.el7.x86_64 os/linux arch/amd64\" http.response.contenttype=\"application/json; charset=utf-8\" http.response.duration=5\n.141901ms http.response.status=200 http.response.written=5697 instance.id=c5bc7d6b-1f0c-4cc3-9612-d3b145cd9afd version=v2.1.1\n192.168.60.12 - - [17/Aug/2015:14:28:01 +0000] \"GET /v2/busybox/manifests/latest HTTP/1.1\" 200 5697 \"\" \"docker/1.8.1 go/go1.4.2 git-commit/d12ea79 kernel/3.10.0-229.11.1.el7.x86_64 os/linux arch/amd64\"\ntime=\"2015-08-17T14:28:58Z\" level=info msg=\"response completed\" http.request.host=\"myreg.net:5000\" http.request.id=5f7ab413-e908-430c-8123-525a374b5118 http.request.method=GET http.request.remoteaddr=\"192.168.60.12:49455\" http.request.uri\n=\"/v2/_catalog\" http.request.useragent=\"curl/7.29.0\" http.response.contenttype=\"application/json; charset=utf-8\" http.response.duration=3.295505ms http.response.status=200 http.response.written=46 instance.id=c5bc7d6b-1f0c-4cc3-9612-d3b14\n5cd9afd version=v2.1.1\n192.168.60.12 - - [17/Aug/2015:14:28:58 +0000] \"GET /v2/_catalog HTTP/1.1\" 200 46 \"\" \"curl/7.29.0\"\n. @dmp42 \nNoted: https://github.com/docker/distribution\nAdding the missing environment variable fixed the issue.\nThank you for your time and help.\n. ",
    "hgomez": "I'm still batling with this one, docker-registry 0.9.1, docker 1.5.0 and Apache HTTPd 2.4\nI used easy_install  --user \"docker-registry==0.9.1\"\nInitial request came with \nAuthorization:Basic ZWNkLWRlcGxveWVyOjEyMw==|Content-type:application/json|X-Docker-Token:true|Accept-Encoding:gzip\nSuccessive came with \nAuthorization:Token Token signature=061R11KL2ETBB6E4,repository=\"library/ecd-busybox\",access=write|Accept-Encoding:gzip\nHTTPd front didn'f find Authorization:Basic and didn't forward the push request.\nWith docker-registry image, started using docker run, I still have Authorization:Basic presented back.\nWhat's difference between docker-registry image and docker-registry via pip/easy_install ?\nThanks\n. Same problem here using docker-registry 0.9.1 and Docker version 1.5.0, build a8a31ef on Mint 17.\nI'm using Apache HTTPd 2.4 in front.\nImage I tried to push is busybox\nDoes it means ancestry layout should be fetched via docker-registry first ?\n. Also updated configuration to allow  /_ping URI too\nAs found in nginx configuration\n. ",
    "harrykao": "Regarding running the registry on standard ports, I don't think that'll break but I haven't actually tried it. (We close those ports in our environment as a policy so it's not trivial for me to test.)\nI'm not opposed to a registry-nginx container as long as the embedded nginx is fully configurable. We use SSL and basic auth and we may set up client authentication in the future. I wonder though, whether there's a big advantage to embedding nginx. If I'm at the point where I need to write the config myself and share it and the certs into the container, it seems just as easy to run nginx on the host.\n. Well, our second son was born in October and it's time to admit that, realistically, I'm just not going to get around to testing this change on standard ports. :(\n. ",
    "visualphoenix": "@dmp42 @shin- would be great to get feedback. would love to get this in the next release.\n. I believe not. A mirror registry is just providing a local hot cache of everything available upstream. We still need to return all possible results reported from the upstream registry. \nIn this case, why would we ever check the local db? Upstream provides us all available (mirrorable) repositories, descriptions, star count, and boolean flags for 'official' or 'automated'. \n. Ah yes - you are correct. I checked the definition of a mirror registry from the hub_registry_spec and you're right. I'm only using a mirror registry as a local hot cache. But it should allow us to push to a mirror registry. Possibly a better name for 'mirror' registry then might be an 'overlay' registry. Since it is a combination of both a mirror and a private registry.\nI'll look into enhancing this PR. So how should we handle merging the results? Should we flag the private registries entries with official=true?\n. I'm not sure this is applicable to the search function. In the context of performing a search against a mirror registry it should probably merge the responses from both upstream and local.\n. A search against the public registry looks like\ncurl https://index.docker.io/v1/search?q=dind-jenkins-slave\n{\"query\":\"dind-jenkins-slave\",\"results\":[{\"is_trusted\":true,\"is_automated\":true,\"is_official\":false,\"star_count\":0,\"name\":\"spiddy/dind-jenkins-slave\",\"description\":\"\"},{\"is_trusted\":false,\"is_automated\":false,\"is_official\":false,\"star_count\":0,\"name\":\"wwadge/dind-jenkins-slave\",\"description\":\"\"},{\"is_trusted\":false,\"is_automated\":false,\"is_official\":false,\"star_count\":0,\"name\":\"quintenk/dind-jenkins-slave\",\"description\":\"\"}],\"num_results\":3}\nSo we should probably just query the local results, query the remote results, and merge the remote results into the local results such that remote > local.\nAccording to @shykes on https://github.com/docker/docker/issues/1988 we should never allow someone to override the remote results.\n. So, I had previously reused the @mirroring.source_lookup(index_route=True) decorator to do less work. Does anyone have a good idea or preferred way of issuing the upstream search request?\n. will close this and reopen a new PR meringing results\n. updated to fix the logic for --reload to match the previous logic\n. @dmp42 7b3b55d only fixes part of the problems with this file. If an alternate GUNICORN_USER or GUNICORN_GROUP is specified, it will still not work.  https://github.com/docker/docker-registry/pull/548 handles all of the issues.\n. I'm also not sure that this branch is ready. I tried checking out this next\nbranch for some development, but gunicorn dies with an error about being\nunable to import extras. I will also submit a PR fixing the documentation\nbecause contrib/gunicorn.py doesn't exist.\nOn Friday, September 19, 2014, Joffrey F notifications@github.com wrote:\n\nA few comments, other than that should be good to go [image: :+1:]\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/pull/563#issuecomment-56207866\n.\n\n\nRaymond \"VisualPhoenix\" Barbiero\n. @dmp42 checkout the next branch\n$ docker build -t='next' . \n$ sudo docker run -i -t --name next-registry next:latest /bin/bash\nroot@d899d71107b0:/# cd docker-registry/\nroot@d899d71107b0:/docker-registry# gunicorn -c contrib/gunicorn_config.py docker_registry.wsgi:application\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    __import__(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 10, in <module>\n    from .extras import newrelic\nImportError: No module named extras\nTraceback (most recent call last):\n  File \"/usr/local/bin/gunicorn\", line 11, in <module>\n    sys.exit(run())\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 74, in run\n    WSGIApplication(\"%(prog)s [OPTIONS] [APP_MODULE]\").run()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 185, in run\n    super(Application, self).run()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 71, in run\n    Arbiter(self).run()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 169, in run\n    self.manage_workers()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 477, in manage_workers\n    self.spawn_workers()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 542, in spawn_workers\n    time.sleep(0.1 * random.random())\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 209, in handle_chld\n    self.reap_workers()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 459, in reap_workers\n    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\ngunicorn.errors.HaltServer: <HaltServer 'Worker failed to boot.' 3>\n. docs fixed in https://github.com/docker/docker-registry/pull/586\n. any idea why travis didnt pick it up? (not a travis expert)\n. So there might be cleaner and more elegant ways of writing this, but I tested it and it works. Happy to address any comments you might have.\n. +1 on a docker-registry ng hackday. Would love to help.\n. Youtube handles resumable uploads in an interesting way\n. Fwiw, the ability to shard/fan out consistently is a huge +1 for me\nOn Wednesday, October 29, 2014, W. Trevor King notifications@github.com\nwrote:\n\nOn Wed, Oct 08, 2014 at 08:51:39PM -0700, Matthew Fisher wrote:\n\nList(delimiter string) []string\n\nThe Docker hub had 14000 \u201capplications\u201d ($namespace/$repository?) in\nJune [1]. I don't know how many layers that translates to, or how\nmany namespaces there were, but I'm concerned about performance with a\nflat List endpoint [2]. I'd prefer:\nCount(prefix string) int\nList(prefix string, size int, from int) []string\nto let you see how many objects matched a (possibly empty) prefix and\niterate through them without making massive requests. I borrowed\nsize/from from the Elasticsearch API [3], but I don't really care what\nthe strings are.\nAnyhow, something like this would allow the (streaming) storage driver\nto shard or fan-out transparently for efficient lookups and listings,\nwhile still preserving a flat namespace for object lookups.\n[1]:\nhttp://blog.docker.com/2014/06/announcing-docker-hub-and-official-repositories/\n[2]:\nhttps://github.com/docker/docker-registry/issues/626#issuecomment-60841195\n[3]:\nhttp://www.elasticsearch.org/guide/en/elasticsearch/reference/current/search-request-from-size.html\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/616#issuecomment-60931440\n.\n\n\nRaymond \"VisualPhoenix\" Barbiero\n. I think semver is the way to go and we should have backwards compatibility\nif possible for at least a major version.\nOn Monday, November 3, 2014, Olivier Gambier notifications@github.com\nwrote:\n\nThe way registry V1 worked was semver compatible (your driver would depend\non core>=X,<X+1 and would work that way).\nThat might be trickier to get right with go, but then I'm ok bumping major\nversions more often (understood that it's the driver interface version, not\nthe main registry version).\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/616#issuecomment-61544710\n.\n\n\nRaymond \"VisualPhoenix\" Barbiero\n. would love to have CAS 2.0+ supported authentication work.\n. So - I can confirm that the docker registry works beautifully on centos 6.5 - I have done all of my primary development on both the registry and extensive docker work on this platform.\nYou should be using the latest docker packages for centos epel6 from koji:\nhttp://kojipkgs.fedoraproject.org/packages/docker-io/1.2.0/3.el6/x86_64/\nYou should NOT be using the lxc backend.\n. @wking said:\n\nIf we use a reverse-proxy for serious deployments (see #623), we can just use that proxy to generate access/error logs.\n\nSo in my case I might actually terminate SSL with an F5 BIG-IP Local Traffic Manager and then run the registry behind that. For instance, use a WIP for routing to the nearest region, a VIP per region, and then put a registry or cluster of registries behind a few HAProxies per region.\nI'm definitely interested in having these registries used a distributed shared storage. But I think @dmp42 has a point in wanting the registries to do the logging. I don't think we should require the proxy to generate the access/error logs.\n. @wking said:\n\nAnd if the Docker client isn't using that \u2018log\u2019 package now, it was previously (before docker/docker@92df943, daemon logging: unifying output and timestamps, 2014-09-25) and likely will be again soon (docker/docker#8745).\n\nThat's super interesting. Looking at the diff didnt make it obvious to me what the interaction was with /dev/me\nFlannel is using https://github.com/golang/glog which looked nice (which I now see was mentioned in the original post).\n@dmcgowan said:\n\nWhile log routing doesn't need to dictate the topics above, some possible routers we may want to integrate and ensure our format works nicely with.\n\nMaybe we take a page out of the Elliptics playbook and let the user configure the log output format and wiring. Then we punt and provide sane defaults to stdout/stderr and test that we provide enough flexibility to customize the output to be compatible with those other utilities.\n. +1 for underscores. +1 for semantic versioning.\n. +1.\nThe registry itself should have a health check that it's up. But if caching and storage are going to be implemented as extensions, maybe we want extensions to implement health checking so we can pass the check through?\n. Or - maybe we don't care and let those external services be monitored independently. But I think a case could be made for centralizing the health of the overall registry in one place.\n. +1 for @bacongobbler 's recommendation. \n. absolutely +1 agreed.\n. Agreed w/ W. Trevor King.\nImportant for exposed registries. Less so for registries behind firewalls.\nOn Tuesday, November 4, 2014, W. Trevor King notifications@github.com\nwrote:\n\nOn Tue, Nov 04, 2014 at 08:19:52PM -0800, Olivier Gambier wrote:\n\nThis is (although related) kind of different from \"traditional\"\nlogging (#635) - and rather about supporting out of the box\nthird-party SAAS monitoring services.\n\nRight. You can do that bit however you like as far as I'm concerned\n;). I'm just putting my use-case out there so it gets included in (or\nat least considered for) whatever the final solution is.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/680#issuecomment-61758269\n.\n\n\nRaymond \"VisualPhoenix\" Barbiero\n. It definitely would be a good idea to allow the API request scope the\namount of data to receive back. But I don't know if something like 0, 100\nis enough to express something meaningful.\nOn Monday, November 3, 2014, W. Trevor King notifications@github.com\nwrote:\n\nIn storagedriver/storagedriver.go:\n\n\n// offset\n// May be used to resume reading a stream by providing a nonzero offset\nReadStream(path string, offset uint64) (io.ReadCloser, error)\n  +\n// WriteStream stores the contents of the provided io.ReadCloser at a location designated by\n// the given path\n// The driver will know it has received the full contents when it has read \"size\" bytes\n// May be used to resume writing a stream by providing a nonzero offset\n// The offset must be no larger than the ResumeWritePosition for this path\nWriteStream(path string, offset, size uint64, readCloser io.ReadCloser) error\n  +\n// ResumeWritePosition retrieves the byte offset at which it is safe to continue writing at the\n// given path\nResumeWritePosition(path string) (uint64, error)\n  +\n// List recursively lists the objects stored at a subpath of the given prefix\n\n\nOn Mon, Nov 03, 2014 at 01:22:40PM -0800, Matthew Fisher wrote: What would\nbe the use case for using from and size? How do you see them being used\nin the registry? If we're concerned about having the prefix at a high\nenough level being slow, we could make it optionally non-recursive:\ngolang Count(prefix string) (int, error) List(prefix string, recursive\nbool) ([]string, error) That should make List(\"/\", false) only\ndisplay [\"images\", \"repositories\"] and List(\"/images\", false)\nsignificantly less time-consuming than recursively listing * every*\ndirectory under /images.\nAnd List(\"/repositories\", false) would list all the namespaces? I'd\nrather say: List(\"namespace/\", 0, 100) for \u201cgive me the first hundred\nnamespaces\u201d. I'd avoid recursion by using namespaced keys (intead of\nheirarchical keys): namespace/alice namespace/bob namespace/library \u2026\nrepository/alice/busybox repository/bob/debian repository/library/debian \u2026\ntag/alice/busybox/0.1 tag/bob/debian/latest tag/library/debian/latest so\nyou could iterate through namespaces (with a \u201cnamespace/\u201d prefix), or\nthrough repositories in a namespace (with a \u201crepositories/alice/\u201d prefix),\nor through all repositories (with a \u201crepositories/\u201d prefix), \u2026\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/pull/643/files#r19766926.\n\n\nRaymond \"VisualPhoenix\" Barbiero\n. ",
    "seryl": "Usually you don't want a top-level array either because you can override Array in javascript.\nYou almost always want to capture in inside of an object.\n\nOn Sep 2, 2014, at 10:00 AM, \"W. Trevor King\" notifications@github.com wrote:\nOn Tue, Sep 02, 2014 at 09:52:58AM -0700, W. Trevor King wrote: \n\nOn Tue, Sep 02, 2014 at 09:43:09AM -0700, Joffrey F wrote: \n\n```\n\nJSON.parse('true') \ntrue \n```\n\nLooks like valid JSON to me =/ \n\nJust because Python's JSON implementation can handle it doesn't mean \nit's valid JSON ;). I think the root node of valid JSON should be \neither an object or a list, but the spec isn't particuarly clear [1]. \n\nAh, ECMAScript 5.1 is more precise, and it does allow a bare 'true' \n[1,2]. \n[1]: http://www.ecma-international.org/ecma-262/5.1/#sec-15.12.1.2 \n[2]: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/JSON\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "razic": "This is what happens when I build the Dockerfile locally. And I think that Vim line might be causing the problem because it pops up like this on the screen when I'm building it (even though it builds fine):\n\n. Docker can build the image when I run it locally. It's only through the registry that I have problems. \nSent from my iPhone\n\nOn Sep 2, 2014, at 9:11 AM, Olivier Gambier notifications@github.com wrote:\nHello @razic\nThis here is the registry. Your issue is likely with docker itself: https://github.com/docker/docker\n\u2014\nReply to this email directly or view it on GitHub.\n. Surprisingly this works now.\n. \n",
    "nicgrayson": "This seems to have worked itself out. Sorry for the noise.\n. ",
    "irachex": "@wking Thanks for your suggestion. I've updated the limit and keyword arguments.\nSince Repository.name = {namespace}/{repository}\nnamespace's limit is 30, repository's limit is 64\nSo the name's limit should be 30 + 1 + 64\nI got the following message in python 2.6 CI. \n./docker_registry/app.py:19:1: H302  import only modules.'from flask.ext.cors import CORS' does not import a module\nIt seems to be irrelevant with this PR and caused by flask.ext magic.\nI must add H302 to flake8 ignore to make CI pass.\n. @vking I've updated as the suggestion.\nCI passed now without ignoring H302 (strange)\n. > although I'd prefer: self._engine.has_table(table_name=Version.tablename)\ntable_name isn't a keyword argument (api doc). It only works fine as a positional argument.\n. @wking I got it wrong. Updated with keyword argument.\n. @wking duplicate comment?\n. ",
    "fantapop": "okay, I'll try this again today.\n. This is actually working for me.  I think it was probably always working but I was seeing the requests back to the origin to check that the image was up to date and thinking it was pulling everything.  After tailing logs on both service the difference is clear.  Thank you.\n. ",
    "zhangpeihao": "Is the checksum field in GET v1/repositories/(namespace)/(repository)/images API now obsolete? Remain the empty field just for previous compatible.\n. ",
    "mattes": "Sure. https://gist.github.com/mattes/83a56c26ea43ff4a80c5\n. > The log states that nginx tries to find \"ping\" on the localfilesystem while it should query the backend (the actual registry).\nAh yes. That makes sense. Actually docker login & docker push/pull works. I will investigate further. Sorry for the confusion.\n. Didn't figure it out, yet. Will re-open if necessary. Thanks! \n. ",
    "yaronr": "Hi\nRunning standalone works.\nCurrently, the attached command fails on boot2docker (see below), and the exact same command (copy-paste) works on CoreOS.\ncore@ip-10-0-4-14 ~ $ docker version\nClient version: 1.1.2\nClient API version: 1.13\nGo version (client): go1.2\nGit commit (client): d84a070\nServer version: 1.1.2\nServer API version: 1.13\nGo version (server): go1.2\nGit commit (server): d84a070\nboot2docker:\nClient version: 1.2.0\nClient API version: 1.14\nGo version (client): go1.3.1\nGit commit (client): fa7b24f\nOS/Arch (client): darwin/amd64\nServer version: 1.2.0\nServer API version: 1.14\nGo version (server): go1.3.1\nGit commit (server): fa7b24f\nn$ curl localhost:5000\ncurl: (52) Empty reply from server\n$ docker logs registry\n2014-09-15 05:50:24,993 WARNING: No S3 region specified, using boto default region, this may affect performance and stability.\n** [Bugsnag] No API key configured, couldn't notify\nTraceback (most recent call last):\n  File \"/usr/local/bin/docker-registry\", line 9, in \n    load_entry_point('docker-registry==0.8.1', 'console_scripts', 'docker-registry')()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 351, in load_entry_point\n    return get_distribution(dist).load_entry_point(group, name)\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2363, in load_entry_point\n    return ep.load()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2088, in load\n    entry = import(self.module_name, globals(),globals(), ['name'])\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/run.py\", line 18, in \n    from .tags import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/tags.py\", line 21, in \n    store = storage.load()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/storage/init.py\", line 38, in load\n    config=cfg)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/drivers/s3.py\", line 65, in init\n    super(Storage, self).init(path, config)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/boto.py\", line 135, in init\n    self._config.boto_bucket)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 471, in get_bucket\n    return self.head_bucket(bucket_name, headers=headers)\n  File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 504, in head_bucket\n    raise err\nboto.exception.S3ResponseError: S3ResponseError: 403 Forbidden\n. guess what. restart of boot2docker solved it. what's going on?\n. @dmp42  confirmed. \nI would recommend to promote this somehow, to help others.\n. ",
    "priyanka5": "Hi, this works fine, but when i execute \"docker run -p 5000:5000 registry:0.8.1\" to run the container on port 5000 , it again gets stuck on -\n2014-09-16 09:46:20,394 WARNING: LRU cache disabled!\n2014-09-16 09:46:20,496 DEBUG: Will return docker-registry.drivers.file.Storage\n2014-09-16 09:46:22,283 WARNING: Cache storage disabled!\n2014-09-16 09:46:22,304 WARNING: LRU cache disabled!\n2014-09-16 09:46:22,459 DEBUG: Will return docker-registry.drivers.file.Storage\n2014-09-16 09:46:33 [1] [INFO] Handling signal: winch\n2014-09-16 09:46:33 [1] [INFO] SIGWINCH ignored. Not daemonized\nI have successfuly installed docker-registry by using \"yum -y install docker-io docker-registry\" on Fedora, but the problem I faced search images didn't work for me, it just returns blank \"http://localhost:5000/v1/search\". So thought of trying this one, moreover i am also not sure how to authenticate this with username and password. It will be really very helpful if you can give solution to this.\nThanks,\nPriya\n. Hi Thanks a lot! it just works fine :) for search back end enablement which file needs to be modified?\n. got it config_sample.yml , will try it\n. Hey one more question for search functionalilty , so according to the link you shared if i do -\nsearch_backend: localhost:5000\nis should work? my registry is running on localhost:5000 here \n. ok thanks, sorry if i am wrong, so it already has sqlite database? no other config is required?\n. Hi, I am also facing the same issue , search doesn't work for me . if you have found solution to  this please let me know. will be really very helpful.\nThanks,\nPriya\n. Hi , thanks , yes the search is working , but where the database \"docker-registry.db\" is getting created ?\nIts nowhere present in my /tmp dir. and search gives name of registry like \"name\": \"library/myregistry\", what is library here?\n. ok thanks for the reply. but still there is no database inside tmp directory of container \"registry:0.8.1\" :(\n. ",
    "peterjc": "Filed upstream as https://github.com/peterjc/backports.lzma/issues/8\n. ",
    "igormoochnick": "Sure. \nThis is how it runs from /etc/init/docker-registry.conf\n```\nenv REGISTRY_HOME=/opt/docker\nenv SETTINGS_FLAVOR=prod\nenv DOCKER_REGISTRY_CONFIG=/opt/docker/config.yml\nscript\nexec gunicorn -k gevent --max-requests 100 --graceful-timeout 3600 -t 3600 -b 0.0.0.0:5000 -w 4 --access-logfile /var/log/docker-registry/access.log --error-logfile /var/log/docker-registry/server.log docker_registry.wsgi:application\nend script\n```\nHere is the config:\ncommon: &common\n    secret_key: secret_key\n    standalone: _env:STANDALONE:true\n    disable_token_auth: true\nprod:\n    <<: *common\n    storage: local\n    storage_path: /opt/docker/images\n    loglevel: info\n. You were 100% right - that is what was missing.  Missed it in the Readme and assumed that the search was working out of the box.\nThanks a lot!\n. Can this file be shared across multiple registries? (i.e. HA configuration)\nIf the file get corrupted, can it be reseeded?\n. ",
    "rvRamakrishnan": "I know this an old issue. But does recent versions of Docker Registry started supporting Redis with Sentinel?. Realized docker/docker-registry repo is deprecated. Filed the same issue against docker/distribution (https://github.com/docker/distribution/issues/2573). Closing this one.. ",
    "zubryan": "Hi Shin,\nActually I want to setup a Docker Registry working as a mirror to official registry. If I understand correctly, the Registry in mirror mode would sync images in official  registry to my environment?\nThanks\n. When pulling an image, i got response 401 as follow. Any idea? something wrong with authentication?\n2014-09-17 13:55:24,970 DEBUG: Request: GET https://index.docker.io/v1/repositories/kozmic/ubuntu-base/images\nHeaders: {'X-Docker-Token': u'true', 'Accept-Encoding': u'gzip', 'Authorization': u'Basic enVicnlhbjp3aG9hbWk=', 'User-Agent': u'docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-24-generic os/linux arch/amd64'}\n2014-09-17 13:55:24,984 INFO: Starting new HTTPS connection (1): index.docker.io\n2014-09-17 13:55:29,197 DEBUG: \"GET /v1/repositories/kozmic/ubuntu-base/images HTTP/1.1\" 401 None\n2014-09-17 13:55:29,198 DEBUG: Source responded to request with non-200 status\n2014-09-17 13:55:29,215 DEBUG: Response: 401\n. Yeah, I did login. Any solution here?\nThanks\n. Here is the command I used.\nsudo docker run -d -p 5000:5000 -e SETTINGS_FLAVOR=local -e STORAGE_PATH=/home/bryan/docker/registry/ -e SEARCH_BACKEND=sqlalchemy -e LOGLEVEL=DEBUG -e MIRROR_SOURCE=https://registry-1.docker.io -e MIRROR_SOURCE_INDEX=https://index.docker.io registry\n. 1. docker login, i got Login Succeeded\n2. sudo docker pull 192.168.129.136:5000/kozmic/ubuntu-base\n   192.168.129.136 is my local VM.\n. How can I make it work then?\nThanks\n. @shin and @dmp42 \nIt's still not working, could you help to have a look?\nActually I have a scenario about registry as follow.\nI need to setup a registry which is a mirror of the official registry in order to speed up image downloading in our country. Is there a way to sync all images to my registry in advance?\nThanks a lot.\n. Thank you shin, but unfortunately, I didn't make it even without authentication, always got 404 response.\n. I ran \"sudo docker pull 192.168.129.136:5000/ncarlson/docker-centos-base\" and started with \"sudo docker run -d -p 5000:5000 -e SETTINGS_FLAVOR=local -e STORAGE_PATH=/home/bryan/docker/registry/ -e SEARCH_BACKEND=sqlalchemy -e LOGLEVEL=DEBUG -e MIRROR_SOURCE=https://registry-1.docker.io -e MIRROR_SOURCE_INDEX=https://index.docker.io registry\"\nHere is the log. \n2014-09-19 12:31:16,948 WARNING: Cache storage disabled!\n2014-09-19 12:31:16,949 WARNING: LRU cache disabled!\n2014-09-19 12:31:16,980 DEBUG: Will return docker-registry.drivers.file.Storage\n2014-09-19 12:31:38,590 DEBUG: args = {'namespace': u'ncarlson', 'repository': u'docker-centos-base'}\n192.168.129.136 - - [19/Sep/2014:12:31:38] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-09-19 12:31:38,592 DEBUG: api_error: images not found\n2014-09-19 12:31:38,591 INFO: 192.168.129.136 - - [19/Sep/2014:12:31:38] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"Go 1.1 package http\"\n2014-09-19 12:31:38,593 DEBUG: Source provided, registry acts as mirror\n2014-09-19 12:31:38,594 DEBUG: Request: GET https://index.docker.io/v1/repositories/ncarlson/docker-centos-base/images\nHeaders: {'X-Docker-Token': u'true', 'Accept-Encoding': u'gzip', 'Authorization': u'Basic enVicnlhbjp3aG9hbWk=', 'User-Agent': u'docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-24-generic os/linux arch/amd64'}\n2014-09-19 12:31:38,619 INFO: Starting new HTTPS connection (1): index.docker.io\n2014-09-19 12:31:44,776 DEBUG: \"GET /v1/repositories/ncarlson/docker-centos-base/images HTTP/1.1\" 401 None\n2014-09-19 12:31:44,777 DEBUG: Source responded to request with non-200 status\n2014-09-19 12:31:44,792 DEBUG: Response: 401\n\"\"\n192.168.129.136 - - [19/Sep/2014:12:31:44] \"GET /v1/repositories/ncarlson/docker-centos-base/images HTTP/1.1\" 404 29 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-24-generic os/linux arch/amd64\"\n2014-09-19 12:31:44,798 INFO: 192.168.129.136 - - [19/Sep/2014:12:31:44] \"GET /v1/repositories/ncarlson/docker-centos-base/images HTTP/1.1\" 404 29 \"-\" \"docker/1.0.1 go/go1.2.1 git-commit/990021a kernel/3.13.0-24-generic os/linux arch/amd64\"\n. Cool, after I removed ~/.dockercfg, it started to pull images, but after a while, I got the error like below.\nI don't know if it's the problem with network.\nPulling repository 192.168.129.136:5000/ncarlson/docker-centos-base\n1ce2d0cc15e4: Pulling dependent layers \n1ce2d0cc15e4: Error pulling image (latest) from 192.168.129.136:5000/ncarlson/docker-centos-base, Get http://192.168.129.136:5000/v1/images/b1bd49907d559b703c2b7c1b0d6f120b5182440f7ac5f08636625d328e96f1ef/layer: read tcp 192.168.129.136:5000: i/o timeout read tcp 192.168.129.136:5000: i/o timeout \nb1bd49907d55: Downloading [===>                                               ] 5.282 MB/74.1 MB 13m51s\nb1bd49907d55: Error downloading dependent layers \n2014/09/19 06:26:01 Could not find repository on any of the indexed registries.\n. yeah, it's the issue with my network, thank you very much.\n. @dmp42 I cannot reopen issues closed\n. ",
    "tmlbl": "ok will do, thanks\n. ",
    "tonicbupt": "sorry for leaving wrong PR\n. ",
    "Boran": "I mean that when the container starts, when I look at its logs, /usr/local/bin/docker-registry is started (within the container). \nAlternatively start the container with /bin/bash and then runnning /usr/local/bin/docker-registry from the prompt gives the same error.\n. The command I use is already there above, since the image was not available, it pulled it down from the docker hub.\nThe complete output is:\nsudo docker run -p 5000:5000 registry\nUnable to find image 'registry' locally\nPulling repository registry\ne42d15ec8417: Download complete\n511136ea3c5a: Download complete\n1c9383292a8f: Download complete\n9942dd43ff21: Download complete\nd92c3c92fa73: Download complete\n0ea0d582fd90: Download complete\ncc58e55aa5a5: Download complete\nc4ff7513909d: Download complete\n2a03160a302f: Download complete\ne01f4e09782b: Download complete\nd45160452d3a: Download complete\n8c3e8f86ecaf: Download complete\n381802b245e1: Download complete\nb1b4fdadbf1b: Download complete\nbab893de1904: Download complete\nf68b51b69ed8: Download complete\n4eb56962f397: Download complete\n5e1179330f72: Download complete\n16731cff304b: Download complete\n2014-09-19 05:20:48,422 WARNING: Cache storage disabled!\n2014-09-19 05:20:48,423 WARNING: LRU cache disabled!\n2014-09-19 05:20:48,426 DEBUG: Will return docker-registry.drivers.file.Storage\n* [Bugsnag] No API key configured, couldn't notify\nTraceback (most recent call last):\n  File \"/usr/local/bin/docker-registry\", line 9, in \n    load_entry_point('docker-registry==0.8.1', 'console_scripts', 'docker-registry')()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 351, in load_entry_point\n    return get_distribution(dist).load_entry_point(group, name)\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2363, in load_entry_point\n    return ep.load()\n  File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2088, in load\n    entry = import(self.module_name, globals(),globals(), ['name'])\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/run.py\", line 23, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 17, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 57, in init\n    self._engine = sqlalchemy.create_engine(database)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/init.py\", line 344, in create_engine\n    return strategy.create(_args, _kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/strategies.py\", line 73, in create\n    dbapi = dialect_cls.dbapi(**dbapi_args)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/dialects/sqlite/pysqlite.py\", line 297, in dbapi\n    raise e\nImportError: No module named pysqlite2\n. Well I only started playing with docker a week ago, working my way through the book :-). Docker is installed on Ubuntu 14.04, from the docker repo. I updated it a day or two ago.\ndocker --version\nDocker version 1.2.0, build fa7b24f\ndocker info\nContainers: 73\nImages: 307\nStorage Driver: devicemapper\n Pool Name: docker-252:0-399601-pool\n Pool Blocksize: 64 Kb\n Data file: /var/lib/docker/devicemapper/devicemapper/data\n Metadata file: /var/lib/docker/devicemapper/devicemapper/metadata\n Data Space Used: 11084.4 Mb\n Data Space Total: 102400.0 Mb\n Metadata Space Used: 19.7 Mb\n Metadata Space Total: 2048.0 Mb\nExecution Driver: native-0.2\nKernel Version: 3.13.0-35-generic\nOperating System: Ubuntu 14.04.1 LTS\nWARNING: No swap limit support\n. sudo docker run -ti -p 5000:5000 registry /bin/bash\nroot@e21b144ba56d:/# python\nPython 2.7.6 (default, Mar 22 2014, 22:59:56)\n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nfrom sqlite3 import dbapi2 as sqlite\nTraceback (most recent call last):\n  File \"\", line 1, in \n  File \"/usr/lib/python2.7/sqlite3/init.py\", line 24, in \n    from dbapi2 import \n  File \"/usr/lib/python2.7/sqlite3/dbapi2.py\", line 27, in \n    from _sqlite3 import \nImportError: /usr/lib/x86_64-linux-gnu/libsqlite3.so.0: file too short\n. I've joined #docker as boran1, is that the right channel?\n\n\n\nroot@67b21f5c013b:/# ls -lA /usr/lib/x86_64-linux-gnu/libsqlite3.so.*\nlrwxrwxrwx 1 root root 19 Jan  2  2014 /usr/lib/x86_64-linux-gnu/libsqlite3.so.0 ->             libsqlite3.so.0.8.6\n-rw-r--r-- 1 root root  0 Jan  2  2014 /usr/lib/x86_64-linux-gnu/libsqlite3.so.0.8.            \nroot@67b21f5c013b:/# file /usr/lib/x86_64-linux-gnu/libsqlite3.so.0.8.6\nfile: error while loading shared libraries: /usr/lib/x86_64-linux-gnu/libmagic.so.1: file too short\n\nHmm. Zero byte files, and its not the only one\napt-get update && apt-get upgrade\napt-get: error while loading shared libraries: /usr/lib/x86_64-linux-gnu/libapt-pkg.so.4.12: fi le too short\nroot@67b21f5c013b:/# ls -al /usr/lib/x86_64-linux-gnu/libapt-pkg.so.4.12*\nlrwxrwxrwx 1 root root 20 Jun 13 17:39 /usr/lib/x86_64-linux-gnu/libapt-pkg.so.4.12 -> libapt-pkg.so.4.12.0\n-rw-r--r-- 1 root root  0 Jun 13 17:40 /usr/lib/x86_64-linux-gnu/libapt-pkg.so.4.12.0\nweird stuff\n. So, created a brand new 14.04 (on linode), installed docker and registry... and ... it worked fine.\nSo I have to do more digging, to see what the difference between these nodes is. I'll get back here.\n. I've already killed the linode. But lets spin up a new one. \nPerhaps their Ubuntu 14.04 image is a bit different from the vanilla I used on my \"broken\" one.\nGenerally Necessary:\n- cgroup hierarchy: properly mounted [/sys/fs/cgroup]\n- CONFIG_NAMESPACES: enabled\n- CONFIG_NET_NS: enabled\n- CONFIG_PID_NS: enabled\n- CONFIG_IPC_NS: enabled\n- CONFIG_UTS_NS: enabled\n- CONFIG_DEVPTS_MULTIPLE_INSTANCES: enabled\n- CONFIG_CGROUPS: enabled\n- CONFIG_CGROUP_CPUACCT: enabled\n- CONFIG_CGROUP_DEVICE: enabled\n- CONFIG_CGROUP_FREEZER: enabled\n- CONFIG_CGROUP_SCHED: enabled\n- CONFIG_MACVLAN: enabled\n- CONFIG_VETH: enabled\n- CONFIG_BRIDGE: enabled\n- CONFIG_NF_NAT_IPV4: enabled\n- CONFIG_IP_NF_TARGET_MASQUERADE: enabled\n- CONFIG_NETFILTER_XT_MATCH_ADDRTYPE: enabled\n- CONFIG_NETFILTER_XT_MATCH_CONNTRACK: enabled\n- CONFIG_NF_NAT: enabled\n- CONFIG_NF_NAT_NEEDED: enabled\n  Optional Features:\n- CONFIG_MEMCG_SWAP: missing\n- CONFIG_RESOURCE_COUNTERS: enabled\n- CONFIG_CGROUP_PERF: enabled\n- Storage Drivers:\n  - \"aufs\":\n    - CONFIG_AUFS_FS: missing\n    - CONFIG_EXT4_FS_POSIX_ACL: enabled\n    - CONFIG_EXT4_FS_SECURITY: enabled\n  - \"btrfs\":\n    - CONFIG_BTRFS_FS: enabled\n  - \"devicemapper\":\n    - CONFIG_BLK_DEV_DM: enabled\n    - CONFIG_DM_THIN_PROVISIONING: enabled\n    - CONFIG_EXT4_FS: enabled\n    - CONFIG_EXT4_FS_POSIX_ACL: enabled\n    - CONFIG_EXT4_FS_SECURITY: enabled\nNote CONFIG_AUFS_FS missing above\nOn the \"broken\" one:\ncurl -Sls https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh | bash\nwarning: /proc/config.gz does not exist, searching other paths for kernel config...\ninfo: reading kernel config from /boot/config-3.13.0-35-generic ...\nGenerally Necessary:\n- cgroup hierarchy: properly mounted [/sys/fs/cgroup]\n- apparmor: enabled and tools installed\n- CONFIG_NAMESPACES: enabled\n- CONFIG_NET_NS: enabled\n- CONFIG_PID_NS: enabled\n- CONFIG_IPC_NS: enabled\n- CONFIG_UTS_NS: enabled\n- CONFIG_DEVPTS_MULTIPLE_INSTANCES: enabled\n- CONFIG_CGROUPS: enabled\n- CONFIG_CGROUP_CPUACCT: enabled\n- CONFIG_CGROUP_DEVICE: enabled\n- CONFIG_CGROUP_FREEZER: enabled\n- CONFIG_CGROUP_SCHED: enabled\n- CONFIG_MACVLAN: enabled\n- CONFIG_VETH: enabled\n- CONFIG_BRIDGE: enabled\n- CONFIG_NF_NAT_IPV4: enabled\n- CONFIG_IP_NF_TARGET_MASQUERADE: enabled\n- CONFIG_NETFILTER_XT_MATCH_ADDRTYPE: enabled\n- CONFIG_NETFILTER_XT_MATCH_CONNTRACK: enabled\n- CONFIG_NF_NAT: enabled\n- CONFIG_NF_NAT_NEEDED: enabled\n  Optional Features:\n- CONFIG_MEMCG_SWAP: enabled\n- CONFIG_RESOURCE_COUNTERS: enabled\n- CONFIG_CGROUP_PERF: enabled\n- Storage Drivers:\n  - \"aufs\":\n    - CONFIG_AUFS_FS: enabled\n    - CONFIG_EXT4_FS_POSIX_ACL: enabled\n    - CONFIG_EXT4_FS_SECURITY: enabled\n  - \"btrfs\":\n    - CONFIG_BTRFS_FS: enabled\n  - \"devicemapper\":\n    - CONFIG_BLK_DEV_DM: enabled\n    - CONFIG_DM_THIN_PROVISIONING: enabled\n    - CONFIG_EXT4_FS: enabled\n    - CONFIG_EXT4_FS_POSIX_ACL: enabled\n    - CONFIG_EXT4_FS_SECURITY: enabled\nNote apparmour enabled above.\n\"devicemapper is really not a good choice\": what should one use, how do I switch?\n. Docker install was done as follows:\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 36A1D7869245C8950F966E92D8576A8BA88D21E9\nsudo sh -c \"echo deb https://get.docker.io/ubuntu docker main\\ > /etc/apt/sources.list.d/docker.list\"\nsudo apt-get update -qq && sudo apt-get -yq install lxc-docker\nI think aufs support was added later with this:\napt-get install linux-image-extra-uname -r\n(so next time I should do that first)\next4 is in use.\nWhen I set up another new host, can I copy of all containers and images by copy scp'ing over /var/lib/docker/containers/, or is that not allowed? THere is probably no way to copy over images and containers except via a registry?\n. I re installed a 14.04 VM and can confirm that the key is to do:\nsudo apt-get install linux-image-extra-uname -r\nbefore installing docker. Then aufs is used, and registry will run fine too.\n. ",
    "sathlan": "Oki,\nI see your point.  I think that having some storage related function somewhere where the driver can't see them (ie mixing registry-facing and storage API) lead to some inflexibility and problems like mine :)\nIn the meantime, I have to duplicate all the _walk_storage function in my test, which is pretty dumb, but for the time being, what you're saying, is that there is no other way.\nHope your patch will come through one way or another :)\n. @dmp42\n\nIt seems to me the bug was with swift's implementation of\nlist_directory, is that correct?\n\nYes, it is.\n\nI understand the manifestation of this was visible through a call to\n_walk_directory, but then again testing the behavior of\nlist_directory would be enough, right?\n\nYes, but a test should be added with a more than one level deep file.\nI was a bit too focused on the _walk_directory function.  Here is a\ntest that trigger the error: two level deep files\nAnd I agree with both of you, it belongs to the docker-registry-core.\n@bacongobbler\n\n@sathlan told me that the issue was specific to the swift driver and\nhow the registry calls list_directory with a swift driver.\n\nThe test is not really \"specific\" to the swift driver.  It's just that\nafter reading @wking answer I felt like I could not properly test it\nfrom docker-registry.\nNow, that I have implemented it, a simple\ntest_list_directory_with_subdir is enough to trigger the error.\nAnd this could definitively be included in docker-registry-core.\nSo @dmp42, would you like me to add the test_list_directory_with_subdir \nto the registry core driver test ?  If so, @bacongobbler, I would refactor my test\nto use it.\nWhat do guys think ?\n. Yes, good to close.  Thanks for you help.\n. Yes @dmp42, I'm working on it :)\nIt's in the mock_boto.py and proceed from the same over simplification of the prefix/delimiter combo.  I cross check for s3, and the reference is there boto api which leads there aws doc.  So the mocked boto list_directory is indeed too simple.  \nThe culprit is the list function.  To make it behave would require a big refactoring.  My approach is simpler, but take the assumption that all files are one level deep with the 'test' prefix.  \nI have a working patch for the `mock_boto.py``, but I'm gonna sleep on it before pushing :) Meanwhile I put the code there if you want to have a look.\nA better solution could be to use the moto library but would require some time ...\n. As a side note, I tested the code live and there was no problem recreating the local database from an existing s3 docker registry bucket.  So the \"bug\" is only there because of the mocked function, the real thing works correctly.\n. @dmp42, I added the code to skip the test for the S3 driver.  Telll me if investigating the moto framework would be helpful.\n++\n. Got It !  Oki, it has been a bit messy.  Sorry about that.  The problem is that the whole test suit for s3 is run twice with some more tests in the test_s3.py.  I did my best to make something clean, hope it's enough.\n. That was cleaner like that :)\n. ",
    "snowsky": "I run into this problem when there are a few servers to push images into docker registry, and later pull from registry.\n. From one host, but with multiple processes.\n. @dmp42 is this a known issue?\n. I forked a few processes to push images, and every process will run at the background with &\n. No issue now, will let you know if it happens again.\n. ",
    "rawlingsj": "I've referenced this issue from the fabric8 project as a possible explanation of why we're seeing the same error on our project.\n. Thanks - in our case they are two different images but share the same base image.  It's probably something we've configured our side.\n. ",
    "philwhln": "Thanks @bacongobbler @dmp42, I'll take a look at the article.\n. ",
    "btrepp": "I was wrong, it seems pushing does take all the dependant layers. Its just building from dockerfiles that doesn't.\nThis is fine. Builds should take the latest from upstream, but I can keep copies locally. Workflow would be.\ndocker pull image/repo\ndocker tag image/repo localrepo:5000/image/repo\ndocker push localrepo:5000/image/repo\nThen all my 'offline' instances running via fleet would have docker pull localhost:5000/image/repo\n:)\n. ",
    "kerr23": "Well, upon further testing, actually it only works because it's not doing auth at all.\nI got that /_auth path from some issues that I read, but I don't think it actually does anything.\nSo it seems like auth_ldap in general is throwing it for a loop.\n. Here's debug from my docker server instance. if that's helpful at all.\n[debug] server.go:1036 Calling POST /images/{name:.*}/push\n[info] POST /images/myprivate.repo/base/push?tag=latest\n[d16b9f29] +job push(myprivate.repo/base)\n[debug] registry.go:190 Error unmarshalling the _ping RegistryInfo: json: cannot unmarshal bool into Go value of type registry.RegistryInfo\n[debug] registry.go:194 Registry version header: '0.8.1'\n[debug] registry.go:197 RegistryInfo.Version: \"0.8.1\"\n[debug] registry.go:200 Registry standalone header: 'mirror'\n[debug] registry.go:208 RegistryInfo.Standalone: %!q(bool=false)\n[debug] registry.go:190 Error unmarshalling the _ping RegistryInfo: json: cannot unmarshal bool into Go value of type registry.RegistryInfo\n[debug] registry.go:194 Registry version header: '0.8.1'\n[debug] registry.go:197 RegistryInfo.Version: \"0.8.1\"\n[debug] registry.go:200 Registry standalone header: 'mirror'\n[debug] registry.go:208 RegistryInfo.Standalone: %!q(bool=false)\n[debug] push.go:66 Local repo: map[latest:c7d7e7c65c8913eb01b22d3600dba31638a839b2023c23b5e0367b54eb208cbf]\n[debug] push.go:58 Image list: [511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158 8cbdf71a8e7fc45394ef8915da4af12a2a7ecb4e7a95f8bfea9a9614fe984235 c31865d83ea17f3fbbcb228dfb614e43129956b114cbf1cdc94291182d357f60 1b2af7d5307ae40b61fdbc27a11396a5560b1d260877664d0d80dcfecc4a7126 311ec46308da5f990e024e68b1bb0edad6fa0b486eed55d965de46281a7781f7 96e1c132acb32a45bf32735e5d90a749c1589861324843c476433dd75b83d7cf c3d5614fecc45f9f7844c5ca5fe5c1543d1aaa8c68e7235d80794023e371b778 fe970733eb2667991cd4a42ad4d7e402bc5990679ecea0d3991577dabd41ed79 96847bad22d4bf67aaab8917a3d511f4fb72352629482f0c75c32c167534c026 a7e262c4f84c3a314237d76a7cd88886a07ccb537ca363b5f3d63faff0d65f64 5a37ad3d62698b3aea6dcccd120fc2531292f4e087d69e215b58ccda92f44024 5e7195bd5d28756d60744956a9231a08c9d0d799647909c8fe228bd44a995983 94b10f7e9e369f4fe43af4b0b80519b21590d6d21c730d00c053208a6b4d7b36 8fe4650196179ab6018fcb6e8fad52ddfc5b6a4c98397c9fcc54bea909c8e9df c7d7e7c65c8913eb01b22d3600dba31638a839b2023c23b5e0367b54eb208cbf]\n[debug] push.go:59 Tags by image: map[c7d7e7c65c8913eb01b22d3600dba31638a839b2023c23b5e0367b54eb208cbf:[latest]]\n[debug] push.go:100 Preparing to push map[latest:c7d7e7c65c8913eb01b22d3600dba31638a839b2023c23b5e0367b54eb208cbf] with the following images and tags\n[debug] push.go:102 Pushing ID: 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158 with Tag:\n[debug] push.go:102 Pushing ID: 8cbdf71a8e7fc45394ef8915da4af12a2a7ecb4e7a95f8bfea9a9614fe984235 with Tag:\n[debug] push.go:102 Pushing ID: c31865d83ea17f3fbbcb228dfb614e43129956b114cbf1cdc94291182d357f60 with Tag:\n[debug] push.go:102 Pushing ID: 1b2af7d5307ae40b61fdbc27a11396a5560b1d260877664d0d80dcfecc4a7126 with Tag:\n[debug] push.go:102 Pushing ID: 311ec46308da5f990e024e68b1bb0edad6fa0b486eed55d965de46281a7781f7 with Tag:\n[debug] push.go:102 Pushing ID: 96e1c132acb32a45bf32735e5d90a749c1589861324843c476433dd75b83d7cf with Tag:\n[debug] push.go:102 Pushing ID: c3d5614fecc45f9f7844c5ca5fe5c1543d1aaa8c68e7235d80794023e371b778 with Tag:\n[debug] push.go:102 Pushing ID: fe970733eb2667991cd4a42ad4d7e402bc5990679ecea0d3991577dabd41ed79 with Tag:\n[debug] push.go:102 Pushing ID: 96847bad22d4bf67aaab8917a3d511f4fb72352629482f0c75c32c167534c026 with Tag:\n[debug] push.go:102 Pushing ID: a7e262c4f84c3a314237d76a7cd88886a07ccb537ca363b5f3d63faff0d65f64 with Tag:\n[debug] push.go:102 Pushing ID: 5a37ad3d62698b3aea6dcccd120fc2531292f4e087d69e215b58ccda92f44024 with Tag:\n[debug] push.go:102 Pushing ID: 5e7195bd5d28756d60744956a9231a08c9d0d799647909c8fe228bd44a995983 with Tag:\n[debug] push.go:102 Pushing ID: 94b10f7e9e369f4fe43af4b0b80519b21590d6d21c730d00c053208a6b4d7b36 with Tag:\n[debug] push.go:102 Pushing ID: 8fe4650196179ab6018fcb6e8fad52ddfc5b6a4c98397c9fcc54bea909c8e9df with Tag:\n[debug] push.go:102 Pushing ID: c7d7e7c65c8913eb01b22d3600dba31638a839b2023c23b5e0367b54eb208cbf with Tag: latest\n[debug] session.go:487 [registry] PUT https://myprivate.repo/v1/repositories/base/\n[debug] session.go:488 Image list pushed to index:\n[{\"id\":\"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"},{\"id\":\"8cbdf71a8e7fc45394ef8915da4af12a2a7ecb4e7a95f8bfea9a9614fe984235\"},{\"id\":\"c31865d83ea17f3fbbcb228dfb614e43129956b114cbf1cdc94291182d357f60\"},{\"id\":\"1b2af7d5307ae40b61fdbc27a11396a5560b1d260877664d0d80dcfecc4a7126\"},{\"id\":\"311ec46308da5f990e024e68b1bb0edad6fa0b486eed55d965de46281a7781f7\"},{\"id\":\"96e1c132acb32a45bf32735e5d90a749c1589861324843c476433dd75b83d7cf\"},{\"id\":\"c3d5614fecc45f9f7844c5ca5fe5c1543d1aaa8c68e7235d80794023e371b778\"},{\"id\":\"fe970733eb2667991cd4a42ad4d7e402bc5990679ecea0d3991577dabd41ed79\"},{\"id\":\"96847bad22d4bf67aaab8917a3d511f4fb72352629482f0c75c32c167534c026\"},{\"id\":\"a7e262c4f84c3a314237d76a7cd88886a07ccb537ca363b5f3d63faff0d65f64\"},{\"id\":\"5a37ad3d62698b3aea6dcccd120fc2531292f4e087d69e215b58ccda92f44024\"},{\"id\":\"5e7195bd5d28756d60744956a9231a08c9d0d799647909c8fe228bd44a995983\"},{\"id\":\"94b10f7e9e369f4fe43af4b0b80519b21590d6d21c730d00c053208a6b4d7b36\"},{\"id\":\"8fe4650196179ab6018fcb6e8fad52ddfc5b6a4c98397c9fcc54bea909c8e9df\"},{\"id\":\"c7d7e7c65c8913eb01b22d3600dba31638a839b2023c23b5e0367b54eb208cbf\",\"Tag\":\"latest\"}]\n[debug] http.go:162 https://myprivate.repo/v1/repositories/base/ -- HEADERS: map[User-Agent:[docker/1.2.0 go/go1.3.1 git-commit/fa7b24f kernel/3.16.1-tinycore64 os/linux arch/amd64]]\n[debug] session.go:538 Auth token: [Token signature=1JYUZD5A7WQHSYUW,repository=\"library/base\",access=write]\n[debug] http.go:162 https://myprivate.repo/v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json -- HEADERS: map[User-Agent:[docker/1.2.0 go/go1.3.1 git-commit/fa7b24f kernel/3.16.1-tinycore64 os/linux arch/amd64]]\n[debug] session.go:365 [registry] Calling PUT https://myprivate.repo/v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json\n[debug] http.go:162 https://myprivate.repo/v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json -- HEADERS: map[User-Agent:[docker/1.2.0 go/go1.3.1 git-commit/fa7b24f kernel/3.16.1-tinycore64 os/linux arch/amd64]]\nHTTP code 401 while uploading metadata: invalid character '<' looking for beginning of value\n[d16b9f29] -job push(myprivate.repo/base) = ERR (1)\n. Interesting. Here's some more debugging.  the GET / PUTs are generating the 401.\n2014-09-30 23:56:00,640 INFO: 172.21.225.1 - - [30/Sep/2014:23:56:00] \"PUT /v1/repositories/base/ HTTP/1.0\" 200 2 \"-\" \"docker/1.2.0 go/go1.3.1 git-commit/fa7b24f kernel/3.16.1-tinycore64 os/linux arch/amd64\"\n172.21.225.1 - - [30/Sep/2014:23:56:01 +0000] \"GET /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1\" 401 194 \"-\" \"docker/1.2.0 go/go1.3.1 git-commit/fa7b24f kernel/3.16.1-tinycore64 os/linux arch/amd64\"\n172.21.225.1 - - [30/Sep/2014:23:56:01 +0000] \"PUT /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1\" 401 194 \"-\" \"docker/1.2.0 go/go1.3.1 git-commit/fa7b24f kernel/3.16.1-tinycore64 os/linux arch/amd64\"\n. You mean the env variables for the registry?\nSETTINGS_FLAVOR=\"local\"\nSTORAGE_PATH=\"/docker/registry\"\nSEARCH_BACKEND=\"sqlalchemy\"\nDISABLE_TOKEN_AUTH=\"true\"\nMIRROR_SOURCE=\"https://registry1.docker.io\"\nMIRROR_SOURCE_INDEX=\"https://index.docker.io\"\nLOGLEVEL=\"debug\"\n. If i add the following to my nginx config stuff starts working.\nlocation /v1/images {\n    auth_ldap off;\n    proxy_pass http://docker-registry;\n  }\n. Ah bummer at the end it did a tag and i got\n172.21.225.1 - - [01/Oct/2014:00:06:14 +0000] \"PUT /v1/repositories/base/tags/latest HTTP/1.1\" 401 194 \"-\" \"docker/1.2.0 go/go1.3.1 git-commit/fa7b24f kernel/3.16.1-tinycore64 os/linux arch/amd64\"\nso /v1/images and /v1/repositories both blow up. (so i suspect that most stuff will blow up)\n. Here's my nginx config.\n```\nldap_server my_ldap {\n  url ldap://myldapstuff;\n  binddn \"cn=stuff,dc=stuff,dc=stuff\";\n  binddn_passwd \"stuff\";\ngroup_attribute memberUid;\n  group_attribute_is_dn off;\n  require valid_user;\n}\nupstream docker-registry {\n  server localhost:5000;\n}\nserver {\n  listen 80;\nproxy_set_header  Host           $http_host;   # required for docker client's sake\n  proxy_set_header X-Real-IP        $remote_addr; # pass on real client's IP\n  proxy_set_header Authorization    \"\"; # see https://github.com/dotcloud/docker-registry/issues/170\nproxy_set_header        Accept-Encoding   \"\";\n  proxy_set_header        X-Forwarded-By    $server_addr:$server_port;\n  proxy_set_header        X-Forwarded-For   $remote_addr;\n  proxy_read_timeout 900;\nclient_max_body_size 0; # disable any limits to avoid HTTP 413 for large image uploads\n# required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\n  chunked_transfer_encoding on;\nlocation / {\n    auth_ldap            \"Restricted\";\n    auth_ldap_servers    my_ldap;\n    proxy_pass           http://docker-registry;\n    proxy_read_timeout   900;\n  }\nlocation /_ping {\n    auth_ldap off;\n    proxy_pass http://docker-registry;\n  }\nlocation /v1/_ping {\n    auth_ldap off;\n    proxy_pass http://docker-registry;\n  }\n}\n```\n. Definitely willing to give that a shot. Although for 320 the error is a 504 where mine is a 401. \n. I'm terminating SSL on my load balancer. X-Forward-Proto is being set properly at that time.\n. ",
    "pyotr777": "Thank you for your answer.\nI ran pip install virtualenv:\nRequirement already satisfied (use --upgrade to upgrade): virtualenv in /Library/Python/2.7/site-packages\nCleaning up...\nI have version 1.11.6 installed:\n$ ls -l /Library/Python/2.7/site-packages/\n-rw-r--r--   1 root  wheel    119 Oct 24  2013 README\ndrwxr-xr-x   4 root  wheel    136 Sep 30 13:10 async-0.6.1-py2.7-macosx-10.9-intel.egg\ndrwxr-xr-x   7 root  wheel    238 Jun 17 18:07 beautifulsoup4-4.3.2-py2.7.egg-info\ndrwxr-xr-x  14 root  wheel    476 Jun 17 18:07 bs4\n-rw-r--r--   1 root  wheel    406 Sep 30 13:10 easy-install.pth\ndrwxr-xr-x  38 root  wheel   1292 Jun 17 18:06 pip\ndrwxr-xr-x   9 root  wheel    306 Jun 17 18:06 pip-1.5.6.dist-info\ndrwxr-xr-x  23 root  wheel    782 Sep 29 11:45 pkginfo\ndrwxr-xr-x  10 root  wheel    340 Sep 29 11:45 pkginfo-1.1-py2.7.egg-info\ndrwxr-xr-x  25 root  wheel    850 Sep 29 10:28 py\ndrwxr-xr-x   8 root  wheel    272 Sep 29 10:28 py-1.4.25-py2.7.egg-info\ndrwxr-xr-x   4 root  wheel    136 Jan 29  2014 requests-2.2.1-py2.7.egg\n-rw-r--r--   1 root  wheel  45538 Sep 30 13:10 smmap-0.8.2-py2.7.egg\ndrwxr-xr-x  25 root  wheel    850 Sep 29 10:28 tox\ndrwxr-xr-x   9 root  wheel    306 Sep 29 10:28 tox-1.8.0-py2.7.egg-info\ndrwxr-xr-x  13 root  wheel    442 Sep 29 11:45 twine\ndrwxr-xr-x   9 root  wheel    306 Sep 29 11:45 twine-1.3.1.dist-info\ndrwxr-xr-x   6 root  wheel    204 Aug  6 16:30 vboxapi\n-rw-r--r--   1 root  wheel    241 Aug  6 16:30 vboxapi-1.0-py2.7.egg-info\ndrwxr-xr-x   9 root  wheel    306 Sep 29 10:28 virtualenv-1.11.6.dist-info\n-rw-r--r--   1 root  wheel  98477 Sep 29 10:28 virtualenv.py\n-rw-r--r--   1 root  wheel  82738 Sep 29 10:28 virtualenv.pyc\ndrwxr-xr-x   6 root  wheel    204 Sep 29 10:28 virtualenv_support\n. Thanks again for your help!\nActually, I still have questions about running test.\nBut first, let me explain my development procedures. I'm not sure if it's a good way, but it works for me and maybe someone will find it useful.\nI forked docker-registry and noxiouz/docker-registry-driver-elliptics repositories on github, and created their clones on my machine (iMac). \nThen I edit backend driver, run python setup.py develop, copy new backend driver .py file to docker-registry/docker_registry/drivers/ and  start registry with \ngunicorn --access-logfile - --log-file - --debug -k gevent -b 0.0.0.0:5000 -w 1 docker_registry.wsgi:application\nIt seems enough to run python setup.py develop only once. Later just copying  new backend .py file is enough. \nI had problems accessing registry after my iMac IP changed. I had to restart docker daemon with option --insecure-registry 172.19.7.24:5000, 172.19.7.24 being my new IP.\nBy the way, I am developing a git backend for docker registry. Here is what it'll look like:\n\nNow I try to run docker registry tests. \nI run tests in a docker registry container built from slightly modified Docker file supplied with docker registry. (https://github.com/docker/docker-registry/blob/master/Dockerfile)\nI added package installation commands:\nRUN pip install nose\nRUN pip install mock\nRUN pip install GitPython\nand changed configuration file:\nENV DOCKER_REGISTRY_CONFIG /docker-registry/config/config.yml\nTo run tests I take the following steps:\nIn docker-registry folder on iMac:\nhost > docker build --rm -t reg .\nhost > docker run -ti --name reg reg /bin/bash\nand in container:\npython setup.py nosetest\nTests seem to be executed.\nBut I always see errors: \nerror: gunicorn executable not found\nand\nERROR: Failure: ImportError (No module named s3)\nI don't quite understand how these tests work.\nAlso, when I install and run tox in this registry container, I get error:\ntox.ConfigError: ConfigError: substitution env:'TOX_INCLUDE': unkown environment variable 'TOX_INCLUDE'\nI don't understand what TOX_INCLUDE and TOX_LIB should be set to.\nSorry for a lengthy post. \nI would like to continue this issue, so that to have all answers in one place.\nKind regards,\nPeter\n. > You need to run python setup.py develop only once (per repository), inside the repository(-ies) you  want to modify (after making sure you un-installed any previous version).\n\nYou then should not need to \"copy\" any file over.\n\nYou're right! I thought docker registry uses backend driver files from docker-registry/docker_registry/drivers/ , but registry works well without them.\n\nAbout starting the registry, I would really suggest you just use docker_registry instead of calling gunicorn.\n\nI used to start registry with docker-registry command, but recently it returns an error:\n```\nTraceback (most recent call last):\n  File \"/usr/local/bin/docker-registry\", line 5, in \n    from pkg_resources import load_entry_point\n  File \"build/bdist.macosx-10.9-intel/egg/pkg_resources.py\", line 2867, in \nFile \"build/bdist.macosx-10.9-intel/egg/pkg_resources.py\", line 432, in _build_master\nFile \"build/bdist.macosx-10.9-intel/egg/pkg_resources.py\", line 728, in require\n    self.platform = platform\n  File \"build/bdist.macosx-10.9-intel/egg/pkg_resources.py\", line 622, in resolve\npkg_resources.DistributionNotFound: docker-registry==0.9.0\n```\n\n\nERROR: Failure: ImportError (No module named s3)\n\nThat one means something is wrong, probably boto is not installed.\nI would have to look at your complete Dockerfile.\n\nMaybe it's my fault: I tried to remove drivers I don't use from the container, but it still seems to be referenced from somewhere.\nI appreciate your help!\nPeter\n. Greetings!\nI have questions about python packages and importing. \nI have two directories: docker-registry -- clone of docker registry, and docker-registry-driver-git -- development directory for my backend driver. \nBefore, in order to use docker registry with my driver,  I would run python setup.py develop in both directories, and docker registry would pick up the latest version of my driver from docker-registry-driver-git/docker_registry/drivers/gitdriver.py. \nThen I installed a new python version with brew and had to reinstall all packages. \nNow after I run \nbash\n~/work/docker/docker-registry > python setup.py develop\n(in docker-registry directory), my driver can be used as:\npython\nimport docker_registry.drivers.gitdriver\nAnd the source file used by registry for my driver is docker-registry/docker_registry/drivers/gitdriver.py -- not from my driver development directory (which is docker-registry-driver-git).\nIt seems to be correct behaviour of python importing system.\nThe question is, how can I make python to use source file from my development directory (docker-registry-driver-git/docker_registry/drivers/gitdriver.py)?\nIf I run \nbash\n~/work/docker/docker-registry-driver-git > python setup.py develop\n...\nFinished processing dependencies for gitdriver==0.7.103\nThe package seems to be installed, by I cannot import it:\n``` bash\n~/work/docker/docker-registry-driver-git > pip list | grep gitdriver\ngitdriver (0.7.103, /Users/peterbryzgalov/work/docker/docker-registry-driver-git)\n~/work/docker/docker-registry-driver-git > python\nPython 2.7.9 (default, Jan  3 2015, 14:21:36)\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.54)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport gitdriver\nTraceback (most recent call last):\n  File \"\", line 1, in \nImportError: No module named gitdriver\n``\n. Thank you! \nI have removeddocker-registry/docker_registry/drivers/gitdriver.pyand could install my driver by runningpython setup.py develop` in my driver top directory.\n\n\n\nNow I want to test my driver. When I test my driver with nosetests, first I copy my driver directory inside a new folder inside docker registry directory, then I build a docker image with Dockerfile and copy docker registry folder into the image.\nAfter container is started I install my driver from its folder inside docker registry directory with python setup.py install and run nosetests with python setup.py nosetests. Copying driver directory into registry directory looks awkward to me, but I see no other way to get my driver inside docker image. \nWhat is the right procedure for testing a driver?\nRegards,\nPeter\n. I ran nosetests as you suggested. \nI get errors, but many of them seem to be irrelevant to my driver. In particular, I get lots of these:\nbash\nTraceback (most recent call last):\n  File \"/Library/Python/2.7/site-packages/nose/case.py\", line 381, in setUp\n    try_run(self.inst, ('setup', 'setUp'))\n  File \"/Library/Python/2.7/site-packages/nose/util.py\", line 470, in try_run\n    return func()\n  File \"/usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/docker_registry/testing/driver.py\", line 40, in setUp\n    storage = driver.fetch(self.scheme)\n  File \"/usr/local/Cellar/python/2.7.9/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/docker_registry/core/driver.py\", line 269, in fetch\n    module.Storage.scheme = name\nAttributeError: 'module' object has no attribute 'Storage'\nMy driver was used during the tests: I can see its output and some errors referenced to my source file. \nOn the last lines of the output I see coverage summary for docker registry modules(?) , but my driver is not there. \nHow can I tell that my driver is OK and can be distributed?\n. Moved my last question to a new issue.\nhttps://github.com/docker/docker-registry/issues/919\n. Greetings!\nNow there is only one failure when I run nosetests:\n```\nFAIL: test_nginx_accel_redirect_layer (tests.test_images.TestImages)\n\nTraceback (most recent call last):\n  File \"/Users/peterbryzgalov/work/docker/docker-registry/tests/test_images.py\", line 30, in test_nginx_accel_redirect_layer\n    'Store must be LocalStorage')\nAssertionError: Store must be LocalStorage\n```\nIn the test code we have:\nself.assertTrue(images.store.scheme == 'file',\n                        'Store must be LocalStorage')\nSo this tests if the storage scheme name is \"file\". My scheme name is different: \"gitdriver\".\nI still have questions about testing and distributing my driver.\nI run nosetests in docker-registry folder.\nIn my driver folder I run flake8. I formatted my code, so now it gives no errors or warnings.\nhttps://github.com/docker/docker-registry/blob/master/CONTRIBUTING.md suggests that I also run tox. \nWhen I run tox in docker-registry folder I get the following error and summary:\n``` bash\nERROR: InvocationError: '/Users/peterbryzgalov/work/docker/docker-registry/.tox/flake8/bin/flake8 /Users/peterbryzgalov/work/docker/docker-registry'\n...\nRan 128 tests in 5.328s\nOK\n_ summary __\nERROR:   flake8: commands failed\n  py26: commands succeeded\n  py27: commands succeeded\n```\nFile .tox/flake8/bin/flake8 exists. \n- Do I have to do anything about tox-flake8 error?\n- When I finished tests, how do I distribute my driver?\nI submitted my package to pypi.python.org and it got registered.\nBut even if it is possible to install it with pip install, how are users supposed to use it?\nDocker registry is run within a container from Docker hub, so I guess it is necessary to include my driver into this image, so that users can do:\nbash\ndocker run -e SETTINGS_FLAVOR=gitdriver -p 5000:5000 registry\n- Is it possible to include gitdriver into registry image?\nRegards,\nPeter\n. Thank you!\n. --preload doesn't help.\ndocker-registry.db file is created by has 0 size. \n. Thank you for your help!\nYes, I work on OSX. \nInstalling registry with sudo pip install . and starting with sudo doesn't help. \nDatabase file was under my home directory. Changing it to default /tmp didn't help either.\nI also have two docker-registry images in boot2docker.  One of them DB error when registry starts in docker container. \nThe image I created before works fine, so I tried copying config.yml file into a docker container with broken registry and changing configuration to default driver. Neither of this helped.\nBefore restarting computer I was installing Symantec Endpoint Protection, so I wonder if it can be related to the errors? \nAfter restart I had to repair disk permissions with Mac OS Disk Utility. Some errors has been corrected, but registry still doesn't work. \nI didn't tried installing python with brew yet. But since the same error occurs inside containers, python version should not matter? \nDoes installing python with brew involves configuration or it works right after install command?\n. Manually creating a table with the same db file on OS X works:\n```\n~/work/docker-registry > python\nPython 2.7.6 (default, Sep  9 2014, 15:04:36)\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.39)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\nimport sqlalchemy\nengine=sqlalchemy.create_engine('sqlite:////tmp/docker-registry.db')\ncon=engine.connect()\nresult = con.execute(\"CREATE TABLE p (id INTEGER PRIMARY KEY ASC, name varchar(250) NOT NULL)\")\ninspector=sqlalchemy.inspect(engine)\nprint inspector.get_table_names()\n[u'p']\n```\n\n\n\nCheckouting registry from git repository to an earlier version that did work before doesn't help.\n. Happy New Year!\nWhat do you mean by \"from scratch\"? To remove local registry folder and copy again? Or go through installation process including installing python libraries etc? Do I need to uninstall all packages first?\nI tried to install registry in a docker container with Ubuntu. I mounted two folders: with docker registry and with my driver into container. After running the following commands registry seem to work:\n1  apt-get update\n2  apt-get install python-dev liblzma-dev libevent1-dev\n3  pip install /docker-registry/depends/docker-registry-core\n4  pip install file:///docker-registry#egg=docker-registry[bugsnag,newrelic,cors]\n5  export DOCKER_REGISTRY_CONFIG=/docker-registry/config/config.yml\n6  export SETTINGS_FLAVOR=dev\n7  cd /docker-registry-driver-git/\n8  pip install .\n7 and 8 are for installing my driver. \nI couldn't install python on OS X with brew. I get a checksum error:\n```\nbrew install python\n==> Downloading http://www.python.org/ftp/python/2.7.7/Python-2.7.7.tgz\n################################################################## 100.0%\n==> ./configure --prefix=/usr/local/Cellar/python/2.7.7_1 --enable-ipv6 --datarootdir=/usr/local/Cellar/python/2.7.7_1/share --datadir=/\n==> make\n==> make install PYTHONAPPSDIR=/usr/local/Cellar/python/2.7.7_1\n==> make frameworkinstallextras PYTHONAPPSDIR=/usr/local/Cellar/python/2.7.7_1/share/python\n==> Downloading https://pypi.python.org/packages/source/s/setuptools/setuptools-4.0.1.tar.gz\n################################################################## 100.0%\nError: SHA1 mismatch\nExpected: a43549f4a01f314bf54567628f8de7d1c03d5930\nActual: 6d417376509eee44c1da34692fb5d805fd2915c6\nArchive: /Library/Caches/Homebrew/python--setuptools-4.0.1.tar.gz\nTo retry an incomplete download, remove the file above.\n```\nI even removed all files in  /Library/Caches/Homebrew, but it didn't help.\n. Thank you for your attention!\nUpdated brew and installed python 2.7.9 on OS X.\nReinstalled registry and backend driver with sudo python setup.py develop.\nBy the way, as for the difference between python setup.py install and python setup.py develop. I suppose, the latter creates a link to the current directory (with source code), whereas the former copies an egg or the whole directory with the source code to a python library folder. Am I correct?\nThe problem with a database persists:\n``` bash\n$ sudo ./registry-start\n2015-01-08 11:44:45 [73530] [INFO] Starting gunicorn 19.1.0\n2015-01-08 11:44:45 [73530] [INFO] Listening at: http://0.0.0.0:5000 (73530)\n2015-01-08 11:44:45 [73530] [INFO] Using worker: gevent\n2015-01-08 11:44:45 [73533] [INFO] Booting worker with pid: 73533\n2015-01-08 11:44:45 [73530] [INFO] 1 workers\n08/Jan/2015:11:44:45 +0900 WARNING: Cache storage disabled!\n08/Jan/2015:11:44:45 +0900 WARNING: LRU cache disabled!\n08/Jan/2015:11:44:45 +0900 DEBUG: Will return docker-registry.drivers.gitdriver.Storage\n08/Jan/2015:11:44:45 +0900 INFO: Git backend driver 0.7.102 initialisation\n08/Jan/2015:11:44:48 +0900 WARNING: DB is disconnected. Reconnect to it.\n2015-01-08 11:44:48,061 ERROR: Exception on /v1/search [GET]\nTraceback (most recent call last):\n  File \"/Library/Python/2.7/site-packages/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1817, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/Library/Python/2.7/site-packages/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1477, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/Library/Python/2.7/site-packages/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1381, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/Library/Python/2.7/site-packages/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1475, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/Library/Python/2.7/site-packages/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1461, in dispatch_request\n    return self.view_functionsrule.endpoint\n  File \"/Library/Python/2.7/site-packages/docker_registry-0.9.0-py2.7.egg/docker_registry/lib/mirroring.py\", line 145, in wrapper\n    resp = f(*args, kwargs)\n  File \"/Library/Python/2.7/site-packages/docker_registry-0.9.0-py2.7.egg/docker_registry/search.py\", line 21, in get_search\n    results = INDEX.results(search_term=search_term)\n  File \"/Library/Python/2.7/site-packages/docker_registry-0.9.0-py2.7.egg/docker_registry/lib/index/db.py\", line 64, in _retry\n    raise e\nOperationalError: (OperationalError) no such table: repository u'SELECT repository.id AS repository_id, repository.name AS repository_name, repository.description AS repository_description \\nFROM repository' ()\n08/Jan/2015:11:44:48 +0900 ERROR: Exception on /v1/search [GET]\nTraceback (most recent call last):\n  File \"/Library/Python/2.7/site-packages/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1817, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/Library/Python/2.7/site-packages/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1477, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/Library/Python/2.7/site-packages/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1381, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/Library/Python/2.7/site-packages/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1475, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/Library/Python/2.7/site-packages/Flask-0.10.1-py2.7.egg/flask/app.py\", line 1461, in dispatch_request\n    return self.view_functionsrule.endpoint\n  File \"/Library/Python/2.7/site-packages/docker_registry-0.9.0-py2.7.egg/docker_registry/lib/mirroring.py\", line 145, in wrapper\n    resp = f(*args, kwargs)\n  File \"/Library/Python/2.7/site-packages/docker_registry-0.9.0-py2.7.egg/docker_registry/search.py\", line 21, in get_search\n    results = INDEX.results(search_term=search_term)\n  File \"/Library/Python/2.7/site-packages/docker_registry-0.9.0-py2.7.egg/docker_registry/lib/index/db.py\", line 64, in _retry\n    raise e\nOperationalError: (OperationalError) no such table: repository u'SELECT repository.id AS repository_id, repository.name AS repository_name, repository.description AS repository_description \\nFROM repository' ()\n127.0.0.1 - - [08/Jan/2015:11:44:48 +0900] \"GET /v1/search HTTP/1.1\" 500 291 \"-\" \"curl/7.37.1\"\n2015-01-08 11:45:15 [73530] [INFO] 1 workers\n```\nin registry-start I have:\ngunicorn --access-logfile - --log-file - --debug -k gevent -b 0.0.0.0:5000 -w 1 docker_registry.wsgi:application\nI also noticed that there is no file.py anymore in drivers directory of the current docker-registry on github. Where did it go?\nAnd in s3.py I see:\npython\nfrom docker_registry.core import compat\nbut there is no core directory anymore. How does this import works?\n. I removed old modules in /Library/Python/2.7 and reinstalled them with pip install.\nStill having errors:\nbash\n[2015-01-09 11:05:25 +0900] [85319] [INFO] Starting gunicorn 19.1.1\n[2015-01-09 11:05:25 +0900] [85319] [INFO] Listening at: http://0.0.0.0:5000 (85319)\n[2015-01-09 11:05:25 +0900] [85319] [INFO] Using worker: gevent\n[2015-01-09 11:05:25 +0900] [85322] [INFO] Booting worker with pid: 85322\n09/Jan/2015:11:05:25 +0900 WARNING: Cache storage disabled!\n09/Jan/2015:11:05:25 +0900 WARNING: LRU cache disabled!\n09/Jan/2015:11:05:25 +0900 DEBUG: Will return docker-registry.drivers.gitdriver.Storage\n09/Jan/2015:11:05:25 +0900 INFO: Git backend driver 0.7.102 initialisation\n09/Jan/2015:11:05:29 +0900 WARNING: DB is disconnected. Reconnect to it.\n2015-01-09 11:05:29,242 ERROR: Exception on /v1/search [GET]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/site-packages/flask/app.py\", line 1817, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/site-packages/flask/app.py\", line 1477, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/site-packages/flask/app.py\", line 1381, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/usr/local/lib/python2.7/site-packages/flask/app.py\", line 1475, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/site-packages/flask/app.py\", line 1461, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/Users/peterbryzgalov/work/docker-registry/docker_registry/lib/mirroring.py\", line 145, in wrapper\n    resp = f(*args, **kwargs)\n  File \"/Users/peterbryzgalov/work/docker-registry/docker_registry/search.py\", line 21, in get_search\n    results = INDEX.results(search_term=search_term)\n  File \"/Users/peterbryzgalov/work/docker-registry/docker_registry/lib/index/db.py\", line 64, in _retry\n    raise e\nOperationalError: (OperationalError) no such table: repository u'SELECT repository.id AS repository_id, repository.name AS repository_name, repository.description AS repository_description \\nFROM repository' ()\nPython version:\n``` python\n~/work/docker-registry > python\nPython 2.7.9 (default, Jan  3 2015, 14:21:36)\n[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.54)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n\n\n\n```\n. I'm running a forked version: https://github.com/pyotr777/docker-registry\nShould I abandon my fork and use master instead?\n. Cloned docker-registry from https://github.com/docker/docker-registry, but cannot install it.\n\n\n\nRunning python setup.py develop or pip install . fails:\n``` bash\n~/work/docker-registry-master > python setup.py develop\nrunning develop\nrunning egg_info\nwriting requirements to docker_registry.egg-info/requires.txt\nwriting docker_registry.egg-info/PKG-INFO\nwriting namespace_packages to docker_registry.egg-info/namespace_packages.txt\nwriting top-level names to docker_registry.egg-info/top_level.txt\nwriting dependency_links to docker_registry.egg-info/dependency_links.txt\nwriting entry points to docker_registry.egg-info/entry_points.txt\nreading manifest file 'docker_registry.egg-info/SOURCES.txt'\nreading manifest template 'MANIFEST.in'\nwriting manifest file 'docker_registry.egg-info/SOURCES.txt'\nrunning build_ext\nCreating /usr/local/lib/python2.7/site-packages/docker-registry.egg-link (link to .)\ndocker-registry 1.0.0-dev is already the active version in easy-install.pth\nInstalling docker-registry script to /usr/local/bin\nInstalled /Users/peterbryzgalov/work/docker-registry-master\nProcessing dependencies for docker-registry==1.0.0-dev\nSearching for M2Crypto==0.22.3\nReading https://pypi.python.org/simple/M2Crypto/\nBest match: M2Crypto 0.22.3\nDownloading https://pypi.python.org/packages/source/M/M2Crypto/M2Crypto-0.22.3.tar.gz#md5=573f21aaac7d5c9549798e72ffcefedd\nProcessing M2Crypto-0.22.3.tar.gz\nWriting /var/folders/ym/nhg3k9pd693gbj_x1vpm0ml80000gn/T/easy_install-h5F6e2/M2Crypto-0.22.3/setup.cfg\nRunning M2Crypto-0.22.3/setup.py -q bdist_egg --dist-dir /var/folders/ym/nhg3k9pd693gbj_x1vpm0ml80000gn/T/easy_install-h5F6e2/M2Crypto-0.22.3/egg-dist-tmp-8tK9j9\nSWIG/_m2crypto.i:30: Error: Unable to find 'openssl/opensslv.h'\nSWIG/_m2crypto.i:33: Error: Unable to find 'openssl/safestack.h'\nSWIG/_evp.i:12: Error: Unable to find 'openssl/opensslconf.h'\nSWIG/_ec.i:7: Error: Unable to find 'openssl/opensslconf.h'\nerror: Setup script exited with error: command 'swig' failed with exit status 1\n```\n. I found similar issue here: https://github.com/Homebrew/homebrew/issues/33552\nRunning xcode-select --install helped. I could install registry with:\nbash\n~/work/docker-registry-master > python setup.py develop\nI had some warnings:\nSWIG/_bio.i:64: Warning 454: Setting a pointer/reference variable may leak memory.\nSWIG/_rand.i:21: Warning 454: Setting a pointer/reference variable may leak memory.\nSWIG/_evp.i:160: Warning 454: Setting a pointer/reference variable may leak memory.\nSWIG/_dh.i:36: Warning 454: Setting a pointer/reference variable may leak memory.\nSWIG/_rsa.i:43: Warning 454: Setting a pointer/reference variable may leak memory.\nSWIG/_dsa.i:31: Warning 454: Setting a pointer/reference variable may leak memory.\nSWIG/_ssl.i:212: Warning 454: Setting a pointer/reference variable may leak memory.\nSWIG/_x509.i:323: Warning 454: Setting a pointer/reference variable may leak memory.\nSWIG/_pkcs7.i:44: Warning 454: Setting a pointer/reference variable may leak memory.\nSWIG/_pkcs7.i:44: Warning 454: Setting a pointer/reference variable may leak memory.\nSWIG/_util.i:11: Warning 454: Setting a pointer/reference variable may leak memory.\nSWIG/_ec.i:111: Warning 454: Setting a pointer/reference variable may leak memory.\nSWIG/_engine.i:168: Warning 454: Setting a pointer/reference variable may leak memory.\nbut registry got installed and is working now!\nbash\n~/work/docker-registry-master > ./registry-start\n[2015-01-09 15:31:44 +0900] [93448] [INFO] Starting gunicorn 19.1.1\n[2015-01-09 15:31:44 +0900] [93448] [INFO] Listening at: http://0.0.0.0:5000 (93448)\n[2015-01-09 15:31:44 +0900] [93448] [INFO] Using worker: gevent\n[2015-01-09 15:31:44 +0900] [93451] [INFO] Booting worker with pid: 93451\n09/Jan/2015:15:31:44 +0900 WARNING: Cache storage disabled!\n09/Jan/2015:15:31:44 +0900 WARNING: LRU cache disabled!\n09/Jan/2015:15:31:44 +0900 DEBUG: Will return docker-registry.drivers.gitdriver.Storage\n09/Jan/2015:15:31:44 +0900 INFO: Git backend driver 0.7.102 initialisation\ngitcom: <git.cmd.Git object at 0x109128730>\nList repositories\n127.0.0.1 - - [09/Jan/2015:15:31:57 +0900] \"GET /v1/search HTTP/1.1\" 200 46 \"-\" \"curl/7.37.1\"\nI shall test my backend driver with this registry version. \nI have questions about nose tests, and shall post them later.\nI appreciate your help! \n. Removed unnecessary files and the errors have gone.\n. ",
    "bobrik": "@dmp42 pushing 0.9 RC to docker hub could help with testing.\n. I'd love to see some kind of p2p image distribution. We have images that are over 1gb in size and need to distribute them to hundreds of servers many times per day. 1gb x 100 servers is 100gb is 800gbit and it would take 800 seconds with saturated 1gb link. That is too much. Even with 3 x 10g links it would take ~30 seconds.\nImage sizes could be improved, though, but you see where it goes.\n. Actually, p2p distribution could be solved by having local registry on each machine + /etc/hosts changes to point docker to local registry.\nMetadata requests and uploads are proxied to main registry without any changes, but GET requests for layers download layers via torrent/btsync before sending them to docker.\nThis is pretty easy to implement and could act as a local cache for main docker registry itself.\n. @dmp42 yep, that looks like mirroring, but current mirroring won't help me to download 1gb images to hundreds of machines faster, while p2p can do that.\nAlso, current mirroring doesn't seem to work with local registries (I might be wrong about that), proxy that I described could be placed in front of any registry.\n. @dmp42 I'm not sure how to make p2p storage driver without client support. If client sticks to http(s), then I see no other way than having proxy on each node for p2p.\nMaybe I'm just missing something. Can you explain how p2p storage driver should work?\n. I made POC for doing p2p docker pull: https://github.com/bobrik/bay\n. As mentioned in docker/docker#9161, there should be a way to enable/disable mirroring for registries. You probably don't want to mirror your own registry that is accessible over 10G network, but want to do so with docker hub and registries hosted by other people.\n. Another thing I'd love to see explained is how mirroring with one storage (like s3) and many registries (like 5 processes on different nodes for HA and scalability) works.\n. Why is it a docker bug? I thought registry downloads images before giving them to docker.\n[docker hub] -> [mirror] -> [docker], am I right?\n. ",
    "g-ovidiupl": "Greetings from Google Kirkland! Not sure this is the best place for a minor suggestion, but I'd be happy to write up a more detailed proposal separately. It might be worth adding an extra knob to the client-server protocol for handling overload and transient failures.\nI believe the current client-side logic uses linear retries up to a max number of failures. First, I and several others would be really happy to see that logic evolve into exponential back-off with jitter for timeouts, disconnects and other transient failures. Ideally, that logic should be \"no back-off on 302, use exponential back-off on 500, 502 and 503\".\nSecond, we'd be really happy if the registry responded with a Retry-After header on 503 (and possibly on 3XX, if it chooses so), and if the client honored the value given in the header.\nThe goal is to avoid self-inflicted denial-of-service states, where Docker clients in a large-scale deployment synchronize their retries after a transient issue (e.g. temporary network failure) and slam registries at the same time. With friendly clients, randomized exponential back-off is the first line of defense, and server-controlled retry delays are the bigger hammer. (With unfriendly clients, there's always the other first line of defense :) ).\n. ",
    "nathanleclaire": "Just a random thought I've had lately: whatever form the v2 registry and mechanics around it take, it'd be really lovely to get rid of the round-tripping messages for \"image layer already pushed, skipping\".  This seems to slow down pushes a ton and, though I'm sure the decision to do it that way was probably made for the right reasons at the time, baffles every single person that experiences it for the first time (\"why can't it check them all at once?\").\n. ",
    "coolbrg": ":+1: from my side for next generation docker registry :smile: \nLooking forward to it.\n. Hi Folks,\nI would like to contribute to the next-generation docker registry as little it may be. \nAlthough, I appreciate the work done in the v1 registry written in Python but equally agreed with @dmp42 decision to rewrite it in Go.\nAnything to prepare myself before coming to IRC meeting?\n. ",
    "progrium": "I spend a lot of time on these things. Here is one system that lets you do native Go plugins, JavaScript plugins, or other language backends:\nhttps://github.com/progrium/go-plugins\nLong term I'm trying to get https://github.com/progrium/duplex (eventually on top of libchan) to implement the protocol for go-plugins so that you can use the same system for remote extensions. And then I would also like to wrap this up for local plugins in the same way mitchell has architected, but in a more general way. Would love to try and collaborate on any of this!\n. ",
    "rlister": "Just as a fun data-point, I started to run into this problem right around 1700 tags for a repo, backed with S3. We are doing continuous builds, so I am able to workaround by periodically deleting large repos and re-pushing as needed.\n. ",
    "duyanghao": "I have the similar problems @bshi @dmp42 .my storage backend is s3-ceph,So has this problem been solved yet? urgently!!!\ni guess it is the problem of api \"/v1/repositories/repo/tags\", it is using python gevent to pull all tags file from storage backend,read it and return to the docker.it is using too much time!\nMaybe it exists someway to achieve that api with more efficiency,i am trying to do that!\n. ",
    "prune998": "can you explain me point between the docker command line on one host sending image to docker-registry on another host ? When is docker daemon involved ? \nI tried setting debug on docker-registry with almost no luck... I see my commands coming in but that's it...\non the sending host : \ndocker version\nClient version: 1.2.0\nClient API version: 1.14\nGo version (client): go1.3.1\nGit commit (client): fa7b24f\nOS/Arch (client): linux/amd64\nServer version: 1.2.0\nServer API version: 1.14\nGo version (server): go1.3.1\nGit commit (server): fa7b24f\non the receiving side : \ndocker_registry-0.8.1\ndocker_registry_core-2.0.2\n. gor the logs : \n```\n[debug] session.go:487 [registry] PUT http://peach:5000/v1/repositories/frontend-production-consumerwall_0.0.20/\n[debug] session.go:488 Image list pushed to index:\n[{\"id\":\"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"},{\"id\":\"1c9383292a8ff4c4196ff4ffa36e5ff24cb217606a8d1f471f4ad27c4690e290\"},{\"id\":\"9942dd43ff211ba917d03637006a83934e847c003bef900e4808be8021dca7bd\"},{\"id\":\"d92c3c92fa73ba974eb409217bb86d8317b0727f42b73ef5a05153b729aaf96b\"},{\"id\":\"0ea0d582fd9027540c1f50c7f0149b237ed483d2b95ac8d107f9db5a912b4240\"},{\"id\":\"cc58e55aa5a53b572f3b9009eb07e50989553b95a1545a27dcec830939892dba\"},{\"id\":\"c4ff7513909dedf4ddf3a450aea68cd817c42e698ebccf54755973576525c416\"},{\"id\":\"f492398adec1473b3ee2ffecf059b8e9856634d5ed84a533adb6c8a926fa8c50\"},{\"id\":\"0e69062a54fcda7bf54cc71331e3f360fd2972cc735f097d671d1ef384f3850c\"},{\"id\":\"99c76845493998a2fd216b06cd51a18b2d93d87cc7f577abc54fc70c5a9abd0b\"},{\"id\":\"0a727da53cca8b16ea54353f5ae41b1b311cfe8a45051f970c8575d96ed385c5\"},{\"id\":\"ced79d616e0af24e349ad4276d99d1e743fe59696047b8720ae2c1f2aa98152e\"},{\"id\":\"e5e29aa0731af63307d79bc49817f65bfaf0fd5095580f2b0e9417877d5dd406\"},{\"id\":\"6c31ece89d3d6a767904eacf7ebc7adbfd531f6969662087f66d923b4d1cd8e6\"},{\"id\":\"9fbd4bb6bde8ec4b1c18fb5bafd93a3c6560adbcf9c8f7d819effbc6ddc653de\"},{\"id\":\"2734658b8dcdaf85f1410d2b9c429f479b2bc56f2422bc0c98077c399a74e850\"},{\"id\":\"6454cabf077d163f39aa9f0eb83bf39a5411306a4cf6e3de6f18c11026b946e2\"},{\"id\":\"4883b61a9562e7d369a12e77f626bfe5825d759c820c0b919dba2e3ab99d1032\"},{\"id\":\"bd86152386c5dad9ade4b19acdf007bd1f4a26f5de28ecd8ec25d8e75f12d8e5\"},{\"id\":\"ed120b903df7b12c7ee208d96b09dc2879fbcdeeb3ec8db15c16227c3dda925c\"},{\"id\":\"d458dd2aaec288cda61517f02ee4ef9ce90db48a34a4d16f5e4e891dc3115b87\"},{\"id\":\"1d8869be43a3227e92d6d27242b065b667ad3208069ab0ea2b383a71ccb956cf\"},{\"id\":\"aea4a3321c18685a9c7ce888bd795bbafaf780ab78498da171d7cffd41568cc1\"},{\"id\":\"f006c2340a938d0546553dc7470b1e0787506a4185e5ad7c4191e6916d3f4d4b\"},{\"id\":\"8e7cad8bd28aaf95c71283d300ee41fda06428c227a841963d876688cb33551b\"},{\"id\":\"4c7eee7b036c45a8f3d6da0967b3c0762887da9711f60fc71bd85ee76e396df1\"},{\"id\":\"3cdbf741dacb2da1592fd5594fa6bdb7288d989b97900569f1bf16c988b944ce\"},{\"id\":\"475d95cdd16e127e60bbdec1d80f3e0fcb8192c66d6ead6a85d2f3c0584cb102\"},{\"id\":\"1bfb8264a1f74bbfe5ec6b753e89d10bc38e2e8c6cbc6b129c33ceb058bd46e9\"},{\"id\":\"869e65eb80daf6003af773cd8db520ed7ce6792d5708182f6643a707dc79ccf1\"},{\"id\":\"fad4e60f4ac6584de4302d6d84ccb43d5ae9c5bcbb31a90e1abc12ca284b7001\"},{\"id\":\"31e7b002cdc8ddce70c5c5251b33928809102719a1b8d8d7f2e96ee3ab274446\"},{\"id\":\"32560250502d943a3e7c5a0a6942a0f6d35a7612d6ac1496ab1a14617bc2d457\"},{\"id\":\"59facb3f25c371adc0e42522d18501c844371cb10a0eba6bc6c81f895492dd3c\",\"Tag\":\"latest\"}]\n[debug] http.go:162 http://peach:5000/v1/repositories/frontend-production-consumerwall_0.0.20/ -- HEADERS: map[User-Agent:[docker/1.2.0 go/go1.3.1 git-commit/fa7b24f kernel/3.13.0-34-generic os/linux arch/amd64]]\nError: Status 405 trying to push repository frontend-production-consumerwall_0.0.20: <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n405 Method Not Allowed\nMethod Not Allowed\nThe method is not allowed for the requested URL.\n[75dad000] -job push(peach:5000/frontend-production-consumerwall_0.0.20) = ERR (1)\n```\n. the same client can push the image to another registry that I haven't updated yet.\nOn the failing registry, I tried re-installing docker-registry and dependencies in another virtualenv but I have the same issue.\nmy config file is like : \n```\n\nThe `common' part is automatically included (and possibly overriden by all\nother flavors)\ncommon:\n    loglevel: debug\n    debug: true\n    standalone: true\n    # Set a random string here\n    secret_key: changeme\n    search_backend: sqlalchemy\n    sqlalchemy_index_database: sqlite:///opt/data/docker-registry/docker-registry.db\nemail_exceptions:\n    smtp_host: localhost\n    smtp_port: 25\n    #smtp_login: _env:SMTP_LOGIN\n    #smtp_password: _env:SMTP_PASSWORD\n    smtp_secure: false\n    from_addr: docker-registry@localdomain.local\n    to_addr: prune@localdomain.local\n\nThis is the default configuration when no flavor is specified\nprod:\n    storage: local\n    #storage: file\n    storage_path: /opt/data/docker-images2\n    secret_key: changeme\n    loglevel: debug\n\n```\n. Ok, this is not clear at all in the docs (even if all this appear in the sample config file... it certainely would require some more docs).\nNow I have another issue with sqlalchemy  : \n[2014-10-09 14:28:57 +0000] [113944] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/opt/data/apps/docker-registry/local/lib/python2.7/site-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/opt/data/apps/docker-registry/local/lib/python2.7/site-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/opt/data/apps/docker-registry/local/lib/python2.7/site-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/opt/data/apps/docker-registry/local/lib/python2.7/site-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/opt/data/apps/docker-registry/local/lib/python2.7/site-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/opt/data/apps/docker-registry/local/lib/python2.7/site-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/opt/data/apps/docker-registry/local/lib/python2.7/site-packages/gunicorn/util.py\", line 356, in import_app\n    __import__(module)\n  File \"/opt/data/apps/docker-registry/local/lib/python2.7/site-packages/docker_registry/wsgi.py\", line 17, in <module>\n    from .run import app\n  File \"/opt/data/apps/docker-registry/local/lib/python2.7/site-packages/docker_registry/run.py\", line 23, in <module>\n    from .search import *  # noqa\n  File \"/opt/data/apps/docker-registry/local/lib/python2.7/site-packages/docker_registry/search.py\", line 17, in <module>\n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/opt/data/apps/docker-registry/local/lib/python2.7/site-packages/docker_registry/lib/index/__init__.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/opt/data/apps/docker-registry/local/lib/python2.7/site-packages/docker_registry/lib/index/db.py\", line 57, in __init__\n    self._engine = sqlalchemy.create_engine(database)\n  File \"/opt/data/apps/docker-registry/local/lib/python2.7/site-packages/sqlalchemy/engine/__init__.py\", line 346, in create_engine\n    return strategy.create(*args, **kwargs)\n  File \"/opt/data/apps/docker-registry/local/lib/python2.7/site-packages/sqlalchemy/engine/strategies.py\", line 51, in create\n    dialect_cls = u.get_dialect()\nAttributeError: 'NoneType' object has no attribute 'get_dialect'\n. If I completely disable the search backend it's OK.\nThe error above was due of a lack of 'sqlalchemy_index_database' that I commented out.\nadding it again gave another set of errors : \nconfig : \nsearch_backend: sqlalchemy\n    sqlalchemy_index_database: 'sqlite:///opt/data/docker-registry/docker-registry.db'\nfile (I made a touch to be sure the file exists, but I have the same error if I remove the empty file) : \nls -l  /opt/data/docker-registry/docker-registry.db\n-rw-rw-r-- 1 belisarius belisarius 0 Oct  9 15:50 /opt/data/docker-registry/docker-registry.db\nerror : \n2014-10-09 16:25:51 [141792] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/gunicorn/arbiter.py\", line 495, in spawn_worker\n    worker.init_process()\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/gunicorn/workers/base.py\", line 106, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/gunicorn/app/base.py\", line 114, in wsgi\n    self.callable = self.load()\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/gunicorn/app/wsgiapp.py\", line 62, in load\n    return self.load_wsgiapp()\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/gunicorn/app/wsgiapp.py\", line 49, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/gunicorn/util.py\", line 354, in import_app\n    __import__(module)\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/docker_registry/wsgi.py\", line 17, in <module>\n    from .run import app\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/docker_registry/run.py\", line 23, in <module>\n    from .search import *  # noqa\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/docker_registry/search.py\", line 17, in <module>\n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/docker_registry/lib/index/__init__.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/docker_registry/lib/index/db.py\", line 60, in __init__\n    self._setup_database()\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/docker_registry/lib/index/db.py\", line 75, in _setup_database\n    self._generate_index(session=session)\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/docker_registry/lib/index/db.py\", line 80, in _generate_index\n    Base.metadata.create_all(self._engine)\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/sqlalchemy/sql/schema.py\", line 3291, in create_all\n    tables=tables)\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1546, in _run_visitor\n    with self._optional_conn_ctx_manager(connection) as conn:\n  File \"/usr/lib/python2.7/contextlib.py\", line 17, in __enter__\n    return self.gen.next()\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1539, in _optional_conn_ctx_manager\n    with self.contextual_connect() as conn:\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py\", line 1729, in contextual_connect\n    self.pool.connect(),\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/sqlalchemy/pool.py\", line 332, in connect\n    return _ConnectionFairy._checkout(self)\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/sqlalchemy/pool.py\", line 626, in _checkout\n    fairy = _ConnectionRecord.checkout(pool)\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/sqlalchemy/pool.py\", line 433, in checkout\n    rec = pool._do_get()\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/sqlalchemy/pool.py\", line 1038, in _do_get\n    return self._create_connection()\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/sqlalchemy/pool.py\", line 278, in _create_connection\n    return _ConnectionRecord(self)\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/sqlalchemy/pool.py\", line 404, in __init__\n    self.connection = self.__connect()\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/sqlalchemy/pool.py\", line 527, in __connect\n    connection = self.__pool._creator()\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/sqlalchemy/engine/strategies.py\", line 95, in connect\n    connection_invalidated=invalidated\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/sqlalchemy/util/compat.py\", line 185, in raise_from_cause\n    reraise(type(exception), exception, tb=exc_tb)\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/sqlalchemy/engine/strategies.py\", line 89, in connect\n    return dialect.connect(*cargs, **cparams)\n  File \"/opt/data/apps/docker-registry2/local/lib/python2.7/site-packages/sqlalchemy/engine/default.py\", line 376, in connect\n    return self.dbapi.connect(*cargs, **cparams)\nOperationalError: (OperationalError) unable to open database file None None\n. any chance to get @wking on irc ? \nI did created a virtualenv then \npip install docker-registry\n. Problem was having only 3 '/' on the database path... \nRelative path (3 /) : \nsearch_backend: 'sqlalchemy'\n    sqlalchemy_index_database: 'sqlite:///docker-registry.db'\nAbsolute path (4 /): \nsearch_backend: 'sqlalchemy'\n    sqlalchemy_index_database: 'sqlite:////opt/data/docker-registry/docker-registry.db'\nMany thanks to @wking and @dmp42.\n. ",
    "BrianBland": "Is there still a need for the Move command in this API with all large files being content-addressable?\nAlso, I'm not aware of any distributed storage systems which benefit from a first-class move command. Most seem to approximate this behavior with a full copy followed by a deletion of the old version.\n. I'm proposing that we add an offset parameter to the ReadStream method for resumable downloads if we're already planning on supporting resumable uploads.\nI see this as looking like ReadStream(path string, offset uint64) (io.ReadCloser, error)\n. > I'm not sure how solid this freeze is intended to be, but I'd love to\n\nsee semantic versioning for the API, with a version endpoint:\nversion() \u2192 [major, minor, patch]\n\nI'm pretty sure the freeze is a 0.X version, so still subject to tweaks before final release.\nI like the idea of checking storage driver compatibility at startup, but I'm not sure that semantic versioning is necessary or real backwards compatibility is worth the complexity.\nWe can use a [Major, Minor] version scheme, as patch version doesn't seem to provide much value when applied to an API (correct me if I'm wrong here). When the driver has a newer minor version than the registry, this should still be usable, but should emit some sort of warning, but if the driver is running an older version than the registry or a newer major version, we should probably exit with a compatibility error for sanity purposes.\n. If the registry has a newer minor version than the storage driver, this could mean that the registry added a new method to the driver api that the storage driver does not respect, as per semantic versioning. We would either have to keep a mapping of which versions support which methods and operate differently depending on the driver version, or we could just reject the storage driver for being out of date.\n. I'm not sure how a multi-version storage driver would function. How would we handle changing the behavior, parameters, or return values of an API method without having to rename the method each time or providing a version parameter with each API call? I'm not a fan of either of these approaches...\n. > On the content-addressable side, it might be easier to have middleware\n\nthat translated between the content-addressable registry-side API and\na key/value storage side API (hashing on the fly and renaming from a\ntemporary filename after it finished writing) to make it easier to\nwrite storage drivers for key/value backends.\n\nThis could be especially valuable in the case of multiple registries pointing at the same storage backend for HA purposes. We can write the layer to layer/<tarsum>.tmp-<some random key> and then only move it to layer/<tarsum> if the sum matches our expectation.\nUpdate: it looks like \"move\" operations in many (most?) storage systems aside from local filesystems involve full copies followed by deletion of the original file, so this might be a huge performance hit.\n. > I'm also missing my independent configs for atomic and streaming storage\n\n... but if this is just a proof-of-concept, \u201clook it works in Go\u201d,\nthen maybe it's too early to be talking about that.\n\nI'm not yet committed to either approach, but if we decide to make these two interfaces, the IPC client Start() method can ask the driver what interface it is implementing and then return the appropriate wrapper.\n. Thanks @noxiouz \nBecause the next-generation branch is a new orphan branch, I'm reconstructing this PR as a completely new commit with your changes incorporated.\n. Docker has just switched over to using logrus as of docker/docker#8770, so I propose that we use this for consistent formatting and hooks across the entire product.\nAs for log rotation, we can use lumberjack as our log output writer. This package supports max file size and age for rotation and claims to be compatible with the standard go logging semantics, the same as logrus, so these should theoretically work together.\n. > As I mentioned, there's a whole lot of code here but no\n\ndocumentation on the storage driver endpoints and with\nthe IPC test suite. Is documentation going to be supplied\nseparately?\n\nSorry the code is lacking on that front right now. Let me start off by explaining the intent behind the IPC storage driver system.\nBecause go is a compiled language, we can either distribute binaries or require each user to have a fully configured go development environment with the docker-registry repository and all other dependencies to allow for any modifications including the addition of third-party storage drivers. With an IPC system in place, the user only needs to include a new driver executable to add or replace a storage driver.\nThe actual IPC implementation is inspired by the plugin system in mitchellh/packer, and as currently written, the StorageDriverClient creates and manages a child process with a shared unix domain socket. As the driver is running in a separate process and the IPC system is written on top of libchan, the storage driver is not tied to the same language as the registry itself, but admittedly the language implementations of libchan are currently limited. The language separation is primarily a side effect of the IPC implementation, but writing storage drivers in python or other languages is not out of the question.\nAs for swapping out drivers without restarting your registry process, this is not in the current design, and would have to be approached carefully. Switching to a different storage system with different state is not well-defined behavior for the registry, so this would only be supported for retaining the same driver name and parameters as originally defined while launching the registry process itself. We would also need to agree on the signal or action that triggers a restart of the storage driver child process.\n\nWith this change, configuration is no longer in a central\nconfig.yml; You have to specify configuration for every driver\nwhen spawning the process.\n\nBecause the registry process is in charge of launching the storage driver, it can forward any driver configuration parameters to the process on startup. In each of the driver mains, the driver is able to read a json-serialized parameters map, just as defined in the configuration file or environment variables on the parent process (see #646).\n. > I don't mind if it's separate or not, but I hope we'll get\n\ndocumentation on the interface, and a test harness to put a storage\ndriver process through its paces.\n\nCurrently any storage driver can use the same test suites than any of the default drivers are using by calling testsuites.RegisterInProcessSuite and testsuites.RegisterIPCSuite. Both of these suites have the same tests and only differ in that one runs the storage driver locally and the other as a child process over IPC.\n. As for the versioning scheme, we should probably use something similar to Semantic Versioning, although I can't think of a need for the patch version when it comes to defining a configuration schema.\nThis leaves us with a major and minor version (X.Y), where minor version increases indicate the addition of a new optional parameter, and any deletions, name changes, or type changes will require a major version increase.\nThe version value should be declared as a top level parameter in the configuration file like so: version: 1.8. We'll convert it to a string internally to maintain the guarantee that 1.9 < 1.10 < 1.11\n. > No dots [1].  FOO_BAR should be unambiguous.  I don't expect\n\ncollisions like:\nfoo:\n\u00a0\u00a0bar: \u2026\nfoo-bar: \u2026\n\nIf we make the assumption that there is no ambiguity here, and also that environment variables cannot distinguish between upper and lower case, then perhaps we should disallow hyphens and underscores in configuration key names. foo-bar should be replaced by foobar, which leads us to the following example:\n``` yaml\nversion: 0.1\nregistry:\n  loglevel: info # REGISTRY_LOGLEVEL=info\n  storage:\n    s3: # REGISTRY_STORAGE=s3\n      region: us-east-1 # REGISTRY_STORAGE_S3_REGION=us-east-1\n      bucket: my-bucket # REGISTRY_STORAGE_S3_BUCKET=my-bucket\n      rootpath: /registry # REGISTRY_STORAGE_S3_ROOTPATH=/registry\n      encrypt: true # REGISTRY_STORAGE_S3_ENCRYPT=true\n      secure: false # REGISTRY_STORAGE_S3_SECURE=false\n      accesskey: SAMPLEACCESSKEY # REGISTRY_STORAGE_S3_ACCESSKEY=SAMPLEACCESSKEY\n      secretkey: SUPERSECRET # REGISTRY_STORAGE_S3_SECRETKEY=SUPERSECRET\n      host: ~\n      port: ~\n```\nIn the above example, I showed the respective environment variables which would alternatively provide the configuration in the YAML file. The driver name and the key-value pairs of its parameters specify the arguments to NewDriverClient as seen in #643 at https://github.com/docker/docker-registry/pull/643/files#diff-c8c4532a664949dfe479be7d227b2807R23 (This map is of type map[string]string and not map[string]interface{} because environment variables are provided only as strings)\n. At the very minimum it should be possible to query the storage driver's health through an endpoint exposed on the registry itself. We should probably include the status for other extensions as well.\nFor extensions and storage drivers where healthcheck metadata is implementation-specific, I propose that we use a unified HealthStatus struct. What I see us using right now is:\ngo\nstruct HealthStatus {\n    Status string\n    Metadata map[string]string\n}\nwhere Status is one of a known set of status strings (ok, warning, error) and Metadata is used for any other relevant information.\n@noxiouz @visualphoenix is there anything you would add to this?\n. @dmp42 @stevvooe +Next-generation\n. > Can we pull out the early driver.socket assignment (which drops\n\nparentSocket) into a separate commit?  It seems like a separate issue.\n\nMaybe I got a bit carried away with refactoring the Start method. I'll pull it out if the maintainers want any significant changes to this PR before merging it.\n. I'll submit a few tweaks first but we'll end up refactoring this a bit in the near future.\n. Ok, I've addressed your comments.\n. It's just unable to download go from source, so it can't validate the \"tip\" build.\n. I made a PR #685 for making a similar change already. If we decide on a specific namespacing I can update that to match.\n. Making the path change to cmd and re-submitting momentarily.\n. LGTM\n. Looks good, but I would suggest changing ErrorCode to a string type alias, map the ErrorCodes to their messages, and drop the opaque int values.\n. > The reasoning behind the opaque integer type is to ensure that the zero-value of ErrorCode always maps to unknown. This allows the equality for unknown error codes to always work without special code.\nThat makes sense.\nLGTM\n. LGTM!\n. @stevvooe @dmp42 \n. LGTM\n. LGTM for the single case that is covered.\n. LGTM\n. Looks like it always returns a 0 exit code, but you can consider the existence of output a failure.\nbash\nGOLINT_OUTPUT=$(golint ./... | tee /dev/stderr)\ntest -z \"$GOLINT_OUTPUT\"\n. ping @dmp42 @stevvooe \n. LGTM, made a couple minor comments\n. If any layers have been fully pushed/pulled before the operation fails, these will not need to be pushed and pulled on retry. It should also be easy to add retry for each individual layer operation.\nAs for resuming the push/pull of a single layer across multiple Push/Pull attempts, this is not yet implemented, but should be added in my next commit/PR. It also requires a minor change to the ObjectStore interface. \n. Rebased\n. LGTM\n. LGTM\n. LGTM\n. Also of note is that I added a BlobLength method to support the new HEAD request against a given image blob.\n. Can't the client just hit the v2 ping endpoint (assuming we have one)?\nOr is this supposed to be the new ping specification?\n. Do we need to retain support for the /v2/_ping endpoint which the 1.3 daemon uses for official v2 image pulls?\n. Thanks for the catch, LGTM\nping @dmp42 \n. Thanks again, LGTM\n. LGTM\nOn Nov 21, 2014 12:30 PM, \"Stephen Day\" notifications@github.com wrote:\n\nLGTM!\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/pull/773#issuecomment-64033051\n.\n. LGTM. Forgot that channels are gc-ed properly without closing for some reason.\n. LGTM, +1 for returning signed manifests from client api calls.\n. Yeah, drone uses the branch head to test instead of the already-merged tip.\n\nIf it's possible, I would suggest changing the behavior so it uses\nrefs/pull/number/merge instead of refs/pull/number/head, but that's a\ndiscussion for another time.\nOn Nov 24, 2014 7:35 PM, \"Stephen Day\" notifications@github.com wrote:\n\nI have to rebase against the #783\nhttps://github.com/docker/docker-registry/pull/783 fixes...\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/pull/782#issuecomment-64305641\n.\n. LGTM\n. LGTM\n. It turns out we don't need any of this for the s3 driver implementation.\n\nTo make the Stat behavior consistent with what has been written and what can be read, we can change the s3 driver to complete the multipart upload after each call to WriteStream. When calling WriteStream against an existing file, we can create a new multipart upload and use the old file as the first part via UploadPartCopy, then complete the upload (commit it). This will always leave us with \"atomic\" write operations.\n. Yeah, we'll have to fork goamz to add support for this s3 api call, but\nthere's no reason they wouldn't accept it back into mainline, as it doesn't\nbreak the package api at all.\nOn Dec 9, 2014 10:08 PM, \"Stephen Day\" notifications@github.com wrote:\n\n@BrianBland https://github.com/BrianBland This is great to hear. We'll\nhold off on the addition of adding a Commit/Complete/Sync method and\nupdate the s3 implementation. Will we have to fork the goamz driver to\nsupport this?\nI'll close out this ticket completely where we hear back from\n@ahmetalpbalkan https://github.com/ahmetalpbalkan on the azure driver.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/814#issuecomment-66408707\n.\n. Has this issue been fully resolved by #844, or is there still more to be done here?\n. LGTM\n. The lint/vet stuff seems pretty wonky right now, but otherwise LGTM\n. @stevvooe this should be working now\n. Just ran with GOMAXPROCS=4. These tests create some nice jet engine sound effects :D\n. The storage driver api doesn't differentiate between tags and blobs, so let's make sure that the repository name specification encompasses all valid tag names as well.\n. LGTM\n. LGTM\n\n+1 for not requiring an empty request\n. Not at the moment, but I was planning on making a change to generalize the environment variable behavior.\n. Actually, let me push a fix for this script before you merge this. Just found an edge case.\n. Alright, I tested this most recent version with various gofmt errors across multiple files and it properly includes the changes in the actual commit.\n. I've removed the auto-fmt behavior in my most recent commit. \n. @dmp42 @stevvooe \n. LGTM\n@dmp42\n. LGTM\n. v2 registry questions should be asked in docker/distribution\nCurrently there is no tag delete API method, with the intent being to keep the delete method \"safe\" in that it only can reference an absolute image manifest instead of a mutable tag, especially in the case of eventually-consistent storage backends.\nIt's not in the v2 REST API, but if you absolutely need to delete a repository from your storage backend you can instantiate a Vacuum (https://github.com/docker/distribution/blob/master/registry/storage/vacuum.go) with a configured StorageDriver and call the RemoveRepository method on it.\n. Good call, I could create a request and response object for each method to do the most compile-time checking.\n. +1\n. Sure thing, I'll remove all of this boilerplate.\n. This seems to be the default behavior of go fmt if you don't split the imports into two logical blocks. Cleaning this up now.\n. The current intention is that each driver will have a main() method that handles (json) serialized parameters when run out-of-process over ipc. If you reference contrib/golang_impl_ng/main/driver/filesystem/filesystem.go, you can see that we're pulling the RootDirectory parameter out of the first command line argument, which is a json blob of all required configuration params.\n. Driver is probably needlessly generic. I'm open to naming suggestions.\n. I think the name Size could incorrectly suggest that it is describing the size of the object after write completion, but it may make sense to rename this to CurrentSize or something similar.\n. I'm not committed to the Move method being part of this interface, although for specific implementations such as the local filesystem driver, this can be a huge win for these operations. I guess the main question is whether or not the registry has any need to move objects in its file storage.\n. As written, the List method is recursive.\nIn the os.FileInfo interface, we'll likely have Name, Size, ModTime, and IsDir defined with reasonable storage drivers, but Mode is unlikely to provide any reasonable information (Sys is fine returning nil).\nShould we create and use our own FileInfo interface/struct with only Name, Size, ModTime and IsDir?\n. And this is why we have CI...\n. Technically if the prefix is at a high enough level, this could include all repositories and layers, which could be quite large.\nWe may want to change this to mimic the behavior that @wking suggested in #616 \ngo\nCount(prefix string) (int, error)\nList(prefix string, size int, from int) ([]string, error)\n. After doing a little bit of research it looks like the from and size arguments would cause some issues with certain storage systems including S3.\nThe S3 list equivalent GET Bucket supports max-keys and a marker argument, which is the last key of then previous result set, or the key before the first that should be returned from the call. Implementing the interface with List(prefix string, size int, from int) with this model seems like it would require loading all results up until the from argument so that we can determine what the appropriate marker is.\nI'm all for namespaced keys, but we should find a good solution for loading all namespaces unless we're fine returning a number of results equal to the number of users who own repositories in the worst case.\n. One thing we could do is change the method signature to List(prefix string, count int, interface{} marker) (keys []string, nextMarker interface{}, err error) and return an untyped (interface{}) continuation marker with the keys. The caller would be expected to only provide a marker they were given from the last call to List, and this would work for both numeric offsets and key markers. nil would have to be accepted by each storage driver as a valid initial marker.\n. Yeah, let's go with the type alias approach. This would simplify a bit of the parser code.\n. I would be in support of type Version [2]uint if it doesn't make specifying the version in YAML needlessly awkward. @stevvooe @dmp42 any thoughts?\n. Good catch about the $GOPATH format. We can either try each component of the $GOPATH in order or simply require it to be findable in $PATH. Alternatively, we may want to define a directory where these executables may be found, as they aren't particularly useful for any other purpose.\n. Yeah it feels weird in both locations, so I'm fine with either.\n. Really nit-picky, but the following regexp is a little cleaner:\n[a-z0-9]{2,}([._-][a-z0-9]+)*\n. If you can remove the -d flag here it should fix the out-of-process storagedriver tests.\n. I guess golint didn't catch this because it's not exported, but resolveBlobPath\n. Should this take a Digest instead of a string?\n. Should we be using 1.4rc1 or a more stable go release?\n. I guess we should use it for now, but if we encounter any instabilities I guess that's actually something we should report back to the go project.\n:+1: \n. Are there any query values we should be passing into this call for generating the new URL?\n. For one, we can actually run gofmt in a way that writes its updates in place pre-commit. Secondly, I want to make sure it's clear to a user why their commit was rejected, which is a lot less obvious without all of the chrome of a CI web interface. We can also port all this syntax over to the circle.yml, but it's not very conducive for complex commands like multiline if-statements.\n. I agree with your concerns about golint, but we're already using it as a gate in our CI setup, so this is no more restrictive. I can also change the gofmt semantics to just fail and complain if you prefer that behavior.\n. Even easier, you can replace the line with str := fmt.Sprintf(\"Bearer realm=%q,service=%q\", ac.realm, ac.service).\nThis goes for all instances of strconv.Quote\n. Yeah I checked that before. Also, if you use %+q it will call strconv.QuoteToASCII for what it's worth.\n. This can be replaced with ioutil.NopCloser(bytes.NewReader(nil))\n. There's no reason for this if statement. accessKey will already be the zero value if ok is false.\n. regionName can actually be nil now, so fmt.Sprint(regionName) won't work in that case, as it will print to \"\". As region must be provided, the type assertion is fine.\n. I'm iffy about this statement, because users may provide \"true\" in their config yaml instead of true. I'm not sure if we should be more strict or forgiving.\n. Could you please add a c.Log statement for any scenario where misswrites > 0?\n. The parameters values are untyped, so regionName can be nil as of this line of code. If you call fmt.Sprint(nil), it will print out <nil>. Sorry, my markdown derped in my previous comment.\n. To be fair, I guess aws.GetRegion(\"<nil>\") will return a zero region...\n. \"rootdirectory\" instead of \"rootDirectory\". Otherwise this can cause confusion.\n. What happens if rootDirectory ends in a slash? Is mypath//asdf a valid key?\n. The properties need to be lowercase, so the error messages should too.\n. ",
    "bruce": "Note I don't see any further logging output from the registry; it seemingly hangs after displaying the AWS signature line.\n. ",
    "jokeyrhyme": "@danzy @coaic @benbarclay @ashishtilara\n. Ah, okay. I misunderstood the storage_path value. From the documentation and looking at the S3 driver source code, I believed it was not used for anything related to S3.\nIf storage_path acts as an S3 prefix, then consider this issue solved. :)\n. @danzy confirms that this is the case. :)\n. Thanks, @ncdc . In addition to needing to store private images on machines under our control (data sovereignty), it is also our desire to limit Docker transactions to a local mirror for performance reasons. We're experiencing poor docker pull performance from Sydney, Australia, poor enough that the 3rd party build / host service we are using is timing out.\nHow should we go about configuring our local Docker clients to use the public registry for authentication, but download images only from our mirror (running docker-registry)? Is there a way to host private images on our own mirror only (protected by authentication), without paying for an otherwise unused subscription on the public registry?\n. ",
    "stonefury": "If I run it as: docker run -p 5000:5000 -e GUNICORN_OPTS=[--preload] registry\nHow do I login? I\"m being prompted for a password.  I'm obviously not understanding something. I was not expecting to be asked for a password. I thought I needed to do my own authentication if I wanted to.\nThanks for your help.\n. Stupid mistake as I figured. Apologies.  I didn't tag the image with that name, I was in some stupid thinking that the image \"ubuntu\" would be pushed to the server. I forgot I needed to retag the image with the servername being part of it. \ndocker tag ubuntu 127.0.0.1:5000/ubuntu\ndocker push 127.0.0.1:5000/ubuntu\nSLAP SLAP SLAP.\n. ",
    "krallin": "Sure, thanks! \nJust updated the commit.\nCheers, \n. Sweet, thanks!\n. ",
    "dnephin": "I believe the concern there was just that the PR wasn't complete enough to keep it around. I haven't had much time to finish it off, but the docker remote API is also quite large (making it hard to document in one pass). Many of the response bodies are huge. The docker-registry API seems to be a more manageable size.\nAs far as I've understood, there was still interest in the approach, but a PR had to show a nearly complete version, not just a minimal prototype (which is what I had).\n. ",
    "dmcgowan": "Instead of creating a new send channel for each request, a single send channel can be created for sending all requests.  A new goroutine can still be spawned for each decoded request.  Since all channels in libchan are nested, the return channels are still unique even though they were sent through the same channel.  This will save a round trip to establish the new channel, but that is likely not as important in the unix socket case.\n. While log routing doesn't need to dictate the topics above, some possible routers we may want to integrate and ensure our format works nicely with.\n- https://github.com/mozilla-services/heka\n- https://github.com/fluent/fluentd\n- https://github.com/elasticsearch/logstash\n. +1 on both logrus and lumberjack, logrus supports pretty tty output, default logfmt output for non-tty and optional json output.\nAlso for tracing and contextual logging, this golang blog post is a few months old but it is relevant to creating a standard. https://blog.golang.org/context\n\nAt Google, we require that Go programmers pass a Context parameter as the first argument to every function on the call path between incoming and outgoing requests. \n. LGTM\n. lol\n. LGTM\n. LGTM\n\nTo elaborate on the client implementation, this will allow doing a streaming upload without requiring a second empty request to specify the size.\n. Refactor mentioned will be a plus, but LGTM either way\n. LGTM\n. This issue has gotten very confusing, let me clarify what should be expected based on Docker 1.5.\ndocker login http://registryname will only work if insecure registry is set as --insecure-registry http://registryname.  However even though login works in this case (a value is set in config), http://registryname is never a valid argument for --insecure-registry since it only expects registry names as would be seen in an image name.  Subsequent actions to registryname/image will not be treated as insecure.  The correct use in this scenario is docker login registryname name paired with --insecure-registry registryname.  Then registryname/image will actually be treated as insecure and you will see http is attempted only after an https connection is failed to be established.\nNow as @dmp42 mentioned, all this is rather pointless since Docker will never send your credentials over http, you must use https if you want to use basic auth.  You should see a message in the daemon logs or console that looks like \"Docker will not send auth headers over HTTP\" if an action is attempted on a registry which uses http and returns 401. Insecure can be used to respect self signed certificates though, this is the recommended solution.\nThe case of including = or not doesn't seem to be an issue in my testing, not sure what is going on there.\nIf the behavior I described is not desired or causing your problems, I would suggest opening up an issue or proposal on the engine to change it. Sending credentials over http has already been discussed at length but here is an example of such an issue https://github.com/docker/docker/issues/9570.\n. Inconsistent date format\n. typo\n. Why is checking the key id important here?  It adds to the JWT spec but considering the signing key is always the x5c[0]  the key id should be able to be omitted.\n. ",
    "michielbdejong": "ok will do, thanks!\n. No, you didn't - I should have read that page, I had only looked at the repo name.\nAnyway, so I emailed them about it. Thanks!\n. ",
    "ianneub": "nginx error log is blank:\n\nRegistry config settings are all in the environment, I'm using registry straight from the Docker hub @ 95099732d4f8\nSETTINGS_FLAVOR=s3\nAWS_BUCKET=my-registry\nSTORAGE_PATH=/registry\nAWS_KEY=asdf\nAWS_SECRET=asdf\nS3_ENCRYPT=true\nS3_REGION=us-east-1\nS3_SECURE=true\nSTORAGE_REDIRECT=true\nStandalone setting:\nX-Docker-Registry-Standalone: True\nHere is the Dockerfile that builds the nginx server:\nhttps://gist.github.com/ianneub/6a63dbe0215df73ee308\nThanks @dmp42.\n. Thanks @shreyu86 and @dmp42 . I'm pretty sure that my issue is actually the SSL certificate that I am using. It is self signed and I can't seem to get boot2docker to accept it. I'll go ahead and close this and reopen if I find otherwise.\nThanks for your time.\n. Thanks @shreyu86 . I've been fighting it by trying to add my self signed cert to the CA cert chain. I was able to get it to work pretty easily in docker-osx on docker 1.2.0. But now that boot2docker was updated to work with fig I thought I'd switch over.\nMaybe I'll just splurge and buy a cheap cert :) Would save me more time than trying to fight it!\n. ",
    "joshk0": "Excellent, it would have been nice if this were in the changelog for the version. Thanks.\n. ",
    "mesco64": "Thanks this solved my problem. This error was happening when attempting to pull the cmdbuild container\nterminalvelocity/cmdbuild. ",
    "gregburek": "The newrelic module had the same problem. Added a commit to fix. \n. The shorter module names are legit. Updated PR with style correction too. \n. @dmp42 updated and passing tests. \n. ",
    "sethdmoore": "Perhaps you are right. In any case, I have discovered an ideal solution that requires no pull requests or changes.\nIf you set the 'cmd' portion in the marathon app to include the environment variable, the container will start on an arbitrary port.\njson\n{\n  \"cmd\": \"REGISTRY_PORT=$PORT /usr/bin/python /usr/local/bin/docker-registry\",\n}\n2:27 [8] [INFO] Listening at: http://0.0.0.0:31873 (8)\nThis issue can be closed, thanks for the feedback.\n. ",
    "ahmetb": "@dmp42 that works as long as the caller (registry) does not assume that Move operation is atomic (transactional) and if the code crashes in the middle of that, you may end up with 2 copies (since Delete isn't executed yet).\n. @dmp42 Thanks, that was quick!\nI'll be sending another PR for readme change to cover more details of Azure configuration (just like the \"storage: s3\" section in readme).\n. @dmp42 shall I move s3 config as well as my addition to ADVANCED.md as well? Or just azure?\n. @dmp42 updated docs with  a44c88f.\n. I'm not sure why this is needed. What you need from a storage driver is to \"behave\" like a filesystem, not \"imitate\" full functionality of what a filesystem will do.\nStuff like ModTime() or IsDir() are not easy things to implement and unless are directly going to use these in the logic there is no point of making an API out of these and currently these are YAGNI. For example I don't get what Path() will  do in case of a cloud storage provider. There no concept of full path in case of cloud storage providers, whatever you pass to that Stat(path) func will be returning the path it is passed.\nFor example if I store the blob in $bucket/$file_path the registry code does not need to know the bucket name at all but that appears like the full path that method is asking for.\nA Stat call makes sense as long as it's intended to reduce roundtrip calls to the underlying storage service but it should not look exactly like the fs stat call, it should rather have the data that is going to be used. Currently what's use case for Path(), ModTime(), IsDir()? If there is none yet (and most likely won't be there any in the future) I'm not in favor of creating this.\n. @stevvooe yep these are all doable but it feels like the API should look like a filesystem API, not a file store. However storage driver is effectively used like a file store; not a filesystem.\nUnless we have a use case for ModTime() and IsDir(), I recommend just having the Size() in FileInfo. I'm not saying we cannot implement these methods in Azure or some other driver; it's rather we don't need this today and if a need arises we can just add it to the storagedriver API and implement in every storagedriver accordingly.\n. @stevvooe makes sense then. In the case of cloud storage drivers FileInfo.Path would be the path passed to the Stat function, am I correct? \n. @stevvooe for example if test fails in the middle of appending to a blob and if the same blob gets picked up again, it tries to write to same blob starting at offset=0 which is not a supported resumable offset in my case. so if you guys would seed the rand with time, likeliness of this happening will be reduced.\n. I'm not sure how that cleanup is done. I can't see any calls to any delete in advance in many of the tests. The root cause is filename collision, that's right. \nI avoided running into this problem by specifying a random bucket name (in azure case container name) to the go test with $RANDOM. It's no big deal for me, I was just wondering if it's intentionally not seeded.\n. @stevvooe does keeping file names (random lowercase ascii letters) the same between tests really help? It's even keeping even the filename length the same. I don't see how it could help consistency or debugging. \n. Alright, that makes sense. However, currently there's nothing in the suite set up cleaning the areas that are going to be used right now. When the control comes to the test package, the env might be contaminated from the previous test run as the filenames/dirnames don't change.\n. @dmp42 Done.\n. @stevvooe WritePartial/WriteCommit will have almost the same functionality, right? (i.e. how will the filesystem implementations of these methods would look like?)\nThe notion of non-atomicity comes in the case of filesystem driver \u2013drivers like inmemory and azure are already atomic. (i.e. if WriteStream returns err, nothing gets written, otherwise it means everything is written successfully.). However; I'm not sure about s3, does it have atomic (all or none) writes?\nOn the other hand, having size in the storagedriver  (especially like the negative size stuff) is a bad idea. In the azure driver I did not use that size param at all. I'm not sure where it comes useful. It'd be better to remove that. \nAbout the WriteCommit, are you planning to call it for the last chunk? When exactly it will get called? Can't it be without an offset or data passed to it? I'm not sure it contains the reader or offset as parameters... (e.g. Write(f,data), Write(f,data), ..., Commit(f, size)  and commit just commits the file if f.size == size?\n. @andreykostov sounds good. However I'm not sure how \"nn\" will be used. WriteStream should be writing whatever it gets until EOF from reader. I see that Go io.Writer.Write returns nn; however, in this case partial writes should be returning error. \nIf we require nn, it will be in favor of filesystem writer and other drivers will need to calculate the stream size to return the correct value. \n. @stevvooe I agree with @AndreyKostov's comment about how the azure driver would look like in this case. Are you asking about anything specific?\n. It's a stateless lightweight API client instance. Storage driver can be shutdown at any time.\n. Yes, tests run the same scenario and they pass.\n. Good catch, will fix.\n. Sure, I thought it could help scaling parameter count in the future but apparently this is a premature optimization. Will fix.\n. would log.Panic(err) do it?\n. If the HTTP transfer gets interrupted, nothing gets written to the cloud storage. Azure Storage API is transactional, therefore there's no \"partially written or corrupt block\" like a local filesystem. If the registry process dies and re-sends the same chunk at the same offset, request will be accepted if the initial upload request was interrupted in the middle. In fact this sounds like a good candidate for storagedriver/testsuite.\n. Okay, here's the story, bear with me. :smile: \nIf the caller sends a 16 MB chunk to WriteStream, it will get split into 4 MB chunks (max block size in Azure) and will get uploaded like this:\n1. Upload Block1\n2. Upload Block2\n3. Upload Block3\n4. Upload Block4\n5. Commit blob with blocks = [\"Block1\", ..., \"Block4\"]\nLet's say if Upload Block3 failed due to some intermittent connectivity issues. At this point, the blob does not exist yet, (because we did not call Commit Block), therefore CurrentSize will be 0 (or effectively ErrPathNotFound). Unless you commit blocks, the blob does not exist. This is where it differs from a local filesystem. \nIf the caller sends the same chunk again upon that failure, this whole process will start from the beginning. Let's say the first 16 MB chunk is uploaded correctly and blocks are committed (remember, all these are transactional) CurrentSize will return 16 MB.\nSecond WriteStream call with the next 5 MB chunk will get uploaded the same way:\n1. Upload Block5 (4 MB)\n2. Upload Block6 (1 MB)\n3. Commit blob with blocks = [\"Block1\", ..., \"Block4\", \"Block5\", \"Block6\"]\nIn this case, let's say Upload Block6 failed, now the CurrentSize will still return 16 MB because that's the total size of blocks committed last time. And if the WriteStream gets initiated again, the only resumable offset is 16_1024_1024, and that's where Block5 and Block6 should get updated.\nIn a nutshell, CurrentSize gets the size of the committed blob and blob is only committed if WriteStream returns with no error. If WriteStream returns with an error, everything in WriteStream is ineffective.\n. Tried to explain this a bit here in this comment: https://github.com/docker/docker-registry/pull/801#discussion_r21128320\n. Alright, will fix, thanks.\n. If you call WriteStream with a 17 MB file and it fails at some point nothing gets committed at all. Again, if WriteStream returns success, means everything just got committed; if it returns failure or process dies or something, nothing done in that call is saved to the storage.\nIn the example I gave above, first WriteStream 16 MB succeeds then the next 5 MB is another WriteStream call. Is it more clear now?\n. Each successful call to WriteStream appends the given chunk to the blob. Subsequent calls to WriteStream can continue adding new blocks to that path, so answer to your first question is yes \u2013in fact this is how it works right now. \nAnswer to your second question is also yes, WriteStream commits the chunk to the blog before returning (line 213). After a successful call to WriteStream, CurrentSize will always return the new size.\n. fixed in 4054cd3e73756d7c8de305e159912c28361f7603.\n. fixed in 4054cd3e73756d7c8de305e159912c28361f7603.\n. fixed in 4054cd3e73756d7c8de305e159912c28361f7603. \n. ",
    "duglin": "I assume this is related to: https://github.com/docker/docker/issues/8256  correct?\n. ",
    "sjackman": "Okay. I've moved this issue to docker/docker#8759 to hopefully get an answer of how to accomplish this from the Docker CLI.\n. ",
    "shuoli84": "@dmp42 Btsync has its own api, through which we can get the file's status. The get_files api endpoint return:\n[\n   {\n     \"have_pieces\": 1,\n     \"name\": \"json\",\n     \"size\": 1479,\n     \"state\": \"created\",\n     \"total_pieces\": 1,\n     \"type\": \"file\"\n   },\n   {\n     \"have_pieces\": 763,\n     \"name\": \"layer\",\n     \"size\": 49995996,\n     \"state\": \"created\",\n     \"total_pieces\": 763,\n     \"type\": \"file\"\n   }\n ]\nThe have_pieces and total_pieces should be the indicator. Although we may need polling for the api to get the status.\n. @dmp42 I will be very busy for the following month, we are hitting deadline. But I am more than happy to contribute on this after that. :)\nCould you pls point me somewhere I can find the v2 documents?\n. ",
    "kfatehi": "Ah thanks @dmp42 -- I had a feeling I was in the wrong place ;) I will copy-pasta into a new support ticket there.\n. ",
    "sybeck2k": "I've found that the cause is the mirror configuration.\nIn the previous configuration, I configured the mirror option, and thus I was having the header\nX-Docker-Registry-Standalone: mirror\nSwitching off the mirror will make the authentication working correctly.\nIn my set-up, I was expecting my local repository to act as an active proxy - where I could push the \"private\" images, and pull the images, either from the private repository or from the index when not available locally. But, it does not work like that, and that's not a big deal :)\n. thanks everyone!\nand for references and @dmp42, the Apache file https://github.com/docker/docker-registry/blob/master/contrib/apache.conf works great :)\n. ",
    "satyrius": "I have the same problem. Trying to run registry as a container (using fig) and to store data in S3 (Frankfurt)\nyaml\nregistry:\n  image: registry:0.8.1\n  environment:\n    SETTINGS_FLAVOR: s3\n    AWS_REGION: eu-central-1\n    AWS_BUCKET: myregistry\n    STORAGE_PATH: /registry\n    AWS_KEY: MYKEY\n    AWS_SECRET: MYSECRET\n    SEARCH_BACKEND: sqlalchemy\n  ports:\n    - \"5000:5000\"\n```\nfig up\nRecreating root_registry_1...\nAttaching to root_registry_1\nregistry_1 | 2014-10-31 11:07:20,234 WARNING: Cache storage disabled!\nregistry_1 | 2014-10-31 11:07:20,235 WARNING: LRU cache disabled!\nregistry_1 | ** [Bugsnag] No API key configured, couldn't notify\nregistry_1 | Traceback (most recent call last):\nregistry_1 |   File \"/usr/local/bin/docker-registry\", line 9, in \nregistry_1 |     load_entry_point('docker-registry==0.8.1', 'console_scripts', 'docker-registry')()\nregistry_1 |   File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 351, in load_entry_point\nregistry_1 |     return get_distribution(dist).load_entry_point(group, name)\nregistry_1 |   File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2363, in load_entry_point\nregistry_1 |     return ep.load()\nregistry_1 |   File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2088, in load\nregistry_1 |     entry = import(self.module_name, globals(),globals(), ['name'])\nregistry_1 |   File \"/usr/local/lib/python2.7/dist-packages/docker_registry/run.py\", line 18, in \nregistry_1 |     from .tags import *  # noqa\nregistry_1 |   File \"/usr/local/lib/python2.7/dist-packages/docker_registry/tags.py\", line 21, in \nregistry_1 |     store = storage.load()\nregistry_1 |   File \"/usr/local/lib/python2.7/dist-packages/docker_registry/storage/init.py\", line 38, in load\nregistry_1 |     config=cfg)\nregistry_1 |   File \"/usr/local/lib/python2.7/dist-packages/docker_registry/drivers/s3.py\", line 65, in init\nregistry_1 |     super(Storage, self).init(path, config)\nregistry_1 |   File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/boto.py\", line 134, in init\nregistry_1 |     self._boto_bucket = self._boto_conn.get_bucket(\nregistry_1 | AttributeError: 'NoneType' object has no attribute 'get_bucket'\nroot_registry_1 exited with code 1\n```\n```\ndocker version\nClient version: 1.3.1\nClient API version: 1.15\nGo version (client): go1.3.3\nGit commit (client): 4e9bbfa\nOS/Arch (client): linux/amd64\nServer version: 1.3.1\nServer API version: 1.15\nGo version (server): go1.3.3\nGit commit (server): 4e9bbfa\n```\n. Skipping the region setting cause another error\nroot@registry:~# fig up\nRecreating root_registry_1...\nAttaching to root_registry_1\nregistry_1 | 2014-11-01 10:29:56,555 WARNING: Cache storage disabled!\nregistry_1 | 2014-11-01 10:29:56,556 WARNING: LRU cache disabled!\nregistry_1 | 2014-11-01 10:29:56,662 WARNING: No S3 region specified, using boto default region, this may affect performance and stability.\nregistry_1 | ** [Bugsnag] No API key configured, couldn't notify\nregistry_1 | Traceback (most recent call last):\nregistry_1 |   File \"/usr/local/bin/docker-registry\", line 9, in <module>\nregistry_1 |     load_entry_point('docker-registry==0.8.1', 'console_scripts', 'docker-registry')()\nregistry_1 |   File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 351, in load_entry_point\nregistry_1 |     return get_distribution(dist).load_entry_point(group, name)\nregistry_1 |   File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2363, in load_entry_point\nregistry_1 |     return ep.load()\nregistry_1 |   File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2088, in load\nregistry_1 |     entry = __import__(self.module_name, globals(),globals(), ['__name__'])\nregistry_1 |   File \"/usr/local/lib/python2.7/dist-packages/docker_registry/run.py\", line 18, in <module>\nregistry_1 |     from .tags import *  # noqa\nregistry_1 |   File \"/usr/local/lib/python2.7/dist-packages/docker_registry/tags.py\", line 21, in <module>\nregistry_1 |     store = storage.load()\nregistry_1 |   File \"/usr/local/lib/python2.7/dist-packages/docker_registry/storage/__init__.py\", line 38, in load\nregistry_1 |     config=cfg)\nregistry_1 |   File \"/usr/local/lib/python2.7/dist-packages/docker_registry/drivers/s3.py\", line 65, in __init__\nregistry_1 |     super(Storage, self).__init__(path, config)\nregistry_1 |   File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/boto.py\", line 135, in __init__\nregistry_1 |     self._config.boto_bucket)\nregistry_1 |   File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 471, in get_bucket\nregistry_1 |     return self.head_bucket(bucket_name, headers=headers)\nregistry_1 |   File \"/usr/local/lib/python2.7/dist-packages/boto/s3/connection.py\", line 518, in head_bucket\nregistry_1 |     response.status, response.reason, body)\nregistry_1 | boto.exception.S3ResponseError: S3ResponseError: 400 Bad Request\nregistry_1 |\nroot_registry_1 exited with code 1\nGracefully stopping... (press Ctrl+C again to force)\n. Any idea?\n. @chuegle updating boto inside a container cause another issue:\nregistry_1 | Traceback (most recent call last):\nregistry_1 |   File \"/usr/local/bin/docker-registry\", line 5, in <module>\nregistry_1 |     from pkg_resources import load_entry_point\nregistry_1 |   File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 2749, in <module>\nregistry_1 |     working_set = WorkingSet._build_master()\nregistry_1 |   File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 446, in _build_master\nregistry_1 |     return cls._build_from_requirements(__requires__)\nregistry_1 |   File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 459, in _build_from_requirements\nregistry_1 |     dists = ws.resolve(reqs, Environment())\nregistry_1 |   File \"/usr/lib/python2.7/dist-packages/pkg_resources.py\", line 628, in resolve\nregistry_1 |     raise DistributionNotFound(req)\nregistry_1 | pkg_resources.DistributionNotFound: boto==2.27.0\nMy custom Dockerfile is pretty simple\ndocker\nFROM registry:0.8.1\nRUN pip install --upgrade simplejson boto\n. @dmp42 where should I use env variable DEPS=loose? I've tried to set up this variable for the container bat still has the same deps pinned deps problem. My Dockerfile is\nDockerfile\nFROM registry:0.8.1\nRUN pip uninstall -y boto && pip install boto==2.34.0\nENV DEPS loose\n. @dmp42 thanks! This works and may be useful for someone who want to run registry with S3 Frankfurt buckets https://github.com/satyrius/docker-registry\n. ",
    "thogg4": "I was able to get past this by removing the region. \n\nOn Nov 2, 2014, at 9:35 AM, Anton Egorov notifications@github.com wrote:\nAny idea?\n\u2014\nReply to this email directly or view it on GitHub.\n. \n",
    "aweiteka": "\nDoes image signing (coming with v2) change that situation for them?\n\n@dmp42 Signing helps but I am assuming image signing here. Third party distribution agreements may exist but I suspect they present a large legal barrier that would kill partnership and innovation. I understand that leveraging the layered image format removes this barrier. That said, I'm not a lawyer. ;)\n\nAlso, if I understand well, crane does 302 to where the actual bits are. So, the company (content owner)\nhas to trust the ISV's registry (/crane) to do what's right here - which to me kind of weakens the \n\"control-point\" - eg: I'm not sure I see a difference then between 302, proxy-pass and mirroring, from a\ncontrol standpoint.\n\nYes, this is how it works. The content owners control and ensure access to the actual bits. If a third party's redirect service fails, that's the third party's problem. This assumes the content owners also have a redirect service for their direct customers.\n\nI don't think we are talking about anything requiring authentication, but rather publicly available content.\nOtherwise, you might end-up with requiring multiple different authentication for one image (eg: one for\neach layer), and that doesn't sound reasonnable.\n\nThis may be a separate but related discussion. It does get tricky. We have an x509 scheme passed to the client that provides access to specific paths on our CDN (Akamai). It would be good to think through and discuss this a bit more.\n\nAbout the v2 protocol - it's quite likely that layers urls are going to be namespaced\n\n@dmp42 You rightly picked up that Crane has a flat namespace. We're assuming world-readable access at the application and control authn/authz using the above mentioned x509 scheme. The proposed namespace has a lot of benefits so I wouldn't want to discourage that. I don't know if you have have it both ways.\nI'm not convinced 302 redirects are ideal, but it works and is flexible. Open to other ideas.\n\nThat doesn't mean content is actually duplicated on the registry - but that inside the registry mechanics,\nthere are \"mount points\" for layers into namespaced url.\n\nPulp does the same thing using symlinks on traditional block storage. Copies are cheap and content is never duplicated.\n\nWhen does the cdv.isv.\u2026 URL get injected into the 8da983e1fdd5 metadata?\n\n@wking I'm assuming URL information is never in image metadata. This is registry metadata only. It's doesn't go with the layer.\n\nhave some mechanism inside the registry to redirect specific layers\n\n@dmp42 Right, \"some mechanism.\" I don't have strong opinions on the specific way users manage registry metadata. Ultimately we're talking about a method to sync distributed metadata efficiently, reliably, securely. Pulse, IPFS, bittorrent etc. all look interesting. @vbatts may have some thoughts here. That may be for V2.$LATER but I suggest we start v2.0 with some fundamental support.\n. > When does the URL get injected into 8da983e1fdd5's registry metadata? Your example push for isv/app\n\ndidn't have a --redirect-url flag, so I'm wondering how that (default?) was set in the JSON.\n\n@wking Good catch. That part of the example suggests the ISV image layers are hosted on the docker hub. That's a valid use case but it doesn't match with my \"pull\" example where ISV layers come from cdn.isv.com.\n. ",
    "mirwan": "Sorry for that misunderstanding \n. ",
    "kmouss": "Awesome!\n. ",
    "coreysa": "That is so freaking cool!! Nice work. \n. ",
    "stp-ip": "@dmp42 sorry I missed that before.\n. ",
    "madhurranjan": "Thanks . I'll try it out . \n. ",
    "MisterMenace": "hey guys, thanks for the quick response! i was indeed missing a boto_bucket entry. with that added to the configuration, docker-registry does now attempt to connect to the radosgw httpd instance. progress!\nthe issue i'm having now is that it doesn't authenticate to radosgw correctly using the S3 keys provided. the updated docker-registry config looks like this: https://gist.github.com/MisterMenace/41d9229ec10288f48a2e\nand docker-registry gives the following output at runtime (from inside the container): https://gist.github.com/MisterMenace/685fc2aaadc6d516c8c0\nit looks like it's doing a HEAD / HTTP/1.1 request, with no headers for authenticating a user (line 15). the aws keys supplied are known to work, as authentication is successful when hitting the API with curl, python, and ruby libs. so the question is, shouldn't docker-registry do something to authenticate itself to radosgw? what am i missing here?\n. update for this: i was able to get docker-registry to start and the gunicorn workers to authenticate to radosgw by doing the following:\n1. run docker-registry on the host directly, not in the docker container. to do this:\n2. clone the git repo, install prereqs as per the docs (i'm using centos): sudo yum install python-devel libevent-devel python-pip gcc xz-devel\n3. install packages from requirements: cd docker-registry && pip install .\n4. at this point i tried to run the daemon (no env vars this time, everything is for the moment hard-coded into the config file): DOCKER_REGISTRY_CONFIG=/etc/docker-registry.yml SETTINGS_FLAVOR=custom, but the gunicorn workers bailed, so...\n5. upgrade gunicorn from 19.1 to 19.1.1: pip install --upgrade gunicorn\n6. change requirements/main.txt from gunicorn==19.1 to gunicorn==19.1.1\n7. run the daemon again as in step 4, and it works!\nnow the problem i'm having is that when i try to use the registry, everything 404's:\n]$ docker ps\n2014/11/04 18:27:04 Error response from daemon: <!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>404 Not Found</title>\n<h1>Not Found</h1>\n<p>The requested URL was not found on the server.  If you entered the URL manually please check your spelling and try again.</p>\nand on the registry server:\n<IP addr> - - [04/Nov/2014:12:22:27 -0500] \"GET /v1.15/containers/json HTTP/1.1\" 404 233 \"-\" \"Docker-Client/1.3.0\"\nbut this happens regardless of storage backend (local or s3), so that must be something else...\n. that was indeed an issue with tagging: specifically we forgot that image names/tags have to have the server name of the registry in them. closing this issue as push/pull is confirmed working via radosgw using S3 protocol with ceph cluster as the backend. thanks again guys!\n-a\n. ",
    "schmitch": "wow that was a fast help. @dmp42 also use-sigv4 isn't easy configurable, should i create a new issue for creating a ENV var AWS_SIGV4 to use sigv4 ? Currently I created my own Image and updated the /etc/boto.cfg\n. @dmp42 yeah I will do it on weekend\n. I created a pull request at #705 currently it will fail hard if no AWS_HOST/boto_host is specified.\nIt's configurable via -e AWS_USE_SIGV4 or in the config file with s3_use_sigv4.\nOther variables are alled like this, too so I used the same scheme.\n. Tim Nolet you need to add [s3] use-sigv4 = True to boto.cfg currently there\nis still an open issue #675\n2014-11-06 22:44 GMT+01:00 Tim Nolet notifications@github.com:\n\nHi @dmp42 https://github.com/dmp42\njust trying this out now with the latest build....but still a nogo on\nusing Frankfurt based S3 buckets. Other regions work fine. I build from the\nmaster branch this afternoon.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/pull/678#issuecomment-62056835\n.\n. I hope this fixed the issue\n\nFine. :)\n. @dmp42 I fixed the issues\n. ",
    "tnolet": "@chuegle @c-schmitt Thanks guys. I figured it out in the end reading through the issue reports. Kudos to the person who debugged the whole mess. \n. ",
    "Rob-Johnson": "Sure.\nDockerfile:\n```\nFROM registry:0.8.1\nRUN apt-get install -y libmysqlclient-dev\nRUN pip install MySQL-python\n```\nConfig:\n```\ncommon: &common\n    loglevel: _env:LOGLEVEL:debug\n    standalone: _env:STANDALONE:true\n    privileged_key: _env:PRIVILEGED_KEY\n    search_backend: sqlalchemy\n    sqlalchemy_index_database: mysql+mysqldb://foo:bar@host/dockerRegistryIndex\nmirroring:\n    source: _env:MIRROR_SOURCE # https://registry-1.docker.io\n    source_index: _env:MIRROR_SOURCE_INDEX # https://index.docker.io\n    tags_cache_ttl: _env:MIRROR_TAGS_CACHE_TTL:172800 # seconds\ncache:\n    host: _env:CACHE_REDIS_HOST:redis\n    port: _env:CACHE_REDIS_PORT:6379\n    db: _env:CACHE_REDIS_DB:1\n    password: _env:CACHE_REDIS_PASSWORD\ncache_lru:\n    host: _env:LRU_CACHE_REDIS_HOST:redis\n    port: _env:LRU_CACHE_REDIS_PORT:6379\n    db: 0\n    password: _env:CACHE_LRU_REDIS_PASSWORD\nlocal: &local\n    <<: *common\n    storage: local\n    storage_path: _env:STORAGE_PATH:/tmp/registry\ns3: &s3\n    <<: *common\n    storage: s3\n    boto_bucket: _env:AWS_BUCKET:my-bucket\n    s3_encrypt: _env:AWS_ENCRYPT:true\n    s3_secure: _env:AWS_SECURE:true\n    s3_access_key: aaaa\n    s3_secret_key: aaaaa\nprod:\n    <<: *s3\n    storage_path: /registry\n```\nThe config is mounted via a volume container running on the same host.\nThe container is run using the following ansible task:\n- name: Run the docker image\n  docker:\n    image: \"{{ image_name }}\"\n    ports: 5000:5000\n    volumes_from: registry_config\n    state: running\n    links: redis:redis\n    env: \"DOCKER_REGISTRY_CONFIG=/etc/docker-registry/config.yml,SETTINGS_FLAVOR=prod,AWS_BUCKET={{ storage_bucket }}\"\nThe registry sits behind an ELB, with requests forwarded to the appropriate port exposed by the container to the host.\n. FWIW, there had been no problems with normal usage before that, and the error came at a period of inactivity (with exception to regular healthchecks from the ELB, which send a GET request to /_v1/ping every 60 seconds to check the app is alive).\n. There had only been access logs for the 15 minutes before the error, and before that only a regular log of a worker exiting and starting so nothing telling in the logs. Here are the logs before the error, with the repeated query to /_v1/ping omitted for brevity.\n```\n2014-11-05 23:45:52 [84] [INFO] Worker exiting (pid: 84)\n2014-11-05 23:45:52,641 INFO: Worker exiting (pid: 84)\n2014-11-05 23:45:53 [104] [INFO] Booting worker with pid: 104\n2014-11-05 23:45:53,753 INFO: Enabling storage cache on Redis\n2014-11-05 23:45:53,753 INFO: Redis host: redis:6379 (db1)\n2014-11-05 23:45:53,754 INFO: Enabling lru cache on Redis\n2014-11-05 23:45:53,754 INFO: Redis lru host: redis:6379 (db0)\n2014-11-05 23:45:53,755 INFO: Enabling storage cache on Redis\n2014-11-05 23:45:53,755 INFO: Redis config: {'path': '/registry', 'host': 'redis', 'password': 'foo', 'db': 0, 'port': 6379}\n2014-11-05 23:45:53,774 DEBUG: Will return docker-registry.drivers.s3.Storage\n2014-11-05 23:45:53,774 WARNING: No S3 region specified, using boto default region, this     may affect performance and stability.\n2014-11-05 23:45:53,775 DEBUG: Using access key provided by client.\n2014-11-05 23:45:53,775 DEBUG: Using secret key provided by client.\n2014-11-05 23:45:53,775 DEBUG: path=/\n2014-11-05 23:45:53,775 DEBUG: auth_path=/path/\n2014-11-05 23:45:53,775 DEBUG: Method: HEAD\n2014-11-05 23:45:53,775 DEBUG: Path: /\n2014-11-05 23:45:53,775 DEBUG: Data:\n2014-11-05 23:45:53,775 DEBUG: Headers: {}\n2014-11-05 23:45:53,776 DEBUG: Host: foo.s3.amazonaws.com\n2014-11-05 23:45:53,776 DEBUG: Port: 443\n2014-11-05 23:45:53,776 DEBUG: Params: {}\n2014-11-05 23:45:53,776 DEBUG: establishing HTTPS connection: host=foo.s3.amazonaws.com, kwargs={'port': 443, 'timeout': 70}\n2014-11-05 23:45:53,776 DEBUG: Token: None\n2014-11-05 23:45:53,776 DEBUG: StringToSign:\nHEAD\nWed, 05 Nov 2014 23:45:53 GMT\n/registry/\n2014-11-05 23:45:53,776 DEBUG: Signature:\nAWS foo\n2014-11-05 23:45:53,986 INFO: Boto based storage initialized\n10.202.0.220 - - [05/Nov/2014:23:46:50] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"ELB-HealthChecker/1.0\"\n2014-11-05 23:46:50,991 INFO: 10.202.0.220 - - [05/Nov/2014:23:46:50] \"GET /v1/_ping HTTP/1.1\" 200 4 \"-\" \"ELB-HealthChecker/1.0\"\n....\n2014-11-06 00:27:39 [94] [ERROR] Exception in worker process: Traceback (most recent call last): File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 495, in spawn_worker worker.init_process() File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 112, in init_process self.run()\n```\nI've not run our image without a real connection to MySQL for any considerable period of time, so couldn't comment on whether that is having an impact. I've run the container again, so I'll report back if I have any similar problems.\n. Great, I'll upgrade and see how I get on.\nThanks!\n. Sorry for the late reply @dmp42.\nThe issue hasn't repeated itself, so I'm happy to close this and let you know if I see it again.\nThanks.\n. ",
    "tiborvass": "Ping @dmp42 @dmcgowan @proppy \n. @dmp42 @proppy @ewindisch \nGeneration of certs are done outside the registry and should be provided under /ssl via --volumes-from or a bind mount. The actual process for users will be documented (in a separate PR + PR to docker/docker/docs ?) from cert generation to including it in the registry container. An additional quick hack could be suggested with something like $(docker run tiborvass/registry:autotls) that would work for the basic needs.\n. @bacongobbler @wking thanks for chiming in!\nThis is a PR to facilitate TLS for registry users. I agree that we should document GUNICORN_OPTS, but mounting certs to /ssl seems to me a more user-friendly API. If people want to customize anything, they can do so with GUNICORN_OPTS.\nWe could debate having REGISTRY_TLS_VERIFY. It's explicit, but can also be redundant with just checking the existence of an /ssl directory.\nUsage: docker run -d -p 5000:5000 -v /my/ssl:/ssl registry\n. @wking The goal is to have simple TLS instructions on README.md. We could add a link saying for production configuration, the recommended way is to use a reverse-proxy as described in ADVANCED.md. What do you think?\n. @dmp42 not that I know of. I must say I didn't know people could use the prefix, I thought it was only the hostname. I'll look into it.\n. @dmp42 we can rewrite it in bash if you want with openssl, will take some time though.\n. correct\n. ",
    "nickandrew": "I'm running a couple of instances of the docker registry under Marathon, which complicates things as the actual host:port numbers of the registry may change over time. Service discovery and load balancing is required to provide a consistent registry name for docker pull/push.\nI tried using GUNICORN_OPTS and the service started and spoke SSL, but only SSL - and that's incompatible with the Marathon health check which supports HTTP and not HTTPS.\nI looked at using nginx as you said but that's likely to multiply the service-discovery issues.\nNow looking at Bamboo (the HAProxy service discovery daemon, not the Atlassian product) to do both SSL and discovery for the docker registry.\n. Yes, it works with HAProxy as a front end and doing the SSL.\nI suggest documenting the GUNICORN_OPTS for users who require SSL and don't have multiple instances to be load-balanced.\n. ",
    "samprog1": "Are there any news regarding quotas? Was there something like that implemented in v2?. ",
    "Tenzer": "I have made a script which works for me for cleaning up unused layers from S3: https://gist.github.com/Tenzer/c6e7b9080e657c3ff29d. Feel free to use it if you want to.\nI don't presume it takes much work to adapt into other storage backends, the same logic should apply, it's just a matter of replacing the file listing, reading and deletion functions.\n. ",
    "galitz": "Coolio... for clarification... our registry runs in an HA mode (under supervisor).  So I assume I'd need to run a single session with --preload before starting our regular sessions?   It has to be handled seperately?\n. Sorry... been busy.  I'll make time to determine just how broken this appears.\n-G\n. ",
    "behemphi": "I realize that I was not clear when I say that this is not a self signed certificate.  It is from a recognized authority.\nIt could be I don't understand my ssl config as stated, however I don't see the connection here.\nIf I sign in by providing my username and password in the URI, the private registry works as expected. I can push, pull and so on. \nIf, however I sign in using the prompts, I get an error. \nThis dichotomy seems to indicate that SSL is properly configured and that the client is doing something unusual when passing the prompt information to the registry.  \nThanks for taking the time on this!\n. ",
    "dgolja": "I had the same issue with the default docker registry (docker-registry==0.9.0) image and sporadically I get the same error. Is there any permanent solution in the future version, except the temporary workaround (GUNICORN_OPTS=[--preload]) ?\n. ",
    "dannyjlaurence": "+1 on the solution of don't provide -e SEARCH_BACKEND\nJust for the google searchers out there, we were getting 404's from S3, and then \"Booting worker with pid: 38\"\nThe client would hang after this happened. (trying to push an image, usually)\nWould love a fix for this - we are unable to use a GUI front end for our repo because of this. \n. ",
    "Nowaker": "Well, no, I encountered it with index.docker.io. I didn't want to report this without checking the source code first, found this repo, and had a look. I didn't know it's a different project, even though I was somewhat hinted.\nStill, even if it's not going to speed things up, it's still very convenient to have a way to limit the number of results.\n. ",
    "chrisfarms": "I just had this issue too. tmp dir was full of GetV2ImageBlobXXXXXX files that appeared to have been orphaned.\nI've had to add a script to periodically clean-up... obviously not ideal.\nCoreOS 633.1.0, Official 0.9.0 registry image, S3 backend, No Redis cache.\n. ",
    "haosdent": "OK, it's no problem. :-) Thank you for your advice.\n. ",
    "chris-jin": "@haosdent ,I've merged your code, and thanks for that.\n@bacongobbler , could you please add this to official site? thanks.\n. ",
    "sunil3s3": "hi ,\nIm facing same issue with swift storage as  yours. any solution for this?\n. ",
    "seanlook": "Sure? But why? I have give username/password in ~/.dockercfg but docker pull operation still try to post without auth? Would that be a bug?\n. ",
    "joda70": "I have already tried that solution but it seems not to work. \nIt is strange because from the docker machine I am able to do:\ncurl -X GET hostname:5000/v1/repositories/test3/tags\nlooking at \"GET /v1/repositories/test3/tags HTTP/1.1\" 200 ... on the docker-registry machine.\nWhile I am not able to run:\ncurl -X GET hostname:5000/v1/search?q=test3\nobtaining 'The requested URL was not found on the server....\"\n. In the docker-registry file I set the search-backend as follows:\nsearch_backend: sqlalchemy\nRestart the docker-registry server, and the search query works. Thanks.\n. Do you mean the docker registry release?\nI have just tried the docker registry version that is automatically installed in a CentOS 6.6 with epel.\n. Yes, it is. I will try the new release.\n. I can test it for you. Do you have any idea how to do the configuration? Or can you provide me with useful documentation?\nBasically only some users can run 'docker push ...' with success. All the others can just run pull.\n. I am going to try what you suggested. Let you know as soon as possible.\nHowever, do you think it is possible to support anonymous access to get images (so for pulling images) and register users that have to perform the push operation? \n. Thanks. \n. @bjaglin I have tried your solution. I confirm the same behaviour obtained by @larrycai. Where should I introduce the http check to obtain a clear error in case of unauthorized request?\n. Have you found a way to return HTTP 401 error for users unregistered?\n. @ssalvatori Are you running the apache + ldap on a container? Why have you decided to use apache instead of Nginx?\n. Have you run docker-registry and nginx in two different containers?\n. Have you tried to change the nginx.conf configuration for the registry:2?\n. I take your point and I am sorry. I will use irc or the forums for the next issues.\nI have run the command both from the docker-registry machine and the docker machine, obtaining different outputs. I think I have to check how I included the certificates. \nFrom the docker-registry:\ncurl -o/dev/null -v \"https://index.docker.io/v1/search&q=centos\"\n- About to connect() to index.docker.io port 443 (#0)\n-   Trying 162.242.195.84... connected\n- Connected to index.docker.io (162.242.195.84) port 443 (#0)\n- Initializing NSS with certpath: sql:/etc/pki/nssdb\n-   CAfile: /etc/pki/tls/certs/ca-bundle.crt\n  CApath: none\n- SSL connection using TLS_DHE_RSA_WITH_AES_128_CBC_SHA\n- Server certificate:\n-       subject: CN=.docker.io,OU=Domain Control Validated - RapidSSL(R),OU=See www.rapidssl.com/resources/cps (c)13,OU=GT98568428,serialNumber=exkd9EjUozUulWIyUDurQPMEPBLSc2Bq\n-       start date: Mar 18 06:53:41 2013 GMT\n-       expire date: Mar 21 08:53:54 2015 GMT\n-       common name: .docker.io\n-       issuer: CN=RapidSSL CA,O=\"GeoTrust, Inc.\",C=US  \n\nGET /v1/search&q=centos HTTP/1.1\nUser-Agent: curl/7.19.7 (x86_64-redhat-linux-gnu) libcurl/7.19.7 NSS/3.16.1 Basic ECC zlib/1.2.3 libidn/1.18 libssh2/1.4.2\nHost: index.docker.io\nAccept: /\n% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0< HTTP/1.1 404 NOT FOUND\n  < Server: nginx\n  < Date: Thu, 20 Nov 2014 09:01:24 GMT\n  < Content-Type: text/html; charset=utf-8\n  < Content-Length: 9121\n  < Connection: close\n  < Vary: Cookie\n  < X-Frame-Options: SAMEORIGIN\n  < Strict-Transport-Security: max-age=31536000\n  <\n  { [data not shown]\n  100  9121  100  9121    0     0   8541      0  0:00:01  0:00:01 --:--:-- 21360* Closing connection #0\n\nFrom the docker machine:\ncurl -o/dev/null -v \"https://index.docker.io/v1/search&q=centos\"\n- Adding handle: conn: 0xe6d9f0\n- Adding handle: send: 0\n- Adding handle: recv: 0\n- Curl_addHandleToPipeline: length: 1\n- - Conn 0 (0xe6d9f0) send_pipe: 1, recv_pipe: 0\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* About to connect() to index.docker.io port 443 (#0)\n-   Trying 162.242.195.84...\n- Connected to index.docker.io (162.242.195.84) port 443 (#0)\n- successfully set certificate verify locations:\n-   CAfile: none\n  CApath: /etc/ssl/certs\n- SSLv3, TLS handshake, Client hello (1):\n  } [data not shown]\n- SSLv3, TLS handshake, Server hello (2):\n  { [data not shown]\n- SSLv3, TLS handshake, CERT (11):\n  { [data not shown]\n- SSLv3, TLS handshake, Server key exchange (12):\n  { [data not shown]\n- SSLv3, TLS handshake, Server finished (14):\n  { [data not shown]\n- SSLv3, TLS handshake, Client key exchange (16):\n  } [data not shown]\n- SSLv3, TLS change cipher, Client hello (1):\n  } [data not shown]\n- SSLv3, TLS handshake, Finished (20):\n  } [data not shown]\n- SSLv3, TLS change cipher, Client hello (1):\n  { [data not shown]\n- SSLv3, TLS handshake, Finished (20):\n  { [data not shown]\n- SSL connection using ECDHE-RSA-AES128-SHA\n- Server certificate:\n-    subject: serialNumber=exkd9EjUozUulWIyUDurQPMEPBLSc2Bq; OU=GT98568428; OU=See www.rapidssl.com/resources/cps (c)13; OU=Domain Control Validated - RapidSSL(R); CN=*.docker.io\n-    start date: 2013-03-18 06:53:41 GMT\n-    expire date: 2015-03-21 08:53:54 GMT\n-    subjectAltName: index.docker.io matched\n-    issuer: C=US; O=GeoTrust, Inc.; CN=RapidSSL CA\n-    SSL certificate verify ok.\n\nGET /v1/search&q=centos HTTP/1.1\nUser-Agent: curl/7.32.0\nHost: index.docker.io\nAccept: /\n< HTTP/1.1 404 NOT FOUND\n- Server nginx is not blacklisted\n  < Server: nginx\n  < Date: Thu, 20 Nov 2014 09:07:10 GMT\n  < Content-Type: text/html; charset=utf-8\n  < Content-Length: 9122\n  < Connection: close\n  < Vary: Cookie\n  < X-Frame-Options: SAMEORIGIN\n  < Strict-Transport-Security: max-age=31536000\n  < \n  { [data not shown]\n  100  9122  100  9122    0     0  14739      0 --:--:-- --:--:-- --:--:-- 14760\n- Closing connection 0\n- SSLv3, TLS alert, Client hello (1):\n  } [data not shown]\n. I have a standalone registry.\n. Yes, but it seems not to be considered. I have also changed the start up script to see the value of the env variable before and after the start of the service. The script returns always local.\n. service docker-registry start\n. I am using the distro provided for CENTOS.\nI first changed the value in the /etc/init.d/docker-registry changing the default value of the SETTINGS_FLAVOR, and restart the service.\nThen I tried to export the env variable SETTINGS_FLAVOR, again without success.\nEach time I restarted the docker-registry service.\n. -e GUNICORN_OPTS=[\"--preload\"] works for me, too\n. Have you done any progress?\n. Looking at #518 I have read that preload can be used if I do not need search. It is not my case.\n\n$ docker run -p 80:5000 -v /opt/docker-registry:/reg -e STORAGE_PATH=/reg/data -e STORAGE_BACKEND=sqlalchemy -e SQLALCHEMY_INDEX_DATABASE:sqlite:////reg/docker-registry.db -e LOGLEVEL=debug -e DEBUG=true -e STANDALONE=true  --name pregistry registry\n.....\n\"GET /v2/ H\"GET /v2/ HTTP/1.1\" 404 233 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [11/May/2015:08:13:36 +0000] \"GET /v1/_ping HTTP/1.1\" 200 1539 \"-\" \"Go 1.1 package http\"\n11/May/2015:08:13:36 +0000 DEBUG: args = {'namespace': 'library', 'repository': u'hello-mine'}\n2015-05-11 08:13:37,739 ERROR: Exception on /v1/repositories/hello-mine/ [PUT]TTP/1.1\" 404 233 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [11/May/2015:08:13:36 +0000] \"GET /v1/_ping HTTP/1.1\" 200 1539 \"-\" \"Go 1.1 package http\"\n11/May/2015:08:13:36 +0000 DEBUG: args = {'namespace': 'library', 'repository': u'hello-mine'}\n2015-05-11 08:13:37,739 ERROR: Exception on /v1/repositories/hello-mine/ [PUT]\n.....\nOSError: [Errno 13] Permission denied: '/reg/data/repositories/library'\n11/May/2015:08:13:37 +0000 ERROR: Exception on /v1/repositories/hello-mine/ [PUT]\n. I fixed the first problem changing STORAGE_BACKEND in SEARCH_BACKEND and using = instead of : when I set SQLALCHEMY_INDEX_DATABASE:\ndocker run -p 80:5000 -v /opt/docker-registry:/reg -e \nSTORAGE_PATH=/reg/data -e SEARCH_BACKEND=sqlalchemy -e SQLALCHEMY_INDEX_DATABASE=sqlite:////reg/docker-registry.db -e LOGLEVEL=debug -e DEBUG=true -e STANDALONE=true -e SETTINGS_FLAVOR=local --name pregistry registry\nFor the second problem I disabled selinux.\n. I am using \ncat /etc/redhat-release \nCentOS Linux release 7.1.1503 (Core)\n. docker run -p 80:5000 -v /etc/group:/etc/group -v /etc/passwd:/etc/passwd --user registry:registry -v /opt/docker-registry:/reg -e STORAGE_PATH=/reg/data -e SEARCH_BACKEND=sqlalchemy -e SQLALCHEMY_INDEX_DATABASE=sqlite:////reg/docker-registry.db -e LOGLEVEL=debug -e DEBUG=true -e STANDALONE=true -e SETTINGS_FLAVOR=local  --name basic_registry registry\n[2015-05-12 11:02:09 +0000] [1] [INFO] Starting gunicorn 19.1.1\n[2015-05-12 11:02:09 +0000] [1] [INFO] Listening at: http://0.0.0.0:5000 (1)\n[2015-05-12 11:02:09 +0000] [1] [INFO] Using worker: gevent\n[2015-05-12 11:02:09 +0000] [13] [INFO] Booting worker with pid: 13\n[2015-05-12 11:02:09 +0000] [14] [INFO] Booting worker with pid: 14\n[2015-05-12 11:02:09 +0000] [15] [INFO] Booting worker with pid: 15\n[2015-05-12 11:02:10 +0000] [16] [INFO] Booting worker with pid: 16\n12/May/2015:11:02:10 +0000 WARNING: Cache storage disabled!\n12/May/2015:11:02:10 +0000 WARNING: Cache storage disabled!\n12/May/2015:11:02:10 +0000 WARNING: LRU cache disabled!\n12/May/2015:11:02:10 +0000 WARNING: LRU cache disabled!\n12/May/2015:11:02:10 +0000 WARNING: Cache storage disabled!\n12/May/2015:11:02:10 +0000 WARNING: LRU cache disabled!\n12/May/2015:11:02:10 +0000 WARNING: Cache storage disabled!\n12/May/2015:11:02:10 +0000 WARNING: LRU cache disabled!\n12/May/2015:11:02:10 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n12/May/2015:11:02:10 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n12/May/2015:11:02:10 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n12/May/2015:11:02:10 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n[2015-05-12 11:02:10 +0000] [14] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 325, in wrapper\n    lock_file = open(lock_path, 'w')\nIOError: [Errno 13] Permission denied: './registry._setup_database.lock'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 325, in wrapper\n    lock_file = open(lock_path, 'w')\nIOError: [Errno 13] Permission denied: './registry._setup_database.lock'\n[2015-05-12 11:02:10 +0000] [16] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 325, in wrapper\n    lock_file = open(lock_path, 'w')\nIOError: [Errno 13] Permission denied: './registry._setup_database.lock'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 325, in wrapper\n    lock_file = open(lock_path, 'w')\nIOError: [Errno 13] Permission denied: './registry._setup_database.lock'\n[2015-05-12 11:02:10 +0000] [14] [INFO] Worker exiting (pid: 14)\n[2015-05-12 11:02:10 +0000] [16] [INFO] Worker exiting (pid: 16)\n[2015-05-12 11:02:10 +0000] [13] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 325, in wrapper\n    lock_file = open(lock_path, 'w')\nIOError: [Errno 13] Permission denied: './registry._setup_database.lock'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 325, in wrapper\n    lock_file = open(lock_path, 'w')\nIOError: [Errno 13] Permission denied: './registry._setup_database.lock'\n[2015-05-12 11:02:10 +0000] [13] [INFO] Worker exiting (pid: 13)\n[2015-05-12 11:02:10 +0000] [15] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 325, in wrapper\n    lock_file = open(lock_path, 'w')\nIOError: [Errno 13] Permission denied: './registry._setup_database.lock'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 325, in wrapper\n    lock_file = open(lock_path, 'w')\nIOError: [Errno 13] Permission denied: './registry._setup_database.lock'\n[2015-05-12 11:02:10 +0000] [15] [INFO] Worker exiting (pid: 15)\n[2015-05-12 11:02:10 +0000] [1] [INFO] Shutting down: Master\n[2015-05-12 11:02:10 +0000] [1] [INFO] Reason: Worker failed to boot.\nIn the local volume I have:\nls -al docker-registry/data/\ntotal 4\ndrwxr-xr-x. 4 registry registry   38 11 mag 15.59 .\ndrwxr-xr-x. 3 registry registry   42 11 mag 15.59 ..\ndrwxr-xr-x. 7 registry registry 4096 11 mag 15.59 images\ndrwxr-xr-x. 3 registry registry   20 11 mag 15.59 repositories\n. It seems that I have the same problem as the one specified in #915\n. On the local machine, I was able to create a file in the /opt/docker-registry by using the registry user.\nOn the other hand, I obtained permission denied \nIOError: [Errno 13] Permission denied: './registry._setup_database.lock'\nwhen I run the registry container that I got from \ndocker.io/registry                     latest\ndocker run -p 80:5000 -v /etc/group:/etc/group -v /etc/passwd:/etc/passwd --user registry:registry -v /opt/docker-registry:/reg -e STORAGE_PATH=/reg/data -e SEARCH_BACKEND=sqlalchemy -e SQLALCHEMY_INDEX_DATABASE=sqlite:////reg/docker-registry.db -e LOGLEVEL=debug -e DEBUG=true -e STANDALONE=true -e SETTINGS_FLAVOR=local  --name basic_registry registry\n. About CA please have a look at http://kb.kerio.com/product/kerio-connect/server-configuration/ssl-certificates/adding-trusted-root-certificates-to-the-server-1605.html\n. ",
    "eolamey": "Hi,\nI have tested a slightly shorter nginx configuration, which seems to allow pull for everyone but restrict push for authenticated users.\nregistry.conf:\n```\nserver {\n  ...\nlocation /v1/_ping {\n    auth_basic off;\n    include registry.proxy;\n  }\nlocation /v1 {\n    include registry.auth;\n    include registry.proxy;\n  }\n}\n```\nregistry.proxy:\n```\nproxy_set_header X-Forwarded-Proto $scheme;\nproxy_set_header Host              $http_host;\nproxy_set_header X-Real-IP         $remote_addr;\nproxy_set_header Authorization     \"\";\nproxy_pass http://127.0.0.1:5000;\n```\nregistry.auth:\nlimit_except GET HEAD {\n  auth_basic \"Docker Registry\";\n  auth_basic_user_file registry.passwd;\n}\nI have split registry.auth in a different file so that I can change its content when I run the docker-registry container. The nginx startup script I use selects an appropriate registry.auth file depending on the value of an environment variable, so I can have one that completely disable authentication, for exemple.\n. ",
    "dothebart": "Hm, meanwhile it seems that if \ndocker login\nrequests\nGET /v2/ \nand doesn't get the request to authenticate it will not save the credentials. \nIf you later on docker push to that site it will break on the HTTP 401s its getting from the post requests and will not try to authenticate then.\n. ",
    "wdq347": "@dothebart \nwhen using registry v2,  nginx is useless. \nyou can use docker_auth image to realize this object, expecially more complex user ACLS. Please looking https://github.com/cesanta/docker_auth for more\n. ",
    "Lawouach": "This. The current documentation is quite unclear unfortunately.\n. Thanks @dmp42 for the feedback.\nThanks also @tcurdt, I had issues with the built-in registry server which hangs sometimes. Might be a network issue, hard to debug. I'll give this one a try.\n. ",
    "tcurdt": "That's kind of good news. But could you set some expectations of when a release (or beta) can be expected, @dmp42?\nAs I only need a local storage registry (on a remote machine) I am tempted to play with https://github.com/mafintosh/docker-registry-server here for the moment.\n. ",
    "rhasselbaum": "Thanks. What bug does this one duplicate? (I'd like to keep an eye on it.)\n. ",
    "databus23": "+1 We are are also seeing these errors. We are running multiple instances of a private registry, all using the same nfs share for storage. we are running version 0.9.1 of the registry.\n. +1 for releasing this. \n. ",
    "BugRoger": "Mounting the NFS shares with the option lookupcache=none magically fixes this problem. Quoting the nfs man page:\nNote:  lookupcache=none can adversely affect per-\n              formance, but may be necessary  if  shared  files\n              created or deleted on the server need to be imme-\n              diately visible to any  applications  running  on\n              NFS clients.\n. As reference this is a duplicate of #775. The same fix applies.\n. ",
    "kadishmal": "I had to disable cache via Redis in the registry config.yml for this error to disappear.\n. ",
    "hex108": "I just sent a pull request(https://github.com/docker/docker-registry/pull/779)  for this problem.\n. It seems that it will fail even without my code change.\n. Update the code. \n@wking @dmp42  Thank you for the help! Could you please help review it? \n. Yes, it is in a 'while True' statement, and it will call f() several times which is specified by 'retry_times' until f() succeeds. \n. ",
    "fionawhim": "Yes, I think @shin- explained it. Would you like me to make a PR to update the README with this info?\n. ",
    "willwnekowicz": "You'll need the root certificate of the registry server, then install it on your client machine using one of the below:\nOSX through Boot2Docker\n\nSSH into your boot2docker virtual machine:\nboot2docker ssh\nAppend certificate to ca-certificates.crt:  \nsudo vi /etc/ssl/certs/ca-certificates.crt\nG a ENTER ctrl-v :x ENTER\nRestart Docker:\nsudo /etc/init.d/docker restart\nExit the VM:\nexit\n\nLinux\n\nmkdir /usr/local/share/ca-certificates/registry-root-cert\ncp registryrootCA.crt /usr/local/share/ca-certificates/registry-root-cert\nupdate-ca-certificates\n. \n",
    "hordemark": "thanks for reply. another issue.\nhow to use --insecure-registry..\n. OK @dmp42 \nnginx.conf \nuser  nginx;\nworker_processes  1;\nerror_log  /var/log/nginx/error.log warn;\npid        /var/run/nginx.pid;\nevents {\n    worker_connections  1024;\n}\nhttp {\n    include       /etc/nginx/mime.types;\n    default_type  application/octet-stream;\n```\nlog_format  main  '$remote_addr - $remote_user [$time_local] \"$request\" '\n                  '$status $body_bytes_sent \"$http_referer\" '\n                  '\"$http_user_agent\" \"$http_x_forwarded_for\"';\naccess_log  /var/log/nginx/access.log  main;\nsendfile        on;\ntcp_nopush     on;\nkeepalive_timeout  65;\ngzip  on;\ninclude /etc/nginx/conf.d/*.conf;\n```\n}\nconf.d/docker-registry.conf\nupstream docker-registry {\n server localhost:5000;\n}\nserver {\n listen 8080;\n server_name docker-registry.rocketsoftware.com;\n# ssl on;\n # ssl_certificate /etc/ssl/certs/docker-registry;\n # ssl_certificate_key /etc/ssl/private/docker-registry;\nproxy_set_header Host       $http_host;   # required for Docker client sake\n proxy_set_header X-Real-IP  $remote_addr; # pass on real client IP\nclient_max_body_size 0; # disable any limits to avoid HTTP 413 for large image uploads\n# required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\n chunked_transfer_encoding on;\nlocation / {\n     # let Nginx know about our auth file\n     auth_basic              \"Restricted\";\n     auth_basic_user_file    docker-registry.htpasswd;\n     proxy_pass http://docker-registry;\n }\n location /_ping {\n     auth_basic off;\n     proxy_pass http://docker-registry;\n }\n location /v1/_ping {\n     auth_basic off;\n     proxy_pass http://docker-registry;\n }\n}\ncurl username:password@docker-registry.rocketsoftware.com:8080\nit will appear 502 error.\nAnd the log .\n2014/12/04 20:17:41 [crit] 1693#0: 7 connect() to [::1]:5000 failed (13: Permission denied) while connecting to upstream, client: 192.168.90.42, server: docker-registry.rocketsoftware.com, request: \"GET / HTTP/1.1\", upstream: \"http://[::1]:5000/\", host: \"docker-registry.rocketsoftware.com:8080\"\n2014/12/04 20:17:41 [crit] 1693#0: 7 connect() to 127.0.0.1:5000 failed (13: Permission denied) while connecting to upstream, client: 192.168.90.42, server: docker-registry.rocketsoftware.com, request: \"GET / HTTP/1.1\", upstream: \"http://127.0.0.1:5000/\", host: \"docker-registry.rocketsoftware.com:8080\"\n. I had resolved but do not understand.\nwhen start daemon with sudo service nginx start, then curl perform error\nwhen sudo nginx, everything goes well.\n. ",
    "koyadume": "@hordemark  Check this out - http://wanderingquandaries.blogspot.in/2014/11/setting-up-insecure-docker-registry.html\n. Thank you @sgirones @dmp42.\n. Yes I looked into that before posting this question. There are only few lines there but it does not say much about when it is useful. Normally a mirror will download all the artifacts from the source in advance but a proxy will do that on demand basis.\n. Not sure if I have understood it correctly but it's not working in a way I expect. \nHere is my setup -\n``````\ncat /etc/default/docker\nDOCKER_OPTS=\"-H tcp://0.0.0.0:5555 -H unix:///var/run/docker.sock --insecure-registry localhost:5000 --registry-mirror http://localhost:5000\"\n========================================\ncat ~/docker-registry/config/config.yml\ncommon: &common\n    mirroring:\n        source: https://registry-1.docker.io\n        source_index: https://index.docker.io\n        tags_cache_ttl: 172800  # seconds\ndev:\n    <<: *common\n    storage: local\n    storage_path: /tmp/registry\n    loglevel: debug\n===================================\nsudo docker run -p 5000:5000 -v ~/docker-registry/config:/registry-conf -e DOCKER_REGISTRY_CONFIG=/registry-conf/config.yml registry\n\na) Running sudo docker pull ubuntu is fetching image from Docker hub and not hitting local registry at all.\nb) Running sudo docker pull localhost:5000/ubuntu hits local registry but returns following response -\nPulling repository localhost:5000/ubuntu\n2014/12/09 07:33:33 HTTP code: 405\n``````\nSo I am unable to run my local registry as proxy for docker hub. \n. ",
    "sgirones": "Same here. The weird thing is that it's not easy reproducible. About 1 out of 4 runs fails for me.\n. @koyadume check this out: https://github.com/docker/docker-registry/issues/518\nThe 'solution' they suggest works for me: \ndocker run -d -e GUNICORN_OPTS=[--preload] -p 5000:5000 registry\n. ",
    "remoe": "Thanks @sgirones. This fixed also this issue on my system (docker-1.4.1, coreos 575.0)\n. ",
    "grossws": "Is there a simple way to get them? E. g. some boto debug flag?\n. I've set debug = 2 in /etc/boto.cfg and don't see any requests to other region, all of them are sent to eu-east-1.\nI had forget that I have also turned on detailed billing statistics. Bucket for statistics is in eu-west-1 but it can be pushed from us-east-1 by Amazon.\nSorry for inconvenience.\n. ",
    "johnmcteague": "Dig:\n```\nubuntu@cf5-bastion:~$ dig docker.example.com\n; <<>> DiG 9.9.5-3-Ubuntu <<>> docker.example.com\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 28742\n;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; udp: 4096\n;; QUESTION SECTION:\n;docker.example.com.  IN      A\n;; ANSWER SECTION:\ndocker.example.com. 3255 IN   A       10.100.138.173\n;; Query time: 47 msec\n;; SERVER: 169.82.112.62#53(169.82.112.62)\n;; WHEN: Sat Nov 29 15:20:48 UTC 2014\n;; MSG SIZE  rcvd: 73\n```\ncurl ( i am running registry with http and the docker daemon has the insecure-registry config option enabled)\nubuntu@cf5-bastion:~$ curl http://docker.example.com:5000/v1/_ping\n{\"host\": [\"Linux\", \"docker-host\", \"3.13.0-36-generic\", \"#63-Ubuntu SMP Wed Sep 3 21:30:07 UTC 2014\", \"x86_64\", \"x86_64\"], \"launch\": [\"/usr/local/bin/gunicorn\", \"--access-logfile\", \"-\", \"--error-logfile\", \"-\", \"--max-requests\", \"100\", \"-k\", \"gevent\", \"--graceful-timeout\", \"3600\", \"-t\", \"3600\", \"-w\", \"4\", \"-b\", \"0.0.0.0:5000\", \"--reload\", \"docker_registry.wsgi:application\"], \"versions\": {\"SocketServer\": \"0.4\", \"argparse\": \"1.1\", \"backports.lzma\": \"0.0.3\", \"blinker\": \"1.3\", \"cPickle\": \"1.71\", \"cgi\": \"2.6\", \"ctypes\": \"1.1.0\", \"decimal\": \"1.70\", \"distutils\": \"2.7.6\", \"docker_registry.app\": \"0.9.0\", \"docker_registry.core\": \"2.0.3\", \"docker_registry.server\": \"0.9.0\", \"email\": \"4.0.3\", \"flask\": \"0.10.1\", \"gevent\": \"1.0.1\", \"greenlet\": \"0.4.2\", \"gunicorn\": \"19.1.0\", \"gunicorn.arbiter\": \"19.1.0\", \"gunicorn.config\": \"19.1.0\", \"gunicorn.six\": \"1.2.0\", \"jinja2\": \"2.7.3\", \"json\": \"2.0.9\", \"logging\": \"0.5.1.2\", \"parser\": \"0.5\", \"pickle\": \"$Revision: 72223 $\", \"platform\": \"1.0.7\", \"python\": \"2.7.6 (default, Mar 22 2014, 22:59:56) \\n[GCC 4.8.2]\", \"re\": \"2.2.1\", \"redis\": \"2.10.3\", \"requests\": \"2.3.0\", \"requests.packages.chardet\": \"2.2.1\", \"requests.packages.urllib3\": \"dev\", \"requests.packages.urllib3.packages.six\": \"1.2.0\", \"requests.utils\": \"2.3.0\", \"rsa\": \"3.1.4\", \"simplejson\": \"3.6.2\", \"sqlalchemy\": \"0.9.4\", \"tarfile\": \"$Revision: 85213 $\", \"urllib\": \"1.17\", \"urllib2\": \"2.7\", \"werkzeug\": \"0.9.6\", \"yaml\": \"3.11\", \"zlib\": \"1.0\"}}\nubuntu@cf5-bastion:~$\nDebug enabled on docker daemon:\nubuntu@cf5-bastion:~$ ps -aef | grep docker\n root     29877     1  0 15:14 ?        00:00:00 /usr/bin/docker -d -D --insecure-registry     docker.example.com:5000\n ubuntu   29992 29794  0 15:27 pts/0    00:00:00 grep --color=auto docker\nRe-running the pull:\nubuntu@cf5-bastion:~$ sudo docker -D pull docker.example.com:5000/john/qpid:latest\nPulling repository docker.example.com:5000/john/qpid\n2014/11/29 15:28:20 Error: image john/qpid not found\n. From c5-bastion when trying to pull my image:\n[debug] server.go:1181 Calling POST /images/create\n[info] POST /v1.15/images/create?fromImage=docker.example.com%3A5000%2Fjohn%2Fqpid%3Alatest\n[0d7de11b] +job pull(docker.cloudlab.jpmchase.net:5000/john/qpid, latest)\n[debug] endpoint.go:59 Error from registry \"https://docker.example.com:5000/v1/\" marked as insecure: Get https://docker.example.com:5000/v1/_ping: Service Unavailable. Insecurely falling back to HTTP\n[debug] endpoint.go:137 Error unmarshalling the _ping RegistryInfo: invalid character '<' looking for beginning of value\n[debug] endpoint.go:144 RegistryInfo.Version: \"\"\n[debug] endpoint.go:147 Registry standalone header: ''\n[debug] endpoint.go:155 RegistryInfo.Standalone: true\n[debug] session.go:266 [registry] Calling GET http://docker.example.com:5000/v1/repositories/john/qpid/images\n[debug] http.go:162 http://docker.example.com:5000/v1/repositories/john/qpid/images -- HEADERS: map[User-Agent:[docker/1.3.2 go/go1.3.3 git-commit/39fa2fa kernel/3.13.0-36-generic os/linux arch/amd64]]\nError: image john/qpid not found\nFrom the same host when pulling busybox:\n[debug] server.go:1181 Calling POST /images/create\n[info] POST /v1.15/images/create?fromImage=busybox%3Alatest\n[0d7de11b] +job pull(busybox, latest)\n[0d7de11b] +job trust_update_base()\n[debug] trusts.go:179 Fetched 1 base graphs at 2014-12-01 09:44:27.003997452 +0000 UTC\n[0d7de11b] -job trust_update_base() = OK (0)\n[debug] pull.go:463 Pulling tag from V2 registry: \"latest\"\n[debug] session_v2.go:127 [registry] Calling \"GET\" https://registry-1.docker.io/v2/manifest/library/busybox/latest\n[debug] http.go:162 https://registry-1.docker.io/v2/manifest/library/busybox/latest -- HEADERS: map[User-Agent:[docker/1.3.2 go/go1.3.3 git-commit/39fa2fa kernel/3.13.0-36-generic os/linux arch/amd64]]\n[debug] registry.go:107 hostDir: /etc/docker/certs.d/registry-1.docker.io\n[debug] trusts.go:109 Reloaded graph with 1 grants expiring at 2014-12-29 00:08:20.565183779 +0000 UTC\n[0d7de11b] +job trust_key_check(/library/busybox)\n[0d7de11b] -job trust_key_check(/library/busybox) = OK (0)\n[debug] pull.go:68 Key check result: \"verified\"\n[debug] image.go:249 Json string: {{\"id\":\"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\"comment\":\"Imported from -\",\"created\":\"2013-06-13T14:03:50.821769-07:00\",\"container_config\":{\"Hostname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":null,\"Cmd\":null,\"Dns\":null,\"Image\":\"\",\"Volumes\":null,\"VolumesFrom\":\"\"},\"docker_version\":\"0.4.0\",\"architecture\":\"x86_64\"}}\n[debug] image.go:249 Json string: {{\"id\":\"df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b\",\"parent\":\"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\"created\":\"2014-10-01T20:46:07.263351912Z\",\"container\":\"2147a17cb1b2d6626ed78e5ef8ba4c71ce82c884bc3b57ab01e6114ff357cea4\",\"container_config\":{\"Hostname\":\"2147a17cb1b2\",\"Domainname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"Cpuset\":\"\",\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"ExposedPorts\":null,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":[\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"],\"Cmd\":[\"/bin/sh\",\"-c\",\"#(nop) MAINTAINER J\u00e9r\u00f4me Petazzoni \\u003cjerome@docker.com\\u003e\"],\"Image\":\"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\"Volumes\":null,\"WorkingDir\":\"\",\"Entrypoint\":null,\"NetworkDisabled\":false,\"OnBuild\":[]},\"docker_version\":\"1.2.0\",\"author\":\"J\u00e9r\u00f4me Petazzoni \\u003cjerome@docker.com\\u003e\",\"config\":{\"Hostname\":\"2147a17cb1b2\",\"Domainname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"Cpuset\":\"\",\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"ExposedPorts\":null,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":[\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"],\"Cmd\":null,\"Image\":\"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\",\"Volumes\":null,\"WorkingDir\":\"\",\"Entrypoint\":null,\"NetworkDisabled\":false,\"OnBuild\":[]},\"architecture\":\"amd64\",\"os\":\"linux\",\"Size\":0}}\n[debug] image.go:249 Json string: {{\"id\":\"e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644\",\"parent\":\"df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b\",\"created\":\"2014-10-01T20:46:08.684962862Z\",\"container\":\"88f18f678e5d6f2b4fc2da1d24f8aee75c5fc7faf4611aca2419af8cfacd87fd\",\"container_config\":{\"Hostname\":\"88f18f678e5d\",\"Domainname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"Cpuset\":\"\",\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"ExposedPorts\":null,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":[\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"],\"Cmd\":[\"/bin/sh\",\"-c\",\"#(nop) ADD file:00c9f115c2de114ee5848f48ac54997e1504e3fe0a8dedc04e3718496ae684dd in /\"],\"Image\":\"df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b\",\"Volumes\":null,\"WorkingDir\":\"\",\"Entrypoint\":null,\"NetworkDisabled\":false,\"OnBuild\":[]},\"docker_version\":\"1.2.0\",\"author\":\"J\u00e9r\u00f4me Petazzoni \\u003cjerome@docker.com\\u003e\",\"config\":{\"Hostname\":\"88f18f678e5d\",\"Domainname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"Cpuset\":\"\",\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"ExposedPorts\":null,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":[\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"],\"Cmd\":null,\"Image\":\"df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b\",\"Volumes\":null,\"WorkingDir\":\"\",\"Entrypoint\":null,\"NetworkDisabled\":false,\"OnBuild\":[]},\"architecture\":\"amd64\",\"os\":\"linux\",\"Size\":2433303}}\n[debug] image.go:249 Json string: {{\"id\":\"e72ac664f4f0c6a061ac4ef332557a70d69b0c624b6add35f1c181ff7fff2287\",\"parent\":\"e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644\",\"created\":\"2014-10-01T20:46:08.914288461Z\",\"container\":\"8e73b239682fe73338323d9af83d3c5aa5bb7d22a3fe84cbfcf5f47e756d6636\",\"container_config\":{\"Hostname\":\"88f18f678e5d\",\"Domainname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"Cpuset\":\"\",\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"ExposedPorts\":null,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":[\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"],\"Cmd\":[\"/bin/sh\",\"-c\",\"#(nop) CMD [/bin/sh]\"],\"Image\":\"e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644\",\"Volumes\":null,\"WorkingDir\":\"\",\"Entrypoint\":null,\"NetworkDisabled\":false,\"OnBuild\":[]},\"docker_version\":\"1.2.0\",\"author\":\"J\u00e9r\u00f4me Petazzoni \\u003cjerome@docker.com\\u003e\",\"config\":{\"Hostname\":\"88f18f678e5d\",\"Domainname\":\"\",\"User\":\"\",\"Memory\":0,\"MemorySwap\":0,\"CpuShares\":0,\"Cpuset\":\"\",\"AttachStdin\":false,\"AttachStdout\":false,\"AttachStderr\":false,\"PortSpecs\":null,\"ExposedPorts\":null,\"Tty\":false,\"OpenStdin\":false,\"StdinOnce\":false,\"Env\":[\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\"],\"Cmd\":[\"/bin/sh\"],\"Image\":\"e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644\",\"Volumes\":null,\"WorkingDir\":\"\",\"Entrypoint\":null,\"NetworkDisabled\":false,\"OnBuild\":[]},\"architecture\":\"amd64\",\"os\":\"linux\",\"Size\":0}}\n[debug] pull.go:516 pulling blob \"tarsum.dev+sha256:324d4cf44ee7daa46266c1df830c61a7df615c0632176a339e7310e34723d67a\" to V1 img 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\n[debug] session_v2.go:242 [registry] Calling \"GET\" https://registry-1.docker.io/v2/blob/library/busybox/tarsum.dev+sha256/324d4cf44ee7daa46266c1df830c61a7df615c0632176a339e7310e34723d67a\n[debug] http.go:162 https://registry-1.docker.io/v2/blob/library/busybox/tarsum.dev+sha256/324d4cf44ee7daa46266c1df830c61a7df615c0632176a339e7310e34723d67a -- HEADERS: map[User-Agent:[docker/1.3.2 go/go1.3.3 git-commit/39fa2fa kernel/3.13.0-36-generic os/linux arch/amd64]]\n[debug] registry.go:107 hostDir: /etc/docker/certs.d/registry-1.docker.io\n[debug] pull.go:516 pulling blob \"tarsum.dev+sha256:1b755912c77197c6a43539f2a708ef89d5849b8ce02642cb702e47afaa8195c3\" to V1 img df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b\n[debug] session_v2.go:242 [registry] Calling \"GET\" https://registry-1.docker.io/v2/blob/library/busybox/tarsum.dev+sha256/1b755912c77197c6a43539f2a708ef89d5849b8ce02642cb702e47afaa8195c3\n[debug] http.go:162 https://registry-1.docker.io/v2/blob/library/busybox/tarsum.dev+sha256/1b755912c77197c6a43539f2a708ef89d5849b8ce02642cb702e47afaa8195c3 -- HEADERS: map[User-Agent:[docker/1.3.2 go/go1.3.3 git-commit/39fa2fa kernel/3.13.0-36-generic os/linux arch/amd64]]\n[debug] registry.go:107 hostDir: /etc/docker/certs.d/registry-1.docker.io\n[debug] pull.go:516 pulling blob \"tarsum.dev+sha256:b943e30d0fbb7aa86cb52c2e6c7adaed3d8a95e115ac26296527957b4c0116cb\" to V1 img e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644\n[debug] session_v2.go:242 [registry] Calling \"GET\" https://registry-1.docker.io/v2/blob/library/busybox/tarsum.dev+sha256/b943e30d0fbb7aa86cb52c2e6c7adaed3d8a95e115ac26296527957b4c0116cb\n[debug] http.go:162 https://registry-1.docker.io/v2/blob/library/busybox/tarsum.dev+sha256/b943e30d0fbb7aa86cb52c2e6c7adaed3d8a95e115ac26296527957b4c0116cb -- HEADERS: map[User-Agent:[docker/1.3.2 go/go1.3.3 git-commit/39fa2fa kernel/3.13.0-36-generic os/linux arch/amd64]]\n[debug] registry.go:107 hostDir: /etc/docker/certs.d/registry-1.docker.io\n[debug] pull.go:516 pulling blob \"tarsum.dev+sha256:1b755912c77197c6a43539f2a708ef89d5849b8ce02642cb702e47afaa8195c3\" to V1 img e72ac664f4f0c6a061ac4ef332557a70d69b0c624b6add35f1c181ff7fff2287\n[debug] session_v2.go:242 [registry] Calling \"GET\" https://registry-1.docker.io/v2/blob/library/busybox/tarsum.dev+sha256/1b755912c77197c6a43539f2a708ef89d5849b8ce02642cb702e47afaa8195c3\n[debug] http.go:162 https://registry-1.docker.io/v2/blob/library/busybox/tarsum.dev+sha256/1b755912c77197c6a43539f2a708ef89d5849b8ce02642cb702e47afaa8195c3 -- HEADERS: map[User-Agent:[docker/1.3.2 go/go1.3.3 git-commit/39fa2fa kernel/3.13.0-36-generic os/linux arch/amd64]]\n[debug] registry.go:107 hostDir: /etc/docker/certs.d/registry-1.docker.io\n[debug] pull.go:542 Downloaded df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b to tempfile /var/lib/docker/tmp/GetV2ImageBlob129806950\n[debug] pull.go:542 Downloaded 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158 to tempfile /var/lib/docker/tmp/GetV2ImageBlob209084515\n[debug] deviceset.go:455 libdevmapper(3): ioctl/libdm-iface.c:1768 (-1) device-mapper: message ioctl on docker-253:1-531786-pool failed: File exists\n[debug] deviceset.go:255 registerDevice(1, 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158)\n[debug] deviceset.go:281 activateDeviceIfNeeded(511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158)\n[debug] fsdiff.go:123 Start untar layer\n[debug] fsdiff.go:127 Untar time: 0.036431205s\n[debug] deviceset.go:1005 [devmapper] UnmountDevice(hash=511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158)\n[debug] deviceset.go:1028 [devmapper] Unmount(/var/lib/docker/devicemapper/mnt/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158)\n[debug] pull.go:542 Downloaded e72ac664f4f0c6a061ac4ef332557a70d69b0c624b6add35f1c181ff7fff2287 to tempfile /var/lib/docker/tmp/GetV2ImageBlob768886408\n[debug] deviceset.go:1032 [devmapper] Unmount done\n[debug] deviceset.go:786 [devmapper] deactivateDevice(511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158)\n[debug] deviceset.go:880 Waiting for unmount of 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158: opencount=0\n[debug] devmapper.go:545 [devmapper] removeDevice START\n[debug] devmapper.go:558 [devmapper] removeDevice END\n[debug] deviceset.go:842 [deviceset docker-253:1-531786] waitRemove(docker-253:1-531786-511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158)\n[debug] deviceset.go:853 Waiting for removal of docker-253:1-531786-511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158: exists=0\n[debug] deviceset.go:866 [deviceset docker-253:1-531786] waitRemove(docker-253:1-531786-511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158) END\n[debug] deviceset.go:805 [devmapper] deactivateDevice END\n[debug] deviceset.go:1040 [devmapper] UnmountDevice END\n[debug] deviceset.go:255 registerDevice(2, df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b)\n[debug] deviceset.go:281 activateDeviceIfNeeded(df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b)\n[debug] fsdiff.go:123 Start untar layer\n[debug] fsdiff.go:127 Untar time: 0.051067054s\n[debug] deviceset.go:281 activateDeviceIfNeeded(511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158)\n[debug] deviceset.go:1005 [devmapper] UnmountDevice(hash=511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158)\n[debug] deviceset.go:1028 [devmapper] Unmount(/var/lib/docker/devicemapper/mnt/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158)\n[debug] deviceset.go:1032 [devmapper] Unmount done\n[debug] deviceset.go:786 [devmapper] deactivateDevice(511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158)\n[debug] deviceset.go:880 Waiting for unmount of 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158: opencount=0\n[debug] devmapper.go:545 [devmapper] removeDevice START\n[debug] devmapper.go:558 [devmapper] removeDevice END\n[debug] deviceset.go:842 [deviceset docker-253:1-531786] waitRemove(docker-253:1-531786-511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158)\n[debug] deviceset.go:853 Waiting for removal of docker-253:1-531786-511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158: exists=0\n[debug] deviceset.go:866 [deviceset docker-253:1-531786] waitRemove(docker-253:1-531786-511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158) END\n[debug] deviceset.go:805 [devmapper] deactivateDevice END\n[debug] deviceset.go:1040 [devmapper] UnmountDevice END\n[debug] deviceset.go:1005 [devmapper] UnmountDevice(hash=df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b)\n[debug] deviceset.go:1028 [devmapper] Unmount(/var/lib/docker/devicemapper/mnt/df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b)\n[debug] deviceset.go:1032 [devmapper] Unmount done\n[debug] deviceset.go:786 [devmapper] deactivateDevice(df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b)\n[debug] deviceset.go:880 Waiting for unmount of df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b: opencount=0\n[debug] devmapper.go:545 [devmapper] removeDevice START\n[debug] devmapper.go:558 [devmapper] removeDevice END\n[debug] deviceset.go:842 [deviceset docker-253:1-531786] waitRemove(docker-253:1-531786-df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b)\n[debug] deviceset.go:853 Waiting for removal of docker-253:1-531786-df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b: exists=0\n[debug] deviceset.go:866 [deviceset docker-253:1-531786] waitRemove(docker-253:1-531786-df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b) END\n[debug] deviceset.go:805 [devmapper] deactivateDevice END\n[debug] deviceset.go:1040 [devmapper] UnmountDevice END\n[debug] pull.go:542 Downloaded e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644 to tempfile /var/lib/docker/tmp/GetV2ImageBlob216127629\n[debug] deviceset.go:255 registerDevice(3, e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644)\n[debug] deviceset.go:281 activateDeviceIfNeeded(e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644)\n[debug] fsdiff.go:123 Start untar layer\n[debug] fsdiff.go:127 Untar time: 0.094659556s\n[debug] deviceset.go:281 activateDeviceIfNeeded(df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b)\n[debug] deviceset.go:1005 [devmapper] UnmountDevice(hash=df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b)\n[debug] deviceset.go:1028 [devmapper] Unmount(/var/lib/docker/devicemapper/mnt/df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b)\n[debug] deviceset.go:1032 [devmapper] Unmount done\n[debug] deviceset.go:786 [devmapper] deactivateDevice(df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b)\n[debug] deviceset.go:880 Waiting for unmount of df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b: opencount=0\n[debug] devmapper.go:545 [devmapper] removeDevice START\n[debug] devmapper.go:558 [devmapper] removeDevice END\n[debug] deviceset.go:842 [deviceset docker-253:1-531786] waitRemove(docker-253:1-531786-df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b)\n[debug] deviceset.go:853 Waiting for removal of docker-253:1-531786-df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b: exists=0\n[debug] deviceset.go:866 [deviceset docker-253:1-531786] waitRemove(docker-253:1-531786-df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b) END\n[debug] deviceset.go:805 [devmapper] deactivateDevice END\n[debug] deviceset.go:1040 [devmapper] UnmountDevice END\n[debug] deviceset.go:1005 [devmapper] UnmountDevice(hash=e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644)\n[debug] deviceset.go:1028 [devmapper] Unmount(/var/lib/docker/devicemapper/mnt/e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644)\n[debug] deviceset.go:1032 [devmapper] Unmount done\n[debug] deviceset.go:786 [devmapper] deactivateDevice(e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644)\n[debug] deviceset.go:880 Waiting for unmount of e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644: opencount=0\n[debug] devmapper.go:545 [devmapper] removeDevice START\n[debug] devmapper.go:558 [devmapper] removeDevice END\n[debug] deviceset.go:842 [deviceset docker-253:1-531786] waitRemove(docker-253:1-531786-e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644)\n[debug] deviceset.go:853 Waiting for removal of docker-253:1-531786-e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644: exists=0\n[debug] deviceset.go:866 [deviceset docker-253:1-531786] waitRemove(docker-253:1-531786-e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644) END\n[debug] deviceset.go:805 [devmapper] deactivateDevice END\n[debug] deviceset.go:1040 [devmapper] UnmountDevice END\n[debug] deviceset.go:255 registerDevice(4, e72ac664f4f0c6a061ac4ef332557a70d69b0c624b6add35f1c181ff7fff2287)\n[debug] deviceset.go:281 activateDeviceIfNeeded(e72ac664f4f0c6a061ac4ef332557a70d69b0c624b6add35f1c181ff7fff2287)\n[debug] fsdiff.go:123 Start untar layer\n[debug] fsdiff.go:127 Untar time: 0.031884673s\n[debug] deviceset.go:281 activateDeviceIfNeeded(e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644)\n[debug] deviceset.go:1005 [devmapper] UnmountDevice(hash=e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644)\n[debug] deviceset.go:1028 [devmapper] Unmount(/var/lib/docker/devicemapper/mnt/e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644)\n[debug] deviceset.go:1032 [devmapper] Unmount done\n[debug] deviceset.go:786 [devmapper] deactivateDevice(e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644)\n[debug] deviceset.go:880 Waiting for unmount of e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644: opencount=0\n[debug] devmapper.go:545 [devmapper] removeDevice START\n[debug] devmapper.go:558 [devmapper] removeDevice END\n[debug] deviceset.go:842 [deviceset docker-253:1-531786] waitRemove(docker-253:1-531786-e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644)\n[debug] deviceset.go:853 Waiting for removal of docker-253:1-531786-e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644: exists=0\n[debug] deviceset.go:866 [deviceset docker-253:1-531786] waitRemove(docker-253:1-531786-e433a6c5b276a31aa38bf6eaba9cd1cfd69ea33f706ed72b3f20bafde5cd8644) END\n[debug] deviceset.go:805 [devmapper] deactivateDevice END\n[debug] deviceset.go:1040 [devmapper] UnmountDevice END\n[debug] deviceset.go:1005 [devmapper] UnmountDevice(hash=e72ac664f4f0c6a061ac4ef332557a70d69b0c624b6add35f1c181ff7fff2287)\n[debug] deviceset.go:1028 [devmapper] Unmount(/var/lib/docker/devicemapper/mnt/e72ac664f4f0c6a061ac4ef332557a70d69b0c624b6add35f1c181ff7fff2287)\n[debug] deviceset.go:1032 [devmapper] Unmount done\n[debug] deviceset.go:786 [devmapper] deactivateDevice(e72ac664f4f0c6a061ac4ef332557a70d69b0c624b6add35f1c181ff7fff2287)\n[debug] deviceset.go:880 Waiting for unmount of e72ac664f4f0c6a061ac4ef332557a70d69b0c624b6add35f1c181ff7fff2287: opencount=0\n[debug] devmapper.go:545 [devmapper] removeDevice START\n[debug] devmapper.go:558 [devmapper] removeDevice END\n[debug] deviceset.go:842 [deviceset docker-253:1-531786] waitRemove(docker-253:1-531786-e72ac664f4f0c6a061ac4ef332557a70d69b0c624b6add35f1c181ff7fff2287)\n[debug] deviceset.go:853 Waiting for removal of docker-253:1-531786-e72ac664f4f0c6a061ac4ef332557a70d69b0c624b6add35f1c181ff7fff2287: exists=0\n[debug] deviceset.go:866 [deviceset docker-253:1-531786] waitRemove(docker-253:1-531786-e72ac664f4f0c6a061ac4ef332557a70d69b0c624b6add35f1c181ff7fff2287) END\n[debug] deviceset.go:805 [devmapper] deactivateDevice END\n[debug] deviceset.go:1040 [devmapper] UnmountDevice END\n[0d7de11b] -job pull(busybox, latest) = OK (0)\nI can pull my image from docker-host (which is where the registry is running)\n[debug] server.go:1181 Calling POST /images/create\n[info] POST /v1.15/images/create?fromImage=docker.example.com%3A5000%2Fjohn%2Fqpid%3Alatest\n[b5e3f943] +job pull(docker.example.com:5000/john/qpid, latest)\n[debug] endpoint.go:59 Error from registry \"https://docker.example.com:5000/v1/\" marked as insecure: Get https://docker.example.com:5000/v1/_ping: EOF. Insecurely falling back to HTTP\n[debug] endpoint.go:144 RegistryInfo.Version: \"\"\n[debug] endpoint.go:147 Registry standalone header: 'True'\n[debug] endpoint.go:155 RegistryInfo.Standalone: true\n[debug] session.go:266 [registry] Calling GET http://docker.example.com:5000/v1/repositories/john/qpid/images\n[debug] http.go:162 http://docker.example.com:5000/v1/repositories/john/qpid/images -- HEADERS: map[User-Agent:[docker/1.3.2 go/go1.3.3 git-commit/39fa2fa kernel/3.13.0-36-generic os/linux arch/amd64]]\n[debug] pull.go:172 Retrieving the tag list\n[debug] http.go:162 http://docker.example.com:5000/v1/repositories/john/qpid/tags -- HEADERS: map[User-Agent:[docker/1.3.2 go/go1.3.3 git-commit/39fa2fa kernel/3.13.0-36-generic os/linux arch/amd64]]\n[debug] session.go:222 Got status code 200 from http://docker.example.com:5000/v1/repositories/john/qpid/tags\n[debug] pull.go:187 Registering tags\n[debug] pull.go:210 () does not match latest (id: 11cd3ea407491a9ca21a619650e6b002c5beb42c391bb5b02362bf678e08c27d), skipping\n[debug] pull.go:210 () does not match latest (id: e2a5f0de25f647830cf2794c3542fda779dddf614b0f1b2e77797e07657a91de), skipping\n[debug] pull.go:210 () does not match latest (id: dc07507cef4200ac81f4adfa08b984e3dd53edee60ee0b70c1c2c16e79cffcbe), skipping\n[debug] pull.go:210 () does not match latest (id: 78e82ee876a2e0f0bed96087aa4d5d17f12720a00be1b441c6a015544cff9d29), skipping\n[debug] pull.go:210 () does not match latest (id: ff34cb32d652795c3a1f184a0c4d3548153433daffbf98ba01cd7eef34436ded), skipping\n[debug] pull.go:210 () does not match latest (id: 86ce37374f40e95cfe8af7327c34ea9919ef216ea965377565fcfad3c378a2c3), skipping\n[debug] pull.go:210 () does not match latest (id: 3f45ca85fedcfc575a9fc19601729387073862a2dcac07051bd2d48e562c5227), skipping\n[debug] pull.go:210 () does not match latest (id: 5bc37dc2dfba434d7551fa17ee9d3ebba6adab648cebc2f962e612855c4c580c), skipping\n[debug] pull.go:210 () does not match latest (id: c938d9bc70e46b52a846ac360888ae0773a3dc72de406fc49e82434caab83dff), skipping\n[debug] pull.go:210 () does not match latest (id: b05c5c3eebe081a111db6de51bbf28eba804e12fa0a58479c2996720a2e5b1a2), skipping\n[debug] pull.go:210 () does not match latest (id: a3abe1e82a8da534c71e3705f659748774b5d7a9daa6edf45b69188c6ff719d8), skipping\n[debug] http.go:162 http://docker.example.com:5000/v1/images/abb17d93e3692f1928bcad632077fdada64a90bad846503c9519632fd4cca48a/ancestry -- HEADERS: map[User-Agent:[docker/1.3.2 go/go1.3.3 git-commit/39fa2fa kernel/3.13.0-36-generic os/linux arch/amd64]]\n[debug] pull.go:210 () does not match latest (id: 7a2d7e3cfd4b97665c01a2d9dc3a9bddf6cecee62f0d73aba10aaf3b99d74eef), skipping\n[debug] pull.go:210 () does not match latest (id: 048a550abbebe0ddae41ff8ac9ba068c693dfb261d509bd5013d2da3745bef05), skipping\n[debug] pull.go:210 () does not match latest (id: 5f297df8c1b1754fb633f0b4b1eb04b5d385717d72bd80b85ac7bfca0d4e608e), skipping\n[debug] pull.go:210 () does not match latest (id: 511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158), skipping\n[debug] pull.go:210 () does not match latest (id: f6df77542c783f3c64b3cf97d540b19cdffa044d2d3db2ef72249900de5cee00), skipping\n[debug] pull.go:210 () does not match latest (id: c8935a8315e167ebcc9b7a00042f416918528ff60aed567ab24d8abf2ef01433), skipping\n[debug] pull.go:210 () does not match latest (id: d4947dfd6bf6722715912864c3f0f688e3077cd3132dc62589339a28edb42961), skipping\n[debug] pull.go:210 () does not match latest (id: 69bad55c0c007cefbd7c7a4f05b1bd606a377d6a1b2986565825c9eae56dadf9), skipping\n[debug] pull.go:210 () does not match latest (id: 0cb9ca8d54b54f1daa25e202f8177d59f19344714d18d79401b890666ea96b87), skipping\n[debug] pull.go:210 () does not match latest (id: 61cb619d86bc60410bf04d249910f8ffffdb586bd0fc94b46c19eb39ecb1cbb1), skipping\n[debug] pull.go:210 () does not match latest (id: 1d6add8470d893d4344734c995f7b18026c08374d1c9f7950586ea4e84ebd7f7), skipping\n[debug] pull.go:210 () does not match latest (id: a9ef5124113e5b5164e2524fe7a9c0be45ffc2a7cb97ea0a65f2a494f46caabf), skipping\n[debug] pull.go:210 () does not match latest (id: a159cd28c9a3127f5b8e3d89bb306d6bceeaf3649a8d720f4bb284b09a7f48e2), skipping\n[debug] session.go:95 Ancestry: [\"abb17d93e3692f1928bcad632077fdada64a90bad846503c9519632fd4cca48a\", \"1d6add8470d893d4344734c995f7b18026c08374d1c9f7950586ea4e84ebd7f7\", \"d4947dfd6bf6722715912864c3f0f688e3077cd3132dc62589339a28edb42961\", \"a159cd28c9a3127f5b8e3d89bb306d6bceeaf3649a8d720f4bb284b09a7f48e2\", \"0cb9ca8d54b54f1daa25e202f8177d59f19344714d18d79401b890666ea96b87\", \"048a550abbebe0ddae41ff8ac9ba068c693dfb261d509bd5013d2da3745bef05\", \"c8935a8315e167ebcc9b7a00042f416918528ff60aed567ab24d8abf2ef01433\", \"a3abe1e82a8da534c71e3705f659748774b5d7a9daa6edf45b69188c6ff719d8\", \"11cd3ea407491a9ca21a619650e6b002c5beb42c391bb5b02362bf678e08c27d\", \"ff34cb32d652795c3a1f184a0c4d3548153433daffbf98ba01cd7eef34436ded\", \"c938d9bc70e46b52a846ac360888ae0773a3dc72de406fc49e82434caab83dff\", \"f6df77542c783f3c64b3cf97d540b19cdffa044d2d3db2ef72249900de5cee00\", \"b05c5c3eebe081a111db6de51bbf28eba804e12fa0a58479c2996720a2e5b1a2\", \"5f297df8c1b1754fb633f0b4b1eb04b5d385717d72bd80b85ac7bfca0d4e608e\", \"e2a5f0de25f647830cf2794c3542fda779dddf614b0f1b2e77797e07657a91de\", \"69bad55c0c007cefbd7c7a4f05b1bd606a377d6a1b2986565825c9eae56dadf9\", \"7a2d7e3cfd4b97665c01a2d9dc3a9bddf6cecee62f0d73aba10aaf3b99d74eef\", \"a9ef5124113e5b5164e2524fe7a9c0be45ffc2a7cb97ea0a65f2a494f46caabf\", \"86ce37374f40e95cfe8af7327c34ea9919ef216ea965377565fcfad3c378a2c3\", \"dc07507cef4200ac81f4adfa08b984e3dd53edee60ee0b70c1c2c16e79cffcbe\", \"78e82ee876a2e0f0bed96087aa4d5d17f12720a00be1b441c6a015544cff9d29\", \"3f45ca85fedcfc575a9fc19601729387073862a2dcac07051bd2d48e562c5227\", \"61cb619d86bc60410bf04d249910f8ffffdb586bd0fc94b46c19eb39ecb1cbb1\", \"5bc37dc2dfba434d7551fa17ee9d3ebba6adab648cebc2f962e612855c4c580c\", \"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"]\n[b5e3f943] -job pull(docker.example.com:5000/john/qpid, latest) = OK (0)\nThe main difference would appear to be the following line:\n[debug] endpoint.go:137 Error unmarshalling the _ping RegistryInfo: invalid character '<' looking for beginning of value\nThis only appears when downloading from c5-bastion, where it fails.\n. From cf5-bastion\n```\nubuntu@cf5-bastion:~$ curl -iv http://docker.example.com:5000/v1/repositories/john/qpid/images\n Hostname was NOT found in DNS cache\n   Trying 10.100.138.173...\n* Connected to docker.example.com (10.100.138.173) port 5000 (#0)\n\nGET /v1/repositories/john/qpid/images HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: docker.example.com:5000\nAccept: /\n< HTTP/1.1 200 OK\nHTTP/1.1 200 OK\n* Server gunicorn/19.1.0 is not blacklisted\n< Server: gunicorn/19.1.0\nServer: gunicorn/19.1.0\n< Date: Mon, 01 Dec 2014 17:04:37 GMT\nDate: Mon, 01 Dec 2014 17:04:37 GMT\n< Connection: keep-alive\nConnection: keep-alive\n< X-Docker-Token: Token signature=PB7F25OCP90JK2EO,repository=\"john/qpid\",access=read\nX-Docker-Token: Token signature=PB7F25OCP90JK2EO,repository=\"john/qpid\",access=read\n< X-Docker-Endpoints: docker.example.com:5000\nX-Docker-Endpoints: docker.example.com:5000\n< Pragma: no-cache\nPragma: no-cache\n< Cache-Control: no-cache\nCache-Control: no-cache\n< Expires: -1\nExpires: -1\n< Content-Type: application/json\nContent-Type: application/json\n< WWW-Authenticate: Token signature=PB7F25OCP90JK2EO,repository=\"john/qpid\",access=read\nWWW-Authenticate: Token signature=PB7F25OCP90JK2EO,repository=\"john/qpid\",access=read\n< Content-Length: 1900\nContent-Length: 1900\n\n<\n[{\"id\": \"86ce37374f40e95cfe8af7327c34ea9919ef216ea965377565fcfad3c378a2c3\"}, {\"id\": \"0cb9ca8d54b54f1daa25e202f8177d59f19344714d18d79401b890666ea96b87\"}, {\"id\": \"048a550abbebe0ddae41ff8ac9ba068c693dfb261d509bd5013d2da3745bef05\"}, {\"id\": \"3f45ca85fedcfc575a9fc19601729387073862a2dcac07051bd2d48e562c5227\"}, {\"id\": \"61cb619d86bc60410bf04d249910f8ffffdb586bd0fc94b46c19eb39ecb1cbb1\"}, {\"id\": \"1d6add8470d893d4344734c995f7b18026c08374d1c9f7950586ea4e84ebd7f7\"}, {\"id\": \"5bc37dc2dfba434d7551fa17ee9d3ebba6adab648cebc2f962e612855c4c580c\"}, {\"id\": \"c938d9bc70e46b52a846ac360888ae0773a3dc72de406fc49e82434caab83dff\"}, {\"id\": \"11cd3ea407491a9ca21a619650e6b002c5beb42c391bb5b02362bf678e08c27d\"}, {\"id\": \"e2a5f0de25f647830cf2794c3542fda779dddf614b0f1b2e77797e07657a91de\"}, {\"id\": \"b05c5c3eebe081a111db6de51bbf28eba804e12fa0a58479c2996720a2e5b1a2\"}, {\"id\": \"5f297df8c1b1754fb633f0b4b1eb04b5d385717d72bd80b85ac7bfca0d4e608e\"}, {\"id\": \"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"}, {\"id\": \"dc07507cef4200ac81f4adfa08b* Connection #0 to host docker.example.com left intact\n984e3dd53edee60ee0b70c1c2c16e79cffcbe\"}, {\"id\": \"f6df77542c783f3c64b3cf97d540b19cdffa044d2d3db2ef72249900de5cee00\"}, {\"id\": \"a9ef5124113e5b5164e2524fe7a9c0be45ffc2a7cb97ea0a65f2a494f46caabf\"}, {\"id\": \"c8935a8315e167ebcc9b7a00042f416918528ff60aed567ab24d8abf2ef01433\"}, {\"id\": \"a159cd28c9a3127f5b8e3d89bb306d6bceeaf3649a8d720f4bb284b09a7f48e2\"}, {\"id\": \"78e82ee876a2e0f0bed96087aa4d5d17f12720a00be1b441c6a015544cff9d29\"}, {\"id\": \"ff34cb32d652795c3a1f184a0c4d3548153433daffbf98ba01cd7eef34436ded\"}, {\"id\": \"7a2d7e3cfd4b97665c01a2d9dc3a9bddf6cecee62f0d73aba10aaf3b99d74eef\"}, {\"id\": \"a3abe1e82a8da534c71e3705f659748774b5d7a9daa6edf45b69188c6ff719d8\"}, {\"id\": \"d4947dfd6bf6722715912864c3f0f688e3077cd3132dc62589339a28edb42961\"}, {\"id\": \"69bad55c0c007cefbd7c7a4f05b1bd606a377d6a1b2986565825c9eae56dadf9\"}, {\"id\": \"abb17d93e3692f1928bcad632077fdada64a90bad846503c9519632fd4cca48a\"}]ubuntu@cf5-bastion:~$\n```\nFrom docker-host\n```\nubuntu@docker-host:~$  curl -iv http://docker.example.com:5000/v1/repositories/john/qpid/images\n Hostname was NOT found in DNS cache\n   Trying 10.100.138.173...\n* Connected to docker.example.com (10.100.138.173) port 5000 (#0)\n\nGET /v1/repositories/john/qpid/images HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: docker.example.com:5000\nAccept: /\n< HTTP/1.1 200 OK\nHTTP/1.1 200 OK\n* Server gunicorn/19.1.0 is not blacklisted\n< Server: gunicorn/19.1.0\nServer: gunicorn/19.1.0\n< Date: Mon, 01 Dec 2014 17:05:36 GMT\nDate: Mon, 01 Dec 2014 17:05:36 GMT\n< Connection: keep-alive\nConnection: keep-alive\n< X-Docker-Token: Token signature=LJFP05IBV7XDSYMT,repository=\"john/qpid\",access=read\nX-Docker-Token: Token signature=LJFP05IBV7XDSYMT,repository=\"john/qpid\",access=read\n< X-Docker-Endpoints: docker.example.com:5000\nX-Docker-Endpoints: docker.example.com:5000\n< Pragma: no-cache\nPragma: no-cache\n< Cache-Control: no-cache\nCache-Control: no-cache\n< Expires: -1\nExpires: -1\n< Content-Type: application/json\nContent-Type: application/json\n< WWW-Authenticate: Token signature=LJFP05IBV7XDSYMT,repository=\"john/qpid\",access=read\nWWW-Authenticate: Token signature=LJFP05IBV7XDSYMT,repository=\"john/qpid\",access=read\n< Content-Length: 1900\nContent-Length: 1900\n\n<\n[{\"id\": \"86ce37374f40e95cfe8af7327c34ea9919ef216ea965377565fcfad3c378a2c3\"}, {\"id\": \"0cb9ca8d54b54f1daa25e202f8177d59f19344714d18d79401b890666ea96b87\"}, {\"id\": \"048a550abbebe0ddae41ff8ac9ba068c693dfb261d509bd5013d2da3745bef05\"}, {\"id\": \"3f45ca85fedcfc575a9fc19601729387073862a2dcac07051bd2d48e562c5227\"}, {\"id\": \"61cb619d86bc60410bf04d249910f8ffffdb586bd0fc94b46c19eb39ecb1cbb1\"}, {\"id\": \"1d6add8470d893d4344734c995f7b18026c08374d1c9f7950586ea4e84ebd7f7\"}, {\"id\": \"5bc37dc2dfba434d7551fa17ee9d3ebba6adab648cebc2f962e612855c4c580c\"}, {\"id\": \"c938d9bc70e46b52a846ac360888ae0773a3dc72de406fc49e82434caab83dff\"}, {\"id\": \"11cd3ea407491a9ca21a619650e6b002c5beb42c391bb5b02362bf678e08c27d\"}, {\"id\": \"e2a5f0de25f647830cf2794c3542fda779dddf614b0f1b2e77797e07657a91de\"}, {\"id\": \"b05c5c3eebe081a111db6de51bbf28eba804e12fa0a58479c2996720a2e5b1a2\"}, {\"id\": \"5f297df8c1b1754fb633f0b4b1eb04b5d385717d72bd80b85ac7bfca0d4e608e\"}, {\"id\": \"511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158\"}, {\"id\": \"dc07507cef4200ac81f4adfa08b Connection #0 to host docker.example.com left intact\n984e3dd53edee60ee0b70c1c2c16e79cffcbe\"}, {\"id\": \"f6df77542c783f3c64b3cf97d540b19cdffa044d2d3db2ef72249900de5cee00\"}, {\"id\": \"a9ef5124113e5b5164e2524fe7a9c0be45ffc2a7cb97ea0a65f2a494f46caabf\"}, {\"id\": \"c8935a8315e167ebcc9b7a00042f416918528ff60aed567ab24d8abf2ef01433\"}, {\"id\": \"a159cd28c9a3127f5b8e3d89bb306d6bceeaf3649a8d720f4bb284b09a7f48e2\"}, {\"id\": \"78e82ee876a2e0f0bed96087aa4d5d17f12720a00be1b441c6a015544cff9d29\"}, {\"id\": \"ff34cb32d652795c3a1f184a0c4d3548153433daffbf98ba01cd7eef34436ded\"}, {\"id\": \"7a2d7e3cfd4b97665c01a2d9dc3a9bddf6cecee62f0d73aba10aaf3b99d74eef\"}, {\"id\": \"a3abe1e82a8da534c71e3705f659748774b5d7a9daa6edf45b69188c6ff719d8\"}, {\"id\": \"d4947dfd6bf6722715912864c3f0f688e3077cd3132dc62589339a28edb42961\"}, {\"id\": \"69bad55c0c007cefbd7c7a4f05b1bd606a377d6a1b2986565825c9eae56dadf9\"}, {\"id\": \"abb17d93e3692f1928bcad632077fdada64a90bad846503c9519632fd4cca48a\"}]\n. I have attempted this with a separate docker registry running within the firm (v0.8.1 according to the index page) and I can push and pull from any host without issue.\n.\nubuntu@docker-host:~/docker-examples/qpid$ curl -ivk https://docker.example.com:5000/v1/_ping\n Hostname was NOT found in DNS cache\n   Trying 10.100.138.173...\n Connected to docker.example.com (10.100.138.173) port 5000 (#0)\n successfully set certificate verify locations:\n   CAfile: none\n  CApath: /etc/ssl/certs\n SSLv3, TLS handshake, Client hello (1):\n Unknown SSL protocol error in connection to docker.example.com:5000\n* Closing connection 0\ncurl: (35) Unknown SSL protocol error in connection to docker.example.com:5000\nubuntu@docker-host:~/docker-examples/qpid$ curl -ivk http://docker.example.com:5000/v1/_ping\n Hostname was NOT found in DNS cache\n   Trying 10.100.138.173...\n* Connected to docker.example.com (10.100.138.173) port 5000 (#0)\n\nGET /v1/_ping HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: docker.example.com:5000\nAccept: /\n< HTTP/1.1 200 OK\nHTTP/1.1 200 OK\n* Server gunicorn/19.1.0 is not blacklisted\n< Server: gunicorn/19.1.0\nServer: gunicorn/19.1.0\n< Date: Mon, 01 Dec 2014 22:01:48 GMT\nDate: Mon, 01 Dec 2014 22:01:48 GMT\n< Connection: keep-alive\nConnection: keep-alive\n< X-Docker-Registry-Config: dev\nX-Docker-Registry-Config: dev\n< Expires: -1\nExpires: -1\n< X-Docker-Registry-Standalone: True\nX-Docker-Registry-Standalone: True\n< Pragma: no-cache\nPragma: no-cache\n< Cache-Control: no-cache\nCache-Control: no-cache\n< Content-Type: application/json\nContent-Type: application/json\n< Content-Length: 1436\nContent-Length: 1436\n\n<\n{\"host\": [\"Linux\", \"docker-host\", \"3.13.0-36-generic\", \"#63-Ubuntu SMP Wed Sep 3 21:30:07 UTC 2014\", \"x86_64\", \"x86_64\"], \"launch\": [\"/usr/local/bin/gunicorn\", \"--access-logfile\", \"-\", \"--error-logfile\", \"-\", \"--max-requests\", \"100\", \"-k\", \"gevent\", \"--graceful-timeout\", \"3600\", \"-t\", \"3600\", \"-w\", \"4\", \"-b\", \"0.0.0.0:5000\", \"--reload\", \"docker_registry.wsgi:application\"], \"versions\": {\"SocketServer\": \"0.4\", \"argparse\": \"1.1\", \"backports.lzma\": \"0.0.3\", \"blinker\": \"1.3\", \"cPickle\": \"1.71\", \"cgi\": \"2.6\", \"ctypes\": \"1.1.0\", \"decimal\": \"1.70\", \"distutils\": \"2.7.6\", \"docker_registry.app\": \"0.9.0\", \"docker_registry.core\": \"2.0.3\", \"docker_registry.server\": \"0.9.0\", \"email\": \"4.0.3\", \"flask\": \"0.10.1\", \"gevent\": \"1.0.1\", \"greenlet\": \"0.4.2\", \"gunicorn\": \"19.1.0\", \"gunicorn.arbiter\": \"19.1.0\", \"gunicorn.config\": \"19.1.0\", \"gunicorn.six\": \"1.2.0\", \"jinja2\": \"2.7.3\", \"json\": \"2.0.9\", \"logging\": \"0.5.1.2\", \"parser\": \"0.5\", \"pickle\": \"$Revision: 72223 $\", \"platform\": \"1.0.7\", \"python\": \"2.7.6 (default, Mar 22 2014, 22:5* Connection #0 to host docker.example.com left intact\n9:56) \\n[GCC 4.8.2]\", \"re\": \"2.2.1\", \"redis\": \"2.10.3\", \"requests\": \"2.3.0\", \"requests.packages.chardet\": \"2.2.1\", \"requests.packages.urllib3\": \"dev\", \"requests.packages.urllib3.packages.six\": \"1.2.0\", \"requests.utils\": \"2.3.0\", \"rsa\": \"3.1.4\", \"simplejson\": \"3.6.2\", \"sqlalchemy\": \"0.9.4\", \"tarfile\": \"$Revision: 85213 $\", \"urllib\": \"1.17\", \"urllib2\": \"2.7\", \"werkzeug\": \"0.9.6\", \"yaml\": \"3.11\", \"zlib\": \"1.0\"}}ubuntu@docker-host:~/docker-examples/qpid$\nubuntu@cf5-bastion:~$ curl -ivk https://docker.example.com:5000/v1/_ping\n Hostname was NOT found in DNS cache\n   Trying 10.100.138.173...\n Connected to docker.example.com (10.100.138.173) port 5000 (#0)\n successfully set certificate verify locations:\n   CAfile: none\n  CApath: /etc/ssl/certs\n SSLv3, TLS handshake, Client hello (1):\n Unknown SSL protocol error in connection to docker.example.com:5000\n Closing connection 0\ncurl: (35) Unknown SSL protocol error in connection to docker.example.com:5000\nubuntu@cf5-bastion:~$ curl -ivk http://docker.example.com:5000/v1/_ping\n Hostname was NOT found in DNS cache\n   Trying 10.100.138.173...\n* Connected to docker.example.com (10.100.138.173) port 5000 (#0)\n\nGET /v1/_ping HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: docker.example.com:5000\nAccept: /\n< HTTP/1.1 200 OK\nHTTP/1.1 200 OK\n* Server gunicorn/19.1.0 is not blacklisted\n< Server: gunicorn/19.1.0\nServer: gunicorn/19.1.0\n< Date: Mon, 01 Dec 2014 22:03:12 GMT\nDate: Mon, 01 Dec 2014 22:03:12 GMT\n< Connection: keep-alive\nConnection: keep-alive\n< X-Docker-Registry-Config: dev\nX-Docker-Registry-Config: dev\n< Expires: -1\nExpires: -1\n< X-Docker-Registry-Standalone: True\nX-Docker-Registry-Standalone: True\n< Pragma: no-cache\nPragma: no-cache\n< Cache-Control: no-cache\nCache-Control: no-cache\n< Content-Type: application/json\nContent-Type: application/json\n< Content-Length: 1436\nContent-Length: 1436\n\n<\n{\"host\": [\"Linux\", \"docker-host\", \"3.13.0-36-generic\", \"#63-Ubuntu SMP Wed Sep 3 21:30:07 UTC 2014\", \"x86_64\", \"x86_64\"], \"launch\": [\"/usr/local/bin/gunicorn\", \"--access-logfile\", \"-\", \"--error-logfile\", \"-\", \"--max-requests\", \"100\", \"-k\", \"gevent\", \"--graceful-timeout\", \"3600\", \"-t\", \"3600\", \"-w\", \"4\", \"-b\", \"0.0.0.0:5000\", \"--reload\", \"docker_registry.wsgi:application\"], \"versions\": {\"SocketServer\": \"0.4\", \"argparse\": \"1.1\", \"backports.lzma\": \"0.0.3\", \"blinker\": \"1.3\", \"cPickle\": \"1.71\", \"cgi\": \"2.6\", \"ctypes\": \"1.1.0\", \"decimal\": \"1.70\", \"distutils\": \"2.7.6\", \"docker_registry.app\": \"0.9.0\", \"docker_registry.core\": \"2.0.3\", \"docker_registry.server\": \"0.9.0\", \"email\": \"4.0.3\", \"flask\": \"0.10.1\", \"gevent\": \"1.0.1\", \"greenlet\": \"0.4.2\", \"gunicorn\": \"19.1.0\", \"gunicorn.arbiter\": \"19.1.0\", \"gunicorn.config\": \"19.1.0\", \"gunicorn.six\": \"1.2.0\", \"jinja2\": \"2.7.3\", \"json\": \"2.0.9\", \"logging\": \"0.5.1.2\", \"parser\": \"0.5\", \"pickle\": \"$Revision: 72223 $\", \"platform\": \"1.0.7\", \"python\": \"2.7.6 (default, Mar 22 2014, 22:5* Connection #0 to host docker.example.com left intact\n9:56) \\n[GCC 4.8.2]\", \"re\": \"2.2.1\", \"redis\": \"2.10.3\", \"requests\": \"2.3.0\", \"requests.packages.chardet\": \"2.2.1\", \"requests.packages.urllib3\": \"dev\", \"requests.packages.urllib3.packages.six\": \"1.2.0\", \"requests.utils\": \"2.3.0\", \"rsa\": \"3.1.4\", \"simplejson\": \"3.6.2\", \"sqlalchemy\": \"0.9.4\", \"tarfile\": \"$Revision: 85213 $\", \"urllib\": \"1.17\", \"urllib2\": \"2.7\", \"werkzeug\": \"0.9.6\", \"yaml\": \"3.11\", \"zlib\": \"1.0\"}}ubuntu@cf5-bastion:~$\n```\n. Sorry, I messed up with my copy and paste. I have edited the previous comment\n. Apologies, you are correct, something was missing.\nAll docker pull requests are fed through a proxy by setting http[s]_proxy in /etc/default/docker\nOn docker-host the no_proxy var was set to exclude the registry. On cf-bastion it was not.\n. ",
    "xiaods": "0.9.1 also not fix it.\n. @dmp42 OK\n. @dmp42 yes, it works. but i need below function:\ncurl https://my.docker.registry.com/v1/_ping\ntrue\ncurrently it now return a {}\n. @dmp42 so {} is correct? can we return some status code?\n. @dmp42 got it. \n. ",
    "mborho": "Sorry, I've used a 0.10.0 client... With version 1.0.1 it works as expected.\n. This error hits me too, it's a rather critical issue for me. \nWould saving the lock-file in /tmp a possible fix for this?\n. ",
    "leslau": "@hordemark Hi i'm having the same problem! Did u got a workaround for the problem!?\n. ",
    "shawinder": "Update:\nI got a fix. It's because nginx proxy by defaul listens to port 80. You must specify registry port 500 in the proxy_pass url. Let me know if that helps.\nHi Guys,\nDid you figure out what the problem was? I am also getting 502 with nginx proxy and docker registry setup.. ",
    "lsm5": "ugh .. working on failed checks ..back in a bit\n. tox seems to be happy now .. hopefully travis will be too :)\n. do we need a better/more descriptive commit message or does this suffice?\n. thanks @shin-, PTAL at update\n. @shin-  thanks a lot for fixing things up, not sure why there are 2 commits visible of mine, but no biggie I guess :\\\n. redis was a PITA on fedora/rhel and iirc not a hard requirement, but yes unrelated to this PR ...I could remove it if that helps \n. update follows\n. ",
    "AndreyKostov": "@ahmetalpbalkan You hit the nail on the head. The size parameter existed mostly because it was necessary for the s3 driver (and potentially other drivers in the future). The issue (as was stated) was that some drivers will require to explicitly be told when an upload is \"done\". Azure is not one of them, hence you did not need to make use of the size parameter (the purpose of which was to say, if you've written a total of size bytes, you're done).\nThat said, you are correct that it is not the cleanest way imaginable to handle the issue from an API perspective.\nI suggest that we go with 2) and Refactor the api into:\n```\n// WriteStream stores the contents of the provided io.Reader at a\n// location designated by the given path and returns the amount of bytes written.\n// May be used to resume writing a stream by providing a nonzero offset.\n// The offset must be no larger than the Size returned by Stat() for this path.\nWriteStream(path string, offset int64, reader io.Reader) (nn int64, err error)\n// WriteCommit signifies that the file at location designated by path is now\n// complete. The registry will call this method after calling Stat(path) and seeing\n// that the size returned is equal to the total size of the file it was uploading.\nWriteCommit(path string) error\n```\nUnder this version, for the Azure driver WriteCommit is a nop.\nThe only question is whether we also need to actually pass the size to WriteCommit, but this won't be\nnecessary if the registry can guarantee that it will only call it after calling Stat() and verifying the size.\n. I agree, as long as enforcing absolute paths is ok with the filesystem and inmemory drivers. As far as the s3 driver is concerned, it's really a key/value store so enforcing it there is trivial.\n. I've added support for IAM instance role authentication in next-generation in the upcoming s3 driver refactor pr\n. Possibly wait for me to add the v4auth option before merging.\n. This line cannot be reached if the move fails. Also returning on error is standard style in go, if I'm not mistaken.\n. done\n. No actually, not providing them is \"valid\" in case you want to authenticate with IAM on an ec2 instance.\n. This was removed, since as I stated to you before, directories do not have mod time on s3, nor can they unless we want to traverse all sub-files for each stat call on a directory.\n. Technically, all 1024 missing is also valid (since all of your writes could not have made it yet), but I put it there since if that is the case then things are much more likely to be broken. The point of this test is to make sure that you never break the assumption that if stat says you have it, then you really do have it.\n. This is the testFileStreams, not the benchmark I think. The reason for the loop is because I did in fact have cases where I didn't error and returned a lower nn than the total size, but have since fixed them. Sorry for leaving this change in here it's my mistake, Ill remove it in the next commit.\n. Will do.\n. Yeah, this is already after significant cleanup. The problem is that the zero fill behaviour of writeStream forces this into an 8-case situation. Ill try to figure out a way to condense it further.\n. fullPath seems to be dead code I forgot to remove, good catch.\n. Looks better, yeah. Ill add it.\n. Will do.\n. Do you find the one in the test description insufficient?\n. Sorry, I don't understand your comment @BrianBland . Can you clarify?\n. Done.\n. They're all lowercase (including accesskey and secretkey) Should I change them all?\n. It's not valid, no. Ill trim slashes off of rootDir.\n. ",
    "pmoosh": "Hi,\nplease note further tests show this also happens with a single worker:\n172.17.0.4 - - [04/Dec/2014:20:35:06 +0000] \"PUT /v1/images/7c9bfcc8353f4f29ebf6642f1e0dca65e287b6bf86234ff6484d362632ee1a89/json HTTP/1.0\" 200 4 \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/50b8feb kernel/3.17.2+ os/linux arch/amd64\"\n172.17.0.4 - - [04/Dec/2014:20:35:08 +0000] \"PUT /v1/images/7c9bfcc8353f4f29ebf6642f1e0dca65e287b6bf86234ff6484d362632ee1a89/layer HTTP/1.0\" 200 4 \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/50b8feb kernel/3.17.2+ os/linux arch/amd64\"\n172.17.0.4 - - [04/Dec/2014:20:35:08 +0000] \"PUT /v1/images/7c9bfcc8353f4f29ebf6642f1e0dca65e287b6bf86234ff6484d362632ee1a89/checksum HTTP/1.0\" 200 4 \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/50b8feb kernel/3.17.2+ os/linux arch/amd64\"\n2014-12-04 20:35:08 [18] [INFO] Autorestarting worker after current request.\n172.17.0.4 - - [04/Dec/2014:20:35:09 +0000] \"PUT /v1/repositories/pzspeed/tags/latest HTTP/1.0\" 200 4 \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/50b8feb kernel/3.17.2+ os/linux arch/amd64\"\n172.17.0.4 - - [04/Dec/2014:20:35:09 +0000] \"PUT /v1/repositories/pzspeed/images HTTP/1.0\" 204 - \"-\" \"docker/1.3.2 go/go1.3.2 git-commit/50b8feb kernel/3.17.2+ os/linux arch/amd64\"\n2014-12-04 20:35:09 [18] [INFO] Worker exiting (pid: 18)\n2014-12-04 20:35:09 [21] [INFO] Booting worker with pid: 21\n2014-12-04 20:35:10 [1] [INFO] 1 workers\n172.17.0.4 - - [04/Dec/2014:20:35:18 +0000] \"GET /v1/_ping HTTP/1.0\" 200 2 \"-\" \"Go 1.1 package http\"\n172.17.0.4 - - [04/Dec/2014:20:35:18 +0000] \"GET /v1/_ping HTTP/1.0\" 200 2 \"-\" \"Go 1.1 package http\"\n. What are we looking for?\nI set debug = 2 in boto.cfg.\nAlso in config.yaml in the common section:\nloglevel: _env:LOGLEVEL:debug\ndebug: _env:DEBUG:true\nI do get a lot of output, that needs to be cleaned ...\nthanks\nPeter\n. BTW - it seems to be simple to reproduce. \nI used the latest coreOS ami (PRETTY_NAME=\"CoreOS 509.1.0). \nThe IAM role is set so the instance has all access to S3 \npull the registry and start it:\n/usr/bin/docker run -d -v /home/core/conf:/registry-conf -e DOCKER_REGISTRY_CONFIG=/registry-conf/config_pz.yaml -e SETTINGS_FLAVOR=prod -e AWS_REGION=us-west-1 -e AWS_BUCKET=testreg -e GUNICORN_OPTS=[--preload] --name=\"pz-registry\" -p 5000:5000 registry\nThen I tag the registry image with localhost:5000/registry. Next I push it to the local docker registry. I do this several times, until it hangs.\n. I can, but then I have either set the worker count to or disable search...\n. running it now (1000 loops) - I am out now. But if that is the case - that would mean one cannot use search on S3 ...\n. I think that was it. When will the fix be in?\n. @rhesus - have you tried to build it from repo. I don't think the 0.9.1 is available yet...\n. Yeah - I built it from master (pmoosh/registry:0.9.1). Not sure if it is right though, as I had to add a few things so it actually builds:\n```\nInstall pip\n && apt-get install -y \\\n     python-pip \\\n\n\nswig \\\n # Install deps for backports.lmza (python2 requires it)\n         python-dev \\\nlibyaml-dev \\\n         liblzma-dev \\\nlibssl-dev \\\n         libevent1-dev \\\n     && rm -rf /var/lib/apt/lists/*\n```\n\nBut search works w/o the --preload option. And I haven't seen hangs, though I didn't do much, just a couple of pushes ...\n. @dbason - that is a know issue: and the remedy is GUNICORN_OPTS=[--preload] ...\nThis is fixed in 0.9.1 or so.... out any day now. I have a container based on master from about 25 days ago here pmoosh/registry\n. @clkao are you using the -e GUNICORN_OPTS=[--preload] option?\n. ",
    "rhesus": "Is 0.9.1 released or do we need to build from the repo?\n. I ended up foregoing search for my repo for now.  I will try upgrading to the latest as soon as I get a chance though.  I have several servers to update docker on for 1.4.0 anyway  :-)  I'll report back if it still hangs on S3.\n. ",
    "clkao": "Is this in the current 0.9.1 image? I was still seeing similar issue where PUT choked.\n. With latest, after several push to the registry, push chokes with the following output:\n172.17.42.1 - - [22/Jan/2015:07:54:54 +0000] \"GET /v1/repositories/stdev/images HTTP/1.1\" 200 6080 \"-\" \"docker/1.4.1 go/go1.3.3 git\n172.17.42.1 - - [22/Jan/2015:07:54:55 +0000] \"GET /v1/repositories/library/stdev/tags HTTP/1.1\" 200 1593 \"-\" \"docker/1.4.1 go/go1.3\n172.17.42.1 - - [22/Jan/2015:07:54:55 +0000] \"GET /v1/images/0a6bcf2ee2656aba58ee80a6a425075b8e5fb416d09ce076a7f250337252ab1c/json\n[2015-01-22 07:54:55 +0000] [40] [INFO] Autorestarting worker after current request.\n172.17.42.1 - - [22/Jan/2015:07:54:55 +0000] \"GET /v1/images/e2fdd382a6037f02ce2642a30b4f7a3e4530ef56fc7a4685d2085062445c523d/ances\n172.17.42.1 - - [22/Jan/2015:07:54:55 +0000] \"GET /v1/images/b61bc8bdcf32f7d76ab23ad2570a8e0fe1e48edb708553d55b081b500ea9a4a5/json\n172.17.42.1 - - [22/Jan/2015:07:54:55 +0000] \"GET /v1/images/8a171986abe91169cdf682052cf28eaafeacf1c94001fe0f94cf70dc75b82112/json\n172.17.42.1 - - [22/Jan/2015:07:54:56 +0000] \"GET /v1/images/f9cc1769dbf9a2ac8e15e1afa834e3c722558d3f8fefe890956cb17b497de71d/json\n172.17.42.1 - - [22/Jan/2015:07:54:56 +0000] \"GET /v1/images/07355d1df2f5e94c2bcedc9c6aaef4ef22e5ce3cce6b7845e19526c253f72b34/json\n[2015-01-22 07:54:56 +0000] [40] [INFO] Worker exiting (pid: 40)\n[2015-01-22 07:54:56 +0000] [57] [INFO] Booting worker with pid: 57\n172.17.42.1 - - [22/Jan/2015:07:54:56 +0000] \"GET /v1/images/ab3678b7f0ff390bcea1d5afbefc5b118c277b9b3352daf0014e0cedb2d0d426/json\n172.17.42.1 - - [22/Jan/2015:07:54:57 +0000] \"GET /v1/images/8575c685cfa3397a9829662437aa6fd58902dbd179ac947801224ab36867e8b4/json\n172.17.42.1 - - [22/Jan/2015:07:54:57 +0000] \"GET /v1/images/3bc745296aa6b4d2b729b5b39dab7467fca9887e802580e5a27cf92588c895de/json\nthe client hangs while starting to actually PUT new layer (note there's no PUT in the output for the registry yet):\nSending image list\nPushing repository localhost:5000/stdev (1 tags)\n[...]\nImage ab3678b7f0ff already pushed, skipping\nImage 8575c685cfa3 already pushed, skipping\nImage 3bc745296aa6 already pushed, skipping\n(hangs)\n. @pmoosh yes, without [--preload] it wouldn't actually start at all!\n. @dmp42, this is the error when I run :latest without --preload: https://gist.github.com/clkao/ec742ffe9df2e9e2212d\n. somehow this now works on startup, even with a fresh image.\n. ",
    "johnbwoodruff": "2014-12-04 23:02:53 [13] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 19, in \n    from .app import app  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/app.py\", line 8, in \n    from . import toolkit\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 22, in \n    cfg = config.load()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/config.py\", line 129, in load\n    _config = _init()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/config.py\", line 93, in _init\n    'Heads-up! File is missing: %s' % config_path)\nFileNotFoundError: Heads-up! File is missing: /home/administrator/dev/mtc-registry/config.yml\n. I used the default config with some overriding and this is now the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 61, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 76, in _setup_database\n    self._generate_index(session=session)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 81, in _generate_index\n    Base.metadata.create_all(self._engine)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/sql/schema.py\", line 3291, in create_all\n    tables=tables)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 1547, in _run_visitor\n    conn._run_visitor(visitorcallable, element, _kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 1194, in _run_visitor\n    _kwargs).traverse_single(element)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/sql/visitors.py\", line 119, in traverse_single\n    return meth(obj, _kw)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/sql/ddl.py\", line 709, in visit_metadata\n    self.traverse_single(table, create_ok=True)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/sql/visitors.py\", line 119, in traverse_single\n    return meth(obj, _kw)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/sql/ddl.py\", line 728, in visit_table\n    self.connection.execute(CreateTable(table))\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 720, in execute\n    return meth(self, multiparams, params)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/sql/ddl.py\", line 67, in _execute_on_connection\n    return connection._execute_ddl(self, multiparams, params)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 774, in _execute_ddl\n    compiled\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 947, in _execute_context\n    context)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 1108, in _handle_dbapi_exception\n    exc_info\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/util/compat.py\", line 185, in raise_from_cause\n    reraise(type(exception), exception, tb=exc_tb)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py\", line 940, in _execute_context\n    context)\n  File \"/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/default.py\", line 435, in do_execute\n    cursor.execute(statement, parameters)\nOperationalError: (OperationalError) table version already exists u'\\nCREATE TABLE version (\\n\\tid INTEGER NOT NULL, \\n\\tPRIMARY KEY (id)\\n)\\n\\n' ()\n. Yep, sure enough that was it! I appreciate the help!\n. ",
    "MitchK": "@dmp42 \nHi Olivier,\nThis is where the log file ends:\n...\n version 2.9.4 doesn't match ==2.34.0\n  Ignoring link https://pypi.python.org/packages/source/b/boto/boto-2.9.5.tar.gz#md5=cc79cc87edb4cce00ca275c8ea7b75ed (from https://pypi.python.org/simple/boto/), version 2.9.5 doesn't match ==2.34.0\n  Ignoring link https://pypi.python.org/packages/source/b/boto/boto-2.9.6.tar.gz#md5=46f8e51001d2e8e17ec50615d0d55201 (from https://pypi.python.org/simple/boto/), version 2.9.6 doesn't match ==2.34.0\n  Ignoring link https://pypi.python.org/packages/source/b/boto/boto-2.9.7.tar.gz#md5=1d148444990292e3b36c35d0a98ff26f (from https://pypi.python.org/simple/boto/), version 2.9.7 doesn't match ==2.34.0\n  Ignoring link https://pypi.python.org/packages/source/b/boto/boto-2.9.8.tar.gz#md5=373b183e721f2e442855ef096aa3482d (from https://pypi.python.org/simple/boto/), version 2.9.8 doesn't match ==2.34.0\n  Ignoring link https://pypi.python.org/packages/source/b/boto/boto-2.9.9.tar.gz#md5=794a7db936864403ab0142a0bd03b0c3 (from https://pypi.python.org/simple/boto/), version 2.9.9 doesn't match ==2.34.0\n  Using version 2.34.0 (newest of versions: 2.34.0, 2.34.0)\nCleaning up...\n  Removing temporary dir /tmp/pip_build_root...\nException:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/pip/basecommand.py\", line 122, in main\n    status = self.run(options, args)\n  File \"/usr/local/lib/python2.7/dist-packages/pip/commands/install.py\", line 278, in run\n    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)\n  File \"/usr/local/lib/python2.7/dist-packages/pip/req.py\", line 1197, in prepare_files\n    do_download,\n  File \"/usr/local/lib/python2.7/dist-packages/pip/req.py\", line 1375, in unpack_url\n    self.session,\n  File \"/usr/local/lib/python2.7/dist-packages/pip/download.py\", line 546, in unpack_http_url\n    resp = session.get(target_url, stream=True)\n  File \"/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/sessions.py\", line 468, in get\n    return self.request('GET', url, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/pip/download.py\", line 237, in request\n    return super(PipSession, self).request(method, url, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/sessions.py\", line 456, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/sessions.py\", line 559, in send\n    r = adapter.send(request, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/pip/_vendor/requests/adapters.py\", line 378, in send\n    raise ProxyError(e)\nProxyError: ('Cannot connect to proxy.', gaierror(-2, 'Name or service not known'))\n. @dmp42 No problem! We installed the registry via the pre-built docker image. This now works for us.\n. Problem solved.\nThe Docker HTTP proxy was set and it does not work with our internal systems.\nWe need to define proxy exceptions: https://github.com/docker/docker/issues/9531\n. Hi this is what I did on one machine:\n$ sudo docker tag username/imagename registry.mycompany.corp/username/imagename:latest\n$ sudo docker push registry.mycompany.corp/username/imagename:latest\nThe push refers to a repository [registry.mycompany.corp/username/imagename] (len: 1)\nSending image list\nPushing repository registry.mycompany.corp/username/imagename (1 tags)\n511136ea3c5a: Pushing 1.536 kB/1.536 kB\n2014/12/05 20:42:46 \n$\nAnd that above is the whole output.\nThen, I try to pull the image from the registry:\n$ sudo docker pull registry.mycompany.corp/username/imagename:latest\nPulling repository registry.mycompany.corp/username/imagename\n2014/12/05 20:46:13 Tag latest not found in repository registry.mycompany.corp/username/imagename\n$\nThis is my Dockerfile:\n```\nFROM ubuntu\nPROXY SETTINGS\nENV my_company_proxy http://proxy:8080\nSET PROXY SETTINGS\nENV http_proxy $my_company_proxy\nENV HTTP_PROXY $my_company_proxy\nENV https_proxy $my_company_proxy\nENV HTTPS_PROXY $my_company_proxy\nENV ftp_proxy $my_company_proxy\nENV FTP_PROXY $my_company_proxy\nENV all_proxy $my_company_proxy\nENV ALL_PROXY $my_company_proxy\nENV no_proxy .local,169.254/16,.mycompany.corp,*.corp.mycompany,.mycompany.corp,.corp.mycompany\nENV NO_PROXY $no_proxy\nRUN echo \"Acquire::http::Proxy \\\"$my_company_proxy\\\";\" >> /etc/apt/apt.conf\nRUN echo \"Acquire::https::Proxy \\\"$my_company_proxy\\\";\" >> /etc/apt/apt.conf\nUPGRADE SYSTEM\nRUN apt-get update && apt-get dist-upgrade -q -y --force-yes\nRUN apt-get install -q -y --force-yes git curl wget \nINSTALL ROOT CERTIFICATES\nRUN curl -k https://github.mycompany.corp/GitHub/Install-Root-Certificates/raw/master/install-certs-ubuntu-12.04.sh | sudo -E sh\n```\n. Hi,\nI also can't pull on the machine I have pushed:\n$ sudo docker push registry.mycompany.corp/username/imagename:latest\nThe push refers to a repository [registry.mycompany.corp/username/imagename] (len: 1)\nSending image list\nPushing repository registry.mycompany.corp/username/imagename (1 tags)\n511136ea3c5a: Pushing 1.536 kB/1.536 kB\n$ sudo docker pull registry.mycompany.corp/username/imagename:latest\nPulling repository registry.mycompany.corp/username/imagename\n2014/12/05 21:43:28 Tag latest not found in repository registry.mycompany.corp/username/imagename\n$\nThis is the log output on the local machine:\n```\n[info] POST /v1.15/images/registry.mycompany.corp/username/imagename/push?tag=latest\n[78b2b721] +job push(registry.mycompany.corp/username/imagename)\nReceived HTTP code 411 while uploading layer: \n411 Length Required\n\n411 Length Required\nnginx/1.1.19\n\n\n[78b2b721] -job push(registry.mycompany.corp/username/imagename) = ERR (1)\n[info] POST /v1.15/images/create?fromImage=registry.mycompany.corp%2Fusername%2Fimagename%3Alatest\n[78b2b721] +job pull(registry.mycompany.corp/username/imagename, latest)\nTag latest not found in repository registry.mycompany.corp/username/imagename\n[78b2b721] -job pull(registry.mycompany.corp/username/imagename, latest) = ERR (1)\n```\nThis is the output of the registry:\n[info] POST /v1.15/images/create?fromImage=registry.mycompany.corp%2Fusername%2Fimagename%3Alatest\n[2c072f9b] +job pull(registry.mycompany.corp/username/imagename, latest)\nTag latest not found in repository registry.mycompany.corp/username/imagename\n[2c072f9b] -job pull(registry.mycompany.corp/username/imagename, latest) = ERR (1)\nBy the way, this is my Nginx configuration: \n```\nFor versions of Nginx > 1.3.9 that include chunked transfer encoding support\nReplace with appropriate values where necessary\nupstream docker-registry {\n server localhost:5000;\n}\nserver {\n listen 443;\n server_name registry.mycompany.corp;\nssl on;\n ssl_certificate /etc/ssl/certs/docker-registry;\n ssl_certificate_key /etc/ssl/private/docker-registry;\nproxy_set_header Host       $http_host;   # required for Docker client sake\n proxy_set_header X-Real-IP  $remote_addr; # pass on real client IP\nclient_max_body_size 0; # disable any limits to avoid HTTP 413 for large image uploads\n# required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\n chunked_transfer_encoding on;\nlocation / {\n     # let Nginx know about our auth file\n     auth_basic              \"Restricted\";\n     auth_basic_user_file    docker-registry.htpasswd;\n proxy_pass http://docker-registry;\n\n}\n location /_ping {\n     auth_basic off;\n     proxy_pass http://docker-registry;\n }\n location /v1/_ping {\n     auth_basic off;\n     proxy_pass http://docker-registry;\n }\n```\nDocker versions on both client machines:\n$ sudo docker --version\nDocker version 1.3.2, build 39fa2fa\nNginx version of the docker registry:\n$ nginx -v\nnginx version: nginx/1.1.19\nThere is no proxy in between.\n. I've upgraded Nginx. Now it works. Thank you very much!\nsudo service nginx stop\nsudo -E add-apt-repository ppa:nginx/stable\nsudo -E apt-get update\nsudo -E apt-get install nginx\nsudo service nginx start\nPushing on one machine:\n$ sudo docker push registry.mycompany.corp/username/imagename:latest\nThe push refers to a repository [registry.mycompany.corp/username/imagename] (len: 1)\nSending image list\nPushing repository registry.mycompany.corp/username/imagename (1 tags)\nImage 511136ea3c5a already pushed, skipping\nImage 01bf15a18638 already pushed, skipping\nImage 30541f8f3062 already pushed, skipping\nImage e1cdf371fbde already pushed, skipping\nImage 9bd07e480c5b already pushed, skipping\nImage 1da1514994fc already pushed, skipping\nImage 3f176e517467 already pushed, skipping\nImage f3c19d099cb3 already pushed, skipping\nImage 2dc6fa032b24 already pushed, skipping\nImage d65f1435b299 already pushed, skipping\nImage 23197138812f already pushed, skipping\nImage 7c882a0ced37 already pushed, skipping\nImage 1f2329e0d138 already pushed, skipping\nImage 73d08b3178b9 already pushed, skipping\nImage 9354c8807b9f already pushed, skipping\nImage 4954335946de already pushed, skipping\nImage d215a39d564d already pushed, skipping\nImage 12b13fdf39f2 already pushed, skipping\nImage 84eaf6cb4aeb already pushed, skipping\nImage 36ead410fe31 already pushed, skipping\nImage 28755ba48ef8 already pushed, skipping\nPushing tag for rev [28755ba48ef8] on {https://registry.mycompany.corp/v1/repositories/username/imagename/tags/latest}\nPulling on another:\n$ sudo docker pull registry.mycompany.corp/username/imagename\nPulling repository registry.mycompany.corp/username/imagename\n28755ba48ef8: Download complete \n511136ea3c5a: Download complete \n01bf15a18638: Download complete \n30541f8f3062: Download complete \ne1cdf371fbde: Download complete \n9bd07e480c5b: Download complete \n1da1514994fc: Download complete \n3f176e517467: Download complete \nf3c19d099cb3: Download complete \n2dc6fa032b24: Download complete \nd65f1435b299: Download complete \n23197138812f: Download complete \n7c882a0ced37: Download complete \n1f2329e0d138: Download complete \n73d08b3178b9: Download complete \n9354c8807b9f: Download complete \n4954335946de: Download complete \nd215a39d564d: Download complete \n12b13fdf39f2: Download complete \n84eaf6cb4aeb: Download complete \n36ead410fe31: Download complete \nStatus: Image is up to date for registry.mycompany.corp/username/imagename:latest\n. :+1: \n. Addition: I've just learned that the registry does not support authentication/authorization. It needs to be accomplished by another service like Docker Hub does. https://docs.docker.com/reference/api/hub_registry_spec/#docker-hub\nSo do you know any existing solutions on how to accomplish this?\n. ",
    "qiyubing": "I also met this problem! And was trouble for more than one week!\n. Everything looks like right,this is the log in my registry server:\n192.168.84.107 - - [21/Jan/2015:11:06:44 +0000] \"GET /v1/_ping HTTP/1.1\" 200 1533 \"-\" \"Go 1.1 package http\"\n21/Jan/2015:11:06:44 +0000 DEBUG: args = {'namespace': u'qiyubing', 'repository': u'redis'}\n192.168.84.107 - - [21/Jan/2015:11:06:44 +0000] \"GET /v1/repositories/qiyubing/redis/images HTTP/1.1\" 200 1345 \"-\" \"docker/1.3.2 go/go1.3.3 kernel/3.10.5-3.el6.x86_64 os/linux arch/amd64\"\n21/Jan/2015:11:06:44 +0000 DEBUG: args = {'namespace': u'qiyubing', 'repository': u'redis'}\n21/Jan/2015:11:06:44 +0000 DEBUG: [get_tags] namespace=qiyubing; repository=redis\n192.168.84.107 - - [21/Jan/2015:11:06:44 +0000] \"GET /v1/repositories/qiyubing/redis/tags HTTP/1.1\" 200 2 \"-\" \"docker/1.3.2 go/go1.3.3 kernel/3.10.5-3.el6.x86_64 os/linux arch/amd64\n. ",
    "vitorarins": "docker push gives:\ndocker push 192.168.110.11:5000/spring-boot\n2014/12/08 19:12:32 Error: Invalid registry endpoint https://192.168.110.11:5000/v1/: Get https://192.168.110.11:5000/v1/_ping: EOF. If this private registry supports only HTTP or HTTPS with an unknown CA certificate, please add `--insecure-registry 192.168.110.11:5000` to the daemon's arguments. In the case of HTTPS, if you have access to the registry's CA certificate, no need for the flag; simply place the CA certificate at /etc/docker/certs.d/192.168.110.11:5000/ca.crt\ndocker info:\n[debug] server.go:1181 Calling GET /info\n[info] GET /v1.15/info\n[ce52d9a4] +job info()\n[ce52d9a4] +job subscribers_count()\n[ce52d9a4] -job subscribers_count() = OK (0)\n[ce52d9a4] -job info() = OK (0)\nContainers: 5\nImages: 94\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Dirs: 104\nExecution Driver: native-0.2\nKernel Version: 3.2.0-4-amd64\nOperating System: Debian GNU/Linux 7 (wheezy)\nDebug mode (server): true\nDebug mode (client): false\nFds: 16\nGoroutines: 15\nEventsListeners: 0\nInit Path: /usr/bin/docker\nWARNING: No memory limit support\nWARNING: No swap limit support\ndocker version:\nClient version: 1.3.2\nClient API version: 1.15\nGo version (client): go1.3.3\nGit commit (client): 39fa2fa\nOS/Arch (client): linux/amd64\n[debug] server.go:1181 Calling GET /version\n[info] GET /v1.15/version\n[ce52d9a4] +job version()\n[ce52d9a4] -job version() = OK (0)\nServer version: 1.3.2\nServer API version: 1.15\nGo version (server): go1.3.3\nGit commit (server): 39fa2fa\ndocker logs:\n[debug] server.go:1181 Calling GET /containers/{name:.*}/json\n[info] GET /v1.15/containers/registry/json\n[ce52d9a4] +job container_inspect(registry)\n[ce52d9a4] -job container_inspect(registry) = OK (0)\n[debug] server.go:1181 Calling GET /containers/{name:.*}/logs\n[info] GET /v1.15/containers/registry/logs?stderr=1&stdout=1&tail=all\n[ce52d9a4] +job container_inspect(registry)\n[ce52d9a4] -job container_inspect(registry) = OK (0)\n[ce52d9a4] +job logs(registry)\n2014-12-08 21:10:42 [1] [INFO] Starting gunicorn 19.1.0\n2014-12-08 21:10:42 [1] [INFO] Listening at: http://0.0.0.0:5000 (1)\n2014-12-08 21:10:42 [1] [INFO] Using worker: gevent\n2014-12-08 21:10:42 [14] [INFO] Booting worker with pid: 14\n[ce52d9a4] -job logs(registry) = OK (0)\n2014-12-08 21:10:43 [15] [INFO] Booting worker with pid: 15\n2014-12-08 21:10:43 [16] [INFO] Booting worker with pid: 16\n2014-12-08 21:10:43 [17] [INFO] Booting worker with pid: 17\n2014-12-08 21:10:43 [1] [INFO] 4 workers\n08/Dec/2014:21:10:43 +0000 WARNING: Cache storage disabled!\n08/Dec/2014:21:10:43 +0000 WARNING: LRU cache disabled!\n08/Dec/2014:21:10:43 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n08/Dec/2014:21:10:43 +0000 WARNING: Cache storage disabled!\n08/Dec/2014:21:10:43 +0000 WARNING: LRU cache disabled!\n08/Dec/2014:21:10:43 +0000 WARNING: Cache storage disabled!\n08/Dec/2014:21:10:43 +0000 WARNING: LRU cache disabled!\n08/Dec/2014:21:10:44 +0000 WARNING: Cache storage disabled!\n08/Dec/2014:21:10:44 +0000 WARNING: LRU cache disabled!\n08/Dec/2014:21:10:44 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n08/Dec/2014:21:10:44 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n08/Dec/2014:21:10:44 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\ndocker registry with nginx:\n```\ndocker login https://registry.com.br\nUsername (vitor.arins):   \nLogin Succeeded\ndocker tag spring-boot registry.com.br/spring-boot\ndocker push registry.com.br/spring-boot\nThe push refers to a repository [registry.com.br/spring-boot] (len: 1)\nSending image list\nPushing repository registry.com.br/spring-boot (1 tags)\n511136ea3c5a: Pushing [==================================================>] 1.536 kB/1.536 kB\n2014/12/08 19:17:54\n```\ndocker logs with nginx:\n[debug] server.go:1181 Calling GET /containers/{name:.*}/json\n[info] GET /v1.15/containers/registry/json\n[ce52d9a4] +job container_inspect(registry)\n[ce52d9a4] -job container_inspect(registry) = OK (0)\n[debug] server.go:1181 Calling GET /containers/{name:.*}/logs\n[info] GET /v1.15/containers/registry/logs?stderr=1&stdout=1&tail=all\n[ce52d9a4] +job container_inspect(registry)\n[ce52d9a4] -job container_inspect(registry) = OK (0)\n[ce52d9a4] +job logs(registry)\n2014-12-08 21:10:42 [1] [INFO] Starting gunicorn 19.1.0\n2014-12-08 21:10:42 [1] [INFO] Listening at: http://0.0.0.0:5000 (1)\n2014-12-08 21:10:42 [1] [INFO] Using worker: gevent\n2014-12-08 21:10:42 [14] [INFO] Booting worker with pid: 14\n2014-12-08 21:10:43 [15] [INFO] Booting worker with pid: 15\n2014-12-08 21:10:43 [16] [INFO] Booting worker with pid: 16\n2014-12-08 21:10:43 [17] [INFO] Booting worker with pid: 17\n2014-12-08 21:10:43 [1] [INFO] 4 workers\n08/Dec/2014:21:10:43 +0000 WARNING: Cache storage disabled!\n08/Dec/2014:21:10:43 +0000 WARNING: LRU cache disabled!\n08/Dec/2014:21:10:43 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n08/Dec/2014:21:10:43 +0000 WARNING: Cache storage disabled!\n08/Dec/2014:21:10:43 +0000 WARNING: LRU cache disabled!\n08/Dec/2014:21:10:43 +0000 WARNING: Cache storage disabled!\n08/Dec/2014:21:10:43 +0000 WARNING: LRU cache disabled!\n08/Dec/2014:21:10:44 +0000 WARNING: Cache storage disabled!\n08/Dec/2014:21:10:44 +0000 WARNING: LRU cache disabled!\n08/Dec/2014:21:10:44 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n08/Dec/2014:21:10:44 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n08/Dec/2014:21:10:44 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n172.17.42.1 - - [08/Dec/2014:21:16:44 +0000] \"GET /v1/_ping HTTP/1.0\" 200 1420 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [08/Dec/2014:21:16:44 +0000] \"GET /v1/_ping HTTP/1.0\" 200 1420 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [08/Dec/2014:21:16:44 +0000] \"GET /v1/users/ HTTP/1.0\" 200 4 \"-\" \"docker/1.3.1 go/go1.3.3 git-commit/4e9bbfa kernel/3.16.4-tinycore64 os/linux arch/amd64\"\n172.17.42.1 - - [08/Dec/2014:21:17:21 +0000] \"GET /v1/_ping HTTP/1.0\" 200 1420 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [08/Dec/2014:21:17:21 +0000] \"GET /v1/_ping HTTP/1.0\" 200 1420 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [08/Dec/2014:21:17:37 +0000] \"GET /v1/_ping HTTP/1.0\" 200 1420 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [08/Dec/2014:21:17:37 +0000] \"GET /v1/_ping HTTP/1.0\" 200 1420 \"-\" \"Go 1.1 package http\"\n08/Dec/2014:21:17:37 +0000 DEBUG: args = {'namespace': 'library', 'repository': u'spring-boot'}\n172.17.42.1 - - [08/Dec/2014:21:17:37 +0000] \"PUT /v1/repositories/spring-boot/ HTTP/1.0\" 200 2 \"-\" \"docker/1.3.1 go/go1.3.3 git-commit/4e9bbfa kernel/3.16.4-tinycore64 os/linux arch/amd64\"\n08/Dec/2014:21:17:37 +0000 DEBUG: args = {'image_id': u'511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158'}\n08/Dec/2014:21:17:37 +0000 DEBUG: api_error: Image is being uploaded, retry later\n172.17.42.1 - - [08/Dec/2014:21:17:37 +0000] \"GET /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.0\" 400 49 \"-\" \"docker/1.3.1 go/go1.3.3 git-commit/4e9bbfa kernel/3.16.4-tinycore64 os/linux arch/amd64\"\n08/Dec/2014:21:17:37 +0000 DEBUG: args = {'image_id': u'511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158'}\n[ce52d9a4] -job logs(registry) = OK (0)\n172.17.42.1 - - [08/Dec/2014:21:17:37 +0000] \"PUT /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.0\" 200 4 \"-\" \"docker/1.3.1 go/go1.3.3 git-commit/4e9bbfa kernel/3.16.4-tinycore64 os/linux arch/amd64\"\nThank you for the quick response!\n. ",
    "barkerd427": "I'm experiencing the identical issue with the latest version. I should note that I am upgrading from a previous version, so I already have images in the registry.\n. Thanks, that worked for me!\n. ",
    "jessfraz": "wow the code highlighted diffs are really a life changer\n. lol sorry I am caught up now\n. ",
    "cruxnet": "curl to the registry on either 4443 or 8443 works identically on either port and has no issues:\ncurl -Li https://myregistry:4443/v1/_ping\nHTTP/1.1 200 OK\nDate: Thu, 11 Dec 2014 20:44:07 GMT\nServer: gunicorn/19.1.0\nX-Docker-Registry-Standalone: True\nExpires: -1\nContent-Type: application/json\nPragma: no-cache\nCache-Control: no-cache\nContent-Length: 2\n{}\nUsing --insecure-registry is not an option for us and besides it is using a proper certificate chain that browsers and curl are ok with. It is just the docker daemon that has issues.\n. I figured out the problem. When @dmp42 asked for -D log it prompted me to look at our configuration from end to end. We had altered /etc/init.d/docker to add our corporate proxy settings. Without them docker couldn't pull images from the public registry. However, the way it was configured all traffic was routed through the proxy (even local traffic) and the corporate firewall is limiting the allowed port range. Many thanks to both @dmp42 and @stevvooe for taking the time to think about and look into the issue.\n. ",
    "zhangwei1234": "now we had deleted the test package!\n. ",
    "estesp": "@wking good point--although the API interactions do not change at all (hmm--I did change the search result JSON to add arch/os to the response), I suppose it makes sense to document the assumption (e.g. arch/os == amd64/linux) unless a user_agent string containing arch/ and os/ exist in the request.  Also, I can augment the example JSON to show that Architecture and OS are useful values with this change, and are used today in the docker daemon (set by the graph to GOOS and GOARCH: https://github.com/docker/docker/blob/master/graph/graph.go#L125-L126).\nI will put together a doc PR for the search response and other notes about over in docker, as it looks like the docs are part of their github repo.\n. @wking Thought I would get to it before my holiday time off, but didn't--however, I've added a docs PR for the search API change. I also fixed all flake8 errors from the CI run that didn't show locally when I ran it, but still have CI failures that seem to be unrelated to my code..any input, suggestions welcome!\n. Thanks @dmp42.  I've fixed the tests that were failing, including a potentially important catch for an issue created by the new code--the mocked up images used in a few tests [no idea if it would be hit in real usage, but worth handling properly] had no image data, so attempting to parse that on GET and PUT repository operations caused a FileNotFoundException that was causing an inadvertent code path that was wrong for both cases.  I've fixed that in the code and now all tests are passing.\n. @wking agreed--might have gotten lost in the noise of earlier comments, but I opened docker/docker#9904 with the change to the search API spec.  Of course I've only succeeded in confusing @jfrazelle so far given these changes aren't available in any registry yet :)\n. ",
    "benhosmer": "I don't get a response either from x.x.x.x/v1 on my registry, but I do get: \n\"\\\"docker-registry server\\\"\"\nFrom x.x.x.x\nCould you try x.x.x.x/v1/_ping? Your registry might actually be running.\n. @uhop: Suspecting you might be on to something with the X-Forwarded-Proto. I checked my nignx config:\n```\nupstream docker-registry {                                                   \n server localhost:5000;                                                      \n}                                                                              \nserver {                                                                     \n  listen 443 default_server;                                                 \n  ssl on;                                                                    \n  ssl_certificate  /etc/nginx/ssl/CA-BUNDLE-.cert.pem;                       \n  ssl_certificate_key /etc/nginx/ssl/server.key.pem;                         \n  proxy_set_header Host       $http_host;   # required for Docker client sake\n  proxy_set_header X-Real-IP  $remote_addr; # pass on real client IP           \nclient_max_body_size 0; # disable any limits to avoid HTTP 413 for large image uploads\n  chunked_transfer_encoding on; # required to avoid HTTP 411: see Issue #1486 (https://github.com/dotcloud/docker/issues/1486)\nlocation / {                                                               \n    auth_basic \"Restricted Location\";                                        \n    auth_basic_user_file /etc/nginx/htpasswd/.htpasswd;                      \n    proxy_pass http://docker-registry;                                       \n    #proxy_set_header    Host             $host;                             \n    #proxy_set_header    X-Real-IP        $remote_addr;                      \n    #proxy_set_header    X-Forwarded-For  $proxy_add_x_forwarded_for;        \n    #proxy_set_header    X-Client-Verify  SUCCESS;                           \n    #proxy_set_header    X-Client-DN      $ssl_client_s_dn;                  \n    #proxy_set_header    X-SSL-Subject    $ssl_client_s_dn;                  \n    #proxy_set_header    X-SSL-Issuer     $ssl_client_i_dn;                  \n    proxy_read_timeout 1800;                                                 \n    proxy_connect_timeout 1800;                                              \n  }                                                                          \n  location /_ping {                                                          \n    auth_basic off;                                                          \n    proxy_pass http://docker-registry;                                       \n  }                                                                          \n  location /v1/_ping {                                                       \n    auth_basic off;                                                          \n    proxy_pass http://docker-registry;                                       \n  }                                                                          \n}\n```\nThis is my working config. I've got them commented out at this point.\n. ",
    "uhop": "@benhosmer My registry is definitely running, because I can connect to it locally, as I indicated in my original post. I cannot access any URLs through a proxy, and, most importantly, docker doesn't think it is logged in. \n. @larrycai I can see that your config passes an extra header X-Forwarded-Proto, which is missing in the config suggested by docker-registry/contrib. It can easily be the real reason for the effect I am observing. Let me try it out, and I'll report results here. \n. It turned out that adding more headers do not help \u2014 I expected as much seeing that @benhosmer config doesn't list anything special. But I found the problem.\nThis is how I tried to connect:\nsudo docker login https://xxx.xxx.xxx/\nIt fails with a cryptic message.\nThat's how it works:\nsudo docker login xxx.xxx.xxx\nFrom docs I understood that a server option should be in URL format:\n```\n$ docker help login\nUsage: docker login [OPTIONS] [SERVER]\nRegister or log in to a Docker registry server, if no server is specified \"https://index.docker.io/v1/\" is the default.\n-e, --email=\"\"       Email\n  -p, --password=\"\"    Password\n  -u, --username=\"\"    Username\n```\nNotice the full-blown URL https://index.docker.io/v1/ as the default.\nThe same is mentioned in docs on login.\nFinally I decided to run @larrycai example (he helpfully lists a sequence of test commands, and even provides mock accounts and security certificates with the image), and noticed that his example uses a straight DNS name without any http / https part. It dawned on me to try it, and it worked.\nIdeally this ticket should be converted to a documentation ticket, but I don't know where to correct the CLI help, and the web site. If it is specific only to a stand-alone version of docker-registry, it should be mentioned in readme, or an FAQ, if there is one.\nI am closing the ticket for now. Thank you @larrycai and @benhosmer for helping me to troubleshoot the problem!\n. ",
    "StrongPa55w0rd": "Getting same error with Nginx and Apache. Tried all things still not working\nFATA[0006] Error response from daemon: (Code: 404; Headers: map[Content-Type:[text/plain; charset=utf-8] Date:[Wed, 17 Jun 2015 17:30:59 GMT] Docker-Distribution-Api-Version:[registry/2.0] Content-Length:[19] Connection:[keep-alive]]) \n. ",
    "isymbo": "You should change the ip 0.0.0.0 to localhost, the real ip or 127.0.0.1\nwould be fine.\nOn Mon Dec 29 2014 at 7:46:49 AM stonefury notifications@github.com wrote:\n\nWhat am I doing wrong here?\ndocker run -p 5000:5000 -v /tmp/registry:/tmp/registry -e\nGUNICORN_OPTS=[--preload] registry\nI've added the --insecure option to DOCKER_OPTS...\nTrying to push my local ubuntu image to the registry...\ndocker push 0.0.0.0:5000/ubuntu\nThe push refers to a repository 0.0.0.0:5000/ubuntu\nFATA[0002] no such id: 0.0.0.0:5000/ubuntu\nI'm really sorry if I'm just not reading the docs right or something. I\njust want a basic registry to start with, with no autentication so I can\npush pull some stuff for myself rather than load/save an image and scp it.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/871.\n. \n",
    "DGollings": "Almost surely own fault:\nTwo way, but never delete, rsync script runs every minute, _inprogress is a file...\n. ",
    "jothep": "Can I set my docker-registry for default and I'll pull images from my private store?\nFor example:\nI use 'docker run Ubuntu /bin/bash'\nIf localhost had no this image,It show 'Unable to find image 'Ubuntu' locally' and docker pull 'Ubuntu' image from my private store.\n. @dmp42 \n. thanks @dmp42 \n. ",
    "hvolpers": "Nevermind, missed -e GUNICORN_OPTS=[--preload]\n. ",
    "wenlock": "It just hangs forever here: \n```\ndocker build --rm -t forj/docker:registry .\nSending build context to Docker daemon 730.6 kB\nSending build context to Docker daemon \nStep 0 : FROM ubuntu:14.04\n ---> 8eaa4ff06b53\nStep 1 : RUN apt-get update     && apt-get install -y         swig         python-pip         python-dev         libssl-dev         liblzma-dev         libevent1-dev     && rm -rf /var/lib/apt/lists/*\n ---> Running in d87734f4d660\n```\nIf I add a set -x -v to Step 1 command, I see this output:\ndocker build --rm -t forj/docker:registry .\nSending build context to Docker daemon 730.6 kB\nSending build context to Docker daemon \nStep 0 : FROM ubuntu:14.04\n ---> 8eaa4ff06b53\nStep 1 : RUN set -x -v ; apt-get update     && apt-get install -y         swig         python-pip         python-dev         libssl-dev         liblzma-dev         libevent1-dev     && rm -rf /var/lib/apt/lists/*\n ---> Running in c52924df14b5\n+ apt-get update\n. I'm thinking things like http_proxy, HTTP_PROXY and apt.conf need to be configured to work through proxy.\n. Usually we set a proxy file in /etc/profile.d and configure apt.conf with proxy configuration.   Let me put a commit together on a copy of my repo to get your thoughts.  Shouldn't take long.\n. @dmp42 this commit on my forked repo makes it much further now, https://github.com/wenlock/docker-registry/commit/81ca0f2b4567ba2cb81ed29895cab7baf8229bc7\nUnfortunately, I'm still having a hickup on Step 8, it's complaining about being able to download Werkzeug.  Wondering if you've seen this one.\n```\n.... ....\nStep 8 : RUN . /opt/contrib/pip_options.sh     && pip install $PIP_OPTIONS file:///docker-registry#egg=docker-registry[bugsnag,newrelic,cors]\n ---> Running in fde2e9548adb\nUnpacking /docker-registry\n  Running setup.py (path:/tmp/pip_build_root/docker-registry/setup.py) egg_info for package docker-registry\nInstalling extra requirements: 'bugsnag,newrelic,cors'\nRequirement already satisfied (use --upgrade to upgrade): docker-registry-core>=2,<3 in /usr/local/lib/python2.7/dist-packages (from docker-registry[bugsnag,newrelic,cors])\nDownloading/unpacking backports.lzma==0.0.3,!=0.0.4 (from docker-registry[bugsnag,newrelic,cors])\n  Downloading backports.lzma-0.0.3.tar.gz\n  Running setup.py (path:/tmp/pip_build_root/backports.lzma/setup.py) egg_info for package backports.lzma\n    This is backports.lzma version 0.0.3\nDownloading/unpacking blinker==1.3 (from docker-registry[bugsnag,newrelic,cors])\n  Running setup.py (path:/tmp/pip_build_root/blinker/setup.py) egg_info for package blinker\nDownloading/unpacking Flask==0.10.1 (from docker-registry[bugsnag,newrelic,cors])\n  Running setup.py (path:/tmp/pip_build_root/Flask/setup.py) egg_info for package Flask\nwarning: no files found matching '*' under directory 'tests'\nwarning: no previously-included files matching '*.pyc' found under directory 'docs'\nwarning: no previously-included files matching '*.pyo' found under directory 'docs'\nwarning: no previously-included files matching '*.pyc' found under directory 'tests'\nwarning: no previously-included files matching '*.pyo' found under directory 'tests'\nwarning: no previously-included files matching '*.pyc' found under directory 'examples'\nwarning: no previously-included files matching '*.pyo' found under directory 'examples'\nno previously-included directories found matching 'docs/_build'\nno previously-included directories found matching 'docs/_themes/.git'\n\nDownloading/unpacking gevent==1.0.1 (from docker-registry[bugsnag,newrelic,cors])\n  Running setup.py (path:/tmp/pip_build_root/gevent/setup.py) egg_info for package gevent\nDownloading/unpacking gunicorn==19.1 (from docker-registry[bugsnag,newrelic,cors])\nDownloading/unpacking PyYAML==3.11 (from docker-registry[bugsnag,newrelic,cors])\n  Running setup.py (path:/tmp/pip_build_root/PyYAML/setup.py) egg_info for package PyYAML\nDownloading/unpacking requests==2.3.0 (from docker-registry[bugsnag,newrelic,cors])\nDownloading/unpacking M2Crypto==0.22.3 (from docker-registry[bugsnag,newrelic,cors])\n  Running setup.py (path:/tmp/pip_build_root/M2Crypto/setup.py) egg_info for package M2Crypto\nDownloading/unpacking sqlalchemy==0.9.4 (from docker-registry[bugsnag,newrelic,cors])\n  Running setup.py (path:/tmp/pip_build_root/sqlalchemy/setup.py) egg_info for package sqlalchemy\nwarning: no files found matching '*.jpg' under directory 'doc'\nwarning: no files found matching 'distribute_setup.py'\nwarning: no files found matching 'sa2to3.py'\nwarning: no files found matching 'ez_setup.py'\nno previously-included directories found matching 'doc/build/output'\n\nRequirement already satisfied (use --upgrade to upgrade): setuptools==5.8 in /usr/local/lib/python2.7/dist-packages (from docker-registry[bugsnag,newrelic,cors])\nDownloading/unpacking bugsnag>=2.0,<2.1 (from docker-registry[bugsnag,newrelic,cors])\n  Downloading bugsnag-2.0.2.tar.gz\n  Running setup.py (path:/tmp/pip_build_root/bugsnag/setup.py) egg_info for package bugsnag\nDownloading/unpacking Flask-cors>=1.8,<2.0 (from docker-registry[bugsnag,newrelic,cors])\n  Downloading Flask_Cors-1.10.2-py2-none-any.whl\nDownloading/unpacking newrelic>=2.22,<2.23 (from docker-registry[bugsnag,newrelic,cors])\n  Running setup.py (path:/tmp/pip_build_root/newrelic/setup.py) egg_info for package newrelic\nRequirement already satisfied (use --upgrade to upgrade): boto==2.34.0 in /usr/local/lib/python2.7/dist-packages (from docker-registry-core>=2,<3->docker-registry[bugsnag,newrelic,cors])\nRequirement already satisfied (use --upgrade to upgrade): redis==2.10.3 in /usr/local/lib/python2.7/dist-packages (from docker-registry-core>=2,<3->docker-registry[bugsnag,newrelic,cors])\nRequirement already satisfied (use --upgrade to upgrade): simplejson==3.6.2 in /usr/local/lib/python2.7/dist-packages (from docker-registry-core>=2,<3->docker-registry[bugsnag,newrelic,cors])\n```\nDownloading/unpacking Werkzeug>=0.7 (from Flask==0.10.1->docker-registry[bugsnag,newrelic,cors])\n  Real name of requirement Werkzeug is werkzeug\n  Could not find any downloads that satisfy the requirement Werkzeug>=0.7 (from Flask==0.10.1->docker-registry[bugsnag,newrelic,cors])\n  Some externally hosted files were ignored (use --allow-external Werkzeug to allow).\nCleaning up...\nNo distributions at all found for Werkzeug>=0.7 (from Flask==0.10.1->docker-registry[bugsnag,newrelic,cors])\nStoring debug log for failure in /root/.pip/pip.log\nINFO[0181] The command [/bin/sh -c . /opt/contrib/pip_options.sh     && pip install $PIP_OPTIONS file:///docker-registry#egg=docker-registry[bugsnag,newrelic,cors]] returned a non-zero code: 1\n. Also, if i add --alow-external Wekzeug, i get a memory allocation failure\n```\nDownloading/unpacking Werkzeug>=0.7 (from Flask==0.10.1->docker-registry[bugsnag,newrelic,cors])\n  Real name of requirement Werkzeug is werkzeug\n  Running setup.py (path:/tmp/pip_build_root/Werkzeug/setup.py) egg_info for package Werkzeug\n    Error [Errno 12] Cannot allocate memory while executing command python setup.py egg_info\nCleaning up...\nException:\nTraceback (most recent call last):\n  File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 122, in main\n    status = self.run(options, args)\n  File \"/usr/lib/python2.7/dist-packages/pip/commands/install.py\", line 278, in run\n    requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle)\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 1229, in prepare_files\n    req_to_install.run_egg_info()\n  File \"/usr/lib/python2.7/dist-packages/pip/req.py\", line 325, in run_egg_info\n    command_desc='python setup.py egg_info')\n  File \"/usr/lib/python2.7/dist-packages/pip/util.py\", line 662, in call_subprocess\n    cwd=cwd, env=env)\n  File \"/usr/lib/python2.7/subprocess.py\", line 710, in init\n    errread, errwrite)\n  File \"/usr/lib/python2.7/subprocess.py\", line 1223, in _execute_child\n    self.pid = os.fork()\nOSError: [Errno 12] Cannot allocate memory\nStoring debug log for failure in /root/.pip/pip.log\nINFO[0165] The command [/bin/sh -c . /opt/contrib/pip_options.sh     && pip install $PIP_OPTIONS --allow-external Werkzeug file:///docker-registry#egg=docker-registry[bugsnag,newrelic,cors]] returned a non-zero code: 2 \n```\n. Ok, I found that my local vm i was using for testing only had 512MB of ram.  I increased this to 1024M, and added options --alow-external Wekzeug  to my build.\nThat works now on my side with this commit:\nhttps://github.com/wenlock/docker-registry/commit/5d274b6a3b5ba8d7404561b6f57c8a9085196cae\nAny suggestions on both commits before it would be accepted in a pull request?\n. Yep, I like that idea to make those ENV's.   I do think however the apt-get update simply won't work without an update to the apt.conf.   I've not been able to make it happy on my end.   I was hoping that maybe someone knew of a plugin kind of like vagrant-proxyconf that could be leveraged here.   I ran into a similar issue under vagrant and that seemed to work really smoothly.   Wondering how others have solved similar issues with broken proxy's on other dockerfile projects.    apt isn't the only config I believe either out there, but it's the one that matters to this particular Dockerfile.\n. @tianon thanks for the input.  Yes, I think it would definitely be possible if corporate networking or services were modified.  Not always the case for teams though, and certainly not the case for me :<  \n@dmp42 I was also thinking last night, after looking at the storage drivers, that maybe there might be a way to integrate some pre-run actions in a similar way.  Maybe not.... still thinking on this for how we can keep the Dockerfile super trim.\n. question, with the new network plugin features, does it make sense to create a proxy network plugin, or are there plans to fix this in some other way?\n. i think build --env could still have some usefulness as well but likely maybe a plugin would work better for the proxy use case.\n. Nice job indeed @friism , can we close this issue now :D \n. cool, i have the power\n. my docker version:\ndocker version\nClient version: 1.5.0\nClient API version: 1.17\nGo version (client): go1.4.1\nGit commit (client): a8a31ef\nOS/Arch (client): linux/amd64\nServer version: 1.5.0\nServer API version: 1.17\nGo version (server): go1.4.1\nGit commit (server): a8a31ef\n. awesome, that added some more debugging, found the log in /var/log/upstart/docker.log\nI wasn't able to reproduce it after a fresh re-build of our system, but i suspect the issue was with a bad docker pull.   I can no longer reproduce this issue on my systems.  Closing.\n. ",
    "radenui": "Hello all,\nDon't know if this is the right thread for this problem, but as it is definitely related to proxy usage, I submit it here:\nI run this registry inside a private network that does not allow a direct access to the internet.\nTo make docker server aware of the proxy, I added:\nexport HTTP_PROXY=http://proxy:3128\nexport HTTPS_PROXY=http://proxy:3128\nexport NO_PROXY=\"*.my.domain.int, 10.0.0.0/8\"\nin the /etc/sysconfig/docker file.\nIt works: I can run the registry directly by using the standard docker run command.\nMy problem comes later: as the registry is using a backend in S3, I also have to provide the same information into the container itself (and the S3 backend engine in particular) so the registry can actually connect the proxy to access the internet and the S3 bucket location.\nUsually, the AWS python SDK is able to interpret same exported variables (HTTP_PROXY, HTTPS_PROXY, NO_PROXY). Would it be possible to enable to container to be passed these variables at start (-e HTTP_PROXY=...) ?\nThanks,\n. It does, thanks.\nJust so you know, I found a workaround for the proxy settings:\nAdding this configuration to the default config yaml:\ns3:\n  boto_proxy: _env:PROXY_HOST\n  boto_proxy_port: _env:PROXY_PORT\nand passing -e PROXY_HOST=myproxy -e PROXY_PORT=12345 to the run command works.\nIt still does not use the default boto configuration options, but at least that's a start.\nThanks!\n. ",
    "andersjanmyr": "I also have the same problem, I cannot build my images inside the proxy.\nI can build if I modify the Dockerfile by adding ENV http_proxy http://my-ip-number:3128/. \nThe problem with this is that then the Dockerfile cannot be used by someone not using a proxy.\nA solution that I believe could work is if I was able to give environment variables to the build command, similarly to the way I give them to the run command.\n```\nThis allows http-client access to my local http proxy.\ndocker run --env http_proxy=http://10.128.46.150:3128/ my/http-client\nThe same could work with build\ndocker build --env http_proxy=http://10.128.46.150:3128/ -t my/http-client .\n```\nWhat do you think?\n. ",
    "dpnl87": "I would love a solution as @andersjanmyr suggests.\n. ",
    "b0c1": "@andersjanmyr  +1\n. ",
    "byF": "+1\n. ",
    "nwinkler": "+1\n. I've tried docker-proxify recently and it was a bit of a pain to use. One of the downsides seems to be that the version of docker bundled in the docker-proxify image is fairly old.\n. ",
    "javiervivanco": "+1\n. ",
    "bjouhier": "@andersjanmyr +1\nNTLM proxy is a real PITA and a build --env option would help a lot. \nI haven't tried https://github.com/wtsi-hgi/docker-proxify yet but it should also solve the problem.\n. I tried it too yesterday and hit the same issue. It needs a refresh.\n. ",
    "jsidhu": "+1\n. ",
    "lsde": "+1\n. ",
    "youngl98": "+1\n. ",
    "danday74": "We are doing ...\nENV http_proxy http://9.9.9.9:9999\nENV https_proxy http://9.9.9.9:9999\nand at end of dockerfile ...\nENV http_proxy \"\"\nENV https_proxy \"\"\nThis, for now (until docker introduces build env vars), allows the proxy vars to be used for build without publicly exposing them\n. ",
    "lgautier": "A solution might be included in release 1.9.0: https://github.com/docker/docker/issues/14634\n. ",
    "ghostsquad": ":+1: \n. @softwareklinic are you setting proxy variables and using them at build time?\n. Can you build if you hard code the env vars or explicitly use -x\nparameter?\nhttp://www.cyberciti.biz/faq/linux-unix-curl-command-with-proxy-username-password-http-options/\nOn Sat, Jul 30, 2016, 11:54 AM Keyur Shah notifications@github.com wrote:\n\nTried that i created a new image from.already pulped image by\nadding/passing.build arguments and i.could see the proxy.variables inside\nthe container bash but still the curl could not connect to outside world.\nSent from my iPhone\nOn Jul 30, 2016, at 8:55 AM, Wes McNamee notifications@github.com<mailto:\nnotifications@github.com> wrote:\n@softwareklinichttps://github.com/softwareklinic are you setting proxy\nvariables and using them at build time?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<\nhttps://github.com/docker/docker-registry/issues/890#issuecomment-236372930>,\nor mute the thread<\nhttps://github.com/notifications/unsubscribe-auth/AHMCifMJee1BeAFfAzRUFdfhOpCjnlGVks5qa3PZgaJpZM4DPgfo>.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/docker/docker-registry/issues/890#issuecomment-236383019,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA3JQLiXGyXZfu9bY3N4uZTtZ-7wkkefks5qa53WgaJpZM4DPgfo\n.\n\n\nThanks,\nWes\n. ",
    "friism": "Docker now supports passing --build-env and I believe it's a good approach for HTTP proxies: https://docs.docker.com/engine/reference/commandline/build/#set-build-time-variables-build-arg\n. ",
    "zhao-ji": "good job! @friism \n. ",
    "softwareklinic": "I have docker 1.12.0 and still unable to communicate to outside world thru corporate proxy - any thoughts?\n. Tried that i created a new image from.already pulped image by adding/passing.build arguments and i.could see the proxy.variables inside the container bash but still the curl could not connect to outside world.\nSent from my iPhone\nOn Jul 30, 2016, at 8:55 AM, Wes McNamee notifications@github.com<mailto:notifications@github.com> wrote:\n@softwareklinichttps://github.com/softwareklinic are you setting proxy variables and using them at build time?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/docker/docker-registry/issues/890#issuecomment-236372930, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AHMCifMJee1BeAFfAzRUFdfhOpCjnlGVks5qa3PZgaJpZM4DPgfo.\n. Will try it out today\nSent from my iPhone\nOn Jul 30, 2016, at 12:23 PM, Wes McNamee notifications@github.com<mailto:notifications@github.com> wrote:\nCan you build if you hard code the env vars or explicitly use -x\nparameter?\nhttp://www.cyberciti.biz/faq/linux-unix-curl-command-with-proxy-username-password-http-options/\nOn Sat, Jul 30, 2016, 11:54 AM Keyur Shah notifications@github.com<mailto:notifications@github.com> wrote:\n\nTried that i created a new image from.already pulped image by\nadding/passing.build arguments and i.could see the proxy.variables inside\nthe container bash but still the curl could not connect to outside world.\nSent from my iPhone\nOn Jul 30, 2016, at 8:55 AM, Wes McNamee notifications@github.com<mailto:notifications@github.com\nnotifications@github.com<mailto:notifications@github.com>> wrote:\n@softwareklinichttps://github.com/softwareklinic are you setting proxy\nvariables and using them at build time?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub<\nhttps://github.com/docker/docker-registry/issues/890#issuecomment-236372930>,\nor mute the thread<\nhttps://github.com/notifications/unsubscribe-auth/AHMCifMJee1BeAFfAzRUFdfhOpCjnlGVks5qa3PZgaJpZM4DPgfo>.\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\nhttps://github.com/docker/docker-registry/issues/890#issuecomment-236383019,\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AA3JQLiXGyXZfu9bY3N4uZTtZ-7wkkefks5qa53WgaJpZM4DPgfo\n.\n\n\nThanks,\nWes\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHubhttps://github.com/docker/docker-registry/issues/890#issuecomment-236384908, or mute the threadhttps://github.com/notifications/unsubscribe-auth/AHMCiVOMfU69BYEkyAC__bb7PWQXnx5_ks5qa6SkgaJpZM4DPgfo.\n. ",
    "Jamlee": "it work for me \n````\nClient:\n Version:      17.05.0-ce\n API version:  1.29\n Go version:   go1.7.5\n Git commit:   89658be\n Built:        Thu May  4 22:10:54 2017\n OS/Arch:      linux/amd64\nServer:\n Version:      17.05.0-ce\n API version:  1.29 (minimum version 1.12)\n Go version:   go1.7.5\n Git commit:   89658be\n Built:        Thu May  4 22:10:54 2017\n OS/Arch:      linux/amd64\n Experimental: false\n````\ndocker build --build-arg http_proxy=http://my.proxy.url  --build-arg foo=bar \nFROM busybox\nRUN <command that need http_proxy>\nARG --description=\"foo's description\" foo\nUSER $foo\nMARK. @rocketraman   I occur a same problem. Do you  solve the problem?\njamlee@jamlee-pc:~/JAM/lab/docker-reg/private-registry$ docker push  docker.dev:5000/node\nThe push refers to a repository docker.dev:5000/node\nSending image list\nPushing repository docker.dev:5000/node (1 tags)\nImage 938e3817ad84 already pushed, skipping\nImage b7820d1ee4ee already pushed, skipping\nImage 511136ea3c5a already pushed, skipping\nImage 65688f7c61c4 already pushed, skipping\nImage d338bb63f151 already pushed, skipping\n2b7102d6f8b7: Pushing \nFATA[0006] HTTP code 500 while uploading metadata: invalid character '<' looking for beginning of value \nmy docker version:\njamlee@jamlee-pc:~/JAM/lab/docker-reg/private-registry$ docker version\nClient version: 1.5.0\nClient API version: 1.17\nGo version (client): go1.4.1\nGit commit (client): a8a31ef\nOS/Arch (client): linux/amd64\nServer version: 1.5.0\nServer API version: 1.17\nGo version (server): go1.4.1\nGit commit (server): a8a31ef\n. @dmp42  is it  really stable  now? how long you use it ? \n. ",
    "pavelz": "does not work for me either newest or 0.8.1 to 0.9.1\nClient version: 1.3.3\nClient API version: 1.15\nGo version (client): go1.3.2\nGit commit (client): 54d900a\nOS/Arch (client): linux/amd64\nServer version: 1.3.3\nServer API version: 1.15\nGo version (server): go1.3.2\nGit commit (server): 54d900a\n. ",
    "rseymour": "Happens to somewhat randomly to me on docker:1.4.1.\nbtrfs on fedora 21.\nShould note that  adding \n-e GUNICORN_OPTS=[--preload]\nfrom issue #796 comments worked fine\n. ",
    "jasonmartens": "I am experiencing this problem running the registry on CoreOS. The first time I run the registry and it has to pull from the registry image from the docker public repository, it always seems to fail. After the images have been pulled, then it seems to start just fine. \n. ",
    "weihanwang": "I reproduced this issue fairly consistently (~1 out of 4) on both boot2docker and a ubuntu 14.04 host. Both run docker 1.4.1 and registry 0.9.1. The same problem happened in earlier versions of docker and registry as well.\nI used the default configurations of both docker and registry.\nThe script that triggered this issue creates a registry container:\n$ docker create -P --name registry-container registry\nAnd then repeats these steps over and over again:\n1. docker start registry-container\n2. on localhost, push ~30 containers to registry\n3. on a remote host, pull all those containers from registry\n4. docker stop registry-container\n5. go to step 1\nLet me know if there's more info I can provide.\n. ",
    "cinterloper": "I had this issue on Amazon Linux.\nIt started for me after i upgraded docker.\nI resolved the issue by running 'docker pull registry' , which updated the copy of the registry image\n. ",
    "jcockhren": "I can confirm @cinterloper findings for Ubuntu x64 14.04.1\n. ",
    "akaspin": "Confirm. EC2, CoreOS.\n. Why it's closed? Problem persists.\n. ",
    "bunop": "Hi all, This bug is due to line 330 in /usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py, in which there is a \"os.remove(lock_path)\" and this lock file doesn't exists using the SETTINGS_FLAVOR=dev and a local storage path, as stated by user guide. I try to fix it by removing file after testing for its existance:\npython\n        finally:\n            #check for file existance\n            if os.path.exists(lock_path):\n                #the original 330 line of /usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\n                os.remove(lock_path)\n        return result\nAnd then docker-registry will start without the GUNICORN_OPTS\n. ",
    "ashish235": "I 'm still getting this error and unable to start the registry container. Going through the comments in this thread I couldn't derive a conclusion. \n[root@localhost files]# docker run -p 5000:5000 registry\n[info] POST /v1.15/containers/create\n[8f91c94d] +job create()\n[8f91c94d] +job log(create, 2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd, registry:latest)\n[8f91c94d] -job log(create, 2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd, registry:latest) = OK (0)\n[8f91c94d] -job create() = OK (0)\n[info] POST /v1.15/containers/2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd/attach?stderr=1&stdout=1&stream=1\n[8f91c94d] +job container_inspect(2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd)\n[8f91c94d] -job container_inspect(2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd) = OK (0)\n[8f91c94d] +job attach(2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd)\n[info] POST /v1.15/containers/2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd/start\n[8f91c94d] +job start(2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd)\n[8f91c94d] +job allocate_interface(2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd)\n[8f91c94d] -job allocate_interface(2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd) = OK (0)\n[8f91c94d] +job allocate_port(2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd)\n[8f91c94d] -job allocate_port(2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd) = OK (0)\n[8f91c94d] +job log(start, 2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd, registry:latest)\n[8f91c94d] -job log(start, 2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd, registry:latest) = OK (0)\n[8f91c94d] -job start(2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd) = OK (0)\n[2015-03-30 07:06:34 +0000] [1] [INFO] Starting gunicorn 19.1.1\n[2015-03-30 07:06:34 +0000] [1] [INFO] Listening at: http://0.0.0.0:5000 (1)\n[2015-03-30 07:06:34 +0000] [1] [INFO] Using worker: gevent\n[2015-03-30 07:06:34 +0000] [12] [INFO] Booting worker with pid: 12\n[2015-03-30 07:06:34 +0000] [13] [INFO] Booting worker with pid: 13\n[2015-03-30 07:06:34 +0000] [14] [INFO] Booting worker with pid: 14\n[2015-03-30 07:06:34 +0000] [15] [INFO] Booting worker with pid: 15\n30/Mar/2015:07:06:35 +0000 WARNING: Cache storage disabled!\n30/Mar/2015:07:06:35 +0000 WARNING: Cache storage disabled!\n30/Mar/2015:07:06:35 +0000 WARNING: LRU cache disabled!\n30/Mar/2015:07:06:35 +0000 WARNING: LRU cache disabled!\n30/Mar/2015:07:06:35 +0000 WARNING: Cache storage disabled!\n30/Mar/2015:07:06:35 +0000 WARNING: Cache storage disabled!\n30/Mar/2015:07:06:35 +0000 WARNING: LRU cache disabled!\n30/Mar/2015:07:06:35 +0000 WARNING: LRU cache disabled!\n30/Mar/2015:07:06:35 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n30/Mar/2015:07:06:35 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n30/Mar/2015:07:06:35 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n30/Mar/2015:07:06:35 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n30/Mar/2015:07:06:36 +0000 WARNING: Another process is creating the search database\n30/Mar/2015:07:06:36 +0000 WARNING: Another process is creating the search database\n30/Mar/2015:07:06:36 +0000 WARNING: DB is disconnected. Reconnect to it.\n[2015-03-30 07:06:36 +0000] [14] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 330, in wrapper\n    os.remove(lock_path)\nOSError: [Errno 2] No such file or directory: './registry._setup_database.lock'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 330, in wrapper\n    os.remove(lock_path)\nOSError: [Errno 2] No such file or directory: './registry._setup_database.lock'\n[2015-03-30 07:06:36 +0000] [14] [INFO] Worker exiting (pid: 14)\n[2015-03-30 07:06:37 +0000] [15] [INFO] Worker exiting (pid: 15)\n[2015-03-30 07:06:37 +0000] [13] [INFO] Worker exiting (pid: 13)\n[2015-03-30 07:06:37 +0000] [12] [INFO] Worker exiting (pid: 12)\n[2015-03-30 07:06:37 +0000] [1] [INFO] Shutting down: Master\n[2015-03-30 07:06:37 +0000] [1] [INFO] Reason: Worker failed to boot.\n[8f91c94d] +job log(die, 2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd, registry:latest)\n[8f91c94d] -job log(die, 2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd, registry:latest) = OK (0)\n[8f91c94d] +job release_interface(2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd)\n[8f91c94d] -job attach(2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd) = OK (0)\n[info] POST /v1.15/containers/2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd/wait\n[8f91c94d] +job wait(2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd)\n[8f91c94d] -job release_interface(2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd) = OK (0)\n[8f91c94d] -job wait(2f6d143842dd4a86c7f9bfdd3eb92b0754291abfde5d9ddc8ee0d31a1475b5bd) = OK (0)\nPlease someone help. \n. Works for me every time. I use -e GUNICORN_OPTS=[\"--preload\"]  by default now to start registry.\n. For some strange reason, I feel the the v2 is slower than v1. The same image used to take 45 secs on my v1 registry to push while on v2 it's almost 2 mins. I 'm not sure why this delay. \nAnyone facing same issue?\n. ",
    "kulbida": "I'm having the same crash here... \n. 2015-04-13 02:39:15 +0000] [1] [INFO] Starting gunicorn 19.1.1\n[2015-04-13 02:39:15 +0000] [1] [INFO] Listening at: http://0.0.0.0:5000 (1)\n[2015-04-13 02:39:15 +0000] [1] [INFO] Using worker: gevent\n[2015-04-13 02:39:15 +0000] [12] [INFO] Booting worker with pid: 12\n[2015-04-13 02:39:15 +0000] [13] [INFO] Booting worker with pid: 13\n[2015-04-13 02:39:15 +0000] [14] [INFO] Booting worker with pid: 14\n[2015-04-13 02:39:15 +0000] [15] [INFO] Booting worker with pid: 15\n13/Apr/2015:02:39:15 +0000 WARNING: Cache storage disabled!\n13/Apr/2015:02:39:15 +0000 WARNING: Cache storage disabled!\n13/Apr/2015:02:39:15 +0000 WARNING: LRU cache disabled!\n13/Apr/2015:02:39:15 +0000 WARNING: LRU cache disabled!\n13/Apr/2015:02:39:15 +0000 WARNING: Cache storage disabled!\n13/Apr/2015:02:39:15 +0000 WARNING: LRU cache disabled!\n13/Apr/2015:02:39:15 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n13/Apr/2015:02:39:15 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n13/Apr/2015:02:39:15 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n13/Apr/2015:02:39:15 +0000 WARNING: Cache storage disabled!\n13/Apr/2015:02:39:15 +0000 WARNING: LRU cache disabled!\n13/Apr/2015:02:39:15 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n13/Apr/2015:02:39:15 +0000 WARNING: Another process is creating the search database\n13/Apr/2015:02:39:16 +0000 WARNING: DB is disconnected. Reconnect to it.\n[2015-04-13 02:39:16 +0000] [13] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 330, in wrapper\n    os.remove(lock_path)\nOSError: [Errno 2] No such file or directory: './registry._setup_database.lock'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 330, in wrapper\n    os.remove(lock_path)\nOSError: [Errno 2] No such file or directory: './registry._setup_database.lock'\n[2015-04-13 02:39:16 +0000] [13] [INFO] Worker exiting (pid: 13)\n13/Apr/2015:02:39:16 +0000 WARNING: DB is disconnected. Reconnect to it.\n[2015-04-13 02:39:16 +0000] [15] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 330, in wrapper\n    os.remove(lock_path)\nOSError: [Errno 2] No such file or directory: './registry._setup_database.lock'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 330, in wrapper\n    os.remove(lock_path)\nOSError: [Errno 2] No such file or directory: './registry._setup_database.lock'\n[2015-04-13 02:39:16 +0000] [15] [INFO] Worker exiting (pid: 15)\n[2015-04-13 02:39:16 +0000] [12] [INFO] Worker exiting (pid: 12)\n[2015-04-13 02:39:17 +0000] [14] [INFO] Worker exiting (pid: 14)\n[2015-04-13 02:39:17 +0000] [1] [INFO] Shutting down: Master\n[2015-04-13 02:39:17 +0000] [1] [INFO] Reason: Worker failed to boot.\n. Used -e GUNICORN_OPTS=[\"--preload\"] as a temp workaround...\n. ",
    "wptad": "-e GUNICORN_OPTS=[\"--preload\"] works for me\n. ",
    "charlielin": "-e GUNICORN_OPTS=[\"--preload\"] works for me, too\n. ",
    "patjlm": "Same here. -e GUNICORN_OPTS=[\"--preload\"] made the trick\n. ",
    "mayaguang": "-e GUNICORN_OPTS=[\"--preload\"] works for me, too\n. ",
    "dalanlan": "Same pb still.\n. Uhh..This prj has been deprecated. You should switch to v2 instead. \n. Fair enough. The progress of v2 is getting a bit slow, i'll admit.\n. ",
    "felixgao": "Using the latest registry from docker and still having the same problem\ndocker logs private_registry\n[2015-07-23 21:47:47 +0000] [1] [INFO] Starting gunicorn 19.1.1\n[2015-07-23 21:47:47 +0000] [1] [INFO] Listening at: http://0.0.0.0:5000 (1)\n[2015-07-23 21:47:47 +0000] [1] [INFO] Using worker: gevent\n[2015-07-23 21:47:47 +0000] [13] [INFO] Booting worker with pid: 13\n[2015-07-23 21:47:48 +0000] [14] [INFO] Booting worker with pid: 14\n[2015-07-23 21:47:48 +0000] [15] [INFO] Booting worker with pid: 15\n[2015-07-23 21:47:48 +0000] [22] [INFO] Booting worker with pid: 22\n23/Jul/2015:21:47:48 +0000 WARNING: Cache storage disabled!\n23/Jul/2015:21:47:48 +0000 WARNING: LRU cache disabled!\n23/Jul/2015:21:47:48 +0000 WARNING: Cache storage disabled!\n23/Jul/2015:21:47:48 +0000 WARNING: LRU cache disabled!\n23/Jul/2015:21:47:48 +0000 WARNING: Cache storage disabled!\n23/Jul/2015:21:47:48 +0000 WARNING: LRU cache disabled!\n23/Jul/2015:21:47:48 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n23/Jul/2015:21:47:48 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n23/Jul/2015:21:47:48 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n23/Jul/2015:21:47:48 +0000 WARNING: Cache storage disabled!\n23/Jul/2015:21:47:48 +0000 WARNING: LRU cache disabled!\n23/Jul/2015:21:47:48 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n23/Jul/2015:21:47:48 +0000 WARNING: DB is disconnected. Reconnect to it.\n[2015-07-23 21:47:48 +0000] [22] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 330, in wrapper\n    os.remove(lock_path)\nOSError: [Errno 2] No such file or directory: './registry._setup_database.lock'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 330, in wrapper\n    os.remove(lock_path)\nOSError: [Errno 2] No such file or directory: './registry._setup_database.lock'\n[2015-07-23 21:47:48 +0000] [22] [INFO] Worker exiting (pid: 22)\n23/Jul/2015:21:47:48 +0000 WARNING: DB is disconnected. Reconnect to it.\n[2015-07-23 21:47:48 +0000] [13] [ERROR] Exception in worker process:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 330, in wrapper\n    os.remove(lock_path)\nOSError: [Errno 2] No such file or directory: './registry._setup_database.lock'\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in spawn_worker\n    worker.init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line 193, in init_process\n    super(GeventWorker, self).init_process()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line 114, in init_process\n    self.wsgi = self.app.wsgi()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in wsgi\n    self.callable = self.load()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65, in load\n    return self.load_wsgiapp()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in import_app\n    import(module)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\", line 27, in \n    from .search import *  # noqa\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\", line 14, in \n    INDEX = index.load(cfg.search_backend.lower())\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/init.py\", line 82, in load\n    return db.SQLAlchemyIndex()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\", line 86, in init\n    self._setup_database()\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 330, in wrapper\n    os.remove(lock_path)\nOSError: [Errno 2] No such file or directory: './registry._setup_database.lock'\n[2015-07-23 21:47:48 +0000] [13] [INFO] Worker exiting (pid: 13)\nTraceback (most recent call last):\n  File \"/usr/local/bin/gunicorn\", line 11, in \n    sys.exit(run())\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 74, in run\n    WSGIApplication(\"%(prog)s [OPTIONS] [APP_MODULE]\").run()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 185, in run\n    super(Application, self).run()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 71, in run\n    Arbiter(self).run()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 196, in run\n    self.halt(reason=inst.reason, exit_status=inst.exit_status)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 292, in halt\n    self.stop()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 343, in stop\n    time.sleep(0.1)\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 209, in handle_chld\n    self.reap_workers()\n  File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 459, in reap_workers\n    raise HaltServer(reason, self.WORKER_BOOT_ERROR)\ngunicorn.errors.HaltServer: \n[2015-07-23 21:47:50 +0000] [1] [INFO] Starting gunicorn 19.1.1\n[2015-07-23 21:47:50 +0000] [1] [INFO] Listening at: http://0.0.0.0:5000 (1)\n[2015-07-23 21:47:50 +0000] [1] [INFO] Using worker: gevent\n[2015-07-23 21:47:50 +0000] [14] [INFO] Booting worker with pid: 14\n[2015-07-23 21:47:50 +0000] [15] [INFO] Booting worker with pid: 15\n[2015-07-23 21:47:50 +0000] [18] [INFO] Booting worker with pid: 18\n[2015-07-23 21:47:50 +0000] [23] [INFO] Booting worker with pid: 23\n23/Jul/2015:21:47:50 +0000 WARNING: Cache storage disabled!\n23/Jul/2015:21:47:50 +0000 WARNING: LRU cache disabled!\n23/Jul/2015:21:47:50 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n23/Jul/2015:21:47:50 +0000 WARNING: Cache storage disabled!\n23/Jul/2015:21:47:50 +0000 WARNING: LRU cache disabled!\n23/Jul/2015:21:47:50 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n23/Jul/2015:21:47:50 +0000 WARNING: Cache storage disabled!\n23/Jul/2015:21:47:50 +0000 WARNING: LRU cache disabled!\n23/Jul/2015:21:47:50 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n23/Jul/2015:21:47:50 +0000 WARNING: Cache storage disabled!\n23/Jul/2015:21:47:50 +0000 WARNING: LRU cache disabled!\n23/Jul/2015:21:47:50 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n. ",
    "andrask": "@dalanlan Unfortunately, v2 has several drawbacks when used in a CI pipeline (with plenty of network bandwidth and storage):\n1. push is slow\n2. layers are pushed repeatedly due to a bug\n3. Additionally, there is no way to browse a v2 registry, thus, UIs don't exist.\nThese provide enough reason, for example, for us to stick with v1 until they get solved.\n. ",
    "heeren-cliqz": "well .. I call simply docker run redis .. \nAnd unlike the previous experience, I am getting above mentioned error message. \nSo basically from my phrase, I dockerise a redis container. \n. ",
    "xiekeyang": "@dmp42 \nHowever, most of docker images registries, such as Docker Hub, quay.io, etc, allow to search and pull freely. \nSo I think It is likely a common behavior, and should be accepted by master branch.\nThanks a lot!\n. @dmp42 \nThanks a lot.\n. @shin- really? However, when I use \nGET http://myregistry.io/v1/repositories/namespace/invalid_repo/properties\nit returns {\"access\": \"private\"}, which is incorrect.\nfor users who build their own index, this kind of rest API is necessary, and the bug will happen possibly.\nAs other APIs code in registry, it seems had better to add the prejudging.\nThanks\n. Yes. So if the condition in set_properties like\nif not data or not isinstance(data, dict):\n    return toolkit.api_error('Invalid data')\nwill also degrade the registry performance?\nDefinitely, the index can and should be aware of data valid.\nIf i should remove this line?\nThanks\n. Right, but I'm just some confused. \nI agree it is important of high performance. But firstly, The registry likely have to avoid illogical circle inside it, right?\nRepository is saved in registry, but not in index. I agree that it can be avoided by checking Repository firstly. But when I use this API directly, and get incorrect response, it make me feel this function not good.\nThere are similar weak points in the registry, like invalid properties data setting making registry return true, invalid image id being accepted when building a new repo, etc.\nI know these can be avoid by pre-checking, but its circle is more likely illogical. Now I'm not sure if you regard them as bug, and if I should make analysis. \nwhat do you think? Thanks.\n. Thanks a lot for your explain!\n. PUT properties methods lie in registry code. but it seems to do nothing.\nmaybe develops can do it in their own index. However, if it had better to be available and to be useful user, to be fixed?\nThanks.\n. @dmp42  Thanks a lot!\nI present some explain on \u201cfix Sending Mail Functionality for Docker registry exception #944\u201d.\nhttps://github.com/xiekeyang/docker-registry/commit/946c21af14f0155020332be84c4f132d0186c92c\nCould you please take a look? If it is OK, I'd put a PR.\n. @dmp42  I think, actually, it is a good idea to add error logeer on each code exception.\nTherefore, the ADVANCED.md may keep unchanged (usage for config _env may be add in md), and logger add on each 'try except' statement. it will not be executed on normal case, so that impact little on performance.\nWhat is your proposal?\nThanks a lot!\n. ",
    "omribahumi": "I see. Nevermind then :)\n. ",
    "wozz": "Registry logs are above, followed by this (2 line overlap):\n[2015-01-14 20:32:10 +0000] [56] [INFO] Worker exiting (pid: 56)\n[2015-01-14 20:32:10 +0000] [62] [INFO] Booting worker with pid: 62\n172.17.42.1 - - [14/Jan/2015:20:41:04 +0000] \"GET /v1/_ping HTTP/1.0\" 200 2 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [14/Jan/2015:20:41:04 +0000] \"GET /v1/_ping HTTP/1.0\" 200 2 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [14/Jan/2015:20:41:04 +0000] \"GET /v1/users/ HTTP/1.0\" 200 4 \"-\" \"docker/1.4.1 go/go1.3.3 git-commit/5bc2ff8 kernel/3.13.0-36-generic os/linux arch/amd64\"\n172.17.42.1 - - [14/Jan/2015:20:41:04 +0000] \"GET /v1/_ping HTTP/1.0\" 200 2 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [14/Jan/2015:20:41:04 +0000] \"GET /v1/_ping HTTP/1.0\" 200 2 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [14/Jan/2015:20:56:13 +0000] \"GET /v1/_ping HTTP/1.0\" 200 2 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [14/Jan/2015:20:56:13 +0000] \"GET /v1/_ping HTTP/1.0\" 200 2 \"-\" \"Go 1.1 package http\"\nCorresponding NGINX log is indeed a timeout:\n2015/01/14 20:48:49 [error] 17048#0: *14004 upstream timed out (110: Connection timed out) while reading response header from upstream, client: [ip], server: registry.[domain].com, request: \"PUT /v1/images/4b06ea88509c2377f94a9d0f4f260247dfa6b195e716fd3a14c1c52b17a01bf1/checksum HTTP/1.1\", upstream: \"http://127.0.0.1:5000/v1/images/4b06ea88509c2377f94a9d0f4f260247dfa6b195e716fd3a14c1c52b17a01bf1/checksum\", host: \"registry.[domain].com\"\nTo me, this appears like the registry crashed and stopped responding.  I can try turning on some higher level debug logs, although I don't know how to do that for docker-registry.\n. Here's my NGINX config\nupstream dkrreg {\n    server 127.0.0.1:5000 max_fails=0;\n    keepalive 512;\n}\nserver {\n    listen 80;\n    return 301 https://$host$request_uri;\n}\nserver {\n    listen 443 ssl;\n    server_name domain;\n    ssl_certificate     /etc/ssl/certs/domain.crt;\n    ssl_certificate_key /etc/ssl/private/domain.key;\n    ssl_session_cache shared:SSL:30m;\n    ssl_session_timeout 30m;\n    ssl_protocols SSLv3 TLSv1 TLSv1.1 TLSv1.2;\n    ssl_prefer_server_ciphers on;\n    ssl_ciphers ECDHE-RSA-AES128-SHA:AES256-GCM-SHA256:ECDHE-RSA-AES256-SHA256:HIGH:!aNULL:!MD5:-LOW:-SSLv2:-EXP;\n    client_max_body_size 900M;\n    location /_ping {\n        auth_basic off;\n        proxy_pass http://dkrreg;\n    }\n    location /v1/_ping {\n        auth_basic off;\n        proxy_pass http://dkrreg;\n    }\n    location / {\n        auth_basic              \"Restricted\";\n        auth_basic_user_file    docker-registry.htpasswd;\n        proxy_pass http://dkrreg;\n        proxy_set_header Host $host;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto https;\n        real_ip_header X-Forwarded-For;\n        real_ip_recursive on;\n        proxy_read_timeout 1000;\n    }\n}\ncurl localhost:5000/_ping returns {}\nThis is how I run the docker container:\ndocker run\n    -e SETTINGS_FLAVOR=s3\n    -e AWS_BUCKET=bucketname\n    -e AWS_REGION=us-east-1\n    -e STORAGE_PATH=/registry\n    -e SEARCH_BACKEND=\n    -e DEBUG=true\n    -e GUNICORN_OPTS='[--preload]'\n    -p 5000:5000\n    registry-master\n(I get the same errors with/without search backend, but it's currently turned off, and DEBUG was just added after you suggested it)\n. Hmmm... maybe I need to increase proxy_read_timeout\n. ",
    "anshuljoshi": "I too was facing the same problem while pushing the image with Docker version 1.7.0, build 0baf609.\nSimply doing:\n\"sudo docker push my/test\"\nresolved it. So, I suppose \"sudo\" is needed because of some permission issue.\n. ",
    "smiller171": "I just ran into this myself on updating my registry. it used to work, but now it's broken\n. ~~@trinitronx working with sudo was just luck. I'm using registry v2, and after hours of fighting with it I determined only that there is some type of bug with the S3 storage backend that fails 90% of the time when doing a push. every once in a while it gets through, and it working when I used sudo was coincidence.~~ unrelated as I'm using v2\n. Thanks @dmp42 @trinitronx I stumbled here from Google trying to research my problem. \n. ",
    "PlugIN73": "@dmp42 yep! This restriction very strange and error message not good. \n@shin- why the name of the repository must be less than 30 characters?\n. ",
    "resouer": "+1 too\n@PlugIN73 @shin- why the name of the repository must be less than 30 characters?\n. ",
    "deimosfr": "+1\n. Hi,\nSorry for the delay. So regarding the command I'm using:\n$ curl -X DELETE 'https://registry/v1/repositories/library/image-name/tags/0.3'\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\">\n<title>500 Internal Server Error</title>\n<h1>Internal Server Error</h1>\n<p>The server encountered an internal error and was unable to complete your request.  Either the server is overloaded or there is an error in the application.</p>\nI do not have the issue without Redis. As said, I just have an issue with the delete request.\nThanks\n. ",
    "soupdiver": "The first output was taken from the browser.\nThis is the curl one\n```\ncurl -i https://registry.XXX.com:8888/v1/_ping\nHTTP/1.1 200 OK\nServer: nginx/1.1.19\nDate: Wed, 21 Jan 2015 00:21:14 GMT\nContent-Type: application/json\nContent-Length: 1547\nConnection: keep-alive\nX-Docker-Registry-Config: dev\nExpires: -1\nX-Docker-Registry-Standalone: True\nPragma: no-cache\nCache-Control: no-cache\n{\"host\": [\"Linux\", \"b45034e2067d\", \"3.5.0-37-generic\", \"#58~precise1-Ubuntu SMP Wed Jul 10 17:48:11 UTC 2013\", \"x86_64\", \"x86_64\"], \"launch\": [\"/usr/local/bin/gunicorn\", \"--access-logfile\", \"-\", \"--error-logfile\", \"-\", \"--max-requests\", \"100\", \"-k\", \"gevent\", \"--graceful-timeout\", \"3600\", \"-t\", \"3600\", \"-w\", \"4\", \"-b\", \"0.0.0.0:5000\", \"--reload\", \"docker_registry.wsgi:application\"], \"versions\": {\"M2Crypto.m2xmlrpclib\": \"0.22\", \"SocketServer\": \"0.4\", \"argparse\": \"1.1\", \"backports.lzma\": \"0.0.3\", \"blinker\": \"1.3\", \"cPickle\": \"1.71\", \"cgi\": \"2.6\", \"ctypes\": \"1.1.0\", \"decimal\": \"1.70\", \"distutils\": \"2.7.6\", \"docker_registry.app\": \"0.9.1\", \"docker_registry.core\": \"2.0.3\", \"docker_registry.server\": \"0.9.1\", \"email\": \"4.0.3\", \"flask\": \"0.10.1\", \"gevent\": \"1.0.1\", \"greenlet\": \"0.4.5\", \"gunicorn\": \"19.1.1\", \"gunicorn.arbiter\": \"19.1.1\", \"gunicorn.config\": \"19.1.1\", \"gunicorn.six\": \"1.2.0\", \"jinja2\": \"2.7.3\", \"json\": \"2.0.9\", \"logging\": \"0.5.1.2\", \"parser\": \"0.5\", \"pickle\": \"$Revision: 72223 $\", \"platform\": \"1.0.7\", \"pyexpat\": \"2.7.6\", \"python\": \"2.7.6 (default, Mar 22 2014, 22:59:56) \\n[GCC 4.8.2]\", \"re\": \"2.2.1\", \"redis\": \"2.10.3\", \"requests\": \"2.3.0\", \"requests.packages.chardet\": \"2.2.1\", \"requests.packages.urllib3\": \"dev\", \"requests.packages.urllib3.packages.six\": \"1.2.0\", \"requests.utils\": \"2.3.0\", \"simplejson\": \"3.6.2\", \"sqlalchemy\": \"0.9.4\", \"tarfile\": \"$Revision: 85213 $\", \"urllib\": \"1.17\", \"urllib2\": \"2.7\", \"werkzeug\": \"0.9.6\", \"xml.parsers.expat\": \"$Revision: 17640 $\", \"xmlrpclib\": \"1.0.1\", \"yaml\": \"3.11\", \"zlib\": \"1.0\"}}\n```\nRegistry machine:\ndocker info\nContainers: 18\nImages: 24\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Dirs: 60\nExecution Driver: native-0.2\nKernel Version: 3.5.0-37-generic\nOperating System: Ubuntu precise (12.04.4 LTS)\nCPUs: 8\nTotal Memory: 31.29 GiB\nName: foobar\nID: MD2M:HVXM:4CNZ:OMOG:KR6E:ANQ2:KLQA:KVTT:AIOE:RCLH:P7BS:VNE5\ndocker version\nClient version: 1.4.1\nClient API version: 1.16\nGo version (client): go1.3.3\nGit commit (client): 5bc2ff8\nOS/Arch (client): linux/amd64\nServer version: 1.4.1\nServer API version: 1.16\nGo version (server): go1.3.3\nGit commit (server): 5bc2ff8\nClient machine:\ndocker info\nContainers: 2\nImages: 458\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Dirs: 462\nExecution Driver: native-0.2\nKernel Version: 3.13.0-36-generic\nOperating System: Ubuntu 14.04.1 LTS\nWARNING: No swap limit support\ndocker version\nClient version: 1.3.3\nClient API version: 1.15\nGo version (client): go1.3.3\nGit commit (client): d344625\nOS/Arch (client): linux/amd64\nServer version: 1.3.3\nServer API version: 1.15\nGo version (server): go1.3.3\nGit commit (server): d344625\n. Ok thank, upgrading nginx solved the problem.\nWeird...\n. ",
    "kyleburnett": "I used pip install docker-registry\n. No, I didn't change anything.\npip install docker-registry is part of our Vagrantfile. This is the output that I see. The first line (Installing Docker Registry) is my print statement from the Vagrantfile.\n==> default: Installing Docker Registry...\n==> default: Downloading/unpacking docker-registry\n==> default:   Running setup.py (path:/tmp/pip_build_root/docker-registry/setup.py) egg_info for package docker-registry\n==> default:     \n==> default: Downloading/unpacking docker-registry-core>=2,<3 (from docker-registry)\n==> default:   Downloading docker-registry-core-2.0.3.tar.gz\n==> default:   Running setup.py (path:/tmp/pip_build_root/docker-registry-core/setup.py) egg_info for package docker-registry-core\n==> default:     \n==> default: Downloading/unpacking backports.lzma==0.0.3,!=0.0.4 (from docker-registry)\n==> default:   Downloading backports.lzma-0.0.3.tar.gz\n==> default:   Running setup.py (path:/tmp/pip_build_root/backports.lzma/setup.py) egg_info for package backports.lzma\n==> default:     This is backports.lzma version 0.0.3\n==> default:     \n==> default: Downloading/unpacking blinker==1.3 (from docker-registry)\n==> default:   Running setup.py (path:/tmp/pip_build_root/blinker/setup.py) egg_info for package blinker\n==> default:     \n==> default: Downloading/unpacking Flask==0.10.1 (from docker-registry)\n==> default:   Running setup.py (path:/tmp/pip_build_root/Flask/setup.py) egg_info for package Flask\n==> default:     \n==> default:     warning: no files found matching '*' under directory 'tests'\n==> default:     warning: no previously-included files matching '*.pyc' found under directory 'docs'\n==> default:     warning: no previously-included files matching '*.pyo' found under directory 'docs'\n==> default:     warning: no previously-included files matching '*.pyc' found under directory 'tests'\n==> default:     warning: no previously-included files matching '*.pyo' found under directory 'tests'\n==> default:     warning: no previously-included files matching '*.pyc' found under directory 'examples'\n==> default:     warning: no previously-included files matching '*.pyo' found under directory 'examples'\n==> default:     no previously-included directories found matching 'docs/_build'\n==> default:     no previously-included directories found matching 'docs/_themes/.git'\n==> default: Downloading/unpacking gevent==1.0.1 (from docker-registry)\n==> default:   Running setup.py (path:/tmp/pip_build_root/gevent/setup.py) egg_info for package gevent\n==> default:     \n==> default: Downloading/unpacking gunicorn==19.1.1 (from docker-registry)\n==> default: Downloading/unpacking PyYAML==3.11 (from docker-registry)\n==> default:   Running setup.py (path:/tmp/pip_build_root/PyYAML/setup.py) egg_info for package PyYAML\n==> default:     \n==> default: Downloading/unpacking requests==2.3.0 (from docker-registry)\n==> default: Downloading/unpacking M2Crypto==0.22.3 (from docker-registry)\n==> default:   Running setup.py (path:/tmp/pip_build_root/M2Crypto/setup.py) egg_info for package M2Crypto\n==> default:     \n==> default: Downloading/unpacking sqlalchemy==0.9.4 (from docker-registry)\n==> default:   Running setup.py (path:/tmp/pip_build_root/sqlalchemy/setup.py) egg_info for package sqlalchemy\n==> default:     \n==> default:     warning: no files found matching '*.jpg' under directory 'doc'\n==> default:     warning: no files found matching 'distribute_setup.py'\n==> default:     warning: no files found matching 'sa2to3.py'\n==> default:     warning: no files found matching 'ez_setup.py'\n==> default:     no previously-included directories found matching 'doc/build/output'\n==> default: Downloading/unpacking setuptools==5.8 (from docker-registry)\n==> default: Downloading/unpacking boto==2.34.0 (from docker-registry-core>=2,<3->docker-registry)\n==> default: Downloading/unpacking redis==2.10.3 (from docker-registry-core>=2,<3->docker-registry)\n==> default:   Running setup.py (path:/tmp/pip_build_root/redis/setup.py) egg_info for package redis\n==> default:     \n==> default:     warning: no previously-included files found matching '__pycache__'\n==> default:     warning: no previously-included files matching '*.pyc' found under directory 'tests'\n==> default: Downloading/unpacking simplejson==3.6.2 (from docker-registry-core>=2,<3->docker-registry)\n==> default:   Running setup.py (path:/tmp/pip_build_root/simplejson/setup.py) egg_info for package simplejson\n==> default:     \n==> default: Downloading/unpacking Werkzeug>=0.7 (from Flask==0.10.1->docker-registry)\n==> default:   Running setup.py (path:/tmp/pip_build_root/Werkzeug/setup.py) egg_info for package Werkzeug\n==> default:     \n==> default:     warning: no files found matching '*' under directory 'werkzeug/debug/templates'\n==> default:     warning: no files found matching '*' under directory 'tests'\n==> default:     warning: no previously-included files matching '*.pyc' found under directory 'docs'\n==> default:     warning: no previously-included files matching '*.pyo' found under directory 'docs'\n==> default:     warning: no previously-included files matching '*.pyc' found under directory 'tests'\n==> default:     warning: no previously-included files matching '*.pyo' found under directory 'tests'\n==> default:     warning: no previously-included files matching '*.pyc' found under directory 'examples'\n==> default:     warning: no previously-included files matching '*.pyo' found under directory 'examples'\n==> default:     no previously-included directories found matching 'docs/_build'\n==> default: Downloading/unpacking Jinja2>=2.4 (from Flask==0.10.1->docker-registry)\n==> default:   Running setup.py (path:/tmp/pip_build_root/Jinja2/setup.py) egg_info for package Jinja2\n==> default:     \n==> default:     warning: no files found matching '*' under directory 'custom_fixers'\n==> default:     warning: no previously-included files matching '*' found under directory 'docs/_build'\n==> default:     warning: no previously-included files matching '*.pyc' found under directory 'jinja2'\n==> default:     warning: no previously-included files matching '*.pyc' found under directory 'docs'\n==> default:     warning: no previously-included files matching '*.pyo' found under directory 'jinja2'\n==> default:     warning: no previously-included files matching '*.pyo' found under directory 'docs'\n==> default: Downloading/unpacking itsdangerous>=0.21 (from Flask==0.10.1->docker-registry)\n==> default:   Running setup.py (path:/tmp/pip_build_root/itsdangerous/setup.py) egg_info for package itsdangerous\n==> default:     \n==> default:     warning: no previously-included files matching '*' found under directory 'docs/_build'\n==> default: Downloading/unpacking greenlet (from gevent==1.0.1->docker-registry)\n==> default:   Running setup.py (path:/tmp/pip_build_root/greenlet/setup.py) egg_info for package greenlet\n==> default:     \n==> default: Downloading/unpacking markupsafe (from Jinja2>=2.4->Flask==0.10.1->docker-registry)\n==> default:   Downloading MarkupSafe-0.23.tar.gz\n==> default:   Running setup.py (path:/tmp/pip_build_root/markupsafe/setup.py) egg_info for package markupsafe\n==> default:     \n==> default: Installing collected packages: docker-registry, docker-registry-core, backports.lzma, blinker, Flask, gevent, gunicorn, PyYAML, requests, M2Crypto, sqlalchemy, setuptools, boto, redis, simplejson, Werkzeug, Jinja2, itsdangerous, greenlet, markupsafe\n==> default:   Running setup.py install for docker-registry\n==> default:     Skipping installation of /usr/local/lib/python2.7/dist-packages/docker_registry/__init__.py (namespace package)\n==> default:     Skipping installation of /usr/local/lib/python2.7/dist-packages/docker_registry/drivers/__init__.py (namespace package)\n==> default:     \n==> default:     Installing /usr/local/lib/python2.7/dist-packages/docker_registry-0.9.1-nspkg.pth\n==> default:     Installing docker-registry script to /usr/local/bin\n==> default:   Running setup.py install for docker-registry-core\n==> default:     Skipping installation of /usr/local/lib/python2.7/dist-packages/docker_registry/__init__.py (namespace package)\n==> default:     Skipping installation of /usr/local/lib/python2.7/dist-packages/docker_registry/drivers/__init__.py (namespace package)\n==> default:     \n==> default:     Installing /usr/local/lib/python2.7/dist-packages/docker_registry_core-2.0.3-nspkg.pth\n==> default:   Running setup.py install for backports.lzma\n==> default:     This is backports.lzma version 0.0.3\n==> default:     building 'backports/lzma/_lzma' extension\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/root/include -I/opt/local/include -I/usr/local/include -I/usr/include/python2.7 -c backports/lzma/_lzmamodule.c -o build/temp.linux-x86_64-2.7/backports/lzma/_lzmamodule.o\n==> default:     x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/backports/lzma/_lzmamodule.o -L/root/lib -L/opt/local/lib -L/usr/local/lib -llzma -o build/lib.linux-x86_64-2.7/backports/lzma/_lzma.so\n==> default:     \n==> default:   Running setup.py install for blinker\n==> default:     \n==> default:   Running setup.py install for Flask\n==> default:     \n==> default:     warning: no files found matching '*' under directory 'tests'\n==> default:     warning: no previously-included files matching '*.pyc' found under directory 'docs'\n==> default:     warning: no previously-included files matching '*.pyo' found under directory 'docs'\n==> default:     warning: no previously-included files matching '*.pyc' found under directory 'tests'\n==> default:     warning: no previously-included files matching '*.pyo' found under directory 'tests'\n==> default:     warning: no previously-included files matching '*.pyc' found under directory 'examples'\n==> default:     warning: no previously-included files matching '*.pyo' found under directory 'examples'\n==> default:     no previously-included directories found matching 'docs/_build'\n==> default:     no previously-included directories found matching 'docs/_themes/.git'\n==> default:   Running setup.py install for gevent\n==> default:     Running '/bin/sh /tmp/pip_build_root/gevent/libev/configure > configure-output.txt' in /tmp/pip_build_root/gevent/build/temp.linux-x86_64-2.7/libev\n==> default:     building 'gevent.core' extension\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DLIBEV_EMBED=1 -DEV_COMMON= -DEV_CLEANUP_ENABLE=0 -DEV_EMBED_ENABLE=0 -DEV_PERIODIC_ENABLE=0 -Ibuild/temp.linux-x86_64-2.7/libev -Ilibev -I/usr/include/python2.7 -c gevent/gevent.core.c -o build/temp.linux-x86_64-2.7/gevent/gevent.core.o\n==> default:     In file included from gevent/libev.h:2:0,\n==> default:                      from gevent/gevent.core.c:313:\n==> default:     libev/ev.c:477:48: warning: \"/*\" within comment [-Wcomment]\n==> default:      /*#define MIN_INTERVAL  0.00000095367431640625 /* 1/2**20, good till 2200 */\n==> default:      ^\n==> default:     In file included from gevent/libev.h:2:0,\n==> default:                      from gevent/gevent.core.c:313:\n==> default:     libev/ev.c:1531:31: warning: \u2018ev_default_loop_ptr\u2019 initialized and declared \u2018extern\u2019 [enabled by default]\n==> default:        EV_API_DECL struct ev_loop *ev_default_loop_ptr = 0; /* needs to be initialised to make it a definition despite extern */\n==> default:                                    ^\n==> default:     In file included from gevent/libev.h:2:0,\n==> default:                      from gevent/gevent.core.c:313:\n==> default:     libev/ev.c: In function \u2018ev_io_start\u2019:\n==> default:     libev/ev.c:3554:34: warning: suggest parentheses around arithmetic in operand of \u2018|\u2019 [-Wparentheses]\n==> default:        fd_change (EV_A_ fd, w->events & EV__IOFDSET | EV_ANFD_REIFY);\n==> default:                                       ^\n==> default:     libev/ev.c: At top level:\n==> default:     libev/ev.c:4795:27: warning: \"/*\" within comment [-Wcomment]\n==> default:      /* EV_STAT     0x00001000 /* stat data changed */\n==> default:      ^\n==> default:     libev/ev.c:4796:27: warning: \"/*\" within comment [-Wcomment]\n==> default:      /* EV_EMBED    0x00010000 /* embedded event loop needs sweep */\n==> default:      ^\n==> default:     In file included from gevent/libev.h:2:0,\n==> default:                      from gevent/gevent.core.c:313:\n==> default:     libev/ev.c: In function \u2018evpipe_write\u2019:\n==> default:     libev/ev.c:2160:17: warning: ignoring return value of \u2018write\u2019, declared with attribute warn_unused_result [-Wunused-result]\n==> default:                write (evpipe [1], &counter, sizeof (uint64_t));\n==> default:                      ^\n==> default:     libev/ev.c:2172:17: warning: ignoring return value of \u2018write\u2019, declared with attribute warn_unused_result [-Wunused-result]\n==> default:                write (evpipe [1], &(evpipe [1]), 1);\n==> default:                      ^\n==> default:     libev/ev.c: In function \u2018pipecb\u2019:\n==> default:     libev/ev.c:2193:16: warning: ignoring return value of \u2018read\u2019, declared with attribute warn_unused_result [-Wunused-result]\n==> default:                read (evpipe [1], &counter, sizeof (uint64_t));\n==> default:                     ^\n==> default:     libev/ev.c:2207:16: warning: ignoring return value of \u2018read\u2019, declared with attribute warn_unused_result [-Wunused-result]\n==> default:                read (evpipe [0], &dummy, sizeof (dummy));\n==> default:                     ^\n==> default:     x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/gevent/gevent.core.o -o build/lib.linux-x86_64-2.7/gevent/core.so\n==> default:     Linking /tmp/pip_build_root/gevent/build/lib.linux-x86_64-2.7/gevent/core.so to /tmp/pip_build_root/gevent/gevent/core.so\n==> default:     Running '/bin/sh /tmp/pip_build_root/gevent/c-ares/configure CONFIG_COMMANDS= CONFIG_FILES= > configure-output.txt' in /tmp/pip_build_root/gevent/build/temp.linux-x86_64-2.7/c-ares\n==> default:     building 'gevent.ares' extension\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c gevent/gevent.ares.c -o build/temp.linux-x86_64-2.7/gevent/gevent.ares.o\n==> default:     In file included from gevent/gevent.ares.c:314:0:\n==> default:     gevent/dnshelper.c: In function \u2018gevent_append_addr\u2019:\n==> default:     gevent/dnshelper.c:51:5: warning: implicit declaration of function \u2018inet_ntop\u2019 [-Wimplicit-function-declaration]\n==> default:          if (ares_inet_ntop(family, src, tmpbuf, tmpsize)) {\n==> default:          ^\n==> default:     gevent/dnshelper.c: In function \u2018gevent_make_sockaddr\u2019:\n==> default:     gevent/dnshelper.c:137:5: warning: implicit declaration of function \u2018inet_pton\u2019 [-Wimplicit-function-declaration]\n==> default:          if ( ares_inet_pton(AF_INET, hostp, &((struct sockaddr_in*)sa6)->sin_addr.s_addr) > 0 ) {\n==> default:          ^\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares__close_sockets.c -o build/temp.linux-x86_64-2.7/c-ares/ares__close_sockets.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares__get_hostent.c -o build/temp.linux-x86_64-2.7/c-ares/ares__get_hostent.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares__read_line.c -o build/temp.linux-x86_64-2.7/c-ares/ares__read_line.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares__timeval.c -o build/temp.linux-x86_64-2.7/c-ares/ares__timeval.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_cancel.c -o build/temp.linux-x86_64-2.7/c-ares/ares_cancel.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_create_query.c -o build/temp.linux-x86_64-2.7/c-ares/ares_create_query.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_data.c -o build/temp.linux-x86_64-2.7/c-ares/ares_data.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_destroy.c -o build/temp.linux-x86_64-2.7/c-ares/ares_destroy.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_expand_name.c -o build/temp.linux-x86_64-2.7/c-ares/ares_expand_name.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_expand_string.c -o build/temp.linux-x86_64-2.7/c-ares/ares_expand_string.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_fds.c -o build/temp.linux-x86_64-2.7/c-ares/ares_fds.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_free_hostent.c -o build/temp.linux-x86_64-2.7/c-ares/ares_free_hostent.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_free_string.c -o build/temp.linux-x86_64-2.7/c-ares/ares_free_string.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_getenv.c -o build/temp.linux-x86_64-2.7/c-ares/ares_getenv.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_gethostbyaddr.c -o build/temp.linux-x86_64-2.7/c-ares/ares_gethostbyaddr.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_gethostbyname.c -o build/temp.linux-x86_64-2.7/c-ares/ares_gethostbyname.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_getnameinfo.c -o build/temp.linux-x86_64-2.7/c-ares/ares_getnameinfo.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_getopt.c -o build/temp.linux-x86_64-2.7/c-ares/ares_getopt.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_getsock.c -o build/temp.linux-x86_64-2.7/c-ares/ares_getsock.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_init.c -o build/temp.linux-x86_64-2.7/c-ares/ares_init.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_library_init.c -o build/temp.linux-x86_64-2.7/c-ares/ares_library_init.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_llist.c -o build/temp.linux-x86_64-2.7/c-ares/ares_llist.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_mkquery.c -o build/temp.linux-x86_64-2.7/c-ares/ares_mkquery.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_nowarn.c -o build/temp.linux-x86_64-2.7/c-ares/ares_nowarn.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_options.c -o build/temp.linux-x86_64-2.7/c-ares/ares_options.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_parse_a_reply.c -o build/temp.linux-x86_64-2.7/c-ares/ares_parse_a_reply.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_parse_aaaa_reply.c -o build/temp.linux-x86_64-2.7/c-ares/ares_parse_aaaa_reply.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_parse_mx_reply.c -o build/temp.linux-x86_64-2.7/c-ares/ares_parse_mx_reply.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_parse_naptr_reply.c -o build/temp.linux-x86_64-2.7/c-ares/ares_parse_naptr_reply.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_parse_ns_reply.c -o build/temp.linux-x86_64-2.7/c-ares/ares_parse_ns_reply.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_parse_ptr_reply.c -o build/temp.linux-x86_64-2.7/c-ares/ares_parse_ptr_reply.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_parse_soa_reply.c -o build/temp.linux-x86_64-2.7/c-ares/ares_parse_soa_reply.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_parse_srv_reply.c -o build/temp.linux-x86_64-2.7/c-ares/ares_parse_srv_reply.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_parse_txt_reply.c -o build/temp.linux-x86_64-2.7/c-ares/ares_parse_txt_reply.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_platform.c -o build/temp.linux-x86_64-2.7/c-ares/ares_platform.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_process.c -o build/temp.linux-x86_64-2.7/c-ares/ares_process.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_query.c -o build/temp.linux-x86_64-2.7/c-ares/ares_query.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_search.c -o build/temp.linux-x86_64-2.7/c-ares/ares_search.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_send.c -o build/temp.linux-x86_64-2.7/c-ares/ares_send.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_strcasecmp.c -o build/temp.linux-x86_64-2.7/c-ares/ares_strcasecmp.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_strdup.c -o build/temp.linux-x86_64-2.7/c-ares/ares_strdup.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_strerror.c -o build/temp.linux-x86_64-2.7/c-ares/ares_strerror.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_timeout.c -o build/temp.linux-x86_64-2.7/c-ares/ares_timeout.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_version.c -o build/temp.linux-x86_64-2.7/c-ares/ares_version.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/ares_writev.c -o build/temp.linux-x86_64-2.7/c-ares/ares_writev.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/bitncmp.c -o build/temp.linux-x86_64-2.7/c-ares/bitncmp.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/inet_net_pton.c -o build/temp.linux-x86_64-2.7/c-ares/inet_net_pton.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/inet_ntop.c -o build/temp.linux-x86_64-2.7/c-ares/inet_ntop.o\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -DHAVE_CONFIG_H= -DCARES_EMBED=1 -Ibuild/temp.linux-x86_64-2.7/c-ares -Ic-ares -I/usr/include/python2.7 -c c-ares/windows_port.c -o build/temp.linux-x86_64-2.7/c-ares/windows_port.o\n==> default:     x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/gevent/gevent.ares.o build/temp.linux-x86_64-2.7/c-ares/ares__close_sockets.o build/temp.linux-x86_64-2.7/c-ares/ares__get_hostent.o build/temp.linux-x86_64-2.7/c-ares/ares__read_line.o build/temp.linux-x86_64-2.7/c-ares/ares__timeval.o build/temp.linux-x86_64-2.7/c-ares/ares_cancel.o build/temp.linux-x86_64-2.7/c-ares/ares_create_query.o build/temp.linux-x86_64-2.7/c-ares/ares_data.o build/temp.linux-x86_64-2.7/c-ares/ares_destroy.o build/temp.linux-x86_64-2.7/c-ares/ares_expand_name.o build/temp.linux-x86_64-2.7/c-ares/ares_expand_string.o build/temp.linux-x86_64-2.7/c-ares/ares_fds.o build/temp.linux-x86_64-2.7/c-ares/ares_free_hostent.o build/temp.linux-x86_64-2.7/c-ares/ares_free_string.o build/temp.linux-x86_64-2.7/c-ares/ares_getenv.o build/temp.linux-x86_64-2.7/c-ares/ares_gethostbyaddr.o build/temp.linux-x86_64-2.7/c-ares/ares_gethostbyname.o build/temp.linux-x86_64-2.7/c-ares/ares_getnameinfo.o build/temp.linux-x86_64-2.7/c-ares/ares_getopt.o build/temp.linux-x86_64-2.7/c-ares/ares_getsock.o build/temp.linux-x86_64-2.7/c-ares/ares_init.o build/temp.linux-x86_64-2.7/c-ares/ares_library_init.o build/temp.linux-x86_64-2.7/c-ares/ares_llist.o build/temp.linux-x86_64-2.7/c-ares/ares_mkquery.o build/temp.linux-x86_64-2.7/c-ares/ares_nowarn.o build/temp.linux-x86_64-2.7/c-ares/ares_options.o build/temp.linux-x86_64-2.7/c-ares/ares_parse_a_reply.o build/temp.linux-x86_64-2.7/c-ares/ares_parse_aaaa_reply.o build/temp.linux-x86_64-2.7/c-ares/ares_parse_mx_reply.o build/temp.linux-x86_64-2.7/c-ares/ares_parse_naptr_reply.o build/temp.linux-x86_64-2.7/c-ares/ares_parse_ns_reply.o build/temp.linux-x86_64-2.7/c-ares/ares_parse_ptr_reply.o build/temp.linux-x86_64-2.7/c-ares/ares_parse_soa_reply.o build/temp.linux-x86_64-2.7/c-ares/ares_parse_srv_reply.o build/temp.linux-x86_64-2.7/c-ares/ares_parse_txt_reply.o build/temp.linux-x86_64-2.7/c-ares/ares_platform.o build/temp.linux-x86_64-2.7/c-ares/ares_process.o build/temp.linux-x86_64-2.7/c-ares/ares_query.o build/temp.linux-x86_64-2.7/c-ares/ares_search.o build/temp.linux-x86_64-2.7/c-ares/ares_send.o build/temp.linux-x86_64-2.7/c-ares/ares_strcasecmp.o build/temp.linux-x86_64-2.7/c-ares/ares_strdup.o build/temp.linux-x86_64-2.7/c-ares/ares_strerror.o build/temp.linux-x86_64-2.7/c-ares/ares_timeout.o build/temp.linux-x86_64-2.7/c-ares/ares_version.o build/temp.linux-x86_64-2.7/c-ares/ares_writev.o build/temp.linux-x86_64-2.7/c-ares/bitncmp.o build/temp.linux-x86_64-2.7/c-ares/inet_net_pton.o build/temp.linux-x86_64-2.7/c-ares/inet_ntop.o build/temp.linux-x86_64-2.7/c-ares/windows_port.o -lrt -o build/lib.linux-x86_64-2.7/gevent/ares.so\n==> default:     Linking /tmp/pip_build_root/gevent/build/lib.linux-x86_64-2.7/gevent/ares.so to /tmp/pip_build_root/gevent/gevent/ares.so\n==> default:     building 'gevent._semaphore' extension\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/usr/include/python2.7 -c gevent/gevent._semaphore.c -o build/temp.linux-x86_64-2.7/gevent/gevent._semaphore.o\n==> default:     x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/gevent/gevent._semaphore.o -o build/lib.linux-x86_64-2.7/gevent/_semaphore.so\n==> default:     Linking /tmp/pip_build_root/gevent/build/lib.linux-x86_64-2.7/gevent/_semaphore.so to /tmp/pip_build_root/gevent/gevent/_semaphore.so\n==> default:     building 'gevent._util' extension\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/usr/include/python2.7 -c gevent/gevent._util.c -o build/temp.linux-x86_64-2.7/gevent/gevent._util.o\n==> default:     x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -D_FORTIFY_SOURCE=2 -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/gevent/gevent._util.o -o build/lib.linux-x86_64-2.7/gevent/_util.so\n==> default:     Linking /tmp/pip_build_root/gevent/build/lib.linux-x86_64-2.7/gevent/_util.so to /tmp/pip_build_root/gevent/gevent/_util.so\n==> default:     \n==> default: Compiling /tmp/pip_build_root/gunicorn/gunicorn/workers/_gaiohttp.py ...\n==> default:   File \"/tmp/pip_build_root/gunicorn/gunicorn/workers/_gaiohttp.py\", line 64\n==> default:     yield from self.wsgi.close()\n==> default:              ^\n==> default: SyntaxError: invalid syntax\n==> default: \n==> default:   Found existing installation: PyYAML 3.10\n==> default:     Uninstalling PyYAML:\n==> default:       Successfully uninstalled PyYAML\n==> default:   Running setup.py install for PyYAML\n==> default:     checking if libyaml is compilable\n==> default:     x86_64-linux-gnu-gcc -pthread -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fPIC -I/usr/include/python2.7 -c build/temp.linux-x86_64-2.7/check_libyaml.c -o build/temp.linux-x86_64-2.7/check_libyaml.o\n==> default:     build/temp.linux-x86_64-2.7/check_libyaml.c:2:18: fatal error: yaml.h: No such file or directory\n==> default:      #include <yaml.h>\n==> default:                       ^\n==> default:     compilation terminated.\n==> default:     \n==> default:     libyaml is not found or a compiler error: forcing --without-libyaml\n==> default:     (if libyaml is installed correctly, you may need to\n==> default:      specify the option --include-dirs or uncomment and\n==> default:      modify the parameter include_dirs in setup.cfg)\n==> default:     \n==> default:   Found existing installation: requests 2.2.1\n==> default:     Uninstalling requests:\n==> default:       Successfully uninstalled requests\n==> default:   Running setup.py install for M2Crypto\n==> default:     building 'M2Crypto.__m2crypto' extension\n==> default:     swigging SWIG/_m2crypto.i to SWIG/_m2crypto_wrap.c\n==> default:     swig -python -I/usr/include/python2.7 -I/usr/include/x86_64-linux-gnu -I/usr/include -I/usr/include/openssl -includeall -modern -o SWIG/_m2crypto_wrap.c SWIG/_m2crypto.i\n==> default:     unable to execute 'swig': No such file or directory\n==> default:     error: command 'swig' failed with exit status 1\n==> default:     Complete output from command /usr/bin/python -c \"import setuptools, tokenize;__file__='/tmp/pip_build_root/M2Crypto/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-ofCXHv-record/install-record.txt --single-version-externally-managed --compile:\n==> default:     running install\n==> default: \n==> default: running build\n==> default: \n==> default: running build_py\n==> default: \n==> default: creating build/lib.linux-x86_64-2.7\n==> default: \n==> default: creating build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/ftpslib.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/SMIME.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/BIO.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/RC4.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/Err.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/X509.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/DH.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/callback.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/threading.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/DSA.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/Engine.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/m2urllib2.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/m2xmlrpclib.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/AuthCookie.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/m2urllib.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/httpslib.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/RSA.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/m2.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/EC.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/ASN1.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/Rand.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/BN.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/__init__.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/EVP.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: copying M2Crypto/util.py -> build/lib.linux-x86_64-2.7/M2Crypto\n==> default: \n==> default: creating build/lib.linux-x86_64-2.7/M2Crypto/SSL\n==> default: \n==> default: copying M2Crypto/SSL/Context.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL\n==> default: \n==> default: copying M2Crypto/SSL/timeout.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL\n==> default: \n==> default: copying M2Crypto/SSL/Session.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL\n==> default: \n==> default: copying M2Crypto/SSL/SSLServer.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL\n==> default: \n==> default: copying M2Crypto/SSL/cb.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL\n==> default: \n==> default: copying M2Crypto/SSL/Cipher.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL\n==> default: \n==> default: copying M2Crypto/SSL/ssl_dispatcher.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL\n==> default: \n==> default: copying M2Crypto/SSL/Connection.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL\n==> default: \n==> default: copying M2Crypto/SSL/__init__.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL\n==> default: \n==> default: copying M2Crypto/SSL/TwistedProtocolWrapper.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL\n==> default: \n==> default: copying M2Crypto/SSL/Checker.py -> build/lib.linux-x86_64-2.7/M2Crypto/SSL\n==> default: \n==> default: running build_ext\n==> default: \n==> default: building 'M2Crypto.__m2crypto' extension\n==> default: \n==> default: swigging SWIG/_m2crypto.i to SWIG/_m2crypto_wrap.c\n==> default: \n==> default: swig -python -I/usr/include/python2.7 -I/usr/include/x86_64-linux-gnu -I/usr/include -I/usr/include/openssl -includeall -modern -o SWIG/_m2crypto_wrap.c SWIG/_m2crypto.i\n==> default: \n==> default: unable to execute 'swig': No such file or directory\n==> default: \n==> default: error: command 'swig' failed with exit status 1\n==> default: \n==> default: ----------------------------------------\n==> default: Cleaning up...\n==> default: Command /usr/bin/python -c \"import setuptools, tokenize;__file__='/tmp/pip_build_root/M2Crypto/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\\r\\n', '\\n'), __file__, 'exec'))\" install --record /tmp/pip-ofCXHv-record/install-record.txt --single-version-externally-managed --compile failed with error code 1 in /tmp/pip_build_root/M2Crypto\n==> default: Traceback (most recent call last):\n==> default:   File \"/usr/bin/pip\", line 9, in <module>\n==> default:     load_entry_point('pip==1.5.4', 'console_scripts', 'pip')()\n==> default:   File \"/usr/lib/python2.7/dist-packages/pip/__init__.py\", line 185, in main\n==> default:     return command.main(cmd_args)\n==> default:   File \"/usr/lib/python2.7/dist-packages/pip/basecommand.py\", line 161, in main\n==> default:     text = '\\n'.join(complete_log)\n==> default: UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 33: ordinal not in range(128)\n==> default: docker-registry start/running, process 3959\nAfter this, I did this from the vagrant VM\n$ sudo service docker-registry status\ndocker-registry stop/waiting\n$ sudo service docker-registry start\ndocker-registry start/running, process 8244\n$ sudo service docker-registry status\ndocker-registry stop/waiting\n. Thank you. I didn't think to look in the travis.yml file. I was missing swig and libssl-dev.\n. ",
    "jiasir": "OK, Thanks! @dmp42\n. ",
    "teodorescuserban": "thank you, @dmcgowan \n. ",
    "ghaering": "Yes, boto requires this. Quoting straight from the boto exception:\nBoto depends on the python rsa library to generate signed URLs for \"CloudFront\"\nAnd I'm using the cloudfronts3 configuration of docker-registry. See \"S3 Storage with Cloudfront\" in ADVANCED.MD.\n. ",
    "HakShak": "Until we get search for registry v2.0, we got around this by extending the image (magic sauce is the WORKDIR):\n```\nFROM registry:0.9.1\nhttps://github.com/docker/docker-registry/issues/892\nENV GUNICORN_OPTS [\"--preload\"]\nRUN adduser --system registry \\\n&& addgroup registry \\\n&& chown -R registry:registry /docker-registry\nUSER registry\nhttps://github.com/docker/docker-registry/issues/915\nWORKDIR /docker-registry\nCMD [\"docker-registry\"]\n```\n. ",
    "mattgiles": "Hi, what are you using to proxy the registry?  I was having similar problems with large images, and had to enable chunked transfer encoding on the proxy (in my case, nginx).  \nSee: https://github.com/docker/docker-registry/blob/master/ADVANCED.md#nginx\n. ",
    "bramswenson": "I'm seeing this issue on Archlinux 2016.01.01 through a newish Linksys router and Comcast cable modem. I'm pushing from my local docker instance to an AWS Elastic Container Repository. Essentially push is consuming every ounce of available upload and bringing the rest of my network to its knees. I've attempted to limit the bandwidth use via tickle to no avail. It would be great if there was a command line switch to control the max amount of up or down stream bandwidth to use. All this to say that it might be the docker push/pull commands that need the attention.\n. ",
    "harrisonfeng": "when search_backend sets to sqlalchemy, first start the container, then the container will stop automatically as that error happened. Actually, that error will happen every time.  So I cannot run \ndocker registry container at all. With my fix, I can run it happily.\n. ",
    "pires": "@dmp42 why is lockfile a better solution?\n. Got it, thanks ;-)\n. ",
    "docwhat": "I'm not sure what @dmp42 is thinking, but as a general rule -- if you can let someone else do the tricky bits, the better. And lockfiles are notoriously tricky.\n. ",
    "pwais": "+1 if this change reduces crashes at least in the interim.  Currently docker run -p 5000:5000 registry is broken out-of-the-box (try running it several times).\n. ",
    "hans-d": "+1. First quick fix the broken part (done with this PR), then in a separate PR improve locking\n. ",
    "coveralls": "\nChanges Unknown when pulling e682fda3c386d7cd87714d076e344b309c54e751 on guoxiuyan:Fix_Token_True into * on docker:master*.\n. ",
    "Caligone": "Same issue here with Docker version 1.4.1, build 5bc2ff8 on an Archlinux\n. ",
    "Aayush-Kasurde": "I think this issue is addressed with PR #927 \n@dmp42 Could you please mark this issue as closed ?\n. ",
    "stream1984": "i have found problem that bucket name can not include slash\nmybucket instead of /mybucket \n. ",
    "mko-x": "+1\n. ",
    "l1x": "Same here.\n. I guess one scenario is using corporate LDAP with Docker. I need to disable all the authentication in Docker, and let all the operations fall through and use a different layer that deals with AAA. I had a little fun with that but setting --insecure-registry on ALL nodes solved the problem. This guy wanted something similar:\nhttp://stackoverflow.com/questions/29172678/connecting-docker-container-to-corporate-ldap-server-through-ssl\n. ",
    "ForbiddenEra": "Hi,\nI am on Docker 1.5 running on CoreOS -- and I am NOT experiencing this problem, another one, yes, but not this one.\nI am launching docker with:\nExecStart=/usr/lib/coreos/dockerd --daemon --insecure-registry=10.254.0.100:5000 --host=fd:// $DOCKER_OPTS $DOCKER_OPT_BIP $DOCKER_OPT_MTU $DOCKER_OPT_IPMASQ\n(no http://) and I am able to connect to my registry without any certificate errors - perhaps the fact I'm using the = sign changes something..? Haven't reviewed the code to know.\n. With nginx handling http_auth, I can login:\ncore@core0 ~ $ docker login 10.254.0.100:5000\nUsername (testing): shaped\nPassword: \nEmail (test@shaped.ca): \nLogin Succeeded\nbut I can't push an image:\ncore@core0 ~ $ docker tag shaped/haproxy0 10.254.0.100:5000/haproxy0       \ncore@core0 ~ $ docker push 10.254.0.100:5000/haproxy0 \nThe push refers to a repository [10.254.0.100:5000/haproxy0] (len: 1)\nSending image list\nPushing repository 10.254.0.100:5000/haproxy0 (1 tags)\n511136ea3c5a: Pushing \nFATA[0000] HTTP code 401, Docker will not send auth headers over HTTP.\nWhich might sort of vaguely relate to #936 and referenced in #541 ..\nSo really - docs are not clear!!\n. Implementing SSL leaves me about here:\nI've added my CA to the machine and run update-ca-certificates\nThen:\ncore@core0 /etc/ssl/certs $ docker login docker-registry:5000\nUsername: shaped\nPassword: \nEmail: \nFATA[0003] Error response from daemon: v1 ping attempt failed with error: Get https://docker-registry:5000/v1/_ping: x509: certificate signed by unknown authority. If this private registry supports only HTTP or HTTPS with an unknown CA certificate, please add `--insecure-registry docker-registry:5000` to the daemon's arguments. In the case of HTTPS, if you have access to the registry's CA certificate, no need for the flag; simply place the CA certificate at /etc/docker/certs.d/docker-registry:5000/ca.crt\nSo, I take it's advice:\ncore@core0 /etc $ docker login docker-registry:5000\nUsername: shaped\nPassword: \nEmail: \nFATA[0002] Error response from daemon: Server Error: Post https://docker-registry:5000/v1/users/: x509: certificate signed by unknown authority\n....hmm\n. I was able to get it to work but only by adding --insecure-registry=\"docker-registry:5000\" to my docker start up line:\ncore@core0 /etc $ docker login docker-registry:5000\nUsername: shaped\nPassword: \nEmail: \nLogin Succeeded\ncore@core0 /etc $ docker tag shaped/haproxy0 docker-registry:5000/haproxy0\ncore@core0 /etc $ docker push docker-registry:5000/haproxy0 \nThe push refers to a repository [docker-registry:5000/haproxy0] (len: 1)\nSending image list\nPushing repository docker-registry:5000/haproxy0 (1 tags)\n511136ea3c5a: Image successfully pushed \n53f858aaaf03: Image successfully pushed \n837339b91538: Image successfully pushed \n615c102e2290: Image successfully pushed \nb39b81afc8ca: Image successfully pushed \n8254ff58b098: Image successfully pushed \nec5f59360a64: Image successfully pushed \n2ce4ac388730: Image successfully pushed \n2eccda511755: Image successfully pushed \n5a14c1498ff4: Image successfully pushed \n8ffd698b4b9a: Image successfully pushed \nc9950e27e2bf: Image successfully pushed \nf5489e95a03b: Image successfully pushed \n13e9704168f6: Image successfully pushed \nd329e079a86b: Image successfully pushed \n9675842043c7: Image successfully pushed \n949a55b1c715: Image successfully pushed \n9205a67b7f7d: Image successfully pushed \n70bee8e8629f: Image successfully pushed \n78934e85029e: Image successfully pushed \nPushing tag for rev [78934e85029e] on {https://docker-registry:5000/v1/repositories/haproxy0/tags/latest}\nMy cert generation:\n```\nroot@registry-gateway:~# openssl genrsa -out devdockerCA.key 2048\nGenerating RSA private key, 2048 bit long modulus\n.......+++\n.....................................................................................................................................................+++\ne is 65537 (0x10001)\nroot@registry-gateway:~# openssl req -x509 -new -nodes -key devdockerCA.key -days 10000 -out devdockerCA.crt\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n\nCountry Name (2 letter code) [AU]:CA\nState or Province Name (full name) [Some-State]:AB\nLocality Name (eg, city) []:Calgary\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Shaped\nOrganizational Unit Name (eg, section) []:\nCommon Name (e.g. server FQDN or YOUR name) []:shaped.ca\nEmail Address []:jai@shaped.ca\nroot@registry-gateway:~# openssl genrsa -out docker-registry.key 2048\nGenerating RSA private key, 2048 bit long modulus\n........+++\n.............+++\ne is 65537 (0x10001)\nroot@registry-gateway:~# openssl req -new -key docker-registry.key -out docker-registry.csr\nYou are about to be asked to enter information that will be incorporated\ninto your certificate request.\nWhat you are about to enter is what is called a Distinguished Name or a DN.\nThere are quite a few fields but you can leave some blank\nFor some fields there will be a default value,\nIf you enter '.', the field will be left blank.\n\nCountry Name (2 letter code) [AU]:CA\nState or Province Name (full name) [Some-State]:AB\nLocality Name (eg, city) []:Calgary\nOrganization Name (eg, company) [Internet Widgits Pty Ltd]:Shaped\nOrganizational Unit Name (eg, section) []:\nCommon Name (e.g. server FQDN or YOUR name) []:docker-registry\nEmail Address []:jai@shaped.ca\nPlease enter the following 'extra' attributes\nto be sent with your certificate request\nA challenge password []:\nAn optional company name []:\nroot@registry-gateway:~# openssl x509 -req -in docker-registry.csr -CA devdockerCA.crt -CAkey devdockerCA.key -CAcreateserial -out docker-registry.crt -days 10000\nSignature ok\nsubject=/C=CA/ST=AB/L=Calgary/O=Shaped/CN=docker-registry/emailAddress=jai@shaped.ca\nGetting CA Private Key\nroot@registry-gateway:~#\n```\nadded devdockerCA.crt as /etc/ssl/certs/docker-dev-crt.pem and ran update-ca-certs which found it and added it and also to /etc/docker/certs.d/docker-registry:5000/ca.crt\nThe FAQ/docs should really say that an auth proxy & ssl is required even to be able to push/pull an image from a private registry - and there's NO other way..?\nStill would rather have it working properly without --insecure-registry ... ideas?\n. Hi,\nAs I said above, running the basic registry directly from the repo would not give me anything usable. I am 100% not talking to another service - you can see clearly that when I access the repo (with no nginx in front) that the log for the registry. The only 401 I got was docker refusing to send auth headers over http - weird that it's a 401 and not a docker error? I thought that was part of the docker daemon.\nAs for adding SSL - I have no problem with that except docker isn't recognizing my CA - I have placed it in the correct location and it does pick it up (see logs) but still gives error.\nHowever I will, for the sake of completion, start over..\nAnd that makes me wonder -- I think maybe I forgot to tag my image for the repo before I pushed it the first time (resulting in that error?) because - now it's working without nginx/ssl / out of the box...?\nI swear, I try to test anything and everything before opening an issue on github - and most of the time I finally get to that point, I find out I missed something small...? Thanks for your feedback though.\ncore@core0 ~ $ docker tag shaped/haproxy0 107.191.40.91:5000/haproxy0 \ncore@core0 ~ $ docker push 107.191.40.91:5000/haproxy0\nThe push refers to a repository [107.191.40.91:5000/haproxy0] (len: 1)\nSending image list\nPushing repository 107.191.40.91:5000/haproxy0 (1 tags)\n511136ea3c5a: Image successfully pushed \n53f858aaaf03: Image successfully pushed \n837339b91538: Image successfully pushed \n615c102e2290: Image successfully pushed \nb39b81afc8ca: Image successfully pushed \n8254ff58b098: Image successfully pushed \nec5f59360a64: Image successfully pushed \n2ce4ac388730: Image successfully pushed \n2eccda511755: Image successfully pushed \n5a14c1498ff4: Image successfully pushed \n8ffd698b4b9a: Image successfully pushed \nc9950e27e2bf: Image successfully pushed \nf5489e95a03b: Image successfully pushed \n13e9704168f6: Image successfully pushed \nd329e079a86b: Image successfully pushed \n9675842043c7: Image successfully pushed \n949a55b1c715: Image successfully pushed \n9205a67b7f7d: Image successfully pushed \n70bee8e8629f: Image successfully pushed \n78934e85029e: Image successfully pushed \nPushing tag for rev [78934e85029e] on {http://107.191.40.91:5000/v1/repositories/haproxy0/tags/latest}\nStill odd I got an incorrect username/password error..? Not a \"could not find image\" error for the un-tagged image?\n```\nFATA[0004] could not find image: no such id: 107.191.40.91:5000/shaped/ubuntu-base \ncore@core0 ~ $ docker push shaped/haproxy\nThe push refers to a repository [shaped/haproxy] (len: 1)\nSending image list\nPlease login prior to push:\nUsername: \nFATA[0001] Error response from daemon: Registration: \"Missing username field\" \ncore@core0 ~ $ \ncore@core0 ~ $ docker push shaped/haproxy\nThe push refers to a repository [shaped/haproxy] (len: 1)\nSending image list\nPlease login prior to push:\nUsername: shaped\nPassword: \nEmail: shaped\nFATA[0002] Error response from daemon: Registration: \"Wrong email format (it has to match \\\"[^@]+@[^@]+\\.[^@]+\\\")\" \ncore@core0 ~ $ docker push shaped/haproxy\nThe push refers to a repository [shaped/haproxy] (len: 1)\nSending image list\nPlease login prior to push:\nUsername: shaped\nPassword: \nEmail: shaped@shaped.ca\nFATA[0004] Error response from daemon: Wrong login/password, please try again \n```\nSo in the end - it does work - however there's still the issue of Documentation, misleading outputs & error messages.\n. I also should say I still can't drop the --insecure-registry even though I've added my CA to the host & to docker.. I showed how I generated above as well, any ideas?\n. If I specify a port, docker uses https (whether that port is say, 5000 or 443)\nIf I don't docker defaults to http and port 80 - though it SEEMS to work if it finds SSL at 80 instead?\n. Yeah, I can login but not push when ssl is on 80.. have to specify port manually. :(\n. I may have not restarted my docker daemon after giving it the cert as the ssl is working properly now.\nI hope, while not actually a real issue, this helps someone in the future setting up their registry.\n. @danielschwartz\u00a0My issue was not restarting the Docker daemon after providing the cert in /etc/docker/certs.d/...\nAlso, make sure that you're specifying the protocol & port.\nI had issues not specifying the port specifically. \n-------- Original message --------\nFrom: Olivier Gambier notifications@github.com \nDate: 02-27-2015  2:36 PM  (GMT-07:00) \nTo: docker/docker-registry docker-registry@noreply.github.com \nCc: Jai Boudreau jason@shaped.ca \nSubject: Re: [docker-registry] Following instructions in readme.md for\n  test/dev/default registry not working (#945) \n@danielschwartz please (with SSL enabled) curl -iv https://yourregistry:port/v1/_ping and:\ncheck if curl is happy\ncopy the resulting output\n\u2014\nReply to this email directly or view it on GitHub.\n. Also,\nEDIT: Apparently, I started typing something here..and I don't remember what. Sorry. Did you ever get it working @danielschwartz \n. ",
    "cgp": "Same issue here.\n. ",
    "shawn-sterling": "I have the same issue, if I have the extra arguments:\n--insecure-registry hostname.goes.here:5000\n--insecure-registry http://hostname.goes.here:5000\nit fails. If I change to:\n--insecure-registry=hostname.goes.here:5000\nIt works.\n. ",
    "pilerou": "Same problem here on my boot2docker after changing /var/lib/boot2docker/profile\ni can login... but I can't push and docker.log prints :\n\ntime=\"2015-04-09T21:07:02Z\" level=\"debug\" msg=\"Calling POST /images/{name:.*}/push\"\ntime=\"2015-04-09T21:07:02Z\" level=\"info\" msg=\"POST /v1.17/images/:5000/helloworld/push?tag=\"\ntime=\"2015-04-09T21:07:02Z\" level=\"info\" msg=\"+job push(:5000/helloworld)\"\ntime=\"2015-04-09T21:07:02Z\" level=\"info\" msg=\"+job resolve_repository(:5000/helloworld)\"\ntime=\"2015-04-09T21:07:02Z\" level=\"info\" msg=\"-job resolve_repository(:5000/helloworld) = OK (0)\"\ntime=\"2015-04-09T21:07:02Z\" level=\"debug\" msg=\"pinging registry endpoint https://:5000/v0/\"\ntime=\"2015-04-09T21:07:02Z\" level=\"debug\" msg=\"attempting v2 ping for registry endpoint https://:5000/v2/\"\ntime=\"2015-04-09T21:07:02Z\" level=\"debug\" msg=\"hostDir: /etc/docker/certs.d/:5000\"\ntime=\"2015-04-09T21:07:02Z\" level=\"debug\" msg=\"attempting v1 ping for registry endpoint https://:5000/v1/\"\ntime=\"2015-04-09T21:07:02Z\" level=\"debug\" msg=\"hostDir: /etc/docker/certs.d/:5000\"\ninvalid registry endpoint https://:5000/v0/: unable to ping registry endpoint https://:5000/v0/\nv2 ping attempt failed with error: Get https://:5000/v2/: tls: oversized record received with length 20527\n v1 ping attempt failed with error: Get https://:5000/v1/_ping: tls: oversized record received with length 20527. If this private registry supports only HTTP or HTTPS with an unknown CA certificate, please add `--insecure-registime=\"2015-04-09T21:07:02Z\" level=\"info\" msg=\"-job push(:5000/helloworld) = ERR (1)\"\ntime=\"2015-04-09T21:07:02Z\" level=\"error\" msg=\"Handler for POST /images/{name:.*}/push returned error:  v1 ping attempt failed with error: Get https://:5000/v1/_ping: tls: oversized record received with length 20527. If this prtime=\"2015-04-09T21:07:02Z\" level=\"error\" msg=\"HTTP Error: statusCode=500  v1 ping attempt failed with error: Get https://:5000/v1/_ping: tls: oversized record received with length 20527. \n\nIt really seems that docker daemon doesn't consider http protocol on push command.\nLog files doesn't print any attempt to connect to server using http. All tries are on https.\n...\n. ",
    "jdoliner": "I'm running in to this issue as well. Is it necessary that --insecure-registry be passed to the daemon at start time? Or can you pass it to the command itself? For example:\nshell\ndocker --insecure-registry registry.host:5000 pull registry.host:5000/image\nI can't get the above command to work. Any ideas?\n. ",
    "shakthimaan": "On Ubuntu 14.10, I used \"... --insecure-registry 192.168.100.1:5000\" and it worked.\nOn Ubuntu 14.04.2 LTS (trusty) using DOCKER_OPTS=\"$DOCKER_OPTS --insecure-registry=192.168.100.1:5000\" solved the problem.\nDocker version and build are the same:\nbash\n    $ docker -v\n    Docker version 1.6.0, build 4749651\n. ",
    "shikhachauhan": "This information was helpful with same issue in Windows boot2docker.\n. ",
    "Tallisado": "debian with docker 1.6.2 -- working for me is -->\nBOTH server hosting registry and node trying to push to it:\nDOCKER_OPTS=\"--insecure-registry=192.168.122.92:5000\"\n. ",
    "kvashishta": "[root@docker ~]# rpm -qa | grep docker\ndocker-1.7.1-115.el7.x86_64\n[root@docker ~]# docker --insecure-registry=ldap.kartikv.com.com:5001 login https://ldap.kartikv.com:5001\nUsername: testuser\nPassword:\nEmail:\nError response from daemon: invalid registry endpoint https://ldap.kartikv.com:5001/v0/: unable to ping registry endpoint https://ldap.kartikv.com:5001/v0/\nv2 ping attempt failed with error: Get https://ldap.kartikv.com:5001/v2/: dial tcp 192.168.1.195:5001: connection refused\n v1 ping attempt failed with error: Get https://ldap.kartikv.com:5001/v1/_ping: dial tcp 192.168.1.195:5001: connection refused. If this private registry supports only HTTP or HTTPS with an unknown CA certificate, please add --insecure-registry ldap.kartikv.com:5001 to the daemon's arguments. In the case of HTTPS, if you have access to the registry's CA certificate, no need for the flag; simply place the CA certificate at /etc/docker/certs.d/ldap.kartikv.com:5001/ca.crt\nPlease help\n. That worked! Thank you for your invaluable advise. \nIs there a way to get a web page describing all the images in the private registry?\nI followed this to create the private registry:\nhttps://www.digitalocean.com/community/tutorials/how-to-set-up-a-private-docker-registry-on-ubuntu-14-04\n. Here are some instructions to install docker private registry in a\ncontainer:\n[root@docker registry]# cat >/etc/yum.repos.d/docker.repo <<-EOF\n[dockerrepo]\nname=Docker Repository\nbaseurl=https://yum.dockerproject.org/repo/main/centos/7\nenabled=1\ngpgcheck=1\ngpgkey=https://yum.dockerproject.org/gpg\nEOF\nyum install docker-engine\nsystemctl enable docker.service\nsystemctl start docker.service\ndocker run -d -p 5000:5000 --name kartiksregistry registry:2\ndocker ps\ndocker pull busybox\ndocker images\ndocker tag busybox localhost:5000/kartiksbusybox\ndocker images\ndocker push localhost:5000/kartiksbusybox\ndocker ps\ndocker stop kartiksregistry && docker rm kartiksregistry\n[root@docker ~]# cat /etc/sysconfig/docker\n/etc/sysconfig/docker\n\nOther arguments to pass to the docker daemon process\nThese will be parsed by the sysv initscript and appended\nto the arguments list passed to docker daemon\nother_args=\"\"\nDOCKER_OPTS=\"--insecure-registry=localhost:5000\"\n[root@docker ~]#\nsystemctl restart docker\ndocker images\nmkdir /root/registry\ncd /root/registry\n[root@docker registry]# cat docker-registry.sh\n!/bin/bash\ndocker run -d -p 5000:5000 \\\n    -v /registry:/var/lib/registry \\\n    --restart=on-failure \\\n    --name docker-registry-v2 \\\n    registry:2\n[root@docker registry]#\n[root@docker registry]# cat docker-registry-web.sh\n!/bin/bash\ndocker run -d -p 8080:8080 \\\n    -e REGISTRY_HOST=172.17.42.1 \\\n    -e REGISTRY_PORT=5000 \\\n    -e REGISTRY_AUTH=\"ZkpyOTVLZmhDaQ==\" \\\n    --restart=on-failure \\\n    --name docker-registry-web \\\n    hyper/docker-registry-web\n[root@docker registry]#\nanother method using docker compose\nyum install epel-release\nyum install -y python-pip\npip install -U docker-compose\ndocker stop $(docker ps -q)\ndocker rm $(docker ps -aq)\n[root@docker registry]# cat docker-registry.yml\nregistry:\n  restart: always\n  image: registry:2\n  ports:\n    - 5000:5000\n  environment:\n    REGISTRY_STORAGE_FILESYSTEM_ROOTDIRECTORY: /var/lib/registry\n  volumes:\n    - /registry:/var/lib/registry\n  container_name:\n    docker-registry-v2\nweb:\n  restart: always\n  image: hyper/docker-registry-web\n  ports:\n    - 8080:8080\n  links:\n    - registry\n  environment:\n    REGISTRY_HOST: docker-registry-v2\n  container_name:\n    docker-registry-web\n[root@docker registry]#\ndocker-compose -f docker-registry.yml up -d\nOn Wed, Nov 4, 2015 at 4:12 PM, Shriram Sharma notifications@github.com\nwrote:\n\nI am facing the same issue as @pilerou https://github.com/pilerou . Is\nthere a fix?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/936#issuecomment-153885621\n.\n. \n",
    "shriramsharma": "I am facing the same issue as @pilerou . Is there a fix?\n. ",
    "GittyHup": "Well I was trying to learn the registry, its easier without the docker container overhead, plus I used all the installation steps from the dockerfile anyway. But my question is where does this command \"docker-registry\" come from. Because when I run it, it tells me to set the env variables.\n```\nubuntu@ip-10-232-1-123:/$ docker-registry -h\nusage: docker-registry [-h]\nrun the docker-registry with gunicorn, honoring the following\nenvironment variables:\nREGISTRY_HOST: TCP host or ip to bind to; default is 0.0.0.0\nREGISTRY_PORT: TCP port to bind to; default is 5000\nGUNICORN_WORKERS: number of worker processes gunicorn should start\nGUNICORN_GRACEFUL_TIMEOUT: timeout in seconds for graceful worker restart\nGUNICORN_SILENT_TIMEOUT: timeout in seconds for restarting silent workers\nGUNICORN_USER: unix user to downgrade priviledges to\nGUNICORN_GROUP: unix group to downgrade priviledges to\nGUNICORN_ACCESS_LOG_FILE: File to log access to\nGUNICORN_ERROR_LOG_FILE: File to log errors to\nGUNICORN_OPTS: extra options to pass to gunicorn\noptional arguments:\n  -h, --help  show this help message and exit\n```\nAnd here are my env variables.\n$ env\nGUNICORN_OPTS=[--preload]\nXDG_SESSION_ID=1\nTERM=xterm\nSHELL=/bin/bash\nDOCKER_REGISTRY_CONFIG=/usr/local/lib/python2.7/dist-packages/config/config.yml\nSSH_CLIENT=10.234.68.88 1711 22\nSSH_TTY=/dev/pts/0\nAWS_BUCKET=itmcc-docker-registry-backend\nUSER=ubuntu\nGUNICORN_ACCESS_LOG_FILE=/var/log/docker-registry/access.log\nLS_COLORS=...\nMAIL=/var/mail/ubuntu\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\nAWS_KEY=******************\nPWD=/\nLANG=en_US.UTF-8\nGUNICORN_ERROR_LOG_FILE=/var/log/docker-registry/server.log\nSHLVL=1\nHOME=/home/ubuntu\nSETTINGS_FLAVOR=prod\nLOGNAME=ubuntu\nSSH_CONNECTION=10.234.68.88 1711 10.232.1.123 22\nLESSOPEN=| /usr/bin/lesspipe %s\nAWS_SECRET=******************************\nXDG_RUNTIME_DIR=/run/user/1000\nLESSCLOSE=/usr/bin/lesspipe %s %s\n_=/usr/bin/env\nOLDPWD=/var/log/docker-registry\n. I followed the directions here (https://github.com/docker/docker-registry/blob/master/ADVANCED.md)and used:\n    pip install docker-registry\nIt is definitely not any custom wrapper script, unless pip install docker-registry pulls from somewhere else other than the official docker-registry.\n. Yup it was the sudo thing! Sorry I am kind of new to linux.\nThe output I get is:\n```\n!/usr/bin/python\nEASY-INSTALL-ENTRY-SCRIPT: 'docker-registry==0.9.1','console_scripts','docker-registry'\nrequires = 'docker-registry==0.9.1'\nimport sys\nfrom pkg_resources import load_entry_point\nif name == 'main':\n    sys.exit(\n        load_entry_point('docker-registry==0.9.1', 'console_scripts', 'docker-registry')()\n    )\n```\n. ",
    "sgandon": "I have the same issue \nFATA[0009] HTTP code 500 while uploading metadata: invalid character '<' looking for beginning of value\nwhen pushing an nginx image to our private registry.\n. We have tried to remove the faulty images without success. Our image comes from a simple Dockerfile inheriting from nginx:latest. \nhere is the client log (version 1.5.0)\ndocker push our-registry:5000/test-dataprep-webapp-nginx:0.0\nThe push refers to a repository [our-registry:5000/test-dataprep-webapp-nginx] (len: 1)\nSending image list\nPushing repository our-registry:5000/test-dataprep-webapp-nginx (1 tags)\nImage 30d39e59ffe2 already pushed, skipping\n511136ea3c5a: Image successfully pushed\nc90d655b99b2: Pushing\nFATA[0009] HTTP code 500 while uploading metadata: invalid character '<' looking for beginning of value\nhere is the server log (version is 1.3.0)\n2015-03-18 14:07:34,292 ERROR: Exception on /v1/images/c90d655b99b2ec5b7e94d38c87f92dce015c17a313caeaae0e980d9b9bed8444/json [PUT]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1817, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1477, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1381, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1475, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1461, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 250, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/images.py\", line 421, in put_image_json\n    layers.generate_ancestry(image_id, parent_id)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/layers.py\", line 55, in generate_ancestry\n    data = store.get_json(store.image_ancestry_path(parent_id))\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/driver.py\", line 184, in get_json\n    return json.loads(self.get_unicode(path))\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/driver.py\", line 190, in get_unicode\n    return self.get_bytes(path).decode('utf8')\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/core/driver.py\", line 196, in get_bytes\n    return self.get_content(path)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/drivers/file.py\", line 55, in get_content\n    raise exceptions.FileNotFoundError('%s is not there' % path)\nFileNotFoundError: /registry/images/30d39e59ffe287f29a41a3f8bd70734afc8728329e3289945cbdc5bbf07cd980/ancestry is not there\n. ",
    "rohitgupta-90": "I faced the same issue with my private registry. \n...\nImage f6808a3e4d9e already pushed, skipping\nca49356dbc38: Pushing\nFATA[0002] HTTP code 500 while uploading metadata: invalid character '<' looking for beginning of value\nI found that my private registry was out of space, fixed it and it seems to work fine after that.\nFWIW this is the docker version I am using: \nServer version: 1.3.2\nServer API version: 1.15\n. ",
    "jankoprowski": "Behaves the same when docker repository run out of space.\n. ",
    "zapient": "We are seeing the same issue but we also see the following error:\n\nReceived HTTP code 404 while uploading layer: \"{\\\"error\\\": \\\"Image not found\\\"}\" \n\nThe only work around was to keep resubmitting the push\n\nfunction docker-push() { docker push $1; while [ $? -ne 0 ]; do docker push $1; done }\n\nDoesnt appear to have anything to do with disk-space for us\nClient version: 1.5.0\nClient API version: 1.17\nGo version (client): go1.3.3\nGit commit (client): a8a31ef-dirty\nOS/Arch (client): linux/amd64\nServer version: 1.5.0\nServer API version: 1.17\nGo version (server): go1.3.3\nGit commit (server): a8a31ef-dirty\n. ",
    "k16wire": "I got the same issues. I can resolve to remove broken image file on the registry server.\nHow about remove 'd338bb63f151' file on the repository and try again!\n. ",
    "mdomke": "Ok. My bad. There was actually a nginx-server getting in the way. I'm now using the hipache again and it works!\n. ",
    "danielschwartz": "I'm actually running into this issue myself. I did a test push with no SSL and no basic auth enabled, everything works, so the registry itself works. \nWhen I do --insecure-registry <host>:<port> I get this error:\nx509: certificate signed by unknown authority. If this private registry supports only HTTP or HTTPS with an unknown CA certificate, please add `--insecure-registry harrys.dyn-o-saur.com:8080` to the daemon's arguments. In the case of HTTPS, if you have access to the registry's CA certificate, no need for the flag; simply place the CA certificate at\nwhile it's trying to post to /v1/_ping\nWhen I do --insecure-registry https://<host>:<port> I get this error: x509: certificate signed by unknown authority while it's trying to post to /v1/users/\nMeaning when I set https:// I get none of the extra messaging, suggesting that it's somehow set more correctly. However, in both formats, the commands don't actually work. \n. I also just tried --insecure-registry=\"<host>:<port>\" to an inverse of the above. Meaning I am now able to docker login <host>:<port> and have it give me the shorter error. Still though, cant move forward and login. Still getting the x509: certificate signed by unknown authority error.\n. - Yes\n- No, the issue is with the cert, not with the login. Because it is a self-signed cert it's failing, even though i've added the registry domain and port to --insecure-registry\n. - Not using the docker login with no auth. When there is no auth for the registry it simply fails on the /_v1/ping call instead of the login call\n- Not with SSL enabled, I get the x509 error. Without SSL enabled, I can push/pull just fine \n- Docker 1.5.0 for the cli, the latest registry docker container, this was all pulled last night\n- Yup, using it on the daemon, not on the cli i've checked by looking at ps aux | grep docker and making sure the daemon is running with it\n. ",
    "mlhamel": "I had this issue with docker and my certificate generated at StartSSL. I've fixed by following those steps:\nhttp://www.startssl.com/?app=42, basically:\nFetch the Root CA and Class 1 Intermediate Server CA certificates:\n$ wget http://www.startssl.com/certs/sub.class1.server.ca.pem\nCreate a unified certificate from your certificate and the CA certificates:\n$ cat ssl.crt sub.class1.server.ca.pem > /etc/nginx/conf/ssl-unified.crt\nAnd then use this new combined certificate in nginx !\n. ",
    "ngpestelos": "I would like to take a closer look, but I'm not familiar with how docker-registry stores its images. Can you tell me which directory to look for?\n. We're using local storage in one of our machines and it's setup to use /tmp/registry. My theory is that the images are flushed when the machine is rebooted.\nThank you very much :+1: \n. ",
    "WhisperingChaos": "Thanks for reporting this issue!  I too am experiencing it.\n. ",
    "tdoan1971": "Hi dmp42,\n- I use the docker-registry-0.9.0-1.el6.noarch rpm from EPEL repository\n- I use the default configuration in /etc/docker-register.yml file which uses dev flavor by default\n- I launch the registry by \"service docker-registry start\" commnad\nThanks,\n. Hi Olivier,\nI am not near the system now to try the curl command. However I have the flag \"--insecure-registry mylocalregistry:5000\" on the remote Docker daemon so that it pulls/pushes successfully without user authentication.\n. Hi Olivier,\nSo, how to enable the search for the registry if the curl command is false?\nThanks,\nTruong Doan\nDISA-EE322 - Perf Analysis & M&S\nRoom: A5D58G\n301-225-7335\n\"Everything happens for a reason!\"\n-----Original Message-----\nFrom: Olivier Gambier [mailto:notifications@github.com] \nSent: Monday, March 02, 2015 2:05 PM\nTo: docker/docker-registry\nCc: Doan, Truong V CIV DISA EE (US)\nSubject: Re: [docker-registry] docker search return unexpected 404 error (#951)\nMy point is: I don't think the search is enabled on your registry - that curl command would make sure about that.\n\u2014\nReply to this email directly or view it on GitHub https://github.com/docker/docker-registry/issues/951#issuecomment-76783328 . https://github.com/notifications/beacon/AHL-oExsbJDA-XuQp5B9GRijzdBsSAurks5nxKvggaJpZM4DoHYT.gif \n. The problem solved. It turns out that the docker-registry conf file at /etc/sysconfig point to the local flavor which does not have the endpoint flag set. Chage it to the dev flavor to solve the problem.\n. ",
    "carletes": "Thanks, @noxiouz! I tried running it with several versions of requests, but to no avail :( I'm running it now in a container, anyway, so problem bypassed.\n. ",
    "benley": "For anybody else who runs into this, it appears that running gunicorn with eventlet workers instead of gevent may be a viable workaround.\ne.g.\nsh\ngunicorn --access-logfile - --error-logfile - -k eventlet -b 0.0.0.0:5000 \\\n         -w 4 --max-requests 100 docker_registry.wsgi:application\n. This bug also manifests itself if you're using an S3 storage backend, for what it's worth.  Probably also any other boto-driven backend where SSL is involved.  My solution was, as you recommended, to run it from the docker image.\n. ",
    "STOIE": "Sorry guys, I finally found what to search on google for... lol\nThis was the fix for suing apache...\nProxyPreserveHost on\n. ",
    "sahilsk": "Hi @dmp42 \ncan you help me understand these log lines? I am guessing that registry is caching images in tempfies so that it don't need to request them from s3. However, in doing so it's ignoring remaining disk space... somehow redis queue length is too big .....something like that.  What do you suggest?\n```\n...\n2015-03-07 00:32:03,479 INFO: ParallelKey: ; size=193320751\n2015-03-07 00:32:03,494 INFO: ParallelKey: ; size=193320751\n2015-03-07 00:32:03,520 INFO: ParallelKey: ; size=193320751\n2015-03-07 00:32:03,521 INFO: ParallelKey: ; size=193320751\n...\n{\"log\":\"2015-03-07 00:33:08,125 INFO: ParallelKey: \\u003cKey: organizationimages,images/43fcdb249be8d2cdb8e46166fc4c399c2f5aa673a3ba90be6ad4d7c1a10a100a/l\nayer\\u003e; buffering complete at 32.8% of the total transfer; now serving straight from the tempfile\\n\",\"stream\":\"stderr\",\"time\":\"2015-03-07T00:33:08\n.126475811Z\"}\n{\"log\":\"2015-03-07 00:33:08,682 INFO: ParallelKey: \\u003cKey: organizationimages,images/43fcdb249be8d2cdb8e46166fc4c399c2f5aa673a3ba90be6ad4d7c1a10a100a/l\nayer\\u003e; buffering complete at 20.0% of the total transfer; now serving straight from the tempfile\\n\",\"stream\":\"stderr\",\"time\":\"2015-03-07T00:33:08\n.682632542Z\"}\n{\"log\":\"2015-03-07 00:33:13,880 INFO: ParallelKey: \\u003cKey: organizationimages,images/43fcdb249be8d2cdb8e46166fc4c399c2f5aa673a3ba90be6ad4d7c1a10a100a/l\nayer\\u003e; buffering complete at 22.5% of the total transfer; now serving straight from the tempfile\\n\",\"stream\":\"stderr\",\"time\":\"2015-03-07T00:33:13\n.881879406Z\"}\n{\"log\":\"2015-03-07 00:33:17,570 INFO: ParallelKey: \\u003cKey: organizationimages,images/43fcdb249be8d2cdb8e46166fc4c399c2f5aa673a3ba90be6ad4d7c1a10a100a/l\nayer\\u003e; buffering complete at 0.1% of the total transfer; now serving straight from the tempfile\\n\",\"stream\":\"stderr\",\"time\":\"2015-03-07T00:33:17.\n570786514Z\"}\n{\"log\":\"2015-03-07 00:33:32,329 INFO: ParallelKey: \\u003cKey: organizationimages,images/43fcdb249be8d2cdb8e46166fc4c399c2f5aa673a3ba90be6ad4d7c1a10a100a/l\nayer\\u003e; buffering complete at 0.8% of the total transfer; now serving straight from the tempfile\\n\",\"stream\":\"stderr\",\"time\":\"2015-03-07T00:33:32.\n330254169Z\"}\n{\"log\":\"2015-03-07 00:34:41,233 INFO: ParallelKey: \\u003cKey: organizationimages,images/d884e0950871aa73997efed77675146931c6fd75fcf0e0606f1fd35b046e079e/l\nayer\\u003e; buffering complete at 0.0% of the total transfer; now serving straight from the tempfile\\n\",\"stream\":\"stderr\",\"time\":\"2015-03-07T00:34:41.\n234784507Z\"}\n{\"log\":\"2015-03-07 00:34:41,312 INFO: ParallelKey: \\u003cKey: organizationimages,images/d884e0950871aa73997efed77675146931c6fd75fcf0e0606f1fd35b046e079e/l\nayer\\u003e; buffering complete at 60.0% of the total transfer; now serving straight from the tempfile\\n\",\"stream\":\"stderr\",\"time\":\"2015-03-07T00:34:41\n.312638242Z\"}\n{\"log\":\"2015-03-07 00:34:41,326 INFO: ParallelKey: \\u003cKey: organizationimages,images/d884e0950871aa73997efed77675146931c6fd75fcf0e0606f1fd35b046e079e/l\nayer\\u003e; buffering complete at 67.5% of the total transfer; now serving straight from the tempfile\\n\",\"stream\":\"stderr\",\"time\":\"2015-03-07T00:34:41\n.326851049Z\"}\n{\"log\":\"2015-03-07 00:34:41,621 INFO: ParallelKey: \\u003cKey: organizationimages,images/d884e0950871aa73997efed77675146931c6fd75fcf0e0606f1fd35b046e079e/l\nayer\\u003e; buffering complete at 53.8% of the total transfer; now serving straight from the tempfile\\n\",\"stream\":\"stderr\",\"time\":\"2015-03-07T00:34:41\n.622623091Z\"}\n```\n. ",
    "clintharris": "@dmp42 Yes, it works if I disable authentication for /v1/search endpoint. Not a great work-around if you're terribly security conscious...\nWhat do you mean by \"engine bug\"?\n. ",
    "calavera": "It looks like this was fixed with #10050. can we close it?\nNevermind, just realized that this is the deprecated repo, sorry for the noise.\n. ",
    "chaiken-verticloud": "Thanks for the quick response, Olivier!  I also noticed issues with tmp files not being cleaned up, and saw that others had already reported similar problems (like issue #954).  It seems like the best thing to do at this point is to simplify the code by removing the ParallelKey class, rather than tracking down the corner cases that lead to orphaned tmpfiles.  I'll take care of removing the code and submit a pull request.\n. Here's a draft of the code, which passes flake8 and seems to work on my test instance:\n  https://github.com/docker/docker-registry/compare/master...Altiscale:dc_OPS-5907\nI'll do a bit more testing, and then submit the pull request.\n. I'm moving my performance test results into this document...\nhttps://docs.google.com/spreadsheets/d/1VbdtRL8w8eAf0sKse1qk7vLmd-J10tIiyeUqCTjSfL4/edit?usp=sharing\ntl;dr: The pull request fixes the problem and doesn't change performance otherwise.  As expected, STORAGE_REDIRECT provides better bandwidth and therefore better latency for multiple simultaneous clients.  For access by a single client, performance mostly depends on the networking between the client and the registry, rather than the code running on the registry. \n. I restored the gevent.monkey.patch code, which causes the following flake8 errors:\ndepends/docker-registry-core/docker_registry/core/boto.py:32:1: E402 module level import not at top of file\ndepends/docker-registry-core/docker_registry/core/boto.py:33:1: E402 module level import not at top of file\ndepends/docker-registry-core/docker_registry/core/boto.py:35:1: E402 module level import not at top of file\ndepends/docker-registry-core/docker_registry/core/boto.py:36:1: E402 module level import not at top of file\ndepends/docker-registry-core/docker_registry/core/boto.py:37:1: E402 module level import not at top of file\nflake8 generated similar errors before I modified the file.\nI'll test the code with the restored gevent.monkey.patch (even thought I don't expect any problems from it).\n. I tested the code with the restored gevent.monkey.patch with both [AWS_SECURE=false; STORAGE_REDIRECT=true] and [AWS_SECURE=true; STORAGE_REDIRECT=false].  Both worked as expected.  Performance looks unchanged.\n. Got it.  I'll put it back in.\n. ",
    "sudoforge": "Whoops, sorry about that! This shouldn't have made its way over here.\n. ",
    "eyz": "Thanks for the merge. We use CentOS and RHEL 7, and only Docker 1.3.2 is in EPEL at the moment; we try to use official packages as much as possible. I see Docker 1.5 is required for the new Go-based registry, and I'm looking forward to it. Single binary deployments are great. Cheers!\n. ",
    "etuttle": "Thank you - that is helpful.\nI'll experiment with some options including docker/distribution.  I see it's in beta but if mirroring isn't really supported by docker-registry, that's what I should be looking at.\n. ",
    "DanielKerrigan": "Hi @dmp42, I am also experiencing the same problem. I am using a similar command as the person above and I am receiving the same 500 internal server error. Here are the logs from the registry:\n2015-06-17 15:07:01,090 ERROR: Exception on /v1/repositories/centos/tags/latest [DELETE]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1817, in wsgi_app\n    response = self.full_dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1477, in full_dispatch_request\n    rv = self.handle_user_exception(e)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1381, in handle_user_exception\n    reraise(exc_type, exc_value, tb)\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1475, in full_dispatch_request\n    rv = self.dispatch_request()\n  File \"/usr/local/lib/python2.7/dist-packages/flask/app.py\", line 1461, in dispatch_request\n    return self.view_functions[rule.endpoint](**req.view_args)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 306, in wrapper\n    return f(namespace=namespace, repository=repository, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line 280, in wrapper\n    return f(*args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/tags.py\", line 233, in _delete_tag\n    delete_tag(namespace=namespace, repository=repository, tag=tag)\n  File \"/usr/local/lib/python2.7/dist-packages/docker_registry/tags.py\", line 214, in delete_tag\n    image = store.get_content(path=tag_path)\nTypeError: wrapper() got an unexpected keyword argument 'path'\n. ",
    "liucc52": "I met the same question\n[root@centos-7 /]# docker push 10.145.157.98/nginx:latest\nFATA[0040] Error: v1 ping attempt failed with error: Get http://10.145.157.98/v1/_ping: read tcp 172.19.64.37:8080: i/o timeout \n. ",
    "sergeyevstifeev": "Experiencing the same issue. @dsw88 Did docker version upgrade help?\n. I'm using a cloudformation setup very similar to the following: https://github.com/mbabineau/cloudformation-docker-registry\nIt ends up with the following docker version on docker registry (1.4.1):\n$> docker version\nsudo: unable to resolve host ip-10-225-15-169\nClient version: 1.4.1\nClient API version: 1.16\nGo version (client): go1.3.3\nGit commit (client): 5bc2ff8\nOS/Arch (client): linux/amd64\nServer version: 1.4.1\nServer API version: 1.16\nGo version (server): go1.3.3\nGit commit (server): 5bc2ff8\nAnd the clients have (1.5.0):\n$> docker version\nClient version: 1.5.0\nClient API version: 1.17\nGo version (client): go1.3.3\nGit commit (client): a8a31ef/1.5.0\nOS/Arch (client): linux/amd64\nServer version: 1.5.0\nServer API version: 1.17\nGo version (server): go1.3.3\nGit commit (server): a8a31ef/1.5.0\n. ",
    "dcharbonnier": "same problem here :\n```\n:~$ docker -v\nDocker version 1.3.3, build d344625\n:~$ docker info\nContainers: 1\nImages: 15\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Dirs: 17\nExecution Driver: native-0.2\nKernel Version: 3.16.0-4-amd64\nOperating System: Debian GNU/Linux 8 (jessie)\nWARNING: No memory limit support\nWARNING: No swap limit support\n:~$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                      NAMES\n8b16ffd795e9        registry:latest     \"docker-registry\"   2 weeks ago         Up 2 weeks          127.0.0.1:5000->5000/tcp   docker-registry\n```\n. Client :\n```\n:~$ docker version\nClient version: 1.3.3\nClient API version: 1.15\nGo version (client): go1.3.3\nGit commit (client): d344625\nOS/Arch (client): linux/amd64\nServer version: 1.3.3\nServer API version: 1.15\nGo version (server): go1.3.3\nGit commit (server): d344625\n:~$ docker info\nContainers: 3\nImages: 67\nStorage Driver: aufs\n Root Dir: /data/docker/aufs\n Dirs: 73\nExecution Driver: native-0.2\nKernel Version: 3.16.0-4-amd64\nOperating System: Debian GNU/Linux 8 (jessie)\nWARNING: No memory limit support\nWARNING: No swap limit support\n```\n. ",
    "aviz": "0.9.1 from docker hub\n. ",
    "fengler": "dibs\n. ",
    "kbrowder": "nginx, however the failure is the same when accessing port 5000 (eg accessing the docker registry dock directly).\n. my docker version is 1.3.2 gotten from EPEL for centos 6, I don't have access to the machine right now, I'll get the rest of the data when I get back to the office.\nHowever i poked at it for a bit before I left and see the following behavior difference between index.docker.io and docker index:\n- curl http://localhost:5000/v1/repositories/centos/images gets redirected to http://localhost:5000/v1/repositories/centos/images/\n- curl http://localhost:5000/v1/repositories/centos/images/ fails with 405\nAnd in the docker index:\n- curl https://index.docker.io/v1/repositories/centos/images doesn't redirect, returns a bunch of json\n- https://index.docker.io/v1/repositories/centos/images/ returns \"\" (as in the empty string json)\nThanks in advance for for the help\n. I did, and I checked the the headers on http://localhost:5000/_ping, X-Docker-Registry-Standalone: mirror, I did configs via -e (env vars) only, it'll probably be tomorrow AM before I get back to the office and grab the full info above.\n. docker info:\nContainers: 4\nImages: 77\nStorage Driver: devicemapper\n Pool Name: docker-253:0-1703803-pool\n Pool Blocksize: 65.54 kB\n Data file: /var/lib/docker/devicemapper/devicemapper/data\n Metadata file: /var/lib/docker/devicemapper/devicemapper/metadata\n Data Space Used: 3.757 GB\n Data Space Total: 107.4 GB\n Metadata Space Used: 5.292 MB\n Metadata Space Total: 2.147 GB\n Library Version: 1.02.89-RHEL6 (2014-09-01)\nExecution Driver: native-0.2\nKernel Version: 2.6.32-504.1.3.el6.x86_64\nOperating System: <unknown>\ndocker version:\nClient version: 1.3.2\nClient API version: 1.15\nGo version (client): go1.3.3\nGit commit (client): 39fa2fa/1.3.2\nOS/Arch (client): linux/amd64\nServer version: 1.3.2\nServer API version: 1.15\nGo version (server): go1.3.3\nGit commit (server): 39fa2fa/1.3.2\ndocker registry launch:\n/usr/bin/docker run -e STANDALONE=false -e MIRROR_SOURCE=https://registry-1.docker.io -e MIRROR_SOURCE_INDEX=https://index.docker.io -e SETTINGS_FLAVOR=local -p 5000:5000 -e SEARCH_BACKEND=sqlalchemy -e SQLALCHEMY_INDEX_DATABASE=sqlite:////opt/sqlitedbs/registry.db -v /opt/sqlitedbs:/opt/sqlitedbs  registry:latest\nAlthough to make things simpler (using the defaults) I've also tried:\n/usr/bin/docker run -p 5000:5000 -e SEARCH_BACKEND=sqlalchemy -e SQLALCHEMY_INDEX_DATABASE=sqlite:////opt/sqlitedbs/registry.db -v /opt/sqlitedbs:/opt/sqlitedbs registry:latest\nLogs from docker-registry after a docker pull localhost:5000/centos:\n```\n$ docker attach 1e1\n172.17.42.1 - - [27/Mar/2015:12:33:56 +0000] \"GET /v1/_ping HTTP/1.1\" 200 1438 \"-\" \"Go 1.1 package http\"\n27/Mar/2015:12:33:56 +0000 DEBUG: args = {'namespace': 'library', 'repository': u'centos'}\n27/Mar/2015:12:33:56 +0000 DEBUG: api_error: images not found\n172.17.42.1 - - [27/Mar/2015:12:33:56 +0000] \"GET /v1/repositories/centos/images HTTP/1.1\" 404 29 \"-\" \"docker/1.3.2 go/go1.3.3 kernel/2.6.32-504.1.3.el6.x86_64 os/linux arch/amd64\"\n```\nHere's most of the logs after everything's started from docker after the aforementioned docker pull when running with -D:\n[0eba0de5] -job container_inspect(917) = ERR (1)\n[error] server.go:1207 Handler for GET /containers/{name:.*}/json returned error: No such container: 917\n[error] server.go:110 HTTP Error: statusCode=404 No such container: 917\n[debug] server.go:1181 Calling GET /containers/json\n[info] GET /v1.15/containers/json\n[0eba0de5] +job containers()\n[0eba0de5] -job containers() = OK (0)\n[debug] server.go:1181 Calling GET /containers/{name:.*}/json\n[info] GET /v1.15/containers/1/json\n[0eba0de5] +job container_inspect(1)\nNo such container: 1\n[0eba0de5] -job container_inspect(1) = ERR (1)\n[error] server.go:1207 Handler for GET /containers/{name:.*}/json returned error: No such container: 1\n[error] server.go:110 HTTP Error: statusCode=404 No such container: 1\n[debug] server.go:1181 Calling GET /containers/{name:.*}/json\n[info] GET /v1.15/containers/1e1/json\n[0eba0de5] +job container_inspect(1e1)\n[0eba0de5] -job container_inspect(1e1) = OK (0)\n[debug] server.go:1181 Calling POST /containers/{name:.*}/attach\n[info] POST /v1.15/containers/1e1/attach?stderr=1&stdout=1&stream=1\n[0eba0de5] +job container_inspect(1e1)\n[0eba0de5] -job container_inspect(1e1) = OK (0)\n[0eba0de5] +job attach(1e1)\n[debug] attach.go:176 attach: stdout: begin\n[debug] attach.go:215 attach: stderr: begin\n[debug] attach.go:263 attach: waiting for job 1/2\n[debug] server.go:1181 Calling POST /images/create\n[info] POST /v1.15/images/create?fromImage=localhost%3A5000%2Fcentos%3Alatest\n[0eba0de5] +job pull(localhost:5000/centos, latest)\n[debug] endpoint.go:59 Error from registry \"https://localhost:5000/v1/\" marked as insecure: Get https://localhost:5000/v1/_ping: EOF. Insecurely falling back to HTTP\n[debug] endpoint.go:144 RegistryInfo.Version: \"\"\n[debug] endpoint.go:147 Registry standalone header: 'True'\n[debug] endpoint.go:155 RegistryInfo.Standalone: true\n[debug] session.go:266 [registry] Calling GET http://localhost:5000/v1/repositories/centos/images\n[debug] http.go:162 http://localhost:5000/v1/repositories/centos/images -- HEADERS: map[User-Agent:[docker/1.3.2 go/go1.3.3 kernel/2.6.32-504.1.3.el6.x86_64 os/linux arch/amd64]]\nError: image centos not found\n[0eba0de5] -job pull(localhost:5000/centos, latest) = ERR (1)\n. @dmp42, now I understand, STANDALONE=true is actually what it should be for my use case, sorry about the PEBKAC.  Now i just can't get to the tags repo, looks like that's an easy thing to fix.  Anyways this was a false alarm, apologies again.\n. Ah OK, hmmm, I was having  a problem before without this, maybe it was something from a previous bad configuration cached in redis?  Anyways I killed my redis and restarted it, and changed this back to the old default and things work fine, thanks, apologies again for the PEBKAC or whatever it was.\n. ",
    "cyberco": "I found that I had to edit /etc/default docker. Now that reads:\ncat /etc/default/docker\nDOCKER_OPTS=\"$DOCKER_OPTS --insecure-registry=site.com:443\"\nNow the error is:\nFATA[0014] Error: v1 ping attempt failed with error: Get https://site.com:443/v1/_ping: dial tcp 1.2.3.4:443: i/o timeout. If this private registry supports only HTTP or HTTPS with an unknown CA certificate, please add `--insecure-registry site.com:443` to the daemon's arguments. In the case of HTTPS, if you have access to the registry's CA certificate, no need for the flag; simply place the CA certificate at /etc/docker/certs.d/site.com:443/ca.crt\nAn i/o timeout, which I don't understand since the server is up with HTTPS and can be pinged.\n. The registry has started and is listening, but doing:\nOpenSSL s_client -connect site.com:443/v1/_ping -prexit -debug\n...gives me:\n```\nCONNECTED(00000003)\nwrite to 0x7f906b700000 [0x7f906d001000] (130 bytes => 130 (0x82))\n0000 - 80 80 01 03 01 00 57 00-00 00 20 00 00 39 00 00   ......W... ..9..\n0010 - 38 00 00 35 00 00 16 00-00 13 00 00 0a 07 00 c0   8..5............\n0020 - 00 00 33 00 00 32 00 00-2f 00 00 9a 00 00 99 00   ..3..2../.......\n0030 - 00 96 03 00 80 00 00 05-00 00 04 01 00 80 00 00   ................\n0040 - 15 00 00 12 00 00 09 06-00 40 00 00 14 00 00 11   .........@......\n0050 - 00 00 08 00 00 06 04 00-80 00 00 03 02 00 80 00   ................\n0060 - 00 ff fe c8 6e d6 d0 17-f7 e9 6c b2 2f ee 09 83   ....n.....l./...\n0070 - e4 c0 71 11 be 86 77 5d-b9 9b 9f 54 c9 07 a6 fa   ..q...w]...T....\n0080 - e2 ef                                             ..\nread from 0x7f906b700000 [0x7f906d006600] (7 bytes => 0 (0x0))\n9308:error:140790E5:SSL routines:SSL23_WRITE:ssl handshake failure:/SourceCache/OpenSSL098/OpenSSL098-52.10.1/src/ssl/s23_lib.c:185:\n\nno peer certificate available\nNo client certificate CA names sent\nSSL handshake has read 0 bytes and written 130 bytes\nNew, (NONE), Cipher is (NONE)\nSecure Renegotiation IS NOT supported\nCompression: NONE\nExpansion: NONE\n```\nI'm too much a newbie to really understand what's going wrong here.\n. OK, found it. It turns out that you have to run the LOCAL docker daemon with the '--insecure-registry' option, not the docker daemon of the remote docker registry.\nI somehow missed that from all the blogs and discussions...\n. ",
    "mkrcah": "If you use docker-machine 0.3.0, you can pass this parameter with --engine-insecure-registry\n. ",
    "ryanhanks-wf": "Myself and another individual in our organization both hit this error on our machines (we're both on OS X). In our situation we were able to resolve the issue by killing and restarting the docker VM.\n. ",
    "michaelneale": "@dmp42 the public docker hub STILL uses the old non notifications format, so for people that want to support both it and private registries you get a very different and clumsy experience (config for private registry is via yml, not via setting a per repo webhook, which is another weirdness). \nDo I have this right? I can't reconcile the inconsistency. \n. Thanks yes I could see that. The events Api is a superset so tools will\nneed to expect either blob of json, which isn't too much of an issue, just\na bit surprising.\nOn Wed, 26 Aug 2015 at 5:55 pm Olivier Gambier notifications@github.com\nwrote:\n\n@michaelneale https://github.com/michaelneale yeah, the Hub and\nprivately operated registries are really two different beasts, with widely\ndifferent targets and use cases.\nThe Hub is really meant as a zero maintenance, pure SAAS system, while\noperating a private registry gives you both more control, and more\n\"responsibility\" in implementing your own pipeline.\nWhether the APIs should be unified or not is open for debate, but if you\nask me, the kind flexibility you find in registry:2 (\nhttps://github.com/docker/distribution) can simply not make its way into\nthe hub, if only for security reasons.\nHope that helps.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/982#issuecomment-134888245\n.\n. @dmp42 makes sense. Do you think the private registry will layer on a hub like \"simple\" webhook or will the generic events api/json be the one true way? (I understand those events aren't appropriate for the saas hub).\n. \n",
    "kmadel": "Will the Docker backed 'Trusted Registry' support either setting endpoints for registry notificaitons and/or webhooks similar to what is available on Docker Hub?\n. ",
    "hammi85": "I think I'm wrong. The timestamp says 18:09:04 CEST and the log entry is 16:09:04 but GMT so it should be fine.\n. Solution: I just restarted with the configuration of everything and now it works. So I guess it was a mistake in my S3 policy.\n. ",
    "squarism": "I don't have an answer but have a similar question.\nI pushed the busybox image to my private 2.0 registry (distribution).\nThis will work for me.  I get a huge json payload.\n```\ncurl -XGET http://private-host:5000/v2/busybox/manifests/latest\nGET included for effect only  :)\n```\nThe API doc says that DELETE should work.  So I just substitute above and get\ncurl -XDELETE http://private-host:5000/v2/busybox/manifests/latest                                                                                            \n{\"errors\":[{\"code\":\"UNSUPPORTED\",\"message\":\"The operation is unsupported.\"}]}\nSo I think that's your answer but it's not working for me.\n. I think the issue here is that I'm getting a method not supported on the DELETE request not that the blobs are sticking around.  If you have an example curl example, that's be great.  I could be doing something wrong here.  If the OP disagrees, I'll open another issue.\n. Aaah!  And I blogged the url to the distribution repo too!  I'm sorry, I knew the difference, I failed to see what issue list I was in.  :sob:  Thanks for the tldr too.  :+1: \n. ",
    "adolphlwq": "+1 with my operation and output:\n```\nadolph@geek:docker_registry$ curl -v -X DELETE http://localhost:5000/v2/busybox/manifests/sha256:0fc02bc170932a0d727de2201c65bfe3fe31448dfed6f6bcbc1b37c2668927f5\n Hostname was NOT found in DNS cache\n   Trying 127.0.0.1...\n* Connected to localhost (127.0.0.1) port 5000 (#0)\n\nDELETE /v2/busybox/manifests/sha256:0fc02bc170932a0d727de2201c65bfe3fe31448dfed6f6bcbc1b37c2668927f5 HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: localhost:5000\nAccept: /\n< HTTP/1.1 405 Method Not Allowed\n< Content-Type: application/json; charset=utf-8\n< Docker-Distribution-Api-Version: registry/2.0\n< X-Content-Type-Options: nosniff\n< Date: Fri, 25 Dec 2015 05:57:29 GMT\n< Content-Length: 78\n< \n{\"errors\":[{\"code\":\"UNSUPPORTED\",\"message\":\"The operation is unsupported.\"}]}\n Connection #0 to host localhost left intact\nadolph@geek:docker_registry$ curl -v -X DELETE http://localhost:5000/v2/busybox/manifests/list\n Hostname was NOT found in DNS cache\n   Trying 127.0.0.1...\n Connected to localhost (127.0.0.1) port 5000 (#0)\nDELETE /v2/busybox/manifests/list HTTP/1.1\nUser-Agent: curl/7.35.0\nHost: localhost:5000\nAccept: /\n< HTTP/1.1 405 Method Not Allowed\n< Content-Type: application/json; charset=utf-8\n< Docker-Distribution-Api-Version: registry/2.0\n< X-Content-Type-Options: nosniff\n< Date: Fri, 25 Dec 2015 05:59:35 GMT\n< Content-Length: 78\n< \n{\"errors\":[{\"code\":\"UNSUPPORTED\",\"message\":\"The operation is unsupported.\"}]}\n* Connection #0 to host localhost left intact\n```\n. \n",
    "CodeJuan": "I sent the same request with @adolphlwq 's request, and got the same response\n```\ncurl -v -X DELETE http://myregistry/v2/busybox/manifests/sha256:blablabla...\n{\"errors\":[{\"code\":\"UNSUPPORTED\",\"message\":\"The operation is unsupported.\"}]}\n```\nupdate\nI got the solution to delete images\nenable delete\nset the environment variable REGISTRY_STORAGE_DELETE_ENABLED = True\nthe API to delete image\n\nget the manifest from registry\n\nget v2/<repoName>/manifests/<tagName>\nthe Docker-Content-Digest is response.Header[\"Docker-Content-Digest\"]\nthe layerDigests is response.body[\"fsLayers\"][\"blobSum\"]\n1. delete layerDigests\ndelete v2/<repoName>/blobs/<layerDigests>\n1. delete Docker-Content-Digest\ndelete v2/<repoName>/manifests/<Docker-Content-Digest>\n1. then pull the image from registry, the response is invalid character '<' looking for beginning of value\nBut when I get 'v2/repoName/tags/list', the tag which was been deleted is still exist.......\n. ",
    "mphanikumars": "Hi,\nI am very new to Docker. \nI have tried like this \"curl -X DELETE localhost:5000/v1/repositories/jenkins_ora_jdk/tags/latest\" but this works for tag deletion and not the image data for that tag. So i had to write my own script to do that Job. Since i am only the one who tested this script please use with your own risk\nThis is how i start registry image \nmkdir -p /docker/registry/latest && docker run -d -p 5000:5000 -v /docker/registry/latest:/tmp registry\nTo delete an Image with tag : (This will also cleans the data specific to that tag)\n./docker_registry_delete_image -d 5000 \"jenkins_ora_jdk:latest\"\nTo see all the Images with tag :\n./docker_registry_delete_image -q 5000 \n: 5000 is Registry PORT\n# NOTE\n1. Registry service must be running\n2. Volume must be mounted locally ( -v /docker/registry/latest:/tmp )\n3. This works only in Linux environment\n4. This should be executed in where docker registry is running\ndocker_registry_delete_image.sh.txt\nThanks,\nPhani Kumar\n. ",
    "lioncui": "@CodeJuan You must edit registry config.xml with \nstorage:\n    delete:\n        enabled: true\nreference https://docs.docker.com/registry/spec/api/\n. ",
    "slartibart70": "Hi,\njust adding my experiences:\n- started registry:2 with my own config (deleting allowed) and own volume mounted storage.\n- Pushing images into my registry works ok, my storage contains the data (approx. 100MB)\nNow trying to delete:\n- curl -v -s -X DELETE \"localhost:5000/v2/collctor/blobs/sha256:cbb2acf4...\"  (see layerDigests above)\n- curl -v -s -X DELETE \"localhost:5000/v2/collctor/manifests/sha256:66846...\" (see Docker-Content-Digest above)\nAfter the above steps, we still have an entry in the catalog:\ncurl -v -s -X GET \"localhost:5000/v2/_catalog\" .. which still returns the deleted image\ncurl -s -X GET localhost:5000/v2//manifests/latest still returns seemingly valid data\nbut\na docker pull localhost:5000/ does not work any more.\nAnd, the storage space (my ~100MB) are still in use and are not deleted.\nAny ideas?\n. ",
    "EugenMayer": "I finally go teached about storage->delete->true in the configuration https://github.com/docker/distribution/blob/master/docs/configuration.md\nand implemented it. Still, while fetching the digest https://github.com/EugenMayer/docker_registry_cli/blob/master/DockerRegistryRequest.rb#L92 getting a string like sha256:XXXX\nUsing this to feed into delete https://github.com/EugenMayer/docker_registry_cli/blob/master/DockerRegistryRequest.rb#L83\ni get notfied, that the digest is wrong.\nUsing the current latest registry:2 (sha256:20f5d95004b71fe14dbe7468eff33f18ee7fa52502423c5d107d4fb0abb05c1d).\nIs this a bug / fixed already?\n. as requested i created a new issue #1068, maybe you continue work with me there @jshapiro26 \n. ups, used the legacy issue, @jshapiro26  see https://github.com/docker/distribution/issues/1637 instead\n. Yes, i already implemented this in my docker_registry_cli:\nhttps://github.com/EugenMayer/docker_registry_cli/blob/master/commands/DockerRegistryCommand.rb#L57\nSo this works - one of the things i missed was the Accept header: application/vnd.docker.distribution.manifest.v2+json]\nBut other then that, it works quiet butifully\nHint: dont forget to run the garbage collector to clean up the space\n. For, the GC is utterly broken in 2.5 - i would assume the registry being completely useless after more then 2 months - you have to wipe it a redo all images. There are severe issues with blobs, pushed will not work because \"the layers are already up to date\". Using the GC is completely useless, the only thing actually really kinda works is going to /var/lib/registry/repos.. and delete the folder manually - restart the registry container.\n. see #1068 \n. i am not alone here, seems its a general issue https://github.com/docker/docker-registry/issues/988#issuecomment-210190903\n. dupe, using https://github.com/docker/distribution/issues/1637 \n. ",
    "jshapiro26": "@EugenMayer I've been messing around with this too. It looks like the digest that you return as a header from: get v2/<repoName>/manifests/<tagName> is the wrong digest. \nInstead, if you docker pull that image from your registry and grab the Digest it spits out after downloading you should should be able to delete using that with: \ncurl -u <username:password> -X DELETE https://yourregistry/v2/<repository>/manifests/<newdigest>\nIf anyone knows of a simple way to get this digest other than pulling the image, please let me know.\nOnce that is deleted, run garbage collection dry run and then garbage collect on your registry to actually delete the data that you have marked for deletion. On the docker host that is running your registry, run: \ndocker exec -it <name of container or container ID> bin/registry garbage-collect --dry-run /etc/docker/registry/config.yml. It shows that you have N blobs eligible for deletion. Run the command again without --dry-run and it will delete the blobs. Now that they're deleted, if you run the dry-run again you should see that the namespace you deleted has no more blobs. \n. @lorenzvth7 and @EugenMayer, I ended up writing an interactive script for the process I mentioned above and will share it soon and post back here, maybe it'll be useful for someone. The issue i've had is that my team typically just uses the latest tag. The latest tag then starts bloating and there isn't a good way to clean up old versions of latest. \nI ended up just parsing the output of the --dry-run garbage collection command and comparing the SHAs against the SHA I get from latest when I run a docker pull. This is because Docker-Content-Digest: <digest_hash> never seems to be correct for the DELETE call to the API.\nI noticed your comment about the header: Accept header: application/vnd.docker.distribution.manifest.v2+json and will check that out. Maybe it'll solve all the problems!\n. Thanks for the clarification about sending the header! I'll try that out. It sounds like you likely have an older version of that image that would be untagged now since you've deleted latest. If you remove that image again then ssh to your registry and run docker exec -it <container ID or name> bin/registry garbage-collect --dry-run /etc/docker/registry/config.yml does it return any SHAs or blobs under myreg/proj2/base? That might be where the \"Layer already exists\" is coming from as lots of layers may have been unchanged.\nThe script i'm cleaning up goes through and deletes all old SHAs for a specified repo, leaving the one tagged as latest in-tact. \n. did you check the output of the --dry-run of garbage collection? \n@EugenMayer here is the script I wrote to actually \"garbage collect\" https://github.com/jshapiro26/registry-tools/blob/master/clean_docker_registry.rb It's a bit of a hassle as it requires user input, but much faster than other methods i've seen.\nI've set the garbage-collect task to run via cron on the docker host that my registry runs on, this script will mark the appropriate SHAs and blobs to be deleted when that cron runs.\n. @zoobab What image tag of registry are you using? I am using registry:2.4.1 and have never ran into the issue you're referring to. Also, do you run the garbage collection without --dry-run after you use the script and before pushing again? The script doesn't actually run the garbage collection after all the images and blobs are marked. I solve this by have a cron run the garbage collection from the docker host early in the morning everyday.\nTo clarify what I wrote the script for, it is used to remove all versions of an image that are not tagged \"latest\". Therefore if you're pushing an image with a tag that's called \"6.2\" or something like that, those images will be removed in addition to all the untagged versions of \"latest\" leaving only the image with the latest tag for the repo you're \"cleaning\".\nI am unable to replicate what you're seeing @zoobab with this script. I use it daily and have an automated build of new images weekly that push successfully. Can you give me some more details so that I might be able to help?\n. are you pushing a new build of the image with a different sha? The script leaves the image with the latest tag in-tact on the registry. If you are pushing the same image up it will throw the error you're seeing.\n. ",
    "sergey-tkalych": "My approach how to delete image tag from docker registry (tested on registry version 2.4.0):\n1) Update registry configuration file (config.yml) and restart registry\nstorage:\n    delete:\n        enabled: true\n2) Get tags list\nGET :///v2//tags/list\n3) Get manifest for selected tag\nGET :///v2//manifests/\n4) Copy digest hash from response header\nDocker-Content-Digest: \n5) Delete manifest (soft delete). This request only marks image tag as deleted and doesn't delete files from file system. If you want to delete data from file system, run this step and go to the next step\nDELETE :///v2//manifests/\nNote! You must set headers for request - Accept: application/vnd.docker.distribution.manifest.v2+json\n6) Delete image data from file system\nRun command from the host machine:\ndocker exec -it  bin/registry garbage-collect \nNote! Usually, =/etc/docker/registry/config.yml\n. ",
    "lvthillo": "There is no API available to trigger the GC so I think it's the only possibility to use docker exec .. ? I'm able to put it in a cron job or something but I don't know how I can get my registry in read-only mode, execute it and disable read-only (we need to push images to the registry, but it has to be forbidden when the GC runs). Or is the only solution to stop the container, start it as read-only, docker exec .., stop container and start it in a normal way? This would take \"a lot of\" time I assume. (I want to run the GC every time we delete an image)\n. @jshapiro26 I'm able to get the right sha of my image with tag:latest.\nI'm using this command:\ncurl -k -v --silent -H \"Accept: application/vnd.docker.distribution.manifest.v2+json\" -X GET https://myreg/v2/proj2/base/manifests/latest 2>&1 | grep Docker-Content-Digest | awk '{print ($3)}'\nsha256:52cfbde9c62b2df9a4fa6f06e56759431166d2374bb0812cf0d6ee781cbe1abe\nCheck in my registry\n$ docker exec -it 86deeac4f25f /bin/sh\n/ # cd /var/lib/registry/docker/registry/v2/repositories/proj2/base/_manifests/revisions/sha256/52cfbde9c62b2df9a4fa6f06e56759431166d2374bb0812cf0d6ee781cbe1abe/\nHope it helps for your. My issue is the following now:\nI delete the sha:\ncurl -k -v --silent -X DELETE https://myreg/v2/proj2/base/manifests/sha256:52cfbde9c62b2df9a4fa6f06e56759431166d2374bb0812cf0d6ee781cbe1abe\nThis is going fine:\n```\n\nAccept: /\n< HTTP/1.1 202 Accepted\n< Docker-Distribution-Api-Version: registry/2.0\n```\n\nI use the GC to clean up:\ndocker exec -it 86deeac4f25f bin/registry garbage-collect  /etc/docker/registry/config.yml\n$ docker exec -it 86deeac4f25f bin/registry garbage-collect /etc/docker/registry/config.yml\nINFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/c8/c8144262002cd241e607d7d3ecda450ce4ae8edf7dac8dbf46897d498ac667d8  go.version=go1.6.3 instance.id=5ba1a41c-a0a4-4f87-a3c0-0cd1a1a2d392\nINFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/ce/cee0974db2b868f0408f7e3eaba93c11fce3a38f612674477653b04c10369da0  go.version=go1.6.3 instance.id=5ba1a41c-a0a4-4f87-a3c0-0cd1a1a2d392\nINFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/06/064f9af025390d8da3dfab763fac261dd67f8807343613239d66304cda8f5d16  go.version=go1.6.3 instance.id=5ba1a41c-a0a4-4f87-a3c0-0cd1a1a2d392\nINFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/39/390957b2f4f0cd72b8577795cd8076cdc21d45c7823bbb5c895a494ae6038267  go.version=go1.6.3 instance.id=5ba1a41c-a0a4-4f87-a3c0-0cd1a1a2d392\nINFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/3d/3d1d7dffe2da492324b1fd62a2939ddc6e50549746cefa9a67684253dfa40b88  go.version=go1.6.3 instance.id=5ba1a41c-a0a4-4f87-a3c0-0cd1a1a2d392\nINFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/52/52cfbde9c62b2df9a4fa6f06e56759431166d2374bb0812cf0d6ee781cbe1abe  go.version=go1.6.3 instance.id=5ba1a41c-a0a4-4f87-a3c0-0cd1a1a2d392\nINFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/6b/6bc128ff8a43e6b26862384494bd2cbfabc5e9d5277121881cbd0fcbf1442508  go.version=go1.6.3 instance.id=5ba1a41c-a0a4-4f87-a3c0-0cd1a1a2d392\nINFO[0000] Deleting blob: /docker/registry/v2/blobs/sha256/a5/a52d570eb492f9fb3cebb345c96395fc6156eef00ac42758e1a2d0f4190ecbf6  go.version=go1.6.3 instance.id=5ba1a41c-a0a4-4f87-a3c0-0cd1a1a2d392\nCheck (seems good to me):\n/var/lib/registry/docker/registry/v2 # du -sh\n63.0K\nBut now the real issue is repushing:\nThe push refers to a repository [myreg/proj2/base]\n5d3accbaa61c: Layer already exists\n3af563f9cad6: Layer already exists\n447f88c8358f: Layer already exists\ndf9a135a6949: Layer already exists\ndbaa8ea1faf9: Layer already exists\n8a14f84e5837: Layer already exists\nlatest: digest: sha256:52cfbde9c62b2df9a4fa6f06e56759431166d2374bb0812cf0d6ee781cbe1abe size: 1576\nEverything seems to exist already in the registry (isn't it cleaned up?).\nThe sha is the same which seems normal to me (it's the same image). \nBut checking in the registry?\n/var/lib/registry/docker/registry/v2 # du -sh\n65.0K\nThe most of space is taken in /bin/registry\n/bin # du -sh registry\n26.0M   registry\nSomething I don't understand.\nafter deleting the sha and rerunning the GC nothing seems to happen (the tag and _manifest is gone in the registry, but it seems not necessary to delete layers again, the space in / remains the same?).\n. I pushed the image only with the latest tag. Than I'm able to delete it correctly but after repushing just the same image with just the same tag it goes wrong. Everything seems to exist, All the folders in /var/lib/registry/docker/registry/v2/blobs/sha256/xx are empty. There is a new tag and a new manifest in the registry but it does not take any space. Even pulling the image from the registry does not work..\nWhen I push another image with the same name and same tag (also latest in this case) it's pushing in a fine way (I build it with --no-cache, don't know if it's relevant). I'm able to pull it and everything so this issue only seems to appear when I try to push, delete, repush and pull the exactly same image.\nMaybe it's all a bit unclear but to resume:\nIt seems to work fine for me (pushing, pulling, deleting, clearing) in any case except when I delete image:x and repush image:x (which is the same image, not only the same name). After the \"repush\" everything still seems to exist but I'm unable to pull.\n. Is there someone who knows what's the best way to delete unused revisions? \nAs far as I understand the normal way is to delete a revision based on its tag and after that you're able to run the GC which will clean all the rest.\nBut in DEV we are retagging an image on the same tag (latest). So if we try to use our delete-job (where you have to specify a tag) it only deletes one image (the newest/latest).\nAll the rest is still there because their revisions aren't deleted.\n. Restarting makes it possbile to repush images. The GC does not seem to clean up my blobs which are still growing..\n. ",
    "zoobab": "@EugenMayer I am running the same issue as yours with the \"being unable to repush an image\", as the deleted layers tells it is already there. When I list the images, it does not appear neither.\nI have a similar shell script, will try to find out the --dry-run trick if it solves my repush issue.\n. @jshapiro26 I am tempted to rewrite your ruby script in shell.\n. @jshapiro26 your ruby script suffers from the same defect as @EugenMayer mentioned. When I repush the same image, I get the same message \"Image already exists\", and when I try to pull it from the repo, I got the error \"Error: image ubuntu not found\".\n. @jshapiro26 yes, I clean up after without --dry-run so that the disk siez diminishes (which is my goal here). My docker registry version is 2.5.0. When I repush the image after the garbage cleanup, I have this error \"Layer already exists\".\n. @jshapiro26 no, I am trying to repush an old image that was deleted, which has the same sha. In your case, you are repushing \"latest\" which has a new sha because it was freshly built.\n. @eesprit I can confirm a restart of the registry solves the \"docker push oldimage\" problem!\n. I thought maybe purging the upload cache would be the root cause of the \"docker push oldimage\" problem? Is there a way to purge this cache? I have seen some bits in the config.yml file.\nmaintenance:\n    uploadpurging:\n      enabled: true\n      age: 168h\n      interval: 24h\n      dryrun: false\nThere is no way to purge the cache manually?\n. ",
    "eesprit": "@zoobab did you restart the registry after the garbage collection ?\nI have the same behavior as you do if I run the registry GC and don't restart it. After it is restarted, no problem (which is totally understandable as the GC is done outside from the server process, which might have some cache).\n. ",
    "messyhead": "I'm trying to delete following your instructions, but get a 403 Forbidden error.\nI've set the REGISTRY_STORAGE_DELETE_ENABLED: true in the yaml file that starts up the registry.\n. ",
    "TranceMaster86": "I builded a docker-reg-gc shellscript for that stuff.\nhttps://github.com/TranceMaster86/docker-reg-gc\n. Look at my script there https://github.com/TranceMaster86/docker-reg-gc\nIt removes everything that is older than 150 days....or the count of days you want it to do. So the newst things are left in the registry\n. ",
    "josselinchevalay": "hi do you see that : https://docs.docker.com/registry/spec/api/#/deleting-an-image. ",
    "smashingx1": "Ive been using reg (https://github.com/jessfraz/reg/tree/master/vendor) to mark the blobs I want to delete from the registry by doing:\ndocker run --rm reg/delete -u username -p password --registry \nmyregistry.com rm image\nI get the output of:\nDeleted image@sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\nThen I ran the command in my server where the docker registry container is, to run the garbage collector like:\nsudo docker exec -it registry bin/registry garbage-\ncollect --dry-run /config/config.yml\nAnd I got a lot of output back like:\nDEBU[0007] filesystem.Stat(\"/docker/registry/v2/blobs/sha256/17/17954313e0f8926875ce0743fe85693e0e9f695908c8f1544360d067a66b5eee/data\")  environment=development go.version=go1.7.3 instance.id=3296f17d-2ba7-4b3f-8530-991cb2ff8e04 service=registry trace.duration=15.631837ms trace.file=/go/src/github.com/docker/distribution/registry/storage/driver/base/base.go trace.func=github.com/docker/distribution/registry/storage/driver/base.(*Base).Stat trace.id=6977da41-fd94-4926-99af-059821e18046 trace.line=137\naccount-portal: marking manifest sha256:17954313e0f8926875ce0743fe85693e0e9f695908c8f1544360d067a66b5eee\nDEBU[0007] (*manifestStore).Get                          environment=development go.version=go1.7.3 instance.id=3296f17d-2ba7-4b3f-8530-991cb2ff8e04 service=registry\nDEBU[0007] filesystem.GetContent(\"/docker/registry/v2/repositories/account-portal/_manifests/revisions/sha256/17954313e0f8926875ce0743fe85693e0e9f695908c8f1544360d067a66b5eee/link\")  environment=development go.version=go1.7.3 instance.id=3296f17d-2ba7-4b3f-8530-991cb2ff8e04 service=registry trace.duration=86.1\u00b5s trace.file=/go/src/github.com/docker/distribution/registry/storage/driver/base/base.go trace.func=github.com/docker/distribution/registry/storage/driver/base.(*Base).GetContent trace.id=ebd03857-56b6-4957-b791-9c48fdb152e9 trace.line=82\nDEBU[0007] filesystem.Stat(\"/docker/registry/v2/blobs/sha256/17/17954313e0f8926875ce0743fe85693e0e9f695908c8f1544360d067a66b5eee/data\")  environment=development go.version=go1.7.3 instance.id=3296f17d-2ba7-4b3f-8530-991cb2ff8e04 service=registry trace.duration=74.19\u00b5s trace.file=/go/src/github.com/docker/distribution/registry/storage/driver/base/base.go trace.func=github.com/docker/distribution/registry/storage/driver/base.(*Base).Stat trace.id=96520ca1-6ba1-470c-869e-cf0a49846390 trace.line=137\nDEBU[0007] filesystem.GetContent(\"/docker/registry/v2/blobs/sha256/17/17954313e0f8926875ce0743fe85693e0e9f695908c8f1544360d067a66b5eee/data\")  environment=development go.version=go1.7.3 instance.id=3296f17d-2ba7-4b3f-8530-991cb2ff8e04 service=registry trace.duration=20.820776ms trace.file=/go/src/github.com/docker/distribution/registry/storage/driver/base/base.go trace.func=github.com/docker/distribution/registry/storage/driver/base.(*Base).GetContent trace.id=6752e130-1b77-49c2-adda-3b9195aac1df trace.line=82\nDEBU[0007] (*schema2ManifestHandler).Unmarshal           environment=development go.version=go1.7.3 instance.id=3296f17d-2ba7-4b3f-8530-991cb2ff8e04 service=registry\naccount-portal: marking blob sha256:90769d06ffb7dd1f8d1bc4f7b18e52fb94b651191640b680f028806622b99236\naccount-portal: marking blob sha256:3b8d91fe75724cafe136236e31d5212cb322d68aa3bfc4bfda97a0a53a55e234\naccount-portal: marking blob sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\naccount-portal: marking blob sha256:dfc6505cfad4d185913ccd772541a0d32e64d4e04d9c249728dd7e194c77340f\naccount-portal: marking blob sha256:47b142d56ce3cc7766143d5fcd836541dac5006fa66076d4261e9e3d4a24a0cd\naccount-portal: marking blob sha256:6397456b1db08c3b5fb104497d15eb849a317c35ffeb18b608e6013593b82300\naccount-portal: marking blob sha256:f1074061d22a0681d732d72a8d11abf0a2f77ee3652ee827c62d696c19697a93\naccount-portal: marking blob sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\naccount-portal: marking blob sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\naccount-portal: marking blob sha256:7b60f5a7551c3bb15dcd5ecd46a2ec18118cc652f6bdbff23bf16c895a8a1586\naccount-portal: marking blob sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\naccount-portal: marking blob sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\naccount-portal: marking blob sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\naccount-portal: marking blob sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\naccount-portal: marking blob sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\naccount-portal: marking blob sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\naccount-portal: marking blob sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\naccount-portal: marking blob sha256:3b36e36d988def39cf035c9a82076bbff95861bd9092701516d937f7dff24a73\naccount-portal: marking blob sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\naccount-portal: marking blob sha256:283fbeeb0d74113ed38891d145bcbff04a96178d2f1e19bb9dc5564ebd1959ab\naccount-portal: marking blob sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\naccount-portal: marking blob sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4\naccount-portal: marking blob sha256:68ad5d9d0e66645895611177d50dfc46bf1bbf9cd1275e43da416af0d2b2c30a\naccount-portal: marking blob sha256:6fae118190fe88ba8487a3a2cfac7b1f2d922d313963cd81329fd37b2516ca3d\naccount-portal: marking blob sha256:9e14db6259dc2696e6fe816ddb106ec1810a4f39075695c66cbeccd58f54aad1\nand at the end I get the recap like this:\n12530 blobs marked, 0 blobs eligible for deletion\nSo according to the output no images were removed from my registry. I confirmed by running df -h and yes I still using the same amount of disk space.\nSo, I don't know what I'm doing wrong, I've followed all the instructions in the docker documentation about the Garbage Collector I don't know what do to do get space back, please help. . ",
    "cybercom-tuomo": "@messyhead,\nI also got 403 responses for the DELETE requests. Turns out I was making the requests through the registry-frontend, which silently overwrote the config variable that should have enabled the DELETE HTTP verb.\nThe solution was to speak to the registry API directly instead of through the frontend. (ie. port 5000 instead of 80 or 443.). ",
    "ggop": "I followed the steps in 224280919 and 224325910. However it ends up deleting all tags for a repository instead of just one. Garbage collection removes some unused layers as expected and the next push with a new set of tags only shows the new tags set, not the ones from earlier. Furthermore, after I push a image with newer tags, they can't be listed. Let me illustrate, please let me know what I'm doing something wrong here.\n/ # /bin/registry --version\n/bin/registry github.com/docker/distribution v2.6.1\n```\n$ curl -s  http://example.com:5000/v2/myrepo/tags/list | python -m json.tool\n{\n    \"name\": \"myrepo\",\n    \"tags\": [\n        \"latest\",\n        \"2017-05-16-19-02-latest\",\n        \"2017-05-16-19-04-latest\"\n    ]\n}\n$ curl -s -H \"Accept: application/vnd.docker.distribution.manifest.v2+json\" -D - \\\n  -o /dev/null http://example.com:5000/v2/myrepo/manifests/2017-05-16-19-02-latest \nHTTP/1.1 200 OK\nContent-Length: 1970\nContent-Type: application/vnd.docker.distribution.manifest.v2+json\nDocker-Content-Digest: sha256:cb5ca4aa9346b738fbaf2a18711a301b23d452293b1526407b6bd5213dc7a58b\nDocker-Distribution-Api-Version: registry/2.0\nEtag: \"sha256:cb5ca4aa9346b738fbaf2a18711a301b23d452293b1526407b6bd5213dc7a58b\"\nX-Content-Type-Options: nosniff\nDate: Wed, 17 May 2017 02:24:06 GMT\n$ curl -vs -H \"Accept: application/vnd.docker.distribution.manifest.v2+json\" -X DELETE \\\n  http://example.com:5000/v2/myrepo/manifests/sha256:cb5ca4aa9346b738fbaf2a18711a301b23d452293b1526407b6bd5213dc7a58b\n   Trying 10.10.10.10...\n TCP_NODELAY set\n* Connected to example.com (10.10.10.10) port 5000 (#0)\n\nDELETE /v2/myrepo/manifests/sha256:cb5ca4aa9346b738fbaf2a18711a301b23d452293b1526407b6bd5213dc7a58b HTTP/1.1\nHost: example.com:5000\nUser-Agent: curl/7.53.1\nAccept: application/vnd.docker.distribution.manifest.v2+json\n< HTTP/1.1 202 Accepted\n< Docker-Distribution-Api-Version: registry/2.0\n< X-Content-Type-Options: nosniff\n< Date: Wed, 17 May 2017 02:24:32 GMT\n< Content-Length: 0\n< Content-Type: text/plain; charset=utf-8\n< \n* Connection #0 to host example.com left intact\n\nALL tags are gone, was expecting to see the remaining two\n$ curl -s http://example.com:5000/v2/myrepo/tags/list| python -m json.tool\n{\n    \"name\": \"myrepo\",\n    \"tags\": null\n}\n```\nNow push a new image with some tags. The tags are listed but can't be queried.\n```\n$ curl -s  http://example.com:5000/v2/myrepo/tags/list| python -m json.tool\n{\n    \"name\": \"myrepo\",\n    \"tags\": [\n        \"latest\",\n        \"2017-05-16-19-39-latest\"\n    ]\n}\n$ curl -s -H \"Accept: application/vnd.docker.distribution.manifest.v2+json\" \\\n  -D - -o /dev/null http://example.com:5000/v2/myrepo/manifests/2017-05-16-19-39-latest\nHTTP/1.1 404 Not Found\nContent-Type: application/json; charset=utf-8\nDocker-Distribution-Api-Version: registry/2.0\nX-Content-Type-Options: nosniff\nDate: Wed, 17 May 2017 03:04:48 GMT\nContent-Length: 184\n```\n. An addendum to my earlier post, two points:\n- The entire process from my previous note worked flawlessly with registry:2.5.1 (in the last step I am able to fetch the digest for the newly uploaded images)\n- the reason 3 tags disappeared when I deleted one digest was that they were all referring to the same image. My understanding earlier was that images hang around till all relevant tags are deleted. Turns out it is the other way around, tags go away when the image is deleted.\nAnyway it's fine now and I will use 2.5.1 for now. ",
    "erikbsap": "what kind of b.s. is this again? this is a basic function of any tool.. ",
    "tarasinf": "@sergey-tkalych How to set up the \nstorage:\n    delete:\n        enabled: true \nif it is gitlab.com?. @CodeJuan can you help with deleting? \nHow to get token with delete rights? As I understand, we need to set sope:pull, push,<some-delete-keyword>, is it? . ",
    "ulm0": "@tarasinf gitlab.com is managed by gitlab, they have their settings for the registry. To delete an image simply get to the registry tab in your repo and there's a delete icon in the tag.. ",
    "ykyuen": "This oneliner command may help\nhttps://gist.github.com/jaytaylor/86d5efaddda926a25fa68c263830dac1#gistcomment-2662407. ",
    "jgangemi": "i've seen this happen if the X-Registry-Auth header is missing from the request, are you sure that's getting through?\n. ",
    "Trefex": "@dmp42\nHere it is, to me it doesn't look very promising\n172.17.42.1 - - [10/Apr/2015:19:45:20 +0000] \"GET /v2/ HTTP/1.0\" 404 233 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [10/Apr/2015:19:45:20 +0000] \"GET /v1/_ping HTTP/1.0\" 200 1517 \"-\" \"Go 1.1 package http\"\n172.17.42.1 - - [10/Apr/2015:19:45:20 +0000] \"GET /v1/_ping HTTP/1.0\" 200 1517 \"-\" \"Go 1.1 package http\"\n10/Apr/2015:19:45:20 +0000 DEBUG: args = {'namespace': u'trefex', 'repository': u'toto'}\n172.17.42.1 - - [10/Apr/2015:19:45:20 +0000] \"PUT /v1/repositories/trefex/toto/ HTTP/1.0\" 200 2 \"-\" \"docker/1.5.0 go/go1.4.1 git-commit/a8a31ef kernel/3.2.0-61-generic os/linux arch/amd64\"\n10/Apr/2015:19:45:20 +0000 DEBUG: args = {'image_id': u'ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2'}\n10/Apr/2015:19:45:20 +0000 DEBUG: api_error: Image not found\n10/Apr/2015:19:45:20 +0000 DEBUG: args = {'image_id': u'4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125'}\n10/Apr/2015:19:45:20 +0000 DEBUG: api_error: Image not found\n172.17.42.1 - - [10/Apr/2015:19:45:20 +0000] \"GET /v1/images/4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125/json HTTP/1.0\" 404 28 \"-\" \"docker/1.5.0 go/go1.4.1 git-commit/a8a31ef kernel/3.2.0-61-generic os/linux arch/amd64\"\n172.17.42.1 - - [10/Apr/2015:19:45:20 +0000] \"GET /v1/images/ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2/json HTTP/1.0\" 404 28 \"-\" \"docker/1.5.0 go/go1.4.1 git-commit/a8a31ef kernel/3.2.0-61-generic os/linux arch/amd64\"\n10/Apr/2015:19:45:20 +0000 DEBUG: args = {'image_id': u'df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b'}\n10/Apr/2015:19:45:20 +0000 DEBUG: api_error: Image not found\n172.17.42.1 - - [10/Apr/2015:19:45:20 +0000] \"GET /v1/images/df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b/json HTTP/1.0\" 404 28 \"-\" \"docker/1.5.0 go/go1.4.1 git-commit/a8a31ef kernel/3.2.0-61-generic os/linux arch/amd64\"\n10/Apr/2015:19:45:20 +0000 DEBUG: args = {'image_id': u'511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158'}\n10/Apr/2015:19:45:20 +0000 DEBUG: api_error: Image is being uploaded, retry later\n172.17.42.1 - - [10/Apr/2015:19:45:20 +0000] \"GET /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.0\" 400 49 \"-\" \"docker/1.5.0 go/go1.4.1 git-commit/a8a31ef kernel/3.2.0-61-generic os/linux arch/amd64\"\n10/Apr/2015:19:45:20 +0000 DEBUG: args = {'image_id': u'511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158'}\n172.17.42.1 - - [10/Apr/2015:19:45:20 +0000] \"PUT /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.0\" 200 4 \"-\" \"docker/1.5.0 go/go1.4.1 git-commit/a8a31ef kernel/3.2.0-61-generic os/linux arch/amd64\"\n. @dmp42\n==> /var/log/nginx/access.log <==\n10.4.0.51 - - [10/Apr/2015:21:59:40 +0200] \"GET /v2/ HTTP/1.1\" 404 233 \"-\" \"Go 1.1 package http\"\n10.4.0.51 - - [10/Apr/2015:21:59:40 +0200] \"GET /v1/_ping HTTP/1.1\" 200 1517 \"-\" \"Go 1.1 package http\"\n10.4.0.51 - - [10/Apr/2015:21:59:40 +0200] \"GET /v1/_ping HTTP/1.1\" 200 1517 \"-\" \"Go 1.1 package http\"\n10.4.0.51 - - [10/Apr/2015:21:59:40 +0200] \"PUT /v1/repositories/trefex/toto/ HTTP/1.1\" 200 2 \"-\" \"docker/1.5.0 go/go1.4.1 git-commit/a8a31ef kernel/3.2.0-61-generic os/linux arch/amd64\"\n10.4.0.51 - - [10/Apr/2015:21:59:40 +0200] \"GET /v1/images/df7546f9f060a2268024c8a230d8639878585defcc1bc6f79d2728a13957871b/json HTTP/1.1\" 404 28 \"-\" \"docker/1.5.0 go/go1.4.1 git-commit/a8a31ef kernel/3.2.0-61-generic os/linux arch/amd64\"\n10.4.0.51 - - [10/Apr/2015:21:59:40 +0200] \"GET /v1/images/ea13149945cb6b1e746bf28032f02e9b5a793523481a0a18645fc77ad53c4ea2/json HTTP/1.1\" 404 28 \"-\" \"docker/1.5.0 go/go1.4.1 git-commit/a8a31ef kernel/3.2.0-61-generic os/linux arch/amd64\"\n10.4.0.51 - - [10/Apr/2015:21:59:40 +0200] \"GET /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1\" 400 49 \"-\" \"docker/1.5.0 go/go1.4.1 git-commit/a8a31ef kernel/3.2.0-61-generic os/linux arch/amd64\"\n10.4.0.51 - - [10/Apr/2015:21:59:40 +0200] \"GET /v1/images/4986bf8c15363d1c5d15512d5266f8777bfba4974ac56e3270e7760f6f0a8125/json HTTP/1.1\" 404 28 \"-\" \"docker/1.5.0 go/go1.4.1 git-commit/a8a31ef kernel/3.2.0-61-generic os/linux arch/amd64\"\n10.4.0.51 - - [10/Apr/2015:21:59:40 +0200] \"PUT /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/json HTTP/1.1\" 200 4 \"-\" \"docker/1.5.0 go/go1.4.1 git-commit/a8a31ef kernel/3.2.0-61-generic os/linux arch/amd64\"\n10.4.0.51 - - [10/Apr/2015:22:00:10 +0200] \"PUT /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/layer HTTP/1.1\" 400 0 \"-\" \"docker/1.5.0 go/go1.4.1 git-commit/a8a31ef kernel/3.2.0-61-generic os/linux arch/amd64\"\n. @dmp42\nHere some more verbose log for the PUT part after switching on debug in nginx\nWhat surprises me is the fact that it says connection closed prematurely by client, but I don't see why this could be.\nAlso, the first layer is only like 1 MB or something.\n2015/04/10 22:04:50 [debug] 5933#0: *16 http request line: \"PUT /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/layer HTTP/1.1\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 http uri: \"/v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/layer\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 http args: \"\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 http exten: \"\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 http process request header line\n2015/04/10 22:04:50 [debug] 5933#0: *16 http header: \"Host: docker-r3lab.uni.lu\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 http header: \"User-Agent: docker/1.5.0 go/go1.4.1 git-commit/a8a31ef kernel/3.2.0-61-generic os/linux arch/amd64\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 http header: \"Transfer-Encoding: chunked\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 http header: \"Authorization: Basic Og==\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 http header: \"Content-Type: application/octet-stream\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 http header: \"Accept-Encoding: gzip\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 http header: \"Client-ip: 10.79.2.28\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 http header: \"X-Forwarded-For: 10.79.2.28\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 http header: \"Via: https/1.1 aratis.uni.lu[FE80000000000000025056FFFE844D03] (ApacheTrafficServer/3.2.4)\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 http header done\n2015/04/10 22:04:50 [debug] 5933#0: *16 event timer del: 17: 1428696350062\n2015/04/10 22:04:50 [debug] 5933#0: *16 generic phase: 0\n2015/04/10 22:04:50 [debug] 5933#0: *16 rewrite phase: 1\n2015/04/10 22:04:50 [debug] 5933#0: *16 test location: \"/\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 test location: \"v1/_ping\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 test location: \"v1/search\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 using configuration \"/\"\n2015/04/10 22:04:50 [debug] 5933#0: *16 http cl:-1 max:0\n2015/04/10 22:04:50 [debug] 5933#0: *16 rewrite phase: 3\n2015/04/10 22:04:50 [debug] 5933#0: *16 post rewrite phase: 4\n2015/04/10 22:04:50 [debug] 5933#0: *16 generic phase: 5\n2015/04/10 22:04:50 [debug] 5933#0: *16 generic phase: 6\n2015/04/10 22:04:50 [debug] 5933#0: *16 generic phase: 7\n2015/04/10 22:04:50 [debug] 5933#0: *16 access phase: 8\n2015/04/10 22:04:50 [debug] 5933#0: *16 access phase: 9\n2015/04/10 22:04:50 [debug] 5933#0: *16 access phase: 10\n2015/04/10 22:04:50 [debug] 5933#0: *16 post access phase: 11\n2015/04/10 22:04:50 [debug] 5933#0: *16 posix_memalign: 00000000008A9450:4096 @16\n2015/04/10 22:04:50 [debug] 5933#0: *16 http request body chunked filter\n2015/04/10 22:04:50 [debug] 5933#0: *16 malloc: 00000000008AA460:8192\n2015/04/10 22:04:50 [debug] 5933#0: *16 http read client request body\n2015/04/10 22:04:50 [debug] 5933#0: *16 SSL_read: -1\n2015/04/10 22:04:50 [debug] 5933#0: *16 SSL_get_error: 2\n2015/04/10 22:04:50 [debug] 5933#0: *16 http client request body recv -2\n2015/04/10 22:04:50 [debug] 5933#0: *16 http client request body rest 3\n2015/04/10 22:04:50 [debug] 5933#0: *16 event timer add: 17: 60000:1428696350070\n2015/04/10 22:04:50 [debug] 5933#0: *16 http finalize request: -4, \"/v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/layer?\" a:1, c:2\n2015/04/10 22:04:50 [debug] 5933#0: *16 http request count:2 blk:0\n2015/04/10 22:05:20 [debug] 5933#0: *16 post event 000000000086C240\n2015/04/10 22:05:20 [debug] 5933#0: *16 delete posted event 000000000086C240\n2015/04/10 22:05:20 [debug] 5933#0: *16 http run request: \"/v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/layer?\"\n2015/04/10 22:05:20 [debug] 5933#0: *16 http read client request body\n2015/04/10 22:05:20 [debug] 5933#0: *16 SSL_read: 0\n2015/04/10 22:05:20 [debug] 5933#0: *16 SSL_get_error: 5\n2015/04/10 22:05:20 [debug] 5933#0: *16 peer shutdown SSL cleanly\n2015/04/10 22:05:20 [debug] 5933#0: *16 http client request body recv 0\n2015/04/10 22:05:20 [info] 5933#0: *16 client prematurely closed connection, client: 10.4.0.51, server: docker-r3lab-server.uni.lu, request: \"PUT /v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/layer HTTP/1.1\", host: \"docker-r3lab.uni.lu\"\n2015/04/10 22:05:20 [debug] 5933#0: *16 http finalize request: 400, \"/v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/layer?\" a:1, c:1\n2015/04/10 22:05:20 [debug] 5933#0: *16 http terminate request count:1\n2015/04/10 22:05:20 [debug] 5933#0: *16 http terminate cleanup count:1 blk:0\n2015/04/10 22:05:20 [debug] 5933#0: *16 http posted request: \"/v1/images/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/layer?\"\n2015/04/10 22:05:20 [debug] 5933#0: *16 http terminate handler count:1\n2015/04/10 22:05:20 [debug] 5933#0: *16 http request count:1 blk:0\n2015/04/10 22:05:20 [debug] 5933#0: *16 http close request\n2015/04/10 22:05:20 [debug] 5933#0: *16 http log handler\n2015/04/10 22:05:20 [debug] 5933#0: *16 free: 00000000008AA460\n2015/04/10 22:05:20 [debug] 5933#0: *16 free: 00000000008A13B0, unused: 4\n2015/04/10 22:05:20 [debug] 5933#0: *16 free: 00000000008A9450, unused: 2293\n2015/04/10 22:05:20 [debug] 5933#0: *16 close http connection: 17\n2015/04/10 22:05:20 [debug] 5933#0: *16 SSL_shutdown: 1\n2015/04/10 22:05:20 [debug] 5933#0: *16 event timer del: 17: 1428696350070\n2015/04/10 22:05:20 [debug] 5933#0: *16 reusable connection: 0\n2015/04/10 22:05:20 [debug] 5933#0: *16 free: 0000000000895740\n2015/04/10 22:05:20 [debug] 5933#0: *16 free: 0000000000893ED0, unused: 0\n2015/04/10 22:05:20 [debug] 5933#0: *16 free: 0000000000893FE0, unused: 72\n2015/04/10 22:05:54 [debug] 5933#0: *1 event timer del: 11: 1428696354803\n2015/04/10 22:05:54 [debug] 5933#0: *1 http keepalive handler\n2015/04/10 22:05:54 [debug] 5933#0: *1 close http connection: 11\n2015/04/10 22:05:54 [debug] 5933#0: *1 SSL_shutdown: 1\n2015/04/10 22:05:54 [debug] 5933#0: *1 reusable connection: 0\n2015/04/10 22:05:54 [debug] 5933#0: *1 free: 0000000000000000\n2015/04/10 22:05:54 [debug] 5933#0: *1 free: 0000000000000000\n2015/04/10 22:05:54 [debug] 5933#0: *1 free: 0000000000839E40, unused: 0\n2015/04/10 22:05:54 [debug] 5933#0: *1 free: 000000000086A060, unused: 56\n2015/04/10 22:05:54 [debug] 5933#0: *1 free: 0000000000842400, unused: 144\n2015/04/10 22:05:54 [debug] 5933#0: *3 event timer del: 12: 1428696354958\n2015/04/10 22:05:54 [debug] 5933#0: *3 http keepalive handler\n2015/04/10 22:05:54 [debug] 5933#0: *3 close http connection: 12\n2015/04/10 22:05:54 [debug] 5933#0: *3 SSL_shutdown: 1\n2015/04/10 22:05:54 [debug] 5933#0: *3 reusable connection: 0\n2015/04/10 22:05:54 [debug] 5933#0: *3 free: 0000000000000000\n2015/04/10 22:05:54 [debug] 5933#0: *3 free: 0000000000000000\n2015/04/10 22:05:54 [debug] 5933#0: *3 free: 00000000008431D0, unused: 0\n2015/04/10 22:05:54 [debug] 5933#0: *3 free: 00000000008A2B00, unused: 56\n2015/04/10 22:05:54 [debug] 5933#0: *3 free: 00000000008A2E70, unused: 144\n2015/04/10 22:05:54 [debug] 5933#0: *5 event timer del: 13: 1428696354963\n2015/04/10 22:05:54 [debug] 5933#0: *5 http keepalive handler\n2015/04/10 22:05:54 [debug] 5933#0: *5 close http connection: 13\n2015/04/10 22:05:54 [debug] 5933#0: *5 SSL_shutdown: 1\n2015/04/10 22:05:54 [debug] 5933#0: *5 reusable connection: 0\n2015/04/10 22:05:54 [debug] 5933#0: *5 free: 0000000000000000\n2015/04/10 22:05:54 [debug] 5933#0: *5 free: 0000000000000000\n2015/04/10 22:05:54 [debug] 5933#0: *5 free: 0000000000848EC0, unused: 0\n2015/04/10 22:05:54 [debug] 5933#0: *5 free: 0000000000848FD0, unused: 56\n2015/04/10 22:05:54 [debug] 5933#0: *5 free: 00000000008A23C0, unused: 144\n2015/04/10 22:05:54 [debug] 5933#0: *7 event timer del: 14: 1428696354966\n2015/04/10 22:05:54 [debug] 5933#0: *7 http keepalive handler\n2015/04/10 22:05:54 [debug] 5933#0: *7 close http connection: 14\n2015/04/10 22:05:54 [debug] 5933#0: *7 SSL_shutdown: 1\n2015/04/10 22:05:54 [debug] 5933#0: *7 reusable connection: 0\n2015/04/10 22:05:54 [debug] 5933#0: *7 free: 0000000000000000\n2015/04/10 22:05:54 [debug] 5933#0: *7 free: 0000000000000000\n2015/04/10 22:05:54 [debug] 5933#0: *7 free: 00000000008A6FA0, unused: 0\n2015/04/10 22:05:54 [debug] 5933#0: *7 free: 00000000008A70B0, unused: 32\n2015/04/10 22:05:54 [debug] 5933#0: *7 free: 00000000008A78F0, unused: 144\n2015/04/10 22:05:54 [debug] 5933#0: *12 event timer del: 15: 1428696354974\n2015/04/10 22:05:54 [debug] 5933#0: *12 http keepalive handler\n2015/04/10 22:05:54 [debug] 5933#0: *12 close http connection: 15\n2015/04/10 22:05:54 [debug] 5933#0: *12 SSL_shutdown: 1\n2015/04/10 22:05:54 [debug] 5933#0: *12 reusable connection: 0\n2015/04/10 22:05:54 [debug] 5933#0: *12 free: 0000000000000000\n2015/04/10 22:05:54 [debug] 5933#0: *12 free: 0000000000000000\n2015/04/10 22:05:54 [debug] 5933#0: *12 free: 00000000008A74E0, unused: 0\n2015/04/10 22:05:54 [debug] 5933#0: *12 free: 00000000008A75F0, unused: 56\n2015/04/10 22:05:54 [debug] 5933#0: *12 free: 00000000008A7700, unused: 144\n2015/04/10 22:05:55 [debug] 5933#0: *14 event timer del: 16: 1428696355041\n2015/04/10 22:05:55 [debug] 5933#0: *14 http keepalive handler\n2015/04/10 22:05:55 [debug] 5933#0: *14 close http connection: 16\n2015/04/10 22:05:55 [debug] 5933#0: *14 SSL_shutdown: 1\n2015/04/10 22:05:55 [debug] 5933#0: *14 reusable connection: 0\n2015/04/10 22:05:55 [debug] 5933#0: *14 free: 0000000000000000\n2015/04/10 22:05:55 [debug] 5933#0: *14 free: 0000000000000000\n2015/04/10 22:05:55 [debug] 5933#0: *14 free: 00000000008933E0, unused: 0\n2015/04/10 22:05:55 [debug] 5933#0: *14 free: 0000000000893BD0, unused: 32\n2015/04/10 22:05:55 [debug] 5933#0: *14 free: 0000000000897A10, unused: 144\n. The error seems to have come from our proxy indeed.\n. @dmg3 I won't be able to be super helpful, but I did debug this by not using the proxy, which means changing the DNS, opening ports temporarily internally and so on.\nWithout proxy, everything was fine, with the proxy, all hell broke loose.\n@luxifr I'm not sure what was done on the proxy side, because I'm not in control of that component :(\n. ",
    "dmg3": "@Trefex I'm experiencing a similar issue, how did you determine that it was your proxy at fault?\n. ",
    "luxifr": "I'd also be interested in the details of the resolution here\n. ",
    "moxiegirl": "LGTM --- Launch\n. https://docs.docker.com/registry/overview\n--> sadly the redirect is not working if you want to wait Ben says he has a fix\n. ",
    "RichardScothern": "Looks good.  Thanks @ayumi \n. I see the following in your logs.  Do you have permissions for that\ndirectory?  Can you touch a file there with non-root?\n> IOError: [Errno 13] Permission denied: './registry.setup_database.lock'\nOn Tue, May 12, 2015 at 4:03 AM, joda70 notifications@github.com wrote:\n\ndocker run -p 80:5000 -v /etc/group:/etc/group -v /etc/passwd:/etc/passwd\n--user registry:registry -v /opt/docker-registry:/reg -e\nSTORAGE_PATH=/reg/data -e SEARCH_BACKEND=sqlalchemy -e\nSQLALCHEMY_INDEX_DATABASE=sqlite:////reg/docker-registry.db -e\nLOGLEVEL=debug -e DEBUG=true -e STANDALONE=true -e SETTINGS_FLAVOR=local -e\nINDEX_ENDPOINT=http://docker.cloud.cnaf.infn.it --name basic_registry\nregistry\n[2015-05-12 11:02:09 +0000] [1] [INFO] Starting gunicorn 19.1.1\n[2015-05-12 11:02:09 +0000] [1] [INFO] Listening at: http://0.0.0.0:5000\n(1)\n[2015-05-12 11:02:09 +0000] [1] [INFO] Using worker: gevent\n[2015-05-12 11:02:09 +0000] [13] [INFO] Booting worker with pid: 13\n[2015-05-12 11:02:09 +0000] [14] [INFO] Booting worker with pid: 14\n[2015-05-12 11:02:09 +0000] [15] [INFO] Booting worker with pid: 15\n[2015-05-12 11:02:10 +0000] [16] [INFO] Booting worker with pid: 16\n12/May/2015:11:02:10 +0000 WARNING: Cache storage disabled!\n12/May/2015:11:02:10 +0000 WARNING: Cache storage disabled!\n12/May/2015:11:02:10 +0000 WARNING: LRU cache disabled!\n12/May/2015:11:02:10 +0000 WARNING: LRU cache disabled!\n12/May/2015:11:02:10 +0000 WARNING: Cache storage disabled!\n12/May/2015:11:02:10 +0000 WARNING: LRU cache disabled!\n12/May/2015:11:02:10 +0000 WARNING: Cache storage disabled!\n12/May/2015:11:02:10 +0000 WARNING: LRU cache disabled!\n12/May/2015:11:02:10 +0000 DEBUG: Will return\ndocker-registry.drivers.file.Storage\n12/May/2015:11:02:10 +0000 DEBUG: Will return\ndocker-registry.drivers.file.Storage\n12/May/2015:11:02:10 +0000 DEBUG: Will return\ndocker-registry.drivers.file.Storage\n12/May/2015:11:02:10 +0000 DEBUG: Will return\ndocker-registry.drivers.file.Storage\n[2015-05-12 11:02:10 +0000] [14] [ERROR] Exception in worker process:\nTraceback (most recent call last):\nFile \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line\n507, in spawn_worker\nworker.init_process()\nFile \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\",\nline 193, in init_process\nsuper(GeventWorker, self).init_process()\nFile \"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\",\nline 114, in init_process\nself.wsgi = self.app.wsgi()\nFile \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line\n66, in wsgi\nself.callable = self.load()\nFile \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\",\nline 65, in load\nreturn self.load_wsgiapp()\nFile \"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\",\nline 52, in load_wsgiapp\nreturn util.import_app(self.app_uri)\nFile \"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356,\nin import_app\nimport(module)\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\",\nline 27, in\nfrom .search import * # noqa\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\",\nline 14, in\nINDEX = index.load(cfg.search_backend.lower())\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/\ninit.py\", line 82, in load\nreturn db.SQLAlchemyIndex()\nFile\n\"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\",\nline 86, in init\nself.\nsetup_database() File\n\"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line\n325, in wrapper lock_file = open(lock_path, 'w') IOError: [Errno 13]\nPermission denied: './registry.setup_database.lock' Traceback (most recent\ncall last): File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in\nspawn_worker worker.init_process() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line\n193, in init_process super(GeventWorker, self).init_process() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line\n114, in init_process self.wsgi = self.app.wsgi() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in\nwsgi self.callable = self.load() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65,\nin load return self.load_wsgiapp() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52,\nin load_wsgiapp return util.import_app(self.app_uri) File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in\nimport_app __import(module)\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\",\nline 27, in\nfrom .search import * # noqa\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\",\nline 14, in\nINDEX = index.load(cfg.search_backend.lower())\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/\ninit.py\", line 82, in load\nreturn db.SQLAlchemyIndex()\nFile\n\"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\",\nline 86, in init\nself.\nsetup_database() File\n\"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line\n325, in wrapper lock_file = open(lock_path, 'w') IOError: [Errno 13]\nPermission denied: './registry.setup_database.lock' [2015-05-12 11:02:10\n+0000] [16] [ERROR] Exception in worker process: Traceback (most recent\ncall last): File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in\nspawn_worker worker.init_process() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line\n193, in init_process super(GeventWorker, self).init_process() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line\n114, in init_process self.wsgi = self.app.wsgi() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in\nwsgi self.callable = self.load() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65,\nin load return self.load_wsgiapp() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52,\nin load_wsgiapp return util.import_app(self.app_uri) File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in\nimport_app __import(module)\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\",\nline 27, in\nfrom .search import * # noqa\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\",\nline 14, in\nINDEX = index.load(cfg.search_backend.lower())\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/\ninit.py\", line 82, in load\nreturn db.SQLAlchemyIndex()\nFile\n\"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\",\nline 86, in init\nself.\nsetup_database() File\n\"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line\n325, in wrapper lock_file = open(lock_path, 'w') IOError: [Errno 13]\nPermission denied: './registry.setup_database.lock' Traceback (most recent\ncall last): File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in\nspawn_worker worker.init_process() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line\n193, in init_process super(GeventWorker, self).init_process() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line\n114, in init_process self.wsgi = self.app.wsgi() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in\nwsgi self.callable = self.load() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65,\nin load return self.load_wsgiapp() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52,\nin load_wsgiapp return util.import_app(self.app_uri) File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in\nimport_app __import(module)\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\",\nline 27, in\nfrom .search import * # noqa\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\",\nline 14, in\nINDEX = index.load(cfg.search_backend.lower())\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/\ninit.py\", line 82, in load\nreturn db.SQLAlchemyIndex()\nFile\n\"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\",\nline 86, in init\nself.\nsetup_database() File\n\"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line\n325, in wrapper lock_file = open(lock_path, 'w') IOError: [Errno 13]\nPermission denied: './registry.setup_database.lock' [2015-05-12 11:02:10\n+0000] [14] [INFO] Worker exiting (pid: 14) [2015-05-12 11:02:10 +0000]\n[16] [INFO] Worker exiting (pid: 16) [2015-05-12 11:02:10 +0000] [13]\n[ERROR] Exception in worker process: Traceback (most recent call last):\nFile \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line\n507, in spawn_worker worker.init_process() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line\n193, in init_process super(GeventWorker, self).init_process() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line\n114, in init_process self.wsgi = self.app.wsgi() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in\nwsgi self.callable = self.load() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65,\nin load return self.load_wsgiapp() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52,\nin load_wsgiapp return util.import_app(self.app_uri) File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in\nimport_app __import(module)\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\",\nline 27, in\nfrom .search import * # noqa\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\",\nline 14, in\nINDEX = index.load(cfg.search_backend.lower())\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/\ninit.py\", line 82, in load\nreturn db.SQLAlchemyIndex()\nFile\n\"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\",\nline 86, in init\nself.\nsetup_database() File\n\"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line\n325, in wrapper lock_file = open(lock_path, 'w') IOError: [Errno 13]\nPermission denied: './registry.setup_database.lock' Traceback (most recent\ncall last): File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in\nspawn_worker worker.init_process() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line\n193, in init_process super(GeventWorker, self).init_process() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line\n114, in init_process self.wsgi = self.app.wsgi() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in\nwsgi self.callable = self.load() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65,\nin load return self.load_wsgiapp() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52,\nin load_wsgiapp return util.import_app(self.app_uri) File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in\nimport_app __import(module)\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\",\nline 27, in\nfrom .search import * # noqa\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\",\nline 14, in\nINDEX = index.load(cfg.search_backend.lower())\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/\ninit.py\", line 82, in load\nreturn db.SQLAlchemyIndex()\nFile\n\"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\",\nline 86, in init\nself.\nsetup_database() File\n\"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line\n325, in wrapper lock_file = open(lock_path, 'w') IOError: [Errno 13]\nPermission denied: './registry.setup_database.lock' [2015-05-12 11:02:10\n+0000] [13] [INFO] Worker exiting (pid: 13) [2015-05-12 11:02:10 +0000]\n[15] [ERROR] Exception in worker process: Traceback (most recent call\nlast): File \"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\",\nline 507, in spawn_worker worker.init_process() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line\n193, in init_process super(GeventWorker, self).init_process() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line\n114, in init_process self.wsgi = self.app.wsgi() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in\nwsgi self.callable = self.load() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65,\nin load return self.load_wsgiapp() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52,\nin load_wsgiapp return util.import_app(self.app_uri) File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in\nimport_app __import(module)\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\",\nline 27, in\nfrom .search import * # noqa\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\",\nline 14, in\nINDEX = index.load(cfg.search_backend.lower())\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/\ninit.py\", line 82, in load\nreturn db.SQLAlchemyIndex()\nFile\n\"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\",\nline 86, in init\nself.\nsetup_database() File\n\"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\", line\n325, in wrapper lock_file = open(lock_path, 'w') IOError: [Errno 13]\nPermission denied: './registry.setup_database.lock' Traceback (most recent\ncall last): File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/arbiter.py\", line 507, in\nspawn_worker worker.init_process() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/ggevent.py\", line\n193, in init_process super(GeventWorker, self).init_process() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/workers/base.py\", line\n114, in init_process self.wsgi = self.app.wsgi() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/base.py\", line 66, in\nwsgi self.callable = self.load() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 65,\nin load return self.load_wsgiapp() File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/app/wsgiapp.py\", line 52,\nin load_wsgiapp return util.import_app(self.app_uri) File\n\"/usr/local/lib/python2.7/dist-packages/gunicorn/util.py\", line 356, in\nimport_app __import(module)\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/wsgi.py\",\nline 27, in\nfrom .search import * # noqa\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/search.py\",\nline 14, in\nINDEX = index.load(cfg.search_backend.lower())\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/\ninit.py\", line 82, in load\nreturn db.SQLAlchemyIndex()\nFile\n\"/usr/local/lib/python2.7/dist-packages/docker_registry/lib/index/db.py\",\nline 86, in init\nself._setup_database()\nFile \"/usr/local/lib/python2.7/dist-packages/docker_registry/toolkit.py\",\nline 325, in wrapper\nlock_file = open(lock_path, 'w')\nIOError: [Errno 13] Permission denied: './registry._setup_database.lock'\n[2015-05-12 11:02:10 +0000] [15] [INFO] Worker exiting (pid: 15)\n[2015-05-12 11:02:10 +0000] [1] [INFO] Shutting down: Master\n[2015-05-12 11:02:10 +0000] [1] [INFO] Reason: Worker failed to boot.\nIn the local volume I have:\nls -al docker-registry/data/\ntotal 4\ndrwxr-xr-x. 4 registry registry 38 11 mag 15.59 .\ndrwxr-xr-x. 3 registry registry 42 11 mag 15.59 ..\ndrwxr-xr-x. 7 registry registry 4096 11 mag 15.59 images\ndrwxr-xr-x. 3 registry registry 20 11 mag 15.59 repositories\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/999#issuecomment-101236545\n.\n. \n",
    "ayumi": "This URL looks to be fixed now (https://docs.docker.com/registry) so I'm closing this PR.\n. ",
    "seraphyong": "@dmp42 \nI have resolved this question,because I didn't run the image what I want to push.\nBut also thank you too.\n. ",
    "montgomery1944": "@dmp42, is there any update on this issue?\n. ",
    "pbecotte": "Nope, looks good.  Thanks for the quick response!\n. ",
    "lox": "I'm currently having the same issue.\n. ",
    "ebaxt": "Same problem here as well\n. ",
    "tnlin": "I got the exactly same problem\n. ",
    "DexterTheDragon": "Having this problem using docker 1.7.0\n. ",
    "newtondev": "Having the exact same problem.\n. ",
    "Master244": "Also having it atm\n. ",
    "haimari": "Same issue\n. ",
    "bengong2015": "I've checked the docker-registry's logs and find an error\nIntegrityError: (IntegrityError) UNIQUE constraint failed: repository.name u'INSERT INTO repository (name, description) VALUES (?, ?)' ('kubernetes/pause', '')\n. It caused by the SELinux, the docker daemon run with SELinux in default ('--selinux-enabled'), It would be fine if adding '--privileged=true' when running docker-registry container.\n. ",
    "griddle81": "I'm having this same issue. How do you delete and rebuild the search index?\n. ",
    "bydga": "This usually happenes when you delete the tag (from some docker registry ui). Seems like docker-registry is not updating its search index. Im digging into it and i will submit a inssu there\n. ",
    "javasparkle": "Just open the the registry db in SQLite browser and delete the entry that is causing the problem. This will ensure that the next subsequent push will be successful.\n. ",
    "driv": "How can I connect to the database? Or how do I delete the index?\n. ",
    "sjfloat": "Having similar trouble. I'd also like to know.\nthanks\n. ",
    "ventres": "I am also seeing this error in my automated build repository: https://registry.hub.docker.com/u/foodlogiq/api/build_id/37077/code/bklej8f97kynzupskwfysmq/ \nI will give it some time and try it again to see if I also see intermittent success.\n. ",
    "benhamner": "@dmp42 I'm also hitting this error with one of my automated build repositories: https://registry.hub.docker.com/u/kaggle/r/build_id/53243/code/bvep6hoqrvyqyy69zmr2cx8/ \n. ",
    "yagey": "yes, I'm logged in to my docker hub repo :)\n. sorry, wrong github I think.\n. hopefully this was a better repo to comment on:\nhttps://github.com/docker/docker/issues/13143\n. ",
    "dexterddit": "Thanks I`ll have a look \n. ",
    "eaoliver": "I'm seeing this problem with the latest Docker registry image.  It seems to have broken overnight.\n. @dmp42 I have tested both registry:2.0.1 and registry:latest.\n. I should add that a curl of the v1/_ping does work.  Returns an empty object.\ncurl -XGET localhost:5000/v1/_ping\n{}\n. I am running Docker version 1.6.2, build 7c8fca2 on boot2docker on a Mac.\nFollowing is the output from docker.log for:\ndocker push localhost:5000/debian\ntime=\"2015-05-23T00:38:35Z\" level=debug msg=\"Calling POST /images/{name:.}/push\"\ntime=\"2015-05-23T00:38:35Z\" level=info msg=\"POST /v1.18/images/localhost:5000/debian/push?tag=\"\ntime=\"2015-05-23T00:38:35Z\" level=info msg=\"+job push(localhost:5000/debian)\"\ntime=\"2015-05-23T00:38:35Z\" level=info msg=\"+job resolve_repository(localhost:5000/debian)\"\ntime=\"2015-05-23T00:38:35Z\" level=info msg=\"-job resolve_repository(localhost:5000/debian) = OK (0)\"\ntime=\"2015-05-23T00:38:35Z\" level=debug msg=\"pinging registry endpoint https://localhost:5000/v0/\"\ntime=\"2015-05-23T00:38:35Z\" level=debug msg=\"attempting v2 ping for registry endpoint https://localhost:5000/v2/\"\ntime=\"2015-05-23T00:38:35Z\" level=debug msg=\"https://localhost:5000/v2/ -- HEADERS: map[User-Agent:[docker/1.6.2 go/go1.4.2 git-commit/7c8fca2 kernel/4.0.3-boot2docker os/linux arch/amd64]]\"\ntime=\"2015-05-23T00:38:35Z\" level=debug msg=\"attempting v1 ping for registry endpoint https://localhost:5000/v1/\"\ntime=\"2015-05-23T00:38:35Z\" level=debug msg=\"https://localhost:5000/v1/_ping -- HEADERS: map[User-Agent:[docker/1.6.2 go/go1.4.2 git-commit/7c8fca2 kernel/4.0.3-boot2docker os/linux arch/amd64]]\"\ntime=\"2015-05-23T00:38:35Z\" level=debug msg=\"Error from registry \\\"https://localhost:5000/v0/\\\" marked as insecure: unable to ping registry endpoint https://localhost:5000/v0/\\nv2 ping attempt failed with error: Get https://localhost:5000/v2/: dial tcp 127.0.0.1:5000: connection refused\\n v1 ping attempt failed with error: Get https://localhost:5000/v1/_ping: dial tcp 127.0.0.1:5000: connection refused. Insecurely falling back to HTTP\"\ntime=\"2015-05-23T00:38:35Z\" level=debug msg=\"attempting v2 ping for registry endpoint http://localhost:5000/v2/\"\ntime=\"2015-05-23T00:38:35Z\" level=debug msg=\"http://localhost:5000/v2/ -- HEADERS: map[User-Agent:[docker/1.6.2 go/go1.4.2 git-commit/7c8fca2 kernel/4.0.3-boot2docker os/linux arch/amd64]]\"\ntime=\"2015-05-23T00:38:35Z\" level=debug msg=\"attempting v1 ping for registry endpoint http://localhost:5000/v1/\"\ntime=\"2015-05-23T00:38:35Z\" level=debug msg=\"http://localhost:5000/v1/_ping -- HEADERS: map[User-Agent:[docker/1.6.2 go/go1.4.2 git-commit/7c8fca2 kernel/4.0.3-boot2docker os/linux arch/amd64]]\"\ninvalid registry endpoint \"http://localhost:5000/v0/\". HTTPS attempt: unable to ping registry endpoint https://localhost:5000/v0/\nv2 ping attempt failed with error: Get https://localhost:5000/v2/: dial tcp 127.0.0.1:5000: connection refused\n v1 ping attempt failed with error: Get https://localhost:5000/v1/_ping: dial tcp 127.0.0.1:5000: connection refused. HTTP attempt: unable to ping registry endpoint http://localhost:5000/v0/\nv2 ping attempt failed with error: Get http://localhost:5000/v2/: dial tcp 127.0.0.1:5000: connection refused\n v1 ping attempt failed with error: Get http://localhost:5000/v1/_ping: dial tcp 127.0.0.1:5000: connection refused\ntime=\"2015-05-23T00:38:35Z\" level=info msg=\"-job push(localhost:5000/debian) = ERR (1)\"\ntime=\"2015-05-23T00:38:35Z\" level=error msg=\"Handler for POST /images/{name:.}/push returned error:  v1 ping attempt failed with error: Get http://localhost:5000/v1/_ping: dial tcp 127.0.0.1:5000: connection refused\"\ntime=\"2015-05-23T00:38:35Z\" level=error msg=\"HTTP Error: statusCode=500  v1 ping attempt failed with error: Get http://localhost:5000/v1/_ping: dial tcp 127.0.0.1:5000: connection refused\"\nNow that I see the log, I can confirm that 127.0.0.1 is not accessible from within the boot2docker.  \ndocker@boot2docker:/var/log$ curl -XGET  http://localhost:5000/v1/search\ncurl: (7) Failed connect to localhost:5000; Connection refused\nThe boot2docker routing table seems to be correct too:\ndocker@boot2docker:/var/log$ route\nKernel IP routing table\nDestination     Gateway         Genmask         Flags Metric Ref    Use Iface\ndefault         10.0.2.2        0.0.0.0         UG    1      0        0 eth0\n10.0.2.0        *               255.255.255.0   U     0      0        0 eth0\n127.0.0.1       *               255.255.255.255 UH    0      0        0 lo\n172.17.0.0      *               255.255.0.0     U     0      0        0 docker0\n192.168.59.0    *               255.255.255.0   U     0      0        0 eth1\nSince this is not an issue with the registry, closing.\n. @dmp42 Thanks.\n. ",
    "arnos": "I'll test it out on monday\nOn 22 May 2015 6:49 pm, \"Olivier Gambier\" notifications@github.com wrote:\n\n@eaoliver https://github.com/eaoliver registry:latest is still pointing\nto the python registry, which is this one repository here.\nregistry:2 is now the recommended way to go, and lives in\nhttps://github.com/docker/distribution\n@eaoliver https://github.com/eaoliver can you clarify what's wrong in\nyour case?\n@arnos https://github.com/arnos problem is that his registry is\nprobably not listening on the public interface (dial tcp 172.27.25.59:5000:\nconnection refused)\nYours might very well be different, but I have no way to know without logs.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/1005#issuecomment-104793166\n.\n. Sorry crazy week last week\n\nstarting from a clean vmdocker\nI've followed @XiaokunHou advice but I am still getting the same error when running \"docker run -d -p 5000:5000 registry:latest\"\nstill getting the same error it's as if it's not even trying to ping the http port and just goes for https\nFATA[0004] Error response from daemon: v1 ping attempt failed with error: Get https://172.27.25.59:5000/v1/_ping: EOF. If this private registry supports only HTTP or HTTPS with an unknown CA certificate, please add `--insecure-registry 172.27.25.59:5000` to the daemon's arguments. In the case of HTTPS, if you have access to the registry's CA certificate, no need for the flag; simply place the CA certificate at /etc/docker/certs.d/172.27.25.59:5000/ca.crt\nand yes curl -1 http://172.27.25.59:5000/v1/_ping works fine and produces the expected result\n```\nHTTP/1.1 200 OK\nServer: gunicorn/19.1.1\nDate: Mon, 01 Jun 2015 13:35:05 GMT\nConnection: keep-alive\nX-Docker-Registry-Config: dev\nExpires: -1\nX-Docker-Registry-Standalone: True\nPragma: no-cache\nCache-Control: no-cache\nContent-Type: application/json\nContent-Length: 1540\n{\"host\": [\"Linux\", \"57b42740ea63\", \"3.19.0-16-generic\", \"#16-Ubuntu SMP Thu Apr 30 16:09:58 UTC 2015\", \"x86_64\", \"x86_64\"], \"launch\": [\"/usr/local/bin/gunicorn\", \"--access-logfile\", \"-\", \"--error-logfile\", \"-\", \"--max-requests\", \"100\", \"-k\", \"gevent\", \"--graceful-timeout\", \"3600\", \"-t\", \"3600\", \"-w\", \"4\", \"-b\", \"0.0.0.0:5000\", \"--reload\", \"docker_registry.wsgi:application\"], \"versions\": {\"M2Crypto.m2xmlrpclib\": \"0.22\", \"SocketServer\": \"0.4\", \"argparse\": \"1.1\", \"backports.lzma\": \"0.0.3\", \"blinker\": \"1.3\", \"cPickle\": \"1.71\", \"cgi\": \"2.6\", \"ctypes\": \"1.1.0\", \"decimal\": \"1.70\", \"distutils\": \"2.7.6\", \"docker_registry.app\": \"0.9.1\", \"docker_registry.core\": \"2.0.3\", \"docker_registry.server\": \"0.9.1\", \"email\": \"4.0.3\", \"flask\": \"0.10.1\", \"gevent\": \"1.0.1\", \"greenlet\": \"0.4.7\", \"gunicorn\": \"19.1.1\", \"gunicorn.arbiter\": \"19.1.1\", \"gunicorn.config\": \"19.1.1\", \"gunicorn.six\": \"1.2.0\", \"jinja2\": \"2.7.3\", \"json\": \"2.0.9\", \"logging\": \"0.5.1.2\", \"parser\": \"0.5\", \"pickle\": \"$Revision: 72223 $\", \"platform\": \"1.0.7\", \"pyexpat\": \"2.7.6\", \"python\": \"2.7.6 (default, Mar 22 2014, 22:59:56) \\n[GCC 4.8.2]\", \"re\": \"2.2.1\", \"redis\": \"2.10.3\", \"requests\": \"2.3.0\", \"requests.packages.chardet\": \"2.2.1\", \"requests.packages.urllib3\": \"dev\", \"requests.packages.urllib3.packages.six\": \"1.2.0\", \"requests.utils\": \"2.3.0\", \"simplejson\": \"3.6.2\", \"sqlalchemy\": \"0.9.4\", \"tarfile\": \"$Revision: 85213 $\", \"urllib\": \"1.17\", \"urllib2\": \"2.7\", \"werkzeug\": \"0.10.4\", \"xml.parsers.expat\": \"$Revision: 17640 $\", \"xmlrpclib\": \"1.0.1\", \"yaml\": \"3.11\", \"zlib\": \"1.0\"}}\n```\nrunning either curl -i https://localhost:5000/v2 or curl -i https://172.27.25.59:5000/v2 produces an error\ncurl: (35) Unknown SSL protocol error in connection to localhost:5000\nthe logs of the registry \n[2015-06-01 13:25:48 +0000] [1] [INFO] Starting gunicorn 19.1.1\n[2015-06-01 13:25:48 +0000] [1] [INFO] Listening at: http://0.0.0.0:5000 (1)\n[2015-06-01 13:25:48 +0000] [1] [INFO] Using worker: gevent\n[2015-06-01 13:25:48 +0000] [14] [INFO] Booting worker with pid: 14\n[2015-06-01 13:25:48 +0000] [15] [INFO] Booting worker with pid: 15\n[2015-06-01 13:25:48 +0000] [18] [INFO] Booting worker with pid: 18\n[2015-06-01 13:25:48 +0000] [19] [INFO] Booting worker with pid: 19\n01/Jun/2015:13:25:48 +0000 WARNING: Cache storage disabled!\n01/Jun/2015:13:25:48 +0000 WARNING: LRU cache disabled!\n01/Jun/2015:13:25:49 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n01/Jun/2015:13:25:49 +0000 WARNING: Cache storage disabled!\n01/Jun/2015:13:25:49 +0000 WARNING: LRU cache disabled!\n01/Jun/2015:13:25:49 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n01/Jun/2015:13:25:49 +0000 WARNING: Cache storage disabled!\n01/Jun/2015:13:25:49 +0000 WARNING: LRU cache disabled!\n01/Jun/2015:13:25:49 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n01/Jun/2015:13:25:49 +0000 WARNING: Cache storage disabled!\n01/Jun/2015:13:25:49 +0000 WARNING: LRU cache disabled!\n01/Jun/2015:13:25:49 +0000 DEBUG: Will return docker-registry.drivers.file.Storage\n01/Jun/2015:13:25:49 +0000 WARNING: Another process is creating the search database\n01/Jun/2015:13:25:49 +0000 WARNING: Another process is creating the search database\n01/Jun/2015:13:25:49 +0000 WARNING: Another process is creating the search database\n172.17.42.1 - - [01/Jun/2015:13:26:36 +0000] \"GET / HTTP/1.1\" 200 28 \"-\" \"curl/7.38.0\"\n172.17.42.1 - - [01/Jun/2015:13:26:52 +0000] \"GET /v2 HTTP/1.1\" 404 233 \"-\" \"curl/7.38.0\"\n172.27.25.59 - - [01/Jun/2015:13:29:47 +0000] \"GET /v1/_ping HTTP/1.1\" 200 1540 \"-\" \"curl/7.38.0\"\nrunning curl -i http://localhost:5000/v2 or curl -i http://172.27.25.59:5000/v2 produces a 404\n```\nHTTP/1.1 404 NOT FOUND\nServer: gunicorn/19.1.1\nDate: Mon, 01 Jun 2015 13:34:13 GMT\nConnection: keep-alive\nContent-Type: text/html\nContent-Length: 233\nHTML PUBLIC \"-//W3C//DTD HTML 3.2 Final//EN\"\n404 Not Found\nNot Found\nThe requested URL was not found on the server.  If you entered the URL manually please check your spelling and try again.\n```\n. I tried various changes in the /etc/defaults/docker file as well\n\nit doesn't seem to work with either of\n  --insecure-registry http://172.27.25.59:5000\n  --insecure-registry=172.27.25.59:5000\n  --insecure-registry 172.27.25.59:5000\n. right now the host and client are one and the same.\nOn Mon, Jun 1, 2015 at 11:03 AM, XiaokunHou notifications@github.com\nwrote:\n\nyou should add these lines in client docker machine, rather than the\nregistry host machine.\nhttp://stackoverflow.com/questions/27792969/using-private-registry-hosted-on-docker/30478338#30478338\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/1005#issuecomment-107559945\n.\n. \n",
    "dotNetDR": "In my CentOS Linux release 7.1.1503 (Core)\nThe following configuration is working.\nfile: /lib/systemd/system/docker.service text:\n```\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network.target docker.socket\nRequires=docker.socket\n[Service]\nEnvironmentFile=-/etc/sysconfig/docker\nExecStart=/usr/bin/docker -d $other_args -H fd://\nMountFlags=slave\nLimitNOFILE=1048576\nLimitNPROC=1048576\nLimitCORE=infinity\n[Install]\nWantedBy=multi-user.target\n```\ncheck here >> EnvironmentFile=-/etc/sysconfig/docker\ncheck here >> ExecStart=/usr/bin/docker -d $other_args -H fd://\n\nfile: /etc/sysconfig/docker  text:\n```\n/etc/sysconfig/docker\n\nOther arguments to pass to the docker daemon process\nThese will be parsed by the sysv initscript and appended\nto the arguments list passed to docker -d\nother_args=\"--insecure-registry yoururl\"\n```\nset registry address >> other_args=\"--insecure-registry yoururl\"\n\n``` text\nsystemctl start docker\ndocker version\nClient version: 1.7.0\nClient API version: 1.19\nGo version (client): go1.4.2\nGit commit (client): 0baf609\nOS/Arch (client): linux/amd64\nServer version: 1.7.0\nServer API version: 1.19\nGo version (server): go1.4.2\nGit commit (server): 0baf609\nOS/Arch (server): linux/amd64\n```\n. ",
    "wharsojo": "here's the steps I do using docker-machine to run docker private registry:\n~$ docker-machine create dev -d virtualbox\n ~$ docker-machine ssh dev\n docker@dev:~$\nadd host-name in \"/etc/host\":\ndocker@dev:~$ sudo vi /etc/hosts \n 127.0.0.1 localhub\nupdate profile env. variable \"EXTRA_ARGS\" in \"/var/lib/boot2docker/profile\"  add \"--insecure-registry localhub:5000\"\ndocker@dev:~$ sudo vi /var/lib/boot2docker/profile\n EXTRA_ARGS='\n --label provider=virtualbox\n --insecure-registry localhub:5000\n '\ncreate folder to host the images:\ndocker@dev:~$ sudo mkdir /mnt/sda1/registry\n docker@dev:~$ sudo chown docker:staff /mnt/sda1/registry\n docker@dev:~$ exit\nexit and back to my mac console & run private registry:\n~$ docker run -p 5000:5000 -v /mnt/sda1/registry:/tmp/registry -e GUNICORN_OPTS='[\"--preload\"]' --restart=always --name=registry registry\nopen another iterm, pull \"hello-world\", create another tag \"localhub:5000/hello-world\" and push it to private registry:\n~$ docker pull hello-world\n ~$ docker tag hello-world localhub:5000/hello-world\n ~$ docker push localhub:5000/hello-world\ntry to use hello-world from private registry:\n~$ docker run localhub:5000/hello-world\nscreenshot  (gif-animation):\n\nyour first comment in this issue mentioned: \nDOCKER_OPTS=--insecure-registry 172.27.25.59:5000\nit should be:\nDOCKER_OPTS=\"--insecure-registry 172.27.25.59:5000\"\n. ",
    "ZYNCMA": "\nyour first comment in this issue mentioned:\nDOCKER_OPTS=--insecure-registry 172.27.25.59:5000\nit should be:\nDOCKER_OPTS=\"--insecure-registry 172.27.25.59:5000\"\n\nI met the same problem.\nWithout quotation, parameters took no effect\nSimply add quotation fixed the issue.\n. ",
    "ozbillwang": "Thanks, @wharsojo \nYour solution works with docker toolbox. The only adjust is on this line in /var/lib/boot2docker/profile\n--insecure-registry 192.168.99.100:5000\n. ",
    "pengfei-xue": "this works you SHOULD add this opts at /var/lib/boot2docker/profile \nthanks, this really sucks.\n. @wharsojo  thanks\ntook two more hours for this issue.\n. @niclarcipretti  you can open the virtualbox client , double click the running vm, input your username/password, it should be ok to go.\n. ",
    "niclarcipretti": "Anyone knows how to add --insecure-registry in windows virtualized solution? I don't want to modify my VM files cause whenever I upgrade it, all will be lost. I think this should be parametrized in the init script (start.sh maybe?).\nCheers\n. ",
    "raghakk": "Setting Local insecure registry in docker along a proxy:\n1) in ubuntu add the following flag --insecure-registry IP:port under DOCKER_OPTS in file /etc/default/docker\n1.1) configure no_proxy env variable to bypass local IP/hostname/domainname...as proxy can throw a interactive msg ...like continue and this intermediate msg confuses docker client and finally timesout...[symptom observed: push done will not reach the regisrty service whose port is open at 5000]\n1.2) if domainname is configured...then don't forget to update /etc/hosts file if not using DNS.\n1.3) in /etc/default/docker set the env variables http_proxy and https_proxy...as it enables to download images from outside company hubs. format http_proxy=http://username:password@proxy:port\n2) restart the docker service...if installed as service, use sudo service docker restart\n3) restart the registry container [sudo docker run -p 5000:5000 registry:2 ]\n4) tag the required image using sudo docker tag imageid IP:port/imagename/tagname ifany\n5) push the image ...sudo docker push ip:port/imagename\n6) If u want to pull the image from another machine say B without TLS/SSL,then in B apply setps 1,1.1 and 2. If these changes are not done in machine B...pull will fail.\n. ",
    "prashantabkari": "I am facing the same issue, hence not opening a new issue.  Following are the details\nMaster Node on which the registry is installed \nOn the file /lib/systemd/system/docker.service\nEnvironmentFile=-/etc/sysconfig/docker\nEnvironmentFile=-/etc/sysconfig/docker-storage\nEnvironmentFile=-/etc/sysconfig/docker-network\nEnvironment=GOTRACEBACK=crash\nExecStart=/usr/bin/docker-current daemon \\\n          --exec-opt native.cgroupdriver=systemd \\\n          $OPTIONS \\\n          $DOCKER_STORAGE_OPTIONS \\\n          $DOCKER_NETWORK_OPTIONS \\\n          $ADD_REGISTRY \\\n          $BLOCK_REGISTRY \\\n          $INSECURE_REGISTRY\nthe file /etc/sysconfig/docker  has following contents\nOPTIONS='--selinux-enabled --log-driver=journald'\nif [ -z \"${DOCKER_CERT_PATH}\" ]; then\n    DOCKER_CERT_PATH=/etc/docker\nfi\nINSECURE_REGISTRY='--insecure-registry 10.143.219.59:5000'**\nWhen i try to do \ndocker pull 10.143.219.59:5000/hello-world \nIt fails. \nHow to setup an insecure registry?\nAlso the documentation in https://docs.docker.com/registry/deploying/  doesnt specify where exactly do we need to run these commands? On the registry host or the remote host? \n. ",
    "zrml": "@prashantabkari those commands are to run on the host supporting the registry you've just spun up.\nHowever I find that they ONLY work if you use \"localhost\". What I mean is:\n-I can only push & pull if I use localhost\n-if I use the hostname (fully dsn'd) inside the VPN I cannot push the image to the registry\n-if I use the ip address, again it's like the previous issue.\nThe errors hints at the fact that my client uses https which I have not told it to nor is it setup as such. \n`$ docker pull busybox\nUsing default tag: latest\nlatest: Pulling from library/busybox\n7520415ce762: Pull complete\nDigest: sha256:32f093055929dbc23dec4d03e09dfe971f5973a9ca5cf059cbfb644c206aa83f\nStatus: Downloaded newer image for busybox:latest\n$ docker images\nREPOSITORY             TAG                 IMAGE ID            CREATED             SIZE\ncache-sd               15                  f7c0c8a91c4d        6 days ago          1.92 GB\nbusybox                latest              00f017a8c2a6        3 weeks ago         1.11 MB\nregistry               2                   047218491f8c        3 weeks ago         33.2 MB\njjones028/apache-csp   latest              19402b7f7207        10 months ago       419 MB\n$ docker tag busybox ub1604rel1:5000/me:1\n$ docker images\nREPOSITORY             TAG                 IMAGE ID            CREATED             SIZE\ncache-sd               15                  f7c0c8a91c4d        6 days ago          1.92 GB\nbusybox                latest              00f017a8c2a6        3 weeks ago         1.11 MB\nub1604rel1:5000/me     1                   00f017a8c2a6        3 weeks ago         1.11 MB\nregistry               2                   047218491f8c        3 weeks ago         33.2 MB\njjones028/apache-csp   latest              19402b7f7207        10 months ago       419 MB\n$ docker push ub1604rel1:5000/me:1\nThe push refers to a repository [ub1604rel1:5000/me]\nGet https://ub1604rel1:5000/v1/_ping: http: server gave HTTP response to HTTPS client`\nPlease note -again, that IF I do the above steps with \"localhost\" vs the hostname or the ip address it works.\n. ",
    "d4rkd0s": "I am on Fedora 28 and the solution I found was changing /etc/sysconfig/docker:\nOPTIONS='--selinux-enabled --log-driver=journald --live-restore'\nto\nOPTIONS='--selinux-enabled --log-driver=journald --live-restore --insecure-registry 172.30.0.0/16'\nreplace 172.16.0.0/16 with whatever you are trying to add as insecure.\nKeep in mind other solutions reference DOCKER_OPTS which is no longer used, as least by how systemctl spins up my docker. I've installed with dnf install docker, and my /lib/systemd/system/docker.service contained the following:\n```\nExecStart=/usr/bin/dockerd-current \\\n          --add-runtime oci=/usr/libexec/docker/docker-runc-current \\\n          --default-runtime=oci \\\n          --authorization-plugin=rhel-push-plugin \\\n          --containerd /run/containerd.sock \\\n          --exec-opt native.cgroupdriver=systemd \\\n          --userland-proxy-path=/usr/libexec/docker/docker-proxy-current \\\n          --init-path=/usr/libexec/docker/docker-init-current \\\n          --seccomp-profile=/etc/docker/seccomp.json \\\n          $OPTIONS \\\n          $DOCKER_STORAGE_OPTIONS \\\n          $DOCKER_NETWORK_OPTIONS \\\n          $ADD_REGISTRY \\\n          $BLOCK_REGISTRY \\\n          $INSECURE_REGISTRY \\\n          $REGISTRIES\n```\nWhere you can see its OPTIONS that you'll want to change/add in your /etc/sysconfig/docker file instead of DOCKER_OPTS. ",
    "funkytaco": "Why move the thread to IRC... now we have no idea what the fix was\n. @dmp42 Thanks. I've asked on Gitter and IRC, and while people were helpful, I've had no luck there.\nI am using registry:2, but I'm confused as to where to put --insecure-registry 0.0.0.0:5000 (my OSX laptop, The remote Docker host on Centos 7, or the registry:2 Docker container on Ubuntu)... so it accepts my self-signed SSL certificate.\n. ls /etc/docker/certs.d\nls: cannot access /etc/docker/certs.d: No such file or directory\n[root@myhost tls]# cat /etc/redhat-release \nCentOS Linux release 7.0.1406 (Core)\n/etc/docker exists, though.\n. ",
    "skuzye": "Did I stumble on a bug or something? Does anyone using the feature have any tip?\n. ",
    "mpas": "Apologies for my possible dumb question, but does this mean that the registry cannot be used when using self signed certificates?\n. @dmp42 Could it be that i need to create /etc/docker/certs.d directory? The directory now only contains a key.json file. I guess that i need to do this on the server running the registry?\n. @dmp42 thanks for the tips, going to test this tomorrow! Does the placement of the .crt file on the client host mean that i do not need to have the --insecure-registry ec2-52-17-207-222.eu-west-1.compute.amazonaws.com:443 anymore?\n. I was unable to get it working with the placing the ca.crt inside the certs.d directory and using the insecure registry. At the end the only thing i needed to do was to import the generated certificate inside the OS. No need for other settings.\n. ",
    "maddy87": "@mpas Could you please let me know how to import the generated certificate in EC2 and make it global so docker registry can pick it up\n. ",
    "minichate": "Still seeing this when pulling the library/redis:2.8 image:\n```\n... snip\n192.168.65.120 - - [06/Aug/2015:14:31:23 +0000] \"GET /v1/images/cd896e1f93ccc82732fe365405cb686252356e36f2dac6f601dd7a5e85011f6e/json HTTP/1.1\" 404 28 \"-\" \"docker/1.6.0 go/go1.4.2 kernel/3.10.0-123.el7.x86_64 os/linux arch/amd64\"\n06/Aug/2015:14:31:24 +0000 DEBUG: args = {'image_id': u'cd896e1f93ccc82732fe365405cb686252356e36f2dac6f601dd7a5e85011f6e'}\n06/Aug/2015:14:31:24 +0000 DEBUG: Auth Token = Token signature=ead278b9411895faf41e28a37d4cbb2c7c780854,repository=\"library/redis\",access=read\n06/Aug/2015:14:31:24 +0000 DEBUG: auth = {u'access': u'read', u'repository': u'library/redis', u'signature': u'ead278b9411895faf41e28a37d4cbb2c7c780854'}\n06/Aug/2015:14:31:24 +0000 INFO: Starting new HTTPS connection (1): index.docker.io\n06/Aug/2015:14:31:24 +0000 DEBUG: \"GET /v1/repositories/library/redis/images HTTP/1.1\" 200 None\n06/Aug/2015:14:31:24 +0000 DEBUG: validate_token: Index returned 200\n06/Aug/2015:14:31:24 +0000 DEBUG: api_error: Image not found\n06/Aug/2015:14:31:24 +0000 DEBUG: Source provided, registry acts as mirror\n06/Aug/2015:14:31:24 +0000 DEBUG: Request: GET https://registry-1.docker.io/v1/images/cd896e1f93ccc82732fe365405cb686252356e36f2dac6f601dd7a5e85011f6e/json\nHeaders: {'Accept-Encoding': u'gzip', 'X-Forwarded-Port': u'80', 'X-Forwarded-For': u'10.6.1.85', 'User-Agent': u'docker/1.6.0 go/go1.4.2 kernel/3.10.0-123.el7.x86_64 os/linux arch/amd64', 'Connection': u'keep-alive', 'X-Forwarded-Proto': u'http', 'Authorization': u'Token signature=ead278b9411895faf41e28a37d4cbb2c7c780854,repository=\"library/redis\",access=read'}\nArgs: ImmutableMultiDict([])\n06/Aug/2015:14:31:24 +0000 INFO: Starting new HTTPS connection (1): registry-1.docker.io\n06/Aug/2015:14:31:25 +0000 DEBUG: \"GET /v1/images/cd896e1f93ccc82732fe365405cb686252356e36f2dac6f601dd7a5e85011f6e/json HTTP/1.1\" 400 49\n06/Aug/2015:14:31:25 +0000 DEBUG: Source responded to request with non-200 status\n06/Aug/2015:14:31:25 +0000 DEBUG: Response: 400\n{\"error\": \"Image is being uploaded, retry later\"}\n192.168.65.254 - - [06/Aug/2015:14:31:25 +0000] \"GET /v1/images/cd896e1f93ccc82732fe365405cb686252356e36f2dac6f601dd7a5e85011f6e/json HTTP/1.1\" 404 28 \"-\" \"docker/1.6.0 go/go1.4.2 kernel/3.10.0-123.el7.x86_64 os/linux arch/amd64\"\n06/Aug/2015:14:31:26 +0000 DEBUG: args = {'image_id': u'cd896e1f93ccc82732fe365405cb686252356e36f2dac6f601dd7a5e85011f6e'}\n06/Aug/2015:14:31:26 +0000 DEBUG: Auth Token = Token signature=ead278b9411895faf41e28a37d4cbb2c7c780854,repository=\"library/redis\",access=read\n06/Aug/2015:14:31:26 +0000 DEBUG: auth = {u'access': u'read', u'repository': u'library/redis', u'signature': u'ead278b9411895faf41e28a37d4cbb2c7c780854'}\n06/Aug/2015:14:31:26 +0000 INFO: Starting new HTTPS connection (1): index.docker.io\n06/Aug/2015:14:31:26 +0000 DEBUG: \"GET /v1/repositories/library/redis/images HTTP/1.1\" 200 None\n06/Aug/2015:14:31:26 +0000 DEBUG: validate_token: Index returned 200\n06/Aug/2015:14:31:26 +0000 DEBUG: api_error: Image not found\n06/Aug/2015:14:31:26 +0000 DEBUG: Source provided, registry acts as mirror\n06/Aug/2015:14:31:26 +0000 DEBUG: Request: GET https://registry-1.docker.io/v1/images/cd896e1f93ccc82732fe365405cb686252356e36f2dac6f601dd7a5e85011f6e/json\nHeaders: {'Accept-Encoding': u'gzip', 'X-Forwarded-Port': u'80', 'X-Forwarded-For': u'10.6.1.85', 'User-Agent': u'docker/1.6.0 go/go1.4.2 kernel/3.10.0-123.el7.x86_64 os/linux arch/amd64', 'Connection': u'keep-alive', 'X-Forwarded-Proto': u'http', 'Authorization': u'Token signature=ead278b9411895faf41e28a37d4cbb2c7c780854,repository=\"library/redis\",access=read'}\nArgs: ImmutableMultiDict([])\n06/Aug/2015:14:31:26 +0000 INFO: Starting new HTTPS connection (1): registry-1.docker.io\n06/Aug/2015:14:31:26 +0000 DEBUG: \"GET /v1/images/cd896e1f93ccc82732fe365405cb686252356e36f2dac6f601dd7a5e85011f6e/json HTTP/1.1\" 400 49\n06/Aug/2015:14:31:26 +0000 DEBUG: Source responded to request with non-200 status\n06/Aug/2015:14:31:26 +0000 DEBUG: Response: 400\n{\"error\": \"Image is being uploaded, retry later\"}\n192.168.65.120 - - [06/Aug/2015:14:31:26 +0000] \"GET /v1/images/cd896e1f93ccc82732fe365405cb686252356e36f2dac6f601dd7a5e85011f6e/json HTTP/1.1\" 404 28 \"-\" \"docker/1.6.0 go/go1.4.2 kernel/3.10.0-123.el7.x86_64 os/linux arch/amd64\"\n06/Aug/2015:14:31:28 +0000 DEBUG: args = {'image_id': u'cd896e1f93ccc82732fe365405cb686252356e36f2dac6f601dd7a5e85011f6e'}\n06/Aug/2015:14:31:28 +0000 DEBUG: Auth Token = Token signature=ead278b9411895faf41e28a37d4cbb2c7c780854,repository=\"library/redis\",access=read\n06/Aug/2015:14:31:28 +0000 DEBUG: auth = {u'access': u'read', u'repository': u'library/redis', u'signature': u'ead278b9411895faf41e28a37d4cbb2c7c780854'}\n06/Aug/2015:14:31:28 +0000 INFO: Starting new HTTPS connection (1): index.docker.io\n06/Aug/2015:14:31:29 +0000 DEBUG: \"GET /v1/repositories/library/redis/images HTTP/1.1\" 200 None\n06/Aug/2015:14:31:29 +0000 DEBUG: validate_token: Index returned 200\n06/Aug/2015:14:31:29 +0000 DEBUG: api_error: Image not found\n06/Aug/2015:14:31:29 +0000 DEBUG: Source provided, registry acts as mirror\n06/Aug/2015:14:31:29 +0000 DEBUG: Request: GET https://registry-1.docker.io/v1/images/cd896e1f93ccc82732fe365405cb686252356e36f2dac6f601dd7a5e85011f6e/json\nHeaders: {'Accept-Encoding': u'gzip', 'X-Forwarded-Port': u'80', 'X-Forwarded-For': u'10.6.1.85', 'User-Agent': u'docker/1.6.0 go/go1.4.2 kernel/3.10.0-123.el7.x86_64 os/linux arch/amd64', 'Connection': u'keep-alive', 'X-Forwarded-Proto': u'http', 'Authorization': u'Token signature=ead278b9411895faf41e28a37d4cbb2c7c780854,repository=\"library/redis\",access=read'}\nArgs: ImmutableMultiDict([])\n06/Aug/2015:14:31:29 +0000 INFO: Starting new HTTPS connection (1): registry-1.docker.io\n06/Aug/2015:14:31:29 +0000 DEBUG: \"GET /v1/images/cd896e1f93ccc82732fe365405cb686252356e36f2dac6f601dd7a5e85011f6e/json HTTP/1.1\" 400 49\n06/Aug/2015:14:31:29 +0000 DEBUG: Source responded to request with non-200 status\n06/Aug/2015:14:31:29 +0000 DEBUG: Response: 400\n{\"error\": \"Image is being uploaded, retry later\"}\n```\nLet me know if there are any other details I can provide!\n. ",
    "marccampbell": "I'm seeing this issue with redis:latest, but only with the docker registry v1, it works with v2.  I don't think this is a client issue, what can we do to make this image pullable in v1 again?\n. ",
    "mgireesh05": "Got answer from docker support. Hence closing this issue. \n. ",
    "evanbeard": "+1. docker registry push/pull also works locally for me but when deployed on ECS I'm seeing the same error. I'm tunneling in though ssh so the push/pull is still to localhost, but just to be safe I'm using --insecure-registry localhost:5000 in my boot2docker docker config. other docker containers on the same server can send and receive network requests through a similar ssh pipe so it seems like it's not a connectivity issue. any advice would be appreciated\n. ",
    "xuedihualu": "@olibob\uff1a\nhi\nI have the same problem,How do you solve the problem?\nthis is my logs:\ntime=\"2015-10-12T09:48:34Z\" level=warning msg=\"No HTTP secret provided - generated random secret. This may cause problems with uploads if multiple registries are behind a load-balancer. To provide a shared secret, fill in http.secret in the configuration file or set the REGISTRY_HTTP_SECRET environment variable.\" instance.id=738e2633-246e-4d53-8ed4-24addef562ee version=v2.1.\n. hi:\nI have the same problem,did you have solved the problem?\n. ",
    "tonton1728": "Hello, I'm facing the issue while migrating old registry to new registry v2\nIs it some normal behaviour or do I miss something ?\nThanks for the answer\nGilles\n. ",
    "xialingsc": "Thanks for your reply.\nThere are some information.\ndocker version\nClient version: 1.6.2\nClient API version: 1.18\nGo version (client): go1.4.2\nGit commit (client): 7c8fca2\nOS/Arch (client): linux/amd64\nServer version: 1.6.2\nServer API version: 1.18\nGo version (server): go1.4.2\nGit commit (server): 7c8fca2\nOS/Arch (server): linux/amd64\ndocker info \nContainers: 21\nImages: 65\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 107\n Dirperm1 Supported: false\nExecution Driver: native-0.2\nKernel Version: 3.13.0-24-generic\nOperating System: Ubuntu 14.04 LTS\nCPUs: 24\nTotal Memory: 62.87 GiB\nName: ubuntu\nID: U3WQ:2EO2:XO6J:ISQA:HIJ4:ICQX:SOG5:OA4T:ALZ7:LVS5:EQVA:OSFV\ndocker exec 8ba39023f45f registry -version\nregistry github.com/docker/distribution v2.0.1\n\nThere are all commands:\ndocker login -u testuser -p testpassword -e '' devregistry:5043\nEmail: \nWARNING: login credentials saved in /home/lingsc/.dockercfg.\nLogin Succeeded\ndocker tag hello-world devregistry:5043/hello\ndocker push devregistry:5043/hello\nThe push refers to a repository [devregistry:5043/hello] (len: 1)\n91c95931e552: Image push failed \nFATA[0000] Error pushing to registry: Put %5Chttps://%5Cdevregistry:5043/v2/hello/blobs/uploads/14f3024d-6581-42e1-b7a1-0f657dfcc791?_state=sXzvsWi71qbBEvM0M2-8Qh_UTq4Ll6j-pEcZjXPOjUd7Ik5hbWUiOiJoZWxsbyIsIlVVSUQiOiIxNGYzMDI0ZC02NTgxLTQyZTEtYjdhMS0wZjY1N2RmY2M3OTEiLCJPZmZzZXQiOjAsIlN0YXJ0ZWRBdCI6IjIwMTUtMDctMjhUMDA6NTc6MzguMjA2ODQ1Mjg5WiJ9&digest=sha256%3Aa3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4: unsupported protocol scheme \"\"\n. @dmp42 Thanks. I works on it referencing  on Installing Docker Registry 2.0.1 using self signed certificates,and I write a artcle for this in chinese at My Blog\n. ",
    "timbod7": "I see the same problem. This is using the s3 storage backend.\nConfiguration:\n$ docker version\nClient version: 1.7.0\nClient API version: 1.19\nGo version (client): go1.4.2\nGit commit (client): 0baf609\nOS/Arch (client): darwin/amd64\nServer version: 1.7.1\nServer API version: 1.19\nGo version (server): go1.4.2\nGit commit (server): 786b29d\nOS/Arch (server): linux/amd64\n$ docker info\nContainers: 9\nImages: 69\nStorage Driver: aufs\n Root Dir: /mnt/sda1/var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 87\n Dirperm1 Supported: true\nExecution Driver: native-0.2\nLogging Driver: json-file\nKernel Version: 4.0.7-boot2docker\nOperating System: Boot2Docker 1.7.1 (TCL 6.3); master : c202798 - Wed Jul 15 00:16:02 UTC 2015\nCPUs: 4\nTotal Memory: 1.955 GiB\nName: boot2docker\nID: 3KYP:GRLV:N37M:BLHA:OTUW:DJDW:MZCR:JELQ:X5L4:MFUI:7SFC:P46D\nDebug mode (server): true\nFile Descriptors: 11\nGoroutines: 17\nSystem Time: 2015-07-30T06:48:31.701666552Z\nEventsListeners: 0\nInit SHA1: \nInit Path: /usr/local/bin/docker\nDocker Root Dir: /mnt/sda1/var/lib/docker\n$\n$ docker exec 9d3a6428e756 registry -version\nregistry github.com/docker/distribution v2.0.1\n$\nCommands:\n$ docker tag -f 115cbcaef498 docker.XXXXXXta.com.au/XXXXXXta/python\n$ docker push docker.XXXXXXta.com.au/XXXXXXta/python:latest\nThe push refers to a repository [docker.XXXXXXta.com.au/XXXXXXta/python] (len: 1)\n115cbcaef498: Image push failed \nError pushing to registry: Put %5Chttps://%5Cdocker.XXXXXXta.com.au/v2/XXXXXXta/python/blobs/uploads/e4c91600-21ad-4ba5-9634-23648944a6bf?_state=MhAArZDANNcopXZFQryt6RbZWSu5etxxTYFGvbBo7DF7Ik5hbWUiOiJoZWxpeHRhL3B5dGhvbiIsIlVVSUQiOiJlNGM5MTYwMC0yMWFkLTRiYTUtOTYzNC0yMzY0ODk0NGE2YmYiLCJPZmZzZXQiOjAsIlN0YXJ0ZWRBdCI6IjIwMTUtMDctMzBUMDY6NTM6NDIuMDgwMTAxMTIyWiJ9&digest=sha256%3A2231bcc2a986f191b84742458a6b966f5265358682d1ac225b4956a77e48c421: unsupported protocol scheme \"\"\n$\nI can confirm this also happens with docker version 1.7.1\nAn excerpt from the docker logs seems to indicate that unnecessary escaping is occurring for some reason:\ntime=\"2015-07-30T07:18:02.310938870Z\" level=info msg=\"Daemon has completed initialization\" \ntime=\"2015-07-30T07:18:02.310969882Z\" level=info msg=\"Docker daemon\" commit=786b29d execdriver=native-0.2 graphdriver=aufs version=1.7.1 \ntime=\"2015-07-30T07:18:02.393402810Z\" level=info msg=\"Listening for HTTP on tcp (0.0.0.0:2376)\" \ntime=\"2015-07-30T07:24:27.971595211Z\" level=debug msg=\"Calling POST /images/{name:.*}/push\" \ntime=\"2015-07-30T07:24:27.971690219Z\" level=info msg=\"POST /v1.19/images/docker.XXXXXXta.com.au/XXXXXXta/python/push?tag=latest\" \ntime=\"2015-07-30T07:24:27.971709868Z\" level=debug msg=\"Warning: client and server don't have the same version (client: 1.7.0, server: 1.7.1)\" \ntime=\"2015-07-30T07:24:28.074679527Z\" level=debug msg=\"pinging registry endpoint https://docker.XXXXXXta.com.au/v0/\" \ntime=\"2015-07-30T07:24:28.074737333Z\" level=debug msg=\"attempting v2 ping for registry endpoint https://docker.XXXXXXta.com.au/v2/\" \ntime=\"2015-07-30T07:24:28.074778228Z\" level=debug msg=\"hostDir: /etc/docker/certs.d/docker.XXXXXXta.com.au\" \ntime=\"2015-07-30T07:24:28.400550951Z\" level=debug msg=\"attempting v2 ping for registry endpoint https://docker.XXXXXXta.com.au/v2/\" \ntime=\"2015-07-30T07:24:28.400585235Z\" level=debug msg=\"hostDir: /etc/docker/certs.d/docker.XXXXXXta.com.au\" \ntime=\"2015-07-30T07:24:28.713713697Z\" level=debug msg=\"pinging registry endpoint https://docker.XXXXXXta.com.au/v0/\" \ntime=\"2015-07-30T07:24:28.713782209Z\" level=debug msg=\"attempting v2 ping for registry endpoint https://docker.XXXXXXta.com.au/v2/\" \ntime=\"2015-07-30T07:24:28.713825847Z\" level=debug msg=\"hostDir: /etc/docker/certs.d/docker.XXXXXXta.com.au\" \ntime=\"2015-07-30T07:24:29.068128629Z\" level=debug msg=\"Checking latest against map[string]string{\\\"latest\\\":\\\"115cbcaef498cef26896ba46aa5473d15e7a4918b0ef5d06c26e88cf18cde6c4\\\"}\" \ntime=\"2015-07-30T07:24:29.068163164Z\" level=debug msg=\"Getting authorization for XXXXXXta/python [pull push]\" \ntime=\"2015-07-30T07:24:29.068178538Z\" level=debug msg=\"Pushing repository: docker.XXXXXXta.com.au/XXXXXXta/python:latest\" \ntime=\"2015-07-30T07:24:29.073881158Z\" level=debug msg=\"Pushing layer: 115cbcaef498cef26896ba46aa5473d15e7a4918b0ef5d06c26e88cf18cde6c4\" \ntime=\"2015-07-30T07:24:32.755598460Z\" level=debug msg=\"rendered layer for 115cbcaef498cef26896ba46aa5473d15e7a4918b0ef5d06c26e88cf18cde6c4 of [9647566] size\" \ntime=\"2015-07-30T07:24:32.755668398Z\" level=debug msg=\"[registry] Calling \\\"POST\\\" https://docker.XXXXXXta.com.au/v2/XXXXXXta/python/blobs/uploads/\" \ntime=\"2015-07-30T07:24:32.755710489Z\" level=debug msg=\"hostDir: /etc/docker/certs.d/docker.XXXXXXta.com.au\" \ntime=\"2015-07-30T07:24:33.273176481Z\" level=debug msg=\"[registry] Calling \\\"PUT\\\" \\\\https://\\\\docker.XXXXXXta.com.au/v2/XXXXXXta/python/blobs/uploads/c95b22e1-7389-4e57-83e6-7afd20a02cc3?_state=zICPaA6_jANqmOJaXi9O8tJ_0yulH\nDZA4wiN_A04WDl7Ik5hbWUiOiJoZWxpeHRhL3B5dGhvbiIsIlVVSUQiOiJjOTViMjJlMS03Mzg5LTRlNTctODNlNi03YWZkMjBhMDJjYzMiLCJPZmZzZXQiOjAsIlN0YXJ0ZWRBdCI6IjIwMTUtMDctMzBUMDc6MjQ6MzMuNzg3ODc0MDg5WiJ9\" \ntime=\"2015-07-30T07:24:33.273550948Z\" level=debug msg=\"Using cached token for XXXXXX\"\n. This issue was resolve for me by removing incorrect backslash characters from the nginx config file.\n. ",
    "deviscalio": "Hello,\nHow I can move away from registry:1 ? I'm already using image registry:2.0.1.\nHere my docker-compose file:\n\nnginx:\n  image: h3nrik/registry-ldap-auth\n  ports:\n    - \"443:443\"\n  links:\n    - registry:docker-registry\n  volumes:\n    - $PWD:/etc/ssl/docker:ro\n    - $PWD:/etc/nginx:ro\n  restart: always\nregistry:\n  image: registry:2.0.1\n  hostname: \"docker-registry\"\n  volumes:\n    - /docker_images:/tmp/registry-dev\n  ports:\n    - \"5000\"\n  restart: always\n\nMoreover your link in your response is not valid.\n. Many thanks Oliver.\nWhen you say \"... please remove the v1 sections from your configuration\", do you mean remove from nginx configuration or from registry configuration? In the second case which is this file?\n. Hello Olivier,\nI've followed the guide and I've had a successful installation of registry.\nThen, as you suggested, I've added LDAP configuration, but of course the nginx image is wrong because  I've an error : \"unknown directive \"ldap_server\".\nI've tried with h3nrik/registry-ldap-auth image but now I've this error:\nError response from daemon: no successful auth challenge for https://localhost:5043/v2/ - errors: [basic auth attempt to https://localhost:5043/v2/ realm \"Forbidden\" failed with status: 401 Unauthorized]\nI've substituted in registry.conf:\n        auth_basic \"Registry realm\";\n        auth_basic_user_file /etc/nginx/conf.d/htpasswd;\nwith\n        auth_ldap \"Forbidden\";\n        auth_ldap_servers ldapserver; \n. Hello Oliver,\nI've resolved!\nI put here the ngix.conf used, hoping it helps someone:\n\nworker_processes  1;\nevents {\n    worker_connections  1024;\n}\nhttp {\n```\nupstream docker-registry {\n    server docker-registry:5000;\n}\nldap_server ldapserver {\nurl ldap://xxxxxxxxx:389/OU=Users,DC=xxxxx,DC=it?samaccountname?sub?(objectClass=user);\n    binddn xxxxx@xxxxxxxxxx.it;\n    binddn_passwd xxxxxxxxxxxx\n    group_attribute uniquemember;\n    group_attribute_is_dn on;\n}\nserver {\nlisten 443;\nserver_name xxxxxxxxxxxxxxxxxx;\n\nerror_log /var/log/nginx/error.log debug;\naccess_log /var/log/nginx/access.log;\n\nssl on;\nssl_certificate /etc/ssl/docker/docker-registry.crt;\nssl_certificate_key /etc/ssl/docker/docker-registry.key;\n\nclient_max_body_size 0;\n\nchunked_transfer_encoding on;\n\nlocation /v2/ {\n        # Do not allow connections from docker 1.5 and earlier\n        # docker pre-1.6.0 did not properly set the user agent on ping, catch \"Go \" user agents\n        if ($http_user_agent ~ \"^(docker\\/1.(3|4|5(?!.[0-9]-dev))|Go ).$\" ) {\n                return 404;\n        }\n        auth_ldap \"Forbidden\";\n        auth_ldap_servers ldapserver; \n        add_header 'Docker-Distribution-Api-Version' 'registry/2.0' always;\n\nproxy_pass                          http://docker-registry;\n    proxy_set_header  Host              $http_host;   # required for docker client's sake\n    proxy_set_header  X-Real-IP         $remote_addr; # pass on real client's IP\n    proxy_set_header  X-Forwarded-For   $proxy_add_x_forwarded_for;\n    proxy_set_header  X-Forwarded-Proto $scheme;\n    proxy_read_timeout                  900;\n\n}\n}\n```\n}\n\nAnd docker-compose.yml used:\n\nnginx:\n  image: h3nrik/registry-ldap-auth\n  ports:\n    - \"443:443\"\n  links:\n    - registry:docker-registry\n  volumes:\n    - $PWD/certs:/etc/ssl/docker:ro\n    - $PWD/conf:/etc/nginx:ro\n  restart: always\nregistry:\n  image: registry:2.0\n  hostname: \"docker-registry\"\n  volumes:\n    - /docker_images:/tmp/registry-dev\n  ports:\n    - \"5000\"\n  restart: always\n\n. https://github.com/tierratelematics/existing-ldap-docker-registry\n. Try to connect to nginx container:\ndocker exec -it container_id /bin/bash \nand check if files are present.\n. ",
    "nagarjung": "@deviscalio : Could you please help me with the link you followed.\nI am trying to setup private registry authentication with SSL self signed certificates + LDAP(not a container, existing server ). \nBasic authentication using SSL is working fine. But unable to figure out configurations with LDAP.\n. ",
    "phucvinh52": "Hi @deviscalio \ni just have got this error. Could you tell me how to resolve this problem?\nThank you.\n. ",
    "Bart187": "Hi @deviscalio \nI'm trying to use Your image, but I got that error:\n```\n[root@D-L-TOOLS docker]# docker-compose up -d\nRecreating b06951df6d83_b06951df6d83_b06951df6d83_docker_registry_1\nERROR: for registry  No such image: sha256:34bccec547938a300eb4a3dc8161ff71ec40ccddc8341961ddcf57fcbf8a73f5\nTraceback (most recent call last):\n  File \"\", line 3, in \n  File \"compose/cli/main.py\", line 63, in main\nAttributeError: 'ProjectError' object has no attribute 'msg'\ndocker-compose returned -1\n```\nSomebody erased some of images? How can I obey it? If I check docker images I have h3nrik/registry-ldap-auth and registry 2.3.1 downloaded. The only thing which I have changed in Your files were the paths that You mentioned here:\nhttps://github.com/tierratelematics/existing-ldap-docker-registry\n\nMy docker-compose.yml file looks like this:\n```\nnginx:\n  image: h3nrik/registry-ldap-auth\n  ports:\n    - \"443:443\"\n  links:\n    - registry:docker-registry\n  volumes:\n    - $PWD/certs:/etc/ssl/docker:ro\n    - $PWD/conf:/etc/nginx:ro\n  restart: always\nregistry:\n  image: registry:2.3.1\n  hostname: \"docker-registry\"\n  environment:\n    - REGISTRY_DELETE_ENABLED=true\n  volumes:\n    - /data/docker_registry/docker:/var/lib/registry\n  ports:\n    - \"5000\"\n  restart: always\n```\n\nNginx conf looks like:\n```\nworker_processes  1;\nevents {\n    worker_connections  1024;\n}\nhttp {\nupstream docker-registry {\n    server docker-registry:5000;\n}\n\nldap_server ldapserver {\n    url \"ldaps://xxxxxxxxxxxxxxx:636/OU=xxxx,DC=xxxx,DC=local?sAMAccountName?sub?(&(memberOf:1.2.840.113556.1.4.1941:=cn=xxxxxx,OU=User_Groups,OU=Groups,OU=GLOB000,OU=Global,OU=ROOT,DC=xxxx,DC=local)(objectClass=person))\" SSL;\n    binddn \"CN=xxxxxxxxxxx,OU=Service_Accounts,OU=Admins,OU=GLOB000,OU=Global,OU=xxxx,DC=xxxxxx,DC=local\";\n    binddn_passwd xxxxxxxxxxxxxx\n    group_attribute uniquemember;\n    group_attribute_is_dn on;\n}\n\nserver {\n\n    listen 443;\n    server_name <my-server-name>;\n\n    error_log /var/log/nginx/error.log debug;\n    access_log /var/log/nginx/access.log;\n\n    ssl on;\n    ssl_certificate /etc/ssl/docker/docker-registry.crt;\n    ssl_certificate_key /etc/ssl/docker/docker-registry.key;\n\n    client_max_body_size 0;\n\n    chunked_transfer_encoding on;\n\n    location / {\n            return 301 https://<my-server-name>/v2;\n    }\n\n\n\n    location /v2/ {\n            # Do not allow connections from docker 1.5 and earlier\n            # docker pre-1.6.0 did not properly set the user agent on ping, catch \"Go *\" user agents\n            if ($http_user_agent ~ \"^(docker\\/1\\.(3|4|5(?!\\.[0-9]-dev))|Go ).*$\" ) {\n                    return 404;\n            }\n\n            auth_ldap \"Forbidden\";\n            auth_ldap_servers ldapserver;\n            add_header 'Docker-Distribution-Api-Version' 'registry/2.0' always;\n\n            proxy_pass                          http://docker-registry;\n            proxy_set_header  Host              $http_host;   # required for docker client's sake\n            proxy_set_header  X-Real-IP         $remote_addr; # pass on real client's IP\n            proxy_set_header  X-Forwarded-For   $proxy_add_x_forwarded_for;\n            proxy_set_header  X-Forwarded-Proto $scheme;\n            proxy_read_timeout                  900;\n    }\n}\n\n}\n```\n\nI have also overwritten Your certs in pwd/certs catalog with my self signed certs which were previously worked with my other private registry (without authentication). Your compose should work, but some old container could mess things up or missing? I have removed with docker rmi -f all unused images of registries and nginx servers, so docker is clean now...\nPlease help.\nBartosz\n. I found the problem (I should remove all the containers that were in memory), but another one came out. During the last part - when I try to log in to server I have:\ndocker login https://\"my_address\".local\nUsername: \"my_user\"\nPassword:\nError response from daemon: Missing client certificate domain.cert for key domain.key\nSeems like my self-signed certs are not visible for environment. But the paths are ok:\nFrom Nginx conf:\nssl_certificate /etc/ssl/docker/docker-registry.crt;\n    ssl_certificate_key /etc/ssl/docker/docker-registry.key;\nFrom docker-compose.yml:\n```\n  volumes:\n    - /data/docker_registry/docker/certs:/etc/ssl/docker:ro\n    - /data/docker_registry/docker/conf:/etc/nginx:ro\n```\n[root@D-L-TOOLS certs]# pwd\n/data/docker_registry/docker/certs\n[root@D-L-TOOLS certs]# ll\ntotal 8\n-rwx------. 1 root root 2187 Jun  1 09:56 docker-registry.crt\n-rwx------. 1 root root 3272 Jun  1 09:56 docker-registry.key\nStill there is something wrong:/\n. Hi,\nHere is an output from nginx container certificates path:\nroot@3e2e82d415bd:/etc/ssl/docker# ls -la\ntotal 12\ndrwx------. 2 root root 4096 Jun  1 06:51 .\ndrwxr-xr-x. 5 root root   63 Jun  1 12:50 ..\n-rwx------. 1 root root 2187 Jun  1 06:56 docker-registry.crt\n-rwx------. 1 root root 3272 Jun  1 06:56 docker-registry.key\nroot@3e2e82d415bd:/etc/ssl/docker# pwd\n/etc/ssl/docker\nChecked the docker-registry.crt file and it seems ok. The cert is the same as in the /data/docker_registry/docker/certs on my server.\nI saw also got that error:\nError response from daemon: Missing client certificate domain.cert for key domain.key\nwhen I login with correct and wrong credentials...\n. Got the same when I'm trying to push image to my registry:\n```\n docker push d-l-xxxxx.xxxxx.local:5000/ubuntu\n The push refers to a repository [d-l-xxxxxx.xxxxx.local:5000/ubuntu]\n Get https://d-l-xxxxx.xxxxxx.local:5000/v1/_ping: dial tcp 10.xxx.xx.xx:5000: getsockopt: connection refused\ndocker push d-l-xxxxx.xxxxxx.local/ubuntu\n Error response from daemon: Missing client certificate domain.cert for key domain.key\n```\nBut after some debugging I think I know what could be wrong:\nnginx_1     | 2016/06/01 12:50:21 [emerg] 8#0: http_auth_ldap: host not found in LDAP hostname  \"ldap.xxxxxx.com\" in /etc/nginx/nginx.conf:15\n      nginx_1     | 2016/06/01 13:02:08 [debug] 13#0: epoll add event: fd:8 op:1 ev:00002001\n      nginx_1     | 2016/06/01 13:07:08 [error] 13#0: http_auth_ldap: ldap_result() failed (-1: Can't contact LDAP server)\n      nginx_1     | 2016/06/02 06:09:48 [error] 13#0: shutdown() failed (107: Transport endpoint is not connected)\n      nginx_1     | 2016/06/02 06:09:52 [alert] 8#0: worker process 13 exited on signal 9\n      nginx_1     | 2016/06/02 06:14:00 [debug] 13#0: epoll add event: fd:8 op:1 ev:00002001\nI will check the it from LDAP side. But it seems that something should be enabled to use TLS... Any thoughts?\nI saw that on other servers (with apache and ldap configuration) there is something like that:\n```\n    \n      LDAPTrustedMode TLS\n      LDAPVerifyServerCert Off\n    \n```\nIs Nginx using TLS too (is it possible)? Can I configure it?\n. Hi Again,\nI have added to configuration two lines (where ldap.pem is CAcertificate from LDAP server):\nssl_check_cert off;\nssl_ca_file /etc/ssl/docker/ldap.pem;\nAnd now I have something like this:\ndocker-compose up\nStarting docker_registry_1\nStarting docker_nginx_1\nAttaching to docker_registry_1, docker_nginx_1\nregistry_1  | time=\"2016-06-02T09:21:26Z\" level=warning msg=\"Ignoring unrecognized environment variable REGISTRY_DELETE_ENABLED\"\nregistry_1  | time=\"2016-06-02T09:21:26Z\" level=warning msg=\"No HTTP secret provided - generated random secret. This may cause problems with uploads if multiple registries are behind a load-balancer. To provide a shared secret, fill in http.secret in the configuration file or set the REGISTRY_HTTP_SECRET environment variable.\" go.version=go1.5.3 instance.id=033e45be-ee04-46c1-acc3-f813ddde18fb version=v2.3.1\nregistry_1  | time=\"2016-06-02T09:21:26Z\" level=info msg=\"redis not configured\" go.version=go1.5.3 instance.id=033e45be-ee04-46c1-acc3-f813ddde18fb version=v2.3.1\nregistry_1  | time=\"2016-06-02T09:21:26Z\" level=info msg=\"using inmemory blob descriptor cache\" go.version=go1.5.3 instance.id=033e45be-ee04-46c1-acc3-f813ddde18fb version=v2.3.1\nregistry_1  | time=\"2016-06-02T09:21:26Z\" level=info msg=\"listening on [::]:5000\" go.version=go1.5.3 instance.id=033e45be-ee04-46c1-acc3-f813ddde18fb version=v2.3.1\nregistry_1  | time=\"2016-06-02T09:21:26Z\" level=info msg=\"Starting upload purge in 18m0s\" go.version=go1.5.3 instance.id=033e45be-ee04-46c1-acc3-f813ddde18fb version=v2.3.1\nnginx_1     | 2016/06/02 09:18:49 [debug] 13#0: epoll add event: fd:8 op:1 ev:00002001\nnginx_1     | 2016/06/02 09:21:27 [debug] 13#0: epoll add event: fd:8 op:1 ev:00002001\nBut got the same issue: Error response from daemon: Missing client certificate domain.cert for key domain.key\nWhen I use it as that:\nssl_check_cert on;\nssl_ca_file /etc/ssl/docker/ldap.pem;\nI have:\nnginx_1     | nginx: [emerg] http_auth_ldap: 'ssl_cert_check': cannot verify remote certificate's domain name because your version of OpenSSL is too old. Please install OpenSSL >= 1.02 and recompile nginx. in /etc/nginx/nginx.conf:20\nnginx_1     | 2016/06/02 09:25:23 [emerg] 10#0: http_auth_ldap: 'ssl_cert_check': cannot verify remote certificate's domain name because your version of OpenSSL is too old. Please install OpenSSL >= 1.02 and recompile nginx. in /etc/nginx/nginx.conf:20\nnginx_1     | 2016/06/02 09:25:23 [debug] 13#0: epoll add event: fd:9 op:1 ev:00002001\nBut I have downloaded and installed new openssl version:\nopenssl version\nOpenSSL 1.0.2g  1 Mar 2016\nDo I need to tell nginx to look for openssl? It's configured to be in a default path:\n    which openssl\n    /bin/openssl\nI think I'm near the end, as I don't have ldap errors any more...\n. I wanted to workaround that issue with openssl and missing certs by using:\nldap_server ldapserver {\n         url ldap://xxxxxx.xxxxx.local:389/OU=xxxx,DC=xxxxx,DC=local?sAMAccountName?sub?(&(memberOf:1.2.840.113556.1.4.1941:=cn=xxxxxxx,OU=xxxxxx,OU=xxxxxx,OU=xxxxx,OU=xxxxx,OU=xxxxx,DC=xxxxxx,DC=local)(objectClass=person));\n        binddn \"CN=xxxxxxxx,OU=xxxxxxxx,OU=Admins,OU=xxxxxxx,OU=xxxxxx,OU=xxxxx,DC=xxxxxx,DC=local\";\n        binddn_passwd xxxxxxxxxxx\n        group_attribute uniquemember;\n        group_attribute_is_dn on;\n        #ssl_check_cert on;\n        #ssl_ca_file /etc/ssl/docker/ldap.pem;\n        #ssl_ca_dir /etc/ssl/docker/;\nI have tested ldap connection with ldapserarch - it's running. Simple docker registry (without unsecure registry option) is working with that self-signed cert (also from other servers)...\nBut at the end I have been asked for certificates, no matter if I'm login with good or bad credentials:\ndocker login https://<my_domain_address>\nUsername: xxxxxx\nPassword:\nError response from daemon: Missing client certificate domain.cert for key domain.key\n. After upgrade openSSL and then downgrade to original one the error has vanished. It also seems like docker needs to have address with port on the end, because https:// is not enough. I've managed to login and push images after typing:\ndocker login https://<my_domain_address>:443\nThank You very much for Your help. My docker works! How can I make docker ask for login everytime somebody wants to push image? I have to logout everytime?\n. ",
    "jasonf20": "@dmp42 Yea of course, I get that message when trying to log in. Here is my compose:\nregistry:\n  environment:\n    REGISTRY_STORAGE: \"s3\"\n    REGISTRY_STORAGE_S3_ACCESSKEY: \"XXXX\"\n    REGISTRY_STORAGE_S3_SECRETKEY: \"XXXX\"\n    REGISTRY_STORAGE_S3_BUCKET: \"my-docker-registry\"\n    REGISTRY_STORAGE_PATH: \"/registry\"\n    REGISTRY_STORAGE_S3_REGION: \"eu-west-1\"\n    REGISTRY_SEARCH_BACKEND: \"sqlalchemy\"\n    REGISTRY_LOG_LEVEL: \"debug\"\n    REGISTRY_HTTP_TLS_CERTIFICATE: /certs/docker.crt\n    REGISTRY_HTTP_TLS_KEY: /certs/docker.key\n    REGISTRY_AUTH_HTPASSWD_REALM: \"Registry Realm\"\n    REGISTRY_AUTH_HTPASSWD_PATH: /auth/htpasswd\n  volumes:\n    - ssl:/certs\n    - auth:/auth\n  image: registry:2\n  ports:\n    - \"5000:5000\"\nAnd here are the logs:\n0-90c1-507f6b77a17d service=registry version=v2.1.0 \nregistry_1 | time=\"2015-08-12T14:35:01.665453805Z\" level=info msg=\"Starting upload purge in 45m0s\" instance.id=2db8d040-8f05-4b70-90c1-507f6b77a17d service=registry version=v2.1.0 \nregistry_1 | time=\"2015-08-12T14:35:01.780740531Z\" level=info msg=\"listening on [::]:5000, tls\" instance.id=2db8d040-8f05-4b70-90c1-507f6b77a17d service=registry version=v2.1.0 \nregistry_1 | time=\"2015-08-12T14:35:12.043408294Z\" level=debug msg=\"s3.List(\\\"/\\\")\" instance.id=2db8d040-8f05-4b70-90c1-507f6b77a17d service=registry trace.duration=358.497214ms trace.file=\"/go/src/github.com/docker/distribution/registry/storage/driver/base/base.go\" trace.func=\"github.com/docker/distribution/registry/storage/driver/base.(*Base).List\" trace.id=4f85aa6b-c9c8-4a3f-9f9d-597de744d037 trace.line=123 version=v2.1.0 \nregistry_1 | time=\"2015-08-12T14:35:13.875133769Z\" level=debug msg=\"authorizing request\" http.request.host=\"localhost:5000\" http.request.id=20854bc8-3998-453c-a0a1-19cf40eab5bb http.request.method=GET http.request.remoteaddr=\"172.17.42.1:36770\" http.request.uri=\"/v2/\" http.request.useragent=\"docker/1.7.1 go/go1.4.2 git-commit/786b29d kernel/3.16.0-38-generic os/linux arch/amd64\" instance.id=2db8d040-8f05-4b70-90c1-507f6b77a17d service=registry version=v2.1.0 \nregistry_1 | time=\"2015-08-12T14:35:13.875202246Z\" level=info msg=\"response completed\" http.request.host=\"localhost:5000\" http.request.id=20854bc8-3998-453c-a0a1-19cf40eab5bb http.request.method=GET http.request.remoteaddr=\"172.17.42.1:36770\" http.request.uri=\"/v2/\" http.request.useragent=\"docker/1.7.1 go/go1.4.2 git-commit/786b29d kernel/3.16.0-38-generic os/linux arch/amd64\" http.response.contenttype=\"application/json; charset=utf-8\" http.response.duration=1.208746ms http.response.status=200 http.response.written=2 instance.id=2db8d040-8f05-4b70-90c1-507f6b77a17d service=registry version=v2.1.0 \nregistry_1 | 172.17.42.1 - - [12/Aug/2015:14:35:13 +0000] \"GET /v2/ HTTP/1.1\" 200 2 \"\" \"docker/1.7.1 go/go1.4.2 git-commit/786b29d kernel/3.16.0-38-generic os/linux arch/amd64\"\nregistry_1 | time=\"2015-08-12T14:35:22.146202208Z\" level=debug msg=\"s3.List(\\\"/\\\")\" instance.id=2db8d040-8f05-4b70-90c1-507f6b77a17d service=registry trace.duration=461.252498ms trace.file=\"/go/src/github.com/docker/distribution/registry/storage/driver/base/base.go\" trace.func=\"github.com/docker/distribution/registry/storage/driver/base.(*Base).List\" trace.id=294c4a8a-603b-486e-9d38-dbc712c443b1 trace.line=123 version=v2.1.0 \nregistry_1 | time=\"2015-08-12T14:35:32.024947938Z\" level=debug msg=\"s3.List(\\\"/\\\")\" instance.id=2db8d040-8f05-4b70-90c1-507f6b77a17d service=registry trace.duration=339.97475ms trace.file=\"/go/src/github.com/docker/distribution/registry/storage/driver/base/base.go\" trace.func=\"github.com/docker/distribution/registry/storage/driver/base.(*Base).List\" trace.id=7f844a00-9317-4a96-ad4c-2cb91bce9326 trace.line=123 version=v2.1.0\n. Will give this a try tomorrow morning. Thanks\nOn Aug 12, 2015 23:39, \"Olivier Gambier\" notifications@github.com wrote:\n\nDocumentation is faulty.\nYou need to use:\nregistry:\n  environment:\n    REGISTRY_STORAGE: \"s3\"\n    REGISTRY_STORAGE_S3_ACCESSKEY: \"XXXX\"\n    REGISTRY_STORAGE_S3_SECRETKEY: \"XXXX\"\n    REGISTRY_STORAGE_S3_BUCKET: \"my-docker-registry\"\n    REGISTRY_STORAGE_PATH: \"/registry\"\n    REGISTRY_STORAGE_S3_REGION: \"eu-west-1\"\n    REGISTRY_SEARCH_BACKEND: \"sqlalchemy\"\n    REGISTRY_LOG_LEVEL: \"debug\"\n    REGISTRY_HTTP_TLS_CERTIFICATE: /certs/docker.crt\n    REGISTRY_HTTP_TLS_KEY: /certs/docker.key\n    REGISTRY_AUTH: \"htpasswd\"\n    REGISTRY_AUTH_HTPASSWD_REALM: \"Registry Realm\"\n    REGISTRY_AUTH_HTPASSWD_PATH: /auth/htpasswd\n  volumes:\n    - ssl:/certs\n    - auth:/auth\n  image: registry:2\n  ports:\n    - \"5000:5000\"\nNote the extra REGISTRY_AUTH: \"htpasswd\" before the other keys.\nWill fix the doc.\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/1027#issuecomment-130440258\n.\n. @dmp42 Works. Thanks\n. \n",
    "krinkere": "I did install it...\nI was able to resolve the issue by shutting everything down and recreating everything from scratch. Perhaps something went wrong the very first time when I was setting up... but yeah, not an issue. \n. ",
    "kopax": "@dmp42 , thanks for guiding me along this.\nUnfortunately, I have already checked the first link, and I have also already read more than once the second link and third link.\nRegarding my logs, it was the only config where registry process was logging something, I have no clue why nginx forward it to a port 60053 which got logged in the correct registry container.\nUsing the config you gave me from the second and third link, I was landing on a http code 502 page with nginx logging the following error :\n[error] 9#9: send() failed (111: Connection refused) while resolving, resolver: 127.0.0.1:53\n. ",
    "tdsparrow": "Wrong repo, shoule be  https://github.com/docker/distribution\n. ",
    "ashb": "That is all well and good if the registry was returning a 404 in this case but it's not \u2013 docker is being given a HTTP 403401 or  status code and it should respect that and act accordingly.\nThe only thing we are disclosing here to an unauthenticated user is that there is a registry present which is our decision to make. If docker gets told you need to log in it would certainly be much more user friendly and a time saver. I've had 6 team members come up to me over the last three days asking why the registry is broken after we introduced basic auth.\n. I can't test for certain that our v2 registry is actually returning a 401 right now but I can test on a v1 registry and that is returning a 401 and docker asks for a login. Is it just the code path for v2 that has this behaviour?\n. Confirmed - the registry gets a 401 on any url at all:\n$ curl --fail http://our-registry.tld/v2/_catalog\ncurl: (22) The requested URL returned error: 401 Unauthorized\n$ curl --fail http://our-registry.tld/v2/i-dont-exist-as-an-endpoint\ncurl: (22) The requested URL returned error: 401 Unauthorized\n. ",
    "johanhaleby": "I'm also experiencing this issue using Docker 1.8.2\n. ",
    "ahansson89": "Happens in 1.9.1 when pushing to tutum.co\n. ",
    "krish7919": "Happens with 1.9.1 here too.\n. Well I am testing the latest registry in this way:\ndocker run -d -p 5000:5000 --name dckrregistry --restart=always \\\n      -v pwd/data:/var/lib/registry \\\n      -v pwd/auth:/auth \\\n      -v pwd/certs:/certs \\\n      -e \"REGISTRY_AUTH=htpasswd\" \\\n      -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\\n      -e \"REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd\" \\\n      -e \"REGISTRY_HTTP_TLS_CERTIFICATE=/certs/test.example.com.crt\" \\\n      -e \"REGISTRY_HTTP_TLS_KEY=/certs/test.example.com.key\" \\\n      registry:2\nOne needs to create the htpasswd file & the certificates; I use self-signed\nones.\nI chucked the older registry in favor of the new one. It does the SSL\ntermination & basic authentication. It has worked well so far.\n\n\u03ba\u03c1\u03b9\u03c3h\u03bd\u03b1\u03bd\nOn Fri, Feb 5, 2016 at 8:38 PM, melvilgit notifications@github.com wrote:\n\nwhat is the resolution for this ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/1035#issuecomment-180393763\n.\n. And I think the push fails because of the API changes. If my memory serves\nright, older one used v1.18 or so, and the new one uses v1.21.\nYou can see the version in the log, when the POST request goes through.\n\n\n\u03ba\u03c1\u03b9\u03c3h\u03bd\u03b1\u03bd\nOn Fri, Feb 5, 2016 at 8:53 PM, Krish krishnan.k.iyer@gmail.com wrote:\n\nWell I am testing the latest registry in this way:\ndocker run -d -p 5000:5000 --name dckrregistry --restart=always \\\n      -v pwd/data:/var/lib/registry \\\n      -v pwd/auth:/auth \\\n      -v pwd/certs:/certs \\\n      -e \"REGISTRY_AUTH=htpasswd\" \\\n      -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\\n      -e \"REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd\" \\\n      -e \"REGISTRY_HTTP_TLS_CERTIFICATE=/certs/test.example.com.crt\" \\\n      -e \"REGISTRY_HTTP_TLS_KEY=/certs/test.example.com.key\" \\\n      registry:2\nOne needs to create the htpasswd file & the certificates; I use\nself-signed ones.\nI chucked the older registry in favor of the new one. It does the SSL\ntermination & basic authentication. It has worked well so far.\n\n\u03ba\u03c1\u03b9\u03c3h\u03bd\u03b1\u03bd\nOn Fri, Feb 5, 2016 at 8:38 PM, melvilgit notifications@github.com\nwrote:\n\nwhat is the resolution for this ?\n\u2014\nReply to this email directly or view it on GitHub\nhttps://github.com/docker/docker-registry/issues/1035#issuecomment-180393763\n.\n. \n\n",
    "melvilgit": "what is the resolution for this ?\n. iam running in insecure environment. \nthis is the error iam getting.\nThe push refers to a repository jenkins002.studio.ev1.inmobi.com:5000/myblog\nSending image list\nPushing repository jenkins002.studio.ev1.inmobi.com:5000/myblog (1 tags)\nd634beec75db: Image successfully pushed \n690ff4b30762: Image successfully pushed \n74f102c754ce: Image successfully pushed \n01bab1f9d044: Image successfully pushed \n367373dfb6e3: Image successfully pushed \nfcca578b8439: Image successfully pushed \n65acc596b170: Image successfully pushed \n63c024821ba9: Image successfully pushed \ne6cde7b59fcd: Pushing [==================================================>]    20 MB/20 MB\nFailed to upload layer: Put http://jenkins002.studio.ev1.inmobi.com:5000/v1/images/b3e7e8de188321398bf0d53249ca16822c10fe96734780202db0d249cee06652/layer: read /var/lib/docker/graph/_tmp/5881f2f77140d82dbc476f57b7d807372bfe057582e5d11cec5e399e657bc201/044932104: bad file descriptor\n. any idea about why this happens ?\n. ",
    "RandyAbernethy": "thanks for the redirect sorry for the noise...\n. ",
    "bchanan03": "me too...as I have 75 tags for same container\n. ",
    "nanachimi": "any uptade on this?. ",
    "roysG": "?. ",
    "SyCode7": "also interested in this .... ",
    "Dreampie": "docker run -d -p 8761:8761 --name eureka-server 192.168.60.41:5000/eureka-server:1.0-SNAPSHOT\nUnable to find image '192.168.60.41:5000/eureka-server:1.0-SNAPSHOT' locally\nError response from daemon: unable to ping registry endpoint https://192.168.60.41:5000/v0/\nv2 ping attempt failed with error: Get https://192.168.60.41:5000/v2/: EOF\n v1 ping attempt failed with error: Get https://192.168.60.41:5000/v1/_ping: EOF\n. ",
    "zhoujw8792": "Thanks,I will be try docker/distribution instead\n. ",
    "seedow": "Hi, i'm having the same issue when running the following command:\ndocker run -d -p 5000:5000 --restart=always --name registry   -v /certs:/certs   -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/domain.crt   -e REGISTRY_HTTP_TLS_KEY=/certs/domain.key registry:2.5.1\n\ntime=\"2016-10-10T16:26:16Z\" level=warning msg=\"No HTTP secret provided - generated random secret. This may cause problems with uploads if multiple registries are behind a load-balancer. To provide a shared secret, fill in http.secret in the configuration file or set the REGISTRY_HTTP_SECRET environment variable.\" go.version=go1.6.3 instance.id=56b82018-5c86-4430-9b22-f8d2e280a0ff version=v2.5.1\ntime=\"2016-10-10T16:26:16Z\" level=info msg=\"redis not configured\" go.version=go1.6.3 instance.id=56b82018-5c86-4430-9b22-f8d2e280a0ff version=v2.5.1\ntime=\"2016-10-10T16:26:16Z\" level=info msg=\"Starting upload purge in 44m0s\" go.version=go1.6.3 instance.id=56b82018-5c86-4430-9b22-f8d2e280a0ff version=v2.5.1\ntime=\"2016-10-10T16:26:16Z\" level=info msg=\"using inmemory blob descriptor cache\" go.version=go1.6.3 instance.id=56b82018-5c86-4430-9b22-f8d2e280a0ff version=v2.5.1\ntime=\"2016-10-10T16:26:16Z\" level=fatal msg=\"crypto/tls: failed to find any PEM data in key input\"\n. ",
    "Crayan": "Found the answer.\nAfter searching through the code, I found in docker-registry/docker_registry/tags.py, there is such a line:\n'last_update': int(time.mktime(datetime.datetime.utcnow().timetuple()))\nAs you can see, the time generated is the UTC time. I'm not sure this specific line is used to create the timestamp I saw, but I think even if it's not, they are all created in the same way, which is proved by the time gap I saw.\n. ",
    "LQBing": "sorry for that, I just copy a wrong nginx conf file\n. ",
    "Perfect-Web": "my run config is\ndocker run -dit --restart=always --net=host -v /root/auth:/auth -e REGISTRY_HTTP_SECRET=fgbhfgy45gf6gyy -e \"REGISTRY_AUTH=htpasswd\" -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd -v /root/certs:/certs:ro -e REGISTRY_HTTP_TLS_CERTIFICATE=/certs/ssl_certificate/certificate.pem -e REGISTRY_HTTP_TLS_KEY=/certs/ssl_certificate/privatekey.pem -v /var/lib/local-registry:/srv/registry -e STANDALONE=true -e MIRROR_SOURCE=https://registry-1.docker.io -e REGISTRY_REDIS_DB=3 -e MIRROR_SOURCE_INDEX=https://index.docker.io -e STORAGE_PATH=/srv/registry -e SETTINGS_FLAVOR=local --user registry:registry --name=local-registry registry:2.2.1\nyes, i read the docs\n. it works if i run as privileged\n. ",
    "zhangdakun": "upgrade docker daemon \uff0cthen not crashed so closed\n. ",
    "jmlrt": "Docker daemon logs:\nFeb 15 18:17:04 docker-1 docker[807]: time=\"2016-02-15T18:17:04.088644060Z\" level=warning msg=\"Your kernel does not support OOM notifications: There is no path for \\\"memory\\\" in state\"\nFeb 15 18:17:04 docker-1 docker[807]: time=\"2016-02-15T18:17:04.088671730Z\" level=warning msg=\"Your kernel does not support OOM notifications: There is no path for \\\"memory\\\" in state\"\nFeb 15 19:17:02 docker-1 docker[807]: time=\"2016-02-15T19:17:02.196443384Z\" level=warning msg=\"Your kernel does not support OOM notifications: There is no path for \\\"memory\\\" in state\"\nFeb 15 19:17:02 docker-1 docker[807]: time=\"2016-02-15T19:17:02.196473207Z\" level=warning msg=\"Your kernel does not support OOM notifications: There is no path for \\\"memory\\\" in state\"\nFeb 15 20:17:02 docker-1 docker[807]: time=\"2016-02-15T20:17:02.039462683Z\" level=warning msg=\"Your kernel does not support OOM notifications: There is no path for \\\"memory\\\" in state\"\nFeb 15 20:17:02 docker-1 docker[807]: time=\"2016-02-15T20:17:02.039495163Z\" level=warning msg=\"Your kernel does not support OOM notifications: There is no path for \\\"memory\\\" in state\"\nFeb 15 21:17:02 docker-1 docker[807]: time=\"2016-02-15T21:17:02.434577006Z\" level=warning msg=\"Your kernel does not support OOM notifications: There is no path for \\\"memory\\\" in state\"\nFeb 15 21:17:02 docker-1 docker[807]: time=\"2016-02-15T21:17:02.434605710Z\" level=warning msg=\"Your kernel does not support OOM notifications: There is no path for \\\"memory\\\" in state\"\nFeb 15 22:17:01 docker-1 docker[807]: time=\"2016-02-15T22:17:01.964408939Z\" level=warning msg=\"Your kernel does not support OOM notifications: There is no path for \\\"memory\\\" in state\"\nFeb 15 22:17:01 docker-1 docker[807]: time=\"2016-02-15T22:17:01.964436258Z\" level=warning msg=\"Your kernel does not support OOM notifications: There is no path for \\\"memory\\\" in state\"\n. Hi @dmp42,\nMy Docker server is used for production environment.\nI can't find a window to restart it with debug mode for now.\nI'll do it & post logs when I can\n. ",
    "vpusher": "A little more precisions:\nFirst, I get the authentication required error:\n$ docker push domain.com:5043/admin/nginx\nThe push refers to a repository [domain.com:5043/admin/nginx] (len: 1)\n1a908c5b60b5: Preparing \nunauthorized: authentication required\nThen, I create the repository through the UI, and I finally get the connection refused error:\n$ docker push domain.com:5043/admin/nginx\nThe push refers to a repository [domain.com:5043/admin/nginx] (len: 1)\n1a908c5b60b5: Pushing 1.024 kB\ndial tcp 10.235.64.84:443: connection refused\n. ",
    "aaronlehmann": "Hi @mikehaertl,\nThe new content addressable storage feature in Docker 1.10 relates to how a Docker daemon storages images locally.\nThere is a different feature in 1.10 called cross-repository push that addresses the use case you're describing. It allows the engine to prove to a registry that it has access to layers in a different repository. This allows it to skip pushing redundant copies of that layer.\nThis feature requires registry-side support, which is available in the brand new Registry 2.3. Docker Hub isn't running this new version quite yet, but once they are, pushing custom images to Hub won't require repushing base layers.\n. ",
    "harryi3t": "+1 same problem\n. Hey I solved it. Actually it was mistake on my part.\nWhile getting the token I was using \nhttps://DTR_NAME/auth/token?service=DTR_SERVICE_NAME&scope=repository:NAMESPACE/REPOSITORY:pull\nand while getting the tags, I was doing\nhttps://DTR_NAME/v2/REPOSITORY/\nJust make sure you use NAMESPACE/REPOSITORY in both places.\nHope I could help. \n. ",
    "hrobertson": "Thanks for your input but I was not making that mistake. Try this yourself:\n```\n$ curl -i https://registry.hub.docker.com/v2/docker/whalesay/tags/list\nHTTP/1.1 401 Unauthorized\nWww-Authenticate: Bearer realm=\"https://auth.docker.io/token\",service=\"registry.docker.io\",scope=\"repository:docker/whalesay:pull\"\n{\"errors\":[{\"code\":\"UNAUTHORIZED\",\"message\":\"authentication required\",\"detail\":[{\"Type\":\"repository\",\"Name\":\"docker/whalesay\",\"Action\":\"pull\"}]}]}\n$ curl -i https://auth.docker.io/token?service=registry.docker.io&scope=repository:docker/whalesay:pull\nHTTP/1.1 200 OK\n{\"token\":\"eyJhbGciOiJFUzI1N...\"}\n$ curl -i -H \"Authorization: Bearer eyJhbGciOiJFUzI1N...\" https://registry.hub.docker.com/v2/docker/whalesay/tags/list\nHTTP/1.1 401 Unauthorized\nWww-Authenticate: Bearer realm=\"https://auth.docker.io/token\",service=\"registry.docker.io\",scope=\"repository:docker/whalesay:pull\",error=\"insufficient_scope\"\n{\"errors\":[{\"code\":\"UNAUTHORIZED\",\"message\":\"authentication required\",\"detail\":[{\"Type\":\"repository\",\"Name\":\"docker/whalesay\",\"Action\":\"pull\"}]}]}\n```\n. Closing as this repo is deprecated in favour of https://github.com/docker/distribution\n. @markriggins Closed because docker/docker-registry is deprecated in favour of docker/distribution.\nIssue here: https://github.com/docker/distribution/issues/1676\nSolution is to quote the URL.\n. ",
    "markriggins": "closed but not fixed or answered?\n. ",
    "AlexFR59": "I didn't write the real error.\nThis is it :\nError response from daemon: missing signature key\nI cant find where is my problem...\n. We finally found the problem :\nApache modify the order of the RequestHeader...\nThe result is : Instead of using V2 API, the docker pull use the V1 API.\nIn order to solve this, I just add this in my Apache VitrualHost :\nRequestHeader set Accept \"application/vnd.docker.distribution.manifest.v2+json\"\n. ",
    "wusisu": "I think the problem is docker http request use Accept multi-lines in header.\nAccept: application/vnd.docker.distribution.manifest.v2+json\nAccept: application/vnd.docker.distribution.manifest.list.v2+json\nAccept: application/vnd.docker.distribution.manifest.v1+prettyjws. ",
    "noymn": "Ops! Thanks for saying that. Can you tell me how is it enabled or how can I see if that is happening? My registry is only growing whilst the number of images or layers in the images have not changed at all.\n. ",
    "kinglion811": "I think this issuse don't fit in with this issuse\uff01\n. ",
    "TheBeeMan": "in addtion, if i build private registry service on localhost, it is fine, no err occur.\n. in addtion, i found no docker messages on the host which run my private registry, seems it's nothing to do with the configuration but network between two virtual machine,.\ni followed the instructions step by step:\nhttps://github.com/docker/distribution/blob/master/docs/deploying.md#running-a-domain-registry \n. ",
    "koifans": "I ran into the same situation. Any update for this issue? @TheBeeMan \n\n[user0@vm ~]$ sudo docker login -u xx.com:5000\nPassword:\nError response from daemon: Get https://xx.com:5000/v1/users/: EOF\n\n\nOne thing I noticed is that I definitely run a v2 registry service but the error msg is still v1/user...\n. ",
    "jeusdi": "You given me a clue. I've dug out a bit and I've realised I used htpasswd command tool without -B option:\nhtpasswd -Bbn user passwd > docker-registry.htpasswd\nIt works fine now using this command:\ndocker login -u user -p passwd -e xxx@xxx.xxx localhost:5000\n\nLogin Succeeded\n\nHowever, as you can see before I've configured registry with TLS certificates.\ndocker login -u user -p passwd -e xxx@xxx.xxx https://localhost:443\nI'm comming up with this message each time:\n\nError response from daemon: Get http://localhost:443/v1/users/: read tcp [::1]:58076->[::1]:443: read: connection reset by peer\n\nregistry is listening on 443 and 5000:\n```\ndocker port registry\n443/tcp -> 0.0.0.0:443\n5000/tcp -> 0.0.0.0:5000\n```\nSo, I'm trying to connect to it using only secured connection...\n. ",
    "testzlx": "I have the same problem,please help. ",
    "aaronisme": "facing the same issue and -B is worked for me as well. but got a question about it. I have read the document for htpasswd.\n'-B' seems like use bcrypt encryption method. \nis it the only encryption method supported ?\n. ",
    "sourav82": "Sorry... Issue not with docker-registry. Regret the inconvenience.\n. ",
    "mortensteenrasmussen": "Hi @lorenzvth7 \nThis repo belongs to the old and deprecated v1 repo. The new repo is at https://github.com/docker/distribution\nAnd I believe there's already an issue about what you mention as the second issue. https://github.com/docker/distribution/issues/2094\nYou should restart your registry after doing a GC, and then you should be okay. You also need to put the registry into read-only mode when doing GC - if someone pushes while GC is running, bad things can happen. I'd personally advise you stop the registry, run GC, and then start it.\nEdit:\nIf this is no longer relevant, please close the issue :). Hi,\nThis repo is for the old and deprecated v1 repo. The new repo is https://github.com/docker/distribution\nWe had the same issue, and I figured out what's going on. Please check this issue.\nSpecifically, these two comments should help you out:\nhttps://github.com/docker/distribution/issues/2190#issuecomment-279323179\nhttps://github.com/docker/distribution/issues/2190#issuecomment-279343010\nIf you're using https or authentication on your registry, please read some of the additional comments (I changed the gist a bit from the first comment, but it should be fairly self-explanatory).\nThis should help you find and delete the manifests that don't have tags associated (which happens when you push an image:tag on top of the same image:tag in the registry.\nRun it once to clean up, and then maybe put it in a cronjob or something like that. Naturally do this at your own risk, but this has helped several other people with the same issue :). ",
    "aimer1124": "same here\n\nDocker info\n\nContainers: 2\n Running: 0\n Paused: 0\n Stopped: 2\nImages: 4\nServer Version: 1.12.6\nStorage Driver: aufs\n Root Dir: /var/lib/docker/aufs\n Backing Filesystem: extfs\n Dirs: 48\n Dirperm1 Supported: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:\n Volume: local\n Network: bridge host null overlay\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nSecurity Options: seccomp\nKernel Version: 4.4.41-moby\nOperating System: Alpine Linux v3.4\nOSType: linux\nArchitecture: x86_64\nCPUs: 4\nTotal Memory: 1.951 GiB\nName: moby\nID: WNKQ:DQBO:BNLO:CB6E:IHBZ:NCS4:NLXB:TNLP:QEZL:L5RO:SEO6:7R4P\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): true\n File Descriptors: 16\n Goroutines: 27\n System Time: 2017-01-19T03:18:44.497701556Z\n EventsListeners: 1\nRegistry: https://index.docker.io/v1/\nWARNING: No kernel memory limit support\nInsecure Registries:\n 127.0.0.0/8\n\n\ndocker hub: https://hub.docker.com/r/cirit/jmeter/\n\n\nPull \n\n\n\u279c  ~ docker pull cirit/jmeter\nUsing default tag: latest\nPulling repository docker.io/cirit/jmeter\nTag latest not found in repository docker.io/cirit/jmeter\n. ",
    "mbert": "Thank you, that looks promising indeed. Since this is the wrong repo for this bug I'm closing this now. . ",
    "jfchevrette": "@goatherder I'm having the same issue. Were you able to fix it?\n. ",
    "goatherder": "Yes, this was an embarrassing one - pagination.  I just needed to append ?n=&last=. ",
    "laurieodgers": "sorry guys I have no idea how this happened! i thought i was in a cli.... ",
    "hjdr4": "Same problem here, after deletion by tag, the image name has at least one reference.\ncurl -s 'http://repos/v2/image/tags/list'                   \n{\"name\":\"image\",\"tags\":null}\nWheras \ncurl -s 'http://repos/v2/nonexisting/tags/list'     \n{\"errors\":[{\"code\":\"NAME_UNKNOWN\",\"message\":\"repository name not known to registry\",\"detail\":{\"name\":\"nonexisting\"}}]}\n. ",
    "de1m": "+1 \nI've the same issue. I've deleted all tags and layers, but I cannot delete repository. ",
    "lcastrooliveira": "Same issue. There is a workaround here\nCheck the last comment, you have to manually delete the repository folders inside the registry volume.\n. ",
    "vitalyzhakov": "+1. I've have same issue.. ",
    "271560sj": "+1,same issue. ",
    "kenneho": "My understanding is that ACL is supported if you use token based authentication, in that the authorization service maintains an ACL. From the documentation I've found so far, I haven't found much details on how to manage these access control lists, like how do I say that \"this group of users are allowed to push to and pull from this repo\". Somehow these rules must be defined in the authorization service and enforced by the registry server, but it's still pretty unclear to me how this actually works. . ",
    "Annihil8ted": "If I push using FQDN, it continually attempts to retry until a 503 is hit. I've done the installation once over already and have produced the same errors. I also tried via enabling self-signed HTTPS and I get the same issue.. ",
    "josephholsten": "is there a reason you use [ -f ... ] form earlier but test -d ... form here?\n. Do you want [ -r ... ] here? And also when loading the defaults above?\n. is there a particular reason you want to run this as root and switch down to $USER:$GROUP? Is it so you can override the user in the defaults file? Or to allow it to bind to a low port?\nOtherwise, the normal upstart notation to change user is\nsetuid www-data\nsetgid www-data\n. ",
    "ekarlso": "It's nice to be able to do both.\n2013/7/2 Joseph Anthony Pasquale Holsten notifications@github.com\n\nIn contrib/docker-registry.conf:\n\n+\n+\n+pre-start script\n-    [ -f /etc/default/docker-registry ] && . /etc/default/docker-registry\n  +\n-    dir=$(dirname $LOG_FILE)\n-    test -d $dir || mkdir -p $dir\n  +end script\n  +\n  +\n  +script\n-    [ -f /etc/default/docker-registry ] && . /etc/default/docker-registry\n  +\n-    [ -r $REGISTRY_DIR/.venv/bin/activate ] && . $REGISTRY_DIR/.venv/bin/activate\n-    cd $REGISTRY_DIR\n-    exec gunicorn -w $NUM_WORKERS --bind=$ADDRESS --user=$USER --group=$GROUP --log-level=$LOG_LEVEL --log-file=$LOG_FILE 2>>$LOG_FILE wsgi:application\n\nis there a particular reason you want to run this as root and switch down\nto $USER:$GROUP? Is it so you can override the user in the defaults file?\nOr to allow it to bind to a low port?\nOtherwise, the normal upstart notation to change userhttp://upstart.ubuntu.com/cookbook/#setuidis\nsetuid www-data\nsetgid www-data\n\u2014\nReply to this email directly or view it on GitHubhttps://github.com/dotcloud/docker-registry/pull/19/files#r4988150\n.\n. \n",
    "jonchu": "This should only be deleted if the copy succeeds. To make this clearer, I'd rather you only did returns at the end of the function, or you did them in the ends of the ifs (which is at the end of the function)\n. Ha, I knew that. I was commenting on style but looks like i read the initial line wrong. Those are parameters in the function not part of an if statement. Nvm, carry on :p\n. "
}